```diff
diff --git a/Android.mk b/Android.mk
index f83c13cdf9..d784cf6842 100644
--- a/Android.mk
+++ b/Android.mk
@@ -28,8 +28,6 @@ include $(art_path)/build/Android.cpplint.mk
 ########################################################################
 # product rules
 
-include $(art_path)/tools/ahat/Android.mk
-
 ART_HOST_DEPENDENCIES := \
   $(ART_HOST_EXECUTABLES) \
   $(ART_HOST_DEX_DEPENDENCIES) \
@@ -276,6 +274,52 @@ build-art: build-art-target
 .PHONY: build-art-target
 build-art-target: $(TARGET_OUT_EXECUTABLES)/art $(ART_TARGET_DEPENDENCIES) $(TARGET_CORE_IMG_OUTS)
 
+TARGET_BOOT_IMAGE_SYSTEM_DIR := $(PRODUCT_OUT)/system/apex/art_boot_images
+TARGET_ART_APEX_SYSTEM := $(PRODUCT_OUT)/system/apex/com.android.art
+TARGET_BOOT_IMAGE_PROFILE := $(TARGET_ART_APEX_SYSTEM).testing/etc/boot-image.prof
+
+.PHONY: build-art-simulator-profile
+build-art-simulator-profile: $(HOST_OUT_EXECUTABLES)/profmand $(TARGET_CORE_IMG_DEX_FILES) \
+		$(PRODUCT_DEX_PREOPT_BOOT_IMAGE_PROFILE_LOCATION)
+	mkdir -p $(dir $(TARGET_BOOT_IMAGE_PROFILE))
+	# Generate a profile from the core boot jars. This allows the simulator and boot image to use a
+	# stable profile that is generated on the host.
+	$(HOST_OUT_EXECUTABLES)/profmand \
+	  --output-profile-type=boot \
+	  --create-profile-from=$(PRODUCT_DEX_PREOPT_BOOT_IMAGE_PROFILE_LOCATION) \
+	  $(foreach jar,$(TARGET_CORE_IMG_DEX_FILES),--apk=$(jar)) \
+	  $(foreach jar,$(TARGET_CORE_IMG_DEX_LOCATIONS),--dex-location=$(jar)) \
+	  --reference-profile-file=$(TARGET_BOOT_IMAGE_PROFILE)
+
+.PHONY: build-art-simulator-boot-image
+build-art-simulator-boot-image: $(HOST_OUT_EXECUTABLES)/generate-boot-image64 \
+		$(HOST_OUT_EXECUTABLES)/dex2oatd $(TARGET_CORE_IMG_DEX_FILES) build-art-simulator-profile
+	# Note: The target boot image needs to be in a trusted system directory to be used by the
+	# zygote or if -Xonly-use-system-oat-files is passed to the runtime.
+	rm -rf $(TARGET_BOOT_IMAGE_SYSTEM_DIR)
+	mkdir -p $(TARGET_BOOT_IMAGE_SYSTEM_DIR)/javalib
+	mkdir -p $(TARGET_ART_APEX_SYSTEM)/javalib
+	# Copy the core boot jars to the expected directory for generate-boot-image.
+	$(foreach i,$(call int_range_list, 1, $(words $(TARGET_CORE_IMG_JARS))), \
+	  cp $(word $(i),$(TARGET_CORE_IMG_DEX_FILES)) \
+	    $(TARGET_ART_APEX_SYSTEM)/javalib/$(word $(i),$(TARGET_CORE_IMG_JARS)).jar;)
+	# Generate a target boot image using the host dex2oat. Note: a boot image using a profile is
+	# required for certain run tests to pass.
+	$(HOST_OUT_EXECUTABLES)/generate-boot-image64 \
+	  --output-dir=$(TARGET_BOOT_IMAGE_SYSTEM_DIR)/javalib \
+	  --compiler-filter=speed-profile \
+	  --use-profile=true \
+	  --profile-file=$(TARGET_BOOT_IMAGE_PROFILE) \
+	  --dex2oat-bin=$(HOST_OUT_EXECUTABLES)/dex2oatd \
+	  --android-root=$(TARGET_OUT) \
+	  --android-root-for-location=true \
+	  --core-only=true \
+	  --instruction-set=$(TARGET_ARCH)
+
+# For simulator, build a target profile and boot image on the host.
+.PHONY: build-art-simulator
+build-art-simulator: build-art-simulator-profile build-art-simulator-boot-image
+
 PRIVATE_ART_APEX_DEPENDENCY_FILES := \
   bin/dalvikvm32 \
   bin/dalvikvm64 \
diff --git a/CPPLINT.cfg b/CPPLINT.cfg
index 71a5a26b0e..27feb34455 100644
--- a/CPPLINT.cfg
+++ b/CPPLINT.cfg
@@ -26,9 +26,15 @@ linelength=100
 # Ignore the following categories of errors, as specified by the filter:
 # (the filter settings are concatenated together)
 filter=-build/c++11
+filter=-build/c++17
 filter=-build/include
 filter=-readability/function,-readability/streams,-readability/todo
 filter=-runtime/printf,-runtime/references,-runtime/sizeof,-runtime/threadsafe_fn
 # TODO: this should be re-enabled.
 filter=-whitespace/line_length
 filter=-whitespace/braces
+filter=-whitespace/indent_namespace
+filter=-whitespace/newline
+filter=-readability/casting
+# TODO: https://github.com/cpplint/cpplint/issues/298
+filter=-readability/nolint
diff --git a/TEST_MAPPING b/TEST_MAPPING
index 095baad00c..9bb66bf864 100644
--- a/TEST_MAPPING
+++ b/TEST_MAPPING
@@ -139,9 +139,6 @@
     {
       "name": "art-run-test-053-wait-some"
     },
-    {
-      "name": "art-run-test-055-enum-performance"
-    },
     {
       "name": "art-run-test-057-math-intrinsics"
     },
@@ -1630,9 +1627,6 @@
     {
       "name": "art-run-test-053-wait-some[com.google.android.art.apex]"
     },
-    {
-      "name": "art-run-test-055-enum-performance[com.google.android.art.apex]"
-    },
     {
       "name": "art-run-test-057-math-intrinsics[com.google.android.art.apex]"
     },
@@ -3136,9 +3130,6 @@
     {
       "name": "art-run-test-053-wait-some"
     },
-    {
-      "name": "art-run-test-055-enum-performance"
-    },
     {
       "name": "art-run-test-057-math-intrinsics"
     },
@@ -4629,9 +4620,6 @@
     {
       "name": "art-run-test-053-wait-some"
     },
-    {
-      "name": "art-run-test-055-enum-performance"
-    },
     {
       "name": "art-run-test-057-math-intrinsics"
     },
diff --git a/artd/artd.cc b/artd/artd.cc
index 41309f91a6..144b783a29 100644
--- a/artd/artd.cc
+++ b/artd/artd.cc
@@ -84,6 +84,7 @@
 #include "fstab/fstab.h"
 #include "oat/oat_file_assistant.h"
 #include "oat/oat_file_assistant_context.h"
+#include "oat/sdc_file.h"
 #include "odrefresh/odrefresh.h"
 #include "path_utils.h"
 #include "profman/profman_result.h"
@@ -115,9 +116,11 @@ using ::aidl::com::android::server::art::IArtdNotification;
 using ::aidl::com::android::server::art::MergeProfileOptions;
 using ::aidl::com::android::server::art::OutputArtifacts;
 using ::aidl::com::android::server::art::OutputProfile;
+using ::aidl::com::android::server::art::OutputSecureDexMetadataCompanion;
 using ::aidl::com::android::server::art::PriorityClass;
 using ::aidl::com::android::server::art::ProfilePath;
 using ::aidl::com::android::server::art::RuntimeArtifactsPath;
+using ::aidl::com::android::server::art::SecureDexMetadataWithCompanionPaths;
 using ::aidl::com::android::server::art::VdexPath;
 using ::android::base::Basename;
 using ::android::base::Dirname;
@@ -235,6 +238,10 @@ ArtifactsLocation ArtifactsLocationToAidl(OatFileAssistant::Location location) {
       return ArtifactsLocation::NEXT_TO_DEX;
     case OatFileAssistant::Location::kLocationDm:
       return ArtifactsLocation::DM;
+    case OatFileAssistant::Location::kLocationSdmOat:
+      return ArtifactsLocation::SDM_DALVIK_CACHE;
+    case OatFileAssistant::Location::kLocationSdmOdex:
+      return ArtifactsLocation::SDM_NEXT_TO_DEX;
       // No default. All cases should be explicitly handled, or the compilation will fail.
   }
   // This should never happen. Just in case we get a non-enumerator value.
@@ -269,20 +276,18 @@ Result<void> PrepareArtifactsDir(const std::string& path, const FsPermission& fs
   return {};
 }
 
-Result<void> PrepareArtifactsDirs(const OutputArtifacts& output_artifacts,
+Result<void> PrepareArtifactsDirs(const std::string& dex_path,
+                                  const std::string& isa_str,
+                                  const FsPermission& dir_fs_permission,
                                   /*out*/ std::string* oat_dir_path) {
-  if (output_artifacts.artifactsPath.isInDalvikCache) {
-    return {};
-  }
-
   std::filesystem::path oat_path(
-      OR_RETURN(BuildArtifactsPath(output_artifacts.artifactsPath)).oat_path);
+      OR_RETURN(BuildOatPath(dex_path, isa_str, /*is_in_dalvik_cache=*/false)));
   std::filesystem::path isa_dir = oat_path.parent_path();
   std::filesystem::path oat_dir = isa_dir.parent_path();
   DCHECK_EQ(oat_dir.filename(), "oat");
 
-  OR_RETURN(PrepareArtifactsDir(oat_dir, output_artifacts.permissionSettings.dirFsPermission));
-  OR_RETURN(PrepareArtifactsDir(isa_dir, output_artifacts.permissionSettings.dirFsPermission));
+  OR_RETURN(PrepareArtifactsDir(oat_dir, dir_fs_permission));
+  OR_RETURN(PrepareArtifactsDir(isa_dir, dir_fs_permission));
   *oat_dir_path = oat_dir;
   return {};
 }
@@ -424,14 +429,11 @@ Result<File> ExtractEmbeddedProfileToFd(const std::string& dex_path) {
     return Error() << error_msg;
   }
   constexpr const char* kEmbeddedProfileEntry = "assets/art-profile/baseline.prof";
-  std::unique_ptr<ZipEntry> zip_entry(zip_archive->Find(kEmbeddedProfileEntry, &error_msg));
+  std::unique_ptr<ZipEntry> zip_entry(zip_archive->FindOrNull(kEmbeddedProfileEntry, &error_msg));
   size_t size;
   if (zip_entry == nullptr || (size = zip_entry->GetUncompressedLength()) == 0) {
-    // From system/libziparchive/zip_error.cpp.
-    constexpr const char* kEntryNotFound = "Entry not found";
-    if (error_msg != kEntryNotFound) {
-      LOG(WARNING) << ART_FORMAT(
-          "Failed to find zip entry '{}' in '{}': {}", kEmbeddedProfileEntry, dex_path, error_msg);
+    if (!error_msg.empty()) {
+      LOG(WARNING) << error_msg;
     }
     // The dex file doesn't necessarily contain a profile. This is expected.
     return File();
@@ -995,6 +997,64 @@ ndk::ScopedAStatus Artd::getDexoptNeeded(const std::string& in_dexFile,
   return ScopedAStatus::ok();
 }
 
+ndk::ScopedAStatus Artd::maybeCreateSdc(const OutputSecureDexMetadataCompanion& in_outputSdc) {
+  RETURN_FATAL_IF_PRE_REBOOT(options_);
+
+  if (in_outputSdc.permissionSettings.seContext.has_value()) {
+    // SDM files are for primary dex files.
+    return Fatal("'seContext' must be null");
+  }
+
+  std::string sdm_path = OR_RETURN_FATAL(BuildSdmPath(in_outputSdc.sdcPath));
+  std::string sdc_path = OR_RETURN_FATAL(BuildSdcPath(in_outputSdc.sdcPath));
+
+  Result<std::unique_ptr<File>> sdm_file = OpenFileForReading(sdm_path);
+  if (!sdm_file.ok()) {
+    if (sdm_file.error().code() == ENOENT) {
+      // No SDM file found. That's typical.
+      return ScopedAStatus::ok();
+    }
+    return NonFatal(sdm_file.error().message());
+  }
+  struct stat sdm_st = OR_RETURN_NON_FATAL(Fstat(*sdm_file.value()));
+
+  std::string error_msg;
+  std::unique_ptr<SdcReader> sdc_reader = SdcReader::Load(sdc_path, &error_msg);
+  if (sdc_reader != nullptr && sdc_reader->GetSdmTimestampNs() == TimeSpecToNs(sdm_st.st_mtim)) {
+    // Already has an SDC file for the SDM file.
+    return ScopedAStatus::ok();
+  }
+
+  std::string oat_dir_path;  // For restorecon, can be empty if the artifacts are in dalvik-cache.
+  if (!in_outputSdc.sdcPath.isInDalvikCache) {
+    OR_RETURN_NON_FATAL(PrepareArtifactsDirs(in_outputSdc.sdcPath.dexPath,
+                                             in_outputSdc.sdcPath.isa,
+                                             in_outputSdc.permissionSettings.dirFsPermission,
+                                             &oat_dir_path));
+
+    // Unlike the two `restorecon_` calls in `dexopt`, we only need one restorecon here because SDM
+    // files are for primary dex files, whose oat directory doesn't have an MLS label.
+    OR_RETURN_NON_FATAL(restorecon_(oat_dir_path, /*se_context=*/std::nullopt, /*recurse=*/true));
+  }
+
+  OatFileAssistantContext* ofa_context = OR_RETURN_NON_FATAL(GetOatFileAssistantContext());
+
+  std::unique_ptr<NewFile> sdc_file = OR_RETURN_NON_FATAL(
+      NewFile::Create(sdc_path, in_outputSdc.permissionSettings.fileFsPermission));
+  SdcWriter writer(File(DupCloexec(sdc_file->Fd()), sdc_file->TempPath(), /*check_usage=*/true));
+
+  writer.SetSdmTimestampNs(TimeSpecToNs(sdm_st.st_mtim));
+  writer.SetApexVersions(ofa_context->GetApexVersions());
+
+  if (!writer.Save(&error_msg)) {
+    return NonFatal(error_msg);
+  }
+
+  OR_RETURN_NON_FATAL(sdc_file->CommitOrAbandon());
+
+  return ScopedAStatus::ok();
+}
+
 ndk::ScopedAStatus Artd::dexopt(
     const OutputArtifacts& in_outputArtifacts,
     const std::string& in_dexFile,
@@ -1032,12 +1092,15 @@ ndk::ScopedAStatus Artd::dexopt(
   }
 
   std::string oat_dir_path;  // For restorecon, can be empty if the artifacts are in dalvik-cache.
-  OR_RETURN_NON_FATAL(PrepareArtifactsDirs(in_outputArtifacts, &oat_dir_path));
-
-  // First-round restorecon. artd doesn't have the permission to create files with the
-  // `apk_data_file` label, so we need to restorecon the "oat" directory first so that files will
-  // inherit `dalvikcache_data_file` rather than `apk_data_file`.
   if (!in_outputArtifacts.artifactsPath.isInDalvikCache) {
+    OR_RETURN_NON_FATAL(PrepareArtifactsDirs(in_outputArtifacts.artifactsPath.dexPath,
+                                             in_outputArtifacts.artifactsPath.isa,
+                                             in_outputArtifacts.permissionSettings.dirFsPermission,
+                                             &oat_dir_path));
+
+    // First-round restorecon. artd doesn't have the permission to create files with the
+    // `apk_data_file` label, so we need to restorecon the "oat" directory first so that files will
+    // inherit `dalvikcache_data_file` rather than `apk_data_file`.
     OR_RETURN_NON_FATAL(restorecon_(
         oat_dir_path, in_outputArtifacts.permissionSettings.seContext, /*recurse=*/true));
   }
@@ -1172,8 +1235,7 @@ ndk::ScopedAStatus Artd::dexopt(
   }
 
   AddBootImageFlags(args);
-  AddCompilerConfigFlags(
-      in_instructionSet, in_compilerFilter, in_priorityClass, in_dexoptOptions, args);
+  AddCompilerConfigFlags(in_instructionSet, in_compilerFilter, in_dexoptOptions, args);
   AddPerfConfigFlags(in_priorityClass, art_exec_args, args);
 
   // For being surfaced in crash reports on crashes.
@@ -1278,12 +1340,14 @@ ScopedAStatus Artd::createCancellationSignal(
   return ScopedAStatus::ok();
 }
 
-ScopedAStatus Artd::cleanup(const std::vector<ProfilePath>& in_profilesToKeep,
-                            const std::vector<ArtifactsPath>& in_artifactsToKeep,
-                            const std::vector<VdexPath>& in_vdexFilesToKeep,
-                            const std::vector<RuntimeArtifactsPath>& in_runtimeArtifactsToKeep,
-                            bool in_keepPreRebootStagedFiles,
-                            int64_t* _aidl_return) {
+ScopedAStatus Artd::cleanup(
+    const std::vector<ProfilePath>& in_profilesToKeep,
+    const std::vector<ArtifactsPath>& in_artifactsToKeep,
+    const std::vector<VdexPath>& in_vdexFilesToKeep,
+    const std::vector<SecureDexMetadataWithCompanionPaths>& in_SdmSdcFilesToKeep,
+    const std::vector<RuntimeArtifactsPath>& in_runtimeArtifactsToKeep,
+    bool in_keepPreRebootStagedFiles,
+    int64_t* _aidl_return) {
   RETURN_FATAL_IF_PRE_REBOOT(options_);
   std::unordered_set<std::string> files_to_keep;
   for (const ProfilePath& profile : in_profilesToKeep) {
@@ -1301,6 +1365,10 @@ ScopedAStatus Artd::cleanup(const std::vector<ProfilePath>& in_profilesToKeep,
     RETURN_FATAL_IF_ARG_IS_PRE_REBOOT(vdex, "vdexFilesToKeep");
     files_to_keep.insert(OR_RETURN_FATAL(BuildVdexPath(vdex)));
   }
+  for (const SecureDexMetadataWithCompanionPaths& sdm_sdc : in_SdmSdcFilesToKeep) {
+    files_to_keep.insert(OR_RETURN_FATAL(BuildSdmPath(sdm_sdc)));
+    files_to_keep.insert(OR_RETURN_FATAL(BuildSdcPath(sdm_sdc)));
+  }
   std::string android_data = OR_RETURN_NON_FATAL(GetAndroidDataOrError());
   std::string android_expand = OR_RETURN_NON_FATAL(GetAndroidExpandOrError());
   for (const RuntimeArtifactsPath& runtime_image_path : in_runtimeArtifactsToKeep) {
@@ -1364,6 +1432,17 @@ ScopedAStatus Artd::isInDalvikCache(const std::string& in_dexFile, bool* _aidl_r
   return NonFatal(ART_FORMAT("Fstab entries not found for '{}'", in_dexFile));
 }
 
+ScopedAStatus Artd::deleteSdmSdcFiles(const SecureDexMetadataWithCompanionPaths& in_SdmSdcPaths,
+                                      int64_t* _aidl_return) {
+  RETURN_FATAL_IF_PRE_REBOOT(options_);
+
+  std::string sdm_path = OR_RETURN_FATAL(BuildSdmPath(in_SdmSdcPaths));
+  std::string sdc_path = OR_RETURN_FATAL(BuildSdcPath(in_SdmSdcPaths));
+
+  *_aidl_return = GetSizeAndDeleteFile(sdm_path) + GetSizeAndDeleteFile(sdc_path);
+  return ScopedAStatus::ok();
+}
+
 ScopedAStatus Artd::deleteRuntimeArtifacts(const RuntimeArtifactsPath& in_runtimeArtifactsPath,
                                            int64_t* _aidl_return) {
   RETURN_FATAL_IF_PRE_REBOOT(options_);
@@ -1397,6 +1476,14 @@ ScopedAStatus Artd::getVdexFileSize(const VdexPath& in_vdexPath, int64_t* _aidl_
   return ScopedAStatus::ok();
 }
 
+ndk::ScopedAStatus Artd::getSdmFileSize(const SecureDexMetadataWithCompanionPaths& in_sdmPath,
+                                        int64_t* _aidl_return) {
+  RETURN_FATAL_IF_PRE_REBOOT(options_);
+  std::string sdm_path = OR_RETURN_FATAL(BuildSdmPath(in_sdmPath));
+  *_aidl_return = GetSize(sdm_path).value_or(0);
+  return ScopedAStatus::ok();
+}
+
 ScopedAStatus Artd::getRuntimeArtifactsSize(const RuntimeArtifactsPath& in_runtimeArtifactsPath,
                                             int64_t* _aidl_return) {
   RETURN_FATAL_IF_PRE_REBOOT(options_);
@@ -1629,7 +1716,6 @@ ScopedAStatus Artd::checkPreRebootSystemRequirements(const std::string& in_chroo
 Result<void> Artd::Start() {
   OR_RETURN(SetLogVerbosity());
   MemMap::Init();
-  Runtime::AllowPageSizeAccess();
 
   ScopedAStatus status = ScopedAStatus::fromStatus(AServiceManager_registerLazyService(
       this->asBinder().get(), options_.is_pre_reboot ? kPreRebootServiceName : kServiceName));
@@ -1795,7 +1881,6 @@ void Artd::AddBootImageFlags(/*out*/ CmdlineBuilder& args) {
 
 void Artd::AddCompilerConfigFlags(const std::string& instruction_set,
                                   const std::string& compiler_filter,
-                                  PriorityClass priority_class,
                                   const DexoptOptions& dexopt_options,
                                   /*out*/ CmdlineBuilder& args) {
   args.Add("--instruction-set=%s", instruction_set);
@@ -1807,8 +1892,6 @@ void Artd::AddCompilerConfigFlags(const std::string& instruction_set,
   args.Add("--compiler-filter=%s", compiler_filter)
       .Add("--compilation-reason=%s", dexopt_options.compilationReason);
 
-  args.AddIf(priority_class >= PriorityClass::INTERACTIVE, "--compact-dex-level=none");
-
   args.AddIfNonEmpty("--max-image-block-size=%s",
                      props_->GetOrEmpty("dalvik.vm.dex2oat-max-image-block-size"))
       .AddIfNonEmpty("--very-large-app-threshold=%s",
@@ -2009,10 +2092,25 @@ Result<void> Artd::PreRebootInitDeriveClasspath(const std::string& path) {
     return ErrnoErrorf("Failed to create '{}'", path);
   }
 
+  if (pre_reboot_build_props_ == nullptr) {
+    pre_reboot_build_props_ = std::make_unique<BuildSystemProperties>(
+        OR_RETURN(BuildSystemProperties::Create("/system/build.prop")));
+  }
+  std::string sdk_version = pre_reboot_build_props_->GetOrEmpty("ro.build.version.sdk");
+  std::string codename = pre_reboot_build_props_->GetOrEmpty("ro.build.version.codename");
+  std::string known_codenames =
+      pre_reboot_build_props_->GetOrEmpty("ro.build.version.known_codenames");
+  if (sdk_version.empty() || codename.empty() || known_codenames.empty()) {
+    return Errorf("Failed to read system properties");
+  }
+
   CmdlineBuilder args = OR_RETURN(GetArtExecCmdlineBuilder());
   args.Add("--keep-fds=%d", output->Fd())
       .Add("--")
       .Add("/apex/com.android.sdkext/bin/derive_classpath")
+      .Add("--override-device-sdk-version=%s", sdk_version)
+      .Add("--override-device-codename=%s", codename)
+      .Add("--override-device-known-codenames=%s", known_codenames)
       .Add("/proc/self/fd/%d", output->Fd());
 
   LOG(INFO) << "Running derive_classpath: " << Join(args.Get(), /*separator=*/" ");
diff --git a/artd/artd.h b/artd/artd.h
index 8ce82e0dd5..d48a209b0d 100644
--- a/artd/artd.h
+++ b/artd/artd.h
@@ -38,6 +38,7 @@
 #include "aidl/com/android/server/art/BnArtd.h"
 #include "aidl/com/android/server/art/BnArtdCancellationSignal.h"
 #include "aidl/com/android/server/art/BnArtdNotification.h"
+#include "aidl/com/android/server/art/SecureDexMetadataWithCompanionPaths.h"
 #include "android-base/result.h"
 #include "android-base/thread_annotations.h"
 #include "android-base/unique_fd.h"
@@ -138,7 +139,8 @@ class Artd : public aidl::com::android::server::art::BnArtd {
                 std::function<MountFn> mount_func = mount,
                 std::function<decltype(Restorecon)> restorecon_func = Restorecon,
                 std::optional<std::string> pre_reboot_tmp_dir = std::nullopt,
-                std::optional<std::string> init_environ_rc_path = std::nullopt)
+                std::optional<std::string> init_environ_rc_path = std::nullopt,
+                std::unique_ptr<art::tools::SystemProperties> pre_reboot_build_props = nullptr)
       : options_(std::move(options)),
         props_(std::move(props)),
         exec_utils_(std::move(exec_utils)),
@@ -148,7 +150,8 @@ class Artd : public aidl::com::android::server::art::BnArtd {
         mount_(std::move(mount_func)),
         restorecon_(std::move(restorecon_func)),
         pre_reboot_tmp_dir_(std::move(pre_reboot_tmp_dir)),
-        init_environ_rc_path_(std::move(init_environ_rc_path)) {}
+        init_environ_rc_path_(std::move(init_environ_rc_path)),
+        pre_reboot_build_props_(std::move(pre_reboot_build_props)) {}
 
   ndk::ScopedAStatus isAlive(bool* _aidl_return) override;
 
@@ -215,6 +218,10 @@ class Artd : public aidl::com::android::server::art::BnArtd {
       int32_t in_dexoptTrigger,
       aidl::com::android::server::art::GetDexoptNeededResult* _aidl_return) override;
 
+  ndk::ScopedAStatus maybeCreateSdc(
+      const aidl::com::android::server::art::OutputSecureDexMetadataCompanion& in_outputSdc)
+      override;
+
   ndk::ScopedAStatus dexopt(
       const aidl::com::android::server::art::OutputArtifacts& in_outputArtifacts,
       const std::string& in_dexFile,
@@ -238,6 +245,8 @@ class Artd : public aidl::com::android::server::art::BnArtd {
       const std::vector<aidl::com::android::server::art::ProfilePath>& in_profilesToKeep,
       const std::vector<aidl::com::android::server::art::ArtifactsPath>& in_artifactsToKeep,
       const std::vector<aidl::com::android::server::art::VdexPath>& in_vdexFilesToKeep,
+      const std::vector<aidl::com::android::server::art::SecureDexMetadataWithCompanionPaths>&
+          in_SdmSdcFilesToKeep,
       const std::vector<aidl::com::android::server::art::RuntimeArtifactsPath>&
           in_runtimeArtifactsToKeep,
       bool in_keepPreRebootStagedFiles,
@@ -247,6 +256,10 @@ class Artd : public aidl::com::android::server::art::BnArtd {
 
   ndk::ScopedAStatus isInDalvikCache(const std::string& in_dexFile, bool* _aidl_return) override;
 
+  ndk::ScopedAStatus deleteSdmSdcFiles(
+      const aidl::com::android::server::art::SecureDexMetadataWithCompanionPaths& in_sdmSdcPaths,
+      int64_t* _aidl_return) override;
+
   ndk::ScopedAStatus deleteRuntimeArtifacts(
       const aidl::com::android::server::art::RuntimeArtifactsPath& in_runtimeArtifactsPath,
       int64_t* _aidl_return) override;
@@ -258,6 +271,10 @@ class Artd : public aidl::com::android::server::art::BnArtd {
   ndk::ScopedAStatus getVdexFileSize(const aidl::com::android::server::art::VdexPath& in_vdexPath,
                                      int64_t* _aidl_return) override;
 
+  ndk::ScopedAStatus getSdmFileSize(
+      const aidl::com::android::server::art::SecureDexMetadataWithCompanionPaths& in_sdmPath,
+      int64_t* _aidl_return) override;
+
   ndk::ScopedAStatus getRuntimeArtifactsSize(
       const aidl::com::android::server::art::RuntimeArtifactsPath& in_runtimeArtifactsPath,
       int64_t* _aidl_return) override;
@@ -332,7 +349,6 @@ class Artd : public aidl::com::android::server::art::BnArtd {
 
   void AddCompilerConfigFlags(const std::string& instruction_set,
                               const std::string& compiler_filter,
-                              aidl::com::android::server::art::PriorityClass priority_class,
                               const aidl::com::android::server::art::DexoptOptions& dexopt_options,
                               /*out*/ art::tools::CmdlineBuilder& args);
 
@@ -379,6 +395,7 @@ class Artd : public aidl::com::android::server::art::BnArtd {
   const std::function<decltype(Restorecon)> restorecon_;
   const std::optional<std::string> pre_reboot_tmp_dir_;
   const std::optional<std::string> init_environ_rc_path_;
+  std::unique_ptr<art::tools::SystemProperties> pre_reboot_build_props_;
 };
 
 // A class for getting system properties from a `build.prop` file.
diff --git a/artd/artd_test.cc b/artd/artd_test.cc
index 0a06a0917d..f6eeda7e57 100644
--- a/artd/artd_test.cc
+++ b/artd/artd_test.cc
@@ -43,6 +43,7 @@
 
 #include "aidl/com/android/server/art/ArtConstants.h"
 #include "aidl/com/android/server/art/BnArtd.h"
+#include "aidl/com/android/server/art/OutputArtifacts.h"
 #include "android-base/collections.h"
 #include "android-base/errors.h"
 #include "android-base/file.h"
@@ -58,6 +59,7 @@
 #include "base/common_art_test.h"
 #include "base/macros.h"
 #include "base/pidfd.h"
+#include "base/time_utils.h"
 #include "exec_utils.h"
 #include "file_utils.h"
 #include "gmock/gmock.h"
@@ -94,6 +96,7 @@ using ::aidl::com::android::server::art::OutputProfile;
 using ::aidl::com::android::server::art::PriorityClass;
 using ::aidl::com::android::server::art::ProfilePath;
 using ::aidl::com::android::server::art::RuntimeArtifactsPath;
+using ::aidl::com::android::server::art::SecureDexMetadataWithCompanionPaths;
 using ::aidl::com::android::server::art::VdexPath;
 using ::android::base::Append;
 using ::android::base::Dirname;
@@ -130,10 +133,12 @@ using ::testing::Property;
 using ::testing::ResultOf;
 using ::testing::Return;
 using ::testing::SetArgPointee;
+using ::testing::StartsWith;
 using ::testing::StrEq;
 using ::testing::UnorderedElementsAreArray;
 using ::testing::WithArg;
 
+using PermissionSettings = OutputArtifacts::PermissionSettings;
 using PrimaryCurProfilePath = ProfilePath::PrimaryCurProfilePath;
 using PrimaryRefProfilePath = ProfilePath::PrimaryRefProfilePath;
 using TmpProfilePath = ProfilePath::TmpProfilePath;
@@ -153,10 +158,10 @@ ScopeGuard<std::function<void()>> ScopedSetLogger(android::base::LogFunction&& l
   });
 }
 
-void CheckContent(const std::string& path, const std::string& expected_content) {
+void CheckContent(const std::string& path, const Matcher<std::string>& expected_content_matcher) {
   std::string actual_content;
   ASSERT_TRUE(ReadFileToString(path, &actual_content));
-  EXPECT_EQ(actual_content, expected_content);
+  EXPECT_THAT(actual_content, expected_content_matcher);
 }
 
 void CheckOtherReadable(const std::string& path, bool expected_value) {
@@ -384,24 +389,24 @@ class ArtdTest : public CommonArtTest {
     };
     struct stat st;
     ASSERT_EQ(stat(scratch_path_.c_str(), &st), 0);
+    permission_settings_ = {
+        .dirFsPermission =
+            FsPermission{
+                .uid = static_cast<int32_t>(st.st_uid),
+                .gid = static_cast<int32_t>(st.st_gid),
+                .isOtherReadable = true,
+                .isOtherExecutable = true,
+            },
+        .fileFsPermission =
+            FsPermission{
+                .uid = static_cast<int32_t>(st.st_uid),
+                .gid = static_cast<int32_t>(st.st_gid),
+                .isOtherReadable = true,
+            },
+    };
     output_artifacts_ = OutputArtifacts{
         .artifactsPath = artifacts_path_,
-        .permissionSettings =
-            OutputArtifacts::PermissionSettings{
-                .dirFsPermission =
-                    FsPermission{
-                        .uid = static_cast<int32_t>(st.st_uid),
-                        .gid = static_cast<int32_t>(st.st_gid),
-                        .isOtherReadable = true,
-                        .isOtherExecutable = true,
-                    },
-                .fileFsPermission =
-                    FsPermission{
-                        .uid = static_cast<int32_t>(st.st_uid),
-                        .gid = static_cast<int32_t>(st.st_gid),
-                        .isOtherReadable = true,
-                    },
-            },
+        .permissionSettings = permission_settings_,
     };
     clc_1_ = GetTestDexFileName("Main");
     clc_2_ = GetTestDexFileName("Nested");
@@ -417,6 +422,12 @@ class ArtdTest : public CommonArtTest {
     dm_path_ = DexMetadataPath{.dexPath = dex_file_};
     std::filesystem::create_directories(
         std::filesystem::path(OR_FATAL(BuildFinalProfilePath(tmp_profile_path_))).parent_path());
+
+    sdm_sdc_paths_ = {
+        .dexPath = dex_file_,
+        .isa = isa_,
+        .isInDalvikCache = false,
+    };
   }
 
   void TearDown() override {
@@ -547,6 +558,7 @@ class ArtdTest : public CommonArtTest {
   std::string dex_file_;
   std::string isa_;
   ArtifactsPath artifacts_path_;
+  PermissionSettings permission_settings_;
   OutputArtifacts output_artifacts_;
   std::string clc_1_;
   std::string clc_2_;
@@ -561,6 +573,8 @@ class ArtdTest : public CommonArtTest {
   bool dex_file_other_readable_ = true;
   bool profile_other_readable_ = true;
 
+  SecureDexMetadataWithCompanionPaths sdm_sdc_paths_;
+
  private:
   void InitFilesBeforeDexopt() {
     // Required files.
@@ -701,6 +715,79 @@ TEST_F(ArtdTest, deleteArtifactsFileIsDir) {
   EXPECT_FALSE(std::filesystem::exists(oat_dir + "/b.art"));
 }
 
+TEST_F(ArtdTest, maybeCreateSdc) {
+  // Unable to create OatFileAssistantContext on host to get APEX versions.
+  TEST_DISABLED_FOR_HOST();
+
+  std::string sdm_file = OR_FAIL(BuildSdmPath(sdm_sdc_paths_));
+  std::string sdc_file = OR_FAIL(BuildSdcPath(sdm_sdc_paths_));
+  CreateFile(sdm_file);
+
+  ASSERT_STATUS_OK(artd_->maybeCreateSdc(
+      {.sdcPath = sdm_sdc_paths_, .permissionSettings = permission_settings_}));
+
+  CheckContent(sdc_file, StartsWith("sdm-timestamp-ns="));
+}
+
+TEST_F(ArtdTest, maybeCreateSdcAlreadyCreated) {
+  // Unable to create OatFileAssistantContext on host to get APEX versions.
+  TEST_DISABLED_FOR_HOST();
+
+  std::string sdm_file = OR_FAIL(BuildSdmPath(sdm_sdc_paths_));
+  std::string sdc_file = OR_FAIL(BuildSdcPath(sdm_sdc_paths_));
+  CreateFile(sdm_file);
+
+  ASSERT_STATUS_OK(artd_->maybeCreateSdc(
+      {.sdcPath = sdm_sdc_paths_, .permissionSettings = permission_settings_}));
+
+  struct stat sdc_st;
+  ASSERT_EQ(stat(sdc_file.c_str(), &sdc_st), 0);
+
+  ASSERT_STATUS_OK(artd_->maybeCreateSdc(
+      {.sdcPath = sdm_sdc_paths_, .permissionSettings = permission_settings_}));
+
+  struct stat new_sdc_st;
+  ASSERT_EQ(stat(sdc_file.c_str(), &new_sdc_st), 0);
+
+  EXPECT_EQ(TimeSpecToNs(sdc_st.st_mtim), TimeSpecToNs(new_sdc_st.st_mtim));
+}
+
+TEST_F(ArtdTest, maybeCreateSdcOutdatedTimestamp) {
+  // Unable to create OatFileAssistantContext on host to get APEX versions.
+  TEST_DISABLED_FOR_HOST();
+
+  std::string sdm_file = OR_FAIL(BuildSdmPath(sdm_sdc_paths_));
+  std::string sdc_file = OR_FAIL(BuildSdcPath(sdm_sdc_paths_));
+  CreateFile(sdm_file);
+
+  ASSERT_STATUS_OK(artd_->maybeCreateSdc(
+      {.sdcPath = sdm_sdc_paths_, .permissionSettings = permission_settings_}));
+
+  struct stat sdc_st;
+  ASSERT_EQ(stat(sdc_file.c_str(), &sdc_st), 0);
+
+  // Simulate that the SDM file is updated.
+  CreateFile(sdm_file);
+
+  ASSERT_STATUS_OK(artd_->maybeCreateSdc(
+      {.sdcPath = sdm_sdc_paths_, .permissionSettings = permission_settings_}));
+
+  struct stat new_sdc_st;
+  ASSERT_EQ(stat(sdc_file.c_str(), &new_sdc_st), 0);
+
+  // The SDC file should be updated.
+  EXPECT_LT(TimeSpecToNs(sdc_st.st_mtim), TimeSpecToNs(new_sdc_st.st_mtim));
+}
+
+TEST_F(ArtdTest, maybeCreateSdcNoSdm) {
+  std::string sdc_file = OR_FAIL(BuildSdcPath(sdm_sdc_paths_));
+
+  ASSERT_STATUS_OK(artd_->maybeCreateSdc(
+      {.sdcPath = sdm_sdc_paths_, .permissionSettings = permission_settings_}));
+
+  EXPECT_FALSE(std::filesystem::exists(sdc_file));
+}
+
 TEST_F(ArtdTest, dexopt) {
   dexopt_options_.generateAppImage = true;
 
@@ -811,7 +898,7 @@ TEST_F(ArtdTest, dexoptPriorityClassBoot) {
               DoExecAndReturnCode(WhenSplitBy("--",
                                               AllOf(Not(Contains(Flag("--set-task-profile=", _))),
                                                     Not(Contains(Flag("--set-priority=", _)))),
-                                              Contains(Flag("--compact-dex-level=", "none"))),
+                                              _),
                                   _,
                                   _))
       .WillOnce(Return(0));
@@ -825,7 +912,7 @@ TEST_F(ArtdTest, dexoptPriorityClassInteractive) {
                   WhenSplitBy("--",
                               AllOf(Contains(Flag("--set-task-profile=", "Dex2OatBootComplete")),
                                     Contains(Flag("--set-priority=", "background"))),
-                              Contains(Flag("--compact-dex-level=", "none"))),
+                              _),
                   _,
                   _))
       .WillOnce(Return(0));
@@ -839,7 +926,7 @@ TEST_F(ArtdTest, dexoptPriorityClassInteractiveFast) {
                   WhenSplitBy("--",
                               AllOf(Contains(Flag("--set-task-profile=", "Dex2OatBootComplete")),
                                     Contains(Flag("--set-priority=", "background"))),
-                              Contains(Flag("--compact-dex-level=", "none"))),
+                              _),
                   _,
                   _))
       .WillOnce(Return(0));
@@ -853,7 +940,7 @@ TEST_F(ArtdTest, dexoptPriorityClassBackground) {
                   WhenSplitBy("--",
                               AllOf(Contains(Flag("--set-task-profile=", "Dex2OatBackground")),
                                     Contains(Flag("--set-priority=", "background"))),
-                              Not(Contains(Flag("--compact-dex-level=", _)))),
+                              _),
                   _,
                   _))
       .WillOnce(Return(0));
@@ -2143,15 +2230,23 @@ TEST_F(ArtdTest, mergeProfilesWithOptionsDumpClassesAndMethods) {
   CheckContent(output_profile.profilePath.tmpPath, "dump");
 }
 
+static std::string EncodeLocationForDalvikCache(const std::string& location) {
+  std::string encoded = location.substr(/*pos=*/1);  // Remove the leading '/';
+  std::replace(encoded.begin(), encoded.end(), '/', '@');
+  return encoded;
+}
+
 class ArtdCleanupTest : public ArtdTest {
  protected:
   void SetUpForCleanup() {
     // Unmanaged files.
     CreateGcKeptFile(android_data_ + "/user_de/0/com.android.foo/1.odex");
+    CreateGcKeptFile(android_data_ + "/user_de/0/com.android.foo/1.arm64.sdm");
     CreateGcKeptFile(android_data_ + "/user_de/0/com.android.foo/oat/1.odex");
     CreateGcKeptFile(android_data_ + "/user_de/0/com.android.foo/oat/1.txt");
     CreateGcKeptFile(android_data_ + "/user_de/0/com.android.foo/oat/arm64/1.txt");
     CreateGcKeptFile(android_data_ + "/user_de/0/com.android.foo/oat/arm64/1.tmp");
+    CreateGcKeptFile(android_data_ + "/user_de/0/com.android.foo/oat/arm64/1.sdc");
 
     // Files to keep.
     CreateGcKeptFile(android_data_ + "/misc/profiles/cur/1/com.android.foo/primary.prof");
@@ -2179,6 +2274,15 @@ class ArtdCleanupTest : public ArtdTest {
                      "/123456-7890/user/1/com.android.foo/cache/oat_primary/arm64/base.art");
     CreateGcKeptFile(android_data_ +
                      "/user/0/com.android.foo/cache/not_oat_dir/oat_primary/arm64/base.art");
+    CreateGcKeptFile(android_data_ +
+                     "/app/~~fadsfgadg==/com.android.baz-fadsfgadg==/base.arm64.sdm");
+    CreateGcKeptFile(android_data_ +
+                     "/app/~~fadsfgadg==/com.android.baz-fadsfgadg==/oat/arm64/base.sdc");
+    CreateGcKeptFile(android_data_ +
+                     "/app/~~jhrwafasr==/com.android.qux-bredcweff==/base.arm64.sdm");
+    CreateGcKeptFile(android_data_ + "/dalvik-cache/arm64/" +
+                     EncodeLocationForDalvikCache(android_data_) +
+                     "@app@~~jhrwafasr==@com.android.qux-bredcweff==@base.apk@classes.sdc");
 
     // Files to remove.
     CreateGcRemovedFile(android_data_ + "/misc/profiles/ref/com.android.foo/primary.prof");
@@ -2220,6 +2324,26 @@ class ArtdCleanupTest : public ArtdTest {
                         "/user/0/com.android.foo/cache/oat_primary/arm64/different_dex.art");
     CreateGcRemovedFile(android_data_ +
                         "/user/0/com.android.foo/cache/oat_primary/different_isa/base.art");
+    CreateGcRemovedFile(android_data_ +
+                        "/app/~~fadsfgadg==/com.android.baz-fadsfgadg==/different_dex.arm64.sdm");
+    CreateGcRemovedFile(
+        android_data_ +
+        "/app/~~fadsfgadg==/com.android.baz-fadsfgadg==/oat/arm64/different_dex.sdc");
+    CreateGcRemovedFile(android_data_ +
+                        "/app/~~fadsfgadg==/com.android.baz-fadsfgadg==/base.different_isa.sdm");
+    CreateGcRemovedFile(
+        android_data_ +
+        "/app/~~fadsfgadg==/com.android.baz-fadsfgadg==/oat/different_isa/base.sdc");
+    CreateGcRemovedFile(android_data_ +
+                        "/app/~~jhrwafasr==/com.android.qux-bredcweff==/different_dex.arm64.sdm");
+    CreateGcRemovedFile(
+        android_data_ + "/dalvik-cache/arm64/" + EncodeLocationForDalvikCache(android_data_) +
+        "@app@~~jhrwafasr==@com.android.qux-bredcweff==@different_dex.apk@classes.sdc");
+    CreateGcRemovedFile(android_data_ +
+                        "/app/~~jhrwafasr==/com.android.qux-bredcweff==/base.different_isa.sdm");
+    CreateGcRemovedFile(android_data_ + "/dalvik-cache/different_isa/" +
+                        EncodeLocationForDalvikCache(android_data_) +
+                        "@app@~~jhrwafasr==@com.android.qux-bredcweff==@base.apk@classes.sdc");
   }
 
   void CreateGcRemovedFile(const std::string& path) {
@@ -2259,6 +2383,18 @@ class ArtdCleanupTest : public ArtdTest {
                               .isa = "arm64",
                               .isInDalvikCache = false}},
         },
+        {
+            SecureDexMetadataWithCompanionPaths{
+                .dexPath =
+                    android_data_ + "/app/~~fadsfgadg==/com.android.baz-fadsfgadg==/base.apk",
+                .isa = "arm64",
+                .isInDalvikCache = false},
+            SecureDexMetadataWithCompanionPaths{
+                .dexPath =
+                    android_data_ + "/app/~~jhrwafasr==/com.android.qux-bredcweff==/base.apk",
+                .isa = "arm64",
+                .isInDalvikCache = true},
+        },
         {
             RuntimeArtifactsPath{
                 .packageName = "com.android.foo", .dexPath = "/a/b/base.apk", .isa = "arm64"},
@@ -2348,6 +2484,19 @@ TEST_F(ArtdTest, isInDalvikCache) {
   EXPECT_THAT(is_in_dalvik_cache("/foo"), HasValue(true));
 }
 
+TEST_F(ArtdTest, deleteSdmSdcFiles) {
+  CreateFile(scratch_path_ + "/a/b.arm64.sdm", "**");     // 2 bytes.
+  CreateFile(scratch_path_ + "/a/oat/arm64/b.sdc", "*");  // 1 byte.
+
+  int64_t result = -1;
+  ASSERT_STATUS_OK(artd_->deleteSdmSdcFiles(
+      {.dexPath = scratch_path_ + "/a/b.apk", .isa = "arm64", .isInDalvikCache = false}, &result));
+  EXPECT_EQ(result, 2 + 1);
+
+  EXPECT_FALSE(std::filesystem::exists(scratch_path_ + "/a/b.arm64.sdm"));
+  EXPECT_FALSE(std::filesystem::exists(scratch_path_ + "/a/oat/arm64/b.sdc"));
+}
+
 TEST_F(ArtdTest, deleteRuntimeArtifacts) {
   std::vector<std::string> removed_files;
   std::vector<std::string> kept_files;
@@ -2413,6 +2562,8 @@ TEST_F(ArtdTest, deleteRuntimeArtifactsAndroidDataNotExist) {
   EXPECT_EQ(aidl_return, 0);
 }
 
+// Verifies that `deleteRuntimeArtifacts` doesn't treat "*" as a wildcard. It should either treat it
+// as a normal character in the path or reject it. The caller is never supposed to use a wildcard.
 TEST_F(ArtdTest, deleteRuntimeArtifactsSpecialChars) {
   std::vector<std::string> removed_files;
   std::vector<std::string> kept_files;
@@ -2430,25 +2581,18 @@ TEST_F(ArtdTest, deleteRuntimeArtifactsSpecialChars) {
   CreateKeptFile(android_data_ + "/user/0/com.android.foo/cache/oat_primary/arm64/base.art");
 
   CreateRemovedFile(android_data_ + "/user/0/*/cache/oat_primary/arm64/base.art");
-  CreateRemovedFile(android_data_ + "/user/0/com.android.foo/cache/oat_primary/*/base.art");
   CreateRemovedFile(android_data_ + "/user/0/com.android.foo/cache/oat_primary/arm64/*.art");
 
   int64_t aidl_return;
-  ASSERT_TRUE(
-      artd_
-          ->deleteRuntimeArtifacts({.packageName = "*", .dexPath = "/a/b/base.apk", .isa = "arm64"},
-                                   &aidl_return)
-          .isOk());
-  ASSERT_TRUE(artd_
-                  ->deleteRuntimeArtifacts(
-                      {.packageName = "com.android.foo", .dexPath = "/a/b/*.apk", .isa = "arm64"},
-                      &aidl_return)
-                  .isOk());
-  ASSERT_TRUE(artd_
-                  ->deleteRuntimeArtifacts(
-                      {.packageName = "com.android.foo", .dexPath = "/a/b/base.apk", .isa = "*"},
-                      &aidl_return)
-                  .isOk());
+  ASSERT_STATUS_OK(artd_->deleteRuntimeArtifacts(
+      {.packageName = "*", .dexPath = "/a/b/base.apk", .isa = "arm64"}, &aidl_return));
+  ASSERT_STATUS_OK(artd_->deleteRuntimeArtifacts(
+      {.packageName = "com.android.foo", .dexPath = "/a/b/*.apk", .isa = "arm64"}, &aidl_return));
+  ASSERT_FALSE(artd_
+                   ->deleteRuntimeArtifacts(
+                       {.packageName = "com.android.foo", .dexPath = "/a/b/base.apk", .isa = "*"},
+                       &aidl_return)
+                   .isOk());
 
   for (const std::string& path : removed_files) {
     EXPECT_FALSE(std::filesystem::exists(path)) << ART_FORMAT("'{}' should be removed", path);
@@ -2497,6 +2641,19 @@ TEST_F(ArtdTest, getVdexFileSize) {
   EXPECT_EQ(aidl_return, 1);
 }
 
+TEST_F(ArtdTest, getSdmFileSize) {
+  CreateFile(scratch_path_ + "/a/b.arm64.sdm", std::string(1, '*'));
+
+  int64_t aidl_return = -1;
+  ASSERT_TRUE(
+      artd_
+          ->getSdmFileSize(
+              {.dexPath = scratch_path_ + "/a/b.apk", .isa = "arm64", .isInDalvikCache = false},
+              &aidl_return)
+          .isOk());
+  EXPECT_EQ(aidl_return, 1);
+}
+
 TEST_F(ArtdTest, getRuntimeArtifactsSize) {
   CreateFile(android_data_ + "/user_de/0/com.android.foo/cache/oat_primary/arm64/base.art",
              std::string(1, '*'));
@@ -2820,6 +2977,17 @@ class ArtdPreRebootTest : public ArtdTest {
     ON_CALL(*mock_props_, GetProperty).WillByDefault(Return(""));
     auto mock_exec_utils = std::make_unique<MockExecUtils>();
     mock_exec_utils_ = mock_exec_utils.get();
+    auto mock_pre_reboot_build_props = std::make_unique<NiceMock<MockSystemProperties>>();
+    mock_pre_reboot_build_props_ = mock_pre_reboot_build_props.get();
+
+    ON_CALL(*mock_pre_reboot_build_props_, GetProperty).WillByDefault(Return(""));
+    ON_CALL(*mock_pre_reboot_build_props_, GetProperty("ro.build.version.sdk"))
+        .WillByDefault(Return("35"));
+    ON_CALL(*mock_pre_reboot_build_props_, GetProperty("ro.build.version.codename"))
+        .WillByDefault(Return("Baklava"));
+    ON_CALL(*mock_pre_reboot_build_props_, GetProperty("ro.build.version.known_codenames"))
+        .WillByDefault(Return("VanillaIceCream,Baklava"));
+
     artd_ = ndk::SharedRefBase::make<Artd>(Options{.is_pre_reboot = true},
                                            std::move(mock_props),
                                            std::move(mock_exec_utils),
@@ -2829,7 +2997,8 @@ class ArtdPreRebootTest : public ArtdTest {
                                            mock_mount_.AsStdFunction(),
                                            mock_restorecon_.AsStdFunction(),
                                            pre_reboot_tmp_dir_,
-                                           init_environ_rc_path_);
+                                           init_environ_rc_path_,
+                                           std::move(mock_pre_reboot_build_props));
 
     ON_CALL(mock_restorecon_, Call).WillByDefault(Return(Result<void>()));
 
@@ -2852,6 +3021,7 @@ class ArtdPreRebootTest : public ArtdTest {
                             const std::optional<OutputArtifacts::PermissionSettings::SeContext>&,
                             bool)>
       mock_restorecon_;
+  MockSystemProperties* mock_pre_reboot_build_props_;
 };
 
 TEST_F(ArtdPreRebootTest, preRebootInit) {
@@ -2871,7 +3041,11 @@ TEST_F(ArtdPreRebootTest, preRebootInit) {
                   AllOf(WhenSplitBy("--",
                                     AllOf(Contains(art_root_ + "/bin/art_exec"),
                                           Contains("--drop-capabilities")),
-                                    Contains("/apex/com.android.sdkext/bin/derive_classpath")),
+                                    AllOf(Contains("/apex/com.android.sdkext/bin/derive_classpath"),
+                                          Contains(Flag("--override-device-sdk-version=", "35")),
+                                          Contains(Flag("--override-device-codename=", "Baklava")),
+                                          Contains(Flag("--override-device-known-codenames=",
+                                                        "VanillaIceCream,Baklava")))),
                         HasKeepFdsFor("/proc/self/fd/")),
                   _,
                   _))
diff --git a/artd/binder/com/android/server/art/ArtifactsLocation.aidl b/artd/binder/com/android/server/art/ArtifactsLocation.aidl
index 1084456dec..bb1d505c29 100644
--- a/artd/binder/com/android/server/art/ArtifactsLocation.aidl
+++ b/artd/binder/com/android/server/art/ArtifactsLocation.aidl
@@ -27,4 +27,15 @@ enum ArtifactsLocation {
     NEXT_TO_DEX = 2,
     /** In the dex metadata file. This means the only usable artifact is the VDEX file. */
     DM = 3,
+    /**
+     * The OAT and ART files are in the SDM file next to the dex file. The VDEX file is in the DM
+     * file next to the dex file. The SDC file is in the global "dalvik-cache" folder. (This happens
+     * typically when the app is in incremental-fs.)
+     */
+    SDM_DALVIK_CACHE = 4,
+    /**
+     * The OAT and ART files are in the SDM file next to the dex file. The VDEX file is in the DM
+     * file next to the dex file. The SDC file is next to the dex file.
+     */
+    SDM_NEXT_TO_DEX = 5,
 }
diff --git a/artd/binder/com/android/server/art/IArtd.aidl b/artd/binder/com/android/server/art/IArtd.aidl
index 7e9cf9da58..705076d0a6 100644
--- a/artd/binder/com/android/server/art/IArtd.aidl
+++ b/artd/binder/com/android/server/art/IArtd.aidl
@@ -152,6 +152,15 @@ interface IArtd {
             @nullable @utf8InCpp String classLoaderContext, @utf8InCpp String compilerFilter,
             int dexoptTrigger);
 
+    /**
+     * Creates a secure dex metadata companion (SDC) file for the secure dex metadata (SDM) file, if
+     * the SDM file exists while the SDC file doesn't exist (meaning the SDM file is seen the first
+     * time).
+     *
+     * Throws fatal and non-fatal errors.
+     */
+    void maybeCreateSdc(in com.android.server.art.OutputSecureDexMetadataCompanion outputSdc);
+
     /**
      * Dexopts a dex file for the given instruction set.
      *
@@ -200,6 +209,7 @@ interface IArtd {
     long cleanup(in List<com.android.server.art.ProfilePath> profilesToKeep,
             in List<com.android.server.art.ArtifactsPath> artifactsToKeep,
             in List<com.android.server.art.VdexPath> vdexFilesToKeep,
+            in List<com.android.server.art.SecureDexMetadataWithCompanionPaths> SdmSdcFilesToKeep,
             in List<com.android.server.art.RuntimeArtifactsPath> runtimeArtifactsToKeep,
             boolean keepPreRebootStagedFiles);
 
@@ -220,6 +230,16 @@ interface IArtd {
      */
     boolean isInDalvikCache(@utf8InCpp String dexFile);
 
+    /**
+     * Deletes the SDM and SDC files and returns the released space, in bytes.
+     *
+     * Not supported in Pre-reboot Dexopt mode.
+     *
+     * Throws fatal errors. Logs and ignores non-fatal errors.
+     */
+    long deleteSdmSdcFiles(
+            in com.android.server.art.SecureDexMetadataWithCompanionPaths sdmSdcPaths);
+
     /**
      * Deletes runtime artifacts and returns the released space, in bytes.
      *
@@ -250,6 +270,16 @@ interface IArtd {
      */
     long getVdexFileSize(in com.android.server.art.VdexPath vdexPath);
 
+    /**
+     * Returns the size of the SDM file, in bytes, or 0 if it doesn't exist or a non-fatal error
+     * occurred.
+     *
+     * Not supported in Pre-reboot Dexopt mode.
+     *
+     * Throws fatal errors. Logs and ignores non-fatal errors.
+     */
+    long getSdmFileSize(in com.android.server.art.SecureDexMetadataWithCompanionPaths sdmPath);
+
     /**
      * Returns the size of the runtime artifacts, in bytes, or 0 if they don't exist or a non-fatal
      * error occurred.
diff --git a/artd/binder/com/android/server/art/OutputSecureDexMetadataCompanion.aidl b/artd/binder/com/android/server/art/OutputSecureDexMetadataCompanion.aidl
new file mode 100644
index 0000000000..df5c84df7f
--- /dev/null
+++ b/artd/binder/com/android/server/art/OutputSecureDexMetadataCompanion.aidl
@@ -0,0 +1,30 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.android.server.art;
+
+/**
+ * Represents output secure dex metadata companion (SDC) file.
+ *
+ * @hide
+ */
+parcelable OutputSecureDexMetadataCompanion {
+    /** The path to the output. */
+    com.android.server.art.SecureDexMetadataWithCompanionPaths sdcPath;
+
+    /** The permissions settings of the output. */
+    com.android.server.art.OutputArtifacts.PermissionSettings permissionSettings;
+}
diff --git a/artd/binder/com/android/server/art/SecureDexMetadataWithCompanionPaths.aidl b/artd/binder/com/android/server/art/SecureDexMetadataWithCompanionPaths.aidl
new file mode 100644
index 0000000000..db0571100e
--- /dev/null
+++ b/artd/binder/com/android/server/art/SecureDexMetadataWithCompanionPaths.aidl
@@ -0,0 +1,39 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.android.server.art;
+
+/**
+ * Represents the paths to a secure dex metadata (SDM) file and its companion (SDC) file.
+ *
+ * @hide
+ */
+parcelable SecureDexMetadataWithCompanionPaths {
+    /**
+     * The absolute path starting with '/' to the dex file that the SDM file is next to.
+     */
+    @utf8InCpp String dexPath;
+    /** The instruction set of the dexopt artifacts. */
+    @utf8InCpp String isa;
+    /**
+     * Whether the SDC file in the dalvik-cache folder. This is true typically when the app is in
+     * incremental-fs.
+     *
+     * Only applicable to the SDC file, because the SDM file is installed with the app and therefore
+     * always to the dex file regardlessly.
+     */
+    boolean isInDalvikCache;
+}
diff --git a/artd/path_utils.cc b/artd/path_utils.cc
index 52bae7097e..0f269e2dbb 100644
--- a/artd/path_utils.cc
+++ b/artd/path_utils.cc
@@ -45,6 +45,7 @@ using ::aidl::com::android::server::art::OutputArtifacts;
 using ::aidl::com::android::server::art::OutputProfile;
 using ::aidl::com::android::server::art::ProfilePath;
 using ::aidl::com::android::server::art::RuntimeArtifactsPath;
+using ::aidl::com::android::server::art::SecureDexMetadataWithCompanionPaths;
 using ::aidl::com::android::server::art::VdexPath;
 using ::android::base::Error;
 using ::android::base::Result;
@@ -107,6 +108,7 @@ std::vector<std::string> ListManagedFiles(const std::string& android_data,
   for (const std::string& data_root : {android_data, android_expand + "/*"}) {
     // Artifacts for primary dex files.
     patterns.push_back(data_root + "/app/*/*/oat/**");
+    patterns.push_back(data_root + "/app/*/*/*.sdm");
 
     for (const char* user_dir : {"/user", "/user_de"}) {
       std::string data_dir = data_root + user_dir + "/*/*";
@@ -148,9 +150,17 @@ std::vector<std::string> ListRuntimeArtifactsFiles(
   return tools::Glob(patterns, gListRootDir);
 }
 
+static Result<InstructionSet> ValidateAndGetIsa(const std::string& isa_str) {
+  InstructionSet isa = GetInstructionSetFromString(isa_str.c_str());
+  if (isa == InstructionSet::kNone) {
+    return Errorf("Instruction set '{}' is invalid", isa_str);
+  }
+  return isa;
+}
+
 Result<void> ValidateRuntimeArtifactsPath(const RuntimeArtifactsPath& runtime_artifacts_path) {
   OR_RETURN(ValidatePathElement(runtime_artifacts_path.packageName, "packageName"));
-  OR_RETURN(ValidatePathElement(runtime_artifacts_path.isa, "isa"));
+  OR_RETURN(ValidateAndGetIsa(runtime_artifacts_path.isa));
   OR_RETURN(ValidateDexPath(runtime_artifacts_path.dexPath));
   return {};
 }
@@ -159,32 +169,36 @@ Result<std::string> BuildArtBinPath(const std::string& binary_name) {
   return ART_FORMAT("{}/bin/{}", OR_RETURN(GetArtRootOrError()), binary_name);
 }
 
-Result<RawArtifactsPath> BuildArtifactsPath(const ArtifactsPath& artifacts_path) {
-  OR_RETURN(ValidateDexPath(artifacts_path.dexPath));
-
-  InstructionSet isa = GetInstructionSetFromString(artifacts_path.isa.c_str());
-  if (isa == InstructionSet::kNone) {
-    return Errorf("Instruction set '{}' is invalid", artifacts_path.isa);
-  }
+Result<std::string> BuildOatPath(const std::string& dex_path,
+                                 const std::string& isa_str,
+                                 bool is_in_dalvik_cache) {
+  OR_RETURN(ValidateDexPath(dex_path));
+  InstructionSet isa = OR_RETURN(ValidateAndGetIsa(isa_str));
 
+  std::string oat_path;
   std::string error_msg;
-  RawArtifactsPath path;
-  if (artifacts_path.isInDalvikCache) {
+  if (is_in_dalvik_cache) {
     // Apps' OAT files are never in ART APEX data.
-    if (!OatFileAssistant::DexLocationToOatFilename(artifacts_path.dexPath,
+    if (!OatFileAssistant::DexLocationToOatFilename(dex_path,
                                                     isa,
                                                     /*deny_art_apex_data_files=*/true,
-                                                    &path.oat_path,
+                                                    &oat_path,
                                                     &error_msg)) {
-      return Error() << error_msg;
+      return Errorf("{}", error_msg);
     }
   } else {
-    if (!OatFileAssistant::DexLocationToOdexFilename(
-            artifacts_path.dexPath, isa, &path.oat_path, &error_msg)) {
-      return Error() << error_msg;
+    if (!OatFileAssistant::DexLocationToOdexFilename(dex_path, isa, &oat_path, &error_msg)) {
+      return Errorf("{}", error_msg);
     }
   }
 
+  return oat_path;
+}
+
+Result<RawArtifactsPath> BuildArtifactsPath(const ArtifactsPath& artifacts_path) {
+  RawArtifactsPath path;
+  path.oat_path = OR_RETURN(
+      BuildOatPath(artifacts_path.dexPath, artifacts_path.isa, artifacts_path.isInDalvikCache));
   path.vdex_path = ReplaceFileExtension(path.oat_path, kVdexExtension);
   path.art_path = ReplaceFileExtension(path.oat_path, kArtExtension);
 
@@ -303,6 +317,19 @@ Result<std::string> BuildVdexPath(const VdexPath& vdex_path) {
   return OR_RETURN(BuildArtifactsPath(vdex_path.get<VdexPath::artifactsPath>())).vdex_path;
 }
 
+Result<std::string> BuildSdmPath(const SecureDexMetadataWithCompanionPaths& sdm_path) {
+  // `sdm_path.isInDalvikCache` is intentionally ignored because it's only applicable to SDC files.
+  OR_RETURN(ValidateDexPath(sdm_path.dexPath));
+  OR_RETURN(ValidateAndGetIsa(sdm_path.isa));
+  return ReplaceFileExtension(sdm_path.dexPath, ART_FORMAT(".{}{}", sdm_path.isa, kSdmExtension));
+}
+
+Result<std::string> BuildSdcPath(const SecureDexMetadataWithCompanionPaths& sdc_path) {
+  std::string oat_path =
+      OR_RETURN(BuildOatPath(sdc_path.dexPath, sdc_path.isa, sdc_path.isInDalvikCache));
+  return ReplaceFileExtension(oat_path, ".sdc");
+}
+
 bool PreRebootFlag(const ProfilePath& profile_path) {
   switch (profile_path.getTag()) {
     case ProfilePath::primaryRefProfilePath:
diff --git a/artd/path_utils.h b/artd/path_utils.h
index 1528d0610b..e31115683b 100644
--- a/artd/path_utils.h
+++ b/artd/path_utils.h
@@ -55,6 +55,10 @@ android::base::Result<void> ValidateRuntimeArtifactsPath(
 
 android::base::Result<std::string> BuildArtBinPath(const std::string& binary_name);
 
+android::base::Result<std::string> BuildOatPath(const std::string& dex_path,
+                                                const std::string& isa_str,
+                                                bool is_in_dalvik_cache);
+
 // Returns the absolute paths to files built from the `ArtifactsPath`.
 android::base::Result<RawArtifactsPath> BuildArtifactsPath(
     const aidl::com::android::server::art::ArtifactsPath& artifacts_path);
@@ -96,6 +100,12 @@ android::base::Result<std::string> BuildProfileOrDmPath(
 android::base::Result<std::string> BuildVdexPath(
     const aidl::com::android::server::art::VdexPath& vdex_path);
 
+android::base::Result<std::string> BuildSdmPath(
+    const aidl::com::android::server::art::SecureDexMetadataWithCompanionPaths& sdm_path);
+
+android::base::Result<std::string> BuildSdcPath(
+    const aidl::com::android::server::art::SecureDexMetadataWithCompanionPaths& sdc_path);
+
 // Takes an argument of type `WritableProfilePath`. Returns the pre-reboot flag by value if the
 // argument is const, or by reference otherwise.
 template <typename T,
diff --git a/artd/path_utils_test.cc b/artd/path_utils_test.cc
index 116177a7a7..8b50ca3549 100644
--- a/artd/path_utils_test.cc
+++ b/artd/path_utils_test.cc
@@ -255,6 +255,21 @@ TEST_F(PathUtilsTest, BuildVdexPath) {
       HasValue("/a/oat/arm64/b.vdex"));
 }
 
+TEST_F(PathUtilsTest, BuildSdmPath) {
+  EXPECT_THAT(BuildSdmPath({.dexPath = "/a/b.apk", .isa = "arm64", .isInDalvikCache = false}),
+              HasValue("/a/b.arm64.sdm"));
+}
+
+TEST_F(PathUtilsTest, BuildSdcPath) {
+  EXPECT_THAT(BuildSdcPath({.dexPath = "/a/b.apk", .isa = "arm64", .isInDalvikCache = false}),
+              HasValue("/a/oat/arm64/b.sdc"));
+}
+
+TEST_F(PathUtilsTest, BuildSdcPathDalvikCache) {
+  EXPECT_THAT(BuildSdcPath({.dexPath = "/a/b.apk", .isa = "arm64", .isInDalvikCache = true}),
+              HasValue(android_data_ + "/dalvik-cache/arm64/a@b.apk@classes.sdc"));
+}
+
 }  // namespace
 }  // namespace artd
 }  // namespace art
diff --git a/build/Android.bp b/build/Android.bp
index e2d724b99e..17f6891c1b 100644
--- a/build/Android.bp
+++ b/build/Android.bp
@@ -177,13 +177,6 @@ art_global_defaults {
             // `--exclude-libs` flag is not supported on windows/darwin.
             ldflags: ["-Wl,--exclude-libs=libziparchive.a"],
         },
-        linux_bionic: {
-            strip: {
-                // Do not strip art libs when building for linux-bionic.
-                // Otherwise we can't get any symbols out of crashes.
-                none: true,
-            },
-        },
         darwin: {
             enabled: false,
         },
@@ -200,6 +193,10 @@ art_global_defaults {
                 // clang/libunwind bugs that cause SEGVs in run-test-004-ThreadStress.
                 "-fno-omit-frame-pointer",
             ],
+            // Keep the symbols for host to symbolize crash stack traces.
+            strip: {
+                none: true,
+            },
         },
         // The build assumes that all our x86/x86_64 hosts (such as buildbots and developer
         // desktops) support at least sse4.2/popcount. This firstly implies that the ART
@@ -362,3 +359,16 @@ java_library {
         },
     },
 }
+
+// A version of core-icu4j only for the ART fuzzer.
+java_library {
+    name: "core-icu4j-fuzzer",
+    visibility: [
+        "//art/tools/fuzzer",
+    ],
+    static_libs: ["core-icu4j-for-host"],
+    stem: "core-icu4j",
+    compile_dex: true,
+    sdk_version: "none",
+    system_modules: "none",
+}
diff --git a/build/Android.common_build.mk b/build/Android.common_build.mk
index f5a95fa0cf..ad551a13e9 100644
--- a/build/Android.common_build.mk
+++ b/build/Android.common_build.mk
@@ -46,8 +46,13 @@ ifeq ($(ART_BUILD_HOST_DEBUG),false)
 $(info Disabling ART_BUILD_HOST_DEBUG)
 endif
 
+ifeq ($(ART_USE_RESTRICTED_MODE),true)
+# TODO(Simulator): Support read barriers.
+ART_USE_READ_BARRIER := false
+else
 # Enable the read barrier by default.
 ART_USE_READ_BARRIER ?= true
+endif
 
 ART_CPP_EXTENSION := .cc
 
diff --git a/build/apex/Android.bp b/build/apex/Android.bp
index 43423f35e6..f91b7863ee 100644
--- a/build/apex/Android.bp
+++ b/build/apex/Android.bp
@@ -74,9 +74,6 @@ apex_defaults {
     bootclasspath_fragments: ["art-bootclasspath-fragment"],
     systemserverclasspath_fragments: ["art-systemserverclasspath-fragment"],
     compat_configs: ["libcore-platform-compat-config"],
-    required: [
-        "com.android.i18n",
-    ],
     prebuilts: [
         "art-linker-config",
         "com.android.art.init.rc",
@@ -223,7 +220,11 @@ apex_test {
     native_shared_libs: ["libart"],
     multilib: {
         both: {
-            binaries: ["imgdiag"],
+            binaries: [
+                "imgdiag",
+                "pageinfo",
+                "find_unshared_pages",
+            ],
         },
         first: {
             binaries: ["odrefresh"],
diff --git a/build/art.go b/build/art.go
index 26794a0a9a..c4e9f9230f 100644
--- a/build/art.go
+++ b/build/art.go
@@ -44,6 +44,19 @@ func globalFlags(ctx android.LoadHookContext) ([]string, []string) {
 		tlab = true
 	}
 
+	if ctx.Config().IsEnvTrue("ART_USE_RESTRICTED_MODE") {
+		cflags = append(cflags, "-DART_USE_RESTRICTED_MODE=1")
+		asflags = append(asflags, "-DART_USE_RESTRICTED_MODE=1")
+
+		// TODO(Simulator): Support other GC types.
+		gcType = "MS"
+	}
+
+	if ctx.Config().IsEnvTrue("ART_USE_SIMULATOR") {
+		cflags = append(cflags, "-DART_USE_SIMULATOR=1")
+		asflags = append(asflags, "-DART_USE_SIMULATOR=1")
+	}
+
 	cflags = append(cflags, "-DART_DEFAULT_GC_TYPE_IS_"+gcType)
 
 	if ctx.Config().IsEnvTrue("ART_HEAP_POISONING") {
@@ -54,7 +67,10 @@ func globalFlags(ctx android.LoadHookContext) ([]string, []string) {
 		cflags = append(cflags, "-DART_USE_CXX_INTERPRETER=1")
 	}
 
-	if !ctx.Config().IsEnvFalse("ART_USE_READ_BARRIER") && ctx.Config().ArtUseReadBarrier() {
+	// TODO: deprecate and then eventually remove ART_USE_GENERATIONAL_CC in favor of
+	// ART_USE_GENERATIONAL_GC
+	if !ctx.Config().IsEnvFalse("ART_USE_READ_BARRIER") && ctx.Config().ArtUseReadBarrier() &&
+	   !ctx.Config().IsEnvTrue("ART_USE_RESTRICTED_MODE") {
 		// Used to change the read barrier type. Valid values are BAKER, TABLELOOKUP.
 		// The default is BAKER.
 		barrierType := ctx.Config().GetenvWithDefault("ART_READ_BARRIER_TYPE", "BAKER")
@@ -65,8 +81,9 @@ func globalFlags(ctx android.LoadHookContext) ([]string, []string) {
 			"-DART_USE_READ_BARRIER=1",
 			"-DART_READ_BARRIER_TYPE_IS_"+barrierType+"=1")
 
-		if !ctx.Config().IsEnvFalse("ART_USE_GENERATIONAL_CC") {
-			cflags = append(cflags, "-DART_USE_GENERATIONAL_CC=1")
+		if !(ctx.Config().IsEnvFalse("ART_USE_GENERATIONAL_CC") ||
+		     ctx.Config().IsEnvFalse("ART_USE_GENERATIONAL_GC")) {
+			cflags = append(cflags, "-DART_USE_GENERATIONAL_GC=1")
 		}
 		// Force CC only if ART_USE_READ_BARRIER was set to true explicitly during
 		// build time.
@@ -76,6 +93,10 @@ func globalFlags(ctx android.LoadHookContext) ([]string, []string) {
 		tlab = true
 	} else if gcType == "CMC" {
 		tlab = true
+		if !(ctx.Config().IsEnvFalse("ART_USE_GENERATIONAL_CC") ||
+		     ctx.Config().IsEnvFalse("ART_USE_GENERATIONAL_GC")) {
+			cflags = append(cflags, "-DART_USE_GENERATIONAL_GC=1")
+		}
 	}
 
 	if tlab {
diff --git a/build/boot/boot-image-profile.txt b/build/boot/boot-image-profile.txt
index b2f916130f..412534c750 100644
--- a/build/boot/boot-image-profile.txt
+++ b/build/boot/boot-image-profile.txt
@@ -1,5 +1,5 @@
 #
-# Copyright (C) 2017 The Android Open Source Project
+# Copyright (C) 2025 The Android Open Source Project
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -76,7 +76,9 @@ HSPLandroid/system/StructTimespec;-><init>(JJ)V
 HSPLandroid/system/StructTimespec;->equals(Ljava/lang/Object;)Z
 HSPLandroid/system/StructTimeval;-><init>(JJ)V
 HSPLandroid/system/StructTimeval;->fromMillis(J)Landroid/system/StructTimeval;
+HSPLandroid/system/SystemCleaner;->cleaner()Ljava/lang/ref/Cleaner;
 HSPLandroid/system/UnixSocketAddress;-><init>([B)V
+HSPLcom/android/libcore/FeatureFlagsImpl;->readOnlyDynamicCodeLoad()Z
 HSPLcom/android/okhttp/Address;-><init>(Ljava/lang/String;ILcom/android/okhttp/Dns;Ljavax/net/SocketFactory;Ljavax/net/ssl/SSLSocketFactory;Ljavax/net/ssl/HostnameVerifier;Lcom/android/okhttp/CertificatePinner;Lcom/android/okhttp/Authenticator;Ljava/net/Proxy;Ljava/util/List;Ljava/util/List;Ljava/net/ProxySelector;)V
 HSPLcom/android/okhttp/Address;->equals(Ljava/lang/Object;)Z
 HSPLcom/android/okhttp/Address;->getCertificatePinner()Lcom/android/okhttp/CertificatePinner;
@@ -937,7 +939,6 @@ HSPLcom/android/org/bouncycastle/jcajce/util/BCJcaJceHelper;-><init>()V
 HSPLcom/android/org/bouncycastle/jce/provider/BouncyCastleProvider;->addAlgorithm(Ljava/lang/String;Lcom/android/org/bouncycastle/asn1/ASN1ObjectIdentifier;Ljava/lang/String;)V
 HSPLcom/android/org/bouncycastle/jce/provider/BouncyCastleProvider;->addAlgorithm(Ljava/lang/String;Ljava/lang/String;)V
 HSPLcom/android/org/bouncycastle/jce/provider/BouncyCastleProvider;->addAttributes(Ljava/lang/String;Ljava/util/Map;)V
-HSPLcom/android/org/bouncycastle/jce/provider/BouncyCastleProvider;->loadAlgorithms([Ljava/lang/String;Ljava/lang/String;)V
 HSPLcom/android/org/bouncycastle/jce/provider/CertStoreCollectionSpi;-><init>(Ljava/security/cert/CertStoreParameters;)V
 HSPLcom/android/org/bouncycastle/util/Arrays;->areEqual([B[B)Z
 HSPLcom/android/org/bouncycastle/util/Arrays;->clone([B)[B
@@ -1092,13 +1093,14 @@ HSPLdalvik/system/SocketTagger;->set(Ldalvik/system/SocketTagger;)V
 HSPLdalvik/system/SocketTagger;->tag(Ljava/net/Socket;)V
 HSPLdalvik/system/SocketTagger;->untag(Ljava/net/Socket;)V
 HSPLdalvik/system/VMRuntime$SdkVersionContainer;->-$$Nest$sfgetsdkVersion()I
+HSPLdalvik/system/VMRuntime;->addPostCleanupCallback(Ljava/lang/Runnable;)V
 HSPLdalvik/system/VMRuntime;->getInstructionSet(Ljava/lang/String;)Ljava/lang/String;
 HSPLdalvik/system/VMRuntime;->getRuntime()Ldalvik/system/VMRuntime;
 HSPLdalvik/system/VMRuntime;->getSdkVersion()I
 HSPLdalvik/system/VMRuntime;->getTargetSdkVersion()I
 HSPLdalvik/system/VMRuntime;->hiddenApiUsed(ILjava/lang/String;Ljava/lang/String;IZ)V
 HSPLdalvik/system/VMRuntime;->notifyNativeAllocation()V
-HSPLdalvik/system/VMRuntime;->onPostCleanup()V+]Ljava/util/Iterator;Ljava/util/ArrayList$Itr;]Ljava/util/List;Ljava/util/ArrayList;
+HSPLdalvik/system/VMRuntime;->onPostCleanup()V
 HSPLdalvik/system/VMRuntime;->registerNativeAllocation(I)V
 HSPLdalvik/system/VMRuntime;->registerNativeFree(I)V
 HSPLdalvik/system/VMRuntime;->runFinalization(J)V
@@ -1221,6 +1223,7 @@ HSPLjava/io/DataInputStream;->skipBytes(I)I
 HSPLjava/io/DataOutputStream;-><init>(Ljava/io/OutputStream;)V
 HSPLjava/io/DataOutputStream;->flush()V
 HSPLjava/io/DataOutputStream;->incCount(I)V
+HSPLjava/io/DataOutputStream;->size()I
 HSPLjava/io/DataOutputStream;->write(I)V
 HSPLjava/io/DataOutputStream;->write([BII)V
 HSPLjava/io/DataOutputStream;->writeBoolean(Z)V
@@ -1289,6 +1292,7 @@ HSPLjava/io/File;->toString()Ljava/lang/String;
 HSPLjava/io/File;->toURI()Ljava/net/URI;
 HSPLjava/io/FileDescriptor$1;->set(Ljava/io/FileDescriptor;I)V
 HSPLjava/io/FileDescriptor;-><init>()V
+HSPLjava/io/FileDescriptor;-><init>(I)V
 HSPLjava/io/FileDescriptor;->cloneForFork()V
 HSPLjava/io/FileDescriptor;->getInt$()I
 HSPLjava/io/FileDescriptor;->getOwnerId$()J
@@ -1887,6 +1891,9 @@ HSPLjava/lang/Character;->codePointAt(Ljava/lang/CharSequence;I)I
 HSPLjava/lang/Character;->codePointAtImpl([CII)I
 HSPLjava/lang/Character;->codePointBefore(Ljava/lang/CharSequence;I)I
 HSPLjava/lang/Character;->codePointCount(Ljava/lang/CharSequence;II)I
+HSPLjava/lang/Character;->compare(CC)I
+HSPLjava/lang/Character;->compareTo(Ljava/lang/Character;)I
+HSPLjava/lang/Character;->compareTo(Ljava/lang/Object;)I
 HSPLjava/lang/Character;->digit(CI)I
 HSPLjava/lang/Character;->digit(II)I
 HSPLjava/lang/Character;->equals(Ljava/lang/Object;)Z
@@ -2040,7 +2047,6 @@ HSPLjava/lang/Daemons$HeapTaskDaemon;->interrupt(Ljava/lang/Thread;)V
 HSPLjava/lang/Daemons$HeapTaskDaemon;->runInternal()V
 HSPLjava/lang/Daemons$ReferenceQueueDaemon;->-$$Nest$fgetprogressCounter(Ljava/lang/Daemons$ReferenceQueueDaemon;)Ljava/util/concurrent/atomic/AtomicInteger;
 HSPLjava/lang/Daemons$ReferenceQueueDaemon;->-$$Nest$sfgetINSTANCE()Ljava/lang/Daemons$ReferenceQueueDaemon;
-HSPLjava/lang/Daemons$ReferenceQueueDaemon;->onRefQueueEmptyAfterGc()V
 HSPLjava/lang/Daemons$ReferenceQueueDaemon;->runInternal()V
 HSPLjava/lang/Daemons;->-$$Nest$sfgetpostZygoteFork()Z
 HSPLjava/lang/Daemons;->-$$Nest$sfgetzygoteStartLatch()Ljava/util/concurrent/CountDownLatch;
@@ -2224,6 +2230,7 @@ HSPLjava/lang/Math;->abs(F)F
 HSPLjava/lang/Math;->abs(I)I
 HSPLjava/lang/Math;->abs(J)J
 HSPLjava/lang/Math;->addExact(JJ)J
+HSPLjava/lang/Math;->clamp(FFF)F
 HSPLjava/lang/Math;->copySign(DD)D
 HSPLjava/lang/Math;->copySign(FF)F
 HSPLjava/lang/Math;->floorDiv(II)I
@@ -2295,6 +2302,7 @@ HSPLjava/lang/ProcessBuilder;->start()Ljava/lang/Process;
 HSPLjava/lang/ProcessEnvironment;->toEnvironmentBlock(Ljava/util/Map;[I)[B
 HSPLjava/lang/ProcessImpl;->start([Ljava/lang/String;Ljava/util/Map;Ljava/lang/String;[Ljava/lang/ProcessBuilder$Redirect;Z)Ljava/lang/Process;
 HSPLjava/lang/ProcessImpl;->toCString(Ljava/lang/String;)[B
+HSPLjava/lang/Record;-><init>()V
 HSPLjava/lang/ReflectiveOperationException;-><init>(Ljava/lang/String;)V
 HSPLjava/lang/ReflectiveOperationException;-><init>(Ljava/lang/String;Ljava/lang/Throwable;)V
 HSPLjava/lang/ReflectiveOperationException;-><init>(Ljava/lang/Throwable;)V
@@ -2556,6 +2564,7 @@ HSPLjava/lang/Thread;->init2(Ljava/lang/Thread;Z)V
 HSPLjava/lang/Thread;->interrupt()V
 HSPLjava/lang/Thread;->isAlive()Z
 HSPLjava/lang/Thread;->isDaemon()Z
+HSPLjava/lang/Thread;->isVirtual()Z
 HSPLjava/lang/Thread;->join()V
 HSPLjava/lang/Thread;->join(J)V
 HSPLjava/lang/Thread;->nextThreadID()J
@@ -2729,7 +2738,7 @@ HSPLjava/lang/ref/Cleaner$1;->apply(Ljava/lang/Object;)Ljava/lang/Object;
 HSPLjava/lang/ref/Cleaner$1;->apply(Ljava/lang/ref/Cleaner;)Ljdk/internal/ref/CleanerImpl;
 HSPLjava/lang/ref/Cleaner;->register(Ljava/lang/Object;Ljava/lang/Runnable;)Ljava/lang/ref/Cleaner$Cleanable;
 HSPLjava/lang/ref/FinalizerReference$Sentinel;-><init>()V
-SPLjava/lang/ref/FinalizerReference$Sentinel;-><init>(Ljava/lang/ref/FinalizerReference-IA;)V
+HSPLjava/lang/ref/FinalizerReference$Sentinel;-><init>(Ljava/lang/ref/FinalizerReference-IA;)V
 HSPLjava/lang/ref/FinalizerReference$Sentinel;->awaitFinalization(J)V
 HSPLjava/lang/ref/FinalizerReference$Sentinel;->finalize()V
 HSPLjava/lang/ref/FinalizerReference;-><init>(Ljava/lang/Object;Ljava/lang/ref/ReferenceQueue;)V
@@ -3605,6 +3614,7 @@ HSPLjava/nio/DirectByteBuffer;->duplicate()Ljava/nio/ByteBuffer;
 HSPLjava/nio/DirectByteBuffer;->duplicate()Ljava/nio/MappedByteBuffer;
 HSPLjava/nio/DirectByteBuffer;->get()B
 HSPLjava/nio/DirectByteBuffer;->get(I)B
+HSPLjava/nio/DirectByteBuffer;->get(I[BII)Ljava/nio/ByteBuffer;
 HSPLjava/nio/DirectByteBuffer;->get(J)B
 HSPLjava/nio/DirectByteBuffer;->get([BII)Ljava/nio/ByteBuffer;
 HSPLjava/nio/DirectByteBuffer;->getChar()C
@@ -3613,6 +3623,7 @@ HSPLjava/nio/DirectByteBuffer;->getCharUnchecked(I)C
 HSPLjava/nio/DirectByteBuffer;->getInt()I
 HSPLjava/nio/DirectByteBuffer;->getInt(I)I
 HSPLjava/nio/DirectByteBuffer;->getInt(J)I
+HSPLjava/nio/DirectByteBuffer;->getLong()J
 HSPLjava/nio/DirectByteBuffer;->getLong(I)J
 HSPLjava/nio/DirectByteBuffer;->getLong(J)J
 HSPLjava/nio/DirectByteBuffer;->getShort()S
@@ -3984,7 +3995,7 @@ HSPLjava/security/Provider$Service;->-$$Nest$fputtype(Ljava/security/Provider$Se
 HSPLjava/security/Provider$Service;->-$$Nest$maddAlias(Ljava/security/Provider$Service;Ljava/lang/String;)V
 HSPLjava/security/Provider$Service;->-$$Nest$misValid(Ljava/security/Provider$Service;)Z
 HSPLjava/security/Provider$Service;-><init>(Ljava/security/Provider;)V
-SPLjava/security/Provider$Service;-><init>(Ljava/security/Provider;Ljava/security/Provider-IA;)V
+HSPLjava/security/Provider$Service;-><init>(Ljava/security/Provider;Ljava/security/Provider-IA;)V
 HSPLjava/security/Provider$Service;->addAlias(Ljava/lang/String;)V
 HSPLjava/security/Provider$Service;->addAttribute(Ljava/lang/String;Ljava/lang/String;)V
 HSPLjava/security/Provider$Service;->getAlgorithm()Ljava/lang/String;
@@ -4983,6 +4994,7 @@ HSPLjava/util/Arrays;->binarySearch0([IIII)I
 HSPLjava/util/Arrays;->binarySearch0([JIIJ)I
 HSPLjava/util/Arrays;->binarySearch0([Ljava/lang/Object;IILjava/lang/Object;)I
 HSPLjava/util/Arrays;->binarySearch0([Ljava/lang/Object;IILjava/lang/Object;Ljava/util/Comparator;)I
+HSPLjava/util/Arrays;->checkLength(II)V
 HSPLjava/util/Arrays;->copyOf([BI)[B
 HSPLjava/util/Arrays;->copyOf([CI)[C
 HSPLjava/util/Arrays;->copyOf([DI)[D
@@ -5131,7 +5143,7 @@ HSPLjava/util/Calendar;->setTimeZone(Ljava/util/TimeZone;)V
 HSPLjava/util/Calendar;->setWeekCountData(Ljava/util/Locale;)V
 HSPLjava/util/Calendar;->setZoneShared(Z)V
 HSPLjava/util/Calendar;->updateTime()V
-HSPLjava/util/Collection;->removeIf(Ljava/util/function/Predicate;)Z+]Ljava/util/Collection;megamorphic_types]Ljava/util/Iterator;megamorphic_types]Ljava/util/function/Predicate;Lcom/android/internal/telephony/data/DataNetworkController$$ExternalSyntheticLambda32;,Lcom/android/internal/telephony/data/DataNetworkController$$ExternalSyntheticLambda42;
+HSPLjava/util/Collection;->removeIf(Ljava/util/function/Predicate;)Z+]Ljava/util/Collection;megamorphic_types]Ljava/util/Iterator;megamorphic_types
 HSPLjava/util/Collection;->spliterator()Ljava/util/Spliterator;
 HSPLjava/util/Collection;->stream()Ljava/util/stream/Stream;+]Ljava/util/Collection;megamorphic_types
 HSPLjava/util/Collections$1;-><init>(Ljava/lang/Object;)V
@@ -5368,7 +5380,7 @@ HSPLjava/util/Comparator;->naturalOrder()Ljava/util/Comparator;
 HSPLjava/util/Comparator;->nullsFirst(Ljava/util/Comparator;)Ljava/util/Comparator;
 HSPLjava/util/Comparator;->reversed()Ljava/util/Comparator;
 HSPLjava/util/Comparator;->thenComparing(Ljava/util/Comparator;)Ljava/util/Comparator;
-HSPLjava/util/Comparator;->thenComparing(Ljava/util/function/Function;)Ljava/util/Comparator;+]Ljava/util/Comparator;Ljava/util/Comparator$$ExternalSyntheticLambda3;
+HSPLjava/util/Comparator;->thenComparing(Ljava/util/function/Function;)Ljava/util/Comparator;
 HSPLjava/util/Comparators$NaturalOrderComparator;->compare(Ljava/lang/Comparable;Ljava/lang/Comparable;)I
 HSPLjava/util/Comparators$NaturalOrderComparator;->compare(Ljava/lang/Object;Ljava/lang/Object;)I
 HSPLjava/util/Comparators$NullComparator;-><init>(ZLjava/util/Comparator;)V
@@ -5802,6 +5814,7 @@ HSPLjava/util/ImmutableCollections$AbstractImmutableMap;-><init>()V
 HSPLjava/util/ImmutableCollections$AbstractImmutableSet;-><init>()V
 HSPLjava/util/ImmutableCollections$Access$1;-><init>()V
 HSPLjava/util/ImmutableCollections$Access$1;->listFromTrustedArray([Ljava/lang/Object;)Ljava/util/List;
+HSPLjava/util/ImmutableCollections$Access$1;->listFromTrustedArrayNullsAllowed([Ljava/lang/Object;)Ljava/util/List;
 HSPLjava/util/ImmutableCollections$Access;-><clinit>()V
 HSPLjava/util/ImmutableCollections$List12;-><init>(Ljava/lang/Object;)V
 HSPLjava/util/ImmutableCollections$List12;-><init>(Ljava/lang/Object;Ljava/lang/Object;)V
@@ -5811,6 +5824,7 @@ HSPLjava/util/ImmutableCollections$ListItr;-><init>(Ljava/util/List;I)V
 HSPLjava/util/ImmutableCollections$ListItr;->hasNext()Z
 HSPLjava/util/ImmutableCollections$ListItr;->next()Ljava/lang/Object;
 HSPLjava/util/ImmutableCollections$ListN;-><init>([Ljava/lang/Object;Z)V
+HSPLjava/util/ImmutableCollections$ListN;-><init>([Ljava/lang/Object;ZLjava/util/ImmutableCollections-IA;)V
 HSPLjava/util/ImmutableCollections$ListN;->get(I)Ljava/lang/Object;
 HSPLjava/util/ImmutableCollections$ListN;->size()I
 HSPLjava/util/ImmutableCollections$Map1;-><init>(Ljava/lang/Object;Ljava/lang/Object;)V
@@ -5820,12 +5834,19 @@ HSPLjava/util/ImmutableCollections$MapN;->containsKey(Ljava/lang/Object;)Z
 HSPLjava/util/ImmutableCollections$MapN;->get(Ljava/lang/Object;)Ljava/lang/Object;
 HSPLjava/util/ImmutableCollections$MapN;->probe(Ljava/lang/Object;)I
 HSPLjava/util/ImmutableCollections$Set12;-><init>(Ljava/lang/Object;Ljava/lang/Object;)V
+HSPLjava/util/ImmutableCollections$SetN$SetNIterator;-><init>(Ljava/util/ImmutableCollections$SetN;)V
+HSPLjava/util/ImmutableCollections$SetN$SetNIterator;->hasNext()Z
+HSPLjava/util/ImmutableCollections$SetN$SetNIterator;->next()Ljava/lang/Object;
 HSPLjava/util/ImmutableCollections$SetN;-><init>([Ljava/lang/Object;)V
 HSPLjava/util/ImmutableCollections$SetN;->contains(Ljava/lang/Object;)Z
+HSPLjava/util/ImmutableCollections$SetN;->iterator()Ljava/util/Iterator;
 HSPLjava/util/ImmutableCollections$SetN;->probe(Ljava/lang/Object;)I
+HSPLjava/util/ImmutableCollections;->-$$Nest$sfgetREVERSE()Z
+HSPLjava/util/ImmutableCollections;->-$$Nest$sfgetSALT32L()J
 HSPLjava/util/ImmutableCollections;-><clinit>()V
 HSPLjava/util/ImmutableCollections;->listCopy(Ljava/util/Collection;)Ljava/util/List;
 HSPLjava/util/ImmutableCollections;->listFromTrustedArray([Ljava/lang/Object;)Ljava/util/List;
+HSPLjava/util/ImmutableCollections;->listFromTrustedArrayNullsAllowed([Ljava/lang/Object;)Ljava/util/List;
 HSPLjava/util/Iterator;->forEachRemaining(Ljava/util/function/Consumer;)V+]Ljava/util/Iterator;Landroid/util/MapCollections$ArrayIterator;,Landroid/util/MapCollections$MapIterator;,Ljava/util/AbstractList$Itr;,Ljava/util/AbstractMap$2$1;]Ljava/util/function/Consumer;megamorphic_types
 HSPLjava/util/JumboEnumSet$EnumSetIterator;-><init>(Ljava/util/JumboEnumSet;)V
 HSPLjava/util/JumboEnumSet$EnumSetIterator;->hasNext()Z
@@ -5958,7 +5979,7 @@ HSPLjava/util/List;->of(Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Lj
 HSPLjava/util/List;->of(Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;)Ljava/util/List;
 HSPLjava/util/List;->of(Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;)Ljava/util/List;
 HSPLjava/util/List;->of([Ljava/lang/Object;)Ljava/util/List;
-HSPLjava/util/List;->sort(Ljava/util/Comparator;)V+]Ljava/util/List;Ljava/util/ArrayList$SubList;,Ljava/util/LinkedList;]Ljava/util/ListIterator;Ljava/util/AbstractList$ListItr;,Ljava/util/ArrayList$SubList$1;,Ljava/util/LinkedList$ListItr;
+HSPLjava/util/List;->sort(Ljava/util/Comparator;)V+]Ljava/util/List;Ljava/util/ArrayList$SubList;,Ljava/util/LinkedList;]Ljava/util/ListIterator;Ljava/util/ArrayList$SubList$1;,Ljava/util/LinkedList$ListItr;
 HSPLjava/util/List;->spliterator()Ljava/util/Spliterator;
 HSPLjava/util/Locale$Builder;-><init>()V
 HSPLjava/util/Locale$Builder;->build()Ljava/util/Locale;
@@ -6022,6 +6043,7 @@ HSPLjava/util/Map;->forEach(Ljava/util/function/BiConsumer;)V
 HSPLjava/util/Map;->getOrDefault(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+]Ljava/util/Map;Landroid/util/ArrayMap;
 HSPLjava/util/Map;->of(Ljava/lang/Object;Ljava/lang/Object;)Ljava/util/Map;
 HSPLjava/util/Map;->ofEntries([Ljava/util/Map$Entry;)Ljava/util/Map;
+HSPLjava/util/Map;->putIfAbsent(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+]Ljava/util/Map;Landroid/util/ArrayMap;
 HSPLjava/util/MissingResourceException;-><init>(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;)V
 HSPLjava/util/NoSuchElementException;-><init>()V
 HSPLjava/util/NoSuchElementException;-><init>(Ljava/lang/String;)V
@@ -6804,6 +6826,10 @@ HSPLjava/util/concurrent/DelayQueue;-><init>()V
 HSPLjava/util/concurrent/DelayQueue;->add(Ljava/util/concurrent/Delayed;)Z
 HSPLjava/util/concurrent/DelayQueue;->offer(Ljava/util/concurrent/Delayed;)Z
 HSPLjava/util/concurrent/ExecutionException;-><init>(Ljava/lang/Throwable;)V
+HSPLjava/util/concurrent/Executors$AutoShutdownDelegatedExecutorService$$ExternalSyntheticLambda0;-><init>(Ljava/util/concurrent/ExecutorService;)V
+HSPLjava/util/concurrent/Executors$AutoShutdownDelegatedExecutorService$$ExternalSyntheticLambda0;->run()V
+HSPLjava/util/concurrent/Executors$AutoShutdownDelegatedExecutorService;-><init>(Ljava/util/concurrent/ExecutorService;)V
+HSPLjava/util/concurrent/Executors$AutoShutdownDelegatedExecutorService;->lambda$new$1(Ljava/util/concurrent/ExecutorService;)V
 HSPLjava/util/concurrent/Executors$DefaultThreadFactory;-><init>()V
 HSPLjava/util/concurrent/Executors$DefaultThreadFactory;->newThread(Ljava/lang/Runnable;)Ljava/lang/Thread;
 HSPLjava/util/concurrent/Executors$DelegatedExecutorService;-><init>(Ljava/util/concurrent/ExecutorService;)V
@@ -6896,6 +6922,12 @@ HSPLjava/util/concurrent/LinkedBlockingQueue;->signalNotEmpty()V
 HSPLjava/util/concurrent/LinkedBlockingQueue;->signalNotFull()V
 HSPLjava/util/concurrent/LinkedBlockingQueue;->size()I
 HSPLjava/util/concurrent/LinkedBlockingQueue;->take()Ljava/lang/Object;
+HSPLjava/util/concurrent/LinkedTransferQueue$DualNode;-><clinit>()V
+HSPLjava/util/concurrent/LinkedTransferQueue$DualNode;-><init>(Ljava/lang/Object;Z)V
+HSPLjava/util/concurrent/LinkedTransferQueue$DualNode;->await(Ljava/lang/Object;JLjava/lang/Object;Z)Ljava/lang/Object;+]Ljava/lang/Thread;missing_types
+HSPLjava/util/concurrent/LinkedTransferQueue$DualNode;->checkForUniprocessor(Z)V
+HSPLjava/util/concurrent/LinkedTransferQueue$DualNode;->cmpExItem(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;
+HSPLjava/util/concurrent/LinkedTransferQueue;->cmpExHead(Ljava/util/concurrent/LinkedTransferQueue$DualNode;Ljava/util/concurrent/LinkedTransferQueue$DualNode;)Ljava/util/concurrent/LinkedTransferQueue$DualNode;
 HSPLjava/util/concurrent/PriorityBlockingQueue;-><init>()V
 HSPLjava/util/concurrent/PriorityBlockingQueue;-><init>(ILjava/util/Comparator;)V
 HSPLjava/util/concurrent/PriorityBlockingQueue;->add(Ljava/lang/Object;)Z
@@ -6992,6 +7024,8 @@ HSPLjava/util/concurrent/Semaphore;->tryAcquire()Z
 HSPLjava/util/concurrent/Semaphore;->tryAcquire(IJLjava/util/concurrent/TimeUnit;)Z
 HSPLjava/util/concurrent/Semaphore;->tryAcquire(JLjava/util/concurrent/TimeUnit;)Z
 HSPLjava/util/concurrent/SynchronousQueue$Transferer;-><init>()V
+HSPLjava/util/concurrent/SynchronousQueue$Transferer;->unspliceLifo(Ljava/util/concurrent/LinkedTransferQueue$DualNode;)V
+HSPLjava/util/concurrent/SynchronousQueue$Transferer;->xferLifo(Ljava/lang/Object;J)Ljava/lang/Object;
 HSPLjava/util/concurrent/SynchronousQueue;-><init>()V
 HSPLjava/util/concurrent/SynchronousQueue;-><init>(Z)V
 HSPLjava/util/concurrent/SynchronousQueue;->isEmpty()Z
@@ -7296,6 +7330,11 @@ HSPLjava/util/concurrent/locks/ReentrantReadWriteLock;->writeLock()Ljava/util/co
 HSPLjava/util/concurrent/locks/ReentrantReadWriteLock;->writeLock()Ljava/util/concurrent/locks/ReentrantReadWriteLock$WriteLock;
 HSPLjava/util/function/BinaryOperator$$ExternalSyntheticLambda0;-><init>(Ljava/util/Comparator;)V
 HSPLjava/util/function/BinaryOperator;->maxBy(Ljava/util/Comparator;)Ljava/util/function/BinaryOperator;
+HSPLjava/util/function/Consumer$$ExternalSyntheticLambda0;-><init>(Ljava/util/function/Consumer;Ljava/util/function/Consumer;)V
+HSPLjava/util/function/Consumer$$ExternalSyntheticLambda0;->accept(Ljava/lang/Object;)V
+HSPLjava/util/function/Consumer;->$r8$lambda$Q_zMx_zAe1OiUreM0xDZCWBmTh8(Ljava/util/function/Consumer;Ljava/util/function/Consumer;Ljava/lang/Object;)V
+HSPLjava/util/function/Consumer;->andThen(Ljava/util/function/Consumer;)Ljava/util/function/Consumer;
+HSPLjava/util/function/Consumer;->lambda$andThen$0(Ljava/util/function/Consumer;Ljava/lang/Object;)V
 HSPLjava/util/function/DoubleUnaryOperator$$ExternalSyntheticLambda1;->applyAsDouble(D)D
 HSPLjava/util/function/DoubleUnaryOperator;->andThen(Ljava/util/function/DoubleUnaryOperator;)Ljava/util/function/DoubleUnaryOperator;
 HSPLjava/util/function/Function$$ExternalSyntheticLambda0;-><init>()V
@@ -7601,6 +7640,7 @@ HSPLjava/util/stream/Collectors$$ExternalSyntheticLambda28;->apply(Ljava/lang/Ob
 HSPLjava/util/stream/Collectors$$ExternalSyntheticLambda42;-><init>()V
 HSPLjava/util/stream/Collectors$$ExternalSyntheticLambda65;->get()Ljava/lang/Object;
 HSPLjava/util/stream/Collectors$$ExternalSyntheticLambda72;->get()Ljava/lang/Object;
+HSPLjava/util/stream/Collectors$$ExternalSyntheticLambda73;->accept(Ljava/lang/Object;Ljava/lang/Object;)V
 HSPLjava/util/stream/Collectors$$ExternalSyntheticLambda74;-><init>()V
 HSPLjava/util/stream/Collectors$$ExternalSyntheticLambda75;-><init>()V
 HSPLjava/util/stream/Collectors$$ExternalSyntheticLambda75;->apply(Ljava/lang/Object;)Ljava/lang/Object;
@@ -7793,6 +7833,8 @@ HSPLjava/util/stream/ReduceOps;->makeInt(ILjava/util/function/IntBinaryOperator;
 HSPLjava/util/stream/ReduceOps;->makeLong(JLjava/util/function/LongBinaryOperator;)Ljava/util/stream/TerminalOp;
 HSPLjava/util/stream/ReduceOps;->makeRef(Ljava/util/function/BinaryOperator;)Ljava/util/stream/TerminalOp;
 HSPLjava/util/stream/ReduceOps;->makeRef(Ljava/util/stream/Collector;)Ljava/util/stream/TerminalOp;
+HSPLjava/util/stream/ReferencePipeline$$ExternalSyntheticLambda1;-><init>()V
+HSPLjava/util/stream/ReferencePipeline$$ExternalSyntheticLambda1;->apply(I)Ljava/lang/Object;
 HSPLjava/util/stream/ReferencePipeline$15$1;-><init>(Ljava/util/stream/ReferencePipeline$15;Ljava/util/stream/Sink;)V
 HSPLjava/util/stream/ReferencePipeline$15$1;->accept(Ljava/lang/Object;)V
 HSPLjava/util/stream/ReferencePipeline$15;-><init>(Ljava/util/stream/ReferencePipeline;Ljava/util/stream/AbstractPipeline;Ljava/util/stream/StreamShape;ILjava/util/function/Consumer;)V
@@ -7842,6 +7884,7 @@ HSPLjava/util/stream/ReferencePipeline;->findFirst()Ljava/util/Optional;
 HSPLjava/util/stream/ReferencePipeline;->flatMap(Ljava/util/function/Function;)Ljava/util/stream/Stream;
 HSPLjava/util/stream/ReferencePipeline;->forEach(Ljava/util/function/Consumer;)V
 HSPLjava/util/stream/ReferencePipeline;->forEachWithCancel(Ljava/util/Spliterator;Ljava/util/stream/Sink;)Z
+HSPLjava/util/stream/ReferencePipeline;->lambda$toArray$0(I)[Ljava/lang/Object;
 HSPLjava/util/stream/ReferencePipeline;->makeNodeBuilder(JLjava/util/function/IntFunction;)Ljava/util/stream/Node$Builder;
 HSPLjava/util/stream/ReferencePipeline;->map(Ljava/util/function/Function;)Ljava/util/stream/Stream;
 HSPLjava/util/stream/ReferencePipeline;->mapToDouble(Ljava/util/function/ToDoubleFunction;)Ljava/util/stream/DoubleStream;
@@ -7853,7 +7896,9 @@ HSPLjava/util/stream/ReferencePipeline;->peek(Ljava/util/function/Consumer;)Ljav
 HSPLjava/util/stream/ReferencePipeline;->reduce(Ljava/util/function/BinaryOperator;)Ljava/util/Optional;
 HSPLjava/util/stream/ReferencePipeline;->sorted()Ljava/util/stream/Stream;
 HSPLjava/util/stream/ReferencePipeline;->sorted(Ljava/util/Comparator;)Ljava/util/stream/Stream;
+HSPLjava/util/stream/ReferencePipeline;->toArray()[Ljava/lang/Object;
 HSPLjava/util/stream/ReferencePipeline;->toArray(Ljava/util/function/IntFunction;)[Ljava/lang/Object;
+HSPLjava/util/stream/ReferencePipeline;->toList()Ljava/util/List;+]Ljdk/internal/access/JavaUtilCollectionAccess;Ljava/util/ImmutableCollections$Access$1;
 HSPLjava/util/stream/ReferencePipeline;->wrap(Ljava/util/stream/PipelineHelper;Ljava/util/function/Supplier;Z)Ljava/util/Spliterator;
 HSPLjava/util/stream/Sink$ChainedInt;-><init>(Ljava/util/stream/Sink;)V
 HSPLjava/util/stream/Sink$ChainedInt;->begin(J)V
@@ -7987,6 +8032,7 @@ HSPLjava/util/zip/Inflater;->inflate([BII)I
 HSPLjava/util/zip/Inflater;->needsDictionary()Z
 HSPLjava/util/zip/Inflater;->needsInput()Z
 HSPLjava/util/zip/Inflater;->reset()V
+HSPLjava/util/zip/Inflater;->setInput([B)V
 HSPLjava/util/zip/Inflater;->setInput([BII)V
 HSPLjava/util/zip/InflaterInputStream;-><init>(Ljava/io/InputStream;Ljava/util/zip/Inflater;)V
 HSPLjava/util/zip/InflaterInputStream;-><init>(Ljava/io/InputStream;Ljava/util/zip/Inflater;I)V
@@ -7998,8 +8044,10 @@ HSPLjava/util/zip/InflaterInputStream;->fill()V
 HSPLjava/util/zip/InflaterInputStream;->read()I
 HSPLjava/util/zip/InflaterInputStream;->read([BII)I
 HSPLjava/util/zip/ZipCoder$UTF8ZipCoder;->checkedHash([BII)I
+HSPLjava/util/zip/ZipCoder$UTF8ZipCoder;->hasTrailingSlash(Ljava/nio/DirectByteBuffer;I)Z
 HSPLjava/util/zip/ZipCoder$UTF8ZipCoder;->hasTrailingSlash([BI)Z
 HSPLjava/util/zip/ZipCoder$UTF8ZipCoder;->isUTF8()Z
+HSPLjava/util/zip/ZipCoder$UTF8ZipCoder;->toString(Ljava/nio/DirectByteBuffer;II)Ljava/lang/String;+]Ljava/nio/DirectByteBuffer;Ljava/nio/DirectByteBuffer;
 HSPLjava/util/zip/ZipCoder$UTF8ZipCoder;->toString([BII)Ljava/lang/String;
 HSPLjava/util/zip/ZipCoder;-><init>(Ljava/nio/charset/Charset;)V
 HSPLjava/util/zip/ZipCoder;->decoder()Ljava/nio/charset/CharsetDecoder;
@@ -8034,17 +8082,16 @@ HSPLjava/util/zip/ZipFile$Source;->-$$Nest$mgetEntryPos(Ljava/util/zip/ZipFile$S
 HSPLjava/util/zip/ZipFile$Source;->-$$Nest$mreadAt(Ljava/util/zip/ZipFile$Source;[BIIJ)I
 HSPLjava/util/zip/ZipFile$Source;->-$$Nest$mreadFullyAt(Ljava/util/zip/ZipFile$Source;[BIIJ)I
 HSPLjava/util/zip/ZipFile$Source;-><init>(Ljava/util/zip/ZipFile$Source$Key;ZLjava/util/zip/ZipCoder;)V
-HSPLjava/util/zip/ZipFile$Source;->checkAndAddEntry(II)I
+HSPLjava/util/zip/ZipFile$Source;->checkAndAddEntry([BII)I
 HSPLjava/util/zip/ZipFile$Source;->close()V
 HSPLjava/util/zip/ZipFile$Source;->findEND()Ljava/util/zip/ZipFile$Source$End;
 HSPLjava/util/zip/ZipFile$Source;->get(Ljava/io/File;ZLjava/util/zip/ZipCoder;Z)Ljava/util/zip/ZipFile$Source;
 HSPLjava/util/zip/ZipFile$Source;->getEntryPos(Ljava/lang/String;Z)I
-HSPLjava/util/zip/ZipFile$Source;->getMetaVersion(II)I
-HSPLjava/util/zip/ZipFile$Source;->initCEN(I)V
-HSPLjava/util/zip/ZipFile$Source;->isManifestName(II)Z
+HSPLjava/util/zip/ZipFile$Source;->getMetaVersion([BII)I
+HSPLjava/util/zip/ZipFile$Source;->initCEN([BI)V
+HSPLjava/util/zip/ZipFile$Source;->isManifestName([BII)Z
 HSPLjava/util/zip/ZipFile$Source;->isMetaName([BII)Z
-HSPLjava/util/zip/ZipFile$Source;->isSignatureRelated(II)Z
-HSPLjava/util/zip/ZipFile$Source;->nextEntryPos(III)I
+HSPLjava/util/zip/ZipFile$Source;->isSignatureRelated([BII)Z
 HSPLjava/util/zip/ZipFile$Source;->readAt([BIIJ)I
 HSPLjava/util/zip/ZipFile$Source;->readFullyAt([BIIJ)I
 HSPLjava/util/zip/ZipFile$Source;->release(Ljava/util/zip/ZipFile$Source;)V
@@ -8057,7 +8104,7 @@ HSPLjava/util/zip/ZipFile$ZipEntryIterator;->nextElement()Ljava/util/zip/ZipEntr
 HSPLjava/util/zip/ZipFile$ZipFileInflaterInputStream;->available()I
 HSPLjava/util/zip/ZipFile$ZipFileInflaterInputStream;->close()V
 HSPLjava/util/zip/ZipFile$ZipFileInflaterInputStream;->fill()V
-HSPLjava/util/zip/ZipFile$ZipFileInputStream;-><init>(Ljava/util/zip/ZipFile;[BI)V
+HSPLjava/util/zip/ZipFile$ZipFileInputStream;-><init>(Ljava/util/zip/ZipFile;Ljava/nio/DirectByteBuffer;I)V
 HSPLjava/util/zip/ZipFile$ZipFileInputStream;->available()I
 HSPLjava/util/zip/ZipFile$ZipFileInputStream;->close()V
 HSPLjava/util/zip/ZipFile$ZipFileInputStream;->initDataOffset()J
@@ -8091,8 +8138,11 @@ HSPLjava/util/zip/ZipInputStream;->readEnd(Ljava/util/zip/ZipEntry;)V
 HSPLjava/util/zip/ZipInputStream;->readFully([BII)V
 HSPLjava/util/zip/ZipInputStream;->readLOC()Ljava/util/zip/ZipEntry;
 HSPLjava/util/zip/ZipUtils;->CENFLG([BI)I
+HSPLjava/util/zip/ZipUtils;->CENLEN(Ljava/nio/DirectByteBuffer;I)J
 HSPLjava/util/zip/ZipUtils;->CENLEN([BI)J
+HSPLjava/util/zip/ZipUtils;->CENOFF(Ljava/nio/DirectByteBuffer;I)J
 HSPLjava/util/zip/ZipUtils;->CENOFF([BI)J
+HSPLjava/util/zip/ZipUtils;->CENSIZ(Ljava/nio/DirectByteBuffer;I)J
 HSPLjava/util/zip/ZipUtils;->CENSIZ([BI)J
 HSPLjava/util/zip/ZipUtils;->ENDCOM([B)I
 HSPLjava/util/zip/ZipUtils;->ENDOFF([B)J
@@ -8105,6 +8155,7 @@ HSPLjava/util/zip/ZipUtils;->LOCNAM([B)I
 HSPLjava/util/zip/ZipUtils;->LOCSIG([B)J
 HSPLjava/util/zip/ZipUtils;->SH([BI)I
 HSPLjava/util/zip/ZipUtils;->get16([BI)I
+HSPLjava/util/zip/ZipUtils;->get32(Ljava/nio/DirectByteBuffer;I)J+]Ljava/nio/DirectByteBuffer;Ljava/nio/DirectByteBuffer;
 HSPLjava/util/zip/ZipUtils;->get32([BI)J
 HSPLjava/util/zip/ZipUtils;->unixTimeToFileTime(J)Ljava/nio/file/attribute/FileTime;
 HSPLjavax/crypto/Cipher$CipherSpiAndProvider;-><init>(Ljavax/crypto/CipherSpi;Ljava/security/Provider;)V
@@ -8311,27 +8362,26 @@ HSPLjavax/xml/parsers/SAXParserFactory;-><init>()V
 HSPLjavax/xml/parsers/SAXParserFactory;->newInstance()Ljavax/xml/parsers/SAXParserFactory;
 HSPLjdk/internal/access/SharedSecrets;->getJavaUtilCollectionAccess()Ljdk/internal/access/JavaUtilCollectionAccess;
 HSPLjdk/internal/access/SharedSecrets;->setJavaUtilCollectionAccess(Ljdk/internal/access/JavaUtilCollectionAccess;)V
-HSPLjdk/internal/math/DoubleToDecimal;->toString(D)Ljava/lang/String;
 HSPLjdk/internal/math/DoubleToDecimal;-><init>(Z)V
-HSPLjdk/internal/math/DoubleToDecimal;->split(DLjdk/internal/math/FormattedFPDecimal;)V
-HSPLjdk/internal/math/DoubleToDecimal;->appendTo(DLjava/lang/Appendable;)Ljava/lang/Appendable;
-HSPLjdk/internal/math/DoubleToDecimal;->toDecimalString(D)Ljava/lang/String;
+HSPLjdk/internal/math/DoubleToDecimal;->append(I)V
+HSPLjdk/internal/math/DoubleToDecimal;->append8Digits(I)V
 HSPLjdk/internal/math/DoubleToDecimal;->appendDecimalTo(DLjava/lang/Appendable;)Ljava/lang/Appendable;
-HSPLjdk/internal/math/DoubleToDecimal;->toDecimal(D)I
-HSPLjdk/internal/math/DoubleToDecimal;->toDecimal(IIILjdk/internal/math/FormattedFPDecimal;)I
+HSPLjdk/internal/math/DoubleToDecimal;->appendDigit(I)V
+HSPLjdk/internal/math/DoubleToDecimal;->appendTo(DLjava/lang/Appendable;)Ljava/lang/Appendable;
+HSPLjdk/internal/math/DoubleToDecimal;->exponent(I)V
+HSPLjdk/internal/math/DoubleToDecimal;->lowDigits(I)V
+HSPLjdk/internal/math/DoubleToDecimal;->removeTrailingZeroes()V
 HSPLjdk/internal/math/DoubleToDecimal;->rop(JJJ)J
+HSPLjdk/internal/math/DoubleToDecimal;->split(DLjdk/internal/math/FormattedFPDecimal;)V
 HSPLjdk/internal/math/DoubleToDecimal;->toChars(JILjdk/internal/math/FormattedFPDecimal;)I
 HSPLjdk/internal/math/DoubleToDecimal;->toChars1(IIII)I
 HSPLjdk/internal/math/DoubleToDecimal;->toChars2(IIII)I
 HSPLjdk/internal/math/DoubleToDecimal;->toChars3(IIII)I
-HSPLjdk/internal/math/DoubleToDecimal;->lowDigits(I)V
-HSPLjdk/internal/math/DoubleToDecimal;->append8Digits(I)V
-HSPLjdk/internal/math/DoubleToDecimal;->removeTrailingZeroes()V
+HSPLjdk/internal/math/DoubleToDecimal;->toDecimal(DLjdk/internal/math/FormattedFPDecimal;)I
+HSPLjdk/internal/math/DoubleToDecimal;->toDecimal(IJILjdk/internal/math/FormattedFPDecimal;)I
+HSPLjdk/internal/math/DoubleToDecimal;->toDecimalString(D)Ljava/lang/String;
+HSPLjdk/internal/math/DoubleToDecimal;->toString(D)Ljava/lang/String;
 HSPLjdk/internal/math/DoubleToDecimal;->y(I)I
-HSPLjdk/internal/math/DoubleToDecimal;->exponent(I)V
-HSPLjdk/internal/math/DoubleToDecimal;->append(I)V
-HSPLjdk/internal/math/DoubleToDecimal;->appendDigit(I)V
-HSPLjdk/internal/math/DoubleToDecimal;->charsToString()V
 HSPLjdk/internal/math/FDBigInteger;-><init>(J[CII)V
 HSPLjdk/internal/math/FDBigInteger;-><init>([II)V
 HSPLjdk/internal/math/FDBigInteger;->add(Ljdk/internal/math/FDBigInteger;)Ljdk/internal/math/FDBigInteger;
@@ -8359,6 +8409,24 @@ HSPLjdk/internal/math/FDBigInteger;->trimLeadingZeros()V
 HSPLjdk/internal/math/FDBigInteger;->valueOfMulPow52(JII)Ljdk/internal/math/FDBigInteger;
 HSPLjdk/internal/math/FDBigInteger;->valueOfPow2(I)Ljdk/internal/math/FDBigInteger;
 HSPLjdk/internal/math/FDBigInteger;->valueOfPow52(II)Ljdk/internal/math/FDBigInteger;
+HSPLjdk/internal/math/FloatToDecimal;-><init>()V
+HSPLjdk/internal/math/FloatToDecimal;->append(I)V
+HSPLjdk/internal/math/FloatToDecimal;->append8Digits(I)V
+HSPLjdk/internal/math/FloatToDecimal;->appendDecimalTo(FLjava/lang/Appendable;)Ljava/lang/Appendable;
+HSPLjdk/internal/math/FloatToDecimal;->appendDigit(I)V
+HSPLjdk/internal/math/FloatToDecimal;->appendTo(FLjava/lang/Appendable;)Ljava/lang/Appendable;
+HSPLjdk/internal/math/FloatToDecimal;->exponent(I)V
+HSPLjdk/internal/math/FloatToDecimal;->removeTrailingZeroes()V
+HSPLjdk/internal/math/FloatToDecimal;->rop(JJ)I
+HSPLjdk/internal/math/FloatToDecimal;->toChars(II)I
+HSPLjdk/internal/math/FloatToDecimal;->toChars1(III)I
+HSPLjdk/internal/math/FloatToDecimal;->toChars2(III)I
+HSPLjdk/internal/math/FloatToDecimal;->toChars3(III)I
+HSPLjdk/internal/math/FloatToDecimal;->toDecimal(F)I
+HSPLjdk/internal/math/FloatToDecimal;->toDecimal(III)I
+HSPLjdk/internal/math/FloatToDecimal;->toDecimalString(F)Ljava/lang/String;
+HSPLjdk/internal/math/FloatToDecimal;->toString(F)Ljava/lang/String;
+HSPLjdk/internal/math/FloatToDecimal;->y(I)I
 HSPLjdk/internal/math/FloatingDecimal$1;->initialValue()Ljava/lang/Object;
 HSPLjdk/internal/math/FloatingDecimal$1;->initialValue()Ljdk/internal/math/FloatingDecimal$BinaryToASCIIBuffer;
 HSPLjdk/internal/math/FloatingDecimal$ASCIIToBinaryBuffer;-><init>(ZI[CI)V
@@ -8391,25 +8459,26 @@ HSPLjdk/internal/math/FloatingDecimal;->parseFloat(Ljava/lang/String;)F
 HSPLjdk/internal/math/FloatingDecimal;->readJavaFormatString(Ljava/lang/String;)Ljdk/internal/math/FloatingDecimal$ASCIIToBinaryConverter;
 HSPLjdk/internal/math/FloatingDecimal;->toJavaFormatString(D)Ljava/lang/String;
 HSPLjdk/internal/math/FloatingDecimal;->toJavaFormatString(F)Ljava/lang/String;
-HSPLjdk/internal/math/FloatToDecimal;->toString(F)Ljava/lang/String;
-HSPLjdk/internal/math/FloatToDecimal;-><init>()V
-HSPLjdk/internal/math/FloatToDecimal;->appendTo(FLjava/lang/Appendable;)Ljava/lang/Appendable;
-HSPLjdk/internal/math/FloatToDecimal;->toDecimalString(F)Ljava/lang/String;
-HSPLjdk/internal/math/FloatToDecimal;->appendDecimalTo(FLjava/lang/Appendable;)Ljava/lang/Appendable;
-HSPLjdk/internal/math/FloatToDecimal;->toDecimal(F)I
-HSPLjdk/internal/math/FloatToDecimal;->toDecimal(III)I
-HSPLjdk/internal/math/FloatToDecimal;->rop(JJ)I
-HSPLjdk/internal/math/FloatToDecimal;->toChars(FI)I
-HSPLjdk/internal/math/FloatToDecimal;->toChars1(III)I
-HSPLjdk/internal/math/FloatToDecimal;->toChars2(III)I
-HSPLjdk/internal/math/FloatToDecimal;->toChars3(III)I
-HSPLjdk/internal/math/FloatToDecimal;->append8Digits(I)V
-HSPLjdk/internal/math/FloatToDecimal;->removeTrailingZeroes()V
-HSPLjdk/internal/math/FloatToDecimal;->y(I)I
-HSPLjdk/internal/math/FloatToDecimal;->exponent(I)V
-HSPLjdk/internal/math/FloatToDecimal;->append(I)V
-HSPLjdk/internal/math/FloatToDecimal;->appendDigit(I)V
-HSPLjdk/internal/math/FloatToDecimal;->charsToString()V
+HSPLjdk/internal/math/FormattedFPDecimal;-><init>()V
+HSPLjdk/internal/math/FormattedFPDecimal;->expChars()V
+HSPLjdk/internal/math/FormattedFPDecimal;->fillWithDigits(JII)J
+HSPLjdk/internal/math/FormattedFPDecimal;->getExponent()[C
+HSPLjdk/internal/math/FormattedFPDecimal;->getExponentRounded()I
+HSPLjdk/internal/math/FormattedFPDecimal;->getMantissa()[C
+HSPLjdk/internal/math/FormattedFPDecimal;->plain(I)Ljdk/internal/math/FormattedFPDecimal;
+HSPLjdk/internal/math/FormattedFPDecimal;->plainChars()Ljdk/internal/math/FormattedFPDecimal;
+HSPLjdk/internal/math/FormattedFPDecimal;->plainCharsMixed()V
+HSPLjdk/internal/math/FormattedFPDecimal;->plainCharsPureFraction()V
+HSPLjdk/internal/math/FormattedFPDecimal;->plainCharsPureInteger()V
+HSPLjdk/internal/math/FormattedFPDecimal;->round(J)V
+HSPLjdk/internal/math/FormattedFPDecimal;->scientific(I)Ljdk/internal/math/FormattedFPDecimal;
+HSPLjdk/internal/math/FormattedFPDecimal;->scientificChars(I)Ljdk/internal/math/FormattedFPDecimal;
+HSPLjdk/internal/math/FormattedFPDecimal;->scientificCharsNoFraction()V
+HSPLjdk/internal/math/FormattedFPDecimal;->scientificCharsWithFraction()V
+HSPLjdk/internal/math/FormattedFPDecimal;->set(JII)V
+HSPLjdk/internal/math/FormattedFPDecimal;->toDigit(I)C
+HSPLjdk/internal/math/FormattedFPDecimal;->toDigit(J)C
+HSPLjdk/internal/math/FormattedFPDecimal;->valueOf(DIC)Ljdk/internal/math/FormattedFPDecimal;
 HSPLjdk/internal/math/FormattedFloatingDecimal$1;-><init>()V
 HSPLjdk/internal/math/FormattedFloatingDecimal$1;->initialValue()Ljava/lang/Object;
 HSPLjdk/internal/math/FormattedFloatingDecimal$Form;-><clinit>()V
@@ -8425,34 +8494,12 @@ HSPLjdk/internal/math/FormattedFloatingDecimal;->getExponent()[C
 HSPLjdk/internal/math/FormattedFloatingDecimal;->getExponentRounded()I
 HSPLjdk/internal/math/FormattedFloatingDecimal;->getMantissa()[C
 HSPLjdk/internal/math/FormattedFloatingDecimal;->valueOf(DILjdk/internal/math/FormattedFloatingDecimal$Form;)Ljdk/internal/math/FormattedFloatingDecimal;
-HSPLjdk/internal/math/FormattedFPDecimal;-><init>()V
-HSPLjdk/internal/math/FormattedFPDecimal;->valueOf(DIC)Ljdk/internal/math/FormattedFPDecimal;
-HSPLjdk/internal/math/FormattedFPDecimal;->set(JII)V
-HSPLjdk/internal/math/FormattedFPDecimal;->getExponent()[C
-HSPLjdk/internal/math/FormattedFPDecimal;->getMantissa()[C
-HSPLjdk/internal/math/FormattedFPDecimal;->getExponentRounded()I
-HSPLjdk/internal/math/FormattedFPDecimal;->plain(I)Ljdk/internal/math/FormattedFPDecimal;
-HSPLjdk/internal/math/FormattedFPDecimal;->plainChars()Ljdk/internal/math/FormattedFPDecimal;
-HSPLjdk/internal/math/FormattedFPDecimal;->plainCharsPureInteger()V
-HSPLjdk/internal/math/FormattedFPDecimal;->plainCharsMixed()V
-HSPLjdk/internal/math/FormattedFPDecimal;->plainCharsPureFraction()V
-HSPLjdk/internal/math/FormattedFPDecimal;->scientific(I)Ljdk/internal/math/FormattedFPDecimal;
-HSPLjdk/internal/math/FormattedFPDecimal;->scientificChars(I)Ljdk/internal/math/FormattedFPDecimal;
-HSPLjdk/internal/math/FormattedFPDecimal;->scientificCharsWithFraction()V
-HSPLjdk/internal/math/FormattedFPDecimal;->scientificCharsNoFraction()V
-HSPLjdk/internal/math/FormattedFPDecimal;->general()Ljdk/internal/math/FormattedFPDecimal;
-HSPLjdk/internal/math/FormattedFPDecimal;->expChars()V
-HSPLjdk/internal/math/FormattedFPDecimal;->round(J)V
-HSPLjdk/internal/math/FormattedFPDecimal;->fillWithDigits(JII)J
-HSPLjdk/internal/math/FormattedFPDecimal;->fillWithZeroes(II)V
-HSPLjdk/internal/math/FormattedFPDecimal;->toDigit(J)C
-HSPLjdk/internal/math/FormattedFPDecimal;->toDigit(I)C
-HSPLjdk/internal/math/MathUtils;->pow10(I)J
 HSPLjdk/internal/math/MathUtils;->flog10pow2(I)I
 HSPLjdk/internal/math/MathUtils;->flog10threeQuartersPow2(I)I
-HSPLjdk/internal/math/MathUtils;->flog2pow10(I)J
-HSPLjdk/internal/math/MathUtils;->g1(I)J
+HSPLjdk/internal/math/MathUtils;->flog2pow10(I)I
 HSPLjdk/internal/math/MathUtils;->g0(I)J
+HSPLjdk/internal/math/MathUtils;->g1(I)J
+HSPLjdk/internal/math/MathUtils;->pow10(I)J
 HSPLjdk/internal/misc/Unsafe;->arrayBaseOffset(Ljava/lang/Class;)I
 HSPLjdk/internal/misc/Unsafe;->compareAndSetObject(Ljava/lang/Object;JLjava/lang/Object;Ljava/lang/Object;)Z
 HSPLjdk/internal/misc/Unsafe;->getAndAddInt(Ljava/lang/Object;JI)I
@@ -8506,7 +8553,12 @@ HSPLjdk/internal/util/ArraysSupport;->newLength(III)I
 HSPLjdk/internal/util/ArraysSupport;->vectorizedHashCode(Ljava/lang/Object;IIII)I
 HSPLjdk/internal/util/ArraysSupport;->vectorizedMismatch(Ljava/lang/Object;JLjava/lang/Object;JII)I
 HSPLjdk/internal/util/Preconditions;->checkFromIndexSize(IIILjava/util/function/BiFunction;)I
+HSPLjdk/internal/util/Preconditions;->checkFromToIndex(IIILjava/util/function/BiFunction;)I
 HSPLjdk/internal/util/Preconditions;->checkIndex(IILjava/util/function/BiFunction;)I
+HSPLjdk/internal/util/StrongReferenceKey;->equals(Ljava/lang/Object;)Z
+HSPLjdk/internal/util/StrongReferenceKey;->get()Ljava/lang/Object;
+HSPLjdk/internal/util/StrongReferenceKey;->hashCode()I
+HSPLjdk/internal/util/WeakReferenceKey;->hashCode()I
 HSPLjdk/internal/util/random/RandomSupport;-><clinit>()V
 HSPLjdk/internal/util/random/RandomSupport;->mixMurmur64(J)J
 HSPLjdk/internal/util/random/RandomSupport;->secureRandomSeedRequested()Z
@@ -8710,6 +8762,9 @@ HSPLlibcore/io/IoUtils;->setBlocking(Ljava/io/FileDescriptor;Z)V
 HSPLlibcore/io/IoUtils;->setFdOwner(Ljava/io/FileDescriptor;Ljava/lang/Object;)V
 HSPLlibcore/io/Libcore;->compareAndSetOs(Llibcore/io/Os;Llibcore/io/Os;)Z
 HSPLlibcore/io/Libcore;->getOs()Llibcore/io/Os;
+HSPLlibcore/io/Linux;->getpid()I
+HSPLlibcore/io/Linux;->gettid()I
+HSPLlibcore/io/Linux;->getuid()I
 HSPLlibcore/io/Linux;->pread(Ljava/io/FileDescriptor;[BIIJ)I
 HSPLlibcore/io/Linux;->read(Ljava/io/FileDescriptor;[BII)I
 HSPLlibcore/io/Linux;->recvfrom(Ljava/io/FileDescriptor;[BIIILjava/net/InetSocketAddress;)I
@@ -8830,13 +8885,30 @@ HSPLlibcore/util/NativeAllocationRegistry$CleanerRunner;->run()V
 HSPLlibcore/util/NativeAllocationRegistry$CleanerThunk;-><init>(Llibcore/util/NativeAllocationRegistry;)V
 HSPLlibcore/util/NativeAllocationRegistry$CleanerThunk;->run()V
 HSPLlibcore/util/NativeAllocationRegistry$CleanerThunk;->setNativePtr(J)V
+HSPLlibcore/util/NativeAllocationRegistry$Metrics;->-$$Nest$fgetclassName(Llibcore/util/NativeAllocationRegistry$Metrics;)Ljava/lang/String;
+HSPLlibcore/util/NativeAllocationRegistry$Metrics;->-$$Nest$madd(Llibcore/util/NativeAllocationRegistry$Metrics;Llibcore/util/NativeAllocationRegistry;)V
+HSPLlibcore/util/NativeAllocationRegistry$Metrics;-><init>(Ljava/lang/String;)V
+HSPLlibcore/util/NativeAllocationRegistry$Metrics;-><init>(Ljava/lang/String;Llibcore/util/NativeAllocationRegistry-IA;)V
+HSPLlibcore/util/NativeAllocationRegistry$Metrics;->add(Llibcore/util/NativeAllocationRegistry;)V
+HSPLlibcore/util/NativeAllocationRegistry$Metrics;->getClassName()Ljava/lang/String;
+HSPLlibcore/util/NativeAllocationRegistry$Metrics;->getMallocedBytes()J
+HSPLlibcore/util/NativeAllocationRegistry$Metrics;->getMallocedCount()J
+HSPLlibcore/util/NativeAllocationRegistry$Metrics;->getNonmallocedBytes()J
+HSPLlibcore/util/NativeAllocationRegistry$Metrics;->getNonmallocedCount()J
+HSPLlibcore/util/NativeAllocationRegistry;->-$$Nest$fgetcounter(Llibcore/util/NativeAllocationRegistry;)I
 HSPLlibcore/util/NativeAllocationRegistry;->-$$Nest$fgetfreeFunction(Llibcore/util/NativeAllocationRegistry;)J
 HSPLlibcore/util/NativeAllocationRegistry;->-$$Nest$fgetsize(Llibcore/util/NativeAllocationRegistry;)J
+HSPLlibcore/util/NativeAllocationRegistry;->-$$Nest$misMalloced(Llibcore/util/NativeAllocationRegistry;)Z
+HSPLlibcore/util/NativeAllocationRegistry;->-$$Nest$sfgetCOUNTER()Ljava/lang/invoke/VarHandle;
 HSPLlibcore/util/NativeAllocationRegistry;->-$$Nest$smregisterNativeFree(J)V
-HSPLlibcore/util/NativeAllocationRegistry;-><init>(Ljava/lang/ClassLoader;Ljava/lang/Class;JJZ)V+]Ljava/util/Map;Ljava/util/WeakHashMap;
+HSPLlibcore/util/NativeAllocationRegistry;-><init>(Ljava/lang/ClassLoader;Ljava/lang/Class;JJZ)V
+HSPLlibcore/util/NativeAllocationRegistry;->createMalloced(Ljava/lang/Class;JJ)Llibcore/util/NativeAllocationRegistry;
 HSPLlibcore/util/NativeAllocationRegistry;->createMalloced(Ljava/lang/ClassLoader;J)Llibcore/util/NativeAllocationRegistry;
 HSPLlibcore/util/NativeAllocationRegistry;->createMalloced(Ljava/lang/ClassLoader;JJ)Llibcore/util/NativeAllocationRegistry;
+HSPLlibcore/util/NativeAllocationRegistry;->createNonmalloced(Ljava/lang/Class;JJ)Llibcore/util/NativeAllocationRegistry;
 HSPLlibcore/util/NativeAllocationRegistry;->createNonmalloced(Ljava/lang/ClassLoader;JJ)Llibcore/util/NativeAllocationRegistry;
+HSPLlibcore/util/NativeAllocationRegistry;->getMetrics()Ljava/util/Collection;+]Ljava/util/Iterator;Ljava/util/WeakHashMap$KeyIterator;]Ljava/util/Map;Ljava/util/WeakHashMap;]Ljava/util/Set;Ljava/util/WeakHashMap$KeySet;
+HSPLlibcore/util/NativeAllocationRegistry;->isMalloced()Z
 HSPLlibcore/util/NativeAllocationRegistry;->registerNativeAllocation(J)V
 HSPLlibcore/util/NativeAllocationRegistry;->registerNativeAllocation(Ljava/lang/Object;J)Ljava/lang/Runnable;
 HSPLlibcore/util/NativeAllocationRegistry;->registerNativeFree(J)V
@@ -9120,6 +9192,7 @@ HSPLsun/nio/ch/DefaultSelectorProvider;->create()Ljava/nio/channels/spi/Selector
 HSPLsun/nio/ch/FileChannelImpl$Unmapper;-><init>(JJILjava/io/FileDescriptor;)V
 HSPLsun/nio/ch/FileChannelImpl$Unmapper;-><init>(JJILjava/io/FileDescriptor;Lsun/nio/ch/FileChannelImpl-IA;)V
 HSPLsun/nio/ch/FileChannelImpl$Unmapper;->run()V
+HSPLsun/nio/ch/FileChannelImpl;->-$$Nest$smunmap0(JJ)I
 HSPLsun/nio/ch/FileChannelImpl;-><init>(Ljava/io/FileDescriptor;Ljava/lang/String;ZZLjava/lang/Object;)V
 HSPLsun/nio/ch/FileChannelImpl;->ensureOpen()V
 HSPLsun/nio/ch/FileChannelImpl;->fileLockTable()Lsun/nio/ch/FileLockTable;
@@ -9378,6 +9451,7 @@ HSPLsun/nio/fs/UnixFileKey;-><init>(JJ)V
 HSPLsun/nio/fs/UnixFileKey;->equals(Ljava/lang/Object;)Z
 HSPLsun/nio/fs/UnixFileModeAttribute;->toUnixMode(I[Ljava/nio/file/attribute/FileAttribute;)I
 HSPLsun/nio/fs/UnixFileSystem;->getPath(Ljava/lang/String;[Ljava/lang/String;)Ljava/nio/file/Path;
+HSPLsun/nio/fs/UnixFileSystem;->isReadOnly()Z
 HSPLsun/nio/fs/UnixFileSystem;->needToResolveAgainstDefaultDirectory()Z
 HSPLsun/nio/fs/UnixFileSystem;->normalizeJavaPath(Ljava/lang/String;)Ljava/lang/String;
 HSPLsun/nio/fs/UnixFileSystem;->normalizeNativePath([C)[C
@@ -9436,6 +9510,7 @@ HSPLsun/security/action/GetPropertyAction;-><init>(Ljava/lang/String;)V
 HSPLsun/security/action/GetPropertyAction;->run()Ljava/lang/Object;
 HSPLsun/security/action/GetPropertyAction;->run()Ljava/lang/String;
 HSPLsun/security/jca/GetInstance$Instance;-><init>(Ljava/security/Provider;Ljava/lang/Object;)V
+HSPLsun/security/jca/GetInstance$Instance;-><init>(Ljava/security/Provider;Ljava/lang/Object;Lsun/security/jca/GetInstance-IA;)V
 HSPLsun/security/jca/GetInstance$Instance;->toArray()[Ljava/lang/Object;
 HSPLsun/security/jca/GetInstance;->checkSuperClass(Ljava/security/Provider$Service;Ljava/lang/Class;Ljava/lang/Class;)V
 HSPLsun/security/jca/GetInstance;->getInstance(Ljava/lang/String;Ljava/lang/Class;Ljava/lang/String;)Lsun/security/jca/GetInstance$Instance;
@@ -10078,6 +10153,10 @@ HSPLsun/util/locale/BaseLocale;->getRegion()Ljava/lang/String;
 HSPLsun/util/locale/BaseLocale;->getScript()Ljava/lang/String;
 HSPLsun/util/locale/BaseLocale;->getVariant()Ljava/lang/String;
 HSPLsun/util/locale/BaseLocale;->hashCode()I
+HSPLsun/util/locale/Extension;->getID()Ljava/lang/String;
+HSPLsun/util/locale/Extension;->setValue(Ljava/lang/String;)V
+HSPLsun/util/locale/Extension;->toString()Ljava/lang/String;+]Lsun/util/locale/Extension;Lsun/util/locale/UnicodeLocaleExtension;
+HSPLsun/util/locale/InternalLocaleBuilder$CaseInsensitiveChar;->hashCode()I
 HSPLsun/util/locale/InternalLocaleBuilder;-><init>()V
 HSPLsun/util/locale/InternalLocaleBuilder;->checkVariants(Ljava/lang/String;Ljava/lang/String;)I
 HSPLsun/util/locale/InternalLocaleBuilder;->clear()Lsun/util/locale/InternalLocaleBuilder;
@@ -10089,6 +10168,7 @@ HSPLsun/util/locale/InternalLocaleBuilder;->setLanguage(Ljava/lang/String;)Lsun/
 HSPLsun/util/locale/InternalLocaleBuilder;->setLanguageTag(Lsun/util/locale/LanguageTag;)Lsun/util/locale/InternalLocaleBuilder;
 HSPLsun/util/locale/InternalLocaleBuilder;->setRegion(Ljava/lang/String;)Lsun/util/locale/InternalLocaleBuilder;
 HSPLsun/util/locale/InternalLocaleBuilder;->setScript(Ljava/lang/String;)Lsun/util/locale/InternalLocaleBuilder;
+HSPLsun/util/locale/InternalLocaleBuilder;->setUnicodeLocaleExtension(Ljava/lang/String;)V+]Ljava/util/Map;Ljava/util/HashMap;
 HSPLsun/util/locale/InternalLocaleBuilder;->setVariant(Ljava/lang/String;)Lsun/util/locale/InternalLocaleBuilder;
 HSPLsun/util/locale/LanguageTag;-><init>()V
 HSPLsun/util/locale/LanguageTag;->canonicalizeLanguage(Ljava/lang/String;)Ljava/lang/String;
@@ -10103,6 +10183,7 @@ HSPLsun/util/locale/LanguageTag;->getScript()Ljava/lang/String;
 HSPLsun/util/locale/LanguageTag;->getVariants()Ljava/util/List;
 HSPLsun/util/locale/LanguageTag;->isExtlang(Ljava/lang/String;)Z
 HSPLsun/util/locale/LanguageTag;->isLanguage(Ljava/lang/String;)Z
+HSPLsun/util/locale/LanguageTag;->isPrivateusePrefixChar(C)Z
 HSPLsun/util/locale/LanguageTag;->isRegion(Ljava/lang/String;)Z
 HSPLsun/util/locale/LanguageTag;->isScript(Ljava/lang/String;)Z
 HSPLsun/util/locale/LanguageTag;->isVariant(Ljava/lang/String;)Z
@@ -10115,6 +10196,10 @@ HSPLsun/util/locale/LanguageTag;->parsePrivateuse(Lsun/util/locale/StringTokenIt
 HSPLsun/util/locale/LanguageTag;->parseRegion(Lsun/util/locale/StringTokenIterator;Lsun/util/locale/ParseStatus;)Z
 HSPLsun/util/locale/LanguageTag;->parseScript(Lsun/util/locale/StringTokenIterator;Lsun/util/locale/ParseStatus;)Z
 HSPLsun/util/locale/LanguageTag;->parseVariants(Lsun/util/locale/StringTokenIterator;Lsun/util/locale/ParseStatus;)Z
+HSPLsun/util/locale/LocaleExtensions;-><init>(Ljava/util/Map;Ljava/util/Set;Ljava/util/Map;)V+]Ljava/util/Iterator;Ljava/util/HashMap$EntryIterator;]Ljava/util/Map$Entry;Ljava/util/HashMap$Node;]Ljava/util/Map;Ljava/util/HashMap;]Ljava/util/Set;Ljava/util/HashMap$EntrySet;
+HSPLsun/util/locale/LocaleExtensions;->equals(Ljava/lang/Object;)Z
+HSPLsun/util/locale/LocaleExtensions;->hashCode()I
+HSPLsun/util/locale/LocaleExtensions;->toID(Ljava/util/SortedMap;)Ljava/lang/String;+]Ljava/util/Iterator;Ljava/util/TreeMap$EntryIterator;]Ljava/util/Map$Entry;Ljava/util/TreeMap$TreeMapEntry;]Ljava/util/Set;Ljava/util/TreeMap$EntrySet;]Ljava/util/SortedMap;Ljava/util/TreeMap;
 HSPLsun/util/locale/LocaleObjectCache$CacheEntry;-><init>(Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/ref/ReferenceQueue;)V
 HSPLsun/util/locale/LocaleObjectCache$CacheEntry;->getKey()Ljava/lang/Object;
 HSPLsun/util/locale/LocaleObjectCache;->cleanStaleEntries()V
@@ -10143,11 +10228,13 @@ HSPLsun/util/locale/ParseStatus;->reset()V
 HSPLsun/util/locale/StringTokenIterator;-><init>(Ljava/lang/String;Ljava/lang/String;)V
 HSPLsun/util/locale/StringTokenIterator;->current()Ljava/lang/String;
 HSPLsun/util/locale/StringTokenIterator;->currentEnd()I
+HSPLsun/util/locale/StringTokenIterator;->currentStart()I
 HSPLsun/util/locale/StringTokenIterator;->hasNext()Z
 HSPLsun/util/locale/StringTokenIterator;->isDone()Z
 HSPLsun/util/locale/StringTokenIterator;->next()Ljava/lang/String;
 HSPLsun/util/locale/StringTokenIterator;->nextDelimiter(I)I
 HSPLsun/util/locale/StringTokenIterator;->setStart(I)Lsun/util/locale/StringTokenIterator;
+HSPLsun/util/locale/UnicodeLocaleExtension;-><init>(Ljava/util/SortedSet;Ljava/util/SortedMap;)V+]Ljava/util/Iterator;Ljava/util/Collections$EmptyIterator;,Ljava/util/TreeMap$EntryIterator;]Ljava/util/Map$Entry;Ljava/util/TreeMap$TreeMapEntry;]Ljava/util/Map;Ljava/util/TreeMap;]Ljava/util/Set;Ljava/util/Collections$EmptySet;,Ljava/util/TreeMap$EntrySet;]Lsun/util/locale/UnicodeLocaleExtension;Lsun/util/locale/UnicodeLocaleExtension;
 HSPLsun/util/locale/provider/CalendarDataUtility;->retrieveFirstDayOfWeek(Ljava/util/Locale;I)I
 HSPLsun/util/logging/LoggingSupport$2;-><init>()V
 HSPLsun/util/logging/LoggingSupport$2;->run()Ljava/lang/Object;
@@ -10159,10 +10246,22 @@ HSPLsun/util/logging/PlatformLogger$JavaLoggerProxy;-><init>(Ljava/lang/String;L
 HSPLsun/util/logging/PlatformLogger$LoggerProxy;-><init>(Ljava/lang/String;)V
 HSPLsun/util/logging/PlatformLogger;-><init>(Ljava/lang/String;)V
 HSPLsun/util/logging/PlatformLogger;->getLogger(Ljava/lang/String;)Lsun/util/logging/PlatformLogger;
+Landroid/app/ActivityThread$AndroidOs;
+Landroid/app/AppOpsManager$$ExternalSyntheticLambda4;
 Landroid/compat/Compatibility$1;
 Landroid/compat/Compatibility$BehaviorChangeDelegate;
 Landroid/compat/Compatibility$ChangeConfig;
 Landroid/compat/Compatibility;
+Landroid/content/ContentCaptureOptions$ContentProtectionOptions$$ExternalSyntheticLambda0;
+Landroid/content/res/Resources$$ExternalSyntheticLambda1;
+Landroid/crypto/hpke/HpkeSpi;
+Landroid/graphics/ColorSpace$Rgb$$ExternalSyntheticLambda4;
+Landroid/graphics/ColorSpace$Rgb$$ExternalSyntheticLambda6;
+Landroid/graphics/ColorSpace$Rgb$$ExternalSyntheticLambda9;
+Landroid/icu/text/RuleBasedBreakIterator;
+Landroid/os/ParcelFileDescriptor$AutoCloseInputStream;
+Landroid/os/StrictMode$AndroidBlockGuardPolicy;
+Landroid/service/notification/StatusBarNotification$$ExternalSyntheticLambda0;
 Landroid/system/ErrnoException;
 Landroid/system/GaiException;
 Landroid/system/Int32Ref;
@@ -10191,9 +10290,19 @@ Landroid/system/StructUtsname;
 Landroid/system/SystemCleaner;
 Landroid/system/UnixSocketAddress;
 Landroid/system/VmSocketAddress;
+Landroid/util/ArrayMap;
+Landroid/util/IndentingPrintWriter;
+Landroid/util/MapCollections$ArrayIterator;
+Landroid/util/MapCollections$EntrySet;
+Landroid/util/MapCollections$KeySet;
+Landroid/util/MapCollections$MapIterator;
+Landroid/util/MapCollections$ValuesCollection;
+Landroid/widget/RemoteViews$ApplicationInfoCache$$ExternalSyntheticLambda0;
 Lcom/android/art/flags/FeatureFlags;
 Lcom/android/art/flags/FeatureFlagsImpl;
 Lcom/android/art/flags/Flags;
+Lcom/android/internal/util/FastPrintWriter;
+Lcom/android/internal/util/IndentingPrintWriter;
 Lcom/android/libcore/FeatureFlags;
 Lcom/android/libcore/FeatureFlagsImpl;
 Lcom/android/libcore/Flags;
@@ -10357,12 +10466,12 @@ Lcom/android/okhttp/okio/Timeout$1;
 Lcom/android/okhttp/okio/Timeout;
 Lcom/android/okhttp/okio/Util;
 Lcom/android/org/bouncycastle/asn1/ASN1ApplicationSpecific;
+Lcom/android/org/bouncycastle/asn1/ASN1ApplicationSpecificParser;
 Lcom/android/org/bouncycastle/asn1/ASN1BitString;
 Lcom/android/org/bouncycastle/asn1/ASN1Boolean;
 Lcom/android/org/bouncycastle/asn1/ASN1Choice;
 Lcom/android/org/bouncycastle/asn1/ASN1Encodable;
 Lcom/android/org/bouncycastle/asn1/ASN1EncodableVector;
-Lcom/android/org/bouncycastle/asn1/ASN1Enumerated$$ExternalSyntheticOutline0;
 Lcom/android/org/bouncycastle/asn1/ASN1Enumerated;
 Lcom/android/org/bouncycastle/asn1/ASN1Exception;
 Lcom/android/org/bouncycastle/asn1/ASN1External;
@@ -10386,6 +10495,7 @@ Lcom/android/org/bouncycastle/asn1/ASN1SetParser;
 Lcom/android/org/bouncycastle/asn1/ASN1StreamParser;
 Lcom/android/org/bouncycastle/asn1/ASN1String;
 Lcom/android/org/bouncycastle/asn1/ASN1TaggedObject;
+Lcom/android/org/bouncycastle/asn1/ASN1TaggedObjectParser;
 Lcom/android/org/bouncycastle/asn1/ASN1UTCTime;
 Lcom/android/org/bouncycastle/asn1/BERApplicationSpecific;
 Lcom/android/org/bouncycastle/asn1/BERApplicationSpecificParser;
@@ -10396,6 +10506,7 @@ Lcom/android/org/bouncycastle/asn1/BERSequenceParser;
 Lcom/android/org/bouncycastle/asn1/BERSet;
 Lcom/android/org/bouncycastle/asn1/BERSetParser;
 Lcom/android/org/bouncycastle/asn1/BERTaggedObjectParser;
+Lcom/android/org/bouncycastle/asn1/BERTags;
 Lcom/android/org/bouncycastle/asn1/ConstructedOctetStream;
 Lcom/android/org/bouncycastle/asn1/DERBMPString;
 Lcom/android/org/bouncycastle/asn1/DERBitString;
@@ -10427,6 +10538,7 @@ Lcom/android/org/bouncycastle/asn1/InMemoryRepresentable;
 Lcom/android/org/bouncycastle/asn1/IndefiniteLengthInputStream;
 Lcom/android/org/bouncycastle/asn1/LazyEncodedSequence;
 Lcom/android/org/bouncycastle/asn1/LimitedInputStream;
+Lcom/android/org/bouncycastle/asn1/OIDTokenizer;
 Lcom/android/org/bouncycastle/asn1/StreamUtil;
 Lcom/android/org/bouncycastle/asn1/bc/BCObjectIdentifiers;
 Lcom/android/org/bouncycastle/asn1/misc/MiscObjectIdentifiers;
@@ -10434,6 +10546,7 @@ Lcom/android/org/bouncycastle/asn1/nist/NISTObjectIdentifiers;
 Lcom/android/org/bouncycastle/asn1/oiw/OIWObjectIdentifiers;
 Lcom/android/org/bouncycastle/asn1/pkcs/PKCSObjectIdentifiers;
 Lcom/android/org/bouncycastle/asn1/x500/X500Name;
+Lcom/android/org/bouncycastle/asn1/x500/X500NameStyle;
 Lcom/android/org/bouncycastle/asn1/x500/style/AbstractX500NameStyle;
 Lcom/android/org/bouncycastle/asn1/x500/style/BCStyle;
 Lcom/android/org/bouncycastle/asn1/x509/AlgorithmIdentifier;
@@ -10456,12 +10569,14 @@ Lcom/android/org/bouncycastle/crypto/DataLengthException;
 Lcom/android/org/bouncycastle/crypto/Digest;
 Lcom/android/org/bouncycastle/crypto/ExtendedDigest;
 Lcom/android/org/bouncycastle/crypto/InvalidCipherTextException;
+Lcom/android/org/bouncycastle/crypto/Mac;
 Lcom/android/org/bouncycastle/crypto/OutputLengthException;
 Lcom/android/org/bouncycastle/crypto/PBEParametersGenerator;
 Lcom/android/org/bouncycastle/crypto/RuntimeCryptoException;
 Lcom/android/org/bouncycastle/crypto/Wrapper;
 Lcom/android/org/bouncycastle/crypto/digests/AndroidDigestFactory;
 Lcom/android/org/bouncycastle/crypto/digests/AndroidDigestFactoryBouncyCastle;
+Lcom/android/org/bouncycastle/crypto/digests/AndroidDigestFactoryInterface;
 Lcom/android/org/bouncycastle/crypto/digests/AndroidDigestFactoryOpenSSL;
 Lcom/android/org/bouncycastle/crypto/digests/EncodableDigest;
 Lcom/android/org/bouncycastle/crypto/digests/GeneralDigest;
@@ -10488,6 +10603,7 @@ Lcom/android/org/bouncycastle/crypto/generators/PKCS5S2ParametersGenerator;
 Lcom/android/org/bouncycastle/crypto/io/MacInputStream;
 Lcom/android/org/bouncycastle/crypto/macs/HMac;
 Lcom/android/org/bouncycastle/crypto/modes/AEADBlockCipher;
+Lcom/android/org/bouncycastle/crypto/modes/AEADCipher;
 Lcom/android/org/bouncycastle/crypto/modes/CBCBlockCipher;
 Lcom/android/org/bouncycastle/crypto/paddings/BlockCipherPadding;
 Lcom/android/org/bouncycastle/crypto/paddings/PKCS7Padding;
@@ -10503,16 +10619,22 @@ Lcom/android/org/bouncycastle/crypto/params/DSAValidationParameters;
 Lcom/android/org/bouncycastle/crypto/params/KeyParameter;
 Lcom/android/org/bouncycastle/crypto/params/ParametersWithIV;
 Lcom/android/org/bouncycastle/crypto/params/ParametersWithRandom;
+Lcom/android/org/bouncycastle/jcajce/PBKDFKey;
 Lcom/android/org/bouncycastle/jcajce/PKCS12Key;
+Lcom/android/org/bouncycastle/jcajce/PKCS12KeyWithParameters;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/DH$Mappings;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/DH;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/DSA$Mappings;
+Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/EC$Mappings;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/RSA$Mappings;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/RSA;
+Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/X509$Mappings;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/dh/KeyFactorySpi;
+Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/dsa/BCDSAPublicKey;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/dsa/DSAUtil;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/dsa/KeyFactorySpi;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/rsa/CipherSpi$NoPadding;
+Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/rsa/CipherSpi;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/rsa/KeyFactorySpi;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/rsa/RSAUtil;
 Lcom/android/org/bouncycastle/jcajce/provider/asymmetric/util/BaseCipherSpi;
@@ -10525,7 +10647,8 @@ Lcom/android/org/bouncycastle/jcajce/provider/config/ConfigurableProvider;
 Lcom/android/org/bouncycastle/jcajce/provider/config/ProviderConfiguration;
 Lcom/android/org/bouncycastle/jcajce/provider/config/ProviderConfigurationPermission;
 Lcom/android/org/bouncycastle/jcajce/provider/digest/DigestAlgorithmProvider;
-Lcom/android/org/bouncycastle/jcajce/provider/digest/SHA1$Mappings$$ExternalSyntheticOutline0;
+Lcom/android/org/bouncycastle/jcajce/provider/digest/MD5$Mappings;
+Lcom/android/org/bouncycastle/jcajce/provider/digest/MD5;
 Lcom/android/org/bouncycastle/jcajce/provider/digest/SHA1$Mappings;
 Lcom/android/org/bouncycastle/jcajce/provider/digest/SHA1;
 Lcom/android/org/bouncycastle/jcajce/provider/digest/SHA224$Mappings;
@@ -10541,6 +10664,7 @@ Lcom/android/org/bouncycastle/jcajce/provider/keystore/PKCS12$Mappings;
 Lcom/android/org/bouncycastle/jcajce/provider/keystore/bc/BcKeyStoreSpi$Std;
 Lcom/android/org/bouncycastle/jcajce/provider/keystore/bc/BcKeyStoreSpi$StoreEntry;
 Lcom/android/org/bouncycastle/jcajce/provider/keystore/bc/BcKeyStoreSpi;
+Lcom/android/org/bouncycastle/jcajce/provider/symmetric/AES$ECB$1;
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/AES$ECB;
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/AES$Mappings;
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/AES;
@@ -10575,6 +10699,7 @@ Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/BaseSecretKeyFactor
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/BaseWrapCipher$ErasableOutputStream;
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/BaseWrapCipher$InvalidKeyOrParametersException;
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/BaseWrapCipher;
+Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/BlockCipherProvider;
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/ClassUtil$1;
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/ClassUtil;
 Lcom/android/org/bouncycastle/jcajce/provider/symmetric/util/GcmSpecUtil$2;
@@ -10585,11 +10710,14 @@ Lcom/android/org/bouncycastle/jcajce/provider/util/AlgorithmProvider;
 Lcom/android/org/bouncycastle/jcajce/provider/util/AsymmetricAlgorithmProvider;
 Lcom/android/org/bouncycastle/jcajce/provider/util/AsymmetricKeyInfoConverter;
 Lcom/android/org/bouncycastle/jcajce/provider/util/DigestFactory;
+Lcom/android/org/bouncycastle/jcajce/spec/AEADParameterSpec;
 Lcom/android/org/bouncycastle/jcajce/spec/PBKDF2KeySpec;
 Lcom/android/org/bouncycastle/jcajce/util/BCJcaJceHelper;
 Lcom/android/org/bouncycastle/jcajce/util/DefaultJcaJceHelper;
 Lcom/android/org/bouncycastle/jcajce/util/JcaJceHelper;
+Lcom/android/org/bouncycastle/jcajce/util/ProviderJcaJceHelper;
 Lcom/android/org/bouncycastle/jce/X509Principal;
+Lcom/android/org/bouncycastle/jce/interfaces/BCKeyStore;
 Lcom/android/org/bouncycastle/jce/interfaces/PKCS12BagAttributeCarrier;
 Lcom/android/org/bouncycastle/jce/provider/BouncyCastleProvider$1;
 Lcom/android/org/bouncycastle/jce/provider/BouncyCastleProvider$PrivateProvider;
@@ -10599,18 +10727,23 @@ Lcom/android/org/bouncycastle/jce/provider/CertStoreCollectionSpi;
 Lcom/android/org/bouncycastle/jce/provider/X509CRLObject;
 Lcom/android/org/bouncycastle/util/Arrays;
 Lcom/android/org/bouncycastle/util/BigIntegers;
+Lcom/android/org/bouncycastle/util/Encodable;
 Lcom/android/org/bouncycastle/util/Integers;
+Lcom/android/org/bouncycastle/util/Iterable;
 Lcom/android/org/bouncycastle/util/Memoable;
 Lcom/android/org/bouncycastle/util/Pack;
 Lcom/android/org/bouncycastle/util/Properties$1;
 Lcom/android/org/bouncycastle/util/Properties;
 Lcom/android/org/bouncycastle/util/Strings$1;
 Lcom/android/org/bouncycastle/util/Strings;
+Lcom/android/org/bouncycastle/util/encoders/Encoder;
 Lcom/android/org/bouncycastle/util/encoders/Hex;
 Lcom/android/org/bouncycastle/util/encoders/HexEncoder;
 Lcom/android/org/bouncycastle/util/io/Streams;
 Lcom/android/org/kxml2/io/KXmlParser$ContentSource;
 Lcom/android/org/kxml2/io/KXmlParser$ValueContext;
+Lcom/android/org/kxml2/io/KXmlParser;
+Lcom/android/org/kxml2/io/KXmlSerializer;
 Lcom/sun/security/cert/internal/x509/X509V1CertImpl;
 Ldalvik/annotation/optimization/CriticalNative;
 Ldalvik/annotation/optimization/FastNative;
@@ -10619,6 +10752,7 @@ Ldalvik/annotation/optimization/NeverInline;
 Ldalvik/system/AppSpecializationHooks;
 Ldalvik/system/BaseDexClassLoader$Reporter;
 Ldalvik/system/BaseDexClassLoader;
+Ldalvik/system/BlockGuard$1;
 Ldalvik/system/BlockGuard$2;
 Ldalvik/system/BlockGuard$3;
 Ldalvik/system/BlockGuard$BlockGuardPolicyException;
@@ -10660,6 +10794,7 @@ Ljava/awt/font/NumericShaper;
 Ljava/awt/font/TextAttribute;
 Ljava/io/Bits;
 Ljava/io/BufferedInputStream;
+Ljava/io/BufferedOutputStream;
 Ljava/io/BufferedReader;
 Ljava/io/BufferedWriter;
 Ljava/io/ByteArrayInputStream;
@@ -10726,6 +10861,7 @@ Ljava/io/ObjectOutputStream$Caches;
 Ljava/io/ObjectOutputStream$DebugTraceInfoStack;
 Ljava/io/ObjectOutputStream$HandleTable;
 Ljava/io/ObjectOutputStream$PutField;
+Ljava/io/ObjectOutputStream$PutFieldImpl;
 Ljava/io/ObjectOutputStream$ReplaceTable;
 Ljava/io/ObjectOutputStream;
 Ljava/io/ObjectStreamClass$$ExternalSyntheticLambda0;
@@ -10938,6 +11074,7 @@ Ljava/lang/Thread$WeakClassKey;
 Ljava/lang/Thread;
 Ljava/lang/ThreadDeath;
 Ljava/lang/ThreadGroup;
+Ljava/lang/ThreadLocal$SuppliedThreadLocal;
 Ljava/lang/ThreadLocal$ThreadLocalMap$Entry;
 Ljava/lang/ThreadLocal$ThreadLocalMap;
 Ljava/lang/ThreadLocal-IA;
@@ -10953,8 +11090,10 @@ Ljava/lang/UNIXProcess$1;
 Ljava/lang/UNIXProcess$2;
 Ljava/lang/UNIXProcess$3;
 Ljava/lang/UNIXProcess$ProcessPipeInputStream;
+Ljava/lang/UNIXProcess$ProcessPipeOutputStream;
 Ljava/lang/UNIXProcess$ProcessReaperThreadFactory$1;
 Ljava/lang/UNIXProcess$ProcessReaperThreadFactory;
+Ljava/lang/UNIXProcess;
 Ljava/lang/UnsatisfiedLinkError;
 Ljava/lang/UnsupportedClassVersionError;
 Ljava/lang/UnsupportedOperationException;
@@ -11209,8 +11348,10 @@ Ljava/net/SocketAddress;
 Ljava/net/SocketException;
 Ljava/net/SocketImpl;
 Ljava/net/SocketImplFactory;
+Ljava/net/SocketInputStream;
 Ljava/net/SocketOption;
 Ljava/net/SocketOptions;
+Ljava/net/SocketOutputStream;
 Ljava/net/SocketPermission;
 Ljava/net/SocketTimeoutException;
 Ljava/net/SocksConsts;
@@ -11237,12 +11378,18 @@ Ljava/nio/BufferUnderflowException;
 Ljava/nio/ByteBuffer;
 Ljava/nio/ByteBufferAsCharBuffer;
 Ljava/nio/ByteBufferAsDoubleBuffer;
+Ljava/nio/ByteBufferAsFloatBuffer;
+Ljava/nio/ByteBufferAsIntBuffer;
+Ljava/nio/ByteBufferAsLongBuffer;
 Ljava/nio/ByteBufferAsShortBuffer;
 Ljava/nio/ByteOrder;
 Ljava/nio/CharBuffer;
 Ljava/nio/DirectByteBuffer$MemoryRef;
+Ljava/nio/DirectByteBuffer;
 Ljava/nio/DoubleBuffer;
 Ljava/nio/FloatBuffer;
+Ljava/nio/HeapByteBuffer;
+Ljava/nio/HeapCharBuffer;
 Ljava/nio/HeapIntBuffer;
 Ljava/nio/IntBuffer;
 Ljava/nio/InvalidMarkException;
@@ -11394,6 +11541,7 @@ Ljava/security/KeyStore$TrustedCertificateEntry;
 Ljava/security/KeyStore;
 Ljava/security/KeyStoreException;
 Ljava/security/KeyStoreSpi;
+Ljava/security/MessageDigest$Delegate;
 Ljava/security/MessageDigest;
 Ljava/security/MessageDigestSpi;
 Ljava/security/NoSuchAlgorithmException;
@@ -11555,6 +11703,7 @@ Ljava/text/ParsePosition;
 Ljava/text/RuleBasedCollator;
 Ljava/text/SimpleDateFormat;
 Ljava/text/StringCharacterIterator;
+Ljava/time/Clock$SystemClock;
 Ljava/time/Clock;
 Ljava/time/DateTimeException;
 Ljava/time/DayOfWeek;
@@ -11573,6 +11722,7 @@ Ljava/time/Period;
 Ljava/time/Year;
 Ljava/time/ZoneId;
 Ljava/time/ZoneOffset;
+Ljava/time/ZoneRegion;
 Ljava/time/ZonedDateTime$$ExternalSyntheticLambda0;
 Ljava/time/ZonedDateTime$1;
 Ljava/time/ZonedDateTime;
@@ -11663,6 +11813,7 @@ Ljava/time/zone/ZoneRules;
 Ljava/time/zone/ZoneRulesException;
 Ljava/time/zone/ZoneRulesProvider;
 Ljava/util/AbstractCollection;
+Ljava/util/AbstractList$Itr;
 Ljava/util/AbstractList$ListItr;
 Ljava/util/AbstractList$RandomAccessSpliterator;
 Ljava/util/AbstractList$RandomAccessSubList;
@@ -11672,6 +11823,7 @@ Ljava/util/AbstractList-IA;
 Ljava/util/AbstractList;
 Ljava/util/AbstractMap$1;
 Ljava/util/AbstractMap$2$1;
+Ljava/util/AbstractMap$2;
 Ljava/util/AbstractMap$SimpleEntry;
 Ljava/util/AbstractMap$SimpleImmutableEntry;
 Ljava/util/AbstractMap$ViewCollection;
@@ -11680,10 +11832,15 @@ Ljava/util/AbstractQueue;
 Ljava/util/AbstractSequentialList;
 Ljava/util/AbstractSet;
 Ljava/util/ArrayDeque$$ExternalSyntheticLambda1;
+Ljava/util/ArrayDeque$DeqIterator;
+Ljava/util/ArrayDeque$DescendingIterator;
 Ljava/util/ArrayDeque;
 Ljava/util/ArrayList$ArrayListSpliterator;
+Ljava/util/ArrayList$Itr;
 Ljava/util/ArrayList$ListItr;
+Ljava/util/ArrayList$SubList$1;
 Ljava/util/ArrayList$SubList$2;
+Ljava/util/ArrayList$SubList;
 Ljava/util/ArrayList;
 Ljava/util/ArrayPrefixHelpers$CumulateTask;
 Ljava/util/ArrayPrefixHelpers$DoubleCumulateTask;
@@ -11693,6 +11850,8 @@ Ljava/util/Arrays$$ExternalSyntheticLambda0;
 Ljava/util/Arrays$$ExternalSyntheticLambda1;
 Ljava/util/Arrays$$ExternalSyntheticLambda2;
 Ljava/util/Arrays$$ExternalSyntheticLambda3;
+Ljava/util/Arrays$ArrayItr;
+Ljava/util/Arrays$ArrayList;
 Ljava/util/Arrays$NaturalOrder;
 Ljava/util/Arrays;
 Ljava/util/ArraysParallelSortHelpers$FJObject$Sorter;
@@ -11704,7 +11863,9 @@ Ljava/util/Calendar$$ExternalSyntheticLambda0;
 Ljava/util/Calendar$Builder;
 Ljava/util/Calendar;
 Ljava/util/Collection;
+Ljava/util/Collections$1;
 Ljava/util/Collections$2;
+Ljava/util/Collections$3;
 Ljava/util/Collections$AsLIFOQueue;
 Ljava/util/Collections$CheckedCollection;
 Ljava/util/Collections$CheckedList;
@@ -11718,31 +11879,50 @@ Ljava/util/Collections$CheckedSortedMap;
 Ljava/util/Collections$CheckedSortedSet;
 Ljava/util/Collections$CopiesList;
 Ljava/util/Collections$EmptyEnumeration;
+Ljava/util/Collections$EmptyIterator;
+Ljava/util/Collections$EmptyList;
 Ljava/util/Collections$EmptyListIterator;
+Ljava/util/Collections$EmptyMap;
+Ljava/util/Collections$EmptySet;
 Ljava/util/Collections$ReverseComparator2;
 Ljava/util/Collections$ReverseComparator;
 Ljava/util/Collections$SequencedSetFromMap;
 Ljava/util/Collections$SetFromMap;
+Ljava/util/Collections$SingletonList;
+Ljava/util/Collections$SingletonMap;
+Ljava/util/Collections$SingletonSet;
+Ljava/util/Collections$SynchronizedCollection;
 Ljava/util/Collections$SynchronizedList;
+Ljava/util/Collections$SynchronizedMap;
 Ljava/util/Collections$SynchronizedNavigableMap;
 Ljava/util/Collections$SynchronizedNavigableSet;
 Ljava/util/Collections$SynchronizedRandomAccessList;
 Ljava/util/Collections$SynchronizedSet;
 Ljava/util/Collections$SynchronizedSortedMap;
 Ljava/util/Collections$SynchronizedSortedSet;
+Ljava/util/Collections$UnmodifiableCollection$1;
+Ljava/util/Collections$UnmodifiableCollection;
 Ljava/util/Collections$UnmodifiableList$1;
 Ljava/util/Collections$UnmodifiableList;
+Ljava/util/Collections$UnmodifiableMap$UnmodifiableEntrySet$1;
+Ljava/util/Collections$UnmodifiableMap$UnmodifiableEntrySet$UnmodifiableEntry;
+Ljava/util/Collections$UnmodifiableMap$UnmodifiableEntrySet;
+Ljava/util/Collections$UnmodifiableMap;
 Ljava/util/Collections$UnmodifiableNavigableMap$EmptyNavigableMap;
 Ljava/util/Collections$UnmodifiableNavigableMap;
 Ljava/util/Collections$UnmodifiableNavigableSet$EmptyNavigableSet;
 Ljava/util/Collections$UnmodifiableNavigableSet;
+Ljava/util/Collections$UnmodifiableRandomAccessList;
 Ljava/util/Collections$UnmodifiableSequencedCollection;
 Ljava/util/Collections$UnmodifiableSequencedMap;
 Ljava/util/Collections$UnmodifiableSequencedSet;
+Ljava/util/Collections$UnmodifiableSet;
 Ljava/util/Collections$UnmodifiableSortedMap;
+Ljava/util/Collections$UnmodifiableSortedSet;
 Ljava/util/Collections-IA;
 Ljava/util/Collections;
 Ljava/util/ComparableTimSort;
+Ljava/util/Comparator$$ExternalSyntheticLambda0;
 Ljava/util/Comparator$$ExternalSyntheticLambda1;
 Ljava/util/Comparator$$ExternalSyntheticLambda2;
 Ljava/util/Comparator$$ExternalSyntheticLambda3;
@@ -11789,15 +11969,25 @@ Ljava/util/Formatter$FormatSpecifierParser;
 Ljava/util/Formatter$FormatString;
 Ljava/util/Formatter;
 Ljava/util/FormatterClosedException;
+Ljava/util/GregorianCalendar;
+Ljava/util/HashMap$EntryIterator;
+Ljava/util/HashMap$EntrySet;
 Ljava/util/HashMap$EntrySpliterator;
 Ljava/util/HashMap$HashIterator;
 Ljava/util/HashMap$HashMapSpliterator;
+Ljava/util/HashMap$KeyIterator;
+Ljava/util/HashMap$KeySet;
 Ljava/util/HashMap$KeySpliterator;
+Ljava/util/HashMap$Node;
+Ljava/util/HashMap$TreeNode;
 Ljava/util/HashMap$UnsafeHolder;
+Ljava/util/HashMap$ValueIterator;
 Ljava/util/HashMap$ValueSpliterator;
+Ljava/util/HashMap$Values;
 Ljava/util/HashMap;
 Ljava/util/HashSet;
 Ljava/util/Hashtable$EntrySet;
+Ljava/util/Hashtable$Enumerator;
 Ljava/util/Hashtable$HashtableEntry;
 Ljava/util/Hashtable$KeySet;
 Ljava/util/Hashtable$ValueCollection;
@@ -11828,8 +12018,16 @@ Ljava/util/ImmutableCollections$AbstractImmutableMap;
 Ljava/util/ImmutableCollections$AbstractImmutableSet;
 Ljava/util/ImmutableCollections$Access$1;
 Ljava/util/ImmutableCollections$Access;
+Ljava/util/ImmutableCollections$List12;
+Ljava/util/ImmutableCollections$ListItr;
+Ljava/util/ImmutableCollections$ListN;
+Ljava/util/ImmutableCollections$Map1;
 Ljava/util/ImmutableCollections$MapN$1;
 Ljava/util/ImmutableCollections$MapN$MapNIterator;
+Ljava/util/ImmutableCollections$MapN;
+Ljava/util/ImmutableCollections$Set12;
+Ljava/util/ImmutableCollections$SetN$SetNIterator;
+Ljava/util/ImmutableCollections$SetN;
 Ljava/util/ImmutableCollections$SubList;
 Ljava/util/ImmutableCollections-IA;
 Ljava/util/ImmutableCollections;
@@ -11838,10 +12036,19 @@ Ljava/util/Iterator;
 Ljava/util/JumboEnumSet$EnumSetIterator;
 Ljava/util/JumboEnumSet;
 Ljava/util/KeyValueHolder;
+Ljava/util/LinkedHashMap$Entry;
+Ljava/util/LinkedHashMap$LinkedEntryIterator;
+Ljava/util/LinkedHashMap$LinkedEntrySet;
 Ljava/util/LinkedHashMap$LinkedHashIterator;
+Ljava/util/LinkedHashMap$LinkedKeyIterator;
+Ljava/util/LinkedHashMap$LinkedKeySet;
+Ljava/util/LinkedHashMap$LinkedValueIterator;
+Ljava/util/LinkedHashMap$LinkedValues;
 Ljava/util/LinkedHashMap$ReversedLinkedHashMapView;
 Ljava/util/LinkedHashMap;
 Ljava/util/LinkedHashSet;
+Ljava/util/LinkedList$DescendingIterator;
+Ljava/util/LinkedList$ListItr;
 Ljava/util/LinkedList$Node;
 Ljava/util/LinkedList;
 Ljava/util/List;
@@ -11878,6 +12085,7 @@ Ljava/util/OptionalInt;
 Ljava/util/PrimitiveIterator$OfInt$$ExternalSyntheticLambda0;
 Ljava/util/PrimitiveIterator$OfInt;
 Ljava/util/PrimitiveIterator;
+Ljava/util/PriorityQueue$Itr;
 Ljava/util/PriorityQueue;
 Ljava/util/Properties$EntrySet;
 Ljava/util/Properties$LineReader;
@@ -11949,13 +12157,18 @@ Ljava/util/TooManyListenersException;
 Ljava/util/TreeMap$AscendingSubMap$AscendingEntrySetView;
 Ljava/util/TreeMap$AscendingSubMap;
 Ljava/util/TreeMap$DescendingSubMap;
+Ljava/util/TreeMap$EntryIterator;
+Ljava/util/TreeMap$EntrySet;
+Ljava/util/TreeMap$KeyIterator;
 Ljava/util/TreeMap$KeySet;
 Ljava/util/TreeMap$NavigableSubMap$DescendingSubMapKeyIterator;
 Ljava/util/TreeMap$NavigableSubMap$EntrySetView;
 Ljava/util/TreeMap$NavigableSubMap$SubMapEntryIterator;
 Ljava/util/TreeMap$NavigableSubMap$SubMapIterator;
+Ljava/util/TreeMap$NavigableSubMap$SubMapKeyIterator;
 Ljava/util/TreeMap$NavigableSubMap;
 Ljava/util/TreeMap$PrivateEntryIterator;
+Ljava/util/TreeMap$TreeMapEntry;
 Ljava/util/TreeMap$ValueIterator;
 Ljava/util/TreeMap$Values;
 Ljava/util/TreeMap;
@@ -11974,6 +12187,8 @@ Ljava/util/WeakHashMap$Entry;
 Ljava/util/WeakHashMap$EntryIterator;
 Ljava/util/WeakHashMap$EntrySet;
 Ljava/util/WeakHashMap$HashIterator;
+Ljava/util/WeakHashMap$KeyIterator;
+Ljava/util/WeakHashMap$KeySet;
 Ljava/util/WeakHashMap$ValueIterator;
 Ljava/util/WeakHashMap$Values;
 Ljava/util/WeakHashMap-IA;
@@ -12000,6 +12215,8 @@ Ljava/util/concurrent/ConcurrentHashMap$BaseIterator;
 Ljava/util/concurrent/ConcurrentHashMap$BulkTask;
 Ljava/util/concurrent/ConcurrentHashMap$CollectionView;
 Ljava/util/concurrent/ConcurrentHashMap$CounterCell;
+Ljava/util/concurrent/ConcurrentHashMap$EntryIterator;
+Ljava/util/concurrent/ConcurrentHashMap$EntrySetView;
 Ljava/util/concurrent/ConcurrentHashMap$ForEachEntryTask;
 Ljava/util/concurrent/ConcurrentHashMap$ForEachKeyTask;
 Ljava/util/concurrent/ConcurrentHashMap$ForEachMappingTask;
@@ -12012,6 +12229,7 @@ Ljava/util/concurrent/ConcurrentHashMap$ForwardingNode;
 Ljava/util/concurrent/ConcurrentHashMap$KeyIterator;
 Ljava/util/concurrent/ConcurrentHashMap$KeySetView;
 Ljava/util/concurrent/ConcurrentHashMap$KeySpliterator;
+Ljava/util/concurrent/ConcurrentHashMap$MapEntry;
 Ljava/util/concurrent/ConcurrentHashMap$MapReduceEntriesTask;
 Ljava/util/concurrent/ConcurrentHashMap$MapReduceEntriesToDoubleTask;
 Ljava/util/concurrent/ConcurrentHashMap$MapReduceEntriesToIntTask;
@@ -12042,11 +12260,14 @@ Ljava/util/concurrent/ConcurrentHashMap$TableStack;
 Ljava/util/concurrent/ConcurrentHashMap$Traverser;
 Ljava/util/concurrent/ConcurrentHashMap$TreeBin;
 Ljava/util/concurrent/ConcurrentHashMap$TreeNode;
+Ljava/util/concurrent/ConcurrentHashMap$ValueIterator;
+Ljava/util/concurrent/ConcurrentHashMap$ValuesView;
 Ljava/util/concurrent/ConcurrentHashMap;
 Ljava/util/concurrent/ConcurrentLinkedDeque$Node;
 Ljava/util/concurrent/ConcurrentLinkedDeque;
 Ljava/util/concurrent/ConcurrentLinkedQueue$$ExternalSyntheticLambda0;
 Ljava/util/concurrent/ConcurrentLinkedQueue$$ExternalSyntheticLambda2;
+Ljava/util/concurrent/ConcurrentLinkedQueue$Itr;
 Ljava/util/concurrent/ConcurrentLinkedQueue$Node;
 Ljava/util/concurrent/ConcurrentLinkedQueue;
 Ljava/util/concurrent/ConcurrentMap$$ExternalSyntheticLambda0;
@@ -12061,6 +12282,8 @@ Ljava/util/concurrent/ConcurrentSkipListMap$Values;
 Ljava/util/concurrent/ConcurrentSkipListMap;
 Ljava/util/concurrent/ConcurrentSkipListSet;
 Ljava/util/concurrent/CopyOnWriteArrayList$$ExternalSyntheticLambda2;
+Ljava/util/concurrent/CopyOnWriteArrayList$COWIterator;
+Ljava/util/concurrent/CopyOnWriteArrayList;
 Ljava/util/concurrent/CopyOnWriteArraySet;
 Ljava/util/concurrent/CountDownLatch$Sync;
 Ljava/util/concurrent/CountDownLatch;
@@ -12071,12 +12294,14 @@ Ljava/util/concurrent/ExecutionException;
 Ljava/util/concurrent/Executor;
 Ljava/util/concurrent/ExecutorService;
 Ljava/util/concurrent/Executors$AutoShutdownDelegatedExecutorService$$ExternalSyntheticLambda0;
+Ljava/util/concurrent/Executors$AutoShutdownDelegatedExecutorService$$ExternalSyntheticLambda1;
+Ljava/util/concurrent/Executors$AutoShutdownDelegatedExecutorService;
 Ljava/util/concurrent/Executors$DefaultThreadFactory;
 Ljava/util/concurrent/Executors$DelegatedExecutorService;
+Ljava/util/concurrent/Executors$DelegatedScheduledExecutorService;
 Ljava/util/concurrent/Executors$RunnableAdapter;
 Ljava/util/concurrent/Executors;
 Ljava/util/concurrent/ForkJoinPool$1;
-Ljava/util/concurrent/ForkJoinPool$DefaultCommonPoolForkJoinWorkerThreadFactory;
 Ljava/util/concurrent/ForkJoinPool$DefaultForkJoinWorkerThreadFactory;
 Ljava/util/concurrent/ForkJoinPool$ForkJoinWorkerThreadFactory;
 Ljava/util/concurrent/ForkJoinPool$ManagedBlocker;
@@ -12108,6 +12333,8 @@ Ljava/util/concurrent/ScheduledExecutorService;
 Ljava/util/concurrent/ScheduledFuture;
 Ljava/util/concurrent/ScheduledThreadPoolExecutor$DelayedWorkQueue$Itr;
 Ljava/util/concurrent/ScheduledThreadPoolExecutor$DelayedWorkQueue;
+Ljava/util/concurrent/ScheduledThreadPoolExecutor$ScheduledFutureTask;
+Ljava/util/concurrent/ScheduledThreadPoolExecutor;
 Ljava/util/concurrent/Semaphore$FairSync;
 Ljava/util/concurrent/Semaphore$NonfairSync;
 Ljava/util/concurrent/Semaphore$Sync;
@@ -12119,6 +12346,7 @@ Ljava/util/concurrent/ThreadLocalRandom;
 Ljava/util/concurrent/ThreadPoolExecutor$AbortPolicy;
 Ljava/util/concurrent/ThreadPoolExecutor$DiscardPolicy;
 Ljava/util/concurrent/ThreadPoolExecutor$Worker;
+Ljava/util/concurrent/ThreadPoolExecutor;
 Ljava/util/concurrent/TimeUnit$1;
 Ljava/util/concurrent/TimeUnit;
 Ljava/util/concurrent/TimeoutException;
@@ -12158,9 +12386,11 @@ Ljava/util/concurrent/locks/ReentrantLock$Sync;
 Ljava/util/concurrent/locks/ReentrantLock;
 Ljava/util/concurrent/locks/ReentrantReadWriteLock$FairSync;
 Ljava/util/concurrent/locks/ReentrantReadWriteLock$NonfairSync;
+Ljava/util/concurrent/locks/ReentrantReadWriteLock$ReadLock;
 Ljava/util/concurrent/locks/ReentrantReadWriteLock$Sync$HoldCounter;
 Ljava/util/concurrent/locks/ReentrantReadWriteLock$Sync$ThreadLocalHoldCounter;
 Ljava/util/concurrent/locks/ReentrantReadWriteLock$Sync;
+Ljava/util/concurrent/locks/ReentrantReadWriteLock$WriteLock;
 Ljava/util/concurrent/locks/ReentrantReadWriteLock;
 Ljava/util/function/BiConsumer;
 Ljava/util/function/BiFunction$$ExternalSyntheticLambda0;
@@ -12176,6 +12406,7 @@ Ljava/util/function/DoubleConsumer;
 Ljava/util/function/DoubleSupplier;
 Ljava/util/function/DoubleUnaryOperator$$ExternalSyntheticLambda0;
 Ljava/util/function/DoubleUnaryOperator$$ExternalSyntheticLambda1;
+Ljava/util/function/DoubleUnaryOperator$$ExternalSyntheticLambda2;
 Ljava/util/function/DoubleUnaryOperator;
 Ljava/util/function/Function$$ExternalSyntheticLambda0;
 Ljava/util/function/Function$$ExternalSyntheticLambda1;
@@ -12329,6 +12560,7 @@ Ljava/util/stream/Collectors$CollectorImpl;
 Ljava/util/stream/Collectors;
 Ljava/util/stream/DistinctOps$1$1;
 Ljava/util/stream/DistinctOps$1$2;
+Ljava/util/stream/DistinctOps$1;
 Ljava/util/stream/DistinctOps;
 Ljava/util/stream/DoublePipeline$$ExternalSyntheticLambda0;
 Ljava/util/stream/DoublePipeline$$ExternalSyntheticLambda4;
@@ -12360,8 +12592,12 @@ Ljava/util/stream/IntPipeline$$ExternalSyntheticLambda5;
 Ljava/util/stream/IntPipeline$$ExternalSyntheticLambda7;
 Ljava/util/stream/IntPipeline$$ExternalSyntheticLambda8;
 Ljava/util/stream/IntPipeline$1$1;
+Ljava/util/stream/IntPipeline$10;
+Ljava/util/stream/IntPipeline$1;
 Ljava/util/stream/IntPipeline$4$1;
+Ljava/util/stream/IntPipeline$4;
 Ljava/util/stream/IntPipeline$9;
+Ljava/util/stream/IntPipeline$Head;
 Ljava/util/stream/IntPipeline$StatelessOp;
 Ljava/util/stream/IntPipeline;
 Ljava/util/stream/IntStream;
@@ -12432,14 +12668,18 @@ Ljava/util/stream/ReferencePipeline$$ExternalSyntheticLambda1;
 Ljava/util/stream/ReferencePipeline$15$1;
 Ljava/util/stream/ReferencePipeline$15;
 Ljava/util/stream/ReferencePipeline$2$1;
+Ljava/util/stream/ReferencePipeline$2;
 Ljava/util/stream/ReferencePipeline$3$1;
+Ljava/util/stream/ReferencePipeline$3;
 Ljava/util/stream/ReferencePipeline$4$1;
+Ljava/util/stream/ReferencePipeline$4;
 Ljava/util/stream/ReferencePipeline$5$1;
 Ljava/util/stream/ReferencePipeline$5;
 Ljava/util/stream/ReferencePipeline$6$1;
 Ljava/util/stream/ReferencePipeline$6;
 Ljava/util/stream/ReferencePipeline$7$1;
 Ljava/util/stream/ReferencePipeline$7;
+Ljava/util/stream/ReferencePipeline$Head;
 Ljava/util/stream/ReferencePipeline$StatefulOp;
 Ljava/util/stream/ReferencePipeline$StatelessOp;
 Ljava/util/stream/ReferencePipeline;
@@ -12453,6 +12693,7 @@ Ljava/util/stream/SliceOps$1$1;
 Ljava/util/stream/SliceOps$1;
 Ljava/util/stream/SliceOps;
 Ljava/util/stream/SortedOps$AbstractRefSortingSink;
+Ljava/util/stream/SortedOps$OfRef;
 Ljava/util/stream/SortedOps$RefSortingSink$$ExternalSyntheticLambda0;
 Ljava/util/stream/SortedOps$RefSortingSink;
 Ljava/util/stream/SortedOps$SizedRefSortingSink;
@@ -12641,7 +12882,9 @@ Ljdk/internal/access/JavaIOFileDescriptorAccess;
 Ljdk/internal/access/JavaObjectInputStreamAccess;
 Ljdk/internal/access/JavaUtilCollectionAccess;
 Ljdk/internal/access/SharedSecrets;
+Ljdk/internal/math/DoubleToDecimal;
 Ljdk/internal/math/FDBigInteger;
+Ljdk/internal/math/FloatToDecimal;
 Ljdk/internal/math/FloatingDecimal$1;
 Ljdk/internal/math/FloatingDecimal$ASCIIToBinaryBuffer;
 Ljdk/internal/math/FloatingDecimal$ASCIIToBinaryConverter;
@@ -12651,9 +12894,11 @@ Ljdk/internal/math/FloatingDecimal$ExceptionalBinaryToASCIIBuffer;
 Ljdk/internal/math/FloatingDecimal$HexFloatPattern;
 Ljdk/internal/math/FloatingDecimal$PreparedASCIIToBinaryBuffer;
 Ljdk/internal/math/FloatingDecimal;
+Ljdk/internal/math/FormattedFPDecimal;
 Ljdk/internal/math/FormattedFloatingDecimal$1;
 Ljdk/internal/math/FormattedFloatingDecimal$Form;
 Ljdk/internal/math/FormattedFloatingDecimal;
+Ljdk/internal/math/MathUtils;
 Ljdk/internal/misc/TerminatingThreadLocal$1;
 Ljdk/internal/misc/TerminatingThreadLocal;
 Ljdk/internal/misc/Unsafe;
@@ -12661,6 +12906,7 @@ Ljdk/internal/misc/UnsafeConstants;
 Ljdk/internal/misc/VM;
 Ljdk/internal/misc/VirtualThreads;
 Ljdk/internal/ref/CleanerFactory;
+Ljdk/internal/ref/CleanerImpl$PhantomCleanableRef;
 Ljdk/internal/ref/CleanerImpl;
 Ljdk/internal/ref/PhantomCleanable;
 Ljdk/internal/reflect/Reflection;
@@ -12756,10 +13002,13 @@ Llibcore/util/FP16;
 Llibcore/util/HexEncoding;
 Llibcore/util/NativeAllocationRegistry$CleanerRunner;
 Llibcore/util/NativeAllocationRegistry$CleanerThunk;
+Llibcore/util/NativeAllocationRegistry$Metrics;
+Llibcore/util/NativeAllocationRegistry-IA;
 Llibcore/util/NativeAllocationRegistry;
 Llibcore/util/Objects;
 Llibcore/util/SneakyThrow;
 Llibcore/util/XmlObjectFactory;
+Llibcore/util/ZoneInfo;
 Lorg/apache/harmony/dalvik/ddmc/Chunk;
 Lorg/apache/harmony/dalvik/ddmc/ChunkHandler;
 Lorg/apache/harmony/dalvik/ddmc/DdmServer;
@@ -12887,6 +13136,7 @@ Lsun/nio/ch/DirectBuffer;
 Lsun/nio/ch/FileChannelImpl$SimpleFileLockTable;
 Lsun/nio/ch/FileChannelImpl$Unmapper;
 Lsun/nio/ch/FileChannelImpl-IA;
+Lsun/nio/ch/FileChannelImpl;
 Lsun/nio/ch/FileDescriptorHolderSocketImpl;
 Lsun/nio/ch/FileDispatcher;
 Lsun/nio/ch/FileDispatcherImpl;
@@ -12964,6 +13214,7 @@ Lsun/nio/fs/UnixFileSystemProvider$3;
 Lsun/nio/fs/UnixFileSystemProvider;
 Lsun/nio/fs/UnixMountEntry;
 Lsun/nio/fs/UnixNativeDispatcher;
+Lsun/nio/fs/UnixPath;
 Lsun/nio/fs/UnixSecureDirectoryStream;
 Lsun/nio/fs/Util;
 Lsun/reflect/Reflection;
@@ -13158,6 +13409,7 @@ Lsun/util/locale/BaseLocale-IA;
 Lsun/util/locale/BaseLocale;
 Lsun/util/locale/Extension;
 Lsun/util/locale/InternalLocaleBuilder$CaseInsensitiveChar;
+Lsun/util/locale/InternalLocaleBuilder$CaseInsensitiveString;
 Lsun/util/locale/InternalLocaleBuilder-IA;
 Lsun/util/locale/InternalLocaleBuilder;
 Lsun/util/locale/LanguageTag;
@@ -13209,22 +13461,29 @@ Lsun/util/logging/PlatformLogger;
 [Ldalvik/system/DexPathList$NativeLibraryElement;
 [Ljava/io/File$PathStatus;
 [Ljava/io/File;
+[Ljava/io/FileDescriptor;
+[Ljava/io/IOException;
 [Ljava/io/InputStream;
 [Ljava/io/ObjectInputStream$HandleTable$HandleList;
 [Ljava/io/ObjectStreamClass$ClassDataSlot;
 [Ljava/io/ObjectStreamClass$MemberSignature;
 [Ljava/io/ObjectStreamField;
+[Ljava/io/Serializable;
+[Ljava/lang/Boolean;
 [Ljava/lang/Byte;
 [Ljava/lang/CharSequence;
 [Ljava/lang/Character$UnicodeBlock;
 [Ljava/lang/Character;
 [Ljava/lang/Class;
+[Ljava/lang/ClassLoader;
 [Ljava/lang/ClassValue$Entry;
 [Ljava/lang/Comparable;
 [Ljava/lang/Daemons$Daemon;
 [Ljava/lang/Double;
 [Ljava/lang/Enum;
+[Ljava/lang/Float;
 [Ljava/lang/Integer;
+[Ljava/lang/Iterable;
 [Ljava/lang/Long;
 [Ljava/lang/Number;
 [Ljava/lang/Object;
@@ -13240,7 +13499,9 @@ Lsun/util/logging/PlatformLogger;
 [Ljava/lang/Thread;
 [Ljava/lang/ThreadGroup;
 [Ljava/lang/ThreadLocal$ThreadLocalMap$Entry;
+[Ljava/lang/ThreadLocal;
 [Ljava/lang/Throwable;
+[Ljava/lang/Void;
 [Ljava/lang/annotation/Annotation;
 [Ljava/lang/constant/ClassDesc;
 [Ljava/lang/constant/Constable;
@@ -13263,6 +13524,7 @@ Lsun/util/logging/PlatformLogger;
 [Ljava/lang/reflect/TypeVariable;
 [Ljava/math/BigDecimal;
 [Ljava/math/BigInteger;
+[Ljava/math/MathContext;
 [Ljava/math/RoundingMode;
 [Ljava/net/Authenticator$RequestorType;
 [Ljava/net/InetAddress;
@@ -13295,6 +13557,7 @@ Lsun/util/logging/PlatformLogger;
 [Ljava/security/cert/X509CRL;
 [Ljava/security/cert/X509Certificate;
 [Ljava/text/DateFormat$Field;
+[Ljava/text/DateFormat;
 [Ljava/text/Format;
 [Ljava/text/Normalizer$Form;
 [Ljava/text/NumberFormat$Style;
@@ -13317,10 +13580,13 @@ Lsun/util/logging/PlatformLogger;
 [Ljava/time/zone/ZoneOffsetTransition;
 [Ljava/time/zone/ZoneOffsetTransitionRule$TimeDefinition;
 [Ljava/time/zone/ZoneOffsetTransitionRule;
+[Ljava/util/ArrayList;
+[Ljava/util/Comparator;
 [Ljava/util/Comparators$NaturalOrderComparator;
 [Ljava/util/Enumeration;
 [Ljava/util/Formatter$Flags;
 [Ljava/util/HashMap$Node;
+[Ljava/util/HashMap;
 [Ljava/util/Hashtable$HashtableEntry;
 [Ljava/util/List;
 [Ljava/util/Locale$Category;
@@ -13328,15 +13594,19 @@ Lsun/util/logging/PlatformLogger;
 [Ljava/util/Locale$IsoCountryCode;
 [Ljava/util/Locale;
 [Ljava/util/Map$Entry;
+[Ljava/util/Set;
 [Ljava/util/TimerTask;
+[Ljava/util/UUID;
 [Ljava/util/WeakHashMap$Entry;
 [Ljava/util/concurrent/ConcurrentHashMap$CounterCell;
 [Ljava/util/concurrent/ConcurrentHashMap$Node;
 [Ljava/util/concurrent/ConcurrentHashMap$Segment;
 [Ljava/util/concurrent/ForkJoinPool$WorkQueue;
 [Ljava/util/concurrent/ForkJoinTask;
+[Ljava/util/concurrent/Future$State;
 [Ljava/util/concurrent/RunnableScheduledFuture;
 [Ljava/util/concurrent/TimeUnit;
+[Ljava/util/concurrent/atomic/AtomicReference;
 [Ljava/util/concurrent/atomic/Striped64$Cell;
 [Ljava/util/logging/Handler;
 [Ljava/util/prefs/AbstractPreferences;
@@ -13353,6 +13623,7 @@ Lsun/util/logging/PlatformLogger;
 [Ljavax/net/ssl/SSLEngineResult$Status;
 [Ljavax/net/ssl/TrustManager;
 [Ljavax/security/auth/callback/Callback;
+[Ljavax/security/auth/x500/X500Principal;
 [Ljavax/security/cert/X509Certificate;
 [Ljdk/internal/math/FDBigInteger;
 [Ljdk/internal/math/FormattedFloatingDecimal$Form;
@@ -13385,14 +13656,20 @@ Lsun/util/logging/PlatformLogger;
 [S
 [Z
 [[B
+[[C
 [[D
+[[F
 [[I
 [[J
 [[Ljava/lang/Byte;
 [[Ljava/lang/Class;
+[[Ljava/lang/Long;
 [[Ljava/lang/Object;
 [[Ljava/lang/String;
 [[Ljava/lang/annotation/Annotation;
 [[Ljava/lang/invoke/MethodHandle;
 [[Ljava/math/BigInteger;
+[[S
 [[Z
+[[[B
+[[[I
diff --git a/build/boot/preloaded-classes b/build/boot/preloaded-classes
index 08735d6365..f857972d6d 100644
--- a/build/boot/preloaded-classes
+++ b/build/boot/preloaded-classes
@@ -1,5 +1,5 @@
 #
-# Copyright (C) 2017 The Android Open Source Project
+# Copyright (C) 2025 The Android Open Source Project
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -21,10 +21,19 @@
 #
 # This file has been derived for mainline phone (and tablet) usage.
 #
+android.app.ActivityThread$AndroidOs
+android.app.AppOpsManager$$ExternalSyntheticLambda4
 android.compat.Compatibility$1
 android.compat.Compatibility$BehaviorChangeDelegate
 android.compat.Compatibility$ChangeConfig
 android.compat.Compatibility
+android.content.ContentCaptureOptions$ContentProtectionOptions$$ExternalSyntheticLambda0
+android.crypto.hpke.HpkeSpi
+android.graphics.ColorSpace$Rgb$$ExternalSyntheticLambda4
+android.graphics.ColorSpace$Rgb$$ExternalSyntheticLambda6
+android.graphics.ColorSpace$Rgb$$ExternalSyntheticLambda9
+android.icu.text.RuleBasedBreakIterator
+android.os.StrictMode$AndroidBlockGuardPolicy
 android.system.ErrnoException
 android.system.GaiException
 android.system.Int32Ref
@@ -53,9 +62,19 @@ android.system.StructUtsname
 android.system.SystemCleaner
 android.system.UnixSocketAddress
 android.system.VmSocketAddress
+android.util.ArrayMap
+android.util.IndentingPrintWriter
+android.util.MapCollections$ArrayIterator
+android.util.MapCollections$EntrySet
+android.util.MapCollections$KeySet
+android.util.MapCollections$MapIterator
+android.util.MapCollections$ValuesCollection
+android.widget.RemoteViews$ApplicationInfoCache$$ExternalSyntheticLambda0
 com.android.art.flags.FeatureFlags
 com.android.art.flags.FeatureFlagsImpl
 com.android.art.flags.Flags
+com.android.internal.util.FastPrintWriter
+com.android.internal.util.IndentingPrintWriter
 com.android.libcore.FeatureFlags
 com.android.libcore.FeatureFlagsImpl
 com.android.libcore.Flags
@@ -219,12 +238,12 @@ com.android.okhttp.okio.Timeout$1
 com.android.okhttp.okio.Timeout
 com.android.okhttp.okio.Util
 com.android.org.bouncycastle.asn1.ASN1ApplicationSpecific
+com.android.org.bouncycastle.asn1.ASN1ApplicationSpecificParser
 com.android.org.bouncycastle.asn1.ASN1BitString
 com.android.org.bouncycastle.asn1.ASN1Boolean
 com.android.org.bouncycastle.asn1.ASN1Choice
 com.android.org.bouncycastle.asn1.ASN1Encodable
 com.android.org.bouncycastle.asn1.ASN1EncodableVector
-com.android.org.bouncycastle.asn1.ASN1Enumerated$$ExternalSyntheticOutline0
 com.android.org.bouncycastle.asn1.ASN1Enumerated
 com.android.org.bouncycastle.asn1.ASN1Exception
 com.android.org.bouncycastle.asn1.ASN1External
@@ -248,6 +267,7 @@ com.android.org.bouncycastle.asn1.ASN1SetParser
 com.android.org.bouncycastle.asn1.ASN1StreamParser
 com.android.org.bouncycastle.asn1.ASN1String
 com.android.org.bouncycastle.asn1.ASN1TaggedObject
+com.android.org.bouncycastle.asn1.ASN1TaggedObjectParser
 com.android.org.bouncycastle.asn1.ASN1UTCTime
 com.android.org.bouncycastle.asn1.BERApplicationSpecific
 com.android.org.bouncycastle.asn1.BERApplicationSpecificParser
@@ -258,6 +278,7 @@ com.android.org.bouncycastle.asn1.BERSequenceParser
 com.android.org.bouncycastle.asn1.BERSet
 com.android.org.bouncycastle.asn1.BERSetParser
 com.android.org.bouncycastle.asn1.BERTaggedObjectParser
+com.android.org.bouncycastle.asn1.BERTags
 com.android.org.bouncycastle.asn1.ConstructedOctetStream
 com.android.org.bouncycastle.asn1.DERBMPString
 com.android.org.bouncycastle.asn1.DERBitString
@@ -289,6 +310,7 @@ com.android.org.bouncycastle.asn1.InMemoryRepresentable
 com.android.org.bouncycastle.asn1.IndefiniteLengthInputStream
 com.android.org.bouncycastle.asn1.LazyEncodedSequence
 com.android.org.bouncycastle.asn1.LimitedInputStream
+com.android.org.bouncycastle.asn1.OIDTokenizer
 com.android.org.bouncycastle.asn1.StreamUtil
 com.android.org.bouncycastle.asn1.bc.BCObjectIdentifiers
 com.android.org.bouncycastle.asn1.misc.MiscObjectIdentifiers
@@ -296,6 +318,7 @@ com.android.org.bouncycastle.asn1.nist.NISTObjectIdentifiers
 com.android.org.bouncycastle.asn1.oiw.OIWObjectIdentifiers
 com.android.org.bouncycastle.asn1.pkcs.PKCSObjectIdentifiers
 com.android.org.bouncycastle.asn1.x500.X500Name
+com.android.org.bouncycastle.asn1.x500.X500NameStyle
 com.android.org.bouncycastle.asn1.x500.style.AbstractX500NameStyle
 com.android.org.bouncycastle.asn1.x500.style.BCStyle
 com.android.org.bouncycastle.asn1.x509.AlgorithmIdentifier
@@ -318,12 +341,14 @@ com.android.org.bouncycastle.crypto.DataLengthException
 com.android.org.bouncycastle.crypto.Digest
 com.android.org.bouncycastle.crypto.ExtendedDigest
 com.android.org.bouncycastle.crypto.InvalidCipherTextException
+com.android.org.bouncycastle.crypto.Mac
 com.android.org.bouncycastle.crypto.OutputLengthException
 com.android.org.bouncycastle.crypto.PBEParametersGenerator
 com.android.org.bouncycastle.crypto.RuntimeCryptoException
 com.android.org.bouncycastle.crypto.Wrapper
 com.android.org.bouncycastle.crypto.digests.AndroidDigestFactory
 com.android.org.bouncycastle.crypto.digests.AndroidDigestFactoryBouncyCastle
+com.android.org.bouncycastle.crypto.digests.AndroidDigestFactoryInterface
 com.android.org.bouncycastle.crypto.digests.AndroidDigestFactoryOpenSSL
 com.android.org.bouncycastle.crypto.digests.EncodableDigest
 com.android.org.bouncycastle.crypto.digests.GeneralDigest
@@ -350,6 +375,7 @@ com.android.org.bouncycastle.crypto.generators.PKCS5S2ParametersGenerator
 com.android.org.bouncycastle.crypto.io.MacInputStream
 com.android.org.bouncycastle.crypto.macs.HMac
 com.android.org.bouncycastle.crypto.modes.AEADBlockCipher
+com.android.org.bouncycastle.crypto.modes.AEADCipher
 com.android.org.bouncycastle.crypto.modes.CBCBlockCipher
 com.android.org.bouncycastle.crypto.paddings.BlockCipherPadding
 com.android.org.bouncycastle.crypto.paddings.PKCS7Padding
@@ -365,16 +391,22 @@ com.android.org.bouncycastle.crypto.params.DSAValidationParameters
 com.android.org.bouncycastle.crypto.params.KeyParameter
 com.android.org.bouncycastle.crypto.params.ParametersWithIV
 com.android.org.bouncycastle.crypto.params.ParametersWithRandom
+com.android.org.bouncycastle.jcajce.PBKDFKey
 com.android.org.bouncycastle.jcajce.PKCS12Key
+com.android.org.bouncycastle.jcajce.PKCS12KeyWithParameters
 com.android.org.bouncycastle.jcajce.provider.asymmetric.DH$Mappings
 com.android.org.bouncycastle.jcajce.provider.asymmetric.DH
 com.android.org.bouncycastle.jcajce.provider.asymmetric.DSA$Mappings
+com.android.org.bouncycastle.jcajce.provider.asymmetric.EC$Mappings
 com.android.org.bouncycastle.jcajce.provider.asymmetric.RSA$Mappings
 com.android.org.bouncycastle.jcajce.provider.asymmetric.RSA
+com.android.org.bouncycastle.jcajce.provider.asymmetric.X509$Mappings
 com.android.org.bouncycastle.jcajce.provider.asymmetric.dh.KeyFactorySpi
+com.android.org.bouncycastle.jcajce.provider.asymmetric.dsa.BCDSAPublicKey
 com.android.org.bouncycastle.jcajce.provider.asymmetric.dsa.DSAUtil
 com.android.org.bouncycastle.jcajce.provider.asymmetric.dsa.KeyFactorySpi
 com.android.org.bouncycastle.jcajce.provider.asymmetric.rsa.CipherSpi$NoPadding
+com.android.org.bouncycastle.jcajce.provider.asymmetric.rsa.CipherSpi
 com.android.org.bouncycastle.jcajce.provider.asymmetric.rsa.KeyFactorySpi
 com.android.org.bouncycastle.jcajce.provider.asymmetric.rsa.RSAUtil
 com.android.org.bouncycastle.jcajce.provider.asymmetric.util.BaseCipherSpi
@@ -387,7 +419,8 @@ com.android.org.bouncycastle.jcajce.provider.config.ConfigurableProvider
 com.android.org.bouncycastle.jcajce.provider.config.ProviderConfiguration
 com.android.org.bouncycastle.jcajce.provider.config.ProviderConfigurationPermission
 com.android.org.bouncycastle.jcajce.provider.digest.DigestAlgorithmProvider
-com.android.org.bouncycastle.jcajce.provider.digest.SHA1$Mappings$$ExternalSyntheticOutline0
+com.android.org.bouncycastle.jcajce.provider.digest.MD5$Mappings
+com.android.org.bouncycastle.jcajce.provider.digest.MD5
 com.android.org.bouncycastle.jcajce.provider.digest.SHA1$Mappings
 com.android.org.bouncycastle.jcajce.provider.digest.SHA1
 com.android.org.bouncycastle.jcajce.provider.digest.SHA224$Mappings
@@ -403,6 +436,7 @@ com.android.org.bouncycastle.jcajce.provider.keystore.PKCS12$Mappings
 com.android.org.bouncycastle.jcajce.provider.keystore.bc.BcKeyStoreSpi$Std
 com.android.org.bouncycastle.jcajce.provider.keystore.bc.BcKeyStoreSpi$StoreEntry
 com.android.org.bouncycastle.jcajce.provider.keystore.bc.BcKeyStoreSpi
+com.android.org.bouncycastle.jcajce.provider.symmetric.AES$ECB$1
 com.android.org.bouncycastle.jcajce.provider.symmetric.AES$ECB
 com.android.org.bouncycastle.jcajce.provider.symmetric.AES$Mappings
 com.android.org.bouncycastle.jcajce.provider.symmetric.AES
@@ -437,6 +471,7 @@ com.android.org.bouncycastle.jcajce.provider.symmetric.util.BaseSecretKeyFactory
 com.android.org.bouncycastle.jcajce.provider.symmetric.util.BaseWrapCipher$ErasableOutputStream
 com.android.org.bouncycastle.jcajce.provider.symmetric.util.BaseWrapCipher$InvalidKeyOrParametersException
 com.android.org.bouncycastle.jcajce.provider.symmetric.util.BaseWrapCipher
+com.android.org.bouncycastle.jcajce.provider.symmetric.util.BlockCipherProvider
 com.android.org.bouncycastle.jcajce.provider.symmetric.util.ClassUtil$1
 com.android.org.bouncycastle.jcajce.provider.symmetric.util.ClassUtil
 com.android.org.bouncycastle.jcajce.provider.symmetric.util.GcmSpecUtil$2
@@ -447,11 +482,14 @@ com.android.org.bouncycastle.jcajce.provider.util.AlgorithmProvider
 com.android.org.bouncycastle.jcajce.provider.util.AsymmetricAlgorithmProvider
 com.android.org.bouncycastle.jcajce.provider.util.AsymmetricKeyInfoConverter
 com.android.org.bouncycastle.jcajce.provider.util.DigestFactory
+com.android.org.bouncycastle.jcajce.spec.AEADParameterSpec
 com.android.org.bouncycastle.jcajce.spec.PBKDF2KeySpec
 com.android.org.bouncycastle.jcajce.util.BCJcaJceHelper
 com.android.org.bouncycastle.jcajce.util.DefaultJcaJceHelper
 com.android.org.bouncycastle.jcajce.util.JcaJceHelper
+com.android.org.bouncycastle.jcajce.util.ProviderJcaJceHelper
 com.android.org.bouncycastle.jce.X509Principal
+com.android.org.bouncycastle.jce.interfaces.BCKeyStore
 com.android.org.bouncycastle.jce.interfaces.PKCS12BagAttributeCarrier
 com.android.org.bouncycastle.jce.provider.BouncyCastleProvider$1
 com.android.org.bouncycastle.jce.provider.BouncyCastleProvider$PrivateProvider
@@ -461,18 +499,23 @@ com.android.org.bouncycastle.jce.provider.CertStoreCollectionSpi
 com.android.org.bouncycastle.jce.provider.X509CRLObject
 com.android.org.bouncycastle.util.Arrays
 com.android.org.bouncycastle.util.BigIntegers
+com.android.org.bouncycastle.util.Encodable
 com.android.org.bouncycastle.util.Integers
+com.android.org.bouncycastle.util.Iterable
 com.android.org.bouncycastle.util.Memoable
 com.android.org.bouncycastle.util.Pack
 com.android.org.bouncycastle.util.Properties$1
 com.android.org.bouncycastle.util.Properties
 com.android.org.bouncycastle.util.Strings$1
 com.android.org.bouncycastle.util.Strings
+com.android.org.bouncycastle.util.encoders.Encoder
 com.android.org.bouncycastle.util.encoders.Hex
 com.android.org.bouncycastle.util.encoders.HexEncoder
 com.android.org.bouncycastle.util.io.Streams
 com.android.org.kxml2.io.KXmlParser$ContentSource
 com.android.org.kxml2.io.KXmlParser$ValueContext
+com.android.org.kxml2.io.KXmlParser
+com.android.org.kxml2.io.KXmlSerializer
 com.sun.security.cert.internal.x509.X509V1CertImpl
 dalvik.annotation.optimization.CriticalNative
 dalvik.annotation.optimization.FastNative
@@ -481,6 +524,7 @@ dalvik.annotation.optimization.NeverInline
 dalvik.system.AppSpecializationHooks
 dalvik.system.BaseDexClassLoader$Reporter
 dalvik.system.BaseDexClassLoader
+dalvik.system.BlockGuard$1
 dalvik.system.BlockGuard$2
 dalvik.system.BlockGuard$3
 dalvik.system.BlockGuard$BlockGuardPolicyException
@@ -522,6 +566,7 @@ java.awt.font.NumericShaper
 java.awt.font.TextAttribute
 java.io.Bits
 java.io.BufferedInputStream
+java.io.BufferedOutputStream
 java.io.BufferedReader
 java.io.BufferedWriter
 java.io.ByteArrayInputStream
@@ -588,6 +633,7 @@ java.io.ObjectOutputStream$Caches
 java.io.ObjectOutputStream$DebugTraceInfoStack
 java.io.ObjectOutputStream$HandleTable
 java.io.ObjectOutputStream$PutField
+java.io.ObjectOutputStream$PutFieldImpl
 java.io.ObjectOutputStream$ReplaceTable
 java.io.ObjectOutputStream
 java.io.ObjectStreamClass$$ExternalSyntheticLambda0
@@ -800,6 +846,7 @@ java.lang.Thread$WeakClassKey
 java.lang.Thread
 java.lang.ThreadDeath
 java.lang.ThreadGroup
+java.lang.ThreadLocal$SuppliedThreadLocal
 java.lang.ThreadLocal$ThreadLocalMap$Entry
 java.lang.ThreadLocal$ThreadLocalMap
 java.lang.ThreadLocal-IA
@@ -815,8 +862,10 @@ java.lang.UNIXProcess$1
 java.lang.UNIXProcess$2
 java.lang.UNIXProcess$3
 java.lang.UNIXProcess$ProcessPipeInputStream
+java.lang.UNIXProcess$ProcessPipeOutputStream
 java.lang.UNIXProcess$ProcessReaperThreadFactory$1
 java.lang.UNIXProcess$ProcessReaperThreadFactory
+java.lang.UNIXProcess
 java.lang.UnsatisfiedLinkError
 java.lang.UnsupportedClassVersionError
 java.lang.UnsupportedOperationException
@@ -1071,8 +1120,10 @@ java.net.SocketAddress
 java.net.SocketException
 java.net.SocketImpl
 java.net.SocketImplFactory
+java.net.SocketInputStream
 java.net.SocketOption
 java.net.SocketOptions
+java.net.SocketOutputStream
 java.net.SocketPermission
 java.net.SocketTimeoutException
 java.net.SocksConsts
@@ -1099,12 +1150,18 @@ java.nio.BufferUnderflowException
 java.nio.ByteBuffer
 java.nio.ByteBufferAsCharBuffer
 java.nio.ByteBufferAsDoubleBuffer
+java.nio.ByteBufferAsFloatBuffer
+java.nio.ByteBufferAsIntBuffer
+java.nio.ByteBufferAsLongBuffer
 java.nio.ByteBufferAsShortBuffer
 java.nio.ByteOrder
 java.nio.CharBuffer
 java.nio.DirectByteBuffer$MemoryRef
+java.nio.DirectByteBuffer
 java.nio.DoubleBuffer
 java.nio.FloatBuffer
+java.nio.HeapByteBuffer
+java.nio.HeapCharBuffer
 java.nio.HeapIntBuffer
 java.nio.IntBuffer
 java.nio.InvalidMarkException
@@ -1256,6 +1313,7 @@ java.security.KeyStore$TrustedCertificateEntry
 java.security.KeyStore
 java.security.KeyStoreException
 java.security.KeyStoreSpi
+java.security.MessageDigest$Delegate
 java.security.MessageDigest
 java.security.MessageDigestSpi
 java.security.NoSuchAlgorithmException
@@ -1417,6 +1475,7 @@ java.text.ParsePosition
 java.text.RuleBasedCollator
 java.text.SimpleDateFormat
 java.text.StringCharacterIterator
+java.time.Clock$SystemClock
 java.time.Clock
 java.time.DateTimeException
 java.time.DayOfWeek
@@ -1435,6 +1494,7 @@ java.time.Period
 java.time.Year
 java.time.ZoneId
 java.time.ZoneOffset
+java.time.ZoneRegion
 java.time.ZonedDateTime$$ExternalSyntheticLambda0
 java.time.ZonedDateTime$1
 java.time.ZonedDateTime
@@ -1525,6 +1585,7 @@ java.time.zone.ZoneRules
 java.time.zone.ZoneRulesException
 java.time.zone.ZoneRulesProvider
 java.util.AbstractCollection
+java.util.AbstractList$Itr
 java.util.AbstractList$ListItr
 java.util.AbstractList$RandomAccessSpliterator
 java.util.AbstractList$RandomAccessSubList
@@ -1534,6 +1595,7 @@ java.util.AbstractList-IA
 java.util.AbstractList
 java.util.AbstractMap$1
 java.util.AbstractMap$2$1
+java.util.AbstractMap$2
 java.util.AbstractMap$SimpleEntry
 java.util.AbstractMap$SimpleImmutableEntry
 java.util.AbstractMap$ViewCollection
@@ -1542,10 +1604,15 @@ java.util.AbstractQueue
 java.util.AbstractSequentialList
 java.util.AbstractSet
 java.util.ArrayDeque$$ExternalSyntheticLambda1
+java.util.ArrayDeque$DeqIterator
+java.util.ArrayDeque$DescendingIterator
 java.util.ArrayDeque
 java.util.ArrayList$ArrayListSpliterator
+java.util.ArrayList$Itr
 java.util.ArrayList$ListItr
+java.util.ArrayList$SubList$1
 java.util.ArrayList$SubList$2
+java.util.ArrayList$SubList
 java.util.ArrayList
 java.util.ArrayPrefixHelpers$CumulateTask
 java.util.ArrayPrefixHelpers$DoubleCumulateTask
@@ -1555,6 +1622,8 @@ java.util.Arrays$$ExternalSyntheticLambda0
 java.util.Arrays$$ExternalSyntheticLambda1
 java.util.Arrays$$ExternalSyntheticLambda2
 java.util.Arrays$$ExternalSyntheticLambda3
+java.util.Arrays$ArrayItr
+java.util.Arrays$ArrayList
 java.util.Arrays$NaturalOrder
 java.util.Arrays
 java.util.ArraysParallelSortHelpers$FJObject$Sorter
@@ -1566,7 +1635,9 @@ java.util.Calendar$$ExternalSyntheticLambda0
 java.util.Calendar$Builder
 java.util.Calendar
 java.util.Collection
+java.util.Collections$1
 java.util.Collections$2
+java.util.Collections$3
 java.util.Collections$AsLIFOQueue
 java.util.Collections$CheckedCollection
 java.util.Collections$CheckedList
@@ -1580,29 +1651,50 @@ java.util.Collections$CheckedSortedMap
 java.util.Collections$CheckedSortedSet
 java.util.Collections$CopiesList
 java.util.Collections$EmptyEnumeration
+java.util.Collections$EmptyIterator
+java.util.Collections$EmptyList
 java.util.Collections$EmptyListIterator
+java.util.Collections$EmptyMap
+java.util.Collections$EmptySet
 java.util.Collections$ReverseComparator2
 java.util.Collections$ReverseComparator
 java.util.Collections$SequencedSetFromMap
 java.util.Collections$SetFromMap
+java.util.Collections$SingletonList
+java.util.Collections$SingletonMap
+java.util.Collections$SingletonSet
+java.util.Collections$SynchronizedCollection
 java.util.Collections$SynchronizedList
+java.util.Collections$SynchronizedMap
 java.util.Collections$SynchronizedNavigableMap
 java.util.Collections$SynchronizedNavigableSet
+java.util.Collections$SynchronizedRandomAccessList
+java.util.Collections$SynchronizedSet
 java.util.Collections$SynchronizedSortedMap
 java.util.Collections$SynchronizedSortedSet
+java.util.Collections$UnmodifiableCollection$1
+java.util.Collections$UnmodifiableCollection
 java.util.Collections$UnmodifiableList$1
 java.util.Collections$UnmodifiableList
+java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet$1
+java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet$UnmodifiableEntry
+java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet
+java.util.Collections$UnmodifiableMap
 java.util.Collections$UnmodifiableNavigableMap$EmptyNavigableMap
 java.util.Collections$UnmodifiableNavigableMap
 java.util.Collections$UnmodifiableNavigableSet$EmptyNavigableSet
 java.util.Collections$UnmodifiableNavigableSet
+java.util.Collections$UnmodifiableRandomAccessList
 java.util.Collections$UnmodifiableSequencedCollection
 java.util.Collections$UnmodifiableSequencedMap
 java.util.Collections$UnmodifiableSequencedSet
+java.util.Collections$UnmodifiableSet
 java.util.Collections$UnmodifiableSortedMap
+java.util.Collections$UnmodifiableSortedSet
 java.util.Collections-IA
 java.util.Collections
 java.util.ComparableTimSort
+java.util.Comparator$$ExternalSyntheticLambda0
 java.util.Comparator$$ExternalSyntheticLambda1
 java.util.Comparator$$ExternalSyntheticLambda2
 java.util.Comparator$$ExternalSyntheticLambda3
@@ -1649,15 +1741,25 @@ java.util.Formatter$FormatSpecifierParser
 java.util.Formatter$FormatString
 java.util.Formatter
 java.util.FormatterClosedException
+java.util.GregorianCalendar
+java.util.HashMap$EntryIterator
+java.util.HashMap$EntrySet
 java.util.HashMap$EntrySpliterator
 java.util.HashMap$HashIterator
 java.util.HashMap$HashMapSpliterator
+java.util.HashMap$KeyIterator
+java.util.HashMap$KeySet
 java.util.HashMap$KeySpliterator
+java.util.HashMap$Node
+java.util.HashMap$TreeNode
 java.util.HashMap$UnsafeHolder
+java.util.HashMap$ValueIterator
 java.util.HashMap$ValueSpliterator
+java.util.HashMap$Values
 java.util.HashMap
 java.util.HashSet
 java.util.Hashtable$EntrySet
+java.util.Hashtable$Enumerator
 java.util.Hashtable$HashtableEntry
 java.util.Hashtable$KeySet
 java.util.Hashtable$ValueCollection
@@ -1688,8 +1790,16 @@ java.util.ImmutableCollections$AbstractImmutableMap
 java.util.ImmutableCollections$AbstractImmutableSet
 java.util.ImmutableCollections$Access$1
 java.util.ImmutableCollections$Access
+java.util.ImmutableCollections$List12
+java.util.ImmutableCollections$ListItr
+java.util.ImmutableCollections$ListN
+java.util.ImmutableCollections$Map1
 java.util.ImmutableCollections$MapN$1
 java.util.ImmutableCollections$MapN$MapNIterator
+java.util.ImmutableCollections$MapN
+java.util.ImmutableCollections$Set12
+java.util.ImmutableCollections$SetN$SetNIterator
+java.util.ImmutableCollections$SetN
 java.util.ImmutableCollections$SubList
 java.util.ImmutableCollections-IA
 java.util.InputMismatchException
@@ -1697,10 +1807,19 @@ java.util.Iterator
 java.util.JumboEnumSet$EnumSetIterator
 java.util.JumboEnumSet
 java.util.KeyValueHolder
+java.util.LinkedHashMap$Entry
+java.util.LinkedHashMap$LinkedEntryIterator
+java.util.LinkedHashMap$LinkedEntrySet
 java.util.LinkedHashMap$LinkedHashIterator
+java.util.LinkedHashMap$LinkedKeyIterator
+java.util.LinkedHashMap$LinkedKeySet
+java.util.LinkedHashMap$LinkedValueIterator
+java.util.LinkedHashMap$LinkedValues
 java.util.LinkedHashMap$ReversedLinkedHashMapView
 java.util.LinkedHashMap
 java.util.LinkedHashSet
+java.util.LinkedList$DescendingIterator
+java.util.LinkedList$ListItr
 java.util.LinkedList$Node
 java.util.LinkedList
 java.util.List
@@ -1737,6 +1856,7 @@ java.util.OptionalInt
 java.util.PrimitiveIterator$OfInt$$ExternalSyntheticLambda0
 java.util.PrimitiveIterator$OfInt
 java.util.PrimitiveIterator
+java.util.PriorityQueue$Itr
 java.util.PriorityQueue
 java.util.Properties$EntrySet
 java.util.Properties$LineReader
@@ -1808,13 +1928,18 @@ java.util.TooManyListenersException
 java.util.TreeMap$AscendingSubMap$AscendingEntrySetView
 java.util.TreeMap$AscendingSubMap
 java.util.TreeMap$DescendingSubMap
+java.util.TreeMap$EntryIterator
+java.util.TreeMap$EntrySet
+java.util.TreeMap$KeyIterator
 java.util.TreeMap$KeySet
 java.util.TreeMap$NavigableSubMap$DescendingSubMapKeyIterator
 java.util.TreeMap$NavigableSubMap$EntrySetView
 java.util.TreeMap$NavigableSubMap$SubMapEntryIterator
 java.util.TreeMap$NavigableSubMap$SubMapIterator
+java.util.TreeMap$NavigableSubMap$SubMapKeyIterator
 java.util.TreeMap$NavigableSubMap
 java.util.TreeMap$PrivateEntryIterator
+java.util.TreeMap$TreeMapEntry
 java.util.TreeMap$ValueIterator
 java.util.TreeMap$Values
 java.util.TreeMap
@@ -1833,6 +1958,8 @@ java.util.WeakHashMap$Entry
 java.util.WeakHashMap$EntryIterator
 java.util.WeakHashMap$EntrySet
 java.util.WeakHashMap$HashIterator
+java.util.WeakHashMap$KeyIterator
+java.util.WeakHashMap$KeySet
 java.util.WeakHashMap$ValueIterator
 java.util.WeakHashMap$Values
 java.util.WeakHashMap-IA
@@ -1859,6 +1986,8 @@ java.util.concurrent.ConcurrentHashMap$BaseIterator
 java.util.concurrent.ConcurrentHashMap$BulkTask
 java.util.concurrent.ConcurrentHashMap$CollectionView
 java.util.concurrent.ConcurrentHashMap$CounterCell
+java.util.concurrent.ConcurrentHashMap$EntryIterator
+java.util.concurrent.ConcurrentHashMap$EntrySetView
 java.util.concurrent.ConcurrentHashMap$ForEachEntryTask
 java.util.concurrent.ConcurrentHashMap$ForEachKeyTask
 java.util.concurrent.ConcurrentHashMap$ForEachMappingTask
@@ -1868,8 +1997,10 @@ java.util.concurrent.ConcurrentHashMap$ForEachTransformedMappingTask
 java.util.concurrent.ConcurrentHashMap$ForEachTransformedValueTask
 java.util.concurrent.ConcurrentHashMap$ForEachValueTask
 java.util.concurrent.ConcurrentHashMap$ForwardingNode
+java.util.concurrent.ConcurrentHashMap$KeyIterator
 java.util.concurrent.ConcurrentHashMap$KeySetView
 java.util.concurrent.ConcurrentHashMap$KeySpliterator
+java.util.concurrent.ConcurrentHashMap$MapEntry
 java.util.concurrent.ConcurrentHashMap$MapReduceEntriesTask
 java.util.concurrent.ConcurrentHashMap$MapReduceEntriesToDoubleTask
 java.util.concurrent.ConcurrentHashMap$MapReduceEntriesToIntTask
@@ -1900,11 +2031,14 @@ java.util.concurrent.ConcurrentHashMap$TableStack
 java.util.concurrent.ConcurrentHashMap$Traverser
 java.util.concurrent.ConcurrentHashMap$TreeBin
 java.util.concurrent.ConcurrentHashMap$TreeNode
+java.util.concurrent.ConcurrentHashMap$ValueIterator
+java.util.concurrent.ConcurrentHashMap$ValuesView
 java.util.concurrent.ConcurrentHashMap
 java.util.concurrent.ConcurrentLinkedDeque$Node
 java.util.concurrent.ConcurrentLinkedDeque
 java.util.concurrent.ConcurrentLinkedQueue$$ExternalSyntheticLambda0
 java.util.concurrent.ConcurrentLinkedQueue$$ExternalSyntheticLambda2
+java.util.concurrent.ConcurrentLinkedQueue$Itr
 java.util.concurrent.ConcurrentLinkedQueue$Node
 java.util.concurrent.ConcurrentLinkedQueue
 java.util.concurrent.ConcurrentMap$$ExternalSyntheticLambda0
@@ -1919,6 +2053,8 @@ java.util.concurrent.ConcurrentSkipListMap$Values
 java.util.concurrent.ConcurrentSkipListMap
 java.util.concurrent.ConcurrentSkipListSet
 java.util.concurrent.CopyOnWriteArrayList$$ExternalSyntheticLambda2
+java.util.concurrent.CopyOnWriteArrayList$COWIterator
+java.util.concurrent.CopyOnWriteArrayList
 java.util.concurrent.CopyOnWriteArraySet
 java.util.concurrent.CountDownLatch$Sync
 java.util.concurrent.CountDownLatch
@@ -1928,12 +2064,15 @@ java.util.concurrent.Delayed
 java.util.concurrent.ExecutionException
 java.util.concurrent.Executor
 java.util.concurrent.ExecutorService
+java.util.concurrent.Executors$AutoShutdownDelegatedExecutorService$$ExternalSyntheticLambda0
+java.util.concurrent.Executors$AutoShutdownDelegatedExecutorService$$ExternalSyntheticLambda1
+java.util.concurrent.Executors$AutoShutdownDelegatedExecutorService
 java.util.concurrent.Executors$DefaultThreadFactory
 java.util.concurrent.Executors$DelegatedExecutorService
+java.util.concurrent.Executors$DelegatedScheduledExecutorService
 java.util.concurrent.Executors$RunnableAdapter
 java.util.concurrent.Executors
 java.util.concurrent.ForkJoinPool$1
-java.util.concurrent.ForkJoinPool$DefaultCommonPoolForkJoinWorkerThreadFactory
 java.util.concurrent.ForkJoinPool$DefaultForkJoinWorkerThreadFactory
 java.util.concurrent.ForkJoinPool$ForkJoinWorkerThreadFactory
 java.util.concurrent.ForkJoinPool$ManagedBlocker
@@ -1942,6 +2081,7 @@ java.util.concurrent.ForkJoinPool
 java.util.concurrent.ForkJoinTask$Aux
 java.util.concurrent.ForkJoinTask
 java.util.concurrent.ForkJoinWorkerThread
+java.util.concurrent.Future$State
 java.util.concurrent.Future
 java.util.concurrent.FutureTask$WaitNode
 java.util.concurrent.FutureTask
@@ -1952,6 +2092,8 @@ java.util.concurrent.LinkedBlockingDeque
 java.util.concurrent.LinkedBlockingQueue$Itr
 java.util.concurrent.LinkedBlockingQueue$Node
 java.util.concurrent.LinkedBlockingQueue
+java.util.concurrent.LinkedTransferQueue$DualNode
+java.util.concurrent.LinkedTransferQueue
 java.util.concurrent.Phaser
 java.util.concurrent.PriorityBlockingQueue
 java.util.concurrent.RejectedExecutionException
@@ -1962,6 +2104,8 @@ java.util.concurrent.ScheduledExecutorService
 java.util.concurrent.ScheduledFuture
 java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue$Itr
 java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue
+java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask
+java.util.concurrent.ScheduledThreadPoolExecutor
 java.util.concurrent.Semaphore$FairSync
 java.util.concurrent.Semaphore$NonfairSync
 java.util.concurrent.Semaphore$Sync
@@ -1972,9 +2116,11 @@ java.util.concurrent.ThreadFactory
 java.util.concurrent.ThreadPoolExecutor$AbortPolicy
 java.util.concurrent.ThreadPoolExecutor$DiscardPolicy
 java.util.concurrent.ThreadPoolExecutor$Worker
+java.util.concurrent.ThreadPoolExecutor
 java.util.concurrent.TimeUnit$1
 java.util.concurrent.TimeUnit
 java.util.concurrent.TimeoutException
+java.util.concurrent.TransferQueue
 java.util.concurrent.atomic.AtomicBoolean
 java.util.concurrent.atomic.AtomicInteger
 java.util.concurrent.atomic.AtomicIntegerArray
@@ -2010,9 +2156,11 @@ java.util.concurrent.locks.ReentrantLock$Sync
 java.util.concurrent.locks.ReentrantLock
 java.util.concurrent.locks.ReentrantReadWriteLock$FairSync
 java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync
+java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock
 java.util.concurrent.locks.ReentrantReadWriteLock$Sync$HoldCounter
 java.util.concurrent.locks.ReentrantReadWriteLock$Sync$ThreadLocalHoldCounter
 java.util.concurrent.locks.ReentrantReadWriteLock$Sync
+java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock
 java.util.concurrent.locks.ReentrantReadWriteLock
 java.util.function.BiConsumer
 java.util.function.BiFunction$$ExternalSyntheticLambda0
@@ -2028,6 +2176,7 @@ java.util.function.DoubleConsumer
 java.util.function.DoubleSupplier
 java.util.function.DoubleUnaryOperator$$ExternalSyntheticLambda0
 java.util.function.DoubleUnaryOperator$$ExternalSyntheticLambda1
+java.util.function.DoubleUnaryOperator$$ExternalSyntheticLambda2
 java.util.function.DoubleUnaryOperator
 java.util.function.Function$$ExternalSyntheticLambda0
 java.util.function.Function$$ExternalSyntheticLambda1
@@ -2181,6 +2330,7 @@ java.util.stream.Collectors$CollectorImpl
 java.util.stream.Collectors
 java.util.stream.DistinctOps$1$1
 java.util.stream.DistinctOps$1$2
+java.util.stream.DistinctOps$1
 java.util.stream.DistinctOps
 java.util.stream.DoublePipeline$$ExternalSyntheticLambda0
 java.util.stream.DoublePipeline$$ExternalSyntheticLambda4
@@ -2212,8 +2362,12 @@ java.util.stream.IntPipeline$$ExternalSyntheticLambda5
 java.util.stream.IntPipeline$$ExternalSyntheticLambda7
 java.util.stream.IntPipeline$$ExternalSyntheticLambda8
 java.util.stream.IntPipeline$1$1
+java.util.stream.IntPipeline$10
+java.util.stream.IntPipeline$1
 java.util.stream.IntPipeline$4$1
+java.util.stream.IntPipeline$4
 java.util.stream.IntPipeline$9
+java.util.stream.IntPipeline$Head
 java.util.stream.IntPipeline$StatelessOp
 java.util.stream.IntPipeline
 java.util.stream.IntStream
@@ -2284,14 +2438,18 @@ java.util.stream.ReferencePipeline$$ExternalSyntheticLambda1
 java.util.stream.ReferencePipeline$15$1
 java.util.stream.ReferencePipeline$15
 java.util.stream.ReferencePipeline$2$1
+java.util.stream.ReferencePipeline$2
 java.util.stream.ReferencePipeline$3$1
+java.util.stream.ReferencePipeline$3
 java.util.stream.ReferencePipeline$4$1
+java.util.stream.ReferencePipeline$4
 java.util.stream.ReferencePipeline$5$1
 java.util.stream.ReferencePipeline$5
 java.util.stream.ReferencePipeline$6$1
 java.util.stream.ReferencePipeline$6
 java.util.stream.ReferencePipeline$7$1
 java.util.stream.ReferencePipeline$7
+java.util.stream.ReferencePipeline$Head
 java.util.stream.ReferencePipeline$StatefulOp
 java.util.stream.ReferencePipeline$StatelessOp
 java.util.stream.ReferencePipeline
@@ -2305,6 +2463,7 @@ java.util.stream.SliceOps$1$1
 java.util.stream.SliceOps$1
 java.util.stream.SliceOps
 java.util.stream.SortedOps$AbstractRefSortingSink
+java.util.stream.SortedOps$OfRef
 java.util.stream.SortedOps$RefSortingSink$$ExternalSyntheticLambda0
 java.util.stream.SortedOps$RefSortingSink
 java.util.stream.SortedOps$SizedRefSortingSink
@@ -2347,6 +2506,7 @@ java.util.zip.Deflater-IA
 java.util.zip.Deflater
 java.util.zip.DeflaterOutputStream
 java.util.zip.GZIPInputStream$1
+java.util.zip.GZIPInputStream
 java.util.zip.GZIPOutputStream
 java.util.zip.Inflater$InflaterZStreamRef
 java.util.zip.Inflater-IA
@@ -2491,7 +2651,9 @@ jdk.internal.access.JavaIOFileDescriptorAccess
 jdk.internal.access.JavaObjectInputStreamAccess
 jdk.internal.access.JavaUtilCollectionAccess
 jdk.internal.access.SharedSecrets
+jdk.internal.math.DoubleToDecimal
 jdk.internal.math.FDBigInteger
+jdk.internal.math.FloatToDecimal
 jdk.internal.math.FloatingDecimal$1
 jdk.internal.math.FloatingDecimal$ASCIIToBinaryBuffer
 jdk.internal.math.FloatingDecimal$ASCIIToBinaryConverter
@@ -2501,9 +2663,11 @@ jdk.internal.math.FloatingDecimal$ExceptionalBinaryToASCIIBuffer
 jdk.internal.math.FloatingDecimal$HexFloatPattern
 jdk.internal.math.FloatingDecimal$PreparedASCIIToBinaryBuffer
 jdk.internal.math.FloatingDecimal
+jdk.internal.math.FormattedFPDecimal
 jdk.internal.math.FormattedFloatingDecimal$1
 jdk.internal.math.FormattedFloatingDecimal$Form
 jdk.internal.math.FormattedFloatingDecimal
+jdk.internal.math.MathUtils
 jdk.internal.misc.TerminatingThreadLocal$1
 jdk.internal.misc.TerminatingThreadLocal
 jdk.internal.misc.Unsafe
@@ -2511,6 +2675,7 @@ jdk.internal.misc.UnsafeConstants
 jdk.internal.misc.VM
 jdk.internal.misc.VirtualThreads
 jdk.internal.ref.CleanerFactory
+jdk.internal.ref.CleanerImpl$PhantomCleanableRef
 jdk.internal.ref.CleanerImpl
 jdk.internal.ref.PhantomCleanable
 jdk.internal.reflect.Reflection
@@ -2606,10 +2771,12 @@ libcore.util.FP16
 libcore.util.HexEncoding
 libcore.util.NativeAllocationRegistry$CleanerRunner
 libcore.util.NativeAllocationRegistry$CleanerThunk
+libcore.util.NativeAllocationRegistry$Metrics
 libcore.util.NativeAllocationRegistry
 libcore.util.Objects
 libcore.util.SneakyThrow
 libcore.util.XmlObjectFactory
+libcore.util.ZoneInfo
 org.apache.harmony.dalvik.ddmc.Chunk
 org.apache.harmony.dalvik.ddmc.ChunkHandler
 org.apache.harmony.dalvik.ddmc.DdmServer
@@ -2627,7 +2794,9 @@ org.apache.harmony.xml.dom.CDATASectionImpl
 org.apache.harmony.xml.dom.CharacterDataImpl
 org.apache.harmony.xml.dom.CommentImpl
 org.apache.harmony.xml.dom.DOMImplementationImpl
+org.apache.harmony.xml.dom.DocumentImpl
 org.apache.harmony.xml.dom.DocumentTypeImpl
+org.apache.harmony.xml.dom.ElementImpl
 org.apache.harmony.xml.dom.EntityReferenceImpl
 org.apache.harmony.xml.dom.InnerNodeImpl
 org.apache.harmony.xml.dom.LeafNodeImpl
@@ -2635,6 +2804,7 @@ org.apache.harmony.xml.dom.NodeImpl$1
 org.apache.harmony.xml.dom.NodeImpl
 org.apache.harmony.xml.dom.NodeListImpl
 org.apache.harmony.xml.dom.ProcessingInstructionImpl
+org.apache.harmony.xml.dom.TextImpl
 org.apache.harmony.xml.parsers.DocumentBuilderFactoryImpl
 org.apache.harmony.xml.parsers.DocumentBuilderImpl
 org.apache.harmony.xml.parsers.SAXParserFactoryImpl
@@ -2734,6 +2904,7 @@ sun.nio.ch.DirectBuffer
 sun.nio.ch.FileChannelImpl$SimpleFileLockTable
 sun.nio.ch.FileChannelImpl$Unmapper
 sun.nio.ch.FileChannelImpl-IA
+sun.nio.ch.FileChannelImpl
 sun.nio.ch.FileDescriptorHolderSocketImpl
 sun.nio.ch.FileDispatcher
 sun.nio.ch.FileDispatcherImpl
@@ -2810,6 +2981,7 @@ sun.nio.fs.UnixFileSystemProvider$3
 sun.nio.fs.UnixFileSystemProvider
 sun.nio.fs.UnixMountEntry
 sun.nio.fs.UnixNativeDispatcher
+sun.nio.fs.UnixPath
 sun.nio.fs.UnixSecureDirectoryStream
 sun.nio.fs.Util
 sun.reflect.Reflection
@@ -3004,6 +3176,7 @@ sun.util.locale.BaseLocale-IA
 sun.util.locale.BaseLocale
 sun.util.locale.Extension
 sun.util.locale.InternalLocaleBuilder$CaseInsensitiveChar
+sun.util.locale.InternalLocaleBuilder$CaseInsensitiveString
 sun.util.locale.InternalLocaleBuilder-IA
 sun.util.locale.InternalLocaleBuilder
 sun.util.locale.LanguageTag
@@ -3055,22 +3228,28 @@ sun.util.logging.PlatformLogger
 [Ldalvik.system.DexPathList$NativeLibraryElement;
 [Ljava.io.File$PathStatus;
 [Ljava.io.File;
+[Ljava.io.FileDescriptor;
+[Ljava.io.IOException;
 [Ljava.io.InputStream;
 [Ljava.io.ObjectInputStream$HandleTable$HandleList;
 [Ljava.io.ObjectStreamClass$ClassDataSlot;
 [Ljava.io.ObjectStreamClass$MemberSignature;
 [Ljava.io.ObjectStreamField;
+[Ljava.io.Serializable;
+[Ljava.lang.Boolean;
 [Ljava.lang.Byte;
 [Ljava.lang.CharSequence;
 [Ljava.lang.Character$UnicodeBlock;
 [Ljava.lang.Character;
 [Ljava.lang.Class;
+[Ljava.lang.ClassLoader;
 [Ljava.lang.ClassValue$Entry;
 [Ljava.lang.Comparable;
 [Ljava.lang.Daemons$Daemon;
 [Ljava.lang.Double;
 [Ljava.lang.Enum;
 [Ljava.lang.Integer;
+[Ljava.lang.Iterable;
 [Ljava.lang.Long;
 [Ljava.lang.Number;
 [Ljava.lang.Object;
@@ -3086,7 +3265,9 @@ sun.util.logging.PlatformLogger
 [Ljava.lang.Thread;
 [Ljava.lang.ThreadGroup;
 [Ljava.lang.ThreadLocal$ThreadLocalMap$Entry;
+[Ljava.lang.ThreadLocal;
 [Ljava.lang.Throwable;
+[Ljava.lang.Void;
 [Ljava.lang.annotation.Annotation;
 [Ljava.lang.constant.ClassDesc;
 [Ljava.lang.constant.Constable;
@@ -3109,6 +3290,7 @@ sun.util.logging.PlatformLogger
 [Ljava.lang.reflect.TypeVariable;
 [Ljava.math.BigDecimal;
 [Ljava.math.BigInteger;
+[Ljava.math.MathContext;
 [Ljava.math.RoundingMode;
 [Ljava.net.Authenticator$RequestorType;
 [Ljava.net.InetAddress;
@@ -3141,6 +3323,7 @@ sun.util.logging.PlatformLogger
 [Ljava.security.cert.X509CRL;
 [Ljava.security.cert.X509Certificate;
 [Ljava.text.DateFormat$Field;
+[Ljava.text.DateFormat;
 [Ljava.text.Format;
 [Ljava.text.Normalizer$Form;
 [Ljava.text.NumberFormat$Style;
@@ -3163,10 +3346,13 @@ sun.util.logging.PlatformLogger
 [Ljava.time.zone.ZoneOffsetTransition;
 [Ljava.time.zone.ZoneOffsetTransitionRule$TimeDefinition;
 [Ljava.time.zone.ZoneOffsetTransitionRule;
+[Ljava.util.ArrayList;
+[Ljava.util.Comparator;
 [Ljava.util.Comparators$NaturalOrderComparator;
 [Ljava.util.Enumeration;
 [Ljava.util.Formatter$Flags;
 [Ljava.util.HashMap$Node;
+[Ljava.util.HashMap;
 [Ljava.util.Hashtable$HashtableEntry;
 [Ljava.util.List;
 [Ljava.util.Locale$Category;
@@ -3174,15 +3360,19 @@ sun.util.logging.PlatformLogger
 [Ljava.util.Locale$IsoCountryCode;
 [Ljava.util.Locale;
 [Ljava.util.Map$Entry;
+[Ljava.util.Set;
 [Ljava.util.TimerTask;
+[Ljava.util.UUID;
 [Ljava.util.WeakHashMap$Entry;
 [Ljava.util.concurrent.ConcurrentHashMap$CounterCell;
 [Ljava.util.concurrent.ConcurrentHashMap$Node;
 [Ljava.util.concurrent.ConcurrentHashMap$Segment;
 [Ljava.util.concurrent.ForkJoinPool$WorkQueue;
 [Ljava.util.concurrent.ForkJoinTask;
+[Ljava.util.concurrent.Future$State;
 [Ljava.util.concurrent.RunnableScheduledFuture;
 [Ljava.util.concurrent.TimeUnit;
+[Ljava.util.concurrent.atomic.AtomicReference;
 [Ljava.util.concurrent.atomic.Striped64$Cell;
 [Ljava.util.logging.Handler;
 [Ljava.util.prefs.AbstractPreferences;
@@ -3199,6 +3389,7 @@ sun.util.logging.PlatformLogger
 [Ljavax.net.ssl.SSLEngineResult$Status;
 [Ljavax.net.ssl.TrustManager;
 [Ljavax.security.auth.callback.Callback;
+[Ljavax.security.auth.x500.X500Principal;
 [Ljavax.security.cert.X509Certificate;
 [Ljdk.internal.math.FDBigInteger;
 [Ljdk.internal.math.FormattedFloatingDecimal$Form;
@@ -3231,14 +3422,20 @@ sun.util.logging.PlatformLogger
 [S
 [Z
 [[B
+[[C
 [[D
+[[F
 [[I
 [[J
 [[Ljava.lang.Byte;
 [[Ljava.lang.Class;
+[[Ljava.lang.Long;
 [[Ljava.lang.Object;
 [[Ljava.lang.String;
 [[Ljava.lang.annotation.Annotation;
 [[Ljava.lang.invoke.MethodHandle;
 [[Ljava.math.BigInteger;
+[[S
 [[Z
+[[[B
+[[[I
diff --git a/build/flags/Android.bp b/build/flags/Android.bp
index 951ca39ba4..e0fbb1ca37 100644
--- a/build/flags/Android.bp
+++ b/build/flags/Android.bp
@@ -67,5 +67,6 @@ java_aconfig_library {
     aconfig_declarations: "art-aconfig-flags",
     visibility: [
         "//cts/hostsidetests/compilation:__subpackages__",
+        "//cts/tests/libcore/vmdebug:__subpackages__",
     ],
 }
diff --git a/build/flags/art-flags.aconfig b/build/flags/art-flags.aconfig
index 825a888330..919c6fa97d 100644
--- a/build/flags/art-flags.aconfig
+++ b/build/flags/art-flags.aconfig
@@ -15,6 +15,16 @@
 package: "com.android.art.flags"
 container: "com.android.art"
 
+# Flag for generational CMC feature
+flag {
+  name: "use_generational_cmc"
+  namespace: "art_performance"
+  description: "Flag to control whether CMC's generational logic should be used or not"
+  bug: "343220989"
+  is_fixed_read_only: true
+  is_exported: false
+}
+
 # Flag for the experimental feature of on-demand method tracing
 flag {
   name: "always_enable_profile_code"
@@ -44,6 +54,15 @@ flag {
   is_fixed_read_only: true
 }
 
+flag {
+  namespace: "system_performance"
+  name: "executable_method_file_offsets_v2"
+  is_exported: true
+  description: "This flag includes the API for getting the compiled native executable offset info for a java executable"
+  bug: "400457896"
+  is_fixed_read_only: true
+}
+
 flag {
   name: "art_service_v3"
   namespace: "art_mainline"
@@ -52,3 +71,21 @@ flag {
   is_fixed_read_only: true
   is_exported: true
 }
+
+flag {
+  name: "virtual_thread_impl_v1"
+  namespace: "core_libraries"
+  description: "Flag for version 1 of Virtual Thread implementation."
+  bug: "346542404"
+  is_fixed_read_only: true
+  is_exported: false
+}
+
+flag {
+  name: "fast_baseline_compiler"
+  namespace: "art_performance"
+  description: "Flag to enable the fast ARM64 baseline compiler"
+  bug: "384433113"
+  is_fixed_read_only: true
+  is_exported: false
+}
diff --git a/cmdline/cmdline_types.h b/cmdline/cmdline_types.h
index fe7a55d559..524f181a8e 100644
--- a/cmdline/cmdline_types.h
+++ b/cmdline/cmdline_types.h
@@ -550,9 +550,9 @@ struct XGcOption {
   // These defaults are used when the command line arguments for -Xgc:
   // are either omitted completely or partially.
   gc::CollectorType collector_type_ = gc::kCollectorTypeDefault;
-  bool verify_pre_gc_heap_ = false;
+  bool verify_pre_gc_heap_ = kIsDebugBuild;
   bool verify_pre_sweeping_heap_ = kIsDebugBuild;
-  bool generational_cc = kEnableGenerationalCCByDefault;
+  bool generational_gc = kEnableGenerationalGCByDefault;
   bool verify_post_gc_heap_ = kIsDebugBuild;
   bool verify_pre_gc_rosalloc_ = kIsDebugBuild;
   bool verify_pre_sweeping_rosalloc_ = false;
@@ -566,7 +566,8 @@ template <>
 struct CmdlineType<XGcOption> : CmdlineTypeParser<XGcOption> {
   Result Parse(const std::string& option) {  // -Xgc: already stripped
     XGcOption xgc{};
-
+    // TODO: Deprecate and eventually remove -Xgc:[no]generational_cc option in
+    // favor of -Xgc:[no]generational_gc.
     std::vector<std::string> gc_options;
     Split(option, ',', &gc_options);
     for (const std::string& gc_option : gc_options) {
@@ -581,20 +582,20 @@ struct CmdlineType<XGcOption> : CmdlineTypeParser<XGcOption> {
         xgc.verify_pre_sweeping_heap_ = true;
       } else if (gc_option == "nopresweepingverify") {
         xgc.verify_pre_sweeping_heap_ = false;
-      } else if (gc_option == "generational_cc") {
-        // Note: Option "-Xgc:generational_cc" can be passed directly by
+      } else if (gc_option == "generational_cc" || gc_option == "generational_gc") {
+        // Note: Option "-Xgc:generational_gc" can be passed directly by
         // app_process/zygote (see `android::AndroidRuntime::startVm`). If this
         // option is ever deprecated, it should still be accepted (but ignored)
         // for compatibility reasons (this should not prevent the runtime from
         // starting up).
-        xgc.generational_cc = true;
-      } else if (gc_option == "nogenerational_cc") {
-        // Note: Option "-Xgc:nogenerational_cc" can be passed directly by
+        xgc.generational_gc = true;
+      } else if (gc_option == "nogenerational_cc" || gc_option == "nogenerational_gc") {
+        // Note: Option "-Xgc:nogenerational_gc" can be passed directly by
         // app_process/zygote (see `android::AndroidRuntime::startVm`). If this
         // option is ever deprecated, it should still be accepted (but ignored)
         // for compatibility reasons (this should not prevent the runtime from
         // starting up).
-        xgc.generational_cc = false;
+        xgc.generational_gc = false;
       } else if (gc_option == "postverify") {
         xgc.verify_post_gc_heap_ = true;
       } else if (gc_option == "nopostverify") {
diff --git a/compiler/Android.bp b/compiler/Android.bp
index ce087dacf7..73994f7d95 100644
--- a/compiler/Android.bp
+++ b/compiler/Android.bp
@@ -153,6 +153,7 @@ art_cc_defaults {
         "optimizing/code_sinking.cc",
         "optimizing/constant_folding.cc",
         "optimizing/constructor_fence_redundancy_elimination.cc",
+        "optimizing/control_flow_simplifier.cc",
         "optimizing/data_type.cc",
         "optimizing/dead_code_elimination.cc",
         "optimizing/escape.cc",
@@ -186,7 +187,6 @@ art_cc_defaults {
         "optimizing/register_allocation_resolver.cc",
         "optimizing/register_allocator.cc",
         "optimizing/register_allocator_linear_scan.cc",
-        "optimizing/select_generator.cc",
         "optimizing/scheduler.cc",
         "optimizing/sharpening.cc",
         "optimizing/side_effects_analysis.cc",
@@ -230,6 +230,7 @@ art_cc_defaults {
                 "optimizing/scheduler_arm64.cc",
                 "optimizing/instruction_simplifier_arm64.cc",
                 "optimizing/intrinsics_arm64.cc",
+                "optimizing/fast_compiler_arm64.cc",
                 "utils/arm64/assembler_arm64.cc",
                 "utils/arm64/jni_macro_assembler_arm64.cc",
                 "utils/arm64/managed_register_arm64.cc",
@@ -437,7 +438,6 @@ art_cc_defaults {
     name: "art_compiler_tests_defaults",
     device_common_data: [
         ":art-gtest-jars-ExceptionHandle",
-        ":art-gtest-jars-Interfaces",
         ":art-gtest-jars-Main",
         ":art-gtest-jars-MyClassNatives",
     ],
@@ -460,6 +460,7 @@ art_cc_defaults {
         "oat/jni_stub_hash_map_test.cc",
         "optimizing/bounds_check_elimination_test.cc",
         "optimizing/constant_folding_test.cc",
+        "optimizing/control_flow_simplifier_test.cc",
         "optimizing/data_type_test.cc",
         "optimizing/dead_code_elimination_test.cc",
         "optimizing/dominator_test.cc",
@@ -478,17 +479,15 @@ art_cc_defaults {
         "optimizing/nodes_test.cc",
         "optimizing/nodes_vector_test.cc",
         "optimizing/parallel_move_test.cc",
+        "optimizing/prepare_for_register_allocation_test.cc",
         "optimizing/pretty_printer_test.cc",
         "optimizing/reference_type_propagation_test.cc",
-        "optimizing/select_generator_test.cc",
         "optimizing/side_effects_test.cc",
         "optimizing/ssa_liveness_analysis_test.cc",
         "optimizing/ssa_test.cc",
         "optimizing/stack_map_test.cc",
         "optimizing/superblock_cloner_test.cc",
         "optimizing/suspend_check_test.cc",
-        "utils/atomic_dex_ref_map_test.cc",
-        "utils/dedupe_set_test.cc",
 
         "optimizing/codegen_test.cc",
         "optimizing/instruction_simplifier_test.cc",
diff --git a/compiler/art_standalone_compiler_tests.xml b/compiler/art_standalone_compiler_tests.xml
index c2065dd766..813a6f10d0 100644
--- a/compiler/art_standalone_compiler_tests.xml
+++ b/compiler/art_standalone_compiler_tests.xml
@@ -26,7 +26,6 @@
     <target_preparer class="com.android.compatibility.common.tradefed.targetprep.FilePusher">
         <option name="cleanup" value="true" />
         <option name="push" value="art-gtest-jars-ExceptionHandle.jar->/data/local/tmp/art_standalone_compiler_tests/art-gtest-jars-ExceptionHandle.jar" />
-        <option name="push" value="art-gtest-jars-Interfaces.jar->/data/local/tmp/art_standalone_compiler_tests/art-gtest-jars-Interfaces.jar" />
         <option name="push" value="art-gtest-jars-Main.jar->/data/local/tmp/art_standalone_compiler_tests/art-gtest-jars-Main.jar" />
         <option name="push" value="art-gtest-jars-MyClassNatives.jar->/data/local/tmp/art_standalone_compiler_tests/art-gtest-jars-MyClassNatives.jar" />
     </target_preparer>
diff --git a/compiler/common_compiler_test.cc b/compiler/common_compiler_test.cc
index f24406e599..ef915e4152 100644
--- a/compiler/common_compiler_test.cc
+++ b/compiler/common_compiler_test.cc
@@ -32,6 +32,7 @@
 #include "driver/compiled_code_storage.h"
 #include "driver/compiler_options.h"
 #include "jni/java_vm_ext.h"
+#include "instrumentation-inl.h"
 #include "interpreter/interpreter.h"
 #include "mirror/class-inl.h"
 #include "mirror/class_loader.h"
@@ -40,7 +41,6 @@
 #include "oat/oat_quick_method_header.h"
 #include "scoped_thread_state_change-inl.h"
 #include "thread-current-inl.h"
-#include "utils/atomic_dex_ref_map-inl.h"
 
 namespace art HIDDEN {
 
@@ -63,7 +63,7 @@ class CommonCompilerTestImpl::CodeAndMetadata {
     const uint32_t capacity = RoundUp(code_offset + code_size, page_size);
 
     // Create a memfd handle with sufficient capacity.
-    android::base::unique_fd mem_fd(art::memfd_create_compat("test code", /*flags=*/ 0));
+    android::base::unique_fd mem_fd(art::memfd_create("test code", /*flags=*/ 0));
     CHECK_GE(mem_fd.get(), 0);
     int err = ftruncate(mem_fd, capacity);
     CHECK_EQ(err, 0);
@@ -306,7 +306,11 @@ void CommonCompilerTestImpl::CompileMethod(ArtMethod* method) {
                                              storage.GetStackMap(),
                                              storage.GetInstructionSet());
     LOG(INFO) << "MakeExecutable " << method->PrettyMethod() << " code=" << method_code;
-    GetRuntime()->GetInstrumentation()->InitializeMethodsCode(method, /*aot_code=*/ method_code);
+    instrumentation::Instrumentation* instr = GetRuntime()->GetInstrumentation();
+    const void* entrypoint = instr->GetInitialEntrypoint(method->GetAccessFlags(), method_code);
+    CHECK(!instr->IsForcedInterpretOnly());
+    CHECK(!instr->EntryExitStubsInstalled());
+    instr->UpdateMethodsCode(method, entrypoint);
   }
 }
 
diff --git a/compiler/debug/elf_debug_info_writer.h b/compiler/debug/elf_debug_info_writer.h
index ae4cbd85ab..f2be4fb655 100644
--- a/compiler/debug/elf_debug_info_writer.h
+++ b/compiler/debug/elf_debug_info_writer.h
@@ -379,8 +379,11 @@ class ElfCompilationUnitWriter {
         }
 
         // Member variables.
-        for (uint32_t i = 0, count = type->NumInstanceFields(); i < count; ++i) {
-          ArtField* field = type->GetInstanceField(i);
+        for (uint32_t i = 0, count = type->NumFields(); i < count; ++i) {
+          ArtField* field = type->GetField(i);
+          if (field->IsStatic()) {
+            continue;
+          }
           info_.StartTag(DW_TAG_member);
           WriteName(field->GetName());
           WriteLazyType(field->GetTypeDescriptor());
diff --git a/compiler/driver/compiler_options.h b/compiler/driver/compiler_options.h
index 36ecf88199..a3957ce232 100644
--- a/compiler/driver/compiler_options.h
+++ b/compiler/driver/compiler_options.h
@@ -101,7 +101,13 @@ class CompilerOptions final {
   }
 
   bool IsJniCompilationEnabled() const {
+#ifdef ART_USE_RESTRICTED_MODE
+    // TODO(Simulator): Support JNICompiler.
+    // Without the JNI compiler, GenericJNITrampoline will be used for JNI calls.
+    return false;
+#else
     return CompilerFilter::IsJniCompilationEnabled(compiler_filter_);
+#endif
   }
 
   bool IsVerificationEnabled() const {
diff --git a/compiler/driver/dex_compilation_unit.cc b/compiler/driver/dex_compilation_unit.cc
index ccebfa9c07..fc139aa000 100644
--- a/compiler/driver/dex_compilation_unit.cc
+++ b/compiler/driver/dex_compilation_unit.cc
@@ -67,9 +67,9 @@ bool DexCompilationUnit::RequiresConstructorBarrier() const {
     // Decoding class data can be slow, so iterate over fields of the compiling class if resolved.
     ScopedObjectAccess soa(Thread::Current());
     ObjPtr<mirror::Class> compiling_class = GetCompilingClass().Get();
-    for (size_t i = 0, size = compiling_class->NumInstanceFields(); i != size; ++i) {
-      ArtField* field = compiling_class->GetInstanceField(i);
-      if (field->IsFinal()) {
+    for (size_t i = 0, size = compiling_class->NumFields(); i != size; ++i) {
+      ArtField* field = compiling_class->GetField(i);
+      if (field->IsFinal() && !field->IsStatic()) {
         return true;
       }
     }
diff --git a/compiler/jit/jit_logger.cc b/compiler/jit/jit_logger.cc
index f192ce7139..29fd2b3e80 100644
--- a/compiler/jit/jit_logger.cc
+++ b/compiler/jit/jit_logger.cc
@@ -201,6 +201,9 @@ static uint32_t GetElfMach() {
 #elif defined(__x86_64__)
   static const uint32_t kElfMachX64 = 0x3E;
   return kElfMachX64;
+#elif defined(__riscv__) || defined(__riscv)
+  static const uint32_t kElfMachRISCV = 0xF3;
+  return kElfMachRISCV;
 #else
   UNIMPLEMENTED(WARNING) << "Unsupported architecture in JitLogger";
   return 0;
diff --git a/compiler/jni/jni_compiler_test.cc b/compiler/jni/jni_compiler_test.cc
index 35ee1edafd..1a408bb724 100644
--- a/compiler/jni/jni_compiler_test.cc
+++ b/compiler/jni/jni_compiler_test.cc
@@ -912,14 +912,14 @@ void JniCompilerTest::CompileAndRun_fooJJ_synchronizedImpl() {
   gLogVerbosity.systrace_lock_logging = true;
   InitEntryPoints(&self->tlsPtr_.jni_entrypoints,
                   &self->tlsPtr_.quick_entrypoints,
-                  self->ReadFlag(ThreadFlag::kMonitorJniEntryExit));
+                  self->ReadFlag(ThreadFlag::kMonitorJniEntryExit, std::memory_order_relaxed));
   result = env_->CallNonvirtualLongMethod(jobj_, jklass_, jmethod_, a, b);
   EXPECT_EQ(a | b, result);
   EXPECT_EQ(5, gJava_MyClassNatives_fooJJ_synchronized_calls[gCurrentJni]);
   gLogVerbosity.systrace_lock_logging = false;
   InitEntryPoints(&self->tlsPtr_.jni_entrypoints,
                   &self->tlsPtr_.quick_entrypoints,
-                  self->ReadFlag(ThreadFlag::kMonitorJniEntryExit));
+                  self->ReadFlag(ThreadFlag::kMonitorJniEntryExit, std::memory_order_relaxed));
 
   gJava_MyClassNatives_fooJJ_synchronized_calls[gCurrentJni] = 0;
 }
diff --git a/compiler/optimizing/bounds_check_elimination.cc b/compiler/optimizing/bounds_check_elimination.cc
index c0d4c37659..1ef4d751a2 100644
--- a/compiler/optimizing/bounds_check_elimination.cc
+++ b/compiler/optimizing/bounds_check_elimination.cc
@@ -1983,7 +1983,7 @@ class BCEVisitor final : public HGraphVisitor {
             }
             user->RemoveAsUserOfInput(index);
             user->SetRawEnvAt(index, phi);
-            phi->AddEnvUseAt(user, index);
+            phi->AddEnvUseAt(GetGraph()->GetAllocator(), user, index);
           }
         }
       }
diff --git a/compiler/optimizing/code_generator.cc b/compiler/optimizing/code_generator.cc
index cfccdb8934..d63b0abcc7 100644
--- a/compiler/optimizing/code_generator.cc
+++ b/compiler/optimizing/code_generator.cc
@@ -348,7 +348,7 @@ void CodeGenerator::Compile() {
     // This ensures that we have correct native line mapping for all native instructions.
     // It is necessary to make stepping over a statement work. Otherwise, any initial
     // instructions (e.g. moves) would be assumed to be the start of next statement.
-    MaybeRecordNativeDebugInfo(/* instruction= */ nullptr, block->GetDexPc());
+    MaybeRecordNativeDebugInfoForBlockEntry(block->GetDexPc());
     for (HInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
       HInstruction* current = it.Current();
       if (current->HasEnvironment()) {
@@ -541,7 +541,7 @@ void CodeGenerator::GenerateInvokeStaticOrDirectRuntimeCall(
       UNREACHABLE();
   }
 
-  InvokeRuntime(entrypoint, invoke, invoke->GetDexPc(), slow_path);
+  InvokeRuntime(entrypoint, invoke, slow_path);
 }
 void CodeGenerator::GenerateInvokeUnresolvedRuntimeCall(HInvokeUnresolved* invoke) {
   MethodReference method_reference(invoke->GetMethodReference());
@@ -570,7 +570,7 @@ void CodeGenerator::GenerateInvokeUnresolvedRuntimeCall(HInvokeUnresolved* invok
       LOG(FATAL) << "Unexpected invoke type: " << invoke->GetInvokeType();
       UNREACHABLE();
   }
-  InvokeRuntime(entrypoint, invoke, invoke->GetDexPc(), nullptr);
+  InvokeRuntime(entrypoint, invoke);
 }
 
 void CodeGenerator::GenerateInvokePolymorphicCall(HInvokePolymorphic* invoke,
@@ -579,13 +579,13 @@ void CodeGenerator::GenerateInvokePolymorphicCall(HInvokePolymorphic* invoke,
   // method index) since it requires multiple info from the instruction (registers A, B, H). Not
   // using the reservation has no effect on the registers used in the runtime call.
   QuickEntrypointEnum entrypoint = kQuickInvokePolymorphic;
-  InvokeRuntime(entrypoint, invoke, invoke->GetDexPc(), slow_path);
+  InvokeRuntime(entrypoint, invoke, slow_path);
 }
 
 void CodeGenerator::GenerateInvokeCustomCall(HInvokeCustom* invoke) {
   MoveConstant(invoke->GetLocations()->GetTemp(0), invoke->GetCallSiteIndex());
   QuickEntrypointEnum entrypoint = kQuickInvokeCustom;
-  InvokeRuntime(entrypoint, invoke, invoke->GetDexPc(), nullptr);
+  InvokeRuntime(entrypoint, invoke);
 }
 
 void CodeGenerator::CreateStringBuilderAppendLocations(HStringBuilderAppend* instruction,
@@ -645,7 +645,7 @@ void CodeGenerator::CreateUnresolvedFieldLocationSummary(
   bool is_get = field_access->IsUnresolvedInstanceFieldGet()
       || field_access->IsUnresolvedStaticFieldGet();
 
-  ArenaAllocator* allocator = field_access->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = GetGraph()->GetAllocator();
   LocationSummary* locations =
       new (allocator) LocationSummary(field_access, LocationSummary::kCallOnMainOnly);
 
@@ -690,7 +690,6 @@ void CodeGenerator::GenerateUnresolvedFieldAccess(
     HInstruction* field_access,
     DataType::Type field_type,
     uint32_t field_index,
-    uint32_t dex_pc,
     const FieldAccessCallingConvention& calling_convention) {
   LocationSummary* locations = field_access->GetLocations();
 
@@ -754,7 +753,7 @@ void CodeGenerator::GenerateUnresolvedFieldAccess(
     default:
       LOG(FATAL) << "Invalid type " << field_type;
   }
-  InvokeRuntime(entrypoint, field_access, dex_pc, nullptr);
+  InvokeRuntime(entrypoint, field_access);
 
   if (is_get && DataType::IsFloatingPointType(field_type)) {
     MoveLocation(locations->Out(), calling_convention.GetReturnLocation(field_type), field_type);
@@ -780,10 +779,10 @@ void CodeGenerator::GenerateLoadClassRuntimeCall(HLoadClass* cls) {
   MoveConstant(locations->GetTemp(0), cls->GetTypeIndex().index_);
   if (cls->NeedsAccessCheck()) {
     CheckEntrypointTypes<kQuickResolveTypeAndVerifyAccess, void*, uint32_t>();
-    InvokeRuntime(kQuickResolveTypeAndVerifyAccess, cls, cls->GetDexPc());
+    InvokeRuntime(kQuickResolveTypeAndVerifyAccess, cls);
   } else {
     CheckEntrypointTypes<kQuickResolveType, void*, uint32_t>();
-    InvokeRuntime(kQuickResolveType, cls, cls->GetDexPc());
+    InvokeRuntime(kQuickResolveType, cls);
   }
 }
 
@@ -804,7 +803,7 @@ void CodeGenerator::GenerateLoadMethodHandleRuntimeCall(HLoadMethodHandle* metho
   LocationSummary* locations = method_handle->GetLocations();
   MoveConstant(locations->GetTemp(0), method_handle->GetMethodHandleIndex());
   CheckEntrypointTypes<kQuickResolveMethodHandle, void*, uint32_t>();
-  InvokeRuntime(kQuickResolveMethodHandle, method_handle, method_handle->GetDexPc());
+  InvokeRuntime(kQuickResolveMethodHandle, method_handle);
 }
 
 void CodeGenerator::CreateLoadMethodTypeRuntimeCallLocationSummary(
@@ -824,7 +823,7 @@ void CodeGenerator::GenerateLoadMethodTypeRuntimeCall(HLoadMethodType* method_ty
   LocationSummary* locations = method_type->GetLocations();
   MoveConstant(locations->GetTemp(0), method_type->GetProtoIndex().index_);
   CheckEntrypointTypes<kQuickResolveMethodType, void*, uint32_t>();
-  InvokeRuntime(kQuickResolveMethodType, method_type, method_type->GetDexPc());
+  InvokeRuntime(kQuickResolveMethodType, method_type);
 }
 
 static uint32_t GetBootImageOffsetImpl(const void* object, ImageHeader::ImageSections section) {
@@ -1138,11 +1137,24 @@ static bool NeedsVregInfo(HInstruction* instruction, bool osr) {
          instruction->CanThrowIntoCatchBlock();
 }
 
+void CodeGenerator::RecordPcInfoForFrameOrBlockEntry(uint32_t dex_pc) {
+  StackMapStream* stack_map_stream = GetStackMapStream();
+  stack_map_stream->BeginStackMapEntry(dex_pc, GetAssembler()->CodePosition());
+  stack_map_stream->EndStackMapEntry();
+}
+
 void CodeGenerator::RecordPcInfo(HInstruction* instruction,
-                                 uint32_t dex_pc,
                                  SlowPathCode* slow_path,
                                  bool native_debug_info) {
-  RecordPcInfo(instruction, dex_pc, GetAssembler()->CodePosition(), slow_path, native_debug_info);
+  // Only for native debuggable apps we take a look at the dex_pc from the instruction itself. For
+  // the regular case, we retrieve the dex_pc from the instruction's environment.
+  DCHECK_IMPLIES(native_debug_info, GetCompilerOptions().GetNativeDebuggable());
+  DCHECK_IMPLIES(!native_debug_info, instruction->HasEnvironment()) << *instruction;
+  RecordPcInfo(instruction,
+               native_debug_info ? instruction->GetDexPc() : kNoDexPc,
+               GetAssembler()->CodePosition(),
+               slow_path,
+               native_debug_info);
 }
 
 void CodeGenerator::RecordPcInfo(HInstruction* instruction,
@@ -1150,37 +1162,11 @@ void CodeGenerator::RecordPcInfo(HInstruction* instruction,
                                  uint32_t native_pc,
                                  SlowPathCode* slow_path,
                                  bool native_debug_info) {
-  if (instruction != nullptr) {
-    // The code generated for some type conversions
-    // may call the runtime, thus normally requiring a subsequent
-    // call to this method. However, the method verifier does not
-    // produce PC information for certain instructions, which are
-    // considered "atomic" (they cannot join a GC).
-    // Therefore we do not currently record PC information for such
-    // instructions.  As this may change later, we added this special
-    // case so that code generators may nevertheless call
-    // CodeGenerator::RecordPcInfo without triggering an error in
-    // CodeGenerator::BuildNativeGCMap ("Missing ref for dex pc 0x")
-    // thereafter.
-    if (instruction->IsTypeConversion()) {
-      return;
-    }
-    if (instruction->IsRem()) {
-      DataType::Type type = instruction->AsRem()->GetResultType();
-      if ((type == DataType::Type::kFloat32) || (type == DataType::Type::kFloat64)) {
-        return;
-      }
-    }
-  }
-
-  StackMapStream* stack_map_stream = GetStackMapStream();
-  if (instruction == nullptr) {
-    // For stack overflow checks and native-debug-info entries without dex register
-    // mapping (i.e. start of basic block or start of slow path).
-    stack_map_stream->BeginStackMapEntry(dex_pc, native_pc);
-    stack_map_stream->EndStackMapEntry();
-    return;
-  }
+  DCHECK(instruction != nullptr);
+  // Only for native debuggable apps we take a look at the dex_pc from the instruction itself. For
+  // the regular case, we retrieve the dex_pc from the instruction's environment.
+  DCHECK_IMPLIES(native_debug_info, GetCompilerOptions().GetNativeDebuggable());
+  DCHECK_IMPLIES(!native_debug_info, instruction->HasEnvironment()) << *instruction;
 
   LocationSummary* locations = instruction->GetLocations();
   uint32_t register_mask = locations->GetRegisterMask();
@@ -1220,6 +1206,7 @@ void CodeGenerator::RecordPcInfo(HInstruction* instruction,
       ? StackMap::Kind::Debug
       : (osr ? StackMap::Kind::OSR : StackMap::Kind::Default);
   bool needs_vreg_info = NeedsVregInfo(instruction, osr);
+  StackMapStream* stack_map_stream = GetStackMapStream();
   stack_map_stream->BeginStackMapEntry(outer_dex_pc,
                                        native_pc,
                                        register_mask,
@@ -1263,6 +1250,16 @@ bool CodeGenerator::HasStackMapAtCurrentPc() {
   return stack_map_stream->GetStackMapNativePcOffset(count - 1) == pc;
 }
 
+void CodeGenerator::MaybeRecordNativeDebugInfoForBlockEntry(uint32_t dex_pc) {
+  if (GetCompilerOptions().GetNativeDebuggable() && dex_pc != kNoDexPc) {
+    if (HasStackMapAtCurrentPc()) {
+      // Ensure that we do not collide with the stack map of the previous instruction.
+      GenerateNop();
+    }
+    RecordPcInfoForFrameOrBlockEntry(dex_pc);
+  }
+}
+
 void CodeGenerator::MaybeRecordNativeDebugInfo(HInstruction* instruction,
                                                uint32_t dex_pc,
                                                SlowPathCode* slow_path) {
@@ -1271,7 +1268,7 @@ void CodeGenerator::MaybeRecordNativeDebugInfo(HInstruction* instruction,
       // Ensure that we do not collide with the stack map of the previous instruction.
       GenerateNop();
     }
-    RecordPcInfo(instruction, dex_pc, slow_path, /* native_debug_info= */ true);
+    RecordPcInfo(instruction, slow_path, /* native_debug_info= */ true);
   }
 }
 
@@ -1572,7 +1569,8 @@ bool CodeGenerator::CanMoveNullCheckToUser(HNullCheck* null_check) {
 void CodeGenerator::MaybeRecordImplicitNullCheck(HInstruction* instr) {
   HNullCheck* null_check = instr->GetImplicitNullCheck();
   if (null_check != nullptr) {
-    RecordPcInfo(null_check, null_check->GetDexPc(), GetAssembler()->CodePosition());
+    DCHECK(compiler_options_.GetImplicitNullChecks());
+    RecordPcInfo(null_check);
   }
 }
 
diff --git a/compiler/optimizing/code_generator.h b/compiler/optimizing/code_generator.h
index 950bae5c8f..b118e9a7d0 100644
--- a/compiler/optimizing/code_generator.h
+++ b/compiler/optimizing/code_generator.h
@@ -339,20 +339,24 @@ class CodeGenerator : public DeletableArenaObject<kArenaAllocCodeGenerator> {
     return GetFrameSize() - FrameEntrySpillSize() - kShouldDeoptimizeFlagSize;
   }
 
-  // Record native to dex mapping for a suspend point. Required by runtime.
-  void RecordPcInfo(HInstruction* instruction,
-                    uint32_t dex_pc,
-                    uint32_t native_pc,
-                    SlowPathCode* slow_path = nullptr,
-                    bool native_debug_info = false);
+  // For stack overflow checks and native-debug-info entries without dex register
+  // mapping i.e. start of basic block or at frame entry.
+  void RecordPcInfoForFrameOrBlockEntry(uint32_t dex_pc = 0);
 
   // Record native to dex mapping for a suspend point.
   // The native_pc is used from Assembler::CodePosition.
   //
   // Note: As Assembler::CodePosition is target dependent, it does not guarantee the exact native_pc
   // for the instruction. If the exact native_pc is required it must be provided explicitly.
+  void RecordPcInfo(HInstruction* instruction,
+                    SlowPathCode* slow_path = nullptr,
+                    bool native_debug_info = false);
+
+  // Record native to dex mapping for a suspend point. Required by runtime.
+  // Do not use directly. Use the method above.
   void RecordPcInfo(HInstruction* instruction,
                     uint32_t dex_pc,
+                    uint32_t native_pc,
                     SlowPathCode* slow_path = nullptr,
                     bool native_debug_info = false);
 
@@ -363,6 +367,7 @@ class CodeGenerator : public DeletableArenaObject<kArenaAllocCodeGenerator> {
   //
   // ARM specific behaviour: The recorded native PC might be a branch over pools to instructions
   // corresponding the dex PC.
+  void MaybeRecordNativeDebugInfoForBlockEntry(uint32_t dex_pc);
   void MaybeRecordNativeDebugInfo(HInstruction* instruction,
                                   uint32_t dex_pc,
                                   SlowPathCode* slow_path = nullptr);
@@ -640,7 +645,6 @@ class CodeGenerator : public DeletableArenaObject<kArenaAllocCodeGenerator> {
       HInstruction* field_access,
       DataType::Type field_type,
       uint32_t field_index,
-      uint32_t dex_pc,
       const FieldAccessCallingConvention& calling_convention);
 
   static void CreateLoadClassRuntimeCallLocationSummary(HLoadClass* cls,
@@ -674,7 +678,6 @@ class CodeGenerator : public DeletableArenaObject<kArenaAllocCodeGenerator> {
 
   virtual void InvokeRuntime(QuickEntrypointEnum entrypoint,
                              HInstruction* instruction,
-                             uint32_t dex_pc,
                              SlowPathCode* slow_path = nullptr) = 0;
 
   // Check if the desired_string_load_kind is supported. If it is, return it,
@@ -700,7 +703,6 @@ class CodeGenerator : public DeletableArenaObject<kArenaAllocCodeGenerator> {
         return EmitReadBarrier()
             ? LocationSummary::kCallOnSlowPath
             : LocationSummary::kNoCall;
-        break;
       default:
         DCHECK(!load->NeedsEnvironment());
         return LocationSummary::kNoCall;
diff --git a/compiler/optimizing/code_generator_arm64.cc b/compiler/optimizing/code_generator_arm64.cc
index 369d21af03..5992f8385b 100644
--- a/compiler/optimizing/code_generator_arm64.cc
+++ b/compiler/optimizing/code_generator_arm64.cc
@@ -243,7 +243,7 @@ class BoundsCheckSlowPathARM64 : public SlowPathCodeARM64 {
     QuickEntrypointEnum entrypoint = instruction_->AsBoundsCheck()->IsStringCharAt()
         ? kQuickThrowStringBounds
         : kQuickThrowArrayBounds;
-    arm64_codegen->InvokeRuntime(entrypoint, instruction_, instruction_->GetDexPc(), this);
+    arm64_codegen->InvokeRuntime(entrypoint, instruction_, this);
     CheckEntrypointTypes<kQuickThrowStringBounds, void, int32_t, int32_t>();
     CheckEntrypointTypes<kQuickThrowArrayBounds, void, int32_t, int32_t>();
   }
@@ -263,7 +263,7 @@ class DivZeroCheckSlowPathARM64 : public SlowPathCodeARM64 {
   void EmitNativeCode(CodeGenerator* codegen) override {
     CodeGeneratorARM64* arm64_codegen = down_cast<CodeGeneratorARM64*>(codegen);
     __ Bind(GetEntryLabel());
-    arm64_codegen->InvokeRuntime(kQuickThrowDivZero, instruction_, instruction_->GetDexPc(), this);
+    arm64_codegen->InvokeRuntime(kQuickThrowDivZero, instruction_, this);
     CheckEntrypointTypes<kQuickThrowDivZero, void, void>();
   }
 
@@ -291,10 +291,7 @@ class LoadMethodTypeSlowPathARM64 : public SlowPathCodeARM64 {
     const dex::ProtoIndex proto_index = instruction_->AsLoadMethodType()->GetProtoIndex();
     __ Mov(calling_convention.GetRegisterAt(0).W(), proto_index.index_);
 
-    arm64_codegen->InvokeRuntime(kQuickResolveMethodType,
-                                 instruction_,
-                                 instruction_->GetDexPc(),
-                                 this);
+    arm64_codegen->InvokeRuntime(kQuickResolveMethodType, instruction_, this);
     CheckEntrypointTypes<kQuickResolveMethodType, void*, uint32_t>();
 
     DataType::Type type = instruction_->GetType();
@@ -322,7 +319,6 @@ class LoadClassSlowPathARM64 : public SlowPathCodeARM64 {
   void EmitNativeCode(CodeGenerator* codegen) override {
     LocationSummary* locations = instruction_->GetLocations();
     Location out = locations->Out();
-    const uint32_t dex_pc = instruction_->GetDexPc();
     bool must_resolve_type = instruction_->IsLoadClass() && cls_->MustResolveTypeOnSlowPath();
     bool must_do_clinit = instruction_->IsClinitCheck() || cls_->MustGenerateClinitCheck();
 
@@ -340,10 +336,10 @@ class LoadClassSlowPathARM64 : public SlowPathCodeARM64 {
       __ Mov(calling_convention.GetRegisterAt(0).W(), type_index.index_);
       if (cls_->NeedsAccessCheck()) {
         CheckEntrypointTypes<kQuickResolveTypeAndVerifyAccess, void*, uint32_t>();
-        arm64_codegen->InvokeRuntime(kQuickResolveTypeAndVerifyAccess, instruction_, dex_pc, this);
+        arm64_codegen->InvokeRuntime(kQuickResolveTypeAndVerifyAccess, instruction_, this);
       } else {
         CheckEntrypointTypes<kQuickResolveType, void*, uint32_t>();
-        arm64_codegen->InvokeRuntime(kQuickResolveType, instruction_, dex_pc, this);
+        arm64_codegen->InvokeRuntime(kQuickResolveType, instruction_, this);
       }
       // If we also must_do_clinit, the resolved type is now in the correct register.
     } else {
@@ -354,7 +350,7 @@ class LoadClassSlowPathARM64 : public SlowPathCodeARM64 {
                                   cls_->GetType());
     }
     if (must_do_clinit) {
-      arm64_codegen->InvokeRuntime(kQuickInitializeStaticStorage, instruction_, dex_pc, this);
+      arm64_codegen->InvokeRuntime(kQuickInitializeStaticStorage, instruction_, this);
       CheckEntrypointTypes<kQuickInitializeStaticStorage, void*, mirror::Class*>();
     }
 
@@ -393,7 +389,7 @@ class LoadStringSlowPathARM64 : public SlowPathCodeARM64 {
     InvokeRuntimeCallingConvention calling_convention;
     const dex::StringIndex string_index = instruction_->AsLoadString()->GetStringIndex();
     __ Mov(calling_convention.GetRegisterAt(0).W(), string_index.index_);
-    arm64_codegen->InvokeRuntime(kQuickResolveString, instruction_, instruction_->GetDexPc(), this);
+    arm64_codegen->InvokeRuntime(kQuickResolveString, instruction_, this);
     CheckEntrypointTypes<kQuickResolveString, void*, uint32_t>();
     DataType::Type type = instruction_->GetType();
     arm64_codegen->MoveLocation(locations->Out(), calling_convention.GetReturnLocation(type), type);
@@ -420,10 +416,7 @@ class NullCheckSlowPathARM64 : public SlowPathCodeARM64 {
       // Live registers will be restored in the catch block if caught.
       SaveLiveRegisters(codegen, instruction_->GetLocations());
     }
-    arm64_codegen->InvokeRuntime(kQuickThrowNullPointer,
-                                 instruction_,
-                                 instruction_->GetDexPc(),
-                                 this);
+    arm64_codegen->InvokeRuntime(kQuickThrowNullPointer, instruction_, this);
     CheckEntrypointTypes<kQuickThrowNullPointer, void, void>();
   }
 
@@ -445,7 +438,7 @@ class SuspendCheckSlowPathARM64 : public SlowPathCodeARM64 {
     CodeGeneratorARM64* arm64_codegen = down_cast<CodeGeneratorARM64*>(codegen);
     __ Bind(GetEntryLabel());
     SaveLiveRegisters(codegen, locations);  // Only saves live vector regs for SIMD.
-    arm64_codegen->InvokeRuntime(kQuickTestSuspend, instruction_, instruction_->GetDexPc(), this);
+    arm64_codegen->InvokeRuntime(kQuickTestSuspend, instruction_, this);
     CheckEntrypointTypes<kQuickTestSuspend, void, void>();
     RestoreLiveRegisters(codegen, locations);  // Only restores live vector regs for SIMD.
     if (successor_ == nullptr) {
@@ -487,7 +480,6 @@ class TypeCheckSlowPathARM64 : public SlowPathCodeARM64 {
     DCHECK(instruction_->IsCheckCast()
            || !locations->GetLiveRegisters()->ContainsCoreRegister(locations->Out().reg()));
     CodeGeneratorARM64* arm64_codegen = down_cast<CodeGeneratorARM64*>(codegen);
-    uint32_t dex_pc = instruction_->GetDexPc();
 
     __ Bind(GetEntryLabel());
 
@@ -505,14 +497,14 @@ class TypeCheckSlowPathARM64 : public SlowPathCodeARM64 {
                                LocationFrom(calling_convention.GetRegisterAt(1)),
                                DataType::Type::kReference);
     if (instruction_->IsInstanceOf()) {
-      arm64_codegen->InvokeRuntime(kQuickInstanceofNonTrivial, instruction_, dex_pc, this);
+      arm64_codegen->InvokeRuntime(kQuickInstanceofNonTrivial, instruction_, this);
       CheckEntrypointTypes<kQuickInstanceofNonTrivial, size_t, mirror::Object*, mirror::Class*>();
       DataType::Type ret_type = instruction_->GetType();
       Location ret_loc = calling_convention.GetReturnLocation(ret_type);
       arm64_codegen->MoveLocation(locations->Out(), ret_loc, ret_type);
     } else {
       DCHECK(instruction_->IsCheckCast());
-      arm64_codegen->InvokeRuntime(kQuickCheckInstanceOf, instruction_, dex_pc, this);
+      arm64_codegen->InvokeRuntime(kQuickCheckInstanceOf, instruction_, this);
       CheckEntrypointTypes<kQuickCheckInstanceOf, void, mirror::Object*, mirror::Class*>();
     }
 
@@ -544,7 +536,7 @@ class DeoptimizationSlowPathARM64 : public SlowPathCodeARM64 {
     InvokeRuntimeCallingConvention calling_convention;
     __ Mov(calling_convention.GetRegisterAt(0),
            static_cast<uint32_t>(instruction_->AsDeoptimize()->GetDeoptimizationKind()));
-    arm64_codegen->InvokeRuntime(kQuickDeoptimize, instruction_, instruction_->GetDexPc(), this);
+    arm64_codegen->InvokeRuntime(kQuickDeoptimize, instruction_, this);
     CheckEntrypointTypes<kQuickDeoptimize, void, DeoptimizationKind>();
   }
 
@@ -583,7 +575,7 @@ class ArraySetSlowPathARM64 : public SlowPathCodeARM64 {
     codegen->GetMoveResolver()->EmitNativeCode(&parallel_move);
 
     CodeGeneratorARM64* arm64_codegen = down_cast<CodeGeneratorARM64*>(codegen);
-    arm64_codegen->InvokeRuntime(kQuickAputObject, instruction_, instruction_->GetDexPc(), this);
+    arm64_codegen->InvokeRuntime(kQuickAputObject, instruction_, this);
     CheckEntrypointTypes<kQuickAputObject, void, mirror::Array*, int32_t, mirror::Object*>();
     RestoreLiveRegisters(codegen, locations);
     __ B(GetExitLabel());
@@ -768,10 +760,7 @@ class ReadBarrierForHeapReferenceSlowPathARM64 : public SlowPathCodeARM64 {
       codegen->GetMoveResolver()->EmitNativeCode(&parallel_move);
       arm64_codegen->MoveConstant(LocationFrom(calling_convention.GetRegisterAt(2)), offset_);
     }
-    arm64_codegen->InvokeRuntime(kQuickReadBarrierSlow,
-                                 instruction_,
-                                 instruction_->GetDexPc(),
-                                 this);
+    arm64_codegen->InvokeRuntime(kQuickReadBarrierSlow, instruction_, this);
     CheckEntrypointTypes<
         kQuickReadBarrierSlow, mirror::Object*, mirror::Object*, mirror::Object*, uint32_t>();
     arm64_codegen->MoveLocation(out_, calling_convention.GetReturnLocation(type), type);
@@ -849,10 +838,7 @@ class ReadBarrierForRootSlowPathARM64 : public SlowPathCodeARM64 {
     // which would emit a 32-bit move, as `type` is a (32-bit wide)
     // reference type (`DataType::Type::kReference`).
     __ Mov(calling_convention.GetRegisterAt(0), XRegisterFrom(out_));
-    arm64_codegen->InvokeRuntime(kQuickReadBarrierForRootSlow,
-                                 instruction_,
-                                 instruction_->GetDexPc(),
-                                 this);
+    arm64_codegen->InvokeRuntime(kQuickReadBarrierForRootSlow, instruction_, this);
     CheckEntrypointTypes<kQuickReadBarrierForRootSlow, mirror::Object*, GcRoot<mirror::Object>*>();
     arm64_codegen->MoveLocation(out_, calling_convention.GetReturnLocation(type), type);
 
@@ -910,7 +896,7 @@ class MethodEntryExitHooksSlowPathARM64 : public SlowPathCodeARM64 {
     if (instruction_->IsMethodExitHook()) {
       __ Mov(vixl::aarch64::x4, arm64_codegen->GetFrameSize());
     }
-    arm64_codegen->InvokeRuntime(entry_point, instruction_, instruction_->GetDexPc(), this);
+    arm64_codegen->InvokeRuntime(entry_point, instruction_, this);
     RestoreLiveRegisters(codegen, locations);
     __ B(GetExitLabel());
   }
@@ -1491,7 +1477,7 @@ void CodeGeneratorARM64::GenerateFrameEntry() {
                              kInstructionSize,
                              CodeBufferCheckScope::kExactSize);
       __ ldr(wzr, MemOperand(temp, 0));
-      RecordPcInfo(nullptr, 0);
+      RecordPcInfoForFrameOrBlockEntry();
     }
   }
 
@@ -1549,39 +1535,51 @@ void CodeGeneratorARM64::GenerateFrameEntry() {
 }
 
 void CodeGeneratorARM64::GenerateFrameExit() {
-  GetAssembler()->cfi().RememberState();
   if (!HasEmptyFrame()) {
     MaybeRecordTraceEvent(/* is_method_entry= */ false);
-
-    int32_t frame_size = dchecked_integral_cast<int32_t>(GetFrameSize());
-    uint32_t core_spills_offset = frame_size - GetCoreSpillSize();
-    CPURegList preserved_core_registers = GetFramePreservedCoreRegisters();
-    DCHECK(!preserved_core_registers.IsEmpty());
-    uint32_t fp_spills_offset = frame_size - FrameEntrySpillSize();
-    CPURegList preserved_fp_registers = GetFramePreservedFPRegisters();
-
-    CPURegister lowest_spill;
-    if (core_spills_offset == kXRegSizeInBytes) {
-      // If there is no gap between the method and the lowest core spill, use
-      // aligned LDP pre-index to pop both. Max difference is 504. We do
-      // that to reduce code size even though the loaded method is unused.
-      DCHECK_LE(frame_size, 504);  // 32 core registers are only 256 bytes.
-      lowest_spill = preserved_core_registers.PopLowestIndex();
-      core_spills_offset += kXRegSizeInBytes;
-    }
-    GetAssembler()->UnspillRegisters(preserved_fp_registers, fp_spills_offset);
-    GetAssembler()->UnspillRegisters(preserved_core_registers, core_spills_offset);
-    if (lowest_spill.IsValid()) {
-      __ Ldp(xzr, lowest_spill, MemOperand(sp, frame_size, PostIndex));
-      GetAssembler()->cfi().Restore(DWARFReg(lowest_spill));
-    } else {
-      __ Drop(frame_size);
-    }
-    GetAssembler()->cfi().AdjustCFAOffset(-frame_size);
+    PopFrameAndReturn(GetAssembler(),
+                      dchecked_integral_cast<int32_t>(GetFrameSize()),
+                      GetFramePreservedCoreRegisters(),
+                      GetFramePreservedFPRegisters());
+  } else {
+    __ Ret();
+  }
+}
+
+void CodeGeneratorARM64::PopFrameAndReturn(Arm64Assembler* assembler,
+                                           int32_t frame_size,
+                                           CPURegList preserved_core_registers,
+                                           CPURegList preserved_fp_registers) {
+  DCHECK(!preserved_core_registers.IsEmpty());
+  uint32_t core_spill_size = preserved_core_registers.GetTotalSizeInBytes();
+  uint32_t frame_entry_spill_size = preserved_fp_registers.GetTotalSizeInBytes() + core_spill_size;
+  uint32_t core_spills_offset = frame_size - core_spill_size;
+  uint32_t fp_spills_offset = frame_size - frame_entry_spill_size;
+  vixl::aarch64::MacroAssembler* vixl_assembler = assembler->GetVIXLAssembler();
+
+  CPURegister lowest_spill;
+  if (core_spills_offset == kXRegSizeInBytes) {
+    // If there is no gap between the method and the lowest core spill, use
+    // aligned LDP pre-index to pop both. Max difference is 504. We do
+    // that to reduce code size even though the loaded method is unused.
+    DCHECK_LE(frame_size, 504);  // 32 core registers are only 256 bytes.
+    lowest_spill = preserved_core_registers.PopLowestIndex();
+    core_spills_offset += kXRegSizeInBytes;
+  }
+
+  assembler->cfi().RememberState();
+  assembler->UnspillRegisters(preserved_fp_registers, fp_spills_offset);
+  assembler->UnspillRegisters(preserved_core_registers, core_spills_offset);
+  if (lowest_spill.IsValid()) {
+    vixl_assembler->Ldp(xzr, lowest_spill, MemOperand(sp, frame_size, PostIndex));
+    assembler->cfi().Restore(DWARFReg(lowest_spill));
+  } else {
+    vixl_assembler->Drop(frame_size);
   }
-  __ Ret();
-  GetAssembler()->cfi().RestoreState();
-  GetAssembler()->cfi().DefCFAOffset(GetFrameSize());
+  assembler->cfi().AdjustCFAOffset(-frame_size);
+  vixl_assembler->Ret();
+  assembler->cfi().RestoreState();
+  assembler->cfi().DefCFAOffset(frame_size);
 }
 
 CPURegList CodeGeneratorARM64::GetFramePreservedCoreRegisters() const {
@@ -2137,7 +2135,6 @@ void CodeGeneratorARM64::StoreRelease(HInstruction* instruction,
 
 void CodeGeneratorARM64::InvokeRuntime(QuickEntrypointEnum entrypoint,
                                        HInstruction* instruction,
-                                       uint32_t dex_pc,
                                        SlowPathCode* slow_path) {
   ValidateInvokeRuntime(entrypoint, instruction, slow_path);
 
@@ -2151,14 +2148,14 @@ void CodeGeneratorARM64::InvokeRuntime(QuickEntrypointEnum entrypoint,
     ExactAssemblyScope eas(GetVIXLAssembler(), kInstructionSize, CodeBufferCheckScope::kExactSize);
     __ blr(lr);
     if (EntrypointRequiresStackMap(entrypoint)) {
-      RecordPcInfo(instruction, dex_pc, slow_path);
+      RecordPcInfo(instruction, slow_path);
     }
   } else {
     // Ensure the pc position is recorded immediately after the `bl` instruction.
     ExactAssemblyScope eas(GetVIXLAssembler(), kInstructionSize, CodeBufferCheckScope::kExactSize);
     EmitEntrypointThunkCall(entrypoint_offset);
     if (EntrypointRequiresStackMap(entrypoint)) {
-      RecordPcInfo(instruction, dex_pc, slow_path);
+      RecordPcInfo(instruction, slow_path);
     }
   }
 }
@@ -2249,7 +2246,7 @@ void InstructionCodeGeneratorARM64::GenerateSuspendCheck(HSuspendCheck* instruct
 
   if (codegen_->CanUseImplicitSuspendCheck()) {
     __ Ldr(kImplicitSuspendCheckRegister, MemOperand(kImplicitSuspendCheckRegister));
-    codegen_->RecordPcInfo(instruction, instruction->GetDexPc());
+    codegen_->RecordPcInfo(instruction);
     if (successor != nullptr) {
       __ B(codegen_->GetLabelOf(successor));
     }
@@ -4849,7 +4846,7 @@ void CodeGeneratorARM64::MaybeGenerateInlineCacheCheck(HInstruction* instruction
       // Fast path for a monomorphic cache.
       __ Cmp(klass.W(), w9);
       __ B(eq, &done);
-      InvokeRuntime(kQuickUpdateInlineCache, instruction, instruction->GetDexPc());
+      InvokeRuntime(kQuickUpdateInlineCache, instruction);
       __ Bind(&done);
     } else {
       // This is unexpected, but we don't guarantee stable compilation across
@@ -4938,7 +4935,7 @@ void InstructionCodeGeneratorARM64::VisitInvokeInterface(HInvokeInterface* invok
     // lr();
     __ blr(lr);
     DCHECK(!codegen_->IsLeafMethod());
-    codegen_->RecordPcInfo(invoke, invoke->GetDexPc());
+    codegen_->RecordPcInfo(invoke);
   }
 
   codegen_->MaybeGenerateMarkingRegisterCheck(/* code= */ __LINE__);
@@ -5095,7 +5092,7 @@ void CodeGeneratorARM64::GenerateStaticOrDirectCall(
                            CodeBufferCheckScope::kExactSize);
     // lr()
     __ blr(lr);
-    RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+    RecordPcInfo(invoke, slow_path);
   };
   switch (invoke->GetCodePtrLocation()) {
     case CodePtrLocation::kCallSelf:
@@ -5106,7 +5103,7 @@ void CodeGeneratorARM64::GenerateStaticOrDirectCall(
                                kInstructionSize,
                                CodeBufferCheckScope::kExactSize);
         __ bl(&frame_entry_label_);
-        RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+        RecordPcInfo(invoke, slow_path);
       }
       break;
     case CodePtrLocation::kCallCriticalNative: {
@@ -5209,7 +5206,7 @@ void CodeGeneratorARM64::GenerateVirtualCall(
     ExactAssemblyScope eas(GetVIXLAssembler(), kInstructionSize, CodeBufferCheckScope::kExactSize);
     // lr();
     __ blr(lr);
-    RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+    RecordPcInfo(invoke, slow_path);
   }
 }
 
@@ -6077,7 +6074,7 @@ void InstructionCodeGeneratorARM64::VisitLoadString(HLoadString* load) NO_THREAD
   InvokeRuntimeCallingConvention calling_convention;
   DCHECK_EQ(calling_convention.GetRegisterAt(0).GetCode(), out.GetCode());
   __ Mov(calling_convention.GetRegisterAt(0).W(), load->GetStringIndex().index_);
-  codegen_->InvokeRuntime(kQuickResolveString, load, load->GetDexPc());
+  codegen_->InvokeRuntime(kQuickResolveString, load);
   CheckEntrypointTypes<kQuickResolveString, void*, uint32_t>();
   codegen_->MaybeGenerateMarkingRegisterCheck(/* code= */ __LINE__);
 }
@@ -6100,8 +6097,7 @@ void LocationsBuilderARM64::VisitMonitorOperation(HMonitorOperation* instruction
 
 void InstructionCodeGeneratorARM64::VisitMonitorOperation(HMonitorOperation* instruction) {
   codegen_->InvokeRuntime(instruction->IsEnter() ? kQuickLockObject : kQuickUnlockObject,
-                          instruction,
-                          instruction->GetDexPc());
+                          instruction);
   if (instruction->IsEnter()) {
     CheckEntrypointTypes<kQuickLockObject, void, mirror::Object*>();
   } else {
@@ -6200,7 +6196,7 @@ void LocationsBuilderARM64::VisitNewArray(HNewArray* instruction) {
 void InstructionCodeGeneratorARM64::VisitNewArray(HNewArray* instruction) {
   // Note: if heap poisoning is enabled, the entry point takes care of poisoning the reference.
   QuickEntrypointEnum entrypoint = CodeGenerator::GetArrayAllocationEntrypoint(instruction);
-  codegen_->InvokeRuntime(entrypoint, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(entrypoint, instruction);
   CheckEntrypointTypes<kQuickAllocArrayResolved, void*, mirror::Class*, int32_t>();
   codegen_->MaybeGenerateMarkingRegisterCheck(/* code= */ __LINE__);
 }
@@ -6214,7 +6210,7 @@ void LocationsBuilderARM64::VisitNewInstance(HNewInstance* instruction) {
 }
 
 void InstructionCodeGeneratorARM64::VisitNewInstance(HNewInstance* instruction) {
-  codegen_->InvokeRuntime(instruction->GetEntrypoint(), instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(instruction->GetEntrypoint(), instruction);
   CheckEntrypointTypes<kQuickAllocObjectWithChecks, void*, mirror::Class*>();
   codegen_->MaybeGenerateMarkingRegisterCheck(/* code= */ __LINE__);
 }
@@ -6261,7 +6257,7 @@ void CodeGeneratorARM64::GenerateImplicitNullCheck(HNullCheck* instruction) {
     EmissionCheckScope guard(GetVIXLAssembler(), kMaxMacroInstructionSizeInBytes);
     Location obj = instruction->GetLocations()->InAt(0);
     __ Ldr(wzr, HeapOperandFrom(obj, Offset(0)));
-    RecordPcInfo(instruction, instruction->GetDexPc());
+    RecordPcInfo(instruction);
   }
 }
 
@@ -6456,7 +6452,7 @@ void InstructionCodeGeneratorARM64::VisitRem(HRem* rem) {
     case DataType::Type::kFloat64: {
       QuickEntrypointEnum entrypoint =
           (type == DataType::Type::kFloat32) ? kQuickFmodf : kQuickFmod;
-      codegen_->InvokeRuntime(entrypoint, rem, rem->GetDexPc());
+      codegen_->InvokeRuntime(entrypoint, rem);
       if (type == DataType::Type::kFloat32) {
         CheckEntrypointTypes<kQuickFmodf, float, float, float>();
       } else {
@@ -6641,7 +6637,7 @@ void LocationsBuilderARM64::VisitStringBuilderAppend(HStringBuilderAppend* instr
 
 void InstructionCodeGeneratorARM64::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
   __ Mov(w0, instruction->GetFormat()->GetValue());
-  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction);
 }
 
 void LocationsBuilderARM64::VisitUnresolvedInstanceFieldGet(
@@ -6657,7 +6653,6 @@ void InstructionCodeGeneratorARM64::VisitUnresolvedInstanceFieldGet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -6674,7 +6669,6 @@ void InstructionCodeGeneratorARM64::VisitUnresolvedInstanceFieldSet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -6691,7 +6685,6 @@ void InstructionCodeGeneratorARM64::VisitUnresolvedStaticFieldGet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -6708,7 +6701,6 @@ void InstructionCodeGeneratorARM64::VisitUnresolvedStaticFieldSet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -6748,7 +6740,7 @@ void LocationsBuilderARM64::VisitThrow(HThrow* instruction) {
 }
 
 void InstructionCodeGeneratorARM64::VisitThrow(HThrow* instruction) {
-  codegen_->InvokeRuntime(kQuickDeliverException, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(kQuickDeliverException, instruction);
   CheckEntrypointTypes<kQuickDeliverException, void, mirror::Object*>();
 }
 
diff --git a/compiler/optimizing/code_generator_arm64.h b/compiler/optimizing/code_generator_arm64.h
index 7fefec93ff..81375871b6 100644
--- a/compiler/optimizing/code_generator_arm64.h
+++ b/compiler/optimizing/code_generator_arm64.h
@@ -186,11 +186,12 @@ class JumpTableARM64 : public DeletableArenaObject<kArenaAllocSwitchTable> {
  public:
   using VIXLInt32Literal = vixl::aarch64::Literal<int32_t>;
 
-  explicit JumpTableARM64(HPackedSwitch* switch_instr)
+  JumpTableARM64(HPackedSwitch* switch_instr, ArenaAllocator* allocator)
       : switch_instr_(switch_instr),
         table_start_(),
-        jump_targets_(switch_instr->GetAllocator()->Adapter(kArenaAllocCodeGenerator)) {
+        jump_targets_(allocator->Adapter(kArenaAllocCodeGenerator)) {
       uint32_t num_entries = switch_instr_->GetNumEntries();
+      jump_targets_.reserve(num_entries);
       for (uint32_t i = 0; i < num_entries; i++) {
         VIXLInt32Literal* lit = new VIXLInt32Literal(0);
         jump_targets_.emplace_back(lit);
@@ -660,6 +661,11 @@ class CodeGeneratorARM64 : public CodeGenerator {
   void GenerateFrameEntry() override;
   void GenerateFrameExit() override;
 
+  static void PopFrameAndReturn(Arm64Assembler* assembler,
+                                int32_t frame_size,
+                                vixl::aarch64::CPURegList preserved_core_registers,
+                                vixl::aarch64::CPURegList preserved_fp_registers);
+
   vixl::aarch64::CPURegList GetFramePreservedCoreRegisters() const;
   vixl::aarch64::CPURegList GetFramePreservedFPRegisters() const;
 
@@ -757,7 +763,8 @@ class CodeGeneratorARM64 : public CodeGenerator {
   uint32_t GetPreferredSlotsAlignment() const override { return vixl::aarch64::kXRegSizeInBytes; }
 
   JumpTableARM64* CreateJumpTable(HPackedSwitch* switch_instr) {
-    jump_tables_.emplace_back(new (GetGraph()->GetAllocator()) JumpTableARM64(switch_instr));
+    ArenaAllocator* allocator = GetGraph()->GetAllocator();
+    jump_tables_.emplace_back(new (allocator) JumpTableARM64(switch_instr, allocator));
     return jump_tables_.back().get();
   }
 
@@ -789,7 +796,6 @@ class CodeGeneratorARM64 : public CodeGenerator {
   // Generate code to invoke a runtime entry point.
   void InvokeRuntime(QuickEntrypointEnum entrypoint,
                      HInstruction* instruction,
-                     uint32_t dex_pc,
                      SlowPathCode* slow_path = nullptr) override;
 
   // Generate code to invoke a runtime entry point, but do not record
diff --git a/compiler/optimizing/code_generator_arm_vixl.cc b/compiler/optimizing/code_generator_arm_vixl.cc
index e88d14b3eb..a7854ac886 100644
--- a/compiler/optimizing/code_generator_arm_vixl.cc
+++ b/compiler/optimizing/code_generator_arm_vixl.cc
@@ -375,10 +375,7 @@ class NullCheckSlowPathARMVIXL : public SlowPathCodeARMVIXL {
       // Live registers will be restored in the catch block if caught.
       SaveLiveRegisters(codegen, instruction_->GetLocations());
     }
-    arm_codegen->InvokeRuntime(kQuickThrowNullPointer,
-                               instruction_,
-                               instruction_->GetDexPc(),
-                               this);
+    arm_codegen->InvokeRuntime(kQuickThrowNullPointer, instruction_, this);
     CheckEntrypointTypes<kQuickThrowNullPointer, void, void>();
   }
 
@@ -398,7 +395,7 @@ class DivZeroCheckSlowPathARMVIXL : public SlowPathCodeARMVIXL {
   void EmitNativeCode(CodeGenerator* codegen) override {
     CodeGeneratorARMVIXL* arm_codegen = down_cast<CodeGeneratorARMVIXL*>(codegen);
     __ Bind(GetEntryLabel());
-    arm_codegen->InvokeRuntime(kQuickThrowDivZero, instruction_, instruction_->GetDexPc(), this);
+    arm_codegen->InvokeRuntime(kQuickThrowDivZero, instruction_, this);
     CheckEntrypointTypes<kQuickThrowDivZero, void, void>();
   }
 
@@ -418,7 +415,7 @@ class SuspendCheckSlowPathARMVIXL : public SlowPathCodeARMVIXL {
   void EmitNativeCode(CodeGenerator* codegen) override {
     CodeGeneratorARMVIXL* arm_codegen = down_cast<CodeGeneratorARMVIXL*>(codegen);
     __ Bind(GetEntryLabel());
-    arm_codegen->InvokeRuntime(kQuickTestSuspend, instruction_, instruction_->GetDexPc(), this);
+    arm_codegen->InvokeRuntime(kQuickTestSuspend, instruction_, this);
     CheckEntrypointTypes<kQuickTestSuspend, void, void>();
     if (successor_ == nullptr) {
       __ B(GetReturnLabel());
@@ -475,7 +472,7 @@ class BoundsCheckSlowPathARMVIXL : public SlowPathCodeARMVIXL {
     QuickEntrypointEnum entrypoint = instruction_->AsBoundsCheck()->IsStringCharAt()
         ? kQuickThrowStringBounds
         : kQuickThrowArrayBounds;
-    arm_codegen->InvokeRuntime(entrypoint, instruction_, instruction_->GetDexPc(), this);
+    arm_codegen->InvokeRuntime(entrypoint, instruction_, this);
     CheckEntrypointTypes<kQuickThrowStringBounds, void, int32_t, int32_t>();
     CheckEntrypointTypes<kQuickThrowArrayBounds, void, int32_t, int32_t>();
   }
@@ -499,7 +496,6 @@ class LoadClassSlowPathARMVIXL : public SlowPathCodeARMVIXL {
   void EmitNativeCode(CodeGenerator* codegen) override {
     LocationSummary* locations = instruction_->GetLocations();
     Location out = locations->Out();
-    const uint32_t dex_pc = instruction_->GetDexPc();
     bool must_resolve_type = instruction_->IsLoadClass() && cls_->MustResolveTypeOnSlowPath();
     bool must_do_clinit = instruction_->IsClinitCheck() || cls_->MustGenerateClinitCheck();
 
@@ -517,10 +513,10 @@ class LoadClassSlowPathARMVIXL : public SlowPathCodeARMVIXL {
       __ Mov(calling_convention.GetRegisterAt(0), type_index.index_);
       if (cls_->NeedsAccessCheck()) {
         CheckEntrypointTypes<kQuickResolveTypeAndVerifyAccess, void*, uint32_t>();
-        arm_codegen->InvokeRuntime(kQuickResolveTypeAndVerifyAccess, instruction_, dex_pc, this);
+        arm_codegen->InvokeRuntime(kQuickResolveTypeAndVerifyAccess, instruction_, this);
       } else {
         CheckEntrypointTypes<kQuickResolveType, void*, uint32_t>();
-        arm_codegen->InvokeRuntime(kQuickResolveType, instruction_, dex_pc, this);
+        arm_codegen->InvokeRuntime(kQuickResolveType, instruction_, this);
       }
       // If we also must_do_clinit, the resolved type is now in the correct register.
     } else {
@@ -529,7 +525,7 @@ class LoadClassSlowPathARMVIXL : public SlowPathCodeARMVIXL {
       arm_codegen->Move32(LocationFrom(calling_convention.GetRegisterAt(0)), source);
     }
     if (must_do_clinit) {
-      arm_codegen->InvokeRuntime(kQuickInitializeStaticStorage, instruction_, dex_pc, this);
+      arm_codegen->InvokeRuntime(kQuickInitializeStaticStorage, instruction_, this);
       CheckEntrypointTypes<kQuickInitializeStaticStorage, void*, mirror::Class*>();
     }
 
@@ -569,7 +565,7 @@ class LoadStringSlowPathARMVIXL : public SlowPathCodeARMVIXL {
 
     InvokeRuntimeCallingConventionARMVIXL calling_convention;
     __ Mov(calling_convention.GetRegisterAt(0), string_index.index_);
-    arm_codegen->InvokeRuntime(kQuickResolveString, instruction_, instruction_->GetDexPc(), this);
+    arm_codegen->InvokeRuntime(kQuickResolveString, instruction_, this);
     CheckEntrypointTypes<kQuickResolveString, void*, uint32_t>();
 
     arm_codegen->Move32(locations->Out(), LocationFrom(r0));
@@ -612,18 +608,12 @@ class TypeCheckSlowPathARMVIXL : public SlowPathCodeARMVIXL {
                                LocationFrom(calling_convention.GetRegisterAt(1)),
                                DataType::Type::kReference);
     if (instruction_->IsInstanceOf()) {
-      arm_codegen->InvokeRuntime(kQuickInstanceofNonTrivial,
-                                 instruction_,
-                                 instruction_->GetDexPc(),
-                                 this);
+      arm_codegen->InvokeRuntime(kQuickInstanceofNonTrivial, instruction_, this);
       CheckEntrypointTypes<kQuickInstanceofNonTrivial, size_t, mirror::Object*, mirror::Class*>();
       arm_codegen->Move32(locations->Out(), LocationFrom(r0));
     } else {
       DCHECK(instruction_->IsCheckCast());
-      arm_codegen->InvokeRuntime(kQuickCheckInstanceOf,
-                                 instruction_,
-                                 instruction_->GetDexPc(),
-                                 this);
+      arm_codegen->InvokeRuntime(kQuickCheckInstanceOf, instruction_, this);
       CheckEntrypointTypes<kQuickCheckInstanceOf, void, mirror::Object*, mirror::Class*>();
     }
 
@@ -657,7 +647,7 @@ class DeoptimizationSlowPathARMVIXL : public SlowPathCodeARMVIXL {
     __ Mov(calling_convention.GetRegisterAt(0),
            static_cast<uint32_t>(instruction_->AsDeoptimize()->GetDeoptimizationKind()));
 
-    arm_codegen->InvokeRuntime(kQuickDeoptimize, instruction_, instruction_->GetDexPc(), this);
+    arm_codegen->InvokeRuntime(kQuickDeoptimize, instruction_, this);
     CheckEntrypointTypes<kQuickDeoptimize, void, DeoptimizationKind>();
   }
 
@@ -696,7 +686,7 @@ class ArraySetSlowPathARMVIXL : public SlowPathCodeARMVIXL {
     codegen->GetMoveResolver()->EmitNativeCode(&parallel_move);
 
     CodeGeneratorARMVIXL* arm_codegen = down_cast<CodeGeneratorARMVIXL*>(codegen);
-    arm_codegen->InvokeRuntime(kQuickAputObject, instruction_, instruction_->GetDexPc(), this);
+    arm_codegen->InvokeRuntime(kQuickAputObject, instruction_, this);
     CheckEntrypointTypes<kQuickAputObject, void, mirror::Array*, int32_t, mirror::Object*>();
     RestoreLiveRegisters(codegen, locations);
     __ B(GetExitLabel());
@@ -854,7 +844,7 @@ class ReadBarrierForHeapReferenceSlowPathARMVIXL : public SlowPathCodeARMVIXL {
       codegen->GetMoveResolver()->EmitNativeCode(&parallel_move);
       __ Mov(calling_convention.GetRegisterAt(2), offset_);
     }
-    arm_codegen->InvokeRuntime(kQuickReadBarrierSlow, instruction_, instruction_->GetDexPc(), this);
+    arm_codegen->InvokeRuntime(kQuickReadBarrierSlow, instruction_, this);
     CheckEntrypointTypes<
         kQuickReadBarrierSlow, mirror::Object*, mirror::Object*, mirror::Object*, uint32_t>();
     arm_codegen->Move32(out_, LocationFrom(r0));
@@ -922,10 +912,7 @@ class ReadBarrierForRootSlowPathARMVIXL : public SlowPathCodeARMVIXL {
     InvokeRuntimeCallingConventionARMVIXL calling_convention;
     CodeGeneratorARMVIXL* arm_codegen = down_cast<CodeGeneratorARMVIXL*>(codegen);
     arm_codegen->Move32(LocationFrom(calling_convention.GetRegisterAt(0)), root_);
-    arm_codegen->InvokeRuntime(kQuickReadBarrierForRootSlow,
-                               instruction_,
-                               instruction_->GetDexPc(),
-                               this);
+    arm_codegen->InvokeRuntime(kQuickReadBarrierForRootSlow, instruction_, this);
     CheckEntrypointTypes<kQuickReadBarrierForRootSlow, mirror::Object*, GcRoot<mirror::Object>*>();
     arm_codegen->Move32(out_, LocationFrom(r0));
 
@@ -958,7 +945,7 @@ class MethodEntryExitHooksSlowPathARMVIXL : public SlowPathCodeARMVIXL {
       // Load frame size to pass to the exit hooks
       __ Mov(vixl::aarch32::Register(R2), arm_codegen->GetFrameSize());
     }
-    arm_codegen->InvokeRuntime(entry_point, instruction_, instruction_->GetDexPc(), this);
+    arm_codegen->InvokeRuntime(entry_point, instruction_, this);
     RestoreLiveRegisters(codegen, locations);
     __ B(GetExitLabel());
   }
@@ -2420,7 +2407,7 @@ void CodeGeneratorARMVIXL::GenerateFrameEntry() {
                            vixl32::kMaxInstructionSizeInBytes,
                            CodeBufferCheckScope::kMaximumSize);
     __ ldr(temp, MemOperand(temp));
-    RecordPcInfo(nullptr, 0);
+    RecordPcInfoForFrameOrBlockEntry();
   }
 
   uint32_t frame_size = GetFrameSize();
@@ -2787,7 +2774,6 @@ void CodeGeneratorARMVIXL::AddLocationAsTemp(Location location, LocationSummary*
 
 void CodeGeneratorARMVIXL::InvokeRuntime(QuickEntrypointEnum entrypoint,
                                          HInstruction* instruction,
-                                         uint32_t dex_pc,
                                          SlowPathCode* slow_path) {
   ValidateInvokeRuntime(entrypoint, instruction, slow_path);
 
@@ -2804,7 +2790,7 @@ void CodeGeneratorARMVIXL::InvokeRuntime(QuickEntrypointEnum entrypoint,
                            CodeBufferCheckScope::kExactSize);
     __ blx(lr);
     if (EntrypointRequiresStackMap(entrypoint)) {
-      RecordPcInfo(instruction, dex_pc, slow_path);
+      RecordPcInfo(instruction, slow_path);
     }
   } else {
     // Ensure the pc position is recorded immediately after the `bl` instruction.
@@ -2813,7 +2799,7 @@ void CodeGeneratorARMVIXL::InvokeRuntime(QuickEntrypointEnum entrypoint,
                            CodeBufferCheckScope::kExactSize);
     EmitEntrypointThunkCall(entrypoint_offset);
     if (EntrypointRequiresStackMap(entrypoint)) {
-      RecordPcInfo(instruction, dex_pc, slow_path);
+      RecordPcInfo(instruction, slow_path);
     }
   }
 }
@@ -3709,7 +3695,7 @@ void CodeGeneratorARMVIXL::MaybeGenerateInlineCacheCheck(HInstruction* instructi
       // Fast path for a monomorphic cache.
       __ Cmp(klass, ip);
       __ B(eq, &done, /* is_far_target= */ false);
-      InvokeRuntime(kQuickUpdateInlineCache, instruction, instruction->GetDexPc());
+      InvokeRuntime(kQuickUpdateInlineCache, instruction);
       __ Bind(&done);
     } else {
       // This is unexpected, but we don't guarantee stable compilation across
@@ -3803,7 +3789,7 @@ void InstructionCodeGeneratorARMVIXL::VisitInvokeInterface(HInvokeInterface* inv
                            CodeBufferCheckScope::kExactSize);
     // LR();
     __ blx(lr);
-    codegen_->RecordPcInfo(invoke, invoke->GetDexPc());
+    codegen_->RecordPcInfo(invoke);
     DCHECK(!codegen_->IsLeafMethod());
   }
 
@@ -4180,12 +4166,12 @@ void InstructionCodeGeneratorARMVIXL::VisitTypeConversion(HTypeConversion* conve
           break;
 
         case DataType::Type::kFloat32:
-          codegen_->InvokeRuntime(kQuickF2l, conversion, conversion->GetDexPc());
+          codegen_->InvokeRuntime(kQuickF2l, conversion);
           CheckEntrypointTypes<kQuickF2l, int64_t, float>();
           break;
 
         case DataType::Type::kFloat64:
-          codegen_->InvokeRuntime(kQuickD2l, conversion, conversion->GetDexPc());
+          codegen_->InvokeRuntime(kQuickD2l, conversion);
           CheckEntrypointTypes<kQuickD2l, int64_t, double>();
           break;
 
@@ -4208,7 +4194,7 @@ void InstructionCodeGeneratorARMVIXL::VisitTypeConversion(HTypeConversion* conve
           break;
 
         case DataType::Type::kInt64:
-          codegen_->InvokeRuntime(kQuickL2f, conversion, conversion->GetDexPc());
+          codegen_->InvokeRuntime(kQuickL2f, conversion);
           CheckEntrypointTypes<kQuickL2f, float, int64_t>();
           break;
 
@@ -4773,7 +4759,7 @@ void InstructionCodeGeneratorARMVIXL::VisitDiv(HDiv* div) {
         DCHECK(calling_convention.GetRegisterAt(1).Is(RegisterFrom(rhs)));
         DCHECK(r0.Is(OutputRegister(div)));
 
-        codegen_->InvokeRuntime(kQuickIdivmod, div, div->GetDexPc());
+        codegen_->InvokeRuntime(kQuickIdivmod, div);
         CheckEntrypointTypes<kQuickIdivmod, int32_t, int32_t, int32_t>();
       }
       break;
@@ -4788,7 +4774,7 @@ void InstructionCodeGeneratorARMVIXL::VisitDiv(HDiv* div) {
       DCHECK(LowRegisterFrom(div->GetLocations()->Out()).Is(r0));
       DCHECK(HighRegisterFrom(div->GetLocations()->Out()).Is(r1));
 
-      codegen_->InvokeRuntime(kQuickLdiv, div, div->GetDexPc());
+      codegen_->InvokeRuntime(kQuickLdiv, div);
       CheckEntrypointTypes<kQuickLdiv, int64_t, int64_t, int64_t>();
       break;
     }
@@ -4909,26 +4895,26 @@ void InstructionCodeGeneratorARMVIXL::VisitRem(HRem* rem) {
         DCHECK(RegisterFrom(second).Is(calling_convention.GetRegisterAt(1)));
         DCHECK(out_reg.Is(r1));
 
-        codegen_->InvokeRuntime(kQuickIdivmod, rem, rem->GetDexPc());
+        codegen_->InvokeRuntime(kQuickIdivmod, rem);
         CheckEntrypointTypes<kQuickIdivmod, int32_t, int32_t, int32_t>();
       }
       break;
     }
 
     case DataType::Type::kInt64: {
-      codegen_->InvokeRuntime(kQuickLmod, rem, rem->GetDexPc());
-        CheckEntrypointTypes<kQuickLmod, int64_t, int64_t, int64_t>();
+      codegen_->InvokeRuntime(kQuickLmod, rem);
+      CheckEntrypointTypes<kQuickLmod, int64_t, int64_t, int64_t>();
       break;
     }
 
     case DataType::Type::kFloat32: {
-      codegen_->InvokeRuntime(kQuickFmodf, rem, rem->GetDexPc());
+      codegen_->InvokeRuntime(kQuickFmodf, rem);
       CheckEntrypointTypes<kQuickFmodf, float, float, float>();
       break;
     }
 
     case DataType::Type::kFloat64: {
-      codegen_->InvokeRuntime(kQuickFmod, rem, rem->GetDexPc());
+      codegen_->InvokeRuntime(kQuickFmod, rem);
       CheckEntrypointTypes<kQuickFmod, double, double, double>();
       break;
     }
@@ -5716,7 +5702,7 @@ void LocationsBuilderARMVIXL::VisitNewInstance(HNewInstance* instruction) {
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitNewInstance(HNewInstance* instruction) {
-  codegen_->InvokeRuntime(instruction->GetEntrypoint(), instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(instruction->GetEntrypoint(), instruction);
   CheckEntrypointTypes<kQuickAllocObjectWithChecks, void*, mirror::Class*>();
   codegen_->MaybeGenerateMarkingRegisterCheck(/* code= */ 12);
 }
@@ -5733,7 +5719,7 @@ void LocationsBuilderARMVIXL::VisitNewArray(HNewArray* instruction) {
 void InstructionCodeGeneratorARMVIXL::VisitNewArray(HNewArray* instruction) {
   // Note: if heap poisoning is enabled, the entry point takes care of poisoning the reference.
   QuickEntrypointEnum entrypoint = CodeGenerator::GetArrayAllocationEntrypoint(instruction);
-  codegen_->InvokeRuntime(entrypoint, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(entrypoint, instruction);
   CheckEntrypointTypes<kQuickAllocArrayResolved, void*, mirror::Class*, int32_t>();
   DCHECK(!codegen_->IsLeafMethod());
   codegen_->MaybeGenerateMarkingRegisterCheck(/* code= */ 13);
@@ -6443,7 +6429,7 @@ void LocationsBuilderARMVIXL::VisitStringBuilderAppend(HStringBuilderAppend* ins
 
 void InstructionCodeGeneratorARMVIXL::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
   __ Mov(r0, instruction->GetFormat()->GetValue());
-  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction);
 }
 
 void LocationsBuilderARMVIXL::VisitUnresolvedInstanceFieldGet(
@@ -6459,7 +6445,6 @@ void InstructionCodeGeneratorARMVIXL::VisitUnresolvedInstanceFieldGet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -6476,7 +6461,6 @@ void InstructionCodeGeneratorARMVIXL::VisitUnresolvedInstanceFieldSet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -6493,7 +6477,6 @@ void InstructionCodeGeneratorARMVIXL::VisitUnresolvedStaticFieldGet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -6510,7 +6493,6 @@ void InstructionCodeGeneratorARMVIXL::VisitUnresolvedStaticFieldSet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -6530,7 +6512,7 @@ void CodeGeneratorARMVIXL::GenerateImplicitNullCheck(HNullCheck* instruction) {
                          vixl32::kMaxInstructionSizeInBytes,
                          CodeBufferCheckScope::kMaximumSize);
   __ ldr(temps.Acquire(), MemOperand(InputRegisterAt(instruction, 0)));
-  RecordPcInfo(instruction, instruction->GetDexPc());
+  RecordPcInfo(instruction);
 }
 
 void CodeGeneratorARMVIXL::GenerateExplicitNullCheck(HNullCheck* instruction) {
@@ -8082,7 +8064,7 @@ void InstructionCodeGeneratorARMVIXL::VisitLoadString(HLoadString* load) NO_THRE
   DCHECK_EQ(load->GetLoadKind(), HLoadString::LoadKind::kRuntimeCall);
   InvokeRuntimeCallingConventionARMVIXL calling_convention;
   __ Mov(calling_convention.GetRegisterAt(0), load->GetStringIndex().index_);
-  codegen_->InvokeRuntime(kQuickResolveString, load, load->GetDexPc());
+  codegen_->InvokeRuntime(kQuickResolveString, load);
   CheckEntrypointTypes<kQuickResolveString, void*, uint32_t>();
   codegen_->MaybeGenerateMarkingRegisterCheck(/* code= */ 18);
 }
@@ -8122,7 +8104,7 @@ void LocationsBuilderARMVIXL::VisitThrow(HThrow* instruction) {
 }
 
 void InstructionCodeGeneratorARMVIXL::VisitThrow(HThrow* instruction) {
-  codegen_->InvokeRuntime(kQuickDeliverException, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(kQuickDeliverException, instruction);
   CheckEntrypointTypes<kQuickDeliverException, void, mirror::Object*>();
 }
 
@@ -8757,8 +8739,7 @@ void LocationsBuilderARMVIXL::VisitMonitorOperation(HMonitorOperation* instructi
 
 void InstructionCodeGeneratorARMVIXL::VisitMonitorOperation(HMonitorOperation* instruction) {
   codegen_->InvokeRuntime(instruction->IsEnter() ? kQuickLockObject : kQuickUnlockObject,
-                          instruction,
-                          instruction->GetDexPc());
+                          instruction);
   if (instruction->IsEnter()) {
     CheckEntrypointTypes<kQuickLockObject, void, mirror::Object*>();
   } else {
@@ -9631,7 +9612,7 @@ void CodeGeneratorARMVIXL::GenerateStaticOrDirectCall(
                              CodeBufferCheckScope::kExactSize);
       // LR()
       __ blx(lr);
-      RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+      RecordPcInfo(invoke, slow_path);
     }
   };
   switch (invoke->GetCodePtrLocation()) {
@@ -9643,7 +9624,7 @@ void CodeGeneratorARMVIXL::GenerateStaticOrDirectCall(
                                vixl32::k32BitT32InstructionSizeInBytes,
                                CodeBufferCheckScope::kMaximumSize);
         __ bl(GetFrameEntryLabel());
-        RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+        RecordPcInfo(invoke, slow_path);
       }
       break;
     case CodePtrLocation::kCallCriticalNative: {
@@ -9733,7 +9714,7 @@ void CodeGeneratorARMVIXL::GenerateVirtualCall(
                            CodeBufferCheckScope::kExactSize);
     // LR();
     __ blx(lr);
-    RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+    RecordPcInfo(invoke, slow_path);
   }
 }
 
diff --git a/compiler/optimizing/code_generator_arm_vixl.h b/compiler/optimizing/code_generator_arm_vixl.h
index ea8ec7e485..bbc519fcf9 100644
--- a/compiler/optimizing/code_generator_arm_vixl.h
+++ b/compiler/optimizing/code_generator_arm_vixl.h
@@ -204,11 +204,12 @@ ALWAYS_INLINE inline StoreOperandType GetStoreOperandType(DataType::Type type) {
 
 class JumpTableARMVIXL : public DeletableArenaObject<kArenaAllocSwitchTable> {
  public:
-  explicit JumpTableARMVIXL(HPackedSwitch* switch_instr)
+  JumpTableARMVIXL(HPackedSwitch* switch_instr, ArenaAllocator* allocator)
       : switch_instr_(switch_instr),
         table_start_(),
-        bb_addresses_(switch_instr->GetAllocator()->Adapter(kArenaAllocCodeGenerator)) {
+        bb_addresses_(allocator->Adapter(kArenaAllocCodeGenerator)) {
     uint32_t num_entries = switch_instr_->GetNumEntries();
+    bb_addresses_.reserve(num_entries);
     for (uint32_t i = 0; i < num_entries; i++) {
       VIXLInt32Literal *lit = new VIXLInt32Literal(0, vixl32::RawLiteral::kManuallyPlaced);
       bb_addresses_.emplace_back(lit);
@@ -615,7 +616,6 @@ class CodeGeneratorARMVIXL : public CodeGenerator {
   // Generate code to invoke a runtime entry point.
   void InvokeRuntime(QuickEntrypointEnum entrypoint,
                      HInstruction* instruction,
-                     uint32_t dex_pc,
                      SlowPathCode* slow_path = nullptr) override;
 
   // Generate code to invoke a runtime entry point, but do not record
@@ -884,7 +884,8 @@ class CodeGeneratorARMVIXL : public CodeGenerator {
   void GenerateExplicitNullCheck(HNullCheck* instruction) override;
 
   JumpTableARMVIXL* CreateJumpTable(HPackedSwitch* switch_instr) {
-    jump_tables_.emplace_back(new (GetGraph()->GetAllocator()) JumpTableARMVIXL(switch_instr));
+    ArenaAllocator* allocator = GetGraph()->GetAllocator();
+    jump_tables_.emplace_back(new (allocator) JumpTableARMVIXL(switch_instr, allocator));
     return jump_tables_.back().get();
   }
   void EmitJumpTables();
diff --git a/compiler/optimizing/code_generator_riscv64.cc b/compiler/optimizing/code_generator_riscv64.cc
index c2d82b8033..ba2e307af5 100644
--- a/compiler/optimizing/code_generator_riscv64.cc
+++ b/compiler/optimizing/code_generator_riscv64.cc
@@ -313,7 +313,7 @@ class SuspendCheckSlowPathRISCV64 : public SlowPathCodeRISCV64 {
     CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
     __ Bind(GetEntryLabel());
     SaveLiveRegisters(codegen, locations);  // Only saves live vector registers for SIMD.
-    riscv64_codegen->InvokeRuntime(kQuickTestSuspend, instruction_, instruction_->GetDexPc(), this);
+    riscv64_codegen->InvokeRuntime(kQuickTestSuspend, instruction_, this);
     CheckEntrypointTypes<kQuickTestSuspend, void, void>();
     RestoreLiveRegisters(codegen, locations);  // Only restores live vector registers for SIMD.
     if (successor_ == nullptr) {
@@ -353,8 +353,7 @@ class NullCheckSlowPathRISCV64 : public SlowPathCodeRISCV64 {
       // Live registers will be restored in the catch block if caught.
       SaveLiveRegisters(codegen, instruction_->GetLocations());
     }
-    riscv64_codegen->InvokeRuntime(
-        kQuickThrowNullPointer, instruction_, instruction_->GetDexPc(), this);
+    riscv64_codegen->InvokeRuntime(kQuickThrowNullPointer, instruction_, this);
     CheckEntrypointTypes<kQuickThrowNullPointer, void, void>();
   }
 
@@ -391,7 +390,7 @@ class BoundsCheckSlowPathRISCV64 : public SlowPathCodeRISCV64 {
     QuickEntrypointEnum entrypoint = instruction_->AsBoundsCheck()->IsStringCharAt() ?
                                          kQuickThrowStringBounds :
                                          kQuickThrowArrayBounds;
-    riscv64_codegen->InvokeRuntime(entrypoint, instruction_, instruction_->GetDexPc(), this);
+    riscv64_codegen->InvokeRuntime(entrypoint, instruction_, this);
     CheckEntrypointTypes<kQuickThrowStringBounds, void, int32_t, int32_t>();
     CheckEntrypointTypes<kQuickThrowArrayBounds, void, int32_t, int32_t>();
   }
@@ -414,7 +413,6 @@ class LoadClassSlowPathRISCV64 : public SlowPathCodeRISCV64 {
   void EmitNativeCode(CodeGenerator* codegen) override {
     LocationSummary* locations = instruction_->GetLocations();
     Location out = locations->Out();
-    const uint32_t dex_pc = instruction_->GetDexPc();
     bool must_resolve_type = instruction_->IsLoadClass() && cls_->MustResolveTypeOnSlowPath();
     bool must_do_clinit = instruction_->IsClinitCheck() || cls_->MustGenerateClinitCheck();
 
@@ -432,11 +430,10 @@ class LoadClassSlowPathRISCV64 : public SlowPathCodeRISCV64 {
       __ LoadConst32(calling_convention.GetRegisterAt(0), type_index.index_);
       if (cls_->NeedsAccessCheck()) {
         CheckEntrypointTypes<kQuickResolveTypeAndVerifyAccess, void*, uint32_t>();
-        riscv64_codegen->InvokeRuntime(
-            kQuickResolveTypeAndVerifyAccess, instruction_, dex_pc, this);
+        riscv64_codegen->InvokeRuntime(kQuickResolveTypeAndVerifyAccess, instruction_, this);
       } else {
         CheckEntrypointTypes<kQuickResolveType, void*, uint32_t>();
-        riscv64_codegen->InvokeRuntime(kQuickResolveType, instruction_, dex_pc, this);
+        riscv64_codegen->InvokeRuntime(kQuickResolveType, instruction_, this);
       }
       // If we also must_do_clinit, the resolved type is now in the correct register.
     } else {
@@ -446,7 +443,7 @@ class LoadClassSlowPathRISCV64 : public SlowPathCodeRISCV64 {
           Location::RegisterLocation(calling_convention.GetRegisterAt(0)), source, cls_->GetType());
     }
     if (must_do_clinit) {
-      riscv64_codegen->InvokeRuntime(kQuickInitializeStaticStorage, instruction_, dex_pc, this);
+      riscv64_codegen->InvokeRuntime(kQuickInitializeStaticStorage, instruction_, this);
       CheckEntrypointTypes<kQuickInitializeStaticStorage, void*, mirror::Class*>();
     }
 
@@ -484,7 +481,7 @@ class DeoptimizationSlowPathRISCV64 : public SlowPathCodeRISCV64 {
     InvokeRuntimeCallingConvention calling_convention;
     __ LoadConst32(calling_convention.GetRegisterAt(0),
                    static_cast<uint32_t>(instruction_->AsDeoptimize()->GetDeoptimizationKind()));
-    riscv64_codegen->InvokeRuntime(kQuickDeoptimize, instruction_, instruction_->GetDexPc(), this);
+    riscv64_codegen->InvokeRuntime(kQuickDeoptimize, instruction_, this);
     CheckEntrypointTypes<kQuickDeoptimize, void, DeoptimizationKind>();
   }
 
@@ -522,10 +519,7 @@ class ReadBarrierForRootSlowPathRISCV64 : public SlowPathCodeRISCV64 {
     riscv64_codegen->MoveLocation(Location::RegisterLocation(calling_convention.GetRegisterAt(0)),
                                   root_,
                                   DataType::Type::kReference);
-    riscv64_codegen->InvokeRuntime(kQuickReadBarrierForRootSlow,
-                                   instruction_,
-                                   instruction_->GetDexPc(),
-                                   this);
+    riscv64_codegen->InvokeRuntime(kQuickReadBarrierForRootSlow, instruction_, this);
     CheckEntrypointTypes<kQuickReadBarrierForRootSlow, mirror::Object*, GcRoot<mirror::Object>*>();
     riscv64_codegen->MoveLocation(out_, calling_convention.GetReturnLocation(type), type);
 
@@ -557,7 +551,7 @@ class MethodEntryExitHooksSlowPathRISCV64 : public SlowPathCodeRISCV64 {
     if (instruction_->IsMethodExitHook()) {
       __ Li(A4, riscv64_codegen->GetFrameSize());
     }
-    riscv64_codegen->InvokeRuntime(entry_point, instruction_, instruction_->GetDexPc(), this);
+    riscv64_codegen->InvokeRuntime(entry_point, instruction_, this);
     RestoreLiveRegisters(codegen, locations);
     __ J(GetExitLabel());
   }
@@ -599,7 +593,7 @@ class ArraySetSlowPathRISCV64 : public SlowPathCodeRISCV64 {
     codegen->GetMoveResolver()->EmitNativeCode(&parallel_move);
 
     CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
-    riscv64_codegen->InvokeRuntime(kQuickAputObject, instruction_, instruction_->GetDexPc(), this);
+    riscv64_codegen->InvokeRuntime(kQuickAputObject, instruction_, this);
     CheckEntrypointTypes<kQuickAputObject, void, mirror::Array*, int32_t, mirror::Object*>();
     RestoreLiveRegisters(codegen, locations);
     __ J(GetExitLabel());
@@ -619,7 +613,6 @@ class TypeCheckSlowPathRISCV64 : public SlowPathCodeRISCV64 {
   void EmitNativeCode(CodeGenerator* codegen) override {
     LocationSummary* locations = instruction_->GetLocations();
 
-    uint32_t dex_pc = instruction_->GetDexPc();
     DCHECK(instruction_->IsCheckCast()
            || !locations->GetLiveRegisters()->ContainsCoreRegister(locations->Out().reg()));
     CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
@@ -639,14 +632,14 @@ class TypeCheckSlowPathRISCV64 : public SlowPathCodeRISCV64 {
                                Location::RegisterLocation(calling_convention.GetRegisterAt(1)),
                                DataType::Type::kReference);
     if (instruction_->IsInstanceOf()) {
-      riscv64_codegen->InvokeRuntime(kQuickInstanceofNonTrivial, instruction_, dex_pc, this);
+      riscv64_codegen->InvokeRuntime(kQuickInstanceofNonTrivial, instruction_, this);
       CheckEntrypointTypes<kQuickInstanceofNonTrivial, size_t, mirror::Object*, mirror::Class*>();
       DataType::Type ret_type = instruction_->GetType();
       Location ret_loc = calling_convention.GetReturnLocation(ret_type);
       riscv64_codegen->MoveLocation(locations->Out(), ret_loc, ret_type);
     } else {
       DCHECK(instruction_->IsCheckCast());
-      riscv64_codegen->InvokeRuntime(kQuickCheckInstanceOf, instruction_, dex_pc, this);
+      riscv64_codegen->InvokeRuntime(kQuickCheckInstanceOf, instruction_, this);
       CheckEntrypointTypes<kQuickCheckInstanceOf, void, mirror::Object*, mirror::Class*>();
     }
 
@@ -674,8 +667,7 @@ class DivZeroCheckSlowPathRISCV64 : public SlowPathCodeRISCV64 {
   void EmitNativeCode(CodeGenerator* codegen) override {
     CodeGeneratorRISCV64* riscv64_codegen = down_cast<CodeGeneratorRISCV64*>(codegen);
     __ Bind(GetEntryLabel());
-    riscv64_codegen->InvokeRuntime(
-        kQuickThrowDivZero, instruction_, instruction_->GetDexPc(), this);
+    riscv64_codegen->InvokeRuntime(kQuickThrowDivZero, instruction_, this);
     CheckEntrypointTypes<kQuickThrowDivZero, void, void>();
   }
 
@@ -768,8 +760,7 @@ class LoadStringSlowPathRISCV64 : public SlowPathCodeRISCV64 {
     SaveLiveRegisters(codegen, locations);
 
     __ LoadConst32(calling_convention.GetRegisterAt(0), string_index.index_);
-    riscv64_codegen->InvokeRuntime(
-        kQuickResolveString, instruction_, instruction_->GetDexPc(), this);
+    riscv64_codegen->InvokeRuntime(kQuickResolveString, instruction_, this);
     CheckEntrypointTypes<kQuickResolveString, void*, uint32_t>();
 
     DataType::Type type = DataType::Type::kReference;
@@ -4207,7 +4198,7 @@ void InstructionCodeGeneratorRISCV64::VisitInvokeInterface(HInvokeInterface* ins
   // RA();
   __ Jalr(RA);
   DCHECK(!codegen_->IsLeafMethod());
-  codegen_->RecordPcInfo(instruction, instruction->GetDexPc());
+  codegen_->RecordPcInfo(instruction);
 }
 
 void LocationsBuilderRISCV64::VisitInvokeStaticOrDirect(HInvokeStaticOrDirect* instruction) {
@@ -4586,7 +4577,7 @@ void InstructionCodeGeneratorRISCV64::VisitLoadString(HLoadString* instruction)
   InvokeRuntimeCallingConvention calling_convention;
   DCHECK(calling_convention.GetReturnLocation(DataType::Type::kReference).Equals(out_loc));
   __ LoadConst32(calling_convention.GetRegisterAt(0), instruction->GetStringIndex().index_);
-  codegen_->InvokeRuntime(kQuickResolveString, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(kQuickResolveString, instruction);
   CheckEntrypointTypes<kQuickResolveString, void*, uint32_t>();
 }
 
@@ -4656,8 +4647,7 @@ void LocationsBuilderRISCV64::VisitMonitorOperation(HMonitorOperation* instructi
 
 void InstructionCodeGeneratorRISCV64::VisitMonitorOperation(HMonitorOperation* instruction) {
   codegen_->InvokeRuntime(instruction->IsEnter() ? kQuickLockObject : kQuickUnlockObject,
-                          instruction,
-                          instruction->GetDexPc());
+                          instruction);
   if (instruction->IsEnter()) {
     CheckEntrypointTypes<kQuickLockObject, void, mirror::Object*>();
   } else {
@@ -4772,7 +4762,7 @@ void LocationsBuilderRISCV64::VisitNewArray(HNewArray* instruction) {
 
 void InstructionCodeGeneratorRISCV64::VisitNewArray(HNewArray* instruction) {
   QuickEntrypointEnum entrypoint = CodeGenerator::GetArrayAllocationEntrypoint(instruction);
-  codegen_->InvokeRuntime(entrypoint, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(entrypoint, instruction);
   CheckEntrypointTypes<kQuickAllocArrayResolved, void*, mirror::Class*, int32_t>();
   DCHECK(!codegen_->IsLeafMethod());
 }
@@ -4786,7 +4776,7 @@ void LocationsBuilderRISCV64::VisitNewInstance(HNewInstance* instruction) {
 }
 
 void InstructionCodeGeneratorRISCV64::VisitNewInstance(HNewInstance* instruction) {
-  codegen_->InvokeRuntime(instruction->GetEntrypoint(), instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(instruction->GetEntrypoint(), instruction);
   CheckEntrypointTypes<kQuickAllocObjectWithChecks, void*, mirror::Class*>();
 }
 
@@ -4981,7 +4971,7 @@ void InstructionCodeGeneratorRISCV64::VisitRem(HRem* instruction) {
     case DataType::Type::kFloat64: {
       QuickEntrypointEnum entrypoint =
           (type == DataType::Type::kFloat32) ? kQuickFmodf : kQuickFmod;
-      codegen_->InvokeRuntime(entrypoint, instruction, instruction->GetDexPc());
+      codegen_->InvokeRuntime(entrypoint, instruction);
       if (type == DataType::Type::kFloat32) {
         CheckEntrypointTypes<kQuickFmodf, float, float, float>();
       } else {
@@ -5079,7 +5069,7 @@ void LocationsBuilderRISCV64::VisitStringBuilderAppend(HStringBuilderAppend* ins
 
 void InstructionCodeGeneratorRISCV64::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
   __ LoadConst32(A0, instruction->GetFormat()->GetValue());
-  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction);
 }
 
 void LocationsBuilderRISCV64::VisitUnresolvedInstanceFieldGet(
@@ -5095,7 +5085,6 @@ void InstructionCodeGeneratorRISCV64::VisitUnresolvedInstanceFieldGet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -5112,7 +5101,6 @@ void InstructionCodeGeneratorRISCV64::VisitUnresolvedInstanceFieldSet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -5129,7 +5117,6 @@ void InstructionCodeGeneratorRISCV64::VisitUnresolvedStaticFieldGet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -5146,7 +5133,6 @@ void InstructionCodeGeneratorRISCV64::VisitUnresolvedStaticFieldSet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -5288,7 +5274,7 @@ void LocationsBuilderRISCV64::VisitThrow(HThrow* instruction) {
 }
 
 void InstructionCodeGeneratorRISCV64::VisitThrow(HThrow* instruction) {
-  codegen_->InvokeRuntime(kQuickDeliverException, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(kQuickDeliverException, instruction);
   CheckEntrypointTypes<kQuickDeliverException, void, mirror::Object*>();
 }
 
@@ -6137,7 +6123,7 @@ void CodeGeneratorRISCV64::GenerateFrameEntry() {
     DCHECK(GetCompilerOptions().GetImplicitStackOverflowChecks());
     __ Loadw(
         Zero, SP, -static_cast<int32_t>(GetStackOverflowReservedBytes(InstructionSet::kRiscv64)));
-    RecordPcInfo(nullptr, 0);
+    RecordPcInfoForFrameOrBlockEntry();
   }
 
   if (!HasEmptyFrame()) {
@@ -6511,7 +6497,6 @@ void CodeGeneratorRISCV64::Finalize() {
 // Generate code to invoke a runtime entry point.
 void CodeGeneratorRISCV64::InvokeRuntime(QuickEntrypointEnum entrypoint,
                                          HInstruction* instruction,
-                                         uint32_t dex_pc,
                                          SlowPathCode* slow_path) {
   ValidateInvokeRuntime(entrypoint, instruction, slow_path);
 
@@ -6522,7 +6507,7 @@ void CodeGeneratorRISCV64::InvokeRuntime(QuickEntrypointEnum entrypoint,
   __ Loadd(RA, TR, entrypoint_offset.Int32Value());
   __ Jalr(RA);
   if (EntrypointRequiresStackMap(entrypoint)) {
-    RecordPcInfo(instruction, dex_pc, slow_path);
+    RecordPcInfo(instruction, slow_path);
   }
 }
 
@@ -6559,7 +6544,7 @@ void CodeGeneratorRISCV64::GenerateImplicitNullCheck(HNullCheck* instruction) {
   Location obj = instruction->GetLocations()->InAt(0);
 
   __ Lw(Zero, obj.AsRegister<XRegister>(), 0);
-  RecordPcInfo(instruction, instruction->GetDexPc());
+  RecordPcInfo(instruction);
 }
 
 void CodeGeneratorRISCV64::GenerateExplicitNullCheck(HNullCheck* instruction) {
@@ -7035,7 +7020,7 @@ void CodeGeneratorRISCV64::GenerateStaticOrDirectCall(HInvokeStaticOrDirect* inv
     case CodePtrLocation::kCallSelf:
       DCHECK(!GetGraph()->HasShouldDeoptimizeFlag());
       __ Jal(&frame_entry_label_);
-      RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+      RecordPcInfo(invoke, slow_path);
       break;
     case CodePtrLocation::kCallArtMethod:
       // RA = callee_method->entry_point_from_quick_compiled_code_;
@@ -7044,7 +7029,7 @@ void CodeGeneratorRISCV64::GenerateStaticOrDirectCall(HInvokeStaticOrDirect* inv
                ArtMethod::EntryPointFromQuickCompiledCodeOffset(kRiscv64PointerSize).Int32Value());
       // RA()
       __ Jalr(RA);
-      RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+      RecordPcInfo(invoke, slow_path);
       break;
     case CodePtrLocation::kCallCriticalNative: {
       size_t out_frame_size =
@@ -7059,7 +7044,7 @@ void CodeGeneratorRISCV64::GenerateStaticOrDirectCall(HInvokeStaticOrDirect* inv
         __ Loadd(RA, callee_method.AsRegister<XRegister>(), offset.Int32Value());
       }
       __ Jalr(RA);
-      RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+      RecordPcInfo(invoke, slow_path);
       // The result is returned the same way in native ABI and managed ABI. No result conversion is
       // needed, see comments in `Riscv64JniCallingConvention::RequiresSmallResultTypeExtension()`.
       if (out_frame_size != 0u) {
@@ -7096,7 +7081,7 @@ void CodeGeneratorRISCV64::MaybeGenerateInlineCacheCheck(HInstruction* instructi
         // Fast path for a monomorphic cache.
         __ Beq(klass, tmp, &done);
       }
-      InvokeRuntime(kQuickUpdateInlineCache, instruction, instruction->GetDexPc());
+      InvokeRuntime(kQuickUpdateInlineCache, instruction);
       __ Bind(&done);
     } else {
       // This is unexpected, but we don't guarantee stable compilation across
@@ -7143,7 +7128,7 @@ void CodeGeneratorRISCV64::GenerateVirtualCall(HInvokeVirtual* invoke,
   __ Loadd(RA, temp, entry_point.Int32Value());
   // RA();
   __ Jalr(RA);
-  RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+  RecordPcInfo(invoke, slow_path);
 }
 
 void CodeGeneratorRISCV64::MoveFromReturnRegister(Location trg, DataType::Type type) {
diff --git a/compiler/optimizing/code_generator_riscv64.h b/compiler/optimizing/code_generator_riscv64.h
index 588243e86d..dc88296be2 100644
--- a/compiler/optimizing/code_generator_riscv64.h
+++ b/compiler/optimizing/code_generator_riscv64.h
@@ -88,7 +88,6 @@ static constexpr int32_t kFClassNaNMinValue = 0x100;
   V(CRC32Update)                                \
   V(CRC32UpdateBytes)                           \
   V(CRC32UpdateByteBuffer)                      \
-  V(MethodHandleInvokeExact)                    \
   V(MethodHandleInvoke)                         \
   V(UnsafeArrayBaseOffset)                      \
   V(JdkUnsafeArrayBaseOffset)                   \
@@ -505,7 +504,6 @@ class CodeGeneratorRISCV64 : public CodeGenerator {
   // Generate code to invoke a runtime entry point.
   void InvokeRuntime(QuickEntrypointEnum entrypoint,
                      HInstruction* instruction,
-                     uint32_t dex_pc,
                      SlowPathCode* slow_path = nullptr) override;
 
   // Generate code to invoke a runtime entry point, but do not record
diff --git a/compiler/optimizing/code_generator_vector_x86_64.cc b/compiler/optimizing/code_generator_vector_x86_64.cc
index 1ecd5f063f..76bd5faab3 100644
--- a/compiler/optimizing/code_generator_vector_x86_64.cc
+++ b/compiler/optimizing/code_generator_vector_x86_64.cc
@@ -70,7 +70,7 @@ void InstructionCodeGeneratorX86_64::VisitVecReplicateScalar(HVecReplicateScalar
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
       DCHECK_EQ(16u, instruction->GetVectorLength());
-      __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>(), /*64-bit*/ false);
+      __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>());
       __ punpcklbw(dst, dst);
       __ punpcklwd(dst, dst);
       __ pshufd(dst, dst, Immediate(0));
@@ -78,18 +78,18 @@ void InstructionCodeGeneratorX86_64::VisitVecReplicateScalar(HVecReplicateScalar
     case DataType::Type::kUint16:
     case DataType::Type::kInt16:
       DCHECK_EQ(8u, instruction->GetVectorLength());
-      __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>(), /*64-bit*/ false);
+      __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>());
       __ punpcklwd(dst, dst);
       __ pshufd(dst, dst, Immediate(0));
       break;
     case DataType::Type::kInt32:
       DCHECK_EQ(4u, instruction->GetVectorLength());
-      __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>(), /*64-bit*/ false);
+      __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>());
       __ pshufd(dst, dst, Immediate(0));
       break;
     case DataType::Type::kInt64:
       DCHECK_EQ(2u, instruction->GetVectorLength());
-      __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>(), /*64-bit*/ true);
+      __ movq(dst, locations->InAt(0).AsRegister<CpuRegister>());
       __ punpcklqdq(dst, dst);
       break;
     case DataType::Type::kFloat32:
@@ -145,11 +145,11 @@ void InstructionCodeGeneratorX86_64::VisitVecExtractScalar(HVecExtractScalar* in
       UNREACHABLE();
     case DataType::Type::kInt32:
       DCHECK_EQ(4u, instruction->GetVectorLength());
-      __ movd(locations->Out().AsRegister<CpuRegister>(), src, /*64-bit*/ false);
+      __ movd(locations->Out().AsRegister<CpuRegister>(), src);
       break;
     case DataType::Type::kInt64:
       DCHECK_EQ(2u, instruction->GetVectorLength());
-      __ movd(locations->Out().AsRegister<CpuRegister>(), src, /*64-bit*/ true);
+      __ movq(locations->Out().AsRegister<CpuRegister>(), src);
       break;
     case DataType::Type::kFloat32:
     case DataType::Type::kFloat64:
@@ -1118,7 +1118,7 @@ void InstructionCodeGeneratorX86_64::VisitVecSetScalars(HVecSetScalars* instruct
       break;
     case DataType::Type::kInt64:
       DCHECK_EQ(2u, instruction->GetVectorLength());
-      __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>());  // is 64-bit
+      __ movq(dst, locations->InAt(0).AsRegister<CpuRegister>());
       break;
     case DataType::Type::kFloat32:
       DCHECK_EQ(4u, instruction->GetVectorLength());
diff --git a/compiler/optimizing/code_generator_x86.cc b/compiler/optimizing/code_generator_x86.cc
index 6db49c7771..ae31e9f198 100644
--- a/compiler/optimizing/code_generator_x86.cc
+++ b/compiler/optimizing/code_generator_x86.cc
@@ -89,10 +89,7 @@ class NullCheckSlowPathX86 : public SlowPathCode {
       // Live registers will be restored in the catch block if caught.
       SaveLiveRegisters(codegen, instruction_->GetLocations());
     }
-    x86_codegen->InvokeRuntime(kQuickThrowNullPointer,
-                               instruction_,
-                               instruction_->GetDexPc(),
-                               this);
+    x86_codegen->InvokeRuntime(kQuickThrowNullPointer, instruction_, this);
     CheckEntrypointTypes<kQuickThrowNullPointer, void, void>();
   }
 
@@ -111,7 +108,7 @@ class DivZeroCheckSlowPathX86 : public SlowPathCode {
   void EmitNativeCode(CodeGenerator* codegen) override {
     CodeGeneratorX86* x86_codegen = down_cast<CodeGeneratorX86*>(codegen);
     __ Bind(GetEntryLabel());
-    x86_codegen->InvokeRuntime(kQuickThrowDivZero, instruction_, instruction_->GetDexPc(), this);
+    x86_codegen->InvokeRuntime(kQuickThrowDivZero, instruction_, this);
     CheckEntrypointTypes<kQuickThrowDivZero, void, void>();
   }
 
@@ -213,7 +210,7 @@ class BoundsCheckSlowPathX86 : public SlowPathCode {
     QuickEntrypointEnum entrypoint = instruction_->AsBoundsCheck()->IsStringCharAt()
         ? kQuickThrowStringBounds
         : kQuickThrowArrayBounds;
-    x86_codegen->InvokeRuntime(entrypoint, instruction_, instruction_->GetDexPc(), this);
+    x86_codegen->InvokeRuntime(entrypoint, instruction_, this);
     CheckEntrypointTypes<kQuickThrowStringBounds, void, int32_t, int32_t>();
     CheckEntrypointTypes<kQuickThrowArrayBounds, void, int32_t, int32_t>();
   }
@@ -236,7 +233,7 @@ class SuspendCheckSlowPathX86 : public SlowPathCode {
     CodeGeneratorX86* x86_codegen = down_cast<CodeGeneratorX86*>(codegen);
     __ Bind(GetEntryLabel());
     SaveLiveRegisters(codegen, locations);  // Only saves full width XMM for SIMD.
-    x86_codegen->InvokeRuntime(kQuickTestSuspend, instruction_, instruction_->GetDexPc(), this);
+    x86_codegen->InvokeRuntime(kQuickTestSuspend, instruction_, this);
     CheckEntrypointTypes<kQuickTestSuspend, void, void>();
     RestoreLiveRegisters(codegen, locations);  // Only restores full width XMM for SIMD.
     if (successor_ == nullptr) {
@@ -279,7 +276,7 @@ class LoadStringSlowPathX86 : public SlowPathCode {
     InvokeRuntimeCallingConvention calling_convention;
     const dex::StringIndex string_index = instruction_->AsLoadString()->GetStringIndex();
     __ movl(calling_convention.GetRegisterAt(0), Immediate(string_index.index_));
-    x86_codegen->InvokeRuntime(kQuickResolveString, instruction_, instruction_->GetDexPc(), this);
+    x86_codegen->InvokeRuntime(kQuickResolveString, instruction_, this);
     CheckEntrypointTypes<kQuickResolveString, void*, uint32_t>();
     x86_codegen->Move32(locations->Out(), Location::RegisterLocation(EAX));
     RestoreLiveRegisters(codegen, locations);
@@ -304,7 +301,6 @@ class LoadClassSlowPathX86 : public SlowPathCode {
   void EmitNativeCode(CodeGenerator* codegen) override {
     LocationSummary* locations = instruction_->GetLocations();
     Location out = locations->Out();
-    const uint32_t dex_pc = instruction_->GetDexPc();
     bool must_resolve_type = instruction_->IsLoadClass() && cls_->MustResolveTypeOnSlowPath();
     bool must_do_clinit = instruction_->IsClinitCheck() || cls_->MustGenerateClinitCheck();
 
@@ -322,10 +318,10 @@ class LoadClassSlowPathX86 : public SlowPathCode {
       __ movl(calling_convention.GetRegisterAt(0), Immediate(type_index.index_));
       if (cls_->NeedsAccessCheck()) {
         CheckEntrypointTypes<kQuickResolveTypeAndVerifyAccess, void*, uint32_t>();
-        x86_codegen->InvokeRuntime(kQuickResolveTypeAndVerifyAccess, instruction_, dex_pc, this);
+        x86_codegen->InvokeRuntime(kQuickResolveTypeAndVerifyAccess, instruction_, this);
       } else {
         CheckEntrypointTypes<kQuickResolveType, void*, uint32_t>();
-        x86_codegen->InvokeRuntime(kQuickResolveType, instruction_, dex_pc, this);
+        x86_codegen->InvokeRuntime(kQuickResolveType, instruction_, this);
       }
       // If we also must_do_clinit, the resolved type is now in the correct register.
     } else {
@@ -334,7 +330,7 @@ class LoadClassSlowPathX86 : public SlowPathCode {
       x86_codegen->Move32(Location::RegisterLocation(calling_convention.GetRegisterAt(0)), source);
     }
     if (must_do_clinit) {
-      x86_codegen->InvokeRuntime(kQuickInitializeStaticStorage, instruction_, dex_pc, this);
+      x86_codegen->InvokeRuntime(kQuickInitializeStaticStorage, instruction_, this);
       CheckEntrypointTypes<kQuickInitializeStaticStorage, void*, mirror::Class*>();
     }
 
@@ -390,17 +386,11 @@ class TypeCheckSlowPathX86 : public SlowPathCode {
                                    Location::RegisterLocation(calling_convention.GetRegisterAt(1)),
                                    DataType::Type::kReference);
     if (instruction_->IsInstanceOf()) {
-      x86_codegen->InvokeRuntime(kQuickInstanceofNonTrivial,
-                                 instruction_,
-                                 instruction_->GetDexPc(),
-                                 this);
+      x86_codegen->InvokeRuntime(kQuickInstanceofNonTrivial, instruction_, this);
       CheckEntrypointTypes<kQuickInstanceofNonTrivial, size_t, mirror::Object*, mirror::Class*>();
     } else {
       DCHECK(instruction_->IsCheckCast());
-      x86_codegen->InvokeRuntime(kQuickCheckInstanceOf,
-                                 instruction_,
-                                 instruction_->GetDexPc(),
-                                 this);
+      x86_codegen->InvokeRuntime(kQuickCheckInstanceOf, instruction_, this);
       CheckEntrypointTypes<kQuickCheckInstanceOf, void, mirror::Object*, mirror::Class*>();
     }
 
@@ -437,7 +427,7 @@ class DeoptimizationSlowPathX86 : public SlowPathCode {
     x86_codegen->Load32BitValue(
         calling_convention.GetRegisterAt(0),
         static_cast<uint32_t>(instruction_->AsDeoptimize()->GetDeoptimizationKind()));
-    x86_codegen->InvokeRuntime(kQuickDeoptimize, instruction_, instruction_->GetDexPc(), this);
+    x86_codegen->InvokeRuntime(kQuickDeoptimize, instruction_, this);
     CheckEntrypointTypes<kQuickDeoptimize, void, DeoptimizationKind>();
   }
 
@@ -476,7 +466,7 @@ class ArraySetSlowPathX86 : public SlowPathCode {
     codegen->GetMoveResolver()->EmitNativeCode(&parallel_move);
 
     CodeGeneratorX86* x86_codegen = down_cast<CodeGeneratorX86*>(codegen);
-    x86_codegen->InvokeRuntime(kQuickAputObject, instruction_, instruction_->GetDexPc(), this);
+    x86_codegen->InvokeRuntime(kQuickAputObject, instruction_, this);
     CheckEntrypointTypes<kQuickAputObject, void, mirror::Array*, int32_t, mirror::Object*>();
     RestoreLiveRegisters(codegen, locations);
     __ jmp(GetExitLabel());
@@ -878,7 +868,7 @@ class ReadBarrierForHeapReferenceSlowPathX86 : public SlowPathCode {
       codegen->GetMoveResolver()->EmitNativeCode(&parallel_move);
       __ movl(calling_convention.GetRegisterAt(2), Immediate(offset_));
     }
-    x86_codegen->InvokeRuntime(kQuickReadBarrierSlow, instruction_, instruction_->GetDexPc(), this);
+    x86_codegen->InvokeRuntime(kQuickReadBarrierSlow, instruction_, this);
     CheckEntrypointTypes<
         kQuickReadBarrierSlow, mirror::Object*, mirror::Object*, mirror::Object*, uint32_t>();
     x86_codegen->Move32(out_, Location::RegisterLocation(EAX));
@@ -942,10 +932,7 @@ class ReadBarrierForRootSlowPathX86 : public SlowPathCode {
     InvokeRuntimeCallingConvention calling_convention;
     CodeGeneratorX86* x86_codegen = down_cast<CodeGeneratorX86*>(codegen);
     x86_codegen->Move32(Location::RegisterLocation(calling_convention.GetRegisterAt(0)), root_);
-    x86_codegen->InvokeRuntime(kQuickReadBarrierForRootSlow,
-                               instruction_,
-                               instruction_->GetDexPc(),
-                               this);
+    x86_codegen->InvokeRuntime(kQuickReadBarrierForRootSlow, instruction_, this);
     CheckEntrypointTypes<kQuickReadBarrierForRootSlow, mirror::Object*, GcRoot<mirror::Object>*>();
     x86_codegen->Move32(out_, Location::RegisterLocation(EAX));
 
@@ -976,7 +963,7 @@ class MethodEntryExitHooksSlowPathX86 : public SlowPathCode {
     if (instruction_->IsMethodExitHook()) {
       __ movl(EBX, Immediate(codegen->GetFrameSize()));
     }
-    x86_codegen->InvokeRuntime(entry_point, instruction_, instruction_->GetDexPc(), this);
+    x86_codegen->InvokeRuntime(entry_point, instruction_, this);
     RestoreLiveRegisters(codegen, locations);
     __ jmp(GetExitLabel());
   }
@@ -1105,12 +1092,11 @@ size_t CodeGeneratorX86::RestoreFloatingPointRegister(size_t stack_index, uint32
 
 void CodeGeneratorX86::InvokeRuntime(QuickEntrypointEnum entrypoint,
                                      HInstruction* instruction,
-                                     uint32_t dex_pc,
                                      SlowPathCode* slow_path) {
   ValidateInvokeRuntime(entrypoint, instruction, slow_path);
   GenerateInvokeRuntime(GetThreadOffset<kX86PointerSize>(entrypoint).Int32Value());
   if (EntrypointRequiresStackMap(entrypoint)) {
-    RecordPcInfo(instruction, dex_pc, slow_path);
+    RecordPcInfo(instruction, slow_path);
   }
 }
 
@@ -1429,7 +1415,7 @@ void CodeGeneratorX86::GenerateFrameEntry() {
   if (!skip_overflow_check) {
     size_t reserved_bytes = GetStackOverflowReservedBytes(InstructionSet::kX86);
     __ testl(EAX, Address(ESP, -static_cast<int32_t>(reserved_bytes)));
-    RecordPcInfo(nullptr, 0);
+    RecordPcInfoForFrameOrBlockEntry();
   }
 
   if (!HasEmptyFrame()) {
@@ -2944,7 +2930,7 @@ void InstructionCodeGeneratorX86::VisitInvokeInterface(HInvokeInterface* invoke)
                   ArtMethod::EntryPointFromQuickCompiledCodeOffset(kX86PointerSize).Int32Value()));
 
   DCHECK(!codegen_->IsLeafMethod());
-  codegen_->RecordPcInfo(invoke, invoke->GetDexPc());
+  codegen_->RecordPcInfo(invoke);
 }
 
 void LocationsBuilderX86::VisitInvokePolymorphic(HInvokePolymorphic* invoke) {
@@ -3470,12 +3456,12 @@ void InstructionCodeGeneratorX86::VisitTypeConversion(HTypeConversion* conversio
           break;
 
         case DataType::Type::kFloat32:
-          codegen_->InvokeRuntime(kQuickF2l, conversion, conversion->GetDexPc());
+          codegen_->InvokeRuntime(kQuickF2l, conversion);
           CheckEntrypointTypes<kQuickF2l, int64_t, float>();
           break;
 
         case DataType::Type::kFloat64:
-          codegen_->InvokeRuntime(kQuickD2l, conversion, conversion->GetDexPc());
+          codegen_->InvokeRuntime(kQuickD2l, conversion);
           CheckEntrypointTypes<kQuickD2l, int64_t, double>();
           break;
 
@@ -4309,10 +4295,10 @@ void InstructionCodeGeneratorX86::GenerateDivRemIntegral(HBinaryOperation* instr
       DCHECK_EQ(EDX, out.AsRegisterPairHigh<Register>());
 
       if (is_div) {
-        codegen_->InvokeRuntime(kQuickLdiv, instruction, instruction->GetDexPc());
+        codegen_->InvokeRuntime(kQuickLdiv, instruction);
         CheckEntrypointTypes<kQuickLdiv, int64_t, int64_t, int64_t>();
       } else {
-        codegen_->InvokeRuntime(kQuickLmod, instruction, instruction->GetDexPc());
+        codegen_->InvokeRuntime(kQuickLmod, instruction);
         CheckEntrypointTypes<kQuickLmod, int64_t, int64_t, int64_t>();
       }
       break;
@@ -5208,7 +5194,7 @@ void LocationsBuilderX86::VisitNewInstance(HNewInstance* instruction) {
 }
 
 void InstructionCodeGeneratorX86::VisitNewInstance(HNewInstance* instruction) {
-  codegen_->InvokeRuntime(instruction->GetEntrypoint(), instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(instruction->GetEntrypoint(), instruction);
   CheckEntrypointTypes<kQuickAllocObjectWithChecks, void*, mirror::Class*>();
   DCHECK(!codegen_->IsLeafMethod());
 }
@@ -5225,7 +5211,7 @@ void LocationsBuilderX86::VisitNewArray(HNewArray* instruction) {
 void InstructionCodeGeneratorX86::VisitNewArray(HNewArray* instruction) {
   // Note: if heap poisoning is enabled, the entry point takes care of poisoning the reference.
   QuickEntrypointEnum entrypoint = CodeGenerator::GetArrayAllocationEntrypoint(instruction);
-  codegen_->InvokeRuntime(entrypoint, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(entrypoint, instruction);
   CheckEntrypointTypes<kQuickAllocArrayResolved, void*, mirror::Class*, int32_t>();
   DCHECK(!codegen_->IsLeafMethod());
 }
@@ -5616,7 +5602,7 @@ void CodeGeneratorX86::GenerateStaticOrDirectCall(
     case CodePtrLocation::kCallSelf:
       DCHECK(!GetGraph()->HasShouldDeoptimizeFlag());
       __ call(GetFrameEntryLabel());
-      RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+      RecordPcInfo(invoke, slow_path);
       break;
     case CodePtrLocation::kCallCriticalNative: {
       size_t out_frame_size =
@@ -5633,7 +5619,7 @@ void CodeGeneratorX86::GenerateStaticOrDirectCall(
         __ call(Address(callee_method.AsRegister<Register>(),
                         ArtMethod::EntryPointFromJniOffset(kX86PointerSize).Int32Value()));
       }
-      RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+      RecordPcInfo(invoke, slow_path);
       if (out_frame_size == 0u && DataType::IsFloatingPointType(invoke->GetType())) {
         // Create space for conversion.
         out_frame_size = 8u;
@@ -5679,7 +5665,7 @@ void CodeGeneratorX86::GenerateStaticOrDirectCall(
       __ call(Address(callee_method.AsRegister<Register>(),
                       ArtMethod::EntryPointFromQuickCompiledCodeOffset(
                           kX86PointerSize).Int32Value()));
-      RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+      RecordPcInfo(invoke, slow_path);
       break;
   }
 
@@ -5718,7 +5704,7 @@ void CodeGeneratorX86::GenerateVirtualCall(
   // call temp->GetEntryPoint();
   __ call(Address(
       temp, ArtMethod::EntryPointFromQuickCompiledCodeOffset(kX86PointerSize).Int32Value()));
-  RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+  RecordPcInfo(invoke, slow_path);
 }
 
 void CodeGeneratorX86::RecordBootImageIntrinsicPatch(HX86ComputeBaseMethodAddress* method_address,
@@ -6395,7 +6381,7 @@ void LocationsBuilderX86::VisitStringBuilderAppend(HStringBuilderAppend* instruc
 
 void InstructionCodeGeneratorX86::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
   __ movl(EAX, Immediate(instruction->GetFormat()->GetValue()));
-  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction);
 }
 
 void LocationsBuilderX86::VisitUnresolvedInstanceFieldGet(
@@ -6411,7 +6397,6 @@ void InstructionCodeGeneratorX86::VisitUnresolvedInstanceFieldGet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -6428,7 +6413,6 @@ void InstructionCodeGeneratorX86::VisitUnresolvedInstanceFieldSet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -6445,7 +6429,6 @@ void InstructionCodeGeneratorX86::VisitUnresolvedStaticFieldGet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -6462,7 +6445,6 @@ void InstructionCodeGeneratorX86::VisitUnresolvedStaticFieldSet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -6482,7 +6464,7 @@ void CodeGeneratorX86::GenerateImplicitNullCheck(HNullCheck* instruction) {
   Location obj = locations->InAt(0);
 
   __ testl(EAX, Address(obj.AsRegister<Register>(), 0));
-  RecordPcInfo(instruction, instruction->GetDexPc());
+  RecordPcInfo(instruction);
 }
 
 void CodeGeneratorX86::GenerateExplicitNullCheck(HNullCheck* instruction) {
@@ -7723,7 +7705,7 @@ void InstructionCodeGeneratorX86::VisitLoadString(HLoadString* load) NO_THREAD_S
   InvokeRuntimeCallingConvention calling_convention;
   DCHECK_EQ(calling_convention.GetRegisterAt(0), out);
   __ movl(calling_convention.GetRegisterAt(0), Immediate(load->GetStringIndex().index_));
-  codegen_->InvokeRuntime(kQuickResolveString, load, load->GetDexPc());
+  codegen_->InvokeRuntime(kQuickResolveString, load);
   CheckEntrypointTypes<kQuickResolveString, void*, uint32_t>();
 }
 
@@ -7757,7 +7739,7 @@ void LocationsBuilderX86::VisitThrow(HThrow* instruction) {
 }
 
 void InstructionCodeGeneratorX86::VisitThrow(HThrow* instruction) {
-  codegen_->InvokeRuntime(kQuickDeliverException, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(kQuickDeliverException, instruction);
   CheckEntrypointTypes<kQuickDeliverException, void, mirror::Object*>();
 }
 
@@ -8390,10 +8372,8 @@ void LocationsBuilderX86::VisitMonitorOperation(HMonitorOperation* instruction)
 }
 
 void InstructionCodeGeneratorX86::VisitMonitorOperation(HMonitorOperation* instruction) {
-  codegen_->InvokeRuntime(instruction->IsEnter() ? kQuickLockObject
-                                                 : kQuickUnlockObject,
-                          instruction,
-                          instruction->GetDexPc());
+  codegen_->InvokeRuntime(instruction->IsEnter() ? kQuickLockObject : kQuickUnlockObject,
+                          instruction);
   if (instruction->IsEnter()) {
     CheckEntrypointTypes<kQuickLockObject, void, mirror::Object*>();
   } else {
diff --git a/compiler/optimizing/code_generator_x86.h b/compiler/optimizing/code_generator_x86.h
index fae6c7f801..2c145b5133 100644
--- a/compiler/optimizing/code_generator_x86.h
+++ b/compiler/optimizing/code_generator_x86.h
@@ -436,7 +436,6 @@ class CodeGeneratorX86 : public CodeGenerator {
   // Generate code to invoke a runtime entry point.
   void InvokeRuntime(QuickEntrypointEnum entrypoint,
                      HInstruction* instruction,
-                     uint32_t dex_pc,
                      SlowPathCode* slow_path = nullptr) override;
 
   // Generate code to invoke a runtime entry point, but do not record
diff --git a/compiler/optimizing/code_generator_x86_64.cc b/compiler/optimizing/code_generator_x86_64.cc
index e9593d6c98..891bdd72e8 100644
--- a/compiler/optimizing/code_generator_x86_64.cc
+++ b/compiler/optimizing/code_generator_x86_64.cc
@@ -89,10 +89,7 @@ class NullCheckSlowPathX86_64 : public SlowPathCode {
       // Live registers will be restored in the catch block if caught.
       SaveLiveRegisters(codegen, instruction_->GetLocations());
     }
-    x86_64_codegen->InvokeRuntime(kQuickThrowNullPointer,
-                                  instruction_,
-                                  instruction_->GetDexPc(),
-                                  this);
+    x86_64_codegen->InvokeRuntime(kQuickThrowNullPointer, instruction_, this);
     CheckEntrypointTypes<kQuickThrowNullPointer, void, void>();
   }
 
@@ -111,7 +108,7 @@ class DivZeroCheckSlowPathX86_64 : public SlowPathCode {
   void EmitNativeCode(CodeGenerator* codegen) override {
     CodeGeneratorX86_64* x86_64_codegen = down_cast<CodeGeneratorX86_64*>(codegen);
     __ Bind(GetEntryLabel());
-    x86_64_codegen->InvokeRuntime(kQuickThrowDivZero, instruction_, instruction_->GetDexPc(), this);
+    x86_64_codegen->InvokeRuntime(kQuickThrowDivZero, instruction_, this);
     CheckEntrypointTypes<kQuickThrowDivZero, void, void>();
   }
 
@@ -167,7 +164,7 @@ class SuspendCheckSlowPathX86_64 : public SlowPathCode {
     CodeGeneratorX86_64* x86_64_codegen = down_cast<CodeGeneratorX86_64*>(codegen);
     __ Bind(GetEntryLabel());
     SaveLiveRegisters(codegen, locations);  // Only saves full width XMM for SIMD.
-    x86_64_codegen->InvokeRuntime(kQuickTestSuspend, instruction_, instruction_->GetDexPc(), this);
+    x86_64_codegen->InvokeRuntime(kQuickTestSuspend, instruction_, this);
     CheckEntrypointTypes<kQuickTestSuspend, void, void>();
     RestoreLiveRegisters(codegen, locations);  // Only restores full width XMM for SIMD.
     if (successor_ == nullptr) {
@@ -257,7 +254,7 @@ class BoundsCheckSlowPathX86_64 : public SlowPathCode {
     QuickEntrypointEnum entrypoint = instruction_->AsBoundsCheck()->IsStringCharAt()
         ? kQuickThrowStringBounds
         : kQuickThrowArrayBounds;
-    x86_64_codegen->InvokeRuntime(entrypoint, instruction_, instruction_->GetDexPc(), this);
+    x86_64_codegen->InvokeRuntime(entrypoint, instruction_, this);
     CheckEntrypointTypes<kQuickThrowStringBounds, void, int32_t, int32_t>();
     CheckEntrypointTypes<kQuickThrowArrayBounds, void, int32_t, int32_t>();
   }
@@ -285,10 +282,7 @@ class LoadMethodTypeSlowPathX86_64: public SlowPathCode {
     const dex::ProtoIndex proto_index = instruction_->AsLoadMethodType()->GetProtoIndex();
     // Custom calling convention: RAX serves as both input and output.
     __ movl(CpuRegister(RAX), Immediate(proto_index.index_));
-    x86_64_codegen->InvokeRuntime(kQuickResolveMethodType,
-                                  instruction_,
-                                  instruction_->GetDexPc(),
-                                  this);
+    x86_64_codegen->InvokeRuntime(kQuickResolveMethodType, instruction_, this);
     CheckEntrypointTypes<kQuickResolveMethodType, void*, uint32_t>();
     x86_64_codegen->Move(locations->Out(), Location::RegisterLocation(RAX));
     RestoreLiveRegisters(codegen, locations);
@@ -313,7 +307,6 @@ class LoadClassSlowPathX86_64 : public SlowPathCode {
   void EmitNativeCode(CodeGenerator* codegen) override {
     LocationSummary* locations = instruction_->GetLocations();
     Location out = locations->Out();
-    const uint32_t dex_pc = instruction_->GetDexPc();
     bool must_resolve_type = instruction_->IsLoadClass() && cls_->MustResolveTypeOnSlowPath();
     bool must_do_clinit = instruction_->IsClinitCheck() || cls_->MustGenerateClinitCheck();
 
@@ -331,10 +324,10 @@ class LoadClassSlowPathX86_64 : public SlowPathCode {
       __ movl(CpuRegister(RAX), Immediate(type_index.index_));
       if (cls_->NeedsAccessCheck()) {
         CheckEntrypointTypes<kQuickResolveTypeAndVerifyAccess, void*, uint32_t>();
-        x86_64_codegen->InvokeRuntime(kQuickResolveTypeAndVerifyAccess, instruction_, dex_pc, this);
+        x86_64_codegen->InvokeRuntime(kQuickResolveTypeAndVerifyAccess, instruction_, this);
       } else {
         CheckEntrypointTypes<kQuickResolveType, void*, uint32_t>();
-        x86_64_codegen->InvokeRuntime(kQuickResolveType, instruction_, dex_pc, this);
+        x86_64_codegen->InvokeRuntime(kQuickResolveType, instruction_, this);
       }
       // If we also must_do_clinit, the resolved type is now in the correct register.
     } else {
@@ -343,7 +336,7 @@ class LoadClassSlowPathX86_64 : public SlowPathCode {
       x86_64_codegen->Move(Location::RegisterLocation(RAX), source);
     }
     if (must_do_clinit) {
-      x86_64_codegen->InvokeRuntime(kQuickInitializeStaticStorage, instruction_, dex_pc, this);
+      x86_64_codegen->InvokeRuntime(kQuickInitializeStaticStorage, instruction_, this);
       CheckEntrypointTypes<kQuickInitializeStaticStorage, void*, mirror::Class*>();
     }
 
@@ -381,10 +374,7 @@ class LoadStringSlowPathX86_64 : public SlowPathCode {
     const dex::StringIndex string_index = instruction_->AsLoadString()->GetStringIndex();
     // Custom calling convention: RAX serves as both input and output.
     __ movl(CpuRegister(RAX), Immediate(string_index.index_));
-    x86_64_codegen->InvokeRuntime(kQuickResolveString,
-                                  instruction_,
-                                  instruction_->GetDexPc(),
-                                  this);
+    x86_64_codegen->InvokeRuntime(kQuickResolveString, instruction_, this);
     CheckEntrypointTypes<kQuickResolveString, void*, uint32_t>();
     x86_64_codegen->Move(locations->Out(), Location::RegisterLocation(RAX));
     RestoreLiveRegisters(codegen, locations);
@@ -405,7 +395,6 @@ class TypeCheckSlowPathX86_64 : public SlowPathCode {
 
   void EmitNativeCode(CodeGenerator* codegen) override {
     LocationSummary* locations = instruction_->GetLocations();
-    uint32_t dex_pc = instruction_->GetDexPc();
     DCHECK(instruction_->IsCheckCast()
            || !locations->GetLiveRegisters()->ContainsCoreRegister(locations->Out().reg()));
 
@@ -433,11 +422,11 @@ class TypeCheckSlowPathX86_64 : public SlowPathCode {
                                Location::RegisterLocation(calling_convention.GetRegisterAt(1)),
                                DataType::Type::kReference);
     if (instruction_->IsInstanceOf()) {
-      x86_64_codegen->InvokeRuntime(kQuickInstanceofNonTrivial, instruction_, dex_pc, this);
+      x86_64_codegen->InvokeRuntime(kQuickInstanceofNonTrivial, instruction_, this);
       CheckEntrypointTypes<kQuickInstanceofNonTrivial, size_t, mirror::Object*, mirror::Class*>();
     } else {
       DCHECK(instruction_->IsCheckCast());
-      x86_64_codegen->InvokeRuntime(kQuickCheckInstanceOf, instruction_, dex_pc, this);
+      x86_64_codegen->InvokeRuntime(kQuickCheckInstanceOf, instruction_, this);
       CheckEntrypointTypes<kQuickCheckInstanceOf, void, mirror::Object*, mirror::Class*>();
     }
 
@@ -475,7 +464,7 @@ class DeoptimizationSlowPathX86_64 : public SlowPathCode {
     x86_64_codegen->Load32BitValue(
         CpuRegister(calling_convention.GetRegisterAt(0)),
         static_cast<uint32_t>(instruction_->AsDeoptimize()->GetDeoptimizationKind()));
-    x86_64_codegen->InvokeRuntime(kQuickDeoptimize, instruction_, instruction_->GetDexPc(), this);
+    x86_64_codegen->InvokeRuntime(kQuickDeoptimize, instruction_, this);
     CheckEntrypointTypes<kQuickDeoptimize, void, DeoptimizationKind>();
   }
 
@@ -514,7 +503,7 @@ class ArraySetSlowPathX86_64 : public SlowPathCode {
     codegen->GetMoveResolver()->EmitNativeCode(&parallel_move);
 
     CodeGeneratorX86_64* x86_64_codegen = down_cast<CodeGeneratorX86_64*>(codegen);
-    x86_64_codegen->InvokeRuntime(kQuickAputObject, instruction_, instruction_->GetDexPc(), this);
+    x86_64_codegen->InvokeRuntime(kQuickAputObject, instruction_, this);
     CheckEntrypointTypes<kQuickAputObject, void, mirror::Array*, int32_t, mirror::Object*>();
     RestoreLiveRegisters(codegen, locations);
     __ jmp(GetExitLabel());
@@ -924,10 +913,7 @@ class ReadBarrierForHeapReferenceSlowPathX86_64 : public SlowPathCode {
       codegen->GetMoveResolver()->EmitNativeCode(&parallel_move);
       __ movl(CpuRegister(calling_convention.GetRegisterAt(2)), Immediate(offset_));
     }
-    x86_64_codegen->InvokeRuntime(kQuickReadBarrierSlow,
-                                  instruction_,
-                                  instruction_->GetDexPc(),
-                                  this);
+    x86_64_codegen->InvokeRuntime(kQuickReadBarrierSlow, instruction_, this);
     CheckEntrypointTypes<
         kQuickReadBarrierSlow, mirror::Object*, mirror::Object*, mirror::Object*, uint32_t>();
     x86_64_codegen->Move(out_, Location::RegisterLocation(RAX));
@@ -992,10 +978,7 @@ class ReadBarrierForRootSlowPathX86_64 : public SlowPathCode {
     InvokeRuntimeCallingConvention calling_convention;
     CodeGeneratorX86_64* x86_64_codegen = down_cast<CodeGeneratorX86_64*>(codegen);
     x86_64_codegen->Move(Location::RegisterLocation(calling_convention.GetRegisterAt(0)), root_);
-    x86_64_codegen->InvokeRuntime(kQuickReadBarrierForRootSlow,
-                                  instruction_,
-                                  instruction_->GetDexPc(),
-                                  this);
+    x86_64_codegen->InvokeRuntime(kQuickReadBarrierForRootSlow, instruction_, this);
     CheckEntrypointTypes<kQuickReadBarrierForRootSlow, mirror::Object*, GcRoot<mirror::Object>*>();
     x86_64_codegen->Move(out_, Location::RegisterLocation(RAX));
 
@@ -1028,7 +1011,7 @@ class MethodEntryExitHooksSlowPathX86_64 : public SlowPathCode {
       // Load FrameSize to pass to the exit hook.
       __ movq(CpuRegister(R8), Immediate(codegen->GetFrameSize()));
     }
-    x86_64_codegen->InvokeRuntime(entry_point, instruction_, instruction_->GetDexPc(), this);
+    x86_64_codegen->InvokeRuntime(entry_point, instruction_, this);
     RestoreLiveRegisters(codegen, locations);
     __ jmp(GetExitLabel());
   }
@@ -1209,7 +1192,7 @@ void CodeGeneratorX86_64::GenerateStaticOrDirectCall(
     case CodePtrLocation::kCallSelf:
       DCHECK(!GetGraph()->HasShouldDeoptimizeFlag());
       __ call(&frame_entry_label_);
-      RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+      RecordPcInfo(invoke, slow_path);
       break;
     case CodePtrLocation::kCallCriticalNative: {
       size_t out_frame_size =
@@ -1225,7 +1208,7 @@ void CodeGeneratorX86_64::GenerateStaticOrDirectCall(
         __ call(Address(callee_method.AsRegister<CpuRegister>(),
                          ArtMethod::EntryPointFromJniOffset(kX86_64PointerSize).SizeValue()));
       }
-      RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+      RecordPcInfo(invoke, slow_path);
       // Zero-/sign-extend the result when needed due to native and managed ABI mismatch.
       switch (invoke->GetType()) {
         case DataType::Type::kBool:
@@ -1260,7 +1243,7 @@ void CodeGeneratorX86_64::GenerateStaticOrDirectCall(
       __ call(Address(callee_method.AsRegister<CpuRegister>(),
                       ArtMethod::EntryPointFromQuickCompiledCodeOffset(
                           kX86_64PointerSize).SizeValue()));
-      RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+      RecordPcInfo(invoke, slow_path);
       break;
   }
 
@@ -1300,7 +1283,7 @@ void CodeGeneratorX86_64::GenerateVirtualCall(
   // call temp->GetEntryPoint();
   __ call(Address(temp, ArtMethod::EntryPointFromQuickCompiledCodeOffset(
       kX86_64PointerSize).SizeValue()));
-  RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+  RecordPcInfo(invoke, slow_path);
 }
 
 void CodeGeneratorX86_64::RecordBootImageIntrinsicPatch(uint32_t intrinsic_data) {
@@ -1562,12 +1545,11 @@ size_t CodeGeneratorX86_64::RestoreFloatingPointRegister(size_t stack_index, uin
 
 void CodeGeneratorX86_64::InvokeRuntime(QuickEntrypointEnum entrypoint,
                                         HInstruction* instruction,
-                                        uint32_t dex_pc,
                                         SlowPathCode* slow_path) {
   ValidateInvokeRuntime(entrypoint, instruction, slow_path);
   GenerateInvokeRuntime(GetThreadOffset<kX86_64PointerSize>(entrypoint).Int32Value());
   if (EntrypointRequiresStackMap(entrypoint)) {
-    RecordPcInfo(instruction, dex_pc, slow_path);
+    RecordPcInfo(instruction, slow_path);
   }
 }
 
@@ -1892,7 +1874,7 @@ void CodeGeneratorX86_64::GenerateFrameEntry() {
   if (!skip_overflow_check) {
     size_t reserved_bytes = GetStackOverflowReservedBytes(InstructionSet::kX86_64);
     __ testq(CpuRegister(RAX), Address(CpuRegister(RSP), -static_cast<int32_t>(reserved_bytes)));
-    RecordPcInfo(nullptr, 0);
+    RecordPcInfoForFrameOrBlockEntry();
   }
 
   if (!HasEmptyFrame()) {
@@ -1983,7 +1965,7 @@ void CodeGeneratorX86_64::Move(Location destination, Location source) {
     if (source.IsRegister()) {
       __ movq(dest, source.AsRegister<CpuRegister>());
     } else if (source.IsFpuRegister()) {
-      __ movd(dest, source.AsFpuRegister<XmmRegister>());
+      __ movq(dest, source.AsFpuRegister<XmmRegister>());
     } else if (source.IsStackSlot()) {
       __ movl(dest, Address(CpuRegister(RSP), source.GetStackIndex()));
     } else if (source.IsConstant()) {
@@ -2002,7 +1984,7 @@ void CodeGeneratorX86_64::Move(Location destination, Location source) {
   } else if (destination.IsFpuRegister()) {
     XmmRegister dest = destination.AsFpuRegister<XmmRegister>();
     if (source.IsRegister()) {
-      __ movd(dest, source.AsRegister<CpuRegister>());
+      __ movq(dest, source.AsRegister<CpuRegister>());
     } else if (source.IsFpuRegister()) {
       __ movaps(dest, source.AsFpuRegister<XmmRegister>());
     } else if (source.IsConstant()) {
@@ -2948,7 +2930,7 @@ void InstructionCodeGeneratorX86_64::VisitReturn(HReturn* ret) {
       // To simplify callers of an OSR method, we put the return value in both
       // floating point and core register.
       if (GetGraph()->IsCompilingOsr()) {
-        __ movd(CpuRegister(RAX), XmmRegister(XMM0), /* is64bit= */ false);
+        __ movd(CpuRegister(RAX), XmmRegister(XMM0));
       }
       break;
     }
@@ -2958,7 +2940,7 @@ void InstructionCodeGeneratorX86_64::VisitReturn(HReturn* ret) {
       // To simplify callers of an OSR method, we put the return value in both
       // floating point and core register.
       if (GetGraph()->IsCompilingOsr()) {
-        __ movd(CpuRegister(RAX), XmmRegister(XMM0), /* is64bit= */ true);
+        __ movq(CpuRegister(RAX), XmmRegister(XMM0));
       }
       break;
     }
@@ -3270,7 +3252,7 @@ void InstructionCodeGeneratorX86_64::VisitInvokeInterface(HInvokeInterface* invo
       temp, ArtMethod::EntryPointFromQuickCompiledCodeOffset(kX86_64PointerSize).SizeValue()));
 
   DCHECK(!codegen_->IsLeafMethod());
-  codegen_->RecordPcInfo(invoke, invoke->GetDexPc());
+  codegen_->RecordPcInfo(invoke);
 }
 
 void LocationsBuilderX86_64::VisitInvokePolymorphic(HInvokePolymorphic* invoke) {
@@ -5171,7 +5153,7 @@ void LocationsBuilderX86_64::VisitNewInstance(HNewInstance* instruction) {
 }
 
 void InstructionCodeGeneratorX86_64::VisitNewInstance(HNewInstance* instruction) {
-  codegen_->InvokeRuntime(instruction->GetEntrypoint(), instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(instruction->GetEntrypoint(), instruction);
   CheckEntrypointTypes<kQuickAllocObjectWithChecks, void*, mirror::Class*>();
   DCHECK(!codegen_->IsLeafMethod());
 }
@@ -5188,7 +5170,7 @@ void LocationsBuilderX86_64::VisitNewArray(HNewArray* instruction) {
 void InstructionCodeGeneratorX86_64::VisitNewArray(HNewArray* instruction) {
   // Note: if heap poisoning is enabled, the entry point takes care of poisoning the reference.
   QuickEntrypointEnum entrypoint = CodeGenerator::GetArrayAllocationEntrypoint(instruction);
-  codegen_->InvokeRuntime(entrypoint, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(entrypoint, instruction);
   CheckEntrypointTypes<kQuickAllocArrayResolved, void*, mirror::Class*, int32_t>();
   DCHECK(!codegen_->IsLeafMethod());
 }
@@ -5460,16 +5442,16 @@ void InstructionCodeGeneratorX86_64::Bswap(Location value,
       break;
     case DataType::Type::kFloat32: {
       DCHECK_NE(temp, nullptr);
-      __ movd(*temp, value.AsFpuRegister<XmmRegister>(), /*is64bit=*/ false);
+      __ movd(*temp, value.AsFpuRegister<XmmRegister>());
       __ bswapl(*temp);
-      __ movd(value.AsFpuRegister<XmmRegister>(), *temp, /*is64bit=*/ false);
+      __ movd(value.AsFpuRegister<XmmRegister>(), *temp);
       break;
     }
     case DataType::Type::kFloat64: {
       DCHECK_NE(temp, nullptr);
-      __ movd(*temp, value.AsFpuRegister<XmmRegister>(), /*is64bit=*/ true);
+      __ movq(*temp, value.AsFpuRegister<XmmRegister>());
       __ bswapq(*temp);
-      __ movd(value.AsFpuRegister<XmmRegister>(), *temp, /*is64bit=*/ true);
+      __ movq(value.AsFpuRegister<XmmRegister>(), *temp);
       break;
     }
     default:
@@ -5711,7 +5693,7 @@ void LocationsBuilderX86_64::VisitStringBuilderAppend(HStringBuilderAppend* inst
 
 void InstructionCodeGeneratorX86_64::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
   __ movl(CpuRegister(RDI), Immediate(instruction->GetFormat()->GetValue()));
-  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction);
 }
 
 void LocationsBuilderX86_64::VisitUnresolvedInstanceFieldGet(
@@ -5727,7 +5709,6 @@ void InstructionCodeGeneratorX86_64::VisitUnresolvedInstanceFieldGet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -5744,7 +5725,6 @@ void InstructionCodeGeneratorX86_64::VisitUnresolvedInstanceFieldSet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -5761,7 +5741,6 @@ void InstructionCodeGeneratorX86_64::VisitUnresolvedStaticFieldGet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -5778,7 +5757,6 @@ void InstructionCodeGeneratorX86_64::VisitUnresolvedStaticFieldSet(
   codegen_->GenerateUnresolvedFieldAccess(instruction,
                                           instruction->GetFieldType(),
                                           instruction->GetFieldIndex(),
-                                          instruction->GetDexPc(),
                                           calling_convention);
 }
 
@@ -5798,7 +5776,7 @@ void CodeGeneratorX86_64::GenerateImplicitNullCheck(HNullCheck* instruction) {
   Location obj = locations->InAt(0);
 
   __ testl(CpuRegister(RAX), Address(obj.AsRegister<CpuRegister>(), 0));
-  RecordPcInfo(instruction, instruction->GetDexPc());
+  RecordPcInfo(instruction);
 }
 
 void CodeGeneratorX86_64::GenerateExplicitNullCheck(HNullCheck* instruction) {
@@ -6561,7 +6539,7 @@ void ParallelMoveResolverX86_64::Exchange32(XmmRegister reg, int mem) {
 void ParallelMoveResolverX86_64::Exchange64(XmmRegister reg, int mem) {
   __ movq(CpuRegister(TMP), Address(CpuRegister(RSP), mem));
   __ movsd(Address(CpuRegister(RSP), mem), reg);
-  __ movd(reg, CpuRegister(TMP));
+  __ movq(reg, CpuRegister(TMP));
 }
 
 void ParallelMoveResolverX86_64::Exchange128(XmmRegister reg, int mem) {
@@ -6626,9 +6604,9 @@ void ParallelMoveResolverX86_64::EmitSwap(size_t index) {
   } else if (source.IsDoubleStackSlot() && destination.IsDoubleStackSlot()) {
     ExchangeMemory64(destination.GetStackIndex(), source.GetStackIndex(), 1);
   } else if (source.IsFpuRegister() && destination.IsFpuRegister()) {
-    __ movd(CpuRegister(TMP), source.AsFpuRegister<XmmRegister>());
+    __ movq(CpuRegister(TMP), source.AsFpuRegister<XmmRegister>());
     __ movaps(source.AsFpuRegister<XmmRegister>(), destination.AsFpuRegister<XmmRegister>());
-    __ movd(destination.AsFpuRegister<XmmRegister>(), CpuRegister(TMP));
+    __ movq(destination.AsFpuRegister<XmmRegister>(), CpuRegister(TMP));
   } else if (source.IsFpuRegister() && destination.IsStackSlot()) {
     Exchange32(source.AsFpuRegister<XmmRegister>(), destination.GetStackIndex());
   } else if (source.IsStackSlot() && destination.IsFpuRegister()) {
@@ -7071,9 +7049,7 @@ void InstructionCodeGeneratorX86_64::VisitLoadString(HLoadString* load) NO_THREA
 
   // Custom calling convention: RAX serves as both input and output.
   __ movl(CpuRegister(RAX), Immediate(load->GetStringIndex().index_));
-  codegen_->InvokeRuntime(kQuickResolveString,
-                          load,
-                          load->GetDexPc());
+  codegen_->InvokeRuntime(kQuickResolveString, load);
   CheckEntrypointTypes<kQuickResolveString, void*, uint32_t>();
 }
 
@@ -7108,7 +7084,7 @@ void LocationsBuilderX86_64::VisitThrow(HThrow* instruction) {
 }
 
 void InstructionCodeGeneratorX86_64::VisitThrow(HThrow* instruction) {
-  codegen_->InvokeRuntime(kQuickDeliverException, instruction, instruction->GetDexPc());
+  codegen_->InvokeRuntime(kQuickDeliverException, instruction);
   CheckEntrypointTypes<kQuickDeliverException, void, mirror::Object*>();
 }
 
@@ -7757,8 +7733,7 @@ void LocationsBuilderX86_64::VisitMonitorOperation(HMonitorOperation* instructio
 
 void InstructionCodeGeneratorX86_64::VisitMonitorOperation(HMonitorOperation* instruction) {
   codegen_->InvokeRuntime(instruction->IsEnter() ? kQuickLockObject : kQuickUnlockObject,
-                          instruction,
-                          instruction->GetDexPc());
+                          instruction);
   if (instruction->IsEnter()) {
     CheckEntrypointTypes<kQuickLockObject, void, mirror::Object*>();
   } else {
diff --git a/compiler/optimizing/code_generator_x86_64.h b/compiler/optimizing/code_generator_x86_64.h
index 3024116402..8a514b21f0 100644
--- a/compiler/optimizing/code_generator_x86_64.h
+++ b/compiler/optimizing/code_generator_x86_64.h
@@ -415,7 +415,6 @@ class CodeGeneratorX86_64 : public CodeGenerator {
   // Generate code to invoke a runtime entry point.
   void InvokeRuntime(QuickEntrypointEnum entrypoint,
                      HInstruction* instruction,
-                     uint32_t dex_pc,
                      SlowPathCode* slow_path = nullptr) override;
 
   // Generate code to invoke a runtime entry point, but do not record
diff --git a/compiler/optimizing/code_sinking.cc b/compiler/optimizing/code_sinking.cc
index 0abcaea719..9ba5416697 100644
--- a/compiler/optimizing/code_sinking.cc
+++ b/compiler/optimizing/code_sinking.cc
@@ -150,8 +150,8 @@ static bool IsInterestingInstruction(HInstruction* instruction) {
 }
 
 static void AddInstruction(HInstruction* instruction,
-                           const ArenaBitVector& processed_instructions,
-                           const ArenaBitVector& discard_blocks,
+                           BitVectorView<size_t> processed_instructions,
+                           BitVectorView<size_t> discard_blocks,
                            ScopedArenaVector<HInstruction*>* worklist) {
   // Add to the work list if the instruction is not in the list of blocks
   // to discard, hasn't been already processed and is of interest.
@@ -163,8 +163,8 @@ static void AddInstruction(HInstruction* instruction,
 }
 
 static void AddInputs(HInstruction* instruction,
-                      const ArenaBitVector& processed_instructions,
-                      const ArenaBitVector& discard_blocks,
+                      BitVectorView<size_t> processed_instructions,
+                      BitVectorView<size_t> discard_blocks,
                       ScopedArenaVector<HInstruction*>* worklist) {
   for (HInstruction* input : instruction->GetInputs()) {
     AddInstruction(input, processed_instructions, discard_blocks, worklist);
@@ -172,8 +172,8 @@ static void AddInputs(HInstruction* instruction,
 }
 
 static void AddInputs(HBasicBlock* block,
-                      const ArenaBitVector& processed_instructions,
-                      const ArenaBitVector& discard_blocks,
+                      BitVectorView<size_t> processed_instructions,
+                      BitVectorView<size_t> discard_blocks,
                       ScopedArenaVector<HInstruction*>* worklist) {
   for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
     AddInputs(it.Current(), processed_instructions, discard_blocks, worklist);
@@ -185,7 +185,7 @@ static void AddInputs(HBasicBlock* block,
 
 static bool ShouldFilterUse(HInstruction* instruction,
                             HInstruction* user,
-                            const ArenaBitVector& post_dominated) {
+                            BitVectorView<size_t> post_dominated) {
   if (instruction->IsNewInstance()) {
     return (user->IsInstanceFieldSet() || user->IsConstructorFence()) &&
         (user->InputAt(0) == instruction) &&
@@ -204,7 +204,7 @@ static bool ShouldFilterUse(HInstruction* instruction,
 // This method is tailored to the sinking algorithm, unlike
 // the generic HInstruction::MoveBeforeFirstUserAndOutOfLoops.
 static HInstruction* FindIdealPosition(HInstruction* instruction,
-                                       const ArenaBitVector& post_dominated,
+                                       BitVectorView<size_t> post_dominated,
                                        bool filter = false) {
   DCHECK(!instruction->IsPhi());  // Makes no sense for Phi.
 
@@ -333,9 +333,10 @@ void CodeSinking::SinkCodeToUncommonBranch(HBasicBlock* end_block) {
 
   size_t number_of_instructions = graph_->GetCurrentInstructionId();
   ScopedArenaVector<HInstruction*> worklist(allocator.Adapter(kArenaAllocMisc));
-  ArenaBitVector processed_instructions(
-      &allocator, number_of_instructions, /* expandable= */ false);
-  ArenaBitVector post_dominated(&allocator, graph_->GetBlocks().size(), /* expandable= */ false);
+  BitVectorView<size_t> processed_instructions =
+      ArenaBitVector::CreateFixedSize(&allocator, number_of_instructions);
+  BitVectorView<size_t> post_dominated =
+      ArenaBitVector::CreateFixedSize(&allocator, graph_->GetBlocks().size());
 
   // Step (1): Visit post order to get a subset of blocks post dominated by `end_block`.
   // TODO(ngeoffray): Getting the full set of post-dominated should be done by
@@ -408,8 +409,8 @@ void CodeSinking::SinkCodeToUncommonBranch(HBasicBlock* end_block) {
   HBasicBlock* common_dominator = finder.Get();
 
   // Step (2): iterate over the worklist to find sinking candidates.
-  ArenaBitVector instructions_that_can_move(
-      &allocator, number_of_instructions, /* expandable= */ false);
+  BitVectorView<size_t> instructions_that_can_move =
+      ArenaBitVector::CreateFixedSize(&allocator, number_of_instructions);
   ScopedArenaVector<ScopedArenaVector<HInstruction*>> instructions_to_move(
       graph_->GetBlocks().size(),
       ScopedArenaVector<HInstruction*>(allocator.Adapter(kArenaAllocMisc)),
diff --git a/compiler/optimizing/constant_folding.cc b/compiler/optimizing/constant_folding.cc
index 0ec60b5ead..66fb04e11a 100644
--- a/compiler/optimizing/constant_folding.cc
+++ b/compiler/optimizing/constant_folding.cc
@@ -54,7 +54,9 @@ class HConstantFoldingVisitor final : public HGraphDelegateVisitor {
   void VisitInvoke(HInvoke* inst) override;
   void VisitTypeConversion(HTypeConversion* inst) override;
 
-  void PropagateValue(HBasicBlock* starting_block, HInstruction* variable, HConstant* constant);
+  void PropagateValue(HBasicBlock* starting_block,
+                      HInstruction* variable,
+                      std::variant<HConstant*, bool> constant);
 
   // Intrinsics foldings
   void FoldReverseIntrinsic(HInvoke* invoke);
@@ -218,7 +220,7 @@ void HConstantFoldingVisitor::VisitDivZeroCheck(HDivZeroCheck* inst) {
 
 void HConstantFoldingVisitor::PropagateValue(HBasicBlock* starting_block,
                                              HInstruction* variable,
-                                             HConstant* constant) {
+                                             std::variant<HConstant*, bool> constant) {
   const bool recording_stats = stats_ != nullptr;
   size_t uses_before = 0;
   size_t uses_after = 0;
@@ -227,8 +229,12 @@ void HConstantFoldingVisitor::PropagateValue(HBasicBlock* starting_block,
   }
 
   if (!variable->GetUses().HasExactlyOneElement()) {
-    variable->ReplaceUsesDominatedBy(
-        starting_block->GetFirstInstruction(), constant, /* strictly_dominated= */ false);
+    HConstant* c = std::holds_alternative<HConstant*>(constant)
+                       ? std::get<HConstant*>(constant)
+                       : GetGraph()->GetIntConstant(std::get<bool>(constant) ? 1 : 0);
+    variable->ReplaceUsesDominatedBy(starting_block->GetFirstInstruction(),
+                                     c,
+                                     /* strictly_dominated= */ false);
   }
 
   if (recording_stats) {
@@ -256,8 +262,8 @@ void HConstantFoldingVisitor::VisitIf(HIf* inst) {
   // } else {
   //   and here false
   // }
-  PropagateValue(inst->IfTrueSuccessor(), if_input, GetGraph()->GetIntConstant(1));
-  PropagateValue(inst->IfFalseSuccessor(), if_input, GetGraph()->GetIntConstant(0));
+  PropagateValue(inst->IfTrueSuccessor(), if_input, true);
+  PropagateValue(inst->IfFalseSuccessor(), if_input, false);
 
   // If the input is a condition, we can propagate the information of the condition itself.
   if (!if_input->IsCondition()) {
@@ -341,12 +347,7 @@ void HConstantFoldingVisitor::VisitIf(HIf* inst) {
     HBasicBlock* other_starting_block =
         condition->IsEqual() ? inst->IfFalseSuccessor() : inst->IfTrueSuccessor();
     DCHECK_NE(other_starting_block, starting_block);
-
-    HConstant* other_constant = constant->AsIntConstant()->IsTrue() ?
-                                    GetGraph()->GetIntConstant(0) :
-                                    GetGraph()->GetIntConstant(1);
-    DCHECK_NE(other_constant, constant);
-    PropagateValue(other_starting_block, variable, other_constant);
+    PropagateValue(other_starting_block, variable, !constant->AsIntConstant()->IsTrue());
   }
 }
 
diff --git a/compiler/optimizing/constructor_fence_redundancy_elimination.cc b/compiler/optimizing/constructor_fence_redundancy_elimination.cc
index 71fc39a956..c89ec171d9 100644
--- a/compiler/optimizing/constructor_fence_redundancy_elimination.cc
+++ b/compiler/optimizing/constructor_fence_redundancy_elimination.cc
@@ -33,7 +33,7 @@ class CFREVisitor final : public HGraphVisitor {
       : HGraphVisitor(graph),
         scoped_allocator_(graph->GetArenaStack()),
         candidate_fences_(scoped_allocator_.Adapter(kArenaAllocCFRE)),
-        candidate_fence_targets_(std::nullopt),
+        candidate_fence_targets_(),
         stats_(stats) {}
 
   void VisitBasicBlock(HBasicBlock* block) override {
@@ -48,14 +48,17 @@ class CFREVisitor final : public HGraphVisitor {
   void VisitConstructorFence(HConstructorFence* constructor_fence) override {
     candidate_fences_.push_back(constructor_fence);
 
-    if (!candidate_fence_targets_.has_value()) {
+    if (candidate_fence_targets_.SizeInBits() == 0u) {
       size_t number_of_instructions = GetGraph()->GetCurrentInstructionId();
-      candidate_fence_targets_.emplace(
-          &scoped_allocator_, number_of_instructions, /*expandable=*/ false, kArenaAllocCFRE);
+      candidate_fence_targets_ = ArenaBitVector::CreateFixedSize(
+          &scoped_allocator_, number_of_instructions, kArenaAllocCFRE);
+    } else {
+      DCHECK_EQ(candidate_fence_targets_.SizeInBits(),
+                static_cast<size_t>(GetGraph()->GetCurrentInstructionId()));
     }
 
     for (HInstruction* input : constructor_fence->GetInputs()) {
-      candidate_fence_targets_->SetBit(input->GetId());
+      candidate_fence_targets_.SetBit(input->GetId());
     }
   }
 
@@ -162,8 +165,7 @@ class CFREVisitor final : public HGraphVisitor {
   void VisitSetLocation([[maybe_unused]] HInstruction* inst, HInstruction* store_input) {
     if (candidate_fences_.empty()) {
       // There is no need to look at inputs if there are no candidate fence targets.
-      DCHECK_IMPLIES(candidate_fence_targets_.has_value(),
-                     !candidate_fence_targets_->IsAnyBitSet());
+      DCHECK(!candidate_fence_targets_.IsAnyBitSet());
       return;
     }
     // An object is considered "published" if it's stored onto the heap.
@@ -179,8 +181,7 @@ class CFREVisitor final : public HGraphVisitor {
   bool HasInterestingPublishTargetAsInput(HInstruction* inst) {
     if (candidate_fences_.empty()) {
       // There is no need to look at inputs if there are no candidate fence targets.
-      DCHECK_IMPLIES(candidate_fence_targets_.has_value(),
-                     !candidate_fence_targets_->IsAnyBitSet());
+      DCHECK(!candidate_fence_targets_.IsAnyBitSet());
       return false;
     }
     for (HInstruction* input : inst->GetInputs()) {
@@ -221,15 +222,17 @@ class CFREVisitor final : public HGraphVisitor {
     // there is no benefit to this extra complexity unless we also reordered
     // the stores to come later.
     candidate_fences_.clear();
-    DCHECK(candidate_fence_targets_.has_value());
-    candidate_fence_targets_->ClearAllBits();
+    DCHECK_EQ(candidate_fence_targets_.SizeInBits(),
+              static_cast<size_t>(GetGraph()->GetCurrentInstructionId()));
+    candidate_fence_targets_.ClearAllBits();
   }
 
   // A publishing 'store' is only interesting if the value being stored
   // is one of the fence `targets` in `candidate_fences`.
   bool IsInterestingPublishTarget(HInstruction* store_input) const {
-    DCHECK(candidate_fence_targets_.has_value());
-    return candidate_fence_targets_->IsBitSet(store_input->GetId());
+    DCHECK_EQ(candidate_fence_targets_.SizeInBits(),
+              static_cast<size_t>(GetGraph()->GetCurrentInstructionId()));
+    return candidate_fence_targets_.IsBitSet(store_input->GetId());
   }
 
   // Phase-local heap memory allocator for CFRE optimizer.
@@ -245,7 +248,7 @@ class CFREVisitor final : public HGraphVisitor {
 
   // Stores a set of the fence targets, to allow faster lookup of whether
   // a detected publish is a target of one of the candidate fences.
-  std::optional<ArenaBitVector> candidate_fence_targets_;
+  BitVectorView<size_t> candidate_fence_targets_;
 
   // Used to record stats about the optimization.
   OptimizingCompilerStats* const stats_;
diff --git a/compiler/optimizing/select_generator.cc b/compiler/optimizing/control_flow_simplifier.cc
similarity index 72%
rename from compiler/optimizing/select_generator.cc
rename to compiler/optimizing/control_flow_simplifier.cc
index d5781c8d38..35efed59da 100644
--- a/compiler/optimizing/select_generator.cc
+++ b/compiler/optimizing/control_flow_simplifier.cc
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-#include "select_generator.h"
+#include "control_flow_simplifier.h"
 
 #include "optimizing/nodes.h"
 #include "reference_type_propagation.h"
@@ -23,9 +23,9 @@ namespace art HIDDEN {
 
 static constexpr size_t kMaxInstructionsInBranch = 1u;
 
-HSelectGenerator::HSelectGenerator(HGraph* graph,
-                                   OptimizingCompilerStats* stats,
-                                   const char* name)
+HControlFlowSimplifier::HControlFlowSimplifier(HGraph* graph,
+                                               OptimizingCompilerStats* stats,
+                                               const char* name)
     : HOptimization(graph, name, stats) {
 }
 
@@ -68,26 +68,34 @@ static bool BlocksMergeTogether(HBasicBlock* block1, HBasicBlock* block2) {
   return block1->GetSingleSuccessor() == block2->GetSingleSuccessor();
 }
 
-// Returns nullptr if `block` has either no phis or there is more than one phi. Otherwise returns
-// that phi.
-static HPhi* GetSinglePhi(HBasicBlock* block, size_t index1, size_t index2) {
+// Search `block` for phis that have different inputs at `index1` and `index2`.
+// If none is found, returns `{true, nullptr}`.
+// If exactly one such `phi` is found, returns `{true, phi}`.
+// Otherwise (if more than one such phi is found), returns `{false, nullptr}`.
+static std::pair<bool, HPhi*> HasAtMostOnePhiWithDifferentInputs(HBasicBlock* block,
+                                                                 size_t index1,
+                                                                 size_t index2) {
   DCHECK_NE(index1, index2);
 
   HPhi* select_phi = nullptr;
   for (HInstructionIterator it(block->GetPhis()); !it.Done(); it.Advance()) {
     HPhi* phi = it.Current()->AsPhi();
+    auto&& inputs = phi->GetInputs();
+    if (inputs[index1] == inputs[index2]) {
+      continue;
+    }
     if (select_phi == nullptr) {
       // First phi found.
       select_phi = phi;
     } else {
       // More than one phi found, return null.
-      return nullptr;
+      return {false, nullptr};
     }
   }
-  return select_phi;
+  return {true, select_phi};
 }
 
-bool HSelectGenerator::TryGenerateSelectSimpleDiamondPattern(
+bool HControlFlowSimplifier::TryGenerateSelectSimpleDiamondPattern(
     HBasicBlock* block, ScopedArenaSafeMap<HInstruction*, HSelect*>* cache) {
   DCHECK(block->GetLastInstruction()->IsIf());
   HIf* if_instruction = block->GetLastInstruction()->AsIf();
@@ -132,78 +140,87 @@ bool HSelectGenerator::TryGenerateSelectSimpleDiamondPattern(
   //     a = 1; b = 2;
   //   }
   //   // use a and b
-  HPhi* phi = GetSinglePhi(merge_block, predecessor_index_true, predecessor_index_false);
-
+  bool at_most_one_phi_with_different_inputs = false;
+  HPhi* phi = nullptr;
   HInstruction* true_value = nullptr;
   HInstruction* false_value = nullptr;
   if (both_successors_return) {
+    // Note: This can create a select with the same then-value and else-value.
     true_value = true_block->GetFirstInstruction()->InputAt(0);
     false_value = false_block->GetFirstInstruction()->InputAt(0);
-  } else if (phi != nullptr) {
-    true_value = phi->InputAt(predecessor_index_true);
-    false_value = phi->InputAt(predecessor_index_false);
   } else {
-    return false;
+    std::tie(at_most_one_phi_with_different_inputs, phi) = HasAtMostOnePhiWithDifferentInputs(
+        merge_block, predecessor_index_true, predecessor_index_false);
+    if (!at_most_one_phi_with_different_inputs) {
+      return false;
+    }
+    if (phi != nullptr) {
+      true_value = phi->InputAt(predecessor_index_true);
+      false_value = phi->InputAt(predecessor_index_false);
+    }  // else we don't need to create a `HSelect` at all.
   }
-  DCHECK(both_successors_return || phi != nullptr);
+  DCHECK(both_successors_return || at_most_one_phi_with_different_inputs);
 
   // Create the Select instruction and insert it in front of the If.
   HInstruction* condition = if_instruction->InputAt(0);
-  HSelect* select = new (graph_->GetAllocator()) HSelect(condition,
-                                                          true_value,
-                                                          false_value,
-                                                          if_instruction->GetDexPc());
-  if (both_successors_return) {
-    if (true_value->GetType() == DataType::Type::kReference) {
-      DCHECK(false_value->GetType() == DataType::Type::kReference);
-      ReferenceTypePropagation::FixUpSelectType(select, graph_->GetHandleCache());
+  HSelect* select = nullptr;
+  if (both_successors_return || phi != nullptr) {
+    select = new (graph_->GetAllocator()) HSelect(condition,
+                                                  true_value,
+                                                  false_value,
+                                                  if_instruction->GetDexPc());
+    block->InsertInstructionBefore(select, if_instruction);
+    if (both_successors_return) {
+      if (true_value->GetType() == DataType::Type::kReference) {
+        DCHECK(false_value->GetType() == DataType::Type::kReference);
+        ReferenceTypePropagation::FixUpSelectType(select, graph_->GetHandleCache());
+      }
+      false_block->GetFirstInstruction()->ReplaceInput(select, 0);
+    } else {
+      if (phi->GetType() == DataType::Type::kReference) {
+        select->SetReferenceTypeInfoIfValid(phi->GetReferenceTypeInfo());
+      }
+      phi->ReplaceInput(select, predecessor_index_false);  // We'll remove the true branch below.
     }
-  } else if (phi->GetType() == DataType::Type::kReference) {
-    select->SetReferenceTypeInfoIfValid(phi->GetReferenceTypeInfo());
-  }
-  block->InsertInstructionBefore(select, if_instruction);
-
-  // Remove the true branch which removes the corresponding Phi
-  // input if needed. If left only with the false branch, the Phi is
-  // automatically removed.
-  if (both_successors_return) {
-    false_block->GetFirstInstruction()->ReplaceInput(select, 0);
-  } else {
-    phi->ReplaceInput(select, predecessor_index_false);
   }
 
-  bool only_two_predecessors = (merge_block->GetPredecessors().size() == 2u);
+  // Remove the true branch which removes the corresponding Phi input if needed.
+  // If left only with the false branch, the Phi is automatically removed.
   true_block->DisconnectAndDelete();
 
   // Merge remaining blocks which are now connected with Goto.
   DCHECK_EQ(block->GetSingleSuccessor(), false_block);
   block->MergeWith(false_block);
-  if (!both_successors_return && only_two_predecessors) {
-    DCHECK_EQ(only_two_predecessors, phi->GetBlock() == nullptr);
+  if (!both_successors_return && merge_block->GetPredecessors().size() == 1u) {
+    DCHECK_IMPLIES(phi != nullptr, phi->GetBlock() == nullptr);
+    DCHECK(merge_block->GetPhis().IsEmpty());
     DCHECK_EQ(block->GetSingleSuccessor(), merge_block);
     block->MergeWith(merge_block);
   }
 
-  MaybeRecordStat(stats_, MethodCompilationStat::kSelectGenerated);
+  MaybeRecordStat(stats_, select != nullptr ? MethodCompilationStat::kControlFlowSelectGenerated
+                                            : MethodCompilationStat::kControlFlowDiamondRemoved);
 
   // Very simple way of finding common subexpressions in the generated HSelect statements
   // (since this runs after GVN). Lookup by condition, and reuse latest one if possible
   // (due to post order, latest select is most likely replacement). If needed, we could
   // improve this by e.g. using the operands in the map as well.
-  auto it = cache->find(condition);
-  if (it == cache->end()) {
-    cache->Put(condition, select);
-  } else {
-    // Found cached value. See if latest can replace cached in the HIR.
-    HSelect* cached_select = it->second;
-    DCHECK_EQ(cached_select->GetCondition(), select->GetCondition());
-    if (cached_select->GetTrueValue() == select->GetTrueValue() &&
-        cached_select->GetFalseValue() == select->GetFalseValue() &&
-        select->StrictlyDominates(cached_select)) {
-      cached_select->ReplaceWith(select);
-      cached_select->GetBlock()->RemoveInstruction(cached_select);
+  if (select != nullptr) {
+    auto it = cache->find(condition);
+    if (it == cache->end()) {
+      cache->Put(condition, select);
+    } else {
+      // Found cached value. See if latest can replace cached in the HIR.
+      HSelect* cached_select = it->second;
+      DCHECK_EQ(cached_select->GetCondition(), select->GetCondition());
+      if (cached_select->GetTrueValue() == select->GetTrueValue() &&
+          cached_select->GetFalseValue() == select->GetFalseValue() &&
+          select->StrictlyDominates(cached_select)) {
+        cached_select->ReplaceWith(select);
+        cached_select->GetBlock()->RemoveInstruction(cached_select);
+      }
+      it->second = select;  // always cache latest
     }
-    it->second = select;  // always cache latest
   }
 
   // No need to update dominance information, as we are simplifying
@@ -214,7 +231,7 @@ bool HSelectGenerator::TryGenerateSelectSimpleDiamondPattern(
   return true;
 }
 
-HBasicBlock* HSelectGenerator::TryFixupDoubleDiamondPattern(HBasicBlock* block) {
+HBasicBlock* HControlFlowSimplifier::TryFixupDoubleDiamondPattern(HBasicBlock* block) {
   DCHECK(block->GetLastInstruction()->IsIf());
   HIf* if_instruction = block->GetLastInstruction()->AsIf();
   HBasicBlock* true_block = if_instruction->IfTrueSuccessor();
@@ -307,12 +324,12 @@ HBasicBlock* HSelectGenerator::TryFixupDoubleDiamondPattern(HBasicBlock* block)
   return inner_if_block;
 }
 
-bool HSelectGenerator::Run() {
+bool HControlFlowSimplifier::Run() {
   bool did_select = false;
   // Select cache with local allocator.
   ScopedArenaAllocator allocator(graph_->GetArenaStack());
-  ScopedArenaSafeMap<HInstruction*, HSelect*> cache(std::less<HInstruction*>(),
-                                                    allocator.Adapter(kArenaAllocSelectGenerator));
+  ScopedArenaSafeMap<HInstruction*, HSelect*> cache(
+      std::less<HInstruction*>(), allocator.Adapter(kArenaAllocControlFlowSimplifier));
 
   // Iterate in post order in the unlikely case that removing one occurrence of
   // the selection pattern empties a branch block of another occurrence.
diff --git a/compiler/optimizing/select_generator.h b/compiler/optimizing/control_flow_simplifier.h
similarity index 85%
rename from compiler/optimizing/select_generator.h
rename to compiler/optimizing/control_flow_simplifier.h
index 7aa0803d89..0c74ec299f 100644
--- a/compiler/optimizing/select_generator.h
+++ b/compiler/optimizing/control_flow_simplifier.h
@@ -54,8 +54,8 @@
  * run after the instruction simplifier has removed redundant suspend checks.
  */
 
-#ifndef ART_COMPILER_OPTIMIZING_SELECT_GENERATOR_H_
-#define ART_COMPILER_OPTIMIZING_SELECT_GENERATOR_H_
+#ifndef ART_COMPILER_OPTIMIZING_CONTROL_FLOW_SIMPLIFIER_H_
+#define ART_COMPILER_OPTIMIZING_CONTROL_FLOW_SIMPLIFIER_H_
 
 #include "base/macros.h"
 #include "base/scoped_arena_containers.h"
@@ -64,15 +64,15 @@
 
 namespace art HIDDEN {
 
-class HSelectGenerator : public HOptimization {
+class HControlFlowSimplifier : public HOptimization {
  public:
-  HSelectGenerator(HGraph* graph,
-                   OptimizingCompilerStats* stats,
-                   const char* name = kSelectGeneratorPassName);
+  HControlFlowSimplifier(HGraph* graph,
+                         OptimizingCompilerStats* stats,
+                         const char* name = kControlFlowSimplifierPassName);
 
   bool Run() override;
 
-  static constexpr const char* kSelectGeneratorPassName = "select_generator";
+  static constexpr const char* kControlFlowSimplifierPassName = "control_flow_simplifier";
 
  private:
   bool TryGenerateSelectSimpleDiamondPattern(HBasicBlock* block,
@@ -112,9 +112,9 @@ class HSelectGenerator : public HOptimization {
   // when that gets resolved we get another one with the outer if.
   HBasicBlock* TryFixupDoubleDiamondPattern(HBasicBlock* block);
 
-  DISALLOW_COPY_AND_ASSIGN(HSelectGenerator);
+  DISALLOW_COPY_AND_ASSIGN(HControlFlowSimplifier);
 };
 
 }  // namespace art
 
-#endif  // ART_COMPILER_OPTIMIZING_SELECT_GENERATOR_H_
+#endif  // ART_COMPILER_OPTIMIZING_CONTROL_FLOW_SIMPLIFIER_H_
diff --git a/compiler/optimizing/control_flow_simplifier_test.cc b/compiler/optimizing/control_flow_simplifier_test.cc
new file mode 100644
index 0000000000..2e88c6c77b
--- /dev/null
+++ b/compiler/optimizing/control_flow_simplifier_test.cc
@@ -0,0 +1,150 @@
+/*
+ * Copyright (C) 2018 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "control_flow_simplifier.h"
+
+#include "base/arena_allocator.h"
+#include "base/macros.h"
+#include "builder.h"
+#include "nodes.h"
+#include "optimizing_unit_test.h"
+#include "side_effects_analysis.h"
+
+namespace art HIDDEN {
+
+class ControlFlowSimplifierTest : public OptimizingUnitTest {
+ protected:
+  HPhi* ConstructBasicGraphForSelect(HBasicBlock* return_block, HInstruction* instr) {
+    HParameterValue* bool_param = MakeParam(DataType::Type::kBool);
+    HIntConstant* const1 =  graph_->GetIntConstant(1);
+
+    auto [if_block, then_block, else_block] = CreateDiamondPattern(return_block, bool_param);
+
+    AddOrInsertInstruction(then_block, instr);
+    HPhi* phi = MakePhi(return_block, {instr, const1});
+    return phi;
+  }
+
+  bool CheckGraphAndTryControlFlowSimplifier() {
+    graph_->BuildDominatorTree();
+    EXPECT_TRUE(CheckGraph());
+
+    SideEffectsAnalysis side_effects(graph_);
+    side_effects.Run();
+    return HControlFlowSimplifier(graph_, /*handles*/ nullptr, /*stats*/ nullptr).Run();
+  }
+};
+
+// HDivZeroCheck might throw and should not be hoisted from the conditional to an unconditional.
+TEST_F(ControlFlowSimplifierTest, testZeroCheckPreventsSelect) {
+  HBasicBlock* return_block = InitEntryMainExitGraphWithReturnVoid();
+  HParameterValue* param = MakeParam(DataType::Type::kInt32);
+  HDivZeroCheck* instr = new (GetAllocator()) HDivZeroCheck(param, 0);
+  HPhi* phi = ConstructBasicGraphForSelect(return_block, instr);
+
+  ManuallyBuildEnvFor(instr, {param, graph_->GetIntConstant(1)});
+
+  EXPECT_FALSE(CheckGraphAndTryControlFlowSimplifier());
+  EXPECT_INS_RETAINED(phi);
+}
+
+// Test that ControlFlowSimplifier succeeds with HAdd.
+TEST_F(ControlFlowSimplifierTest, testSelectWithAdd) {
+  HBasicBlock* return_block = InitEntryMainExitGraphWithReturnVoid();
+  HParameterValue* param = MakeParam(DataType::Type::kInt32);
+  HAdd* instr = new (GetAllocator()) HAdd(DataType::Type::kInt32, param, param, /*dex_pc=*/ 0);
+  HPhi* phi = ConstructBasicGraphForSelect(return_block, instr);
+  EXPECT_TRUE(CheckGraphAndTryControlFlowSimplifier());
+  EXPECT_INS_REMOVED(phi);
+}
+
+// Test that ControlFlowSimplifier succeeds if there is an additional `HPhi` with identical inputs.
+TEST_F(ControlFlowSimplifierTest, testSelectWithAddAndExtraPhi) {
+  // Create a graph with three blocks merging to the `return_block`.
+  HBasicBlock* return_block = InitEntryMainExitGraphWithReturnVoid();
+  HParameterValue* bool_param1 = MakeParam(DataType::Type::kBool);
+  HParameterValue* bool_param2 = MakeParam(DataType::Type::kBool);
+  HParameterValue* param = MakeParam(DataType::Type::kInt32);
+  HInstruction* const0 = graph_->GetIntConstant(0);
+  auto [if_block1, left, mid] = CreateDiamondPattern(return_block, bool_param1);
+  HBasicBlock* if_block2 = AddNewBlock();
+  if_block1->ReplaceSuccessor(mid, if_block2);
+  HBasicBlock* right = AddNewBlock();
+  if_block2->AddSuccessor(mid);
+  if_block2->AddSuccessor(right);
+  HIf* if2 = MakeIf(if_block2, bool_param2);
+  right->AddSuccessor(return_block);
+  MakeGoto(right);
+  ASSERT_TRUE(PredecessorsEqual(return_block, {left, mid, right}));
+  HAdd* add = MakeBinOp<HAdd>(right, DataType::Type::kInt32, param, param);
+  HPhi* phi1 = MakePhi(return_block, {param, param, add});
+  HPhi* phi2 = MakePhi(return_block, {param, const0, const0});
+
+  // Prevent second `HSelect` match. Do not rely on the "instructions per branch" limit.
+  MakeInvokeStatic(left, DataType::Type::kVoid, {}, {});
+
+  EXPECT_TRUE(CheckGraphAndTryControlFlowSimplifier());
+
+  ASSERT_BLOCK_RETAINED(left);
+  ASSERT_BLOCK_REMOVED(mid);
+  ASSERT_BLOCK_REMOVED(right);
+  HInstruction* select = if2->GetPrevious();  // `HSelect` is inserted before `HIf`.
+  ASSERT_TRUE(select->IsSelect());
+  ASSERT_INS_RETAINED(phi1);
+  ASSERT_TRUE(InputsEqual(phi1, {param, select}));
+  ASSERT_INS_RETAINED(phi2);
+  ASSERT_TRUE(InputsEqual(phi2, {param, const0}));
+}
+
+// Test `HSelect` optimization in an irreducible loop.
+TEST_F(ControlFlowSimplifierTest, testSelectInIrreducibleLoop) {
+  HBasicBlock* return_block = InitEntryMainExitGraphWithReturnVoid();
+  auto [split, left_header, right_header, body] = CreateIrreducibleLoop(return_block);
+
+  HParameterValue* split_param = MakeParam(DataType::Type::kBool);
+  HParameterValue* bool_param = MakeParam(DataType::Type::kBool);
+  HParameterValue* n_param = MakeParam(DataType::Type::kInt32);
+
+  MakeIf(split, split_param);
+
+  HInstruction* const0 = graph_->GetIntConstant(0);
+  HInstruction* const1 = graph_->GetIntConstant(1);
+  auto [left_phi, right_phi, add] =
+      MakeLinearIrreducibleLoopVar(left_header, right_header, body, const1, const0, const1);
+  HCondition* condition = MakeCondition(left_header, kCondGE, left_phi, n_param);
+  MakeIf(left_header, condition);
+
+  auto [if_block, then_block, else_block] = CreateDiamondPattern(body, bool_param);
+  HPhi* phi = MakePhi(body, {const1, const0});
+
+  EXPECT_TRUE(CheckGraphAndTryControlFlowSimplifier());
+  HLoopInformation* loop_info = left_header->GetLoopInformation();
+  ASSERT_TRUE(loop_info != nullptr);
+  ASSERT_TRUE(loop_info->IsIrreducible());
+
+  EXPECT_INS_REMOVED(phi);
+  ASSERT_TRUE(if_block->GetFirstInstruction()->IsSelect());
+
+  ASSERT_EQ(if_block, add->GetBlock());  // Moved when merging blocks.
+
+  for (HBasicBlock* removed_block : {then_block, else_block, body}) {
+    ASSERT_BLOCK_REMOVED(removed_block);
+    uint32_t removed_block_id = removed_block->GetBlockId();
+    ASSERT_FALSE(loop_info->GetBlocks().IsBitSet(removed_block_id)) << removed_block_id;
+  }
+}
+
+}  // namespace art
diff --git a/compiler/optimizing/dead_code_elimination.cc b/compiler/optimizing/dead_code_elimination.cc
index f44f4b577b..c367a20a06 100644
--- a/compiler/optimizing/dead_code_elimination.cc
+++ b/compiler/optimizing/dead_code_elimination.cc
@@ -29,21 +29,21 @@
 
 namespace art HIDDEN {
 
-static void MarkReachableBlocks(HGraph* graph, ArenaBitVector* visited) {
+static void MarkReachableBlocks(HGraph* graph, BitVectorView<size_t> visited) {
   // Use local allocator for allocating memory.
   ScopedArenaAllocator allocator(graph->GetArenaStack());
 
   ScopedArenaVector<HBasicBlock*> worklist(allocator.Adapter(kArenaAllocDCE));
   constexpr size_t kDefaultWorlistSize = 8;
   worklist.reserve(kDefaultWorlistSize);
-  visited->SetBit(graph->GetEntryBlock()->GetBlockId());
+  visited.SetBit(graph->GetEntryBlock()->GetBlockId());
   worklist.push_back(graph->GetEntryBlock());
 
   while (!worklist.empty()) {
     HBasicBlock* block = worklist.back();
     worklist.pop_back();
     int block_id = block->GetBlockId();
-    DCHECK(visited->IsBitSet(block_id));
+    DCHECK(visited.IsBitSet(block_id));
 
     ArrayRef<HBasicBlock* const> live_successors(block->GetSuccessors());
     HInstruction* last_instruction = block->GetLastInstruction();
@@ -83,8 +83,8 @@ static void MarkReachableBlocks(HGraph* graph, ArenaBitVector* visited) {
 
     for (HBasicBlock* successor : live_successors) {
       // Add only those successors that have not been visited yet.
-      if (!visited->IsBitSet(successor->GetBlockId())) {
-        visited->SetBit(successor->GetBlockId());
+      if (!visited.IsBitSet(successor->GetBlockId())) {
+        visited.SetBit(successor->GetBlockId());
         worklist.push_back(successor);
       }
     }
@@ -201,7 +201,7 @@ static bool RemoveNonNullControlDependences(HBasicBlock* block, HBasicBlock* thr
           user_block != throws &&
           block->Dominates(user_block)) {
         if (bound == nullptr) {
-          bound = new (obj->GetBlock()->GetGraph()->GetAllocator()) HBoundType(obj);
+          bound = new (block->GetGraph()->GetAllocator()) HBoundType(obj);
           bound->SetUpperBound(ti, /*can_be_null*/ false);
           bound->SetReferenceTypeInfo(ti);
           bound->SetCanBeNull(false);
@@ -488,7 +488,8 @@ void HDeadCodeElimination::MaybeAddPhi(HBasicBlock* block) {
 
     if (block_cond->GetLeft() != dominator_cond->GetLeft() ||
         block_cond->GetRight() != dominator_cond->GetRight() ||
-        block_cond->GetOppositeCondition() != dominator_cond->GetCondition()) {
+        block_cond->GetOppositeCondition() != dominator_cond->GetCondition() ||
+        block_cond->GetBias() != dominator_cond->GetBias()) {
       return;
     }
   }
@@ -526,10 +527,10 @@ void HDeadCodeElimination::MaybeAddPhi(HBasicBlock* block) {
       //         |
       //         8
       // `7` (which would be `block` in this example), and `6` will come from both the true path and
-      // the false path of `1`. We bumped into something similar in SelectGenerator. See
-      // HSelectGenerator::TryFixupDoubleDiamondPattern.
+      // the false path of `1`. We bumped into something similar in `HControlFlowSimplifier`. See
+      // `HControlFlowSimplifier::TryFixupDoubleDiamondPattern()`.
       // TODO(solanes): Figure out if we can fix up the graph into a double diamond in a generic way
-      // so that DeadCodeElimination and SelectGenerator can take advantage of it.
+      // so that `HDeadCodeElimination` and `HControlFlowSimplifier` can take advantage of it.
 
       if (!same_input) {
         // `1` and `7` having the opposite condition is a case we are missing. We could potentially
@@ -590,14 +591,15 @@ void HDeadCodeElimination::ConnectSuccessiveBlocks() {
 
 struct HDeadCodeElimination::TryBelongingInformation {
   TryBelongingInformation(HGraph* graph, ScopedArenaAllocator* allocator)
-      : blocks_in_try(allocator, graph->GetBlocks().size(), /*expandable=*/false, kArenaAllocDCE),
-        coalesced_try_entries(
-            allocator, graph->GetBlocks().size(), /*expandable=*/false, kArenaAllocDCE) {}
+      : blocks_in_try(ArenaBitVector::CreateFixedSize(
+            allocator, graph->GetBlocks().size(), kArenaAllocDCE)),
+        coalesced_try_entries(ArenaBitVector::CreateFixedSize(
+            allocator, graph->GetBlocks().size(), kArenaAllocDCE)) {}
 
   // Which blocks belong in the try.
-  ArenaBitVector blocks_in_try;
+  BitVectorView<size_t> blocks_in_try;
   // Which other try entries are referencing this same try.
-  ArenaBitVector coalesced_try_entries;
+  BitVectorView<size_t> coalesced_try_entries;
 };
 
 bool HDeadCodeElimination::CanPerformTryRemoval(const TryBelongingInformation& try_belonging_info) {
@@ -724,7 +726,7 @@ bool HDeadCodeElimination::RemoveUnneededTries() {
       if (try_boundary->HasSameExceptionHandlersAs(*other_try_boundary)) {
         // Merge the entries as they are really the same one.
         // Block merging.
-        it->second.blocks_in_try.Union(&other_it->second.blocks_in_try);
+        it->second.blocks_in_try.Union(other_it->second.blocks_in_try);
 
         // Add the coalesced try entry to update it too.
         it->second.coalesced_try_entries.SetBit(other_block->GetBlockId());
@@ -798,8 +800,8 @@ bool HDeadCodeElimination::RemoveEmptyIfs() {
     //   5
     // where 2, 3, and 4 are single HGoto blocks, and block 5 has Phis.
     ScopedArenaAllocator allocator(graph_->GetArenaStack());
-    ArenaBitVector visited_blocks(
-        &allocator, graph_->GetBlocks().size(), /*expandable=*/ false, kArenaAllocDCE);
+    BitVectorView<size_t> visited_blocks =
+        ArenaBitVector::CreateFixedSize(&allocator, graph_->GetBlocks().size(), kArenaAllocDCE);
     HBasicBlock* merge_true = true_block;
     visited_blocks.SetBit(merge_true->GetBlockId());
     while (merge_true->IsSingleGoto()) {
@@ -821,8 +823,8 @@ bool HDeadCodeElimination::RemoveEmptyIfs() {
 
     // Data structures to help remove now-dead instructions.
     ScopedArenaQueue<HInstruction*> maybe_remove(allocator.Adapter(kArenaAllocDCE));
-    ArenaBitVector visited(
-        &allocator, graph_->GetCurrentInstructionId(), /*expandable=*/ false, kArenaAllocDCE);
+    BitVectorView<size_t> visited = ArenaBitVector::CreateFixedSize(
+        &allocator, graph_->GetCurrentInstructionId(), kArenaAllocDCE);
     maybe_remove.push(if_instr->InputAt(0));
     visited.SetBit(if_instr->GetId());
 
@@ -873,9 +875,10 @@ bool HDeadCodeElimination::RemoveDeadBlocks(bool force_recomputation,
   ScopedArenaAllocator allocator(graph_->GetArenaStack());
 
   // Classify blocks as reachable/unreachable.
-  ArenaBitVector live_blocks(&allocator, graph_->GetBlocks().size(), false, kArenaAllocDCE);
+  BitVectorView<size_t> live_blocks =
+      ArenaBitVector::CreateFixedSize(&allocator, graph_->GetBlocks().size(), kArenaAllocDCE);
 
-  MarkReachableBlocks(graph_, &live_blocks);
+  MarkReachableBlocks(graph_, live_blocks);
   bool removed_one_or_more_blocks = false;
   bool rerun_dominance_and_loop_analysis = false;
 
diff --git a/compiler/optimizing/fast_compiler.h b/compiler/optimizing/fast_compiler.h
new file mode 100644
index 0000000000..f9a9f58146
--- /dev/null
+++ b/compiler/optimizing/fast_compiler.h
@@ -0,0 +1,97 @@
+/*
+ * Copyright (C) 2023 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_COMPILER_OPTIMIZING_FAST_COMPILER_H_
+#define ART_COMPILER_OPTIMIZING_FAST_COMPILER_H_
+
+#include <memory>
+#include <vector>
+
+#include "arch/instruction_set.h"
+#include "base/array_ref.h"
+#include "base/macros.h"
+#include "base/scoped_arena_containers.h"
+#include "driver/compiler_options.h"
+#include "handle_scope.h"
+
+namespace art HIDDEN {
+
+class ArenaAllocator;
+class ArtMethod;
+class DexCompilationUnit;
+class VariableSizedHandleScope;
+
+namespace mirror {
+class Object;
+}
+
+/**
+ * A lightweight, one-pass compiler. Goes over each dex instruction and emits
+ * native code for it.
+ */
+class FastCompiler {
+ public:
+  static std::unique_ptr<FastCompiler> Compile(
+      ArtMethod* method,
+      [[maybe_unused]] ArenaAllocator* allocator,
+      [[maybe_unused]] ArenaStack* arena_stack,
+      [[maybe_unused]] VariableSizedHandleScope* handles,
+      [[maybe_unused]] const CompilerOptions& compiler_options,
+      [[maybe_unused]] const DexCompilationUnit& dex_compilation_unit) {
+    if (method == nullptr) {
+      return nullptr;
+    }
+    switch (compiler_options.GetInstructionSet()) {
+#ifdef ART_ENABLE_CODEGEN_arm64
+      case InstructionSet::kArm64:
+        return CompileARM64(method,
+                            allocator,
+                            arena_stack,
+                            handles,
+                            compiler_options,
+                            dex_compilation_unit);
+#endif
+      default:
+        return nullptr;
+    }
+  }
+
+  virtual ArrayRef<const uint8_t> GetCode() const = 0;
+  virtual ScopedArenaVector<uint8_t> BuildStackMaps() const = 0;
+  virtual ArrayRef<const uint8_t> GetCfiData() const = 0;
+  virtual int32_t GetFrameSize() const = 0;
+  virtual uint32_t GetNumberOfJitRoots() const = 0;
+  virtual void EmitJitRoots(uint8_t* code,
+                            const uint8_t* roots_data,
+                            /*out*/std::vector<Handle<mirror::Object>>* roots)
+      REQUIRES_SHARED(Locks::mutator_lock_) = 0;
+
+  virtual ~FastCompiler() = default;
+
+ private:
+#ifdef ART_ENABLE_CODEGEN_arm64
+  static std::unique_ptr<FastCompiler> CompileARM64(ArtMethod* method,
+                                                    ArenaAllocator* allocator,
+                                                    ArenaStack* arena_stack,
+                                                    VariableSizedHandleScope* handles,
+                                                    const CompilerOptions& compiler_options,
+                                                    const DexCompilationUnit& dex_compilation_unit);
+#endif
+};
+
+}  // namespace art
+
+#endif  // ART_COMPILER_OPTIMIZING_FAST_COMPILER_H_
diff --git a/compiler/optimizing/fast_compiler_arm64.cc b/compiler/optimizing/fast_compiler_arm64.cc
new file mode 100644
index 0000000000..a820fc9422
--- /dev/null
+++ b/compiler/optimizing/fast_compiler_arm64.cc
@@ -0,0 +1,2303 @@
+/*
+ * Copyright (C) 2023 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "fast_compiler.h"
+
+// TODO(VIXL): Make VIXL compile cleanly with -Wshadow, -Wdeprecated-declarations.
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wshadow"
+#pragma GCC diagnostic ignored "-Wdeprecated-declarations"
+#include "aarch64/disasm-aarch64.h"
+#include "aarch64/macro-assembler-aarch64.h"
+#include "aarch64/disasm-aarch64.h"
+#pragma GCC diagnostic pop
+
+#include "code_generation_data.h"
+#include "code_generator_arm64.h"
+#include "data_type-inl.h"
+#include "dex/code_item_accessors-inl.h"
+#include "dex/dex_instruction-inl.h"
+#include "driver/dex_compilation_unit.h"
+#include "entrypoints/entrypoint_utils-inl.h"
+#include "jit_patches_arm64.h"
+#include "nodes.h"
+#include "thread-inl.h"
+#include "utils/arm64/assembler_arm64.h"
+
+
+using namespace vixl::aarch64;  // NOLINT(build/namespaces)
+using vixl::ExactAssemblyScope;
+using vixl::CodeBufferCheckScope;
+using vixl::EmissionCheckScope;
+
+#ifdef __
+#error "ARM64 Codegen VIXL macro-assembler macro already defined."
+#endif
+#define __ GetVIXLAssembler()->
+
+namespace art HIDDEN {
+namespace arm64 {
+
+using helpers::CPURegisterFrom;
+using helpers::HeapOperand;
+using helpers::LocationFrom;
+using helpers::RegisterFrom;
+using helpers::WRegisterFrom;
+using helpers::DRegisterFrom;
+using helpers::SRegisterFrom;
+
+static const vixl::aarch64::Register kAvailableCalleeSaveRegisters[] = {
+  vixl::aarch64::x22,
+  vixl::aarch64::x23,
+  vixl::aarch64::x24,
+  vixl::aarch64::x25,
+  vixl::aarch64::x26,
+  vixl::aarch64::x27,
+  vixl::aarch64::x28,
+  vixl::aarch64::x29,
+};
+
+static const vixl::aarch64::Register kAvailableTempRegisters[] = {
+  vixl::aarch64::x8,
+  vixl::aarch64::x9,
+  vixl::aarch64::x10,
+  vixl::aarch64::x11,
+  vixl::aarch64::x12,
+  vixl::aarch64::x13,
+  vixl::aarch64::x14,
+  vixl::aarch64::x15,
+};
+
+static const vixl::aarch64::VRegister kAvailableCalleeSaveFpuRegisters[] = {
+  vixl::aarch64::d8,
+  vixl::aarch64::d9,
+  vixl::aarch64::d10,
+  vixl::aarch64::d11,
+  vixl::aarch64::d12,
+  vixl::aarch64::d13,
+  vixl::aarch64::d14,
+  vixl::aarch64::d15,
+};
+
+static const vixl::aarch64::VRegister kAvailableTempFpuRegisters[] = {
+  vixl::aarch64::d0,
+  vixl::aarch64::d1,
+  vixl::aarch64::d2,
+  vixl::aarch64::d3,
+  vixl::aarch64::d4,
+  vixl::aarch64::d5,
+  vixl::aarch64::d6,
+  vixl::aarch64::d7,
+};
+
+class FastCompilerARM64 : public FastCompiler {
+ public:
+  FastCompilerARM64(ArtMethod* method,
+                    ArenaAllocator* allocator,
+                    ArenaStack* arena_stack,
+                    VariableSizedHandleScope* handles,
+                    const CompilerOptions& compiler_options,
+                    const DexCompilationUnit& dex_compilation_unit)
+      : method_(method),
+        allocator_(allocator),
+        handles_(handles),
+        assembler_(allocator,
+                   compiler_options.GetInstructionSetFeatures()->AsArm64InstructionSetFeatures()),
+        jit_patches_(&assembler_, allocator),
+        compiler_options_(compiler_options),
+        dex_compilation_unit_(dex_compilation_unit),
+        code_generation_data_(CodeGenerationData::Create(arena_stack, InstructionSet::kArm64)),
+        vreg_locations_(dex_compilation_unit.GetCodeItemAccessor().RegistersSize(),
+                        allocator->Adapter()),
+        branch_targets_(dex_compilation_unit.GetCodeItemAccessor().InsnsSizeInCodeUnits(),
+                        allocator->Adapter()),
+        object_register_masks_(dex_compilation_unit.GetCodeItemAccessor().InsnsSizeInCodeUnits(),
+                               allocator->Adapter()),
+        is_non_null_masks_(dex_compilation_unit.GetCodeItemAccessor().InsnsSizeInCodeUnits(),
+                           allocator->Adapter()),
+        has_frame_(false),
+        core_spill_mask_(0u),
+        fpu_spill_mask_(0u),
+        object_register_mask_(0u),
+        is_non_null_mask_(0u) {
+    memset(is_non_null_masks_.data(), ~0, is_non_null_masks_.size() * sizeof(uint64_t));
+    memset(object_register_masks_.data(), ~0, object_register_masks_.size() * sizeof(uint64_t));
+    GetAssembler()->cfi().SetEnabled(compiler_options.GenerateAnyDebugInfo());
+  }
+
+  // Top-level method to generate code for `method_`.
+  bool Compile();
+
+  ArrayRef<const uint8_t> GetCode() const override {
+    return ArrayRef<const uint8_t>(assembler_.CodeBufferBaseAddress(), assembler_.CodeSize());
+  }
+
+  ScopedArenaVector<uint8_t> BuildStackMaps() const override {
+    return code_generation_data_->GetStackMapStream()->Encode();
+  }
+
+  ArrayRef<const uint8_t> GetCfiData() const override {
+    return ArrayRef<const uint8_t>(*assembler_.cfi().data());
+  }
+
+  int32_t GetFrameSize() const override {
+    if (!has_frame_) {
+      return 0;
+    }
+    size_t size = FrameEntrySpillSize() +
+        /* method */ static_cast<size_t>(kArm64PointerSize) +
+        /* out registers */ GetCodeItemAccessor().OutsSize() * kVRegSize;
+    return RoundUp(size, kStackAlignment);
+  }
+
+  uint32_t GetNumberOfJitRoots() const override {
+    return code_generation_data_->GetNumberOfJitRoots();
+  }
+
+  void EmitJitRoots(uint8_t* code,
+                    const uint8_t* roots_data,
+                    /*out*/std::vector<Handle<mirror::Object>>* roots) override
+       REQUIRES_SHARED(Locks::mutator_lock_) {
+    code_generation_data_->EmitJitRoots(roots);
+    jit_patches_.EmitJitRootPatches(code, roots_data, *code_generation_data_);
+  }
+
+  ~FastCompilerARM64() override {
+    GetVIXLAssembler()->Reset();
+  }
+
+  const char* GetUnimplementedReason() const {
+    return unimplemented_reason_;
+  }
+
+ private:
+  // Go over each instruction of the method, and generate code for them.
+  bool ProcessInstructions();
+
+  // Initialize the locations of parameters for this method.
+  bool InitializeParameters();
+
+  // Generate code for the frame entry. Only called when needed. If the frame
+  // entry has already been generated, do nothing.
+  bool EnsureHasFrame();
+
+  // Generate code for a frame exit.
+  void PopFrameAndReturn();
+
+  // Record a stack map at the given dex_pc.
+  void RecordPcInfo(uint32_t dex_pc);
+
+  // Generate code to move from one location to another.
+  bool MoveLocation(Location destination, Location source, DataType::Type dst_type);
+
+  // Get a register location for the dex register `reg`. Saves the location into
+  // `vreg_locations_` for next uses of `reg`.
+  // `next` should be the next dex instruction, to help choose the register.
+  Location CreateNewRegisterLocation(uint32_t reg, DataType::Type type, const Instruction* next);
+
+  // Return the existing register location for `reg`.
+  Location GetExistingRegisterLocation(uint32_t reg, DataType::Type type);
+
+  // Move dex registers holding constants into physical registers. Used when
+  // branching.
+  void MoveConstantsToRegisters();
+
+  // Update the masks associated to the given dex_pc. Used when dex_pc is a
+  // branch target.
+  void UpdateMasks(uint32_t dex_pc);
+
+  // Generate code for one instruction.
+  bool ProcessDexInstruction(const Instruction& instruction,
+                             uint32_t dex_pc,
+                             const Instruction* next);
+
+  // Setup the arguments for an invoke.
+  bool SetupArguments(InvokeType invoke_type,
+                      const InstructionOperands& operands,
+                      const char* shorty,
+                      /* out */ uint32_t* obj_reg);
+
+  // Generate code for doing a Java invoke.
+  bool HandleInvoke(const Instruction& instruction, uint32_t dex_pc, InvokeType invoke_type);
+
+  // Generate code for IF_* instructions.
+  template<vixl::aarch64::Condition kCond, bool kCompareWithZero>
+  bool If_21_22t(const Instruction& instruction, uint32_t dex_pc);
+
+  // Generate code for doing a runtime invoke.
+  void InvokeRuntime(QuickEntrypointEnum entrypoint, uint32_t dex_pc);
+
+  bool BuildLoadString(uint32_t vreg, dex::StringIndex string_index, const Instruction* next);
+  bool BuildNewInstance(
+      uint32_t vreg, dex::TypeIndex string_index, uint32_t dex_pc, const Instruction* next);
+  bool BuildCheckCast(uint32_t vreg, dex::TypeIndex type_index, uint32_t dex_pc);
+  bool LoadMethod(Register reg, ArtMethod* method);
+  void DoReadBarrierOn(Register reg, vixl::aarch64::Label* exit = nullptr, bool do_mr_check = true);
+  bool CanGenerateCodeFor(ArtField* field, bool can_receiver_be_null)
+      REQUIRES_SHARED(Locks::mutator_lock_);
+
+  // Mark whether dex register `vreg_index` is an object.
+  void UpdateRegisterMask(uint32_t vreg_index, bool is_object) {
+    // Note that the register mask is only useful when there is a frame, so we
+    // use the callee save registers for the mask.
+    if (is_object) {
+      object_register_mask_ |= (1 << kAvailableCalleeSaveRegisters[vreg_index].GetCode());
+    } else {
+      object_register_mask_ &= ~(1 << kAvailableCalleeSaveRegisters[vreg_index].GetCode());
+    }
+  }
+
+  // Mark whether dex register `vreg_index` can be null.
+  void UpdateNonNullMask(uint32_t vreg_index, bool can_be_null) {
+    if (can_be_null) {
+      is_non_null_mask_ &= ~(1 << vreg_index);
+    } else {
+      is_non_null_mask_ |= (1 << vreg_index);
+    }
+  }
+
+  // Update information about dex register `vreg_index`.
+  void UpdateLocal(uint32_t vreg_index, bool is_object, bool can_be_null = true) {
+    UpdateRegisterMask(vreg_index, is_object);
+    UpdateNonNullMask(vreg_index, can_be_null);
+  }
+
+  // Whether dex register `vreg_index` can be null.
+  bool CanBeNull(uint32_t vreg_index) const {
+    return (is_non_null_mask_ & (1 << vreg_index)) == 0;
+  }
+
+  // Get the label associated with the given `dex_pc`.
+  vixl::aarch64::Label* GetLabelOf(uint32_t dex_pc) {
+    return &branch_targets_[dex_pc];
+  }
+
+  // If we need to abort compilation, clear branch targets, required by vixl.
+  void AbortCompilation() {
+    for (vixl::aarch64::Label& label : branch_targets_) {
+      if (label.IsLinked()) {
+        __ Bind(&label);
+      }
+    }
+  }
+
+
+  // Compiler utilities.
+  //
+  Arm64Assembler* GetAssembler() { return &assembler_; }
+  vixl::aarch64::MacroAssembler* GetVIXLAssembler() { return GetAssembler()->GetVIXLAssembler(); }
+  const DexFile& GetDexFile() const { return *dex_compilation_unit_.GetDexFile(); }
+  const CodeItemDataAccessor& GetCodeItemAccessor() const {
+    return dex_compilation_unit_.GetCodeItemAccessor();
+  }
+  bool HitUnimplemented() const {
+    return unimplemented_reason_ != nullptr;
+  }
+
+  // Frame related utilities.
+  //
+  uint32_t GetCoreSpillSize() const {
+    return GetFramePreservedCoreRegisters().GetTotalSizeInBytes();
+  }
+  uint32_t FrameEntrySpillSize() const {
+    return GetFramePreservedFPRegisters().GetTotalSizeInBytes() + GetCoreSpillSize();
+  }
+  CPURegList GetFramePreservedCoreRegisters() const {
+    return CPURegList(CPURegister::kRegister, kXRegSize, core_spill_mask_);
+  }
+  CPURegList GetFramePreservedFPRegisters() const {
+    return CPURegList(CPURegister::kVRegister, kDRegSize, fpu_spill_mask_);
+  }
+
+  // Method being compiled.
+  ArtMethod* method_;
+
+  // Allocator for any allocation happening in the compiler.
+  ArenaAllocator* allocator_;
+
+  VariableSizedHandleScope* handles_;
+
+  // Compilation utilities.
+  Arm64Assembler assembler_;
+  JitPatchesARM64 jit_patches_;
+  const CompilerOptions& compiler_options_;
+  const DexCompilationUnit& dex_compilation_unit_;
+  std::unique_ptr<CodeGenerationData> code_generation_data_;
+
+  // The current location of each dex register.
+  ArenaVector<Location> vreg_locations_;
+
+  // A vector of size code units for dex pcs that are branch targets.
+  ArenaVector<vixl::aarch64::Label> branch_targets_;
+
+  // For dex pcs that are branch targets, the register mask that will be used at
+  // the point of that pc.
+  ArenaVector<uint64_t> object_register_masks_;
+
+  // For dex pcs that are branch targets, the mask for non-null objects that will
+  // be used at the point of that pc.
+  ArenaVector<uint64_t> is_non_null_masks_;
+
+  // Whether we've created a frame for this compiled method.
+  bool has_frame_;
+
+  // CPU registers that have been spilled in the frame.
+  uint32_t core_spill_mask_;
+
+  // FPU registers that have been spilled in the frame.
+  uint32_t fpu_spill_mask_;
+
+  // The current mask to know which physical register holds an object.
+  uint64_t object_register_mask_;
+
+  // The current mask to know if a dex register is known non-null.
+  uint64_t is_non_null_mask_;
+
+  // The return type of the compiled method. Saved to avoid re-computing it on
+  // the return instruction.
+  DataType::Type return_type_;
+
+  // The return type of the last invoke instruction.
+  DataType::Type previous_invoke_return_type_;
+
+  // If non-empty, the reason the compilation could not be finished.
+  const char* unimplemented_reason_ = nullptr;
+};
+
+bool FastCompilerARM64::InitializeParameters() {
+  if (GetCodeItemAccessor().TriesSize() != 0) {
+    // TODO: Support try/catch.
+    unimplemented_reason_ = "TryCatch";
+    return false;
+  }
+  const char* shorty = dex_compilation_unit_.GetShorty();
+  uint16_t number_of_vregs = GetCodeItemAccessor().RegistersSize();
+  uint16_t number_of_parameters = GetCodeItemAccessor().InsSize();
+  uint16_t vreg_parameter_index = number_of_vregs - number_of_parameters;
+
+  if (number_of_vregs > arraysize(kAvailableTempRegisters) ||
+      number_of_vregs > arraysize(kAvailableCalleeSaveRegisters) ||
+      number_of_vregs > arraysize(kAvailableTempFpuRegisters) ||
+      number_of_vregs > arraysize(kAvailableCalleeSaveFpuRegisters)) {
+    // Too many registers for this compiler.
+    unimplemented_reason_ = "TooManyRegisters";
+    return false;
+  }
+
+  InvokeDexCallingConventionVisitorARM64 convention;
+  if (!dex_compilation_unit_.IsStatic()) {
+    // Add the implicit 'this' argument, not expressed in the signature.
+    vreg_locations_[vreg_parameter_index] = convention.GetNextLocation(DataType::Type::kReference);
+    UpdateLocal(vreg_parameter_index, /* is_object= */ true, /* can_be_null= */ false);
+    ++vreg_parameter_index;
+    --number_of_parameters;
+  }
+
+  for (int i = 0, shorty_pos = 1;
+       i < number_of_parameters;
+       i++, shorty_pos++, vreg_parameter_index++) {
+    DataType::Type type = DataType::FromShorty(shorty[shorty_pos]);
+    vreg_locations_[vreg_parameter_index] = convention.GetNextLocation(type);
+    UpdateLocal(vreg_parameter_index,
+                /* is_object= */ (type == DataType::Type::kReference),
+                /* can_be_null= */ true);
+    if (DataType::Is64BitType(type)) {
+      ++i;
+      ++vreg_parameter_index;
+    }
+  }
+  return_type_ = DataType::FromShorty(shorty[0]);
+  return true;
+}
+
+void FastCompilerARM64::MoveConstantsToRegisters() {
+  for (uint32_t i = 0; i < vreg_locations_.size(); ++i) {
+    Location location  = vreg_locations_[i];
+    if (location.IsConstant()) {
+      vreg_locations_[i] =
+          CreateNewRegisterLocation(i, DataType::Type::kInt32, /* next= */ nullptr);
+      MoveLocation(vreg_locations_[i], location, DataType::Type::kInt32);
+      DCHECK(!HitUnimplemented());
+    }
+  }
+}
+
+void FastCompilerARM64::UpdateMasks(uint32_t dex_pc) {
+  object_register_masks_[dex_pc] &= object_register_mask_;
+  is_non_null_masks_[dex_pc] &= is_non_null_mask_;
+}
+
+bool FastCompilerARM64::ProcessInstructions() {
+  DCHECK(GetCodeItemAccessor().HasCodeItem());
+
+  DexInstructionIterator it = GetCodeItemAccessor().begin();
+  DexInstructionIterator end = GetCodeItemAccessor().end();
+  DCHECK(it != end);
+  do {
+    DexInstructionPcPair pair = *it;
+    ++it;
+
+    // Fetch the next instruction as a micro-optimization currently only used
+    // for optimizing returns.
+    const Instruction* next = nullptr;
+    if (it != end) {
+      const DexInstructionPcPair& next_pair = *it;
+      next = &next_pair.Inst();
+      if (GetLabelOf(next_pair.DexPc())->IsLinked()) {
+        // Disable the micro-optimization, as the next instruction is a branch
+        // target.
+        next = nullptr;
+      }
+    }
+    vixl::aarch64::Label* label = GetLabelOf(pair.DexPc());
+    if (label->IsLinked()) {
+      // Emulate a branch to this pc.
+      MoveConstantsToRegisters();
+      UpdateMasks(pair.DexPc());
+      // Set new masks based on all incoming edges.
+      is_non_null_mask_ = is_non_null_masks_[pair.DexPc()];
+      object_register_mask_ = object_register_masks_[pair.DexPc()];
+      __ Bind(label);
+    }
+
+    if (!ProcessDexInstruction(pair.Inst(), pair.DexPc(), next)) {
+      DCHECK(HitUnimplemented());
+      return false;
+    }
+    // Note: There may be no Thread for gtests.
+    DCHECK(Thread::Current() == nullptr || !Thread::Current()->IsExceptionPending())
+        << GetDexFile().PrettyMethod(dex_compilation_unit_.GetDexMethodIndex())
+        << " " << pair.Inst().Name() << "@" << pair.DexPc();
+
+    DCHECK(!HitUnimplemented()) << GetUnimplementedReason();
+  } while (it != end);
+  return true;
+}
+
+bool FastCompilerARM64::MoveLocation(Location destination,
+                                     Location source,
+                                     DataType::Type dst_type) {
+  if (source.Equals(destination)) {
+    return true;
+  }
+  if (source.IsRegister() && destination.IsRegister()) {
+    CPURegister dst = CPURegisterFrom(destination, dst_type);
+    __ Mov(Register(dst), RegisterFrom(source, dst_type));
+    return true;
+  }
+  if (source.IsConstant() && destination.IsRegister()) {
+    if (source.GetConstant()->IsIntConstant()) {
+      __ Mov(RegisterFrom(destination, DataType::Type::kInt32),
+             source.GetConstant()->AsIntConstant()->GetValue());
+      return true;
+    }
+  }
+  unimplemented_reason_ = "MoveLocation";
+  return false;
+}
+
+Location FastCompilerARM64::CreateNewRegisterLocation(uint32_t reg,
+                                                      DataType::Type type,
+                                                      const Instruction* next) {
+  if (next != nullptr &&
+      (next->Opcode() == Instruction::RETURN_OBJECT || next->Opcode() == Instruction::RETURN) &&
+      (next->VRegA_11x() == reg)) {
+    // If the next instruction is a return, use the return register from the calling
+    // convention.
+    InvokeDexCallingConventionVisitorARM64 convention;
+    vreg_locations_[reg] = convention.GetReturnLocation(return_type_);
+    return vreg_locations_[reg];
+  } else if (vreg_locations_[reg].IsStackSlot() ||
+             vreg_locations_[reg].IsDoubleStackSlot()) {
+    unimplemented_reason_ = "MoveStackSlot";
+    // Return a phony location.
+    return DataType::IsFloatingPointType(type)
+        ? Location::FpuRegisterLocation(1)
+        : Location::RegisterLocation(1);
+  } else if (DataType::IsFloatingPointType(type)) {
+    if (vreg_locations_[reg].IsFpuRegister()) {
+      // Re-use existing register.
+      return vreg_locations_[reg];
+    } else if (has_frame_) {
+      // TODO: Regenerate the method with floating point support.
+      unimplemented_reason_ = "FpuRegisterAllocation";
+      vreg_locations_[reg] = Location::FpuRegisterLocation(1);
+      return vreg_locations_[reg];
+    } else {
+      vreg_locations_[reg] =
+          Location::FpuRegisterLocation(kAvailableTempFpuRegisters[reg].GetCode());
+      return vreg_locations_[reg];
+    }
+  } else if (vreg_locations_[reg].IsRegister()) {
+    // Re-use existing register.
+    return vreg_locations_[reg];
+  } else {
+    // Get the associated register with `reg`.
+    uint32_t register_code = has_frame_
+        ? kAvailableCalleeSaveRegisters[reg].GetCode()
+        : kAvailableTempRegisters[reg].GetCode();
+    vreg_locations_[reg] = Location::RegisterLocation(register_code);
+    return vreg_locations_[reg];
+  }
+}
+
+Location FastCompilerARM64::GetExistingRegisterLocation(uint32_t reg, DataType::Type type) {
+  if (vreg_locations_[reg].IsStackSlot() || vreg_locations_[reg].IsDoubleStackSlot()) {
+    unimplemented_reason_ = "MoveStackSlot";
+    // Return a phony location.
+    return DataType::IsFloatingPointType(type)
+        ? Location::FpuRegisterLocation(1)
+        : Location::RegisterLocation(1);
+  } else if (DataType::IsFloatingPointType(type)) {
+    if (vreg_locations_[reg].IsFpuRegister()) {
+      return vreg_locations_[reg];
+    } else {
+      // TODO: Regenerate the method with floating point support.
+      unimplemented_reason_ = "FpuRegisterAllocation";
+      vreg_locations_[reg] = Location::FpuRegisterLocation(1);
+      return vreg_locations_[reg];
+    }
+  } else if (vreg_locations_[reg].IsRegister()) {
+    return vreg_locations_[reg];
+  } else {
+    unimplemented_reason_ = "UnknownLocation";
+    vreg_locations_[reg] = Location::RegisterLocation(1);
+    return Location::RegisterLocation(1);
+  }
+}
+
+void FastCompilerARM64::RecordPcInfo(uint32_t dex_pc) {
+  DCHECK(has_frame_);
+  uint32_t native_pc = GetAssembler()->CodePosition();
+  StackMapStream* stack_map_stream = code_generation_data_->GetStackMapStream();
+  CHECK_EQ(object_register_mask_ & callee_saved_core_registers.GetList(), object_register_mask_);
+  stack_map_stream->BeginStackMapEntry(dex_pc, native_pc, object_register_mask_);
+  stack_map_stream->EndStackMapEntry();
+}
+
+void FastCompilerARM64::PopFrameAndReturn() {
+  if (has_frame_) {
+    CodeGeneratorARM64::PopFrameAndReturn(GetAssembler(),
+                                          GetFrameSize(),
+                                          GetFramePreservedCoreRegisters(),
+                                          GetFramePreservedFPRegisters());
+  } else {
+    DCHECK_EQ(GetFrameSize(), 0);
+    __ Ret();
+  }
+}
+
+bool FastCompilerARM64::EnsureHasFrame() {
+  if (has_frame_) {
+    // Frame entry has already been generated.
+    return true;
+  }
+  has_frame_ = true;
+  uint16_t number_of_vregs = GetCodeItemAccessor().RegistersSize();
+  for (int i = 0; i < number_of_vregs; ++i) {
+    // Assume any vreg will be held in a callee-save register.
+    core_spill_mask_ |= (1 << kAvailableCalleeSaveRegisters[i].GetCode());
+    if (vreg_locations_[i].IsFpuRegister()) {
+      // TODO: Re-generate method with floating points.
+      unimplemented_reason_ = "FloatingPoint";
+      return false;
+    }
+  }
+  core_spill_mask_ |= (1 << lr.GetCode());
+
+  code_generation_data_->GetStackMapStream()->BeginMethod(GetFrameSize(),
+                                                          core_spill_mask_,
+                                                          fpu_spill_mask_,
+                                                          GetCodeItemAccessor().RegistersSize(),
+                                                          /* is_compiling_baseline= */ true,
+                                                          /* is_debuggable= */ false);
+  MacroAssembler* masm = GetVIXLAssembler();
+  {
+    UseScratchRegisterScope temps(masm);
+    Register temp = temps.AcquireX();
+    __ Sub(temp, sp, static_cast<int32_t>(GetStackOverflowReservedBytes(InstructionSet::kArm64)));
+    // Ensure that between load and RecordPcInfo there are no pools emitted.
+    ExactAssemblyScope eas(GetVIXLAssembler(),
+                           kInstructionSize,
+                           CodeBufferCheckScope::kExactSize);
+    __ ldr(wzr, MemOperand(temp, 0));
+    RecordPcInfo(0);
+  }
+
+  // Stack layout:
+  //      sp[frame_size - 8]        : lr.
+  //      ...                       : other preserved core registers.
+  //      ...                       : other preserved fp registers.
+  //      ...                       : reserved frame space.
+  //      sp[0]                     : current method.
+  int32_t frame_size = GetFrameSize();
+  uint32_t core_spills_offset = frame_size - GetCoreSpillSize();
+  CPURegList preserved_core_registers = GetFramePreservedCoreRegisters();
+  DCHECK(!preserved_core_registers.IsEmpty());
+  uint32_t fp_spills_offset = frame_size - FrameEntrySpillSize();
+  CPURegList preserved_fp_registers = GetFramePreservedFPRegisters();
+
+  // Save the current method if we need it, or if using STP reduces code
+  // size. Note that we do not do this in HCurrentMethod, as the
+  // instruction might have been removed in the SSA graph.
+  CPURegister lowest_spill;
+  if (core_spills_offset == kXRegSizeInBytes) {
+    // If there is no gap between the method and the lowest core spill, use
+    // aligned STP pre-index to store both. Max difference is 512. We do
+    // that to reduce code size even if we do not have to save the method.
+    DCHECK_LE(frame_size, 512);  // 32 core registers are only 256 bytes.
+    lowest_spill = preserved_core_registers.PopLowestIndex();
+    __ Stp(kArtMethodRegister, lowest_spill, MemOperand(sp, -frame_size, PreIndex));
+  } else {
+    __ Str(kArtMethodRegister, MemOperand(sp, -frame_size, PreIndex));
+  }
+  GetAssembler()->cfi().AdjustCFAOffset(frame_size);
+  if (lowest_spill.IsValid()) {
+    GetAssembler()->cfi().RelOffset(DWARFReg(lowest_spill), core_spills_offset);
+    core_spills_offset += kXRegSizeInBytes;
+  }
+  GetAssembler()->SpillRegisters(preserved_core_registers, core_spills_offset);
+  GetAssembler()->SpillRegisters(preserved_fp_registers, fp_spills_offset);
+
+  // Move registers which are currently allocated from caller-saves to callee-saves.
+  for (int i = 0; i < number_of_vregs; ++i) {
+    if (vreg_locations_[i].IsRegister()) {
+      Location new_location =
+          Location::RegisterLocation(kAvailableCalleeSaveRegisters[i].GetCode());
+      if (!MoveLocation(new_location, vreg_locations_[i], DataType::Type::kInt64)) {
+        return false;
+      }
+      vreg_locations_[i] = new_location;
+    } else if (vreg_locations_[i].IsFpuRegister()) {
+      Location new_location =
+          Location::FpuRegisterLocation(kAvailableCalleeSaveFpuRegisters[i].GetCode());
+      if (!MoveLocation(new_location, vreg_locations_[i], DataType::Type::kFloat64)) {
+        return false;
+      }
+      vreg_locations_[i] = new_location;
+    }
+  }
+
+  // Increment hotness. We use the ArtMethod's counter as we're not allocating a
+  // `ProfilingInfo` object in the fast baseline compiler.
+  if (!Runtime::Current()->IsAotCompiler()) {
+    uint64_t address = reinterpret_cast64<uint64_t>(method_);
+    UseScratchRegisterScope temps(masm);
+    Register counter = temps.AcquireW();
+    vixl::aarch64::Label increment, done;
+    uint32_t entrypoint_offset =
+        GetThreadOffset<kArm64PointerSize>(kQuickCompileOptimized).Int32Value();
+
+    __ Ldrh(counter, MemOperand(kArtMethodRegister, ArtMethod::HotnessCountOffset().Int32Value()));
+    __ Cbnz(counter, &increment);
+    __ Ldr(lr, MemOperand(tr, entrypoint_offset));
+    // Note: we don't record the call here (and therefore don't generate a stack
+    // map), as the entrypoint should never be suspended.
+    __ Blr(lr);
+    __ Bind(&increment);
+    __ Add(counter, counter, -1);
+    __ Strh(counter, MemOperand(kArtMethodRegister, ArtMethod::HotnessCountOffset().Int32Value()));
+    __ Bind(&done);
+  }
+
+  // Do the suspend check.
+  if (compiler_options_.GetImplicitSuspendChecks()) {
+    ExactAssemblyScope eas(GetVIXLAssembler(),
+                           kInstructionSize,
+                           CodeBufferCheckScope::kExactSize);
+    __ ldr(kImplicitSuspendCheckRegister, MemOperand(kImplicitSuspendCheckRegister));
+    RecordPcInfo(0);
+  } else {
+    UseScratchRegisterScope temps(masm);
+    Register temp = temps.AcquireW();
+    vixl::aarch64::Label continue_label;
+    __ Ldr(temp, MemOperand(tr, Thread::ThreadFlagsOffset<kArm64PointerSize>().SizeValue()));
+    __ Tst(temp, Thread::SuspendOrCheckpointRequestFlags());
+    __ B(eq, &continue_label);
+    uint32_t entrypoint_offset =
+        GetThreadOffset<kArm64PointerSize>(kQuickTestSuspend).Int32Value();
+    __ Ldr(lr, MemOperand(tr, entrypoint_offset));
+    {
+      ExactAssemblyScope eas(GetVIXLAssembler(),
+                             kInstructionSize,
+                             CodeBufferCheckScope::kExactSize);
+      __ blr(lr);
+      RecordPcInfo(0);
+    }
+    __ Bind(&continue_label);
+  }
+  return true;
+}
+
+
+bool FastCompilerARM64::SetupArguments(InvokeType invoke_type,
+                                       const InstructionOperands& operands,
+                                       const char* shorty,
+                                       /* out */ uint32_t* obj_reg) {
+  const size_t number_of_operands = operands.GetNumberOfOperands();
+
+  size_t start_index = 0u;
+  size_t argument_index = 0u;
+  InvokeDexCallingConventionVisitorARM64 convention;
+
+  // Handle 'this' parameter.
+  if (invoke_type != kStatic) {
+    if (number_of_operands == 0u) {
+      unimplemented_reason_ = "BogusSignature";
+      return false;
+    }
+    start_index = 1u;
+    *obj_reg = operands.GetOperand(0u);
+    if (!MoveLocation(convention.GetNextLocation(DataType::Type::kReference),
+                      vreg_locations_[*obj_reg],
+                      DataType::Type::kReference)) {
+      return false;
+    }
+  }
+
+  uint32_t shorty_index = 1;  // Skip the return type.
+  // Handle all parameters except 'this'.
+  for (size_t i = start_index; i < number_of_operands; ++i, ++argument_index, ++shorty_index) {
+    // Make sure we don't go over the expected arguments or over the number of
+    // dex registers given. If the instruction was seen as dead by the verifier,
+    // it hasn't been properly checked.
+    char c = shorty[shorty_index];
+    if (UNLIKELY(c == 0)) {
+      unimplemented_reason_ = "BogusSignature";
+      return false;
+    }
+    DataType::Type type = DataType::FromShorty(c);
+    bool is_wide = (type == DataType::Type::kInt64) || (type == DataType::Type::kFloat64);
+    if (is_wide && ((i + 1 == number_of_operands) ||
+                    (operands.GetOperand(i) + 1 != operands.GetOperand(i + 1)))) {
+      unimplemented_reason_ = "BogusSignature";
+      return false;
+    }
+    if (!MoveLocation(convention.GetNextLocation(type),
+                      vreg_locations_[operands.GetOperand(i)],
+                      type)) {
+      return false;
+    }
+    if (is_wide) {
+      ++i;
+    }
+  }
+  return true;
+}
+
+bool FastCompilerARM64::LoadMethod(Register reg, ArtMethod* method) {
+  if (Runtime::Current()->IsAotCompiler()) {
+    unimplemented_reason_ = "AOTLoadMethod";
+    return false;
+  }
+  __ Ldr(reg, jit_patches_.DeduplicateUint64Literal(reinterpret_cast<uint64_t>(method)));
+  return true;
+}
+
+bool FastCompilerARM64::HandleInvoke(const Instruction& instruction,
+                                     uint32_t dex_pc,
+                                     InvokeType invoke_type) {
+  Instruction::Code opcode = instruction.Opcode();
+  uint16_t method_index = (opcode >= Instruction::INVOKE_VIRTUAL_RANGE)
+      ? instruction.VRegB_3rc()
+      : instruction.VRegB_35c();
+  ArtMethod* resolved_method = nullptr;
+  size_t offset = 0u;
+  {
+    Thread* self = Thread::Current();
+    ScopedObjectAccess soa(self);
+    ClassLinker* const class_linker = dex_compilation_unit_.GetClassLinker();
+    resolved_method = method_->SkipAccessChecks()
+        ? class_linker->ResolveMethodId(method_index, method_)
+        : class_linker->ResolveMethodWithChecks(
+              method_index, method_, invoke_type);
+    if (resolved_method == nullptr) {
+      DCHECK(self->IsExceptionPending());
+      self->ClearException();
+      unimplemented_reason_ = "UnresolvedInvoke";
+      return false;
+    }
+
+    if (resolved_method->IsConstructor() && resolved_method->GetDeclaringClass()->IsObjectClass()) {
+      // Object.<init> is always empty. Return early to not generate a frame.
+      if (kIsDebugBuild) {
+        CHECK(resolved_method->GetDeclaringClass()->IsVerified());
+        CodeItemDataAccessor accessor(*resolved_method->GetDexFile(),
+                                      resolved_method->GetCodeItem());
+        CHECK_EQ(accessor.InsnsSizeInCodeUnits(), 1u);
+        CHECK_EQ(accessor.begin().Inst().Opcode(), Instruction::RETURN_VOID);
+      }
+      // No need to update `previous_invoke_return_type_`, we know it is not going the
+      // be used.
+      return true;
+    }
+
+    if (invoke_type == kSuper) {
+      resolved_method = method_->SkipAccessChecks()
+          ? FindSuperMethodToCall</*access_check=*/false>(method_index,
+                                                          resolved_method,
+                                                          method_,
+                                                          self)
+          : FindSuperMethodToCall</*access_check=*/true>(method_index,
+                                                         resolved_method,
+                                                         method_,
+                                                         self);
+      if (resolved_method == nullptr) {
+        DCHECK(self->IsExceptionPending()) << method_->PrettyMethod();
+        self->ClearException();
+        unimplemented_reason_ = "UnresolvedInvokeSuper";
+        return false;
+      }
+    } else if (invoke_type == kVirtual) {
+      offset = resolved_method->GetVtableIndex();
+    } else if (invoke_type == kInterface) {
+      offset = resolved_method->GetImtIndex();
+    }
+
+    if (resolved_method->IsStringConstructor()) {
+      unimplemented_reason_ = "StringConstructor";
+      return false;
+    }
+  }
+
+  // Given we are calling a method, generate a frame.
+  if (!EnsureHasFrame()) {
+    return false;
+  }
+
+  // Setup the arguments for the call.
+  uint32_t obj_reg = -1;
+  const char* shorty = dex_compilation_unit_.GetDexFile()->GetMethodShorty(method_index);
+  if (opcode >= Instruction::INVOKE_VIRTUAL_RANGE) {
+    RangeInstructionOperands operands(instruction.VRegC(), instruction.VRegA_3rc());
+    if (!SetupArguments(invoke_type, operands, shorty, &obj_reg)) {
+      return false;
+    }
+  } else {
+    uint32_t args[5];
+    uint32_t number_of_vreg_arguments = instruction.GetVarArgs(args);
+    VarArgsInstructionOperands operands(args, number_of_vreg_arguments);
+    if (!SetupArguments(invoke_type, operands, shorty, &obj_reg)) {
+      return false;
+    }
+  }
+  // Save the invoke return type for the next move-result instruction.
+  previous_invoke_return_type_ = DataType::FromShorty(shorty[0]);
+
+  if (invoke_type != kStatic) {
+    bool can_be_null = CanBeNull(obj_reg);
+    // Load the class of the instance. For kDirect and kSuper, this acts as a
+    // null check.
+    if (can_be_null || invoke_type == kVirtual || invoke_type == kInterface) {
+      InvokeDexCallingConvention calling_convention;
+      Register receiver = calling_convention.GetRegisterAt(0);
+      Offset class_offset = mirror::Object::ClassOffset();
+      EmissionCheckScope guard(GetVIXLAssembler(), kMaxMacroInstructionSizeInBytes);
+      __ Ldr(kArtMethodRegister.W(), HeapOperand(receiver.W(), class_offset));
+      if (can_be_null) {
+        RecordPcInfo(dex_pc);
+      }
+    }
+  }
+
+  if (invoke_type == kVirtual) {
+    size_t method_offset =
+        mirror::Class::EmbeddedVTableEntryOffset(offset, kArm64PointerSize).SizeValue();
+    __ Ldr(kArtMethodRegister, MemOperand(kArtMethodRegister, method_offset));
+  } else if (invoke_type == kInterface) {
+    __ Ldr(kArtMethodRegister,
+           MemOperand(kArtMethodRegister,
+                      mirror::Class::ImtPtrOffset(kArm64PointerSize).Uint32Value()));
+    uint32_t method_offset =
+        static_cast<uint32_t>(ImTable::OffsetOfElement(offset, kArm64PointerSize));
+    __ Ldr(kArtMethodRegister, MemOperand(kArtMethodRegister, method_offset));
+    if (!LoadMethod(ip1, resolved_method)) {
+      return false;
+    }
+  } else {
+    DCHECK(invoke_type == kDirect || invoke_type == kSuper || invoke_type == kStatic);
+    if (!LoadMethod(kArtMethodRegister, resolved_method)) {
+      return false;
+    }
+  }
+
+  Offset entry_point = ArtMethod::EntryPointFromQuickCompiledCodeOffset(kArm64PointerSize);
+  __ Ldr(lr, MemOperand(kArtMethodRegister, entry_point.SizeValue()));
+  {
+    // Use a scope to help guarantee that `RecordPcInfo()` records the correct pc.
+    ExactAssemblyScope eas(GetVIXLAssembler(), kInstructionSize, CodeBufferCheckScope::kExactSize);
+    __ blr(lr);
+    RecordPcInfo(dex_pc);
+  }
+  return true;
+}
+
+void FastCompilerARM64::InvokeRuntime(QuickEntrypointEnum entrypoint, uint32_t dex_pc) {
+  ThreadOffset64 entrypoint_offset = GetThreadOffset<kArm64PointerSize>(entrypoint);
+  __ Ldr(lr, MemOperand(tr, entrypoint_offset.Int32Value()));
+  // Ensure the pc position is recorded immediately after the `blr` instruction.
+  ExactAssemblyScope eas(GetVIXLAssembler(), kInstructionSize, CodeBufferCheckScope::kExactSize);
+  __ blr(lr);
+  if (EntrypointRequiresStackMap(entrypoint)) {
+    RecordPcInfo(dex_pc);
+  }
+}
+
+bool FastCompilerARM64::BuildLoadString(uint32_t vreg,
+                                        dex::StringIndex string_index,
+                                        const Instruction* next) {
+  // Generate a frame because of the read barrier.
+  if (!EnsureHasFrame()) {
+    return false;
+  }
+  Location loc = CreateNewRegisterLocation(vreg, DataType::Type::kReference, next);
+  if (HitUnimplemented()) {
+    return false;
+  }
+  if (Runtime::Current()->IsAotCompiler()) {
+    unimplemented_reason_ = "AOTLoadString";
+    return false;
+  }
+
+  ScopedObjectAccess soa(Thread::Current());
+  ClassLinker* const class_linker = dex_compilation_unit_.GetClassLinker();
+  ObjPtr<mirror::String> str = class_linker->ResolveString(string_index, method_);
+  if (str == nullptr) {
+    soa.Self()->ClearException();
+    unimplemented_reason_ = "NullString";
+    return false;
+  }
+
+  Handle<mirror::String> h_str = handles_->NewHandle(str);
+  Register dst = RegisterFrom(loc, DataType::Type::kReference);
+  __ Ldr(dst.W(), jit_patches_.DeduplicateJitStringLiteral(GetDexFile(),
+                                                           string_index,
+                                                           h_str,
+                                                           code_generation_data_.get()));
+  __ Ldr(dst.W(), MemOperand(dst.X()));
+  DoReadBarrierOn(dst);
+  UpdateLocal(vreg, /* is_object= */ true, /* can_be_null= */ false);
+  return true;
+}
+
+bool FastCompilerARM64::BuildNewInstance(uint32_t vreg,
+                                         dex::TypeIndex type_index,
+                                         uint32_t dex_pc,
+                                         const Instruction* next) {
+  if (!EnsureHasFrame()) {
+    return false;
+  }
+  if (Runtime::Current()->IsAotCompiler()) {
+    unimplemented_reason_ = "AOTNewInstance";
+    return false;
+  }
+
+  ScopedObjectAccess soa(Thread::Current());
+  ObjPtr<mirror::Class> klass = dex_compilation_unit_.GetClassLinker()->ResolveType(
+      type_index, dex_compilation_unit_.GetDexCache(), dex_compilation_unit_.GetClassLoader());
+  if (klass == nullptr ||
+      !method_->GetDeclaringClass()->CanAccess(klass) ||
+      klass->IsStringClass()) {
+    soa.Self()->ClearException();
+    unimplemented_reason_ = "UnsupportedClassForNewInstance";
+    return false;
+  }
+
+  InvokeRuntimeCallingConvention calling_convention;
+  Register cls_reg = calling_convention.GetRegisterAt(0);
+  Handle<mirror::Class> h_klass = handles_->NewHandle(klass);
+  __ Ldr(cls_reg.W(), jit_patches_.DeduplicateJitClassLiteral(GetDexFile(),
+                                                              type_index,
+                                                              h_klass ,
+                                                              code_generation_data_.get()));
+  __ Ldr(cls_reg.W(), MemOperand(cls_reg.X()));
+  DoReadBarrierOn(cls_reg);
+
+  QuickEntrypointEnum entrypoint = kQuickAllocObjectInitialized;
+  if (h_klass->IsFinalizable() ||
+      !h_klass->IsVisiblyInitialized() ||
+      h_klass->IsClassClass() ||  // Classes cannot be allocated in code
+      !klass->IsInstantiable()) {
+    entrypoint = kQuickAllocObjectWithChecks;
+  }
+  InvokeRuntime(entrypoint, dex_pc);
+  __ Dmb(InnerShareable, BarrierWrites);
+  if (!MoveLocation(CreateNewRegisterLocation(vreg, DataType::Type::kReference, next),
+                    calling_convention.GetReturnLocation(DataType::Type::kReference),
+                    DataType::Type::kReference)) {
+    return false;
+  }
+  if (HitUnimplemented()) {
+    return false;
+  }
+  UpdateLocal(vreg, /* is_object= */ true, /* can_be_null= */ false);
+  return true;
+}
+
+bool FastCompilerARM64::BuildCheckCast(uint32_t vreg, dex::TypeIndex type_index, uint32_t dex_pc) {
+  if (!EnsureHasFrame()) {
+    return false;
+  }
+
+  InvokeRuntimeCallingConvention calling_convention;
+  UseScratchRegisterScope temps(GetVIXLAssembler());
+  Register cls = calling_convention.GetRegisterAt(1);
+  Register obj_cls = calling_convention.GetRegisterAt(2);
+  Register obj = WRegisterFrom(GetExistingRegisterLocation(vreg, DataType::Type::kReference));
+  if (HitUnimplemented()) {
+    return false;
+  }
+
+  ScopedObjectAccess soa(Thread::Current());
+  ObjPtr<mirror::Class> klass = dex_compilation_unit_.GetClassLinker()->ResolveType(
+      type_index, dex_compilation_unit_.GetDexCache(), dex_compilation_unit_.GetClassLoader());
+  if (klass == nullptr || !method_->GetDeclaringClass()->CanAccess(klass)) {
+    soa.Self()->ClearException();
+    unimplemented_reason_ = "UnsupportedCheckCast";
+    return false;
+  }
+  Handle<mirror::Class> h_klass = handles_->NewHandle(klass);
+
+  vixl::aarch64::Label exit, read_barrier_exit;
+  __ Cbz(obj, &exit);
+  __ Ldr(cls.W(), jit_patches_.DeduplicateJitClassLiteral(GetDexFile(),
+                                                          type_index,
+                                                          h_klass ,
+                                                          code_generation_data_.get()));
+  __ Ldr(cls.W(), MemOperand(cls.X()));
+  __ Ldr(obj_cls.W(), MemOperand(obj.X()));
+  __ Cmp(cls.W(), obj_cls.W());
+  __ B(eq, &exit);
+
+  // Read barrier on the GC Root.
+  DoReadBarrierOn(cls, &read_barrier_exit);
+  // Read barrier on the object's class.
+  DoReadBarrierOn(obj_cls, &read_barrier_exit, /* do_mr_check= */ false);
+
+  __ Bind(&read_barrier_exit);
+  __ Cmp(cls.W(), obj_cls.W());
+  __ B(eq, &exit);
+  if (!MoveLocation(LocationFrom(calling_convention.GetRegisterAt(0)),
+                    LocationFrom(obj),
+                    DataType::Type::kReference)) {
+    return false;
+  }
+  InvokeRuntime(kQuickCheckInstanceOf, dex_pc);
+
+  __ Bind(&exit);
+  return true;
+}
+
+void FastCompilerARM64::DoReadBarrierOn(Register reg,
+                                        vixl::aarch64::Label* exit,
+                                        bool do_mr_check) {
+  DCHECK(has_frame_);
+  vixl::aarch64::Label local_exit;
+  if (do_mr_check) {
+    __ Cbz(mr, (exit != nullptr) ? exit : &local_exit);
+  }
+  int32_t entry_point_offset =
+      Thread::ReadBarrierMarkEntryPointsOffset<kArm64PointerSize>(reg.GetCode());
+  __ Ldr(lr, MemOperand(tr, entry_point_offset));
+  __ Blr(lr);
+  if (exit == nullptr && do_mr_check) {
+    __ Bind(&local_exit);
+  }
+}
+
+bool FastCompilerARM64::CanGenerateCodeFor(ArtField* field, bool can_receiver_be_null) {
+  if (field == nullptr) {
+    // Clear potential resolution exception.
+    Thread::Current()->ClearException();
+    unimplemented_reason_ = "UnresolvedField";
+    return false;
+  }
+  if (field->IsVolatile()) {
+    unimplemented_reason_ = "VolatileField";
+    return false;
+  }
+
+  if (can_receiver_be_null) {
+    if (!CanDoImplicitNullCheckOn(field->GetOffset().Uint32Value())) {
+      unimplemented_reason_ = "TooLargeFieldOffset";
+      return false;
+    }
+  }
+  return true;
+}
+
+#define DO_CASE(arm_op, op, other) \
+    case arm_op: { \
+      if (constant op other) { \
+        __ B(label); \
+      } \
+      return true; \
+    } \
+
+template<vixl::aarch64::Condition kCond, bool kCompareWithZero>
+bool FastCompilerARM64::If_21_22t(const Instruction& instruction, uint32_t dex_pc) {
+  DCHECK_EQ(kCompareWithZero ? Instruction::Format::k21t : Instruction::Format::k22t,
+            Instruction::FormatOf(instruction.Opcode()));
+  if (!EnsureHasFrame()) {
+    return false;
+  }
+  int32_t target_offset = kCompareWithZero ? instruction.VRegB_21t() : instruction.VRegC_22t();
+  DCHECK_EQ(target_offset, instruction.GetTargetOffset());
+  if (target_offset < 0) {
+    // TODO: Support for negative branches requires two passes.
+    unimplemented_reason_ = "NegativeBranch";
+    return false;
+  }
+  int32_t register_index = kCompareWithZero ? instruction.VRegA_21t() : instruction.VRegA_22t();
+  vixl::aarch64::Label* label = GetLabelOf(dex_pc + target_offset);
+  Location location = vreg_locations_[register_index];
+
+  if (kCompareWithZero) {
+    // We are going to branch, move all constants to registers to make the merge
+    // point use the same locations.
+    MoveConstantsToRegisters();
+    UpdateMasks(dex_pc + target_offset);
+    if (location.IsConstant()) {
+      DCHECK(location.GetConstant()->IsIntConstant());
+      int32_t constant = location.GetConstant()->AsIntConstant()->GetValue();
+      switch (kCond) {
+        DO_CASE(vixl::aarch64::eq, ==, 0);
+        DO_CASE(vixl::aarch64::ne, !=, 0);
+        DO_CASE(vixl::aarch64::lt, <, 0);
+        DO_CASE(vixl::aarch64::le, <=, 0);
+        DO_CASE(vixl::aarch64::gt, >, 0);
+        DO_CASE(vixl::aarch64::ge, >=, 0);
+      }
+      return true;
+    } else if (location.IsRegister()) {
+      CPURegister reg = CPURegisterFrom(location, DataType::Type::kInt32);
+      switch (kCond) {
+        case vixl::aarch64::eq: {
+          __ Cbz(Register(reg), label);
+          return true;
+        }
+        case vixl::aarch64::ne: {
+          __ Cbnz(Register(reg), label);
+          return true;
+        }
+        default: {
+          __ Cmp(Register(reg), 0);
+          __ B(kCond, label);
+          return true;
+        }
+      }
+    } else {
+      DCHECK(location.IsStackSlot());
+      unimplemented_reason_ = "CompareWithZeroOnStackSlot";
+    }
+    return false;
+  }
+
+  // !kCompareWithZero
+  Location other_location = vreg_locations_[instruction.VRegB_22t()];
+  // We are going to branch, move all constants to registers to make the merge
+  // point use the same locations.
+  MoveConstantsToRegisters();
+  UpdateMasks(dex_pc + target_offset);
+  if (location.IsConstant() && other_location.IsConstant()) {
+    int32_t constant = location.GetConstant()->AsIntConstant()->GetValue();
+    int32_t other_constant = other_location.GetConstant()->AsIntConstant()->GetValue();
+    switch (kCond) {
+      DO_CASE(vixl::aarch64::eq, ==, other_constant);
+      DO_CASE(vixl::aarch64::ne, !=, other_constant);
+      DO_CASE(vixl::aarch64::lt, <, other_constant);
+      DO_CASE(vixl::aarch64::le, <=, other_constant);
+      DO_CASE(vixl::aarch64::gt, >, other_constant);
+      DO_CASE(vixl::aarch64::ge, >=, other_constant);
+    }
+    return true;
+  }
+  // Reload the locations, which can now be registers.
+  location = vreg_locations_[register_index];
+  other_location = vreg_locations_[instruction.VRegB_22t()];
+  if (location.IsRegister() && other_location.IsRegister()) {
+    CPURegister reg = CPURegisterFrom(location, DataType::Type::kInt32);
+    CPURegister other_reg = CPURegisterFrom(other_location, DataType::Type::kInt32);
+    __ Cmp(Register(reg), Register(other_reg));
+    __ B(kCond, label);
+    return true;
+  }
+
+  unimplemented_reason_ = "UnimplementedCompare";
+  return false;
+}
+#undef DO_CASE
+
+bool FastCompilerARM64::ProcessDexInstruction(const Instruction& instruction,
+                                              uint32_t dex_pc,
+                                              const Instruction* next) {
+  bool is_object = false;
+  switch (instruction.Opcode()) {
+    case Instruction::CONST_4: {
+      int32_t register_index = instruction.VRegA_11n();
+      int32_t constant = instruction.VRegB_11n();
+      vreg_locations_[register_index] =
+          Location::ConstantLocation(new (allocator_) HIntConstant(constant));
+      UpdateLocal(register_index, /* is_object= */ false);
+      return true;
+    }
+
+    case Instruction::CONST_16: {
+      int32_t register_index = instruction.VRegA_21s();
+      int32_t constant = instruction.VRegB_21s();
+      vreg_locations_[register_index] =
+          Location::ConstantLocation(new (allocator_) HIntConstant(constant));
+      UpdateLocal(register_index, /* is_object= */ false);
+      return true;
+    }
+
+    case Instruction::CONST: {
+      break;
+    }
+
+    case Instruction::CONST_HIGH16: {
+      break;
+    }
+
+    case Instruction::CONST_WIDE_16: {
+      break;
+    }
+
+    case Instruction::CONST_WIDE_32: {
+      break;
+    }
+
+    case Instruction::CONST_WIDE: {
+      break;
+    }
+
+    case Instruction::CONST_WIDE_HIGH16: {
+      break;
+    }
+
+    case Instruction::MOVE:
+    case Instruction::MOVE_FROM16:
+    case Instruction::MOVE_16: {
+      break;
+    }
+
+    case Instruction::MOVE_WIDE:
+    case Instruction::MOVE_WIDE_FROM16:
+    case Instruction::MOVE_WIDE_16: {
+      break;
+    }
+
+    case Instruction::MOVE_OBJECT:
+    case Instruction::MOVE_OBJECT_16:
+    case Instruction::MOVE_OBJECT_FROM16: {
+      break;
+    }
+
+    case Instruction::RETURN_VOID: {
+      if (method_->IsConstructor() &&
+          !method_->IsStatic() &&
+          dex_compilation_unit_.RequiresConstructorBarrier()) {
+        __ Dmb(InnerShareable, BarrierWrites);
+      }
+      PopFrameAndReturn();
+      return true;
+    }
+
+#define IF_XX(comparison, cond) \
+    case Instruction::IF_##cond: \
+      return If_21_22t<comparison, /* kCompareWithZero= */ false>(instruction, dex_pc); \
+    case Instruction::IF_##cond##Z: \
+      return If_21_22t<comparison, /* kCompareWithZero= */ true>(instruction, dex_pc);
+
+    IF_XX(vixl::aarch64::eq, EQ);
+    IF_XX(vixl::aarch64::ne, NE);
+    IF_XX(vixl::aarch64::lt, LT);
+    IF_XX(vixl::aarch64::le, LE);
+    IF_XX(vixl::aarch64::gt, GT);
+    IF_XX(vixl::aarch64::ge, GE);
+
+    case Instruction::GOTO:
+    case Instruction::GOTO_16:
+    case Instruction::GOTO_32: {
+      break;
+    }
+
+    case Instruction::RETURN:
+    case Instruction::RETURN_OBJECT: {
+      int32_t register_index = instruction.VRegA_11x();
+      InvokeDexCallingConventionVisitorARM64 convention;
+      if (!MoveLocation(convention.GetReturnLocation(return_type_),
+                        vreg_locations_[register_index],
+                        return_type_)) {
+        return false;
+      }
+      if (has_frame_) {
+        // We may have used the "record last instruction before return in return
+        // register" optimization (see `CreateNewRegisterLocation`),
+        // so set the returned register back to a callee save location in case the
+        // method has a frame and there are instructions after this return that
+        // may use this register.
+        uint32_t register_code = kAvailableCalleeSaveRegisters[register_index].GetCode();
+        vreg_locations_[register_index] = Location::RegisterLocation(register_code);
+      }
+      PopFrameAndReturn();
+      return true;
+    }
+
+    case Instruction::RETURN_WIDE: {
+      break;
+    }
+
+    case Instruction::INVOKE_DIRECT:
+    case Instruction::INVOKE_DIRECT_RANGE:
+      return HandleInvoke(instruction, dex_pc, kDirect);
+    case Instruction::INVOKE_INTERFACE:
+    case Instruction::INVOKE_INTERFACE_RANGE:
+      return HandleInvoke(instruction, dex_pc, kInterface);
+    case Instruction::INVOKE_STATIC:
+    case Instruction::INVOKE_STATIC_RANGE:
+      return HandleInvoke(instruction, dex_pc, kStatic);
+    case Instruction::INVOKE_SUPER:
+    case Instruction::INVOKE_SUPER_RANGE:
+      return HandleInvoke(instruction, dex_pc, kSuper);
+    case Instruction::INVOKE_VIRTUAL:
+    case Instruction::INVOKE_VIRTUAL_RANGE: {
+      return HandleInvoke(instruction, dex_pc, kVirtual);
+    }
+
+    case Instruction::INVOKE_POLYMORPHIC: {
+      break;
+    }
+
+    case Instruction::INVOKE_POLYMORPHIC_RANGE: {
+      break;
+    }
+
+    case Instruction::INVOKE_CUSTOM: {
+      break;
+    }
+
+    case Instruction::INVOKE_CUSTOM_RANGE: {
+      break;
+    }
+
+    case Instruction::NEG_INT: {
+      break;
+    }
+
+    case Instruction::NEG_LONG: {
+      break;
+    }
+
+    case Instruction::NEG_FLOAT: {
+      break;
+    }
+
+    case Instruction::NEG_DOUBLE: {
+      break;
+    }
+
+    case Instruction::NOT_INT: {
+      break;
+    }
+
+    case Instruction::NOT_LONG: {
+      break;
+    }
+
+    case Instruction::INT_TO_LONG: {
+      break;
+    }
+
+    case Instruction::INT_TO_FLOAT: {
+      break;
+    }
+
+    case Instruction::INT_TO_DOUBLE: {
+      break;
+    }
+
+    case Instruction::LONG_TO_INT: {
+      break;
+    }
+
+    case Instruction::LONG_TO_FLOAT: {
+      break;
+    }
+
+    case Instruction::LONG_TO_DOUBLE: {
+      break;
+    }
+
+    case Instruction::FLOAT_TO_INT: {
+      break;
+    }
+
+    case Instruction::FLOAT_TO_LONG: {
+      break;
+    }
+
+    case Instruction::FLOAT_TO_DOUBLE: {
+      break;
+    }
+
+    case Instruction::DOUBLE_TO_INT: {
+      break;
+    }
+
+    case Instruction::DOUBLE_TO_LONG: {
+      break;
+    }
+
+    case Instruction::DOUBLE_TO_FLOAT: {
+      break;
+    }
+
+    case Instruction::INT_TO_BYTE: {
+      break;
+    }
+
+    case Instruction::INT_TO_SHORT: {
+      break;
+    }
+
+    case Instruction::INT_TO_CHAR: {
+      break;
+    }
+
+    case Instruction::ADD_INT: {
+      break;
+    }
+
+    case Instruction::ADD_LONG: {
+      break;
+    }
+
+    case Instruction::ADD_DOUBLE: {
+      break;
+    }
+
+    case Instruction::ADD_FLOAT: {
+      break;
+    }
+
+    case Instruction::SUB_INT: {
+      break;
+    }
+
+    case Instruction::SUB_LONG: {
+      break;
+    }
+
+    case Instruction::SUB_FLOAT: {
+      break;
+    }
+
+    case Instruction::SUB_DOUBLE: {
+      break;
+    }
+
+    case Instruction::ADD_INT_2ADDR: {
+      break;
+    }
+
+    case Instruction::MUL_INT: {
+      break;
+    }
+
+    case Instruction::MUL_LONG: {
+      break;
+    }
+
+    case Instruction::MUL_FLOAT: {
+      break;
+    }
+
+    case Instruction::MUL_DOUBLE: {
+      break;
+    }
+
+    case Instruction::DIV_INT: {
+      break;
+    }
+
+    case Instruction::DIV_LONG: {
+      break;
+    }
+
+    case Instruction::DIV_FLOAT: {
+      break;
+    }
+
+    case Instruction::DIV_DOUBLE: {
+      break;
+    }
+
+    case Instruction::REM_INT: {
+      break;
+    }
+
+    case Instruction::REM_LONG: {
+      break;
+    }
+
+    case Instruction::REM_FLOAT: {
+      break;
+    }
+
+    case Instruction::REM_DOUBLE: {
+      break;
+    }
+
+    case Instruction::AND_INT: {
+      break;
+    }
+
+    case Instruction::AND_LONG: {
+      break;
+    }
+
+    case Instruction::SHL_INT: {
+      break;
+    }
+
+    case Instruction::SHL_LONG: {
+      break;
+    }
+
+    case Instruction::SHR_INT: {
+      break;
+    }
+
+    case Instruction::SHR_LONG: {
+      break;
+    }
+
+    case Instruction::USHR_INT: {
+      break;
+    }
+
+    case Instruction::USHR_LONG: {
+      break;
+    }
+
+    case Instruction::OR_INT: {
+      break;
+    }
+
+    case Instruction::OR_LONG: {
+      break;
+    }
+
+    case Instruction::XOR_INT: {
+      break;
+    }
+
+    case Instruction::XOR_LONG: {
+      break;
+    }
+
+    case Instruction::ADD_LONG_2ADDR: {
+      break;
+    }
+
+    case Instruction::ADD_DOUBLE_2ADDR: {
+      break;
+    }
+
+    case Instruction::ADD_FLOAT_2ADDR: {
+      break;
+    }
+
+    case Instruction::SUB_INT_2ADDR: {
+      break;
+    }
+
+    case Instruction::SUB_LONG_2ADDR: {
+      break;
+    }
+
+    case Instruction::SUB_FLOAT_2ADDR: {
+      break;
+    }
+
+    case Instruction::SUB_DOUBLE_2ADDR: {
+      break;
+    }
+
+    case Instruction::MUL_INT_2ADDR: {
+      break;
+    }
+
+    case Instruction::MUL_LONG_2ADDR: {
+      break;
+    }
+
+    case Instruction::MUL_FLOAT_2ADDR: {
+      break;
+    }
+
+    case Instruction::MUL_DOUBLE_2ADDR: {
+      break;
+    }
+
+    case Instruction::DIV_INT_2ADDR: {
+      break;
+    }
+
+    case Instruction::DIV_LONG_2ADDR: {
+      break;
+    }
+
+    case Instruction::REM_INT_2ADDR: {
+      break;
+    }
+
+    case Instruction::REM_LONG_2ADDR: {
+      break;
+    }
+
+    case Instruction::REM_FLOAT_2ADDR: {
+      break;
+    }
+
+    case Instruction::REM_DOUBLE_2ADDR: {
+      break;
+    }
+
+    case Instruction::SHL_INT_2ADDR: {
+      break;
+    }
+
+    case Instruction::SHL_LONG_2ADDR: {
+      break;
+    }
+
+    case Instruction::SHR_INT_2ADDR: {
+      break;
+    }
+
+    case Instruction::SHR_LONG_2ADDR: {
+      break;
+    }
+
+    case Instruction::USHR_INT_2ADDR: {
+      break;
+    }
+
+    case Instruction::USHR_LONG_2ADDR: {
+      break;
+    }
+
+    case Instruction::DIV_FLOAT_2ADDR: {
+      break;
+    }
+
+    case Instruction::DIV_DOUBLE_2ADDR: {
+      break;
+    }
+
+    case Instruction::AND_INT_2ADDR: {
+      break;
+    }
+
+    case Instruction::AND_LONG_2ADDR: {
+      break;
+    }
+
+    case Instruction::OR_INT_2ADDR: {
+      break;
+    }
+
+    case Instruction::OR_LONG_2ADDR: {
+      break;
+    }
+
+    case Instruction::XOR_INT_2ADDR: {
+      break;
+    }
+
+    case Instruction::XOR_LONG_2ADDR: {
+      break;
+    }
+
+    case Instruction::ADD_INT_LIT16: {
+      break;
+    }
+
+    case Instruction::AND_INT_LIT16: {
+      break;
+    }
+
+    case Instruction::OR_INT_LIT16: {
+      break;
+    }
+
+    case Instruction::XOR_INT_LIT16: {
+      break;
+    }
+
+    case Instruction::RSUB_INT: {
+      break;
+    }
+
+    case Instruction::MUL_INT_LIT16: {
+      break;
+    }
+
+    case Instruction::ADD_INT_LIT8: {
+      break;
+    }
+
+    case Instruction::AND_INT_LIT8: {
+      break;
+    }
+
+    case Instruction::OR_INT_LIT8: {
+      break;
+    }
+
+    case Instruction::XOR_INT_LIT8: {
+      break;
+    }
+
+    case Instruction::RSUB_INT_LIT8: {
+      break;
+    }
+
+    case Instruction::MUL_INT_LIT8: {
+      break;
+    }
+
+    case Instruction::DIV_INT_LIT16:
+    case Instruction::DIV_INT_LIT8: {
+      break;
+    }
+
+    case Instruction::REM_INT_LIT16:
+    case Instruction::REM_INT_LIT8: {
+      break;
+    }
+
+    case Instruction::SHL_INT_LIT8: {
+      break;
+    }
+
+    case Instruction::SHR_INT_LIT8: {
+      break;
+    }
+
+    case Instruction::USHR_INT_LIT8: {
+      break;
+    }
+
+    case Instruction::NEW_INSTANCE: {
+      dex::TypeIndex type_index(instruction.VRegB_21c());
+      return BuildNewInstance(instruction.VRegA_21c(), type_index, dex_pc, next);
+    }
+
+    case Instruction::NEW_ARRAY: {
+      break;
+    }
+
+    case Instruction::FILLED_NEW_ARRAY: {
+      break;
+    }
+
+    case Instruction::FILLED_NEW_ARRAY_RANGE: {
+      break;
+    }
+
+    case Instruction::FILL_ARRAY_DATA: {
+      break;
+    }
+
+    case Instruction::MOVE_RESULT_OBJECT:
+      is_object = true;
+      FALLTHROUGH_INTENDED;
+    case Instruction::MOVE_RESULT: {
+      int32_t register_index = instruction.VRegA_11x();
+      InvokeDexCallingConventionVisitorARM64 convention;
+      if (!MoveLocation(
+              CreateNewRegisterLocation(register_index, previous_invoke_return_type_, next),
+              convention.GetReturnLocation(previous_invoke_return_type_),
+              previous_invoke_return_type_)) {
+        return false;
+      }
+      if (HitUnimplemented()) {
+        return false;
+      }
+      UpdateLocal(register_index, is_object);
+      return true;
+    }
+
+    case Instruction::MOVE_RESULT_WIDE: {
+      break;
+    }
+
+    case Instruction::CMP_LONG: {
+      break;
+    }
+
+    case Instruction::CMPG_FLOAT: {
+      break;
+    }
+
+    case Instruction::CMPG_DOUBLE: {
+      break;
+    }
+
+    case Instruction::CMPL_FLOAT: {
+      break;
+    }
+
+    case Instruction::CMPL_DOUBLE: {
+      break;
+    }
+
+    case Instruction::NOP:
+      return true;
+
+    case Instruction::IGET_OBJECT:
+      is_object = true;
+      FALLTHROUGH_INTENDED;
+    case Instruction::IGET:
+    case Instruction::IGET_WIDE:
+    case Instruction::IGET_BOOLEAN:
+    case Instruction::IGET_BYTE:
+    case Instruction::IGET_CHAR:
+    case Instruction::IGET_SHORT: {
+      uint32_t source_or_dest_reg = instruction.VRegA_22c();
+      uint32_t obj_reg = instruction.VRegB_22c();
+      uint16_t field_index = instruction.VRegC_22c();
+      bool can_receiver_be_null = CanBeNull(obj_reg);
+      ArtField* field = nullptr;
+      {
+        ScopedObjectAccess soa(Thread::Current());
+        field = ResolveFieldWithAccessChecks(soa.Self(),
+                                             dex_compilation_unit_.GetClassLinker(),
+                                             field_index,
+                                             method_,
+                                             /* is_static= */ false,
+                                             /* is_put= */ false,
+                                             /* resolve_field_type= */ 0u);
+        if (!CanGenerateCodeFor(field, can_receiver_be_null)) {
+          return false;
+        }
+      }
+
+      if (can_receiver_be_null || is_object) {
+        // We need a frame in case the null check throws or there is a read
+        // barrier.
+        if (!EnsureHasFrame()) {
+          return false;
+        }
+      }
+
+      MemOperand mem = HeapOperand(
+          RegisterFrom(GetExistingRegisterLocation(obj_reg, DataType::Type::kReference),
+                       DataType::Type::kReference),
+          field->GetOffset());
+      if (HitUnimplemented()) {
+        return false;
+      }
+      if (is_object) {
+        Register dst = WRegisterFrom(
+            CreateNewRegisterLocation(source_or_dest_reg, DataType::Type::kReference, next));
+        if (HitUnimplemented()) {
+          return false;
+        }
+        {
+          // Ensure the pc position is recorded immediately after the load instruction.
+          EmissionCheckScope guard(GetVIXLAssembler(), kMaxMacroInstructionSizeInBytes);
+          __ Ldr(dst, mem);
+          if (can_receiver_be_null) {
+            RecordPcInfo(dex_pc);
+          }
+        }
+        UpdateLocal(source_or_dest_reg, /* is_object= */ true);
+        DoReadBarrierOn(dst);
+        return true;
+      }
+      // Ensure the pc position is recorded immediately after the load instruction.
+      EmissionCheckScope guard(GetVIXLAssembler(), kMaxMacroInstructionSizeInBytes);
+      switch (instruction.Opcode()) {
+        case Instruction::IGET_BOOLEAN: {
+          Register dst = WRegisterFrom(
+              CreateNewRegisterLocation(source_or_dest_reg, DataType::Type::kInt32, next));
+          __ Ldrb(Register(dst), mem);
+          break;
+        }
+        case Instruction::IGET_BYTE: {
+          Register dst = WRegisterFrom(
+              CreateNewRegisterLocation(source_or_dest_reg, DataType::Type::kInt32, next));
+          __ Ldrsb(Register(dst), mem);
+          break;
+        }
+        case Instruction::IGET_CHAR: {
+          Register dst = WRegisterFrom(
+              CreateNewRegisterLocation(source_or_dest_reg, DataType::Type::kInt32, next));
+          __ Ldrh(Register(dst), mem);
+          break;
+        }
+        case Instruction::IGET_SHORT: {
+          Register dst = WRegisterFrom(
+              CreateNewRegisterLocation(source_or_dest_reg, DataType::Type::kInt32, next));
+          __ Ldrsh(Register(dst), mem);
+          break;
+        }
+        case Instruction::IGET: {
+          const dex::FieldId& field_id = GetDexFile().GetFieldId(field_index);
+          const char* type = GetDexFile().GetFieldTypeDescriptor(field_id);
+          DataType::Type field_type = DataType::FromShorty(type[0]);
+          if (DataType::IsFloatingPointType(field_type)) {
+            VRegister dst = SRegisterFrom(
+                CreateNewRegisterLocation(source_or_dest_reg, field_type, next));
+            __ Ldr(dst, mem);
+          } else {
+            Register dst = WRegisterFrom(
+                CreateNewRegisterLocation(source_or_dest_reg, DataType::Type::kInt32, next));
+            __ Ldr(dst, mem);
+          }
+          if (HitUnimplemented()) {
+            return false;
+          }
+          break;
+        }
+        default:
+          unimplemented_reason_ = "UnimplementedIGet";
+          return false;
+      }
+      UpdateLocal(source_or_dest_reg, /* is_object= */ false);
+      if (can_receiver_be_null) {
+        RecordPcInfo(dex_pc);
+      }
+      return true;
+    }
+
+    case Instruction::IPUT_OBJECT:
+      is_object = true;
+      FALLTHROUGH_INTENDED;
+    case Instruction::IPUT:
+    case Instruction::IPUT_WIDE:
+    case Instruction::IPUT_BOOLEAN:
+    case Instruction::IPUT_BYTE:
+    case Instruction::IPUT_CHAR:
+    case Instruction::IPUT_SHORT: {
+      uint32_t source_reg = instruction.VRegA_22c();
+      uint32_t obj_reg = instruction.VRegB_22c();
+      uint16_t field_index = instruction.VRegC_22c();
+      bool can_receiver_be_null = CanBeNull(obj_reg);
+      ArtField* field = nullptr;
+      {
+        ScopedObjectAccess soa(Thread::Current());
+        field = ResolveFieldWithAccessChecks(soa.Self(),
+                                             dex_compilation_unit_.GetClassLinker(),
+                                             field_index,
+                                             method_,
+                                             /* is_static= */ false,
+                                             /* is_put= */ true,
+                                             /* resolve_field_type= */ is_object);
+        if (!CanGenerateCodeFor(field, can_receiver_be_null)) {
+          return false;
+        }
+      }
+
+      if (can_receiver_be_null) {
+        // We need a frame in case the null check throws.
+        if (!EnsureHasFrame()) {
+          return false;
+        }
+      }
+
+      Register holder = RegisterFrom(
+          GetExistingRegisterLocation(obj_reg, DataType::Type::kReference),
+          DataType::Type::kReference);
+      if (HitUnimplemented()) {
+        return false;
+      }
+      MemOperand mem = HeapOperand(holder, field->GetOffset());
+
+      // Need one temp if the stored value is a constant.
+      UseScratchRegisterScope temps(GetVIXLAssembler());
+      Location src = vreg_locations_[source_reg];
+      bool assigning_constant = false;
+      if (src.IsConstant()) {
+        assigning_constant = true;
+        if (src.GetConstant()->IsIntConstant() &&
+            src.GetConstant()->AsIntConstant()->GetValue() == 0) {
+          src = Location::RegisterLocation(XZR);
+        } else {
+          src = Location::RegisterLocation(temps.AcquireW().GetCode());
+          if (!MoveLocation(src, vreg_locations_[source_reg], DataType::Type::kInt32)) {
+            return false;
+          }
+        }
+      } else if (src.IsStackSlot() || src.IsDoubleStackSlot()) {
+        unimplemented_reason_ = "IPUTOnStackSlot";
+        return false;
+      }
+      if (instruction.Opcode() == Instruction::IPUT_OBJECT) {
+        Register reg = WRegisterFrom(src);
+        {
+          // Ensure the pc position is recorded immediately after the store instruction.
+          EmissionCheckScope guard(GetVIXLAssembler(), kMaxMacroInstructionSizeInBytes);
+          __ Str(reg, mem);
+          if (can_receiver_be_null) {
+            RecordPcInfo(dex_pc);
+          }
+        }
+        // If we assign a constant (only null for iput-object), no need for the write
+        // barrier.
+        if (!assigning_constant) {
+          vixl::aarch64::Label exit;
+          __ Cbz(reg, &exit);
+          Register card = temps.AcquireX();
+          Register temp = temps.AcquireW();
+          __ Ldr(card, MemOperand(tr, Thread::CardTableOffset<kArm64PointerSize>().Int32Value()));
+          __ Lsr(temp, holder, gc::accounting::CardTable::kCardShift);
+          __ Strb(card, MemOperand(card, temp.X()));
+          __ Bind(&exit);
+        }
+        return true;
+      }
+      // Ensure the pc position is recorded immediately after the store instruction.
+      EmissionCheckScope guard(GetVIXLAssembler(), kMaxMacroInstructionSizeInBytes);
+      switch (instruction.Opcode()) {
+        case Instruction::IPUT_BOOLEAN:
+        case Instruction::IPUT_BYTE: {
+          __ Strb(WRegisterFrom(src), mem);
+          break;
+        }
+        case Instruction::IPUT_CHAR:
+        case Instruction::IPUT_SHORT: {
+          __ Strh(WRegisterFrom(src), mem);
+          break;
+        }
+        case Instruction::IPUT: {
+          if (src.IsFpuRegister()) {
+            __ Str(SRegisterFrom(src), mem);
+          } else {
+            __ Str(WRegisterFrom(src), mem);
+          }
+          break;
+        }
+        default:
+          unimplemented_reason_ = "UnimplementedIPut";
+          return false;
+      }
+      if (can_receiver_be_null) {
+        RecordPcInfo(dex_pc);
+      }
+      return true;
+    }
+
+    case Instruction::SGET:
+    case Instruction::SGET_WIDE:
+    case Instruction::SGET_OBJECT:
+    case Instruction::SGET_BOOLEAN:
+    case Instruction::SGET_BYTE:
+    case Instruction::SGET_CHAR:
+    case Instruction::SGET_SHORT: {
+      break;
+    }
+
+    case Instruction::SPUT:
+    case Instruction::SPUT_WIDE:
+    case Instruction::SPUT_OBJECT:
+    case Instruction::SPUT_BOOLEAN:
+    case Instruction::SPUT_BYTE:
+    case Instruction::SPUT_CHAR:
+    case Instruction::SPUT_SHORT: {
+      break;
+    }
+
+#define ARRAY_XX(kind, anticipated_type)                                          \
+    case Instruction::AGET##kind: {                                               \
+      break;                                                                      \
+    }                                                                             \
+    case Instruction::APUT##kind: {                                               \
+      break;                                                                      \
+    }
+
+    ARRAY_XX(, DataType::Type::kInt32);
+    ARRAY_XX(_WIDE, DataType::Type::kInt64);
+    ARRAY_XX(_OBJECT, DataType::Type::kReference);
+    ARRAY_XX(_BOOLEAN, DataType::Type::kBool);
+    ARRAY_XX(_BYTE, DataType::Type::kInt8);
+    ARRAY_XX(_CHAR, DataType::Type::kUint16);
+    ARRAY_XX(_SHORT, DataType::Type::kInt16);
+
+    case Instruction::ARRAY_LENGTH: {
+      break;
+    }
+
+    case Instruction::CONST_STRING: {
+      dex::StringIndex string_index(instruction.VRegB_21c());
+      return BuildLoadString(instruction.VRegA_21c(), string_index, next);
+    }
+
+    case Instruction::CONST_STRING_JUMBO: {
+      dex::StringIndex string_index(instruction.VRegB_31c());
+      return BuildLoadString(instruction.VRegA_31c(), string_index, next);
+    }
+
+    case Instruction::CONST_CLASS: {
+      break;
+    }
+
+    case Instruction::CONST_METHOD_HANDLE: {
+      break;
+    }
+
+    case Instruction::CONST_METHOD_TYPE: {
+      break;
+    }
+
+    case Instruction::MOVE_EXCEPTION: {
+      break;
+    }
+
+    case Instruction::THROW: {
+      if (!EnsureHasFrame()) {
+        return false;
+      }
+      int32_t reg = instruction.VRegA_11x();
+      InvokeRuntimeCallingConvention calling_convention;
+      if (!MoveLocation(LocationFrom(calling_convention.GetRegisterAt(0)),
+                        vreg_locations_[reg],
+                        DataType::Type::kReference)) {
+        return false;
+      }
+      InvokeRuntime(kQuickDeliverException, dex_pc);
+      return true;
+    }
+
+    case Instruction::INSTANCE_OF: {
+      break;
+    }
+
+    case Instruction::CHECK_CAST: {
+      uint8_t reference = instruction.VRegA_21c();
+      dex::TypeIndex type_index(instruction.VRegB_21c());
+      return BuildCheckCast(reference, type_index, dex_pc);
+    }
+
+    case Instruction::MONITOR_ENTER: {
+      break;
+    }
+
+    case Instruction::MONITOR_EXIT: {
+      break;
+    }
+
+    case Instruction::SPARSE_SWITCH:
+    case Instruction::PACKED_SWITCH: {
+      break;
+    }
+
+    case Instruction::UNUSED_3E ... Instruction::UNUSED_43:
+    case Instruction::UNUSED_73:
+    case Instruction::UNUSED_79:
+    case Instruction::UNUSED_7A:
+    case Instruction::UNUSED_E3 ... Instruction::UNUSED_F9: {
+      break;
+    }
+  }
+  unimplemented_reason_ = instruction.Name();
+  return false;
+}  // NOLINT(readability/fn_size)
+
+bool FastCompilerARM64::Compile() {
+  if (!InitializeParameters()) {
+    DCHECK(HitUnimplemented());
+    AbortCompilation();
+    return false;
+  }
+  if (!ProcessInstructions()) {
+    DCHECK(HitUnimplemented());
+    AbortCompilation();
+    return false;
+  }
+  DCHECK(!HitUnimplemented()) << GetUnimplementedReason();
+  if (!has_frame_) {
+    code_generation_data_->GetStackMapStream()->BeginMethod(/* frame_size= */ 0u,
+                                                            /* core_spill_mask= */ 0u,
+                                                            /* fp_spill_mask= */ 0u,
+                                                            GetCodeItemAccessor().RegistersSize(),
+                                                            /* is_compiling_baseline= */ true,
+                                                            /* is_debuggable= */ false);
+  }
+  code_generation_data_->GetStackMapStream()->EndMethod(assembler_.CodeSize());
+  assembler_.FinalizeCode();
+
+  if (VLOG_IS_ON(jit)) {
+    // Dump the generated code
+    {
+      ScopedObjectAccess soa(Thread::Current());
+      VLOG(jit) << "Dumping generated fast baseline code for " << method_->PrettyMethod();
+    }
+    FILE* file = tmpfile();
+    MacroAssembler* masm = GetVIXLAssembler();
+    PrintDisassembler print_disasm(file);
+    vixl::aarch64::Instruction* dis_start =
+        masm->GetBuffer()->GetStartAddress<vixl::aarch64::Instruction*>();
+    vixl::aarch64::Instruction* dis_end =
+        masm->GetBuffer()->GetEndAddress<vixl::aarch64::Instruction*>();
+    print_disasm.DisassembleBuffer(dis_start, dis_end);
+    fseek(file, 0L, SEEK_SET);
+    char buffer[1024];
+    const char* line;
+    while ((line = fgets(buffer, sizeof(buffer), file)) != nullptr) {
+      VLOG(jit) << std::string(line);
+    }
+    fclose(file);
+  }
+  return true;
+}
+
+}  // namespace arm64
+
+std::unique_ptr<FastCompiler> FastCompiler::CompileARM64(
+    ArtMethod* method,
+    ArenaAllocator* allocator,
+    ArenaStack* arena_stack,
+    VariableSizedHandleScope* handles,
+    const CompilerOptions& compiler_options,
+    const DexCompilationUnit& dex_compilation_unit) {
+  if (!compiler_options.GetImplicitNullChecks() ||
+      !compiler_options.GetImplicitStackOverflowChecks() ||
+      kUseTableLookupReadBarrier ||
+      !kReserveMarkingRegister ||
+      kPoisonHeapReferences) {
+    // Configurations we don't support.
+    return nullptr;
+  }
+  std::unique_ptr<arm64::FastCompilerARM64> compiler(new arm64::FastCompilerARM64(
+      method,
+      allocator,
+      arena_stack,
+      handles,
+      compiler_options,
+      dex_compilation_unit));
+  if (compiler->Compile()) {
+    return compiler;
+  }
+  VLOG(jit) << "Did not fast compile because of " << compiler->GetUnimplementedReason();
+  return nullptr;
+}
+
+}  // namespace art
diff --git a/compiler/optimizing/graph_visualizer.cc b/compiler/optimizing/graph_visualizer.cc
index a0ec99ffc3..a8d487e51a 100644
--- a/compiler/optimizing/graph_visualizer.cc
+++ b/compiler/optimizing/graph_visualizer.cc
@@ -512,6 +512,8 @@ class HGraphVisualizerPrinter final : public HGraphDelegateVisitor {
 
   void VisitCondition(HCondition* condition) override {
     StartAttributeStream("bias") << condition->GetBias();
+    StartAttributeStream("emitted_at_use_site")
+        << std::boolalpha << condition->IsEmittedAtUseSite() << std::noboolalpha;
   }
 
   void VisitIf(HIf* if_instr) override {
diff --git a/compiler/optimizing/gvn.cc b/compiler/optimizing/gvn.cc
index 8a5a4742ba..188bfa473d 100644
--- a/compiler/optimizing/gvn.cc
+++ b/compiler/optimizing/gvn.cc
@@ -41,7 +41,7 @@ class ValueSet : public ArenaObject<kArenaAllocGvn> {
       : allocator_(allocator),
         num_buckets_(kMinimumNumberOfBuckets),
         buckets_(allocator->AllocArray<Node*>(num_buckets_, kArenaAllocGvn)),
-        buckets_owned_(allocator, num_buckets_, false, kArenaAllocGvn),
+        buckets_owned_(ArenaBitVector::CreateFixedSize(allocator, num_buckets_, kArenaAllocGvn)),
         num_entries_(0u) {
     DCHECK(IsPowerOfTwo(num_buckets_));
     std::fill_n(buckets_, num_buckets_, nullptr);
@@ -54,7 +54,7 @@ class ValueSet : public ArenaObject<kArenaAllocGvn> {
       : allocator_(allocator),
         num_buckets_(other.IdealBucketCount()),
         buckets_(allocator->AllocArray<Node*>(num_buckets_, kArenaAllocGvn)),
-        buckets_owned_(allocator, num_buckets_, false, kArenaAllocGvn),
+        buckets_owned_(ArenaBitVector::CreateFixedSize(allocator, num_buckets_, kArenaAllocGvn)),
         num_entries_(0u) {
     DCHECK(IsPowerOfTwo(num_buckets_));
     PopulateFromInternal(other);
@@ -125,9 +125,13 @@ class ValueSet : public ArenaObject<kArenaAllocGvn> {
 
   // Removes all instructions in the set affected by the given side effects.
   void Kill(SideEffects side_effects) {
-    DeleteAllImpureWhich([side_effects](Node* node) {
-      return node->GetSideEffects().MayDependOn(side_effects);
-    });
+    // Nothing to do if the side effects don't have any change bit set, as MayDependOn will always
+    // return false.
+    if (side_effects.HasSideEffects()) {
+      DeleteAllImpureWhich([side_effects](Node* node) {
+        return node->GetSideEffects().MayDependOn(side_effects);
+      });
+    }
   }
 
   void Clear() {
@@ -338,7 +342,7 @@ class ValueSet : public ArenaObject<kArenaAllocGvn> {
   // Flags specifying which buckets were copied into the set from its parent.
   // If a flag is not set, the corresponding bucket points to entries in the
   // parent and must be cloned prior to making changes.
-  ArenaBitVector buckets_owned_;
+  BitVectorView<size_t> buckets_owned_;
 
   // The number of entries in the set.
   size_t num_entries_;
@@ -358,9 +362,18 @@ class GlobalValueNumberer : public ValueObject {
         allocator_(graph->GetArenaStack()),
         side_effects_(side_effects),
         sets_(graph->GetBlocks().size(), nullptr, allocator_.Adapter(kArenaAllocGvn)),
-        visited_blocks_(
-            &allocator_, graph->GetBlocks().size(), /* expandable= */ false, kArenaAllocGvn),
-        did_optimization_(false) {}
+        dominated_to_visit_(graph->GetBlocks().size(), allocator_.Adapter(kArenaAllocGvn)),
+        successors_to_visit_(graph->GetBlocks().size(), allocator_.Adapter(kArenaAllocGvn)),
+        free_sets_(ArenaBitVector::CreateFixedSize(
+            &allocator_, graph->GetBlocks().size(), kArenaAllocGvn)),
+        visited_blocks_(ArenaBitVector::CreateFixedSize(
+            &allocator_, graph->GetBlocks().size(), kArenaAllocGvn)),
+        did_optimization_(false) {
+    for (HBasicBlock* block : graph->GetReversePostOrder()) {
+      dominated_to_visit_[block->GetBlockId()] = block->GetDominatedBlocks().size();
+      successors_to_visit_[block->GetBlockId()] = block->GetSuccessors().size();
+    }
+  }
 
   bool Run();
 
@@ -383,26 +396,34 @@ class GlobalValueNumberer : public ValueObject {
     DCHECK(sets_[block->GetBlockId()] != nullptr)
         << "Block B" << block->GetBlockId() << " expected to have a set";
     sets_[block->GetBlockId()] = nullptr;
+    free_sets_.ClearBit(block->GetBlockId());
   }
 
   // Returns false if the GlobalValueNumberer has already visited all blocks
   // which may reference `block`.
-  bool WillBeReferencedAgain(HBasicBlock* block) const;
+  bool WillBeReferencedAgain(uint32_t block_id) const;
 
   // Iterates over visited blocks and finds one which has a ValueSet such that:
   // (a) it will not be referenced in the future, and
   // (b) it can hold a copy of `reference_set` with a reasonable load factor.
-  HBasicBlock* FindVisitedBlockWithRecyclableSet(HBasicBlock* block,
-                                                 const ValueSet& reference_set) const;
+  HBasicBlock* FindVisitedBlockWithRecyclableSet(const ValueSet& reference_set) const;
 
   // ValueSet for blocks. Initially null, but for an individual block they
   // are allocated and populated by the dominator, and updated by all blocks
   // in the path from the dominator to the block.
   ScopedArenaVector<ValueSet*> sets_;
 
+  // Extra bookkeeping to speed up GVN, indexed by block id.
+  // Number of dominated blocks left to visit.
+  ScopedArenaVector<uint32_t> dominated_to_visit_;
+  // Number of successor blocks left to visit.
+  ScopedArenaVector<uint32_t> successors_to_visit_;
+  // True iff the block's ValueSet is free to be reused by another block.
+  BitVectorView<size_t> free_sets_;
+
   // BitVector which serves as a fast-access map from block id to
   // visited/unvisited Boolean.
-  ArenaBitVector visited_blocks_;
+  BitVectorView<size_t> visited_blocks_;
 
   // True if GVN did at least one removal.
   bool did_optimization_;
@@ -445,7 +466,7 @@ void GlobalValueNumberer::VisitBasicBlock(HBasicBlock* block) {
       // Try to find a basic block which will never be referenced again and whose
       // ValueSet can therefore be recycled. We will need to copy `dominator_set`
       // into the recycled set, so we pass `dominator_set` as a reference for size.
-      HBasicBlock* recyclable = FindVisitedBlockWithRecyclableSet(block, *dominator_set);
+      HBasicBlock* recyclable = FindVisitedBlockWithRecyclableSet(*dominator_set);
       if (recyclable == nullptr) {
         // No block with a suitable ValueSet found. Allocate a new one and
         // copy `dominator_set` into it.
@@ -524,54 +545,56 @@ void GlobalValueNumberer::VisitBasicBlock(HBasicBlock* block) {
   }
 
   visited_blocks_.SetBit(block->GetBlockId());
-}
-
-bool GlobalValueNumberer::WillBeReferencedAgain(HBasicBlock* block) const {
-  DCHECK(visited_blocks_.IsBitSet(block->GetBlockId()));
 
-  for (const HBasicBlock* dominated_block : block->GetDominatedBlocks()) {
-    if (!visited_blocks_.IsBitSet(dominated_block->GetBlockId())) {
-      return true;
+  // Bookkeeping to mark ValueSets to be reused. We mark them as free if the dominator /
+  // predecessor will never be referenced again, and if it hasn't been reused already by this
+  // method (See the `dominator->GetSuccessors().size() == 1` case).
+  if (block->GetDominator() != nullptr) {
+    const uint32_t id = block->GetDominator()->GetBlockId();
+    dominated_to_visit_[id]--;
+    if (!WillBeReferencedAgain(id) && sets_[id] != nullptr) {
+      free_sets_.SetBit(id);
     }
   }
-
-  for (const HBasicBlock* successor : block->GetSuccessors()) {
-    if (!visited_blocks_.IsBitSet(successor->GetBlockId())) {
-      return true;
+  for (HBasicBlock* pred : predecessors) {
+    const uint32_t id = pred->GetBlockId();
+    successors_to_visit_[id]--;
+    if (!WillBeReferencedAgain(id) && sets_[id] != nullptr) {
+      free_sets_.SetBit(id);
     }
   }
+}
 
-  return false;
+bool GlobalValueNumberer::WillBeReferencedAgain(uint32_t block_id) const {
+  // Block itself hasn't been visited, or
+  // a dominated block has yet to be visited, or
+  // a successor is yet to be visited.
+  return !visited_blocks_.IsBitSet(block_id) ||
+         dominated_to_visit_[block_id] != 0 ||
+         successors_to_visit_[block_id] != 0;
 }
 
 HBasicBlock* GlobalValueNumberer::FindVisitedBlockWithRecyclableSet(
-    HBasicBlock* block, const ValueSet& reference_set) const {
+    const ValueSet& reference_set) const {
   HBasicBlock* secondary_match = nullptr;
 
-  for (size_t block_id : visited_blocks_.Indexes()) {
+  // TODO(solanes): If we keep `free_sets_` sorted by size we could do a binary search instead of a
+  // linear one. Note that while a HashMap<size, free_sets> is better for knowing if there's an
+  // exact match, that data structure is worse for the exact_match=false case.
+  for (size_t block_id : free_sets_.Indexes()) {
+    DCHECK(!WillBeReferencedAgain(block_id));
     ValueSet* current_set = sets_[block_id];
-    if (current_set == nullptr) {
-      // Set was already recycled.
-      continue;
-    }
-
-    HBasicBlock* current_block = block->GetGraph()->GetBlocks()[block_id];
+    DCHECK_NE(current_set, nullptr);
 
     // We test if `current_set` has enough buckets to store a copy of
     // `reference_set` with a reasonable load factor. If we find a set whose
     // number of buckets matches perfectly, we return right away. If we find one
     // that is larger, we return it if no perfectly-matching set is found.
-    // Note that we defer testing WillBeReferencedAgain until all other criteria
-    // have been satisfied because it might be expensive.
     if (current_set->CanHoldCopyOf(reference_set, /* exact_match= */ true)) {
-      if (!WillBeReferencedAgain(current_block)) {
-        return current_block;
-      }
+      return graph_->GetBlocks()[block_id];
     } else if (secondary_match == nullptr &&
                current_set->CanHoldCopyOf(reference_set, /* exact_match= */ false)) {
-      if (!WillBeReferencedAgain(current_block)) {
-        secondary_match = current_block;
-      }
+      secondary_match = graph_->GetBlocks()[block_id];
     }
   }
 
diff --git a/compiler/optimizing/induction_var_range.cc b/compiler/optimizing/induction_var_range.cc
index 2f5a22ec0e..1fa2724641 100644
--- a/compiler/optimizing/induction_var_range.cc
+++ b/compiler/optimizing/induction_var_range.cc
@@ -48,10 +48,11 @@ static bool IsSafeDiv(int32_t c1, int32_t c2) {
 
 /** Computes a * b for a,b > 0 (at least until first overflow happens). */
 static int64_t SafeMul(int64_t a, int64_t b, /*out*/ bool* overflow) {
-  if (a > 0 && b > 0 && a > (std::numeric_limits<int64_t>::max() / b)) {
+  int64_t result;
+  if (__builtin_mul_overflow(a, b, &result)) {
     *overflow = true;
   }
-  return a * b;
+  return result;
 }
 
 /** Returns b^e for b,e > 0. Sets overflow if arithmetic wrap-around occurred. */
diff --git a/compiler/optimizing/inliner.cc b/compiler/optimizing/inliner.cc
index d679261d42..251867d512 100644
--- a/compiler/optimizing/inliner.cc
+++ b/compiler/optimizing/inliner.cc
@@ -22,6 +22,7 @@
 #include "builder.h"
 #include "class_linker.h"
 #include "class_root-inl.h"
+#include "compiler_callbacks.h"
 #include "constant_folding.h"
 #include "data_type-inl.h"
 #include "dead_code_elimination.h"
@@ -407,10 +408,15 @@ static bool IsMethodVerified(ArtMethod* method)
   // At runtime, we know this is cold code if the class is not verified, so don't
   // bother analyzing.
   if (Runtime::Current()->IsAotCompiler()) {
-    if (method->GetDeclaringClass()->IsVerifiedNeedsAccessChecks() ||
-        method->GetDeclaringClass()->ShouldVerifyAtRuntime()) {
+    if (method->GetDeclaringClass()->IsVerifiedNeedsAccessChecks()) {
+      DCHECK(!Runtime::Current()->GetCompilerCallbacks()->IsUncompilableMethod(
+                  MethodReference(method->GetDexFile(), method->GetDexMethodIndex())));
       return true;
     }
+    if (method->GetDeclaringClass()->ShouldVerifyAtRuntime()) {
+      return !Runtime::Current()->GetCompilerCallbacks()->IsUncompilableMethod(
+          MethodReference(method->GetDexFile(), method->GetDexMethodIndex()));
+    }
   }
   return false;
 }
@@ -801,11 +807,9 @@ HInliner::InlineCacheType HInliner::GetInlineCacheAOT(
   return GetInlineCacheType(*classes);
 }
 
-HInstanceFieldGet* HInliner::BuildGetReceiverClass(ClassLinker* class_linker,
-                                                   HInstruction* receiver,
+HInstanceFieldGet* HInliner::BuildGetReceiverClass(HInstruction* receiver,
                                                    uint32_t dex_pc) const {
-  ArtField* field = GetClassRoot<mirror::Object>(class_linker)->GetInstanceField(0);
-  DCHECK_EQ(std::string(field->GetName()), "shadow$_klass_");
+  ArtField* field = WellKnownClasses::java_lang_Object_shadowKlass;
   HInstanceFieldGet* result = new (graph_->GetAllocator()) HInstanceFieldGet(
       receiver,
       field,
@@ -955,7 +959,7 @@ HInstruction* HInliner::AddTypeGuard(HInstruction* receiver,
                                      bool with_deoptimization) {
   ClassLinker* class_linker = caller_compilation_unit_.GetClassLinker();
   HInstanceFieldGet* receiver_class = BuildGetReceiverClass(
-      class_linker, receiver, invoke_instruction->GetDexPc());
+      receiver, invoke_instruction->GetDexPc());
   if (cursor != nullptr) {
     bb_cursor->InsertInstructionAfter(receiver_class, cursor);
   } else {
@@ -1270,7 +1274,7 @@ bool HInliner::TryInlinePolymorphicCallToSameTarget(
 
   // We successfully inlined, now add a guard.
   HInstanceFieldGet* receiver_class = BuildGetReceiverClass(
-      class_linker, receiver, invoke_instruction->GetDexPc());
+      receiver, invoke_instruction->GetDexPc());
 
   DataType::Type type = Is64BitInstructionSet(graph_->GetInstructionSet())
       ? DataType::Type::kInt64
@@ -1593,6 +1597,8 @@ bool HInliner::TryBuildAndInline(HInvoke* invoke_instruction,
                                  ReferenceTypeInfo receiver_type,
                                  HInstruction** return_replacement,
                                  bool is_speculative) {
+  DCHECK_IMPLIES(method->IsStatic(), !receiver_type.IsValid());
+  DCHECK_IMPLIES(!method->IsStatic(), receiver_type.IsValid());
   // If invoke_instruction is devirtualized to a different method, give intrinsics
   // another chance before we try to inline it.
   if (invoke_instruction->GetResolvedMethod() != method &&
@@ -2450,8 +2456,8 @@ bool HInliner::ReturnTypeMoreSpecific(HInstruction* return_replacement,
         return true;
       } else if (return_replacement->IsInstanceFieldGet()) {
         HInstanceFieldGet* field_get = return_replacement->AsInstanceFieldGet();
-        if (field_get->GetFieldInfo().GetField() ==
-                GetClassRoot<mirror::Object>()->GetInstanceField(0)) {
+        ArtField* cls_field = WellKnownClasses::java_lang_Object_shadowKlass;
+        if (field_get->GetFieldInfo().GetField() == cls_field) {
           return true;
         }
       }
diff --git a/compiler/optimizing/inliner.h b/compiler/optimizing/inliner.h
index 57d3364051..2ca286ea6a 100644
--- a/compiler/optimizing/inliner.h
+++ b/compiler/optimizing/inliner.h
@@ -246,9 +246,7 @@ class HInliner : public HOptimization {
                    HInstruction* cursor,
                    HBasicBlock* bb_cursor);
 
-  HInstanceFieldGet* BuildGetReceiverClass(ClassLinker* class_linker,
-                                           HInstruction* receiver,
-                                           uint32_t dex_pc) const
+  HInstanceFieldGet* BuildGetReceiverClass(HInstruction* receiver, uint32_t dex_pc) const
     REQUIRES_SHARED(Locks::mutator_lock_);
 
   void MaybeRunReferenceTypePropagation(HInstruction* replacement,
diff --git a/compiler/optimizing/instruction_builder.cc b/compiler/optimizing/instruction_builder.cc
index c66fd3bb26..8cc79c2424 100644
--- a/compiler/optimizing/instruction_builder.cc
+++ b/compiler/optimizing/instruction_builder.cc
@@ -32,6 +32,7 @@
 #include "handle_cache-inl.h"
 #include "imtable-inl.h"
 #include "intrinsics.h"
+#include "intrinsics_enum.h"
 #include "intrinsics_utils.h"
 #include "jit/jit.h"
 #include "jit/profiling_info.h"
@@ -304,7 +305,7 @@ void HInstructionBuilder::InitializeInstruction(HInstruction* instruction) {
         graph_->GetArtMethod(),
         instruction->GetDexPc(),
         instruction);
-    environment->CopyFrom(ArrayRef<HInstruction* const>(*current_locals_));
+    environment->CopyFrom(allocator_, ArrayRef<HInstruction* const>(*current_locals_));
     instruction->SetRawEnvironment(environment);
   }
 }
@@ -992,6 +993,10 @@ static ArtMethod* ResolveMethod(uint16_t method_idx,
         (!resolved_method->IsPublic() && !declaring_class_accessible)) {
       return nullptr;
     }
+
+    if (UNLIKELY(resolved_method->CheckIncompatibleClassChange(*invoke_type))) {
+      return nullptr;
+    }
   }
 
   // We have to special case the invoke-super case, as ClassLinker::ResolveMethod does not.
@@ -1047,6 +1052,23 @@ static ArtMethod* ResolveMethod(uint16_t method_idx,
   return resolved_method;
 }
 
+static bool IsSignaturePolymorphic(ArtMethod* method) {
+  if (!method->IsIntrinsic()) {
+    return false;
+  }
+  Intrinsics intrinsic = method->GetIntrinsic();
+
+  switch (intrinsic) {
+#define IS_POLYMOPHIC(Name, ...) \
+    case Intrinsics::k ## Name:
+      ART_SIGNATURE_POLYMORPHIC_INTRINSICS_LIST(IS_POLYMOPHIC)
+#undef IS_POLYMOPHIC
+      return true;
+    default:
+      return false;
+  }
+}
+
 bool HInstructionBuilder::BuildInvoke(const Instruction& instruction,
                                       uint32_t dex_pc,
                                       uint32_t method_idx,
@@ -1074,10 +1096,28 @@ bool HInstructionBuilder::BuildInvoke(const Instruction& instruction,
                                              &is_string_constructor);
 
   MethodReference method_reference(&graph_->GetDexFile(), method_idx);
-  if (UNLIKELY(resolved_method == nullptr)) {
+
+  // In the wild there are apps which have invoke-virtual targeting signature polymorphic methods
+  // like MethodHandle.invokeExact. It never worked in the first place: such calls were dispatched
+  // to the JNI implementation, which throws UOE.
+  // Now, when a signature-polymorphic method is implemented as an intrinsic, compiler's attempt to
+  // devirtualize such ill-formed virtual calls can lead to compiler crashes as an intrinsic
+  // (like MethodHandle.invokeExact) might expect arguments to be set up in a different manner than
+  // it's done for virtual calls.
+  // Create HInvokeUnresolved to make sure that such invoke-virtual calls are not devirtualized
+  // and are treated as native method calls.
+  if (kIsDebugBuild && resolved_method != nullptr) {
+    ScopedObjectAccess soa(Thread::Current());
+    CHECK_EQ(IsSignaturePolymorphic(resolved_method), resolved_method->IsSignaturePolymorphic());
+  }
+
+  if (UNLIKELY(resolved_method == nullptr ||
+               (invoke_type != kPolymorphic && IsSignaturePolymorphic(resolved_method)))) {
     DCHECK(!Thread::Current()->IsExceptionPending());
-    MaybeRecordStat(compilation_stats_,
-                    MethodCompilationStat::kUnresolvedMethod);
+    if (resolved_method == nullptr) {
+      MaybeRecordStat(compilation_stats_,
+                      MethodCompilationStat::kUnresolvedMethod);
+    }
     HInvoke* invoke = new (allocator_) HInvokeUnresolved(allocator_,
                                                          number_of_arguments,
                                                          operands.GetNumberOfOperands(),
@@ -1594,7 +1634,7 @@ static bool HasTrivialClinit(ObjPtr<mirror::Class> klass, PointerSize pointer_si
     REQUIRES_SHARED(Locks::mutator_lock_) {
   // Check if the class has encoded fields that trigger bytecode execution.
   // (Encoded fields are just a different representation of <clinit>.)
-  if (klass->NumStaticFields() != 0u) {
+  if (klass->HasStaticFields()) {
     DCHECK(klass->GetClassDef() != nullptr);
     EncodedStaticFieldValueIterator it(klass->GetDexFile(), *klass->GetClassDef());
     for (; it.HasNext(); it.Next()) {
@@ -2502,9 +2542,9 @@ HNewArray* HInstructionBuilder::BuildNewArray(uint32_t dex_pc,
   return new_array;
 }
 
-HNewArray* HInstructionBuilder::BuildFilledNewArray(uint32_t dex_pc,
-                                                    dex::TypeIndex type_index,
-                                                    const InstructionOperands& operands) {
+bool HInstructionBuilder::BuildFilledNewArray(uint32_t dex_pc,
+                                              dex::TypeIndex type_index,
+                                              const InstructionOperands& operands) {
   const size_t number_of_operands = operands.GetNumberOfOperands();
   HInstruction* length = graph_->GetIntConstant(number_of_operands);
 
@@ -2512,9 +2552,11 @@ HNewArray* HInstructionBuilder::BuildFilledNewArray(uint32_t dex_pc,
   const char* descriptor = dex_file_->GetTypeDescriptor(type_index);
   DCHECK_EQ(descriptor[0], '[') << descriptor;
   char primitive = descriptor[1];
-  DCHECK(primitive == 'I'
-      || primitive == 'L'
-      || primitive == '[') << descriptor;
+  if (primitive != 'I' && primitive != 'L' && primitive != '[') {
+    DCHECK(primitive != 'J' && primitive != 'D');  // Rejected by the verifier.
+    MaybeRecordStat(compilation_stats_, MethodCompilationStat::kNotCompiledMalformedOpcode);
+    return false;
+  }
   bool is_reference_array = (primitive == 'L') || (primitive == '[');
   DataType::Type type = is_reference_array ? DataType::Type::kReference : DataType::Type::kInt32;
 
@@ -2527,7 +2569,8 @@ HNewArray* HInstructionBuilder::BuildFilledNewArray(uint32_t dex_pc,
   }
   latest_result_ = new_array;
 
-  return new_array;
+  BuildConstructorFenceForAllocation(new_array);
+  return true;
 }
 
 template <typename T>
@@ -3680,16 +3723,18 @@ bool HInstructionBuilder::ProcessDexInstruction(const Instruction& instruction,
       uint32_t args[5];
       uint32_t number_of_vreg_arguments = instruction.GetVarArgs(args);
       VarArgsInstructionOperands operands(args, number_of_vreg_arguments);
-      HNewArray* new_array = BuildFilledNewArray(dex_pc, type_index, operands);
-      BuildConstructorFenceForAllocation(new_array);
+      if (!BuildFilledNewArray(dex_pc, type_index, operands)) {
+        return false;
+      }
       break;
     }
 
     case Instruction::FILLED_NEW_ARRAY_RANGE: {
       dex::TypeIndex type_index(instruction.VRegB_3rc());
       RangeInstructionOperands operands(instruction.VRegC_3rc(), instruction.VRegA_3rc());
-      HNewArray* new_array = BuildFilledNewArray(dex_pc, type_index, operands);
-      BuildConstructorFenceForAllocation(new_array);
+      if (!BuildFilledNewArray(dex_pc, type_index, operands)) {
+        return false;
+      }
       break;
     }
 
diff --git a/compiler/optimizing/instruction_builder.h b/compiler/optimizing/instruction_builder.h
index b3b5deae59..90ab75ec92 100644
--- a/compiler/optimizing/instruction_builder.h
+++ b/compiler/optimizing/instruction_builder.h
@@ -177,9 +177,9 @@ class HInstructionBuilder : public ValueObject {
   HNewArray* BuildNewArray(uint32_t dex_pc, dex::TypeIndex type_index, HInstruction* length);
 
   // Builds a new array node and the instructions that fill it.
-  HNewArray* BuildFilledNewArray(uint32_t dex_pc,
-                                 dex::TypeIndex type_index,
-                                 const InstructionOperands& operands);
+  bool BuildFilledNewArray(uint32_t dex_pc,
+                           dex::TypeIndex type_index,
+                           const InstructionOperands& operands);
 
   void BuildFillArrayData(const Instruction& instruction, uint32_t dex_pc);
 
diff --git a/compiler/optimizing/instruction_simplifier.cc b/compiler/optimizing/instruction_simplifier.cc
index 703f6c77e8..4ef0fc907a 100644
--- a/compiler/optimizing/instruction_simplifier.cc
+++ b/compiler/optimizing/instruction_simplifier.cc
@@ -301,7 +301,7 @@ bool InstructionSimplifierVisitor::TryCombineVecMultiplyAccumulate(HVecMul* mul)
       return false;
   }
 
-  ArenaAllocator* allocator = mul->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = GetGraph()->GetAllocator();
   if (!mul->HasOnlyOneNonEnvironmentUse()) {
     return false;
   }
@@ -1800,8 +1800,7 @@ static bool RecognizeAndSimplifyClassCheck(HCondition* condition) {
 
   {
     ScopedObjectAccess soa(Thread::Current());
-    ArtField* field = GetClassRoot<mirror::Object>()->GetInstanceField(0);
-    DCHECK_EQ(std::string(field->GetName()), "shadow$_klass_");
+    ArtField* field = WellKnownClasses::java_lang_Object_shadowKlass;
     if (field_get->GetFieldInfo().GetField() != field) {
       return false;
     }
@@ -3562,7 +3561,7 @@ bool InstructionSimplifierVisitor::TryHandleAssociativeAndCommutativeOperation(
   return true;
 }
 
-static HBinaryOperation* AsAddOrSub(HInstruction* binop) {
+static HBinaryOperation* AsAddOrSubOrNull(HInstruction* binop) {
   return (binop->IsAdd() || binop->IsSub()) ? binop->AsBinaryOperation() : nullptr;
 }
 
@@ -3607,9 +3606,9 @@ bool InstructionSimplifierVisitor::TrySubtractionChainSimplification(
     return false;
   }
 
-  HBinaryOperation* y = (AsAddOrSub(left) != nullptr)
+  HBinaryOperation* y = (AsAddOrSubOrNull(left) != nullptr)
       ? left->AsBinaryOperation()
-      : AsAddOrSub(right);
+      : AsAddOrSubOrNull(right);
   // If y has more than one use, we do not perform the optimization because
   // it might increase code size (e.g. if the new constant is no longer
   // encodable as an immediate operand in the target ISA).
@@ -3638,8 +3637,8 @@ bool InstructionSimplifierVisitor::TrySubtractionChainSimplification(
   bool is_x_negated = is_y_negated ^ ((x == right) && y->IsSub());
   int64_t const3_val = ComputeAddition(type, const1_val, const2_val);
   HBasicBlock* block = instruction->GetBlock();
-  HConstant* const3 = block->GetGraph()->GetConstant(type, const3_val);
-  ArenaAllocator* allocator = instruction->GetAllocator();
+  HConstant* const3 = GetGraph()->GetConstant(type, const3_val);
+  ArenaAllocator* allocator = GetGraph()->GetAllocator();
   HInstruction* z;
 
   if (is_x_negated) {
diff --git a/compiler/optimizing/instruction_simplifier_test.cc b/compiler/optimizing/instruction_simplifier_test.cc
index a2e3882c19..448dc32006 100644
--- a/compiler/optimizing/instruction_simplifier_test.cc
+++ b/compiler/optimizing/instruction_simplifier_test.cc
@@ -234,9 +234,9 @@ TEST_P(InstanceOfInstructionSimplifierTestGroup, ExactClassCheckCastOther) {
 
 INSTANTIATE_TEST_SUITE_P(InstructionSimplifierTest,
                          InstanceOfInstructionSimplifierTestGroup,
-                         testing::Values(InstanceOfKind::kSelf,
-                                         InstanceOfKind::kUnrelatedLoaded,
-                                         InstanceOfKind::kUnrelatedUnloaded,
-                                         InstanceOfKind::kSupertype));
+                         ::testing::Values(InstanceOfKind::kSelf,
+                                           InstanceOfKind::kUnrelatedLoaded,
+                                           InstanceOfKind::kUnrelatedUnloaded,
+                                           InstanceOfKind::kSupertype));
 
 }  // namespace art
diff --git a/compiler/optimizing/intrinsics.cc b/compiler/optimizing/intrinsics.cc
index b87f6f3975..edd454c93e 100644
--- a/compiler/optimizing/intrinsics.cc
+++ b/compiler/optimizing/intrinsics.cc
@@ -172,19 +172,11 @@ IntrinsicVisitor::ValueOfInfo IntrinsicVisitor::ComputeValueOfInfo(
 }
 
 MemberOffset IntrinsicVisitor::GetReferenceDisableIntrinsicOffset() {
-  ScopedObjectAccess soa(Thread::Current());
-  // The "disableIntrinsic" is the first static field.
-  ArtField* field = GetClassRoot<mirror::Reference>()->GetStaticField(0);
-  DCHECK_STREQ(field->GetName(), "disableIntrinsic");
-  return field->GetOffset();
+  return WellKnownClasses::java_lang_ref_Reference_disableIntrinsic->GetOffset();
 }
 
 MemberOffset IntrinsicVisitor::GetReferenceSlowPathEnabledOffset() {
-  ScopedObjectAccess soa(Thread::Current());
-  // The "slowPathEnabled" is the second static field.
-  ArtField* field = GetClassRoot<mirror::Reference>()->GetStaticField(1);
-  DCHECK_STREQ(field->GetName(), "slowPathEnabled");
-  return field->GetOffset();
+  return WellKnownClasses::java_lang_ref_Reference_slowPathEnabled->GetOffset();
 }
 
 void IntrinsicVisitor::CreateReferenceGetReferentLocations(HInvoke* invoke,
@@ -206,7 +198,7 @@ void IntrinsicVisitor::CreateReferenceRefersToLocations(HInvoke* invoke, CodeGen
     return;
   }
 
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations =
       new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
diff --git a/compiler/optimizing/intrinsics.h b/compiler/optimizing/intrinsics.h
index 7a27b2506b..4f164e10c6 100644
--- a/compiler/optimizing/intrinsics.h
+++ b/compiler/optimizing/intrinsics.h
@@ -270,7 +270,7 @@ class VarHandleOptimizations : public IntrinsicOptimizations {
   INTRINSIC_OPTIMIZATION(DoNotIntrinsify, 0);  // One of the checks is statically known to fail.
   INTRINSIC_OPTIMIZATION(SkipObjectNullCheck, 1);  // Not applicable for static fields.
 
-  // Use known `VarHandle` from the boot image. To apply this optimization, the following
+  // Use known `VarHandle` from the boot/app image. To apply this optimization, the following
   // `VarHandle` checks must pass based on static analysis:
   //   - `VarHandle` type check (must match the coordinate count),
   //   - access mode check,
diff --git a/compiler/optimizing/intrinsics_arm64.cc b/compiler/optimizing/intrinsics_arm64.cc
index 1974d75c47..4eb73019fd 100644
--- a/compiler/optimizing/intrinsics_arm64.cc
+++ b/compiler/optimizing/intrinsics_arm64.cc
@@ -16,6 +16,8 @@
 
 #include "intrinsics_arm64.h"
 
+#include "aarch64/assembler-aarch64.h"
+#include "aarch64/operands-aarch64.h"
 #include "arch/arm64/callee_save_frame_arm64.h"
 #include "arch/arm64/instruction_set_features_arm64.h"
 #include "art_method.h"
@@ -23,6 +25,7 @@
 #include "code_generator_arm64.h"
 #include "common_arm64.h"
 #include "data_type-inl.h"
+#include "dex/modifiers.h"
 #include "entrypoints/quick/quick_entrypoints.h"
 #include "heap_poisoning.h"
 #include "intrinsic_objects.h"
@@ -30,7 +33,10 @@
 #include "intrinsics_utils.h"
 #include "lock_word.h"
 #include "mirror/array-inl.h"
+#include "mirror/class.h"
 #include "mirror/method_handle_impl.h"
+#include "mirror/method_type.h"
+#include "mirror/object.h"
 #include "mirror/object_array-inl.h"
 #include "mirror/reference.h"
 #include "mirror/string-inl.h"
@@ -181,8 +187,7 @@ class InvokePolymorphicSlowPathARM64 : public SlowPathCodeARM64 {
     // Passing `MethodHandle` object as hidden argument.
     __ Mov(w0, method_handle_.W());
     codegen->InvokeRuntime(QuickEntrypointEnum::kQuickInvokePolymorphicWithHiddenReceiver,
-                           instruction_,
-                           instruction_->GetDexPc());
+                           instruction_);
 
     RestoreLiveRegisters(codegen, instruction_->GetLocations());
     __ B(GetExitLabel());
@@ -198,12 +203,18 @@ class InvokePolymorphicSlowPathARM64 : public SlowPathCodeARM64 {
 #undef __
 
 bool IntrinsicLocationsBuilderARM64::TryDispatch(HInvoke* invoke) {
+#ifdef ART_USE_RESTRICTED_MODE
+  // TODO(Simulator): support intrinsics.
+  USE(invoke);
+  return false;
+#else
   Dispatch(invoke);
   LocationSummary* res = invoke->GetLocations();
   if (res == nullptr) {
     return false;
   }
   return res->Intrinsified();
+#endif  // ART_USE_RESTRICTED_MODE
 }
 
 #define __ masm->
@@ -1018,8 +1029,8 @@ void IntrinsicLocationsBuilderARM64::VisitUnsafePut(HInvoke* invoke) {
 void IntrinsicLocationsBuilderARM64::VisitUnsafePutAbsolute(HInvoke* invoke) {
   VisitJdkUnsafePutAbsolute(invoke);
 }
-void IntrinsicLocationsBuilderARM64::VisitUnsafePutOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutOrdered(invoke);
+void IntrinsicLocationsBuilderARM64::VisitUnsafePutOrderedInt(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedInt(invoke);
 }
 void IntrinsicLocationsBuilderARM64::VisitUnsafePutVolatile(HInvoke* invoke) {
   VisitJdkUnsafePutVolatile(invoke);
@@ -1027,8 +1038,8 @@ void IntrinsicLocationsBuilderARM64::VisitUnsafePutVolatile(HInvoke* invoke) {
 void IntrinsicLocationsBuilderARM64::VisitUnsafePutObject(HInvoke* invoke) {
   VisitJdkUnsafePutReference(invoke);
 }
-void IntrinsicLocationsBuilderARM64::VisitUnsafePutObjectOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutObjectOrdered(invoke);
+void IntrinsicLocationsBuilderARM64::VisitUnsafePutOrderedObject(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedObject(invoke);
 }
 void IntrinsicLocationsBuilderARM64::VisitUnsafePutObjectVolatile(HInvoke* invoke) {
   VisitJdkUnsafePutReferenceVolatile(invoke);
@@ -1052,7 +1063,7 @@ void IntrinsicLocationsBuilderARM64::VisitJdkUnsafePut(HInvoke* invoke) {
 void IntrinsicLocationsBuilderARM64::VisitJdkUnsafePutAbsolute(HInvoke* invoke) {
   CreateUnsafePutAbsoluteLocations(allocator_, invoke);
 }
-void IntrinsicLocationsBuilderARM64::VisitJdkUnsafePutOrdered(HInvoke* invoke) {
+void IntrinsicLocationsBuilderARM64::VisitJdkUnsafePutOrderedInt(HInvoke* invoke) {
   CreateUnsafePutLocations(allocator_, invoke);
 }
 void IntrinsicLocationsBuilderARM64::VisitJdkUnsafePutVolatile(HInvoke* invoke) {
@@ -1064,7 +1075,7 @@ void IntrinsicLocationsBuilderARM64::VisitJdkUnsafePutRelease(HInvoke* invoke) {
 void IntrinsicLocationsBuilderARM64::VisitJdkUnsafePutReference(HInvoke* invoke) {
   CreateUnsafePutLocations(allocator_, invoke);
 }
-void IntrinsicLocationsBuilderARM64::VisitJdkUnsafePutObjectOrdered(HInvoke* invoke) {
+void IntrinsicLocationsBuilderARM64::VisitJdkUnsafePutOrderedObject(HInvoke* invoke) {
   CreateUnsafePutLocations(allocator_, invoke);
 }
 void IntrinsicLocationsBuilderARM64::VisitJdkUnsafePutReferenceVolatile(HInvoke* invoke) {
@@ -1164,8 +1175,8 @@ void IntrinsicCodeGeneratorARM64::VisitUnsafePut(HInvoke* invoke) {
 void IntrinsicCodeGeneratorARM64::VisitUnsafePutAbsolute(HInvoke* invoke) {
   VisitJdkUnsafePutAbsolute(invoke);
 }
-void IntrinsicCodeGeneratorARM64::VisitUnsafePutOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutOrdered(invoke);
+void IntrinsicCodeGeneratorARM64::VisitUnsafePutOrderedInt(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedInt(invoke);
 }
 void IntrinsicCodeGeneratorARM64::VisitUnsafePutVolatile(HInvoke* invoke) {
   VisitJdkUnsafePutVolatile(invoke);
@@ -1173,8 +1184,8 @@ void IntrinsicCodeGeneratorARM64::VisitUnsafePutVolatile(HInvoke* invoke) {
 void IntrinsicCodeGeneratorARM64::VisitUnsafePutObject(HInvoke* invoke) {
   VisitJdkUnsafePutReference(invoke);
 }
-void IntrinsicCodeGeneratorARM64::VisitUnsafePutObjectOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutObjectOrdered(invoke);
+void IntrinsicCodeGeneratorARM64::VisitUnsafePutOrderedObject(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedObject(invoke);
 }
 void IntrinsicCodeGeneratorARM64::VisitUnsafePutObjectVolatile(HInvoke* invoke) {
   VisitJdkUnsafePutReferenceVolatile(invoke);
@@ -1206,7 +1217,7 @@ void IntrinsicCodeGeneratorARM64::VisitJdkUnsafePutAbsolute(HInvoke* invoke) {
                        /*is_ordered=*/ false,
                        codegen_);
 }
-void IntrinsicCodeGeneratorARM64::VisitJdkUnsafePutOrdered(HInvoke* invoke) {
+void IntrinsicCodeGeneratorARM64::VisitJdkUnsafePutOrderedInt(HInvoke* invoke) {
   GenUnsafePut(invoke,
                DataType::Type::kInt32,
                /*is_volatile=*/ false,
@@ -1234,7 +1245,7 @@ void IntrinsicCodeGeneratorARM64::VisitJdkUnsafePutReference(HInvoke* invoke) {
                /*is_ordered=*/ false,
                codegen_);
 }
-void IntrinsicCodeGeneratorARM64::VisitJdkUnsafePutObjectOrdered(HInvoke* invoke) {
+void IntrinsicCodeGeneratorARM64::VisitJdkUnsafePutOrderedObject(HInvoke* invoke) {
   GenUnsafePut(invoke,
                DataType::Type::kReference,
                /*is_volatile=*/ false,
@@ -2473,7 +2484,7 @@ static void GenerateVisitStringIndexOf(HInvoke* invoke,
     __ Mov(tmp_reg, 0);
   }
 
-  codegen->InvokeRuntime(kQuickIndexOf, invoke, invoke->GetDexPc(), slow_path);
+  codegen->InvokeRuntime(kQuickIndexOf, invoke, slow_path);
   CheckEntrypointTypes<kQuickIndexOf, int32_t, void*, uint32_t, uint32_t>();
 
   if (slow_path != nullptr) {
@@ -2537,7 +2548,7 @@ void IntrinsicCodeGeneratorARM64::VisitStringNewStringFromBytes(HInvoke* invoke)
   codegen_->AddSlowPath(slow_path);
   __ B(eq, slow_path->GetEntryLabel());
 
-  codegen_->InvokeRuntime(kQuickAllocStringFromBytes, invoke, invoke->GetDexPc(), slow_path);
+  codegen_->InvokeRuntime(kQuickAllocStringFromBytes, invoke, slow_path);
   CheckEntrypointTypes<kQuickAllocStringFromBytes, void*, void*, int32_t, int32_t, int32_t>();
   __ Bind(slow_path->GetExitLabel());
 }
@@ -2559,7 +2570,7 @@ void IntrinsicCodeGeneratorARM64::VisitStringNewStringFromChars(HInvoke* invoke)
   //   java.lang.StringFactory.newStringFromChars(int offset, int charCount, char[] data)
   //
   // all include a null check on `data` before calling that method.
-  codegen_->InvokeRuntime(kQuickAllocStringFromChars, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickAllocStringFromChars, invoke);
   CheckEntrypointTypes<kQuickAllocStringFromChars, void*, int32_t, int32_t, void*>();
 }
 
@@ -2582,7 +2593,7 @@ void IntrinsicCodeGeneratorARM64::VisitStringNewStringFromString(HInvoke* invoke
   codegen_->AddSlowPath(slow_path);
   __ B(eq, slow_path->GetEntryLabel());
 
-  codegen_->InvokeRuntime(kQuickAllocStringFromString, invoke, invoke->GetDexPc(), slow_path);
+  codegen_->InvokeRuntime(kQuickAllocStringFromString, invoke, slow_path);
   CheckEntrypointTypes<kQuickAllocStringFromString, void*, void*>();
   __ Bind(slow_path->GetExitLabel());
 }
@@ -2634,7 +2645,7 @@ static void CreateFPFPFPToFPLocations(ArenaAllocator* allocator, HInvoke* invoke
 static void GenFPToFPCall(HInvoke* invoke,
                           CodeGeneratorARM64* codegen,
                           QuickEntrypointEnum entry) {
-  codegen->InvokeRuntime(entry, invoke, invoke->GetDexPc());
+  codegen->InvokeRuntime(entry, invoke);
 }
 
 void IntrinsicLocationsBuilderARM64::VisitMathCos(HInvoke* invoke) {
@@ -2953,9 +2964,8 @@ void IntrinsicLocationsBuilderARM64::VisitSystemArrayCopyChar(HInvoke* invoke) {
     }
   }
 
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
   LocationSummary* locations =
-      new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
+      new (allocator_) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   // arraycopy(char[] src, int src_pos, char[] dst, int dst_pos, int length).
   locations->SetInAt(0, Location::RequiresRegister());
   locations->SetInAt(1, LocationForSystemArrayCopyInput(invoke->InputAt(1)));
@@ -3666,7 +3676,7 @@ void IntrinsicCodeGeneratorARM64::HandleValueOf(HInvoke* invoke,
   auto allocate_instance = [&]() {
     DCHECK(out.X().Is(InvokeRuntimeCallingConvention().GetRegisterAt(0)));
     codegen_->LoadIntrinsicDeclaringClass(out, invoke);
-    codegen_->InvokeRuntime(kQuickAllocObjectInitialized, invoke, invoke->GetDexPc());
+    codegen_->InvokeRuntime(kQuickAllocObjectInitialized, invoke);
     CheckEntrypointTypes<kQuickAllocObjectWithChecks, void*, mirror::Class*>();
   };
   if (invoke->InputAt(0)->IsIntConstant()) {
@@ -4860,7 +4870,7 @@ static void GenerateVarHandleTarget(HInvoke* invoke,
   if (expected_coordinates_count <= 1u) {
     if (VarHandleOptimizations(invoke).GetUseKnownImageVarHandle()) {
       ScopedObjectAccess soa(Thread::Current());
-      ArtField* target_field = GetBootImageVarHandleField(invoke);
+      ArtField* target_field = GetImageVarHandleField(invoke);
       if (expected_coordinates_count == 0u) {
         ObjPtr<mirror::Class> declaring_class = target_field->GetDeclaringClass();
         if (Runtime::Current()->GetHeap()->ObjectIsInBootImageSpace(declaring_class)) {
@@ -4917,7 +4927,7 @@ static LocationSummary* CreateVarHandleCommonLocations(HInvoke* invoke,
   size_t expected_coordinates_count = GetExpectedVarHandleCoordinatesCount(invoke);
   DataType::Type return_type = invoke->GetType();
 
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations =
       new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
@@ -5968,57 +5978,175 @@ void VarHandleSlowPathARM64::EmitByteArrayViewCode(CodeGenerator* codegen_in) {
 }
 
 void IntrinsicLocationsBuilderARM64::VisitMethodHandleInvokeExact(HInvoke* invoke) {
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
-  LocationSummary* locations = new (allocator)
+  LocationSummary* locations = new (allocator_)
       LocationSummary(invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
 
   InvokeDexCallingConventionVisitorARM64 calling_convention;
   locations->SetOut(calling_convention.GetReturnLocation(invoke->GetType()));
 
-  locations->SetInAt(0, Location::RequiresRegister());
-
   // Accomodating LocationSummary for underlying invoke-* call.
   uint32_t number_of_args = invoke->GetNumberOfArguments();
+
   for (uint32_t i = 1; i < number_of_args; ++i) {
     locations->SetInAt(i, calling_convention.GetNextLocation(invoke->InputAt(i)->GetType()));
   }
 
+  // Passing MethodHandle object as the last parameter: accessors implementation rely on it.
+  DCHECK_EQ(invoke->InputAt(0)->GetType(), DataType::Type::kReference);
+  Location receiver_mh_loc = calling_convention.GetNextLocation(DataType::Type::kReference);
+  locations->SetInAt(0, receiver_mh_loc);
+
   // The last input is MethodType object corresponding to the call-site.
   locations->SetInAt(number_of_args, Location::RequiresRegister());
 
-  locations->AddTemp(Location::RequiresRegister());
   locations->AddTemp(calling_convention.GetMethodLocation());
+  locations->AddRegisterTemps(4);
+
+  if (!receiver_mh_loc.IsRegister()) {
+    locations->AddTemp(Location::RequiresRegister());
+  }
 }
 
 void IntrinsicCodeGeneratorARM64::VisitMethodHandleInvokeExact(HInvoke* invoke) {
   LocationSummary* locations = invoke->GetLocations();
+  MacroAssembler* masm = codegen_->GetVIXLAssembler();
 
-  Register method_handle = InputRegisterAt(invoke, 0);
+  Location receiver_mh_loc = locations->InAt(0);
+  Register method_handle = receiver_mh_loc.IsRegister()
+      ? InputRegisterAt(invoke, 0)
+      : WRegisterFrom(locations->GetTemp(5));
+
+  if (!receiver_mh_loc.IsRegister()) {
+    DCHECK(receiver_mh_loc.IsStackSlot());
+    __ Ldr(method_handle.W(), MemOperand(sp, receiver_mh_loc.GetStackIndex()));
+  }
 
   SlowPathCodeARM64* slow_path =
       new (codegen_->GetScopedAllocator()) InvokePolymorphicSlowPathARM64(invoke, method_handle);
   codegen_->AddSlowPath(slow_path);
-  MacroAssembler* masm = codegen_->GetVIXLAssembler();
 
   Register call_site_type = InputRegisterAt(invoke, invoke->GetNumberOfArguments());
 
   // Call site should match with MethodHandle's type.
-  Register temp = WRegisterFrom(locations->GetTemp(0));
+  Register temp = WRegisterFrom(locations->GetTemp(1));
   __ Ldr(temp, HeapOperand(method_handle.W(), mirror::MethodHandle::MethodTypeOffset()));
   codegen_->GetAssembler()->MaybeUnpoisonHeapReference(temp);
   __ Cmp(call_site_type, temp);
   __ B(ne, slow_path->GetEntryLabel());
 
-  __ Ldr(temp, HeapOperand(method_handle.W(), mirror::MethodHandle::HandleKindOffset()));
-  __ Cmp(temp, Operand(mirror::MethodHandle::Kind::kInvokeStatic));
-  __ B(ne, slow_path->GetEntryLabel());
-
-  Register method = XRegisterFrom(locations->GetTemp(1));
+  Register method = XRegisterFrom(locations->GetTemp(0));
   __ Ldr(method, HeapOperand(method_handle.W(), mirror::MethodHandle::ArtFieldOrMethodOffset()));
+
+  vixl::aarch64::Label execute_target_method;
+  vixl::aarch64::Label method_dispatch;
+
+  Register method_handle_kind = WRegisterFrom(locations->GetTemp(2));
+  __ Ldr(method_handle_kind,
+         HeapOperand(method_handle.W(), mirror::MethodHandle::HandleKindOffset()));
+
+  __ Cmp(method_handle_kind, Operand(mirror::MethodHandle::Kind::kFirstAccessorKind));
+  __ B(lt, &method_dispatch);
+  __ Ldr(method, HeapOperand(method_handle.W(), mirror::MethodHandleImpl::TargetOffset()));
+  __ B(&execute_target_method);
+
+  __ Bind(&method_dispatch);
+  __ Cmp(method_handle_kind, Operand(mirror::MethodHandle::Kind::kInvokeStatic));
+  __ B(eq, &execute_target_method);
+
+  if (invoke->AsInvokePolymorphic()->CanTargetInstanceMethod()) {
+    Register receiver = InputRegisterAt(invoke, 1);
+
+    // Receiver shouldn't be null for all the following cases.
+    __ Cbz(receiver, slow_path->GetEntryLabel());
+
+    __ Cmp(method_handle_kind, Operand(mirror::MethodHandle::Kind::kInvokeDirect));
+    // No dispatch is needed for invoke-direct.
+    __ B(eq, &execute_target_method);
+
+    vixl::aarch64::Label non_virtual_dispatch;
+    __ Cmp(method_handle_kind, Operand(mirror::MethodHandle::Kind::kInvokeVirtual));
+    __ B(ne, &non_virtual_dispatch);
+
+    // Skip virtual dispatch if `method` is private.
+    __ Ldr(temp, MemOperand(method, ArtMethod::AccessFlagsOffset().Int32Value()));
+    __ And(temp, temp, Operand(kAccPrivate));
+    __ Cbnz(temp, &execute_target_method);
+
+    Register receiver_class = WRegisterFrom(locations->GetTemp(3));
+    // If method is defined in the receiver's class, execute it as it is.
+    __ Ldr(temp, MemOperand(method, ArtMethod::DeclaringClassOffset().Int32Value()));
+    __ Ldr(receiver_class, HeapOperand(receiver.W(), mirror::Object::ClassOffset().Int32Value()));
+    codegen_->GetAssembler()->MaybeUnpoisonHeapReference(receiver_class.W());
+    // `receiver_class` is read w/o read barriers: false negatives go through virtual dispatch.
+    __ Cmp(temp, receiver_class);
+    __ B(eq, &execute_target_method);
+
+    // MethodIndex is uint16_t.
+    __ Ldrh(temp, MemOperand(method, ArtMethod::MethodIndexOffset().Int32Value()));
+
+    // Re-using receiver class register to store vtable.
+    constexpr uint32_t vtable_offset =
+        mirror::Class::EmbeddedVTableOffset(art::PointerSize::k64).Int32Value();
+    __ Add(receiver_class.X(), receiver_class.X(), vtable_offset);
+    __ Ldr(method, MemOperand(receiver_class.X(), temp, Extend::UXTW, 3u));
+    __ B(&execute_target_method);
+
+    __ Bind(&non_virtual_dispatch);
+    __ Cmp(method_handle_kind, Operand(mirror::MethodHandle::Kind::kInvokeInterface));
+    __ B(ne, slow_path->GetEntryLabel());
+
+    // Skip virtual dispatch if `method` is private.
+    // Re-using method_handle_kind to store access flags.
+    Register access_flags = WRegisterFrom(locations->GetTemp(4));
+    __ Ldr(access_flags, MemOperand(method, ArtMethod::AccessFlagsOffset().Int32Value()));
+    __ And(temp, access_flags, Operand(kAccPrivate));
+    __ Cbnz(temp, &execute_target_method);
+
+    // The register ip1 is required to be used for the hidden argument in
+    // art_quick_imt_conflict_trampoline, so prevent VIXL from using it.
+    UseScratchRegisterScope scratch_scope(masm);
+    scratch_scope.Exclude(ip1);
+
+    // Set the hidden argument.
+    __ Mov(ip1, method);
+
+    vixl::aarch64::Label get_imt_index_from_method_index;
+    vixl::aarch64::Label do_imt_dispatch;
+
+    // Get IMT index.
+    // Not doing default conflict check as IMT index is set for all method which have
+    // kAccAbstract bit.
+    __ And(temp, access_flags, Operand(kAccAbstract));
+    __ Cbz(temp, &get_imt_index_from_method_index);
+
+    // imt_index is uint16_t
+    __ Ldrh(temp, MemOperand(method, ArtMethod::ImtIndexOffset().Int32Value()));
+    __ B(&do_imt_dispatch);
+
+    // Default method, do method->GetMethodIndex() & (ImTable::kSizeTruncToPowerOfTwo - 1);
+    __ Bind(&get_imt_index_from_method_index);
+    __ Ldr(temp, MemOperand(method, ArtMethod::MethodIndexOffset().Int32Value()));
+    __ And(temp, temp, Operand(ImTable::kSizeTruncToPowerOfTwo - 1));
+
+    __ Bind(&do_imt_dispatch);
+    // Re-using `method` to store receiver class and ImTableEntry.
+    __ Ldr(method.W(), HeapOperand(receiver.W(), mirror::Object::ClassOffset()));
+    codegen_->GetAssembler()->MaybeUnpoisonHeapReference(method.W());
+
+    __ Ldr(method, MemOperand(method, mirror::Class::ImtPtrOffset(PointerSize::k64).Int32Value()));
+    __ Ldr(method, MemOperand(method, temp, Extend::UXTW, 3u));
+
+    __ B(&execute_target_method);
+  } else {
+    // Not invoke-static and the first argument is not a reference type.
+    __ B(slow_path->GetEntryLabel());
+  }
+
+  __ Bind(&execute_target_method);
   Offset entry_point = ArtMethod::EntryPointFromQuickCompiledCodeOffset(kArm64PointerSize);
   __ Ldr(lr, MemOperand(method, entry_point.SizeValue()));
   __ Blr(lr);
-  codegen_->RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+  codegen_->RecordPcInfo(invoke, slow_path);
   __ Bind(slow_path->GetExitLabel());
 }
 
diff --git a/compiler/optimizing/intrinsics_arm_vixl.cc b/compiler/optimizing/intrinsics_arm_vixl.cc
index 2360ca6d29..9e60090a03 100644
--- a/compiler/optimizing/intrinsics_arm_vixl.cc
+++ b/compiler/optimizing/intrinsics_arm_vixl.cc
@@ -1081,7 +1081,7 @@ static void GenerateVisitStringIndexOf(HInvoke* invoke,
     __ Mov(tmp_reg, 0);
   }
 
-  codegen->InvokeRuntime(kQuickIndexOf, invoke, invoke->GetDexPc(), slow_path);
+  codegen->InvokeRuntime(kQuickIndexOf, invoke, slow_path);
   CheckEntrypointTypes<kQuickIndexOf, int32_t, void*, uint32_t, uint32_t>();
 
   if (slow_path != nullptr) {
@@ -1143,7 +1143,7 @@ void IntrinsicCodeGeneratorARMVIXL::VisitStringNewStringFromBytes(HInvoke* invok
   codegen_->AddSlowPath(slow_path);
   __ B(eq, slow_path->GetEntryLabel());
 
-  codegen_->InvokeRuntime(kQuickAllocStringFromBytes, invoke, invoke->GetDexPc(), slow_path);
+  codegen_->InvokeRuntime(kQuickAllocStringFromBytes, invoke, slow_path);
   CheckEntrypointTypes<kQuickAllocStringFromBytes, void*, void*, int32_t, int32_t, int32_t>();
   __ Bind(slow_path->GetExitLabel());
 }
@@ -1165,7 +1165,7 @@ void IntrinsicCodeGeneratorARMVIXL::VisitStringNewStringFromChars(HInvoke* invok
   //   java.lang.StringFactory.newStringFromChars(int offset, int charCount, char[] data)
   //
   // all include a null check on `data` before calling that method.
-  codegen_->InvokeRuntime(kQuickAllocStringFromChars, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickAllocStringFromChars, invoke);
   CheckEntrypointTypes<kQuickAllocStringFromChars, void*, int32_t, int32_t, void*>();
 }
 
@@ -1186,7 +1186,7 @@ void IntrinsicCodeGeneratorARMVIXL::VisitStringNewStringFromString(HInvoke* invo
   codegen_->AddSlowPath(slow_path);
   __ B(eq, slow_path->GetEntryLabel());
 
-  codegen_->InvokeRuntime(kQuickAllocStringFromString, invoke, invoke->GetDexPc(), slow_path);
+  codegen_->InvokeRuntime(kQuickAllocStringFromString, invoke, slow_path);
   CheckEntrypointTypes<kQuickAllocStringFromString, void*, void*>();
 
   __ Bind(slow_path->GetExitLabel());
@@ -1666,7 +1666,7 @@ static void GenFPToFPCall(HInvoke* invoke,
   __ Vmov(RegisterFrom(locations->GetTemp(0)),
           RegisterFrom(locations->GetTemp(1)),
           InputDRegisterAt(invoke, 0));
-  codegen->InvokeRuntime(entry, invoke, invoke->GetDexPc());
+  codegen->InvokeRuntime(entry, invoke);
   __ Vmov(OutputDRegister(invoke),
           RegisterFrom(locations->GetTemp(0)),
           RegisterFrom(locations->GetTemp(1)));
@@ -1688,7 +1688,7 @@ static void GenFPFPToFPCall(HInvoke* invoke,
   __ Vmov(RegisterFrom(locations->GetTemp(2)),
           RegisterFrom(locations->GetTemp(3)),
           InputDRegisterAt(invoke, 1));
-  codegen->InvokeRuntime(entry, invoke, invoke->GetDexPc());
+  codegen->InvokeRuntime(entry, invoke);
   __ Vmov(OutputDRegister(invoke),
           RegisterFrom(locations->GetTemp(0)),
           RegisterFrom(locations->GetTemp(1)));
@@ -2338,7 +2338,7 @@ void IntrinsicCodeGeneratorARMVIXL::HandleValueOf(HInvoke* invoke,
   auto allocate_instance = [&]() {
     DCHECK(out.Is(InvokeRuntimeCallingConventionARMVIXL().GetRegisterAt(0)));
     codegen_->LoadIntrinsicDeclaringClass(out, invoke);
-    codegen_->InvokeRuntime(kQuickAllocObjectInitialized, invoke, invoke->GetDexPc());
+    codegen_->InvokeRuntime(kQuickAllocObjectInitialized, invoke);
     CheckEntrypointTypes<kQuickAllocObjectWithChecks, void*, mirror::Class*>();
   };
   if (invoke->InputAt(0)->IsIntConstant()) {
@@ -2690,7 +2690,7 @@ static void CreateUnsafeGetLocations(HInvoke* invoke,
                                      DataType::Type type,
                                      bool atomic) {
   bool can_call = codegen->EmitReadBarrier() && IsUnsafeGetReference(invoke);
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations =
       new (allocator) LocationSummary(invoke,
                                       can_call
@@ -3101,7 +3101,7 @@ static void CreateUnsafePutLocations(HInvoke* invoke,
                                      CodeGeneratorARMVIXL* codegen,
                                      DataType::Type type,
                                      bool atomic) {
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations =
       new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
@@ -3115,7 +3115,7 @@ static void CreateUnsafePutAbsoluteLocations(HInvoke* invoke,
                                      CodeGeneratorARMVIXL* codegen,
                                      DataType::Type type,
                                      bool atomic) {
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations =
       new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
   locations->SetInAt(0, Location::NoLocation());        // Unused receiver.
@@ -3204,12 +3204,12 @@ void IntrinsicCodeGeneratorARMVIXL::VisitUnsafePutAbsolute(HInvoke* invoke) {
   VisitJdkUnsafePutAbsolute(invoke);
 }
 
-void IntrinsicLocationsBuilderARMVIXL::VisitUnsafePutOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutOrdered(invoke);
+void IntrinsicLocationsBuilderARMVIXL::VisitUnsafePutOrderedInt(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedInt(invoke);
 }
 
-void IntrinsicCodeGeneratorARMVIXL::VisitUnsafePutOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutOrdered(invoke);
+void IntrinsicCodeGeneratorARMVIXL::VisitUnsafePutOrderedInt(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedInt(invoke);
 }
 
 void IntrinsicLocationsBuilderARMVIXL::VisitUnsafePutVolatile(HInvoke* invoke) {
@@ -3227,12 +3227,12 @@ void IntrinsicCodeGeneratorARMVIXL::VisitUnsafePutObject(HInvoke* invoke) {
   VisitJdkUnsafePutReference(invoke);
 }
 
-void IntrinsicLocationsBuilderARMVIXL::VisitUnsafePutObjectOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutObjectOrdered(invoke);
+void IntrinsicLocationsBuilderARMVIXL::VisitUnsafePutOrderedObject(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedObject(invoke);
 }
 
-void IntrinsicCodeGeneratorARMVIXL::VisitUnsafePutObjectOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutObjectOrdered(invoke);
+void IntrinsicCodeGeneratorARMVIXL::VisitUnsafePutOrderedObject(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedObject(invoke);
 }
 
 void IntrinsicLocationsBuilderARMVIXL::VisitUnsafePutObjectVolatile(HInvoke* invoke) {
@@ -3311,11 +3311,11 @@ void IntrinsicCodeGeneratorARMVIXL::VisitJdkUnsafePutByte(HInvoke* invoke) {
                codegen_);
 }
 
-void IntrinsicLocationsBuilderARMVIXL::VisitJdkUnsafePutOrdered(HInvoke* invoke) {
+void IntrinsicLocationsBuilderARMVIXL::VisitJdkUnsafePutOrderedInt(HInvoke* invoke) {
   CreateUnsafePutLocations(invoke, codegen_, DataType::Type::kInt32, /*atomic=*/ true);
 }
 
-void IntrinsicCodeGeneratorARMVIXL::VisitJdkUnsafePutOrdered(HInvoke* invoke) {
+void IntrinsicCodeGeneratorARMVIXL::VisitJdkUnsafePutOrderedInt(HInvoke* invoke) {
   GenUnsafePut(invoke,
                DataType::Type::kInt32,
                std::memory_order_release,
@@ -3359,11 +3359,11 @@ void IntrinsicCodeGeneratorARMVIXL::VisitJdkUnsafePutReference(HInvoke* invoke)
                codegen_);
 }
 
-void IntrinsicLocationsBuilderARMVIXL::VisitJdkUnsafePutObjectOrdered(HInvoke* invoke) {
+void IntrinsicLocationsBuilderARMVIXL::VisitJdkUnsafePutOrderedObject(HInvoke* invoke) {
   CreateUnsafePutLocations(invoke, codegen_, DataType::Type::kReference, /*atomic=*/ true);
 }
 
-void IntrinsicCodeGeneratorARMVIXL::VisitJdkUnsafePutObjectOrdered(HInvoke* invoke) {
+void IntrinsicCodeGeneratorARMVIXL::VisitJdkUnsafePutOrderedObject(HInvoke* invoke) {
   GenUnsafePut(invoke,
                DataType::Type::kReference,
                std::memory_order_release,
@@ -3752,7 +3752,7 @@ class ReadBarrierCasSlowPathARMVIXL : public SlowPathCodeARMVIXL {
 
 static void CreateUnsafeCASLocations(HInvoke* invoke, CodeGeneratorARMVIXL* codegen) {
   const bool can_call = codegen->EmitReadBarrier() && IsUnsafeCASReference(invoke);
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations =
       new (allocator) LocationSummary(invoke,
                                       can_call
@@ -4046,7 +4046,7 @@ static void CreateUnsafeGetAndUpdateLocations(HInvoke* invoke,
                                               DataType::Type type,
                                               GetAndUpdateOp get_and_update_op) {
   const bool can_call = codegen->EmitReadBarrier() && IsUnsafeGetAndSetReference(invoke);
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations =
       new (allocator) LocationSummary(invoke,
                                       can_call
@@ -4597,7 +4597,7 @@ static void GenerateVarHandleTarget(HInvoke* invoke,
   if (expected_coordinates_count <= 1u) {
     if (VarHandleOptimizations(invoke).GetUseKnownImageVarHandle()) {
       ScopedObjectAccess soa(Thread::Current());
-      ArtField* target_field = GetBootImageVarHandleField(invoke);
+      ArtField* target_field = GetImageVarHandleField(invoke);
       if (expected_coordinates_count == 0u) {
         ObjPtr<mirror::Class> declaring_class = target_field->GetDeclaringClass();
         if (Runtime::Current()->GetHeap()->ObjectIsInBootImageSpace(declaring_class)) {
@@ -4653,7 +4653,7 @@ static LocationSummary* CreateVarHandleCommonLocations(HInvoke* invoke,
   size_t expected_coordinates_count = GetExpectedVarHandleCoordinatesCount(invoke);
   DataType::Type return_type = invoke->GetType();
 
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations =
       new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
diff --git a/compiler/optimizing/intrinsics_riscv64.cc b/compiler/optimizing/intrinsics_riscv64.cc
index d2a0e97527..9379c8d3f6 100644
--- a/compiler/optimizing/intrinsics_riscv64.cc
+++ b/compiler/optimizing/intrinsics_riscv64.cc
@@ -90,6 +90,38 @@ class ReadBarrierSystemArrayCopySlowPathRISCV64 : public SlowPathCodeRISCV64 {
   DISALLOW_COPY_AND_ASSIGN(ReadBarrierSystemArrayCopySlowPathRISCV64);
 };
 
+// The MethodHandle.invokeExact intrinsic sets up arguments to match the target method call. If we
+// need to go to the slow path, we call art_quick_invoke_polymorphic_with_hidden_receiver, which
+// expects the MethodHandle object in a0 (in place of the actual ArtMethod).
+class InvokePolymorphicSlowPathRISCV64 : public SlowPathCodeRISCV64 {
+ public:
+  InvokePolymorphicSlowPathRISCV64(HInstruction* instruction, XRegister method_handle)
+      : SlowPathCodeRISCV64(instruction), method_handle_(method_handle) {
+    DCHECK(instruction->IsInvokePolymorphic());
+  }
+
+  void EmitNativeCode(CodeGenerator* codegen_in) override {
+    CodeGeneratorRISCV64* codegen = down_cast<CodeGeneratorRISCV64*>(codegen_in);
+    Riscv64Assembler* assembler = codegen->GetAssembler();
+    __ Bind(GetEntryLabel());
+
+    SaveLiveRegisters(codegen, instruction_->GetLocations());
+    // Passing `MethodHandle` object as hidden argument.
+    __ Mv(A0, method_handle_);
+    codegen->InvokeRuntime(QuickEntrypointEnum::kQuickInvokePolymorphicWithHiddenReceiver,
+                           instruction_);
+
+    RestoreLiveRegisters(codegen, instruction_->GetLocations());
+    __ J(GetExitLabel());
+  }
+
+  const char* GetDescription() const override { return "InvokePolymorphicSlowPathRISCV64"; }
+
+ private:
+  const XRegister method_handle_;
+  DISALLOW_COPY_AND_ASSIGN(InvokePolymorphicSlowPathRISCV64);
+};
+
 bool IntrinsicLocationsBuilderRISCV64::TryDispatch(HInvoke* invoke) {
   Dispatch(invoke);
   LocationSummary* res = invoke->GetLocations();
@@ -698,7 +730,7 @@ void IntrinsicCodeGeneratorRISCV64::HandleValueOf(HInvoke* invoke,
   auto allocate_instance = [&]() {
     DCHECK_EQ(out, InvokeRuntimeCallingConvention().GetRegisterAt(0));
     codegen_->LoadIntrinsicDeclaringClass(out, invoke);
-    codegen_->InvokeRuntime(kQuickAllocObjectInitialized, invoke, invoke->GetDexPc());
+    codegen_->InvokeRuntime(kQuickAllocObjectInitialized, invoke);
     CheckEntrypointTypes<kQuickAllocObjectWithChecks, void*, mirror::Class*>();
   };
   if (invoke->InputAt(0)->IsIntConstant()) {
@@ -913,7 +945,7 @@ static void GenerateVisitStringIndexOf(HInvoke* invoke,
     __ Li(tmp_reg, 0);
   }
 
-  codegen->InvokeRuntime(kQuickIndexOf, invoke, invoke->GetDexPc(), slow_path);
+  codegen->InvokeRuntime(kQuickIndexOf, invoke, slow_path);
   CheckEntrypointTypes<kQuickIndexOf, int32_t, void*, uint32_t, uint32_t>();
 
   if (slow_path != nullptr) {
@@ -976,7 +1008,7 @@ void IntrinsicCodeGeneratorRISCV64::VisitStringNewStringFromBytes(HInvoke* invok
   codegen_->AddSlowPath(slow_path);
   __ Beqz(byte_array, slow_path->GetEntryLabel());
 
-  codegen_->InvokeRuntime(kQuickAllocStringFromBytes, invoke, invoke->GetDexPc(), slow_path);
+  codegen_->InvokeRuntime(kQuickAllocStringFromBytes, invoke, slow_path);
   CheckEntrypointTypes<kQuickAllocStringFromBytes, void*, void*, int32_t, int32_t, int32_t>();
   __ Bind(slow_path->GetExitLabel());
 }
@@ -998,7 +1030,7 @@ void IntrinsicCodeGeneratorRISCV64::VisitStringNewStringFromChars(HInvoke* invok
   //   java.lang.StringFactory.newStringFromChars(int offset, int charCount, char[] data)
   //
   // all include a null check on `data` before calling that method.
-  codegen_->InvokeRuntime(kQuickAllocStringFromChars, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickAllocStringFromChars, invoke);
   CheckEntrypointTypes<kQuickAllocStringFromChars, void*, int32_t, int32_t, void*>();
 }
 
@@ -1020,7 +1052,7 @@ void IntrinsicCodeGeneratorRISCV64::VisitStringNewStringFromString(HInvoke* invo
   codegen_->AddSlowPath(slow_path);
   __ Beqz(string_to_copy, slow_path->GetEntryLabel());
 
-  codegen_->InvokeRuntime(kQuickAllocStringFromString, invoke, invoke->GetDexPc(), slow_path);
+  codegen_->InvokeRuntime(kQuickAllocStringFromString, invoke, slow_path);
   CheckEntrypointTypes<kQuickAllocStringFromString, void*, void*>();
   __ Bind(slow_path->GetExitLabel());
 }
@@ -2648,12 +2680,12 @@ void IntrinsicCodeGeneratorRISCV64::VisitUnsafePutAbsolute(HInvoke* invoke) {
   VisitJdkUnsafePutAbsolute(invoke);
 }
 
-void IntrinsicLocationsBuilderRISCV64::VisitUnsafePutOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutOrdered(invoke);
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafePutOrderedInt(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedInt(invoke);
 }
 
-void IntrinsicCodeGeneratorRISCV64::VisitUnsafePutOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutOrdered(invoke);
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafePutOrderedInt(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedInt(invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitUnsafePutVolatile(HInvoke* invoke) {
@@ -2672,12 +2704,12 @@ void IntrinsicCodeGeneratorRISCV64::VisitUnsafePutObject(HInvoke* invoke) {
   VisitJdkUnsafePutReference(invoke);
 }
 
-void IntrinsicLocationsBuilderRISCV64::VisitUnsafePutObjectOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutObjectOrdered(invoke);
+void IntrinsicLocationsBuilderRISCV64::VisitUnsafePutOrderedObject(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedObject(invoke);
 }
 
-void IntrinsicCodeGeneratorRISCV64::VisitUnsafePutObjectOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutObjectOrdered(invoke);
+void IntrinsicCodeGeneratorRISCV64::VisitUnsafePutOrderedObject(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedObject(invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitUnsafePutObjectVolatile(HInvoke* invoke) {
@@ -2736,11 +2768,11 @@ void IntrinsicCodeGeneratorRISCV64::VisitJdkUnsafePutAbsolute(HInvoke* invoke) {
   GenUnsafePutAbsolute(invoke, codegen_, std::memory_order_relaxed, DataType::Type::kInt32);
 }
 
-void IntrinsicLocationsBuilderRISCV64::VisitJdkUnsafePutOrdered(HInvoke* invoke) {
+void IntrinsicLocationsBuilderRISCV64::VisitJdkUnsafePutOrderedInt(HInvoke* invoke) {
   CreateUnsafePutLocations(allocator_, invoke);
 }
 
-void IntrinsicCodeGeneratorRISCV64::VisitJdkUnsafePutOrdered(HInvoke* invoke) {
+void IntrinsicCodeGeneratorRISCV64::VisitJdkUnsafePutOrderedInt(HInvoke* invoke) {
   GenUnsafePut(invoke, codegen_, std::memory_order_release, DataType::Type::kInt32);
 }
 
@@ -2768,11 +2800,11 @@ void IntrinsicCodeGeneratorRISCV64::VisitJdkUnsafePutReference(HInvoke* invoke)
   GenUnsafePut(invoke, codegen_, std::memory_order_relaxed, DataType::Type::kReference);
 }
 
-void IntrinsicLocationsBuilderRISCV64::VisitJdkUnsafePutObjectOrdered(HInvoke* invoke) {
+void IntrinsicLocationsBuilderRISCV64::VisitJdkUnsafePutOrderedObject(HInvoke* invoke) {
   CreateUnsafePutLocations(allocator_, invoke);
 }
 
-void IntrinsicCodeGeneratorRISCV64::VisitJdkUnsafePutObjectOrdered(HInvoke* invoke) {
+void IntrinsicCodeGeneratorRISCV64::VisitJdkUnsafePutOrderedObject(HInvoke* invoke) {
   GenUnsafePut(invoke, codegen_, std::memory_order_release, DataType::Type::kReference);
 }
 
@@ -3774,7 +3806,7 @@ static void GenerateVarHandleTarget(HInvoke* invoke,
   if (expected_coordinates_count <= 1u) {
     if (VarHandleOptimizations(invoke).GetUseKnownImageVarHandle()) {
       ScopedObjectAccess soa(Thread::Current());
-      ArtField* target_field = GetBootImageVarHandleField(invoke);
+      ArtField* target_field = GetImageVarHandleField(invoke);
       if (expected_coordinates_count == 0u) {
         ObjPtr<mirror::Class> declaring_class = target_field->GetDeclaringClass();
         if (Runtime::Current()->GetHeap()->ObjectIsInBootImageSpace(declaring_class)) {
@@ -3826,7 +3858,7 @@ static LocationSummary* CreateVarHandleCommonLocations(HInvoke* invoke,
   size_t expected_coordinates_count = GetExpectedVarHandleCoordinatesCount(invoke);
   DataType::Type return_type = invoke->GetType();
 
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations =
       new (allocator) LocationSummary(invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
@@ -5191,7 +5223,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathCos(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathCos(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickCos, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickCos, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathSin(HInvoke* invoke) {
@@ -5199,7 +5231,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathSin(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathSin(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickSin, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickSin, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathAcos(HInvoke* invoke) {
@@ -5207,7 +5239,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathAcos(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathAcos(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickAcos, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickAcos, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathAsin(HInvoke* invoke) {
@@ -5215,7 +5247,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathAsin(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathAsin(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickAsin, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickAsin, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathAtan(HInvoke* invoke) {
@@ -5223,7 +5255,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathAtan(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathAtan(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickAtan, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickAtan, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathAtan2(HInvoke* invoke) {
@@ -5231,7 +5263,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathAtan2(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathAtan2(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickAtan2, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickAtan2, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathPow(HInvoke* invoke) {
@@ -5239,7 +5271,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathPow(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathPow(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickPow, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickPow, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathCbrt(HInvoke* invoke) {
@@ -5247,7 +5279,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathCbrt(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathCbrt(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickCbrt, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickCbrt, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathCosh(HInvoke* invoke) {
@@ -5255,7 +5287,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathCosh(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathCosh(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickCosh, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickCosh, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathExp(HInvoke* invoke) {
@@ -5263,7 +5295,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathExp(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathExp(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickExp, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickExp, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathExpm1(HInvoke* invoke) {
@@ -5271,7 +5303,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathExpm1(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathExpm1(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickExpm1, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickExpm1, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathHypot(HInvoke* invoke) {
@@ -5279,7 +5311,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathHypot(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathHypot(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickHypot, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickHypot, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathLog(HInvoke* invoke) {
@@ -5287,7 +5319,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathLog(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathLog(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickLog, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickLog, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathLog10(HInvoke* invoke) {
@@ -5295,7 +5327,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathLog10(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathLog10(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickLog10, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickLog10, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathNextAfter(HInvoke* invoke) {
@@ -5303,7 +5335,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathNextAfter(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathNextAfter(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickNextAfter, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickNextAfter, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathSinh(HInvoke* invoke) {
@@ -5311,7 +5343,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathSinh(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathSinh(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickSinh, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickSinh, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathTan(HInvoke* invoke) {
@@ -5319,7 +5351,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathTan(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathTan(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickTan, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickTan, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathTanh(HInvoke* invoke) {
@@ -5327,7 +5359,7 @@ void IntrinsicLocationsBuilderRISCV64::VisitMathTanh(HInvoke* invoke) {
 }
 
 void IntrinsicCodeGeneratorRISCV64::VisitMathTanh(HInvoke* invoke) {
-  codegen_->InvokeRuntime(kQuickTanh, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickTanh, invoke);
 }
 
 void IntrinsicLocationsBuilderRISCV64::VisitMathSqrt(HInvoke* invoke) {
@@ -5733,6 +5765,108 @@ void IntrinsicCodeGeneratorRISCV64::VisitMathCopySignFloat(HInvoke* invoke) {
   GenMathCopySign(codegen_, invoke, DataType::Type::kFloat32);
 }
 
+void IntrinsicLocationsBuilderRISCV64::VisitMethodHandleInvokeExact(HInvoke* invoke) {
+  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  LocationSummary* locations = new (allocator)
+      LocationSummary(invoke, LocationSummary::kCallOnMainAndSlowPath, kIntrinsified);
+
+  InvokeDexCallingConventionVisitorRISCV64 calling_convention;
+  locations->SetOut(calling_convention.GetReturnLocation(invoke->GetType()));
+  locations->SetInAt(0, Location::RequiresRegister());
+
+  // Accomodating LocationSummary for underlying invoke-* call.
+  uint32_t number_of_args = invoke->GetNumberOfArguments();
+  for (uint32_t i = 1; i < number_of_args; ++i) {
+    locations->SetInAt(i, calling_convention.GetNextLocation(invoke->InputAt(i)->GetType()));
+  }
+
+  // The last input is MethodType object corresponding to the call-site.
+  locations->SetInAt(number_of_args, Location::RequiresRegister());
+
+  locations->AddTemp(calling_convention.GetMethodLocation());
+  locations->AddRegisterTemps(2);
+}
+
+void IntrinsicCodeGeneratorRISCV64::VisitMethodHandleInvokeExact(HInvoke* invoke) {
+  LocationSummary* locations = invoke->GetLocations();
+  XRegister method_handle = locations->InAt(0).AsRegister<XRegister>();
+  SlowPathCodeRISCV64* slow_path =
+      new (codegen_->GetScopedAllocator()) InvokePolymorphicSlowPathRISCV64(invoke, method_handle);
+
+  codegen_->AddSlowPath(slow_path);
+  Riscv64Assembler* assembler = GetAssembler();
+  XRegister call_site_type =
+      locations->InAt(invoke->GetNumberOfArguments()).AsRegister<XRegister>();
+
+  // Call site should match with MethodHandle's type.
+  XRegister temp = locations->GetTemp(1).AsRegister<XRegister>();
+  __ Loadwu(temp, method_handle, mirror::MethodHandle::MethodTypeOffset().Int32Value());
+  codegen_->MaybeUnpoisonHeapReference(temp);
+  __ Bne(call_site_type, temp, slow_path->GetEntryLabel());
+
+  XRegister method = locations->GetTemp(0).AsRegister<XRegister>();
+  __ Loadd(method, method_handle, mirror::MethodHandle::ArtFieldOrMethodOffset().Int32Value());
+
+  Riscv64Label execute_target_method;
+
+  XRegister method_handle_kind = locations->GetTemp(2).AsRegister<XRegister>();
+  __ Loadd(method_handle_kind,
+           method_handle, mirror::MethodHandle::HandleKindOffset().Int32Value());
+  __ Li(temp, mirror::MethodHandle::Kind::kInvokeStatic);
+  __ Beq(method_handle_kind, temp, &execute_target_method);
+
+  if (invoke->AsInvokePolymorphic()->CanTargetInstanceMethod()) {
+    XRegister receiver = locations->InAt(1).AsRegister<XRegister>();
+
+    // Receiver shouldn't be null for all the following cases.
+    __ Beqz(receiver, slow_path->GetEntryLabel());
+
+    __ Li(temp, mirror::MethodHandle::Kind::kInvokeDirect);
+    // No dispatch is needed for invoke-direct.
+    __ Beq(method_handle_kind, temp, &execute_target_method);
+
+    Riscv64Label non_virtual_dispatch;
+    __ Li(temp, mirror::MethodHandle::Kind::kInvokeVirtual);
+    __ Bne(method_handle_kind, temp, &non_virtual_dispatch);
+
+    // Skip virtual dispatch if `method` is private.
+    __ Loadd(temp, method, ArtMethod::AccessFlagsOffset().Int32Value());
+    __ Andi(temp, temp, kAccPrivate);
+    __ Bnez(temp, &execute_target_method);
+
+    XRegister receiver_class = locations->GetTemp(2).AsRegister<XRegister>();
+    // If method is defined in the receiver's class, execute it as it is.
+    __ Loadd(temp, method, ArtMethod::DeclaringClassOffset().Int32Value());
+    __ Loadd(receiver_class, receiver, mirror::Object::ClassOffset().Int32Value());
+    codegen_->MaybeUnpoisonHeapReference(receiver_class);
+
+    // We're not emitting the read barrier for the receiver_class, so false negatives just go
+    // through the virtual dispath below.
+    __ Beq(temp, receiver_class, &execute_target_method);
+
+    // MethodIndex is uint16_t.
+    __ Loadhu(temp, method, ArtMethod::MethodIndexOffset().Int32Value());
+
+    constexpr uint32_t vtable_offset =
+        mirror::Class::EmbeddedVTableOffset(art::PointerSize::k64).Int32Value();
+    __ Sh3Add(temp, temp, receiver_class);
+    __ Loadd(method, temp, vtable_offset);
+    __ J(&execute_target_method);
+    __ Bind(&non_virtual_dispatch);
+  }
+
+  // Checks above are jumping to `execute_target_method` is they succeed. If none match, try to
+  // handle in the slow path.
+  __ J(slow_path->GetEntryLabel());
+
+  __ Bind(&execute_target_method);
+  Offset entry_point = ArtMethod::EntryPointFromQuickCompiledCodeOffset(kRiscv64PointerSize);
+  __ Loadd(RA, method, entry_point.SizeValue());
+  __ Jalr(RA);
+  codegen_->RecordPcInfo(invoke, slow_path);
+  __ Bind(slow_path->GetExitLabel());
+}
+
 #define MARK_UNIMPLEMENTED(Name) UNIMPLEMENTED_INTRINSIC(RISCV64, Name)
 UNIMPLEMENTED_INTRINSIC_LIST_RISCV64(MARK_UNIMPLEMENTED);
 #undef MARK_UNIMPLEMENTED
diff --git a/compiler/optimizing/intrinsics_utils.h b/compiler/optimizing/intrinsics_utils.h
index 6c08cea3f8..c2f32ae708 100644
--- a/compiler/optimizing/intrinsics_utils.h
+++ b/compiler/optimizing/intrinsics_utils.h
@@ -207,7 +207,7 @@ static inline DataType::Type GetVarHandleExpectedValueType(HInvoke* invoke,
   }
 }
 
-static inline ArtField* GetBootImageVarHandleField(HInvoke* invoke)
+static inline ArtField* GetImageVarHandleField(HInvoke* invoke)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   DCHECK_LE(GetExpectedVarHandleCoordinatesCount(invoke), 1u);
   DCHECK(VarHandleOptimizations(invoke).GetUseKnownImageVarHandle());
diff --git a/compiler/optimizing/intrinsics_x86.cc b/compiler/optimizing/intrinsics_x86.cc
index 952fb855be..5710ce42bb 100644
--- a/compiler/optimizing/intrinsics_x86.cc
+++ b/compiler/optimizing/intrinsics_x86.cc
@@ -471,7 +471,7 @@ static void GenFPToFPCall(HInvoke* invoke, CodeGeneratorX86* codegen, QuickEntry
   }
 
   // Now do the actual call.
-  codegen->InvokeRuntime(entry, invoke, invoke->GetDexPc());
+  codegen->InvokeRuntime(entry, invoke);
 
   // Extract the return value from the FP stack.
   __ fstpl(Address(ESP, 0));
@@ -1012,7 +1012,7 @@ void IntrinsicCodeGeneratorX86::VisitStringCompareTo(HInvoke* invoke) {
   codegen_->AddSlowPath(slow_path);
   __ j(kEqual, slow_path->GetEntryLabel());
 
-  codegen_->InvokeRuntime(kQuickStringCompareTo, invoke, invoke->GetDexPc(), slow_path);
+  codegen_->InvokeRuntime(kQuickStringCompareTo, invoke, slow_path);
   __ Bind(slow_path->GetExitLabel());
 }
 
@@ -1351,7 +1351,7 @@ void IntrinsicCodeGeneratorX86::VisitStringNewStringFromBytes(HInvoke* invoke) {
   codegen_->AddSlowPath(slow_path);
   __ j(kEqual, slow_path->GetEntryLabel());
 
-  codegen_->InvokeRuntime(kQuickAllocStringFromBytes, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickAllocStringFromBytes, invoke);
   CheckEntrypointTypes<kQuickAllocStringFromBytes, void*, void*, int32_t, int32_t, int32_t>();
   __ Bind(slow_path->GetExitLabel());
 }
@@ -1373,7 +1373,7 @@ void IntrinsicCodeGeneratorX86::VisitStringNewStringFromChars(HInvoke* invoke) {
   //   java.lang.StringFactory.newStringFromChars(int offset, int charCount, char[] data)
   //
   // all include a null check on `data` before calling that method.
-  codegen_->InvokeRuntime(kQuickAllocStringFromChars, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickAllocStringFromChars, invoke);
   CheckEntrypointTypes<kQuickAllocStringFromChars, void*, int32_t, int32_t, void*>();
 }
 
@@ -1395,7 +1395,7 @@ void IntrinsicCodeGeneratorX86::VisitStringNewStringFromString(HInvoke* invoke)
   codegen_->AddSlowPath(slow_path);
   __ j(kEqual, slow_path->GetEntryLabel());
 
-  codegen_->InvokeRuntime(kQuickAllocStringFromString, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickAllocStringFromString, invoke);
   CheckEntrypointTypes<kQuickAllocStringFromString, void*, void*>();
   __ Bind(slow_path->GetExitLabel());
 }
@@ -2011,8 +2011,8 @@ void IntrinsicLocationsBuilderX86::VisitUnsafePut(HInvoke* invoke) {
 void IntrinsicLocationsBuilderX86::VisitUnsafePutAbsolute(HInvoke* invoke) {
   VisitJdkUnsafePutAbsolute(invoke);
 }
-void IntrinsicLocationsBuilderX86::VisitUnsafePutOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutOrdered(invoke);
+void IntrinsicLocationsBuilderX86::VisitUnsafePutOrderedInt(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedInt(invoke);
 }
 void IntrinsicLocationsBuilderX86::VisitUnsafePutVolatile(HInvoke* invoke) {
   VisitJdkUnsafePutVolatile(invoke);
@@ -2020,8 +2020,8 @@ void IntrinsicLocationsBuilderX86::VisitUnsafePutVolatile(HInvoke* invoke) {
 void IntrinsicLocationsBuilderX86::VisitUnsafePutObject(HInvoke* invoke) {
   VisitJdkUnsafePutReference(invoke);
 }
-void IntrinsicLocationsBuilderX86::VisitUnsafePutObjectOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutObjectOrdered(invoke);
+void IntrinsicLocationsBuilderX86::VisitUnsafePutOrderedObject(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedObject(invoke);
 }
 void IntrinsicLocationsBuilderX86::VisitUnsafePutObjectVolatile(HInvoke* invoke) {
   VisitJdkUnsafePutReferenceVolatile(invoke);
@@ -2047,7 +2047,7 @@ void IntrinsicLocationsBuilderX86::VisitJdkUnsafePutAbsolute(HInvoke* invoke) {
   CreateIntIntIntToVoidPlusTempsLocations(
       allocator_, DataType::Type::kInt64, invoke, /*is_volatile=*/ false);
 }
-void IntrinsicLocationsBuilderX86::VisitJdkUnsafePutOrdered(HInvoke* invoke) {
+void IntrinsicLocationsBuilderX86::VisitJdkUnsafePutOrderedInt(HInvoke* invoke) {
   CreateIntIntIntIntToVoidPlusTempsLocations(
       allocator_, DataType::Type::kInt32, invoke, /*is_volatile=*/ false);
 }
@@ -2063,7 +2063,7 @@ void IntrinsicLocationsBuilderX86::VisitJdkUnsafePutReference(HInvoke* invoke) {
   CreateIntIntIntIntToVoidPlusTempsLocations(
       allocator_, DataType::Type::kReference, invoke, /*is_volatile=*/ false);
 }
-void IntrinsicLocationsBuilderX86::VisitJdkUnsafePutObjectOrdered(HInvoke* invoke) {
+void IntrinsicLocationsBuilderX86::VisitJdkUnsafePutOrderedObject(HInvoke* invoke) {
   CreateIntIntIntIntToVoidPlusTempsLocations(
       allocator_, DataType::Type::kReference, invoke, /*is_volatile=*/ false);
 }
@@ -2200,8 +2200,8 @@ void IntrinsicCodeGeneratorX86::VisitUnsafePut(HInvoke* invoke) {
 void IntrinsicCodeGeneratorX86::VisitUnsafePutAbsolute(HInvoke* invoke) {
   VisitJdkUnsafePutAbsolute(invoke);
 }
-void IntrinsicCodeGeneratorX86::VisitUnsafePutOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutOrdered(invoke);
+void IntrinsicCodeGeneratorX86::VisitUnsafePutOrderedInt(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedInt(invoke);
 }
 void IntrinsicCodeGeneratorX86::VisitUnsafePutVolatile(HInvoke* invoke) {
   VisitJdkUnsafePutVolatile(invoke);
@@ -2209,8 +2209,8 @@ void IntrinsicCodeGeneratorX86::VisitUnsafePutVolatile(HInvoke* invoke) {
 void IntrinsicCodeGeneratorX86::VisitUnsafePutObject(HInvoke* invoke) {
   VisitJdkUnsafePutReference(invoke);
 }
-void IntrinsicCodeGeneratorX86::VisitUnsafePutObjectOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutObjectOrdered(invoke);
+void IntrinsicCodeGeneratorX86::VisitUnsafePutOrderedObject(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedObject(invoke);
 }
 void IntrinsicCodeGeneratorX86::VisitUnsafePutObjectVolatile(HInvoke* invoke) {
   VisitJdkUnsafePutReferenceVolatile(invoke);
@@ -2235,7 +2235,7 @@ void IntrinsicCodeGeneratorX86::VisitJdkUnsafePutAbsolute(HInvoke* invoke) {
   GenUnsafePutAbsolute(
       invoke->GetLocations(), DataType::Type::kInt32, /*is_volatile=*/false, codegen_);
 }
-void IntrinsicCodeGeneratorX86::VisitJdkUnsafePutOrdered(HInvoke* invoke) {
+void IntrinsicCodeGeneratorX86::VisitJdkUnsafePutOrderedInt(HInvoke* invoke) {
   GenUnsafePut(invoke->GetLocations(), DataType::Type::kInt32, /*is_volatile=*/ false, codegen_);
 }
 void IntrinsicCodeGeneratorX86::VisitJdkUnsafePutVolatile(HInvoke* invoke) {
@@ -2248,7 +2248,7 @@ void IntrinsicCodeGeneratorX86::VisitJdkUnsafePutReference(HInvoke* invoke) {
   GenUnsafePut(
       invoke->GetLocations(), DataType::Type::kReference, /*is_volatile=*/ false, codegen_);
 }
-void IntrinsicCodeGeneratorX86::VisitJdkUnsafePutObjectOrdered(HInvoke* invoke) {
+void IntrinsicCodeGeneratorX86::VisitJdkUnsafePutOrderedObject(HInvoke* invoke) {
   GenUnsafePut(
       invoke->GetLocations(), DataType::Type::kReference, /*is_volatile=*/ false, codegen_);
 }
@@ -3606,7 +3606,7 @@ void IntrinsicCodeGeneratorX86::HandleValueOf(HInvoke* invoke,
   auto allocate_instance = [&]() {
     DCHECK_EQ(out, InvokeRuntimeCallingConvention().GetRegisterAt(0));
     codegen_->LoadIntrinsicDeclaringClass(out, invoke->AsInvokeStaticOrDirect());
-    codegen_->InvokeRuntime(kQuickAllocObjectInitialized, invoke, invoke->GetDexPc());
+    codegen_->InvokeRuntime(kQuickAllocObjectInitialized, invoke);
     CheckEntrypointTypes<kQuickAllocObjectWithChecks, void*, mirror::Class*>();
   };
   if (invoke->InputAt(0)->IsIntConstant()) {
@@ -4117,7 +4117,7 @@ static void CreateVarHandleGetLocations(HInvoke* invoke, CodeGeneratorX86* codeg
     return;
   }
 
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations = new (allocator) LocationSummary(
       invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
@@ -4253,7 +4253,7 @@ static void CreateVarHandleSetLocations(HInvoke* invoke, CodeGeneratorX86* codeg
     return;
   }
 
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations = new (allocator) LocationSummary(
       invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->SetInAt(0, Location::RequiresRegister());
@@ -4430,7 +4430,7 @@ static void CreateVarHandleGetAndSetLocations(HInvoke* invoke, CodeGeneratorX86*
     return;
   }
 
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations = new (allocator) LocationSummary(
       invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->AddRegisterTemps(2);
@@ -4630,7 +4630,7 @@ static void CreateVarHandleCompareAndSetOrExchangeLocations(HInvoke* invoke,
     return;
   }
 
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations = new (allocator) LocationSummary(
       invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->AddRegisterTemps(2);
@@ -4810,7 +4810,7 @@ static void CreateVarHandleGetAndAddLocations(HInvoke* invoke, CodeGeneratorX86*
     return;
   }
 
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations = new (allocator) LocationSummary(
       invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   locations->AddRegisterTemps(2);
@@ -4985,7 +4985,7 @@ static void CreateVarHandleGetAndBitwiseOpLocations(HInvoke* invoke, CodeGenerat
     return;
   }
 
-  ArenaAllocator* allocator = invoke->GetBlock()->GetGraph()->GetAllocator();
+  ArenaAllocator* allocator = codegen->GetGraph()->GetAllocator();
   LocationSummary* locations = new (allocator) LocationSummary(
       invoke, LocationSummary::kCallOnSlowPath, kIntrinsified);
   // We need a byte register temp to store the result of the bitwise operation
diff --git a/compiler/optimizing/intrinsics_x86_64.cc b/compiler/optimizing/intrinsics_x86_64.cc
index e5d4dc4cc2..d3ee1759e0 100644
--- a/compiler/optimizing/intrinsics_x86_64.cc
+++ b/compiler/optimizing/intrinsics_x86_64.cc
@@ -32,6 +32,7 @@
 #include "intrinsics_utils.h"
 #include "lock_word.h"
 #include "mirror/array-inl.h"
+#include "mirror/method_handle_impl.h"
 #include "mirror/object_array-inl.h"
 #include "mirror/reference.h"
 #include "mirror/string.h"
@@ -166,8 +167,7 @@ class InvokePolymorphicSlowPathX86_64 : public SlowPathCode {
     // Passing `MethodHandle` object as hidden argument.
     __ movl(CpuRegister(RDI), method_handle_);
     x86_64_codegen->InvokeRuntime(QuickEntrypointEnum::kQuickInvokePolymorphicWithHiddenReceiver,
-                                  instruction_,
-                                  instruction_->GetDexPc());
+                                  instruction_);
 
     RestoreLiveRegisters(codegen, instruction_->GetLocations());
     __ jmp(GetExitLabel());
@@ -194,16 +194,34 @@ static void CreateIntToFPLocations(ArenaAllocator* allocator, HInvoke* invoke) {
   locations->SetOut(Location::RequiresFpuRegister());
 }
 
+static void MoveFPToInt(
+    CpuRegister dst, XmmRegister src, bool is64bit, X86_64Assembler* assembler) {
+  if (is64bit) {
+    __ movq(dst, src);
+  } else {
+    __ movd(dst, src);
+  }
+}
+
+static void MoveIntToFP(
+    XmmRegister dst, CpuRegister src, bool is64bit, X86_64Assembler* assembler) {
+  if (is64bit) {
+    __ movq(dst, src);
+  } else {
+    __ movd(dst, src);
+  }
+}
+
 static void MoveFPToInt(LocationSummary* locations, bool is64bit, X86_64Assembler* assembler) {
-  Location input = locations->InAt(0);
-  Location output = locations->Out();
-  __ movd(output.AsRegister<CpuRegister>(), input.AsFpuRegister<XmmRegister>(), is64bit);
+  XmmRegister input = locations->InAt(0).AsFpuRegister<XmmRegister>();
+  CpuRegister output = locations->Out().AsRegister<CpuRegister>();
+  MoveFPToInt(output, input, is64bit, assembler);
 }
 
 static void MoveIntToFP(LocationSummary* locations, bool is64bit, X86_64Assembler* assembler) {
-  Location input = locations->InAt(0);
-  Location output = locations->Out();
-  __ movd(output.AsFpuRegister<XmmRegister>(), input.AsRegister<CpuRegister>(), is64bit);
+  CpuRegister input = locations->InAt(0).AsRegister<CpuRegister>();
+  XmmRegister output = locations->Out().AsFpuRegister<XmmRegister>();
+  MoveIntToFP(output, input, is64bit, assembler);
 }
 
 void IntrinsicLocationsBuilderX86_64::VisitDoubleDoubleToRawLongBits(HInvoke* invoke) {
@@ -502,7 +520,7 @@ static void GenFPToFPCall(HInvoke* invoke, CodeGeneratorX86_64* codegen,
   DCHECK(locations->WillCall());
   DCHECK(invoke->IsInvokeStaticOrDirect());
 
-  codegen->InvokeRuntime(entry, invoke, invoke->GetDexPc());
+  codegen->InvokeRuntime(entry, invoke);
 }
 
 void IntrinsicLocationsBuilderX86_64::VisitMathCos(HInvoke* invoke) {
@@ -1222,7 +1240,7 @@ void IntrinsicCodeGeneratorX86_64::VisitStringCompareTo(HInvoke* invoke) {
   codegen_->AddSlowPath(slow_path);
   __ j(kEqual, slow_path->GetEntryLabel());
 
-  codegen_->InvokeRuntime(kQuickStringCompareTo, invoke, invoke->GetDexPc(), slow_path);
+  codegen_->InvokeRuntime(kQuickStringCompareTo, invoke, slow_path);
   __ Bind(slow_path->GetExitLabel());
 }
 
@@ -1547,7 +1565,7 @@ void IntrinsicCodeGeneratorX86_64::VisitStringNewStringFromBytes(HInvoke* invoke
   codegen_->AddSlowPath(slow_path);
   __ j(kEqual, slow_path->GetEntryLabel());
 
-  codegen_->InvokeRuntime(kQuickAllocStringFromBytes, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickAllocStringFromBytes, invoke);
   CheckEntrypointTypes<kQuickAllocStringFromBytes, void*, void*, int32_t, int32_t, int32_t>();
   __ Bind(slow_path->GetExitLabel());
 }
@@ -1569,7 +1587,7 @@ void IntrinsicCodeGeneratorX86_64::VisitStringNewStringFromChars(HInvoke* invoke
   //   java.lang.StringFactory.newStringFromChars(int offset, int charCount, char[] data)
   //
   // all include a null check on `data` before calling that method.
-  codegen_->InvokeRuntime(kQuickAllocStringFromChars, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickAllocStringFromChars, invoke);
   CheckEntrypointTypes<kQuickAllocStringFromChars, void*, int32_t, int32_t, void*>();
 }
 
@@ -1591,7 +1609,7 @@ void IntrinsicCodeGeneratorX86_64::VisitStringNewStringFromString(HInvoke* invok
   codegen_->AddSlowPath(slow_path);
   __ j(kEqual, slow_path->GetEntryLabel());
 
-  codegen_->InvokeRuntime(kQuickAllocStringFromString, invoke, invoke->GetDexPc());
+  codegen_->InvokeRuntime(kQuickAllocStringFromString, invoke);
   CheckEntrypointTypes<kQuickAllocStringFromString, void*, void*>();
   __ Bind(slow_path->GetExitLabel());
 }
@@ -2097,8 +2115,8 @@ void IntrinsicLocationsBuilderX86_64::VisitUnsafePut(HInvoke* invoke) {
 void IntrinsicLocationsBuilderX86_64::VisitUnsafePutAbsolute(HInvoke* invoke) {
   VisitJdkUnsafePutAbsolute(invoke);
 }
-void IntrinsicLocationsBuilderX86_64::VisitUnsafePutOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutOrdered(invoke);
+void IntrinsicLocationsBuilderX86_64::VisitUnsafePutOrderedInt(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedInt(invoke);
 }
 void IntrinsicLocationsBuilderX86_64::VisitUnsafePutVolatile(HInvoke* invoke) {
   VisitJdkUnsafePutVolatile(invoke);
@@ -2106,8 +2124,8 @@ void IntrinsicLocationsBuilderX86_64::VisitUnsafePutVolatile(HInvoke* invoke) {
 void IntrinsicLocationsBuilderX86_64::VisitUnsafePutObject(HInvoke* invoke) {
   VisitJdkUnsafePutReference(invoke);
 }
-void IntrinsicLocationsBuilderX86_64::VisitUnsafePutObjectOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutObjectOrdered(invoke);
+void IntrinsicLocationsBuilderX86_64::VisitUnsafePutOrderedObject(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedObject(invoke);
 }
 void IntrinsicLocationsBuilderX86_64::VisitUnsafePutObjectVolatile(HInvoke* invoke) {
   VisitJdkUnsafePutReferenceVolatile(invoke);
@@ -2131,7 +2149,7 @@ void IntrinsicLocationsBuilderX86_64::VisitJdkUnsafePut(HInvoke* invoke) {
 void IntrinsicLocationsBuilderX86_64::VisitJdkUnsafePutAbsolute(HInvoke* invoke) {
   CreateIntIntIntToVoidPlusTempsLocations(allocator_, DataType::Type::kInt32, invoke);
 }
-void IntrinsicLocationsBuilderX86_64::VisitJdkUnsafePutOrdered(HInvoke* invoke) {
+void IntrinsicLocationsBuilderX86_64::VisitJdkUnsafePutOrderedInt(HInvoke* invoke) {
   CreateIntIntIntIntToVoidPlusTempsLocations(allocator_, DataType::Type::kInt32, invoke);
 }
 void IntrinsicLocationsBuilderX86_64::VisitJdkUnsafePutVolatile(HInvoke* invoke) {
@@ -2143,7 +2161,7 @@ void IntrinsicLocationsBuilderX86_64::VisitJdkUnsafePutRelease(HInvoke* invoke)
 void IntrinsicLocationsBuilderX86_64::VisitJdkUnsafePutReference(HInvoke* invoke) {
   CreateIntIntIntIntToVoidPlusTempsLocations(allocator_, DataType::Type::kReference, invoke);
 }
-void IntrinsicLocationsBuilderX86_64::VisitJdkUnsafePutObjectOrdered(HInvoke* invoke) {
+void IntrinsicLocationsBuilderX86_64::VisitJdkUnsafePutOrderedObject(HInvoke* invoke) {
   CreateIntIntIntIntToVoidPlusTempsLocations(allocator_, DataType::Type::kReference, invoke);
 }
 void IntrinsicLocationsBuilderX86_64::VisitJdkUnsafePutReferenceVolatile(HInvoke* invoke) {
@@ -2236,8 +2254,8 @@ void IntrinsicCodeGeneratorX86_64::VisitUnsafePut(HInvoke* invoke) {
 void IntrinsicCodeGeneratorX86_64::VisitUnsafePutAbsolute(HInvoke* invoke) {
   VisitJdkUnsafePutAbsolute(invoke);
 }
-void IntrinsicCodeGeneratorX86_64::VisitUnsafePutOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutOrdered(invoke);
+void IntrinsicCodeGeneratorX86_64::VisitUnsafePutOrderedInt(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedInt(invoke);
 }
 void IntrinsicCodeGeneratorX86_64::VisitUnsafePutVolatile(HInvoke* invoke) {
   VisitJdkUnsafePutVolatile(invoke);
@@ -2245,8 +2263,8 @@ void IntrinsicCodeGeneratorX86_64::VisitUnsafePutVolatile(HInvoke* invoke) {
 void IntrinsicCodeGeneratorX86_64::VisitUnsafePutObject(HInvoke* invoke) {
   VisitJdkUnsafePutReference(invoke);
 }
-void IntrinsicCodeGeneratorX86_64::VisitUnsafePutObjectOrdered(HInvoke* invoke) {
-  VisitJdkUnsafePutObjectOrdered(invoke);
+void IntrinsicCodeGeneratorX86_64::VisitUnsafePutOrderedObject(HInvoke* invoke) {
+  VisitJdkUnsafePutOrderedObject(invoke);
 }
 void IntrinsicCodeGeneratorX86_64::VisitUnsafePutObjectVolatile(HInvoke* invoke) {
   VisitJdkUnsafePutReferenceVolatile(invoke);
@@ -2271,7 +2289,7 @@ void IntrinsicCodeGeneratorX86_64::VisitJdkUnsafePutAbsolute(HInvoke* invoke) {
   GenUnsafePutAbsolute(
       invoke->GetLocations(), DataType::Type::kInt32, /*is_volatile=*/false, codegen_);
 }
-void IntrinsicCodeGeneratorX86_64::VisitJdkUnsafePutOrdered(HInvoke* invoke) {
+void IntrinsicCodeGeneratorX86_64::VisitJdkUnsafePutOrderedInt(HInvoke* invoke) {
   GenUnsafePut(invoke->GetLocations(), DataType::Type::kInt32, /*is_volatile=*/ false, codegen_);
 }
 void IntrinsicCodeGeneratorX86_64::VisitJdkUnsafePutVolatile(HInvoke* invoke) {
@@ -2284,7 +2302,7 @@ void IntrinsicCodeGeneratorX86_64::VisitJdkUnsafePutReference(HInvoke* invoke) {
   GenUnsafePut(
       invoke->GetLocations(), DataType::Type::kReference, /*is_volatile=*/ false, codegen_);
 }
-void IntrinsicCodeGeneratorX86_64::VisitJdkUnsafePutObjectOrdered(HInvoke* invoke) {
+void IntrinsicCodeGeneratorX86_64::VisitJdkUnsafePutOrderedObject(HInvoke* invoke) {
   GenUnsafePut(
       invoke->GetLocations(), DataType::Type::kReference, /*is_volatile=*/ false, codegen_);
 }
@@ -2505,7 +2523,7 @@ static void GenCompareAndSetOrExchangeFP(CodeGeneratorX86_64* codegen,
     if (byte_swap) {
       instr_codegen->Bswap(rax_loc, type);
     }
-    __ movd(out.AsFpuRegister<XmmRegister>(), CpuRegister(RAX), is64bit);
+    MoveIntToFP(out.AsFpuRegister<XmmRegister>(), CpuRegister(RAX), is64bit, assembler);
   } else {
     GenZFlagToResult(assembler, out.AsRegister<CpuRegister>());
   }
@@ -3398,7 +3416,7 @@ void IntrinsicCodeGeneratorX86_64::HandleValueOf(HInvoke* invoke,
   CpuRegister argument = CpuRegister(calling_convention.GetRegisterAt(0));
   auto allocate_instance = [&]() {
     codegen_->LoadIntrinsicDeclaringClass(argument, invoke);
-    codegen_->InvokeRuntime(kQuickAllocObjectInitialized, invoke, invoke->GetDexPc());
+    codegen_->InvokeRuntime(kQuickAllocObjectInitialized, invoke);
     CheckEntrypointTypes<kQuickAllocObjectWithChecks, void*, mirror::Class*>();
   };
   if (invoke->InputAt(0)->IsIntConstant()) {
@@ -4056,7 +4074,7 @@ static void GenerateVarHandleTarget(HInvoke* invoke,
   if (expected_coordinates_count <= 1u) {
     if (VarHandleOptimizations(invoke).GetUseKnownImageVarHandle()) {
       ScopedObjectAccess soa(Thread::Current());
-      ArtField* target_field = GetBootImageVarHandleField(invoke);
+      ArtField* target_field = GetImageVarHandleField(invoke);
       if (expected_coordinates_count == 0u) {
         ObjPtr<mirror::Class> declaring_class = target_field->GetDeclaringClass();
         __ movl(CpuRegister(target.object),
@@ -4224,47 +4242,81 @@ void IntrinsicLocationsBuilderX86_64::VisitMethodHandleInvokeExact(HInvoke* invo
   InvokeDexCallingConventionVisitorX86_64 calling_convention;
   locations->SetOut(calling_convention.GetReturnLocation(invoke->GetType()));
 
-  locations->SetInAt(0, Location::RequiresRegister());
+  uint32_t number_of_args = invoke->GetNumberOfArguments();
 
   // Accomodating LocationSummary for underlying invoke-* call.
-  uint32_t number_of_args = invoke->GetNumberOfArguments();
   for (uint32_t i = 1; i < number_of_args; ++i) {
     locations->SetInAt(i, calling_convention.GetNextLocation(invoke->InputAt(i)->GetType()));
   }
 
+  // Passing MethodHandle object as the last parameter: accessors implementation rely on it.
+  DCHECK_EQ(invoke->InputAt(0)->GetType(), DataType::Type::kReference);
+  Location receiver_mh_loc = calling_convention.GetNextLocation(DataType::Type::kReference);
+  locations->SetInAt(0, receiver_mh_loc);
+
   // The last input is MethodType object corresponding to the call-site.
   locations->SetInAt(number_of_args, Location::RequiresRegister());
 
   locations->AddTemp(Location::RequiresRegister());
   // Hidden arg for invoke-interface.
   locations->AddTemp(Location::RegisterLocation(RAX));
+
+  if (!receiver_mh_loc.IsRegister()) {
+    locations->AddTemp(Location::RequiresRegister());
+  }
 }
 
 void IntrinsicCodeGeneratorX86_64::VisitMethodHandleInvokeExact(HInvoke* invoke) {
   LocationSummary* locations = invoke->GetLocations();
+  X86_64Assembler* assembler = codegen_->GetAssembler();
+
+  Location receiver_mh_loc = locations->InAt(0);
+  CpuRegister method_handle = receiver_mh_loc.IsRegister()
+      ? receiver_mh_loc.AsRegister<CpuRegister>()
+      : locations->GetTemp(2).AsRegister<CpuRegister>();
 
-  CpuRegister method_handle = locations->InAt(0).AsRegister<CpuRegister>();
+  if (!receiver_mh_loc.IsRegister()) {
+    DCHECK(receiver_mh_loc.IsStackSlot());
+    __ movl(method_handle, Address(CpuRegister(RSP), receiver_mh_loc.GetStackIndex()));
+  }
 
   SlowPathCode* slow_path =
       new (codegen_->GetScopedAllocator()) InvokePolymorphicSlowPathX86_64(invoke, method_handle);
   codegen_->AddSlowPath(slow_path);
-  X86_64Assembler* assembler = codegen_->GetAssembler();
 
   CpuRegister call_site_type =
       locations->InAt(invoke->GetNumberOfArguments()).AsRegister<CpuRegister>();
 
+  CpuRegister temp = locations->GetTemp(0).AsRegister<CpuRegister>();
+
   // Call site should match with MethodHandle's type.
-  __ MaybePoisonHeapReference(call_site_type);
-  __ cmpl(call_site_type, Address(method_handle, mirror::MethodHandle::MethodTypeOffset()));
-  __ j(kNotEqual, slow_path->GetEntryLabel());
+  if (kPoisonHeapReferences) {
+    // call_site_type should be left intact as it 1) might be in callee-saved register 2) is known
+    // for GC to contain a reference.
+    __ movl(temp, call_site_type);
+    __ PoisonHeapReference(temp);
+    __ cmpl(temp, Address(method_handle, mirror::MethodHandle::MethodTypeOffset()));
+    __ j(kNotEqual, slow_path->GetEntryLabel());
+  } else {
+    __ cmpl(call_site_type, Address(method_handle, mirror::MethodHandle::MethodTypeOffset()));
+    __ j(kNotEqual, slow_path->GetEntryLabel());
+  }
 
   CpuRegister method = CpuRegister(kMethodRegisterArgument);
   __ movq(method, Address(method_handle, mirror::MethodHandle::ArtFieldOrMethodOffset()));
 
-  Label static_dispatch;
   Label execute_target_method;
+  Label method_dispatch;
+  Label static_dispatch;
 
   Address method_handle_kind = Address(method_handle, mirror::MethodHandle::HandleKindOffset());
+
+  __ cmpl(method_handle_kind, Immediate(mirror::MethodHandle::kFirstAccessorKind));
+  __ j(kLess, &method_dispatch);
+  __ movq(method, Address(method_handle, mirror::MethodHandleImpl::TargetOffset()));
+  __ Jump(&execute_target_method);
+
+  __ Bind(&method_dispatch);
   if (invoke->AsInvokePolymorphic()->CanTargetInstanceMethod()) {
     CpuRegister receiver = locations->InAt(1).AsRegister<CpuRegister>();
 
@@ -4285,8 +4337,6 @@ void IntrinsicCodeGeneratorX86_64::VisitMethodHandleInvokeExact(HInvoke* invoke)
     __ testl(Address(method, ArtMethod::AccessFlagsOffset()), Immediate(kAccPrivate));
     __ j(kNotZero, &execute_target_method);
 
-    CpuRegister temp = locations->GetTemp(0).AsRegister<CpuRegister>();
-
     __ movl(temp, Address(method, ArtMethod::DeclaringClassOffset()));
     __ cmpl(temp, Address(receiver, mirror::Object::ClassOffset()));
     // If method is defined in the receiver's class, execute it as it is.
@@ -4357,7 +4407,7 @@ void IntrinsicCodeGeneratorX86_64::VisitMethodHandleInvokeExact(HInvoke* invoke)
   __ call(Address(
       method,
       ArtMethod::EntryPointFromQuickCompiledCodeOffset(art::PointerSize::k64).SizeValue()));
-  codegen_->RecordPcInfo(invoke, invoke->GetDexPc(), slow_path);
+  codegen_->RecordPcInfo(invoke, slow_path);
   __ Bind(slow_path->GetExitLabel());
 }
 
@@ -4738,7 +4788,8 @@ static void GenerateVarHandleGetAndSet(HInvoke* invoke,
       codegen->GetInstructionCodegen()->Bswap(temp, bswap_type);
     }
     if (!is_void) {
-      __ movd(out.AsFpuRegister<XmmRegister>(), temp.AsRegister<CpuRegister>(), is64bit);
+      MoveIntToFP(
+          out.AsFpuRegister<XmmRegister>(), temp.AsRegister<CpuRegister>(), is64bit, assembler);
     }
   } else if (type == DataType::Type::kReference) {
     // `getAndSet` for references: load reference and atomically exchange it with the field.
@@ -5088,11 +5139,11 @@ static void GenerateVarHandleGetAndAdd(HInvoke* invoke,
     } else {
       __ movss(fptemp, field_addr);
     }
-    __ movd(CpuRegister(RAX), fptemp, is64bit);
+    MoveFPToInt(CpuRegister(RAX), fptemp, is64bit, assembler);
     // If necessary, byte swap RAX and update the value in FP register to also be byte-swapped.
     if (byte_swap) {
       codegen->GetInstructionCodegen()->Bswap(rax_loc, bswap_type);
-      __ movd(fptemp, CpuRegister(RAX), is64bit);
+      MoveIntToFP(fptemp, CpuRegister(RAX), is64bit, assembler);
     }
     // Perform the FP addition and move it to a temporary register to prepare for CMPXCHG.
     if (is64bit) {
@@ -5100,7 +5151,7 @@ static void GenerateVarHandleGetAndAdd(HInvoke* invoke,
     } else {
       __ addss(fptemp, value.AsFpuRegister<XmmRegister>());
     }
-    __ movd(temp, fptemp, is64bit);
+    MoveFPToInt(temp, fptemp, is64bit, assembler);
     // If necessary, byte swap RAX before CMPXCHG and the temporary before copying to FP register.
     if (byte_swap) {
       codegen->GetInstructionCodegen()->Bswap(temp_loc, bswap_type);
@@ -5119,7 +5170,7 @@ static void GenerateVarHandleGetAndAdd(HInvoke* invoke,
       codegen->GetInstructionCodegen()->Bswap(rax_loc, bswap_type);
     }
     if (!is_void) {
-      __ movd(out.AsFpuRegister<XmmRegister>(), CpuRegister(RAX), is64bit);
+      MoveIntToFP(out.AsFpuRegister<XmmRegister>(), CpuRegister(RAX), is64bit, assembler);
     }
   } else {
     if (byte_swap) {
diff --git a/compiler/optimizing/licm.cc b/compiler/optimizing/licm.cc
index 0c791b640d..729a277ee0 100644
--- a/compiler/optimizing/licm.cc
+++ b/compiler/optimizing/licm.cc
@@ -64,7 +64,9 @@ static bool InputsAreDefinedBeforeLoop(HInstruction* instruction) {
 /**
  * If `environment` has a loop header phi, we replace it with its first input.
  */
-static void UpdateLoopPhisIn(HEnvironment* environment, HLoopInformation* info) {
+static void UpdateLoopPhisIn(ArenaAllocator* allocator,
+                             HEnvironment* environment,
+                             HLoopInformation* info) {
   for (; environment != nullptr; environment = environment->GetParent()) {
     for (size_t i = 0, e = environment->Size(); i < e; ++i) {
       HInstruction* input = environment->GetInstructionAt(i);
@@ -72,7 +74,7 @@ static void UpdateLoopPhisIn(HEnvironment* environment, HLoopInformation* info)
         environment->RemoveAsUserOfInput(i);
         HInstruction* incoming = input->InputAt(0);
         environment->SetRawEnvAt(i, incoming);
-        incoming->AddEnvUseAt(environment, i);
+        incoming->AddEnvUseAt(allocator, environment, i);
       }
     }
   }
@@ -152,7 +154,7 @@ bool LICM::Run() {
           // We need to update the environment if the instruction has a loop header
           // phi in it.
           if (instruction->NeedsEnvironment()) {
-            UpdateLoopPhisIn(instruction->GetEnvironment(), loop_info);
+            UpdateLoopPhisIn(graph_->GetAllocator(), instruction->GetEnvironment(), loop_info);
           } else {
             DCHECK(!instruction->HasEnvironment());
           }
diff --git a/compiler/optimizing/liveness_test.cc b/compiler/optimizing/liveness_test.cc
index 0b421cf9e6..1e4ef8f867 100644
--- a/compiler/optimizing/liveness_test.cc
+++ b/compiler/optimizing/liveness_test.cc
@@ -33,14 +33,14 @@ class LivenessTest : public CommonCompilerTest, public OptimizingUnitTestHelper
   void TestCode(const std::vector<uint16_t>& data, const char* expected);
 };
 
-static void DumpBitVector(BitVector* vector,
+static void DumpBitVector(BitVectorView<size_t> vector,
                           std::ostream& buffer,
                           size_t count,
                           const char* prefix) {
   buffer << prefix;
   buffer << '(';
   for (size_t i = 0; i < count; ++i) {
-    buffer << vector->IsBitSet(i);
+    buffer << vector.IsBitSet(i);
   }
   buffer << ")\n";
 }
@@ -59,11 +59,11 @@ void LivenessTest::TestCode(const std::vector<uint16_t>& data, const char* expec
   for (HBasicBlock* block : graph->GetBlocks()) {
     buffer << "Block " << block->GetBlockId() << std::endl;
     size_t ssa_values = liveness.GetNumberOfSsaValues();
-    BitVector* live_in = liveness.GetLiveInSet(*block);
+    BitVectorView<size_t> live_in = liveness.GetLiveInSet(*block);
     DumpBitVector(live_in, buffer, ssa_values, "  live in: ");
-    BitVector* live_out = liveness.GetLiveOutSet(*block);
+    BitVectorView<size_t> live_out = liveness.GetLiveOutSet(*block);
     DumpBitVector(live_out, buffer, ssa_values, "  live out: ");
-    BitVector* kill = liveness.GetKillSet(*block);
+    BitVectorView<size_t> kill = liveness.GetKillSet(*block);
     DumpBitVector(kill, buffer, ssa_values, "  kill: ");
   }
   ASSERT_STREQ(expected, buffer.str().c_str());
diff --git a/compiler/optimizing/load_store_analysis.h b/compiler/optimizing/load_store_analysis.h
index 088f5b710f..65c35e8372 100644
--- a/compiler/optimizing/load_store_analysis.h
+++ b/compiler/optimizing/load_store_analysis.h
@@ -78,7 +78,9 @@ class ReferenceInfo : public DeletableArenaObject<kArenaAllocLSA> {
 
  private:
   HInstruction* const reference_;
-  const size_t position_;  // position in HeapLocationCollector's ref_info_array_.
+  // Position in which it was inserted into the ref_infos_ vector. A smaller number means that this
+  // reference was seen before a reference with a bigger number.
+  const size_t position_;
 
   // Can only be referred to by a single name in the method.
   bool is_singleton_;
@@ -176,7 +178,7 @@ class HeapLocation : public ArenaObject<kArenaAllocLSA> {
 
 // A HeapLocationCollector collects all relevant heap locations and keeps
 // an aliasing matrix for all locations.
-class HeapLocationCollector : public HGraphVisitor {
+class HeapLocationCollector final : public HGraphVisitor {
  public:
   static constexpr size_t kHeapLocationNotFound = -1;
   // Start with a single uint32_t word. That's enough bits for pair-wise
@@ -186,7 +188,8 @@ class HeapLocationCollector : public HGraphVisitor {
   HeapLocationCollector(HGraph* graph, ScopedArenaAllocator* allocator)
       : HGraphVisitor(graph),
         allocator_(allocator),
-        ref_info_array_(allocator->Adapter(kArenaAllocLSA)),
+        ref_infos_(graph->GetCurrentInstructionId(), nullptr, allocator->Adapter(kArenaAllocLSA)),
+        ref_infos_created_(0u),
         heap_locations_(allocator->Adapter(kArenaAllocLSA)),
         aliasing_matrix_(allocator, kInitialAliasingMatrixBitVectorSize, true, kArenaAllocLSA),
         has_heap_stores_(false) {}
@@ -197,8 +200,8 @@ class HeapLocationCollector : public HGraphVisitor {
 
   void CleanUp() {
     heap_locations_.clear();
-    STLDeleteContainerPointers(ref_info_array_.begin(), ref_info_array_.end());
-    ref_info_array_.clear();
+    STLDeleteContainerPointers(ref_infos_.begin(), ref_infos_.end());
+    ref_infos_.clear();
   }
 
   size_t GetNumberOfHeapLocations() const {
@@ -227,14 +230,7 @@ class HeapLocationCollector : public HGraphVisitor {
   }
 
   ReferenceInfo* FindReferenceInfoOf(HInstruction* ref) const {
-    for (size_t i = 0; i < ref_info_array_.size(); i++) {
-      ReferenceInfo* ref_info = ref_info_array_[i];
-      if (ref_info->GetReference() == ref) {
-        DCHECK_EQ(i, ref_info->GetPosition());
-        return ref_info;
-      }
-    }
-    return nullptr;
+    return ref_infos_[ref->GetId()];
   }
 
   size_t GetFieldHeapLocation(HInstruction* object, const FieldInfo* field) const {
@@ -429,9 +425,9 @@ class HeapLocationCollector : public HGraphVisitor {
   ReferenceInfo* GetOrCreateReferenceInfo(HInstruction* instruction) {
     ReferenceInfo* ref_info = FindReferenceInfoOf(instruction);
     if (ref_info == nullptr) {
-      size_t pos = ref_info_array_.size();
-      ref_info = new (allocator_) ReferenceInfo(instruction, pos);
-      ref_info_array_.push_back(ref_info);
+      ref_info = new (allocator_) ReferenceInfo(instruction, ref_infos_created_);
+      ref_infos_created_++;
+      ref_infos_[instruction->GetId()] = ref_info;
     }
     return ref_info;
   }
@@ -462,7 +458,9 @@ class HeapLocationCollector : public HGraphVisitor {
     }
   }
 
-  void VisitFieldAccess(HInstruction* ref, const FieldInfo& field_info) {
+  void VisitFieldAccess(HFieldAccess* instruction) override {
+    HInstruction* ref = instruction->InputAt(0);
+    const FieldInfo& field_info = instruction->GetFieldInfo();
     DataType::Type type = field_info.GetFieldType();
     const uint16_t declaring_class_def_index = field_info.GetDeclaringClassDefIndex();
     const size_t offset = field_info.GetFieldOffset().SizeValue();
@@ -490,23 +488,23 @@ class HeapLocationCollector : public HGraphVisitor {
   }
 
   void VisitInstanceFieldGet(HInstanceFieldGet* instruction) override {
-    VisitFieldAccess(instruction->InputAt(0), instruction->GetFieldInfo());
     CreateReferenceInfoForReferenceType(instruction);
+    VisitFieldAccess(instruction);
   }
 
   void VisitInstanceFieldSet(HInstanceFieldSet* instruction) override {
-    VisitFieldAccess(instruction->InputAt(0), instruction->GetFieldInfo());
     has_heap_stores_ = true;
+    VisitFieldAccess(instruction);
   }
 
   void VisitStaticFieldGet(HStaticFieldGet* instruction) override {
-    VisitFieldAccess(instruction->InputAt(0), instruction->GetFieldInfo());
     CreateReferenceInfoForReferenceType(instruction);
+    VisitFieldAccess(instruction);
   }
 
   void VisitStaticFieldSet(HStaticFieldSet* instruction) override {
-    VisitFieldAccess(instruction->InputAt(0), instruction->GetFieldInfo());
     has_heap_stores_ = true;
+    VisitFieldAccess(instruction);
   }
 
   // We intentionally don't collect HUnresolvedInstanceField/HUnresolvedStaticField accesses
@@ -548,9 +546,8 @@ class HeapLocationCollector : public HGraphVisitor {
 
   void VisitInstruction(HInstruction* instruction) override {
     // Any new-instance or new-array cannot alias with references that
-    // pre-exist the new-instance/new-array. We append entries into
-    // ref_info_array_ which keeps track of the order of creation
-    // of reference values since we visit the blocks in reverse post order.
+    // pre-exist the new-instance/new-array. The entries of ref_infos_ keep track of the order of
+    // creation of reference values since we visit the blocks in reverse post order.
     //
     // By default, VisitXXX() (including VisitPhi()) calls VisitInstruction(),
     // unless VisitXXX() is overridden. VisitInstanceFieldGet() etc. above
@@ -559,7 +556,10 @@ class HeapLocationCollector : public HGraphVisitor {
   }
 
   ScopedArenaAllocator* allocator_;
-  ScopedArenaVector<ReferenceInfo*> ref_info_array_;   // All references used for heap accesses.
+  // All references used for heap accesses, accessed via instruction id.
+  ScopedArenaVector<ReferenceInfo*> ref_infos_;
+  // How many non-null ReferenceInfo* are in ref_infos_.
+  size_t ref_infos_created_;
   ScopedArenaVector<HeapLocation*> heap_locations_;    // All heap locations.
   ArenaBitVector aliasing_matrix_;    // aliasing info between each pair of locations.
   bool has_heap_stores_;    // If there is no heap stores, LSE acts as GVN with better
diff --git a/compiler/optimizing/load_store_elimination.cc b/compiler/optimizing/load_store_elimination.cc
index c4b68e41c9..799c1dcbf0 100644
--- a/compiler/optimizing/load_store_elimination.cc
+++ b/compiler/optimizing/load_store_elimination.cc
@@ -24,6 +24,7 @@
 #include "base/arena_allocator.h"
 #include "base/arena_bit_vector.h"
 #include "base/array_ref.h"
+#include "base/bit_utils_iterator.h"
 #include "base/bit_vector-inl.h"
 #include "base/bit_vector.h"
 #include "base/globals.h"
@@ -304,9 +305,12 @@ class LSEVisitor final : private HGraphDelegateVisitor {
     struct NeedsNonLoopPhiMarker {
       PhiPlaceholder phi_;
     };
-    struct NeedsLoopPhiMarker {
+    struct NeedsPlainLoopPhiMarker {
       PhiPlaceholder phi_;
     };
+    struct NeedsConvertedLoopPhiMarker {
+      HInstruction* load_;  // Load from a narrower location than the loop phi it needs.
+    };
 
     static constexpr Value Invalid() {
       return Value(ValuelessType::kInvalid);
@@ -334,12 +338,16 @@ class LSEVisitor final : private HGraphDelegateVisitor {
       return Value(NeedsNonLoopPhiMarker{phi_placeholder});
     }
 
-    static constexpr Value ForLoopPhiPlaceholder(PhiPlaceholder phi_placeholder) {
-      return Value(NeedsLoopPhiMarker{phi_placeholder});
+    static constexpr Value ForPlainLoopPhiPlaceholder(PhiPlaceholder phi_placeholder) {
+      return Value(NeedsPlainLoopPhiMarker{phi_placeholder});
+    }
+
+    static constexpr Value ForConvertedLoopPhiPlaceholder(HInstruction* load) {
+      return Value(NeedsConvertedLoopPhiMarker{load});
     }
 
     static constexpr Value ForPhiPlaceholder(PhiPlaceholder phi_placeholder, bool needs_loop_phi) {
-      return needs_loop_phi ? ForLoopPhiPlaceholder(phi_placeholder)
+      return needs_loop_phi ? ForPlainLoopPhiPlaceholder(phi_placeholder)
                             : ForNonLoopPhiPlaceholder(phi_placeholder);
     }
 
@@ -370,8 +378,16 @@ class LSEVisitor final : private HGraphDelegateVisitor {
       return std::holds_alternative<NeedsNonLoopPhiMarker>(value_);
     }
 
+    bool NeedsPlainLoopPhi() const {
+      return std::holds_alternative<NeedsPlainLoopPhiMarker>(value_);
+    }
+
+    bool NeedsConvertedLoopPhi() const {
+      return std::holds_alternative<NeedsConvertedLoopPhiMarker>(value_);
+    }
+
     bool NeedsLoopPhi() const {
-      return std::holds_alternative<NeedsLoopPhiMarker>(value_);
+      return NeedsPlainLoopPhi() || NeedsConvertedLoopPhi();
     }
 
     bool NeedsPhi() const {
@@ -384,11 +400,11 @@ class LSEVisitor final : private HGraphDelegateVisitor {
     }
 
     PhiPlaceholder GetPhiPlaceholder() const {
-      DCHECK(NeedsPhi());
       if (NeedsNonLoopPhi()) {
         return std::get<NeedsNonLoopPhiMarker>(value_).phi_;
       } else {
-        return std::get<NeedsLoopPhiMarker>(value_).phi_;
+        DCHECK(NeedsPlainLoopPhi());
+        return std::get<NeedsPlainLoopPhiMarker>(value_).phi_;
       }
     }
 
@@ -397,6 +413,11 @@ class LSEVisitor final : private HGraphDelegateVisitor {
       return GetPhiPlaceholder().GetHeapLocation();
     }
 
+    HInstruction* GetLoopPhiConversionLoad() const {
+      DCHECK(NeedsConvertedLoopPhi());
+      return std::get<NeedsConvertedLoopPhiMarker>(value_).load_;
+    }
+
     constexpr bool ExactEquals(Value other) const;
 
     constexpr bool Equals(Value other) const;
@@ -414,7 +435,8 @@ class LSEVisitor final : private HGraphDelegateVisitor {
     using ValueHolder = std::variant<ValuelessType,
                                      HInstruction*,
                                      NeedsNonLoopPhiMarker,
-                                     NeedsLoopPhiMarker>;
+                                     NeedsPlainLoopPhiMarker,
+                                     NeedsConvertedLoopPhiMarker>;
     constexpr ValuelessType GetValuelessType() const {
       return std::get<ValuelessType>(value_);
     }
@@ -428,11 +450,68 @@ class LSEVisitor final : private HGraphDelegateVisitor {
     static_assert(std::is_move_assignable<PhiPlaceholder>::value);
   };
 
-  friend constexpr bool operator==(const Value::NeedsLoopPhiMarker& p1,
-                                   const Value::NeedsLoopPhiMarker& p2);
+  friend constexpr bool operator==(const Value::NeedsPlainLoopPhiMarker& p1,
+                                   const Value::NeedsPlainLoopPhiMarker& p2);
+  friend constexpr bool operator==(const Value::NeedsConvertedLoopPhiMarker& p1,
+                                   const Value::NeedsConvertedLoopPhiMarker& p2);
   friend constexpr bool operator==(const Value::NeedsNonLoopPhiMarker& p1,
                                    const Value::NeedsNonLoopPhiMarker& p2);
 
+  class TypeConversionSet {
+   public:
+    TypeConversionSet() : type_conversions_(0u) {}
+
+    void Add(DataType::Type result_type) {
+      static_assert(enum_cast<>(DataType::Type::kLast) < BitSizeOf<uint32_t>());
+      type_conversions_ |= 1u << enum_cast<>(result_type);
+    }
+
+    void Add(TypeConversionSet other) {
+      type_conversions_ |= other.type_conversions_;
+    }
+
+    bool AreAllTypeConversionsImplicit(HInstruction* input) const {
+      if (type_conversions_ != 0u) {
+        if (input->IsIntConstant()) {
+          int32_t value = input->AsIntConstant()->GetValue();
+          for (uint32_t raw_type : LowToHighBits(type_conversions_)) {
+            DataType::Type type = enum_cast<DataType::Type>(raw_type);
+            if (!DataType::IsTypeConversionImplicit(value, type)) {
+              return false;
+            }
+          }
+        } else {
+          DataType::Type input_type = input->GetType();
+          for (uint32_t raw_type : LowToHighBits(type_conversions_)) {
+            DataType::Type type = enum_cast<DataType::Type>(raw_type);
+            if (!DataType::IsTypeConversionImplicit(input_type, type)) {
+              return false;
+            }
+          }
+        }
+      }
+      return true;
+    }
+
+   private:
+    uint32_t type_conversions_;
+  };
+
+  Value SkipTypeConversions(Value value,
+                            /*inout*/ TypeConversionSet* type_conversions = nullptr) const {
+    while (value.NeedsConvertedLoopPhi()) {
+      HInstruction* conversion_load = value.GetLoopPhiConversionLoad();
+      DCHECK(!conversion_load->IsVecLoad());
+      if (type_conversions != nullptr) {
+        type_conversions->Add(conversion_load->GetType());
+      }
+      ValueRecord* prev_record = loads_requiring_loop_phi_[conversion_load->GetId()];
+      DCHECK(prev_record != nullptr);
+      value = prev_record->value;
+    }
+    return value;
+  }
+
   // Get Phi placeholder index for access to `phi_placeholder_replacements_`
   // and "visited" bit vectors during depth-first searches.
   size_t PhiPlaceholderIndex(PhiPlaceholder phi_placeholder) const {
@@ -446,7 +525,7 @@ class LSEVisitor final : private HGraphDelegateVisitor {
   }
 
   size_t PhiPlaceholderIndex(Value phi_placeholder) const {
-    return PhiPlaceholderIndex(phi_placeholder.GetPhiPlaceholder());
+    return PhiPlaceholderIndex(SkipTypeConversions(phi_placeholder).GetPhiPlaceholder());
   }
 
   bool IsEscapingObject(ReferenceInfo* info) { return !info->IsSingletonAndRemovable(); }
@@ -466,7 +545,8 @@ class LSEVisitor final : private HGraphDelegateVisitor {
   }
 
   Value Replacement(Value value) const {
-    DCHECK(value.NeedsPhi()) << value << " phase: " << current_phase_;
+    DCHECK(value.NeedsNonLoopPhi() || value.NeedsPlainLoopPhi())
+        << value << " phase: " << current_phase_;
     Value replacement = phi_placeholder_replacements_[PhiPlaceholderIndex(value)];
     DCHECK(replacement.IsUnknown() || replacement.IsInstruction());
     DCHECK(replacement.IsUnknown() ||
@@ -475,7 +555,12 @@ class LSEVisitor final : private HGraphDelegateVisitor {
   }
 
   Value ReplacementOrValue(Value value) const {
-    if (value.NeedsPhi() && phi_placeholder_replacements_[PhiPlaceholderIndex(value)].IsValid()) {
+    if (value.NeedsConvertedLoopPhi() &&
+        substitute_instructions_for_loads_[value.GetLoopPhiConversionLoad()->GetId()] != nullptr) {
+      return Value::ForInstruction(
+          substitute_instructions_for_loads_[value.GetLoopPhiConversionLoad()->GetId()]);
+    } else if ((value.NeedsNonLoopPhi() || value.NeedsPlainLoopPhi()) &&
+               phi_placeholder_replacements_[PhiPlaceholderIndex(value)].IsValid()) {
       return Replacement(value);
     } else {
       DCHECK_IMPLIES(value.IsInstruction(),
@@ -490,6 +575,66 @@ class LSEVisitor final : private HGraphDelegateVisitor {
     Value stored_by;
   };
 
+  // Calculate the value stored in location `idx` for a loop Phi placeholder-dependent `load`.
+  Value StoredValueForLoopPhiPlaceholderDependentLoad(size_t idx, HInstruction* load) const {
+    DCHECK(IsLoad(load));
+    DCHECK_LT(static_cast<size_t>(load->GetId()), loads_requiring_loop_phi_.size());
+    DCHECK(loads_requiring_loop_phi_[load->GetId()] != nullptr);
+    Value loaded_value = loads_requiring_loop_phi_[load->GetId()]->value;
+    DCHECK(loaded_value.NeedsLoopPhi());
+    DataType::Type load_type = load->GetType();
+    size_t load_size = DataType::Size(load_type);
+    size_t store_size = DataType::Size(heap_location_collector_.GetHeapLocation(idx)->GetType());
+
+    if (kIsDebugBuild && load->IsVecLoad()) {
+      // For vector operations, the load type is always `Float64` and therefore the store size is
+      // never higher and we do not record any conversions below. This is OK because we currently
+      // do not vectorize any loops with widening operations.
+      CHECK_EQ(load_size, DataType::Size(DataType::Type::kFloat64));
+      CHECK_LE(store_size, load_size);
+      CHECK(!loaded_value.NeedsConvertedLoopPhi());
+    } else if (kIsDebugBuild) {
+      // There are no implicit conversions between 64-bit types and smaller types.
+      // We shall not record any conversions for 64-bit types.
+      CHECK_EQ(load_size == DataType::Size(DataType::Type::kInt64),
+               store_size == DataType::Size(DataType::Type::kInt64));
+      CHECK_IMPLIES(load_size == DataType::Size(DataType::Type::kInt64),
+                    !loaded_value.NeedsConvertedLoopPhi());
+    }
+    // The `loaded_value` can record a conversion only if the `load` was from
+    // a wider field than the previous converting load.
+    DCHECK_IMPLIES(loaded_value.NeedsConvertedLoopPhi(),
+                   load_size > DataType::Size(loaded_value.GetLoopPhiConversionLoad()->GetType()));
+
+    Value value = loaded_value;
+    if (load_size < store_size) {
+      // Add a type conversion to a narrow type unless it's an implicit conversion
+      // from an already converted value.
+      if (!loaded_value.NeedsConvertedLoopPhi() ||
+          !DataType::IsTypeConversionImplicit(loaded_value.GetLoopPhiConversionLoad()->GetType(),
+                                              load_type)) {
+        value = Value::ForConvertedLoopPhiPlaceholder(load);
+      } else {
+        DCHECK(value.Equals(loaded_value));
+      }
+    } else {
+      // Remove conversions to types at least as wide as the field we're storing to.
+      // We record only conversions that define sign-/zero-extension bits to store.
+      while (value.NeedsConvertedLoopPhi() &&
+             DataType::Size(value.GetLoopPhiConversionLoad()->GetType()) >= store_size) {
+        HInstruction* conversion_load = value.GetLoopPhiConversionLoad();
+        ValueRecord* prev_record =
+            loads_requiring_loop_phi_[value.GetLoopPhiConversionLoad()->GetId()];
+        DCHECK(prev_record != nullptr);
+        value = prev_record->value;
+        DCHECK(value.NeedsLoopPhi());
+      }
+    }
+
+    DCHECK_EQ(PhiPlaceholderIndex(loaded_value), PhiPlaceholderIndex(value));
+    return value;
+  }
+
   HTypeConversion* FindOrAddTypeConversionIfNecessary(HInstruction* instruction,
                                                       HInstruction* value,
                                                       DataType::Type expected_type) {
@@ -502,6 +647,10 @@ class LSEVisitor final : private HGraphDelegateVisitor {
       return nullptr;
     }
 
+    // All vector instructions report their type as `Float64`, so the conversion is implicit.
+    // This is OK because we currently do not vectorize any loops with widening operations.
+    DCHECK(!instruction->IsVecLoad());
+
     // Check if there is already a suitable TypeConversion we can reuse.
     for (const HUseListNode<HInstruction*>& use : value->GetUses()) {
       if (use.GetUser()->IsTypeConversion() &&
@@ -716,6 +865,7 @@ class LSEVisitor final : private HGraphDelegateVisitor {
       /*out*/ ArenaBitVector* phi_placeholders_to_materialize,
       DataType::Type type,
       bool can_use_default_or_phi);
+  void MaterializeTypeConversionsIfNeeded(Value value);
   bool MaterializeLoopPhis(const ScopedArenaVector<size_t>& phi_placeholder_indexes,
                            DataType::Type type);
   bool MaterializeLoopPhis(ArrayRef<const size_t> phi_placeholder_indexes, DataType::Type type);
@@ -785,8 +935,10 @@ class LSEVisitor final : private HGraphDelegateVisitor {
       // Treat it as a normal load if it is a removable singleton.
     }
 
-    const FieldInfo& field = instruction->GetFieldInfo();
-    VisitGetLocation(instruction, heap_location_collector_.GetFieldHeapLocation(object, &field));
+    const FieldInfo& field_info = instruction->GetFieldInfo();
+    size_t idx = heap_location_collector_.GetFieldHeapLocation(object, &field_info);
+    RecordFieldInfo(&field_info, idx);
+    VisitGetLocation(instruction, idx);
   }
 
   void VisitInstanceFieldSet(HInstanceFieldSet* instruction) override {
@@ -801,9 +953,10 @@ class LSEVisitor final : private HGraphDelegateVisitor {
       // Treat it as a normal store if it is a removable singleton.
     }
 
-    const FieldInfo& field = instruction->GetFieldInfo();
+    const FieldInfo& field_info = instruction->GetFieldInfo();
     HInstruction* value = instruction->InputAt(1);
-    size_t idx = heap_location_collector_.GetFieldHeapLocation(object, &field);
+    size_t idx = heap_location_collector_.GetFieldHeapLocation(object, &field_info);
+    RecordFieldInfo(&field_info, idx);
     VisitSetLocation(instruction, idx, value);
   }
 
@@ -813,9 +966,11 @@ class LSEVisitor final : private HGraphDelegateVisitor {
       return;
     }
 
-    const FieldInfo& field = instruction->GetFieldInfo();
+    const FieldInfo& field_info = instruction->GetFieldInfo();
     HInstruction* cls = instruction->InputAt(0);
-    VisitGetLocation(instruction, heap_location_collector_.GetFieldHeapLocation(cls, &field));
+    size_t idx = heap_location_collector_.GetFieldHeapLocation(cls, &field_info);
+    RecordFieldInfo(&field_info, idx);
+    VisitGetLocation(instruction, idx);
   }
 
   void VisitStaticFieldSet(HStaticFieldSet* instruction) override {
@@ -824,10 +979,11 @@ class LSEVisitor final : private HGraphDelegateVisitor {
       return;
     }
 
-    const FieldInfo& field = instruction->GetFieldInfo();
+    const FieldInfo& field_info = instruction->GetFieldInfo();
     HInstruction* cls = instruction->InputAt(0);
     HInstruction* value = instruction->InputAt(1);
-    size_t idx = heap_location_collector_.GetFieldHeapLocation(cls, &field);
+    size_t idx = heap_location_collector_.GetFieldHeapLocation(cls, &field_info);
+    RecordFieldInfo(&field_info, idx);
     VisitSetLocation(instruction, idx, value);
   }
 
@@ -1205,11 +1361,16 @@ constexpr bool operator==(const LSEVisitor::PhiPlaceholder& p1,
   return p1.Equals(p2);
 }
 
-constexpr bool operator==(const LSEVisitor::Value::NeedsLoopPhiMarker& p1,
-                          const LSEVisitor::Value::NeedsLoopPhiMarker& p2) {
+constexpr bool operator==(const LSEVisitor::Value::NeedsPlainLoopPhiMarker& p1,
+                          const LSEVisitor::Value::NeedsPlainLoopPhiMarker& p2) {
   return p1.phi_ == p2.phi_;
 }
 
+constexpr bool operator==(const LSEVisitor::Value::NeedsConvertedLoopPhiMarker& p1,
+                          const LSEVisitor::Value::NeedsConvertedLoopPhiMarker& p2) {
+  return p1.load_ == p2.load_;
+}
+
 constexpr bool operator==(const LSEVisitor::Value::NeedsNonLoopPhiMarker& p1,
                           const LSEVisitor::Value::NeedsNonLoopPhiMarker& p2) {
   return p1.phi_ == p2.phi_;
@@ -1251,9 +1412,12 @@ std::ostream& LSEVisitor::Value::Dump(std::ostream& os) const {
   } else if (IsInstruction()) {
     return os << "Instruction[id: " << GetInstruction()->GetId()
               << ", block: " << GetInstruction()->GetBlock()->GetBlockId() << "]";
-  } else if (NeedsLoopPhi()) {
-    return os << "NeedsLoopPhi[block: " << GetPhiPlaceholder().GetBlockId()
+  } else if (NeedsPlainLoopPhi()) {
+    return os << "NeedsPlainLoopPhi[block: " << GetPhiPlaceholder().GetBlockId()
               << ", heap_loc: " << GetPhiPlaceholder().GetHeapLocation() << "]";
+  } else if (NeedsConvertedLoopPhi()) {
+    return os << "NeedsConvertedLoopPhi[id: " << GetLoopPhiConversionLoad()->GetId()
+              << ", block: " << GetLoopPhiConversionLoad()->GetBlock()->GetBlockId() << "]";
   } else {
     return os << "NeedsNonLoopPhi[block: " << GetPhiPlaceholder().GetBlockId()
               << ", heap_loc: " << GetPhiPlaceholder().GetHeapLocation() << "]";
@@ -1326,7 +1490,7 @@ LSEVisitor::Value LSEVisitor::PrepareLoopValue(HBasicBlock* block, size_t idx) {
     }
   }
   PhiPlaceholder phi_placeholder = GetPhiPlaceholder(block->GetBlockId(), idx);
-  return ReplacementOrValue(Value::ForLoopPhiPlaceholder(phi_placeholder));
+  return ReplacementOrValue(Value::ForPlainLoopPhiPlaceholder(phi_placeholder));
 }
 
 LSEVisitor::Value LSEVisitor::PrepareLoopStoredBy(HBasicBlock* block, size_t idx) {
@@ -1344,7 +1508,7 @@ LSEVisitor::Value LSEVisitor::PrepareLoopStoredBy(HBasicBlock* block, size_t idx
     return Value::Unknown();
   }
   PhiPlaceholder phi_placeholder = GetPhiPlaceholder(block->GetBlockId(), idx);
-  return Value::ForLoopPhiPlaceholder(phi_placeholder);
+  return Value::ForPlainLoopPhiPlaceholder(phi_placeholder);
 }
 
 void LSEVisitor::PrepareLoopRecords(HBasicBlock* block) {
@@ -1364,7 +1528,7 @@ void LSEVisitor::PrepareLoopRecords(HBasicBlock* block) {
                        {/*value=*/Value::Unknown(), /*stored_by=*/Value::Unknown()});
     // Also keep the stores before the loop header, including in blocks that were not visited yet.
     for (size_t idx = 0u; idx != num_heap_locations; ++idx) {
-      KeepStores(Value::ForLoopPhiPlaceholder(GetPhiPlaceholder(block->GetBlockId(), idx)));
+      KeepStores(Value::ForPlainLoopPhiPlaceholder(GetPhiPlaceholder(block->GetBlockId(), idx)));
     }
     return;
   }
@@ -1541,12 +1705,12 @@ void LSEVisitor::MaterializeNonLoopPhis(PhiPlaceholder phi_placeholder, DataType
 
 void LSEVisitor::VisitGetLocation(HInstruction* instruction, size_t idx) {
   DCHECK_NE(idx, HeapLocationCollector::kHeapLocationNotFound);
+  DCHECK_EQ(DataType::Size(heap_location_collector_.GetHeapLocation(idx)->GetType()),
+            DataType::Size(instruction->IsVecLoad() ? instruction->AsVecLoad()->GetPackedType()
+                                                    : instruction->GetType()));
   uint32_t block_id = instruction->GetBlock()->GetBlockId();
   ScopedArenaVector<ValueRecord>& heap_values = heap_values_for_[block_id];
   ValueRecord& record = heap_values[idx];
-  if (instruction->IsFieldAccess()) {
-    RecordFieldInfo(&instruction->GetFieldInfo(), idx);
-  }
   DCHECK(record.value.IsUnknown() || record.value.Equals(ReplacementOrValue(record.value)));
   loads_and_stores_.push_back({ instruction, idx });
   if ((record.value.IsDefault() || record.value.NeedsNonLoopPhi()) &&
@@ -1587,10 +1751,7 @@ void LSEVisitor::VisitGetLocation(HInstruction* instruction, size_t idx) {
 void LSEVisitor::VisitSetLocation(HInstruction* instruction, size_t idx, HInstruction* value) {
   DCHECK_NE(idx, HeapLocationCollector::kHeapLocationNotFound);
   DCHECK(!IsStore(value)) << value->DebugName();
-  if (instruction->IsFieldAccess()) {
-    RecordFieldInfo(&instruction->GetFieldInfo(), idx);
-  }
-  // value may already have a substitute.
+  // The `value` may already have a substitute.
   value = FindSubstitute(value);
   HBasicBlock* block = instruction->GetBlock();
   ScopedArenaVector<ValueRecord>& heap_values = heap_values_for_[block->GetBlockId()];
@@ -1598,6 +1759,19 @@ void LSEVisitor::VisitSetLocation(HInstruction* instruction, size_t idx, HInstru
   DCHECK_IMPLIES(record.value.IsInstruction(),
                  FindSubstitute(record.value.GetInstruction()) == record.value.GetInstruction());
 
+  // Calculate the new `Value` to store to the `record`.
+  Value new_value = Value::ForInstruction(value);
+  // Note that the `value` can be a newly created `Phi` with an id that falls outside
+  // the allocated `loads_requiring_loop_phi_` range.
+  DCHECK_IMPLIES(IsLoad(value) && !loads_requiring_loop_phi_.empty(),
+                 static_cast<size_t>(value->GetId()) < loads_requiring_loop_phi_.size());
+  if (static_cast<size_t>(value->GetId()) < loads_requiring_loop_phi_.size() &&
+      loads_requiring_loop_phi_[value->GetId()] != nullptr) {
+    // Propapate the Phi placeholder or appropriate converting load to the record.
+    new_value = StoredValueForLoopPhiPlaceholderDependentLoad(idx, value);
+    DCHECK(new_value.NeedsLoopPhi());
+  }
+
   if (record.value.Equals(value)) {
     // Store into the heap location with the same value.
     // This store can be eliminated right away.
@@ -1630,18 +1804,7 @@ void LSEVisitor::VisitSetLocation(HInstruction* instruction, size_t idx, HInstru
   }
 
   // Update the record.
-  // Note that the `value` can be a newly created `Phi` with an id that falls outside
-  // the allocated `loads_requiring_loop_phi_` range.
-  DCHECK_IMPLIES(IsLoad(value) && !loads_requiring_loop_phi_.empty(),
-                 static_cast<size_t>(value->GetId()) < loads_requiring_loop_phi_.size());
-  if (static_cast<size_t>(value->GetId()) < loads_requiring_loop_phi_.size() &&
-      loads_requiring_loop_phi_[value->GetId()] != nullptr) {
-    // Propapate the Phi placeholder to the record.
-    record.value = loads_requiring_loop_phi_[value->GetId()]->value;
-    DCHECK(record.value.NeedsLoopPhi());
-  } else {
-    record.value = Value::ForInstruction(value);
-  }
+  record.value = new_value;
   // Track the store in the value record. If the value is loaded or needed after
   // return/deoptimization later, this store isn't really redundant.
   record.stored_by = Value::ForInstruction(instruction);
@@ -1713,6 +1876,16 @@ bool LSEVisitor::TryReplacingLoopPhiPlaceholderWithDefault(
                          kArenaAllocLSE);
   ScopedArenaVector<PhiPlaceholder> work_queue(allocator.Adapter(kArenaAllocLSE));
 
+  auto maybe_add_to_work_queue = [&](Value predecessor_value) {
+    // Visit the predecessor Phi placeholder if it's not visited yet.
+    DCHECK(predecessor_value.NeedsNonLoopPhi() || predecessor_value.NeedsPlainLoopPhi());
+    PhiPlaceholder predecessor_phi_placeholder = predecessor_value.GetPhiPlaceholder();
+    if (!visited.IsBitSet(PhiPlaceholderIndex(predecessor_phi_placeholder))) {
+      visited.SetBit(PhiPlaceholderIndex(predecessor_phi_placeholder));
+      work_queue.push_back(predecessor_phi_placeholder);
+    }
+  };
+
   // Use depth first search to check if any non-Phi input is unknown.
   const ArenaVector<HBasicBlock*>& blocks = GetGraph()->GetBlocks();
   size_t num_heap_locations = heap_location_collector_.GetNumberOfHeapLocations();
@@ -1726,12 +1899,10 @@ bool LSEVisitor::TryReplacingLoopPhiPlaceholderWithDefault(
     size_t idx = current_phi_placeholder.GetHeapLocation();
     for (HBasicBlock* predecessor : block->GetPredecessors()) {
       Value value = ReplacementOrValue(heap_values_for_[predecessor->GetBlockId()][idx].value);
+      // Skip over type conversions (these are unnecessary for the default value).
+      value = SkipTypeConversions(value);
       if (value.NeedsPhi()) {
-        // Visit the predecessor Phi placeholder if it's not visited yet.
-        if (!visited.IsBitSet(PhiPlaceholderIndex(value))) {
-          visited.SetBit(PhiPlaceholderIndex(value));
-          work_queue.push_back(value.GetPhiPlaceholder());
-        }
+        maybe_add_to_work_queue(value);
       } else if (!value.Equals(Value::Default())) {
         return false;  // Report failure.
       }
@@ -1754,19 +1925,18 @@ bool LSEVisitor::TryReplacingLoopPhiPlaceholderWithDefault(
           // allow replacing the non-vector load of loop-invariant default values
           // anyway, skip over paths that do not have any writes.
           ValueRecord record = heap_values_for_[predecessor->GetBlockId()][i];
-          while (record.stored_by.NeedsLoopPhi() &&
+          while (record.stored_by.NeedsPlainLoopPhi() &&
                  blocks[record.stored_by.GetPhiPlaceholder().GetBlockId()]->IsLoopHeader()) {
             HLoopInformation* loop_info =
                 blocks[record.stored_by.GetPhiPlaceholder().GetBlockId()]->GetLoopInformation();
             record = heap_values_for_[loop_info->GetPreHeader()->GetBlockId()][i];
           }
+          DCHECK(!record.stored_by.NeedsConvertedLoopPhi());
           Value value = ReplacementOrValue(record.value);
+          // Skip over type conversions (these are unnecessary for the default value).
+          value = SkipTypeConversions(value);
           if (value.NeedsPhi()) {
-            // Visit the predecessor Phi placeholder if it's not visited yet.
-            if (!visited.IsBitSet(PhiPlaceholderIndex(value))) {
-              visited.SetBit(PhiPlaceholderIndex(value));
-              work_queue.push_back(value.GetPhiPlaceholder());
-            }
+            maybe_add_to_work_queue(value);
           } else if (!value.Equals(Value::Default())) {
             return false;  // Report failure.
           }
@@ -1802,6 +1972,8 @@ bool LSEVisitor::TryReplacingLoopPhiPlaceholderWithSingleInput(
                          kArenaAllocLSE);
   ScopedArenaVector<PhiPlaceholder> work_queue(allocator.Adapter(kArenaAllocLSE));
 
+  TypeConversionSet type_conversions;
+
   // Use depth first search to check if any non-Phi input is unknown.
   HInstruction* replacement = nullptr;
   const ArenaVector<HBasicBlock*>& blocks = GetGraph()->GetBlocks();
@@ -1815,6 +1987,8 @@ bool LSEVisitor::TryReplacingLoopPhiPlaceholderWithSingleInput(
     size_t idx = current_phi_placeholder.GetHeapLocation();
     for (HBasicBlock* predecessor : current_block->GetPredecessors()) {
       Value value = ReplacementOrValue(heap_values_for_[predecessor->GetBlockId()][idx].value);
+      // Skip type conversions but record them for checking later.
+      value = SkipTypeConversions(value, &type_conversions);
       if (value.NeedsPhi()) {
         // Visit the predecessor Phi placeholder if it's not visited yet.
         if (!visited.IsBitSet(PhiPlaceholderIndex(value))) {
@@ -1837,8 +2011,16 @@ bool LSEVisitor::TryReplacingLoopPhiPlaceholderWithSingleInput(
     // would invalidate the heap location in `VisitSetLocation()`.
   }
 
-  // Record replacement and report success.
+  // Check that there are no type conversions that would change the stored value.
   DCHECK(replacement != nullptr);
+  if (!type_conversions.AreAllTypeConversionsImplicit(replacement)) {
+    return false;
+  }
+
+  // Record replacement and report success.
+  // Note: Replacements for the loads where we skipped type conversions above (and do not really
+  // need the type conversion) shall be recorded later, either when we process the loads in
+  // `ProcessLoadsRequiringLoopPhis()` or when needed to materialize another Phi.
   for (uint32_t phi_placeholder_index : visited.Indexes()) {
     DCHECK(phi_placeholder_replacements_[phi_placeholder_index].IsInvalid());
     PhiPlaceholder curr = GetPhiPlaceholderAt(phi_placeholder_index);
@@ -1932,7 +2114,9 @@ std::optional<LSEVisitor::PhiPlaceholder> LSEVisitor::FindLoopPhisToMaterialize(
           }
         }
       }
-      if (value.NeedsLoopPhi()) {
+      // Skip type conversions. We're looking for the Phi placeholders now.
+      value = SkipTypeConversions(value);
+      if (value.NeedsPlainLoopPhi()) {
         // Visit the predecessor Phi placeholder if it's not visited yet.
         if (!phi_placeholders_to_materialize->IsBitSet(PhiPlaceholderIndex(value))) {
           phi_placeholders_to_materialize->SetBit(PhiPlaceholderIndex(value));
@@ -1948,6 +2132,41 @@ std::optional<LSEVisitor::PhiPlaceholder> LSEVisitor::FindLoopPhisToMaterialize(
   return std::nullopt;
 }
 
+void LSEVisitor::MaterializeTypeConversionsIfNeeded(Value value) {
+  if (!value.NeedsConvertedLoopPhi()) {
+    return;
+  }
+  // There are at most 2 conversions (Uint8+Int16 or Int8+Uint16). Conversion to Int32
+  // is implicit and conversions to same or smaller size replace previous conversions.
+  static constexpr size_t kMaxConversionLoads = 2u;
+  HInstruction* conversion_loads[kMaxConversionLoads];
+  size_t num_conversion_loads = 0u;
+  do {
+    DCHECK_LT(num_conversion_loads, kMaxConversionLoads);
+    HInstruction* conversion_load = value.GetLoopPhiConversionLoad();
+    DCHECK(!conversion_load->IsVecLoad());
+    HInstruction* substitute = FindSubstitute(conversion_load);
+    if (substitute != conversion_load) {
+      value = Value::ForInstruction(substitute);
+      break;
+    }
+    conversion_loads[num_conversion_loads] = conversion_load;
+    ++num_conversion_loads;
+    ValueRecord* prev_record = loads_requiring_loop_phi_[conversion_load->GetId()];
+    DCHECK(prev_record != nullptr);
+    value = prev_record->value;
+  } while (value.NeedsConvertedLoopPhi());
+  value = value.NeedsPlainLoopPhi() ? Replacement(value) : value;
+  HInstruction* replacement = value.GetInstruction();
+  ArrayRef<HInstruction*> conversion_loads_array(conversion_loads, num_conversion_loads);
+  for (HInstruction* conversion_load : ReverseRange(conversion_loads_array)) {
+    AddRemovedLoad(conversion_load, replacement);
+    replacement = substitute_instructions_for_loads_[conversion_load->GetId()];
+    DCHECK(replacement != nullptr);
+    DCHECK(replacement->IsTypeConversion());
+  }
+}
+
 bool LSEVisitor::MaterializeLoopPhis(const ScopedArenaVector<size_t>& phi_placeholder_indexes,
                                      DataType::Type type) {
   return MaterializeLoopPhis(ArrayRef<const size_t>(phi_placeholder_indexes), type);
@@ -1958,6 +2177,7 @@ bool LSEVisitor::MaterializeLoopPhis(ArrayRef<const size_t> phi_placeholder_inde
   // Materialize all predecessors that do not need a loop Phi and determine if all inputs
   // other than loop Phis are the same.
   const ArenaVector<HBasicBlock*>& blocks = GetGraph()->GetBlocks();
+  TypeConversionSet type_conversions;
   std::optional<Value> other_value = std::nullopt;
   for (size_t phi_placeholder_index : phi_placeholder_indexes) {
     PhiPlaceholder phi_placeholder = GetPhiPlaceholderAt(phi_placeholder_index);
@@ -1970,8 +2190,19 @@ bool LSEVisitor::MaterializeLoopPhis(ArrayRef<const size_t> phi_placeholder_inde
         DCHECK(current_phase_ == Phase::kLoadElimination) << current_phase_;
         MaterializeNonLoopPhis(value.GetPhiPlaceholder(), type);
         value = Replacement(value);
+      } else if (value.NeedsConvertedLoopPhi()) {
+        TypeConversionSet local_type_conversions;
+        Value without_conversions = SkipTypeConversions(value, &local_type_conversions);
+        DCHECK(!without_conversions.NeedsNonLoopPhi());  // Would have been already materialized.
+        if (without_conversions.NeedsPlainLoopPhi()) {
+          type_conversions.Add(local_type_conversions);
+          value = without_conversions;
+        } else {
+          MaterializeTypeConversionsIfNeeded(value);
+          value = ReplacementOrValue(value);
+        }
       }
-      if (!value.NeedsLoopPhi()) {
+      if (!value.NeedsPlainLoopPhi()) {
         if (!other_value) {
           // The first other value we found.
           other_value = value;
@@ -1986,9 +2217,13 @@ bool LSEVisitor::MaterializeLoopPhis(ArrayRef<const size_t> phi_placeholder_inde
   }
 
   DCHECK(other_value.has_value());
-  if (!other_value->IsInvalid()) {
+  DCHECK(other_value->IsInvalid() || other_value->IsDefault() || other_value->IsInstruction());
+  if (other_value->IsDefault() ||  // Default value does not need type conversions.
+      (other_value->IsInstruction() &&
+          type_conversions.AreAllTypeConversionsImplicit(other_value->GetInstruction()))) {
     HInstruction* replacement =
         (other_value->IsDefault()) ? GetDefaultValue(type) : other_value->GetInstruction();
+    DCHECK(type_conversions.AreAllTypeConversionsImplicit(replacement));
     for (size_t phi_placeholder_index : phi_placeholder_indexes) {
       phi_placeholder_replacements_[phi_placeholder_index] = Value::ForInstruction(replacement);
     }
@@ -2010,12 +2245,24 @@ bool LSEVisitor::MaterializeLoopPhis(ArrayRef<const size_t> phi_placeholder_inde
       ArrayRef<HUserRecord<HInstruction*>> phi_inputs = phi->GetInputRecords();
       auto cmp = [=, this](const HUserRecord<HInstruction*>& lhs, HBasicBlock* rhs) {
         Value value = ReplacementOrValue(heap_values_for_[rhs->GetBlockId()][idx].value);
-        if (value.NeedsPhi()) {
-          DCHECK(value.GetPhiPlaceholder() == phi_placeholder);
-          return lhs.GetInstruction() == phi;
+        HInstruction* lhs_instruction = lhs.GetInstruction();
+        while (value.NeedsConvertedLoopPhi()) {
+          HInstruction* conversion_load = value.GetLoopPhiConversionLoad();
+          if (!lhs_instruction->IsTypeConversion() ||
+              lhs_instruction->GetType() != conversion_load->GetType()) {
+            return false;
+          }
+          lhs_instruction = lhs_instruction->InputAt(0);
+          ValueRecord* prev_record = loads_requiring_loop_phi_[conversion_load->GetId()];
+          DCHECK(prev_record != nullptr);
+          value = prev_record->value;
+        }
+        if (value.NeedsPlainLoopPhi() && value.GetPhiPlaceholder().Equals(phi_placeholder)) {
+          return lhs_instruction == phi;
         } else {
+          value = ReplacementOrValue(value);
           DCHECK(value.IsDefault() || value.IsInstruction());
-          return value.Equals(lhs.GetInstruction());
+          return value.Equals(lhs_instruction);
         }
       };
       if (std::equal(phi_inputs.begin(), phi_inputs.end(), predecessors.begin(), cmp)) {
@@ -2049,7 +2296,9 @@ bool LSEVisitor::MaterializeLoopPhis(ArrayRef<const size_t> phi_placeholder_inde
         << "type=" << type << " vs phi-type=" << phi->GetType();
     for (size_t i = 0, size = block->GetPredecessors().size(); i != size; ++i) {
       HBasicBlock* predecessor = block->GetPredecessors()[i];
-      Value value = ReplacementOrValue(heap_values_for_[predecessor->GetBlockId()][idx].value);
+      Value predecessor_value = heap_values_for_[predecessor->GetBlockId()][idx].value;
+      MaterializeTypeConversionsIfNeeded(predecessor_value);
+      Value value = ReplacementOrValue(predecessor_value);
       HInstruction* input = value.IsDefault() ? GetDefaultValue(type) : value.GetInstruction();
       DCHECK_NE(input->GetType(), DataType::Type::kVoid);
       phi->SetRawInputAt(i, input);
@@ -2319,9 +2568,10 @@ void LSEVisitor::ProcessLoopPhiWithUnknownInput(PhiPlaceholder loop_phi_with_unk
         // at the end of the block.
         Value replacement = ReplacementOrValue(record->value);
         if (replacement.NeedsLoopPhi()) {
-          // No replacement yet, use the Phi placeholder from the load.
+          // No replacement yet. Use the Phi placeholder or an appropriate converting load.
           DCHECK(record->value.NeedsLoopPhi());
-          local_heap_values[idx] = record->value;
+          local_heap_values[idx] = StoredValueForLoopPhiPlaceholderDependentLoad(idx, stored_value);
+          DCHECK(local_heap_values[idx].NeedsLoopPhi());
         } else {
           // If the load fetched a known value, use it, otherwise use the load.
           local_heap_values[idx] = Value::ForInstruction(
@@ -2401,12 +2651,17 @@ void LSEVisitor::ProcessLoadsRequiringLoopPhis() {
       continue;
     }
     HInstruction* load = load_store_record.load_or_store;
-    while (record->value.NeedsLoopPhi() &&
-           phi_placeholder_replacements_[PhiPlaceholderIndex(record->value)].IsInvalid()) {
+    while (record->value.NeedsLoopPhi()) {
+      Value without_conversions = SkipTypeConversions(record->value);
+      if (!without_conversions.NeedsPlainLoopPhi() ||
+          phi_placeholder_replacements_[PhiPlaceholderIndex(without_conversions)].IsValid()) {
+        break;
+      }
       std::optional<PhiPlaceholder> loop_phi_with_unknown_input =
-          TryToMaterializeLoopPhis(record->value.GetPhiPlaceholder(), load);
-      DCHECK_EQ(loop_phi_with_unknown_input.has_value(),
-                phi_placeholder_replacements_[PhiPlaceholderIndex(record->value)].IsInvalid());
+          TryToMaterializeLoopPhis(without_conversions.GetPhiPlaceholder(), load);
+      DCHECK_EQ(
+          loop_phi_with_unknown_input.has_value(),
+          phi_placeholder_replacements_[PhiPlaceholderIndex(without_conversions)].IsInvalid());
       if (loop_phi_with_unknown_input) {
         DCHECK_GE(GetGraph()
                       ->GetBlocks()[loop_phi_with_unknown_input->GetBlockId()]
@@ -2416,15 +2671,25 @@ void LSEVisitor::ProcessLoadsRequiringLoopPhis() {
         ProcessLoopPhiWithUnknownInput(*loop_phi_with_unknown_input);
       }
     }
-    // The load could have been marked as unreplaceable (and stores marked for keeping)
-    // or marked for replacement with an instruction in ProcessLoopPhiWithUnknownInput().
+    // The load, or converting load's underlying phi placeholder, could have been marked
+    // as unreplaceable (and stores marked for keeping) or marked for replacement with an
+    // instruction in `ProcessLoopPhiWithUnknownInput()`.
     DCHECK(record->value.IsUnknown() ||
            record->value.IsInstruction() ||
            record->value.NeedsLoopPhi());
     if (record->value.NeedsLoopPhi()) {
-      record->value = Replacement(record->value);
+      MaterializeTypeConversionsIfNeeded(record->value);
+      record->value = ReplacementOrValue(record->value);
       HInstruction* heap_value = record->value.GetInstruction();
-      AddRemovedLoad(load, heap_value);
+      // Type conversion substitutes can be created by `MaterializeTypeConversionsIfNeeded()`,
+      // either in the call directly above, or while materializing Phis. For all loads that did
+      // not have a substitute recorded, record it now; this can also be a type conversion.
+      HInstruction* substitute = FindSubstitute(load);
+      if (substitute == load) {
+        AddRemovedLoad(load, heap_value);
+      } else {
+        DCHECK(substitute->IsTypeConversion());
+      }
     }
   }
 }
@@ -2495,7 +2760,8 @@ void LSEVisitor::UpdateValueRecordForStoreElimination(/*inout*/ValueRecord* valu
   if (value_record->value.NeedsNonLoopPhi()) {
     // Treat all Phi placeholders as requiring loop Phis at this point.
     // We do not want MaterializeLoopPhis() to call MaterializeNonLoopPhis().
-    value_record->value = Value::ForLoopPhiPlaceholder(value_record->value.GetPhiPlaceholder());
+    value_record->value =
+        Value::ForPlainLoopPhiPlaceholder(value_record->value.GetPhiPlaceholder());
   }
 }
 
diff --git a/compiler/optimizing/load_store_elimination_test.cc b/compiler/optimizing/load_store_elimination_test.cc
index 1e5c7082a4..af039229af 100644
--- a/compiler/optimizing/load_store_elimination_test.cc
+++ b/compiler/optimizing/load_store_elimination_test.cc
@@ -123,7 +123,7 @@ class LoadStoreEliminationTestBase : public SuperTest, public OptimizingUnitTest
   // Return the pre-header and loop block.
   std::tuple<HBasicBlock*, HBasicBlock*> CreateDoWhileLoopWithInstructions(
       HBasicBlock* loop_exit, std::initializer_list<HInstruction*> suspend_check_env = {}) {
-    auto [pre_header, loop] = CreateDoWhileLoop(loop_exit);
+    auto [pre_header, loop, back_edge] = CreateWhileLoop(loop_exit);
     MakeSimpleLoopInstructions(loop, loop, suspend_check_env);
     return {pre_header, loop};
   }
@@ -1525,18 +1525,19 @@ TEST_P(TwoTypesConversionsTestGroup, StoreLoopLoadStoreLoad) {
   EXPECT_INS_REMOVED(read1);
   EXPECT_INS_REMOVED(read2);
 
-  if (load_type1 != DataType::Type::kInt32 && load_type2 != load_type1) {
-    GTEST_SKIP() << "FIXME: Missing type conversions. Bug: 341476044";
-  }
-  // Note: Sometimes we create two type conversions when one is enough (Int32->Int16->Int8).
-  // We currently rely on the instruction simplifier to remove the intermediate conversion.
+  // Note: If the `load_type2` is not larger than the `load_type1`, we avoid
+  // the intermediate conversion and use `param` directly for the second load.
+  DataType::Type read2_input_type = DataType::Size(load_type2) <= DataType::Size(load_type1)
+      ? DataType::Type::kInt32
+      : load_type1;
   HInstruction* current = ret->InputAt(0);
-  if (!DataType::IsTypeConversionImplicit(load_type1, load_type2)) {
+  if (!DataType::IsTypeConversionImplicit(read2_input_type, load_type2)) {
     ASSERT_TRUE(current->IsTypeConversion()) << current->DebugName();
     ASSERT_EQ(load_type2, current->GetType());
     current = current->InputAt(0);
   }
-  if (!DataType::IsTypeConversionImplicit(DataType::Type::kInt32, load_type1)) {
+  if (!DataType::IsTypeConversionImplicit(DataType::Type::kInt32, read2_input_type)) {
+    ASSERT_EQ(read2_input_type, load_type1);
     ASSERT_TRUE(current->IsTypeConversion()) << current->DebugName();
     ASSERT_EQ(load_type1, current->GetType()) << load_type2;
     current = current->InputAt(0);
@@ -1589,7 +1590,6 @@ TEST_P(TwoTypesConversionsTestGroup, MergingConvertedValueStore) {
     EXPECT_INS_REMOVED(phi_write) << "\n" << param_type << "/" << load_type;
     ASSERT_EQ(param, ret_input) << ret_input->DebugName();
   } else {
-    GTEST_SKIP() << "FIXME: Missing type conversions. Bug: 341476044";
     EXPECT_INS_RETAINED(phi_write) << "\n" << param_type << "/" << load_type;
     ASSERT_TRUE(ret_input->IsPhi()) << ret_input->DebugName();
     HInstruction* pre_header_input = ret_input->InputAt(0);
@@ -1654,22 +1654,25 @@ TEST_P(TwoTypesConversionsTestGroup, MergingTwiceConvertedValueStore) {
     EXPECT_INS_REMOVED(phi_write) << "\n" << load_type1 << "/" << load_type2;
     ASSERT_EQ(param, ret_input) << ret_input->DebugName();
   } else {
-    GTEST_SKIP() << "FIXME: Missing type conversions. Bug: 341476044";
     EXPECT_INS_RETAINED(phi_write) << "\n" << load_type1 << "/" << load_type2;
     ASSERT_TRUE(ret_input->IsPhi()) << ret_input->DebugName();
     HInstruction* pre_header_input = ret_input->InputAt(0);
     HInstruction* loop_body_input = ret_input->InputAt(1);
     ASSERT_EQ(param, pre_header_input) << pre_header_input->DebugName();
     ASSERT_TRUE(loop_body_input->IsTypeConversion());
-    // Note: Sometimes we create two type conversions when one is enough (Int32->Int16->Int8).
-    // We currently rely on the instruction simplifier to remove the intermediate conversion.
     HInstruction* current = loop_body_input;
-    if (!DataType::IsTypeConversionImplicit(load_type1, load_type2)) {
+    // Note: If the `load_type2` is not larger than the `load_type1`, we avoid
+    // the intermediate conversion and use Phi directly for the second load.
+    DataType::Type read2_input_type = DataType::Size(load_type2) <= DataType::Size(load_type1)
+        ? DataType::Type::kInt32
+        : load_type1;
+    if (!DataType::IsTypeConversionImplicit(read2_input_type, load_type2)) {
       ASSERT_TRUE(current->IsTypeConversion()) << current->DebugName();
       ASSERT_EQ(load_type2, current->GetType());
       current = current->InputAt(0);
     }
-    if (!DataType::IsTypeConversionImplicit(DataType::Type::kInt32, load_type1)) {
+    if (!DataType::IsTypeConversionImplicit(DataType::Type::kInt32, read2_input_type)) {
+      ASSERT_EQ(read2_input_type, load_type1);
       ASSERT_TRUE(current->IsTypeConversion()) << current->DebugName();
       ASSERT_EQ(load_type1, current->GetType()) << load_type2;
       current = current->InputAt(0);
@@ -1678,18 +1681,115 @@ TEST_P(TwoTypesConversionsTestGroup, MergingTwiceConvertedValueStore) {
   }
 }
 
+TEST_P(TwoTypesConversionsTestGroup, MergingConvertedValueStorePhiDeduplication) {
+  auto [load_type1, load_type2] = GetParam();
+  DataType::Type field_type1 = FieldTypeForLoadType(load_type1);
+  DataType::Type field_type2 = FieldTypeForLoadType(load_type2);
+  DataType::Type phi_field_type = DataType::Type::kInt32;  // "phi field" can hold the full value.
+  CHECK(DataType::IsTypeConversionImplicit(load_type1, phi_field_type));
+  CHECK(DataType::IsTypeConversionImplicit(load_type2, phi_field_type));
+  DataType::Type param_type = DataType::Type::kInt32;
+
+  HBasicBlock* return_block = InitEntryMainExitGraph();
+  auto [pre_header, loop_header, loop_body] = CreateForLoopWithInstructions(return_block);
+
+  HInstruction* param = MakeParam(param_type);
+  HInstruction* object = MakeParam(DataType::Type::kReference);
+
+  // Initialize the two "phi fields".
+  HInstruction* pre_header_write1 =
+      MakeIFieldSet(pre_header, object, param, phi_field_type, MemberOffset(40));
+  HInstruction* pre_header_write2 =
+      MakeIFieldSet(pre_header, object, param, phi_field_type, MemberOffset(60));
+
+  // In the body, we read the "phi fields", store and load the values to different fields
+  // to force type conversion, and store back to the "phi fields".
+  HInstanceFieldGet* phi_read1 = MakeIFieldGet(loop_body, object, phi_field_type, MemberOffset(40));
+  HInstanceFieldGet* phi_read2 = MakeIFieldGet(loop_body, object, phi_field_type, MemberOffset(60));
+  HInstruction* conversion_write1 =
+      MakeIFieldSet(loop_body, object, phi_read1, field_type1, MemberOffset(32));
+  HInstruction* conversion_write2 =
+      MakeIFieldSet(loop_body, object, phi_read2, field_type2, MemberOffset(52));
+  HInstanceFieldGet* conversion_read1 =
+      MakeIFieldGet(loop_body, object, field_type1, MemberOffset(32));
+  conversion_read1->SetType(load_type1);
+  HInstanceFieldGet* conversion_read2 =
+      MakeIFieldGet(loop_body, object, field_type2, MemberOffset(52));
+  conversion_read2->SetType(load_type2);
+  HInstruction* phi_write1 =
+      MakeIFieldSet(loop_body, object, conversion_read1, phi_field_type, MemberOffset(40));
+  HInstruction* phi_write2 =
+      MakeIFieldSet(loop_body, object, conversion_read2, phi_field_type, MemberOffset(60));
+
+  HInstanceFieldGet* final_read1 =
+      MakeIFieldGet(return_block, object, phi_field_type, MemberOffset(40));
+  HInstanceFieldGet* final_read2 =
+      MakeIFieldGet(return_block, object, phi_field_type, MemberOffset(60));
+  HAdd* add = MakeBinOp<HAdd>(return_block, DataType::Type::kInt32, final_read1, final_read2);
+  HInstruction* ret = MakeReturn(return_block, add);
+
+  PerformLSE();
+
+  EXPECT_INS_RETAINED(pre_header_write1);
+  EXPECT_INS_RETAINED(pre_header_write2);
+  EXPECT_INS_RETAINED(conversion_write1);
+  EXPECT_INS_RETAINED(conversion_write2);
+  EXPECT_INS_REMOVED(phi_read1);
+  EXPECT_INS_REMOVED(phi_read2);
+  EXPECT_INS_REMOVED(conversion_read1);
+  EXPECT_INS_REMOVED(conversion_read2);
+  EXPECT_INS_REMOVED(final_read1);
+  EXPECT_INS_REMOVED(final_read2);
+
+  HInstruction* ret_input = ret->InputAt(0);
+  ASSERT_EQ(add, ret_input) << ret_input->DebugName();
+  HInstruction* add_lhs = add->InputAt(0);
+  HInstruction* add_rhs = add->InputAt(1);
+  if (load_type1 == load_type2) {
+    ASSERT_EQ(add_lhs, add_rhs);
+  } else {
+    ASSERT_NE(add_lhs, add_rhs);
+  }
+  if (DataType::IsTypeConversionImplicit(param_type, load_type1)) {
+    EXPECT_INS_REMOVED(phi_write1) << "\n" << load_type1 << "/" << load_type2;
+    ASSERT_EQ(param, add_lhs) << ret_input->DebugName();
+  } else {
+    EXPECT_INS_RETAINED(phi_write1) << "\n" << load_type1 << "/" << load_type2;
+    ASSERT_TRUE(add_lhs->IsPhi()) << add_lhs->DebugName();
+    HInstruction* pre_header_input = add_lhs->InputAt(0);
+    HInstruction* loop_body_input = add_lhs->InputAt(1);
+    ASSERT_EQ(param, pre_header_input) << pre_header_input->DebugName();
+    ASSERT_TRUE(loop_body_input->IsTypeConversion());
+    ASSERT_EQ(load_type1, loop_body_input->GetType());
+    ASSERT_EQ(add_lhs, loop_body_input->InputAt(0));
+  }
+  if (DataType::IsTypeConversionImplicit(param_type, load_type2)) {
+    EXPECT_INS_REMOVED(phi_write2) << "\n" << load_type1 << "/" << load_type2;
+    ASSERT_EQ(param, add_rhs) << ret_input->DebugName();
+  } else {
+    EXPECT_INS_RETAINED(phi_write2) << "\n" << load_type1 << "/" << load_type2;
+    ASSERT_TRUE(add_rhs->IsPhi()) << add_rhs->DebugName();
+    HInstruction* pre_header_input = add_rhs->InputAt(0);
+    HInstruction* loop_body_input = add_rhs->InputAt(1);
+    ASSERT_EQ(param, pre_header_input) << pre_header_input->DebugName();
+    ASSERT_TRUE(loop_body_input->IsTypeConversion());
+    ASSERT_EQ(load_type2, loop_body_input->GetType());
+    ASSERT_EQ(add_rhs, loop_body_input->InputAt(0));
+  }
+}
+
 auto Int32AndSmallerTypesGenerator() {
-  return testing::Values(DataType::Type::kInt32,
-                         DataType::Type::kInt16,
-                         DataType::Type::kInt8,
-                         DataType::Type::kUint16,
-                         DataType::Type::kUint8);
+  return ::testing::Values(DataType::Type::kInt32,
+                           DataType::Type::kInt16,
+                           DataType::Type::kInt8,
+                           DataType::Type::kUint16,
+                           DataType::Type::kUint8);
 }
 
-INSTANTIATE_TEST_SUITE_P(
-    LoadStoreEliminationTest,
-    TwoTypesConversionsTestGroup,
-    testing::Combine(Int32AndSmallerTypesGenerator(), Int32AndSmallerTypesGenerator()));
+INSTANTIATE_TEST_SUITE_P(LoadStoreEliminationTest,
+                         TwoTypesConversionsTestGroup,
+                         ::testing::Combine(Int32AndSmallerTypesGenerator(),
+                                            Int32AndSmallerTypesGenerator()));
 
 // // ENTRY
 // obj = new Obj();
diff --git a/compiler/optimizing/locations.cc b/compiler/optimizing/locations.cc
index 4189bc4053..f419263f62 100644
--- a/compiler/optimizing/locations.cc
+++ b/compiler/optimizing/locations.cc
@@ -26,17 +26,24 @@ namespace art HIDDEN {
 // Verify that Location is trivially copyable.
 static_assert(std::is_trivially_copyable<Location>::value, "Location should be trivially copyable");
 
+static inline ArrayRef<Location> AllocateInputLocations(HInstruction* instruction,
+                                                        ArenaAllocator* allocator) {
+  size_t input_count = instruction->InputCount();
+  Location* array = allocator->AllocArray<Location>(input_count, kArenaAllocLocationSummary);
+  return {array, input_count};
+}
+
 LocationSummary::LocationSummary(HInstruction* instruction,
                                  CallKind call_kind,
                                  bool intrinsified,
                                  ArenaAllocator* allocator)
-    : inputs_(instruction->InputCount(), allocator->Adapter(kArenaAllocLocationSummary)),
+    : inputs_(AllocateInputLocations(instruction, allocator)),
       temps_(allocator->Adapter(kArenaAllocLocationSummary)),
+      stack_mask_(nullptr),
       call_kind_(call_kind),
       intrinsified_(intrinsified),
       has_custom_slow_path_calling_convention_(false),
       output_overlaps_(Location::kOutputOverlap),
-      stack_mask_(nullptr),
       register_mask_(0),
       live_registers_(RegisterSet::Empty()),
       custom_slow_path_caller_saves_(RegisterSet::Empty()) {
diff --git a/compiler/optimizing/locations.h b/compiler/optimizing/locations.h
index 2209f05c0b..b8fe29c621 100644
--- a/compiler/optimizing/locations.h
+++ b/compiler/optimizing/locations.h
@@ -19,6 +19,7 @@
 
 #include "base/arena_containers.h"
 #include "base/arena_object.h"
+#include "base/array_ref.h"
 #include "base/bit_field.h"
 #include "base/bit_utils.h"
 #include "base/bit_vector.h"
@@ -39,7 +40,7 @@ std::ostream& operator<<(std::ostream& os, const Location& location);
  */
 class Location : public ValueObject {
  public:
-  enum OutputOverlap {
+  enum OutputOverlap : uint8_t {
     // The liveness of the output overlaps the liveness of one or
     // several input(s); the register allocator cannot reuse an
     // input's location for the output's location.
@@ -534,7 +535,7 @@ static constexpr bool kIntrinsified = true;
  */
 class LocationSummary : public ArenaObject<kArenaAllocLocationSummary> {
  public:
-  enum CallKind {
+  enum CallKind : uint8_t {
     kNoCall,
     kCallOnMainAndSlowPath,
     kCallOnSlowPath,
@@ -713,8 +714,13 @@ class LocationSummary : public ArenaObject<kArenaAllocLocationSummary> {
                   bool intrinsified,
                   ArenaAllocator* allocator);
 
-  ArenaVector<Location> inputs_;
+  ArrayRef<Location> inputs_;
   ArenaVector<Location> temps_;
+  Location output_;
+
+  // Mask of objects that live in the stack.
+  BitVector* stack_mask_;
+
   const CallKind call_kind_;
   // Whether these are locations for an intrinsified call.
   const bool intrinsified_;
@@ -723,10 +729,6 @@ class LocationSummary : public ArenaObject<kArenaAllocLocationSummary> {
   // Whether the output overlaps with any of the inputs. If it overlaps, then it cannot
   // share the same register as the inputs.
   Location::OutputOverlap output_overlaps_;
-  Location output_;
-
-  // Mask of objects that live in the stack.
-  BitVector* stack_mask_;
 
   // Mask of objects that live in register.
   uint32_t register_mask_;
@@ -734,7 +736,8 @@ class LocationSummary : public ArenaObject<kArenaAllocLocationSummary> {
   // Registers that are in use at this position.
   RegisterSet live_registers_;
 
-  // Custom slow path caller saves. Valid only if indicated by slow_path_calling_convention_.
+  // Custom slow path caller saves. Valid only if indicated by
+  // `has_custom_slow_path_calling_convention_`.
   RegisterSet custom_slow_path_caller_saves_;
 
   ART_FRIEND_TEST(RegisterAllocatorTest, ExpectedInRegisterHint);
diff --git a/compiler/optimizing/loop_optimization.cc b/compiler/optimizing/loop_optimization.cc
index 2f1aea68aa..18b2b56d00 100644
--- a/compiler/optimizing/loop_optimization.cc
+++ b/compiler/optimizing/loop_optimization.cc
@@ -3095,7 +3095,7 @@ bool HLoopOptimization::TryReplaceWithLastValue(HLoopInformation* loop_info,
         if (other_loop_info == nullptr || !other_loop_info->IsIn(*loop_info)) {
           user->RemoveAsUserOfInput(index);
           user->SetRawEnvAt(index, replacement);
-          replacement->AddEnvUseAt(user, index);
+          replacement->AddEnvUseAt(graph_->GetAllocator(), user, index);
         }
       }
     }
diff --git a/compiler/optimizing/nodes.cc b/compiler/optimizing/nodes.cc
index fc7d0a52ab..6b74e7246e 100644
--- a/compiler/optimizing/nodes.cc
+++ b/compiler/optimizing/nodes.cc
@@ -56,15 +56,20 @@ void HGraph::AddBlock(HBasicBlock* block) {
   blocks_.push_back(block);
 }
 
-void HGraph::FindBackEdges(ArenaBitVector* visited) {
+inline int32_t HGraph::AllocateInstructionId() {
+  CHECK_NE(current_instruction_id_, INT32_MAX);
+  return current_instruction_id_++;
+}
+
+void HGraph::FindBackEdges(/*out*/ BitVectorView<size_t> visited) {
   // "visited" must be empty on entry, it's an output argument for all visited (i.e. live) blocks.
-  DCHECK_EQ(visited->GetHighestBitSet(), -1);
+  DCHECK(!visited.IsAnyBitSet());
 
   // Allocate memory from local ScopedArenaAllocator.
   ScopedArenaAllocator allocator(GetArenaStack());
   // Nodes that we're currently visiting, indexed by block id.
-  ArenaBitVector visiting(
-      &allocator, blocks_.size(), /* expandable= */ false, kArenaAllocGraphBuilder);
+  BitVectorView<size_t> visiting =
+      ArenaBitVector::CreateFixedSize(&allocator, blocks_.size(), kArenaAllocGraphBuilder);
   // Number of successors visited from a given node, indexed by block id.
   ScopedArenaVector<size_t> successors_visited(blocks_.size(),
                                                0u,
@@ -73,7 +78,7 @@ void HGraph::FindBackEdges(ArenaBitVector* visited) {
   ScopedArenaVector<HBasicBlock*> worklist(allocator.Adapter(kArenaAllocGraphBuilder));
   constexpr size_t kDefaultWorklistSize = 8;
   worklist.reserve(kDefaultWorklistSize);
-  visited->SetBit(entry_block_->GetBlockId());
+  visited.SetBit(entry_block_->GetBlockId());
   visiting.SetBit(entry_block_->GetBlockId());
   worklist.push_back(entry_block_);
 
@@ -89,8 +94,8 @@ void HGraph::FindBackEdges(ArenaBitVector* visited) {
       if (visiting.IsBitSet(successor_id)) {
         DCHECK(ContainsElement(worklist, successor));
         successor->AddBackEdge(current);
-      } else if (!visited->IsBitSet(successor_id)) {
-        visited->SetBit(successor_id);
+      } else if (!visited.IsBitSet(successor_id)) {
+        visited.SetBit(successor_id);
         visiting.SetBit(successor_id);
         worklist.push_back(successor);
       }
@@ -145,7 +150,8 @@ static void RemoveAsUser(HInstruction* instruction) {
   RemoveEnvironmentUses(instruction);
 }
 
-void HGraph::RemoveDeadBlocksInstructionsAsUsersAndDisconnect(const ArenaBitVector& visited) const {
+void HGraph::RemoveDeadBlocksInstructionsAsUsersAndDisconnect(
+    BitVectorView<const size_t> visited) const {
   for (size_t i = 0; i < blocks_.size(); ++i) {
     if (!visited.IsBitSet(i)) {
       HBasicBlock* block = blocks_[i];
@@ -160,7 +166,7 @@ void HGraph::RemoveDeadBlocksInstructionsAsUsersAndDisconnect(const ArenaBitVect
       }
 
       // Remove non-catch phi uses, and disconnect the block.
-      block->DisconnectFromSuccessors(&visited);
+      block->DisconnectFromSuccessors(visited);
     }
   }
 }
@@ -183,7 +189,7 @@ static void RemoveCatchPhiUsesOfDeadInstruction(HInstruction* insn) {
   }
 }
 
-void HGraph::RemoveDeadBlocks(const ArenaBitVector& visited) {
+void HGraph::RemoveDeadBlocks(BitVectorView<const size_t> visited) {
   DCHECK(reverse_post_order_.empty()) << "We shouldn't have dominance information.";
   for (size_t i = 0; i < blocks_.size(); ++i) {
     if (!visited.IsBitSet(i)) {
@@ -210,10 +216,11 @@ GraphAnalysisResult HGraph::BuildDominatorTree() {
   // Allocate memory from local ScopedArenaAllocator.
   ScopedArenaAllocator allocator(GetArenaStack());
 
-  ArenaBitVector visited(&allocator, blocks_.size(), false, kArenaAllocGraphBuilder);
+  BitVectorView<size_t> visited =
+      ArenaBitVector::CreateFixedSize(&allocator, blocks_.size(), kArenaAllocGraphBuilder);
 
   // (1) Find the back edges in the graph doing a DFS traversal.
-  FindBackEdges(&visited);
+  FindBackEdges(visited);
 
   // (2) Remove instructions and phis from blocks not visited during
   //     the initial DFS as users from other instructions, so that
@@ -1035,10 +1042,13 @@ bool HBasicBlock::Dominates(const HBasicBlock* other) const {
   return false;
 }
 
-static void UpdateInputsUsers(HInstruction* instruction) {
+static void UpdateInputsUsers(HGraph* graph, HInstruction* instruction) {
   HInputsRef inputs = instruction->GetInputs();
-  for (size_t i = 0; i < inputs.size(); ++i) {
-    inputs[i]->AddUseAt(instruction, i);
+  if (inputs.size() != 0u) {
+    ArenaAllocator* allocator = graph->GetAllocator();
+    for (size_t i = 0; i < inputs.size(); ++i) {
+      inputs[i]->AddUseAt(allocator, instruction, i);
+    }
   }
   // Environment should be created later.
   DCHECK(!instruction->HasEnvironment());
@@ -1064,9 +1074,10 @@ void HBasicBlock::ReplaceAndRemoveInstructionWith(HInstruction* initial,
     DCHECK(initial->GetUses().empty());
     DCHECK(initial->GetEnvUses().empty());
     replacement->SetBlock(this);
-    replacement->SetId(GetGraph()->GetNextInstructionId());
+    HGraph* graph = GetGraph();
+    replacement->SetId(graph->AllocateInstructionId());
     instructions_.InsertInstructionBefore(replacement, initial);
-    UpdateInputsUsers(replacement);
+    UpdateInputsUsers(graph, replacement);
   } else {
     InsertInstructionBefore(replacement, initial);
     initial->ReplaceWith(replacement);
@@ -1080,8 +1091,9 @@ static void Add(HInstructionList* instruction_list,
   DCHECK(instruction->GetBlock() == nullptr);
   DCHECK_EQ(instruction->GetId(), -1);
   instruction->SetBlock(block);
-  instruction->SetId(block->GetGraph()->GetNextInstructionId());
-  UpdateInputsUsers(instruction);
+  HGraph* graph = block->GetGraph();
+  instruction->SetId(graph->AllocateInstructionId());
+  UpdateInputsUsers(graph, instruction);
   instruction_list->AddInstruction(instruction);
 }
 
@@ -1101,8 +1113,9 @@ void HBasicBlock::InsertInstructionBefore(HInstruction* instruction, HInstructio
   DCHECK_EQ(cursor->GetBlock(), this);
   DCHECK(!instruction->IsControlFlow());
   instruction->SetBlock(this);
-  instruction->SetId(GetGraph()->GetNextInstructionId());
-  UpdateInputsUsers(instruction);
+  HGraph* graph = GetGraph();
+  instruction->SetId(graph->AllocateInstructionId());
+  UpdateInputsUsers(graph, instruction);
   instructions_.InsertInstructionBefore(instruction, cursor);
 }
 
@@ -1115,8 +1128,9 @@ void HBasicBlock::InsertInstructionAfter(HInstruction* instruction, HInstruction
   DCHECK(!instruction->IsControlFlow());
   DCHECK(!cursor->IsControlFlow());
   instruction->SetBlock(this);
-  instruction->SetId(GetGraph()->GetNextInstructionId());
-  UpdateInputsUsers(instruction);
+  HGraph* graph = GetGraph();
+  instruction->SetId(graph->AllocateInstructionId());
+  UpdateInputsUsers(graph, instruction);
   instructions_.InsertInstructionAfter(instruction, cursor);
 }
 
@@ -1125,8 +1139,9 @@ void HBasicBlock::InsertPhiAfter(HPhi* phi, HPhi* cursor) {
   DCHECK_NE(cursor->GetId(), -1);
   DCHECK_EQ(cursor->GetBlock(), this);
   phi->SetBlock(this);
-  phi->SetId(GetGraph()->GetNextInstructionId());
-  UpdateInputsUsers(phi);
+  HGraph* graph = GetGraph();
+  phi->SetId(graph->AllocateInstructionId());
+  UpdateInputsUsers(graph, phi);
   phis_.InsertInstructionAfter(phi, cursor);
 }
 
@@ -1161,27 +1176,30 @@ void HBasicBlock::RemoveInstructionOrPhi(HInstruction* instruction, bool ensure_
   }
 }
 
-void HEnvironment::CopyFrom(ArrayRef<HInstruction* const> locals) {
+void HEnvironment::CopyFrom(ArenaAllocator* allocator, ArrayRef<HInstruction* const> locals) {
+  DCHECK_EQ(locals.size(), Size());
   for (size_t i = 0; i < locals.size(); i++) {
     HInstruction* instruction = locals[i];
     SetRawEnvAt(i, instruction);
     if (instruction != nullptr) {
-      instruction->AddEnvUseAt(this, i);
+      instruction->AddEnvUseAt(allocator, this, i);
     }
   }
 }
 
-void HEnvironment::CopyFrom(const HEnvironment* env) {
+void HEnvironment::CopyFrom(ArenaAllocator* allocator, const HEnvironment* env) {
+  DCHECK_EQ(env->Size(), Size());
   for (size_t i = 0; i < env->Size(); i++) {
     HInstruction* instruction = env->GetInstructionAt(i);
     SetRawEnvAt(i, instruction);
     if (instruction != nullptr) {
-      instruction->AddEnvUseAt(this, i);
+      instruction->AddEnvUseAt(allocator, this, i);
     }
   }
 }
 
-void HEnvironment::CopyFromWithLoopPhiAdjustment(HEnvironment* env,
+void HEnvironment::CopyFromWithLoopPhiAdjustment(ArenaAllocator* allocator,
+                                                 HEnvironment* env,
                                                  HBasicBlock* loop_header) {
   DCHECK(loop_header->IsLoopHeader());
   for (size_t i = 0; i < env->Size(); i++) {
@@ -1195,9 +1213,9 @@ void HEnvironment::CopyFromWithLoopPhiAdjustment(HEnvironment* env,
       // is the first input of the phi.
       HInstruction* initial = instruction->AsPhi()->InputAt(0);
       SetRawEnvAt(i, initial);
-      initial->AddEnvUseAt(this, i);
+      initial->AddEnvUseAt(allocator, this, i);
     } else {
-      instruction->AddEnvUseAt(this, i);
+      instruction->AddEnvUseAt(allocator, this, i);
     }
   }
 }
@@ -1332,17 +1350,19 @@ void HInstructionList::InsertInstructionAfter(HInstruction* instruction, HInstru
 }
 
 void HInstructionList::RemoveInstruction(HInstruction* instruction) {
-  if (instruction->previous_ != nullptr) {
-    instruction->previous_->next_ = instruction->next_;
-  }
-  if (instruction->next_ != nullptr) {
-    instruction->next_->previous_ = instruction->previous_;
-  }
+  DCHECK_EQ(instruction->previous_ == nullptr, instruction == first_instruction_);
+  DCHECK_EQ(instruction->next_ == nullptr, instruction == last_instruction_);
+
   if (instruction == first_instruction_) {
     first_instruction_ = instruction->next_;
+  } else {
+    instruction->previous_->next_ = instruction->next_;
   }
+
   if (instruction == last_instruction_) {
     last_instruction_ = instruction->previous_;
+  } else {
+    instruction->next_->previous_ = instruction->previous_;
   }
 }
 
@@ -1437,18 +1457,17 @@ void HInstruction::ReplaceUsesDominatedBy(HInstruction* dominator,
                                           HInstruction* replacement,
                                           bool strictly_dominated) {
   HBasicBlock* dominator_block = dominator->GetBlock();
-  std::optional<ArenaBitVector> visited_blocks;
+  BitVectorView<size_t> visited_blocks;
 
   // Lazily compute the dominated blocks to faster calculation of domination afterwards.
   auto maybe_generate_visited_blocks = [&visited_blocks, this, dominator_block]() {
-    if (visited_blocks.has_value()) {
+    if (visited_blocks.SizeInBits() != 0u) {
+      DCHECK_EQ(visited_blocks.SizeInBits(), GetBlock()->GetGraph()->GetBlocks().size());
       return;
     }
     HGraph* graph = GetBlock()->GetGraph();
-    visited_blocks.emplace(graph->GetAllocator(),
-                           graph->GetBlocks().size(),
-                           /* expandable= */ false,
-                           kArenaAllocMisc);
+    visited_blocks = ArenaBitVector::CreateFixedSize(
+        graph->GetAllocator(), graph->GetBlocks().size(), kArenaAllocMisc);
     ScopedArenaAllocator allocator(graph->GetArenaStack());
     ScopedArenaQueue<const HBasicBlock*> worklist(allocator.Adapter(kArenaAllocMisc));
     worklist.push(dominator_block);
@@ -1456,9 +1475,9 @@ void HInstruction::ReplaceUsesDominatedBy(HInstruction* dominator,
     while (!worklist.empty()) {
       const HBasicBlock* current = worklist.front();
       worklist.pop();
-      visited_blocks->SetBit(current->GetBlockId());
+      visited_blocks.SetBit(current->GetBlockId());
       for (HBasicBlock* dominated : current->GetDominatedBlocks()) {
-        if (visited_blocks->IsBitSet(dominated->GetBlockId())) {
+        if (visited_blocks.IsBitSet(dominated->GetBlockId())) {
           continue;
         }
         worklist.push(dominated);
@@ -1481,7 +1500,7 @@ void HInstruction::ReplaceUsesDominatedBy(HInstruction* dominator,
     } else {
       // Block domination.
       maybe_generate_visited_blocks();
-      dominated = visited_blocks->IsBitSet(block->GetBlockId());
+      dominated = visited_blocks.IsBitSet(block->GetBlockId());
     }
 
     if (dominated) {
@@ -1492,7 +1511,7 @@ void HInstruction::ReplaceUsesDominatedBy(HInstruction* dominator,
       // for their inputs.
       HBasicBlock* predecessor = block->GetPredecessors()[index];
       maybe_generate_visited_blocks();
-      if (visited_blocks->IsBitSet(predecessor->GetBlockId())) {
+      if (visited_blocks.IsBitSet(predecessor->GetBlockId())) {
         user->ReplaceInput(replacement, index);
       }
     }
@@ -1536,17 +1555,19 @@ size_t HInstruction::EnvironmentSize() const {
 void HVariableInputSizeInstruction::AddInput(HInstruction* input) {
   DCHECK(input->GetBlock() != nullptr);
   inputs_.push_back(HUserRecord<HInstruction*>(input));
-  input->AddUseAt(this, inputs_.size() - 1);
+  input->AddUseAt(GetBlock()->GetGraph()->GetAllocator(), this, inputs_.size() - 1);
 }
 
 void HVariableInputSizeInstruction::InsertInputAt(size_t index, HInstruction* input) {
   inputs_.insert(inputs_.begin() + index, HUserRecord<HInstruction*>(input));
-  input->AddUseAt(this, index);
   // Update indexes in use nodes of inputs that have been pushed further back by the insert().
   for (size_t i = index + 1u, e = inputs_.size(); i < e; ++i) {
     DCHECK_EQ(inputs_[i].GetUseNode()->GetIndex(), i - 1u);
     inputs_[i].GetUseNode()->SetIndex(i);
   }
+  // Add the use after updating the indexes. If the `input` is already used by `this`,
+  // the fixup after use insertion can use those indexes.
+  input->AddUseAt(GetBlock()->GetGraph()->GetAllocator(), this, index);
 }
 
 void HVariableInputSizeInstruction::RemoveInputAt(size_t index) {
@@ -1735,6 +1756,13 @@ void HGraphVisitor::VisitNonPhiInstructions(HBasicBlock* block) {
   }
 }
 
+void HGraphVisitor::VisitNonPhiInstructionsHandleChanges(HBasicBlock* block) {
+  for (HInstructionIteratorHandleChanges it(block->GetInstructions()); !it.Done(); it.Advance()) {
+    DCHECK(!it.Current()->IsPhi());
+    it.Current()->Accept(this);
+  }
+}
+
 HConstant* HTypeConversion::TryStaticEvaluation() const { return TryStaticEvaluation(GetInput()); }
 
 HConstant* HTypeConversion::TryStaticEvaluation(HInstruction* input) const {
@@ -2478,13 +2506,14 @@ void HBasicBlock::DisconnectAndDelete() {
   graph_->DeleteDeadEmptyBlock(this);
 }
 
-void HBasicBlock::DisconnectFromSuccessors(const ArenaBitVector* visited) {
+void HBasicBlock::DisconnectFromSuccessors(BitVectorView<const size_t> visited) {
+  DCHECK_IMPLIES(visited.SizeInBits() != 0u, visited.SizeInBits() == graph_->GetBlocks().size());
   for (HBasicBlock* successor : successors_) {
     // Delete this block from the list of predecessors.
     size_t this_index = successor->GetPredecessorIndexOf(this);
     successor->predecessors_.erase(successor->predecessors_.begin() + this_index);
 
-    if (visited != nullptr && !visited->IsBitSet(successor->GetBlockId())) {
+    if (visited.SizeInBits() != 0u && !visited.IsBitSet(successor->GetBlockId())) {
       // `successor` itself is dead. Therefore, there is no need to update its phis.
       continue;
     }
diff --git a/compiler/optimizing/nodes.h b/compiler/optimizing/nodes.h
index e0913c821c..7b86695670 100644
--- a/compiler/optimizing/nodes.h
+++ b/compiler/optimizing/nodes.h
@@ -289,7 +289,7 @@ class HGraph : public ArenaObject<kArenaAllocGraph> {
   void ComputeDominanceInformation();
   void ClearDominanceInformation();
   void ClearLoopInformation();
-  void FindBackEdges(ArenaBitVector* visited);
+  void FindBackEdges(/*out*/ BitVectorView<size_t> visited);
   GraphAnalysisResult BuildDominatorTree();
   GraphAnalysisResult RecomputeDominatorTree();
   void SimplifyCFG();
@@ -359,10 +359,7 @@ class HGraph : public ArenaObject<kArenaAllocGraph> {
 
   void SimplifyLoop(HBasicBlock* header);
 
-  int32_t GetNextInstructionId() {
-    CHECK_NE(current_instruction_id_, INT32_MAX);
-    return current_instruction_id_++;
-  }
+  ALWAYS_INLINE int32_t AllocateInstructionId();
 
   int32_t GetCurrentInstructionId() const {
     return current_instruction_id_;
@@ -548,8 +545,8 @@ class HGraph : public ArenaObject<kArenaAllocGraph> {
   bool IsUsefulOptimizing() const { return useful_optimizing_; }
 
  private:
-  void RemoveDeadBlocksInstructionsAsUsersAndDisconnect(const ArenaBitVector& visited) const;
-  void RemoveDeadBlocks(const ArenaBitVector& visited);
+  void RemoveDeadBlocksInstructionsAsUsersAndDisconnect(BitVectorView<const size_t> visited) const;
+  void RemoveDeadBlocks(BitVectorView<const size_t> visited);
 
   template <class InstructionType, typename ValueType>
   InstructionType* CreateConstant(ValueType value,
@@ -1136,7 +1133,7 @@ class HBasicBlock : public ArenaObject<kArenaAllocBasicBlock> {
   // Disconnects `this` from all its successors and updates their phis, if the successors have them.
   // If `visited` is provided, it will use the information to know if a successor is reachable and
   // skip updating those phis.
-  void DisconnectFromSuccessors(const ArenaBitVector* visited = nullptr);
+  void DisconnectFromSuccessors(BitVectorView<const size_t> visited = {});
 
   // Removes the catch phi uses of the instructions in `this`, and then remove the instruction
   // itself. If `building_dominator_tree` is true, it will not remove the instruction as user, since
@@ -1329,8 +1326,8 @@ class HLoopInformationOutwardIterator : public ValueObject {
   M(GreaterThan, Condition)                                             \
   M(GreaterThanOrEqual, Condition)                                      \
   M(If, Instruction)                                                    \
-  M(InstanceFieldGet, Instruction)                                      \
-  M(InstanceFieldSet, Instruction)                                      \
+  M(InstanceFieldGet, FieldAccess)                                      \
+  M(InstanceFieldSet, FieldAccess)                                      \
   M(InstanceOf, Instruction)                                            \
   M(IntConstant, Constant)                                              \
   M(IntermediateAddress, Instruction)                                   \
@@ -1375,8 +1372,8 @@ class HLoopInformationOutwardIterator : public ValueObject {
   M(Ror, BinaryOperation)                                               \
   M(Shl, BinaryOperation)                                               \
   M(Shr, BinaryOperation)                                               \
-  M(StaticFieldGet, Instruction)                                        \
-  M(StaticFieldSet, Instruction)                                        \
+  M(StaticFieldGet, FieldAccess)                                        \
+  M(StaticFieldSet, FieldAccess)                                        \
   M(StringBuilderAppend, Instruction)                                   \
   M(UnresolvedInstanceFieldGet, Instruction)                            \
   M(UnresolvedInstanceFieldSet, Instruction)                            \
@@ -1497,6 +1494,7 @@ class HLoopInformationOutwardIterator : public ValueObject {
   M(Constant, Instruction)                                              \
   M(UnaryOperation, Instruction)                                        \
   M(BinaryOperation, Instruction)                                       \
+  M(FieldAccess, Instruction)                                           \
   M(Invoke, Instruction)                                                \
   M(VecOperation, Instruction)                                          \
   M(VecUnaryOperation, VecOperation)                                    \
@@ -1733,26 +1731,26 @@ class SideEffects : public ValueObject {
   }
 
   bool HasSideEffects() const {
-    return (flags_ & kAllChangeBits);
+    return (flags_ & kAllChangeBits) != 0u;
   }
 
   bool HasDependencies() const {
-    return (flags_ & kAllDependOnBits);
+    return (flags_ & kAllDependOnBits) != 0u;
   }
 
   // Returns true if there are no side effects or dependencies.
   bool DoesNothing() const {
-    return flags_ == 0;
+    return flags_ == 0u;
   }
 
   // Returns true if something is written.
   bool DoesAnyWrite() const {
-    return (flags_ & kAllWrites);
+    return (flags_ & kAllWrites) != 0u;
   }
 
   // Returns true if something is read.
   bool DoesAnyRead() const {
-    return (flags_ & kAllReads);
+    return (flags_ & kAllReads) != 0u;
   }
 
   // Returns true if potentially everything is written and read
@@ -1768,7 +1766,7 @@ class SideEffects : public ValueObject {
   // Returns true if `this` may read something written by `other`.
   bool MayDependOn(SideEffects other) const {
     const uint64_t depends_on_flags = (flags_ & kAllDependOnBits) >> kChangeBits;
-    return (other.flags_ & depends_on_flags);
+    return (other.flags_ & depends_on_flags) != 0u;
   }
 
   // Returns string representation of flags (for debugging only).
@@ -1893,20 +1891,22 @@ class HEnvironment : public ArenaObject<kArenaAllocEnvironment> {
       parent_->SetAndCopyParentChain(allocator, parent);
     } else {
       parent_ = Create(allocator, *parent, holder_);
-      parent_->CopyFrom(parent);
+      parent_->CopyFrom(allocator, parent);
       if (parent->GetParent() != nullptr) {
         parent_->SetAndCopyParentChain(allocator, parent->GetParent());
       }
     }
   }
 
-  void CopyFrom(ArrayRef<HInstruction* const> locals);
-  void CopyFrom(const HEnvironment* environment);
+  void CopyFrom(ArenaAllocator* allocator, ArrayRef<HInstruction* const> locals);
+  void CopyFrom(ArenaAllocator* allocator, const HEnvironment* environment);
 
   // Copy from `env`. If it's a loop phi for `loop_header`, copy the first
   // input to the loop phi instead. This is for inserting instructions that
   // require an environment (like HDeoptimization) in the loop pre-header.
-  void CopyFromWithLoopPhiAdjustment(HEnvironment* env, HBasicBlock* loop_header);
+  void CopyFromWithLoopPhiAdjustment(ArenaAllocator* allocator,
+                                     HEnvironment* env,
+                                     HBasicBlock* loop_header);
 
   void SetRawEnvAt(size_t index, HInstruction* instruction) {
     GetVRegs()[index] = HUserRecord<HEnvironment*>(instruction);
@@ -2114,7 +2114,6 @@ class HInstruction : public ArenaObject<kArenaAllocInstruction> {
   HInstruction* GetPreviousDisregardingMoves() const;
 
   HBasicBlock* GetBlock() const { return block_; }
-  ArenaAllocator* GetAllocator() const { return block_->GetGraph()->GetAllocator(); }
   void SetBlock(HBasicBlock* block) { block_ = block; }
   bool IsInBlock() const { return block_ != nullptr; }
   bool IsInLoop() const { return block_->IsInLoop(); }
@@ -2227,25 +2226,46 @@ class HInstruction : public ArenaObject<kArenaAllocInstruction> {
                                               GetPackedFlag<kFlagReferenceTypeIsExact>());
   }
 
-  void AddUseAt(HInstruction* user, size_t index) {
+  void AddUseAt(ArenaAllocator* allocator, HInstruction* user, size_t index) {
     DCHECK(user != nullptr);
-    // Note: fixup_end remains valid across push_front().
-    auto fixup_end = uses_.empty() ? uses_.begin() : ++uses_.begin();
-    ArenaAllocator* allocator = user->GetBlock()->GetGraph()->GetAllocator();
     HUseListNode<HInstruction*>* new_node =
         new (allocator) HUseListNode<HInstruction*>(user, index);
+    // Note: `old_begin` remains valid across `push_front()`.
+    auto old_begin = uses_.begin();
     uses_.push_front(*new_node);
-    FixUpUserRecordsAfterUseInsertion(fixup_end);
+    // To speed up this code, we inline the
+    //     FixUpUserRecordsAfterUseInsertion(
+    //         old_begin != uses_.end() ? ++old_begin : old_begin);
+    // to reduce branching as we know that we're going to fix up either one or two entries.
+    auto new_begin = uses_.begin();
+    user->SetRawInputRecordAt(index, HUserRecord<HInstruction*>(this, uses_.before_begin()));
+    if (old_begin != uses_.end()) {
+      HInstruction* old_begin_user = old_begin->GetUser();
+      size_t old_begin_index = old_begin->GetIndex();
+      old_begin_user->SetRawInputRecordAt(
+          old_begin_index, HUserRecord<HInstruction*>(this, new_begin));
+    }
   }
 
-  void AddEnvUseAt(HEnvironment* user, size_t index) {
+  void AddEnvUseAt(ArenaAllocator* allocator, HEnvironment* user, size_t index) {
     DCHECK(user != nullptr);
-    // Note: env_fixup_end remains valid across push_front().
-    auto env_fixup_end = env_uses_.empty() ? env_uses_.begin() : ++env_uses_.begin();
     HUseListNode<HEnvironment*>* new_node =
-        new (GetBlock()->GetGraph()->GetAllocator()) HUseListNode<HEnvironment*>(user, index);
+        new (allocator) HUseListNode<HEnvironment*>(user, index);
+    // Note: `old_env_begin` remains valid across `push_front()`.
+    auto old_env_begin = env_uses_.begin();
     env_uses_.push_front(*new_node);
-    FixUpUserRecordsAfterEnvUseInsertion(env_fixup_end);
+    // To speed up this code, we inline the
+    //     FixUpUserRecordsAfterEnvUseInsertion(
+    //         old_env_begin != env_uses_.end() ? ++old_env_begin : old_env_begin);
+    // to reduce branching as we know that we're going to fix up either one or two entries.
+    auto new_env_begin = env_uses_.begin();
+    user->GetVRegs()[index] = HUserRecord<HEnvironment*>(this, env_uses_.before_begin());
+    if (old_env_begin != env_uses_.end()) {
+      HEnvironment* old_env_begin_user = old_env_begin->GetUser();
+      size_t old_env_begin_index = old_env_begin->GetIndex();
+      old_env_begin_user->GetVRegs()[old_env_begin_index] =
+          HUserRecord<HEnvironment*>(this, new_env_begin);
+    }
   }
 
   void RemoveAsUserOfInput(size_t input) {
@@ -2343,18 +2363,18 @@ class HInstruction : public ArenaObject<kArenaAllocInstruction> {
     DCHECK(environment_ == nullptr);
     ArenaAllocator* allocator = GetBlock()->GetGraph()->GetAllocator();
     environment_ = HEnvironment::Create(allocator, *environment, this);
-    environment_->CopyFrom(environment);
+    environment_->CopyFrom(allocator, environment);
     if (environment->GetParent() != nullptr) {
       environment_->SetAndCopyParentChain(allocator, environment->GetParent());
     }
   }
 
   void CopyEnvironmentFromWithLoopPhiAdjustment(HEnvironment* environment,
-                                                HBasicBlock* block) {
+                                                HBasicBlock* loop_header) {
     DCHECK(environment_ == nullptr);
-    ArenaAllocator* allocator = GetBlock()->GetGraph()->GetAllocator();
+    ArenaAllocator* allocator = loop_header->GetGraph()->GetAllocator();
     environment_ = HEnvironment::Create(allocator, *environment, this);
-    environment_->CopyFromWithLoopPhiAdjustment(environment, block);
+    environment_->CopyFromWithLoopPhiAdjustment(allocator, environment, loop_header);
     if (environment->GetParent() != nullptr) {
       environment_->SetAndCopyParentChain(allocator, environment->GetParent());
     }
@@ -2423,17 +2443,6 @@ class HInstruction : public ArenaObject<kArenaAllocInstruction> {
     UNREACHABLE();
   }
 
-  virtual bool IsFieldAccess() const {
-    return false;
-  }
-
-  virtual const FieldInfo& GetFieldInfo() const {
-    CHECK(IsFieldAccess()) << "Only callable on field accessors not " << DebugName() << " "
-                           << *this;
-    LOG(FATAL) << "Must be overridden by field accessors. Not implemented by " << *this;
-    UNREACHABLE();
-  }
-
   // Return whether instruction can be cloned (copied).
   virtual bool IsClonable() const { return false; }
 
@@ -2850,16 +2859,13 @@ class HVariableInputSizeInstruction : public HInstruction {
   ArenaVector<HUserRecord<HInstruction*>> inputs_;
 };
 
-template<size_t N>
-class HExpression : public HInstruction {
+template<size_t N, typename Base = HInstruction>
+class HExpression : public Base {
  public:
-  HExpression<N>(InstructionKind kind, SideEffects side_effects, uint32_t dex_pc)
-      : HInstruction(kind, side_effects, dex_pc), inputs_() {}
-  HExpression<N>(InstructionKind kind,
-                 DataType::Type type,
-                 SideEffects side_effects,
-                 uint32_t dex_pc)
-      : HInstruction(kind, type, side_effects, dex_pc), inputs_() {}
+  template <typename... Args>
+  explicit HExpression(Args&&... args)
+      : Base(std::forward<Args>(args)...), inputs_() {}
+
   virtual ~HExpression() {}
 
   using HInstruction::GetInputRecords;  // Keep the const version visible.
@@ -2868,7 +2874,7 @@ class HExpression : public HInstruction {
   }
 
  protected:
-  DEFAULT_COPY_CONSTRUCTOR(Expression<N>);
+  DEFAULT_COPY_CONSTRUCTOR(Expression);
 
  private:
   std::array<HUserRecord<HInstruction*>, N> inputs_;
@@ -2877,10 +2883,12 @@ class HExpression : public HInstruction {
 };
 
 // HExpression specialization for N=0.
-template<>
-class HExpression<0> : public HInstruction {
+template<typename Base>
+class HExpression<0, Base> : public Base {
  public:
-  using HInstruction::HInstruction;
+  template <typename... Args>
+  explicit HExpression(Args&&... args)
+      : Base(std::forward<Args>(args)...) {}
 
   virtual ~HExpression() {}
 
@@ -2890,7 +2898,7 @@ class HExpression<0> : public HInstruction {
   }
 
  protected:
-  DEFAULT_COPY_CONSTRUCTOR(Expression<0>);
+  DEFAULT_COPY_CONSTRUCTOR(Expression);
 
  private:
   friend class SsaBuilder;
@@ -3179,12 +3187,6 @@ class HIntConstant final : public HConstant {
   bool IsTrue() const { return GetValue() == 1; }
   bool IsFalse() const { return GetValue() == 0; }
 
-  DECLARE_INSTRUCTION(IntConstant);
-
- protected:
-  DEFAULT_COPY_CONSTRUCTOR(IntConstant);
-
- private:
   explicit HIntConstant(int32_t value)
       : HConstant(kIntConstant, DataType::Type::kInt32), value_(value) {
   }
@@ -3193,6 +3195,12 @@ class HIntConstant final : public HConstant {
         value_(value ? 1 : 0) {
   }
 
+  DECLARE_INSTRUCTION(IntConstant);
+
+ protected:
+  DEFAULT_COPY_CONSTRUCTOR(IntConstant);
+
+ private:
   const int32_t value_;
 
   friend class HGraph;
@@ -3697,7 +3705,7 @@ class HUnaryOperation : public HExpression<1> {
   HInstruction* GetInput() const { return InputAt(0); }
   DataType::Type GetResultType() const { return GetType(); }
 
-  bool CanBeMoved() const override { return true; }
+  bool CanBeMoved() const final { return true; }
   bool InstructionDataEquals([[maybe_unused]] const HInstruction* other) const override {
     return true;
   }
@@ -3788,7 +3796,7 @@ class HBinaryOperation : public HExpression<2> {
     }
   }
 
-  bool CanBeMoved() const override { return true; }
+  bool CanBeMoved() const final { return true; }
   bool InstructionDataEquals([[maybe_unused]] const HInstruction* other) const override {
     return true;
   }
@@ -5872,7 +5880,6 @@ class HNot final : public HUnaryOperation {
       : HUnaryOperation(kNot, result_type, input, dex_pc) {
   }
 
-  bool CanBeMoved() const override { return true; }
   bool InstructionDataEquals([[maybe_unused]] const HInstruction* other) const override {
     return true;
   }
@@ -5898,7 +5905,6 @@ class HBooleanNot final : public HUnaryOperation {
       : HUnaryOperation(kBooleanNot, DataType::Type::kBool, input, dex_pc) {
   }
 
-  bool CanBeMoved() const override { return true; }
   bool InstructionDataEquals([[maybe_unused]] const HInstruction* other) const override {
     return true;
   }
@@ -6047,7 +6053,42 @@ inline std::ostream& operator<<(std::ostream& os, const FieldInfo& a) {
   return a.Dump(os);
 }
 
-class HInstanceFieldGet final : public HExpression<1> {
+class HFieldAccess : public HInstruction {
+ public:
+  HFieldAccess(InstructionKind kind,
+               SideEffects side_effects,
+               ArtField* field,
+               DataType::Type field_type,
+               MemberOffset field_offset,
+               bool is_volatile,
+               uint32_t field_idx,
+               uint16_t declaring_class_def_index,
+               const DexFile& dex_file,
+               uint32_t dex_pc)
+      : HInstruction(kind, field_type, side_effects, dex_pc),
+        field_info_(field,
+                    field_offset,
+                    field_type,
+                    is_volatile,
+                    field_idx,
+                    declaring_class_def_index,
+                    dex_file) {}
+
+  const FieldInfo& GetFieldInfo() const { return field_info_; }
+  MemberOffset GetFieldOffset() const { return field_info_.GetFieldOffset(); }
+  DataType::Type GetFieldType() const { return field_info_.GetFieldType(); }
+  bool IsVolatile() const { return field_info_.IsVolatile(); }
+
+  DECLARE_ABSTRACT_INSTRUCTION(FieldAccess);
+
+ protected:
+  DEFAULT_COPY_CONSTRUCTOR(FieldAccess);
+
+ private:
+  const FieldInfo field_info_;
+};
+
+class HInstanceFieldGet final : public HExpression<1, HFieldAccess> {
  public:
   HInstanceFieldGet(HInstruction* object,
                     ArtField* field,
@@ -6059,16 +6100,15 @@ class HInstanceFieldGet final : public HExpression<1> {
                     const DexFile& dex_file,
                     uint32_t dex_pc)
       : HExpression(kInstanceFieldGet,
-                    field_type,
                     SideEffects::FieldReadOfType(field_type, is_volatile),
-                    dex_pc),
-        field_info_(field,
-                    field_offset,
+                    field,
                     field_type,
+                    field_offset,
                     is_volatile,
                     field_idx,
                     declaring_class_def_index,
-                    dex_file) {
+                    dex_file,
+                    dex_pc) {
     SetRawInputAt(0, object);
   }
 
@@ -6088,12 +6128,6 @@ class HInstanceFieldGet final : public HExpression<1> {
     return (HInstruction::ComputeHashCode() << 7) | GetFieldOffset().SizeValue();
   }
 
-  bool IsFieldAccess() const override { return true; }
-  const FieldInfo& GetFieldInfo() const override { return field_info_; }
-  MemberOffset GetFieldOffset() const { return field_info_.GetFieldOffset(); }
-  DataType::Type GetFieldType() const { return field_info_.GetFieldType(); }
-  bool IsVolatile() const { return field_info_.IsVolatile(); }
-
   void SetType(DataType::Type new_type) {
     DCHECK(DataType::IsIntegralType(GetType()));
     DCHECK(DataType::IsIntegralType(new_type));
@@ -6105,9 +6139,6 @@ class HInstanceFieldGet final : public HExpression<1> {
 
  protected:
   DEFAULT_COPY_CONSTRUCTOR(InstanceFieldGet);
-
- private:
-  const FieldInfo field_info_;
 };
 
 enum class WriteBarrierKind {
@@ -6125,7 +6156,7 @@ enum class WriteBarrierKind {
 };
 std::ostream& operator<<(std::ostream& os, WriteBarrierKind rhs);
 
-class HInstanceFieldSet final : public HExpression<2> {
+class HInstanceFieldSet final : public HExpression<2, HFieldAccess> {
  public:
   HInstanceFieldSet(HInstruction* object,
                     HInstruction* value,
@@ -6139,16 +6170,19 @@ class HInstanceFieldSet final : public HExpression<2> {
                     uint32_t dex_pc)
       : HExpression(kInstanceFieldSet,
                     SideEffects::FieldWriteOfType(field_type, is_volatile),
-                    dex_pc),
-        field_info_(field,
-                    field_offset,
+                    field,
                     field_type,
+                    field_offset,
                     is_volatile,
                     field_idx,
                     declaring_class_def_index,
-                    dex_file) {
+                    dex_file,
+                    dex_pc) {
     SetPackedFlag<kFlagValueCanBeNull>(true);
-    SetPackedField<WriteBarrierKindField>(WriteBarrierKind::kEmitNotBeingReliedOn);
+    SetPackedField<WriteBarrierKindField>(
+        field_type == DataType::Type::kReference
+            ? WriteBarrierKind::kEmitNotBeingReliedOn
+            : WriteBarrierKind::kDontEmit);
     SetRawInputAt(0, object);
     SetRawInputAt(1, value);
   }
@@ -6159,11 +6193,6 @@ class HInstanceFieldSet final : public HExpression<2> {
     return (obj == InputAt(0)) && art::CanDoImplicitNullCheckOn(GetFieldOffset().Uint32Value());
   }
 
-  bool IsFieldAccess() const override { return true; }
-  const FieldInfo& GetFieldInfo() const override { return field_info_; }
-  MemberOffset GetFieldOffset() const { return field_info_.GetFieldOffset(); }
-  DataType::Type GetFieldType() const { return field_info_.GetFieldType(); }
-  bool IsVolatile() const { return field_info_.IsVolatile(); }
   HInstruction* GetValue() const { return InputAt(1); }
   bool GetValueCanBeNull() const { return GetPackedFlag<kFlagValueCanBeNull>(); }
   void ClearValueCanBeNull() { SetPackedFlag<kFlagValueCanBeNull>(false); }
@@ -6192,7 +6221,6 @@ class HInstanceFieldSet final : public HExpression<2> {
   static_assert(kNumberOfInstanceFieldSetPackedBits <= kMaxNumberOfPackedBits,
                 "Too many packed fields.");
 
-  const FieldInfo field_info_;
   using WriteBarrierKindField =
       BitField<WriteBarrierKind, kWriteBarrierKind, kWriteBarrierKindSize>;
 };
@@ -6315,7 +6343,10 @@ class HArraySet final : public HExpression<3> {
     SetPackedFlag<kFlagNeedsTypeCheck>(value->GetType() == DataType::Type::kReference);
     SetPackedFlag<kFlagValueCanBeNull>(true);
     SetPackedFlag<kFlagStaticTypeOfArrayIsObjectArray>(false);
-    SetPackedField<WriteBarrierKindField>(WriteBarrierKind::kEmitNotBeingReliedOn);
+    SetPackedField<WriteBarrierKindField>(
+        value->GetType() == DataType::Type::kReference
+            ? WriteBarrierKind::kEmitNotBeingReliedOn
+            : WriteBarrierKind::kDontEmit);
     SetRawInputAt(0, array);
     SetRawInputAt(1, index);
     SetRawInputAt(2, value);
@@ -6841,7 +6872,7 @@ inline void HLoadClass::AddSpecialInput(HInstruction* special_input) {
          GetLoadKind() == LoadKind::kJitBootImageAddress) << GetLoadKind();
   DCHECK(special_input_.GetInstruction() == nullptr);
   special_input_ = HUserRecord<HInstruction*>(special_input);
-  special_input->AddUseAt(this, 0);
+  special_input->AddUseAt(GetBlock()->GetGraph()->GetAllocator(), this, 0);
 }
 
 class HLoadString final : public HInstruction {
@@ -7009,7 +7040,7 @@ inline void HLoadString::AddSpecialInput(HInstruction* special_input) {
   // so use the GetInputRecords() from the base class to set the input record.
   DCHECK(special_input_.GetInstruction() == nullptr);
   special_input_ = HUserRecord<HInstruction*>(special_input);
-  special_input->AddUseAt(this, 0);
+  special_input->AddUseAt(GetBlock()->GetGraph()->GetAllocator(), this, 0);
 }
 
 class HLoadMethodHandle final : public HInstruction {
@@ -7191,7 +7222,7 @@ class HClinitCheck final : public HExpression<1> {
   DEFAULT_COPY_CONSTRUCTOR(ClinitCheck);
 };
 
-class HStaticFieldGet final : public HExpression<1> {
+class HStaticFieldGet final : public HExpression<1, HFieldAccess> {
  public:
   HStaticFieldGet(HInstruction* cls,
                   ArtField* field,
@@ -7203,16 +7234,15 @@ class HStaticFieldGet final : public HExpression<1> {
                   const DexFile& dex_file,
                   uint32_t dex_pc)
       : HExpression(kStaticFieldGet,
-                    field_type,
                     SideEffects::FieldReadOfType(field_type, is_volatile),
-                    dex_pc),
-        field_info_(field,
-                    field_offset,
+                    field,
                     field_type,
+                    field_offset,
                     is_volatile,
                     field_idx,
                     declaring_class_def_index,
-                    dex_file) {
+                    dex_file,
+                    dex_pc) {
     SetRawInputAt(0, cls);
   }
 
@@ -7229,12 +7259,6 @@ class HStaticFieldGet final : public HExpression<1> {
     return (HInstruction::ComputeHashCode() << 7) | GetFieldOffset().SizeValue();
   }
 
-  bool IsFieldAccess() const override { return true; }
-  const FieldInfo& GetFieldInfo() const override { return field_info_; }
-  MemberOffset GetFieldOffset() const { return field_info_.GetFieldOffset(); }
-  DataType::Type GetFieldType() const { return field_info_.GetFieldType(); }
-  bool IsVolatile() const { return field_info_.IsVolatile(); }
-
   void SetType(DataType::Type new_type) {
     DCHECK(DataType::IsIntegralType(GetType()));
     DCHECK(DataType::IsIntegralType(new_type));
@@ -7246,12 +7270,9 @@ class HStaticFieldGet final : public HExpression<1> {
 
  protected:
   DEFAULT_COPY_CONSTRUCTOR(StaticFieldGet);
-
- private:
-  const FieldInfo field_info_;
 };
 
-class HStaticFieldSet final : public HExpression<2> {
+class HStaticFieldSet final : public HExpression<2, HFieldAccess> {
  public:
   HStaticFieldSet(HInstruction* cls,
                   HInstruction* value,
@@ -7265,26 +7286,24 @@ class HStaticFieldSet final : public HExpression<2> {
                   uint32_t dex_pc)
       : HExpression(kStaticFieldSet,
                     SideEffects::FieldWriteOfType(field_type, is_volatile),
-                    dex_pc),
-        field_info_(field,
-                    field_offset,
+                    field,
                     field_type,
+                    field_offset,
                     is_volatile,
                     field_idx,
                     declaring_class_def_index,
-                    dex_file) {
+                    dex_file,
+                    dex_pc) {
     SetPackedFlag<kFlagValueCanBeNull>(true);
-    SetPackedField<WriteBarrierKindField>(WriteBarrierKind::kEmitNotBeingReliedOn);
+    SetPackedField<WriteBarrierKindField>(
+        field_type == DataType::Type::kReference
+            ? WriteBarrierKind::kEmitNotBeingReliedOn
+            : WriteBarrierKind::kDontEmit);
     SetRawInputAt(0, cls);
     SetRawInputAt(1, value);
   }
 
   bool IsClonable() const override { return true; }
-  bool IsFieldAccess() const override { return true; }
-  const FieldInfo& GetFieldInfo() const override { return field_info_; }
-  MemberOffset GetFieldOffset() const { return field_info_.GetFieldOffset(); }
-  DataType::Type GetFieldType() const { return field_info_.GetFieldType(); }
-  bool IsVolatile() const { return field_info_.IsVolatile(); }
 
   HInstruction* GetValue() const { return InputAt(1); }
   bool GetValueCanBeNull() const { return GetPackedFlag<kFlagValueCanBeNull>(); }
@@ -7315,7 +7334,6 @@ class HStaticFieldSet final : public HExpression<2> {
   static_assert(kNumberOfStaticFieldSetPackedBits <= kMaxNumberOfPackedBits,
                 "Too many packed fields.");
 
-  const FieldInfo field_info_;
   using WriteBarrierKindField =
       BitField<WriteBarrierKind, kWriteBarrierKind, kWriteBarrierKindSize>;
 };
@@ -8392,6 +8410,7 @@ class HGraphVisitor : public ValueObject {
  protected:
   void VisitPhis(HBasicBlock* block);
   void VisitNonPhiInstructions(HBasicBlock* block);
+  void VisitNonPhiInstructionsHandleChanges(HBasicBlock* block);
 
   OptimizingCompilerStats* stats_;
 
diff --git a/compiler/optimizing/nodes_test.cc b/compiler/optimizing/nodes_test.cc
index 0302298b9c..97f74cb7b9 100644
--- a/compiler/optimizing/nodes_test.cc
+++ b/compiler/optimizing/nodes_test.cc
@@ -197,6 +197,36 @@ TEST_F(NodeTest, AddInstruction) {
   ASSERT_TRUE(parameter->GetUses().HasExactlyOneElement());
 }
 
+TEST_F(NodeTest, InsertDuplicateInstructionAt) {
+  HBasicBlock* ret = InitEntryMainExitGraphWithReturnVoid();
+  HInstruction* const0 = graph_->GetIntConstant(0);
+  HInstruction* const1 = graph_->GetIntConstant(1);
+  HInstruction* const2 = graph_->GetIntConstant(0);
+  HInstruction* const3 = graph_->GetIntConstant(1);
+
+  // We should be able to insert a duplicate input to `HPhi`s if we want to
+  // make a graph transformation that adds another predecessor to a block.
+
+  // This used to accidentally end up with correct use information but unexpectedly
+  // using the old `HUseListNode<>` for the new use and the new one for the old use.
+  HPhi* phi1 = MakePhi(ret, {const0, const1});
+  struct AccessProtected : HVariableInputSizeInstruction {
+    using HVariableInputSizeInstruction::InputRecordAt;
+  };
+  const HUseListNode<HInstruction*>* old_use_node_before =
+      std::addressof(*(phi1->*&AccessProtected::InputRecordAt)(1u).GetUseNode());
+  phi1->InsertInputAt(1u, const1);  // Moves the old use from position 1 to position 2.
+  const HUseListNode<HInstruction*>* old_use_node_after =
+      std::addressof(*(phi1->*&AccessProtected::InputRecordAt)(2u).GetUseNode());
+  EXPECT_EQ(old_use_node_before, old_use_node_after);
+  EXPECT_EQ(1u, (phi1->*&AccessProtected::InputRecordAt)(1u).GetUseNode()->GetIndex());
+  EXPECT_EQ(2u, (phi1->*&AccessProtected::InputRecordAt)(2u).GetUseNode()->GetIndex());
+
+  // This used to hit a `DCHECK()`.
+  HPhi* phi2 = MakePhi(ret, {const2, const3, const3});
+  phi2->InsertInputAt(1u, const3);
+}
+
 TEST_F(NodeTest, ParentEnvironment) {
   HGraph* graph = CreateGraph();
   HBasicBlock* entry = new (GetAllocator()) HBasicBlock(graph);
@@ -218,7 +248,7 @@ TEST_F(NodeTest, ParentEnvironment) {
       graph->GetArtMethod(),
       /*dex_pc=*/ 0,
       /*holder=*/ nullptr);
-  parent1->CopyFrom(ArrayRef<HInstruction* const>(&parameter1, 1u));
+  parent1->CopyFrom(GetAllocator(), ArrayRef<HInstruction* const>(&parameter1, 1u));
 
   ASSERT_EQ(parameter1->GetEnvUses().SizeSlow(), 2u);
 
@@ -228,7 +258,7 @@ TEST_F(NodeTest, ParentEnvironment) {
       graph->GetArtMethod(),
       /*dex_pc=*/ 0,
       /*holder=*/ nullptr);
-  parent2->CopyFrom(ArrayRef<HInstruction* const>(&parameter1, 1u));
+  parent2->CopyFrom(GetAllocator(), ArrayRef<HInstruction* const>(&parameter1, 1u));
   parent1->SetAndCopyParentChain(GetAllocator(), parent2);
 
   // One use for parent2, and one other use for the new parent of parent1.
diff --git a/compiler/optimizing/nodes_x86.h b/compiler/optimizing/nodes_x86.h
index 71c4f7aeeb..491045de99 100644
--- a/compiler/optimizing/nodes_x86.h
+++ b/compiler/optimizing/nodes_x86.h
@@ -59,6 +59,8 @@ class HX86LoadFromConstantTable final : public HExpression<2> {
     return InputAt(1)->AsConstant();
   }
 
+  bool CanBeMoved() const override { return true; }
+
   DECLARE_INSTRUCTION(X86LoadFromConstantTable);
 
  protected:
diff --git a/compiler/optimizing/optimization.cc b/compiler/optimizing/optimization.cc
index bd8fbdf1d2..31780739bc 100644
--- a/compiler/optimizing/optimization.cc
+++ b/compiler/optimizing/optimization.cc
@@ -43,6 +43,7 @@
 #include "code_sinking.h"
 #include "constant_folding.h"
 #include "constructor_fence_redundancy_elimination.h"
+#include "control_flow_simplifier.h"
 #include "dead_code_elimination.h"
 #include "dex/code_item_accessors-inl.h"
 #include "driver/compiler_options.h"
@@ -57,7 +58,6 @@
 #include "loop_optimization.h"
 #include "reference_type_propagation.h"
 #include "scheduler.h"
-#include "select_generator.h"
 #include "sharpening.h"
 #include "side_effects_analysis.h"
 #include "write_barrier_elimination.h"
@@ -88,8 +88,8 @@ const char* OptimizationPassName(OptimizationPass pass) {
       return HDeadCodeElimination::kDeadCodeEliminationPassName;
     case OptimizationPass::kInliner:
       return HInliner::kInlinerPassName;
-    case OptimizationPass::kSelectGenerator:
-      return HSelectGenerator::kSelectGeneratorPassName;
+    case OptimizationPass::kControlFlowSimplifier:
+      return HControlFlowSimplifier::kControlFlowSimplifierPassName;
     case OptimizationPass::kAggressiveInstructionSimplifier:
     case OptimizationPass::kInstructionSimplifier:
       return InstructionSimplifier::kInstructionSimplifierPassName;
@@ -149,6 +149,7 @@ OptimizationPass OptimizationPassByName(const std::string& pass_name) {
   X(OptimizationPass::kCodeSinking);
   X(OptimizationPass::kConstantFolding);
   X(OptimizationPass::kConstructorFenceRedundancyElimination);
+  X(OptimizationPass::kControlFlowSimplifier);
   X(OptimizationPass::kDeadCodeElimination);
   X(OptimizationPass::kGlobalValueNumbering);
   X(OptimizationPass::kInductionVarAnalysis);
@@ -159,7 +160,6 @@ OptimizationPass OptimizationPassByName(const std::string& pass_name) {
   X(OptimizationPass::kLoopOptimization);
   X(OptimizationPass::kReferenceTypePropagation);
   X(OptimizationPass::kScheduling);
-  X(OptimizationPass::kSelectGenerator);
   X(OptimizationPass::kSideEffectsAnalysis);
 #ifdef ART_ENABLE_CODEGEN_arm
   X(OptimizationPass::kInstructionSimplifierArm);
@@ -266,8 +266,8 @@ ArenaVector<HOptimization*> ConstructOptimizations(
                                        pass_name);
         break;
       }
-      case OptimizationPass::kSelectGenerator:
-        opt = new (allocator) HSelectGenerator(graph, stats, pass_name);
+      case OptimizationPass::kControlFlowSimplifier:
+        opt = new (allocator) HControlFlowSimplifier(graph, stats, pass_name);
         break;
       case OptimizationPass::kInstructionSimplifier:
         opt = new (allocator) InstructionSimplifier(graph, codegen, stats, pass_name);
diff --git a/compiler/optimizing/optimization.h b/compiler/optimizing/optimization.h
index abc4361b44..0f0a15f7c9 100644
--- a/compiler/optimizing/optimization.h
+++ b/compiler/optimizing/optimization.h
@@ -73,6 +73,7 @@ enum class OptimizationPass {
   kCodeSinking,
   kConstantFolding,
   kConstructorFenceRedundancyElimination,
+  kControlFlowSimplifier,
   kDeadCodeElimination,
   kGlobalValueNumbering,
   kInductionVarAnalysis,
@@ -83,7 +84,6 @@ enum class OptimizationPass {
   kLoopOptimization,
   kReferenceTypePropagation,
   kScheduling,
-  kSelectGenerator,
   kSideEffectsAnalysis,
   kWriteBarrierElimination,
 #ifdef ART_ENABLE_CODEGEN_arm
diff --git a/compiler/optimizing/optimizing_compiler.cc b/compiler/optimizing/optimizing_compiler.cc
index cb9933c9c0..18b9413c22 100644
--- a/compiler/optimizing/optimizing_compiler.cc
+++ b/compiler/optimizing/optimizing_compiler.cc
@@ -35,12 +35,14 @@
 #include "builder.h"
 #include "code_generator.h"
 #include "compiler.h"
+#include "com_android_art_flags.h"
 #include "debug/elf_debug_writer.h"
 #include "debug/method_debug_info.h"
 #include "dex/dex_file_types.h"
 #include "driver/compiled_code_storage.h"
 #include "driver/compiler_options.h"
 #include "driver/dex_compilation_unit.h"
+#include "fast_compiler.h"
 #include "graph_checker.h"
 #include "graph_visualizer.h"
 #include "inliner.h"
@@ -57,10 +59,8 @@
 #include "profiling_info_builder.h"
 #include "reference_type_propagation.h"
 #include "register_allocator_linear_scan.h"
-#include "select_generator.h"
 #include "ssa_builder.h"
 #include "ssa_liveness_analysis.h"
-#include "ssa_phi_elimination.h"
 #include "stack_map_stream.h"
 #include "utils/assembler.h"
 
@@ -668,7 +668,7 @@ void OptimizingCompiler::RunOptimizations(HGraph* graph,
              "reference_type_propagation$after_gvn",
              OptimizationPass::kGlobalValueNumbering),
       // Simplification (TODO: only if GVN occurred).
-      OptDef(OptimizationPass::kSelectGenerator),
+      OptDef(OptimizationPass::kControlFlowSimplifier),
       OptDef(OptimizationPass::kConstantFolding,
              "constant_folding$after_gvn"),
       OptDef(OptimizationPass::kInstructionSimplifier,
@@ -759,6 +759,51 @@ CompiledMethod* OptimizingCompiler::Emit(ArenaAllocator* allocator,
   return compiled_method;
 }
 
+#ifdef ART_USE_RESTRICTED_MODE
+
+// This class acts as a filter and enables gradual enablement of ART Simulator work - we
+// compile (and hence simulate) only limited types of methods.
+class CompilationFilterForRestrictedMode : public HGraphDelegateVisitor {
+ public:
+  explicit CompilationFilterForRestrictedMode(HGraph* graph)
+      : HGraphDelegateVisitor(graph),
+        has_unsupported_instructions_(false) {}
+
+  // Returns true if the graph contains instructions which are not currently supported in
+  // the restricted mode.
+  bool GraphRejected() const { return has_unsupported_instructions_; }
+
+ private:
+  void VisitInstruction(HInstruction*) override {
+    // Currently we don't support compiling methods unless they were annotated with $compile$.
+    RejectGraph();
+  }
+  void RejectGraph() {
+    has_unsupported_instructions_ = true;
+  }
+
+  bool has_unsupported_instructions_;
+};
+
+// Returns whether an ArtMethod, specified by a name, should be compiled. Used in restricted
+// mode.
+//
+// In restricted mode, the simulator will execute only those methods which are compiled; thus
+// this is going to be an effective filter for methods to be simulated.
+//
+// TODO(Simulator): compile and simulate all the methods as in regular host mode.
+bool ShouldMethodBeCompiled(HGraph* graph, const std::string& method_name) {
+  if (method_name.find("$compile$") != std::string::npos) {
+    return true;
+  }
+
+  CompilationFilterForRestrictedMode filter_visitor(graph);
+  filter_visitor.VisitReversePostOrder();
+
+  return !filter_visitor.GraphRejected();
+}
+#endif  // ART_USE_RESTRICTED_MODE
+
 CodeGenerator* OptimizingCompiler::TryCompile(ArenaAllocator* allocator,
                                               ArenaStack* arena_stack,
                                               const DexCompilationUnit& dex_compilation_unit,
@@ -958,6 +1003,17 @@ CodeGenerator* OptimizingCompiler::TryCompile(ArenaAllocator* allocator,
     return nullptr;
   }
 
+#ifdef ART_USE_RESTRICTED_MODE
+  // Check whether the method should be compiled according to the compilation filter. Note: this
+  // relies on a LocationSummary being available for each instruction so should take place after
+  // register allocation does liveness analysis.
+  // TODO(Simulator): support and compile all methods.
+  std::string method_name = dex_file.PrettyMethod(method_idx);
+  if (!ShouldMethodBeCompiled(graph, method_name)) {
+    return nullptr;
+  }
+#endif  // ART_USE_RESTRICTED_MODE
+
   codegen->Compile();
   pass_observer.DumpDisassembly();
 
@@ -977,6 +1033,11 @@ CodeGenerator* OptimizingCompiler::TryCompileIntrinsic(
   const DexFile& dex_file = *dex_compilation_unit.GetDexFile();
   uint32_t method_idx = dex_compilation_unit.GetDexMethodIndex();
 
+  // TODO(Simulator): Reenable compilation of intrinsics.
+#ifdef ART_USE_RESTRICTED_MODE
+  return nullptr;
+#endif  // ART_USE_RESTRICTED_MODE
+
   // Always use the Thumb-2 assembler: some runtime functionality
   // (like implicit stack overflow checks) assume Thumb-2.
   DCHECK_NE(instruction_set, InstructionSet::kArm);
@@ -1149,6 +1210,8 @@ CompiledMethod* OptimizingCompiler::Compile(const dex::CodeItem* code_item,
     }
   }
 
+  // TODO(Simulator): Check for $opt$ in method name and that such method is compiled.
+#ifndef ART_USE_RESTRICTED_MODE
   if (kIsDebugBuild &&
       compiler_options.CompileArtTest() &&
       IsInstructionSetSupported(compiler_options.GetInstructionSet())) {
@@ -1160,6 +1223,7 @@ CompiledMethod* OptimizingCompiler::Compile(const dex::CodeItem* code_item,
     bool shouldCompile = method_name.find("$opt$") != std::string::npos;
     DCHECK_IMPLIES(compiled_method == nullptr, !shouldCompile) << "Didn't compile " << method_name;
   }
+#endif  // #ifndef ART_USE_RESTRICTED_MODE
 
   return compiled_method;
 }
@@ -1222,10 +1286,7 @@ CompiledMethod* OptimizingCompiler::JniCompile(uint32_t access_flags,
                               method,
                               &handles));
       if (codegen != nullptr) {
-        return Emit(&allocator,
-                    codegen.get(),
-                    /*is_intrinsic=*/ true,
-                    /*item=*/ nullptr);
+        return Emit(&allocator, codegen.get(), /*is_intrinsic=*/ true, /*item=*/ nullptr);
       }
     }
   }
@@ -1283,6 +1344,24 @@ bool OptimizingCompiler::JitCompile(Thread* self,
   Runtime* runtime = Runtime::Current();
   ArenaAllocator allocator(runtime->GetJitArenaPool());
 
+  std::vector<uint8_t> debug_info;
+
+  auto create_method_debug_info = [&]() {
+    debug::MethodDebugInfo method_debug_info = {};
+    DCHECK(method_debug_info.custom_name.empty());
+    method_debug_info.dex_file = dex_file;
+    method_debug_info.class_def_index = class_def_idx;
+    method_debug_info.dex_method_index = method_idx;
+    method_debug_info.access_flags = access_flags;
+    method_debug_info.code_item = code_item;
+    method_debug_info.isa = compiler_options.GetInstructionSet();
+    method_debug_info.deduped = false;
+    method_debug_info.is_native_debuggable = compiler_options.GetNativeDebuggable();
+    method_debug_info.is_code_address_text_relative = false;
+    method_debug_info.is_optimized = true;
+    return method_debug_info;
+  };
+
   if (UNLIKELY(method->IsNative())) {
     // Use GenericJniTrampoline for critical native methods in debuggable runtimes. We don't
     // support calling method entry / exit hooks for critical native methods yet.
@@ -1327,27 +1406,16 @@ bool OptimizingCompiler::JitCompile(Thread* self,
     const uint8_t* code = reserved_code.data() + OatQuickMethodHeader::InstructionAlignedSize();
 
     // Add debug info after we know the code location but before we update entry-point.
-    std::vector<uint8_t> debug_info;
     if (compiler_options.GenerateAnyDebugInfo()) {
-      debug::MethodDebugInfo info = {};
+      debug::MethodDebugInfo method_debug_info = create_method_debug_info();
       // Simpleperf relies on art_jni_trampoline to detect jni methods.
-      info.custom_name = "art_jni_trampoline";
-      info.dex_file = dex_file;
-      info.class_def_index = class_def_idx;
-      info.dex_method_index = method_idx;
-      info.access_flags = access_flags;
-      info.code_item = code_item;
-      info.isa = jni_compiled_method.GetInstructionSet();
-      info.deduped = false;
-      info.is_native_debuggable = compiler_options.GetNativeDebuggable();
-      info.is_optimized = true;
-      info.is_code_address_text_relative = false;
-      info.code_address = reinterpret_cast<uintptr_t>(code);
-      info.code_size = jni_compiled_method.GetCode().size();
-      info.frame_size_in_bytes = jni_compiled_method.GetFrameSize();
-      info.code_info = nullptr;
-      info.cfi = jni_compiled_method.GetCfi();
-      debug_info = GenerateJitDebugInfo(info);
+      method_debug_info.custom_name = "art_jni_trampoline";
+      method_debug_info.code_address = reinterpret_cast<uintptr_t>(code);
+      method_debug_info.code_size = jni_compiled_method.GetCode().size();
+      method_debug_info.frame_size_in_bytes = jni_compiled_method.GetFrameSize();
+      method_debug_info.code_info = nullptr;
+      method_debug_info.cfi = jni_compiled_method.GetCfi();
+      debug_info = GenerateJitDebugInfo(method_debug_info);
     }
 
     if (!code_cache->Commit(self,
@@ -1377,113 +1445,175 @@ bool OptimizingCompiler::JitCompile(Thread* self,
   VariableSizedHandleScope handles(self);
 
   std::unique_ptr<CodeGenerator> codegen;
+  std::unique_ptr<FastCompiler> fast_compiler;
+  Handle<mirror::Class> compiling_class = handles.NewHandle(method->GetDeclaringClass());
+  DexCompilationUnit dex_compilation_unit(
+      class_loader,
+      runtime->GetClassLinker(),
+      *dex_file,
+      code_item,
+      class_def_idx,
+      method_idx,
+      access_flags,
+      /*verified_method=*/ nullptr,
+      dex_cache,
+      compiling_class);
   {
-    Handle<mirror::Class> compiling_class = handles.NewHandle(method->GetDeclaringClass());
-    DexCompilationUnit dex_compilation_unit(
-        class_loader,
-        runtime->GetClassLinker(),
-        *dex_file,
-        code_item,
-        class_def_idx,
-        method_idx,
-        access_flags,
-        /*verified_method=*/ nullptr,
-        dex_cache,
-        compiling_class);
-
     // Go to native so that we don't block GC during compilation.
     ScopedThreadSuspension sts(self, ThreadState::kNative);
-    codegen.reset(
-        TryCompile(&allocator,
-                   &arena_stack,
-                   dex_compilation_unit,
-                   method,
-                   compilation_kind,
-                   &handles));
-    if (codegen.get() == nullptr) {
-      return false;
+    if (com::android::art::flags::fast_baseline_compiler() &&
+        compilation_kind == CompilationKind::kBaseline &&
+        !compiler_options.GetDebuggable()) {
+      fast_compiler = FastCompiler::Compile(method,
+                                            &allocator,
+                                            &arena_stack,
+                                            &handles,
+                                            compiler_options,
+                                            dex_compilation_unit);
+    }
+    if (fast_compiler == nullptr) {
+      codegen.reset(
+          TryCompile(&allocator,
+                     &arena_stack,
+                     dex_compilation_unit,
+                     method,
+                     compilation_kind,
+                     &handles));
+      if (codegen.get() == nullptr) {
+        return false;
+      }
     }
   }
 
-  ScopedArenaVector<uint8_t> stack_map = codegen->BuildStackMaps(code_item);
-
-  ArrayRef<const uint8_t> reserved_code;
-  ArrayRef<const uint8_t> reserved_data;
-  if (!code_cache->Reserve(self,
-                           region,
-                           codegen->GetAssembler()->CodeSize(),
-                           stack_map.size(),
-                           /*number_of_roots=*/codegen->GetNumberOfJitRoots(),
-                           method,
-                           /*out*/ &reserved_code,
-                           /*out*/ &reserved_data)) {
-    MaybeRecordStat(compilation_stats_.get(), MethodCompilationStat::kJitOutOfMemoryForCommit);
-    return false;
-  }
-  const uint8_t* code = reserved_code.data() + OatQuickMethodHeader::InstructionAlignedSize();
-  const uint8_t* roots_data = reserved_data.data();
+  if (fast_compiler != nullptr) {
+    ArrayRef<const uint8_t> reserved_code;
+    ArrayRef<const uint8_t> reserved_data;
+    ScopedArenaVector<uint8_t> stack_maps = fast_compiler->BuildStackMaps();
+    if (!code_cache->Reserve(self,
+                             region,
+                             fast_compiler->GetCode().size(),
+                             stack_maps.size(),
+                             fast_compiler->GetNumberOfJitRoots(),
+                             method,
+                             /*out*/ &reserved_code,
+                             /*out*/ &reserved_data)) {
+      MaybeRecordStat(compilation_stats_.get(), MethodCompilationStat::kJitOutOfMemoryForCommit);
+      return false;
+    }
+    const uint8_t* code = reserved_code.data() + OatQuickMethodHeader::InstructionAlignedSize();
+    if (compiler_options.GenerateAnyDebugInfo()) {
+      debug::MethodDebugInfo method_debug_info = create_method_debug_info();
+      method_debug_info.code_address = reinterpret_cast<uintptr_t>(code);
+      method_debug_info.code_size = fast_compiler->GetCode().size();
+      method_debug_info.frame_size_in_bytes = fast_compiler->GetFrameSize();
+      method_debug_info.code_info = stack_maps.size() == 0 ? nullptr : stack_maps.data();
+      method_debug_info.cfi = ArrayRef<const uint8_t>(fast_compiler->GetCfiData());
+      debug_info = GenerateJitDebugInfo(method_debug_info);
+    }
 
-  std::vector<Handle<mirror::Object>> roots;
-  codegen->EmitJitRoots(const_cast<uint8_t*>(codegen->GetAssembler()->CodeBufferBaseAddress()),
+    const uint8_t* roots_data = reserved_data.data();
+    std::vector<Handle<mirror::Object>> roots;
+    fast_compiler->EmitJitRoots(const_cast<uint8_t*>(fast_compiler->GetCode().data()),
+                                roots_data,
+                                &roots);
+    // The root Handle<>s filled by the codegen reference entries in the VariableSizedHandleScope.
+    DCHECK(std::all_of(roots.begin(),
+                       roots.end(),
+                       [&handles](Handle<mirror::Object> root){
+                         return handles.Contains(root.GetReference());
+                       }));
+    ArenaSet<ArtMethod*> cha_single_implementation_list(allocator.Adapter(kArenaAllocCHA));
+    if (!code_cache->Commit(self,
+                            region,
+                            method,
+                            reserved_code,
+                            fast_compiler->GetCode(),
+                            reserved_data,
+                            roots,
+                            ArrayRef<const uint8_t>(stack_maps),
+                            debug_info,
+                            /* is_full_debug_info= */ compiler_options.GetGenerateDebugInfo(),
+                            compilation_kind,
+                            cha_single_implementation_list)) {
+      code_cache->Free(self, region, reserved_code.data(), reserved_data.data());
+      return false;
+    }
+    if (jit_logger != nullptr) {
+      jit_logger->WriteLog(code, fast_compiler->GetCode().size(), method);
+    }
+    VLOG(jit) << "Fast compiled " << method->PrettyMethod();
+  } else {
+    ScopedArenaVector<uint8_t> stack_map = codegen->BuildStackMaps(code_item);
+    ArrayRef<const uint8_t> reserved_code;
+    ArrayRef<const uint8_t> reserved_data;
+    if (!code_cache->Reserve(self,
+                             region,
+                             codegen->GetAssembler()->CodeSize(),
+                             stack_map.size(),
+                             /*number_of_roots=*/codegen->GetNumberOfJitRoots(),
+                             method,
+                             /*out*/ &reserved_code,
+                             /*out*/ &reserved_data)) {
+      MaybeRecordStat(compilation_stats_.get(), MethodCompilationStat::kJitOutOfMemoryForCommit);
+      return false;
+    }
+    const uint8_t* code = reserved_code.data() + OatQuickMethodHeader::InstructionAlignedSize();
+    const uint8_t* roots_data = reserved_data.data();
+
+    std::vector<Handle<mirror::Object>> roots;
+    codegen->EmitJitRoots(const_cast<uint8_t*>(codegen->GetAssembler()->CodeBufferBaseAddress()),
                         roots_data,
                         &roots);
-  // The root Handle<>s filled by the codegen reference entries in the VariableSizedHandleScope.
-  DCHECK(std::all_of(roots.begin(),
-                     roots.end(),
-                     [&handles](Handle<mirror::Object> root){
-                       return handles.Contains(root.GetReference());
-                     }));
-
-  // Add debug info after we know the code location but before we update entry-point.
-  std::vector<uint8_t> debug_info;
-  if (compiler_options.GenerateAnyDebugInfo()) {
-    debug::MethodDebugInfo info = {};
-    DCHECK(info.custom_name.empty());
-    info.dex_file = dex_file;
-    info.class_def_index = class_def_idx;
-    info.dex_method_index = method_idx;
-    info.access_flags = access_flags;
-    info.code_item = code_item;
-    info.isa = codegen->GetInstructionSet();
-    info.deduped = false;
-    info.is_native_debuggable = compiler_options.GetNativeDebuggable();
-    info.is_optimized = true;
-    info.is_code_address_text_relative = false;
-    info.code_address = reinterpret_cast<uintptr_t>(code);
-    info.code_size = codegen->GetAssembler()->CodeSize(),
-    info.frame_size_in_bytes = codegen->GetFrameSize();
-    info.code_info = stack_map.size() == 0 ? nullptr : stack_map.data();
-    info.cfi = ArrayRef<const uint8_t>(*codegen->GetAssembler()->cfi().data());
-    debug_info = GenerateJitDebugInfo(info);
-  }
+    // The root Handle<>s filled by the codegen reference entries in the VariableSizedHandleScope.
+    DCHECK(std::all_of(roots.begin(),
+                       roots.end(),
+                       [&handles](Handle<mirror::Object> root){
+                         return handles.Contains(root.GetReference());
+                       }));
 
-  if (compilation_kind == CompilationKind::kBaseline &&
-      !codegen->GetGraph()->IsUsefulOptimizing()) {
-    compilation_kind = CompilationKind::kOptimized;
-  }
+    // Add debug info after we know the code location but before we update entry-point.
+    if (compiler_options.GenerateAnyDebugInfo()) {
+      debug::MethodDebugInfo method_debug_info = create_method_debug_info();
+      method_debug_info.code_address = reinterpret_cast<uintptr_t>(code);
+      method_debug_info.code_size = codegen->GetAssembler()->CodeSize();
+      method_debug_info.frame_size_in_bytes = codegen->GetFrameSize();
+      method_debug_info.code_info = stack_map.size() == 0 ? nullptr : stack_map.data();
+      method_debug_info.cfi = ArrayRef<const uint8_t>(*codegen->GetAssembler()->cfi().data());
+      debug_info = GenerateJitDebugInfo(method_debug_info);
+    }
 
-  if (!code_cache->Commit(self,
-                          region,
-                          method,
-                          reserved_code,
-                          codegen->GetCode(),
-                          reserved_data,
-                          roots,
-                          ArrayRef<const uint8_t>(stack_map),
-                          debug_info,
-                          /* is_full_debug_info= */ compiler_options.GetGenerateDebugInfo(),
-                          compilation_kind,
-                          codegen->GetGraph()->GetCHASingleImplementationList())) {
-    CHECK_EQ(CodeInfo::HasShouldDeoptimizeFlag(stack_map.data()),
-             codegen->GetGraph()->HasShouldDeoptimizeFlag());
-    code_cache->Free(self, region, reserved_code.data(), reserved_data.data());
-    return false;
+    if (compilation_kind == CompilationKind::kBaseline &&
+        !codegen->GetGraph()->IsUsefulOptimizing()) {
+      // The baseline compilation detected that it has done all the optimizations
+      // that the full compiler would do. Therefore we set the compilation kind to
+      // be `kOptimized`
+      compilation_kind = CompilationKind::kOptimized;
+    }
+
+    if (!code_cache->Commit(self,
+                            region,
+                            method,
+                            reserved_code,
+                            codegen->GetCode(),
+                            reserved_data,
+                            roots,
+                            ArrayRef<const uint8_t>(stack_map),
+                            debug_info,
+                            /* is_full_debug_info= */ compiler_options.GetGenerateDebugInfo(),
+                            compilation_kind,
+                            codegen->GetGraph()->GetCHASingleImplementationList())) {
+      CHECK_EQ(CodeInfo::HasShouldDeoptimizeFlag(stack_map.data()),
+               codegen->GetGraph()->HasShouldDeoptimizeFlag());
+      code_cache->Free(self, region, reserved_code.data(), reserved_data.data());
+      return false;
+    }
+
+    if (jit_logger != nullptr) {
+      jit_logger->WriteLog(code, codegen->GetAssembler()->CodeSize(), method);
+    }
   }
 
   Runtime::Current()->GetJit()->AddMemoryUsage(method, allocator.BytesUsed());
-  if (jit_logger != nullptr) {
-    jit_logger->WriteLog(code, codegen->GetAssembler()->CodeSize(), method);
-  }
 
   if (kArenaAllocatorCountAllocations) {
     codegen.reset();  // Release codegen's ScopedArenaAllocator for memory accounting.
diff --git a/compiler/optimizing/optimizing_compiler_stats.h b/compiler/optimizing/optimizing_compiler_stats.h
index 22c43fc7ad..e6c7cefb16 100644
--- a/compiler/optimizing/optimizing_compiler_stats.h
+++ b/compiler/optimizing/optimizing_compiler_stats.h
@@ -58,7 +58,6 @@ enum class MethodCompilationStat {
   kNotCompiledThrowCatchLoop,
   kNotCompiledAmbiguousArrayOp,
   kNotCompiledHugeMethod,
-  kNotCompiledLargeMethodNoBranches,
   kNotCompiledMalformedOpcode,
   kNotCompiledNoCodegen,
   kNotCompiledPathological,
@@ -79,13 +78,14 @@ enum class MethodCompilationStat {
   kLoopInvariantMoved,
   kLoopVectorized,
   kLoopVectorizedIdiom,
-  kSelectGenerated,
   kRemovedInstanceOf,
   kPropagatedIfValue,
   kInlinedInvokeVirtualOrInterface,
   kInlinedLastInvokeVirtualOrInterface,
   kImplicitNullCheckGenerated,
   kExplicitNullCheckGenerated,
+  kControlFlowSelectGenerated,
+  kControlFlowDiamondRemoved,
   kSimplifyIf,
   kSimplifyIfAddedPhi,
   kSimplifyThrowingInvoke,
@@ -131,10 +131,6 @@ enum class MethodCompilationStat {
   kJitOutOfMemoryForCommit,
   kFullLSEAllocationRemoved,
   kFullLSEPossible,
-  kNonPartialLoadRemoved,
-  kPartialLSEPossible,
-  kPartialStoreRemoved,
-  kPartialAllocationMoved,
   kDevirtualized,
   kLastStat
 };
diff --git a/compiler/optimizing/optimizing_unit_test.h b/compiler/optimizing/optimizing_unit_test.h
index 018ffce196..8dc17f1618 100644
--- a/compiler/optimizing/optimizing_unit_test.h
+++ b/compiler/optimizing/optimizing_unit_test.h
@@ -90,6 +90,11 @@ inline std::ostream& operator<<(std::ostream& os, const InstructionDumper& id) {
 #define ASSERT_INS_REMOVED(a) ASSERT_TRUE(IsRemoved(a)) << "Not removed: " << (InstructionDumper{a})
 #define ASSERT_INS_RETAINED(a) ASSERT_FALSE(IsRemoved(a)) << "Removed: " << (InstructionDumper{a})
 
+#define EXPECT_BLOCK_REMOVED(b) EXPECT_TRUE(IsRemoved(b)) << "Not removed: B" << b->GetBlockId()
+#define EXPECT_BLOCK_RETAINED(b) EXPECT_FALSE(IsRemoved(b)) << "Removed: B" << b->GetBlockId()
+#define ASSERT_BLOCK_REMOVED(b) ASSERT_TRUE(IsRemoved(b)) << "Not removed: B" << b->GetBlockId()
+#define ASSERT_BLOCK_RETAINED(b) ASSERT_FALSE(IsRemoved(b)) << "Removed: B" << b->GetBlockId()
+
 inline LiveInterval* BuildInterval(const size_t ranges[][2],
                                    size_t number_of_ranges,
                                    ScopedArenaAllocator* allocator,
@@ -348,6 +353,8 @@ class OptimizingUnitTestHelper {
   // empty, leaving the construction of an appropriate condition and `HIf` to the caller.
   // Note: The `loop_exit` shall be the "then" successor of the "loop-header". If the `loop_exit`
   // is needed as the "else" successor, use `HBlock::SwapSuccessors()` to adjust the order.
+  // Note: A `do { ... } while (...);` loop pattern has the same block structure, except that
+  // the `loop_body` is a single-goto block that exists purely to avoid a critical edge.
   std::tuple<HBasicBlock*, HBasicBlock*, HBasicBlock*> CreateWhileLoop(HBasicBlock* loop_exit) {
     HBasicBlock* pre_header = AddNewBlock();
     HBasicBlock* loop_header = AddNewBlock();
@@ -367,26 +374,54 @@ class OptimizingUnitTestHelper {
     return {pre_header, loop_header, loop_body};
   }
 
-  // Insert "pre-header" and "loop" blocks before a given `loop_exit` block and connect them in a
-  // `do { ... } while (...);` loop pattern. Return the new blocks. Adds `HGoto` to the "pre-header"
-  // block but leaves the "loop" block empty, leaving the construction of an appropriate condition
-  // and `HIf` to the caller.
-  // Note: The `loop_exit` shall be the "then" successor of the "loop". If the `loop_exit`
-  // is needed as the "else" successor, use `HBlock::SwapSuccessors()` to adjust the order.
-  std::tuple<HBasicBlock*, HBasicBlock*> CreateDoWhileLoop(HBasicBlock* loop_exit) {
-    HBasicBlock* pre_header = AddNewBlock();
-    HBasicBlock* loop = AddNewBlock();
+  // Insert blocks for an irreducible loop before the `loop_exit`:
+  //
+  //      <loop_exit's old predecessor>
+  //                    |
+  //                  split
+  //                 /     \
+  //   left_preheader       right_preheader
+  //         |                     |
+  //    left_header <------- right_header <-+
+  //     |  |                               |
+  //     |  +------------> body ------------+
+  //     |
+  //    loop_exit
+  //
+  // Note that `left_preheader`, `right_preheader` and `body` are needed to avoid critical edges.
+  //
+  // `HGoto` instructions are added to `left_preheader`, `right_preheader`, `body`
+  // and `right_header`. To complete the control flow, the caller should add `HIf`
+  // to `split` and `left_header`.
+  //
+  // Returns `{split, left_header, right_header, body}`.
+  std::tuple<HBasicBlock*, HBasicBlock*, HBasicBlock*, HBasicBlock*> CreateIrreducibleLoop(
+      HBasicBlock* loop_exit) {
+    HBasicBlock* split = AddNewBlock();
+    HBasicBlock* left_preheader = AddNewBlock();
+    HBasicBlock* right_preheader = AddNewBlock();
+    HBasicBlock* left_header = AddNewBlock();
+    HBasicBlock* right_header = AddNewBlock();
+    HBasicBlock* body = AddNewBlock();
 
     HBasicBlock* predecessor = loop_exit->GetSinglePredecessor();
-    predecessor->ReplaceSuccessor(loop_exit, pre_header);
+    predecessor->ReplaceSuccessor(loop_exit, split);
 
-    pre_header->AddSuccessor(loop);
-    loop->AddSuccessor(loop_exit);  // true successor
-    loop->AddSuccessor(loop);  // false successor
+    split->AddSuccessor(left_preheader);  // true successor
+    split->AddSuccessor(right_preheader);  // false successor
+    left_preheader->AddSuccessor(left_header);
+    right_preheader->AddSuccessor(right_header);
+    left_header->AddSuccessor(loop_exit);  // true successor
+    left_header->AddSuccessor(body);  // false successor
+    body->AddSuccessor(right_header);
+    right_header->AddSuccessor(left_header);
 
-    MakeGoto(pre_header);
+    MakeGoto(left_preheader);
+    MakeGoto(right_preheader);
+    MakeGoto(body);
+    MakeGoto(right_header);
 
-    return {pre_header, loop};
+    return {split, left_header, right_header, body};
   }
 
   HBasicBlock* AddNewBlock() {
@@ -411,7 +446,7 @@ class OptimizingUnitTestHelper {
         instruction->GetDexPc(),
         instruction);
 
-    environment->CopyFrom(ArrayRef<HInstruction* const>(*current_locals));
+    environment->CopyFrom(GetAllocator(), ArrayRef<HInstruction* const>(*current_locals));
     instruction->SetRawEnvironment(environment);
     return environment;
   }
@@ -873,6 +908,19 @@ class OptimizingUnitTestHelper {
     return {phi, add};
   }
 
+  std::tuple<HPhi*, HPhi*, HAdd*> MakeLinearIrreducibleLoopVar(HBasicBlock* left_header,
+                                                               HBasicBlock* right_header,
+                                                               HBasicBlock* body,
+                                                               HInstruction* left_initial,
+                                                               HInstruction* right_initial,
+                                                               HInstruction* increment) {
+    HPhi* left_phi = MakePhi(left_header, {left_initial, /* placeholder */ left_initial});
+    HAdd* add = MakeBinOp<HAdd>(body, left_phi->GetType(), left_phi, increment);
+    HPhi* right_phi = MakePhi(right_header, {right_initial, add});
+    left_phi->ReplaceInput(right_phi, 1u);  // Update back-edge input.
+    return {left_phi, right_phi, add};
+  }
+
   dex::TypeIndex DefaultTypeIndexForType(DataType::Type type) {
     switch (type) {
       case DataType::Type::kBool:
@@ -909,6 +957,26 @@ class OptimizingUnitTestHelper {
     return val;
   }
 
+  static bool PredecessorsEqual(HBasicBlock* block,
+                                std::initializer_list<HBasicBlock*> expected) {
+    return RangeEquals(block->GetPredecessors(), expected);
+  }
+
+  static bool InputsEqual(HInstruction* instruction,
+                          std::initializer_list<HInstruction*> expected) {
+    return RangeEquals(instruction->GetInputs(), expected);
+  }
+
+  // Returns if the `instruction` is removed from the graph.
+  static inline bool IsRemoved(HInstruction* instruction) {
+    return instruction->GetBlock() == nullptr;
+  }
+
+  // Returns if the `block` is removed from the graph.
+  static inline bool IsRemoved(HBasicBlock* block) {
+    return block->GetGraph() == nullptr;
+  }
+
  protected:
   bool CheckGraph(HGraph* graph, std::ostream& oss) {
     GraphChecker checker(graph);
@@ -917,6 +985,12 @@ class OptimizingUnitTestHelper {
     return checker.IsValid();
   }
 
+  template <typename Range, typename ElementType>
+  static bool RangeEquals(Range&& range, std::initializer_list<ElementType> expected) {
+    return std::distance(range.begin(), range.end()) == expected.size() &&
+           std::equal(range.begin(), range.end(), expected.begin());
+  }
+
   std::vector<std::unique_ptr<const StandardDexFile>> dex_files_;
   std::unique_ptr<ArenaPoolAndAllocator> pool_and_allocator_;
 
@@ -956,165 +1030,10 @@ inline std::string Patch(const std::string& original, const diff_t& diff) {
   return result;
 }
 
-// Returns if the instruction is removed from the graph.
-inline bool IsRemoved(HInstruction* instruction) {
-  return instruction->GetBlock() == nullptr;
-}
-
 inline std::ostream& operator<<(std::ostream& oss, const AdjacencyListGraph& alg) {
   return alg.Dump(oss);
 }
 
-class PatternMatchGraphVisitor final : public HGraphVisitor {
- private:
-  struct HandlerWrapper {
-   public:
-    virtual ~HandlerWrapper() {}
-    virtual void operator()(HInstruction* h) = 0;
-  };
-
-  template <HInstruction::InstructionKind kKind, typename F>
-  struct KindWrapper;
-
-#define GEN_HANDLER(nm, unused)                                                         \
-  template <typename F>                                                                 \
-  struct KindWrapper<HInstruction::InstructionKind::k##nm, F> : public HandlerWrapper { \
-   public:                                                                              \
-    explicit KindWrapper(F f) : f_(f) {}                                                \
-    void operator()(HInstruction* h) override {                                         \
-      if constexpr (std::is_invocable_v<F, H##nm*>) {                                   \
-        f_(h->As##nm());                                                                \
-      } else {                                                                          \
-        LOG(FATAL) << "Incorrect call with " << #nm;                                    \
-      }                                                                                 \
-    }                                                                                   \
-                                                                                        \
-   private:                                                                             \
-    F f_;                                                                               \
-  };
-
-  FOR_EACH_CONCRETE_INSTRUCTION(GEN_HANDLER)
-#undef GEN_HANDLER
-
-  template <typename F>
-  std::unique_ptr<HandlerWrapper> GetWrapper(HInstruction::InstructionKind kind, F f) {
-    switch (kind) {
-#define GEN_GETTER(nm, unused)               \
-  case HInstruction::InstructionKind::k##nm: \
-    return std::unique_ptr<HandlerWrapper>(  \
-        new KindWrapper<HInstruction::InstructionKind::k##nm, F>(f));
-      FOR_EACH_CONCRETE_INSTRUCTION(GEN_GETTER)
-#undef GEN_GETTER
-      default:
-        LOG(FATAL) << "Unable to handle kind " << kind;
-        return nullptr;
-    }
-  }
-
- public:
-  template <typename... Inst>
-  explicit PatternMatchGraphVisitor(HGraph* graph, Inst... handlers) : HGraphVisitor(graph) {
-    FillHandlers(handlers...);
-  }
-
-  void VisitInstruction(HInstruction* instruction) override {
-    auto& h = handlers_[instruction->GetKind()];
-    if (h.get() != nullptr) {
-      (*h)(instruction);
-    }
-  }
-
- private:
-  template <typename Func>
-  constexpr HInstruction::InstructionKind GetKind() {
-#define CHECK_INST(nm, unused)                       \
-    if constexpr (std::is_invocable_v<Func, H##nm*>) { \
-      return HInstruction::InstructionKind::k##nm;     \
-    }
-    FOR_EACH_CONCRETE_INSTRUCTION(CHECK_INST);
-#undef CHECK_INST
-    static_assert(!std::is_invocable_v<Func, HInstruction*>,
-                  "Use on generic HInstruction not allowed");
-#define STATIC_ASSERT_ABSTRACT(nm, unused) && !std::is_invocable_v<Func, H##nm*>
-    static_assert(true FOR_EACH_ABSTRACT_INSTRUCTION(STATIC_ASSERT_ABSTRACT),
-                  "Must not be abstract instruction");
-#undef STATIC_ASSERT_ABSTRACT
-#define STATIC_ASSERT_CONCRETE(nm, unused) || std::is_invocable_v<Func, H##nm*>
-    static_assert(false FOR_EACH_CONCRETE_INSTRUCTION(STATIC_ASSERT_CONCRETE),
-                  "Must be a concrete instruction");
-#undef STATIC_ASSERT_CONCRETE
-    return HInstruction::InstructionKind::kLastInstructionKind;
-  }
-  template <typename First>
-  void FillHandlers(First h1) {
-    HInstruction::InstructionKind type = GetKind<First>();
-    CHECK_NE(type, HInstruction::kLastInstructionKind)
-        << "Unknown instruction kind. Only concrete ones please.";
-    handlers_[type] = GetWrapper(type, h1);
-  }
-
-  template <typename First, typename... Inst>
-  void FillHandlers(First h1, Inst... handlers) {
-    FillHandlers(h1);
-    FillHandlers<Inst...>(handlers...);
-  }
-
-  std::array<std::unique_ptr<HandlerWrapper>, HInstruction::InstructionKind::kLastInstructionKind>
-      handlers_;
-};
-
-template <typename... Target>
-std::tuple<std::vector<Target*>...> FindAllInstructions(
-    HGraph* graph,
-    std::variant<std::nullopt_t, HBasicBlock*, std::initializer_list<HBasicBlock*>> blks =
-        std::nullopt) {
-  std::tuple<std::vector<Target*>...> res;
-  PatternMatchGraphVisitor vis(
-      graph, [&](Target* t) { std::get<std::vector<Target*>>(res).push_back(t); }...);
-
-  if (std::holds_alternative<std::initializer_list<HBasicBlock*>>(blks)) {
-    for (HBasicBlock* blk : std::get<std::initializer_list<HBasicBlock*>>(blks)) {
-      vis.VisitBasicBlock(blk);
-    }
-  } else if (std::holds_alternative<std::nullopt_t>(blks)) {
-    vis.VisitInsertionOrder();
-  } else {
-    vis.VisitBasicBlock(std::get<HBasicBlock*>(blks));
-  }
-  return res;
-}
-
-template <typename... Target>
-std::tuple<Target*...> FindSingleInstructions(
-    HGraph* graph,
-    std::variant<std::nullopt_t, HBasicBlock*, std::initializer_list<HBasicBlock*>> blks =
-        std::nullopt) {
-  std::tuple<Target*...> res;
-  PatternMatchGraphVisitor vis(graph, [&](Target* t) {
-    EXPECT_EQ(std::get<Target*>(res), nullptr)
-        << *std::get<Target*>(res) << " already found but found " << *t << "!";
-    std::get<Target*>(res) = t;
-  }...);
-  if (std::holds_alternative<std::initializer_list<HBasicBlock*>>(blks)) {
-    for (HBasicBlock* blk : std::get<std::initializer_list<HBasicBlock*>>(blks)) {
-      vis.VisitBasicBlock(blk);
-    }
-  } else if (std::holds_alternative<std::nullopt_t>(blks)) {
-    vis.VisitInsertionOrder();
-  } else {
-    vis.VisitBasicBlock(std::get<HBasicBlock*>(blks));
-  }
-  return res;
-}
-
-template <typename Target>
-Target* FindSingleInstruction(
-    HGraph* graph,
-    std::variant<std::nullopt_t, HBasicBlock*, std::initializer_list<HBasicBlock*>> blks =
-        std::nullopt) {
-  return std::get<Target*>(FindSingleInstructions<Target>(graph, blks));
-}
-
 }  // namespace art
 
 #endif  // ART_COMPILER_OPTIMIZING_OPTIMIZING_UNIT_TEST_H_
diff --git a/compiler/optimizing/prepare_for_register_allocation.cc b/compiler/optimizing/prepare_for_register_allocation.cc
index 1eb340a9b4..c5dbab5f79 100644
--- a/compiler/optimizing/prepare_for_register_allocation.cc
+++ b/compiler/optimizing/prepare_for_register_allocation.cc
@@ -42,7 +42,8 @@ class PrepareForRegisterAllocationVisitor final : public HGraphDelegateVisitor {
   void VisitBoundType(HBoundType* bound_type) override;
   void VisitArraySet(HArraySet* instruction) override;
   void VisitClinitCheck(HClinitCheck* check) override;
-  void VisitCondition(HCondition* condition) override;
+  void VisitIf(HIf* if_instr) override;
+  void VisitSelect(HSelect* select) override;
   void VisitConstructorFence(HConstructorFence* constructor_fence) override;
   void VisitInvokeStaticOrDirect(HInvokeStaticOrDirect* invoke) override;
   void VisitDeoptimize(HDeoptimize* deoptimize) override;
@@ -50,6 +51,7 @@ class PrepareForRegisterAllocationVisitor final : public HGraphDelegateVisitor {
 
   bool CanMoveClinitCheck(HInstruction* input, HInstruction* user) const;
   bool CanEmitConditionAt(HCondition* condition, HInstruction* user) const;
+  void TryToMoveConditionToUser(HInstruction* maybe_condition, HInstruction* user);
 
   const CompilerOptions& compiler_options_;
 };
@@ -108,6 +110,7 @@ void PrepareForRegisterAllocationVisitor::VisitDeoptimize(HDeoptimize* deoptimiz
     deoptimize->ReplaceWith(deoptimize->GuardedInput());
     deoptimize->RemoveGuard();
   }
+  TryToMoveConditionToUser(deoptimize->InputAt(0), deoptimize);
 }
 
 void PrepareForRegisterAllocationVisitor::VisitBoundsCheck(HBoundsCheck* check) {
@@ -206,37 +209,114 @@ void PrepareForRegisterAllocationVisitor::VisitClinitCheck(HClinitCheck* check)
   }
 }
 
-bool PrepareForRegisterAllocationVisitor::CanEmitConditionAt(HCondition* condition,
-                                                             HInstruction* user) const {
-  if (condition->GetNext() != user) {
+// Determine if moving `condition` to `user` would observably extend the lifetime of a reference.
+// By "observably" we understand that the reference would need to be visible to the GC for longer.
+// We're not concerned with the lifetime for the purposes of register allocation here.
+static bool ConditionMoveWouldExtendReferenceLifetime(HCondition* condition, HInstruction* user) {
+  HInstruction* lhs = condition->InputAt(0);
+  if (lhs->GetType() != DataType::Type::kReference) {
+    return false;
+  }
+  HInstruction* rhs = condition->InputAt(1);
+  DCHECK_EQ(rhs->GetType(), DataType::Type::kReference);
+  if (lhs->IsNullConstant() && rhs->IsNullConstant()) {
+    return false;
+  }
+  // Check if the last instruction with environment before `user` has all non-null
+  // inputs in the environment. If so, we would not be extending the lifetime.
+  HInstruction* instruction_with_env = user->GetPrevious();
+  while (instruction_with_env != nullptr &&
+         instruction_with_env != condition &&
+         instruction_with_env->GetEnvironment() == nullptr) {
+    DCHECK(!instruction_with_env->GetSideEffects().Includes(SideEffects::CanTriggerGC()));
+    instruction_with_env = instruction_with_env->GetPrevious();
+  }
+  if (instruction_with_env == nullptr) {
+    // No env use in the user's block. Do not search other blocks. Conservatively assume that
+    // moving the `condition` to the `user` would indeed extend the lifetime of a reference.
+    return true;
+  }
+  if (instruction_with_env == condition) {
+    // There is no instruction with an environment between `condition` and `user`, so moving
+    // the condition before the user shall not observably extend the lifetime of the reference.
     return false;
   }
+  DCHECK(instruction_with_env->HasEnvironment());
+  auto env_inputs = instruction_with_env->GetEnvironment()->GetEnvInputs();
+  auto extends_lifetime = [&](HInstruction* instruction) {
+    return !instruction->IsNullConstant() &&
+           std::find(env_inputs.begin(), env_inputs.end(), instruction) == env_inputs.end();
+  };
+  return extends_lifetime(lhs) || extends_lifetime(rhs);
+}
+
+bool PrepareForRegisterAllocationVisitor::CanEmitConditionAt(HCondition* condition,
+                                                             HInstruction* user) const {
+  DCHECK(user->IsIf() || user->IsDeoptimize() || user->IsSelect());
 
   if (GetGraph()->IsCompilingBaseline() && compiler_options_.ProfileBranches()) {
     // To do branch profiling, we cannot emit conditions at use site.
     return false;
   }
 
-  if (user->IsIf() || user->IsDeoptimize()) {
-    return true;
+  // Move only a single-user `HCondition` to the `user`.
+  if (!condition->HasOnlyOneNonEnvironmentUse()) {
+    return false;
   }
+  DCHECK(condition->GetUses().front().GetUser() == user);
 
-  if (user->IsSelect() && user->AsSelect()->GetCondition() == condition) {
-    return true;
+  if (condition->GetNext() != user) {
+    // Avoid moving across blocks if the graph has any irreducible loops.
+    if (condition->GetBlock() != user->GetBlock() && GetGraph()->HasIrreducibleLoops()) {
+      return false;
+    }
+    // Avoid extending the lifetime of references by moving the condition.
+    if (ConditionMoveWouldExtendReferenceLifetime(condition, user)) {
+      return false;
+    }
   }
 
-  return false;
+  return true;
 }
 
-void PrepareForRegisterAllocationVisitor::VisitCondition(HCondition* condition) {
-  if (condition->HasOnlyOneNonEnvironmentUse()) {
-    HInstruction* user = condition->GetUses().front().GetUser();
-    if (CanEmitConditionAt(condition, user)) {
-      condition->MarkEmittedAtUseSite();
+void PrepareForRegisterAllocationVisitor::TryToMoveConditionToUser(HInstruction* maybe_condition,
+                                                                   HInstruction* user) {
+  DCHECK(user->IsIf() || user->IsDeoptimize() || user->IsSelect());
+  if (maybe_condition->IsCondition() && CanEmitConditionAt(maybe_condition->AsCondition(), user)) {
+    if (maybe_condition->GetNext() != user) {
+      maybe_condition->MoveBefore(user);
+#ifdef ART_ENABLE_CODEGEN_x86
+      for (HInstruction* input : maybe_condition->GetInputs()) {
+        if (input->IsEmittedAtUseSite()) {
+          DCHECK(input->IsX86LoadFromConstantTable());
+          input->MoveBefore(maybe_condition);
+          HInstruction* inputs_input = input->InputAt(0);
+          DCHECK(inputs_input->IsX86ComputeBaseMethodAddress());
+          if (inputs_input->HasOnlyOneNonEnvironmentUse()) {
+            inputs_input->MoveBefore(input);
+          }
+        }
+      }
+#else  // ART_ENABLE_CODEGEN_x86
+      if (kIsDebugBuild) {
+        for (HInstruction* input : maybe_condition->GetInputs()) {
+          CHECK(!input->IsEmittedAtUseSite()) << input->DebugName() << "#" << input->GetId();
+        }
+      }
+#endif
     }
+    maybe_condition->MarkEmittedAtUseSite();
   }
 }
 
+void PrepareForRegisterAllocationVisitor::VisitIf(HIf* if_instr) {
+  TryToMoveConditionToUser(if_instr->InputAt(0), if_instr);
+}
+
+void PrepareForRegisterAllocationVisitor::VisitSelect(HSelect* select) {
+  TryToMoveConditionToUser(select->GetCondition(), select);
+}
+
 void PrepareForRegisterAllocationVisitor::VisitConstructorFence(
     HConstructorFence* constructor_fence) {
   // Trivially remove redundant HConstructorFence when it immediately follows an HNewInstance
diff --git a/compiler/optimizing/prepare_for_register_allocation_test.cc b/compiler/optimizing/prepare_for_register_allocation_test.cc
new file mode 100644
index 0000000000..a5bbae19a2
--- /dev/null
+++ b/compiler/optimizing/prepare_for_register_allocation_test.cc
@@ -0,0 +1,312 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "prepare_for_register_allocation.h"
+
+#include <gtest/gtest.h>
+
+#include "base/macros.h"
+#include "optimizing_unit_test.h"
+
+namespace art HIDDEN {
+
+class PrepareForRegisterAllocationTest
+    : public CommonCompilerTest, public OptimizingUnitTestHelper {
+ protected:
+  void RunPass() {
+    graph_->BuildDominatorTree();
+    PrepareForRegisterAllocation(graph_, *compiler_options_).Run();
+  }
+};
+
+TEST_F(PrepareForRegisterAllocationTest, MergeConditionToSelect) {
+  HBasicBlock* ret = InitEntryMainExitGraphWithReturnVoid();
+
+  HInstruction* param = MakeParam(DataType::Type::kInt32);
+  HInstruction* zero_const = graph_->GetIntConstant(0);
+  HCondition* condition = MakeCondition(ret, kCondLT, param, zero_const);
+  HSelect* select = MakeSelect(ret, condition, zero_const, param);
+
+  RunPass();
+
+  ASSERT_TRUE(condition->IsEmittedAtUseSite());
+  ASSERT_EQ(condition->GetNext(), select);
+}
+
+TEST_F(PrepareForRegisterAllocationTest, MergeConditionToDeoptimize) {
+  HBasicBlock* ret = InitEntryMainExitGraphWithReturnVoid();
+
+  HInstruction* param = MakeParam(DataType::Type::kInt32);
+  HInstruction* zero_const = graph_->GetIntConstant(0);
+  HCondition* condition = MakeCondition(ret, kCondLT, param, zero_const);
+  HDeoptimize* deopt = new (GetAllocator()) HDeoptimize(
+      GetAllocator(), condition, DeoptimizationKind::kAotInlineCache, /*dex_pc=*/ 0u);
+  AddOrInsertInstruction(ret, deopt);
+
+  RunPass();
+
+  ASSERT_TRUE(condition->IsEmittedAtUseSite());
+  ASSERT_EQ(condition->GetNext(), deopt);
+}
+
+TEST_F(PrepareForRegisterAllocationTest, MergeConditionToIf) {
+  HBasicBlock* ret = InitEntryMainExitGraphWithReturnVoid();
+  auto [start, left, right] = CreateDiamondPattern(ret);
+
+  HInstruction* param = MakeParam(DataType::Type::kInt32);
+  HInstruction* zero_const = graph_->GetIntConstant(0);
+  HCondition* condition = MakeCondition(start, kCondLT, param, zero_const);
+  HIf* start_if = MakeIf(start, condition);
+
+  RunPass();
+
+  ASSERT_TRUE(condition->IsEmittedAtUseSite());
+  ASSERT_EQ(condition->GetNext(), start_if);
+}
+
+TEST_F(PrepareForRegisterAllocationTest, MergeConditionToIfWithMove) {
+  HBasicBlock* ret = InitEntryMainExitGraphWithReturnVoid();
+  auto [start, left, right] = CreateDiamondPattern(ret);
+
+  HInstruction* param = MakeParam(DataType::Type::kInt32);
+  HInstruction* zero_const = graph_->GetIntConstant(0);
+  HCondition* condition = MakeCondition(start, kCondLT, param, zero_const);
+  HInstruction* add = MakeBinOp<HAdd>(start, DataType::Type::kInt32, param, param);
+  HIf* start_if = MakeIf(start, condition);
+
+  ASSERT_EQ(condition->GetNext(), add);
+  ASSERT_EQ(add->GetNext(), start_if);
+
+  RunPass();
+
+  ASSERT_TRUE(condition->IsEmittedAtUseSite());
+  ASSERT_EQ(add->GetNext(), condition);
+  ASSERT_EQ(condition->GetNext(), start_if);
+}
+
+TEST_F(PrepareForRegisterAllocationTest, MergeConditionToIfWithMoveFromPredecessor) {
+  HBasicBlock* ret = InitEntryMainExitGraphWithReturnVoid();
+  auto [start, left, right_end] = CreateDiamondPattern(ret);
+  auto [right_start, right_left, right_right] = CreateDiamondPattern(right_end);
+
+  HInstruction* cond_param = MakeParam(DataType::Type::kBool);
+  HInstruction* param = MakeParam(DataType::Type::kInt32);
+  HInstruction* zero_const = graph_->GetIntConstant(0);
+  HCondition* condition = MakeCondition(start, kCondLT, param, zero_const);
+  MakeIf(start, cond_param);
+  // Note: The condition for this `HIf` is in the predecessor block.
+  HIf* right_start_if = MakeIf(right_start, condition);
+
+  ASSERT_NE(condition->GetBlock(), right_start_if->GetBlock());
+
+  RunPass();
+
+  ASSERT_TRUE(condition->IsEmittedAtUseSite());
+  ASSERT_EQ(condition->GetBlock(), right_start_if->GetBlock());
+  ASSERT_EQ(condition->GetNext(), right_start_if);
+}
+
+TEST_F(PrepareForRegisterAllocationTest, MergeConditionPreventedByOtherUse) {
+  HBasicBlock* ret = InitEntryMainExitGraphWithReturnVoid();
+  auto [start, left, right] = CreateDiamondPattern(ret);
+
+  HInstruction* param = MakeParam(DataType::Type::kInt32);
+  HInstruction* zero_const = graph_->GetIntConstant(0);
+  HCondition* condition = MakeCondition(start, kCondLT, param, zero_const);
+  HIf* start_if = MakeIf(start, condition);
+
+  // Other use.
+  MakeBinOp<HAdd>(ret, DataType::Type::kInt32, param, condition);
+
+  RunPass();
+
+  ASSERT_TRUE(!condition->IsEmittedAtUseSite());
+  ASSERT_EQ(condition->GetNext(), start_if);
+}
+
+TEST_F(PrepareForRegisterAllocationTest, MergeConditionPreventedByEnvUse) {
+  HBasicBlock* ret = InitEntryMainExitGraphWithReturnVoid();
+  auto [start, left, right] = CreateDiamondPattern(ret);
+
+  HInstruction* param = MakeParam(DataType::Type::kInt32);
+  HInstruction* zero_const = graph_->GetIntConstant(0);
+  HCondition* condition = MakeCondition(start, kCondLT, param, zero_const);
+  HIf* start_if = MakeIf(start, condition);
+
+  // Environment use.
+  MakeInvokeStatic(ret, DataType::Type::kVoid, /*args=*/ {}, /*env=*/ {condition});
+
+  RunPass();
+
+  ASSERT_TRUE(!condition->IsEmittedAtUseSite());
+  ASSERT_EQ(condition->GetNext(), start_if);
+}
+
+TEST_F(PrepareForRegisterAllocationTest, MergeConditionPrevented_RefNoEnvInBlock) {
+  ScopedObjectAccess soa(Thread::Current());
+  VariableSizedHandleScope vshs(soa.Self());
+  HBasicBlock* ret = InitEntryMainExitGraphWithReturnVoid(&vshs);
+  auto [start, left, right_end] = CreateDiamondPattern(ret);
+  auto [right_start, right_left, right_right] = CreateDiamondPattern(right_end);
+
+  HInstruction* cond_param = MakeParam(DataType::Type::kBool);
+  HInstruction* param = MakeParam(DataType::Type::kReference);
+  HInstruction* null_const = graph_->GetNullConstant();
+  HCondition* condition = MakeCondition(start, kCondEQ, param, null_const);
+  MakeIf(start, cond_param);
+  // Note: The condition for this `HIf` is in the predecessor block.
+  HIf* right_start_if = MakeIf(right_start, condition);
+
+  RunPass();
+
+  ASSERT_TRUE(!condition->IsEmittedAtUseSite());
+  ASSERT_NE(condition->GetBlock(), right_start_if->GetBlock());  // Not moved to the `HIf`.
+}
+
+TEST_F(PrepareForRegisterAllocationTest, MergeCondition_RefsInEnv) {
+  ScopedObjectAccess soa(Thread::Current());
+  VariableSizedHandleScope vshs(soa.Self());
+  HBasicBlock* ret = InitEntryMainExitGraphWithReturnVoid(&vshs);
+  auto [start, left, right_end] = CreateDiamondPattern(ret);
+
+  HInstruction* param1 = MakeParam(DataType::Type::kReference);
+  HInstruction* param2 = MakeParam(DataType::Type::kReference);
+  HCondition* condition = MakeCondition(start, kCondEQ, param1, param2);
+
+  // This invoke's environment already contains `param1` and `param2`, so reordering
+  // the `condition` after the invoke would not extend their lifetime for the purpose of GC.
+  HInvoke* invoke =
+      MakeInvokeStatic(start, DataType::Type::kVoid, /*args=*/ {}, /*env=*/ {param1, param2});
+
+  HIf* start_if = MakeIf(start, condition);
+
+  ASSERT_EQ(condition->GetNext(), invoke);
+  ASSERT_EQ(invoke->GetNext(), start_if);
+
+  RunPass();
+
+  ASSERT_TRUE(condition->IsEmittedAtUseSite());
+  ASSERT_EQ(invoke->GetNext(), condition);
+  ASSERT_EQ(condition->GetNext(), start_if);
+}
+
+TEST_F(PrepareForRegisterAllocationTest, MergeCondition_RefLhsInEnv) {
+  ScopedObjectAccess soa(Thread::Current());
+  VariableSizedHandleScope vshs(soa.Self());
+  HBasicBlock* ret = InitEntryMainExitGraphWithReturnVoid(&vshs);
+  auto [start, left, right_end] = CreateDiamondPattern(ret);
+
+  HInstruction* param = MakeParam(DataType::Type::kReference);
+  HInstruction* null_const = graph_->GetNullConstant();
+  HCondition* condition = MakeCondition(start, kCondEQ, param, null_const);
+
+  // This invoke's environment already contains `param`, so reordering the `condition`
+  // after the invoke would not extend its lifetime for the purpose of GC.
+  HInvoke* invoke = MakeInvokeStatic(start, DataType::Type::kVoid, /*args=*/ {}, /*env=*/ {param});
+
+  HIf* start_if = MakeIf(start, condition);
+
+  ASSERT_EQ(condition->GetNext(), invoke);
+  ASSERT_EQ(invoke->GetNext(), start_if);
+
+  RunPass();
+
+  ASSERT_TRUE(condition->IsEmittedAtUseSite());
+  ASSERT_EQ(invoke->GetNext(), condition);
+  ASSERT_EQ(condition->GetNext(), start_if);
+}
+
+TEST_F(PrepareForRegisterAllocationTest, MergeCondition_RefRhsInEnv) {
+  ScopedObjectAccess soa(Thread::Current());
+  VariableSizedHandleScope vshs(soa.Self());
+  HBasicBlock* ret = InitEntryMainExitGraphWithReturnVoid(&vshs);
+  auto [start, left, right_end] = CreateDiamondPattern(ret);
+
+  HInstruction* param = MakeParam(DataType::Type::kReference);
+  HInstruction* null_const = graph_->GetNullConstant();
+  HCondition* condition = MakeCondition(start, kCondEQ, null_const, param);
+
+  // This invoke's environment already contains `param`, so reordering the `condition`
+  // after the invoke would not extend its lifetime for the purpose of GC.
+  HInvoke* invoke = MakeInvokeStatic(start, DataType::Type::kVoid, /*args=*/ {}, /*env=*/ {param});
+
+  HIf* start_if = MakeIf(start, condition);
+
+  ASSERT_EQ(condition->GetNext(), invoke);
+  ASSERT_EQ(invoke->GetNext(), start_if);
+
+  RunPass();
+
+  ASSERT_TRUE(condition->IsEmittedAtUseSite());
+  ASSERT_EQ(invoke->GetNext(), condition);
+  ASSERT_EQ(condition->GetNext(), start_if);
+}
+
+TEST_F(PrepareForRegisterAllocationTest, MergeConditionPrevented_RefLhsNotInEnv) {
+  ScopedObjectAccess soa(Thread::Current());
+  VariableSizedHandleScope vshs(soa.Self());
+  HBasicBlock* ret = InitEntryMainExitGraphWithReturnVoid(&vshs);
+  auto [start, left, right_end] = CreateDiamondPattern(ret);
+
+  HInstruction* param1 = MakeParam(DataType::Type::kReference);
+  HInstruction* param2 = MakeParam(DataType::Type::kReference);
+  HCondition* condition = MakeCondition(start, kCondEQ, param1, param2);
+
+  // This invoke's environment does not contain `param1`, so reordering the `condition`
+  // after the invoke would need to extend the lifetime of `param1` for the purpose of GC.
+  // We do not want to extend lifetime of references, therefore the optimization is skipped.
+  HInvoke* invoke = MakeInvokeStatic(start, DataType::Type::kVoid, /*args=*/ {}, /*env=*/ {param2});
+
+  HIf* start_if = MakeIf(start, condition);
+
+  ASSERT_EQ(condition->GetNext(), invoke);
+  ASSERT_EQ(invoke->GetNext(), start_if);
+
+  RunPass();
+
+  ASSERT_TRUE(!condition->IsEmittedAtUseSite());
+  ASSERT_EQ(condition->GetNext(), invoke);
+  ASSERT_EQ(invoke->GetNext(), start_if);
+}
+
+TEST_F(PrepareForRegisterAllocationTest, MergeConditionPrevented_RefRhsNotInEnv) {
+  ScopedObjectAccess soa(Thread::Current());
+  VariableSizedHandleScope vshs(soa.Self());
+  HBasicBlock* ret = InitEntryMainExitGraphWithReturnVoid(&vshs);
+  auto [start, left, right_end] = CreateDiamondPattern(ret);
+
+  HInstruction* param1 = MakeParam(DataType::Type::kReference);
+  HInstruction* param2 = MakeParam(DataType::Type::kReference);
+  HCondition* condition = MakeCondition(start, kCondEQ, param1, param2);
+
+  // This invoke's environment does not contain `param2`, so reordering the `condition`
+  // after the invoke would need to extend the lifetime of `param2` for the purpose of GC.
+  // We do not want to extend lifetime of references, therefore the optimization is skipped.
+  HInvoke* invoke = MakeInvokeStatic(start, DataType::Type::kVoid, /*args=*/ {}, /*env=*/ {param1});
+
+  HIf* start_if = MakeIf(start, condition);
+
+  ASSERT_EQ(condition->GetNext(), invoke);
+  ASSERT_EQ(invoke->GetNext(), start_if);
+
+  RunPass();
+
+  ASSERT_TRUE(!condition->IsEmittedAtUseSite());
+  ASSERT_EQ(condition->GetNext(), invoke);
+  ASSERT_EQ(invoke->GetNext(), start_if);
+}
+
+}  // namespace art
diff --git a/compiler/optimizing/reference_type_propagation.cc b/compiler/optimizing/reference_type_propagation.cc
index 1eef0ce6d5..6d74b0c3f7 100644
--- a/compiler/optimizing/reference_type_propagation.cc
+++ b/compiler/optimizing/reference_type_propagation.cc
@@ -218,7 +218,7 @@ static void BoundTypeIn(HInstruction* receiver,
           : start_block->GetFirstInstruction();
       if (ShouldCreateBoundType(
             insert_point, receiver, class_rti, start_instruction, start_block)) {
-        bound_type = new (receiver->GetBlock()->GetGraph()->GetAllocator()) HBoundType(receiver);
+        bound_type = new (start_block->GetGraph()->GetAllocator()) HBoundType(receiver);
         bound_type->SetUpperBound(class_rti, /* can_be_null= */ false);
         start_block->InsertInstructionBefore(bound_type, insert_point);
         // To comply with the RTP algorithm, don't type the bound type just yet, it will
@@ -276,13 +276,9 @@ static void BoundTypeForClassCheck(HInstruction* check) {
     return;
   }
 
-  {
-    ScopedObjectAccess soa(Thread::Current());
-    ArtField* field = GetClassRoot<mirror::Object>()->GetInstanceField(0);
-    DCHECK_EQ(std::string(field->GetName()), "shadow$_klass_");
-    if (field_get->GetFieldInfo().GetField() != field) {
-      return;
-    }
+  if (field_get->AsInstanceFieldGet()->GetFieldInfo().GetField() !=
+          WellKnownClasses::java_lang_Object_shadowKlass) {
+    return;
   }
 
   if (check->IsIf()) {
@@ -321,7 +317,7 @@ void ReferenceTypePropagation::RTPVisitor::VisitBasicBlock(HBasicBlock* block) {
 
   // Handle instructions. Since RTP may add HBoundType instructions just after the
   // last visited instruction, use `HInstructionIteratorHandleChanges` iterator.
-  VisitNonPhiInstructions(block);
+  VisitNonPhiInstructionsHandleChanges(block);
 
   // Add extra nodes to bound types.
   BoundTypeForIfNotNull(block);
diff --git a/compiler/optimizing/reference_type_propagation_test.cc b/compiler/optimizing/reference_type_propagation_test.cc
index 0e0acd11c2..f720b8d911 100644
--- a/compiler/optimizing/reference_type_propagation_test.cc
+++ b/compiler/optimizing/reference_type_propagation_test.cc
@@ -496,16 +496,16 @@ TEST_P(LoopReferenceTypePropagationTestGroup, RunVisitTest) {
 
 INSTANTIATE_TEST_SUITE_P(ReferenceTypePropagationTest,
                          LoopReferenceTypePropagationTestGroup,
-                         testing::Combine(testing::Values(ShuffleOrder::kAlmostTopological,
-                                                          ShuffleOrder::kReverseTopological,
-                                                          ShuffleOrder::kTopological,
-                                                          ShuffleOrder::kRandom),
-                                          testing::Values(-1, 10, 40),
-                                          testing::Values(0, 1),
-                                          testing::Values(InitialNullState::kAllNonNull,
-                                                          InitialNullState::kAllNull,
-                                                          InitialNullState::kHalfNull,
-                                                          InitialNullState::kRandom)));
+                         ::testing::Combine(::testing::Values(ShuffleOrder::kAlmostTopological,
+                                                              ShuffleOrder::kReverseTopological,
+                                                              ShuffleOrder::kTopological,
+                                                              ShuffleOrder::kRandom),
+                                            ::testing::Values(-1, 10, 40),
+                                            ::testing::Values(0, 1),
+                                            ::testing::Values(InitialNullState::kAllNonNull,
+                                                              InitialNullState::kAllNull,
+                                                              InitialNullState::kHalfNull,
+                                                              InitialNullState::kRandom)));
 
 TEST_P(NonLoopReferenceTypePropagationTestGroup, RunVisitTest) {
   RunVisitListTest([&](std::vector<HInstruction*>& lst) { MutateList(lst, GetParam()); });
@@ -513,9 +513,9 @@ TEST_P(NonLoopReferenceTypePropagationTestGroup, RunVisitTest) {
 
 INSTANTIATE_TEST_SUITE_P(ReferenceTypePropagationTest,
                          NonLoopReferenceTypePropagationTestGroup,
-                         testing::Values(ShuffleOrder::kAlmostTopological,
-                                         ShuffleOrder::kReverseTopological,
-                                         ShuffleOrder::kTopological,
-                                         ShuffleOrder::kRandom));
+                         ::testing::Values(ShuffleOrder::kAlmostTopological,
+                                           ShuffleOrder::kReverseTopological,
+                                           ShuffleOrder::kTopological,
+                                           ShuffleOrder::kRandom));
 
 }  // namespace art
diff --git a/compiler/optimizing/register_allocation_resolver.cc b/compiler/optimizing/register_allocation_resolver.cc
index a4b1698b8d..e847755978 100644
--- a/compiler/optimizing/register_allocation_resolver.cc
+++ b/compiler/optimizing/register_allocation_resolver.cc
@@ -155,8 +155,8 @@ void RegisterAllocationResolver::Resolve(ArrayRef<HInstruction* const> safepoint
       // Instructions live at the top of catch blocks or irreducible loop header
       // were forced to spill.
       if (kIsDebugBuild) {
-        BitVector* live = liveness_.GetLiveInSet(*block);
-        for (uint32_t idx : live->Indexes()) {
+        BitVectorView<size_t> live = liveness_.GetLiveInSet(*block);
+        for (uint32_t idx : live.Indexes()) {
           LiveInterval* interval = liveness_.GetInstructionFromSsaIndex(idx)->GetLiveInterval();
           LiveInterval* sibling = interval->GetSiblingAt(block->GetLifetimeStart());
           // `GetSiblingAt` returns the sibling that contains a position, but there could be
@@ -168,8 +168,8 @@ void RegisterAllocationResolver::Resolve(ArrayRef<HInstruction* const> safepoint
         }
       }
     } else {
-      BitVector* live = liveness_.GetLiveInSet(*block);
-      for (uint32_t idx : live->Indexes()) {
+      BitVectorView<size_t> live = liveness_.GetLiveInSet(*block);
+      for (uint32_t idx : live.Indexes()) {
         LiveInterval* interval = liveness_.GetInstructionFromSsaIndex(idx)->GetLiveInterval();
         for (HBasicBlock* predecessor : block->GetPredecessors()) {
           ConnectSplitSiblings(interval, predecessor, block);
diff --git a/compiler/optimizing/register_allocator_linear_scan.cc b/compiler/optimizing/register_allocator_linear_scan.cc
index 35a0ab404e..d75f1b8b46 100644
--- a/compiler/optimizing/register_allocator_linear_scan.cc
+++ b/compiler/optimizing/register_allocator_linear_scan.cc
@@ -595,6 +595,7 @@ void RegisterAllocatorLinearScan::DumpAllIntervals(std::ostream& stream) const {
 
 // By the book implementation of a linear scan register allocator.
 void RegisterAllocatorLinearScan::LinearScan() {
+  size_t last_position = std::numeric_limits<size_t>::max();
   while (!unhandled_->empty()) {
     // (1) Remove interval with the lowest start position from unhandled.
     LiveInterval* current = unhandled_->back();
@@ -612,49 +613,64 @@ void RegisterAllocatorLinearScan::LinearScan() {
            !unhandled_->back()->IsHighInterval());
 
     size_t position = current->GetStart();
+    if (position != last_position) {
+      // Remember the inactive_ size here since the ones moved to inactive_ from
+      // active_ below shouldn't need to be re-checked.
+      size_t inactive_intervals_to_handle = inactive_.size();
+
+      // (2) Remove currently active intervals that are dead at this position.
+      //     Move active intervals that have a lifetime hole at this position
+      //     to inactive.
+      auto active_kept_end = std::remove_if(
+          active_.begin(),
+          active_.end(),
+          [this, position](LiveInterval* interval) {
+            if (interval->IsDeadAt(position)) {
+              handled_.push_back(interval);
+              return true;
+            } else if (!interval->Covers(position)) {
+              inactive_.push_back(interval);
+              return true;
+            } else {
+              return false;  // Keep this interval.
+            }
+          });
+      active_.erase(active_kept_end, active_.end());
+
+      // (3) Remove currently inactive intervals that are dead at this position.
+      //     Move inactive intervals that cover this position to active.
+      auto inactive_to_handle_end = inactive_.begin() + inactive_intervals_to_handle;
+      auto inactive_kept_end = std::remove_if(
+          inactive_.begin(),
+          inactive_to_handle_end,
+          [this, position](LiveInterval* interval) {
+            DCHECK(interval->GetStart() < position || interval->IsFixed());
+            if (interval->IsDeadAt(position)) {
+              handled_.push_back(interval);
+              return true;
+            } else if (interval->Covers(position)) {
+              active_.push_back(interval);
+              return true;
+            } else {
+              return false;  // Keep this interval.
+            }
+          });
+      inactive_.erase(inactive_kept_end, inactive_to_handle_end);
 
-    // Remember the inactive_ size here since the ones moved to inactive_ from
-    // active_ below shouldn't need to be re-checked.
-    size_t inactive_intervals_to_handle = inactive_.size();
-
-    // (2) Remove currently active intervals that are dead at this position.
-    //     Move active intervals that have a lifetime hole at this position
-    //     to inactive.
-    auto active_kept_end = std::remove_if(
-        active_.begin(),
-        active_.end(),
-        [this, position](LiveInterval* interval) {
-          if (interval->IsDeadAt(position)) {
-            handled_.push_back(interval);
-            return true;
-          } else if (!interval->Covers(position)) {
-            inactive_.push_back(interval);
-            return true;
-          } else {
-            return false;  // Keep this interval.
-          }
-        });
-    active_.erase(active_kept_end, active_.end());
-
-    // (3) Remove currently inactive intervals that are dead at this position.
-    //     Move inactive intervals that cover this position to active.
-    auto inactive_to_handle_end = inactive_.begin() + inactive_intervals_to_handle;
-    auto inactive_kept_end = std::remove_if(
-        inactive_.begin(),
-        inactive_to_handle_end,
-        [this, position](LiveInterval* interval) {
-          DCHECK(interval->GetStart() < position || interval->IsFixed());
-          if (interval->IsDeadAt(position)) {
-            handled_.push_back(interval);
-            return true;
-          } else if (interval->Covers(position)) {
-            active_.push_back(interval);
-            return true;
-          } else {
-            return false;  // Keep this interval.
-          }
-        });
-    inactive_.erase(inactive_kept_end, inactive_to_handle_end);
+      last_position = position;
+    } else {
+      // Active and inactive intervals should not change for the same position.
+      DCHECK(std::none_of(active_.begin(),
+                          active_.end(),
+                          [position](LiveInterval* interval) {
+                            return interval->IsDeadAt(position) || !interval->Covers(position);
+                          }));
+      DCHECK(std::none_of(inactive_.begin(),
+                          inactive_.end(),
+                          [position](LiveInterval* interval) {
+                            return interval->IsDeadAt(position) || interval->Covers(position);
+                          }));
+    }
 
     if (current->IsHighInterval() && !current->GetLowInterval()->HasRegister()) {
       DCHECK(!current->HasRegister());
diff --git a/compiler/optimizing/scheduler.cc b/compiler/optimizing/scheduler.cc
index f4cf7b0a49..9deb37d106 100644
--- a/compiler/optimizing/scheduler.cc
+++ b/compiler/optimizing/scheduler.cc
@@ -118,7 +118,7 @@ static bool IsFieldAccess(const HInstruction* instruction) {
 }
 
 static const FieldInfo* GetFieldInfo(const HInstruction* instruction) {
-  return &instruction->GetFieldInfo();
+  return &instruction->AsFieldAccess()->GetFieldInfo();
 }
 
 size_t SideEffectDependencyAnalysis::MemoryDependencyAnalysis::FieldAccessHeapLocation(
diff --git a/compiler/optimizing/scheduler_test.cc b/compiler/optimizing/scheduler_test.cc
index 7003bd2715..6359119a87 100644
--- a/compiler/optimizing/scheduler_test.cc
+++ b/compiler/optimizing/scheduler_test.cc
@@ -118,9 +118,9 @@ class SchedulerTest : public CommonCompilerTest, public OptimizingUnitTestHelper
                                                      div_check);
     div_check->SetRawEnvironment(environment);
     environment->SetRawEnvAt(0, add2);
-    add2->AddEnvUseAt(div_check->GetEnvironment(), 0);
+    add2->AddEnvUseAt(GetAllocator(), div_check->GetEnvironment(), 0);
     environment->SetRawEnvAt(1, mul);
-    mul->AddEnvUseAt(div_check->GetEnvironment(), 1);
+    mul->AddEnvUseAt(GetAllocator(), div_check->GetEnvironment(), 1);
 
     TestSchedulingGraph scheduling_graph(GetScopedAllocator());
     // Instructions must be inserted in reverse order into the scheduling graph.
diff --git a/compiler/optimizing/select_generator_test.cc b/compiler/optimizing/select_generator_test.cc
deleted file mode 100644
index 9cd6604d34..0000000000
--- a/compiler/optimizing/select_generator_test.cc
+++ /dev/null
@@ -1,74 +0,0 @@
-/*
- * Copyright (C) 2018 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "select_generator.h"
-
-#include "base/arena_allocator.h"
-#include "base/macros.h"
-#include "builder.h"
-#include "nodes.h"
-#include "optimizing_unit_test.h"
-#include "side_effects_analysis.h"
-
-namespace art HIDDEN {
-
-class SelectGeneratorTest : public OptimizingUnitTest {
- protected:
-  HPhi* ConstructBasicGraphForSelect(HBasicBlock* return_block, HInstruction* instr) {
-    HParameterValue* bool_param = MakeParam(DataType::Type::kBool);
-    HIntConstant* const1 =  graph_->GetIntConstant(1);
-
-    auto [if_block, then_block, else_block] = CreateDiamondPattern(return_block, bool_param);
-
-    AddOrInsertInstruction(then_block, instr);
-    HPhi* phi = MakePhi(return_block, {instr, const1});
-    return phi;
-  }
-
-  bool CheckGraphAndTrySelectGenerator() {
-    graph_->BuildDominatorTree();
-    EXPECT_TRUE(CheckGraph());
-
-    SideEffectsAnalysis side_effects(graph_);
-    side_effects.Run();
-    return HSelectGenerator(graph_, /*handles*/ nullptr, /*stats*/ nullptr).Run();
-  }
-};
-
-// HDivZeroCheck might throw and should not be hoisted from the conditional to an unconditional.
-TEST_F(SelectGeneratorTest, testZeroCheck) {
-  HBasicBlock* return_block = InitEntryMainExitGraphWithReturnVoid();
-  HParameterValue* param = MakeParam(DataType::Type::kInt32);
-  HDivZeroCheck* instr = new (GetAllocator()) HDivZeroCheck(param, 0);
-  HPhi* phi = ConstructBasicGraphForSelect(return_block, instr);
-
-  ManuallyBuildEnvFor(instr, {param, graph_->GetIntConstant(1)});
-
-  EXPECT_FALSE(CheckGraphAndTrySelectGenerator());
-  EXPECT_FALSE(phi->GetBlock() == nullptr);
-}
-
-// Test that SelectGenerator succeeds with HAdd.
-TEST_F(SelectGeneratorTest, testAdd) {
-  HBasicBlock* return_block = InitEntryMainExitGraphWithReturnVoid();
-  HParameterValue* param = MakeParam(DataType::Type::kInt32);
-  HAdd* instr = new (GetAllocator()) HAdd(DataType::Type::kInt32, param, param, /*dex_pc=*/ 0);
-  HPhi* phi = ConstructBasicGraphForSelect(return_block, instr);
-  EXPECT_TRUE(CheckGraphAndTrySelectGenerator());
-  EXPECT_TRUE(phi->GetBlock() == nullptr);
-}
-
-}  // namespace art
diff --git a/compiler/optimizing/ssa_liveness_analysis.cc b/compiler/optimizing/ssa_liveness_analysis.cc
index 317e0999d7..8d727a660a 100644
--- a/compiler/optimizing/ssa_liveness_analysis.cc
+++ b/compiler/optimizing/ssa_liveness_analysis.cc
@@ -105,7 +105,7 @@ void SsaLivenessAnalysis::ComputeLiveness() {
 
 void SsaLivenessAnalysis::RecursivelyProcessInputs(HInstruction* current,
                                                    HInstruction* actual_user,
-                                                   BitVector* live_in) {
+                                                   BitVectorView<size_t> live_in) {
   HInputsRef inputs = current->GetInputs();
   for (size_t i = 0; i < inputs.size(); ++i) {
     HInstruction* input = inputs[i];
@@ -121,7 +121,7 @@ void SsaLivenessAnalysis::RecursivelyProcessInputs(HInstruction* current,
       // `input` generates a result used by `current`. Add use and update
       // the live-in set.
       input->GetLiveInterval()->AddUse(current, /* environment= */ nullptr, i, actual_user);
-      live_in->SetBit(input->GetSsaIndex());
+      live_in.SetBit(input->GetSsaIndex());
     } else if (has_out_location) {
       // `input` generates a result but it is not used by `current`.
     } else {
@@ -139,7 +139,7 @@ void SsaLivenessAnalysis::RecursivelyProcessInputs(HInstruction* current,
 
 void SsaLivenessAnalysis::ProcessEnvironment(HInstruction* current,
                                              HInstruction* actual_user,
-                                             BitVector* live_in) {
+                                             BitVectorView<size_t> live_in) {
   for (HEnvironment* environment = current->GetEnvironment();
        environment != nullptr;
        environment = environment->GetParent()) {
@@ -155,7 +155,7 @@ void SsaLivenessAnalysis::ProcessEnvironment(HInstruction* current,
       // affect the live range of that instruction.
       if (should_be_live) {
         CHECK(instruction->HasSsaIndex()) << instruction->DebugName();
-        live_in->SetBit(instruction->GetSsaIndex());
+        live_in.SetBit(instruction->GetSsaIndex());
         instruction->GetLiveInterval()->AddUse(current,
                                                environment,
                                                i,
@@ -169,13 +169,13 @@ void SsaLivenessAnalysis::ComputeLiveRanges() {
   // Do a post order visit, adding inputs of instructions live in the block where
   // that instruction is defined, and killing instructions that are being visited.
   for (HBasicBlock* block : ReverseRange(graph_->GetLinearOrder())) {
-    BitVector* kill = GetKillSet(*block);
-    BitVector* live_in = GetLiveInSet(*block);
+    BitVectorView kill = GetKillSet(*block);
+    BitVectorView live_in = GetLiveInSet(*block);
 
     // Set phi inputs of successors of this block corresponding to this block
     // as live_in.
     for (HBasicBlock* successor : block->GetSuccessors()) {
-      live_in->Union(GetLiveInSet(*successor));
+      live_in.Union(GetLiveInSet(*successor));
       if (successor->IsCatchBlock()) {
         // Inputs of catch phis will be kept alive through their environment
         // uses, allowing the runtime to copy their values to the corresponding
@@ -193,14 +193,14 @@ void SsaLivenessAnalysis::ComputeLiveRanges() {
           input->GetLiveInterval()->AddPhiUse(phi, phi_input_index, block);
           // A phi input whose last user is the phi dies at the end of the predecessor block,
           // and not at the phi's lifetime position.
-          live_in->SetBit(input->GetSsaIndex());
+          live_in.SetBit(input->GetSsaIndex());
         }
       }
     }
 
     // Add a range that covers this block to all instructions live_in because of successors.
     // Instructions defined in this block will have their start of the range adjusted.
-    for (uint32_t idx : live_in->Indexes()) {
+    for (uint32_t idx : live_in.Indexes()) {
       HInstruction* current = GetInstructionFromSsaIndex(idx);
       current->GetLiveInterval()->AddRange(block->GetLifetimeStart(), block->GetLifetimeEnd());
     }
@@ -210,8 +210,8 @@ void SsaLivenessAnalysis::ComputeLiveRanges() {
       HInstruction* current = back_it.Current();
       if (current->HasSsaIndex()) {
         // Kill the instruction and shorten its interval.
-        kill->SetBit(current->GetSsaIndex());
-        live_in->ClearBit(current->GetSsaIndex());
+        kill.SetBit(current->GetSsaIndex());
+        live_in.ClearBit(current->GetSsaIndex());
         current->GetLiveInterval()->SetFrom(current->GetLifetimePosition());
       }
 
@@ -245,8 +245,8 @@ void SsaLivenessAnalysis::ComputeLiveRanges() {
     for (HInstructionIterator inst_it(block->GetPhis()); !inst_it.Done(); inst_it.Advance()) {
       HInstruction* current = inst_it.Current();
       if (current->HasSsaIndex()) {
-        kill->SetBit(current->GetSsaIndex());
-        live_in->ClearBit(current->GetSsaIndex());
+        kill.SetBit(current->GetSsaIndex());
+        live_in.ClearBit(current->GetSsaIndex());
         LiveInterval* interval = current->GetLiveInterval();
         DCHECK((interval->GetFirstRange() == nullptr)
                || (interval->GetStart() == current->GetLifetimePosition()));
@@ -261,7 +261,7 @@ void SsaLivenessAnalysis::ComputeLiveRanges() {
       size_t last_position = block->GetLoopInformation()->GetLifetimeEnd();
       // For all live_in instructions at the loop header, we need to create a range
       // that covers the full loop.
-      for (uint32_t idx : live_in->Indexes()) {
+      for (uint32_t idx : live_in.Indexes()) {
         HInstruction* current = GetInstructionFromSsaIndex(idx);
         current->GetLiveInterval()->AddLoopRange(block->GetLifetimeStart(), last_position);
       }
@@ -289,26 +289,41 @@ void SsaLivenessAnalysis::ComputeLiveInAndLiveOutSets() {
 }
 
 bool SsaLivenessAnalysis::UpdateLiveOut(const HBasicBlock& block) {
-  BitVector* live_out = GetLiveOutSet(block);
+  BitVectorView<size_t> live_out = GetLiveOutSet(block);
   bool changed = false;
   // The live_out set of a block is the union of live_in sets of its successors.
   for (HBasicBlock* successor : block.GetSuccessors()) {
-    if (live_out->Union(GetLiveInSet(*successor))) {
+    if (live_out.Union(GetLiveInSet(*successor))) {
       changed = true;
     }
   }
   return changed;
 }
 
-
 bool SsaLivenessAnalysis::UpdateLiveIn(const HBasicBlock& block) {
-  BitVector* live_out = GetLiveOutSet(block);
-  BitVector* kill = GetKillSet(block);
-  BitVector* live_in = GetLiveInSet(block);
+  BitVectorView<size_t> live_out = GetLiveOutSet(block);
+  BitVectorView<size_t> kill = GetKillSet(block);
+  BitVectorView<size_t> live_in = GetLiveInSet(block);
   // If live_out is updated (because of backward branches), we need to make
   // sure instructions in live_out are also in live_in, unless they are killed
   // by this block.
-  return live_in->UnionIfNotIn(live_out, kill);
+  return live_in.UnionIfNotIn(live_out, kill);
+}
+
+void SsaLivenessAnalysis::DoCheckNoLiveInIrreducibleLoop(const HBasicBlock& block) const {
+  DCHECK(block.IsLoopHeader());
+  DCHECK(block.GetLoopInformation()->IsIrreducible());
+  BitVectorView<size_t> live_in = GetLiveInSet(block);
+  // To satisfy our liveness algorithm, we need to ensure loop headers of
+  // irreducible loops do not have any live-in instructions, except constants
+  // and the current method, which can be trivially re-materialized.
+  for (uint32_t idx : live_in.Indexes()) {
+    HInstruction* instruction = GetInstructionFromSsaIndex(idx);
+    DCHECK(instruction->GetBlock()->IsEntryBlock()) << instruction->DebugName();
+    DCHECK(!instruction->IsParameterValue());
+    DCHECK(instruction->IsCurrentMethod() || instruction->IsConstant())
+        << instruction->DebugName();
+  }
 }
 
 void LiveInterval::DumpWithContext(std::ostream& stream,
diff --git a/compiler/optimizing/ssa_liveness_analysis.h b/compiler/optimizing/ssa_liveness_analysis.h
index e9422edb15..5a6ad84915 100644
--- a/compiler/optimizing/ssa_liveness_analysis.h
+++ b/compiler/optimizing/ssa_liveness_analysis.h
@@ -19,6 +19,8 @@
 
 #include <iostream>
 
+#include "base/arena_bit_vector.h"
+#include "base/bit_vector.h"
 #include "base/intrusive_forward_list.h"
 #include "base/iteration_range.h"
 #include "base/macros.h"
@@ -37,17 +39,20 @@ class BlockInfo : public ArenaObject<kArenaAllocSsaLiveness> {
  public:
   BlockInfo(ScopedArenaAllocator* allocator, const HBasicBlock& block, size_t number_of_ssa_values)
       : block_(block),
-        live_in_(allocator, number_of_ssa_values, false, kArenaAllocSsaLiveness),
-        live_out_(allocator, number_of_ssa_values, false, kArenaAllocSsaLiveness),
-        kill_(allocator, number_of_ssa_values, false, kArenaAllocSsaLiveness) {
+        live_in_(ArenaBitVector::CreateFixedSize(
+            allocator, number_of_ssa_values, kArenaAllocSsaLiveness)),
+        live_out_(ArenaBitVector::CreateFixedSize(
+            allocator, number_of_ssa_values, kArenaAllocSsaLiveness)),
+        kill_(ArenaBitVector::CreateFixedSize(
+            allocator, number_of_ssa_values, kArenaAllocSsaLiveness)) {
     UNUSED(block_);
   }
 
  private:
   const HBasicBlock& block_;
-  ArenaBitVector live_in_;
-  ArenaBitVector live_out_;
-  ArenaBitVector kill_;
+  BitVectorView<size_t> live_in_;
+  BitVectorView<size_t> live_out_;
+  BitVectorView<size_t> kill_;
 
   friend class SsaLivenessAnalysis;
 
@@ -1184,16 +1189,16 @@ class SsaLivenessAnalysis : public ValueObject {
 
   void Analyze();
 
-  BitVector* GetLiveInSet(const HBasicBlock& block) const {
-    return &block_infos_[block.GetBlockId()]->live_in_;
+  BitVectorView<size_t> GetLiveInSet(const HBasicBlock& block) const {
+    return block_infos_[block.GetBlockId()]->live_in_;
   }
 
-  BitVector* GetLiveOutSet(const HBasicBlock& block) const {
-    return &block_infos_[block.GetBlockId()]->live_out_;
+  BitVectorView<size_t> GetLiveOutSet(const HBasicBlock& block) const {
+    return block_infos_[block.GetBlockId()]->live_out_;
   }
 
-  BitVector* GetKillSet(const HBasicBlock& block) const {
-    return &block_infos_[block.GetBlockId()]->kill_;
+  BitVectorView<size_t> GetKillSet(const HBasicBlock& block) const {
+    return block_infos_[block.GetBlockId()]->kill_;
   }
 
   HInstruction* GetInstructionFromSsaIndex(size_t index) const {
@@ -1266,10 +1271,10 @@ class SsaLivenessAnalysis : public ValueObject {
 
   static void ProcessEnvironment(HInstruction* instruction,
                                  HInstruction* actual_user,
-                                 BitVector* live_in);
+                                 BitVectorView<size_t> live_in);
   static void RecursivelyProcessInputs(HInstruction* instruction,
                                        HInstruction* actual_user,
-                                       BitVector* live_in);
+                                       BitVectorView<size_t> live_in);
 
   // Returns whether `instruction` in an HEnvironment held by `env_holder`
   // should be kept live by the HEnvironment.
@@ -1294,19 +1299,11 @@ class SsaLivenessAnalysis : public ValueObject {
     if (!block.IsLoopHeader() || !block.GetLoopInformation()->IsIrreducible()) {
       return;
     }
-    BitVector* live_in = GetLiveInSet(block);
-    // To satisfy our liveness algorithm, we need to ensure loop headers of
-    // irreducible loops do not have any live-in instructions, except constants
-    // and the current method, which can be trivially re-materialized.
-    for (uint32_t idx : live_in->Indexes()) {
-      HInstruction* instruction = GetInstructionFromSsaIndex(idx);
-      DCHECK(instruction->GetBlock()->IsEntryBlock()) << instruction->DebugName();
-      DCHECK(!instruction->IsParameterValue());
-      DCHECK(instruction->IsCurrentMethod() || instruction->IsConstant())
-          << instruction->DebugName();
-    }
+    DoCheckNoLiveInIrreducibleLoop(block);
   }
 
+  void DoCheckNoLiveInIrreducibleLoop(const HBasicBlock& block) const;
+
   HGraph* const graph_;
   CodeGenerator* const codegen_;
 
diff --git a/compiler/optimizing/ssa_phi_elimination.cc b/compiler/optimizing/ssa_phi_elimination.cc
index 0796acc687..b2a3846dc4 100644
--- a/compiler/optimizing/ssa_phi_elimination.cc
+++ b/compiler/optimizing/ssa_phi_elimination.cc
@@ -139,10 +139,8 @@ bool SsaRedundantPhiElimination::Run() {
     }
   }
 
-  ArenaBitVector visited_phis_in_cycle(&allocator,
-                                       graph_->GetCurrentInstructionId(),
-                                       /* expandable= */ false,
-                                       kArenaAllocSsaPhiElimination);
+  BitVectorView<size_t> visited_phis_in_cycle = ArenaBitVector::CreateFixedSize(
+      &allocator, graph_->GetCurrentInstructionId(), kArenaAllocSsaPhiElimination);
   ScopedArenaVector<HPhi*> cycle_worklist(allocator.Adapter(kArenaAllocSsaPhiElimination));
 
   while (!worklist.empty()) {
diff --git a/compiler/optimizing/superblock_cloner.cc b/compiler/optimizing/superblock_cloner.cc
index 276d2246cb..5ab34fb1a8 100644
--- a/compiler/optimizing/superblock_cloner.cc
+++ b/compiler/optimizing/superblock_cloner.cc
@@ -144,7 +144,7 @@ void SuperblockCloner::DeepCloneEnvironmentWithRemapping(HInstruction* copy_inst
     }
     copy_env->SetRawEnvAt(i, env_input);
     if (env_input != nullptr) {
-      env_input->AddEnvUseAt(copy_env, i);
+      env_input->AddEnvUseAt(graph_->GetAllocator(), copy_env, i);
     }
   }
   // InsertRawEnvironment assumes that instruction already has an environment that's why we use
@@ -227,40 +227,6 @@ void SuperblockCloner::RemapCopyInternalEdge(HBasicBlock* orig_block,
   }
 }
 
-bool SuperblockCloner::IsRemapInfoForVersioning() const {
-  return remap_incoming_->empty() &&
-         remap_orig_internal_->empty() &&
-         remap_copy_internal_->empty();
-}
-
-void SuperblockCloner::CopyIncomingEdgesForVersioning() {
-  for (uint32_t orig_block_id : orig_bb_set_.Indexes()) {
-    HBasicBlock* orig_block = GetBlockById(orig_block_id);
-    size_t incoming_edge_count = 0;
-    for (HBasicBlock* orig_pred : orig_block->GetPredecessors()) {
-      uint32_t orig_pred_id = orig_pred->GetBlockId();
-      if (IsInOrigBBSet(orig_pred_id)) {
-        continue;
-      }
-
-      HBasicBlock* copy_block = GetBlockCopy(orig_block);
-      // This corresponds to the requirement on the order of predecessors: all the incoming
-      // edges must be seen before the internal ones. This is always true for natural loops.
-      // TODO: remove this requirement.
-      DCHECK_EQ(orig_block->GetPredecessorIndexOf(orig_pred), incoming_edge_count);
-      for (HInstructionIterator it(orig_block->GetPhis()); !it.Done(); it.Advance()) {
-        HPhi* orig_phi = it.Current()->AsPhi();
-        HPhi* copy_phi = GetInstrCopy(orig_phi)->AsPhi();
-        HInstruction* orig_phi_input = orig_phi->InputAt(incoming_edge_count);
-        // Add the corresponding input of the original phi to the copy one.
-        copy_phi->AddInput(orig_phi_input);
-      }
-      copy_block->AddPredecessor(orig_pred);
-      incoming_edge_count++;
-    }
-  }
-}
-
 //
 // Local versions of CF calculation/adjustment routines.
 //
@@ -484,12 +450,6 @@ void SuperblockCloner::FindAndSetLocalAreaForAdjustments() {
 }
 
 void SuperblockCloner::RemapEdgesSuccessors() {
-  // By this stage all the blocks have been copied, copy phis - created with no inputs;
-  // no copy edges have been created so far.
-  if (IsRemapInfoForVersioning()) {
-    CopyIncomingEdgesForVersioning();
-  }
-
   // Redirect incoming edges.
   for (HEdge e : *remap_incoming_) {
     HBasicBlock* orig_block = GetBlockById(e.GetFrom());
@@ -897,7 +857,7 @@ bool SuperblockCloner::IsSubgraphClonable() const {
   return true;
 }
 
-// Checks that loop unrolling/peeling/versioning is being conducted.
+// Checks that loop unrolling/peeling is being conducted.
 bool SuperblockCloner::IsFastCase() const {
   // Check that all the basic blocks belong to the same loop.
   bool flag = false;
@@ -914,15 +874,11 @@ bool SuperblockCloner::IsFastCase() const {
     }
   }
 
-  // Check that orig_bb_set_ corresponds to loop peeling/unrolling/versioning.
+  // Check that orig_bb_set_ corresponds to loop peeling/unrolling.
   if (common_loop_info == nullptr || !orig_bb_set_.SameBitsSet(&common_loop_info->GetBlocks())) {
     return false;
   }
 
-  if (IsRemapInfoForVersioning()) {
-    return true;
-  }
-
   bool peeling_or_unrolling = false;
   HEdgeSet remap_orig_internal(graph_->GetAllocator()->Adapter(kArenaAllocSuperblockCloner));
   HEdgeSet remap_copy_internal(graph_->GetAllocator()->Adapter(kArenaAllocSuperblockCloner));
@@ -1171,9 +1127,6 @@ HBasicBlock* LoopClonerHelper::DoLoopTransformationImpl(TransformationKind trans
       case TransformationKind::kUnrolling:
         oss<< "unrolling";
         break;
-      case TransformationKind::kVersioning:
-        oss << "versioning";
-        break;
     }
     oss << " was applied to the loop <" << loop_header->GetBlockId() << ">.";
     LOG(INFO) << oss.str();
@@ -1185,14 +1138,11 @@ HBasicBlock* LoopClonerHelper::DoLoopTransformationImpl(TransformationKind trans
   HEdgeSet remap_copy_internal(graph->GetAllocator()->Adapter(kArenaAllocSuperblockCloner));
   HEdgeSet remap_incoming(graph->GetAllocator()->Adapter(kArenaAllocSuperblockCloner));
 
-  // No remapping needed for loop versioning.
-  if (transformation != TransformationKind::kVersioning) {
-    CollectRemappingInfoForPeelUnroll(transformation == TransformationKind::kUnrolling,
-                                      loop_info_,
-                                      &remap_orig_internal,
-                                      &remap_copy_internal,
-                                      &remap_incoming);
-  }
+  CollectRemappingInfoForPeelUnroll(transformation == TransformationKind::kUnrolling,
+                                    loop_info_,
+                                    &remap_orig_internal,
+                                    &remap_copy_internal,
+                                    &remap_incoming);
 
   cloner_.SetSuccessorRemappingInfo(&remap_orig_internal, &remap_copy_internal, &remap_incoming);
   cloner_.Run();
diff --git a/compiler/optimizing/superblock_cloner.h b/compiler/optimizing/superblock_cloner.h
index c327867342..d4db0b3852 100644
--- a/compiler/optimizing/superblock_cloner.h
+++ b/compiler/optimizing/superblock_cloner.h
@@ -90,8 +90,7 @@ inline bool IsEdgeValid(HEdge edge, HGraph* graph) {
 // fine grain manipulation with IR; data flow and graph properties are resolved/adjusted
 // automatically. The clone transformation is defined by specifying a set of basic blocks to copy
 // and a set of rules how to treat edges, remap their successors. By using this approach such
-// optimizations as Branch Target Expansion, Loop Peeling, Loop Unrolling, Loop Versioning can be
-// implemented.
+// optimizations as Branch Target Expansion, Loop Peeling, Loop Unrolling can be implemented.
 //
 // The idea of the transformation is based on "Superblock cloning" technique described in the book
 // "Engineering a Compiler. Second Edition", Keith D. Cooper, Linda Torczon, Rice University
@@ -163,7 +162,7 @@ class SuperblockCloner : public ValueObject {
   //
   // TODO: formally describe the criteria.
   //
-  // Loop peeling, unrolling and versioning satisfy the criteria.
+  // Loop peeling and unrolling satisfy the criteria.
   bool IsFastCase() const;
 
   // Runs the copy algorithm according to the description.
@@ -299,18 +298,6 @@ class SuperblockCloner : public ValueObject {
   // Remaps copy internal edge to its origin, adjusts the phi inputs in orig_succ.
   void RemapCopyInternalEdge(HBasicBlock* orig_block, HBasicBlock* orig_succ);
 
-  // Checks whether the edges remapping info corresponds to the subgraph versioning case:
-  //  - none of the incoming edges are to be remapped (they are being duplicated).
-  //  - none of the internal edges are to be remapped.
-  bool IsRemapInfoForVersioning() const;
-
-  // Processes incoming edges for subgraph versioning case: for each incoming edge (X, Y) adds
-  // an edge (X, Y_1) where Y_1 = Copy(Y) and add corresponding phi input to copy phi.
-  //
-  // Note: such node X will now have two successors, its unconditional branch instruction
-  // will be invalid and should be adjusted to some conditional branch by the client code.
-  void CopyIncomingEdgesForVersioning();
-
   //
   // Local versions of control flow calculation/adjustment routines.
   //
@@ -376,7 +363,7 @@ class SuperblockCloner : public ValueObject {
   DISALLOW_COPY_AND_ASSIGN(SuperblockCloner);
 };
 
-// Helper class to perform loop peeling/unrolling/versioning.
+// Helper class to perform loop peeling/unrolling.
 //
 // This helper should be used when correspondence map between original and copied
 // basic blocks/instructions are demanded.
@@ -456,40 +443,12 @@ class LoopClonerHelper : public ValueObject {
     return DoLoopTransformationImpl(TransformationKind::kUnrolling);
   }
 
-  // Perform loop versioning.
-  //
-  // Control flow of an example (ignoring critical edges splitting).
-  //
-  //       Before                    After
-  //
-  //         |B|                      |B|
-  //          |                        |
-  //          v                        v
-  //         |1|                      |1|_________
-  //          |                        |          |
-  //          v                        v          v
-  //         |2|<-\                   |2|<-\     |2A|<-\
-  //         / \  /                   / \  /     /  \  /
-  //        v   v/                   |   v/      |   v/
-  //        |   |3|                  |  |3|      | |3A|
-  //        |                        | __________|
-  //        |                        ||
-  //        v                        vv
-  //       |4|                       |4|
-  //        |                         |
-  //        v                         v
-  //       |E|                       |E|
-  HBasicBlock* DoVersioning() {
-    return DoLoopTransformationImpl(TransformationKind::kVersioning);
-  }
-
   HLoopInformation* GetRegionToBeAdjusted() const { return cloner_.GetRegionToBeAdjusted(); }
 
  protected:
   enum class TransformationKind {
     kPeeling,
     kUnrolling,
-    kVersioning,
   };
 
   // Applies a specific loop transformation to the loop.
@@ -502,7 +461,7 @@ class LoopClonerHelper : public ValueObject {
   DISALLOW_COPY_AND_ASSIGN(LoopClonerHelper);
 };
 
-// Helper class to perform loop peeling/unrolling/versioning.
+// Helper class to perform loop peeling/unrolling.
 //
 // This helper should be used when there is no need to get correspondence information between
 // original and copied basic blocks/instructions.
@@ -512,7 +471,6 @@ class LoopClonerSimpleHelper : public ValueObject {
   bool IsLoopClonable() const { return helper_.IsLoopClonable(); }
   HBasicBlock* DoPeeling() { return helper_.DoPeeling(); }
   HBasicBlock* DoUnrolling() { return helper_.DoUnrolling(); }
-  HBasicBlock* DoVersioning() { return helper_.DoVersioning(); }
   HLoopInformation* GetRegionToBeAdjusted() const { return helper_.GetRegionToBeAdjusted(); }
 
   const SuperblockCloner::HBasicBlockMap* GetBasicBlockMap() const { return &bb_map_; }
diff --git a/compiler/optimizing/superblock_cloner_test.cc b/compiler/optimizing/superblock_cloner_test.cc
index 5190dae033..1bef8a4e9d 100644
--- a/compiler/optimizing/superblock_cloner_test.cc
+++ b/compiler/optimizing/superblock_cloner_test.cc
@@ -301,52 +301,6 @@ TEST_F(SuperblockClonerTest, LoopUnrolling) {
   EXPECT_EQ(loop_info->GetBackEdges()[0], bb_map.Get(loop_body));
 }
 
-// Tests SuperblockCloner for loop versioning case.
-//
-// See an ASCII graphics example near LoopClonerHelper::DoVersioning.
-TEST_F(SuperblockClonerTest, LoopVersioning) {
-  HBasicBlock* return_block = InitGraphAndParameters();
-  auto [preheader, header, loop_body] = CreateWhileLoop(return_block);
-  CreateBasicLoopDataFlow(header, loop_body);
-  graph_->BuildDominatorTree();
-  EXPECT_TRUE(CheckGraph());
-
-  HBasicBlockMap bb_map(
-      std::less<HBasicBlock*>(), graph_->GetAllocator()->Adapter(kArenaAllocSuperblockCloner));
-  HInstructionMap hir_map(
-      std::less<HInstruction*>(), graph_->GetAllocator()->Adapter(kArenaAllocSuperblockCloner));
-
-  HLoopInformation* loop_info = header->GetLoopInformation();
-  HBasicBlock* original_preheader = loop_info->GetPreHeader();
-  LoopClonerHelper helper(loop_info, &bb_map, &hir_map, /* induction_range= */ nullptr);
-  EXPECT_TRUE(helper.IsLoopClonable());
-  HBasicBlock* new_header = helper.DoVersioning();
-  EXPECT_EQ(header, new_header);
-
-  EXPECT_TRUE(CheckGraph());
-
-  HBasicBlock* second_header = bb_map.Get(header);
-  HBasicBlock* second_body = bb_map.Get(loop_body);
-  HLoopInformation* second_loop_info = second_header->GetLoopInformation();
-
-  // Check loop body successors.
-  EXPECT_EQ(loop_body->GetSingleSuccessor(), header);
-  EXPECT_EQ(second_body->GetSingleSuccessor(), second_header);
-
-  // Check loop structure.
-  EXPECT_EQ(loop_info, header->GetLoopInformation());
-  EXPECT_EQ(loop_info->GetHeader(), header);
-  EXPECT_EQ(second_loop_info->GetHeader(), second_header);
-
-  EXPECT_EQ(loop_info->GetBackEdges().size(), 1u);
-  EXPECT_EQ(second_loop_info->GetBackEdges().size(), 1u);
-
-  EXPECT_EQ(loop_info->GetBackEdges()[0], loop_body);
-  EXPECT_EQ(second_loop_info->GetBackEdges()[0], second_body);
-
-  EXPECT_EQ(original_preheader->GetSuccessors().size(), 2u);
-}
-
 // Checks that loop unrolling works fine for a loop with multiple back edges. Tests that after
 // the transformation the loop has a single preheader.
 TEST_F(SuperblockClonerTest, LoopPeelingMultipleBackEdges) {
diff --git a/compiler/utils/assembler.h b/compiler/utils/assembler.h
index 0548cf8325..d1e4675492 100644
--- a/compiler/utils/assembler.h
+++ b/compiler/utils/assembler.h
@@ -396,6 +396,7 @@ class Assembler : public DeletableArenaObject<kArenaAllocAssembler> {
    * @details It is used by debuggers and other tools to unwind the call stack.
    */
   DebugFrameOpCodeWriterForAssembler& cfi() { return cfi_; }
+  const DebugFrameOpCodeWriterForAssembler& cfi() const { return cfi_; }
 
   ArenaAllocator* GetAllocator() {
     return buffer_.GetAllocator();
diff --git a/compiler/utils/assembler_test.h b/compiler/utils/assembler_test.h
index 05c79f3b21..e90187ccbc 100644
--- a/compiler/utils/assembler_test.h
+++ b/compiler/utils/assembler_test.h
@@ -316,9 +316,8 @@ class AssemblerTest : public AssemblerTestBase {
 
     for (auto reg : registers) {
       for (int64_t imm : imms) {
-        ImmType new_imm = CreateImmediate(imm);
         if (f != nullptr) {
-          (assembler_.get()->*f)(reg, new_imm + bias);
+          (assembler_.get()->*f)(reg, CreateImmediate(imm + bias));
         }
         std::string base = fmt;
 
@@ -516,6 +515,18 @@ class AssemblerTest : public AssemblerTestBase {
         bias);
   }
 
+  template <typename ImmType>
+  std::string RepeatFI(void (Ass::*f)(FPReg, ImmType),
+                       size_t imm_bits,
+                       const std::string& fmt) {
+    return RepeatTemplatedRegisterImmBits<FPReg, ImmType>(f,
+                                                          imm_bits,
+                                                          GetFPRegisters(),
+                                                          &AssemblerTest::GetFPRegName,
+                                                          fmt,
+                                                          /*bias=*/ 0);
+  }
+
   std::string RepeatFF(void (Ass::*f)(FPReg, FPReg), const std::string& fmt) {
     return RepeatTemplatedRegisters<FPReg, FPReg>(f,
                                                   GetFPRegisters(),
diff --git a/compiler/utils/assembler_test_base.h b/compiler/utils/assembler_test_base.h
index 0a89ad1299..515361af00 100644
--- a/compiler/utils/assembler_test_base.h
+++ b/compiler/utils/assembler_test_base.h
@@ -42,7 +42,7 @@ namespace art HIDDEN {
 static constexpr bool kKeepDisassembledFiles = false;
 
 // We put this into a class as gtests are self-contained, so this helper needs to be in an h-file.
-class AssemblerTestBase : public testing::Test {
+class AssemblerTestBase : public ::testing::Test {
  public:
   AssemblerTestBase() {}
 
diff --git a/compiler/utils/x86/assembler_x86_test.cc b/compiler/utils/x86/assembler_x86_test.cc
index 91ee2302db..b9205a9b31 100644
--- a/compiler/utils/x86/assembler_x86_test.cc
+++ b/compiler/utils/x86/assembler_x86_test.cc
@@ -1248,44 +1248,36 @@ TEST_F(AssemblerX86Test, Punpckhqdq) {
   DriverStr(RepeatFF(&x86::X86Assembler::punpckhqdq, "punpckhqdq %{reg2}, %{reg1}"), "punpckhqdq");
 }
 
-TEST_F(AssemblerX86Test, psllw) {
-  GetAssembler()->psllw(x86::XMM0, CreateImmediate(16));
-  DriverStr("psllw $0x10, %xmm0\n", "psllwi");
+TEST_F(AssemblerX86Test, Psllw) {
+  DriverStr(RepeatFI(&x86::X86Assembler::psllw, 4u, "psllw ${imm}, %{reg}"), "psllwi");
 }
 
-TEST_F(AssemblerX86Test, pslld) {
-  GetAssembler()->pslld(x86::XMM0, CreateImmediate(16));
-  DriverStr("pslld $0x10, %xmm0\n", "pslldi");
+TEST_F(AssemblerX86Test, Pslld) {
+  DriverStr(RepeatFI(&x86::X86Assembler::pslld, 5u, "pslld ${imm}, %{reg}"), "pslldi");
 }
 
-TEST_F(AssemblerX86Test, psllq) {
-  GetAssembler()->psllq(x86::XMM0, CreateImmediate(16));
-  DriverStr("psllq $0x10, %xmm0\n", "psllqi");
+TEST_F(AssemblerX86Test, Psllq) {
+  DriverStr(RepeatFI(&x86::X86Assembler::psllq, 6u, "psllq ${imm}, %{reg}"), "psllqi");
 }
 
-TEST_F(AssemblerX86Test, psraw) {
-  GetAssembler()->psraw(x86::XMM0, CreateImmediate(16));
-  DriverStr("psraw $0x10, %xmm0\n", "psrawi");
+TEST_F(AssemblerX86Test, Psraw) {
+  DriverStr(RepeatFI(&x86::X86Assembler::psraw, 4u, "psraw ${imm}, %{reg}"), "psrawi");
 }
 
-TEST_F(AssemblerX86Test, psrad) {
-  GetAssembler()->psrad(x86::XMM0, CreateImmediate(16));
-  DriverStr("psrad $0x10, %xmm0\n", "psradi");
+TEST_F(AssemblerX86Test, Psrad) {
+  DriverStr(RepeatFI(&x86::X86Assembler::psrad, 5u, "psrad ${imm}, %{reg}"), "psradi");
 }
 
-TEST_F(AssemblerX86Test, psrlw) {
-  GetAssembler()->psrlw(x86::XMM0, CreateImmediate(16));
-  DriverStr("psrlw $0x10, %xmm0\n", "psrlwi");
+TEST_F(AssemblerX86Test, Psrlw) {
+  DriverStr(RepeatFI(&x86::X86Assembler::psrlw, 4u, "psrlw ${imm}, %{reg}"), "psrlwi");
 }
 
-TEST_F(AssemblerX86Test, psrld) {
-  GetAssembler()->psrld(x86::XMM0, CreateImmediate(16));
-  DriverStr("psrld $0x10, %xmm0\n", "psrldi");
+TEST_F(AssemblerX86Test, Psrld) {
+  DriverStr(RepeatFI(&x86::X86Assembler::psrld, 5u, "psrld ${imm}, %{reg}"), "psrldi");
 }
 
-TEST_F(AssemblerX86Test, psrlq) {
-  GetAssembler()->psrlq(x86::XMM0, CreateImmediate(16));
-  DriverStr("psrlq $0x10, %xmm0\n", "psrlqi");
+TEST_F(AssemblerX86Test, Psrlq) {
+  DriverStr(RepeatFI(&x86::X86Assembler::psrlq, 6u, "psrlq ${imm}, %{reg}"), "psrlqi");
 }
 
 TEST_F(AssemblerX86Test, psrldq) {
diff --git a/compiler/utils/x86_64/assembler_x86_64.cc b/compiler/utils/x86_64/assembler_x86_64.cc
index 91bc4fe6e3..6330cc5d62 100644
--- a/compiler/utils/x86_64/assembler_x86_64.cc
+++ b/compiler/utils/x86_64/assembler_x86_64.cc
@@ -491,36 +491,8 @@ void X86_64Assembler::movaps(XmmRegister dst, const Address& src) {
 void X86_64Assembler::vmovaps(XmmRegister dst, const Address& src) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  uint8_t ByteZero, ByteOne, ByteTwo;
-  bool is_twobyte_form = false;
   // Instruction VEX Prefix
-  uint8_t rex = src.rex();
-  bool Rex_x = rex & GET_REX_X;
-  bool Rex_b = rex & GET_REX_B;
-  if (!Rex_b && !Rex_x) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   Rex_x,
-                                   Rex_b,
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
+  EmitVexPrefixForAddress(src, dst.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_NONE);
   // Instruction Opcode
   EmitUint8(0x28);
   // Instruction Operands
@@ -543,36 +515,8 @@ void X86_64Assembler::movups(XmmRegister dst, const Address& src) {
 void X86_64Assembler::vmovups(XmmRegister dst, const Address& src) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  uint8_t ByteZero, ByteOne, ByteTwo;
-  bool is_twobyte_form = false;
   // Instruction VEX Prefix
-  uint8_t rex = src.rex();
-  bool Rex_x = rex & GET_REX_X;
-  bool Rex_b = rex & GET_REX_B;
-  if (!Rex_x && !Rex_b) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   Rex_x,
-                                   Rex_b,
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
+  EmitVexPrefixForAddress(src, dst.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_NONE);
   // Instruction Opcode
   EmitUint8(0x10);
   // Instruction Operands
@@ -596,37 +540,8 @@ void X86_64Assembler::movaps(const Address& dst, XmmRegister src) {
 void X86_64Assembler::vmovaps(const Address& dst, XmmRegister src) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  uint8_t ByteZero, ByteOne, ByteTwo;
-  bool is_twobyte_form = false;
-
   // Instruction VEX Prefix
-  uint8_t rex = dst.rex();
-  bool Rex_x = rex & GET_REX_X;
-  bool Rex_b = rex & GET_REX_B;
-  if (!Rex_b && !Rex_x) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   Rex_x,
-                                   Rex_b,
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
+  EmitVexPrefixForAddress(dst, src.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_NONE);
   // Instruction Opcode
   EmitUint8(0x29);
   // Instruction Operands
@@ -649,37 +564,8 @@ void X86_64Assembler::movups(const Address& dst, XmmRegister src) {
 void X86_64Assembler::vmovups(const Address& dst, XmmRegister src) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  uint8_t ByteZero, ByteOne, ByteTwo;
-  bool is_twobyte_form = false;
-
   // Instruction VEX Prefix
-  uint8_t rex = dst.rex();
-  bool Rex_x = rex & GET_REX_X;
-  bool Rex_b = rex & GET_REX_B;
-  if (!Rex_b && !Rex_x) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   Rex_x,
-                                   Rex_b,
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
+  EmitVexPrefixForAddress(dst, src.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_NONE);
   // Instruction Opcode
   EmitUint8(0x11);
   // Instruction Operands
@@ -733,32 +619,26 @@ void X86_64Assembler::movsxd(CpuRegister dst, const Address& src) {
 }
 
 
-void X86_64Assembler::movd(XmmRegister dst, CpuRegister src) {
-  movd(dst, src, true);
+void X86_64Assembler::movq(XmmRegister dst, CpuRegister src) {
+  EmitMovCpuFpu(dst, src, /*is64bit=*/ true, /*opcode=*/ 0x6E);
 }
 
-void X86_64Assembler::movd(CpuRegister dst, XmmRegister src) {
-  movd(dst, src, true);
+
+void X86_64Assembler::movq(CpuRegister dst, XmmRegister src) {
+  EmitMovCpuFpu(src, dst, /*is64bit=*/ true, /*opcode=*/ 0x7E);
 }
 
-void X86_64Assembler::movd(XmmRegister dst, CpuRegister src, bool is64bit) {
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  EmitUint8(0x66);
-  EmitOptionalRex(false, is64bit, dst.NeedsRex(), false, src.NeedsRex());
-  EmitUint8(0x0F);
-  EmitUint8(0x6E);
-  EmitOperand(dst.LowBits(), Operand(src));
+
+void X86_64Assembler::movd(XmmRegister dst, CpuRegister src) {
+  EmitMovCpuFpu(dst, src, /*is64bit=*/ false, /*opcode=*/ 0x6E);
 }
 
-void X86_64Assembler::movd(CpuRegister dst, XmmRegister src, bool is64bit) {
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  EmitUint8(0x66);
-  EmitOptionalRex(false, is64bit, src.NeedsRex(), false, dst.NeedsRex());
-  EmitUint8(0x0F);
-  EmitUint8(0x7E);
-  EmitOperand(src.LowBits(), Operand(dst));
+
+void X86_64Assembler::movd(CpuRegister dst, XmmRegister src) {
+  EmitMovCpuFpu(src, dst, /*is64bit=*/ false, /*opcode=*/ 0x7E);
 }
 
+
 void X86_64Assembler::addss(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF3);
@@ -856,59 +736,12 @@ void X86_64Assembler::subps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vaddps(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!add_left.NeedsRex()) {
-    return vaddps(dst, add_right, add_left);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x58);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  EmitVecArithAndLogicalOperation(
+      dst, add_left, add_right, /*opcode=*/ 0x58, SET_VEX_PP_NONE, /*is_commutative=*/ true);
 }
 
 void X86_64Assembler::vsubps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t byte_zero = 0x00, byte_one = 0x00, byte_two = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), /*X=*/ false, src2.NeedsRex(), SET_VEX_M_0F);
-    byte_two = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(byte_zero);
-  EmitUint8(byte_one);
-  if (!is_twobyte_form) {
-    EmitUint8(byte_two);
-  }
-  EmitUint8(0x5C);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, /*opcode=*/ 0x5C, SET_VEX_PP_NONE);
 }
 
 
@@ -921,34 +754,8 @@ void X86_64Assembler::mulps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vmulps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vmulps(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x59);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0x59, SET_VEX_PP_NONE, /*is_commutative=*/ true);
 }
 
 void X86_64Assembler::divps(XmmRegister dst, XmmRegister src) {
@@ -960,32 +767,7 @@ void X86_64Assembler::divps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vdivps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x5E);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, /*opcode=*/ 0x5E, SET_VEX_PP_NONE);
 }
 
 void X86_64Assembler::vfmadd213ss(XmmRegister acc, XmmRegister left, XmmRegister right) {
@@ -1124,37 +906,8 @@ void X86_64Assembler::movapd(XmmRegister dst, const Address& src) {
 void X86_64Assembler::vmovapd(XmmRegister dst, const Address& src) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  uint8_t ByteZero, ByteOne, ByteTwo;
-  bool is_twobyte_form = false;
-
   // Instruction VEX Prefix
-  uint8_t rex = src.rex();
-  bool Rex_x = rex & GET_REX_X;
-  bool Rex_b = rex & GET_REX_B;
-  if (!Rex_b && !Rex_x) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   Rex_x,
-                                   Rex_b,
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
+  EmitVexPrefixForAddress(src, dst.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_66);
   // Instruction Opcode
   EmitUint8(0x28);
   // Instruction Operands
@@ -1178,36 +931,8 @@ void X86_64Assembler::movupd(XmmRegister dst, const Address& src) {
 void X86_64Assembler::vmovupd(XmmRegister dst, const Address& src) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero, ByteOne, ByteTwo;
-
   // Instruction VEX Prefix
-  uint8_t rex = src.rex();
-  bool Rex_x = rex & GET_REX_X;
-  bool Rex_b = rex & GET_REX_B;
-  if (!Rex_b && !Rex_x) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   Rex_x,
-                                   Rex_b,
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form)
-  EmitUint8(ByteTwo);
+  EmitVexPrefixForAddress(src, dst.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_66);
   // Instruction Opcode
   EmitUint8(0x10);
   // Instruction Operands
@@ -1231,36 +956,8 @@ void X86_64Assembler::movapd(const Address& dst, XmmRegister src) {
 void X86_64Assembler::vmovapd(const Address& dst, XmmRegister src) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero, ByteOne, ByteTwo;
   // Instruction VEX Prefix
-  uint8_t rex = dst.rex();
-  bool Rex_x = rex & GET_REX_X;
-  bool Rex_b = rex & GET_REX_B;
-  if (!Rex_x && !Rex_b) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   Rex_x,
-                                   Rex_b,
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
+  EmitVexPrefixForAddress(dst, src.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_66);
   // Instruction Opcode
   EmitUint8(0x29);
   // Instruction Operands
@@ -1284,37 +981,8 @@ void X86_64Assembler::movupd(const Address& dst, XmmRegister src) {
 void X86_64Assembler::vmovupd(const Address& dst, XmmRegister src) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero, ByteOne, ByteTwo;
-
   // Instruction VEX Prefix
-  uint8_t rex = dst.rex();
-  bool Rex_x = rex & GET_REX_X;
-  bool Rex_b = rex & GET_REX_B;
-  if (!Rex_x && !Rex_b) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   Rex_x,
-                                   Rex_b,
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
+  EmitVexPrefixForAddress(dst, src.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_66);
   // Instruction Opcode
   EmitUint8(0x11);
   // Instruction Operands
@@ -1443,37 +1111,12 @@ void X86_64Assembler::addpd(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vaddpd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!add_left.NeedsRex()) {
-    return vaddpd(dst, add_right, add_left);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x58);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
-}
-
-
-void X86_64Assembler::subpd(XmmRegister dst, XmmRegister src) {
+  EmitVecArithAndLogicalOperation(
+      dst, add_left, add_right, /*opcode=*/ 0x58, SET_VEX_PP_66, /*is_commutative=*/ true);
+}
+
+
+void X86_64Assembler::subpd(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1484,31 +1127,7 @@ void X86_64Assembler::subpd(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vsubpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x5C);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, /*opcode=*/ 0x5C, SET_VEX_PP_66);
 }
 
 
@@ -1522,34 +1141,8 @@ void X86_64Assembler::mulpd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vmulpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vmulpd(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x59);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0x59, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 void X86_64Assembler::divpd(XmmRegister dst, XmmRegister src) {
@@ -1563,32 +1156,7 @@ void X86_64Assembler::divpd(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vdivpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x5E);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, /*opcode=*/ 0x5E, SET_VEX_PP_66);
 }
 
 
@@ -1670,37 +1238,8 @@ void X86_64Assembler::movdqa(XmmRegister dst, const Address& src) {
 void X86_64Assembler::vmovdqa(XmmRegister dst, const Address& src) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  uint8_t  ByteZero, ByteOne, ByteTwo;
-  bool is_twobyte_form = false;
-
   // Instruction VEX Prefix
-  uint8_t rex = src.rex();
-  bool Rex_x = rex & GET_REX_X;
-  bool Rex_b = rex & GET_REX_B;
-  if (!Rex_x && !Rex_b) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   Rex_x,
-                                   Rex_b,
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
+  EmitVexPrefixForAddress(src, dst.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_66);
   // Instruction Opcode
   EmitUint8(0x6F);
   // Instruction Operands
@@ -1725,37 +1264,8 @@ Load Unaligned */
 void X86_64Assembler::vmovdqu(XmmRegister dst, const Address& src) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  uint8_t ByteZero, ByteOne, ByteTwo;
-  bool is_twobyte_form = false;
-
   // Instruction VEX Prefix
-  uint8_t rex = src.rex();
-  bool Rex_x = rex & GET_REX_X;
-  bool Rex_b = rex & GET_REX_B;
-  if (!Rex_x && !Rex_b) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_F3);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   Rex_x,
-                                   Rex_b,
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_F3);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
+  EmitVexPrefixForAddress(src, dst.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_F3);
   // Instruction Opcode
   EmitUint8(0x6F);
   // Instruction Operands
@@ -1779,36 +1289,8 @@ void X86_64Assembler::movdqa(const Address& dst, XmmRegister src) {
 void X86_64Assembler::vmovdqa(const Address& dst, XmmRegister src) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero, ByteOne, ByteTwo;
   // Instruction VEX Prefix
-  uint8_t rex = dst.rex();
-  bool Rex_x = rex & GET_REX_X;
-  bool Rex_b = rex & GET_REX_B;
-  if (!Rex_x && !Rex_b) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   Rex_x,
-                                   Rex_b,
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
+  EmitVexPrefixForAddress(dst, src.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_66);
   // Instruction Opcode
   EmitUint8(0x7F);
   // Instruction Operands
@@ -1832,37 +1314,8 @@ void X86_64Assembler::movdqu(const Address& dst, XmmRegister src) {
 void X86_64Assembler::vmovdqu(const Address& dst, XmmRegister src) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  uint8_t ByteZero, ByteOne, ByteTwo;
-  bool is_twobyte_form = false;
-
   // Instruction VEX Prefix
-  uint8_t rex = dst.rex();
-  bool Rex_x = rex & GET_REX_X;
-  bool Rex_b = rex & GET_REX_B;
-  if (!Rex_b && !Rex_x) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_F3);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   Rex_x,
-                                   Rex_b,
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_F3);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
+  EmitVexPrefixForAddress(dst, src.NeedsRex(), SET_VEX_L_128, SET_VEX_PP_F3);
   // Instruction Opcode
   EmitUint8(0x7F);
   // Instruction Operands
@@ -1880,34 +1333,8 @@ void X86_64Assembler::paddb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vpaddb(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  uint8_t ByteOne = 0x00, ByteZero = 0x00, ByteTwo = 0x00;
-  bool is_twobyte_form = false;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!add_left.NeedsRex()) {
-    return vpaddb(dst, add_right, add_left);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xFC);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  EmitVecArithAndLogicalOperation(
+      dst, add_left, add_right, /*opcode=*/ 0xFC, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 
@@ -1922,32 +1349,7 @@ void X86_64Assembler::psubb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vpsubb(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xF8);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  EmitVecArithAndLogicalOperation(dst, add_left, add_right, /*opcode=*/ 0xF8, SET_VEX_PP_66);
 }
 
 
@@ -1961,34 +1363,8 @@ void X86_64Assembler::paddw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpaddw(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!add_left.NeedsRex()) {
-    return vpaddw(dst, add_right, add_left);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xFD);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  EmitVecArithAndLogicalOperation(
+      dst, add_left, add_right, /*opcode=*/ 0xFD, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 
@@ -2002,32 +1378,7 @@ void X86_64Assembler::psubw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpsubw(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xF9);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  EmitVecArithAndLogicalOperation(dst, add_left, add_right, /*opcode=*/ 0xF9, SET_VEX_PP_66);
 }
 
 
@@ -2041,34 +1392,8 @@ void X86_64Assembler::pmullw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpmullw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vpmullw(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xD5);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0xD5, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 void X86_64Assembler::paddd(XmmRegister dst, XmmRegister src) {
@@ -2081,34 +1406,8 @@ void X86_64Assembler::paddd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpaddd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!add_left.NeedsRex()) {
-    return vpaddd(dst, add_right, add_left);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xFE);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  EmitVecArithAndLogicalOperation(
+      dst, add_left, add_right, /*opcode=*/ 0xFE, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 void X86_64Assembler::psubd(XmmRegister dst, XmmRegister src) {
@@ -2161,34 +1460,8 @@ void X86_64Assembler::paddq(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vpaddq(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!add_left.NeedsRex()) {
-    return vpaddq(dst, add_right, add_left);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xD4);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  EmitVecArithAndLogicalOperation(
+      dst, add_left, add_right, /*opcode=*/ 0xD4, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 
@@ -2202,32 +1475,7 @@ void X86_64Assembler::psubq(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpsubq(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xFB);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  EmitVecArithAndLogicalOperation(dst, add_left, add_right, /*opcode=*/ 0xFB, SET_VEX_PP_66);
 }
 
 
@@ -2292,32 +1540,7 @@ void X86_64Assembler::psubsb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vpsubd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xFA);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  EmitVecArithAndLogicalOperation(dst, add_left, add_right, /*opcode=*/ 0xFA, SET_VEX_PP_66);
 }
 
 
@@ -2692,104 +1915,26 @@ void X86_64Assembler::pxor(XmmRegister dst, XmmRegister src) {
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
-  EmitUint8(0xEF);
-  EmitXmmRegisterOperand(dst.LowBits(), src);
-}
-
-/* VEX.128.66.0F.WIG EF /r VPXOR xmm1, xmm2, xmm3/m128 */
-void X86_64Assembler::vpxor(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vpxor(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xEF);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
-}
-
-/* VEX.128.0F.WIG 57 /r VXORPS xmm1,xmm2, xmm3/m128 */
-void X86_64Assembler::vxorps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vxorps(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x57);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
-}
-
-/* VEX.128.66.0F.WIG 57 /r VXORPD xmm1,xmm2, xmm3/m128 */
-void X86_64Assembler::vxorpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vxorpd(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x57);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitUint8(0xEF);
+  EmitXmmRegisterOperand(dst.LowBits(), src);
+}
+
+/* VEX.128.66.0F.WIG EF /r VPXOR xmm1, xmm2, xmm3/m128 */
+void X86_64Assembler::vpxor(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0xEF, SET_VEX_PP_66, /*is_commutative=*/ true);
+}
+
+/* VEX.128.0F.WIG 57 /r VXORPS xmm1,xmm2, xmm3/m128 */
+void X86_64Assembler::vxorps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0x57, SET_VEX_PP_NONE, /*is_commutative=*/ true);
+}
+
+/* VEX.128.66.0F.WIG 57 /r VXORPD xmm1,xmm2, xmm3/m128 */
+void X86_64Assembler::vxorpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0x57, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 void X86_64Assembler::andpd(XmmRegister dst, const Address& src) {
@@ -2829,98 +1974,20 @@ void X86_64Assembler::pand(XmmRegister dst, XmmRegister src) {
 
 /* VEX.128.66.0F.WIG DB /r VPAND xmm1, xmm2, xmm3/m128 */
 void X86_64Assembler::vpand(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vpand(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xDB);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0xDB, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 /* VEX.128.0F 54 /r VANDPS xmm1,xmm2, xmm3/m128 */
 void X86_64Assembler::vandps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vandps(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x54);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0x54, SET_VEX_PP_NONE, /*is_commutative=*/ true);
 }
 
 /* VEX.128.66.0F 54 /r VANDPD xmm1, xmm2, xmm3/m128 */
 void X86_64Assembler::vandpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vandpd(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x54);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0x54, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 void X86_64Assembler::andn(CpuRegister dst, CpuRegister src1, CpuRegister src2) {
@@ -2970,92 +2037,17 @@ void X86_64Assembler::pandn(XmmRegister dst, XmmRegister src) {
 
 /* VEX.128.66.0F.WIG DF /r VPANDN xmm1, xmm2, xmm3/m128 */
 void X86_64Assembler::vpandn(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xDF);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, /*opcode=*/ 0xDF, SET_VEX_PP_66);
 }
 
 /* VEX.128.0F 55 /r VANDNPS xmm1, xmm2, xmm3/m128 */
 void X86_64Assembler::vandnps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x55);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, /*opcode=*/ 0x55, SET_VEX_PP_NONE);
 }
 
 /* VEX.128.66.0F 55 /r VANDNPD xmm1, xmm2, xmm3/m128 */
 void X86_64Assembler::vandnpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x55);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, /*opcode=*/ 0x55, SET_VEX_PP_66);
 }
 
 void X86_64Assembler::orpd(XmmRegister dst, XmmRegister src) {
@@ -3086,98 +2078,20 @@ void X86_64Assembler::por(XmmRegister dst, XmmRegister src) {
 
 /* VEX.128.66.0F.WIG EB /r VPOR xmm1, xmm2, xmm3/m128 */
 void X86_64Assembler::vpor(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vpor(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xEB);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0xEB, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 /* VEX.128.0F 56 /r VORPS xmm1,xmm2, xmm3/m128 */
 void X86_64Assembler::vorps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vorps(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x56);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0x56, SET_VEX_PP_NONE, /*is_commutative=*/ true);
 }
 
 /* VEX.128.66.0F 56 /r VORPD xmm1,xmm2, xmm3/m128 */
 void X86_64Assembler::vorpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vorpd(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x56);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0x56, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 void X86_64Assembler::pavgb(XmmRegister dst, XmmRegister src) {
@@ -3217,34 +2131,8 @@ void X86_64Assembler::pmaddwd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpmaddwd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vpmaddwd(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xF5);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(
+      dst, src1, src2, /*opcode=*/ 0xF5, SET_VEX_PP_66, /*is_commutative=*/ true);
 }
 
 void X86_64Assembler::phaddw(XmmRegister dst, XmmRegister src) {
@@ -5520,6 +4408,16 @@ void X86_64Assembler::EmitGenericShift(bool wide,
   EmitOperand(reg_or_opcode, Operand(operand));
 }
 
+void X86_64Assembler::EmitMovCpuFpu(
+    XmmRegister fp_reg, CpuRegister cpu_reg, bool is64bit, uint8_t opcode) {
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  EmitUint8(0x66);
+  EmitOptionalRex(false, is64bit, fp_reg.NeedsRex(), false, cpu_reg.NeedsRex());
+  EmitUint8(0x0F);
+  EmitUint8(opcode);
+  EmitOperand(fp_reg.LowBits(), Operand(cpu_reg));
+}
+
 void X86_64Assembler::EmitOptionalRex(bool force, bool w, bool r, bool x, bool b) {
   // REX.WRXB
   // W - 64-bit operand
@@ -5732,6 +4630,27 @@ uint8_t X86_64Assembler::EmitVexPrefixByteZero(bool is_twobyte_form) {
   return vex_prefix;
 }
 
+void X86_64Assembler::EmitVexPrefixForAddress(const Address& addr, bool r, int vex_l, int vex_pp) {
+  uint8_t rex = addr.rex();
+  bool rex_x = (rex & GET_REX_X) != 0u;
+  bool rex_b = (rex & GET_REX_B) != 0u;
+  bool is_twobyte_form = (!rex_b && !rex_x);
+  uint8_t byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  uint8_t byte_one, byte_two;
+  if (is_twobyte_form) {
+    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+    byte_one = EmitVexPrefixByteOne(r, vvvv_reg, vex_l, vex_pp);
+  } else {
+    byte_one = EmitVexPrefixByteOne(r, rex_x, rex_b, SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/ false, vex_l, vex_pp);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+}
+
 uint8_t X86_64Assembler::EmitVexPrefixByteOne(bool R, bool X, bool B, int SET_VEX_M) {
   // Vex Byte 1,
   uint8_t vex_prefix = VEX_INIT;
@@ -5847,5 +4766,35 @@ uint8_t X86_64Assembler::EmitVexPrefixByteTwo(bool W,
   return vex_prefix;
 }
 
+void X86_64Assembler::EmitVecArithAndLogicalOperation(XmmRegister dst,
+                                                      XmmRegister src1,
+                                                      XmmRegister src2,
+                                                      uint8_t opcode,
+                                                      int vex_pp,
+                                                      bool is_commutative) {
+  if (is_commutative && src2.NeedsRex() && !src1.NeedsRex()) {
+    return EmitVecArithAndLogicalOperation(dst, src2, src1, opcode, vex_pp, is_commutative);
+  }
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+  bool is_twobyte_form = !src2.NeedsRex();
+  uint8_t byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  uint8_t byte_one, byte_two;
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, vex_pp);
+  } else {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), /*X=*/ false, src2.NeedsRex(), SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, vex_pp);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+  EmitUint8(opcode);
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
+}
+
 }  // namespace x86_64
 }  // namespace art
diff --git a/compiler/utils/x86_64/assembler_x86_64.h b/compiler/utils/x86_64/assembler_x86_64.h
index b7475cd367..2c3b3c44ee 100644
--- a/compiler/utils/x86_64/assembler_x86_64.h
+++ b/compiler/utils/x86_64/assembler_x86_64.h
@@ -498,10 +498,10 @@ class X86_64Assembler final : public Assembler {
   void movsxd(CpuRegister dst, CpuRegister src);
   void movsxd(CpuRegister dst, const Address& src);
 
-  void movd(XmmRegister dst, CpuRegister src);  // Note: this is the r64 version, formally movq.
-  void movd(CpuRegister dst, XmmRegister src);  // Note: this is the r64 version, formally movq.
-  void movd(XmmRegister dst, CpuRegister src, bool is64bit);
-  void movd(CpuRegister dst, XmmRegister src, bool is64bit);
+  void movq(XmmRegister dst, CpuRegister src);
+  void movq(CpuRegister dst, XmmRegister src);
+  void movd(XmmRegister dst, CpuRegister src);
+  void movd(CpuRegister dst, XmmRegister src);
 
   void addss(XmmRegister dst, XmmRegister src);
   void addss(XmmRegister dst, const Address& src);
@@ -1132,6 +1132,8 @@ class X86_64Assembler final : public Assembler {
   void EmitGenericShift(bool wide, int rm, CpuRegister reg, const Immediate& imm);
   void EmitGenericShift(bool wide, int rm, CpuRegister operand, CpuRegister shifter);
 
+  void EmitMovCpuFpu(XmmRegister fp_reg, CpuRegister cpu_reg, bool is64bit, uint8_t opcode);
+
   // If any input is not false, output the necessary rex prefix.
   void EmitOptionalRex(bool force, bool w, bool r, bool x, bool b);
 
@@ -1165,6 +1167,10 @@ class X86_64Assembler final : public Assembler {
                                            bool normalize_both = false);
   void EmitOptionalByteRegNormalizingRex32(CpuRegister dst, const Operand& operand);
 
+  void EmitVexPrefixForAddress(const Address& addr, bool r, int vex_l, int vex_pp);
+
+  // TODO: Rename these functions. They calculate the byte but they do not "emit" that
+  // byte to the code buffer.
   uint8_t EmitVexPrefixByteZero(bool is_twobyte_form);
   uint8_t EmitVexPrefixByteOne(bool R, bool X, bool B, int SET_VEX_M);
   uint8_t EmitVexPrefixByteOne(bool R,
@@ -1179,6 +1185,13 @@ class X86_64Assembler final : public Assembler {
                                int SET_VEX_L,
                                int SET_VEX_PP);
 
+  void EmitVecArithAndLogicalOperation(XmmRegister dst,
+                                       XmmRegister src1,
+                                       XmmRegister src2,
+                                       uint8_t opcode,
+                                       int vex_pp,
+                                       bool is_commutative = false);
+
   // Helper function to emit a shorter variant of XCHG if at least one operand is RAX/EAX/AX.
   bool try_xchg_rax(CpuRegister dst,
                     CpuRegister src,
diff --git a/compiler/utils/x86_64/assembler_x86_64_test.cc b/compiler/utils/x86_64/assembler_x86_64_test.cc
index e351baafee..2d72bf6239 100644
--- a/compiler/utils/x86_64/assembler_x86_64_test.cc
+++ b/compiler/utils/x86_64/assembler_x86_64_test.cc
@@ -1449,12 +1449,20 @@ TEST_F(AssemblerX86_64AVXTest, MovdquLoad) {
   DriverStr(RepeatFA(&x86_64::X86_64Assembler::movdqu, "vmovdqu {mem}, %{reg}"), "avx_movdqu_l");
 }
 
+TEST_F(AssemblerX86_64Test, Movq1) {
+  DriverStr(RepeatFR(&x86_64::X86_64Assembler::movq, "movq %{reg2}, %{reg1}"), "movq.1");
+}
+
+TEST_F(AssemblerX86_64Test, Movq2) {
+  DriverStr(RepeatRF(&x86_64::X86_64Assembler::movq, "movq %{reg2}, %{reg1}"), "movq.2");
+}
+
 TEST_F(AssemblerX86_64Test, Movd1) {
-  DriverStr(RepeatFR(&x86_64::X86_64Assembler::movd, "movd %{reg2}, %{reg1}"), "movd.1");
+  DriverStr(RepeatFr(&x86_64::X86_64Assembler::movd, "movd %{reg2}, %{reg1}"), "movd.1");
 }
 
 TEST_F(AssemblerX86_64Test, Movd2) {
-  DriverStr(RepeatRF(&x86_64::X86_64Assembler::movd, "movd %{reg2}, %{reg1}"), "movd.2");
+  DriverStr(RepeatrF(&x86_64::X86_64Assembler::movd, "movd %{reg2}, %{reg1}"), "movd.2");
 }
 
 TEST_F(AssemblerX86_64Test, Addss) {
@@ -2085,59 +2093,35 @@ TEST_F(AssemblerX86_64Test, Punpckhqdq) {
 }
 
 TEST_F(AssemblerX86_64Test, Psllw) {
-  GetAssembler()->psllw(x86_64::XmmRegister(x86_64::XMM0),  x86_64::Immediate(1));
-  GetAssembler()->psllw(x86_64::XmmRegister(x86_64::XMM15), x86_64::Immediate(2));
-  DriverStr("psllw $1, %xmm0\n"
-            "psllw $2, %xmm15\n", "psllwi");
+  DriverStr(RepeatFI(&x86_64::X86_64Assembler::psllw, 4u, "psllw ${imm}, %{reg}"), "psllwi");
 }
 
 TEST_F(AssemblerX86_64Test, Pslld) {
-  GetAssembler()->pslld(x86_64::XmmRegister(x86_64::XMM0),  x86_64::Immediate(1));
-  GetAssembler()->pslld(x86_64::XmmRegister(x86_64::XMM15), x86_64::Immediate(2));
-  DriverStr("pslld $1, %xmm0\n"
-            "pslld $2, %xmm15\n", "pslldi");
+  DriverStr(RepeatFI(&x86_64::X86_64Assembler::pslld, 5u, "pslld ${imm}, %{reg}"), "pslldi");
 }
 
 TEST_F(AssemblerX86_64Test, Psllq) {
-  GetAssembler()->psllq(x86_64::XmmRegister(x86_64::XMM0),  x86_64::Immediate(1));
-  GetAssembler()->psllq(x86_64::XmmRegister(x86_64::XMM15), x86_64::Immediate(2));
-  DriverStr("psllq $1, %xmm0\n"
-            "psllq $2, %xmm15\n", "psllqi");
+  DriverStr(RepeatFI(&x86_64::X86_64Assembler::psllq, 6u, "psllq ${imm}, %{reg}"), "psllqi");
 }
 
 TEST_F(AssemblerX86_64Test, Psraw) {
-  GetAssembler()->psraw(x86_64::XmmRegister(x86_64::XMM0),  x86_64::Immediate(1));
-  GetAssembler()->psraw(x86_64::XmmRegister(x86_64::XMM15), x86_64::Immediate(2));
-  DriverStr("psraw $1, %xmm0\n"
-            "psraw $2, %xmm15\n", "psrawi");
+  DriverStr(RepeatFI(&x86_64::X86_64Assembler::psraw, 4u, "psraw ${imm}, %{reg}"), "psrawi");
 }
 
 TEST_F(AssemblerX86_64Test, Psrad) {
-  GetAssembler()->psrad(x86_64::XmmRegister(x86_64::XMM0),  x86_64::Immediate(1));
-  GetAssembler()->psrad(x86_64::XmmRegister(x86_64::XMM15), x86_64::Immediate(2));
-  DriverStr("psrad $1, %xmm0\n"
-            "psrad $2, %xmm15\n", "psradi");
+  DriverStr(RepeatFI(&x86_64::X86_64Assembler::psrad, 5u, "psrad ${imm}, %{reg}"), "psradi");
 }
 
 TEST_F(AssemblerX86_64Test, Psrlw) {
-  GetAssembler()->psrlw(x86_64::XmmRegister(x86_64::XMM0),  x86_64::Immediate(1));
-  GetAssembler()->psrlw(x86_64::XmmRegister(x86_64::XMM15), x86_64::Immediate(2));
-  DriverStr("psrlw $1, %xmm0\n"
-            "psrlw $2, %xmm15\n", "psrlwi");
+  DriverStr(RepeatFI(&x86_64::X86_64Assembler::psrlw, 4u, "psrlw ${imm}, %{reg}"), "psrlwi");
 }
 
 TEST_F(AssemblerX86_64Test, Psrld) {
-  GetAssembler()->psrld(x86_64::XmmRegister(x86_64::XMM0),  x86_64::Immediate(1));
-  GetAssembler()->psrld(x86_64::XmmRegister(x86_64::XMM15), x86_64::Immediate(2));
-  DriverStr("psrld $1, %xmm0\n"
-            "psrld $2, %xmm15\n", "psrldi");
+  DriverStr(RepeatFI(&x86_64::X86_64Assembler::psrld, 5u, "psrld ${imm}, %{reg}"), "psrldi");
 }
 
 TEST_F(AssemblerX86_64Test, Psrlq) {
-  GetAssembler()->psrlq(x86_64::XmmRegister(x86_64::XMM0),  x86_64::Immediate(1));
-  GetAssembler()->psrlq(x86_64::XmmRegister(x86_64::XMM15), x86_64::Immediate(2));
-  DriverStr("psrlq $1, %xmm0\n"
-            "psrlq $2, %xmm15\n", "psrlqi");
+  DriverStr(RepeatFI(&x86_64::X86_64Assembler::psrlq, 6u, "psrlq ${imm}, %{reg}"), "psrlqi");
 }
 
 TEST_F(AssemblerX86_64Test, Psrldq) {
diff --git a/dalvikvm/Android.bp b/dalvikvm/Android.bp
index 1ab408c484..026689ed6c 100644
--- a/dalvikvm/Android.bp
+++ b/dalvikvm/Android.bp
@@ -40,8 +40,12 @@ art_cc_binary {
     ],
     target: {
         android: {
+            header_libs: [
+                "libnativeloader-headers",
+            ],
             shared_libs: [
                 "libsigchain",
+                "libdl_android",
             ],
         },
         linux: {
diff --git a/dalvikvm/dalvikvm.cc b/dalvikvm/dalvikvm.cc
index 27709fda4a..ebdcef1775 100644
--- a/dalvikvm/dalvikvm.cc
+++ b/dalvikvm/dalvikvm.cc
@@ -14,10 +14,12 @@
  * limitations under the License.
  */
 
+#include <dlfcn.h>
 #include <signal.h>
 #include <stdio.h>
 #include <stdlib.h>
 #include <string.h>
+
 #include <algorithm>
 #include <memory>
 
@@ -27,8 +29,42 @@
 #include "nativehelper/ScopedLocalRef.h"
 #include "nativehelper/toStringArray.h"
 
+#ifdef ART_TARGET_ANDROID
+#include "nativeloader/dlext_namespaces.h"
+#endif
+
 namespace art {
 
+// This complements the treatment of NATIVELOADER_DEFAULT_NAMESPACE_LIBS in
+// art/libnativeloader/native_loader.cpp: The libraries listed in that variable
+// are added to the default namespace, which for dalvikvm runs means they can
+// access all internal libs in com_android_art. However, to allow the opposite
+// direction we need links for them from com_android_art back to default, and
+// that's done here. See comments in native_loader.cpp for full discussion.
+static bool initNativeloaderExtraLibsLinks() {
+#ifdef ART_TARGET_ANDROID
+  const char* links = getenv("NATIVELOADER_DEFAULT_NAMESPACE_LIBS");
+  if (links == nullptr || *links == 0) {
+    return true;
+  }
+  struct android_namespace_t* art_ns = android_get_exported_namespace("com_android_art");
+  if (art_ns == nullptr) {
+    fprintf(stderr,
+            "Warning: com_android_art namespace not found - "
+            "NATIVELOADER_DEFAULT_NAMESPACE_LIBS ignored\n");
+    return true;
+  }
+  if (!android_link_namespaces(art_ns, nullptr, links)) {
+    fprintf(stderr,
+            "Error adding linker namespace links from com_android_art to default for %s: %s",
+            links,
+            dlerror());
+    return false;
+  }
+#endif  // ART_TARGET_ANDROID
+  return true;
+}
+
 // Determine whether or not the specified method is public.
 static bool IsMethodPublic(JNIEnv* env, jclass c, jmethodID method_id) {
   ScopedLocalRef<jobject> reflected(env, env->ToReflectedMethod(c, method_id, JNI_FALSE));
@@ -148,6 +184,10 @@ static int dalvikvm(int argc, char** argv) {
     }
   }
 
+  if (!initNativeloaderExtraLibsLinks()) {
+    return EXIT_FAILURE;
+  }
+
   if (need_extra) {
     fprintf(stderr, "%s must be followed by an additional argument giving a value\n", what);
     return EXIT_FAILURE;
diff --git a/dex2oat/Android.bp b/dex2oat/Android.bp
index 76620b8429..f97aef2deb 100644
--- a/dex2oat/Android.bp
+++ b/dex2oat/Android.bp
@@ -37,7 +37,6 @@ art_cc_defaults {
         "driver/compiler_driver.cc",
         "interpreter/interpreter_switch_impl1.cc",
         "linker/code_info_table_deduper.cc",
-        "linker/elf_writer.cc",
         "linker/elf_writer_quick.cc",
         "linker/image_writer.cc",
         "linker/multi_oat_relative_patcher.cc",
@@ -85,8 +84,8 @@ art_cc_defaults {
         "liblz4",
         "libz",
     ],
-    static_libs: [
-        "libcrypto_for_art", // For SHA-1 checksumming of build ID
+    whole_static_libs: [
+        "libcrypto_static", // Not FIPS tested - for SHA-1 checksumming of build ID only.
     ],
     export_include_dirs: ["."],
 }
@@ -105,15 +104,11 @@ cc_defaults {
     name: "libart-dex2oat_static_base_defaults",
     whole_static_libs: [
         "libbase",
+        "libcrypto_static", // Not FIPS tested - for SHA-1 checksumming of build ID only.
         "liblog",
         "liblz4",
         "libz",
     ],
-    static_libs: [
-        // Cannot use whole_static_libs for libcrypto_for_art since it's a
-        // subset that contains unused functions that depend on missing symbols.
-        "libcrypto_for_art",
-    ],
 }
 
 art_cc_library_static {
@@ -230,9 +225,6 @@ cc_defaults {
                 "libsigchain",
                 "libz",
             ],
-            static_libs: [
-                "libcrypto_for_art",
-            ],
         },
     },
 }
@@ -274,11 +266,6 @@ art_cc_binary {
         },
         host: {
             static_libs: [
-                // Since libcrypto_for_art cannot be a whole_static_libs in
-                // libart-dex2oat_static_base_defaults, and we cannot use a
-                // defaults directly only for host here, we have to repeat this
-                // dependency.
-                "libcrypto_for_art",
                 // Make the host binary static, except for system libraries.
                 // This avoids having to bundle host dynamic libs in prebuilts.
                 "libdex2oat_static",
@@ -338,7 +325,6 @@ art_cc_binary {
         host: {
             // Comments for host in dex2oat apply here too.
             static_libs: [
-                "libcrypto_for_art",
                 "libdex2oatd_static",
             ],
             stl: "c++_static",
@@ -377,7 +363,6 @@ art_cc_binary {
     name: "dex2oats",
     defaults: ["dex2oats-defaults"],
     static_libs: [
-        "libcrypto_for_art",
         "libdex2oat_static",
     ],
     visibility: ["//tools/vendor/google_prebuilts/arc"],
@@ -390,7 +375,6 @@ art_cc_binary {
         "dex2oats-defaults",
     ],
     static_libs: [
-        "libcrypto_for_art",
         "libdex2oatd_static",
     ],
 }
@@ -426,6 +410,9 @@ art_cc_library_static {
 
 art_cc_defaults {
     name: "art_dex2oat_tests_defaults",
+    defaults: [
+        "elfutils_transitive_defaults", // For libelf
+    ],
     device_common_data: [
         ":art-gtest-jars-AbstractMethod",
         ":art-gtest-jars-ArrayClassWithUnresolvedComponent",
@@ -434,6 +421,7 @@ art_cc_defaults {
         ":art-gtest-jars-Dex2oatVdexTestDex",
         ":art-gtest-jars-ImageLayoutA",
         ":art-gtest-jars-ImageLayoutB",
+        ":art-gtest-jars-Interfaces",
         ":art-gtest-jars-LinkageTest",
         ":art-gtest-jars-Main",
         ":art-gtest-jars-MainEmptyUncompressed",
@@ -479,6 +467,8 @@ art_cc_defaults {
         "linker/oat_writer_test.cc",
         "transaction_test.cc",
         "verifier_deps_test.cc",
+        "utils/atomic_dex_ref_map_test.cc",
+        "utils/dedupe_set_test.cc",
         "utils/swap_space_test.cc",
     ],
     target: {
@@ -522,7 +512,7 @@ art_cc_defaults {
         "liblog",
     ],
     static_libs: [
-        "libcrypto_for_art",
+        "libelf",
         "libgmock",
         "liblz4", // libart(d)-dex2oat dependency; must be repeated here since it's a static lib.
     ],
diff --git a/dex2oat/art_standalone_dex2oat_tests.xml b/dex2oat/art_standalone_dex2oat_tests.xml
index d86eb154bb..d58d691556 100644
--- a/dex2oat/art_standalone_dex2oat_tests.xml
+++ b/dex2oat/art_standalone_dex2oat_tests.xml
@@ -33,6 +33,7 @@
         <option name="push" value="art-gtest-jars-Dex2oatVdexTestDex.jar->/data/local/tmp/art_standalone_dex2oat_tests/art-gtest-jars-Dex2oatVdexTestDex.jar" />
         <option name="push" value="art-gtest-jars-ImageLayoutA.jar->/data/local/tmp/art_standalone_dex2oat_tests/art-gtest-jars-ImageLayoutA.jar" />
         <option name="push" value="art-gtest-jars-ImageLayoutB.jar->/data/local/tmp/art_standalone_dex2oat_tests/art-gtest-jars-ImageLayoutB.jar" />
+        <option name="push" value="art-gtest-jars-Interfaces.jar->/data/local/tmp/art_standalone_dex2oat_tests/art-gtest-jars-Interfaces.jar" />
         <option name="push" value="art-gtest-jars-LinkageTest.dex->/data/local/tmp/art_standalone_dex2oat_tests/art-gtest-jars-LinkageTest.dex" />
         <option name="push" value="art-gtest-jars-Main.jar->/data/local/tmp/art_standalone_dex2oat_tests/art-gtest-jars-Main.jar" />
         <option name="push" value="art-gtest-jars-MainEmptyUncompressed.jar->/data/local/tmp/art_standalone_dex2oat_tests/art-gtest-jars-MainEmptyUncompressed.jar" />
diff --git a/dex2oat/common_compiler_driver_test.cc b/dex2oat/common_compiler_driver_test.cc
index 81e06b2345..7ce5e94bcd 100644
--- a/dex2oat/common_compiler_driver_test.cc
+++ b/dex2oat/common_compiler_driver_test.cc
@@ -28,7 +28,7 @@
 
 namespace art {
 
-CommonCompilerDriverTest::CommonCompilerDriverTest() : inaccessible_page_(nullptr) {}
+CommonCompilerDriverTest::CommonCompilerDriverTest() {}
 CommonCompilerDriverTest::~CommonCompilerDriverTest() {}
 
 void CommonCompilerDriverTest::CompileAll(jobject class_loader,
@@ -44,13 +44,7 @@ void CommonCompilerDriverTest::CompileAll(jobject class_loader,
                                timings,
                                &compiler_options_->image_classes_);
 
-  // Verification results in the `callback_` should not be used during compilation.
-  down_cast<QuickCompilerCallbacks*>(callbacks_.get())->SetVerificationResults(
-      reinterpret_cast<VerificationResults*>(inaccessible_page_));
   compiler_driver_->CompileAll(class_loader, dex_files, timings);
-  down_cast<QuickCompilerCallbacks*>(callbacks_.get())->SetVerificationResults(
-      verification_results_.get());
-
   compiler_driver_->FreeThreadPools();
 }
 
@@ -107,19 +101,9 @@ void CommonCompilerDriverTest::SetUp() {
   CommonCompilerTest::SetUp();
 
   CreateCompilerDriver();
-
-  // Note: We cannot use MemMap because some tests tear down the Runtime and destroy
-  // the gMaps, so when destroying the MemMap, the test would crash.
-  const size_t page_size = MemMap::GetPageSize();
-  inaccessible_page_ = mmap(nullptr, page_size, PROT_NONE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
-  CHECK(inaccessible_page_ != MAP_FAILED) << strerror(errno);
 }
 
 void CommonCompilerDriverTest::TearDown() {
-  if (inaccessible_page_ != nullptr) {
-    munmap(inaccessible_page_, MemMap::GetPageSize());
-    inaccessible_page_ = nullptr;
-  }
   image_reservation_.Reset();
   compiler_driver_.reset();
   verification_results_.reset();
diff --git a/dex2oat/common_compiler_driver_test.h b/dex2oat/common_compiler_driver_test.h
index b49d18fa10..36090c0644 100644
--- a/dex2oat/common_compiler_driver_test.h
+++ b/dex2oat/common_compiler_driver_test.h
@@ -68,7 +68,6 @@ class CommonCompilerDriverTest : public CommonCompilerTest {
 
  private:
   MemMap image_reservation_;
-  void* inaccessible_page_;
 };
 
 }  // namespace art
diff --git a/dex2oat/common_transaction_test.cc b/dex2oat/common_transaction_test.cc
index d24acd575d..cd0671de4d 100644
--- a/dex2oat/common_transaction_test.cc
+++ b/dex2oat/common_transaction_test.cc
@@ -33,6 +33,7 @@ class CommonTransactionTestCompilerCallbacks : public CompilerCallbacks {
   void AddUncompilableMethod([[maybe_unused]] MethodReference ref) override {}
   void AddUncompilableClass([[maybe_unused]] ClassReference ref) override {}
   void ClassRejected([[maybe_unused]] ClassReference ref) override {}
+  bool IsUncompilableMethod([[maybe_unused]] MethodReference ref) override { return false; }
 
   verifier::VerifierDeps* GetVerifierDeps() const override { return nullptr; }
 
diff --git a/dex2oat/dex/quick_compiler_callbacks.cc b/dex2oat/dex/quick_compiler_callbacks.cc
index 6261cc27c5..ee055fbfd9 100644
--- a/dex2oat/dex/quick_compiler_callbacks.cc
+++ b/dex2oat/dex/quick_compiler_callbacks.cc
@@ -45,6 +45,13 @@ void QuickCompilerCallbacks::ClassRejected(ClassReference ref) {
   }
 }
 
+bool QuickCompilerCallbacks::IsUncompilableMethod(MethodReference ref) {
+  if (verification_results_ != nullptr) {
+    return verification_results_->IsUncompilableMethod(ref);
+  }
+  return false;
+}
+
 ClassStatus QuickCompilerCallbacks::GetPreviousClassState(ClassReference ref) {
   // If we don't have class unloading enabled in the compiler, we will never see class that were
   // previously verified. Return false to avoid overhead from the lookup in the compiler driver.
diff --git a/dex2oat/dex/quick_compiler_callbacks.h b/dex2oat/dex/quick_compiler_callbacks.h
index bb5bed38a2..9e82f2b99b 100644
--- a/dex2oat/dex/quick_compiler_callbacks.h
+++ b/dex2oat/dex/quick_compiler_callbacks.h
@@ -37,6 +37,7 @@ class QuickCompilerCallbacks final : public CompilerCallbacks {
 
   void AddUncompilableMethod(MethodReference ref) override;
   void AddUncompilableClass(ClassReference ref) override;
+  bool IsUncompilableMethod(MethodReference ref) override;
 
   void ClassRejected(ClassReference ref) override;
 
diff --git a/dex2oat/dex2oat.cc b/dex2oat/dex2oat.cc
index a8d3d9346c..5586fafb47 100644
--- a/dex2oat/dex2oat.cc
+++ b/dex2oat/dex2oat.cc
@@ -33,10 +33,6 @@
 
 #if defined(__linux__)
 #include <sched.h>
-#if defined(__arm__)
-#include <sys/personality.h>
-#include <sys/utsname.h>
-#endif  // __arm__
 #endif
 
 #include <android-base/parseint.h>
@@ -500,15 +496,6 @@ class ThreadLocalHashOverride {
   Handle<mirror::Object> old_field_value_;
 };
 
-class OatKeyValueStore : public SafeMap<std::string, std::string> {
- public:
-  using SafeMap::Put;
-
-  iterator Put(const std::string& k, bool v) {
-    return SafeMap::Put(k, v ? OatHeader::kTrueValue : OatHeader::kFalseValue);
-  }
-};
-
 class Dex2Oat final {
  public:
   explicit Dex2Oat(TimingLogger* timings)
@@ -878,6 +865,12 @@ class Dex2Oat final {
         break;
     }
 
+#ifdef ART_USE_RESTRICTED_MODE
+    // TODO(Simulator): support signal handling and implicit checks.
+    compiler_options_->implicit_suspend_checks_ = false;
+    compiler_options_->implicit_null_checks_ = false;
+#endif  // ART_USE_RESTRICTED_MODE
+
     // Done with usage checks, enable watchdog if requested
     if (parser_options->watch_dog_enabled) {
       int64_t timeout = parser_options->watch_dog_timeout_in_ms > 0
@@ -887,7 +880,7 @@ class Dex2Oat final {
     }
 
     // Fill some values into the key-value store for the oat header.
-    key_value_store_.reset(new OatKeyValueStore());
+    key_value_store_.reset(new linker::OatKeyValueStore());
 
     // Automatically force determinism for the boot image and boot image extensions in a host build.
     if (!kIsTargetBuild && (IsBootImage() || IsBootImageExtension())) {
@@ -972,7 +965,8 @@ class Dex2Oat final {
         }
         oss << argv[i];
       }
-      key_value_store_->Put(OatHeader::kDex2OatCmdLineKey, oss.str());
+      key_value_store_->PutNonDeterministic(
+          OatHeader::kDex2OatCmdLineKey, oss.str(), /*allow_truncation=*/true);
     }
     key_value_store_->Put(OatHeader::kDebuggableKey, compiler_options_->debuggable_);
     key_value_store_->Put(OatHeader::kNativeDebuggableKey,
@@ -1270,8 +1264,7 @@ class Dex2Oat final {
         if (!input_vdex_.empty()) {
           std::string error_msg;
           input_vdex_file_ = VdexFile::Open(input_vdex_,
-                                            /* writable */ false,
-                                            /* low_4gb */ false,
+                                            /*low_4gb=*/false,
                                             &error_msg);
         }
 
@@ -1321,8 +1314,7 @@ class Dex2Oat final {
           input_vdex_file_ = VdexFile::Open(input_vdex_fd_,
                                             s.st_size,
                                             "vdex",
-                                            /* writable */ false,
-                                            /* low_4gb */ false,
+                                            /*low_4gb=*/false,
                                             &error_msg);
           // If there's any problem with the passed vdex, just warn and proceed
           // without it.
@@ -1375,9 +1367,12 @@ class Dex2Oat final {
     // In theory the files should be the same.
     if (dm_file_ != nullptr) {
       if (input_vdex_file_ == nullptr) {
-        input_vdex_file_ = VdexFile::OpenFromDm(dm_file_location_, *dm_file_);
+        std::string error_msg;
+        input_vdex_file_ = VdexFile::OpenFromDm(dm_file_location_, *dm_file_, &error_msg);
         if (input_vdex_file_ != nullptr) {
           VLOG(verifier) << "Doing fast verification with vdex from DexMetadata archive";
+        } else {
+          LOG(WARNING) << error_msg;
         }
       } else {
         LOG(INFO) << "Ignoring vdex file in dex metadata due to vdex file already being passed";
@@ -1689,7 +1684,10 @@ class Dex2Oat final {
         CompilerFilter::DependsOnImageChecksum(original_compiler_filter)) {
       std::string versions =
           apex_versions_argument_.empty() ? runtime->GetApexVersions() : apex_versions_argument_;
-      key_value_store_->Put(OatHeader::kApexVersionsKey, versions);
+      if (!key_value_store_->PutNonDeterministic(OatHeader::kApexVersionsKey, versions)) {
+        LOG(ERROR) << "Cannot store apex versions string because it's too long";
+        return dex2oat::ReturnCode::kOther;
+      }
     }
 
     // Now that we have adjusted whether we generate an image, encode it in the
@@ -1989,7 +1987,6 @@ class Dex2Oat final {
                         dex_files,
                         timings_,
                         &compiler_options_->image_classes_);
-    callbacks_->SetVerificationResults(nullptr);  // Should not be needed anymore.
     driver_->CompileAll(class_loader, dex_files, timings_);
     driver_->FreeThreadPools();
     return class_loader;
@@ -2198,11 +2195,17 @@ class Dex2Oat final {
         }
 
         elf_writer->WriteDynamicSection();
-        elf_writer->WriteDebugInfo(oat_writer->GetDebugInfo());
+        {
+          TimingLogger::ScopedTiming t_wdi("Write DebugInfo", timings_);
+          elf_writer->WriteDebugInfo(oat_writer->GetDebugInfo());
+        }
 
-        if (!elf_writer->End()) {
-          LOG(ERROR) << "Failed to write ELF file " << oat_file->GetPath();
-          return false;
+        {
+          TimingLogger::ScopedTiming t_end("Write ELF End", timings_);
+          if (!elf_writer->End()) {
+            LOG(ERROR) << "Failed to write ELF file " << oat_file->GetPath();
+            return false;
+          }
         }
 
         if (!FlushOutputFile(&vdex_files_[i]) || !FlushOutputFile(&oat_files_[i])) {
@@ -2211,7 +2214,10 @@ class Dex2Oat final {
 
         VLOG(compiler) << "Oat file written successfully: " << oat_filenames_[i];
 
-        oat_writer.reset();
+        {
+          TimingLogger::ScopedTiming t_dow("Destroy OatWriter", timings_);
+          oat_writer.reset();
+        }
         // We may still need the ELF writer later for stripping.
       }
     }
@@ -2900,7 +2906,7 @@ class Dex2Oat final {
 
   std::unique_ptr<CompilerOptions> compiler_options_;
 
-  std::unique_ptr<OatKeyValueStore> key_value_store_;
+  std::unique_ptr<linker::OatKeyValueStore> key_value_store_;
 
   std::unique_ptr<VerificationResults> verification_results_;
 
@@ -3030,26 +3036,6 @@ class Dex2Oat final {
   DISALLOW_IMPLICIT_CONSTRUCTORS(Dex2Oat);
 };
 
-static void b13564922() {
-#if defined(__linux__) && defined(__arm__)
-  int major, minor;
-  struct utsname uts;
-  if (uname(&uts) != -1 &&
-      sscanf(uts.release, "%d.%d", &major, &minor) == 2 &&
-      ((major < 3) || ((major == 3) && (minor < 4)))) {
-    // Kernels before 3.4 don't handle the ASLR well and we can run out of address
-    // space (http://b/13564922). Work around the issue by inhibiting further mmap() randomization.
-    int old_personality = personality(0xffffffff);
-    if ((old_personality & ADDR_NO_RANDOMIZE) == 0) {
-      int new_personality = personality(old_personality | ADDR_NO_RANDOMIZE);
-      if (new_personality == -1) {
-        LOG(WARNING) << "personality(. | ADDR_NO_RANDOMIZE) failed.";
-      }
-    }
-  }
-#endif
-}
-
 class ScopedGlobalRef {
  public:
   explicit ScopedGlobalRef(jobject obj) : obj_(obj) {}
@@ -3113,8 +3099,6 @@ static dex2oat::ReturnCode DoCompilation(Dex2Oat& dex2oat) REQUIRES(!Locks::muta
 }
 
 static dex2oat::ReturnCode Dex2oat(int argc, char** argv) {
-  b13564922();
-
   TimingLogger timings("compiler", false, false);
 
   // Allocate `dex2oat` on the heap instead of on the stack, as Clang
diff --git a/dex2oat/dex2oat_test.cc b/dex2oat/dex2oat_test.cc
index f81345b345..31891fd2b5 100644
--- a/dex2oat/dex2oat_test.cc
+++ b/dex2oat/dex2oat_test.cc
@@ -1430,7 +1430,6 @@ TEST_F(Dex2oatTest, DontExtract) {
   {
     // Check the vdex doesn't have dex.
     std::unique_ptr<VdexFile> vdex(VdexFile::Open(vdex_location,
-                                                  /*writable=*/false,
                                                   /*low_4gb=*/false,
                                                   &error_msg));
     ASSERT_TRUE(vdex != nullptr);
@@ -1776,7 +1775,6 @@ TEST_F(Dex2oatTest, DontCopyPlainDex) {
 
   // Check that the vdex doesn't have dex code.
   std::unique_ptr<VdexFile> vdex(VdexFile::Open(vdex_location,
-                                                /*writable=*/false,
                                                 /*low_4gb=*/false,
                                                 &error_msg));
   ASSERT_TRUE(vdex != nullptr);
@@ -1797,7 +1795,7 @@ TEST_F(Dex2oatTest, AppImageResolveStrings) {
           bool mutated_successfully = false;
           // Change the dex instructions to make an opcode that spans past the end of the code item.
           for (ClassAccessor accessor : dex->GetClasses()) {
-            if (accessor.GetDescriptor() == std::string("LStringLiterals$StartupClass;")) {
+            if (accessor.GetDescriptorView() == "LStringLiterals$StartupClass;") {
               classes.push_back(accessor.GetClassIdx());
             }
             for (const ClassAccessor::Method& method : accessor.GetMethods()) {
@@ -1816,13 +1814,6 @@ TEST_F(Dex2oatTest, AppImageResolveStrings) {
                 const_cast<Instruction&>(last_instruction.Inst())
                     .SetOpcode(Instruction::CONST_STRING_JUMBO);
                 mutated_successfully = true;
-                // Test that the safe iterator doesn't go past the end.
-                SafeDexInstructionIterator it2(instructions.begin(), instructions.end());
-                while (!it2.IsErrorState()) {
-                  ++it2;
-                }
-                EXPECT_TRUE(it2 == last_instruction);
-                EXPECT_TRUE(it2 < instructions.end());
                 methods.push_back(method.GetIndex());
                 mutated_successfully = true;
               } else if (method_name == "startUpMethod") {
@@ -1890,8 +1881,6 @@ TEST_F(Dex2oatTest, AppImageResolveStrings) {
     // Classes initializers
     EXPECT_TRUE(seen.find("Startup init") != seen.end());
     EXPECT_TRUE(seen.find("Other class init") == seen.end());
-    // Expect the sets match.
-    EXPECT_GE(seen.size(), seen.size());
 
     // Verify what strings are marked as boot image.
     std::set<std::string> boot_image_strings;
@@ -2040,19 +2029,15 @@ TEST_F(Dex2oatTest, LoadOutOfDateOatFile) {
     {
       std::string error_msg;
       std::unique_ptr<ElfFile> elf_file(ElfFile::Open(file.get(),
-                                                      /*writable=*/false,
-                                                      /*program_header_only=*/true,
                                                       /*low_4gb=*/false,
                                                       &error_msg));
       ASSERT_TRUE(elf_file != nullptr) << error_msg;
-      ASSERT_TRUE(elf_file->Load(file.get(),
-                                 /*executable=*/false,
+      ASSERT_TRUE(elf_file->Load(/*executable=*/false,
                                  /*low_4gb=*/false,
                                  /*reservation=*/nullptr,
                                  &error_msg))
           << error_msg;
-      const uint8_t* base_address = elf_file->Is64Bit() ? elf_file->GetImpl64()->GetBaseAddress() :
-                                                          elf_file->GetImpl32()->GetBaseAddress();
+      const uint8_t* base_address = elf_file->GetBaseAddress();
       const uint8_t* oatdata = elf_file->FindDynamicSymbolAddress("oatdata");
       ASSERT_TRUE(oatdata != nullptr);
       ASSERT_TRUE(oatdata > base_address);
diff --git a/dex2oat/dex2oat_vdex_test.cc b/dex2oat/dex2oat_vdex_test.cc
index afea20cadf..4dc64e3289 100644
--- a/dex2oat/dex2oat_vdex_test.cc
+++ b/dex2oat/dex2oat_vdex_test.cc
@@ -74,7 +74,6 @@ class Dex2oatVdexTest : public Dex2oatEnvironmentTest {
     // Verify the vdex file content: only the classes using public APIs should be verified.
     std::string error_msg;
     std::unique_ptr<VdexFile> vdex(VdexFile::Open(vdex_location,
-                                                  /*writable=*/false,
                                                   /*low_4gb=*/false,
                                                   &error_msg));
     // Check the vdex doesn't have dex.
diff --git a/dex2oat/driver/compiler_driver-inl.h b/dex2oat/driver/compiler_driver-inl.h
index a928b6dd02..e416770f91 100644
--- a/dex2oat/driver/compiler_driver-inl.h
+++ b/dex2oat/driver/compiler_driver-inl.h
@@ -29,6 +29,7 @@
 #include "mirror/dex_cache-inl.h"
 #include "runtime.h"
 #include "scoped_thread_state_change-inl.h"
+#include "utils/atomic_dex_ref_map-inl.h"
 
 namespace art {
 
@@ -99,6 +100,11 @@ inline std::pair<bool, bool> CompilerDriver::IsFastInstanceField(
   return std::make_pair(fast_get, fast_put);
 }
 
+inline const CompilerDriver::CompiledMethodArray* CompilerDriver::GetCompiledMethods(
+    const DexFile* dex_file) const {
+  return compiled_methods_.GetArray(dex_file);
+}
+
 }  // namespace art
 
 #endif  // ART_DEX2OAT_DRIVER_COMPILER_DRIVER_INL_H_
diff --git a/dex2oat/driver/compiler_driver.cc b/dex2oat/driver/compiler_driver.cc
index bbb70e4ef1..7f4ec13f1b 100644
--- a/dex2oat/driver/compiler_driver.cc
+++ b/dex2oat/driver/compiler_driver.cc
@@ -565,6 +565,7 @@ static void CompileMethodQuick(
 void CompilerDriver::Resolve(jobject class_loader,
                              const std::vector<const DexFile*>& dex_files,
                              TimingLogger* timings) {
+  TimingLogger::ScopedTiming t("Resolve Types", timings);
   // Resolution allocates classes and needs to run single-threaded to be deterministic.
   bool force_determinism = GetCompilerOptions().IsForceDeterminism();
   ThreadPool* resolve_thread_pool = force_determinism
@@ -586,6 +587,7 @@ void CompilerDriver::Resolve(jobject class_loader,
 void CompilerDriver::ResolveConstStrings(const std::vector<const DexFile*>& dex_files,
                                          bool only_startup_strings,
                                          TimingLogger* timings) {
+  TimingLogger::ScopedTiming t("Resolve const-string Strings", timings);
   const ProfileCompilationInfo* profile_compilation_info =
       GetCompilerOptions().GetProfileCompilationInfo();
   if (only_startup_strings && profile_compilation_info == nullptr) {
@@ -601,7 +603,6 @@ void CompilerDriver::ResolveConstStrings(const std::vector<const DexFile*>& dex_
 
   for (const DexFile* dex_file : dex_files) {
     dex_cache.Assign(class_linker->FindDexCache(soa.Self(), *dex_file));
-    TimingLogger::ScopedTiming t("Resolve const-string Strings", timings);
 
     ProfileCompilationInfo::ProfileIndexType profile_index =
         ProfileCompilationInfo::MaxProfileIndex();
@@ -1404,10 +1405,9 @@ class ClinitImageUpdate {
       } else if (can_include_in_image) {
         // Check whether the class is initialized and has a clinit or static fields.
         // Such classes must be kept too.
-        if (klass->IsInitialized()) {
+        if (klass->IsInitialized() && !klass->IsArrayClass()) {
           PointerSize pointer_size = Runtime::Current()->GetClassLinker()->GetImagePointerSize();
-          if (klass->FindClassInitializer(pointer_size) != nullptr ||
-              klass->NumStaticFields() != 0) {
+          if (klass->FindClassInitializer(pointer_size) != nullptr || klass->HasStaticFields()) {
             DCHECK(!Runtime::Current()->GetHeap()->ObjectIsInBootImageSpace(klass->GetDexCache()))
                 << klass->PrettyDescriptor();
             data_->image_classes_.push_back(data_->hs_.NewHandle(klass));
@@ -1768,7 +1768,7 @@ void CompilerDriver::ResolveDexFile(jobject class_loader,
                                     size_t thread_count,
                                     TimingLogger* timings) {
   ScopedTrace trace(__FUNCTION__);
-  TimingLogger::ScopedTiming t("Resolve Types", timings);
+  TimingLogger::ScopedTiming t("Resolve Dex File", timings);
   ClassLinker* class_linker = Runtime::Current()->GetClassLinker();
 
   // TODO: we could resolve strings here, although the string table is largely filled with class
@@ -1814,7 +1814,8 @@ static void LoadAndUpdateStatus(const ClassAccessor& accessor,
     // a boot image class, or a class in a different dex file for multidex, and
     // we should not update the status in that case.
     if (&cls->GetDexFile() == &accessor.GetDexFile()) {
-      VLOG(compiler) << "Updating class status of " << accessor.GetDescriptor() << " to " << status;
+      VLOG(compiler) << "Updating class status of " << accessor.GetDescriptorView()
+                     << " to " << status;
       ObjectLock<mirror::Class> lock(self, cls);
       mirror::Class::SetStatus(cls, status, self);
     }
@@ -1881,7 +1882,7 @@ bool CompilerDriver::FastVerify(jobject jclass_loader,
           // If the class will be in the image, we can rely on the ArtMethods
           // telling that they need access checks.
           VLOG(compiler) << "Promoting "
-                         << std::string(accessor.GetDescriptor())
+                         << accessor.GetDescriptorView()
                          << " from needs access checks to verified given it is an image class";
           status = ClassStatus::kVerified;
         }
@@ -1932,18 +1933,21 @@ void CompilerDriver::Verify(jobject jclass_loader,
     }
   }
 
-  // Verification updates VerifierDeps and needs to run single-threaded to be deterministic.
-  bool force_determinism = GetCompilerOptions().IsForceDeterminism();
-  ThreadPool* verify_thread_pool =
-      force_determinism ? single_thread_pool_.get() : parallel_thread_pool_.get();
-  size_t verify_thread_count = force_determinism ? 1U : parallel_thread_count_;
-  for (const DexFile* dex_file : dex_files) {
-    CHECK(dex_file != nullptr);
-    VerifyDexFile(jclass_loader,
-                  *dex_file,
-                  verify_thread_pool,
-                  verify_thread_count,
-                  timings);
+  {
+    TimingLogger::ScopedTiming t("Verify Classes", timings);
+    // Verification updates VerifierDeps and needs to run single-threaded to be deterministic.
+    bool force_determinism = GetCompilerOptions().IsForceDeterminism();
+    ThreadPool* verify_thread_pool =
+        force_determinism ? single_thread_pool_.get() : parallel_thread_pool_.get();
+    size_t verify_thread_count = force_determinism ? 1U : parallel_thread_count_;
+    for (const DexFile* dex_file : dex_files) {
+      CHECK(dex_file != nullptr);
+      VerifyDexFile(jclass_loader,
+                    *dex_file,
+                    verify_thread_pool,
+                    verify_thread_count,
+                    timings);
+    }
   }
 
   if (main_verifier_deps != nullptr) {
@@ -2275,8 +2279,9 @@ class InitializeClassVisitor : public CompilationVisitor {
         // cannot be initialized, no need to proceed.
         old_status = klass->GetStatus();
 
+        ClassAccessor accessor(klass->GetDexFile(), klass->GetDexClassDefIndex());
         bool too_many_encoded_fields = (!is_boot_image && !is_boot_image_extension) &&
-            klass->NumStaticFields() > kMaxEncodedFields;
+            accessor.NumStaticFields() > kMaxEncodedFields;
 
         bool have_profile = (compiler_options.GetProfileCompilationInfo() != nullptr) &&
             !compiler_options.GetProfileCompilationInfo()->IsEmpty();
@@ -2608,7 +2613,7 @@ class InitializeClassVisitor : public CompilationVisitor {
 void CompilerDriver::InitializeClasses(jobject jni_class_loader,
                                        const DexFile& dex_file,
                                        TimingLogger* timings) {
-  TimingLogger::ScopedTiming t("InitializeNoClinit", timings);
+  TimingLogger::ScopedTiming t("Initialize Classes Dex File", timings);
 
   // Initialization allocates objects and needs to run single-threaded to be deterministic.
   bool force_determinism = GetCompilerOptions().IsForceDeterminism();
@@ -2639,6 +2644,7 @@ void CompilerDriver::InitializeClasses(jobject jni_class_loader,
 void CompilerDriver::InitializeClasses(jobject class_loader,
                                        const std::vector<const DexFile*>& dex_files,
                                        TimingLogger* timings) {
+  TimingLogger::ScopedTiming t("Initialize Classes", timings);
   for (const DexFile* dex_file : dex_files) {
     CHECK(dex_file != nullptr);
     InitializeClasses(class_loader, *dex_file, timings);
@@ -2740,6 +2746,7 @@ static void CompileDexFile(CompilerDriver* driver,
 void CompilerDriver::Compile(jobject class_loader,
                              const std::vector<const DexFile*>& dex_files,
                              TimingLogger* timings) {
+  TimingLogger::ScopedTiming t("Compile Methods", timings);
   if (kDebugProfileGuidedCompilation) {
     const ProfileCompilationInfo* profile_compilation_info =
         GetCompilerOptions().GetProfileCompilationInfo();
diff --git a/dex2oat/driver/compiler_driver.h b/dex2oat/driver/compiler_driver.h
index db03ab672e..06c81fb7b7 100644
--- a/dex2oat/driver/compiler_driver.h
+++ b/dex2oat/driver/compiler_driver.h
@@ -135,7 +135,11 @@ class CompilerDriver {
   ClassStatus GetClassStatus(const ClassReference& ref) const;
   bool GetCompiledClass(const ClassReference& ref, ClassStatus* status) const;
 
+  using CompiledMethodArray = dchecked_vector<Atomic<CompiledMethod*>>;
+  const CompiledMethodArray* GetCompiledMethods(const DexFile* dex_file) const;
+
   CompiledMethod* GetCompiledMethod(MethodReference ref) const;
+
   // Add a compiled method.
   void AddCompiledMethod(const MethodReference& method_ref, CompiledMethod* const compiled_method);
   CompiledMethod* RemoveCompiledMethod(const MethodReference& method_ref);
diff --git a/dex2oat/driver/compiler_driver_test.cc b/dex2oat/driver/compiler_driver_test.cc
index 7d4c32b25c..5dc6589d53 100644
--- a/dex2oat/driver/compiler_driver_test.cc
+++ b/dex2oat/driver/compiler_driver_test.cc
@@ -31,6 +31,7 @@
 #include "dex/dex_file_types.h"
 #include "gc/heap.h"
 #include "handle_scope-inl.h"
+#include "instrumentation-inl.h"
 #include "mirror/class-inl.h"
 #include "mirror/class_loader.h"
 #include "mirror/dex_cache-inl.h"
@@ -93,7 +94,11 @@ class CompilerDriverTest : public CommonCompilerDriverTest {
         LOG(INFO) << "MakeExecutable " << method->PrettyMethod() << " code=" << method_code;
       }
     }
-    runtime_->GetInstrumentation()->InitializeMethodsCode(method, /*aot_code=*/ method_code);
+    instrumentation::Instrumentation* instr = runtime_->GetInstrumentation();
+    const void* entrypoint = instr->GetInitialEntrypoint(method->GetAccessFlags(), method_code);
+    CHECK(!instr->IsForcedInterpretOnly());
+    CHECK(!instr->EntryExitStubsInstalled());
+    instr->UpdateMethodsCode(method, entrypoint);
   }
 
   void MakeDexFileExecutable(jobject class_loader, const DexFile& dex_file) {
diff --git a/dex2oat/linker/elf_writer.cc b/dex2oat/linker/elf_writer.cc
deleted file mode 100644
index c050695e32..0000000000
--- a/dex2oat/linker/elf_writer.cc
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Copyright (C) 2012 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "elf_writer.h"
-
-#include "base/unix_file/fd_file.h"
-#include "oat/elf_file.h"
-
-namespace art {
-namespace linker {
-
-uintptr_t ElfWriter::GetOatDataAddress(ElfFile* elf_file) {
-  uintptr_t oatdata_address = elf_file->FindSymbolAddress(SHT_DYNSYM,
-                                                           "oatdata",
-                                                           false);
-  CHECK_NE(0U, oatdata_address);
-  return oatdata_address;
-}
-
-void ElfWriter::GetOatElfInformation(File* file,
-                                     size_t* oat_loaded_size,
-                                     size_t* oat_data_offset) {
-  std::string error_msg;
-  std::unique_ptr<ElfFile> elf_file(ElfFile::Open(file,
-                                                  false,
-                                                  false,
-                                                  /*low_4gb*/false,
-                                                  &error_msg));
-  CHECK(elf_file.get() != nullptr) << error_msg;
-
-  bool success = elf_file->GetLoadedSize(oat_loaded_size, &error_msg);
-  CHECK(success) << error_msg;
-  CHECK_NE(0U, *oat_loaded_size);
-  *oat_data_offset = GetOatDataAddress(elf_file.get());
-  CHECK_NE(0U, *oat_data_offset);
-}
-
-}  // namespace linker
-}  // namespace art
diff --git a/dex2oat/linker/elf_writer.h b/dex2oat/linker/elf_writer.h
index d27d4fa92c..b60be6c728 100644
--- a/dex2oat/linker/elf_writer.h
+++ b/dex2oat/linker/elf_writer.h
@@ -43,15 +43,6 @@ namespace linker {
 
 class ElfWriter {
  public:
-  // Looks up information about location of oat file in elf file container.
-  // Used for ImageWriter to perform memory layout.
-  static void GetOatElfInformation(File* file,
-                                   size_t* oat_loaded_size,
-                                   size_t* oat_data_offset);
-
-  // Returns runtime oat_data runtime address for an opened ElfFile.
-  static uintptr_t GetOatDataAddress(ElfFile* elf_file);
-
   virtual ~ElfWriter() {}
 
   virtual void Start() = 0;
diff --git a/dex2oat/linker/elf_writer_quick.cc b/dex2oat/linker/elf_writer_quick.cc
index 425f9bd554..a98e38026f 100644
--- a/dex2oat/linker/elf_writer_quick.cc
+++ b/dex2oat/linker/elf_writer_quick.cc
@@ -170,6 +170,7 @@ void ElfWriterQuick<ElfTypes>::Start() {
     builder_->GetBuildId()->AllocateVirtualMemory(builder_->GetBuildId()->GetSize());
     builder_->WriteBuildIdSection();
   }
+  builder_->ReserveSpaceForDynamicSection(elf_file_->GetPath());
 }
 
 template <typename ElfTypes>
diff --git a/dex2oat/linker/elf_writer_test.cc b/dex2oat/linker/elf_writer_test.cc
index e1ef575502..4f7c3b4452 100644
--- a/dex2oat/linker/elf_writer_test.cc
+++ b/dex2oat/linker/elf_writer_test.cc
@@ -14,13 +14,19 @@
  * limitations under the License.
  */
 
+#include <gelf.h>
+#include <libelf.h>
 #include <sys/mman.h>  // For the PROT_NONE constant.
 
+#include <cstdint>
+
+#include "android-base/scopeguard.h"
 #include "base/file_utils.h"
 #include "base/mem_map.h"
 #include "base/unix_file/fd_file.h"
 #include "base/utils.h"
 #include "common_compiler_driver_test.h"
+#include "driver/compiler_driver.h"
 #include "elf/elf_builder.h"
 #include "elf_writer_quick.h"
 #include "oat/elf_file.h"
@@ -35,21 +41,85 @@ class ElfWriterTest : public CommonCompilerDriverTest {
   void SetUp() override {
     ReserveImageSpace();
     CommonCompilerTest::SetUp();
+    CreateCompilerDriver();
+  }
+
+  void WriteElf(File* oat_file,
+                const std::vector<uint8_t>& rodata,
+                const std::vector<uint8_t>& text,
+                const std::vector<uint8_t>& data_img_rel_ro,
+                size_t data_img_rel_ro_app_image_offset,
+                size_t bss_size,
+                size_t bss_methods_offset,
+                size_t bss_roots_offset,
+                size_t dex_section_size) {
+    std::unique_ptr<ElfWriter> elf_writer = CreateElfWriterQuick(
+      compiler_driver_->GetCompilerOptions(),
+      oat_file);
+
+    elf_writer->Start();
+    OutputStream* rodata_section = elf_writer->StartRoData();
+
+    elf_writer->PrepareDynamicSection(rodata.size(),
+                                      text.size(),
+                                      data_img_rel_ro.size(),
+                                      data_img_rel_ro_app_image_offset,
+                                      bss_size,
+                                      bss_methods_offset,
+                                      bss_roots_offset,
+                                      dex_section_size);
+
+    ASSERT_TRUE(rodata_section->WriteFully(rodata.data(), rodata.size()));
+    elf_writer->EndRoData(rodata_section);
+
+    OutputStream* text_section = elf_writer->StartText();
+    ASSERT_TRUE(text_section->WriteFully(text.data(), text.size()));
+    elf_writer->EndText(text_section);
+
+    if (!data_img_rel_ro.empty()) {
+      OutputStream* data_img_rel_ro_section = elf_writer->StartDataImgRelRo();
+      ASSERT_TRUE(data_img_rel_ro_section->WriteFully(data_img_rel_ro.data(),
+          data_img_rel_ro.size()));
+      elf_writer->EndDataImgRelRo(data_img_rel_ro_section);
+    }
+
+    elf_writer->WriteDynamicSection();
+    ASSERT_TRUE(elf_writer->End());
   }
 };
 
-#define EXPECT_ELF_FILE_ADDRESS(ef, expected_value, symbol_name, build_map) \
-  do { \
-    void* addr = reinterpret_cast<void*>((ef)->FindSymbolAddress(SHT_DYNSYM, \
-                                                                 symbol_name, \
-                                                                 build_map)); \
-    EXPECT_NE(nullptr, addr); \
-    if ((expected_value) == nullptr) { \
-      (expected_value) = addr; \
-    }                        \
-    EXPECT_EQ(expected_value, addr); \
-    EXPECT_EQ(expected_value, (ef)->FindDynamicSymbolAddress(symbol_name)); \
-  } while (false)
+static void FindSymbolAddress(File* file, const char* symbol_name, /*out*/ uint8_t** addr) {
+  ASSERT_NE(elf_version(EV_CURRENT), EV_NONE) << "libelf initialization failed: " << elf_errmsg(-1);
+
+  Elf* elf = elf_begin(file->Fd(), ELF_C_READ, /*ref=*/nullptr);
+  ASSERT_NE(elf, nullptr) << elf_errmsg(-1);
+  auto elf_cleanup = android::base::make_scope_guard([&]() { elf_end(elf); });
+
+  Elf_Scn* dyn_scn = nullptr;
+  GElf_Shdr scn_hdr;
+  while ((dyn_scn = elf_nextscn(elf, dyn_scn)) != nullptr) {
+    ASSERT_EQ(gelf_getshdr(dyn_scn, &scn_hdr), &scn_hdr) << elf_errmsg(-1);
+    if (scn_hdr.sh_type == SHT_DYNSYM) {
+      break;
+    }
+  }
+  ASSERT_NE(dyn_scn, nullptr) << "Section SHT_DYNSYM not found";
+
+  Elf_Data* data = elf_getdata(dyn_scn, /*data=*/nullptr);
+
+  // Iterate through dynamic section entries.
+  for (int i = 0; i < scn_hdr.sh_size / scn_hdr.sh_entsize; i++) {
+    GElf_Sym sym;
+    ASSERT_EQ(gelf_getsym(data, i, &sym), &sym) << elf_errmsg(-1);
+    const char* name = elf_strptr(elf, scn_hdr.sh_link, sym.st_name);
+    if (strcmp(name, symbol_name) == 0) {
+      *addr = reinterpret_cast<uint8_t*>(sym.st_value);
+      break;
+    }
+  }
+
+  ASSERT_NE(*addr, nullptr) << "Symbol " << symbol_name << "not found";
+}
 
 TEST_F(ElfWriterTest, dlsym) {
   std::string elf_location = GetCoreOatLocation();
@@ -57,67 +127,66 @@ TEST_F(ElfWriterTest, dlsym) {
   LOG(INFO) << "elf_filename=" << elf_filename;
 
   UnreserveImageSpace();
-  void* dl_oatdata = nullptr;
-  void* dl_oatexec = nullptr;
-  void* dl_oatlastword = nullptr;
+  uint8_t* dl_oatdata = nullptr;
+  uint8_t* dl_oatexec = nullptr;
+  uint8_t* dl_oatlastword = nullptr;
 
   std::unique_ptr<File> file(OS::OpenFileForReading(elf_filename.c_str()));
   ASSERT_TRUE(file.get() != nullptr) << elf_filename;
+  ASSERT_NO_FATAL_FAILURE(FindSymbolAddress(file.get(), "oatdata", &dl_oatdata));
+  ASSERT_NO_FATAL_FAILURE(FindSymbolAddress(file.get(), "oatexec", &dl_oatexec));
+  ASSERT_NO_FATAL_FAILURE(FindSymbolAddress(file.get(), "oatlastword", &dl_oatlastword));
   {
     std::string error_msg;
     std::unique_ptr<ElfFile> ef(ElfFile::Open(file.get(),
-                                              /*writable=*/ false,
-                                              /*program_header_only=*/ false,
                                               /*low_4gb=*/false,
                                               &error_msg));
     CHECK(ef.get() != nullptr) << error_msg;
-    EXPECT_ELF_FILE_ADDRESS(ef, dl_oatdata, "oatdata", false);
-    EXPECT_ELF_FILE_ADDRESS(ef, dl_oatexec, "oatexec", false);
-    EXPECT_ELF_FILE_ADDRESS(ef, dl_oatlastword, "oatlastword", false);
-  }
-  {
-    std::string error_msg;
-    std::unique_ptr<ElfFile> ef(ElfFile::Open(file.get(),
-                                              /*writable=*/ false,
-                                              /*program_header_only=*/ false,
-                                              /*low_4gb=*/ false,
-                                              &error_msg));
-    CHECK(ef.get() != nullptr) << error_msg;
-    EXPECT_ELF_FILE_ADDRESS(ef, dl_oatdata, "oatdata", true);
-    EXPECT_ELF_FILE_ADDRESS(ef, dl_oatexec, "oatexec", true);
-    EXPECT_ELF_FILE_ADDRESS(ef, dl_oatlastword, "oatlastword", true);
-  }
-  {
-    std::string error_msg;
-    std::unique_ptr<ElfFile> ef(ElfFile::Open(file.get(),
-                                              /*writable=*/ false,
-                                              /*program_header_only=*/ true,
-                                              /*low_4gb=*/ false,
-                                              &error_msg));
-    CHECK(ef.get() != nullptr) << error_msg;
     size_t size;
     bool success = ef->GetLoadedSize(&size, &error_msg);
     CHECK(success) << error_msg;
     MemMap reservation = MemMap::MapAnonymous("ElfWriterTest#dlsym reservation",
                                               RoundUp(size, MemMap::GetPageSize()),
                                               PROT_NONE,
-                                              /*low_4gb=*/ true,
+                                              /*low_4gb=*/true,
                                               &error_msg);
     CHECK(reservation.IsValid()) << error_msg;
     uint8_t* base = reservation.Begin();
-    success =
-        ef->Load(file.get(), /*executable=*/ false, /*low_4gb=*/ false, &reservation, &error_msg);
+    success = ef->Load(/*executable=*/false, /*low_4gb=*/false, &reservation, &error_msg);
     CHECK(success) << error_msg;
     CHECK(!reservation.IsValid());
     EXPECT_EQ(reinterpret_cast<uintptr_t>(dl_oatdata) + reinterpret_cast<uintptr_t>(base),
-        reinterpret_cast<uintptr_t>(ef->FindDynamicSymbolAddress("oatdata")));
+              reinterpret_cast<uintptr_t>(ef->FindDynamicSymbolAddress("oatdata")));
     EXPECT_EQ(reinterpret_cast<uintptr_t>(dl_oatexec) + reinterpret_cast<uintptr_t>(base),
-        reinterpret_cast<uintptr_t>(ef->FindDynamicSymbolAddress("oatexec")));
+              reinterpret_cast<uintptr_t>(ef->FindDynamicSymbolAddress("oatexec")));
     EXPECT_EQ(reinterpret_cast<uintptr_t>(dl_oatlastword) + reinterpret_cast<uintptr_t>(base),
-        reinterpret_cast<uintptr_t>(ef->FindDynamicSymbolAddress("oatlastword")));
+              reinterpret_cast<uintptr_t>(ef->FindDynamicSymbolAddress("oatlastword")));
   }
 }
 
+static void HasSection(File* file, const char* section_name, /*out*/ bool* result) {
+  ASSERT_NE(elf_version(EV_CURRENT), EV_NONE) << "libelf initialization failed: " << elf_errmsg(-1);
+
+  Elf* elf = elf_begin(file->Fd(), ELF_C_READ, /*ref=*/nullptr);
+  ASSERT_NE(elf, nullptr) << elf_errmsg(-1);
+  auto elf_cleanup = android::base::make_scope_guard([&]() { elf_end(elf); });
+
+  size_t shstrndx = 0;
+  ASSERT_EQ(elf_getshdrstrndx(elf, &shstrndx), 0) << elf_errmsg(-1);
+
+  Elf_Scn* dyn_scn = nullptr;
+  GElf_Shdr scn_hdr;
+  while ((dyn_scn = elf_nextscn(elf, dyn_scn)) != nullptr) {
+    ASSERT_EQ(gelf_getshdr(dyn_scn, &scn_hdr), &scn_hdr) << elf_errmsg(-1);
+    const char* name = elf_strptr(elf, shstrndx, scn_hdr.sh_name);
+    if (strcmp(name, section_name) == 0) {
+      *result = true;
+      return;
+    }
+  }
+  *result = false;
+}
+
 TEST_F(ElfWriterTest, CheckBuildIdPresent) {
   std::string elf_location = GetCoreOatLocation();
   std::string elf_filename = GetSystemImageFilename(elf_location.c_str(), kRuntimeISA);
@@ -125,15 +194,222 @@ TEST_F(ElfWriterTest, CheckBuildIdPresent) {
 
   std::unique_ptr<File> file(OS::OpenFileForReading(elf_filename.c_str()));
   ASSERT_TRUE(file.get() != nullptr);
-  {
+
+  bool result;
+  ASSERT_NO_FATAL_FAILURE(HasSection(file.get(), ".note.gnu.build-id", &result));
+  EXPECT_TRUE(result);
+}
+
+// Check that dynamic sections (.dynamic, .dynsym, .dynstr, .hash) in an oat file are formed
+// correctly so that dynamic symbols can be successfully looked up.
+TEST_F(ElfWriterTest, CheckDynamicSection) {
+  // This function generates an oat file with the specified oat data sizes and offsets and
+  // verifies it:
+  // * Checks that the file can be loaded by the ELF loader.
+  // * Checks that the expected dynamic symbols exist and point to the corresponding data
+  //   in the loaded file.
+  // * Checks the alignment of the oat data.
+  // The function returns the number of dynamic symbols (excluding "lastword" ones) in the
+  // generated oat file.
+  auto verify = [this](size_t rodata_size,
+                       size_t text_size,
+                       size_t data_img_rel_ro_size,
+                       size_t data_img_rel_ro_app_image_offset,
+                       size_t bss_size,
+                       size_t bss_methods_offset,
+                       size_t bss_roots_offset,
+                       size_t dex_section_size,
+                       /*out*/ size_t* number_of_dynamic_symbols) {
+    SCOPED_TRACE(::testing::Message()
+                 << "rodata_size: " << rodata_size << ", text_size: " << text_size
+                 << ", data_img_rel_ro_size: " << data_img_rel_ro_size
+                 << ", data_img_rel_ro_app_image_offset: " << data_img_rel_ro_app_image_offset
+                 << ", bss_size: " << bss_size << ", bss_methods_offset: " << bss_methods_offset
+                 << ", bss_roots_offset: " << bss_roots_offset
+                 << ", dex_section_size: " << dex_section_size);
+
+    *number_of_dynamic_symbols = 1;  // "oatdata".
+    std::vector<uint8_t> rodata(rodata_size, 0xAA);
+    std::vector<uint8_t> text(text_size, 0xBB);
+    std::vector<uint8_t> data_img_rel_ro(data_img_rel_ro_app_image_offset, 0xCC);
+    size_t data_img_rel_ro_app_image_size = data_img_rel_ro_size - data_img_rel_ro_app_image_offset;
+    data_img_rel_ro.insert(data_img_rel_ro.cend(), data_img_rel_ro_app_image_size, 0xDD);
+
+    ScratchFile tmp_base, tmp_oat(tmp_base, ".oat");
+    WriteElf(tmp_oat.GetFile(),
+             rodata,
+             text,
+             data_img_rel_ro,
+             data_img_rel_ro_app_image_offset,
+             bss_size,
+             bss_methods_offset,
+             bss_roots_offset,
+             dex_section_size);
+
     std::string error_msg;
-    std::unique_ptr<ElfFile> ef(ElfFile::Open(file.get(),
-                                              /*writable=*/ false,
-                                              /*program_header_only=*/ false,
-                                              /*low_4gb=*/ false,
+    std::unique_ptr<ElfFile> ef(ElfFile::Open(tmp_oat.GetFile(),
+                                              /*low_4gb=*/false,
                                               &error_msg));
-    CHECK(ef.get() != nullptr) << error_msg;
-    EXPECT_TRUE(ef->HasSection(".note.gnu.build-id"));
+    ASSERT_NE(ef.get(), nullptr) << error_msg;
+    ASSERT_TRUE(ef->Load(/*executable=*/false,
+                         /*low_4gb=*/false,
+                         /*reservation=*/nullptr,
+                         &error_msg))
+        << error_msg;
+
+    const uint8_t* oatdata_ptr = ef->FindDynamicSymbolAddress("oatdata");
+    ASSERT_NE(oatdata_ptr, nullptr);
+    EXPECT_EQ(memcmp(oatdata_ptr, rodata.data(), rodata.size()), 0);
+
+    size_t page_size = GetPageSizeSlow();
+    size_t elf_word_size = ef->Is64Bit() ? sizeof(ElfTypes64::Word) : sizeof(ElfTypes32::Word);
+
+    if (text_size != 0u) {
+      *number_of_dynamic_symbols += 1;
+      const uint8_t* text_ptr = ef->FindDynamicSymbolAddress("oatexec");
+      ASSERT_NE(text_ptr, nullptr);
+      ASSERT_TRUE(IsAlignedParam(text_ptr, page_size));
+      EXPECT_EQ(memcmp(text_ptr, text.data(), text.size()), 0);
+
+      const uint8_t* oatlastword_ptr = ef->FindDynamicSymbolAddress("oatlastword");
+      ASSERT_NE(oatlastword_ptr, nullptr);
+      EXPECT_EQ(static_cast<size_t>(oatlastword_ptr - text_ptr), text_size - elf_word_size);
+    } else if (rodata_size != 0u) {
+      const uint8_t* oatlastword_ptr = ef->FindDynamicSymbolAddress("oatlastword");
+      ASSERT_NE(oatlastword_ptr, nullptr);
+      EXPECT_EQ(static_cast<size_t>(oatlastword_ptr - oatdata_ptr), rodata_size - elf_word_size);
+    }
+
+    if (data_img_rel_ro_size != 0u) {
+      *number_of_dynamic_symbols += 1;
+      const uint8_t* oatdataimgrelro_ptr = ef->FindDynamicSymbolAddress("oatdataimgrelro");
+      ASSERT_NE(oatdataimgrelro_ptr, nullptr);
+      ASSERT_TRUE(IsAlignedParam(oatdataimgrelro_ptr, page_size));
+      EXPECT_EQ(memcmp(oatdataimgrelro_ptr, data_img_rel_ro.data(), data_img_rel_ro.size()), 0);
+
+      const uint8_t* oatdataimgrelrolastword_ptr =
+          ef->FindDynamicSymbolAddress("oatdataimgrelrolastword");
+      ASSERT_NE(oatdataimgrelrolastword_ptr, nullptr);
+      EXPECT_EQ(static_cast<size_t>(oatdataimgrelrolastword_ptr - oatdataimgrelro_ptr),
+          data_img_rel_ro_size - elf_word_size);
+
+      if (data_img_rel_ro_app_image_offset != data_img_rel_ro_size) {
+        *number_of_dynamic_symbols += 1;
+        const uint8_t* oatdataimgrelroappimage_ptr =
+            ef->FindDynamicSymbolAddress("oatdataimgrelroappimage");
+        ASSERT_NE(oatdataimgrelroappimage_ptr, nullptr);
+        EXPECT_EQ(static_cast<size_t>(oatdataimgrelroappimage_ptr - oatdataimgrelro_ptr),
+          data_img_rel_ro_app_image_offset);
+      }
+
+      if (bss_size != 0u) {
+        *number_of_dynamic_symbols += 1;
+        const uint8_t* bss_ptr = ef->FindDynamicSymbolAddress("oatbss");
+        ASSERT_NE(bss_ptr, nullptr);
+        ASSERT_TRUE(IsAlignedParam(bss_ptr, page_size));
+
+        if (bss_methods_offset != bss_roots_offset) {
+          *number_of_dynamic_symbols += 1;
+          const uint8_t* oatbssmethods_ptr = ef->FindDynamicSymbolAddress("oatbssmethods");
+          ASSERT_NE(oatbssmethods_ptr, nullptr);
+          EXPECT_EQ(static_cast<size_t>(oatbssmethods_ptr - bss_ptr), bss_methods_offset);
+        }
+
+        if (bss_roots_offset != bss_size) {
+          *number_of_dynamic_symbols += 1;
+          const uint8_t* oatbssroots_ptr = ef->FindDynamicSymbolAddress("oatbssroots");
+          ASSERT_NE(oatbssroots_ptr, nullptr);
+          EXPECT_EQ(static_cast<size_t>(oatbssroots_ptr - bss_ptr), bss_roots_offset);
+        }
+
+        const uint8_t* oatbsslastword_ptr = ef->FindDynamicSymbolAddress("oatbsslastword");
+        ASSERT_NE(oatbsslastword_ptr, nullptr);
+        EXPECT_EQ(static_cast<size_t>(oatbsslastword_ptr - bss_ptr), bss_size - elf_word_size);
+      }
+    }
+
+    if (dex_section_size != 0u) {
+      *number_of_dynamic_symbols += 1;
+      const uint8_t* dex_ptr = ef->FindDynamicSymbolAddress("oatdex");
+      ASSERT_NE(dex_ptr, nullptr);
+      ASSERT_TRUE(IsAlignedParam(dex_ptr, page_size));
+      const uint8_t* oatdexlastword_ptr = ef->FindDynamicSymbolAddress("oatdexlastword");
+      EXPECT_EQ(static_cast<size_t>(oatdexlastword_ptr - dex_ptr),
+          dex_section_size - elf_word_size);
+    }
+  };
+
+  // If a symbol requires some other ones (e.g. kBssMethods requires kBss),
+  // it should be listed after them.
+  enum class Symbol {
+    kRodata,
+    kText,
+    kDataImgRelRo,
+    kDataImgRelRoAppImage,
+    kBss,
+    kBssMethods,
+    kBssRoots,
+    kDex,
+    kLast = kDex
+  };
+
+  constexpr size_t kNumberOfSymbols = static_cast<size_t>(Symbol::kLast) + 1;
+
+  // Use an unaligned section size to verify that ElfWriter properly aligns sections in this case.
+  // We can use an arbitrary value that is greater than or equal to an ElfWord (4 bytes).
+  constexpr size_t kSectionSize = 127u;
+  // Offset in .data.img.rel.ro section from its beginning. We can use any value in the range
+  // [0, kSectionSize).
+  constexpr size_t kDataImgRelRoAppImageOffset = kSectionSize / 2;
+  // Offsets in .bss from its beginning. We can use any value in the range [0, kSectionSize),
+  // kBssMethodsOffset should be less than or equal to kBssRootsOffset.
+  constexpr size_t kBssMethodsOffset = kSectionSize / 3;
+  constexpr size_t kBssRootsOffset = 2 * kBssMethodsOffset;
+
+  auto exists = [](Symbol symbol, const std::bitset<kNumberOfSymbols> &symbols) {
+    return symbols.test(static_cast<size_t>(symbol));
+  };
+
+  auto get_size = [&](Symbol symbol, const std::bitset<kNumberOfSymbols> &symbols) -> size_t {
+    return exists(symbol, symbols) ? kSectionSize : 0;
+  };
+
+  std::bitset<kNumberOfSymbols> symbols;
+  symbols.set();
+
+  // Check cases that lead to a different number of dynamic symbols in an oat file.
+  // We start with the case where all symbols exist (corresponding to the bitset 11111111)
+  // and continue to the case where only "oatdata" exists:
+  //  11111111 - all symbols exist.
+  //  01111111 - "oatdex" doesn't exist (least significant bit corresponds to "oatdata").
+  //  00111111 - "oatdex" and "oatbss" don't exist.
+  //  ...
+  //  00000001 - only "oatdata" exists.
+  while (symbols.any()) {
+    DCHECK_IMPLIES(exists(Symbol::kDataImgRelRoAppImage, symbols),
+        exists(Symbol::kDataImgRelRo, symbols));
+    DCHECK_IMPLIES(exists(Symbol::kBssMethods, symbols), exists(Symbol::kBss, symbols));
+    DCHECK_IMPLIES(exists(Symbol::kBssRoots, symbols), exists(Symbol::kBss, symbols));
+    DCHECK_IMPLIES(exists(Symbol::kBssRoots, symbols), exists(Symbol::kBssMethods, symbols));
+
+    size_t data_img_rel_ro_size = get_size(Symbol::kDataImgRelRo, symbols);
+    size_t bss_size = get_size(Symbol::kBss, symbols);
+    size_t number_of_dynamic_symbols = 0;
+    verify(get_size(Symbol::kRodata, symbols),
+           get_size(Symbol::kText, symbols),
+           data_img_rel_ro_size,
+           exists(Symbol::kDataImgRelRoAppImage, symbols)
+              ? kDataImgRelRoAppImageOffset
+              : data_img_rel_ro_size,
+           bss_size,
+           exists(Symbol::kBssMethods, symbols) ? kBssMethodsOffset : bss_size,
+           exists(Symbol::kBssRoots, symbols) ? kBssRootsOffset : bss_size,
+           get_size(Symbol::kDex, symbols),
+           &number_of_dynamic_symbols);
+    EXPECT_EQ(number_of_dynamic_symbols, symbols.count())
+      << "number_of_dynamic_symbols: " << number_of_dynamic_symbols
+      << ", symbols: " << symbols;
+    symbols >>= 1;
   }
 }
 
diff --git a/dex2oat/linker/image_test.h b/dex2oat/linker/image_test.h
index 3706b685fa..349462e098 100644
--- a/dex2oat/linker/image_test.h
+++ b/dex2oat/linker/image_test.h
@@ -231,13 +231,12 @@ inline void ImageTest::DoCompile(ImageHeader::StorageMode storage_mode,
       CompileAll(class_loader, class_path, &timings);
 
       TimingLogger::ScopedTiming t("WriteElf", &timings);
-      SafeMap<std::string, std::string> key_value_store;
+      OatKeyValueStore key_value_store;
       key_value_store.Put(OatHeader::kBootClassPathKey,
                           android::base::Join(out_helper.dex_file_locations, ':'));
-      key_value_store.Put(OatHeader::kApexVersionsKey, Runtime::Current()->GetApexVersions());
-      key_value_store.Put(
-          OatHeader::kConcurrentCopying,
-          compiler_options_->EmitReadBarrier() ? OatHeader::kTrueValue : OatHeader::kFalseValue);
+      key_value_store.PutNonDeterministic(OatHeader::kApexVersionsKey,
+                                          Runtime::Current()->GetApexVersions());
+      key_value_store.Put(OatHeader::kConcurrentCopying, compiler_options_->EmitReadBarrier());
 
       std::vector<std::unique_ptr<ElfWriter>> elf_writers;
       std::vector<std::unique_ptr<OatWriter>> oat_writers;
diff --git a/dex2oat/linker/image_writer.cc b/dex2oat/linker/image_writer.cc
index 179a0bb4f7..04bf4833cd 100644
--- a/dex2oat/linker/image_writer.cc
+++ b/dex2oat/linker/image_writer.cc
@@ -761,23 +761,12 @@ ImageWriter::Bin ImageWriter::GetImageBin(mirror::Object* object) {
 
         // If the class's static fields are all final, put it into a separate bin
         // since it's very likely it will stay clean.
-        uint32_t num_static_fields = as_klass->NumStaticFields();
-        if (num_static_fields == 0) {
+        auto fields = as_klass->GetFields();
+        bool all_final = std::all_of(fields.begin(),
+                                     fields.end(),
+                                     [](ArtField& f) { return !f.IsStatic() || f.IsFinal(); });
+        if (all_final) {
           bin = Bin::kClassInitializedFinalStatics;
-        } else {
-          // Maybe all the statics are final?
-          bool all_final = true;
-          for (uint32_t i = 0; i < num_static_fields; ++i) {
-            ArtField* field = as_klass->GetStaticField(i);
-            if (!field->IsFinal()) {
-              all_final = false;
-              break;
-            }
-          }
-
-          if (all_final) {
-            bin = Bin::kClassInitializedFinalStatics;
-          }
         }
       }
     } else if (!klass->HasSuperClass()) {
@@ -1415,28 +1404,24 @@ void ImageWriter::RecordNativeRelocations(ObjPtr<mirror::Class> klass, size_t oa
     // Extra consistency check: no boot loader classes should be left!
     CHECK(!klass->IsBootStrapClassLoaded()) << klass->PrettyClass();
   }
-  LengthPrefixedArray<ArtField>* fields[] = {
-      klass->GetSFieldsPtr(), klass->GetIFieldsPtr(),
-  };
   ImageInfo& image_info = GetImageInfo(oat_index);
-  for (LengthPrefixedArray<ArtField>* cur_fields : fields) {
-    // Total array length including header.
-    if (cur_fields != nullptr) {
-      // Forward the entire array at once.
-      size_t offset = image_info.GetBinSlotSize(Bin::kArtField);
-      DCHECK(!IsInBootImage(cur_fields));
-      bool inserted =
-          native_object_relocations_.insert(std::make_pair(
-              cur_fields,
-              NativeObjectRelocation{
-                  oat_index, offset, NativeObjectRelocationType::kArtFieldArray
-              })).second;
-      CHECK(inserted) << "Field array " << cur_fields << " already forwarded";
-      const size_t size = LengthPrefixedArray<ArtField>::ComputeSize(cur_fields->size());
-      offset += size;
-      image_info.IncrementBinSlotSize(Bin::kArtField, size);
-      DCHECK_EQ(offset, image_info.GetBinSlotSize(Bin::kArtField));
-    }
+  LengthPrefixedArray<ArtField>* fields = klass->GetFieldsPtr();
+  // Total array length including header.
+  if (fields != nullptr) {
+    // Forward the entire array at once.
+    size_t offset = image_info.GetBinSlotSize(Bin::kArtField);
+    DCHECK(!IsInBootImage(fields));
+    bool inserted =
+        native_object_relocations_.insert(std::make_pair(
+            fields,
+            NativeObjectRelocation{
+                oat_index, offset, NativeObjectRelocationType::kArtFieldArray
+            })).second;
+    CHECK(inserted) << "Field array " << fields << " already forwarded";
+    const size_t size = LengthPrefixedArray<ArtField>::ComputeSize(fields->size());
+    offset += size;
+    image_info.IncrementBinSlotSize(Bin::kArtField, size);
+    DCHECK_EQ(offset, image_info.GetBinSlotSize(Bin::kArtField));
   }
   // Visit and assign offsets for methods.
   size_t num_methods = klass->NumMethods();
@@ -3334,8 +3319,7 @@ T* ImageWriter::NativeLocationInImage(T* obj) {
 ArtField* ImageWriter::NativeLocationInImage(ArtField* src_field) {
   // Fields are not individually stored in the native relocation map. Use the field array.
   ObjPtr<mirror::Class> declaring_class = src_field->GetDeclaringClass<kWithoutReadBarrier>();
-  LengthPrefixedArray<ArtField>* src_fields =
-      src_field->IsStatic() ? declaring_class->GetSFieldsPtr() : declaring_class->GetIFieldsPtr();
+  LengthPrefixedArray<ArtField>* src_fields = declaring_class->GetFieldsPtr();
   DCHECK(src_fields != nullptr);
   LengthPrefixedArray<ArtField>* dst_fields = NativeLocationInImage(src_fields);
   DCHECK(dst_fields != nullptr);
diff --git a/dex2oat/linker/multi_oat_relative_patcher.cc b/dex2oat/linker/multi_oat_relative_patcher.cc
index 15f495e05e..df17314720 100644
--- a/dex2oat/linker/multi_oat_relative_patcher.cc
+++ b/dex2oat/linker/multi_oat_relative_patcher.cc
@@ -50,7 +50,6 @@ MultiOatRelativePatcher::MultiOatRelativePatcher(InstructionSet instruction_set,
 }
 
 void MultiOatRelativePatcher::StartOatFile(uint32_t adjustment) {
-  DCHECK_ALIGNED(adjustment, kElfSegmentAlignment);
   adjustment_ = adjustment;
 
   start_size_code_alignment_ = relative_patcher_->CodeAlignmentSize();
diff --git a/dex2oat/linker/multi_oat_relative_patcher.h b/dex2oat/linker/multi_oat_relative_patcher.h
index 2daada44bc..58f2922fd2 100644
--- a/dex2oat/linker/multi_oat_relative_patcher.h
+++ b/dex2oat/linker/multi_oat_relative_patcher.h
@@ -52,7 +52,8 @@ class MultiOatRelativePatcher final {
   // It must must never point directly to a method's code to avoid relative offsets
   // with value 0 because this value is used as a missing offset indication in
   // GetOffset() and an error indication in WriteThunks(). Additionally, it must be
-  // page-aligned, so that it does not skew alignment calculations, say arm64 ADRP.
+  // relative to a page-aligned boundary, so that it does not skew alignment calculations,
+  // say arm64 ADRP.
   void StartOatFile(uint32_t adjustment);
 
   // Get relative offset. Returns 0 when the offset has not been set yet.
diff --git a/dex2oat/linker/multi_oat_relative_patcher_test.cc b/dex2oat/linker/multi_oat_relative_patcher_test.cc
index b2aa619337..a610bde6e9 100644
--- a/dex2oat/linker/multi_oat_relative_patcher_test.cc
+++ b/dex2oat/linker/multi_oat_relative_patcher_test.cc
@@ -27,7 +27,7 @@ namespace linker {
 
 static const MethodReference kNullMethodRef = MethodReference(nullptr, 0u);
 
-class MultiOatRelativePatcherTest : public testing::Test {
+class MultiOatRelativePatcherTest : public ::testing::Test {
  protected:
   class MockPatcher : public RelativePatcher {
    public:
diff --git a/dex2oat/linker/oat_writer.cc b/dex2oat/linker/oat_writer.cc
index a2500bd38a..6bd9e57725 100644
--- a/dex2oat/linker/oat_writer.cc
+++ b/dex2oat/linker/oat_writer.cc
@@ -111,6 +111,33 @@ inline uint32_t CodeAlignmentSize(uint32_t header_offset, const CompiledMethod&
 
 }  // anonymous namespace
 
+bool OatKeyValueStore::PutNonDeterministic(const std::string& k,
+                                           const std::string& v,
+                                           bool allow_truncation) {
+  size_t length = OatHeader::GetNonDeterministicFieldLength(k);
+  DCHECK_GT(length, 0u);
+  if (v.length() <= length) {
+    map_.Put(k, v);
+    return true;
+  }
+  if (allow_truncation) {
+    LOG(WARNING) << "Key value store field " << k << " too long. Truncating";
+    map_.Put(k, v.substr(/*pos=*/0, length));
+    return true;
+  }
+  return false;
+}
+
+void OatKeyValueStore::Put(const std::string& k, const std::string& v) {
+  DCHECK(OatHeader::IsDeterministicField(k));
+  map_.Put(k, v);
+}
+
+void OatKeyValueStore::Put(const std::string& k, bool v) {
+  DCHECK(OatHeader::IsDeterministicField(k));
+  map_.Put(k, v ? OatHeader::kTrueValue : OatHeader::kFalseValue);
+}
+
 // .bss mapping offsets used for BCP DexFiles.
 struct OatWriter::BssMappingInfo {
   // Offsets set in PrepareLayout.
@@ -550,7 +577,7 @@ bool OatWriter::WriteAndOpenDexFiles(
 
 bool OatWriter::StartRoData(const std::vector<const DexFile*>& dex_files,
                             OutputStream* oat_rodata,
-                            SafeMap<std::string, std::string>* key_value_store) {
+                            OatKeyValueStore* key_value_store) {
   CHECK(write_state_ == WriteState::kStartRoData);
 
   // Record the ELF rodata section offset, i.e. the beginning of the OAT data.
@@ -601,6 +628,11 @@ void OatWriter::PrepareLayout(MultiOatRelativePatcher* relative_patcher) {
   InstructionSet instruction_set = compiler_options_.GetInstructionSet();
   CHECK_EQ(instruction_set, oat_header_->GetInstructionSet());
 
+  {
+    TimingLogger::ScopedTiming split("InitBssAndRelRoData", timings_);
+    InitBssAndRelRoData();
+  }
+
   {
     TimingLogger::ScopedTiming split("InitBssLayout", timings_);
     InitBssLayout(instruction_set);
@@ -646,7 +678,8 @@ void OatWriter::PrepareLayout(MultiOatRelativePatcher* relative_patcher) {
     offset = InitDataImgRelRoLayout(offset);
   }
   oat_size_ = offset;  // .bss does not count towards oat_size_.
-  bss_start_ = (bss_size_ != 0u) ? RoundUp(oat_size_, kElfSegmentAlignment) : 0u;
+  bss_start_ = (bss_size_ != 0u) ?
+    GetOffsetFromOatDataAlignedToFile(oat_size_, kElfSegmentAlignment) : 0u;
 
   CHECK_EQ(dex_files_->size(), oat_dex_files_.size());
 
@@ -654,6 +687,7 @@ void OatWriter::PrepareLayout(MultiOatRelativePatcher* relative_patcher) {
 }
 
 OatWriter::~OatWriter() {
+  OatHeader::Delete(oat_header_);
 }
 
 class OatWriter::DexMethodVisitor {
@@ -730,88 +764,84 @@ static bool HasCompiledCode(const CompiledMethod* method) {
   return method != nullptr && !method->GetQuickCode().empty();
 }
 
-class OatWriter::InitBssLayoutMethodVisitor : public DexMethodVisitor {
- public:
-  explicit InitBssLayoutMethodVisitor(OatWriter* writer)
-      : DexMethodVisitor(writer, /* offset */ 0u) {}
-
-  bool VisitMethod([[maybe_unused]] size_t class_def_method_index,
-                   const ClassAccessor::Method& method) override {
-    // Look for patches with .bss references and prepare maps with placeholders for their offsets.
-    CompiledMethod* compiled_method = writer_->compiler_driver_->GetCompiledMethod(
-        MethodReference(dex_file_, method.GetIndex()));
-    if (HasCompiledCode(compiled_method)) {
+void OatWriter::InitBssAndRelRoData() {
+  for (const DexFile* dex_file : *dex_files_) {
+    const dchecked_vector<Atomic<CompiledMethod*>>* compiled_methods =
+        compiler_driver_->GetCompiledMethods(dex_file);
+    if (compiled_methods == nullptr) {
+      continue;
+    }
+    for (const Atomic<CompiledMethod*>& entry : *compiled_methods) {
+      CompiledMethod* compiled_method = entry.load(std::memory_order_relaxed);
+      if (compiled_method == nullptr) {
+        continue;
+      }
+      DCHECK_IMPLIES(!compiled_method->GetPatches().empty(), HasCompiledCode(compiled_method));
       for (const LinkerPatch& patch : compiled_method->GetPatches()) {
         if (patch.GetType() == LinkerPatch::Type::kBootImageRelRo) {
-          writer_->boot_image_rel_ro_entries_.Overwrite(patch.BootImageOffset(),
-                                                        /* placeholder */ 0u);
+          boot_image_rel_ro_entries_.Overwrite(patch.BootImageOffset(), /* placeholder */ 0u);
         } else if (patch.GetType() == LinkerPatch::Type::kMethodAppImageRelRo) {
           MethodReference target_method = patch.TargetMethod();
-          writer_->app_image_rel_ro_method_entries_.Overwrite(target_method, /* placeholder */ 0u);
+          app_image_rel_ro_method_entries_.Overwrite(target_method, /* placeholder */ 0u);
         } else if (patch.GetType() == LinkerPatch::Type::kMethodBssEntry) {
           MethodReference target_method = patch.TargetMethod();
           AddBssReference(target_method,
                           target_method.dex_file->NumMethodIds(),
-                          &writer_->bss_method_entry_references_);
-          writer_->bss_method_entries_.Overwrite(target_method, /* placeholder */ 0u);
+                          &bss_method_entry_references_);
+          bss_method_entries_.Overwrite(target_method, /* placeholder */ 0u);
         } else if (patch.GetType() == LinkerPatch::Type::kTypeAppImageRelRo) {
-          writer_->app_image_rel_ro_type_entries_.Overwrite(patch.TargetType(),
-                                                            /* placeholder */ 0u);
+          app_image_rel_ro_type_entries_.Overwrite(patch.TargetType(), /* placeholder */ 0u);
         } else if (patch.GetType() == LinkerPatch::Type::kTypeBssEntry) {
           TypeReference target_type = patch.TargetType();
           AddBssReference(target_type,
                           target_type.dex_file->NumTypeIds(),
-                          &writer_->bss_type_entry_references_);
-          writer_->bss_type_entries_.Overwrite(target_type, /* placeholder */ 0u);
+                          &bss_type_entry_references_);
+          bss_type_entries_.Overwrite(target_type, /* placeholder */ 0u);
         } else if (patch.GetType() == LinkerPatch::Type::kPublicTypeBssEntry) {
           TypeReference target_type = patch.TargetType();
           AddBssReference(target_type,
                           target_type.dex_file->NumTypeIds(),
-                          &writer_->bss_public_type_entry_references_);
-          writer_->bss_public_type_entries_.Overwrite(target_type, /* placeholder */ 0u);
+                          &bss_public_type_entry_references_);
+          bss_public_type_entries_.Overwrite(target_type, /* placeholder */ 0u);
         } else if (patch.GetType() == LinkerPatch::Type::kPackageTypeBssEntry) {
           TypeReference target_type = patch.TargetType();
           AddBssReference(target_type,
                           target_type.dex_file->NumTypeIds(),
-                          &writer_->bss_package_type_entry_references_);
-          writer_->bss_package_type_entries_.Overwrite(target_type, /* placeholder */ 0u);
+                          &bss_package_type_entry_references_);
+          bss_package_type_entries_.Overwrite(target_type, /* placeholder */ 0u);
         } else if (patch.GetType() == LinkerPatch::Type::kStringBssEntry) {
           StringReference target_string = patch.TargetString();
           AddBssReference(target_string,
                           target_string.dex_file->NumStringIds(),
-                          &writer_->bss_string_entry_references_);
-          writer_->bss_string_entries_.Overwrite(target_string, /* placeholder */ 0u);
+                          &bss_string_entry_references_);
+          bss_string_entries_.Overwrite(target_string, /* placeholder */ 0u);
         } else if (patch.GetType() == LinkerPatch::Type::kMethodTypeBssEntry) {
           ProtoReference target_proto = patch.TargetProto();
           AddBssReference(target_proto,
                           target_proto.dex_file->NumProtoIds(),
-                          &writer_->bss_method_type_entry_references_);
-          writer_->bss_method_type_entries_.Overwrite(target_proto, /* placeholder */ 0u);
+                          &bss_method_type_entry_references_);
+          bss_method_type_entries_.Overwrite(target_proto, /* placeholder */ 0u);
         }
       }
-    } else {
-      DCHECK(compiled_method == nullptr || compiled_method->GetPatches().empty());
     }
-    return true;
   }
+}
 
- private:
-  void AddBssReference(const DexFileReference& ref,
-                       size_t number_of_indexes,
-                       /*inout*/ SafeMap<const DexFile*, BitVector>* references) {
-    DCHECK(ContainsElement(*writer_->dex_files_, ref.dex_file) ||
-           ContainsElement(Runtime::Current()->GetClassLinker()->GetBootClassPath(), ref.dex_file));
-    DCHECK_LT(ref.index, number_of_indexes);
-
-    auto refs_it = references->find(ref.dex_file);
-    if (refs_it == references->end()) {
-      refs_it = references->Put(
-          ref.dex_file,
-          BitVector(number_of_indexes, /* expandable */ false, Allocator::GetCallocAllocator()));
-    }
-    refs_it->second.SetBit(ref.index);
+inline void OatWriter::AddBssReference(const DexFileReference& ref,
+                                       size_t number_of_indexes,
+                                       /*inout*/ SafeMap<const DexFile*, BitVector>* references) {
+  DCHECK(ContainsElement(*dex_files_, ref.dex_file) ||
+         ContainsElement(Runtime::Current()->GetClassLinker()->GetBootClassPath(), ref.dex_file));
+  DCHECK_LT(ref.index, number_of_indexes);
+
+  auto refs_it = references->find(ref.dex_file);
+  if (refs_it == references->end()) {
+    refs_it = references->Put(
+        ref.dex_file,
+        BitVector(number_of_indexes, /* expandable */ false, Allocator::GetCallocAllocator()));
   }
-};
+  refs_it->second.SetBit(ref.index);
+}
 
 class OatWriter::InitOatClassesMethodVisitor : public DexMethodVisitor {
  public:
@@ -1310,8 +1340,11 @@ class OatWriter::LayoutReserveOffsetCodeMethodVisitor : public OrderedMethodVisi
                               const MethodReference& method_ref,
                               uint32_t thumb_offset) {
     offset_ = relative_patcher_->ReserveSpace(offset_, compiled_method, method_ref);
-    offset_ += CodeAlignmentSize(offset_, *compiled_method);
-    DCHECK_ALIGNED_PARAM(offset_ + sizeof(OatQuickMethodHeader),
+    // `offset_` is relative to the oat data, but we need to align the code relative to the
+    // beginning of the oat file to make it aligned in the memory, so we need to use the file
+    // offset here.
+    offset_ += CodeAlignmentSize(writer_->GetFileOffset(offset_), *compiled_method);
+    DCHECK_ALIGNED_PARAM(writer_->GetFileOffset(offset_) + sizeof(OatQuickMethodHeader),
                          GetInstructionSetCodeAlignment(compiled_method->GetInstructionSet()));
     return offset_ + sizeof(OatQuickMethodHeader) + thumb_offset;
   }
@@ -1613,7 +1646,11 @@ class OatWriter::WriteCodeMethodVisitor : public OrderedMethodVisitor {
         ReportWriteFailure("relative call thunk", method_ref);
         return false;
       }
-      uint32_t alignment_size = CodeAlignmentSize(offset_, *compiled_method);
+      // `offset_` is relative to the oat data, but we need to align the code relative to the
+      // beginning of the oat file to make it aligned in the memory, so we need to use the file
+      // offset here.
+      uint32_t alignment_size =
+          CodeAlignmentSize(writer_->GetFileOffset(offset_), *compiled_method);
       if (alignment_size != 0) {
         if (!writer_->WriteCodeAlignment(out, alignment_size)) {
           ReportWriteFailure("code alignment padding", method_ref);
@@ -1622,7 +1659,7 @@ class OatWriter::WriteCodeMethodVisitor : public OrderedMethodVisitor {
         offset_ += alignment_size;
         DCHECK_OFFSET_();
       }
-      DCHECK_ALIGNED_PARAM(offset_ + sizeof(OatQuickMethodHeader),
+      DCHECK_ALIGNED_PARAM(writer_->GetFileOffset(offset_) + sizeof(OatQuickMethodHeader),
                            GetInstructionSetCodeAlignment(compiled_method->GetInstructionSet()));
       DCHECK_EQ(
           method_offsets.code_offset_,
@@ -1962,17 +1999,27 @@ bool OatWriter::VisitDexMethods(DexMethodVisitor* visitor) {
   return true;
 }
 
-size_t OatWriter::InitOatHeader(uint32_t num_dex_files,
-                                SafeMap<std::string, std::string>* key_value_store) {
+size_t OatWriter::InitOatHeader(uint32_t num_dex_files, OatKeyValueStore* key_value_store) {
   TimingLogger::ScopedTiming split("InitOatHeader", timings_);
+
+  // `key_value_store` only exists in the first oat file in a multi-image boot image.
+  if (key_value_store != nullptr) {
+    // Add non-deterministic fields if they don't exist. These fields should always exist with fixed
+    // lengths.
+    for (auto [field, length] : OatHeader::kNonDeterministicFieldsAndLengths) {
+      key_value_store->map_.FindOrAdd(std::string(field));
+    }
+  }
+
   // Check that oat version when runtime was compiled matches the oat version
   // when dex2oat was compiled. We have seen cases where they got out of sync.
   constexpr std::array<uint8_t, 4> dex2oat_oat_version = OatHeader::kOatVersion;
   OatHeader::CheckOatVersion(dex2oat_oat_version);
-  oat_header_.reset(OatHeader::Create(GetCompilerOptions().GetInstructionSet(),
-                                      GetCompilerOptions().GetInstructionSetFeatures(),
-                                      num_dex_files,
-                                      key_value_store));
+  oat_header_ = OatHeader::Create(GetCompilerOptions().GetInstructionSet(),
+                                  GetCompilerOptions().GetInstructionSetFeatures(),
+                                  num_dex_files,
+                                  key_value_store != nullptr ? &key_value_store->map_ : nullptr,
+                                  oat_data_offset_);
   size_oat_header_ += sizeof(OatHeader);
   size_oat_header_key_value_store_ += oat_header_->GetHeaderSize() - sizeof(OatHeader);
   return oat_header_->GetHeaderSize();
@@ -2278,7 +2325,7 @@ size_t OatWriter::InitOatCode(size_t offset) {
   // calculate the offsets within OatHeader to executable code
   size_t old_offset = offset;
   // required to be on a new page boundary
-  offset = RoundUp(offset, kElfSegmentAlignment);
+  offset = GetOffsetFromOatDataAlignedToFile(offset, kElfSegmentAlignment);
   oat_header_->SetExecutableOffset(offset);
   size_executable_offset_alignment_ = offset - old_offset;
   InstructionSet instruction_set = compiler_options_.GetInstructionSet();
@@ -2288,7 +2335,8 @@ size_t OatWriter::InitOatCode(size_t offset) {
 
     #define DO_TRAMPOLINE(field, fn_name)                                                 \
       /* Pad with at least four 0xFFs so we can do DCHECKs in OatQuickMethodHeader */     \
-      offset = CompiledCode::AlignCode(offset + 4, instruction_set);                      \
+      offset = GetOffsetFromOatDataAlignedToFile(offset + 4,                              \
+          GetInstructionSetCodeAlignment(instruction_set));                               \
       adjusted_offset = offset + GetInstructionSetEntryPointAdjustment(instruction_set);  \
       oat_header_->Set ## fn_name ## Offset(adjusted_offset);                             \
       (field) = compiler_driver_->Create ## fn_name();                                    \
@@ -2394,7 +2442,7 @@ size_t OatWriter::InitDataImgRelRoLayout(size_t offset) {
     return offset;
   }
 
-  data_img_rel_ro_start_ = RoundUp(offset, kElfSegmentAlignment);
+  data_img_rel_ro_start_ = GetOffsetFromOatDataAlignedToFile(offset, kElfSegmentAlignment);
 
   for (auto& entry : boot_image_rel_ro_entries_) {
     size_t& entry_offset = entry.second;
@@ -2421,12 +2469,6 @@ size_t OatWriter::InitDataImgRelRoLayout(size_t offset) {
 }
 
 void OatWriter::InitBssLayout(InstructionSet instruction_set) {
-  {
-    InitBssLayoutMethodVisitor visitor(this);
-    bool success = VisitDexMethods(&visitor);
-    DCHECK(success);
-  }
-
   DCHECK_EQ(bss_size_, 0u);
   if (bss_method_entries_.empty() &&
       bss_type_entries_.empty() &&
@@ -2483,6 +2525,7 @@ void OatWriter::InitBssLayout(InstructionSet instruction_set) {
 }
 
 bool OatWriter::WriteRodata(OutputStream* out) {
+  TimingLogger::ScopedTiming split("WriteRodata", timings_);
   CHECK(write_state_ == WriteState::kWriteRoData);
 
   size_t file_offset = oat_data_offset_;
@@ -2536,7 +2579,7 @@ bool OatWriter::WriteRodata(OutputStream* out) {
   // Write padding.
   off_t new_offset = out->Seek(size_executable_offset_alignment_, kSeekCurrent);
   relative_offset += size_executable_offset_alignment_;
-  DCHECK_EQ(relative_offset, oat_header_->GetExecutableOffset());
+  DCHECK_EQ(relative_offset, GetOatHeader().GetExecutableOffset());
   size_t expected_file_offset = file_offset + relative_offset;
   if (static_cast<uint32_t>(new_offset) != expected_file_offset) {
     PLOG(ERROR) << "Failed to seek to oat code section. Actual: " << new_offset
@@ -2574,6 +2617,7 @@ void OatWriter::WriteVerifierDeps(verifier::VerifierDeps* verifier_deps,
 }
 
 bool OatWriter::WriteCode(OutputStream* out) {
+  TimingLogger::ScopedTiming split("WriteCode", timings_);
   CHECK(write_state_ == WriteState::kWriteText);
 
   // Wrap out to update checksum with each write.
@@ -2610,6 +2654,7 @@ bool OatWriter::WriteCode(OutputStream* out) {
 }
 
 bool OatWriter::WriteDataImgRelRo(OutputStream* out) {
+  TimingLogger::ScopedTiming split("WriteDataImgRelRo", timings_);
   CHECK(write_state_ == WriteState::kWriteDataImgRelRo);
 
   // Wrap out to update checksum with each write.
@@ -2622,7 +2667,7 @@ bool OatWriter::WriteDataImgRelRo(OutputStream* out) {
   // Record the padding before the .data.img.rel.ro section.
   // Do not write anything, this zero-filled part was skipped (Seek()) when starting the section.
   size_t code_end = GetOatHeader().GetExecutableOffset() + code_size_;
-  DCHECK_EQ(RoundUp(code_end, kElfSegmentAlignment), relative_offset);
+  DCHECK_EQ(GetOffsetFromOatDataAlignedToFile(code_end, kElfSegmentAlignment), relative_offset);
   size_t padding_size = relative_offset - code_end;
   DCHECK_EQ(size_data_img_rel_ro_alignment_, 0u);
   size_data_img_rel_ro_alignment_ = padding_size;
@@ -2736,14 +2781,13 @@ bool OatWriter::CheckOatSize(OutputStream* out, size_t file_offset, size_t relat
 }
 
 bool OatWriter::WriteHeader(OutputStream* out) {
+  TimingLogger::ScopedTiming split("WriteHeader", timings_);
+
   CHECK(write_state_ == WriteState::kWriteHeader);
 
   // Update checksum with header data.
   DCHECK_EQ(oat_header_->GetChecksum(), 0u);  // For checksum calculation.
-  const uint8_t* header_begin = reinterpret_cast<const uint8_t*>(oat_header_.get());
-  const uint8_t* header_end = oat_header_->GetKeyValueStore() + oat_header_->GetKeyValueStoreSize();
-  uint32_t old_checksum = oat_checksum_;
-  oat_checksum_ = adler32(old_checksum, header_begin, header_end - header_begin);
+  oat_header_->ComputeChecksum(&oat_checksum_);
   oat_header_->SetChecksum(oat_checksum_);
 
   const size_t file_offset = oat_data_offset_;
@@ -2766,7 +2810,7 @@ bool OatWriter::WriteHeader(OutputStream* out) {
   }
   // Write the header.
   size_t header_size = oat_header_->GetHeaderSize();
-  if (!out->WriteFully(oat_header_.get(), header_size)) {
+  if (!out->WriteFully(oat_header_, header_size)) {
     PLOG(ERROR) << "Failed to write oat header to " << out->GetLocation();
     return false;
   }
@@ -3029,7 +3073,6 @@ size_t OatWriter::WriteIndexBssMappingsHelper(OutputStream* out,
 size_t OatWriter::WriteIndexBssMappings(OutputStream* out,
                                         size_t file_offset,
                                         size_t relative_offset) {
-  TimingLogger::ScopedTiming split("WriteMethodBssMappings", timings_);
   if (bss_method_entry_references_.empty() &&
       bss_type_entry_references_.empty() &&
       bss_public_type_entry_references_.empty() &&
@@ -3099,8 +3142,6 @@ size_t OatWriter::WriteIndexBssMappings(OutputStream* out,
 }
 
 size_t OatWriter::WriteOatDexFiles(OutputStream* out, size_t file_offset, size_t relative_offset) {
-  TimingLogger::ScopedTiming split("WriteOatDexFiles", timings_);
-
   for (size_t i = 0, size = oat_dex_files_.size(); i != size; ++i) {
     OatDexFile* oat_dex_file = &oat_dex_files_[i];
     DCHECK_EQ(relative_offset, oat_dex_file->offset_);
@@ -3117,8 +3158,6 @@ size_t OatWriter::WriteOatDexFiles(OutputStream* out, size_t file_offset, size_t
 }
 
 size_t OatWriter::WriteBcpBssInfo(OutputStream* out, size_t file_offset, size_t relative_offset) {
-  TimingLogger::ScopedTiming split("WriteBcpBssInfo", timings_);
-
   const uint32_t number_of_bcp_dexfiles = bcp_bss_info_.size();
   // We skip adding the number of DexFiles if we have no .bss mappings.
   if (number_of_bcp_dexfiles == 0) {
@@ -3150,7 +3189,8 @@ size_t OatWriter::WriteCode(OutputStream* out, size_t file_offset, size_t relati
     #define DO_TRAMPOLINE(field) \
       do { \
         /* Pad with at least four 0xFFs so we can do DCHECKs in OatQuickMethodHeader */ \
-        uint32_t aligned_offset = CompiledCode::AlignCode(relative_offset + 4, instruction_set); \
+        uint32_t aligned_offset = GetOffsetFromOatDataAlignedToFile( \
+            relative_offset + 4, GetInstructionSetCodeAlignment(instruction_set)); \
         uint32_t alignment_padding = aligned_offset - relative_offset; \
         for (size_t i = 0; i < alignment_padding; i++) { \
           uint8_t padding = 0xFF; \
@@ -3540,8 +3580,9 @@ bool OatWriter::WriteDexLayoutSections(OutputStream* oat_rodata,
     DCHECK_EQ(oat_dex_file->dex_sections_layout_offset_, 0u);
 
     // Write dex layout section alignment bytes.
+    size_t rodata_file_offset = GetFileOffset(rodata_offset);
     const size_t padding_size =
-        RoundUp(rodata_offset, alignof(DexLayoutSections)) - rodata_offset;
+        RoundUp(rodata_file_offset, alignof(DexLayoutSections)) - rodata_file_offset;
     if (padding_size != 0u) {
       std::vector<uint8_t> buffer(padding_size, 0u);
       if (!oat_rodata->WriteFully(buffer.data(), padding_size)) {
@@ -3788,12 +3829,15 @@ void OatWriter::SetMultiOatRelativePatcherAdjustment() {
   DCHECK(dex_files_ != nullptr);
   DCHECK(relative_patcher_ != nullptr);
   DCHECK_NE(oat_data_offset_, 0u);
+  size_t elf_file_offset = 0;
   if (image_writer_ != nullptr && !dex_files_->empty()) {
     // The oat data begin may not be initialized yet but the oat file offset is ready.
     size_t oat_index = image_writer_->GetOatIndexForDexFile(dex_files_->front());
-    size_t elf_file_offset = image_writer_->GetOatFileOffset(oat_index);
-    relative_patcher_->StartOatFile(elf_file_offset + oat_data_offset_);
+    elf_file_offset = image_writer_->GetOatFileOffset(oat_index);
   }
+  // Relative patcher expects offsets from the page-aligned boundary, as the oat data is
+  // unaligned in the ELF file we always need to set its correct start.
+  relative_patcher_->StartOatFile(elf_file_offset + oat_data_offset_);
 }
 
 OatWriter::OatDexFile::OatDexFile(std::unique_ptr<const DexFile> dex_file)
diff --git a/dex2oat/linker/oat_writer.h b/dex2oat/linker/oat_writer.h
index c985812f94..4f50b1a39c 100644
--- a/dex2oat/linker/oat_writer.h
+++ b/dex2oat/linker/oat_writer.h
@@ -70,6 +70,29 @@ enum class CopyOption {
   kOnlyIfCompressed
 };
 
+class OatKeyValueStore {
+ public:
+  // Puts a key value pair whose key is in `OatHeader::kNonDeterministicFieldsAndLengths`.
+  bool PutNonDeterministic(const std::string& k,
+                           const std::string& v,
+                           bool allow_truncation = false);
+
+  // Puts a key value pair whose key is in `OatHeader::kDeterministicFields`.
+  void Put(const std::string& k, const std::string& v);
+
+  // Puts a key value pair whose key is in `OatHeader::kDeterministicFields`.
+  void Put(const std::string& k, bool v);
+
+  // Makes sure calls with `const char*` falls into the overload for `std::string`, not the one for
+  // `bool`.
+  void Put(const std::string& k, const char* v) { Put(k, std::string(v)); }
+
+ private:
+  SafeMap<std::string, std::string> map_;
+
+  friend class OatWriter;
+};
+
 // OatHeader         variable length with count of D OatDexFiles
 //
 // TypeLookupTable[0] one descriptor to class def index hash table for each OatDexFile.
@@ -167,7 +190,7 @@ class OatWriter {
   // Start writing .rodata, including supporting data structures for dex files.
   bool StartRoData(const std::vector<const DexFile*>& dex_files,
                    OutputStream* oat_rodata,
-                   SafeMap<std::string, std::string>* key_value_store);
+                   OatKeyValueStore* key_value_store);
   // Initialize the writer with the given parameters.
   void Initialize(const CompilerDriver* compiler_driver,
                   const VerificationResults* verification_results,
@@ -261,7 +284,6 @@ class OatWriter {
   // to actually write it.
   class DexMethodVisitor;
   class OatDexMethodVisitor;
-  class InitBssLayoutMethodVisitor;
   class InitOatClassesMethodVisitor;
   class LayoutCodeMethodVisitor;
   class LayoutReserveOffsetCodeMethodVisitor;
@@ -292,7 +314,7 @@ class OatWriter {
   void WriteVerifierDeps(verifier::VerifierDeps* verifier_deps,
                          /*out*/std::vector<uint8_t>* buffer);
 
-  size_t InitOatHeader(uint32_t num_dex_files, SafeMap<std::string, std::string>* key_value_store);
+  size_t InitOatHeader(uint32_t num_dex_files, OatKeyValueStore* key_value_store);
   size_t InitClassOffsets(size_t offset);
   size_t InitOatClasses(size_t offset);
   size_t InitOatMaps(size_t offset);
@@ -302,7 +324,11 @@ class OatWriter {
   size_t InitOatCode(size_t offset);
   size_t InitOatCodeDexFiles(size_t offset);
   size_t InitDataImgRelRoLayout(size_t offset);
+  void InitBssAndRelRoData();
   void InitBssLayout(InstructionSet instruction_set);
+  void AddBssReference(const DexFileReference& ref,
+                       size_t number_of_indexes,
+                       /*inout*/ SafeMap<const DexFile*, BitVector>* references);
 
   size_t WriteClassOffsets(OutputStream* out, size_t file_offset, size_t relative_offset);
   size_t WriteClasses(OutputStream* out, size_t file_offset, size_t relative_offset);
@@ -355,6 +381,18 @@ class OatWriter {
     return dex_files_ != nullptr && extract_dex_files_into_vdex_;
   }
 
+  // Return the file offset that corresponds to `offset_from_oat_data`.
+  size_t GetFileOffset(size_t offset_from_oat_data) const {
+    DCHECK_NE(oat_data_offset_, 0u);
+    return offset_from_oat_data + oat_data_offset_;
+  }
+
+  // Return the next offset (relative to the oat data) that is on or after `offset_from_oat_data`,
+  // that is aligned by `alignment` to the beginning of the file.
+  size_t GetOffsetFromOatDataAlignedToFile(size_t offset_from_oat_data, size_t alignment) const {
+    return RoundUp(GetFileOffset(offset_from_oat_data), alignment) - oat_data_offset_;
+  }
+
   enum class WriteState {
     kAddingDexFileSources,
     kStartRoData,
@@ -505,7 +543,7 @@ class OatWriter {
   std::vector<std::unique_ptr<art::OatDexFile>> type_lookup_table_oat_dex_files_;
 
   // data to write
-  std::unique_ptr<OatHeader> oat_header_;
+  OatHeader* oat_header_;
   dchecked_vector<OatDexFile> oat_dex_files_;
   dchecked_vector<OatClassHeader> oat_class_headers_;
   dchecked_vector<OatClass> oat_classes_;
diff --git a/dex2oat/linker/oat_writer_test.cc b/dex2oat/linker/oat_writer_test.cc
index 0a585edb53..538c1abc9a 100644
--- a/dex2oat/linker/oat_writer_test.cc
+++ b/dex2oat/linker/oat_writer_test.cc
@@ -14,8 +14,11 @@
  * limitations under the License.
  */
 
-#include "android-base/stringprintf.h"
+#include "oat_writer.h"
 
+#include <cstdint>
+
+#include "android-base/stringprintf.h"
 #include "arch/instruction_set_features.h"
 #include "art_method-inl.h"
 #include "base/file_utils.h"
@@ -35,6 +38,7 @@
 #include "driver/compiler_driver.h"
 #include "driver/compiler_options.h"
 #include "entrypoints/quick/quick_entrypoints.h"
+#include "gtest/gtest.h"
 #include "linker/elf_writer.h"
 #include "linker/elf_writer_quick.h"
 #include "linker/multi_oat_relative_patcher.h"
@@ -76,6 +80,8 @@ class OatTest : public CommonCompilerDriverTest {
       const void* quick_oat_code = oat_method.GetQuickCode();
       EXPECT_TRUE(quick_oat_code != nullptr) << method->PrettyMethod();
       uintptr_t oat_code_aligned = RoundDown(reinterpret_cast<uintptr_t>(quick_oat_code), 2);
+      EXPECT_EQ(RoundDown(oat_code_aligned,
+          GetInstructionSetCodeAlignment(compiled_method->GetInstructionSet())), oat_code_aligned);
       quick_oat_code = reinterpret_cast<const void*>(oat_code_aligned);
       ArrayRef<const uint8_t> quick_code = compiled_method->GetQuickCode();
       EXPECT_FALSE(quick_code.empty());
@@ -102,7 +108,7 @@ class OatTest : public CommonCompilerDriverTest {
   bool WriteElf(File* vdex_file,
                 File* oat_file,
                 const std::vector<const DexFile*>& dex_files,
-                SafeMap<std::string, std::string>& key_value_store,
+                OatKeyValueStore& key_value_store,
                 bool verify) {
     TimingLogger timings("WriteElf", false, false);
     ClearBootImageOption();
@@ -122,7 +128,7 @@ class OatTest : public CommonCompilerDriverTest {
   bool WriteElf(File* vdex_file,
                 File* oat_file,
                 const std::vector<const char*>& dex_filenames,
-                SafeMap<std::string, std::string>& key_value_store,
+                OatKeyValueStore& key_value_store,
                 bool verify,
                 CopyOption copy,
                 ProfileCompilationInfo* profile_compilation_info) {
@@ -141,7 +147,7 @@ class OatTest : public CommonCompilerDriverTest {
                 File* oat_file,
                 File&& dex_file_fd,
                 const char* location,
-                SafeMap<std::string, std::string>& key_value_store,
+                OatKeyValueStore& key_value_store,
                 bool verify,
                 CopyOption copy,
                 ProfileCompilationInfo* profile_compilation_info = nullptr) {
@@ -157,7 +163,7 @@ class OatTest : public CommonCompilerDriverTest {
   bool DoWriteElf(File* vdex_file,
                   File* oat_file,
                   OatWriter& oat_writer,
-                  SafeMap<std::string, std::string>& key_value_store,
+                  OatKeyValueStore& key_value_store,
                   bool verify,
                   CopyOption copy) {
     std::unique_ptr<ElfWriter> elf_writer = CreateElfWriterQuick(
@@ -424,7 +430,7 @@ TEST_F(OatTest, WriteRead) {
   }
 
   ScratchFile tmp_base, tmp_oat(tmp_base, ".oat"), tmp_vdex(tmp_base, ".vdex");
-  SafeMap<std::string, std::string> key_value_store;
+  OatKeyValueStore key_value_store;
   key_value_store.Put(OatHeader::kBootClassPathChecksumsKey, "testkey");
   bool success = WriteElf(tmp_vdex.GetFile(),
                           tmp_oat.GetFile(),
@@ -445,6 +451,12 @@ TEST_F(OatTest, WriteRead) {
   ASSERT_TRUE(oat_file.get() != nullptr) << error_msg;
   const OatHeader& oat_header = oat_file->GetOatHeader();
   ASSERT_TRUE(oat_header.IsValid());
+  // .text section in the ELF program header is specified to be aligned to kElfSegmentAlignment.
+  // However, ART's ELF loader does not adhere to this and only guarantees to align it to the
+  // runtime page size. Therefore, we assert that the executable segment is page-aligned in
+  // virtual memory.
+  const uint8_t* text_section = oat_file->Begin() + oat_header.GetExecutableOffset();
+  ASSERT_TRUE(IsAlignedParam(text_section, GetPageSizeSlow()));
   ASSERT_EQ(class_linker->GetBootClassPath().size(), oat_header.GetDexFileCount());  // core
   ASSERT_TRUE(oat_header.GetStoreValueByKey(OatHeader::kBootClassPathChecksumsKey) != nullptr);
   ASSERT_STREQ("testkey", oat_header.GetStoreValueByKey(OatHeader::kBootClassPathChecksumsKey));
@@ -486,10 +498,71 @@ TEST_F(OatTest, WriteRead) {
   }
 }
 
+TEST_F(OatTest, ChecksumDeterminism) {
+  ClassLinker* class_linker = Runtime::Current()->GetClassLinker();
+  SetupCompiler(/*compiler_options=*/{});
+
+  if (kCompile) {
+    TimingLogger timings("OatTest::ChecksumDeterminism", /*precise=*/false, /*verbose=*/false);
+    CompileAll(/*class_loader=*/nullptr, class_linker->GetBootClassPath(), &timings);
+  }
+
+  auto write_elf_and_get_checksum = [&](OatKeyValueStore& key_value_store,
+                                        /*out*/ uint32_t* checksum) {
+    ScratchFile tmp_base, tmp_oat(tmp_base, ".oat"), tmp_vdex(tmp_base, ".vdex");
+
+    bool success = WriteElf(tmp_vdex.GetFile(),
+                            tmp_oat.GetFile(),
+                            class_linker->GetBootClassPath(),
+                            key_value_store,
+                            /*verify=*/false);
+    ASSERT_TRUE(success);
+
+    std::string error_msg;
+    std::unique_ptr<OatFile> oat_file(OatFile::Open(/*zip_fd=*/-1,
+                                                    tmp_oat.GetFilename(),
+                                                    tmp_oat.GetFilename(),
+                                                    /*executable=*/false,
+                                                    /*low_4gb=*/true,
+                                                    &error_msg));
+    ASSERT_TRUE(oat_file.get() != nullptr) << error_msg;
+    const OatHeader& oat_header = oat_file->GetOatHeader();
+    ASSERT_TRUE(oat_header.IsValid());
+    *checksum = oat_header.GetChecksum();
+  };
+
+  uint32_t checksum_1, checksum_2, checksum_3;
+
+  {
+    OatKeyValueStore key_value_store;
+    key_value_store.Put(OatHeader::kBootClassPathChecksumsKey, "testkey");
+    ASSERT_NO_FATAL_FAILURE(write_elf_and_get_checksum(key_value_store, &checksum_1));
+  }
+
+  {
+    // Put non-deterministic fields. This should not affect the checksum.
+    OatKeyValueStore key_value_store;
+    key_value_store.Put(OatHeader::kBootClassPathChecksumsKey, "testkey");
+    key_value_store.PutNonDeterministic(OatHeader::kDex2OatCmdLineKey, "cmdline");
+    key_value_store.PutNonDeterministic(OatHeader::kApexVersionsKey, "apex-versions");
+    ASSERT_NO_FATAL_FAILURE(write_elf_and_get_checksum(key_value_store, &checksum_2));
+    EXPECT_EQ(checksum_1, checksum_2);
+  }
+
+  {
+    // Put deterministic fields. This should affect the checksum.
+    OatKeyValueStore key_value_store;
+    key_value_store.Put(OatHeader::kBootClassPathChecksumsKey, "testkey");
+    key_value_store.Put(OatHeader::kClassPathKey, "classpath");
+    ASSERT_NO_FATAL_FAILURE(write_elf_and_get_checksum(key_value_store, &checksum_3));
+    EXPECT_NE(checksum_1, checksum_3);
+  }
+}
+
 TEST_F(OatTest, OatHeaderSizeCheck) {
   // If this test is failing and you have to update these constants,
   // it is time to update OatHeader::kOatVersion
-  EXPECT_EQ(68U, sizeof(OatHeader));
+  EXPECT_EQ(72U, sizeof(OatHeader));
   EXPECT_EQ(4U, sizeof(OatMethodOffsets));
   EXPECT_EQ(4U, sizeof(OatQuickMethodHeader));
   EXPECT_EQ(173 * static_cast<size_t>(GetInstructionSetPointerSize(kRuntimeISA)),
@@ -540,7 +613,7 @@ TEST_F(OatTest, EmptyTextSection) {
   CompileAll(class_loader, dex_files, &timings);
 
   ScratchFile tmp_base, tmp_oat(tmp_base, ".oat"), tmp_vdex(tmp_base, ".vdex");
-  SafeMap<std::string, std::string> key_value_store;
+  OatKeyValueStore key_value_store;
   bool success = WriteElf(tmp_vdex.GetFile(),
                           tmp_oat.GetFile(),
                           dex_files,
@@ -609,7 +682,7 @@ void OatTest::TestDexFileInput(bool verify, bool low_4gb, bool use_profile) {
   input_dexfiles.push_back(std::move(dex_file2_data));
   scratch_files.push_back(&dex_file2);
 
-  SafeMap<std::string, std::string> key_value_store;
+  OatKeyValueStore key_value_store;
   {
     // Test using the AddDexFileSource() interface with the dex files.
     ScratchFile tmp_base, tmp_oat(tmp_base, ".oat"), tmp_vdex(tmp_base, ".vdex");
@@ -738,7 +811,7 @@ void OatTest::TestZipFileInput(bool verify, CopyOption copy) {
   success = zip_builder.Finish();
   ASSERT_TRUE(success) << strerror(errno);
 
-  SafeMap<std::string, std::string> key_value_store;
+  OatKeyValueStore key_value_store;
   {
     // Test using the AddDexFileSource() interface with the zip file.
     std::vector<const char*> input_filenames = { zip_file.GetFilename().c_str() };
@@ -857,7 +930,7 @@ void OatTest::TestZipFileInputWithEmptyDex() {
   success = zip_builder.Finish();
   ASSERT_TRUE(success) << strerror(errno);
 
-  SafeMap<std::string, std::string> key_value_store;
+  OatKeyValueStore key_value_store;
   std::vector<const char*> input_filenames = { zip_file.GetFilename().c_str() };
   ScratchFile oat_file, vdex_file(oat_file, ".vdex");
   std::unique_ptr<ProfileCompilationInfo> profile_compilation_info(new ProfileCompilationInfo());
@@ -875,5 +948,88 @@ TEST_F(OatTest, ZipFileInputWithEmptyDex) {
   TestZipFileInputWithEmptyDex();
 }
 
+TEST_F(OatTest, AlignmentCheck) {
+  TimingLogger timings("OatTest::AlignmentCheck", false, false);
+
+  // OatWriter sets trampoline offsets to non-zero values only for primary boot oat
+  // file (e.g. boot.oat), so we use it to check trampolines alignment.
+  std::string location = GetCoreOatLocation();
+  std::string filename = GetSystemImageFilename(location.c_str(), kRuntimeISA);
+
+  // Find the absolute path for core-oj.jar and use it to open boot.oat. Otherwise,
+  // OatFile::Open will attempt to open the dex file using its relative location,
+  // which may result in a "file not found" error.
+  ASSERT_TRUE(java_lang_dex_file_ != nullptr);
+  const DexFile& dex_file = *java_lang_dex_file_;
+  std::string dex_location = dex_file.GetLocation();
+  std::vector<std::string> filenames = GetLibCoreDexFileNames();
+  auto it = std::find_if(
+      filenames.cbegin(),
+      filenames.cend(),
+      [&dex_location](const std::string& filename) {
+        return filename.ends_with(dex_location);
+      });
+  ASSERT_NE(it, filenames.cend())
+    << "cannot find: " << dex_location << " in libcore dex filenames";
+
+  std::string dex_filename = *it;
+  std::string error_msg;
+  std::unique_ptr<OatFile> oat_file(OatFile::Open(/*zip_fd=*/ -1,
+                                                  filename,
+                                                  filename,
+                                                  /*executable=*/ false,
+                                                  /*low_4gb=*/ false,
+                                                  dex_filename,
+                                                  &error_msg));
+  ASSERT_NE(oat_file, nullptr) << error_msg;
+  ASSERT_TRUE(IsAligned<alignof(OatHeader)>(oat_file->Begin()))
+      << "oat header: " << reinterpret_cast<const void*>(oat_file->Begin())
+      << ", alignment: " << alignof(OatHeader);
+
+  const OatHeader& oat_header = oat_file->GetOatHeader();
+  ASSERT_TRUE(oat_header.IsValid());
+
+  // Check trampolines alignment.
+  size_t alignment = GetInstructionSetCodeAlignment(instruction_set_);
+  size_t adjustment = GetInstructionSetEntryPointAdjustment(instruction_set_);
+  for (size_t i = 0; i <= static_cast<size_t>(StubType::kLast); i++) {
+    StubType stub_type = static_cast<StubType>(i);
+    const uint8_t* address = oat_header.GetOatAddress(stub_type);
+    ASSERT_NE(address, nullptr);
+    const uint8_t* adjusted_address = address - adjustment;
+    EXPECT_TRUE(IsAlignedParam(adjusted_address, alignment))
+        << "stub: " << stub_type
+        << ", address: " << reinterpret_cast<const void*>(adjusted_address)
+        << ", code alignment: " << alignment;
+  }
+
+  // Check code alignment.
+  const OatDexFile* oat_dex_file = oat_file->GetOatDexFile(dex_file.GetLocation().c_str());
+  for (ClassAccessor accessor : dex_file.GetClasses()) {
+    const OatFile::OatClass oat_class = oat_dex_file->GetOatClass(accessor.GetClassDefIndex());
+    if (oat_class.GetType() == OatClassType::kNoneCompiled) {
+      continue;
+    }
+
+    uint32_t method_index = 0;
+    for (const ClassAccessor::Method& method : accessor.GetMethods()) {
+      const OatFile::OatMethod& oat_method = oat_class.GetOatMethod(method_index++);
+      uintptr_t code = reinterpret_cast<uintptr_t>(oat_method.GetQuickCode());
+      if (code == 0) {
+        continue;
+      }
+      const void* adjusted_address = reinterpret_cast<const void*>(code - adjustment);
+      EXPECT_TRUE(IsAlignedParam(adjusted_address, alignment))
+          << "method: " << method.GetReference().PrettyMethod()
+          << ", code: " << adjusted_address
+          << ", code alignment: " << alignment;
+    }
+    EXPECT_EQ(method_index, accessor.NumMethods());
+  }
+
+  // Check DexLayoutSections alignment.
+  EXPECT_TRUE(IsAligned<alignof(DexLayoutSections)>(oat_dex_file->GetDexLayoutSections()));
+}
+
 }  // namespace linker
 }  // namespace art
diff --git a/dex2oat/linker/relative_patcher_test.h b/dex2oat/linker/relative_patcher_test.h
index 3e7ba796e0..16f263ec21 100644
--- a/dex2oat/linker/relative_patcher_test.h
+++ b/dex2oat/linker/relative_patcher_test.h
@@ -36,7 +36,7 @@ namespace art {
 namespace linker {
 
 // Base class providing infrastructure for architecture-specific tests.
-class RelativePatcherTest : public testing::Test {
+class RelativePatcherTest : public ::testing::Test {
  protected:
   RelativePatcherTest(InstructionSet instruction_set, const std::string& variant)
       : storage_(/*swap_fd=*/ -1),
diff --git a/compiler/utils/atomic_dex_ref_map-inl.h b/dex2oat/utils/atomic_dex_ref_map-inl.h
similarity index 97%
rename from compiler/utils/atomic_dex_ref_map-inl.h
rename to dex2oat/utils/atomic_dex_ref_map-inl.h
index 653d21b3ea..b36671c347 100644
--- a/compiler/utils/atomic_dex_ref_map-inl.h
+++ b/dex2oat/utils/atomic_dex_ref_map-inl.h
@@ -14,8 +14,8 @@
  * limitations under the License.
  */
 
-#ifndef ART_COMPILER_UTILS_ATOMIC_DEX_REF_MAP_INL_H_
-#define ART_COMPILER_UTILS_ATOMIC_DEX_REF_MAP_INL_H_
+#ifndef ART_DEX2OAT_UTILS_ATOMIC_DEX_REF_MAP_INL_H_
+#define ART_DEX2OAT_UTILS_ATOMIC_DEX_REF_MAP_INL_H_
 
 #include "atomic_dex_ref_map.h"
 
@@ -147,4 +147,4 @@ inline std::vector<const DexFile*> AtomicDexRefMap<DexFileReferenceType, Value>:
 
 }  // namespace art
 
-#endif  // ART_COMPILER_UTILS_ATOMIC_DEX_REF_MAP_INL_H_
+#endif  // ART_DEX2OAT_UTILS_ATOMIC_DEX_REF_MAP_INL_H_
diff --git a/compiler/utils/atomic_dex_ref_map.h b/dex2oat/utils/atomic_dex_ref_map.h
similarity index 94%
rename from compiler/utils/atomic_dex_ref_map.h
rename to dex2oat/utils/atomic_dex_ref_map.h
index b10fef50c5..59ecd3c504 100644
--- a/compiler/utils/atomic_dex_ref_map.h
+++ b/dex2oat/utils/atomic_dex_ref_map.h
@@ -14,8 +14,8 @@
  * limitations under the License.
  */
 
-#ifndef ART_COMPILER_UTILS_ATOMIC_DEX_REF_MAP_H_
-#define ART_COMPILER_UTILS_ATOMIC_DEX_REF_MAP_H_
+#ifndef ART_DEX2OAT_UTILS_ATOMIC_DEX_REF_MAP_H_
+#define ART_DEX2OAT_UTILS_ATOMIC_DEX_REF_MAP_H_
 
 #include "base/atomic.h"
 #include "base/dchecked_vector.h"
@@ -31,6 +31,9 @@ class DexFile;
 template <typename DexFileReferenceType, typename Value>
 class AtomicDexRefMap {
  public:
+  // Verified methods. The method array is fixed to avoid needing a lock to extend it.
+  using ElementArray = dchecked_vector<Atomic<Value>>;
+
   AtomicDexRefMap() {}
   ~AtomicDexRefMap() {}
 
@@ -68,14 +71,12 @@ class AtomicDexRefMap {
 
   void ClearEntries();
 
- private:
-  // Verified methods. The method array is fixed to avoid needing a lock to extend it.
-  using ElementArray = dchecked_vector<Atomic<Value>>;
-  using DexFileArrays = SafeMap<const DexFile*, ElementArray>;
-
   const ElementArray* GetArray(const DexFile* dex_file) const;
   ElementArray* GetArray(const DexFile* dex_file);
 
+ private:
+  using DexFileArrays = SafeMap<const DexFile*, ElementArray>;
+
   static size_t NumberOfDexIndices(const DexFile* dex_file);
 
   DexFileArrays arrays_;
@@ -83,4 +84,4 @@ class AtomicDexRefMap {
 
 }  // namespace art
 
-#endif  // ART_COMPILER_UTILS_ATOMIC_DEX_REF_MAP_H_
+#endif  // ART_DEX2OAT_UTILS_ATOMIC_DEX_REF_MAP_H_
diff --git a/compiler/utils/atomic_dex_ref_map_test.cc b/dex2oat/utils/atomic_dex_ref_map_test.cc
similarity index 100%
rename from compiler/utils/atomic_dex_ref_map_test.cc
rename to dex2oat/utils/atomic_dex_ref_map_test.cc
diff --git a/compiler/utils/dedupe_set-inl.h b/dex2oat/utils/dedupe_set-inl.h
similarity index 98%
rename from compiler/utils/dedupe_set-inl.h
rename to dex2oat/utils/dedupe_set-inl.h
index db744c53f7..5de82c534b 100644
--- a/compiler/utils/dedupe_set-inl.h
+++ b/dex2oat/utils/dedupe_set-inl.h
@@ -14,8 +14,8 @@
  * limitations under the License.
  */
 
-#ifndef ART_COMPILER_UTILS_DEDUPE_SET_INL_H_
-#define ART_COMPILER_UTILS_DEDUPE_SET_INL_H_
+#ifndef ART_DEX2OAT_UTILS_DEDUPE_SET_INL_H_
+#define ART_DEX2OAT_UTILS_DEDUPE_SET_INL_H_
 
 #include "dedupe_set.h"
 
@@ -272,4 +272,4 @@ std::string DedupeSet<InKey, StoreKey, Alloc, HashType, HashFunc, kShard>::DumpS
 
 }  // namespace art
 
-#endif  // ART_COMPILER_UTILS_DEDUPE_SET_INL_H_
+#endif  // ART_DEX2OAT_UTILS_DEDUPE_SET_INL_H_
diff --git a/compiler/utils/dedupe_set.h b/dex2oat/utils/dedupe_set.h
similarity index 92%
rename from compiler/utils/dedupe_set.h
rename to dex2oat/utils/dedupe_set.h
index 42db8e3ca0..2b8d713cb8 100644
--- a/compiler/utils/dedupe_set.h
+++ b/dex2oat/utils/dedupe_set.h
@@ -14,8 +14,8 @@
  * limitations under the License.
  */
 
-#ifndef ART_COMPILER_UTILS_DEDUPE_SET_H_
-#define ART_COMPILER_UTILS_DEDUPE_SET_H_
+#ifndef ART_DEX2OAT_UTILS_DEDUPE_SET_H_
+#define ART_DEX2OAT_UTILS_DEDUPE_SET_H_
 
 #include <stdint.h>
 #include <memory>
@@ -61,4 +61,4 @@ class DedupeSet {
 
 }  // namespace art
 
-#endif  // ART_COMPILER_UTILS_DEDUPE_SET_H_
+#endif  // ART_DEX2OAT_UTILS_DEDUPE_SET_H_
diff --git a/compiler/utils/dedupe_set_test.cc b/dex2oat/utils/dedupe_set_test.cc
similarity index 100%
rename from compiler/utils/dedupe_set_test.cc
rename to dex2oat/utils/dedupe_set_test.cc
diff --git a/dex2oat/verifier_deps_test.cc b/dex2oat/verifier_deps_test.cc
index 828691d3e2..bffb3bccc5 100644
--- a/dex2oat/verifier_deps_test.cc
+++ b/dex2oat/verifier_deps_test.cc
@@ -55,6 +55,7 @@ class VerifierDepsCompilerCallbacks : public CompilerCallbacks {
   void AddUncompilableMethod([[maybe_unused]] MethodReference ref) override {}
   void AddUncompilableClass([[maybe_unused]] ClassReference ref) override {}
   void ClassRejected([[maybe_unused]] ClassReference ref) override {}
+  bool IsUncompilableMethod([[maybe_unused]] MethodReference ref) override { return false; }
 
   verifier::VerifierDeps* GetVerifierDeps() const override { return deps_; }
   void SetVerifierDeps(verifier::VerifierDeps* deps) override { deps_ = deps; }
diff --git a/dexdump/dexdump.cc b/dexdump/dexdump.cc
index 43ed224b01..dd90d90cf2 100644
--- a/dexdump/dexdump.cc
+++ b/dexdump/dexdump.cc
@@ -1341,7 +1341,7 @@ static void dumpCode(const DexFile* pDexFile, u4 idx, u4 flags,
 
 static std::string GetHiddenapiFlagStr(uint32_t hiddenapi_flags) {
   std::stringstream ss;
-  hiddenapi::ApiList api_list(hiddenapi_flags);
+  hiddenapi::ApiList api_list = hiddenapi::ApiList::FromDexFlags(hiddenapi_flags);
   api_list.Dump(ss);
   std::string str_api_list = ss.str();
   std::transform(str_api_list.begin(), str_api_list.end(), str_api_list.begin(), ::toupper);
diff --git a/dexopt_chroot_setup/dexopt_chroot_setup.cc b/dexopt_chroot_setup/dexopt_chroot_setup.cc
index a88195d443..2e3c9f46d4 100644
--- a/dexopt_chroot_setup/dexopt_chroot_setup.cc
+++ b/dexopt_chroot_setup/dexopt_chroot_setup.cc
@@ -194,9 +194,15 @@ Result<void> BindMountDirect(const std::string& source, const std::string& targe
 }
 
 // Bind-mounts `source` at `target` with the mount propagation type being "slave+shared".
-Result<void> BindMount(const std::string& source, const std::string& target) {
+// By default, this function rejects `source` in chroot, to avoid accidental repeated bind-mounting.
+// If you intentionally want `source` to be in chroot, set `check_source_is_not_in_chroot` to false.
+Result<void> BindMount(const std::string& source,
+                       const std::string& target,
+                       bool check_source_is_not_in_chroot = true) {
   // Don't bind-mount repeatedly.
-  CHECK(!PathStartsWith(source, DexoptChrootSetup::CHROOT_DIR));
+  if (check_source_is_not_in_chroot) {
+    CHECK(!PathStartsWith(source, DexoptChrootSetup::CHROOT_DIR));
+  }
   // Don't follow symlinks.
   CHECK(!OR_RETURN(IsSelfOrParentSymlink(target))) << target;
   // system_server has a different mount namespace from init, and it uses slave mounts. E.g:
@@ -319,14 +325,24 @@ Result<void> BindMountRecursive(const std::string& source, const std::string& ta
       // Match paths for the "u:object_r:apk_tmp_file:s0" file context in
       // system/sepolicy/private/file_contexts.
       std::regex apk_tmp_file_re(R"re((/data|/mnt/expand/[^/]+)/app/vmdl[^/]+\.tmp(/.*)?)re");
-      std::smatch match;
-      if (std::regex_match(entry.mount_point, match, apk_tmp_file_re)) {
+      if (std::regex_match(entry.mount_point, apk_tmp_file_re)) {
         // Don't bother. The mount point is a temporary directory created by Package Manager during
         // app install. We won't be able to dexopt the app there anyway because it's not in the
         // Package Manager's snapshot.
         LOG(INFO) << ART_FORMAT("Skipped temporary mount point '{}'", entry.mount_point);
         continue;
       }
+
+      std::regex vendor_file_re(R"re(/data/vendor(/.*)?)re");
+      if (std::regex_match(entry.mount_point, vendor_file_re)) {
+        // We can't reliably bind-mount vendor-specific files because those files can have
+        // vendor-specific SELinux file contexts, which by design cannot be referenced by
+        // `dexopt_chroot_setup.te`. In practice, we don't need to bind-mount those files because
+        // they are unlikely to contain things useful to us.
+        LOG(INFO) << ART_FORMAT("Skipped vendor mount point '{}'", entry.mount_point);
+        continue;
+      }
+
       return result;
     }
   }
@@ -477,7 +493,7 @@ Result<void> PrepareExternalLibDirs() {
   // to do next.
   Result<void> result = BindMount(existing_lib_dirs[0], PathInChroot(existing_lib_dirs[0]));
   if (result.ok()) {
-    for (size_t i = 1; i < existing_lib_dirs.size(); i++) {
+    for (size_t i = 1; i < existing_lib_dirs.size(); ++i) {
       OR_RETURN(BindMount(existing_lib_dirs[i], PathInChroot(existing_lib_dirs[i])));
     }
   } else if (result.error().code() == EACCES) {
@@ -502,6 +518,36 @@ Result<void> PrepareExternalLibDirs() {
     return result;
   }
 
+  // Back up the new classpaths dir before bind-mounting etc dirs. We need the new classpaths dir
+  // for derive_classpath.
+  std::string classpaths_tmp_dir = PathInChroot("/mnt/classpaths");
+  OR_RETURN(CreateDir(classpaths_tmp_dir));
+  OR_RETURN(BindMount(PathInChroot("/system/etc/classpaths"),
+                      classpaths_tmp_dir,
+                      /*check_source_is_not_in_chroot=*/false));
+
+  // Old platform libraries expect old etc dirs, so we should bind-mount them as well.
+  OR_RETURN(BindMount("/system/etc", PathInChroot("/system/etc")));
+  OR_RETURN(BindMount("/system_ext/etc", PathInChroot("/system_ext/etc")));
+  OR_RETURN(BindMount("/product/etc", PathInChroot("/product/etc")));
+  result = BindMount("/vendor/etc", PathInChroot("/vendor/etc"));
+  if (!result.ok()) {
+    if (result.error().code() == EACCES) {
+      // We don't have the permission to do so on V. That's fine because the V version of the
+      // platform libraries are fine with the B version of /vendor/etc at the time of writing. Even
+      // if it's not fine, there is nothing we can do.
+      LOG(WARNING) << result.error().message();
+    } else {
+      return result;
+    }
+  }
+
+  // Restore the classpaths dir.
+  OR_RETURN(BindMount(classpaths_tmp_dir,
+                      PathInChroot("/system/etc/classpaths"),
+                      /*check_source_is_not_in_chroot=*/false));
+  OR_RETURN(Unmount(classpaths_tmp_dir));
+
   return {};
 }
 
@@ -705,19 +751,28 @@ Result<void> DexoptChrootSetup::InitChroot() const {
 }
 
 Result<void> DexoptChrootSetup::TearDownChroot() const {
-  // For mount points in `kExternalLibDirs`, make sure we have unmounted them before running apexd,
-  // as apexd expects new libraries.
+  // For platform library dirs and etc dirs, make sure we have unmounted them before running apexd,
+  // as apexd expects new libraries (and probably new etc dirs).
   // For mount points under "/mnt/compat_env", make sure we have unmounted them before running
   // apexd, as apexd doesn't expect apexes to be in-use.
+  // The list is in mount order.
   std::vector<FstabEntry> entries = OR_RETURN(GetProcMountsDescendantsOfPath(CHROOT_DIR));
-  for (const FstabEntry entry : entries) {
+  for (auto it = entries.rbegin(); it != entries.rend(); ++it) {
+    const FstabEntry& entry = *it;
     std::string_view mount_point_in_chroot = entry.mount_point;
     CHECK(ConsumePrefix(&mount_point_in_chroot, CHROOT_DIR));
     if (mount_point_in_chroot.empty()) {
       continue;  // The root mount.
     }
     if (ContainsElement(kExternalLibDirs, mount_point_in_chroot) ||
-        PathStartsWith(mount_point_in_chroot, "/mnt/compat_env")) {
+        PathStartsWith(mount_point_in_chroot, "/mnt/compat_env") ||
+        ContainsElement({"/system/etc",
+                         "/system_ext/etc",
+                         "/product/etc",
+                         "/vendor/etc",
+                         "/system/etc/classpaths",
+                         "/mnt/classpaths"},
+                        mount_point_in_chroot)) {
       OR_RETURN(Unmount(entry.mount_point));
     }
   }
@@ -748,7 +803,7 @@ Result<void> DexoptChrootSetup::TearDownChroot() const {
 
   // The list is in mount order.
   entries = OR_RETURN(GetProcMountsDescendantsOfPath(CHROOT_DIR));
-  for (auto it = entries.rbegin(); it != entries.rend(); it++) {
+  for (auto it = entries.rbegin(); it != entries.rend(); ++it) {
     OR_RETURN(Unmount(it->mount_point));
   }
 
diff --git a/disassembler/disassembler_arm.cc b/disassembler/disassembler_arm.cc
index c2156ca5e1..dd2a49fde9 100644
--- a/disassembler/disassembler_arm.cc
+++ b/disassembler/disassembler_arm.cc
@@ -220,6 +220,9 @@ void DisassemblerArm::CustomDisassembler::CustomDisassemblerStream::PrintLiteral
 DisassemblerArm::DisassemblerArm(DisassemblerOptions* options)
     : Disassembler(options), disasm_(std::make_unique<CustomDisassembler>(output_, options)) {}
 
+DisassemblerArm::~DisassemblerArm() {
+}
+
 size_t DisassemblerArm::Dump(std::ostream& os, const uint8_t* begin) {
   uintptr_t next;
   // Remove the Thumb specifier bit; no effect if begin does not point to T32 code.
diff --git a/disassembler/disassembler_arm.h b/disassembler/disassembler_arm.h
index dd6621d344..7e530afc5e 100644
--- a/disassembler/disassembler_arm.h
+++ b/disassembler/disassembler_arm.h
@@ -31,6 +31,7 @@ class DisassemblerArm final : public Disassembler {
 
  public:
   explicit DisassemblerArm(DisassemblerOptions* options);
+  ~DisassemblerArm();
 
   size_t Dump(std::ostream& os, const uint8_t* begin) override;
   void Dump(std::ostream& os, const uint8_t* begin, const uint8_t* end) override;
diff --git a/imgdiag/Android.bp b/imgdiag/Android.bp
index cb4efa8c49..c20d30b9ac 100644
--- a/imgdiag/Android.bp
+++ b/imgdiag/Android.bp
@@ -110,10 +110,9 @@ art_cc_test {
 }
 
 cc_defaults {
-    name: "pageinfo-defaults",
+    name: "page_util-defaults",
     host_supported: true,
     srcs: [
-        "page_info.cc",
         "page_util.cc",
     ],
     defaults: ["art_defaults"],
@@ -122,6 +121,8 @@ cc_defaults {
 
     shared_libs: [
         "libbase",
+        "libartbase",
+        "libart",
     ],
     static_libs: [
         "libprocinfo",
@@ -142,10 +143,21 @@ cc_defaults {
 
 art_cc_binary {
     name: "pageinfo",
-    defaults: ["pageinfo-defaults"],
-    shared_libs: [
-        "libart",
-        "libartbase",
+    defaults: ["page_util-defaults"],
+    srcs: [
+        "page_info.cc",
+    ],
+    apex_available: [
+        "com.android.art",
+        "com.android.art.debug",
+    ],
+}
+
+art_cc_binary {
+    name: "find_unshared_pages",
+    defaults: ["page_util-defaults"],
+    srcs: [
+        "find_unshared_pages.cc",
     ],
     apex_available: [
         "com.android.art",
diff --git a/imgdiag/ati_download_artifacts.py b/imgdiag/ati_download_artifacts.py
new file mode 100755
index 0000000000..8c36d2701a
--- /dev/null
+++ b/imgdiag/ati_download_artifacts.py
@@ -0,0 +1,195 @@
+#! /usr/bin/env python3
+
+"""
+Download test artifacts from ATI (Android Test Investigate).
+
+This script downloads imgdiag artifacts for specified ATI test runs.
+
+Usage:
+    # Download artifacts from specific runs:
+    ./ati_download_artifacts.py --invocation-id I79100010355578895 --invocation-id I84900010357346481
+
+    # Download latest 10 imgdiag runs:
+    ./ati_download_artifacts.py --imgdiag-atp 10
+
+    # Check all command line flags:
+    ./ati_download_artifacts.py --help
+"""
+
+import tempfile
+import argparse
+import subprocess
+import time
+import os
+import concurrent.futures
+import json
+
+try:
+  from tqdm import tqdm
+except:
+  def tqdm(x, *args, **kwargs):
+    return x
+
+LIST_ARTIFACTS_QUERY = """
+SELECT
+  CONCAT('https://android-build.corp.google.com/builds/', invocation_id, '/', name) AS url
+FROM android_build.testartifacts.latest
+WHERE invocation_id = '{invocation_id}';
+"""
+
+LIST_IMGDIAG_RUNS_QUERY = """
+SELECT t.invocation_id,
+ t.test.name,
+ t.timing.creation_timestamp
+FROM android_build.invocations.latest AS t
+WHERE t.test.name like '%imgdiag_top_100%'
+ORDER BY t.timing.creation_timestamp DESC
+LIMIT {};
+"""
+
+REQUIRED_IMGDIAG_FILES = [
+  "combined_imgdiag_data",
+  "all-dirty-objects",
+  "dirty-image-objects-art",
+  "dirty-image-objects-framework",
+  "dirty-page-counts",
+]
+
+def filter_artifacts(artifacts):
+  return [a for a in artifacts if any(x in a for x in REQUIRED_IMGDIAG_FILES)]
+
+def list_last_imgdiag_runs(run_count):
+  query_file = tempfile.NamedTemporaryFile()
+  out_file = tempfile.NamedTemporaryFile()
+  with open(query_file.name, 'w') as f:
+    f.write(LIST_IMGDIAG_RUNS_QUERY.format(run_count))
+  cmd = f'f1-sql --input_file={query_file.name} --output_file={out_file.name} --csv_output --print_queries=false'
+  res = subprocess.run(cmd, shell=True, check=True, capture_output=True)
+  with open(out_file.name) as f:
+    content = f.read()
+  content = content.split()[1:]
+  for i in range(len(content)):
+    content[i] = content[i].replace('"', '').split(',')
+  return content
+
+def list_artifacts(invocation_id):
+  if not invocation_id:
+    raise ValueError(f'Invalid invocation: {invocation_id}')
+
+  query_file = tempfile.NamedTemporaryFile()
+  out_file = tempfile.NamedTemporaryFile()
+  with open(query_file.name, 'w') as f:
+    f.write(LIST_ARTIFACTS_QUERY.format(invocation_id=invocation_id))
+  cmd = f'f1-sql --input_file={query_file.name} --output_file={out_file.name} --csv_output --print_queries=false --quiet'
+  execute_command(cmd)
+  with open(out_file.name) as f:
+    content = f.read()
+  content = content.split()
+  content = content[1:]
+  for i in range(len(content)):
+    content[i] = content[i].replace('"', '')
+  return content
+
+def execute_command(cmd):
+  for i in range(5):
+    try:
+      subprocess.run(cmd, shell=True, check=True)
+      return
+    except Exception as e:
+      print(f'Failed to run: {cmd}\nException: {e}')
+      time.sleep(2 ** i)
+
+  raise RuntimeError(f'Failed to run: {cmd}')
+
+def download_artifacts(res_dir, artifacts):
+  os.makedirs(res_dir, exist_ok=True)
+
+  commands = []
+  for url in artifacts:
+    filename = url.split('/')[-1]
+    out_path = os.path.join(res_dir, filename)
+    cmd = f'sso_client {url} --connect_timeout=120 --dns_timeout=120 --request_timeout=600 --location > {out_path}'
+    commands.append(cmd)
+
+  if not commands:
+    return
+
+  with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
+    res = list(tqdm(executor.map(execute_command, commands), total=len(commands), leave=False))
+
+
+def download_invocations(args, invocations):
+  for invoc_id in tqdm(invocations):
+    artifacts = list_artifacts(invoc_id)
+    if not args.download_all:
+      artifacts = filter_artifacts(artifacts)
+
+    res_dir = os.path.join(args.out_dir, invoc_id)
+    download_artifacts(res_dir, artifacts)
+
+
+def main():
+  parser = argparse.ArgumentParser(
+    description='Download artifacts from ATI',
+    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
+  )
+  parser.add_argument(
+    '--out-dir',
+    default='./',
+    help='Output dir for downloads',
+  )
+  parser.add_argument(
+    '--invocation-id',
+    action='append',
+    default=None,
+    help='Download artifacts from the specified invocations',
+  )
+  parser.add_argument(
+    '--imgdiag-atp',
+    metavar='N',
+    dest='atp_run_count',
+    type=int,
+    default=None,
+    help='Download latest N imgdiag runs',
+  )
+  parser.add_argument(
+    '--download-all',
+    action=argparse.BooleanOptionalAction,
+    default=False,
+    help='Whether to download all artifacts or combined imgdiag data only',
+  )
+  parser.add_argument(
+    '--overwrite',
+    action=argparse.BooleanOptionalAction,
+    default=False,
+    help='Download artifacts again even if the invocation_id dir already exists',
+  )
+  args = parser.parse_args()
+  if not args.invocation_id and not args.atp_run_count:
+    print('Must specify at least one of: --invocation-id or --imgdiag-atp')
+    return
+
+  invocations = set()
+  if args.invocation_id:
+    invocations.update(args.invocation_id)
+  if args.atp_run_count:
+    recent_runs = list_last_imgdiag_runs(args.atp_run_count)
+    invocations.update({invoc_id for invoc_id, name, timestamp in recent_runs})
+
+  if not args.overwrite:
+    existing_downloads = set()
+    for invoc_id in invocations:
+      res_dir = os.path.join(args.out_dir, invoc_id)
+      if os.path.isdir(res_dir):
+        existing_downloads.add(invoc_id)
+
+    if existing_downloads:
+      print(f'Skipping existing downloads: {existing_downloads}')
+      invocations = invocations - existing_downloads
+
+  print(f'Downloading: {invocations}')
+  download_invocations(args, invocations)
+
+
+if __name__ == '__main__':
+  main()
diff --git a/imgdiag/compare_imgdiag_runs.py b/imgdiag/compare_imgdiag_runs.py
new file mode 100755
index 0000000000..780ec2dd22
--- /dev/null
+++ b/imgdiag/compare_imgdiag_runs.py
@@ -0,0 +1,126 @@
+#! /usr/bin/env python3
+
+"""
+Compare private dirty page counts between imgdiag test runs.
+
+This script compares private dirty page counts in the boot-image object section
+between specified imgdiag runs.
+
+Usage:
+    # Compare multiple imgdiag runs:
+    ./compare_imgdiag_runs.py ./I1234 ./I1235 ./I1236
+
+    # Check all command line flags:
+    ./compare_imgdiag_runs.py --help
+"""
+
+import argparse
+import glob
+import gzip
+import json
+import os
+import pprint
+import statistics
+
+"""
+These thresholds are used to verify that process sets from different runs
+are similar enough for meaningful comparison. Constants are based on
+the imgdiag run data of 2025-01. Update if necessary.
+"""
+
+# There are about 100 apps in imgdiag_top_100 ATP config + more than 100 system processes.
+MIN_COMMON_PROC_COUNT = 200
+# Allow for a relatively small (<20%) mismatch in process sets between runs.
+MAX_MISMATCH_PROC_COUNT = 40
+
+def main():
+  pp = pprint.PrettyPrinter(indent=2)
+  parser = argparse.ArgumentParser(
+    description='Compare private dirty page counts between imgdiag ATP runs',
+    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
+  )
+
+  parser.add_argument(
+    'invocation_dirs',
+    nargs='+',
+    help='Directories with imgdiag output files',
+  )
+
+  parser.add_argument(
+    '--verbose',
+    action=argparse.BooleanOptionalAction,
+    default=False,
+    help='Print out all mismatched processes and more info about dirty page diffs between individual processes',
+  )
+  args = parser.parse_args()
+
+  runs = []
+  for invoc_dir in args.invocation_dirs:
+    res = glob.glob(os.path.join(invoc_dir, '*dirty-page-counts*'))
+    if len(res) != 1:
+      raise ValueError(f"Expected to find exactly one *dirty-page-counts* file in {invoc_dir}, but found: {res}")
+
+    try:
+      if res[0].endswith('.gz'):
+          with gzip.open(res[0], 'rb') as f:
+            contents = json.load(f)
+      else:
+        with open(res[0], 'r') as f:
+          contents = json.load(f)
+    except:
+      print('Failed to read: ', res[0])
+      raise
+    runs.append(contents)
+
+  basename = lambda p: os.path.basename(os.path.normpath(p))
+  invoc_ids = [basename(path) for path in args.invocation_dirs]
+  print('Comparing: ', invoc_ids)
+
+  items = list()
+  for r in runs:
+    items.append({k[:k.rfind('_')]: v for k, v in r.items()})
+
+  proc_names = list(set(i.keys()) for i in items)
+  common_proc_names = set.intersection(*proc_names)
+  mismatch_proc_names = set.union(*proc_names) - common_proc_names
+  print('Common proc count (used in the comparison): ', len(common_proc_names))
+  if len(common_proc_names) < MIN_COMMON_PROC_COUNT:
+      print('WARNING: common processes count is too low.')
+      print(f'Should be at least {MIN_COMMON_PROC_COUNT}.')
+
+  print('Mismatching proc count (not present in all runs): ', len(mismatch_proc_names))
+  if len(mismatch_proc_names) > MAX_MISMATCH_PROC_COUNT:
+      print('WARNING: too many mismatching process names.')
+      print(f'Should be lower than {MAX_MISMATCH_PROC_COUNT}.')
+
+  if args.verbose:
+    print("Mismatching process names:")
+    pp.pprint(mismatch_proc_names)
+
+  dirty_page_sums = list()
+  for r in items:
+    dirty_page_sums.append(sum(r[k] for k in common_proc_names))
+
+  print(f'Total dirty pages:\n{dirty_page_sums}\n')
+
+  mean_dirty_pages = [s / len(common_proc_names) for s in dirty_page_sums]
+  print(f'Mean dirty pages:\n{mean_dirty_pages}\n')
+
+  median_dirty_pages = [statistics.median(r[name] for name in common_proc_names) for r in items]
+  print(f'Median dirty pages:\n{median_dirty_pages}\n')
+
+  if args.verbose:
+    print(f'Largest dirty page diffs:')
+    for i in range(len(invoc_ids)):
+      for j in range(len(invoc_ids)):
+        if j <= i:
+          continue
+
+        page_count_diffs = [(proc_name, items[i][proc_name], items[j][proc_name], items[j][proc_name] - items[i][proc_name]) for proc_name in common_proc_names]
+        page_count_diffs = sorted(page_count_diffs, key=lambda x: abs(x[3]), reverse=True)
+        print(f'Between {invoc_ids[i]} and {invoc_ids[j]}: ')
+        pp.pprint(page_count_diffs[:10])
+
+
+if __name__ == '__main__':
+  main()
diff --git a/imgdiag/create_dirty_image_objects.py b/imgdiag/create_dirty_image_objects.py
index da0ed2c918..cabb097015 100755
--- a/imgdiag/create_dirty_image_objects.py
+++ b/imgdiag/create_dirty_image_objects.py
@@ -101,6 +101,18 @@ def process_dirty_entries(entries, sort_type):
 
   return (dirty_obj_lines, sort_keys)
 
+def split_dirty_objects(dirty_objects):
+  art_objects = list()
+  framework_objects = list()
+  for obj in dirty_objects:
+    obj_without_location = obj.split(' ', 1)[1]
+    is_art_module_object = obj.startswith('/apex/com.android.art/')
+    is_primitive_array = obj.startswith('primitive')
+    if is_art_module_object or is_primitive_array:
+      art_objects.append(obj_without_location)
+    else:
+      framework_objects.append(obj_without_location)
+  return art_objects, framework_objects
 
 def main():
   parser = argparse.ArgumentParser(
@@ -172,6 +184,12 @@ def main():
   with open(args.output_filename, 'w') as f:
     f.writelines(dirty_image_objects)
 
+  art_objs, framework_objs = split_dirty_objects(dirty_image_objects)
+  with open('art_' + args.output_filename, 'w') as f:
+    f.writelines(art_objs)
+  with open('framework_' + args.output_filename, 'w') as f:
+    f.writelines(framework_objs)
+
   if args.print_stats:
     print(','.join(k for k, v in entries), ',obj_count')
     total_count = 0
diff --git a/imgdiag/dirty_image_objects.md b/imgdiag/dirty_image_objects.md
index 5d28b04ee6..6726c6fce6 100644
--- a/imgdiag/dirty_image_objects.md
+++ b/imgdiag/dirty_image_objects.md
@@ -1,6 +1,118 @@
-# How to update dirty-image-objects
+# How to update dirty-image-objects (from ATP top 100 run)
 
-1. Install ART APEX with imgdiag and reboot, e.g.:
+## 1. Collect dirty objects from most used apps using ATP top100 test run (use specific branch/Android release)
+
+**IMPORTANT**: Make sure that pinned build for `GIT_TRUNK_RELEASE` branch is up-to-date. Update in `BUILD_IDS` here:
+<https://source.corp.google.com/piper///depot/google3/configs/wireless/android/testing/atp/prod/mainline-engprod/common.gcl;l=28-44>
+
+
+**NOTE**: New branches can be added in `master-art_imgdiag_tests` definition here:
+<https://source.corp.google.com/piper///depot/google3/configs/wireless/android/testing/atp/prod/mainline-engprod/config.gcl;l=1384-1395>
+
+
+Go to <http://go/abtd> and create a test run with the following parameters:
+
+* Branch name: `git_main`
+
+* Test type: `ATP`
+
+* Test name: select one of the supported imgdiag ATP configurations, see the list here:
+<https://atp.googleplex.com/test_configs?testName=%25imgdiag_top_100%25>
+
+
+This will generate a profile based on the following:
+
+* ART module  latest version from `git_main`.
+
+* Top 100 apps  up-to-date list of app versions.
+
+* Platform  pinned version from <https://source.corp.google.com/piper///depot/google3/configs/wireless/android/testing/atp/prod/mainline-engprod/common.gcl;l=28-44>
+
+
+## 2. Create two CLs with new dirty-image-objects files (one for ART, one for frameworks)
+
+
+Download new dirty-image-objects files from the run, either manually from the
+website or using the script. Example:
+```
+ati_download_artifacts.py --invocation-id I70700010348949160
+```
+
+
+**NOTE**: Use `ati_download_artifacts.py -h` to see all command line flags.
+
+
+There should be two files named like this:
+```
+subprocess-dirty-image-objects-art_9607835265532903390.txt.gz
+subprocess-dirty-image-objects-framework_10599183892981195850.txt.gz
+```
+
+dirty-image-objects-art goes into platform/art repo: <https://cs.android.com/android/platform/superproject/main/+/main:art/build/apex/dirty-image-objects>
+
+
+dirty-image-objects-framework goes into platform/frameworks/base repo: <https://cs.android.com/android/platform/superproject/main/+/main:frameworks/base/config/dirty-image-objects>
+
+
+Upload the two CLs and put them in the same topic.
+
+## 3. Start an ABTD build from the same branch with dirty-image-objects CLs on top
+
+Create a build for the same branch and target that was used in step 1, apply CLs with new dirty-image-objects.
+For `v2/mainline-engprod/apct/mainline/art/GIT_TRUNK_RELEASE/panther/imgdiag_top_100`:
+
+* Branch: `git_trunk-release`
+
+* Target: `panther-userdebug`
+
+## 4. Run ATP test again, this time with custom build-id from step 3 and with CLs from step 2
+
+Rerun the test from step 1 with the following additions:
+* Add the CLs with new dirty-image-objects (this will force ABTD to rebuild ART module with the change)
+* Select "Configure this test in **Advanced mode**". Scroll down to "**Extra Arguments**" and add **build-id**, set it to the build id from step 3.
+
+## 5. Download ATP results from step 1 and step 4 and compare them
+
+Use the script to download both ATP run results, example:
+```
+ati_download_artifacts.py --invocation-id I79100010355578895 --invocation-id I84900010357346481
+```
+
+Compare the results:
+```
+~/compare_imgdiag_runs.py I79100010355578895 I84900010357346481
+
+# Example comparison output:
+
+Comparing:  ['I79100010355578895', 'I84900010357346481']
+Common proc count (used in the comparison):  233
+Mismatching proc count (not present in all runs):  10
+Total dirty pages:
+[17066, 14799]
+
+Mean dirty pages:
+[73.24463519313305, 63.51502145922747]
+
+Median dirty pages:
+[69, 60]
+```
+
+Note the number of common processes, it should be at least 200 (approx 100 from top100 apps + another 100+ system processes). If it is lower than that, it might indicate missing processes in one or both runs, which would invalidate the comparison.
+The lower the number of mismatching processes the better. Typically less than 40 is fine.
+
+
+In there is a significant difference between the mean and median dirty page counts, it may be useful to check page count diffs for specific processess. Use `--verbose` flag with comparison script to show such info.
+
+The key number to look at for comparison is "mean dirty pages". In this example new profile saves about 10 pages per process (a mean reduction of 40 KiB memory per process).
+
+## 6. Submit new dirty-image-objects CLs if the result is better/good enough
+
+Typical measurement noise for mean dirty pages is less than 2. A new profile with dirty page reduction greater than this threshold is considered an improvement.
+
+
+# How to update dirty-image-objects (manually)
+
+## 1. Install imgdiag ART APEX on a device with AOSP build, e.g.:
 
 ```
 . ./build/envsetup.sh
@@ -10,7 +122,7 @@ adb install out/dist/test_imgdiag_com.android.art.apex
 adb reboot
 ```
 
-2. Collect imgdiag output.
+## 2. Collect imgdiag output.
 
 ```
 # To see all options check: art/imgdiag/run_imgdiag.py -h
@@ -18,7 +130,7 @@ adb reboot
 art/imgdiag/run_imgdiag.py
 ```
 
-3. Create new dirty-image-objects.
+## 3. Create new dirty-image-objects files.
 
 ```
 # To see all options check: art/imgdiag/create_dirty_image_objects.py -h
@@ -36,8 +148,14 @@ art/imgdiag/create_dirty_image_objects.py \
   ./imgdiag_com.google.android.gms.unstable.txt
 ```
 
-The resulting file will contain a list of dirty objects with optional
-(enabled by default) sort keys in the following format:
+This will generate 3 files:
+* dirty-image-objects.txt -- contains all dirty objects + info about the jar they are defined in.
+* art\_dirty-image-objects.txt -- dirty objects defined in the ART module, these objects are defined in art/build/apex/dirty-image-objects
+* framework\_dirty\_image\_objects.txt -- dirty objects defined in frameworks, these objects are defined in frameworks/base/config/dirty-image-objects
+
+
+The resulting art/framework files will contain lists of dirty objects with
+optional (enabled by default) sort keys in the following format:
 ```
 <class_descriptor>[.<reference_field_name>:<reference_field_type>]* [<sort_key>]
 ```
@@ -54,15 +172,17 @@ All dirty objects will be placed in the dirty bin of the boot image and sorted
 by the sort\_key values. I.e., dirty entries with sort\_key==N will have lower
 address than entries with sort\_key==N+1.
 
-4. Push new dirty-image-objects to the device.
+## 4. Push new frameworks dirty-image-objects to the device.
 
 ```
-adb push dirty-image-objects.txt /etc/dirty-image-objects
+adb push framework_dirty-image-objects.txt /etc/dirty-image-objects
 ```
 
-5. Reinstall ART APEX to update the boot image.
+## 5. Install ART module with new dirty-image-objects
 
 ```
+cp ./art_dirty-image-objects.txt $ANDROID_BUILD_TOP/art/build/apex/dirty-image-objects
+m apps_only dist
 adb install out/dist/com.android.art.apex
 adb reboot
 ```
diff --git a/imgdiag/find_unshared_pages.cc b/imgdiag/find_unshared_pages.cc
new file mode 100644
index 0000000000..993a8d3f81
--- /dev/null
+++ b/imgdiag/find_unshared_pages.cc
@@ -0,0 +1,282 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <signal.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <sys/stat.h>
+#include <sys/types.h>
+
+#include <ostream>
+#include <string>
+#include <vector>
+
+#include "android-base/parseint.h"
+#include "android-base/stringprintf.h"
+#include "base/os.h"
+#include "cmdline.h"
+#include "page_util.h"
+#include "procinfo/process_map.h"
+#include "scoped_thread_state_change-inl.h"
+
+namespace art {
+
+using android::base::StringPrintf;
+
+struct PageInfo {
+  // Page start address.
+  uint64_t start = -1;
+  // Page end address.
+  uint64_t end = -1;
+  // Number of times the physical page is mapped.
+  uint64_t page_count = -1;
+  // Physical frame number of the page.
+  uint64_t pfn = -1;
+  // Number of zero bytes in the page.
+  uint64_t zero_bytes_count = -1;
+  // Memory region of the page.
+  const android::procinfo::MapInfo* mem_map = nullptr;
+};
+
+struct ProcPages {
+  // Memory maps of the process.
+  std::vector<android::procinfo::MapInfo> maps;
+  // Page contents hash -> PageInfo
+  std::unordered_map<size_t, std::vector<PageInfo>> pages;
+};
+
+bool ReadProcessPages(
+    std::ostream& os, pid_t pid, ProcPages& proc_pages, ProcFiles& proc_files, uint64_t page_size) {
+  if (!android::procinfo::ReadProcessMaps(pid, &proc_pages.maps)) {
+    os << "Could not read process maps for " << pid;
+    return false;
+  }
+
+  std::string error_msg;
+  std::vector<uint8_t> page_contents(page_size);
+  for (const android::procinfo::MapInfo& map_info : proc_pages.maps) {
+    for (uint64_t start = map_info.start; start < map_info.end; start += page_size) {
+      PageInfo page_info;
+      page_info.start = start;
+      page_info.end = start + page_size;
+      page_info.mem_map = &map_info;
+      const size_t virtual_page_index = start / page_size;
+      if (!GetPageFrameNumber(proc_files.pagemap, virtual_page_index, page_info.pfn, error_msg)) {
+        os << error_msg;
+        return false;
+      }
+      if (!GetPageFlagsOrCounts(proc_files.kpagecount,
+                                ArrayRef<const uint64_t>(&page_info.pfn, 1),
+                                /*out*/ ArrayRef<uint64_t>(&page_info.page_count, 1),
+                                /*out*/ error_msg)) {
+        os << error_msg << std::endl;
+        os << "mem_map name: " << map_info.name << std::endl;
+        os << "pfn: " << page_info.pfn << std::endl;
+        os << "page_start: " << page_info.start << std::endl;
+        os << "mem_map start: " << map_info.start << std::endl;
+        // start = map_info.end;
+        continue;
+      }
+
+      if (page_info.page_count == 0) {
+        // Page is not present in memory.
+        continue;
+      }
+
+      // Handle present page.
+      if (!proc_files.mem.PreadFully(page_contents.data(), page_contents.size(), start)) {
+        os << StringPrintf("Failed to read present page %" PRIx64 " for mem_map %s\n",
+                           start,
+                           map_info.name.c_str());
+        return false;
+      }
+
+      page_info.zero_bytes_count =
+          static_cast<uint64_t>(std::count(page_contents.begin(), page_contents.end(), 0));
+
+      const size_t content_hash = DataHash::HashBytes(page_contents.data(), page_contents.size());
+      proc_pages.pages[content_hash].push_back(page_info);
+    }
+  }
+
+  return true;
+}
+
+int FindUnsharedPages(std::ostream& os, pid_t pid1, pid_t pid2, size_t page_size) {
+  ProcFiles proc_files1;
+  ProcFiles proc_files2;
+  std::string error_msg;
+  if (!OpenProcFiles(pid1, proc_files1, error_msg)) {
+    os << error_msg;
+    return EXIT_FAILURE;
+  }
+  if (!OpenProcFiles(pid2, proc_files2, error_msg)) {
+    os << error_msg;
+    return EXIT_FAILURE;
+  }
+
+  ProcPages proc_pages1;
+  if (!ReadProcessPages(os, pid1, proc_pages1, proc_files1, page_size)) {
+    return EXIT_FAILURE;
+  }
+
+  ProcPages proc_pages2;
+  if (!ReadProcessPages(os, pid2, proc_pages2, proc_files2, page_size)) {
+    return EXIT_FAILURE;
+  }
+
+  for (const auto& [hash, pages1] : proc_pages1.pages) {
+    // Skip zero pages.
+    if (pages1.front().zero_bytes_count == page_size) {
+      continue;
+    }
+
+    // Find pages with the same content in the second process.
+    if (proc_pages2.pages.find(hash) != proc_pages2.pages.end()) {
+      const auto& pages2 = proc_pages2.pages[hash];
+
+      std::unordered_set<uint64_t> pfns1, pfns2;
+      for (const auto& p : pages1) {
+        pfns1.insert(p.pfn);
+      }
+      for (const auto& p : pages2) {
+        pfns2.insert(p.pfn);
+      }
+
+      const bool is_different = (pfns1 != pfns2);
+      if (is_different) {
+        os << "\nDuplicate pages (pfn, start_addr, mem_map, zero_bytes_count)\nPID1:\n";
+        for (const auto& page1 : pages1) {
+          os << ART_FORMAT("\t{} {} {} {}\n",
+                           page1.pfn,
+                           page1.start,
+                           page1.mem_map->name,
+                           page1.zero_bytes_count);
+        }
+        os << "PID2:\n";
+        for (const auto& page2 : pages2) {
+          os << ART_FORMAT("\t{} {} {} {}\n",
+                           page2.pfn,
+                           page2.start,
+                           page2.mem_map->name,
+                           page2.zero_bytes_count);
+        }
+      }
+    }
+  }
+
+  return EXIT_SUCCESS;
+}
+
+struct FindUnsharedPagesArgs : public CmdlineArgs {
+ protected:
+  using Base = CmdlineArgs;
+
+  ParseStatus ParseCustom(const char* raw_option,
+                          size_t raw_option_length,
+                          std::string* error_msg) override {
+    DCHECK_EQ(strlen(raw_option), raw_option_length);
+    {
+      ParseStatus base_parse = Base::ParseCustom(raw_option, raw_option_length, error_msg);
+      if (base_parse != kParseUnknownArgument) {
+        return base_parse;
+      }
+    }
+
+    std::string_view option(raw_option, raw_option_length);
+    if (option.starts_with("--pid1=")) {
+      const char* value = raw_option + strlen("--pid1=");
+      if (!android::base::ParseInt(value, &pid1_)) {
+        *error_msg = "Failed to parse pid1";
+        return kParseError;
+      }
+    } else if (option.starts_with("--pid2=")) {
+      const char* value = raw_option + strlen("--pid2=");
+      if (!android::base::ParseInt(value, &pid2_)) {
+        *error_msg = "Failed to parse pid2";
+        return kParseError;
+      }
+    } else {
+      return kParseUnknownArgument;
+    }
+
+    return kParseOk;
+  }
+
+  ParseStatus ParseChecks(std::string* error_msg) override {
+    ParseStatus parent_checks = Base::ParseChecks(error_msg);
+    if (parent_checks != kParseOk) {
+      return parent_checks;
+    }
+
+    if (pid1_ == -1 || pid2_ == -1) {
+      *error_msg = "Missing --pid=";
+      return kParseError;
+    }
+
+    for (pid_t pid : {pid1_, pid2_}) {
+      if (kill(pid, /*sig*/ 0) != 0) {  // No signal is sent, perform error-checking only.
+        // Check if the pid exists before proceeding.
+        if (errno == ESRCH) {
+          *error_msg = "Process specified does not exist, pid: " + std::to_string(pid);
+        } else {
+          *error_msg = StringPrintf("Failed to check process status: %s", strerror(errno));
+        }
+        return kParseError;
+      }
+    }
+    return kParseOk;
+  }
+
+  std::string GetUsage() const override {
+    std::string usage;
+
+    usage +=
+        "Usage: find_unshared_pages [options] ...\n"
+        "    Example: find_unshared_pages --pid1=$(pidof system_server) --pid2=$(pidof "
+        "com.android.camera2)\n"
+        "\n";
+
+    usage += Base::GetUsage();
+
+    usage += "  --pid1=<pid> --pid2=<pid>: PIDs of the processes to analyze.\n";
+
+    return usage;
+  }
+
+ public:
+  pid_t pid1_ = -1;
+  pid_t pid2_ = -1;
+};
+
+struct FindUnsharedPagesMain : public CmdlineMain<FindUnsharedPagesArgs> {
+  bool ExecuteWithoutRuntime() override {
+    CHECK(args_ != nullptr);
+    CHECK(args_->os_ != nullptr);
+
+    return FindUnsharedPages(*args_->os_, args_->pid1_, args_->pid2_, MemMap::GetPageSize()) ==
+           EXIT_SUCCESS;
+  }
+
+  bool NeedsRuntime() override { return false; }
+};
+
+}  // namespace art
+
+int main(int argc, char** argv) {
+  art::FindUnsharedPagesMain main;
+  return main.Main(argc, argv);
+}
diff --git a/imgdiag/page_info.cc b/imgdiag/page_info.cc
index 025580a849..cdd1399d1a 100644
--- a/imgdiag/page_info.cc
+++ b/imgdiag/page_info.cc
@@ -41,47 +41,6 @@ using android::base::StringPrintf;
 
 namespace {
 
-struct ProcFiles {
-  // A File for reading /proc/<pid>/mem.
-  File mem;
-  // A File for reading /proc/<pid>/pagemap.
-  File pagemap;
-  // A File for reading /proc/kpageflags.
-  File kpageflags;
-  // A File for reading /proc/kpagecount.
-  File kpagecount;
-};
-
-bool OpenFile(const char* file_name, /*out*/ File& file, /*out*/ std::string& error_msg) {
-  std::unique_ptr<File> file_ptr = std::unique_ptr<File>{OS::OpenFileForReading(file_name)};
-  if (file_ptr == nullptr) {
-    error_msg = StringPrintf("Failed to open file: %s", file_name);
-    return false;
-  }
-  file = std::move(*file_ptr);
-  return true;
-}
-
-bool OpenProcFiles(pid_t pid, /*out*/ ProcFiles& files, /*out*/ std::string& error_msg) {
-  if (!OpenFile("/proc/kpageflags", files.kpageflags, error_msg)) {
-    return false;
-  }
-  if (!OpenFile("/proc/kpagecount", files.kpagecount, error_msg)) {
-    return false;
-  }
-  std::string mem_file_name =
-      StringPrintf("/proc/%ld/mem", static_cast<long>(pid));  // NOLINT [runtime/int]
-  if (!OpenFile(mem_file_name.c_str(), files.mem, error_msg)) {
-    return false;
-  }
-  std::string pagemap_file_name =
-      StringPrintf("/proc/%ld/pagemap", static_cast<long>(pid));  // NOLINT [runtime/int]
-  if (!OpenFile(pagemap_file_name.c_str(), files.pagemap, error_msg)) {
-    return false;
-  }
-  return true;
-}
-
 void DumpPageInfo(uint64_t virtual_page_index, ProcFiles& proc_files, std::ostream& os,
                   size_t page_size) {
   const uint64_t virtual_page_addr = virtual_page_index * page_size;
diff --git a/imgdiag/page_util.cc b/imgdiag/page_util.cc
index 0b765f48c0..66ea978c32 100644
--- a/imgdiag/page_util.cc
+++ b/imgdiag/page_util.cc
@@ -97,4 +97,34 @@ bool GetPageFrameNumbers(File& page_map_file,
   return true;
 }
 
+bool OpenFile(const char* file_name, /*out*/ File& file, /*out*/ std::string& error_msg) {
+  std::unique_ptr<File> file_ptr = std::unique_ptr<File>{OS::OpenFileForReading(file_name)};
+  if (file_ptr == nullptr) {
+    error_msg = StringPrintf("Failed to open file: %s", file_name);
+    return false;
+  }
+  file = std::move(*file_ptr);
+  return true;
+}
+
+bool OpenProcFiles(pid_t pid, /*out*/ ProcFiles& files, /*out*/ std::string& error_msg) {
+  if (!OpenFile("/proc/kpageflags", files.kpageflags, error_msg)) {
+    return false;
+  }
+  if (!OpenFile("/proc/kpagecount", files.kpagecount, error_msg)) {
+    return false;
+  }
+  std::string mem_file_name =
+      StringPrintf("/proc/%ld/mem", static_cast<long>(pid));  // NOLINT [runtime/int]
+  if (!OpenFile(mem_file_name.c_str(), files.mem, error_msg)) {
+    return false;
+  }
+  std::string pagemap_file_name =
+      StringPrintf("/proc/%ld/pagemap", static_cast<long>(pid));  // NOLINT [runtime/int]
+  if (!OpenFile(pagemap_file_name.c_str(), files.pagemap, error_msg)) {
+    return false;
+  }
+  return true;
+}
+
 }  // namespace art
diff --git a/imgdiag/page_util.h b/imgdiag/page_util.h
index dc3ba25581..1553868ff1 100644
--- a/imgdiag/page_util.h
+++ b/imgdiag/page_util.h
@@ -56,6 +56,19 @@ bool GetPageFrameNumbers(art::File& page_map_file,
                          /*out*/ ArrayRef<uint64_t> page_frame_numbers,
                          /*out*/ std::string& error_msg);
 
+struct ProcFiles {
+  // A File for reading /proc/<pid>/mem.
+  File mem;
+  // A File for reading /proc/<pid>/pagemap.
+  File pagemap;
+  // A File for reading /proc/kpageflags.
+  File kpageflags;
+  // A File for reading /proc/kpagecount.
+  File kpagecount;
+};
+
+bool OpenProcFiles(pid_t pid, /*out*/ ProcFiles& files, /*out*/ std::string& error_msg);
+
 }  // namespace art
 
 #endif  // ART_IMGDIAG_PAGE_UTIL_H_
diff --git a/libartbase/Android.bp b/libartbase/Android.bp
index d608272906..24603c1220 100644
--- a/libartbase/Android.bp
+++ b/libartbase/Android.bp
@@ -108,7 +108,7 @@ cc_defaults {
             ],
             export_shared_lib_headers: ["libbase"], // ART's macros.h depends on libbase's macros.h.
         },
-        linux_glibc: {
+        host_linux: {
             static_libs: [
                 "libcap",
             ],
@@ -180,7 +180,7 @@ cc_defaults {
                 "libcap",
             ],
         },
-        linux_glibc: {
+        host_linux: {
             whole_static_libs: [
                 "libcap",
             ],
@@ -323,10 +323,8 @@ art_cc_library_static {
     srcs: [
         "base/testing.cc",
     ],
-    header_libs: [
-        "libbase_headers",
-        "art_libartbase_headers",
-    ],
+    header_libs: ["art_libartbase_headers"],
+    export_header_lib_headers: ["art_libartbase_headers"],
 }
 
 art_cc_defaults {
@@ -337,6 +335,7 @@ art_cc_defaults {
     ],
     srcs: [
         "arch/instruction_set_test.cc",
+        "base/aconfig_flags_test.cc",
         "base/arena_allocator_test.cc",
         "base/bit_field_test.cc",
         "base/bit_memory_region_test.cc",
@@ -356,10 +355,10 @@ art_cc_defaults {
         "base/intrusive_forward_list_test.cc",
         "base/leb128_test.cc",
         "base/logging_test.cc",
-        "base/memfd_test.cc",
+        "base/mem_map_test.cc",
         "base/membarrier_test.cc",
+        "base/memfd_test.cc",
         "base/memory_region_test.cc",
-        "base/mem_map_test.cc",
         "base/metrics/metrics_test.cc",
         "base/scoped_flock_test.cc",
         "base/time_utils_test.cc",
diff --git a/libartbase/base/aconfig_flags_test.cc b/libartbase/base/aconfig_flags_test.cc
new file mode 100644
index 0000000000..8d873255bf
--- /dev/null
+++ b/libartbase/base/aconfig_flags_test.cc
@@ -0,0 +1,26 @@
+/*
+ * Copyright (C) 2024 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "com_android_art_flags.h"
+#include "gtest/gtest.h"
+
+namespace art {
+
+static_assert(COM_ANDROID_ART_FLAGS_TEST == true);
+
+TEST(AconfigFlagsTest, TestFlag) { EXPECT_TRUE(com::android::art::flags::test()); }
+
+}  // namespace art
diff --git a/libartbase/base/arena_allocator.cc b/libartbase/base/arena_allocator.cc
index a44d1ba0b2..6e51402045 100644
--- a/libartbase/base/arena_allocator.cc
+++ b/libartbase/base/arena_allocator.cc
@@ -77,7 +77,7 @@ const char* const ArenaAllocatorStatsImpl<kCount>::kAllocNames[] = {
   "SsaLiveness  ",
   "SsaPhiElim   ",
   "RefTypeProp  ",
-  "SelectGen    ",
+  "CtrlFlowSimp ",
   "SideEffects  ",
   "RegAllocator ",
   "RegAllocVldt ",
diff --git a/libartbase/base/arena_allocator.h b/libartbase/base/arena_allocator.h
index f7cda26f43..e47319c53e 100644
--- a/libartbase/base/arena_allocator.h
+++ b/libartbase/base/arena_allocator.h
@@ -88,7 +88,7 @@ enum ArenaAllocKind {
   kArenaAllocSsaLiveness,
   kArenaAllocSsaPhiElimination,
   kArenaAllocReferenceTypePropagation,
-  kArenaAllocSelectGenerator,
+  kArenaAllocControlFlowSimplifier,
   kArenaAllocSideEffectsAnalysis,
   kArenaAllocRegisterAllocator,
   kArenaAllocRegisterAllocatorValidate,
diff --git a/libartbase/base/arena_allocator_test.cc b/libartbase/base/arena_allocator_test.cc
index 6323a2b97c..d956b4ab5e 100644
--- a/libartbase/base/arena_allocator_test.cc
+++ b/libartbase/base/arena_allocator_test.cc
@@ -23,7 +23,7 @@
 
 namespace art {
 
-class ArenaAllocatorTest : public testing::Test {
+class ArenaAllocatorTest : public ::testing::Test {
  protected:
   size_t NumberOfArenas(ArenaAllocator* allocator) {
     size_t result = 0u;
diff --git a/libartbase/base/arena_bit_vector.h b/libartbase/base/arena_bit_vector.h
index 52ba24cecf..757f481b24 100644
--- a/libartbase/base/arena_bit_vector.h
+++ b/libartbase/base/arena_bit_vector.h
@@ -17,13 +17,14 @@
 #ifndef ART_LIBARTBASE_BASE_ARENA_BIT_VECTOR_H_
 #define ART_LIBARTBASE_BASE_ARENA_BIT_VECTOR_H_
 
+#include <algorithm>
+
 #include "arena_object.h"
 #include "base/arena_allocator.h"
 #include "bit_vector.h"
 
 namespace art {
 
-class ArenaAllocator;
 class ScopedArenaAllocator;
 
 /*
@@ -53,6 +54,22 @@ class ArenaBitVector : public BitVector, public ArenaObject<kArenaAllocGrowableB
 
   ArenaBitVector(ArenaBitVector&&) = default;
   ArenaBitVector(const ArenaBitVector&) = delete;
+
+  template <typename StorageType = size_t, typename Allocator>
+  static BitVectorView<StorageType> CreateFixedSize(
+      Allocator* allocator, size_t bits, ArenaAllocKind kind = kArenaAllocGrowableBitMap) {
+    static_assert(std::is_same_v<Allocator, ArenaAllocator> ||
+                  std::is_same_v<Allocator, ScopedArenaAllocator>);
+    size_t num_elements = BitVectorView<StorageType>::BitsToWords(bits);
+    StorageType* storage = allocator->template AllocArray<StorageType>(num_elements, kind);
+    BitVectorView<StorageType> result(storage, bits);
+    if (std::is_same_v<Allocator, ScopedArenaAllocator>) {
+      result.ClearAllBits();
+    } else {
+      DCHECK(!result.IsAnyBitSet());
+    }
+    return result;
+  }
 };
 
 }  // namespace art
diff --git a/libartbase/base/arena_containers.h b/libartbase/base/arena_containers.h
index e93c8d7bd0..78c5d43d59 100644
--- a/libartbase/base/arena_containers.h
+++ b/libartbase/base/arena_containers.h
@@ -19,6 +19,7 @@
 
 #include <deque>
 #include <forward_list>
+#include <list>
 #include <queue>
 #include <set>
 #include <stack>
@@ -53,6 +54,9 @@ using ArenaDeque = std::deque<T, ArenaAllocatorAdapter<T>>;
 template <typename T>
 using ArenaForwardList = std::forward_list<T, ArenaAllocatorAdapter<T>>;
 
+template <typename T>
+using ArenaList = std::list<T, ArenaAllocatorAdapter<T>>;
+
 template <typename T>
 using ArenaQueue = std::queue<T, ArenaDeque<T>>;
 
diff --git a/libartbase/base/bit_vector-inl.h b/libartbase/base/bit_vector-inl.h
index 2bdc14ebe9..60d905717c 100644
--- a/libartbase/base/bit_vector-inl.h
+++ b/libartbase/base/bit_vector-inl.h
@@ -20,73 +20,167 @@
 #include "bit_vector.h"
 
 #include <android-base/logging.h>
+#include <cstring>
 
 #include "bit_utils.h"
 
 namespace art {
 
-inline bool BitVector::IndexIterator::operator==(const IndexIterator& other) const {
-  DCHECK(bit_storage_ == other.bit_storage_);
-  DCHECK_EQ(storage_size_, other.storage_size_);
+template <typename StorageType>
+inline void BitVectorView<StorageType>::ClearAllBits() {
+  // Note: We do not `DCheckTrailingBitsClear()` here as this may be the initial call
+  // to clear the storage and the trailing bits may not be clear after allocation.
+  memset(storage_, 0, SizeInWords() * sizeof(WordType));
+}
+
+template <typename StorageType>
+inline void BitVectorView<StorageType>::SetInitialBits(uint32_t num_bits) {
+  // Note: We do not `DCheckTrailingBitsClear()` here as this may be the initial call
+  // to clear the storage and the trailing bits may not be clear after allocation.
+  DCHECK_LE(num_bits, SizeInBits());
+  size_t words = WordIndex(num_bits);
+  // Set initial full words.
+  std::fill_n(storage_, words, std::numeric_limits<WordType>::max());
+  if (num_bits % kWordBits != 0) {
+    // Set all bits below the first clear bit in the boundary storage word.
+    storage_[words] = BitMask(num_bits) - static_cast<StorageType>(1u);
+    ++words;
+  }
+  // Set clear words if any.
+  std::fill_n(storage_ + words, SizeInWords() - words, static_cast<StorageType>(0));
+}
+
+template <typename StorageType>
+inline bool BitVectorView<StorageType>::Union(BitVectorView<const StorageType> union_with) {
+  DCHECK_EQ(SizeInBits(), union_with.SizeInBits());
+  DCheckTrailingBitsClear();
+  union_with.DCheckTrailingBitsClear();
+  StorageType added_bits = 0u;
+  for (size_t i = 0, size = SizeInWords(); i != size; ++i) {
+    StorageType word = storage_[i];
+    StorageType union_with_word = union_with.storage_[i];
+    storage_[i] = union_with_word | word;
+    added_bits |= union_with_word & ~word;
+  }
+  return added_bits != 0u;
+}
+
+template <typename StorageType>
+inline bool BitVectorView<StorageType>::UnionIfNotIn(BitVectorView<const StorageType> union_with,
+                                                     BitVectorView<const StorageType> not_in) {
+  DCHECK_EQ(SizeInBits(), union_with.SizeInBits());
+  DCHECK_EQ(SizeInBits(), not_in.SizeInBits());
+  DCheckTrailingBitsClear();
+  union_with.DCheckTrailingBitsClear();
+  not_in.DCheckTrailingBitsClear();
+  StorageType added_bits = 0u;
+  for (size_t i = 0, size = SizeInWords(); i != size; ++i) {
+    StorageType word = storage_[i];
+    StorageType union_with_word = union_with.storage_[i] & ~not_in.storage_[i];
+    storage_[i] = union_with_word | word;
+    added_bits |= union_with_word & ~word;
+  }
+  return added_bits != 0u;
+}
+
+template <typename StorageType>
+inline bool BitVectorIndexIterator<StorageType>::operator==(
+    const BitVectorIndexIterator<StorageType>& other) const {
+  DCHECK_EQ(bit_vector_view_.storage_, other.bit_vector_view_.storage_);
+  DCHECK_EQ(bit_vector_view_.size_in_bits_, other.bit_vector_view_.size_in_bits_);
   return bit_index_ == other.bit_index_;
 }
 
-inline uint32_t BitVector::IndexIterator::operator*() const {
-  DCHECK_LT(bit_index_, BitSize());
+template <typename StorageType>
+inline bool BitVectorIndexIterator<StorageType>::operator!=(
+    const BitVectorIndexIterator<StorageType>& other) const {
+  return !(*this == other);
+}
+
+template <typename StorageType>
+inline size_t BitVectorIndexIterator<StorageType>::operator*() const {
+  DCHECK_LT(bit_index_, bit_vector_view_.size_in_bits_);
   return bit_index_;
 }
 
-inline BitVector::IndexIterator& BitVector::IndexIterator::operator++() {
-  DCHECK_LT(bit_index_, BitSize());
+template <typename StorageType>
+inline BitVectorIndexIterator<StorageType>& BitVectorIndexIterator<StorageType>::operator++() {
+  DCHECK_LT(bit_index_, bit_vector_view_.size_in_bits_);
   bit_index_ = FindIndex(bit_index_ + 1u);
   return *this;
 }
 
-inline BitVector::IndexIterator BitVector::IndexIterator::operator++(int) {
-  IndexIterator result(*this);
+template <typename StorageType>
+inline BitVectorIndexIterator<StorageType> BitVectorIndexIterator<StorageType>::operator++(int) {
+  BitVectorIndexIterator result(*this);
   ++*this;
   return result;
 }
 
-inline uint32_t BitVector::IndexIterator::FindIndex(uint32_t start_index) const {
-  DCHECK_LE(start_index, BitSize());
-  uint32_t word_index = start_index / kWordBits;
-  if (UNLIKELY(word_index == storage_size_)) {
+template <typename StorageType>
+inline size_t BitVectorIndexIterator<StorageType>::FindIndex(size_t start_index) const {
+  DCHECK_LE(start_index, bit_vector_view_.size_in_bits_);
+  bit_vector_view_.DCheckTrailingBitsClear();
+  if (UNLIKELY(start_index == bit_vector_view_.size_in_bits_)) {
     return start_index;
   }
-  uint32_t word = bit_storage_[word_index];
+  size_t word_index = start_index / kWordBits;
+  DCHECK_LT(word_index, bit_vector_view_.SizeInWords());
+  std::remove_const_t<StorageType> word = bit_vector_view_.storage_[word_index];
   // Mask out any bits in the first word we've already considered.
-  word &= static_cast<uint32_t>(-1) << (start_index & 0x1f);
-  while (word == 0u) {
-    ++word_index;
-    if (UNLIKELY(word_index == storage_size_)) {
-      return BitSize();
-    }
-    word = bit_storage_[word_index];
+  word &= std::numeric_limits<StorageType>::max() << (start_index % kWordBits);
+  if (word == 0u) {
+    size_t size_in_words = bit_vector_view_.SizeInWords();
+    do {
+      ++word_index;
+      if (UNLIKELY(word_index == size_in_words)) {
+        return bit_vector_view_.size_in_bits_;
+      }
+      word = bit_vector_view_.storage_[word_index];
+    } while (word == 0u);
   }
-  return word_index * 32u + CTZ(word);
+  return word_index * kWordBits + CTZ(word);
 }
 
-inline BitVector::IndexIterator::IndexIterator(const BitVector* bit_vector, begin_tag)
-  : bit_storage_(bit_vector->GetRawStorage()),
-    storage_size_(bit_vector->storage_size_),
-    bit_index_(FindIndex(0u)) { }
+template <typename StorageType>
+inline BitVectorIndexIterator<StorageType>::BitVectorIndexIterator(
+    BitVectorView<StorageType> bit_vector_view, begin_tag)
+    : bit_vector_view_(bit_vector_view),
+      bit_index_(FindIndex(0u)) { }
 
-inline BitVector::IndexIterator::IndexIterator(const BitVector* bit_vector, end_tag)
-  : bit_storage_(bit_vector->GetRawStorage()),
-    storage_size_(bit_vector->storage_size_),
-    bit_index_(BitSize()) { }
+template <typename StorageType>
+inline BitVectorIndexIterator<StorageType>::BitVectorIndexIterator(
+    BitVectorView<StorageType> bit_vector_view, end_tag)
+    : bit_vector_view_(bit_vector_view),
+      bit_index_(bit_vector_view_.size_in_bits_) { }
 
-inline BitVector::IndexIterator BitVector::IndexContainer::begin() const {
-  return IndexIterator(bit_vector_, IndexIterator::begin_tag());
-}
+template <typename StorageType>
+class BitVectorView<StorageType>::IndexContainerImpl {
+  static_assert(std::is_const_v<StorageType>);
+
+ public:
+  explicit IndexContainerImpl(BitVectorView<StorageType> bit_vector_view)
+      : bit_vector_view_(bit_vector_view) { }
+
+  BitVectorIndexIterator<StorageType> begin() const {
+    return {bit_vector_view_, typename BitVectorIndexIterator<StorageType>::begin_tag()};
+  }
 
-inline BitVector::IndexIterator BitVector::IndexContainer::end() const {
-  return IndexIterator(bit_vector_, IndexIterator::end_tag());
+  BitVectorIndexIterator<StorageType> end() const {
+    return {bit_vector_view_, typename BitVectorIndexIterator<StorageType>::end_tag()};
+  }
+
+ private:
+  BitVectorView<StorageType> bit_vector_view_;
+};
+
+template <typename StorageType>
+BitVectorView<StorageType>::IndexContainer BitVectorView<StorageType>::Indexes() const {
+  return BitVectorView<StorageType>::IndexContainer(*this);
 }
 
 inline void BitVector::ClearAllBits() {
-  memset(storage_, 0, storage_size_ * kWordBytes);
+  AsView().ClearAllBits();
 }
 
 inline bool BitVector::Equal(const BitVector* src) const {
@@ -95,6 +189,10 @@ inline bool BitVector::Equal(const BitVector* src) const {
     (memcmp(storage_, src->GetRawStorage(), storage_size_ * sizeof(uint32_t)) == 0);
 }
 
+inline BitVector::IndexContainer BitVector::Indexes() const {
+  return AsView().Indexes();
+}
+
 }  // namespace art
 
 #endif  // ART_LIBARTBASE_BASE_BIT_VECTOR_INL_H_
diff --git a/libartbase/base/bit_vector.h b/libartbase/base/bit_vector.h
index ec94efb09f..369f9171a2 100644
--- a/libartbase/base/bit_vector.h
+++ b/libartbase/base/bit_vector.h
@@ -19,97 +19,198 @@
 
 #include <stdint.h>
 
+#include <algorithm>
 #include <iterator>
+#include <limits>
 
 #include "bit_utils.h"
 #include "globals.h"
+#include "logging.h"
 
 namespace art {
 
 class Allocator;
-class ArenaBitVector;
 
-/*
- * Expanding bitmap. Bits are numbered starting from zero. All operations on a BitVector are
- * unsynchronized. New BitVectors are not necessarily zeroed out. If the used allocator doesn't do
- * clear the vector (e.g. ScopedArenaAllocator), the responsibility of clearing it relies on the
- * caller (e.g. ArenaBitVector).
- */
-class BitVector {
+// A bit vector view encapsulating externally-provided fixed-size storage for bits.
+//
+// The size in bits does not need to specify whole number of storage words but the view
+// is intended to work only on the specified number of bits. Single-bit functions
+// `SetBit()`, `ClearBit()` and `IsBitSet()` verify the passed index with `DCHECK()`
+// and do not care about trailing bits in the last storage word, if any. Multi-bit
+// functions require that the trailing bits are cleared on entry, except for functions
+// `ClearAllBits()` and `SetInitialBits()` that are used for storage initialization
+// and clear the trailing bits, if any.
+template <typename StorageType = size_t>
+class BitVectorView {
  public:
-  static constexpr uint32_t kWordBytes = sizeof(uint32_t);
-  static constexpr uint32_t kWordBits = kWordBytes * 8;
+  using WordType = StorageType;
+  static_assert(std::numeric_limits<WordType>::is_integer);
+  static_assert(!std::numeric_limits<WordType>::is_signed);
+  static constexpr size_t kWordBits = BitSizeOf<WordType>();
+  static_assert(IsPowerOfTwo(kWordBits));
+
+  static constexpr size_t BitsToWords(size_t bits) {
+    return (bits + /* round up */ (kWordBits - 1)) / kWordBits;
+  }
 
-  class IndexContainer;
+  // Construct an empty `BitVectorView`.
+  constexpr BitVectorView()
+      : storage_(nullptr), size_in_bits_(0u) {}
 
-  /**
-   * @brief Convenient iterator across the indexes of the BitVector's set bits.
-   *
-   * @details IndexIterator is a Forward iterator (C++11: 24.2.5) from the lowest
-   * to the highest index of the BitVector's set bits. Instances can be retrieved
-   * only through BitVector::Indexes() which returns an IndexContainer wrapper
-   * object with begin() and end() suitable for range-based loops:
-   *   for (uint32_t idx : bit_vector.Indexes()) {
-   *     // Use idx.
-   *   }
-   */
-  class IndexIterator {
-   public:
-    using iterator_category = std::forward_iterator_tag;
-    using value_type = uint32_t;
-    using difference_type = ptrdiff_t;
-    using pointer = void;
-    using reference = void;
-
-    bool operator==(const IndexIterator& other) const;
-
-    bool operator!=(const IndexIterator& other) const {
-      return !(*this == other);
-    }
+  // Construct a `BitVectorView` referencing the provided backing storage.
+  constexpr BitVectorView(WordType* storage, size_t size_in_bits)
+      : storage_(storage), size_in_bits_(size_in_bits) {}
 
-    uint32_t operator*() const;
+  // The `BitVectorView<>` can be copied and passed to functions by value.
+  // The new copy shall reference the same underlying data, similarly to `std::string_view`.
+  BitVectorView(const BitVectorView& src) = default;
 
-    IndexIterator& operator++();
+  // Implicit conversion to a view with constant storage.
+  template <typename ST,
+            typename = std::enable_if_t<std::is_const_v<StorageType> &&
+                                        std::is_same_v<ST, std::remove_const_t<StorageType>>>>
+  BitVectorView(const BitVectorView<ST>& src)
+      : storage_(src.storage_), size_in_bits_(src.size_in_bits_) {}
 
-    IndexIterator operator++(int);
+  // Get the size of the bit vector view in bits.
+  constexpr size_t SizeInBits() const {
+    return size_in_bits_;
+  }
 
-    // Helper function to check for end without comparing with bit_vector.Indexes().end().
-    bool Done() const {
-      return bit_index_ == BitSize();
-    }
+  // Get the size of the bit vector view in storage words.
+  constexpr size_t SizeInWords() const {
+    return BitsToWords(SizeInBits());
+  }
+
+  // Mark the specified bit as "set".
+  void SetBit(size_t index) {
+    DCHECK_LT(index, size_in_bits_);
+    storage_[WordIndex(index)] |= BitMask(index);
+  }
 
-   private:
-    struct begin_tag { };
-    struct end_tag { };
+  // Mark the specified bit as "clear".
+  void ClearBit(size_t index) {
+    DCHECK_LT(index, size_in_bits_);
+    storage_[WordIndex(index)] &= ~BitMask(index);
+  }
 
-    IndexIterator(const BitVector* bit_vector, begin_tag);
-    IndexIterator(const BitVector* bit_vector, end_tag);
+  // Determine whether or not the specified bit is set.
+  constexpr bool IsBitSet(size_t index) const {
+    DCHECK_LT(index, size_in_bits_);
+    return (storage_[WordIndex(index)] & BitMask(index)) != 0u;
+  }
 
-    uint32_t BitSize() const {
-      return storage_size_ * kWordBits;
-    }
+  // Mark all bits as "clear".
+  void ClearAllBits();
 
-    uint32_t FindIndex(uint32_t start_index) const;
-    const uint32_t* const bit_storage_;
-    const uint32_t storage_size_;  // Size of vector in words.
-    uint32_t bit_index_;           // Current index (size in bits).
+  // Mark specified number of initial bits as "set" and clear all bits after that.
+  void SetInitialBits(uint32_t num_bits);
 
-    friend class BitVector::IndexContainer;
-  };
+  // Return true if there are any bits set, false otherwise.
+  bool IsAnyBitSet() const {
+    DCheckTrailingBitsClear();
+    return std::any_of(storage_, storage_ + SizeInWords(), [](WordType w) { return w != 0u; });
+  }
 
-  /**
-   * @brief BitVector wrapper class for iteration across indexes of set bits.
-   */
-  class IndexContainer {
-   public:
-    explicit IndexContainer(const BitVector* bit_vector) : bit_vector_(bit_vector) { }
+  // Union with another bit vector view of the same size.
+  bool Union(BitVectorView<const StorageType> union_with);
 
-    IndexIterator begin() const;
-    IndexIterator end() const;
+  // Union with the bits in `union_with` but not in `not_in`. All views must have the same size.
+  bool UnionIfNotIn(BitVectorView<const StorageType> union_with,
+                    BitVectorView<const StorageType> not_in);
+
+  // `BitVectorView` wrapper class for iteration across indexes of set bits.
+  class IndexContainerImpl;
+  using IndexContainer = BitVectorView<const StorageType>::IndexContainerImpl;
+
+  IndexContainer Indexes() const;
+
+ private:
+  static constexpr size_t WordIndex(size_t index) {
+    return index >> WhichPowerOf2(kWordBits);
+  }
+
+  static constexpr WordType BitMask(size_t index) {
+    return static_cast<WordType>(1) << (index % kWordBits);
+  }
+
+  constexpr void DCheckTrailingBitsClear() const {
+    DCHECK_IMPLIES(SizeInBits() % kWordBits != 0u,
+                   (storage_[WordIndex(SizeInBits())] & ~(BitMask(SizeInBits()) - 1u)) == 0u);
+  }
 
-   private:
-    const BitVector* const bit_vector_;
-  };
+  WordType* storage_;
+  size_t size_in_bits_;
+
+  template <typename ST> friend class BitVectorIndexIterator;
+  template <typename ST> friend class BitVectorView;
+};
+
+/**
+ * @brief Convenient iterator across the indexes of the bits in `BitVector` or `BitVectorView<>`.
+ *
+ * @details BitVectorIndexIterator is a Forward iterator (C++11: 24.2.5) from the lowest
+ * to the highest index of the BitVector's set bits. Instances can be retrieved
+ * only through `BitVector{,View}::Indexes()` which return an index container wrapper
+ * object with begin() and end() suitable for range-based loops:
+ *   for (uint32_t idx : bit_vector.Indexes()) {
+ *     // Use idx.
+ *   }
+ */
+template <typename StorageType>
+class BitVectorIndexIterator {
+  static_assert(std::is_const_v<StorageType>);
+
+ public:
+  using iterator_category = std::forward_iterator_tag;
+  using value_type = size_t;
+  using difference_type = ptrdiff_t;
+  using pointer = void;
+  using reference = void;
+
+  bool operator==(const BitVectorIndexIterator& other) const;
+  bool operator!=(const BitVectorIndexIterator& other) const;
+
+  size_t operator*() const;
+
+  BitVectorIndexIterator& operator++();
+  BitVectorIndexIterator operator++(int);
+
+  // Helper function to check for end without comparing with bit_vector.Indexes().end().
+  bool Done() const {
+    return bit_index_ == bit_vector_view_.SizeInBits();
+  }
+
+ private:
+  struct begin_tag { };
+  struct end_tag { };
+
+  BitVectorIndexIterator(BitVectorView<StorageType> bit_vector_view, begin_tag);
+  BitVectorIndexIterator(BitVectorView<StorageType> bit_vector_view, end_tag);
+
+  size_t FindIndex(size_t start_index) const;
+
+  static constexpr size_t kWordBits = BitVectorView<StorageType>::kWordBits;
+
+  const BitVectorView<StorageType> bit_vector_view_;
+  size_t bit_index_;  // Current index (size in bits).
+
+  template <typename ST>
+  friend class BitVectorView;
+};
+
+/*
+ * Expanding bitmap. Bits are numbered starting from zero. All operations on a BitVector are
+ * unsynchronized. New BitVectors are not necessarily zeroed out. If the used allocator doesn't do
+ * clear the vector (e.g. ScopedArenaAllocator), the responsibility of clearing it relies on the
+ * caller (e.g. ArenaBitVector).
+ */
+class BitVector {
+ public:
+  static constexpr uint32_t kWordBytes = sizeof(uint32_t);
+  static constexpr uint32_t kWordBits = kWordBytes * 8;
+
+  using IndexContainer = BitVectorView<uint32_t>::IndexContainer;
 
   // MoveConstructible but not MoveAssignable, CopyConstructible or CopyAssignable.
 
@@ -154,15 +255,15 @@ class BitVector {
     if (idx >= storage_size_ * kWordBits) {
       EnsureSize(idx);
     }
-    storage_[WordIndex(idx)] |= BitMask(idx);
+    AsView().SetBit(idx);
   }
 
-  // Mark the specified bit as "unset".
+  // Mark the specified bit as "clear".
   void ClearBit(uint32_t idx) {
     // If the index is over the size, we don't have to do anything, it is cleared.
     if (idx < storage_size_ * kWordBits) {
       // Otherwise, go ahead and clear it.
-      storage_[WordIndex(idx)] &= ~BitMask(idx);
+      AsView().ClearBit(idx);
     }
   }
 
@@ -170,10 +271,10 @@ class BitVector {
   bool IsBitSet(uint32_t idx) const {
     // If the index is over the size, whether it is expandable or not, this bit does not exist:
     // thus it is not set.
-    return (idx < (storage_size_ * kWordBits)) && IsBitSet(storage_, idx);
+    return (idx < (storage_size_ * kWordBits)) && AsView().IsBitSet(idx);
   }
 
-  // Mark all bits bit as "clear".
+  // Mark all bits as "clear".
   void ClearAllBits();
 
   // Mark specified number of bits as "set". Cannot set all bits like ClearAll since there might
@@ -210,9 +311,7 @@ class BitVector {
   // Count the number of bits that are set in range [0, end).
   uint32_t NumSetBits(uint32_t end) const;
 
-  IndexContainer Indexes() const {
-    return IndexContainer(this);
-  }
+  IndexContainer Indexes() const;
 
   uint32_t GetStorageSize() const {
     return storage_size_;
@@ -251,7 +350,7 @@ class BitVector {
    * @return true if there are any bits set, false otherwise.
    */
   bool IsAnyBitSet() const {
-    return GetHighestBitSet() != -1;
+    return AsView().IsAnyBitSet();
   }
 
   // Minimum number of bits required to store this vector, 0 if none are set.
@@ -291,6 +390,14 @@ class BitVector {
    */
   void DumpHelper(const char* prefix, std::ostringstream& buffer) const;
 
+  BitVectorView<uint32_t> AsView() {
+    return {storage_, storage_size_ * kWordBits};
+  }
+
+  BitVectorView<const uint32_t> AsView() const {
+    return {storage_, storage_size_ * kWordBits};
+  }
+
   // Ensure there is space for a bit at idx.
   void EnsureSize(uint32_t idx);
 
diff --git a/libartbase/base/bit_vector_test.cc b/libartbase/base/bit_vector_test.cc
index 244cff1cb4..33a8aac873 100644
--- a/libartbase/base/bit_vector_test.cc
+++ b/libartbase/base/bit_vector_test.cc
@@ -16,6 +16,7 @@
 
 #include <memory>
 #include <random>
+#include <vector>
 
 #include "allocator.h"
 #include "base/stl_util.h"
@@ -25,6 +26,330 @@
 
 namespace art {
 
+template <typename StorageType, StorageType kWord0, StorageType kWord1>
+void TestBitVectorViewSetBitAndClearBit() {
+  static constexpr StorageType kStorage[2] = { kWord0, kWord1 };
+  static constexpr size_t kSizeInBits = 2 * BitSizeOf<StorageType>();
+  static constexpr BitVectorView<const StorageType> kBvv(kStorage, kSizeInBits);
+  auto get_bit_from_params = [](size_t index) constexpr {
+    StorageType word = (index < BitSizeOf<StorageType>()) ? kWord0 : kWord1;
+    size_t shift = index % BitSizeOf<StorageType>();
+    return (word & (static_cast<StorageType>(1u) << shift)) != 0u;
+  };
+  auto verify_is_bit_set = [get_bit_from_params]() constexpr {
+    for (size_t index = 0; index != kSizeInBits; ++index) {
+      // If the `CHECK_EQ()` fails, the `static_assert` evaluation fails at compile time.
+      CHECK_EQ(get_bit_from_params(index), kBvv.IsBitSet(index)) << index;
+    }
+    return true;
+  };
+  static_assert(verify_is_bit_set());
+
+  auto verify_size = []() constexpr {
+    for (size_t size = 0; size != kSizeInBits; ++size) {
+      // If the `CHECK_EQ()` fails, the `static_assert` evaluation fails at compile time.
+      CHECK_EQ(size, BitVectorView(kStorage, size).SizeInBits());
+      size_t words = RoundUp(size, BitSizeOf<StorageType>()) / BitSizeOf<StorageType>();
+      CHECK_EQ(words, BitVectorView(kStorage, size).SizeInWords());
+    }
+    return true;
+  };
+  static_assert(verify_size());
+
+  StorageType storage[2] = {0u, 0u};
+  size_t size_in_bits = 2 * BitSizeOf<StorageType>();
+  BitVectorView<StorageType> bvv(storage, size_in_bits);
+  for (size_t index = 0; index != size_in_bits; ++index) {
+    ASSERT_FALSE(bvv.IsBitSet(index));
+  }
+  // Set one bit at a time, then clear it.
+  for (size_t bit_to_set = 0; bit_to_set != size_in_bits; ++bit_to_set) {
+    bvv.SetBit(bit_to_set);
+    for (size_t index = 0; index != size_in_bits; ++index) {
+      ASSERT_EQ(index == bit_to_set, bvv.IsBitSet(index));
+    }
+    ASSERT_TRUE(bvv.IsAnyBitSet());
+    bvv.ClearBit(bit_to_set);
+    for (size_t index = 0; index != size_in_bits; ++index) {
+      ASSERT_FALSE(bvv.IsBitSet(index));
+    }
+    ASSERT_FALSE(bvv.IsAnyBitSet());
+  }
+  // Set bits for `kWord0` and `kWord1`.
+  for (size_t index = 0; index != size_in_bits; ++index) {
+    if (get_bit_from_params(index)) {
+      bvv.SetBit(index);
+    }
+  }
+  ASSERT_EQ(kWord0, storage[0]);
+  ASSERT_EQ(kWord1, storage[1]);
+  // Clear all bits that are already clear.
+  for (size_t index = 0; index != size_in_bits; ++index) {
+    if (!get_bit_from_params(index)) {
+      bvv.ClearBit(index);
+    }
+  }
+  ASSERT_EQ(kWord0, storage[0]);
+  ASSERT_EQ(kWord1, storage[1]);
+  // Clear all bits that are set.
+  for (size_t index = 0; index != size_in_bits; ++index) {
+    if (get_bit_from_params(index)) {
+      bvv.ClearBit(index);
+    }
+  }
+  ASSERT_EQ(0u, storage[0]);
+  ASSERT_EQ(0u, storage[1]);
+}
+
+TEST(BitVectorView, Uint32T) {
+  TestBitVectorViewSetBitAndClearBit<uint32_t, 0x12345678u, 0x87654321u>();
+}
+
+TEST(BitVectorView, Uint64T) {
+  TestBitVectorViewSetBitAndClearBit<uint64_t,
+                                     UINT64_C(0x1234567890abcdef),
+                                     UINT64_C(0xfedcba0987654321)>();
+}
+
+TEST(BitVectorView, SizeT) {
+  // Note: The constants below are truncated on 32-bit architectures.
+  TestBitVectorViewSetBitAndClearBit<size_t,
+                                     static_cast<size_t>(UINT64_C(0xfedcba0987654321)),
+                                     static_cast<size_t>(UINT64_C(0x1234567890abcdef))>();
+}
+
+TEST(BitVectorView, ConversionToConstStorage) {
+  uint32_t storage[] = {1u, 2u, 3u};
+  size_t size = 2 * BitSizeOf<uint32_t>() + MinimumBitsToStore(storage[2]);
+  BitVectorView<uint32_t> bvv(storage, size);
+  auto is_bit_set = [](BitVectorView<const uint32_t> cbvv, size_t index) {
+    return cbvv.IsBitSet(index);
+  };
+  for (size_t index = 0; index != size; ++index) {
+    ASSERT_EQ(bvv.IsBitSet(index), is_bit_set(bvv, index));
+  }
+}
+
+TEST(BitVectorView, DefaultConstructor) {
+  BitVectorView<> bvv;
+  ASSERT_EQ(0u, bvv.SizeInBits());
+  ASSERT_EQ(0u, bvv.SizeInWords());
+}
+
+TEST(BitVectorView, ClearAllBits) {
+  uint32_t storage[] = {1u, 2u, 0xffffffffu};
+  size_t size = 2 * BitSizeOf<uint32_t>() + 1u;
+  BitVectorView<uint32_t> bvv(storage, size);  // Construction allowed with bogus trailing bits.
+  ASSERT_EQ(1u, storage[0]);
+  ASSERT_EQ(2u, storage[1]);
+  ASSERT_EQ(0xffffffffu, storage[2]);
+  bvv.ClearAllBits();
+  ASSERT_EQ(0u, storage[0]);
+  ASSERT_EQ(0u, storage[1]);
+  ASSERT_EQ(0u, storage[2]);
+}
+
+TEST(BitVectorView, SetInitialBits) {
+  uint32_t storage[] = {1u, 2u, 0xffffffffu};
+  size_t size = 2 * BitSizeOf<uint32_t>() + 1u;
+  BitVectorView<uint32_t> bvv(storage, size);  // Construction allowed with bogus trailing bits.
+  ASSERT_EQ(1u, storage[0]);
+  ASSERT_EQ(2u, storage[1]);
+  ASSERT_EQ(0xffffffffu, storage[2]);
+  bvv.SetInitialBits(40u);
+  ASSERT_EQ(0xffffffffu, storage[0]);
+  ASSERT_EQ(0xffu, storage[1]);
+  ASSERT_EQ(0u, storage[2]);
+  bvv.SetInitialBits(0u);
+  ASSERT_EQ(0u, storage[0]);
+  ASSERT_EQ(0u, storage[1]);
+  ASSERT_EQ(0u, storage[2]);
+  bvv.SetInitialBits(17u);
+  ASSERT_EQ(0x1ffffu, storage[0]);
+  ASSERT_EQ(0u, storage[1]);
+  ASSERT_EQ(0u, storage[2]);
+  bvv.SetInitialBits(64u);
+  ASSERT_EQ(0xffffffffu, storage[0]);
+  ASSERT_EQ(0xffffffffu, storage[1]);
+  ASSERT_EQ(0u, storage[2]);
+  bvv.SetInitialBits(65u);
+  ASSERT_EQ(0xffffffffu, storage[0]);
+  ASSERT_EQ(0xffffffffu, storage[1]);
+  ASSERT_EQ(1u, storage[2]);
+}
+
+template <typename StorageType, StorageType kWord0, StorageType kWord1>
+void TestBitVectorViewIndexes() {
+  StorageType storage[] = {kWord0, kWord1};
+  size_t size = 2u * BitSizeOf<StorageType>();
+  BitVectorView bvv(storage, size);
+
+  std::vector<size_t> indexes1;
+  for (size_t index = 0; index != size; ++index) {
+    if (bvv.IsBitSet(index)) {
+      indexes1.push_back(index);
+    }
+  }
+
+  std::vector<size_t> indexes2;
+  for (size_t index : bvv.Indexes()) {
+    indexes2.push_back(index);
+  }
+  ASSERT_EQ(indexes1, indexes2);
+
+  std::vector<size_t> indexes3;
+  for (auto it = bvv.Indexes().begin(); !it.Done(); ++it) {
+    indexes3.push_back(*it);
+  }
+  ASSERT_EQ(indexes1, indexes3);
+
+  StorageType empty_storage[] = {0u, 0u, 0u};
+  BitVectorView empty(empty_storage, 3 * BitSizeOf<StorageType>() - 1u);
+  for (size_t index : empty.Indexes()) {
+    FAIL();
+  }
+  ASSERT_TRUE(empty.Indexes().begin().Done());
+}
+
+TEST(BitVectorView, IndexesUint32T) {
+  TestBitVectorViewIndexes<uint32_t, 0x12345678u, 0x87654321u>();
+}
+
+TEST(BitVectorView, IndexesUint64T) {
+  TestBitVectorViewIndexes<uint64_t,
+                           UINT64_C(0x1234567890abcdef),
+                           UINT64_C(0xfedcba0987654321)>();
+}
+
+TEST(BitVectorView, IndexesSizeT) {
+  // Note: The constants below are truncated on 32-bit architectures.
+  TestBitVectorViewIndexes<size_t,
+                           static_cast<size_t>(UINT64_C(0xfedcba0987654321)),
+                           static_cast<size_t>(UINT64_C(0x1234567890abcdef))>();
+}
+
+template <typename StorageType>
+void TestBitVectorViewUnion() {
+  // Truncated if the constants do not fit in `StorageType`.
+  static constexpr StorageType kInitWord0 = static_cast<StorageType>(UINT64_C(0xfedcba0987654321));
+  static constexpr StorageType kInitWord1 = static_cast<StorageType>(UINT64_C(0x1234567890abcdef));
+  StorageType storage[] = { kInitWord0, kInitWord1 };
+  size_t size = 2u * BitSizeOf<StorageType>();
+  BitVectorView<StorageType> bvv(storage, size);
+
+  StorageType equal_storage[] = { kInitWord0, kInitWord1 };
+  BitVectorView<StorageType> equal_bvv(equal_storage, size);
+  ASSERT_FALSE(bvv.Union(equal_bvv));
+  ASSERT_EQ(kInitWord0, storage[0]);
+  ASSERT_EQ(kInitWord1, storage[1]);
+
+  StorageType mask = static_cast<StorageType>(UINT64_C(0x5555555555555555));
+  StorageType subset_storage[] = { kInitWord0 & mask, kInitWord1 & mask };
+  BitVectorView<StorageType> subset_bvv(subset_storage, size);
+  ASSERT_FALSE(bvv.Union(subset_bvv));
+  ASSERT_EQ(kInitWord0, storage[0]);
+  ASSERT_EQ(kInitWord1, storage[1]);
+
+  static constexpr StorageType kOtherWord0 = kInitWord1;
+  static constexpr StorageType kOtherWord1 = kInitWord0;
+  StorageType other_storage[] = { kOtherWord0, kOtherWord1 };
+  BitVectorView<StorageType> other_bvv(other_storage, size);
+  ASSERT_TRUE(bvv.Union(other_bvv));
+  ASSERT_EQ(kInitWord0 | kOtherWord0, storage[0]);
+  ASSERT_EQ(kInitWord1 | kOtherWord1, storage[1]);
+}
+
+TEST(BitVectorView, UnionUint32T) {
+  TestBitVectorViewUnion<uint32_t>();
+}
+
+TEST(BitVectorView, UnionUint64T) {
+  TestBitVectorViewUnion<uint64_t>();
+}
+
+TEST(BitVectorView, UnionSizeT) {
+  // Note: The constants below are truncated on 32-bit architectures.
+  TestBitVectorViewUnion<size_t>();
+}
+
+template <typename StorageType>
+void TestBitVectorViewUnionIfNotIn() {
+  // Truncated if the constants do not fit in `StorageType`.
+  static constexpr StorageType kInitWord0 = static_cast<StorageType>(UINT64_C(0xfedcba0987654321));
+  static constexpr StorageType kInitWord1 = static_cast<StorageType>(UINT64_C(0x1234567890abcdef));
+  StorageType storage[] = { kInitWord0, kInitWord1 };
+  size_t size = 2u * BitSizeOf<StorageType>();
+  BitVectorView<StorageType> bvv(storage, size);
+  StorageType equal_storage[] = { kInitWord0, kInitWord1 };
+  BitVectorView<StorageType> equal_bvv(equal_storage, size);
+  StorageType mask = static_cast<StorageType>(UINT64_C(0x5555555555555555));
+  StorageType subset_storage[] = { kInitWord0 & mask, kInitWord1 & mask };
+  BitVectorView<StorageType> subset_bvv(subset_storage, size);
+  StorageType empty_storage[] = { 0u, 0u };
+  BitVectorView<StorageType> empty_bvv(subset_storage, size);
+  static constexpr StorageType kOtherWord0 = kInitWord1;
+  static constexpr StorageType kOtherWord1 = kInitWord0;
+  StorageType other_storage[] = { kOtherWord0, kOtherWord1 };
+  BitVectorView<StorageType> other_bvv(other_storage, size);
+  StorageType mask_storage[] = { mask, mask };
+  BitVectorView<StorageType> mask_bvv(mask_storage, size);
+
+  // Test cases where we add bits and the `not_in` is relevant.
+  ASSERT_TRUE(bvv.UnionIfNotIn(other_bvv, mask_bvv));
+  ASSERT_EQ(kInitWord0 | (kOtherWord0 & ~mask), storage[0]);
+  ASSERT_EQ(kInitWord1 | (kOtherWord1 & ~mask), storage[1]);
+  storage[0] = kInitWord0;  // Reset `bvv` storage.
+  storage[1] = kInitWord1;
+  ASSERT_TRUE(bvv.UnionIfNotIn(mask_bvv, other_bvv));
+  ASSERT_EQ(kInitWord0 | (mask & ~kOtherWord0), storage[0]);
+  ASSERT_EQ(kInitWord1 | (mask & ~kOtherWord1), storage[1]);
+  storage[0] = kInitWord0;  // Reset `bvv` storage.
+  storage[1] = kInitWord1;
+
+  // Test cases where we add bits but the `not_in` is irrelevant because it's a subset of `bvv`.
+  for (BitVectorView<StorageType> not_in : { equal_bvv, subset_bvv, empty_bvv }) {
+    ASSERT_TRUE(bvv.UnionIfNotIn(other_bvv, not_in));
+    ASSERT_EQ(kInitWord0 | kOtherWord0, storage[0]);
+    ASSERT_EQ(kInitWord1 | kOtherWord1, storage[1]);
+    storage[0] = kInitWord0;  // Reset `bvv` storage.
+    storage[1] = kInitWord1;
+    ASSERT_TRUE(bvv.UnionIfNotIn(mask_bvv, not_in));
+    ASSERT_EQ(kInitWord0 | mask, storage[0]);
+    ASSERT_EQ(kInitWord1 | mask, storage[1]);
+    storage[0] = kInitWord0;  // Reset `bvv` storage.
+    storage[1] = kInitWord1;
+  }
+
+  // Test various cases where we add no bits.
+  for (BitVectorView<StorageType> union_with : { equal_bvv, subset_bvv, empty_bvv }) {
+    for (BitVectorView<StorageType> not_in :
+             { equal_bvv, subset_bvv, empty_bvv, other_bvv, mask_bvv }) {
+      ASSERT_FALSE(bvv.UnionIfNotIn(union_with, not_in));
+      ASSERT_EQ(kInitWord0, storage[0]);
+      ASSERT_EQ(kInitWord1, storage[1]);
+    }
+  }
+  ASSERT_FALSE(bvv.UnionIfNotIn(other_bvv, other_bvv));
+  ASSERT_EQ(kInitWord0, storage[0]);
+  ASSERT_EQ(kInitWord1, storage[1]);
+  ASSERT_FALSE(bvv.UnionIfNotIn(mask_bvv, mask_bvv));
+  ASSERT_EQ(kInitWord0, storage[0]);
+  ASSERT_EQ(kInitWord1, storage[1]);
+}
+
+TEST(BitVectorView, UnionIfNotInUint32T) {
+  TestBitVectorViewUnionIfNotIn<uint32_t>();
+}
+
+TEST(BitVectorView, UnionIfNotInUint64T) {
+  TestBitVectorViewUnionIfNotIn<uint64_t>();
+}
+
+TEST(BitVectorView, UnionIfNotInSizeT) {
+  // Note: The constants below are truncated on 32-bit architectures.
+  TestBitVectorViewUnionIfNotIn<size_t>();
+}
+
 TEST(BitVector, Test) {
   const size_t kBits = 32;
 
@@ -58,7 +383,7 @@ TEST(BitVector, Test) {
   EXPECT_EQ(0x80000001U, bv.GetRawStorageWord(0));
   EXPECT_EQ(0x80000001U, *bv.GetRawStorage());
 
-  BitVector::IndexIterator iterator = bv.Indexes().begin();
+  BitVectorIndexIterator<const uint32_t> iterator = bv.Indexes().begin();
   EXPECT_TRUE(iterator != bv.Indexes().end());
   EXPECT_EQ(0u, *iterator);
   ++iterator;
diff --git a/libartbase/base/common_art_test.cc b/libartbase/base/common_art_test.cc
index 29cc88a8f5..41ed98d651 100644
--- a/libartbase/base/common_art_test.cc
+++ b/libartbase/base/common_art_test.cc
@@ -34,7 +34,6 @@
 #include "android-base/process.h"
 #include "android-base/scopeguard.h"
 #include "android-base/stringprintf.h"
-#include "android-base/strings.h"
 #include "android-base/unique_fd.h"
 #include "art_field-inl.h"
 #include "base/file_utils.h"
@@ -163,107 +162,6 @@ android::base::ScopeGuard<std::function<void()>> ScopedInaccessible(const std::s
   return android::base::make_scope_guard([=]() { std::filesystem::permissions(path, old_perms); });
 }
 
-std::string CommonArtTestImpl::GetAndroidBuildTop() {
-  CHECK(IsHost());
-  std::string android_build_top;
-
-  // Look at how we were invoked to find the expected directory.
-  std::string argv;
-  if (android::base::ReadFileToString("/proc/self/cmdline", &argv)) {
-    // /proc/self/cmdline is the programs 'argv' with elements delimited by '\0'.
-    std::filesystem::path path(argv.substr(0, argv.find('\0')));
-    path = std::filesystem::absolute(path);
-    // Walk up until we find the one of the well-known directories.
-    for (; path.parent_path() != path; path = path.parent_path()) {
-      // We are running tests from out/host/linux-x86 on developer machine.
-      if (path.filename() == std::filesystem::path("linux-x86")) {
-        android_build_top = path.parent_path().parent_path().parent_path();
-        break;
-      }
-      // We are running tests from testcases (extracted from zip) on tradefed.
-      // The first path is for remote runs and the second path for local runs.
-      if (path.filename() == std::filesystem::path("testcases") ||
-          path.filename().string().starts_with("host_testcases")) {
-        android_build_top = path.append("art_common");
-        break;
-      }
-    }
-  }
-  CHECK(!android_build_top.empty());
-
-  // Check that the expected directory matches the environment variable.
-  const char* android_build_top_from_env = getenv("ANDROID_BUILD_TOP");
-  android_build_top = std::filesystem::path(android_build_top).string();
-  CHECK(!android_build_top.empty());
-  if (android_build_top_from_env != nullptr) {
-    if (std::filesystem::weakly_canonical(android_build_top).string() !=
-        std::filesystem::weakly_canonical(android_build_top_from_env).string()) {
-      android_build_top = android_build_top_from_env;
-    }
-  } else {
-    setenv("ANDROID_BUILD_TOP", android_build_top.c_str(), /*overwrite=*/0);
-  }
-  if (android_build_top.back() != '/') {
-    android_build_top += '/';
-  }
-  return android_build_top;
-}
-
-std::string CommonArtTestImpl::GetAndroidHostOut() {
-  CHECK(IsHost());
-
-  // Check that the expected directory matches the environment variable.
-  // ANDROID_HOST_OUT is set by envsetup or unset and is the full path to host binaries/libs
-  const char* android_host_out_from_env = getenv("ANDROID_HOST_OUT");
-  // OUT_DIR is a user-settable ENV_VAR that controls where soong puts build artifacts. It can
-  // either be relative to ANDROID_BUILD_TOP or a concrete path.
-  const char* android_out_dir = getenv("OUT_DIR");
-  // Take account of OUT_DIR setting.
-  if (android_out_dir == nullptr) {
-    android_out_dir = "out";
-  }
-  std::string android_host_out;
-  if (android_out_dir[0] == '/') {
-    android_host_out = (std::filesystem::path(android_out_dir) / "host" / "linux-x86").string();
-  } else {
-    android_host_out =
-        (std::filesystem::path(GetAndroidBuildTop()) / android_out_dir / "host" / "linux-x86")
-            .string();
-  }
-  std::filesystem::path expected(android_host_out);
-  if (android_host_out_from_env != nullptr) {
-    std::filesystem::path from_env(std::filesystem::weakly_canonical(android_host_out_from_env));
-    if (std::filesystem::weakly_canonical(expected).string() != from_env.string()) {
-      LOG(WARNING) << "Execution path (" << expected << ") not below ANDROID_HOST_OUT ("
-                   << from_env << ")! Using env-var.";
-      expected = from_env;
-    }
-  } else {
-    setenv("ANDROID_HOST_OUT", android_host_out.c_str(), /*overwrite=*/0);
-  }
-  return expected.string();
-}
-
-std::string CommonArtTestImpl::GetHostBootClasspathInstallRoot() {
-  CHECK(IsHost());
-  std::string build_install_root = GetAndroidHostOut() + "/testcases/art_common/out/host/linux-x86";
-  // Look for the `apex` subdirectory as a discriminator to check the location.
-  if (OS::DirectoryExists((build_install_root + "/apex").c_str())) {
-    // This is the path where "m art-host-tests" installs support files for host
-    // tests, so use it when the tests are run in a build tree (which is the
-    // case when testing locally).
-    return build_install_root;
-  }
-  if (OS::DirectoryExists((GetAndroidRoot() + "/apex").c_str())) {
-    // This is the location for host tests in CI when the files are unzipped
-    // from art-host-tests.zip.
-    return GetAndroidRoot();
-  }
-  LOG(ERROR) << "Neither location has a boot classpath (forgot \"m art-host-tests\"?): "
-             << build_install_root << " or " << GetAndroidRoot();
-  return "<no boot classpath found>";
-}
-
 void CommonArtTestImpl::SetUpAndroidRootEnvVars() {
   if (IsHost()) {
     std::string android_host_out = GetAndroidHostOut();
@@ -417,9 +315,9 @@ std::unique_ptr<const DexFile> CommonArtTestImpl::LoadExpectSingleDexFile(const
 }
 
 void CommonArtTestImpl::ClearDirectory(const char* dirpath, bool recursive) {
-  ASSERT_TRUE(dirpath != nullptr);
+  CHECK(dirpath != nullptr) << std::string(dirpath);
   DIR* dir = opendir(dirpath);
-  ASSERT_TRUE(dir != nullptr);
+  CHECK(dir != nullptr) << std::string(dirpath);
   dirent* e;
   struct stat s;
   while ((e = readdir(dir)) != nullptr) {
@@ -463,41 +361,6 @@ std::vector<std::string> CommonArtTestImpl::GetLibCoreModuleNames() const {
   return art::testing::GetLibCoreModuleNames();
 }
 
-std::vector<std::string> CommonArtTestImpl::GetLibCoreDexFileNames(
-    const std::vector<std::string>& modules) const {
-  return art::testing::GetLibCoreDexFileNames(
-      kIsTargetBuild ? "" : GetHostBootClasspathInstallRoot(), modules);
-}
-
-std::vector<std::string> CommonArtTestImpl::GetLibCoreDexFileNames() const {
-  std::vector<std::string> modules = GetLibCoreModuleNames();
-  return art::testing::GetLibCoreDexFileNames(
-      kIsTargetBuild ? "" : GetHostBootClasspathInstallRoot(), modules);
-}
-
-std::vector<std::string> CommonArtTestImpl::GetLibCoreDexLocations(
-    const std::vector<std::string>& modules) const {
-  std::string prefix = "";
-  if (IsHost()) {
-    std::string android_root = GetAndroidRoot();
-    std::string build_top = GetAndroidBuildTop();
-    CHECK(android_root.starts_with(build_top))
-        << " android_root=" << android_root << " build_top=" << build_top;
-    prefix = android_root.substr(build_top.size());
-  }
-  return art::testing::GetLibCoreDexFileNames(prefix, modules);
-}
-
-std::vector<std::string> CommonArtTestImpl::GetLibCoreDexLocations() const {
-  std::vector<std::string> modules = GetLibCoreModuleNames();
-  return GetLibCoreDexLocations(modules);
-}
-
-std::string CommonArtTestImpl::GetClassPathOption(const char* option,
-                                                  const std::vector<std::string>& class_path) {
-  return option + android::base::Join(class_path, ':');
-}
-
 // Check that for target builds we have ART_TARGET_NATIVETEST_DIR set.
 #ifdef ART_TARGET
 #ifndef ART_TARGET_NATIVETEST_DIR
diff --git a/libartbase/base/common_art_test.h b/libartbase/base/common_art_test.h
index 1a0b910ced..b27199c386 100644
--- a/libartbase/base/common_art_test.h
+++ b/libartbase/base/common_art_test.h
@@ -32,6 +32,7 @@
 #include "base/memory_tool.h"
 #include "base/mutex.h"
 #include "base/os.h"
+#include "base/testing.h"
 #include "base/unix_file/fd_file.h"
 #include "dex/art_dex_file_loader.h"
 #include "dex/compact_dex_file.h"
@@ -152,19 +153,29 @@ class CommonArtTestImpl {
   virtual std::vector<std::string> GetLibCoreModuleNames() const;
 
   // Gets the paths of the libcore dex files for given modules.
-  std::vector<std::string> GetLibCoreDexFileNames(const std::vector<std::string>& modules) const;
+  std::vector<std::string> GetLibCoreDexFileNames(const std::vector<std::string>& modules) const {
+    return art::testing::GetLibCoreDexFileNames(modules);
+  }
 
   // Gets the paths of the libcore dex files.
-  std::vector<std::string> GetLibCoreDexFileNames() const;
+  std::vector<std::string> GetLibCoreDexFileNames() const {
+    return GetLibCoreDexFileNames(GetLibCoreModuleNames());
+  }
 
   // Gets the on-host or on-device locations of the libcore dex files for given modules.
-  std::vector<std::string> GetLibCoreDexLocations(const std::vector<std::string>& modules) const;
+  std::vector<std::string> GetLibCoreDexLocations(const std::vector<std::string>& modules) const {
+    return art::testing::GetLibCoreDexLocations(modules);
+  }
 
   // Gets the on-host or on-device locations of the libcore dex files.
-  std::vector<std::string> GetLibCoreDexLocations() const;
+  std::vector<std::string> GetLibCoreDexLocations() const {
+    return GetLibCoreDexLocations(GetLibCoreModuleNames());
+  }
 
   static std::string GetClassPathOption(const char* option,
-                                        const std::vector<std::string>& class_path);
+                                        const std::vector<std::string>& class_path) {
+    return art::testing::GetClassPathOption(option, class_path);
+  }
 
   // Retuerns the filename for a test dex (i.e. XandY or ManyMethods).
   std::string GetTestDexFileName(const char* name) const;
@@ -227,21 +238,15 @@ class CommonArtTestImpl {
   static std::string GetAndroidTool(const char* name, InstructionSet isa = InstructionSet::kX86_64);
 
  protected:
-  static bool IsHost() {
-    return !kIsTargetBuild;
-  }
+  static bool IsHost() { return art::testing::IsHost(); }
 
-  // Returns ${ANDROID_BUILD_TOP}. Ensure it has tailing /.
-  static std::string GetAndroidBuildTop();
+  static std::string GetAndroidBuildTop() { return art::testing::GetAndroidBuildTop(); }
 
-  // Returns ${ANDROID_HOST_OUT}.
-  static std::string GetAndroidHostOut();
+  static std::string GetAndroidHostOut() { return art::testing::GetAndroidHostOut(); }
 
-  // Returns the path where boot classpath and boot image files are installed
-  // for host tests (by the art_common mk module, typically built through "m
-  // art-host-tests"). Different in CI where they are unpacked from the
-  // art-host-tests.zip file.
-  static std::string GetHostBootClasspathInstallRoot();
+  static std::string GetHostBootClasspathInstallRoot() {
+    return art::testing::GetHostBootClasspathInstallRoot();
+  }
 
   // File location to boot.art, e.g. /apex/com.android.art/javalib/boot.art
   static std::string GetCoreArtLocation();
@@ -300,54 +305,54 @@ class CommonArtTestBase : public TestType, public CommonArtTestImpl {
   }
 };
 
-using CommonArtTest = CommonArtTestBase<testing::Test>;
+using CommonArtTest = CommonArtTestBase<::testing::Test>;
 
 template <typename Param>
-using CommonArtTestWithParam = CommonArtTestBase<testing::TestWithParam<Param>>;
+using CommonArtTestWithParam = CommonArtTestBase<::testing::TestWithParam<Param>>;
 
 // Returns a list of PIDs of the processes whose process name (the first commandline argument) fully
 // matches the given name.
 std::vector<pid_t> GetPidByName(const std::string& process_name);
 
 #define TEST_DISABLED_FOR_TARGET()                       \
-  if (kIsTargetBuild) {                                  \
+  if (art::kIsTargetBuild) {                                  \
     GTEST_SKIP() << "WARNING: TEST DISABLED FOR TARGET"; \
   }
 
 #define TEST_DISABLED_FOR_HOST()                       \
-  if (!kIsTargetBuild) {                               \
+  if (!art::kIsTargetBuild) {                               \
     GTEST_SKIP() << "WARNING: TEST DISABLED FOR HOST"; \
   }
 
 #define TEST_DISABLED_FOR_NON_STATIC_HOST_BUILDS()                       \
-  if (!kHostStaticBuildEnabled) {                                        \
+  if (!art::kHostStaticBuildEnabled) {                                        \
     GTEST_SKIP() << "WARNING: TEST DISABLED FOR NON-STATIC HOST BUILDS"; \
   }
 
 #define TEST_DISABLED_FOR_DEBUG_BUILD()                       \
-  if (kIsDebugBuild) {                                        \
+  if (art::kIsDebugBuild) {                                        \
     GTEST_SKIP() << "WARNING: TEST DISABLED FOR DEBUG BUILD"; \
   }
 
 #define TEST_DISABLED_FOR_MEMORY_TOOL()                       \
-  if (kRunningOnMemoryTool) {                                 \
+  if (art::kRunningOnMemoryTool) {                                 \
     GTEST_SKIP() << "WARNING: TEST DISABLED FOR MEMORY TOOL"; \
   }
 
 #define TEST_DISABLED_FOR_HEAP_POISONING()                       \
-  if (kPoisonHeapReferences) {                                   \
+  if (art::kPoisonHeapReferences) {                                   \
     GTEST_SKIP() << "WARNING: TEST DISABLED FOR HEAP POISONING"; \
   }
 }  // namespace art
 
 #define TEST_DISABLED_FOR_MEMORY_TOOL_WITH_HEAP_POISONING()                       \
-  if (kRunningOnMemoryTool && kPoisonHeapReferences) {                            \
+  if (art::kRunningOnMemoryTool && art::kPoisonHeapReferences) {                            \
     GTEST_SKIP() << "WARNING: TEST DISABLED FOR MEMORY TOOL WITH HEAP POISONING"; \
   }
 
 #define TEST_DISABLED_FOR_USER_BUILD()                                          \
   if (std::string build_type = android::base::GetProperty("ro.build.type", ""); \
-      kIsTargetBuild && build_type != "userdebug" && build_type != "eng") {     \
+      art::kIsTargetBuild && build_type != "userdebug" && build_type != "eng") {     \
     GTEST_SKIP() << "WARNING: TEST DISABLED FOR USER BUILD";                    \
   }
 
diff --git a/libartbase/base/debugstore.h b/libartbase/base/debugstore.h
index b3e6563c60..9129bc156f 100644
--- a/libartbase/base/debugstore.h
+++ b/libartbase/base/debugstore.h
@@ -17,21 +17,21 @@
 #ifndef ART_LIBARTBASE_BASE_DEBUGSTORE_H_
 #define ART_LIBARTBASE_BASE_DEBUGSTORE_H_
 
-#include <array>
+#include <memory>
 #include <string>
 
 #include "palette/palette.h"
 
 namespace art {
-static constexpr size_t STORE_MAX_SIZE = 1024;
+inline constexpr size_t kDebugStoreMaxSize = 4096;
 
 inline std::string DebugStoreGetString() {
-  std::array<char, STORE_MAX_SIZE> result{};
+  auto result = std::make_unique<char[]>(kDebugStoreMaxSize);
   // If PaletteDebugStoreGetString returns PALETTE_STATUS_NOT_SUPPORTED,
   // set an empty string as the result.
   result[0] = '\0';
-  PaletteDebugStoreGetString(result.data(), result.size());
-  return std::string(result.data());
+  PaletteDebugStoreGetString(result.get(), kDebugStoreMaxSize);
+  return std::string(result.get());
 }
 
 }  // namespace art
diff --git a/libartbase/base/file_utils.cc b/libartbase/base/file_utils.cc
index 0b3811e5e4..4253fa1ce7 100644
--- a/libartbase/base/file_utils.cc
+++ b/libartbase/base/file_utils.cc
@@ -88,7 +88,6 @@ static constexpr const char* kAndroidExpandEnvVar = "ANDROID_EXPAND";
 static constexpr const char* kAndroidExpandDefaultPath = "/mnt/expand";
 static constexpr const char* kAndroidArtRootEnvVar = "ANDROID_ART_ROOT";
 static constexpr const char* kAndroidConscryptRootEnvVar = "ANDROID_CONSCRYPT_ROOT";
-static constexpr const char* kAndroidI18nRootEnvVar = "ANDROID_I18N_ROOT";
 static constexpr const char* kApexDefaultPath = "/apex/";
 static constexpr const char* kArtApexDataEnvVar = "ART_APEX_DATA";
 static constexpr const char* kBootImageStem = "boot";
@@ -716,6 +715,15 @@ std::string GetDmFilename(const std::string& dex_location) {
   return ReplaceFileExtension(dex_location, kDmExtension);
 }
 
+std::string GetSdmFilename(const std::string& dex_location, InstructionSet isa) {
+  return ReplaceFileExtension(dex_location,
+                              StringPrintf("%s%s", GetInstructionSetString(isa), kSdmExtension));
+}
+
+std::string GetSdcFilename(const std::string& oat_location) {
+  return ReplaceFileExtension(oat_location, kSdcExtension);
+}
+
 // check for the file in /system, followed by /system_ext
 std::string GetSystemOdexFilenameForApex(std::string_view location, InstructionSet isa) {
   DCHECK(LocationIsOnApex(location));
@@ -855,10 +863,6 @@ bool LocationIsOnConscryptModule(std::string_view full_path) {
   return IsLocationOn(full_path, kAndroidConscryptRootEnvVar, kAndroidConscryptApexDefaultPath);
 }
 
-bool LocationIsOnI18nModule(std::string_view full_path) {
-  return IsLocationOn(full_path, kAndroidI18nRootEnvVar, kAndroidI18nApexDefaultPath);
-}
-
 bool LocationIsOnApex(std::string_view full_path) {
   return full_path.starts_with(kApexDefaultPath);
 }
diff --git a/libartbase/base/file_utils.h b/libartbase/base/file_utils.h
index 7f52d03497..89f2420aa4 100644
--- a/libartbase/base/file_utils.h
+++ b/libartbase/base/file_utils.h
@@ -39,6 +39,7 @@ static constexpr const char* kVdexExtension = ".vdex";
 static constexpr const char* kArtExtension = ".art";
 static constexpr const char* kDmExtension = ".dm";
 static constexpr const char* kSdmExtension = ".sdm";
+static constexpr const char* kSdcExtension = ".sdc";
 
 // These methods return the Android Root, which is the historical location of
 // the Android "system" directory, containing the built Android artifacts. On
@@ -174,6 +175,12 @@ std::string GetVdexFilename(const std::string& oat_filename);
 // Returns the dm filename for the given dex location.
 std::string GetDmFilename(const std::string& dex_location);
 
+// Returns the sdm filename for the given dex location.
+std::string GetSdmFilename(const std::string& dex_location, InstructionSet isa);
+
+// Returns the sdc filename for the given oat filename.
+std::string GetSdcFilename(const std::string& oat_filename);
+
 // Returns the odex location on /system for a DEX file on /apex. The caller must make sure that
 // `location` is on /apex.
 std::string GetSystemOdexFilenameForApex(std::string_view location, InstructionSet isa);
@@ -197,9 +204,6 @@ bool LocationIsOnArtApexData(std::string_view location);
 // Return whether the location is on /apex/com.android.conscrypt
 bool LocationIsOnConscryptModule(std::string_view location);
 
-// Return whether the location is on /apex/com.android.i18n
-bool LocationIsOnI18nModule(std::string_view location);
-
 // Return whether the location is on system (i.e. android root).
 bool LocationIsOnSystem(const std::string& location);
 
diff --git a/libartbase/base/globals.h b/libartbase/base/globals.h
index 7348444e1c..cab7fab63d 100644
--- a/libartbase/base/globals.h
+++ b/libartbase/base/globals.h
@@ -53,9 +53,7 @@ static constexpr size_t kMaxPageSize = kMinPageSize;
 // to be able to generate OAT (ELF) and other image files with alignment other than the host page
 // size. kElfSegmentAlignment needs to be equal to the largest page size supported. Effectively,
 // this is the value to be used in images files for aligning contents to page size.
-// However, it's temporarily set to 4096 now, to prevent dex2oat from creating sparse files.
-// TODO(b/378794327): Fix this.
-static constexpr size_t kElfSegmentAlignment = kMinPageSize;
+static constexpr size_t kElfSegmentAlignment = kMaxPageSize;
 
 // Clion, clang analyzer, etc can falsely believe that "if (kIsDebugBuild)" always
 // returns the same value. By wrapping into a call to another constexpr function, we force it
diff --git a/libartbase/base/hash_set.h b/libartbase/base/hash_set.h
index 91766a75d1..b4979d56af 100644
--- a/libartbase/base/hash_set.h
+++ b/libartbase/base/hash_set.h
@@ -191,7 +191,7 @@ class HashSet {
 
   static constexpr double kDefaultMinLoadFactor = 0.4;
   static constexpr double kDefaultMaxLoadFactor = 0.7;
-  static constexpr size_t kMinBuckets = 1000;
+  static constexpr size_t kMinBuckets = 10;
 
   // If we don't own the data, this will create a new array which owns the data.
   void clear() {
diff --git a/libartbase/base/hash_set_test.cc b/libartbase/base/hash_set_test.cc
index b7b289cf65..959ab5ebf5 100644
--- a/libartbase/base/hash_set_test.cc
+++ b/libartbase/base/hash_set_test.cc
@@ -39,7 +39,7 @@ struct IsEmptyFnString {
   }
 };
 
-class HashSetTest : public testing::Test {
+class HashSetTest : public ::testing::Test {
  public:
   HashSetTest() : seed_(97421), unique_number_(0) {
   }
diff --git a/libartbase/base/hiddenapi_domain.h b/libartbase/base/hiddenapi_domain.h
index a32909082a..222b4ff502 100644
--- a/libartbase/base/hiddenapi_domain.h
+++ b/libartbase/base/hiddenapi_domain.h
@@ -23,14 +23,14 @@ namespace hiddenapi {
 // List of domains supported by the hidden API access checks. Domain with a lower
 // ordinal is considered more "trusted", i.e. always allowed to access members of
 // domains with a greater ordinal. Access checks are performed when code tries to
-// access a method/field from a more trusted domain than itself.
+// access a method/field in a more trusted domain than itself.
 enum class Domain : char {
   kCorePlatform = 0,
   kPlatform,
   kApplication,
 };
 
-inline bool IsDomainMoreTrustedThan(Domain domainA, Domain domainB) {
+inline bool IsDomainAtLeastAsTrustedAs(Domain domainA, Domain domainB) {
   return static_cast<char>(domainA) <= static_cast<char>(domainB);
 }
 
diff --git a/libartbase/base/hiddenapi_flags.h b/libartbase/base/hiddenapi_flags.h
index 0d7938aca1..9ab8c759c3 100644
--- a/libartbase/base/hiddenapi_flags.h
+++ b/libartbase/base/hiddenapi_flags.h
@@ -17,15 +17,15 @@
 #ifndef ART_LIBARTBASE_BASE_HIDDENAPI_FLAGS_H_
 #define ART_LIBARTBASE_BASE_HIDDENAPI_FLAGS_H_
 
-#include "sdk_version.h"
+#include <android-base/logging.h>
 
 #include <vector>
 
-#include "android-base/logging.h"
 #include "base/bit_utils.h"
 #include "base/dumpable.h"
-#include "base/macros.h"
 #include "base/hiddenapi_stubs.h"
+#include "base/macros.h"
+#include "sdk_version.h"
 
 namespace art {
 namespace hiddenapi {
@@ -80,15 +80,20 @@ namespace helper {
  */
 class ApiList {
  private:
+  // The representation in dex_flags_ is a combination of a Value in the lowest
+  // kValueBitSize bits, and bit flags corresponding to DomainApi in bits above
+  // that.
+  uint32_t dex_flags_;
+
   // Number of bits reserved for Value in dex flags, and the corresponding bit mask.
   static constexpr uint32_t kValueBitSize = 4;
   static constexpr uint32_t kValueBitMask = helper::BitMask(kValueBitSize);
 
   enum class Value : uint32_t {
     // Values independent of target SDK version of app
-    kSdk =    0,
-    kUnsupported =     1,
-    kBlocked =    2,
+    kSdk = 0,
+    kUnsupported = 1,  // @UnsupportedAppUsage
+    kBlocked = 2,
 
     // Values dependent on target SDK version of app. Put these last as
     // their list will be extended in future releases.
@@ -100,21 +105,24 @@ class ApiList {
     kMaxTargetR = 6,
     kMaxTargetS = 7,
 
-    // Special values
-    kInvalid =      (static_cast<uint32_t>(-1) & kValueBitMask),
-    kMin =          kSdk,
-    kMax =          kMaxTargetS,
+    // Invalid value. Does not imply the DomainApi is invalid.
+    kInvalid = (static_cast<uint32_t>(-1) & kValueBitMask),
+
+    kMin = kSdk,
+    kMax = kMaxTargetS,
+    kFuture = kMax + 1,  // Only for testing
   };
 
-  // Additional bit flags after the first kValueBitSize bits in dex flags.
-  // These are used for domain-specific API.
+  // Additional bit flags after the first kValueBitSize bits in dex flags. These
+  // are used for domain-specific APIs. The app domain is the default when no
+  // bits are set.
   enum class DomainApi : uint32_t {
     kCorePlatformApi = kValueBitSize,
     kTestApi = kValueBitSize + 1,
 
     // Special values
-    kMin =             kCorePlatformApi,
-    kMax =             kTestApi,
+    kMin = kCorePlatformApi,
+    kMax = kTestApi,
   };
 
   // Bit mask of all domain API flags.
@@ -124,12 +132,22 @@ class ApiList {
   static_assert(kValueBitSize >= MinimumBitsToStore(helper::ToUint(Value::kMax)),
                 "Not enough bits to store all ApiList values");
 
-  // Checks that all Values are covered by kValueBitMask.
+  // Check that all Values are covered by kValueBitMask.
   static_assert(helper::MatchesBitMask(Value::kMin, kValueBitMask));
   static_assert(helper::MatchesBitMask(Value::kMax, kValueBitMask));
+  static_assert(helper::MatchesBitMask(Value::kFuture, kValueBitMask));
+  static_assert(helper::MatchesBitMask(Value::kInvalid, kValueBitMask));
+
+  // Check that there's no offset between Values and the corresponding uint32
+  // dex flags, so they can be converted between each other without any change.
+  static_assert(helper::ToUint(Value::kMin) == 0);
 
-  // Assert that Value::kInvalid is larger than the maximum Value.
-  static_assert(helper::ToUint(Value::kMax) < helper::ToUint(Value::kInvalid));
+  // Check that Value::kInvalid is larger than kFuture (which is larger than kMax).
+  static_assert(helper::ToUint(Value::kFuture) < helper::ToUint(Value::kInvalid));
+
+  // Check that no DomainApi bit flag is covered by kValueBitMask.
+  static_assert((helper::ToBit(DomainApi::kMin) & kValueBitMask) == 0);
+  static_assert((helper::ToBit(DomainApi::kMax) & kValueBitMask) == 0);
 
   // Names corresponding to Values.
   static constexpr const char* kValueNames[] = {
@@ -165,13 +183,32 @@ class ApiList {
     /* max-target-s */ SdkVersion::kS,
   };
 
-  explicit ApiList(Value val, uint32_t domain_apis = 0u)
-      : dex_flags_(helper::ToUint(val) | domain_apis) {
-    DCHECK(GetValue() == val);
-    DCHECK_EQ(GetDomainApis(), domain_apis);
+  explicit ApiList(uint32_t dex_flags) : dex_flags_(dex_flags) {
+    DCHECK_EQ(dex_flags_, (dex_flags_ & kValueBitMask) | (dex_flags_ & kDomainApiBitMask));
   }
 
-  explicit ApiList(DomainApi val) : ApiList(Value::kInvalid, helper::ToBit(val)) {}
+  static ApiList FromValue(Value val) {
+    ApiList api_list(helper::ToUint(val));
+    DCHECK(api_list.GetValue() == val);
+    DCHECK_EQ(api_list.GetDomainApis(), 0u);
+    return api_list;
+  }
+
+  // Returns an ApiList with only a DomainApi bit set - the Value is invalid. It
+  // can be Combine'd with another ApiList with a Value to produce a valid combination.
+  static ApiList FromDomainApi(DomainApi domain_api) {
+    ApiList api_list(helper::ToUint(Value::kInvalid) | helper::ToBit(domain_api));
+    DCHECK(api_list.GetValue() == Value::kInvalid);
+    DCHECK_EQ(api_list.GetDomainApis(), helper::ToBit(domain_api));
+    return api_list;
+  }
+
+  static ApiList FromValueAndDomainApis(Value val, uint32_t domain_apis) {
+    ApiList api_list(helper::ToUint(val) | domain_apis);
+    DCHECK(api_list.GetValue() == val);
+    DCHECK_EQ(api_list.GetDomainApis(), domain_apis);
+    return api_list;
+  }
 
   Value GetValue() const {
     uint32_t value = (dex_flags_ & kValueBitMask);
@@ -190,47 +227,80 @@ class ApiList {
 
   uint32_t GetDomainApis() const { return (dex_flags_ & kDomainApiBitMask); }
 
-  uint32_t dex_flags_;
-
- public:
-  ApiList() : ApiList(Value::kInvalid) {}
+  // In order to correctly handle flagged changes from Unsupported to the Sdk, where both will be
+  // set when the flag is enabled, consider Sdk to take precedence over any form of unsupported.
+  // Note, this is not necessary in the inverse direction, because API flagging does not currently
+  // support API removal. Moving from the blocklist to unsupported is also a case we don't have to
+  // consider.
+  // If this is true, the conflict resolves to Value::kSdk.
+  static bool IsConflictingFlagsAcceptable(Value x, Value y) {
+    const auto predicate_non_symmetric = [](auto l, auto r) {
+      if (l != Value::kSdk) {
+        return false;
+      }
+      switch (r) {
+        case Value::kSdk:
+        case Value::kUnsupported:
+        case Value::kMaxTargetO:
+        case Value::kMaxTargetP:
+        case Value::kMaxTargetQ:
+        case Value::kMaxTargetR:
+        case Value::kMaxTargetS:
+          return true;
+        default:
+          return false;
+      }
+    };
+    return predicate_non_symmetric(x, y) || predicate_non_symmetric(y, x);
+  }
 
-  explicit ApiList(uint32_t dex_flags) : dex_flags_(dex_flags) {
-    DCHECK_EQ(dex_flags_, (dex_flags_ & kValueBitMask) | (dex_flags_ & kDomainApiBitMask));
+  // Returns true if combining this ApiList with `other` will succeed.
+  bool CanCombineWith(const ApiList& other) const {
+    const Value val1 = GetValue();
+    const Value val2 = other.GetValue();
+    return (val1 == val2) || (val1 == Value::kInvalid) || (val2 == Value::kInvalid) ||
+           IsConflictingFlagsAcceptable(val1, val2);
   }
 
+ public:
   // Helpers for conveniently constructing ApiList instances.
-  static ApiList Sdk() { return ApiList(Value::kSdk); }
-  static ApiList Unsupported() { return ApiList(Value::kUnsupported); }
-  static ApiList Blocked() { return ApiList(Value::kBlocked); }
-  static ApiList MaxTargetO() { return ApiList(Value::kMaxTargetO); }
-  static ApiList MaxTargetP() { return ApiList(Value::kMaxTargetP); }
-  static ApiList MaxTargetQ() { return ApiList(Value::kMaxTargetQ); }
-  static ApiList MaxTargetR() { return ApiList(Value::kMaxTargetR); }
-  static ApiList MaxTargetS() { return ApiList(Value::kMaxTargetS); }
-  static ApiList CorePlatformApi() { return ApiList(DomainApi::kCorePlatformApi); }
-  static ApiList TestApi() { return ApiList(DomainApi::kTestApi); }
+  static ApiList Sdk() { return FromValue(Value::kSdk); }
+  static ApiList Unsupported() { return FromValue(Value::kUnsupported); }
+  static ApiList Blocked() { return FromValue(Value::kBlocked); }
+  static ApiList MaxTargetO() { return FromValue(Value::kMaxTargetO); }
+  static ApiList MaxTargetP() { return FromValue(Value::kMaxTargetP); }
+  static ApiList MaxTargetQ() { return FromValue(Value::kMaxTargetQ); }
+  static ApiList MaxTargetR() { return FromValue(Value::kMaxTargetR); }
+  static ApiList MaxTargetS() { return FromValue(Value::kMaxTargetS); }
+  static ApiList Invalid() { return FromValue(Value::kInvalid); }
+  static ApiList CorePlatformApi() { return FromDomainApi(DomainApi::kCorePlatformApi); }
+  static ApiList TestApi() { return FromDomainApi(DomainApi::kTestApi); }
 
   uint32_t GetDexFlags() const { return dex_flags_; }
-  uint32_t GetIntValue() const { return helper::ToUint(GetValue()) - helper::ToUint(Value::kMin); }
+  uint32_t GetIntValue() const { return helper::ToUint(GetValue()); }
+
+  static ApiList FromDexFlags(uint32_t dex_flags) { return ApiList(dex_flags); }
+
+  static ApiList FromIntValue(uint32_t int_val) {
+    return FromValue(helper::GetEnumAt<Value>(int_val));
+  }
 
   // Returns the ApiList with a flag of a given name, or an empty ApiList if not matched.
   static ApiList FromName(const std::string& str) {
     for (uint32_t i = 0; i < kValueCount; ++i) {
       if (str == kValueNames[i]) {
-        return ApiList(helper::GetEnumAt<Value>(i));
+        return FromIntValue(i);
       }
     }
     for (uint32_t i = 0; i < kDomainApiCount; ++i) {
       if (str == kDomainApiNames[i]) {
-        return ApiList(helper::GetEnumAt<DomainApi>(i));
+        return FromDomainApi(helper::GetEnumAt<DomainApi>(i));
       }
     }
     if (str == kFutureValueName) {
-      static_assert(helper::ToUint(Value::kMax) + 1 < helper::ToUint(Value::kInvalid));
-      return ApiList(helper::ToUint(Value::kMax) + 1);
+      return FromValue(Value::kFuture);
     }
-    return ApiList();
+    return Invalid();
   }
 
   // Parses a vector of flag names into a single ApiList value. If successful,
@@ -238,7 +308,7 @@ class ApiList {
   static bool FromNames(std::vector<std::string>::iterator begin,
                         std::vector<std::string>::iterator end,
                         /* out */ ApiList* out_api_list) {
-    ApiList api_list;
+    ApiList api_list = Invalid();
     for (std::vector<std::string>::iterator it = begin; it != end; it++) {
       ApiList current = FromName(*it);
       if (current.IsEmpty() || !api_list.CanCombineWith(current)) {
@@ -250,7 +320,7 @@ class ApiList {
         }
         return false;
       }
-      api_list |= current;
+      api_list = Combine(api_list, current);
     }
     if (out_api_list != nullptr) {
       *out_api_list = api_list;
@@ -260,71 +330,36 @@ class ApiList {
 
   bool operator==(const ApiList& other) const { return dex_flags_ == other.dex_flags_; }
   bool operator!=(const ApiList& other) const { return !(*this == other); }
-  bool operator<(const ApiList& other) const { return dex_flags_ < other.dex_flags_; }
-  bool operator>(const ApiList& other) const { return dex_flags_ > other.dex_flags_; }
 
-  // In order to correctly handle flagged changes from Unsupported to the Sdk, where both will be
-  // set when the flag is enabled, consider Sdk to take precedence over any form of unsupported.
-  // Note, this is not necessary in the inverse direction, because API flagging does not currently
-  // support API removal. Moving from the blocklist to unsupported is also a case we don't have to
-  // consider.
-  // If this is true, the conflict resolves to Value::kSdk.
-  static bool is_conflicting_flags_acceptable(Value x, Value y) {
-    const auto predicate_non_symmetric = [] (auto l, auto r) {
-      if (l != Value::kSdk) return false;
-      switch (r) {
-        case Value::kSdk:
-        case Value::kUnsupported:
-        case Value::kMaxTargetO:
-        case Value::kMaxTargetP:
-        case Value::kMaxTargetQ:
-        case Value::kMaxTargetR:
-        case Value::kMaxTargetS:
-          return true;
-        default:
-          return false;
-      }
-    };
-    return predicate_non_symmetric(x, y) || predicate_non_symmetric(y, x);
-  }
-
-  // Returns true if combining this ApiList with `other` will succeed.
-  bool CanCombineWith(const ApiList& other) const {
-    const Value val1 = GetValue();
-    const Value val2 = other.GetValue();
-    return (val1 == val2) || (val1 == Value::kInvalid) || (val2 == Value::kInvalid) ||
-           is_conflicting_flags_acceptable(val1, val2);
-  }
+  // The order doesn't have any significance - only for ordering in containers.
+  bool operator<(const ApiList& other) const { return dex_flags_ < other.dex_flags_; }
 
-  // Combine two ApiList instances.
-  ApiList operator|(const ApiList& other) {
+  // Combine two ApiList instances. The returned value has the union of the API
+  // domains. Values are mutually exclusive, so they either have to be identical
+  // or one of them can be safely ignored, which includes being kInvalid.
+  static ApiList Combine(const ApiList& api1, const ApiList& api2) {
     // DomainApis are not mutually exclusive. Simply OR them.
-    const uint32_t domain_apis = GetDomainApis() | other.GetDomainApis();
+    // TODO: This is suspect since the app domain doesn't have any bit and hence
+    // implicitly disappears if OR'ed with any other domain.
+    const uint32_t domain_apis = api1.GetDomainApis() | api2.GetDomainApis();
 
-    // Values are mutually exclusive. Check if `this` and `other` have the same Value
-    // or if at most one is set.
-    const Value val1 = GetValue();
-    const Value val2 = other.GetValue();
+    const Value val1 = api1.GetValue();
+    const Value val2 = api2.GetValue();
     if (val1 == val2) {
-      return ApiList(val1, domain_apis);
+      return FromValueAndDomainApis(val1, domain_apis);
     } else if (val1 == Value::kInvalid) {
-      return ApiList(val2, domain_apis);
+      return FromValueAndDomainApis(val2, domain_apis);
     } else if (val2 == Value::kInvalid) {
-      return ApiList(val1, domain_apis);
-    } else if (is_conflicting_flags_acceptable(val1, val2)) {
-      return ApiList(Value::kSdk, domain_apis);
+      return FromValueAndDomainApis(val1, domain_apis);
+    } else if (IsConflictingFlagsAcceptable(val1, val2)) {
+      return FromValueAndDomainApis(Value::kSdk, domain_apis);
     } else {
-      LOG(FATAL) << "Invalid combination of values " << Dumpable(ApiList(val1))
-          << " and " << Dumpable(ApiList(val2));
+      LOG(FATAL) << "Invalid combination of values " << Dumpable(FromValue(val1)) << " and "
+                 << Dumpable(FromValue(val2));
       UNREACHABLE();
     }
   }
 
-  const ApiList& operator|=(const ApiList& other) {
-    (*this) = (*this) | other;
-    return *this;
-  }
-
   // Returns true if all flags set in `other` are also set in `this`.
   bool Contains(const ApiList& other) const {
     return ((other.GetValue() == Value::kInvalid) || (GetValue() == other.GetValue())) &&
@@ -338,13 +373,9 @@ class ApiList {
   bool IsEmpty() const { return (GetValue() == Value::kInvalid) && (GetDomainApis() == 0); }
 
   // Returns true if the ApiList is on blocklist.
-  bool IsBlocked() const {
-    return GetValue() == Value::kBlocked;
-  }
+  bool IsBlocked() const { return GetValue() == Value::kBlocked; }
 
-  bool IsSdkApi() const {
-    return GetValue() == Value::kSdk;
-  }
+  bool IsSdkApi() const { return GetValue() == Value::kSdk; }
 
   // Returns true if the ApiList is a test API.
   bool IsTestApi() const {
diff --git a/libartbase/base/intrusive_forward_list_test.cc b/libartbase/base/intrusive_forward_list_test.cc
index f96fc9d516..095a94ceeb 100644
--- a/libartbase/base/intrusive_forward_list_test.cc
+++ b/libartbase/base/intrusive_forward_list_test.cc
@@ -67,7 +67,7 @@ bool operator<(const IFLTestValue2& lhs, const IFLTestValue2& rhs) {
     ASSERT_TRUE(std::equal((expected).begin(), (expected).end(), (value).begin())); \
   } while (false)
 
-class IntrusiveForwardListTest : public testing::Test {
+class IntrusiveForwardListTest : public ::testing::Test {
  public:
   template <typename ListType>
   void IteratorToConstIterator();
diff --git a/libartbase/base/logging_test.cc b/libartbase/base/logging_test.cc
index 1fa3209f7f..985a8eafae 100644
--- a/libartbase/base/logging_test.cc
+++ b/libartbase/base/logging_test.cc
@@ -32,7 +32,7 @@ static void SimpleAborter(const char* msg) {
   _exit(1);
 }
 
-class LoggingTest : public testing::Test {
+class LoggingTest : public ::testing::Test {
  protected:
   LoggingTest() {
     // In our abort tests we really don't want the runtime to create a real dump.
diff --git a/libartbase/base/mem_map.cc b/libartbase/base/mem_map.cc
index 5c785618ba..bdc3c3dc27 100644
--- a/libartbase/base/mem_map.cc
+++ b/libartbase/base/mem_map.cc
@@ -30,10 +30,9 @@
 #include <memory>
 #include <sstream>
 
+#include "allocator.h"
 #include "android-base/stringprintf.h"
 #include "android-base/unique_fd.h"
-
-#include "allocator.h"
 #include "bit_utils.h"
 #include "globals.h"
 #include "logging.h"  // For VLOG_IS_ON.
@@ -346,15 +345,18 @@ MemMap MemMap::MapAnonymous(const char* name,
 
   void* actual = nullptr;
 
-  // New Ubuntu linux kerners seem to ignore the address hint, so make it a firm request.
-  // Whereas old kernels allocated at 'addr' if provided, newer kernels seem to ignore it.
-  // However, MAP_FIXED_NOREPLACE tells the kernel it must allocate at the address or fail.
-  // Do this only on host since android kernels still obey the hint without flag (for now).
-  if (!kIsTargetBuild && (flags & MAP_FIXED) == 0 && addr != nullptr) {
+#if defined(__linux__)
+  // Recent kernels have a bug where the address hint might be ignored.
+  // See https://lore.kernel.org/all/20241115215256.578125-1-kaleshsingh@google.com/
+  // We use MAP_FIXED_NOREPLACE to tell the kernel it must allocate at the address or fail.
+  // If the fixed-address allocation fails, we fallback to the default path (random address).
+  // Therefore, non-null 'addr' still behaves as hint-only as far as ART api is concerned.
+  if ((flags & MAP_FIXED) == 0 && addr != nullptr && IsKernelVersionAtLeast(4, 17)) {
     actual = MapInternal(
         addr, page_aligned_byte_count, prot, flags | MAP_FIXED_NOREPLACE, fd.get(), 0, low_4gb);
-    // If the fixed-address allocation failed, fallback to the default path (random address).
   }
+#endif  // __linux__
+
   if (actual == nullptr || actual == MAP_FAILED) {
     actual = MapInternal(addr, page_aligned_byte_count, prot, flags, fd.get(), 0, low_4gb);
   }
diff --git a/libartbase/base/mem_map.h b/libartbase/base/mem_map.h
index 4b4a56a23f..db60c9443e 100644
--- a/libartbase/base/mem_map.h
+++ b/libartbase/base/mem_map.h
@@ -37,13 +37,9 @@
 
 namespace art {
 
-#if defined(__LP64__) && !defined(__Fuchsia__) && \
-    (defined(__aarch64__) || defined(__riscv) || defined(__APPLE__))
+#if defined(__LP64__) && !defined(__Fuchsia__) && !defined(_WINDOWS_)
 #define USE_ART_LOW_4G_ALLOCATOR 1
 #else
-#if defined(__LP64__) && !defined(__Fuchsia__) && !defined(__x86_64__)
-#error "Unrecognized 64-bit architecture."
-#endif
 #define USE_ART_LOW_4G_ALLOCATOR 0
 #endif
 
diff --git a/libartbase/base/mem_map_test.cc b/libartbase/base/mem_map_test.cc
index 37f5b5a9fe..76a50a59e4 100644
--- a/libartbase/base/mem_map_test.cc
+++ b/libartbase/base/mem_map_test.cc
@@ -354,16 +354,21 @@ TEST_F(MemMapTest, MapAnonymousEmpty) {
 }
 
 TEST_F(MemMapTest, MapAnonymousFailNullError) {
+  // Host system's mmap_min_addr configuration could allow for arbitrarily low addresses to be
+  // successfully mapped, breaking the expectation that the MapAnonymous call should fail.
+  TEST_DISABLED_FOR_HOST();
+
   CommonInit();
+  uint8_t* invalid_page[16];  // Use this address as mmap hint address.
   const size_t page_size = MemMap::GetPageSize();
   // Test that we don't crash with a null error_str when mapping at an invalid location.
   MemMap map = MemMap::MapAnonymous("MapAnonymousInvalid",
-                                    reinterpret_cast<uint8_t*>(static_cast<size_t>(page_size)),
+                                    reinterpret_cast<uint8_t*>(AlignDown(invalid_page, page_size)),
                                     0x20000,
                                     PROT_READ | PROT_WRITE,
-                                    /*low_4gb=*/ false,
-                                    /*reuse=*/ false,
-                                    /*reservation=*/ nullptr,
+                                    /*low_4gb=*/false,
+                                    /*reuse=*/false,
+                                    /*reservation=*/nullptr,
                                     nullptr);
   ASSERT_FALSE(map.IsValid());
 }
@@ -894,17 +899,17 @@ TEST_F(MemMapTest, Reservation) {
 
 namespace {
 
-class DumpMapsOnFailListener : public testing::EmptyTestEventListener {
-  void OnTestPartResult(const testing::TestPartResult& result) override {
+class DumpMapsOnFailListener : public ::testing::EmptyTestEventListener {
+  void OnTestPartResult(const ::testing::TestPartResult& result) override {
     switch (result.type()) {
-      case testing::TestPartResult::kFatalFailure:
+      case ::testing::TestPartResult::kFatalFailure:
         art::PrintFileToLog("/proc/self/maps", android::base::LogSeverity::ERROR);
         break;
 
       // TODO: Could consider logging on EXPECT failures.
-      case testing::TestPartResult::kNonFatalFailure:
-      case testing::TestPartResult::kSkip:
-      case testing::TestPartResult::kSuccess:
+      case ::testing::TestPartResult::kNonFatalFailure:
+      case ::testing::TestPartResult::kSkip:
+      case ::testing::TestPartResult::kSuccess:
         break;
     }
   }
@@ -916,5 +921,5 @@ class DumpMapsOnFailListener : public testing::EmptyTestEventListener {
 extern "C"
 __attribute__((visibility("default"))) __attribute__((used))
 void ArtTestGlobalInit() {
-  testing::UnitTest::GetInstance()->listeners().Append(new DumpMapsOnFailListener());
+  ::testing::UnitTest::GetInstance()->listeners().Append(new DumpMapsOnFailListener());
 }
diff --git a/libartbase/base/membarrier.cc b/libartbase/base/membarrier.cc
index 328afb4701..07b132ee02 100644
--- a/libartbase/base/membarrier.cc
+++ b/libartbase/base/membarrier.cc
@@ -21,10 +21,11 @@
 
 #if !defined(_WIN32)
 #include <sys/syscall.h>
-#include <sys/utsname.h>
 #include <unistd.h>
 #endif
+
 #include "macros.h"
+#include "utils.h"
 
 #if __has_include(<linux/membarrier.h>)
 
@@ -52,26 +53,7 @@ static bool IsMemBarrierSupported() {
   // MEMBARRIER_CMD_PRIVATE_EXPEDITED is supported since Linux 4.14.
   // MEMBARRIER_CMD_PRIVATE_EXPEDITED_SYNC_CORE is supported since Linux 4.16.
   // Lowest Linux version useful for ART is 4.14.
-  static constexpr int kRequiredMajor = 4;
-  static constexpr int kRequiredMinor = 14;
-  struct utsname uts;
-  int major, minor;
-  if (uname(&uts) != 0 ||
-      strcmp(uts.sysname, "Linux") != 0 ||
-      sscanf(uts.release, "%d.%d", &major, &minor) != 2 ||
-      (major < kRequiredMajor || (major == kRequiredMajor && minor < kRequiredMinor))) {
-    return false;
-  }
-#if defined(__BIONIC__)
-  // Avoid calling membarrier on older Android versions where membarrier may be barred by seccomp
-  // causing the current process to be killed. The probing here could be considered expensive so
-  // endeavour not to repeat too often.
-  int api_level = android_get_device_api_level();
-  if (api_level < __ANDROID_API_Q__) {
-    return false;
-  }
-#endif  // __BIONIC__
-  return true;
+  return IsKernelVersionAtLeast(4, 14);
 }
 
 int membarrier(MembarrierCommand command) {
diff --git a/libartbase/base/memfd.cc b/libartbase/base/memfd.cc
index 3b9872b295..f965a08780 100644
--- a/libartbase/base/memfd.cc
+++ b/libartbase/base/memfd.cc
@@ -20,13 +20,11 @@
 #include <stdio.h>
 #if !defined(_WIN32)
 #include <fcntl.h>
+#include <sys/mman.h>
 #include <sys/syscall.h>
 #include <sys/utsname.h>
 #include <unistd.h>
 #endif
-#if defined(__BIONIC__)
-#include <linux/memfd.h>  // To access memfd flags.
-#endif
 
 #include <android-base/logging.h>
 #include <android-base/unique_fd.h>
@@ -35,7 +33,7 @@
 
 namespace art {
 
-#if defined(__NR_memfd_create)
+#if defined(__linux__)
 
 int memfd_create(const char* name, unsigned int flags) {
   // Check kernel version supports memfd_create(). Some older kernels segfault executing
@@ -55,40 +53,6 @@ int memfd_create(const char* name, unsigned int flags) {
   return syscall(__NR_memfd_create, name, flags);
 }
 
-#else  // __NR_memfd_create
-
-int memfd_create([[maybe_unused]] const char* name, [[maybe_unused]] unsigned int flags) {
-  errno = ENOSYS;
-  return -1;
-}
-
-#endif  // __NR_memfd_create
-
-// This is a wrapper that will attempt to simulate memfd_create if normal running fails.
-int memfd_create_compat(const char* name, unsigned int flags) {
-  int res = memfd_create(name, flags);
-  if (res >= 0) {
-    return res;
-  }
-#if !defined(_WIN32)
-  // Try to create an anonymous file with tmpfile that we can use instead.
-  if (flags == 0) {
-    FILE* file = tmpfile();
-    if (file != nullptr) {
-      // We want the normal 'dup' semantics since memfd_create without any flags isn't CLOEXEC.
-      // Unfortunately on some android targets we will compiler error if we use dup directly and so
-      // need to use fcntl.
-      int nfd = fcntl(fileno(file), F_DUPFD, /*lowest allowed fd*/ 0);
-      fclose(file);
-      return nfd;
-    }
-  }
-#endif
-  return res;
-}
-
-#if defined(__BIONIC__)
-
 static bool IsSealFutureWriteSupportedInternal() {
   android::base::unique_fd fd(art::memfd_create("test_android_memfd", MFD_ALLOW_SEALING));
   if (fd == -1) {
@@ -110,12 +74,17 @@ bool IsSealFutureWriteSupported() {
   return is_seal_future_write_supported;
 }
 
-#else
+#else  // __linux__
+
+int memfd_create([[maybe_unused]] const char* name, [[maybe_unused]] unsigned int flags) {
+  errno = ENOSYS;
+  return -1;
+}
 
 bool IsSealFutureWriteSupported() {
   return false;
 }
 
-#endif
+#endif  // __linux__
 
 }  // namespace art
diff --git a/libartbase/base/memfd.h b/libartbase/base/memfd.h
index 3c27dcb9e3..b288f7bc37 100644
--- a/libartbase/base/memfd.h
+++ b/libartbase/base/memfd.h
@@ -69,10 +69,6 @@ namespace art {
 // check for safety on older kernels (b/116769556)..
 int memfd_create(const char* name, unsigned int flags);
 
-// Call memfd(2) if available on platform and return result. Try to give us an unlinked FD in some
-// other way if memfd fails or isn't supported.
-int memfd_create_compat(const char* name, unsigned int flags);
-
 // Return whether the kernel supports sealing future writes of a memfd.
 bool IsSealFutureWriteSupported();
 
diff --git a/libartbase/base/metrics/metrics.h b/libartbase/base/metrics/metrics.h
index 0fe090fdbc..9f1e018bfd 100644
--- a/libartbase/base/metrics/metrics.h
+++ b/libartbase/base/metrics/metrics.h
@@ -71,25 +71,27 @@
   METRIC(FullGcDuration, MetricsCounter)
 
 // Increasing counter metrics, reported as Value Metrics in delta increments.
-#define ART_VALUE_METRICS(METRIC)                              \
-  METRIC(GcWorldStopTimeDelta, MetricsDeltaCounter)            \
-  METRIC(GcWorldStopCountDelta, MetricsDeltaCounter)           \
-  METRIC(YoungGcScannedBytesDelta, MetricsDeltaCounter)        \
-  METRIC(YoungGcFreedBytesDelta, MetricsDeltaCounter)          \
-  METRIC(YoungGcDurationDelta, MetricsDeltaCounter)            \
-  METRIC(FullGcScannedBytesDelta, MetricsDeltaCounter)         \
-  METRIC(FullGcFreedBytesDelta, MetricsDeltaCounter)           \
-  METRIC(FullGcDurationDelta, MetricsDeltaCounter)             \
-  METRIC(JitMethodCompileTotalTimeDelta, MetricsDeltaCounter)  \
-  METRIC(JitMethodCompileCountDelta, MetricsDeltaCounter)      \
-  METRIC(ClassVerificationTotalTimeDelta, MetricsDeltaCounter) \
-  METRIC(ClassVerificationCountDelta, MetricsDeltaCounter)     \
-  METRIC(ClassLoadingTotalTimeDelta, MetricsDeltaCounter)      \
-  METRIC(TotalBytesAllocatedDelta, MetricsDeltaCounter)        \
-  METRIC(TotalGcCollectionTimeDelta, MetricsDeltaCounter)      \
-  METRIC(YoungGcCountDelta, MetricsDeltaCounter)               \
-  METRIC(FullGcCountDelta, MetricsDeltaCounter)                \
-  METRIC(TimeElapsedDelta, MetricsDeltaCounter)
+#define ART_VALUE_METRICS(METRIC)                                    \
+  METRIC(GcWorldStopTimeDelta, MetricsDeltaCounter)                  \
+  METRIC(GcWorldStopCountDelta, MetricsDeltaCounter)                 \
+  METRIC(YoungGcScannedBytesDelta, MetricsDeltaCounter)              \
+  METRIC(YoungGcFreedBytesDelta, MetricsDeltaCounter)                \
+  METRIC(YoungGcDurationDelta, MetricsDeltaCounter)                  \
+  METRIC(FullGcScannedBytesDelta, MetricsDeltaCounter)               \
+  METRIC(FullGcFreedBytesDelta, MetricsDeltaCounter)                 \
+  METRIC(FullGcDurationDelta, MetricsDeltaCounter)                   \
+  METRIC(JitMethodCompileTotalTimeDelta, MetricsDeltaCounter)        \
+  METRIC(JitMethodCompileCountDelta, MetricsDeltaCounter)            \
+  METRIC(ClassVerificationTotalTimeDelta, MetricsDeltaCounter)       \
+  METRIC(ClassVerificationCountDelta, MetricsDeltaCounter)           \
+  METRIC(ClassLoadingTotalTimeDelta, MetricsDeltaCounter)            \
+  METRIC(TotalBytesAllocatedDelta, MetricsDeltaCounter)              \
+  METRIC(TotalGcCollectionTimeDelta, MetricsDeltaCounter)            \
+  METRIC(YoungGcCountDelta, MetricsDeltaCounter)                     \
+  METRIC(FullGcCountDelta, MetricsDeltaCounter)                      \
+  METRIC(TimeElapsedDelta, MetricsDeltaCounter)                      \
+  METRIC(AppSlowPathDuringYoungGcDurationDelta, MetricsDeltaCounter) \
+  METRIC(AppSlowPathDuringFullGcDurationDelta, MetricsDeltaCounter)
 
 #define ART_METRICS(METRIC) \
   ART_EVENT_METRICS(METRIC) \
diff --git a/libartbase/base/metrics/metrics_test.cc b/libartbase/base/metrics/metrics_test.cc
index bcc4da4e06..3581615514 100644
--- a/libartbase/base/metrics/metrics_test.cc
+++ b/libartbase/base/metrics/metrics_test.cc
@@ -31,7 +31,7 @@ using test::CounterValue;
 using test::GetBuckets;
 using test::TestBackendBase;
 
-class MetricsTest : public testing::Test {};
+class MetricsTest : public ::testing::Test {};
 
 TEST_F(MetricsTest, SimpleCounter) {
   MetricsCounter<DatumId::kClassVerificationTotalTime> test_counter;
diff --git a/libartbase/base/os.h b/libartbase/base/os.h
index cb71d211af..a10455d051 100644
--- a/libartbase/base/os.h
+++ b/libartbase/base/os.h
@@ -18,17 +18,27 @@
 #define ART_LIBARTBASE_BASE_OS_H_
 
 #include <stdint.h>
+#include <sys/types.h>
 
-namespace unix_file {
-class FdFile;
-}  // namespace unix_file
+#include <cstddef>
+#include <memory>
+#include <string>
+
+#include "unix_file/fd_file.h"
 
 namespace art {
 
 using File = ::unix_file::FdFile;
 
-// Interface to the underlying OS platform.
+struct FileWithRange {
+  std::unique_ptr<File> file;
+  off_t start;
+  size_t length;
+
+  static FileWithRange Invalid();
+};
 
+// Interface to the underlying OS platform.
 class OS {
  public:
   // Open an existing file with read only access.
@@ -56,6 +66,20 @@ class OS {
 
   // Get the size of a file (or -1 if it does not exist).
   static int64_t GetFileSizeBytes(const char* name);
+
+  // Open an existing file or an entry in a zip file with read only access.
+  // `name_and_zip_entry` should be either a path to an existing file, or a path to a zip file and
+  // the name of the zip entry, separated by `zip_separator`.
+  // `alignment` is the expected alignment of the specified zip entry, in bytes. Only applicable if
+  // `name_and_zip_entry` points to a zip entry.
+  // Returns `file` being the file at the specified path and the range being the entire range of the
+  // file, if `name_and_zip_entry` points to a file. Returns `file` being the zip file and the range
+  // being the range of the zip entry, if `name_and_zip_entry` points to a zip entry. Returns `file`
+  // being nullptr on failure.
+  static FileWithRange OpenFileDirectlyOrFromZip(const std::string& name_and_zip_entry,
+                                                 const char* zip_separator,
+                                                 size_t alignment,
+                                                 std::string* error_msg);
 };
 
 }  // namespace art
diff --git a/libartbase/base/os_linux.cc b/libartbase/base/os_linux.cc
index 337c54f1d1..6ea200169e 100644
--- a/libartbase/base/os_linux.cc
+++ b/libartbase/base/os_linux.cc
@@ -14,21 +14,23 @@
  * limitations under the License.
  */
 
-#include "os.h"
-
+#include <android-base/logging.h>
 #include <fcntl.h>
 #include <sys/stat.h>
 #include <sys/types.h>
 
 #include <cstddef>
+#include <cstring>
 #include <memory>
 
-#include <android-base/logging.h>
-
+#include "base/zip_archive.h"
+#include "os.h"
 #include "unix_file/fd_file.h"
 
 namespace art {
 
+FileWithRange FileWithRange::Invalid() { return {.file = nullptr, .start = 0, .length = 0}; }
+
 File* OS::OpenFileForReading(const char* name) {
   return OpenFileWithFlags(name, O_RDONLY);
 }
@@ -103,4 +105,69 @@ int64_t OS::GetFileSizeBytes(const char* name) {
   }
 }
 
+FileWithRange OS::OpenFileDirectlyOrFromZip(const std::string& name_and_zip_entry,
+                                            const char* zip_separator,
+                                            size_t alignment,
+                                            std::string* error_msg) {
+  std::string filename = name_and_zip_entry;
+  std::string zip_entry_name;
+  size_t pos = filename.find(zip_separator);
+  if (pos != std::string::npos) {
+    zip_entry_name = filename.substr(pos + strlen(zip_separator));
+    filename.resize(pos);
+    if (filename.empty() || zip_entry_name.empty()) {
+      *error_msg = ART_FORMAT("Malformed zip path '{}'", name_and_zip_entry);
+      return FileWithRange::Invalid();
+    }
+  }
+
+  std::unique_ptr<File> file(OS::OpenFileForReading(filename.c_str()));
+  if (file == nullptr) {
+    *error_msg = ART_FORMAT("Failed to open '{}' for reading: {}", filename, strerror(errno));
+    return FileWithRange::Invalid();
+  }
+
+  off_t start = 0;
+  int64_t total_file_length = file->GetLength();
+  if (total_file_length < 0) {
+    *error_msg = ART_FORMAT("Failed to get file length of '{}': {}", filename, strerror(errno));
+    return FileWithRange::Invalid();
+  }
+  size_t length = total_file_length;
+
+  if (!zip_entry_name.empty()) {
+    std::unique_ptr<ZipArchive> zip_archive(
+        ZipArchive::OpenFromOwnedFd(file->Fd(), filename.c_str(), error_msg));
+    if (zip_archive == nullptr) {
+      *error_msg = ART_FORMAT("Failed to open '{}' as zip", filename);
+      return FileWithRange::Invalid();
+    }
+    std::unique_ptr<ZipEntry> zip_entry(zip_archive->Find(zip_entry_name.c_str(), error_msg));
+    if (zip_entry == nullptr) {
+      *error_msg = ART_FORMAT("Failed to find entry '{}' in zip '{}'", zip_entry_name, filename);
+      return FileWithRange::Invalid();
+    }
+    if (!zip_entry->IsUncompressed() || !zip_entry->IsAlignedTo(alignment)) {
+      *error_msg =
+          ART_FORMAT("The entry '{}' in zip '{}' must be uncompressed and aligned to {} bytes",
+                     zip_entry_name,
+                     filename,
+                     alignment);
+      return FileWithRange::Invalid();
+    }
+    start = zip_entry->GetOffset();
+    length = zip_entry->GetUncompressedLength();
+    if (start + length > static_cast<size_t>(total_file_length)) {
+      *error_msg = ART_FORMAT(
+          "Invalid zip entry offset or length (offset: {}, length: {}, total_file_length: {})",
+          start,
+          length,
+          total_file_length);
+      return FileWithRange::Invalid();
+    }
+  }
+
+  return {.file = std::move(file), .start = start, .length = length};
+}
+
 }  // namespace art
diff --git a/libartbase/base/pidfd.h b/libartbase/base/pidfd.h
index d209d931c8..965a1a8f6b 100644
--- a/libartbase/base/pidfd.h
+++ b/libartbase/base/pidfd.h
@@ -24,6 +24,8 @@
 
 #ifdef __BIONIC__
 #include <sys/pidfd.h>
+#else
+#include <sys/syscall.h>
 #endif
 
 namespace art {
@@ -33,9 +35,6 @@ namespace art {
   return android::base::unique_fd(pidfd_open(pid, flags));
 #else
   // There is no glibc wrapper for pidfd_open.
-#ifndef SYS_pidfd_open
-  constexpr int SYS_pidfd_open = 434;
-#endif
   return android::base::unique_fd(syscall(SYS_pidfd_open, pid, flags));
 #endif
 }
diff --git a/libartbase/base/scoped_arena_containers.h b/libartbase/base/scoped_arena_containers.h
index ee9a7461f8..2fbf555a00 100644
--- a/libartbase/base/scoped_arena_containers.h
+++ b/libartbase/base/scoped_arena_containers.h
@@ -19,6 +19,7 @@
 
 #include <deque>
 #include <forward_list>
+#include <list>
 #include <queue>
 #include <set>
 #include <type_traits>
@@ -51,6 +52,9 @@ using ScopedArenaDeque = std::deque<T, ScopedArenaAllocatorAdapter<T>>;
 template <typename T>
 using ScopedArenaForwardList = std::forward_list<T, ScopedArenaAllocatorAdapter<T>>;
 
+template <typename T>
+using ScopedArenaList = std::list<T, ScopedArenaAllocatorAdapter<T>>;
+
 template <typename T>
 using ScopedArenaQueue = std::queue<T, ScopedArenaDeque<T>>;
 
diff --git a/libartbase/base/testing.cc b/libartbase/base/testing.cc
index 3cd2836876..6ec207eded 100644
--- a/libartbase/base/testing.cc
+++ b/libartbase/base/testing.cc
@@ -14,19 +14,123 @@
  * limitations under the License.
  */
 
+#include "testing.h"
+
 #include <string>
 #include <vector>
 
+#include "android-base/file.h"
 #include "android-base/stringprintf.h"
+#include "android-base/strings.h"
 #include "base/file_utils.h"
 #include "base/globals.h"
+#include "base/os.h"
 
 namespace art {
 namespace testing {
 
-namespace {
+std::string GetAndroidBuildTop() {
+  CHECK(IsHost());
+  std::string android_build_top;
+
+  // Look at how we were invoked to find the expected directory.
+  std::string argv;
+  if (android::base::ReadFileToString("/proc/self/cmdline", &argv)) {
+    // /proc/self/cmdline is the programs 'argv' with elements delimited by '\0'.
+    std::filesystem::path path(argv.substr(0, argv.find('\0')));
+    path = std::filesystem::absolute(path);
+    // Walk up until we find the one of the well-known directories.
+    for (; path.parent_path() != path; path = path.parent_path()) {
+      // We are running tests from out/host/linux-x86 on developer machine.
+      if (path.filename() == std::filesystem::path("linux-x86")) {
+        android_build_top = path.parent_path().parent_path().parent_path();
+        break;
+      }
+      // We are running tests from testcases (extracted from zip) on tradefed.
+      // The first path is for remote runs and the second path for local runs.
+      if (path.filename() == std::filesystem::path("testcases") ||
+          path.filename().string().starts_with("host_testcases")) {
+        android_build_top = path.append("art_common");
+        break;
+      }
+    }
+  }
+  CHECK(!android_build_top.empty());
+
+  // Check that the expected directory matches the environment variable.
+  const char* android_build_top_from_env = getenv("ANDROID_BUILD_TOP");
+  android_build_top = std::filesystem::path(android_build_top).string();
+  CHECK(!android_build_top.empty());
+  if (android_build_top_from_env != nullptr) {
+    if (std::filesystem::weakly_canonical(android_build_top).string() !=
+        std::filesystem::weakly_canonical(android_build_top_from_env).string()) {
+      android_build_top = android_build_top_from_env;
+    }
+  } else {
+    setenv("ANDROID_BUILD_TOP", android_build_top.c_str(), /*overwrite=*/0);
+  }
+  if (android_build_top.back() != '/') {
+    android_build_top += '/';
+  }
+  return android_build_top;
+}
+
+std::string GetAndroidHostOut() {
+  CHECK(IsHost());
+
+  // Check that the expected directory matches the environment variable.
+  // ANDROID_HOST_OUT is set by envsetup or unset and is the full path to host binaries/libs
+  const char* android_host_out_from_env = getenv("ANDROID_HOST_OUT");
+  // OUT_DIR is a user-settable ENV_VAR that controls where soong puts build artifacts. It can
+  // either be relative to ANDROID_BUILD_TOP or a concrete path.
+  const char* android_out_dir = getenv("OUT_DIR");
+  // Take account of OUT_DIR setting.
+  if (android_out_dir == nullptr) {
+    android_out_dir = "out";
+  }
+  std::string android_host_out;
+  if (android_out_dir[0] == '/') {
+    android_host_out = (std::filesystem::path(android_out_dir) / "host" / "linux-x86").string();
+  } else {
+    android_host_out =
+        (std::filesystem::path(GetAndroidBuildTop()) / android_out_dir / "host" / "linux-x86")
+            .string();
+  }
+  std::filesystem::path expected(android_host_out);
+  if (android_host_out_from_env != nullptr) {
+    std::filesystem::path from_env(std::filesystem::weakly_canonical(android_host_out_from_env));
+    if (std::filesystem::weakly_canonical(expected).string() != from_env.string()) {
+      LOG(WARNING) << "Execution path (" << expected << ") not below ANDROID_HOST_OUT (" << from_env
+                   << ")! Using env-var.";
+      expected = from_env;
+    }
+  } else {
+    setenv("ANDROID_HOST_OUT", android_host_out.c_str(), /*overwrite=*/0);
+  }
+  return expected.string();
+}
+
+std::string GetHostBootClasspathInstallRoot() {
+  CHECK(IsHost());
+  std::string build_install_root = GetAndroidHostOut() + "/testcases/art_common/out/host/linux-x86";
+  // Look for the `apex` subdirectory as a discriminator to check the location.
+  if (OS::DirectoryExists((build_install_root + "/apex").c_str())) {
+    // This is the path where "m art-host-tests" installs support files for host
+    // tests, so use it when the tests are run in a build tree (which is the
+    // case when testing locally).
+    return build_install_root;
+  }
+  if (OS::DirectoryExists((GetAndroidRoot() + "/apex").c_str())) {
+    // This is the location for host tests in CI when the files are unzipped
+    // from art-host-tests.zip.
+    return GetAndroidRoot();
+  }
+  LOG(ERROR) << "Neither location has a boot classpath (forgot \"m art-host-tests\"?): "
+             << build_install_root << " or " << GetAndroidRoot();
+  return "<no boot classpath found>";
+}
 
-std::string GetDexFileName(const std::string& jar_prefix, const std::string& prefix) {
+static std::string GetDexFileName(const std::string& jar_prefix, const std::string& prefix) {
   const char* apexPath =
       (jar_prefix == "conscrypt") ?
           kAndroidConscryptApexDefaultPath :
@@ -35,7 +139,15 @@ std::string GetDexFileName(const std::string& jar_prefix, const std::string& pre
       "%s%s/javalib/%s.jar", prefix.c_str(), apexPath, jar_prefix.c_str());
 }
 
-}  // namespace
+static std::vector<std::string> GetPrefixedDexFileNames(const std::string& prefix,
+                                                        const std::vector<std::string>& modules) {
+  std::vector<std::string> result;
+  result.reserve(modules.size());
+  for (const std::string& module : modules) {
+    result.push_back(GetDexFileName(module, prefix));
+  }
+  return result;
+}
 
 std::vector<std::string> GetLibCoreModuleNames(bool core_only) {
   // Note: This must start with the CORE_IMG_JARS in Android.common_path.mk because that's what we
@@ -59,23 +171,25 @@ std::vector<std::string> GetLibCoreModuleNames(bool core_only) {
   return modules;
 }
 
-std::vector<std::string> GetLibCoreDexFileNames(const std::string& prefix,
-                                                const std::vector<std::string>& modules) {
-  std::vector<std::string> result;
-  result.reserve(modules.size());
-  for (const std::string& module : modules) {
-    result.push_back(GetDexFileName(module, prefix));
-  }
-  return result;
+std::vector<std::string> GetLibCoreDexFileNames(const std::vector<std::string>& modules) {
+  return GetPrefixedDexFileNames(kIsTargetBuild ? "" : GetHostBootClasspathInstallRoot(), modules);
 }
 
 std::vector<std::string> GetLibCoreDexFileNames(const std::string& prefix, bool core_only) {
   std::vector<std::string> modules = GetLibCoreModuleNames(core_only);
-  return GetLibCoreDexFileNames(prefix, modules);
+  return GetPrefixedDexFileNames(prefix, modules);
 }
 
 std::vector<std::string> GetLibCoreDexLocations(const std::vector<std::string>& modules) {
-  return GetLibCoreDexFileNames(/*prefix=*/"", modules);
+  std::string prefix = "";
+  if (IsHost()) {
+    std::string android_root = GetAndroidRoot();
+    std::string build_top = GetAndroidBuildTop();
+    CHECK(android_root.starts_with(build_top))
+        << " android_root=" << android_root << " build_top=" << build_top;
+    prefix = android_root.substr(build_top.size());
+  }
+  return GetPrefixedDexFileNames(prefix, modules);
 }
 
 std::vector<std::string> GetLibCoreDexLocations(bool core_only) {
@@ -83,5 +197,9 @@ std::vector<std::string> GetLibCoreDexLocations(bool core_only) {
   return GetLibCoreDexLocations(modules);
 }
 
+std::string GetClassPathOption(const char* option, const std::vector<std::string>& class_path) {
+  return option + android::base::Join(class_path, ':');
+}
+
 }  // namespace testing
 }  // namespace art
diff --git a/libartbase/base/testing.h b/libartbase/base/testing.h
index 88d0aee6e0..55d7428436 100644
--- a/libartbase/base/testing.h
+++ b/libartbase/base/testing.h
@@ -22,20 +22,41 @@
 #include <string>
 #include <vector>
 
+#include "base/globals.h"
+
 namespace art {
 namespace testing {
 
+inline bool IsHost() { return !art::kIsTargetBuild; }
+
+// Returns ${ANDROID_BUILD_TOP}. Ensure it has tailing /.
+std::string GetAndroidBuildTop();
+
+// Returns ${ANDROID_HOST_OUT}.
+std::string GetAndroidHostOut();
+
+// Returns the path where boot classpath and boot image files are installed
+// for host tests (by the art_common mk module, typically built through "m
+// art-host-tests"). Different in CI where they are unpacked from the
+// art-host-tests.zip file.
+std::string GetHostBootClasspathInstallRoot();
+
 // Note: "libcore" here means art + conscrypt + icu.
 
 // Gets the names of the libcore modules.
 // If `core_only` is true, only returns the names of CORE_IMG_JARS in Android.common_path.mk.
 std::vector<std::string> GetLibCoreModuleNames(bool core_only = false);
 
-// Gets the paths of the libcore dex files for given modules.
-std::vector<std::string> GetLibCoreDexFileNames(const std::string& prefix,
-                                                const std::vector<std::string>& modules);
+// Gets the paths of the libcore dex files for given modules, prefixed appropriately for host or
+// target tests.
+std::vector<std::string> GetLibCoreDexFileNames(const std::vector<std::string>& modules);
+
+// Gets the paths of the libcore module dex files, prefixed appropriately for host or target tests.
+inline std::vector<std::string> GetLibCoreDexFileNames() {
+  return GetLibCoreDexFileNames(GetLibCoreModuleNames());
+}
 
-// Gets the paths of the libcore dex files.
+// Gets the paths of the libcore dex files, prefixed by the given string.
 // If `core_only` is true, only returns the filenames of CORE_IMG_JARS in Android.common_path.mk.
 std::vector<std::string> GetLibCoreDexFileNames(const std::string& prefix, bool core_only = false);
 
@@ -46,6 +67,8 @@ std::vector<std::string> GetLibCoreDexLocations(const std::vector<std::string>&
 // If `core_only` is true, only returns the filenames of CORE_IMG_JARS in Android.common_path.mk.
 std::vector<std::string> GetLibCoreDexLocations(bool core_only = false);
 
+std::string GetClassPathOption(const char* option, const std::vector<std::string>& class_path);
+
 }  // namespace testing
 }  // namespace art
 
diff --git a/libartbase/base/time_utils.h b/libartbase/base/time_utils.h
index dd73b1c951..ddabb1289f 100644
--- a/libartbase/base/time_utils.h
+++ b/libartbase/base/time_utils.h
@@ -26,6 +26,8 @@
 #include <cstdint>
 #include <string>
 
+#include "android-base/logging.h"
+
 namespace art {
 
 enum TimeUnit {
@@ -123,6 +125,13 @@ void NanoSleep(uint64_t ns);
 // time corresponding to the indicated clock value plus the supplied offset.
 void InitTimeSpec(bool absolute, int clock, int64_t ms, int32_t ns, timespec* ts);
 
+// Converts `timespec` to nanoseconds. The return value can be negative, which should be interpreted
+// as a time before the epoch.
+static constexpr int64_t TimeSpecToNs(timespec ts) {
+  DCHECK_GE(ts.tv_nsec, 0);  // According to POSIX.
+  return static_cast<int64_t>(ts.tv_sec) * INT64_C(1000000000) + ts.tv_nsec;
+}
+
 }  // namespace art
 
 #endif  // ART_LIBARTBASE_BASE_TIME_UTILS_H_
diff --git a/libartbase/base/unix_file/fd_file.cc b/libartbase/base/unix_file/fd_file.cc
index 1387bfe9b8..2bdeb0cf52 100644
--- a/libartbase/base/unix_file/fd_file.cc
+++ b/libartbase/base/unix_file/fd_file.cc
@@ -24,6 +24,7 @@
 
 #if defined(__BIONIC__)
 #include <android/fdsan.h>
+#include <android/api-level.h>
 #endif
 
 #if defined(_WIN32)
@@ -35,6 +36,7 @@
 
 #include <android-base/file.h>
 #include <android-base/logging.h>
+#include <android-base/properties.h>
 
 // Includes needed for FdFile::Copy().
 #include "base/globals.h"
@@ -49,6 +51,23 @@
 
 namespace unix_file {
 
+// f2fs decompress issue.
+static bool b376814207() {
+#ifdef __BIONIC__
+  if (android_get_device_api_level() >= 35) {
+    return false;
+  }
+#endif
+  std::string property = android::base::GetProperty("ro.product.build.fingerprint", "");
+  return property.starts_with("samsung");
+}
+
+// Used to work around kernel bugs.
+bool AllowSparseFiles() {
+  static bool allow = !b376814207();
+  return allow;
+}
+
 #if defined(_WIN32)
 // RAII wrapper for an event object to allow asynchronous I/O to correctly signal completion.
 class ScopedEvent {
@@ -534,7 +553,7 @@ bool FdFile::SparseWrite(const uint8_t* data,
                          size_t size,
                          const std::vector<uint8_t>& zeroes) {
   DCHECK_GE(zeroes.size(), size);
-  if (memcmp(zeroes.data(), data, size) == 0) {
+  if (memcmp(zeroes.data(), data, size) == 0 && AllowSparseFiles()) {
     // These bytes are all zeroes, skip them by moving the file offset via lseek SEEK_CUR (available
     // since linux kernel 3.1).
     if (TEMP_FAILURE_RETRY(lseek(Fd(), size, SEEK_CUR)) < 0) {
diff --git a/libartbase/base/unix_file/fd_file.h b/libartbase/base/unix_file/fd_file.h
index a46ef81586..035518ab8c 100644
--- a/libartbase/base/unix_file/fd_file.h
+++ b/libartbase/base/unix_file/fd_file.h
@@ -29,6 +29,9 @@ namespace unix_file {
 // If true, check whether Flush and Close are called before destruction.
 static constexpr bool kCheckSafeUsage = true;
 
+// Used to work around kernel bugs.
+bool AllowSparseFiles();
+
 // A RandomAccessFile implementation backed by a file descriptor.
 //
 // Not thread safe.
diff --git a/libartbase/base/unix_file/fd_file_test.cc b/libartbase/base/unix_file/fd_file_test.cc
index d5c3056393..3de3bd91a2 100644
--- a/libartbase/base/unix_file/fd_file_test.cc
+++ b/libartbase/base/unix_file/fd_file_test.cc
@@ -326,6 +326,9 @@ void FdFileTest::TestDataMatches(const FdFile* src,
 // Test that the file created by FdFileTest::CreateSparseSourceFile is sparse on the test
 // environment.
 TEST_F(FdFileTest, CopySparseCreateSparseFile) {
+  // Disable on host as sparsity is filesystem dependent and some hosts may break test assumptions.
+  TEST_DISABLED_FOR_HOST();
+
   // Create file with no empty prefix or suffix.
   std::unique_ptr<art::ScratchFile> src1;
   ASSERT_NO_FATAL_FAILURE(CreateSparseSourceFile(/*empty_prefix=*/0, /*empty_suffix=*/0, src1));
@@ -350,8 +353,11 @@ TEST_F(FdFileTest, CopySparseCreateSparseFile) {
 
 // Test complete copies of the source file produced by FdFileTest::CreateSparseSourceFile.
 TEST_F(FdFileTest, CopySparseFullCopy) {
+  // Disable on host as sparsity is filesystem dependent and some hosts may break test assumptions.
+  TEST_DISABLED_FOR_HOST();
+
   auto verify_fullcopy = [&](size_t empty_prefix, size_t empty_suffix) {
-    SCOPED_TRACE(testing::Message() << "prefix:" << empty_prefix << ", suffix:" << empty_suffix);
+    SCOPED_TRACE(::testing::Message() << "prefix:" << empty_prefix << ", suffix:" << empty_suffix);
 
     std::unique_ptr<art::ScratchFile> src;
     ASSERT_NO_FATAL_FAILURE(CreateSparseSourceFile(empty_prefix, empty_suffix, src));
@@ -417,6 +423,9 @@ size_t FdFileTest::GetFilesystemBlockSize() {
 
 // Test partial copies of the source file produced by FdFileTest::CreateSparseSourceFile.
 TEST_F(FdFileTest, CopySparsePartialCopy) {
+  // Disable on host as sparsity is filesystem dependent and some hosts may break test assumptions.
+  TEST_DISABLED_FOR_HOST();
+
   size_t blocksize = GetFilesystemBlockSize();
   ASSERT_GT(blocksize, 0u);
 
@@ -426,9 +435,9 @@ TEST_F(FdFileTest, CopySparsePartialCopy) {
                                 size_t copy_end_offset) {
     // The copy starts <copy_start_offset> from the start of the source file.
     // The copy ends <copy_end_offset> from the end of the source file.
-    SCOPED_TRACE(testing::Message() << "prefix:" << empty_prefix << ", suffix:" << empty_suffix
-                 << ", copy_start_offset:" << copy_start_offset << ", copy_end_offset:"
-                 << copy_end_offset);
+    SCOPED_TRACE(::testing::Message() << "prefix:" << empty_prefix << ", suffix:" << empty_suffix
+                                      << ", copy_start_offset:" << copy_start_offset
+                                      << ", copy_end_offset:" << copy_end_offset);
 
     std::unique_ptr<art::ScratchFile> src;
     ASSERT_NO_FATAL_FAILURE(CreateSparseSourceFile(empty_prefix, empty_suffix, src));
@@ -502,6 +511,9 @@ TEST_F(FdFileTest, CopySparsePartialCopy) {
 
 // Test the case where the destination file's FD offset is non-zero before the copy.
 TEST_F(FdFileTest, CopySparseToNonZeroOffset) {
+  // Disable on host as sparsity is filesystem dependent and some hosts may break test assumptions.
+  TEST_DISABLED_FOR_HOST();
+
   std::unique_ptr<art::ScratchFile> src;
   ASSERT_NO_FATAL_FAILURE(CreateSparseSourceFile(/*empty_prefix=*/0u, /*empty_suffix=*/0u, src));
 
diff --git a/libartbase/base/unix_file/random_access_file_test.h b/libartbase/base/unix_file/random_access_file_test.h
index 0592256291..abcc161aab 100644
--- a/libartbase/base/unix_file/random_access_file_test.h
+++ b/libartbase/base/unix_file/random_access_file_test.h
@@ -25,7 +25,7 @@
 
 namespace unix_file {
 
-class RandomAccessFileTest : public testing::Test {
+class RandomAccessFileTest : public ::testing::Test {
  protected:
   virtual ~RandomAccessFileTest() {
   }
diff --git a/libartbase/base/utils.cc b/libartbase/base/utils.cc
index 2c95986809..3f057604c1 100644
--- a/libartbase/base/utils.cc
+++ b/libartbase/base/utils.cc
@@ -44,13 +44,6 @@
 #include "AvailabilityMacros.h"  // For MAC_OS_X_VERSION_MAX_ALLOWED
 #endif
 
-#if defined(__BIONIC__)
-// membarrier(2) is only supported for target builds (b/111199492).
-#include <linux/membarrier.h>
-// NOLINTNEXTLINE - inclusion of syscall is dependent on arch
-#include <sys/syscall.h>
-#endif
-
 #if defined(__linux__)
 #include <linux/unistd.h>
 // NOLINTNEXTLINE - inclusion of syscall is dependent on arch
@@ -167,12 +160,17 @@ bool FlushCpuCaches(void* begin, void* end) {
 
 #if defined(__linux__)
 bool IsKernelVersionAtLeast(int reqd_major, int reqd_minor) {
-  struct utsname uts;
-  int major, minor;
-  CHECK_EQ(uname(&uts), 0);
-  CHECK_EQ(strcmp(uts.sysname, "Linux"), 0);
-  CHECK_EQ(sscanf(uts.release, "%d.%d:", &major, &minor), 2);
-  return major > reqd_major || (major == reqd_major && minor >= reqd_minor);
+  static auto version = []() -> std::pair<int, int> {
+    struct utsname uts;
+    int res, major, minor;
+    res = uname(&uts);
+    CHECK_EQ(res, 0);
+    CHECK_EQ(strcmp(uts.sysname, "Linux"), 0);
+    res = sscanf(uts.release, "%d.%d:", &major, &minor);
+    CHECK_EQ(res, 2);
+    return std::make_pair(major, minor);
+  }();
+  return version >= std::make_pair(reqd_major, reqd_minor);
 }
 #endif
 
@@ -406,17 +404,29 @@ size_t GetOsThreadStat(pid_t tid, char* buf, size_t len) {
 }
 
 std::string GetOsThreadStatQuick(pid_t tid) {
+#if defined(__linux__)
   static constexpr int BUF_SIZE = 100;
   char buf[BUF_SIZE];
-#if defined(__linux__)
   if (GetOsThreadStat(tid, buf, BUF_SIZE) == 0) {
     snprintf(buf, BUF_SIZE, "Unknown state: %d", tid);
   }
+  return buf;
 #else
   UNUSED(tid);
-  strcpy(buf, "Unknown state");  // snprintf may not be usable.
+  return "Unknown state";
 #endif
-  return buf;
+}
+
+char GetStateFromStatString(const std::string& stat_output) {
+  size_t rparen_pos = stat_output.find(")");
+  if (rparen_pos == std::string::npos || rparen_pos >= stat_output.length() - 3) {
+    return '?';
+  }
+  size_t state_pos = stat_output.find_first_not_of(" ", rparen_pos + 1);
+  if (rparen_pos == std::string::npos) {
+    return '?';
+  }
+  return stat_output[state_pos];
 }
 
 std::string GetOtherThreadOsStats() {
diff --git a/libartbase/base/utils.h b/libartbase/base/utils.h
index 4b86651f95..fdb7f6650a 100644
--- a/libartbase/base/utils.h
+++ b/libartbase/base/utils.h
@@ -152,14 +152,18 @@ inline void ForceRead(const T* pointer) {
 // there is an I/O error.
 std::string GetProcessStatus(const char* key);
 
-// Copy a prefix of /proc/tid/stat of the given length into buf. Return the number of bytes
-// actually read, 0 on error.
+// Copy a prefix of /proc/pid/task/tid/stat of the given length into buf. Return the number of
+// bytes actually read, 0 on error.
 size_t GetOsThreadStat(pid_t tid, char* buf, size_t len);
 
-// Return a short prefix of /proc/tid/stat as quickly and robustly as possible. Used for debugging
-// timing issues and possibly issues with /proc itself. Always atomic.
+// Return a short prefix of /proc/pid/task/tid/stat as quickly and robustly as possible. Used for
+// debugging timing issues and possibly issues with /proc itself. Always atomic.
 std::string GetOsThreadStatQuick(pid_t tid);
 
+// Given a /proc/.../stat string or prefix, such as those returned by the above, return the single
+// character representation of the thread state from that string, or '?' if it can't be found.
+char GetStateFromStatString(const std::string& stat_output);
+
 // Return a concatenation of the output of GetOsThreadStatQuick(tid) for all other tids.
 // Less robust against concurrent change, but individual stat strings should still always
 // be consistent. Called only when we are nearly certain to crash anyway.
diff --git a/libartbase/base/utils_test.cc b/libartbase/base/utils_test.cc
index ab8627fa7d..88d8f07741 100644
--- a/libartbase/base/utils_test.cc
+++ b/libartbase/base/utils_test.cc
@@ -22,7 +22,7 @@
 
 namespace art {
 
-class UtilsTest : public testing::Test {};
+class UtilsTest : public ::testing::Test {};
 
 TEST_F(UtilsTest, PrettySize) {
   EXPECT_EQ("1024MB", PrettySize(1 * GB));
@@ -114,16 +114,23 @@ TEST_F(UtilsTest, Split) {
 }
 
 TEST_F(UtilsTest, GetProcessStatus) {
-  EXPECT_THAT(GetProcessStatus("Name"),
-              testing::AnyOf(
-                  "art_libartbase_",    // Test binary name: `art_libartbase_test`.
-                  "art_standalone_"));  // Test binary name: `art_standalone_libartbase_test`.
+  EXPECT_THAT(
+      GetProcessStatus("Name"),
+      ::testing::AnyOf("art_libartbase_",    // Test binary name: `art_libartbase_test`.
+                       "art_standalone_"));  // Test binary name: `art_standalone_libartbase_test`.
   EXPECT_EQ("R (running)", GetProcessStatus("State"));
   EXPECT_EQ("<unknown>", GetProcessStatus("tate"));
   EXPECT_EQ("<unknown>", GetProcessStatus("e"));
   EXPECT_EQ("<unknown>", GetProcessStatus("InvalidFieldName"));
 }
 
+TEST_F(UtilsTest, GetOsThreadStatQuick) {
+  std::string my_stat = GetOsThreadStatQuick(GetTid());
+  EXPECT_GT(my_stat.length(), 20);
+  EXPECT_LT(my_stat.length(), 1000);
+  EXPECT_EQ('R', GetStateFromStatString(my_stat));
+}
+
 TEST_F(UtilsTest, StringSplit) {
   auto range = SplitString("[ab[c[[d[e[", '[');
   auto it = range.begin();
diff --git a/libartbase/base/zip_archive.cc b/libartbase/base/zip_archive.cc
index ba44096be5..f90a92d769 100644
--- a/libartbase/base/zip_archive.cc
+++ b/libartbase/base/zip_archive.cc
@@ -37,23 +37,19 @@ static constexpr const bool kDebugZipMapDirectly = false;
 
 using android::base::StringPrintf;
 
-uint32_t ZipEntry::GetUncompressedLength() {
-  return zip_entry_->uncompressed_length;
-}
+uint32_t ZipEntry::GetUncompressedLength() const { return zip_entry_->uncompressed_length; }
 
-uint32_t ZipEntry::GetCrc32() {
-  return zip_entry_->crc32;
-}
+uint32_t ZipEntry::GetCrc32() const { return zip_entry_->crc32; }
 
-bool ZipEntry::IsUncompressed() {
-  return zip_entry_->method == kCompressStored;
-}
+bool ZipEntry::IsUncompressed() const { return zip_entry_->method == kCompressStored; }
 
 bool ZipEntry::IsAlignedTo(size_t alignment) const {
   DCHECK(IsPowerOfTwo(alignment)) << alignment;
   return IsAlignedParam(zip_entry_->offset, static_cast<int>(alignment));
 }
 
+off_t ZipEntry::GetOffset() const { return zip_entry_->offset; }
+
 ZipEntry::~ZipEntry() {
   delete zip_entry_;
 }
@@ -61,7 +57,7 @@ ZipEntry::~ZipEntry() {
 bool ZipEntry::ExtractToFile(File& file, std::string* error_msg) {
   const int32_t error = ExtractEntryToFile(handle_, zip_entry_, file.Fd());
   if (error != 0) {
-    *error_msg = std::string(ErrorCodeString(error));
+    *error_msg = "Failed to extract '" + entry_name_ + "': " + ErrorCodeString(error);
     return false;
   }
 
@@ -95,7 +91,7 @@ MemMap ZipEntry::ExtractToMemMap(const char* zip_filename,
 bool ZipEntry::ExtractToMemory(/*out*/uint8_t* buffer, /*out*/std::string* error_msg) {
   const int32_t error = ::ExtractToMemory(handle_, zip_entry_, buffer, GetUncompressedLength());
   if (error != 0) {
-    *error_msg = std::string(ErrorCodeString(error));
+    *error_msg = "Failed to extract '" + entry_name_ + "': " + ErrorCodeString(error);
     return false;
   }
   return true;
@@ -233,10 +229,17 @@ static void SetCloseOnExec(int fd) {
 ZipArchive* ZipArchive::Open(const char* filename, std::string* error_msg) {
   DCHECK(filename != nullptr);
 
+  // Don't call into `OpenArchive` on file absence. `OpenArchive` prints a warning even if the file
+  // absence is expected.
+  if (!OS::FileExists(filename)) {
+    *error_msg = StringPrintf("Failed to open '%s': File not found", filename);
+    return nullptr;
+  }
+
   ZipArchiveHandle handle;
   const int32_t error = OpenArchive(filename, &handle);
   if (error != 0) {
-    *error_msg = std::string(ErrorCodeString(error));
+    *error_msg = StringPrintf("Failed to open '%s': %s", filename, ErrorCodeString(error));
     CloseArchive(handle);
     return nullptr;
   }
@@ -263,7 +266,8 @@ ZipArchive* ZipArchive::OpenFromMemory(const uint8_t* data,
   ZipArchiveHandle handle;
   const int32_t error = OpenArchiveFromMemory(data, size, filename, &handle);
   if (error != 0) {
-    *error_msg = std::string(ErrorCodeString(error));
+    *error_msg =
+        StringPrintf("Failed to open '%s' from memory: %s", filename, ErrorCodeString(error));
     CloseArchive(handle);
     return nullptr;
   }
@@ -281,7 +285,7 @@ ZipArchive* ZipArchive::OpenFromFdInternal(int fd,
   ZipArchiveHandle handle;
   const int32_t error = OpenArchiveFd(fd, filename, &handle, assume_ownership);
   if (error != 0) {
-    *error_msg = std::string(ErrorCodeString(error));
+    *error_msg = StringPrintf("Failed to open '%s' from fd: %s", filename, ErrorCodeString(error));
     CloseArchive(handle);
     return nullptr;
   }
@@ -291,13 +295,27 @@ ZipArchive* ZipArchive::OpenFromFdInternal(int fd,
 }
 
 ZipEntry* ZipArchive::Find(const char* name, std::string* error_msg) const {
+  return FindImpl(name, /*allow_entry_not_found=*/false, error_msg);
+}
+
+ZipEntry* ZipArchive::FindOrNull(const char* name, std::string* error_msg) const {
+  return FindImpl(name, /*allow_entry_not_found=*/true, error_msg);
+}
+
+ZipEntry* ZipArchive::FindImpl(const char* name,
+                               bool allow_entry_not_found,
+                               std::string* error_msg) const {
   DCHECK(name != nullptr);
 
   // Resist the urge to delete the space. <: is a bigraph sequence.
   std::unique_ptr< ::ZipEntry> zip_entry(new ::ZipEntry);
   const int32_t error = FindEntry(handle_, name, zip_entry.get());
   if (error != 0) {
-    *error_msg = std::string(ErrorCodeString(error));
+    // From system/libziparchive/zip_error.cpp.
+    constexpr std::string_view kEntryNotFound = "Entry not found";
+    if (!allow_entry_not_found || ErrorCodeString(error) != kEntryNotFound) {
+      *error_msg = StringPrintf("Failed to find entry '%s': %s", name, ErrorCodeString(error));
+    }
     return nullptr;
   }
 
diff --git a/libartbase/base/zip_archive.h b/libartbase/base/zip_archive.h
index e740c9f0f0..8991a572ab 100644
--- a/libartbase/base/zip_archive.h
+++ b/libartbase/base/zip_archive.h
@@ -18,11 +18,10 @@
 #define ART_LIBARTBASE_BASE_ZIP_ARCHIVE_H_
 
 #include <stdint.h>
+
 #include <memory>
 #include <string>
 
-#include <android-base/logging.h>
-
 #include "globals.h"
 #include "mem_map.h"
 #include "os.h"
@@ -67,11 +66,12 @@ class ZipEntry {
                               std::string* error_msg,
                               size_t alignment);
 
-  uint32_t GetUncompressedLength();
-  uint32_t GetCrc32();
+  uint32_t GetUncompressedLength() const;
+  uint32_t GetCrc32() const;
 
-  bool IsUncompressed();
+  bool IsUncompressed() const;
   bool IsAlignedTo(size_t alignment) const;
+  off_t GetOffset() const;
 
  private:
   ZipEntry(ZipArchiveHandle handle,
@@ -100,6 +100,10 @@ class ZipArchive {
 
   ZipEntry* Find(const char* name, std::string* error_msg) const;
 
+  // Same as Find, but doesn't return an error message if the entry is not found. The callers
+  // should expect that the returned pointer is null while the error message is empty.
+  ZipEntry* FindOrNull(const char* name, std::string* error_msg) const;
+
   ~ZipArchive();
 
  private:
@@ -110,6 +114,8 @@ class ZipArchive {
 
   explicit ZipArchive(ZipArchiveHandle handle) : handle_(handle) {}
 
+  ZipEntry* FindImpl(const char* name, bool allow_entry_not_found, std::string* error_msg) const;
+
   friend class ZipEntry;
 
   ZipArchiveHandle handle_;
diff --git a/libartbase/base/zip_archive_test.cc b/libartbase/base/zip_archive_test.cc
index 969cf1297c..f4053abc8f 100644
--- a/libartbase/base/zip_archive_test.cc
+++ b/libartbase/base/zip_archive_test.cc
@@ -64,4 +64,21 @@ TEST_F(ZipArchiveTest, FindAndExtract) {
   EXPECT_EQ(zip_entry->GetCrc32(), computed_crc);
 }
 
+TEST_F(ZipArchiveTest, FindEntryNotFound) {
+  std::string error_msg;
+  std::unique_ptr<ZipArchive> zip_archive(
+      ZipArchive::Open(GetLibCoreDexFileNames()[0].c_str(), &error_msg));
+  ASSERT_TRUE(zip_archive.get() != nullptr) << error_msg;
+  ASSERT_TRUE(error_msg.empty());
+
+  std::unique_ptr<ZipEntry> zip_entry(zip_archive->Find("non-existent-entry", &error_msg));
+  ASSERT_EQ(zip_entry, nullptr);
+  ASSERT_FALSE(error_msg.empty());
+  error_msg = "";
+
+  std::unique_ptr<ZipEntry> zip_entry_2(zip_archive->FindOrNull("non-existent-entry", &error_msg));
+  ASSERT_EQ(zip_entry, nullptr);
+  ASSERT_TRUE(error_msg.empty());
+}
+
 }  // namespace art
diff --git a/libartpalette/Android.bp b/libartpalette/Android.bp
index 89607219cc..29ce22bcc4 100644
--- a/libartpalette/Android.bp
+++ b/libartpalette/Android.bp
@@ -141,8 +141,7 @@ art_cc_defaults {
     },
 }
 
-// Version of ART gtest `art_libartpalette_tests` for host.
-// TODO(b/192274705): Remove this module when the migration to standalone ART gtests is complete.
+// Version of API coverage test for host.
 art_cc_test {
     name: "art_libartpalette_tests",
     defaults: [
@@ -153,17 +152,41 @@ art_cc_test {
     device_supported: false,
 }
 
-// Standalone version of ART gtest `art_libartpalette_tests`, not bundled with the ART APEX on
-// target.
+// MCTS test for API coverage. This test starts a VM to check the JNI
+// notification callbacks, so it should not use art_standalone_gtest_defaults,
+// which statically links a runtime via libart-gtest.
 art_cc_test {
     name: "art_standalone_libartpalette_tests",
     defaults: [
-        "art_standalone_gtest_defaults",
+        "art_standalone_test_defaults",
         "art_libartpalette_tests_defaults",
     ],
+    static_libs: [
+        "libartbase-testing",
+        "libartpalette",
+    ],
+    shared_libs: [
+        "liblog",
+        // Bypass stubs to get access to the platform-only JniInvocation APIs.
+        // They're not NDK APIs, but have the same stability requirements.
+        "libnativehelper#impl",
+    ],
     test_config_template: ":art-gtests-target-standalone-cts-template",
     test_suites: [
         "cts",
+        "general-tests",
         "mcts-art",
+        "mts-art",
     ],
+
+    // Duplicated from art_standalone_gtest_defaults
+    compile_multilib: "both",
+    multilib: {
+        lib32: {
+            suffix: "32",
+        },
+        lib64: {
+            suffix: "64",
+        },
+    },
 }
diff --git a/libartpalette/apex/palette_test.cc b/libartpalette/apex/palette_test.cc
index 47eec51df9..a72e523365 100644
--- a/libartpalette/apex/palette_test.cc
+++ b/libartpalette/apex/palette_test.cc
@@ -23,12 +23,13 @@
 
 #include <cstring>
 
-#include "base/common_art_test.h"
+#include "base/testing.h"
 #include "gtest/gtest.h"
 
 #ifdef ART_TARGET_ANDROID
 #include "android-modules-utils/sdk_level.h"
 #include "android/api-level.h"
+#include "nativehelper/JniInvocation.h"
 #endif
 
 namespace {
@@ -58,7 +59,7 @@ bool PaletteDebugStoreIsSupported() {
 
 }  // namespace
 
-class PaletteClientTest : public testing::Test {};
+class PaletteClientTest : public ::testing::Test {};
 
 TEST_F(PaletteClientTest, SchedPriority) {
   int32_t tid = GetTid();
@@ -92,16 +93,25 @@ TEST_F(PaletteClientTest, Ashmem) {
 #endif
 }
 
-class PaletteClientJniTest : public art::CommonArtTest {};
-
-TEST_F(PaletteClientJniTest, JniInvocation) {
+TEST_F(PaletteClientTest, JniInvocation) {
+#ifndef ART_TARGET_ANDROID
+  // On host we need to use the runtime linked into the test to start a VM (e.g.
+  // by inheriting CommonArtTest), while on device it needs to launch the
+  // runtime through libnativehelper. Let's not bother on host since this test
+  // is only for native API coverage on device.
+  GTEST_SKIP() << "Will only spin up a VM on Android";
+#else
   bool enabled;
   EXPECT_EQ(PALETTE_STATUS_OK, PaletteShouldReportJniInvocations(&enabled));
 
+  // Load the default JNI_CreateJavaVM implementation, i.e., libart.so.
+  JniInvocation jni_invocation;
+  ASSERT_TRUE(jni_invocation.Init(/*library=*/ nullptr));
+
   std::string boot_class_path_string =
-      GetClassPathOption("-Xbootclasspath:", GetLibCoreDexFileNames());
-  std::string boot_class_path_locations_string =
-      GetClassPathOption("-Xbootclasspath-locations:", GetLibCoreDexLocations());
+      art::testing::GetClassPathOption("-Xbootclasspath:", art::testing::GetLibCoreDexFileNames());
+  std::string boot_class_path_locations_string = art::testing::GetClassPathOption(
+      "-Xbootclasspath-locations:", art::testing::GetLibCoreDexLocations());
 
   JavaVMOption options[] = {
       {.optionString = boot_class_path_string.c_str(), .extraInfo = nullptr},
@@ -123,6 +133,7 @@ TEST_F(PaletteClientJniTest, JniInvocation) {
   PaletteNotifyEndJniInvocation(env);
 
   EXPECT_EQ(JNI_OK, jvm->DestroyJavaVM());
+#endif
 }
 
 TEST_F(PaletteClientTest, SetTaskProfiles) {
@@ -177,7 +188,7 @@ TEST_F(PaletteClientTest, DebugStore) {
   EXPECT_TRUE(len < result.size());
 
   const char* start = "1,0,";
-  const char* end = "::";
+  const char* end = "::;;";
   EXPECT_TRUE(len > strlen(start) + strlen(end));
   EXPECT_EQ(strncmp(result.data() + len - strlen(end), end, strlen(end)), 0);
 #endif
diff --git a/libartpalette/system/palette_fake.cc b/libartpalette/system/palette_fake.cc
index f2fc76d68c..444ea0941c 100644
--- a/libartpalette/system/palette_fake.cc
+++ b/libartpalette/system/palette_fake.cc
@@ -151,5 +151,6 @@ palette_status_t PaletteSetTaskProfiles([[maybe_unused]] int32_t tid,
 // Methods in version 4 API, corresponding to SDK level 36.
 palette_status_t PaletteDebugStoreGetString([[maybe_unused]] char* result,
                                             [[maybe_unused]] size_t max_size) {
+  result[0] = '\0';
   return PALETTE_STATUS_OK;
 }
diff --git a/libartservice/service/Android.bp b/libartservice/service/Android.bp
index 9df3d4ef7b..e1a16197b0 100644
--- a/libartservice/service/Android.bp
+++ b/libartservice/service/Android.bp
@@ -102,6 +102,7 @@ java_defaults {
     ],
     static_libs: [
         "android.content.pm.flags-aconfig-java-export",
+        "android.os.flags-aconfig-java-export",
         "art-statslog-art-java",
         "artd-aidl-java",
         "dexopt_chroot_setup-aidl-java",
@@ -256,6 +257,7 @@ android_test {
         "androidx.test.ext.truth",
         "androidx.test.runner",
         "artd-aidl-java",
+        "flag-junit",
         "framework-annotations-lib",
         // We need ExtendedMockito to mock static methods.
         "mockito-target-extended-minus-junit4",
diff --git a/libartservice/service/README.internal.md b/libartservice/service/README.internal.md
new file mode 100644
index 0000000000..690b25355f
--- /dev/null
+++ b/libartservice/service/README.internal.md
@@ -0,0 +1,56 @@
+# ART Service internal doc
+
+Warning: The contents in this doc can become stale while the code evolves.
+
+## Pre-reboot Dexopt
+
+Pre-reboot Dexopt is a successor of otapreopt, available on Android V+.
+
+### On Mainline update
+
+On Mainline update, `ArtManagerLocal.onApexStaged` is called. The method
+schedules an asynchronous job and returns immediately. Later, when the device is
+idle and charging, the job will be run by the job scheduler.
+
+### On OTA update
+
+On Mainline update, the shell command `pm art on-ota-staged` is called. The
+behavior depends on the platform version and the configuration.
+
+- On Android V
+
+  By default, Pre-reboot Dexopt runs in synchronous mode from the postinstall
+  script while update_engine keeps the snapshot devices mapped. The command
+  blocks until Pre-reboot Dexopt finishes.
+
+- On Android V, with asynchronous mode enabled
+
+  The asynchronous mode can be enabled by
+  `dalvik.vm.pr_dexopt_async_for_ota=true`. In this case, the command schedules
+  an asynchronous job and returns immediately. Later, when the device is idle
+  and charging, the job will be run by the job scheduler. The job uses
+  `snapshotctl` to map snapshot devices.
+
+  Note that this mode has a risk of racing with update_engine on snapshot
+  devices. Particularly, if update_engine wants to unmap snapshot devices, to
+  revoke an OTA update, the job may be running and preventing update_engine from
+  successfully doing so.
+
+- On Android B+
+
+  Pre-reboot Dexopt is always in asynchronous mode. The command schedules an
+  asynchronous job and returns immediately. Later, when the device is idle and
+  charging, the job will be run by the job scheduler. The job will call
+  `UpdateEngine.triggerPostinstall` to ask update_engine to map snapshot
+  devices, and update_engine will call this command again with '--start' through
+  the postinstall script, to notify the job that the snapshot devices are ready.
+
+### On shell command
+
+Pre-reboot Dexopt can be triggered by a shell command `pm art pr-dexopt-job`.
+It is synchronous if called with `--run`, and is asynchronous if called with
+`--schedule`.
+
+Regardless of being synchronous or asynchronous, it always tries to map snapshot
+devices if called with `--slot`. On Android V, it does so through `snapshotctl`,
+and on Android B+, it does so through `UpdateEngine.triggerPostinstall`.
diff --git a/libartservice/service/jarjar-rules.txt b/libartservice/service/jarjar-rules.txt
index 014ff22579..cb25669b72 100644
--- a/libartservice/service/jarjar-rules.txt
+++ b/libartservice/service/jarjar-rules.txt
@@ -4,5 +4,10 @@ rule android.content.pm.FakeFeatureFlagsImpl com.android.server.art.jarjar.@0
 rule android.content.pm.FeatureFlags com.android.server.art.jarjar.@0
 rule android.content.pm.FeatureFlagsImpl com.android.server.art.jarjar.@0
 rule android.content.pm.Flags com.android.server.art.jarjar.@0
+rule android.os.CustomFeatureFlags com.android.server.art.jarjar.@0
+rule android.os.FakeFeatureFlagsImpl com.android.server.art.jarjar.@0
+rule android.os.FeatureFlags com.android.server.art.jarjar.@0
+rule android.os.FeatureFlagsImpl com.android.server.art.jarjar.@0
+rule android.os.Flags com.android.server.art.jarjar.@0
 rule com.android.modules.utils.** com.android.server.art.jarjar.@0
 rule com.google.protobuf.** com.android.server.art.jarjar.@0
diff --git a/libartservice/service/java/com/android/server/art/AidlUtils.java b/libartservice/service/java/com/android/server/art/AidlUtils.java
index 055014fb43..0932087289 100644
--- a/libartservice/service/java/com/android/server/art/AidlUtils.java
+++ b/libartservice/service/java/com/android/server/art/AidlUtils.java
@@ -145,13 +145,20 @@ public final class AidlUtils {
     }
 
     @NonNull
-    public static ProfilePath buildProfilePathForPrimaryCur(
+    public static PrimaryCurProfilePath buildPrimaryCurProfilePath(
             int userId, @NonNull String packageName, @NonNull String profileName) {
         var primaryCurProfilePath = new PrimaryCurProfilePath();
         primaryCurProfilePath.userId = userId;
         primaryCurProfilePath.packageName = packageName;
         primaryCurProfilePath.profileName = profileName;
-        return ProfilePath.primaryCurProfilePath(primaryCurProfilePath);
+        return primaryCurProfilePath;
+    }
+
+    @NonNull
+    public static ProfilePath buildProfilePathForPrimaryCur(
+            int userId, @NonNull String packageName, @NonNull String profileName) {
+        return ProfilePath.primaryCurProfilePath(
+                buildPrimaryCurProfilePath(userId, packageName, profileName));
     }
 
     @NonNull
@@ -215,6 +222,26 @@ public final class AidlUtils {
         return runtimeArtifactsPath;
     }
 
+    @NonNull
+    public static SecureDexMetadataWithCompanionPaths buildSecureDexMetadataWithCompanionPaths(
+            @NonNull String dexPath, @NonNull String isa, boolean isInDalvikCache) {
+        var paths = new SecureDexMetadataWithCompanionPaths();
+        paths.dexPath = dexPath;
+        paths.isa = isa;
+        paths.isInDalvikCache = isInDalvikCache;
+        return paths;
+    }
+
+    @NonNull
+    public static OutputSecureDexMetadataCompanion buildOutputSecureDexMetadataCompanion(
+            @NonNull String dexPath, @NonNull String isa, boolean isInDalvikCache,
+            @NonNull PermissionSettings permissionSettings) {
+        var outputSdc = new OutputSecureDexMetadataCompanion();
+        outputSdc.sdcPath = buildSecureDexMetadataWithCompanionPaths(dexPath, isa, isInDalvikCache);
+        outputSdc.permissionSettings = permissionSettings;
+        return outputSdc;
+    }
+
     @NonNull
     public static WritableProfilePath toWritableProfilePath(@NonNull ProfilePath profile) {
         switch (profile.getTag()) {
@@ -281,4 +308,11 @@ public final class AidlUtils {
                         + "got " + profile.getTag());
         }
     }
+
+    @NonNull
+    public static String toString(@NonNull SecureDexMetadataWithCompanionPaths paths) {
+        return String.format(
+                "SecureDexMetadataWithCompanionPaths[dexPath = %s, isa = %s, isInDalvikCache = %b]",
+                paths.dexPath, paths.isa, paths.isInDalvikCache);
+    }
 }
diff --git a/libartservice/service/java/com/android/server/art/ArtFileManager.java b/libartservice/service/java/com/android/server/art/ArtFileManager.java
index 754b9ec1dd..534952a304 100644
--- a/libartservice/service/java/com/android/server/art/ArtFileManager.java
+++ b/libartservice/service/java/com/android/server/art/ArtFileManager.java
@@ -45,7 +45,6 @@ import dalvik.system.DexFile;
 import com.google.auto.value.AutoValue;
 
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Objects;
 
@@ -102,6 +101,7 @@ public class ArtFileManager {
     public WritableArtifactLists getWritableArtifacts(@NonNull PackageState pkgState,
             @NonNull AndroidPackage pkg, @NonNull Options options) throws RemoteException {
         List<ArtifactsPath> artifacts = new ArrayList<>();
+        List<SecureDexMetadataWithCompanionPaths> sdmFiles = new ArrayList<>();
         List<RuntimeArtifactsPath> runtimeArtifacts = new ArrayList<>();
 
         if (options.forPrimaryDex()) {
@@ -110,6 +110,9 @@ public class ArtFileManager {
                 for (Abi abi : Utils.getAllAbis(pkgState)) {
                     artifacts.add(AidlUtils.buildArtifactsPathAsInput(
                             dexInfo.dexPath(), abi.isa(), isInDalvikCache));
+                    // SDM files are only for primary dex files.
+                    sdmFiles.add(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                            dexInfo.dexPath(), abi.isa(), isInDalvikCache));
                     // Runtime images are only generated for primary dex files.
                     runtimeArtifacts.add(AidlUtils.buildRuntimeArtifactsPath(
                             pkgState.getPackageName(), dexInfo.dexPath(), abi.isa()));
@@ -126,7 +129,7 @@ public class ArtFileManager {
             }
         }
 
-        return WritableArtifactLists.create(artifacts, runtimeArtifacts);
+        return new WritableArtifactLists(artifacts, sdmFiles, runtimeArtifacts);
     }
 
     /** Returns artifacts that are usable, regardless of whether they are writable. */
@@ -135,6 +138,7 @@ public class ArtFileManager {
             @NonNull PackageState pkgState, @NonNull AndroidPackage pkg) throws RemoteException {
         List<ArtifactsPath> artifacts = new ArrayList<>();
         List<VdexPath> vdexFiles = new ArrayList<>();
+        List<SecureDexMetadataWithCompanionPaths> sdmFiles = new ArrayList<>();
         List<RuntimeArtifactsPath> runtimeArtifacts = new ArrayList<>();
 
         var options = ArtFileManager.Options.builder()
@@ -159,9 +163,30 @@ public class ArtFileManager {
                     } else {
                         artifacts.add(thisArtifacts);
                     }
+                } else if (result.artifactsLocation == ArtifactsLocation.SDM_DALVIK_CACHE
+                        || result.artifactsLocation == ArtifactsLocation.SDM_NEXT_TO_DEX) {
+                    sdmFiles.add(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                            dexInfo.dexPath(), abi.isa(),
+                            result.artifactsLocation == ArtifactsLocation.SDM_DALVIK_CACHE));
+                }
+
+                if (result.artifactsLocation != ArtifactsLocation.NONE_OR_ERROR) {
                     // Runtime images are only generated for primary dex files.
                     if (dexInfo instanceof DetailedPrimaryDexInfo
                             && !DexFile.isOptimizedCompilerFilter(result.compilerFilter)) {
+                        // Those not added to the list are definitely unusable, but those added to
+                        // the list are not necessarily usable. For example, runtime artifacts can
+                        // be outdated when the corresponding dex file is updated, but they may
+                        // still show up in this list.
+                        //
+                        // However, this is not a severe problem. For `ArtManagerLocal.cleanup`, the
+                        // worst result is only that we are keeping more runtime artifacts than
+                        // needed. For `ArtManagerLocal.getArtManagedFileStats`, this is an edge
+                        // case because the API call is transitively initiated by the app itself,
+                        // and the runtime refreshes unusable runtime artifacts as soon as the app
+                        // starts.
+                        //
+                        // TODO(jiakaiz): Improve this.
                         runtimeArtifacts.add(AidlUtils.buildRuntimeArtifactsPath(
                                 pkgState.getPackageName(), dexInfo.dexPath(), abi.isa()));
                     }
@@ -176,7 +201,7 @@ public class ArtFileManager {
             }
         }
 
-        return UsableArtifactLists.create(artifacts, vdexFiles, runtimeArtifacts);
+        return new UsableArtifactLists(artifacts, vdexFiles, sdmFiles, runtimeArtifacts);
     }
 
     @NonNull
@@ -209,7 +234,7 @@ public class ArtFileManager {
             }
         }
 
-        return ProfileLists.create(refProfiles, curProfiles);
+        return new ProfileLists(refProfiles, curProfiles);
     }
 
     @NonNull
@@ -221,71 +246,17 @@ public class ArtFileManager {
                 : mInjector.getDexUseManager().getSecondaryDexInfo(pkgState.getPackageName());
     }
 
-    @Immutable
-    @AutoValue
-    @SuppressWarnings("AutoValueImmutableFields") // Can't use ImmutableList because it's in Guava.
-    public abstract static class WritableArtifactLists {
-        protected WritableArtifactLists() {}
-
-        public static @NonNull WritableArtifactLists create(@NonNull List<ArtifactsPath> artifacts,
-                @NonNull List<RuntimeArtifactsPath> runtimeArtifacts) {
-            return new AutoValue_ArtFileManager_WritableArtifactLists(
-                    Collections.unmodifiableList(artifacts),
-                    Collections.unmodifiableList(runtimeArtifacts));
-        }
-
-        public abstract @NonNull List<ArtifactsPath> artifacts();
-        public abstract @NonNull List<RuntimeArtifactsPath> runtimeArtifacts();
-    }
-
-    @Immutable
-    @AutoValue
-    @SuppressWarnings("AutoValueImmutableFields") // Can't use ImmutableList because it's in Guava.
-    public abstract static class UsableArtifactLists {
-        protected UsableArtifactLists() {}
-
-        public static @NonNull UsableArtifactLists create(@NonNull List<ArtifactsPath> artifacts,
-                @NonNull List<VdexPath> vdexFiles,
-                @NonNull List<RuntimeArtifactsPath> runtimeArtifacts) {
-            return new AutoValue_ArtFileManager_UsableArtifactLists(
-                    Collections.unmodifiableList(artifacts),
-                    Collections.unmodifiableList(vdexFiles),
-                    Collections.unmodifiableList(runtimeArtifacts));
-        }
-
-        public abstract @NonNull List<ArtifactsPath> artifacts();
-        public abstract @NonNull List<VdexPath> vdexFiles();
-
-        // Those not added to the list are definitely unusable, but those added to the list are not
-        // necessarily usable. For example, runtime artifacts can be outdated when the corresponding
-        // dex file is updated, but they may still show up in this list.
-        //
-        // However, this is not a severe problem. For `ArtManagerLocal.cleanup`, the worst result is
-        // only that we are keeping more runtime artifacts than needed. For
-        // `ArtManagerLocal.getArtManagedFileStats`, this is an edge case because the API call is
-        // transitively initiated by the app itself, and the runtime refreshes unusable runtime
-        // artifacts as soon as the app starts.
-        //
-        // TODO(jiakaiz): Improve this.
-        public abstract @NonNull List<RuntimeArtifactsPath> runtimeArtifacts();
-    }
-
-    @Immutable
-    @AutoValue
-    @SuppressWarnings("AutoValueImmutableFields") // Can't use ImmutableList because it's in Guava.
-    public abstract static class ProfileLists {
-        protected ProfileLists() {}
-
-        public static @NonNull ProfileLists create(
-                @NonNull List<ProfilePath> refProfiles, @NonNull List<ProfilePath> curProfiles) {
-            return new AutoValue_ArtFileManager_ProfileLists(
-                    Collections.unmodifiableList(refProfiles),
-                    Collections.unmodifiableList(curProfiles));
-        }
+    public record WritableArtifactLists(@NonNull List<ArtifactsPath> artifacts,
+            @NonNull List<SecureDexMetadataWithCompanionPaths> sdmFiles,
+            @NonNull List<RuntimeArtifactsPath> runtimeArtifacts) {}
 
-        public abstract @NonNull List<ProfilePath> refProfiles();
-        public abstract @NonNull List<ProfilePath> curProfiles();
+    public record UsableArtifactLists(@NonNull List<ArtifactsPath> artifacts,
+            @NonNull List<VdexPath> vdexFiles,
+            @NonNull List<SecureDexMetadataWithCompanionPaths> sdmFiles,
+            @NonNull List<RuntimeArtifactsPath> runtimeArtifacts) {}
 
+    public record ProfileLists(
+            @NonNull List<ProfilePath> refProfiles, @NonNull List<ProfilePath> curProfiles) {
         public @NonNull List<ProfilePath> allProfiles() {
             List<ProfilePath> profiles = new ArrayList<>();
             profiles.addAll(refProfiles());
diff --git a/libartservice/service/java/com/android/server/art/ArtManagedInstallFileHelper.java b/libartservice/service/java/com/android/server/art/ArtManagedInstallFileHelper.java
index dd040165e3..53d88c2588 100644
--- a/libartservice/service/java/com/android/server/art/ArtManagedInstallFileHelper.java
+++ b/libartservice/service/java/com/android/server/art/ArtManagedInstallFileHelper.java
@@ -25,9 +25,12 @@ import androidx.annotation.RequiresApi;
 
 import com.android.art.flags.Flags;
 
+import java.nio.file.Path;
+import java.nio.file.Paths;
 import java.util.List;
 import java.util.Set;
 import java.util.stream.Collectors;
+import java.util.stream.Stream;
 
 /**
  * Helper class for <i>ART-managed install files</i> (files installed by Package Manager
@@ -41,6 +44,11 @@ import java.util.stream.Collectors;
 public final class ArtManagedInstallFileHelper {
     private static final List<String> FILE_TYPES = List.of(ArtConstants.DEX_METADATA_FILE_EXT,
             ArtConstants.PROFILE_FILE_EXT, ArtConstants.SECURE_DEX_METADATA_FILE_EXT);
+    private static final List<String> SDM_SUFFIXES =
+            Utils.getNativeIsas()
+                    .stream()
+                    .map(isa -> "." + isa + ArtConstants.SECURE_DEX_METADATA_FILE_EXT)
+                    .toList();
 
     private ArtManagedInstallFileHelper() {}
 
@@ -64,9 +72,14 @@ public final class ArtManagedInstallFileHelper {
     @FlaggedApi(Flags.FLAG_ART_SERVICE_V3)
     public static @NonNull List<String> filterPathsForApk(
             @NonNull List<String> paths, @NonNull String apkPath) {
-        Set<String> candidates = FILE_TYPES.stream()
-                                         .map(ext -> Utils.replaceFileExtension(apkPath, ext))
-                                         .collect(Collectors.toSet());
+        Set<String> candidates =
+                FILE_TYPES.stream()
+                        .flatMap(ext
+                                -> ext.equals(ArtConstants.SECURE_DEX_METADATA_FILE_EXT)
+                                        ? SDM_SUFFIXES.stream().map(suffix
+                                                  -> Utils.replaceFileExtension(apkPath, suffix))
+                                        : Stream.of(Utils.replaceFileExtension(apkPath, ext)))
+                        .collect(Collectors.toSet());
         return paths.stream().filter(path -> candidates.contains(path)).toList();
     }
 
@@ -84,10 +97,23 @@ public final class ArtManagedInstallFileHelper {
     public static @NonNull String getTargetPathForApk(
             @NonNull String originalPath, @NonNull String apkPath) {
         for (String ext : FILE_TYPES) {
-            if (originalPath.endsWith(ext)) {
+            if (!ext.equals(ArtConstants.SECURE_DEX_METADATA_FILE_EXT)
+                    && originalPath.endsWith(ext)) {
                 return Utils.replaceFileExtension(apkPath, ext);
             }
         }
+        if (originalPath.endsWith(ArtConstants.SECURE_DEX_METADATA_FILE_EXT)) {
+            for (String suffix : SDM_SUFFIXES) {
+                if (originalPath.endsWith(suffix)) {
+                    return Utils.replaceFileExtension(apkPath, suffix);
+                }
+            }
+            AsLog.w("SDM filename '" + originalPath
+                    + "' does not contain a valid instruction set name");
+            Path dirname = Paths.get(apkPath).getParent();
+            Path basename = Paths.get(originalPath).getFileName();
+            return (dirname != null ? dirname.resolve(basename) : basename).toString();
+        }
         throw new IllegalArgumentException(
                 "Illegal ART managed install file path '" + originalPath + "'");
     }
diff --git a/libartservice/service/java/com/android/server/art/ArtManagerLocal.java b/libartservice/service/java/com/android/server/art/ArtManagerLocal.java
index 0afdf03d9c..993cb2c558 100644
--- a/libartservice/service/java/com/android/server/art/ArtManagerLocal.java
+++ b/libartservice/service/java/com/android/server/art/ArtManagerLocal.java
@@ -16,12 +16,15 @@
 
 package com.android.server.art;
 
+import static android.app.ActivityManager.RunningAppProcessInfo;
+
 import static com.android.server.art.ArtFileManager.ProfileLists;
 import static com.android.server.art.ArtFileManager.UsableArtifactLists;
 import static com.android.server.art.ArtFileManager.WritableArtifactLists;
 import static com.android.server.art.DexMetadataHelper.DexMetadataInfo;
 import static com.android.server.art.PrimaryDexUtils.DetailedPrimaryDexInfo;
 import static com.android.server.art.PrimaryDexUtils.PrimaryDexInfo;
+import static com.android.server.art.ProfilePath.PrimaryCurProfilePath;
 import static com.android.server.art.ProfilePath.WritableProfilePath;
 import static com.android.server.art.ReasonMapping.BatchDexoptReason;
 import static com.android.server.art.ReasonMapping.BootReason;
@@ -37,6 +40,7 @@ import android.annotation.NonNull;
 import android.annotation.Nullable;
 import android.annotation.SystemApi;
 import android.annotation.SystemService;
+import android.app.ActivityManager;
 import android.app.job.JobInfo;
 import android.apphibernation.AppHibernationManager;
 import android.content.BroadcastReceiver;
@@ -54,6 +58,9 @@ import android.os.SystemProperties;
 import android.os.UserHandle;
 import android.os.UserManager;
 import android.os.storage.StorageManager;
+import android.system.ErrnoException;
+import android.system.Os;
+import android.system.OsConstants;
 import android.text.TextUtils;
 import android.util.Pair;
 
@@ -106,7 +113,6 @@ import java.util.concurrent.ThreadPoolExecutor;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 import java.util.function.Consumer;
-import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
 /**
@@ -184,7 +190,7 @@ public final class ArtManagerLocal {
     public int handleShellCommand(@NonNull Binder target, @NonNull ParcelFileDescriptor in,
             @NonNull ParcelFileDescriptor out, @NonNull ParcelFileDescriptor err,
             @NonNull String[] args) {
-        return new ArtShellCommand(this, mInjector.getPackageManagerLocal(), mInjector.getContext())
+        return new ArtShellCommand(this, mInjector.getPackageManagerLocal())
                 .exec(target, in.getFileDescriptor(), out.getFileDescriptor(),
                         err.getFileDescriptor(), args);
     }
@@ -196,8 +202,8 @@ public final class ArtManagerLocal {
     }
 
     /**
-     * Deletes dexopt artifacts of a package, including the artifacts for primary dex files and the
-     * ones for secondary dex files. This includes VDEX, ODEX, and ART files.
+     * Deletes dexopt artifacts (including cloud dexopt artifacts) of a package, for primary dex
+     * files and for secondary dex files. This includes VDEX, ODEX, ART, SDM, and SDC files.
      *
      * Also deletes runtime artifacts of the package, though they are not dexopt artifacts.
      *
@@ -226,6 +232,9 @@ public final class ArtManagerLocal {
             for (RuntimeArtifactsPath runtimeArtifacts : list.runtimeArtifacts()) {
                 freedBytes += mInjector.getArtd().deleteRuntimeArtifacts(runtimeArtifacts);
             }
+            for (SecureDexMetadataWithCompanionPaths sdmSdcFiles : list.sdmFiles()) {
+                freedBytes += mInjector.getArtd().deleteSdmSdcFiles(sdmSdcFiles);
+            }
             return DeleteResult.create(freedBytes);
         } catch (RemoteException e) {
             Utils.logArtdException(e);
@@ -384,13 +393,17 @@ public final class ArtManagerLocal {
     }
 
     /**
-     * Resets the dexopt state of the package as if the package is newly installed.
+     * Resets the dexopt state of the package as if the package is newly installed without cloud
+     * dexopt artifacts (SDM files).
      *
-     * More specifically, it clears reference profiles, current profiles, any code compiled from
-     * those local profiles, and runtime artifacts. If there is an external profile (e.g., a cloud
-     * profile), the code compiled from that profile will be kept.
+     * More specifically,
+     * - It clears current profiles, reference profiles, and all dexopt artifacts (including cloud
+     *   dexopt artifacts).
+     * - If there is an external profile (e.g., a cloud profile), the reference profile will be
+     *   re-created from that profile, and dexopt artifacts will be regenerated for that profile.
      *
-     * For secondary dex files, it also clears all dexopt artifacts.
+     * For secondary dex files, it clears all profiles and dexopt artifacts without regeneration
+     * because secondary dex files are supposed to be unknown at install time.
      *
      * @hide
      */
@@ -889,7 +902,7 @@ public final class ArtManagerLocal {
                                         .map(envVar -> Constants.getenv(envVar))
                                         .filter(classpath -> !TextUtils.isEmpty(classpath))
                                         .flatMap(classpath -> Arrays.stream(classpath.split(":")))
-                                        .collect(Collectors.toList());
+                                        .toList();
 
         var options = new MergeProfileOptions();
         options.forceMerge = true;
@@ -996,19 +1009,8 @@ public final class ArtManagerLocal {
     @RequiresApi(Build.VERSION_CODES.UPSIDE_DOWN_CAKE)
     public void dump(
             @NonNull PrintWriter pw, @NonNull PackageManagerLocal.FilteredSnapshot snapshot) {
-        dump(pw, snapshot, false /* verifySdmSignatures */);
-    }
-
-    /**
-     * Same as above, but allows to specify options.
-     *
-     * @hide
-     */
-    @RequiresApi(Build.VERSION_CODES.UPSIDE_DOWN_CAKE)
-    public void dump(@NonNull PrintWriter pw,
-            @NonNull PackageManagerLocal.FilteredSnapshot snapshot, boolean verifySdmSignatures) {
         try (var pin = mInjector.createArtdPin()) {
-            new DumpHelper(this).dump(pw, snapshot, verifySdmSignatures);
+            new DumpHelper(this).dump(pw, snapshot);
         }
     }
 
@@ -1024,21 +1026,9 @@ public final class ArtManagerLocal {
     @RequiresApi(Build.VERSION_CODES.UPSIDE_DOWN_CAKE)
     public void dumpPackage(@NonNull PrintWriter pw,
             @NonNull PackageManagerLocal.FilteredSnapshot snapshot, @NonNull String packageName) {
-        dumpPackage(pw, snapshot, packageName, false /* verifySdmSignatures */);
-    }
-
-    /**
-     * Same as above, but allows to specify options.
-     *
-     * @hide
-     */
-    @RequiresApi(Build.VERSION_CODES.UPSIDE_DOWN_CAKE)
-    public void dumpPackage(@NonNull PrintWriter pw,
-            @NonNull PackageManagerLocal.FilteredSnapshot snapshot, @NonNull String packageName,
-            boolean verifySdmSignatures) {
         try (var pin = mInjector.createArtdPin()) {
-            new DumpHelper(this).dumpPackage(pw, snapshot,
-                    Utils.getPackageStateOrThrow(snapshot, packageName), verifySdmSignatures);
+            new DumpHelper(this).dumpPackage(
+                    pw, snapshot, Utils.getPackageStateOrThrow(snapshot, packageName));
         }
     }
 
@@ -1073,6 +1063,10 @@ public final class ArtManagerLocal {
             for (RuntimeArtifactsPath runtimeArtifacts : artifactLists.runtimeArtifacts()) {
                 artifactsSize += artd.getRuntimeArtifactsSize(runtimeArtifacts);
             }
+            for (SecureDexMetadataWithCompanionPaths sdmFile : artifactLists.sdmFiles()) {
+                // We don't count SDC files because they are presumed to be tiny.
+                artifactsSize += artd.getSdmFileSize(sdmFile);
+            }
 
             ProfileLists profileLists = mInjector.getArtFileManager().getProfiles(pkgState, pkg,
                     ArtFileManager.Options.builder()
@@ -1136,6 +1130,7 @@ public final class ArtManagerLocal {
             // - The dexopt artifacts, if they are up-to-date and the app is not hibernating.
             // - Only the VDEX part of the dexopt artifacts, if the dexopt artifacts are outdated
             //   but the VDEX part is still usable and the app is not hibernating.
+            // - The SDM and SDC files, if they are up-to-date and the app is not hibernating.
             // - The runtime artifacts, if dexopt artifacts are fully or partially usable and the
             //   usable parts don't contain AOT-compiled code. (This logic must be aligned with the
             //   one that determines when runtime images can be loaded in
@@ -1143,6 +1138,7 @@ public final class ArtManagerLocal {
             List<ProfilePath> profilesToKeep = new ArrayList<>();
             List<ArtifactsPath> artifactsToKeep = new ArrayList<>();
             List<VdexPath> vdexFilesToKeep = new ArrayList<>();
+            List<SecureDexMetadataWithCompanionPaths> sdmSdcFilesToKeep = new ArrayList<>();
             List<RuntimeArtifactsPath> runtimeArtifactsToKeep = new ArrayList<>();
 
             for (PackageState pkgState : snapshot.getPackageStates().values()) {
@@ -1163,11 +1159,12 @@ public final class ArtManagerLocal {
                             mInjector.getArtFileManager().getUsableArtifacts(pkgState, pkg);
                     artifactsToKeep.addAll(artifactLists.artifacts());
                     vdexFilesToKeep.addAll(artifactLists.vdexFiles());
+                    sdmSdcFilesToKeep.addAll(artifactLists.sdmFiles());
                     runtimeArtifactsToKeep.addAll(artifactLists.runtimeArtifacts());
                 }
             }
             return mInjector.getArtd().cleanup(profilesToKeep, artifactsToKeep, vdexFilesToKeep,
-                    runtimeArtifactsToKeep,
+                    sdmSdcFilesToKeep, runtimeArtifactsToKeep,
                     SdkLevel.isAtLeastV() && mInjector.getPreRebootDexoptJob().hasStarted());
         } catch (RemoteException e) {
             Utils.logArtdException(e);
@@ -1208,7 +1205,7 @@ public final class ArtManagerLocal {
                                                              .refProfiles()
                                                              .stream()
                                                              .map(AidlUtils::toWritableProfilePath)
-                                                             .collect(Collectors.toList());
+                                                             .toList();
                 try {
                     // The artd method commits all files somewhat transactionally. Here, we are
                     // committing files transactionally at the package level just for simplicity. In
@@ -1233,6 +1230,57 @@ public final class ArtManagerLocal {
         }
     }
 
+    /**
+     * Forces all running processes of the given package to flush profiles to the disk.
+     *
+     * @return true on success; false on timeout or artd crash.
+     *
+     * @hide
+     */
+    @RequiresApi(Build.VERSION_CODES.UPSIDE_DOWN_CAKE)
+    public boolean flushProfiles(
+            @NonNull PackageManagerLocal.FilteredSnapshot snapshot, @NonNull String packageName) {
+        PackageState pkgState = Utils.getPackageStateOrThrow(snapshot, packageName);
+        List<RunningAppProcessInfo> infoList =
+                Utils.getRunningProcessInfoForPackage(mInjector.getActivityManager(), pkgState);
+
+        try (var pin = mInjector.createArtdPin()) {
+            boolean success = true;
+            for (RunningAppProcessInfo info : infoList) {
+                PrimaryCurProfilePath profilePath = AidlUtils.buildPrimaryCurProfilePath(
+                        UserHandle.getUserHandleForUid(info.uid).getIdentifier(), packageName,
+                        PrimaryDexUtils.getProfileName(null /* splitName */));
+                IArtdNotification notification =
+                        mInjector.getArtd().initProfileSaveNotification(profilePath, info.pid);
+
+                // Check if the process is still there.
+                if (!Utils.getRunningProcessInfoForPackage(mInjector.getActivityManager(), pkgState)
+                                .stream()
+                                .anyMatch(running_info -> running_info.pid == info.pid)) {
+                    continue;
+                }
+
+                // Send signal and wait one by one, to avoid the race among processes on the same
+                // profile file.
+                try {
+                    mInjector.kill(info.pid, OsConstants.SIGUSR1);
+                    success &= notification.wait(1000 /* timeoutMs */);
+                } catch (ErrnoException | ServiceSpecificException e) {
+                    if (e instanceof ErrnoException ee) {
+                        if (ee.errno == OsConstants.ESRCH) {
+                            continue;
+                        }
+                    }
+                    AsLog.w("Failed to flush profile on pid " + info.pid, e);
+                }
+            }
+            return success;
+        } catch (RemoteException e) {
+            Utils.logArtdException(e);
+            return false;
+        }
+    }
+
     /**
      * Should be used by {@link BackgroundDexoptJobService} ONLY.
      *
@@ -1267,7 +1315,7 @@ public final class ArtManagerLocal {
             List<String> packages = getDefaultPackages(snapshot, ReasonMapping.REASON_INACTIVE)
                                             .stream()
                                             .filter(pkg -> !excludedPackages.contains(pkg))
-                                            .collect(Collectors.toList());
+                                            .toList();
             if (!packages.isEmpty()) {
                 AsLog.i("Storage is low. Downgrading " + packages.size() + " inactive packages");
                 DexoptParams params =
@@ -1314,14 +1362,14 @@ public final class ArtManagerLocal {
                         .stream()
                         .filter(packageResult
                                 -> packageResult.getDexContainerFileDexoptResults()
-                                           .stream()
-                                           .anyMatch(fileResult
-                                                   -> DexFile.isProfileGuidedCompilerFilter(
-                                                              fileResult.getActualCompilerFilter())
-                                                           && fileResult.getStatus()
-                                                                   == DexoptResult.DEXOPT_SKIPPED))
+                                        .stream()
+                                        .anyMatch(fileResult
+                                                -> DexFile.isProfileGuidedCompilerFilter(
+                                                           fileResult.getActualCompilerFilter())
+                                                        && fileResult.getStatus()
+                                                                == DexoptResult.DEXOPT_SKIPPED))
                         .map(packageResult -> packageResult.getPackageName())
-                        .collect(Collectors.toList());
+                        .toList();
 
         DexoptParams dexoptParams = mainParams.toBuilder()
                                             .setFlags(ArtFlags.FLAG_FORCE_MERGE_PROFILE,
@@ -1370,7 +1418,7 @@ public final class ArtManagerLocal {
                         packages, true /* keepRecent */, true /* descending */);
         }
 
-        return packages.map(PackageState::getPackageName).collect(Collectors.toList());
+        return packages.map(PackageState::getPackageName).toList();
     }
 
     @RequiresApi(Build.VERSION_CODES.UPSIDE_DOWN_CAKE)
@@ -1622,6 +1670,7 @@ public final class ArtManagerLocal {
             getUserManager();
             getDexUseManager();
             getStorageManager();
+            getActivityManager();
             GlobalInjector.getInstance().checkArtModuleServiceManager();
 
             // `PreRebootDexoptJob` does not depend on external dependencies, so unlike the calls
@@ -1764,5 +1813,16 @@ public final class ArtManagerLocal {
         public PreRebootStatsReporter getPreRebootStatsReporter() {
             return new PreRebootStatsReporter();
         }
+
+        @RequiresApi(Build.VERSION_CODES.UPSIDE_DOWN_CAKE)
+        @NonNull
+        public ActivityManager getActivityManager() {
+            return Objects.requireNonNull(mContext.getSystemService(ActivityManager.class));
+        }
+
+        @RequiresApi(Build.VERSION_CODES.UPSIDE_DOWN_CAKE)
+        public void kill(int pid, int signal) throws ErrnoException {
+            Os.kill(pid, signal);
+        }
     }
 }
diff --git a/libartservice/service/java/com/android/server/art/ArtShellCommand.java b/libartservice/service/java/com/android/server/art/ArtShellCommand.java
index 1c19dad9c4..a3b2fba42a 100644
--- a/libartservice/service/java/com/android/server/art/ArtShellCommand.java
+++ b/libartservice/service/java/com/android/server/art/ArtShellCommand.java
@@ -31,7 +31,6 @@ import static com.android.server.art.model.DexoptStatus.DexContainerFileDexoptSt
 
 import android.annotation.NonNull;
 import android.annotation.Nullable;
-import android.content.Context;
 import android.os.Binder;
 import android.os.Build;
 import android.os.CancellationSignal;
@@ -44,6 +43,7 @@ import android.system.StructStat;
 import androidx.annotation.RequiresApi;
 
 import com.android.internal.annotations.GuardedBy;
+import com.android.internal.annotations.VisibleForTesting;
 import com.android.modules.utils.BasicShellCommandHandler;
 import com.android.modules.utils.build.SdkLevel;
 import com.android.server.art.model.ArtFlags;
@@ -82,7 +82,6 @@ import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
 import java.util.function.Consumer;
 import java.util.function.Function;
-import java.util.stream.Collectors;
 
 /**
  * This class handles ART shell commands.
@@ -94,19 +93,20 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
     /** The default location for profile dumps. */
     private final static String PROFILE_DEBUG_LOCATION = "/data/misc/profman";
 
-    private final ArtManagerLocal mArtManagerLocal;
-    private final PackageManagerLocal mPackageManagerLocal;
-    private final Context mContext;
+    @NonNull private final Injector mInjector;
 
     @GuardedBy("sCancellationSignalMap")
     @NonNull
     private static final Map<String, CancellationSignal> sCancellationSignalMap = new HashMap<>();
 
     public ArtShellCommand(@NonNull ArtManagerLocal artManagerLocal,
-            @NonNull PackageManagerLocal packageManagerLocal, @NonNull Context context) {
-        mArtManagerLocal = artManagerLocal;
-        mPackageManagerLocal = packageManagerLocal;
-        mContext = context;
+            @NonNull PackageManagerLocal packageManagerLocal) {
+        this(new Injector(artManagerLocal, packageManagerLocal));
+    }
+
+    @VisibleForTesting
+    public ArtShellCommand(@NonNull Injector injector) {
+        mInjector = injector;
     }
 
     @Override
@@ -114,7 +114,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
         // Apps shouldn't call ART Service shell commands, not even for dexopting themselves.
         enforceRootOrShell();
         PrintWriter pw = getOutPrintWriter();
-        try (var snapshot = mPackageManagerLocal.withFilteredSnapshot()) {
+        try (var snapshot = mInjector.getPackageManagerLocal().withFilteredSnapshot()) {
             switch (cmd) {
                 case "compile":
                     return handleCompile(pw, snapshot);
@@ -171,22 +171,11 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
                 return 0;
             }
             case "dump": {
-                boolean verifySdmSignatures = false;
-
-                String opt;
-                while ((opt = getNextOption()) != null) {
-                    switch (opt) {
-                        case "--verify-sdm-signatures":
-                            verifySdmSignatures = true;
-                            break;
-                    }
-                }
-
                 String packageName = getNextArg();
                 if (packageName != null) {
-                    mArtManagerLocal.dumpPackage(pw, snapshot, packageName, verifySdmSignatures);
+                    mInjector.getArtManagerLocal().dumpPackage(pw, snapshot, packageName);
                 } else {
-                    mArtManagerLocal.dump(pw, snapshot, verifySdmSignatures);
+                    mInjector.getArtManagerLocal().dump(pw, snapshot);
                 }
                 return 0;
             }
@@ -194,7 +183,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
                 return handleCleanup(pw, snapshot);
             }
             case "clear-app-profiles": {
-                mArtManagerLocal.clearAppProfiles(snapshot, getNextArgRequired());
+                mInjector.getArtManagerLocal().clearAppProfiles(snapshot, getNextArgRequired());
                 pw.println("Profiles cleared");
                 return 0;
             }
@@ -372,7 +361,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
                 // For compat only. Combining this with dexopt usually produces in undesired
                 // results.
                 for (String packageName : packageNames) {
-                    mArtManagerLocal.clearAppProfiles(snapshot, packageName);
+                    mInjector.getArtManagerLocal().clearAppProfiles(snapshot, packageName);
                 }
             }
             return dexoptPackages(pw, snapshot, packageNames, paramsBuilder.build(), verbose);
@@ -412,7 +401,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
             }
 
             CompletableFuture<BackgroundDexoptJob.Result> runningJob =
-                    mArtManagerLocal.getRunningBackgroundDexoptJob();
+                    mInjector.getArtManagerLocal().getRunningBackgroundDexoptJob();
             if (runningJob != null) {
                 pw.println("Another job already running. Waiting for it to finish... To cancel it, "
                         + "run 'pm bg-dexopt-job --cancel'. in a separate shell.");
@@ -420,7 +409,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
                 Utils.getFuture(runningJob);
             }
             CompletableFuture<BackgroundDexoptJob.Result> future =
-                    mArtManagerLocal.startBackgroundDexoptJobAndReturnFuture();
+                    mInjector.getArtManagerLocal().startBackgroundDexoptJobAndReturnFuture();
             pw.println("Job running...  To cancel it, run 'pm bg-dexopt-job --cancel'. in a "
                     + "separate shell.");
             pw.flush();
@@ -446,7 +435,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
                 // This operation requires the uid to be "system" (1000).
                 long identityToken = Binder.clearCallingIdentity();
                 try {
-                    mArtManagerLocal.scheduleBackgroundDexoptJob();
+                    mInjector.getArtManagerLocal().scheduleBackgroundDexoptJob();
                 } finally {
                     Binder.restoreCallingIdentity(identityToken);
                 }
@@ -457,7 +446,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
                 // This operation requires the uid to be "system" (1000).
                 long identityToken = Binder.clearCallingIdentity();
                 try {
-                    mArtManagerLocal.unscheduleBackgroundDexoptJob();
+                    mInjector.getArtManagerLocal().unscheduleBackgroundDexoptJob();
                 } finally {
                     Binder.restoreCallingIdentity(identityToken);
                 }
@@ -471,22 +460,22 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
     }
 
     private int handleCancelBgDexoptJob(@NonNull PrintWriter pw) {
-        mArtManagerLocal.cancelBackgroundDexoptJob();
+        mInjector.getArtManagerLocal().cancelBackgroundDexoptJob();
         pw.println("Background dexopt job cancelled");
         return 0;
     }
 
     private int handleCleanup(
             @NonNull PrintWriter pw, @NonNull PackageManagerLocal.FilteredSnapshot snapshot) {
-        long freedBytes = mArtManagerLocal.cleanup(snapshot);
+        long freedBytes = mInjector.getArtManagerLocal().cleanup(snapshot);
         pw.printf("Freed %d bytes\n", freedBytes);
         return 0;
     }
 
     private int handleDeleteDexopt(
             @NonNull PrintWriter pw, @NonNull PackageManagerLocal.FilteredSnapshot snapshot) {
-        DeleteResult result =
-                mArtManagerLocal.deleteDexoptArtifacts(snapshot, getNextArgRequired());
+        DeleteResult result = mInjector.getArtManagerLocal().deleteDexoptArtifacts(
+                snapshot, getNextArgRequired());
         pw.printf("Freed %d bytes\n", result.getFreedBytes());
         return 0;
     }
@@ -549,7 +538,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
             @NonNull PrintWriter pw, @NonNull PackageManagerLocal.FilteredSnapshot snapshot)
             throws SnapshotProfileException {
         String outputRelativePath = "android.prof";
-        ParcelFileDescriptor fd = mArtManagerLocal.snapshotBootImageProfile(snapshot);
+        ParcelFileDescriptor fd = mInjector.getArtManagerLocal().snapshotBootImageProfile(snapshot);
         writeProfileFdContentsToFile(pw, fd, outputRelativePath);
         return 0;
     }
@@ -560,7 +549,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
         String outputRelativePath = String.format("%s%s.prof", packageName,
                 splitName != null ? String.format("-split_%s.apk", splitName) : "");
         ParcelFileDescriptor fd =
-                mArtManagerLocal.snapshotAppProfile(snapshot, packageName, splitName);
+                mInjector.getArtManagerLocal().snapshotAppProfile(snapshot, packageName, splitName);
         writeProfileFdContentsToFile(pw, fd, outputRelativePath);
         return 0;
     }
@@ -588,13 +577,27 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
         PackageState pkgState = Utils.getPackageStateOrThrow(snapshot, packageName);
         AndroidPackage pkg = Utils.getPackageOrThrow(pkgState);
         try (var tracing = new Utils.Tracing("dump profiles")) {
+            // `flushProfiles` may take time and may have unexpected side-effects (e.g., when the
+            // app has its own thread waiting for SIGUSR1). Therefore, We call it in the shell
+            // command handler instead of in `dumpAppProfile` to prevent existing Java API users
+            // from being impacted by this behavior.
+            pw.println("Waiting for app processes to flush profiles...");
+            pw.flush();
+            long startTimeMs = System.currentTimeMillis();
+            if (mInjector.getArtManagerLocal().flushProfiles(snapshot, packageName)) {
+                pw.printf("App processes flushed profiles in %dms\n",
+                        System.currentTimeMillis() - startTimeMs);
+            } else {
+                pw.println("Timed out while waiting for app processes to flush profiles");
+            }
+
             for (PrimaryDexInfo dexInfo : PrimaryDexUtils.getDexInfo(pkg)) {
                 String profileName = PrimaryDexUtils.getProfileName(dexInfo.splitName());
                 // The path is intentionally inconsistent with the one for "snapshot-profile". This
                 // is to match the behavior of the legacy PM shell command.
                 String outputRelativePath =
                         String.format("%s-%s.prof.txt", packageName, profileName);
-                ParcelFileDescriptor fd = mArtManagerLocal.dumpAppProfile(
+                ParcelFileDescriptor fd = mInjector.getArtManagerLocal().dumpAppProfile(
                         snapshot, packageName, dexInfo.splitName(), dumpClassesAndMethods);
                 writeProfileFdContentsToFile(pw, fd, outputRelativePath);
             }
@@ -641,8 +644,9 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
 
         ExecutorService progressCallbackExecutor = Executors.newSingleThreadExecutor();
         try (var signal = new WithCancellationSignal(pw, true /* verbose */)) {
-            Map<Integer, DexoptResult> results = mArtManagerLocal.dexoptPackages(snapshot,
-                    finalReason, signal.get(), progressCallbackExecutor, progressCallbacks);
+            Map<Integer, DexoptResult> results =
+                    mInjector.getArtManagerLocal().dexoptPackages(snapshot, finalReason,
+                            signal.get(), progressCallbackExecutor, progressCallbacks);
 
             Utils.executeAndWait(progressCallbackExecutor, () -> {
                 for (@BatchDexoptPass int pass : ArtFlags.BATCH_DEXOPT_PASSES) {
@@ -669,11 +673,12 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
             return 1;
         }
 
-        int uid = Binder.getCallingUid();
+        int uid = mInjector.getCallingUid();
         if (uid != Process.ROOT_UID) {
             throw new SecurityException("Only root can call 'on-ota-staged'");
         }
 
+        String mode = null;
         String otaSlot = null;
 
         String opt;
@@ -682,23 +687,33 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
                 case "--slot":
                     otaSlot = getNextArgRequired();
                     break;
+                case "--start":
+                    mode = opt;
+                    break;
                 default:
                     pw.println("Error: Unknown option: " + opt);
                     return 1;
             }
         }
 
-        if (otaSlot == null) {
-            pw.println("Error: '--slot' must be specified");
-            return 1;
-        }
-
-        if (mArtManagerLocal.getPreRebootDexoptJob().isAsyncForOta()) {
-            return handleSchedulePrDexoptJob(pw, otaSlot);
+        if ("--start".equals(mode)) {
+            if (otaSlot != null) {
+                pw.println("Error: '--slot' cannot be specified together with '--start'");
+                return 1;
+            }
+            return handleOnOtaStagedStart(pw);
         } else {
-            // Don't map snapshots when running synchronously. `update_engine` maps snapshots for
-            // us.
-            return handleRunPrDexoptJob(pw, otaSlot, false /* mapSnapshotsForOta */);
+            if (otaSlot == null) {
+                pw.println("Error: '--slot' must be specified");
+                return 1;
+            }
+
+            if (mInjector.getArtManagerLocal().getPreRebootDexoptJob().isAsyncForOta()) {
+                return handleSchedulePrDexoptJob(pw, otaSlot);
+            } else {
+                // In the synchronous case, `update_engine` has already mapped snapshots for us.
+                return handleRunPrDexoptJob(pw, otaSlot, true /* isUpdateEngineReady */);
+            }
         }
     }
 
@@ -739,7 +754,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
             return 1;
         }
 
-        if (otaSlot != null && Binder.getCallingUid() != Process.ROOT_UID) {
+        if (otaSlot != null && mInjector.getCallingUid() != Process.ROOT_UID) {
             throw new SecurityException("Only root can specify '--slot'");
         }
 
@@ -750,7 +765,11 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
             case "--test":
                 return handleTestPrDexoptJob(pw);
             case "--run":
-                return handleRunPrDexoptJob(pw, otaSlot, true /* mapSnapshotsForOta */);
+                // Passing isUpdateEngineReady=false will make the job call update_engine's
+                // triggerPostinstall to map the snapshot devices if the API is available.
+                // It's always safe to do so because triggerPostinstall can be called at any time
+                // any number of times to map the snapshots if any are available.
+                return handleRunPrDexoptJob(pw, otaSlot, false /* isUpdateEngineReady */);
             case "--schedule":
                 return handleSchedulePrDexoptJob(pw, otaSlot);
             case "--cancel":
@@ -764,7 +783,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
     @RequiresApi(Build.VERSION_CODES.VANILLA_ICE_CREAM)
     private int handleTestPrDexoptJob(@NonNull PrintWriter pw) {
         try {
-            mArtManagerLocal.getPreRebootDexoptJob().test();
+            mInjector.getArtManagerLocal().getPreRebootDexoptJob().test();
             pw.println("Success");
             return 0;
         } catch (Exception e) {
@@ -776,15 +795,41 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
 
     @RequiresApi(Build.VERSION_CODES.VANILLA_ICE_CREAM)
     private int handleRunPrDexoptJob(
-            @NonNull PrintWriter pw, @Nullable String otaSlot, boolean mapSnapshotsForOta) {
-        PreRebootDexoptJob job = mArtManagerLocal.getPreRebootDexoptJob();
+            @NonNull PrintWriter pw, @Nullable String otaSlot, boolean isUpdateEngineReady) {
+        PreRebootDexoptJob job = mInjector.getArtManagerLocal().getPreRebootDexoptJob();
 
-        CompletableFuture<Void> future = job.onUpdateReadyStartNow(otaSlot, mapSnapshotsForOta);
+        CompletableFuture<Void> future = job.onUpdateReadyStartNow(otaSlot, isUpdateEngineReady);
         if (future == null) {
             pw.println("Job disabled by system property");
             return 1;
         }
 
+        return handlePrDexoptJobRunning(pw, future);
+    }
+
+    @RequiresApi(Build.VERSION_CODES.VANILLA_ICE_CREAM)
+    private int handleOnOtaStagedStart(@NonNull PrintWriter pw) {
+        PreRebootDexoptJob job = mInjector.getArtManagerLocal().getPreRebootDexoptJob();
+
+        // We assume we're being invoked from within `UpdateEngine.triggerPostinstall` in
+        // `PreRebootDexoptJob.triggerUpdateEnginePostinstallAndWait`, so a Pre-reboot Dexopt job is
+        // waiting.
+        CompletableFuture<Void> future = job.notifyUpdateEngineReady();
+        if (future == null) {
+            pw.println("No waiting job found");
+            return 1;
+        }
+
+        return handlePrDexoptJobRunning(pw, future);
+    }
+
+    @RequiresApi(Build.VERSION_CODES.VANILLA_ICE_CREAM)
+    private int handlePrDexoptJobRunning(
+            @NonNull PrintWriter pw, @NonNull CompletableFuture<Void> future) {
+        PreRebootDexoptJob job = mInjector.getArtManagerLocal().getPreRebootDexoptJob();
+
+        // Read stdin and cancel on broken pipe, to detect if the caller (e.g. update_engine) has
+        // killed the postinstall script.
         // Put the read in a separate thread because there isn't an easy way in Java to wait for
         // both the `Future` and the read.
         var readThread = new Thread(() -> {
@@ -831,7 +876,8 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
 
     @RequiresApi(Build.VERSION_CODES.VANILLA_ICE_CREAM)
     private int handleSchedulePrDexoptJob(@NonNull PrintWriter pw, @Nullable String otaSlot) {
-        int code = mArtManagerLocal.getPreRebootDexoptJob().onUpdateReadyImpl(otaSlot);
+        int code =
+                mInjector.getArtManagerLocal().getPreRebootDexoptJob().onUpdateReadyImpl(otaSlot);
         switch (code) {
             case ArtFlags.SCHEDULE_SUCCESS:
                 pw.println("Pre-reboot Dexopt job scheduled");
@@ -850,7 +896,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
 
     @RequiresApi(Build.VERSION_CODES.VANILLA_ICE_CREAM)
     private int handleCancelPrDexoptJob(@NonNull PrintWriter pw) {
-        mArtManagerLocal.getPreRebootDexoptJob().cancelAny();
+        mInjector.getArtManagerLocal().getPreRebootDexoptJob().cancelAny();
         pw.println("Pre-reboot Dexopt job cancelled");
         return 0;
     }
@@ -876,7 +922,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
 
         // Variables used in lambda needs to be effectively final.
         String finalInputReason = inputReason;
-        mArtManagerLocal.setBatchDexoptStartCallback(
+        mInjector.getArtManagerLocal().setBatchDexoptStartCallback(
                 Runnable::run, (snapshot, reason, defaultPackages, builder, cancellationSignal) -> {
                     if (reason.equals(finalInputReason)) {
                         if (!packages.isEmpty()) {
@@ -921,12 +967,16 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
         pw.println("    -f Force dexopt, also when the compiler filter being applied is not");
         pw.println("       better than that of the current dexopt artifacts for a package.");
         pw.println("    --reset Reset the dexopt state of the package as if the package is newly");
-        pw.println("       installed.");
-        pw.println("       More specifically, it clears reference profiles, current profiles, and");
-        pw.println("       any code compiled from those local profiles. If there is an external");
-        pw.println("       profile (e.g., a cloud profile), the code compiled from that profile");
-        pw.println("       will be kept.");
-        pw.println("       For secondary dex files, it also clears all dexopt artifacts.");
+        pw.println("       installed without cloud dexopt artifacts (SDM files).");
+        pw.println("       More specifically,");
+        pw.println("       - It clears current profiles, reference profiles, and all dexopt");
+        pw.println("         artifacts (including cloud dexopt artifacts).");
+        pw.println("       - If there is an external profile (e.g., a cloud profile), the");
+        pw.println("         reference profile will be re-created from that profile, and dexopt");
+        pw.println("         artifacts will be regenerated for that profile.");
+        pw.println("       For secondary dex files, it clears all profiles and dexopt artifacts");
+        pw.println("       without regeneration because secondary dex files are supposed to be");
+        pw.println("       unknown at install time.");
         pw.println("       When this flag is set, all the other flags are ignored.");
         pw.println("    -v Verbose mode. This mode prints detailed results.");
         pw.println("    --force-merge-profile Force merge profiles even if the difference between");
@@ -953,7 +1003,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
         pw.println();
         pw.println("delete-dexopt PACKAGE_NAME");
         pw.println("  Delete the dexopt artifacts of both primary dex files and secondary dex");
-        pw.println("  files of a package.");
+        pw.println("  files of a package, including cloud dexopt artifacts (SDM files).");
         pw.println();
         pw.println("bg-dexopt-job [--cancel | --disable | --enable]");
         pw.println("  Control the background dexopt job.");
@@ -1025,13 +1075,10 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
         pw.println("    Cleanup obsolete files, such as dexopt artifacts that are outdated or");
         pw.println("    correspond to dex container files that no longer exist.");
         pw.println();
-        pw.println("  dump [--verify-sdm-signatures] [PACKAGE_NAME]");
+        pw.println("  dump [PACKAGE_NAME]");
         pw.println("    Dump the dexopt state in text format to stdout.");
         pw.println("    If PACKAGE_NAME is empty, the command is for all packages. Otherwise, it");
         pw.println("    is for the given package.");
-        pw.println("    Options:");
-        pw.println("      --verify-sdm-signatures Also verify SDM file signatures and include");
-        pw.println("        their statuses.");
         pw.println();
         pw.println("  dexopt-packages -r REASON");
         pw.println("    Run batch dexopt for the given reason.");
@@ -1041,7 +1088,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
         pw.println("    only dexopts a subset of apps, and it runs dexopt in parallel. See the");
         pw.println("    API documentation for 'ArtManagerLocal.dexoptPackages' for details.");
         pw.println();
-        pw.println("  on-ota-staged --slot SLOT");
+        pw.println("  on-ota-staged [--slot SLOT | --start]");
         pw.println("    Notify ART Service that an OTA update is staged. ART Service decides what");
         pw.println("    to do with this notification:");
         pw.println("    - If Pre-reboot Dexopt is disabled or unsupported, the command returns");
@@ -1054,6 +1101,9 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
         pw.println("      then run by the job scheduler when the device is idle and charging.");
         pw.println("    Options:");
         pw.println("      --slot SLOT The slot that contains the OTA update, '_a' or '_b'.");
+        pw.println("      --start Notify the asynchronous job that the snapshot devices are");
+        pw.println("        ready. The command blocks until the job finishes, and returns zero no");
+        pw.println("        matter it succeeds or not.");
         pw.println("    Note: This command is only supposed to be used by the system. To manually");
         pw.println("    control the Pre-reboot Dexopt job, use 'pr-dexopt-job' instead.");
         pw.println();
@@ -1096,7 +1146,7 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
     }
 
     private void enforceRootOrShell() {
-        final int uid = Binder.getCallingUid();
+        final int uid = mInjector.getCallingUid();
         if (uid != Process.ROOT_UID && uid != Process.SHELL_UID) {
             throw new SecurityException("ART service shell commands need root or shell access");
         }
@@ -1162,8 +1212,8 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
             @NonNull List<String> packageNames, boolean verbose) {
         try (var signal = new WithCancellationSignal(pw, verbose)) {
             for (String packageName : packageNames) {
-                DexoptResult result =
-                        mArtManagerLocal.resetDexoptStatus(snapshot, packageName, signal.get());
+                DexoptResult result = mInjector.getArtManagerLocal().resetDexoptStatus(
+                        snapshot, packageName, signal.get());
                 printDexoptResult(pw, result, verbose, packageNames.size() > 1);
             }
         }
@@ -1175,8 +1225,8 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
             @NonNull List<String> packageNames, @NonNull DexoptParams params, boolean verbose) {
         try (var signal = new WithCancellationSignal(pw, verbose)) {
             for (String packageName : packageNames) {
-                DexoptResult result =
-                        mArtManagerLocal.dexoptPackage(snapshot, packageName, params, signal.get());
+                DexoptResult result = mInjector.getArtManagerLocal().dexoptPackage(
+                        snapshot, packageName, params, signal.get());
                 printDexoptResult(pw, result, verbose, packageNames.size() > 1);
             }
         }
@@ -1289,4 +1339,31 @@ public final class ArtShellCommand extends BasicShellCommandHandler {
             }
         }
     }
+
+    /** Injector pattern for testing purpose. */
+    @VisibleForTesting
+    public static class Injector {
+        @NonNull private final ArtManagerLocal mArtManagerLocal;
+        @NonNull private final PackageManagerLocal mPackageManagerLocal;
+
+        public Injector(@NonNull ArtManagerLocal artManagerLocal,
+                @NonNull PackageManagerLocal packageManagerLocal) {
+            mArtManagerLocal = artManagerLocal;
+            mPackageManagerLocal = packageManagerLocal;
+        }
+
+        @NonNull
+        public ArtManagerLocal getArtManagerLocal() {
+            return mArtManagerLocal;
+        }
+
+        @NonNull
+        public PackageManagerLocal getPackageManagerLocal() {
+            return mPackageManagerLocal;
+        }
+
+        public int getCallingUid() {
+            return Binder.getCallingUid();
+        }
+    }
 }
diff --git a/libartservice/service/java/com/android/server/art/BackgroundDexoptJobService.java b/libartservice/service/java/com/android/server/art/BackgroundDexoptJobService.java
index dcea7084fa..7cfb4ee1ac 100644
--- a/libartservice/service/java/com/android/server/art/BackgroundDexoptJobService.java
+++ b/libartservice/service/java/com/android/server/art/BackgroundDexoptJobService.java
@@ -31,6 +31,9 @@ import com.android.server.art.model.ArtServiceJobInterface;
  * Entry point for the callback from the job scheduler. This class is instantiated by the system
  * automatically.
  *
+ * This class is for all ART Service jobs, not only the background dexopt job but also the
+ * Pre-reboot Dexopt job. We cannot change its name because its hardcoded on the platform side.
+ *
  * @hide
  */
 @RequiresApi(Build.VERSION_CODES.UPSIDE_DOWN_CAKE)
diff --git a/libartservice/service/java/com/android/server/art/BackgroundDexoptJobStatsReporter.java b/libartservice/service/java/com/android/server/art/BackgroundDexoptJobStatsReporter.java
index 106b1c6c2a..251eec20af 100644
--- a/libartservice/service/java/com/android/server/art/BackgroundDexoptJobStatsReporter.java
+++ b/libartservice/service/java/com/android/server/art/BackgroundDexoptJobStatsReporter.java
@@ -31,7 +31,6 @@ import dalvik.system.DexFile;
 
 import java.util.List;
 import java.util.Optional;
-import java.util.stream.Collectors;
 
 /**
  * This is a helper class to report the background DexOpt job metrics to StatsD.
@@ -90,7 +89,7 @@ public class BackgroundDexoptJobStatsReporter {
                                 -> (fileResult.getExtendedStatusFlags()
                                            & DexoptResult.EXTENDED_SKIPPED_NO_DEX_CODE)
                                         == 0))
-                .collect(Collectors.toList());
+                .toList();
     }
 
     private static int getStatusForStats(
diff --git a/libartservice/service/java/com/android/server/art/Dex2OatStatsReporter.java b/libartservice/service/java/com/android/server/art/Dex2OatStatsReporter.java
index d553113301..2e45c01590 100644
--- a/libartservice/service/java/com/android/server/art/Dex2OatStatsReporter.java
+++ b/libartservice/service/java/com/android/server/art/Dex2OatStatsReporter.java
@@ -16,6 +16,8 @@
 
 package com.android.server.art;
 
+import static com.android.server.art.Utils.Abi;
+
 import android.os.Build;
 
 import androidx.annotation.NonNull;
@@ -23,6 +25,9 @@ import androidx.annotation.RequiresApi;
 
 import com.android.server.art.model.DetailedDexInfo;
 import com.android.server.art.model.DexMetadata;
+import com.android.server.art.model.DexoptParams;
+
+import java.util.List;
 
 /**
  * A class to report dex2oat metrics to StatsD.
@@ -43,6 +48,23 @@ public class Dex2OatStatsReporter {
                 (int) compilationTime);
     }
 
+    public static void reportSkipped(int appId, @NonNull String compilationReason,
+            @DexMetadata.Type int dexMetadataType, @NonNull DetailedDexInfo dexInfo,
+            @NonNull List<Abi> abis) {
+        Dex2OatResult skipped = Dex2OatResult.notRun();
+
+        for (Abi abi : abis) {
+            ArtStatsLog.write(ArtStatsLog.ART_DEX2OAT_REPORTED, appId,
+                    translateCompilerFilter(DexoptParams.COMPILER_FILTER_NOOP),
+                    translateCompilationReason(compilationReason), dexMetadataType,
+                    getApkType(dexInfo), translateIsa(abi.isa()), skipped.status, skipped.exitCode,
+                    skipped.signal,
+                    0, // artifacts size
+                    0 // compilation time
+            );
+        }
+    }
+
     private static int translateCompilerFilter(String compilerFilter) {
         return switch (compilerFilter) {
             case "assume-verified" ->
@@ -66,6 +88,8 @@ public class Dex2OatStatsReporter {
             case "everything" ->
                 ArtStatsLog
                         .ART_DEX2_OAT_REPORTED__COMPILER_FILTER__ART_COMPILATION_FILTER_EVERYTHING;
+            case "skip" ->
+                ArtStatsLog.ART_DEX2_OAT_REPORTED__COMPILER_FILTER__ART_COMPILATION_FILTER_SKIP;
             default ->
                 ArtStatsLog.ART_DEX2_OAT_REPORTED__COMPILER_FILTER__ART_COMPILATION_FILTER_UNKNOWN;
         };
diff --git a/libartservice/service/java/com/android/server/art/DexMetadataHelper.java b/libartservice/service/java/com/android/server/art/DexMetadataHelper.java
index 9742c5f40e..759189991c 100644
--- a/libartservice/service/java/com/android/server/art/DexMetadataHelper.java
+++ b/libartservice/service/java/com/android/server/art/DexMetadataHelper.java
@@ -72,10 +72,12 @@ public class DexMetadataHelper {
                         dmPath, DexMetadataConfig.parseFrom(stream), getType(zipFile));
             }
         } catch (IOException e) {
-            if (!(e instanceof FileNotFoundException || e instanceof NoSuchFileException)) {
+            if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {
+                return getDefaultDexMetadataInfo(DexMetadata.TYPE_NONE);
+            } else {
                 AsLog.e(String.format("Failed to read dm file '%s'", realDmPath), e);
+                return getDefaultDexMetadataInfo(DexMetadata.TYPE_ERROR);
             }
-            return getDefaultDexMetadataInfo(DexMetadata.TYPE_ERROR);
         }
     }
 
diff --git a/libartservice/service/java/com/android/server/art/DexUseManagerLocal.java b/libartservice/service/java/com/android/server/art/DexUseManagerLocal.java
index 51b63ca758..92a0dab04d 100644
--- a/libartservice/service/java/com/android/server/art/DexUseManagerLocal.java
+++ b/libartservice/service/java/com/android/server/art/DexUseManagerLocal.java
@@ -113,6 +113,14 @@ public class DexUseManagerLocal {
      */
     @VisibleForTesting public static final long INTERVAL_MS = 15_000;
 
+    // Impose a limit on the input accepted by notifyDexContainersLoaded per owning package.
+    /** @hide */
+    @VisibleForTesting public static final int MAX_PATH_LENGTH = 4096;
+    /** @hide */
+    @VisibleForTesting public static final int MAX_CLASS_LOADER_CONTEXT_LENGTH = 10000;
+    /** @hide */
+    private static final int MAX_SECONDARY_DEX_FILES_PER_OWNER = 500;
+
     private static final Object sLock = new Object();
 
     // The static field is associated with the class and the class loader that loads it. In the
@@ -207,7 +215,7 @@ public class DexUseManagerLocal {
                                         .stream()
                                         .map(loader -> loader.loadingPackageName())
                                         .collect(Collectors.toSet())))
-                .collect(Collectors.toList());
+                .toList();
     }
 
     /**
@@ -354,7 +362,7 @@ public class DexUseManagerLocal {
                                         -> !clc.equals(
                                                 SecondaryDexInfo.UNSUPPORTED_CLASS_LOADER_CONTEXT))
                                 .distinct()
-                                .collect(Collectors.toList());
+                                .toList();
                 String clc;
                 if (distinctClcList.size() == 0) {
                     clc = SecondaryDexInfo.UNSUPPORTED_CLASS_LOADER_CONTEXT;
@@ -413,7 +421,7 @@ public class DexUseManagerLocal {
 
         // TODO(jiakaiz): Investigate whether it should also be considered as isolated process if
         // `Process.isSdkSandboxUid` returns true.
-        boolean isolatedProcess = Process.isIsolatedUid(mInjector.getCallingUid());
+        boolean isolatedProcess = mInjector.isIsolatedUid(mInjector.getCallingUid());
         long lastUsedAtMs = mInjector.getCurrentTimeMillis();
 
         for (var entry : classLoaderContextByDexContainerFile.entrySet()) {
@@ -527,7 +535,7 @@ public class DexUseManagerLocal {
             }
 
             // Check remaining packages. Don't check for shared libraries because it might be too
-            // expansive to do so and the time complexity is O(n) no matter we do it or not.
+            // expensive to do so and the time complexity is O(n) no matter we do it or not.
             for (PackageState pkgState : packageStates.values()) {
                 if (visitedPackages.contains(pkgState.getPackageName())) {
                     continue;
@@ -657,16 +665,27 @@ public class DexUseManagerLocal {
     private void addSecondaryDexUse(@NonNull String owningPackageName, @NonNull String dexPath,
             @NonNull String loadingPackageName, boolean isolatedProcess,
             @NonNull String classLoaderContext, @NonNull String abiName, long lastUsedAtMs) {
+        DexLoader loader = DexLoader.create(loadingPackageName, isolatedProcess);
         synchronized (mLock) {
+            PackageDexUse packageDexUse = mDexUse.mPackageDexUseByOwningPackageName.computeIfAbsent(
+                    owningPackageName, k -> new PackageDexUse());
             SecondaryDexUse secondaryDexUse =
-                    mDexUse.mPackageDexUseByOwningPackageName
-                            .computeIfAbsent(owningPackageName, k -> new PackageDexUse())
-                            .mSecondaryDexUseByDexFile.computeIfAbsent(
-                                    dexPath, k -> new SecondaryDexUse());
+                    packageDexUse.mSecondaryDexUseByDexFile.computeIfAbsent(dexPath, k -> {
+                        if (packageDexUse.mSecondaryDexUseByDexFile.size()
+                                >= mInjector.getMaxSecondaryDexFilesPerOwner()) {
+                            AsLog.w("Not recording too many secondary dex use entries for "
+                                    + owningPackageName);
+                            return null;
+                        }
+                        return new SecondaryDexUse();
+                    });
+            if (secondaryDexUse == null) {
+                return;
+            }
             secondaryDexUse.mUserHandle = mInjector.getCallingUserHandle();
-            SecondaryDexUseRecord record = secondaryDexUse.mRecordByLoader.computeIfAbsent(
-                    DexLoader.create(loadingPackageName, isolatedProcess),
-                    k -> new SecondaryDexUseRecord());
+            SecondaryDexUseRecord record =
+                    secondaryDexUse.mRecordByLoader.computeIfAbsent(
+                            loader, k -> new SecondaryDexUseRecord());
             record.mClassLoaderContext = classLoaderContext;
             record.mAbiName = abiName;
             record.mLastUsedAtMs = lastUsedAtMs;
@@ -772,13 +791,23 @@ public class DexUseManagerLocal {
         }
 
         for (var entry : classLoaderContextByDexContainerFile.entrySet()) {
-            Utils.assertNonEmpty(entry.getKey());
-            String errorMsg = ArtJni.validateDexPath(entry.getKey());
+            String dexPath = entry.getKey();
+            String classLoaderContext = entry.getValue();
+            Utils.assertNonEmpty(dexPath);
+            if (dexPath.length() > MAX_PATH_LENGTH) {
+                throw new IllegalArgumentException(
+                        "Dex path too long - exceeds " + MAX_PATH_LENGTH + " chars");
+            }
+            String errorMsg = ArtJni.validateDexPath(dexPath);
             if (errorMsg != null) {
                 throw new IllegalArgumentException(errorMsg);
             }
-            Utils.assertNonEmpty(entry.getValue());
-            errorMsg = ArtJni.validateClassLoaderContext(entry.getKey(), entry.getValue());
+            Utils.assertNonEmpty(classLoaderContext);
+            if (classLoaderContext.length() > MAX_CLASS_LOADER_CONTEXT_LENGTH) {
+                throw new IllegalArgumentException("Class loader context too long - exceeds "
+                        + MAX_CLASS_LOADER_CONTEXT_LENGTH + " chars");
+            }
+            errorMsg = ArtJni.validateClassLoaderContext(dexPath, classLoaderContext);
             if (errorMsg != null) {
                 throw new IllegalArgumentException(errorMsg);
             }
@@ -1400,5 +1429,13 @@ public class DexUseManagerLocal {
         public int getCallingUid() {
             return Binder.getCallingUid();
         }
+
+        public boolean isIsolatedUid(int uid) {
+            return Process.isIsolatedUid(uid);
+        }
+
+        public int getMaxSecondaryDexFilesPerOwner() {
+            return MAX_SECONDARY_DEX_FILES_PER_OWNER;
+        }
     }
 }
diff --git a/libartservice/service/java/com/android/server/art/DexoptHelper.java b/libartservice/service/java/com/android/server/art/DexoptHelper.java
index 828a05c656..b5f02ae5e7 100644
--- a/libartservice/service/java/com/android/server/art/DexoptHelper.java
+++ b/libartservice/service/java/com/android/server/art/DexoptHelper.java
@@ -56,7 +56,6 @@ import java.util.concurrent.Executor;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.function.Consumer;
 import java.util.function.Function;
-import java.util.stream.Collectors;
 
 /**
  * A helper class to handle dexopt.
@@ -127,9 +126,7 @@ public class DexoptHelper {
             // create a separate cancellation signal for each of them so that the listeners don't
             // overwrite each other.
             List<CancellationSignal> childCancellationSignals =
-                    pkgStates.stream()
-                            .map(pkgState -> new CancellationSignal())
-                            .collect(Collectors.toList());
+                    pkgStates.stream().map(pkgState -> new CancellationSignal()).toList();
             cancellationSignal.setOnCancelListener(() -> {
                 for (CancellationSignal childCancellationSignal : childCancellationSignals) {
                     childCancellationSignal.cancel();
@@ -174,8 +171,7 @@ public class DexoptHelper {
                 }
             }
 
-            List<PackageDexoptResult> results =
-                    futures.stream().map(Utils::getFuture).collect(Collectors.toList());
+            List<PackageDexoptResult> results = futures.stream().map(Utils::getFuture).toList();
 
             var result =
                     DexoptResult.create(params.getCompilerFilter(), params.getReason(), results);
@@ -187,7 +183,7 @@ public class DexoptHelper {
                     List<PackageDexoptResult> filteredResults =
                             results.stream()
                                     .filter(PackageDexoptResult::hasUpdatedArtifacts)
-                                    .collect(Collectors.toList());
+                                    .toList();
                     if (!filteredResults.isEmpty()) {
                         var resultForCallback = DexoptResult.create(
                                 params.getCompilerFilter(), params.getReason(), filteredResults);
diff --git a/libartservice/service/java/com/android/server/art/Dexopter.java b/libartservice/service/java/com/android/server/art/Dexopter.java
index b13ec66831..1c6abe6c41 100644
--- a/libartservice/service/java/com/android/server/art/Dexopter.java
+++ b/libartservice/service/java/com/android/server/art/Dexopter.java
@@ -113,8 +113,17 @@ public abstract class Dexopter<DexInfoType extends DetailedDexInfo> {
                     continue;
                 }
 
+                onDexoptStart(dexInfo);
+
                 String compilerFilter = adjustCompilerFilter(mParams.getCompilerFilter(), dexInfo);
+                DexMetadataInfo dmInfo =
+                        mInjector.getDexMetadataHelper().getDexMetadataInfo(buildDmPath(dexInfo));
                 if (compilerFilter.equals(DexoptParams.COMPILER_FILTER_NOOP)) {
+                    mInjector.getReporterExecutor().execute(
+                            ()
+                                    -> Dex2OatStatsReporter.reportSkipped(mPkgState.getAppId(),
+                                            mParams.getReason(), dmInfo.type(), dexInfo,
+                                            getAllAbis(dexInfo)));
                     continue;
                 }
 
@@ -126,9 +135,6 @@ public abstract class Dexopter<DexInfoType extends DetailedDexInfo> {
                     continue;
                 }
 
-                DexMetadataInfo dmInfo =
-                        mInjector.getDexMetadataHelper().getDexMetadataInfo(buildDmPath(dexInfo));
-
                 boolean needsToBeShared = needsToBeShared(dexInfo);
                 boolean isOtherReadable = true;
                 // If true, implies that the profile has changed since the last compilation.
@@ -195,14 +201,15 @@ public abstract class Dexopter<DexInfoType extends DetailedDexInfo> {
                     long sizeBeforeBytes = 0;
                     Dex2OatResult dex2OatResult = Dex2OatResult.notRun();
                     @DexoptResult.DexoptResultExtendedStatusFlags int extendedStatusFlags = 0;
+                    DexoptTarget<DexInfoType> target = null;
                     try {
-                        var target = DexoptTarget.<DexInfoType>builder()
-                                             .setDexInfo(dexInfo)
-                                             .setIsa(abi.isa())
-                                             .setIsInDalvikCache(isInDalvikCache)
-                                             .setCompilerFilter(compilerFilter)
-                                             .setDmPath(dmInfo.dmPath())
-                                             .build();
+                        target = DexoptTarget.<DexInfoType>builder()
+                                         .setDexInfo(dexInfo)
+                                         .setIsa(abi.isa())
+                                         .setIsInDalvikCache(isInDalvikCache)
+                                         .setCompilerFilter(compilerFilter)
+                                         .setDmPath(dmInfo.dmPath())
+                                         .build();
                         var options = GetDexoptNeededOptions.builder()
                                               .setProfileMerged(profileMerged)
                                               .setFlags(mParams.getFlags())
@@ -312,6 +319,9 @@ public abstract class Dexopter<DexInfoType extends DetailedDexInfo> {
                         AsLog.i(String.format("Dexopt result: [packageName = %s] %s",
                                 mPkgState.getPackageName(), result));
                         results.add(result);
+
+                        onDexoptTargetResult(target, status);
+
                         if (status != DexoptResult.DEXOPT_SKIPPED
                                 && status != DexoptResult.DEXOPT_PERFORMED) {
                             succeeded = false;
@@ -622,7 +632,10 @@ public abstract class Dexopter<DexInfoType extends DetailedDexInfo> {
                 return VdexPath.artifactsPath(AidlUtils.buildArtifactsPathAsInput(
                         dexPath, isa, false /* isInDalvikCache */));
             case ArtifactsLocation.DM:
-                // The DM file is passed to dex2oat as a separate flag whenever it exists.
+            case ArtifactsLocation.SDM_DALVIK_CACHE:
+            case ArtifactsLocation.SDM_NEXT_TO_DEX:
+                // In these cases, the VDEX file is in the DM file. The whole DM file is passed to
+                // dex2oat as a separate flag whenever it exists.
                 return null;
             default:
                 // This should never happen as the value is got from artd.
@@ -730,6 +743,18 @@ public abstract class Dexopter<DexInfoType extends DetailedDexInfo> {
      */
     @Nullable protected abstract DexMetadataPath buildDmPath(@NonNull DexInfoType dexInfo);
 
+    /**
+     * Called at an early stage during dexopt of every dex file, even before dexopt is skipped by
+     * the noop compiler filter.
+     */
+    protected void onDexoptStart(@NonNull DexInfoType dexInfo) throws RemoteException {}
+
+    /**
+     * Called once for every dex file and every ABI when dexopt has a result.
+     */
+    protected void onDexoptTargetResult(@NonNull DexoptTarget<DexInfoType> target,
+            @DexoptResult.DexoptResultStatus int status) throws RemoteException {}
+
     @AutoValue
     abstract static class DexoptTarget<DexInfoType extends DetailedDexInfo> {
         abstract @NonNull DexInfoType dexInfo();
diff --git a/libartservice/service/java/com/android/server/art/DumpHelper.java b/libartservice/service/java/com/android/server/art/DumpHelper.java
index 419c55f077..9c3bb3f087 100644
--- a/libartservice/service/java/com/android/server/art/DumpHelper.java
+++ b/libartservice/service/java/com/android/server/art/DumpHelper.java
@@ -20,12 +20,7 @@ import static com.android.server.art.DexUseManagerLocal.CheckedSecondaryDexInfo;
 import static com.android.server.art.DexUseManagerLocal.DexLoader;
 import static com.android.server.art.model.DexoptStatus.DexContainerFileDexoptStatus;
 
-import android.annotation.FlaggedApi;
 import android.annotation.NonNull;
-import android.annotation.SuppressLint;
-import android.content.pm.PackageManager;
-import android.content.pm.SigningInfo;
-import android.content.pm.SigningInfoException;
 import android.os.Build;
 import android.os.RemoteException;
 import android.os.ServiceSpecificException;
@@ -39,7 +34,6 @@ import com.android.server.pm.pkg.PackageState;
 
 import dalvik.system.VMRuntime;
 
-import java.io.File;
 import java.io.PrintWriter;
 import java.util.ArrayList;
 import java.util.Comparator;
@@ -71,13 +65,13 @@ public class DumpHelper {
     }
 
     /** Handles {@link ArtManagerLocal#dump(PrintWriter, PackageManagerLocal.FilteredSnapshot)}. */
-    public void dump(@NonNull PrintWriter pw,
-            @NonNull PackageManagerLocal.FilteredSnapshot snapshot, boolean verifySdmSignatures) {
+    public void dump(
+            @NonNull PrintWriter pw, @NonNull PackageManagerLocal.FilteredSnapshot snapshot) {
         snapshot.getPackageStates()
                 .values()
                 .stream()
                 .sorted(Comparator.comparing(PackageState::getPackageName))
-                .forEach(pkgState -> dumpPackage(pw, snapshot, pkgState, verifySdmSignatures));
+                .forEach(pkgState -> dumpPackage(pw, snapshot, pkgState));
         pw.printf("\nCurrent GC: %s\n", ArtJni.getGarbageCollector());
     }
 
@@ -86,8 +80,8 @@ public class DumpHelper {
      * ArtManagerLocal#dumpPackage(PrintWriter, PackageManagerLocal.FilteredSnapshot, String)}.
      */
     public void dumpPackage(@NonNull PrintWriter pw,
-            @NonNull PackageManagerLocal.FilteredSnapshot snapshot, @NonNull PackageState pkgState,
-            boolean verifySdmSignatures) {
+            @NonNull PackageManagerLocal.FilteredSnapshot snapshot,
+            @NonNull PackageState pkgState) {
         if (pkgState.isApex() || pkgState.getAndroidPackage() == null) {
             return;
         }
@@ -130,7 +124,7 @@ public class DumpHelper {
 
         ipw.increaseIndent();
         for (List<DexContainerFileDexoptStatus> fileStatuses : primaryStatusesByDexPath.values()) {
-            dumpPrimaryDex(ipw, snapshot, fileStatuses, packageName, verifySdmSignatures);
+            dumpPrimaryDex(ipw, snapshot, fileStatuses, packageName);
         }
         if (!secondaryStatusesByDexPath.isEmpty()) {
             ipw.println("known secondary dex files:");
@@ -147,8 +141,7 @@ public class DumpHelper {
 
     private void dumpPrimaryDex(@NonNull IndentingPrintWriter ipw,
             @NonNull PackageManagerLocal.FilteredSnapshot snapshot,
-            List<DexContainerFileDexoptStatus> fileStatuses, @NonNull String packageName,
-            boolean verifySdmSignatures) {
+            List<DexContainerFileDexoptStatus> fileStatuses, @NonNull String packageName) {
         String dexPath = fileStatuses.get(0).getDexContainerFile();
         ipw.printf("path: %s\n", dexPath);
         ipw.increaseIndent();
@@ -156,7 +149,6 @@ public class DumpHelper {
         dumpUsedByOtherApps(ipw, snapshot,
                 mInjector.getDexUseManager().getPrimaryDexLoaders(packageName, dexPath),
                 packageName);
-        dumpSdmStatus(ipw, dexPath, verifySdmSignatures);
         ipw.decreaseIndent();
     }
 
@@ -199,9 +191,9 @@ public class DumpHelper {
     private void dumpFileStatuses(
             @NonNull IndentingPrintWriter ipw, List<DexContainerFileDexoptStatus> fileStatuses) {
         for (DexContainerFileDexoptStatus fileStatus : fileStatuses) {
-            ipw.printf("%s: [status=%s] [reason=%s]%s\n",
-                    VMRuntime.getInstructionSet(fileStatus.getAbi()),
-                    fileStatus.getCompilerFilter(), fileStatus.getCompilationReason(),
+            String isa = VMRuntime.getInstructionSet(fileStatus.getAbi());
+            ipw.printf("%s: [status=%s] [reason=%s]%s\n", isa, fileStatus.getCompilerFilter(),
+                    fileStatus.getCompilationReason(),
                     fileStatus.isPrimaryAbi() ? " [primary-abi]" : "");
             ipw.increaseIndent();
             ipw.printf("[location is %s]\n", fileStatus.getLocationDebugString());
@@ -215,7 +207,7 @@ public class DumpHelper {
         List<DexLoader> otherApps =
                 dexLoaders.stream()
                         .filter(loader -> DexUseManagerLocal.isLoaderOtherApp(loader, packageName))
-                        .collect(Collectors.toList());
+                        .toList();
         if (!otherApps.isEmpty()) {
             ipw.printf("used by other apps: [%s]\n",
                     otherApps.stream()
@@ -225,68 +217,6 @@ public class DumpHelper {
         }
     }
 
-    private void dumpSdmStatus(@NonNull IndentingPrintWriter ipw, @NonNull String dexPath,
-            boolean verifySdmSignatures) {
-        if (!android.content.pm.Flags.cloudCompilationPm()) {
-            return;
-        }
-
-        String sdmPath = getSdmPath(dexPath);
-        String status = "";
-        String signature = "skipped";
-        if (mInjector.fileExists(sdmPath)) {
-            // "Pending" means yet to be picked up by dexopt. For now, "pending" is the only status
-            // because SDM files are not supported yet.
-            status = "pending";
-            // This operation is expensive, so hide it behind a flag.
-            if (verifySdmSignatures) {
-                signature = getSdmSignatureStatus(dexPath, sdmPath);
-            }
-        }
-        if (!status.isEmpty()) {
-            ipw.printf("sdm: [sdm-status=%s] [sdm-signature=%s]\n", status, signature);
-        }
-    }
-
-    // The new API usage is safe because it's guarded by a flag. The "NewApi" lint is wrong because
-    // it's meaningless (b/380891026). We have to work around the lint error because there is no
-    // `isAtLeastB` to check yet.
-    // TODO(jiakaiz): Remove this workaround, change @FlaggedApi to @RequiresApi here, and check
-    // `isAtLeastB` at the call site after B SDK is finalized.
-    @FlaggedApi(android.content.pm.Flags.FLAG_CLOUD_COMPILATION_PM)
-    @SuppressLint("NewApi")
-    @NonNull
-    private String getSdmSignatureStatus(@NonNull String dexPath, @NonNull String sdmPath) {
-        SigningInfo sdmSigningInfo;
-        try {
-            sdmSigningInfo =
-                    mInjector.getVerifiedSigningInfo(sdmPath, SigningInfo.VERSION_SIGNING_BLOCK_V3);
-        } catch (SigningInfoException e) {
-            AsLog.w("Failed to verify SDM signature", e);
-            return "invalid-sdm-signature";
-        }
-
-        SigningInfo apkSigningInfo;
-        try {
-            apkSigningInfo =
-                    mInjector.getVerifiedSigningInfo(dexPath, SigningInfo.VERSION_SIGNING_BLOCK_V3);
-        } catch (SigningInfoException e) {
-            AsLog.w("Failed to verify SDM signature", e);
-            return "invalid-apk-signature";
-        }
-
-        if (!sdmSigningInfo.signersMatchExactly(apkSigningInfo)) {
-            return "mismatched-signers";
-        }
-
-        return "verified";
-    }
-
-    @NonNull
-    private static String getSdmPath(@NonNull String dexPath) {
-        return Utils.replaceFileExtension(dexPath, ArtConstants.SECURE_DEX_METADATA_FILE_EXT);
-    }
-
     @NonNull
     private String getLoaderState(
             @NonNull PackageManagerLocal.FilteredSnapshot snapshot, @NonNull DexLoader loader) {
@@ -322,18 +252,5 @@ public class DumpHelper {
         public DexUseManagerLocal getDexUseManager() {
             return GlobalInjector.getInstance().getDexUseManager();
         }
-
-        public boolean fileExists(@NonNull String path) {
-            return new File(path).exists();
-        }
-
-        // TODO(jiakaiz): See another comment about "NewApi" above.
-        @FlaggedApi(android.content.pm.Flags.FLAG_CLOUD_COMPILATION_PM)
-        @SuppressLint("NewApi")
-        @NonNull
-        public SigningInfo getVerifiedSigningInfo(
-                @NonNull String path, int minAppSigningSchemeVersion) throws SigningInfoException {
-            return PackageManager.getVerifiedSigningInfo(path, minAppSigningSchemeVersion);
-        }
     }
 }
diff --git a/libartservice/service/java/com/android/server/art/PreRebootDexoptJob.java b/libartservice/service/java/com/android/server/art/PreRebootDexoptJob.java
index b7f47543fc..093ea40ec7 100644
--- a/libartservice/service/java/com/android/server/art/PreRebootDexoptJob.java
+++ b/libartservice/service/java/com/android/server/art/PreRebootDexoptJob.java
@@ -17,10 +17,12 @@
 package com.android.server.art;
 
 import static com.android.server.art.model.ArtFlags.ScheduleStatus;
+import static com.android.server.art.prereboot.PreRebootDriver.PreRebootResult;
 import static com.android.server.art.proto.PreRebootStats.Status;
 
 import android.annotation.NonNull;
 import android.annotation.Nullable;
+import android.annotation.SuppressLint;
 import android.app.job.JobInfo;
 import android.app.job.JobParameters;
 import android.app.job.JobScheduler;
@@ -33,6 +35,7 @@ import android.os.PersistableBundle;
 import android.os.RemoteException;
 import android.os.ServiceSpecificException;
 import android.os.SystemProperties;
+import android.os.UpdateEngine;
 import android.provider.DeviceConfig;
 
 import androidx.annotation.RequiresApi;
@@ -71,12 +74,20 @@ public class PreRebootDexoptJob implements ArtServiceJobInterface {
     /** An arbitrary number. Must be unique among all jobs owned by the system uid. */
     public static final int JOB_ID = 27873781;
 
+    private static final long UPDATE_ENGINE_TIMEOUT_MS = 10000;
+
     @NonNull private final Injector mInjector;
 
-    // Job state variables. The monitor of `this` is notified when `mRunningJob` is changed.
+    // Job state variables.
+    // The monitor of `this` is notified when `mRunningJob` or `mIsUpdateEngineReady` is changed.
+    // Also, an optimization to make `triggerUpdateEnginePostinstallAndWait` return early, if
+    // `mCancellationSignal` is fired **before `triggerUpdateEnginePostinstallAndWait` returns**, it
+    // should be guaranteed that the monitor of `this` is notified when it happens.
     // `mRunningJob` and `mCancellationSignal` have the same nullness.
     @GuardedBy("this") @Nullable private CompletableFuture<Void> mRunningJob = null;
     @GuardedBy("this") @Nullable private CancellationSignal mCancellationSignal = null;
+    /** Whether update_engine has mapped snapshot devices. Only applicable to an OTA update. */
+    @GuardedBy("this") private boolean mIsUpdateEngineReady = false;
 
     /** Whether `mRunningJob` is running from the job scheduler's perspective. */
     @GuardedBy("this") private boolean mIsRunningJobKnownByJobScheduler = false;
@@ -84,7 +95,10 @@ public class PreRebootDexoptJob implements ArtServiceJobInterface {
     /** The slot that contains the OTA update, "_a" or "_b", or null for a Mainline update. */
     @GuardedBy("this") @Nullable private String mOtaSlot = null;
 
-    /** Whether to map/unmap snapshots. Only applicable to an OTA update. */
+    /**
+     * Whether to map/unmap snapshots ourselves rather than using update_engine. Only applicable to
+     * an OTA update. For legacy use only.
+     */
     @GuardedBy("this") private boolean mMapSnapshotsForOta = false;
 
     /**
@@ -153,14 +167,22 @@ public class PreRebootDexoptJob implements ArtServiceJobInterface {
         Runnable onJobFinishedLocked = () -> {
             Utils.check(mIsRunningJobKnownByJobScheduler);
             mIsRunningJobKnownByJobScheduler = false;
-            // If it failed, it means something went wrong, so we don't reschedule the job because
-            // it will likely fail again. If it's cancelled, the job will be rescheduled because the
-            // return value of `onStopJob` will be respected, and this call will be ignored.
+            // There can be four cases when we reach here:
+            // 1. The job has completed: No need to reschedule.
+            // 2. The job failed: It means something went wrong, so we don't reschedule the job
+            //    because it will likely fail again.
+            // 3. The job was killed by update_engine, probably because the OTA was revoked: We
+            //    should definitely give up.
+            // 4. The job was cancelled by the job scheduler: The job will be rescheduled regardless
+            //    of the arguments we pass here because the return value of `onStopJob` will be
+            //    respected, and this call will be ignored.
+            // Therefore, we can always pass `false` to the `wantsReschedule` parameter.
             jobService.jobFinished(params, false /* wantsReschedule */);
         };
-        // No need to handle exceptions thrown by the future because exceptions are handled inside
-        // the future itself.
-        startLocked(onJobFinishedLocked);
+        startLocked(onJobFinishedLocked, false /* isUpdateEngineReady */).exceptionally(t -> {
+            AsLog.e("Fatal error", t);
+            return null;
+        });
     }
 
     @Override
@@ -194,7 +216,10 @@ public class PreRebootDexoptJob implements ArtServiceJobInterface {
         cancelAnyLocked();
         resetLocked();
         updateOtaSlotLocked(otaSlot);
-        mMapSnapshotsForOta = true;
+        // If we can't call update_engine to map snapshot devices, then we have to map snapshot
+        // devices ourselves. This only happens on a few OEM devices that have
+        // "dalvik.vm.pr_dexopt_async_for_ota=true" and only on Android V.
+        mMapSnapshotsForOta = !android.os.Flags.updateEngineApi();
         return scheduleLocked();
     }
 
@@ -202,23 +227,28 @@ public class PreRebootDexoptJob implements ArtServiceJobInterface {
      * Same as {@link #onUpdateReady}, but starts the job immediately, instead of going through the
      * job scheduler.
      *
-     * @param mapSnapshotsForOta whether to map/unmap snapshots. Only applicable to an OTA update.
+     * @param isUpdateEngineReady whether update_engine has mapped snapshot devices. Only applicable
+     *         to an OTA update.
      * @return The future of the job, or null if Pre-reboot Dexopt is not enabled.
      */
     @Nullable
     public synchronized CompletableFuture<Void> onUpdateReadyStartNow(
-            @Nullable String otaSlot, boolean mapSnapshotsForOta) {
+            @Nullable String otaSlot, boolean isUpdateEngineReady) {
         cancelAnyLocked();
         resetLocked();
         updateOtaSlotLocked(otaSlot);
-        mMapSnapshotsForOta = mapSnapshotsForOta;
+        // If update_engine hasn't mapped snapshot devices and we can't call update_engine to map
+        // snapshot devices, then we have to map snapshot devices ourselves. This only happens on
+        // the `pm art pr-dexopt-job --run` command for local development purposes and only on
+        // Android V.
+        mMapSnapshotsForOta = !isUpdateEngineReady && !android.os.Flags.updateEngineApi();
         if (!isEnabled()) {
             mInjector.getStatsReporter().recordJobNotScheduled(
                     Status.STATUS_NOT_SCHEDULED_DISABLED, isOtaUpdate());
             return null;
         }
         mInjector.getStatsReporter().recordJobScheduled(false /* isAsync */, isOtaUpdate());
-        return startLocked(null /* onJobFinishedLocked */);
+        return startLocked(null /* onJobFinishedLocked */, isUpdateEngineReady);
     }
 
     public synchronized void test() {
@@ -349,18 +379,41 @@ public class PreRebootDexoptJob implements ArtServiceJobInterface {
      */
     @GuardedBy("this")
     @NonNull
-    private CompletableFuture<Void> startLocked(@Nullable Runnable onJobFinishedLocked) {
+    private CompletableFuture<Void> startLocked(
+            @Nullable Runnable onJobFinishedLocked, boolean isUpdateEngineReady) {
         Utils.check(mRunningJob == null);
 
         String otaSlot = mOtaSlot;
         boolean mapSnapshotsForOta = mMapSnapshotsForOta;
         var cancellationSignal = mCancellationSignal = new CancellationSignal();
+        mIsUpdateEngineReady = isUpdateEngineReady;
         mRunningJob = new CompletableFuture().runAsync(() -> {
             markHasStarted(true);
+            PreRebootStatsReporter statsReporter = mInjector.getStatsReporter();
             try {
-                mInjector.getPreRebootDriver().run(otaSlot, mapSnapshotsForOta, cancellationSignal);
+                statsReporter.recordJobStarted();
+                if (otaSlot != null && !isUpdateEngineReady && !mapSnapshotsForOta) {
+                    triggerUpdateEnginePostinstallAndWait();
+                    synchronized (this) {
+                        // This check is not strictly necessary, but is an optimization to return
+                        // early.
+                        if (mCancellationSignal.isCanceled()) {
+                            // The stats reporter translates success=true to STATUS_CANCELLED.
+                            statsReporter.recordJobEnded(new PreRebootResult(true /* success */));
+                            return;
+                        }
+                        Utils.check(mIsUpdateEngineReady);
+                    }
+                }
+                PreRebootResult result = mInjector.getPreRebootDriver().run(
+                        otaSlot, mapSnapshotsForOta, cancellationSignal);
+                statsReporter.recordJobEnded(result);
+            } catch (UpdateEngineException e) {
+                AsLog.e("update_engine error", e);
+                statsReporter.recordJobEnded(new PreRebootResult(false /* success */));
             } catch (RuntimeException e) {
-                AsLog.e("Fatal error", e);
+                statsReporter.recordJobEnded(new PreRebootResult(false /* success */));
+                throw e;
             } finally {
                 synchronized (this) {
                     if (onJobFinishedLocked != null) {
@@ -372,6 +425,7 @@ public class PreRebootDexoptJob implements ArtServiceJobInterface {
                     }
                     mRunningJob = null;
                     mCancellationSignal = null;
+                    mIsUpdateEngineReady = false;
                     this.notifyAll();
                 }
             }
@@ -380,6 +434,60 @@ public class PreRebootDexoptJob implements ArtServiceJobInterface {
         return mRunningJob;
     }
 
+    // The new API usage is safe because it's guarded by a flag. The "NewApi" lint is wrong because
+    // it's meaningless (b/380891026). We can't change the flag check to `isAtLeastB` because we use
+    // `SetFlagsRule` in tests to test the behavior with and without the API support.
+    @SuppressLint("NewApi")
+    private void triggerUpdateEnginePostinstallAndWait() throws UpdateEngineException {
+        if (!android.os.Flags.updateEngineApi()) {
+            // Should never happen.
+            throw new UnsupportedOperationException();
+        }
+        // When we need snapshot devices, we trigger update_engine postinstall. update_engine will
+        // map the snapshot devices for us and run the postinstall script, which will call
+        // `pm art on-ota-staged --start` to notify us that the snapshot device are ready.
+        // See art/libartservice/service/README.internal.md for typical flows.
+        AsLog.i("Waiting for update_engine to map snapshots...");
+        try {
+            mInjector.getUpdateEngine().triggerPostinstall("system" /* partition */);
+        } catch (ServiceSpecificException e) {
+            throw new UpdateEngineException("Failed to trigger postinstall: " + e.getMessage());
+        }
+        long startTime = System.currentTimeMillis();
+        synchronized (this) {
+            while (true) {
+                if (mIsUpdateEngineReady || mCancellationSignal.isCanceled()) {
+                    return;
+                }
+                long remainingTime =
+                        UPDATE_ENGINE_TIMEOUT_MS - (System.currentTimeMillis() - startTime);
+                if (remainingTime <= 0) {
+                    throw new UpdateEngineException("Timed out while waiting for update_engine");
+                }
+                try {
+                    this.wait(remainingTime);
+                } catch (InterruptedException e) {
+                    AsLog.wtf("Interrupted", e);
+                }
+            }
+        }
+    }
+
+    @Nullable
+    public CompletableFuture<Void> notifyUpdateEngineReady() {
+        synchronized (this) {
+            if (mRunningJob == null) {
+                AsLog.e("No waiting job found");
+                return null;
+            }
+            AsLog.i("update_engine finished mapping snapshots");
+            mIsUpdateEngineReady = true;
+            // Notify triggerUpdateEnginePostinstallAndWait to stop waiting.
+            this.notifyAll();
+            return mRunningJob;
+        }
+    }
+
     /**
      * Cancels the given job and waits for it to exit, if it's running. Temporarily releases the
      * lock when waiting for the job to exit.
@@ -395,6 +503,9 @@ public class PreRebootDexoptJob implements ArtServiceJobInterface {
         while (mRunningJob == job) {
             if (!mCancellationSignal.isCanceled()) {
                 mCancellationSignal.cancel();
+                // This is not strictly necessary, but is an optimization to make
+                // `triggerUpdateEnginePostinstallAndWait` return early.
+                this.notifyAll();
                 AsLog.i("Job cancelled");
             }
             try {
@@ -421,6 +532,9 @@ public class PreRebootDexoptJob implements ArtServiceJobInterface {
         while (mRunningJob != null) {
             if (!mCancellationSignal.isCanceled()) {
                 mCancellationSignal.cancel();
+                // This is not strictly necessary, but is an optimization to make
+                // `triggerUpdateEnginePostinstallAndWait` return early.
+                this.notifyAll();
                 AsLog.i("Job cancelled");
             }
             try {
@@ -471,6 +585,10 @@ public class PreRebootDexoptJob implements ArtServiceJobInterface {
     }
 
     public boolean isAsyncForOta() {
+        if (android.os.Flags.updateEngineApi()) {
+            return true;
+        }
+        // Legacy flag in Android V.
         return SystemProperties.getBoolean("dalvik.vm.pr_dexopt_async_for_ota", false /* def */);
     }
 
@@ -506,6 +624,12 @@ public class PreRebootDexoptJob implements ArtServiceJobInterface {
         return mOtaSlot != null;
     }
 
+    private static class UpdateEngineException extends Exception {
+        public UpdateEngineException(@NonNull String message) {
+            super(message);
+        }
+    }
+
     /**
      * Injector pattern for testing purpose.
      *
@@ -548,5 +672,10 @@ public class PreRebootDexoptJob implements ArtServiceJobInterface {
                 @NonNull String namespace, @NonNull String name, boolean defaultValue) {
             return DeviceConfig.getBoolean(namespace, name, defaultValue);
         }
+
+        @NonNull
+        public UpdateEngine getUpdateEngine() {
+            return new UpdateEngine();
+        }
     }
 }
diff --git a/libartservice/service/java/com/android/server/art/PrimaryDexUtils.java b/libartservice/service/java/com/android/server/art/PrimaryDexUtils.java
index c37fdbe6e3..3fb8bda6cb 100644
--- a/libartservice/service/java/com/android/server/art/PrimaryDexUtils.java
+++ b/libartservice/service/java/com/android/server/art/PrimaryDexUtils.java
@@ -60,7 +60,7 @@ public class PrimaryDexUtils {
                 .stream()
                 .map(builder -> builder.build())
                 .filter(info -> info.hasCode())
-                .collect(Collectors.toList());
+                .toList();
     }
 
     /**
@@ -74,7 +74,7 @@ public class PrimaryDexUtils {
                 .stream()
                 .map(builder -> builder.buildDetailed())
                 .filter(info -> info.hasCode())
-                .collect(Collectors.toList());
+                .toList();
     }
 
     /** Returns the basic information about a dex file specified by {@code splitName}. */
diff --git a/libartservice/service/java/com/android/server/art/PrimaryDexopter.java b/libartservice/service/java/com/android/server/art/PrimaryDexopter.java
index eb200592aa..94ab94e213 100644
--- a/libartservice/service/java/com/android/server/art/PrimaryDexopter.java
+++ b/libartservice/service/java/com/android/server/art/PrimaryDexopter.java
@@ -38,6 +38,7 @@ import com.android.modules.utils.pm.PackageStateModulesUtils;
 import com.android.server.art.model.ArtFlags;
 import com.android.server.art.model.Config;
 import com.android.server.art.model.DexoptParams;
+import com.android.server.art.model.DexoptResult;
 import com.android.server.pm.pkg.AndroidPackage;
 import com.android.server.pm.pkg.PackageState;
 
@@ -178,6 +179,44 @@ public class PrimaryDexopter extends Dexopter<DetailedPrimaryDexInfo> {
         return AidlUtils.buildDexMetadataPath(dexInfo.dexPath());
     }
 
+    @Override
+    protected void onDexoptStart(@NonNull DetailedPrimaryDexInfo dexInfo) throws RemoteException {
+        if (!mInjector.isPreReboot() && android.content.pm.Flags.cloudCompilationPm()) {
+            boolean isInDalvikCache = isInDalvikCache();
+            for (Abi abi : getAllAbis(dexInfo)) {
+                maybeCreateSdc(dexInfo, abi.isa(), isInDalvikCache);
+            }
+        }
+    }
+
+    private void maybeCreateSdc(@NonNull DetailedPrimaryDexInfo dexInfo, @NonNull String isa,
+            boolean isInDalvikCache) throws RemoteException {
+        // SDC file doesn't contain sensitive data, so it can always to public.
+        PermissionSettings permissionSettings =
+                getPermissionSettings(dexInfo, true /* canBePublic */);
+        OutputSecureDexMetadataCompanion outputSdc =
+                AidlUtils.buildOutputSecureDexMetadataCompanion(
+                        dexInfo.dexPath(), isa, isInDalvikCache, permissionSettings);
+
+        try {
+            mInjector.getArtd().maybeCreateSdc(outputSdc);
+        } catch (ServiceSpecificException e) {
+            AsLog.e("Failed to create sdc for " + AidlUtils.toString(outputSdc.sdcPath), e);
+        }
+    }
+
+    @Override
+    protected void onDexoptTargetResult(@NonNull DexoptTarget<DetailedPrimaryDexInfo> target,
+            @DexoptResult.DexoptResultStatus int status) throws RemoteException {
+        // An optimization to release disk space as soon as possible. The SDM and SDC files would be
+        // deleted by the file GC anyway if not deleted here.
+        if (status == DexoptResult.DEXOPT_PERFORMED && !mInjector.isPreReboot()) {
+            mInjector.getArtd().deleteSdmSdcFiles(
+                    AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                            target.dexInfo().dexPath(), target.isa(), target.isInDalvikCache()));
+        }
+    }
+
     private boolean isSharedLibrary() {
         return PackageStateModulesUtils.isLoadableInOtherProcesses(mPkgState, true /* codeOnly */);
     }
diff --git a/libartservice/service/java/com/android/server/art/Utils.java b/libartservice/service/java/com/android/server/art/Utils.java
index e042c3e106..3dcbc54141 100644
--- a/libartservice/service/java/com/android/server/art/Utils.java
+++ b/libartservice/service/java/com/android/server/art/Utils.java
@@ -16,11 +16,14 @@
 
 package com.android.server.art;
 
+import static android.app.ActivityManager.RunningAppProcessInfo;
+
 import static com.android.server.art.ProfilePath.TmpProfilePath;
 
 import android.R;
 import android.annotation.NonNull;
 import android.annotation.Nullable;
+import android.app.ActivityManager;
 import android.app.role.RoleManager;
 import android.apphibernation.AppHibernationManager;
 import android.content.Context;
@@ -33,6 +36,7 @@ import android.os.ServiceSpecificException;
 import android.os.SystemClock;
 import android.os.SystemProperties;
 import android.os.Trace;
+import android.os.UserHandle;
 import android.os.UserManager;
 import android.text.TextUtils;
 import android.util.Log;
@@ -58,17 +62,18 @@ import java.io.IOException;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
 import java.util.Comparator;
 import java.util.List;
+import java.util.Objects;
 import java.util.Set;
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.Executor;
 import java.util.concurrent.Future;
 import java.util.function.Supplier;
-import java.util.stream.Collectors;
 
 /** @hide */
 @RequiresApi(Build.VERSION_CODES.UPSIDE_DOWN_CAKE)
@@ -138,7 +143,7 @@ public final class Utils {
                         -> Abi.create(name, VMRuntime.getInstructionSet(name),
                                 name.equals(pkgPrimaryAbi.name())))
                 .sorted(Comparator.comparing(Abi::isPrimaryAbi).reversed())
-                .collect(Collectors.toList());
+                .toList();
     }
 
     @NonNull
@@ -200,6 +205,14 @@ public final class Utils {
                 || abiName.equals(Constants.getNative32BitAbi());
     }
 
+    public static List<String> getNativeIsas() {
+        return Arrays.asList(Constants.getNative64BitAbi(), Constants.getNative32BitAbi())
+                .stream()
+                .filter(Objects::nonNull)
+                .map(VMRuntime::getInstructionSet)
+                .toList();
+    }
+
     /**
      * Returns whether the artifacts of the primary dex files should be in the global dalvik-cache
      * directory.
@@ -514,6 +527,21 @@ public final class Utils {
         return ((pos != -1 && slashPos == -1) ? path.substring(0, pos) : path) + newExtension;
     }
 
+    public static List<RunningAppProcessInfo> getRunningProcessInfoForPackage(
+            @NonNull ActivityManager am, @NonNull PackageState pkgState) {
+        return am.getRunningAppProcesses()
+                .stream()
+                .filter(info -> UserHandle.getAppId(info.uid) == pkgState.getAppId())
+                .filter(info
+                        -> Arrays.stream(info.pkgList)
+                                .anyMatch(name -> name.equals(pkgState.getPackageName())))
+                // Filter by importance to only include running processes.
+                // The intention of this filter is to filter out `IMPORTANCE_CACHED`. Cached
+                // processes can be frozen by Cached apps freezer and don't respond to signals.
+                .filter(info -> info.importance <= RunningAppProcessInfo.IMPORTANCE_SERVICE)
+                .toList();
+    }
+
     @AutoValue
     public abstract static class Abi {
         static @NonNull Abi create(
diff --git a/libartservice/service/java/com/android/server/art/prereboot/PreRebootDriver.java b/libartservice/service/java/com/android/server/art/prereboot/PreRebootDriver.java
index 7c1a1c55fa..c29e0b4b2c 100644
--- a/libartservice/service/java/com/android/server/art/prereboot/PreRebootDriver.java
+++ b/libartservice/service/java/com/android/server/art/prereboot/PreRebootDriver.java
@@ -86,20 +86,16 @@ public class PreRebootDriver {
     }
 
     /**
-     * Runs Pre-reboot Dexopt and returns whether it is successful. Returns false if Pre-reboot
-     * dexopt failed, the system requirement check failed, or system requirements are not met.
+     * Runs Pre-reboot Dexopt and returns the result.
      *
      * @param otaSlot The slot that contains the OTA update, "_a" or "_b", or null for a Mainline
      *         update.
      * @param mapSnapshotsForOta Whether to map/unmap snapshots. Only applicable to an OTA update.
      */
-    public boolean run(@Nullable String otaSlot, boolean mapSnapshotsForOta,
+    public @NonNull PreRebootResult run(@Nullable String otaSlot, boolean mapSnapshotsForOta,
             @NonNull CancellationSignal cancellationSignal) {
-        var statsReporter = new PreRebootStatsReporter();
-        boolean success = false;
         boolean systemRequirementCheckFailed = false;
         try {
-            statsReporter.recordJobStarted();
             try (var snapshot = mInjector.getPackageManagerLocal().withFilteredSnapshot()) {
                 BatchDexoptParams params = mInjector.getArtManagerLocal().getBatchDexoptParams(
                         snapshot, ReasonMapping.REASON_PRE_REBOOT_DEXOPT, cancellationSignal);
@@ -108,8 +104,7 @@ public class PreRebootDriver {
                     runFromChroot(cancellationSignal, snapshot, params);
                 }
             }
-            success = true;
-            return true;
+            return new PreRebootResult(true /* success */);
         } catch (RemoteException e) {
             Utils.logArtdException(e);
         } catch (ServiceSpecificException e) {
@@ -139,11 +134,9 @@ public class PreRebootDriver {
                 Utils.logArtdException(e);
             } catch (ServiceSpecificException | IOException e) {
                 AsLog.e("Failed to tear down chroot", e);
-            } finally {
-                statsReporter.recordJobEnded(success, systemRequirementCheckFailed);
             }
         }
-        return false;
+        return new PreRebootResult(false /* success */, systemRequirementCheckFailed);
     }
 
     public void test() {
@@ -256,6 +249,16 @@ public class PreRebootDriver {
                         params.toProto().toByteArray());
     }
 
+    /**
+     * @param success whether Pre-reboot Dexopt is successful. False if Pre-reboot dexopt failed,
+     *         the system requirement check failed, or system requirements are not met.
+     */
+    public record PreRebootResult(boolean success, boolean systemRequirementCheckFailed) {
+        public PreRebootResult(boolean success) {
+            this(success, false /* systemRequirementCheckFailed */);
+        }
+    }
+
     /**
      * Injector pattern for testing purpose.
      *
diff --git a/libartservice/service/java/com/android/server/art/prereboot/PreRebootStatsReporter.java b/libartservice/service/java/com/android/server/art/prereboot/PreRebootStatsReporter.java
index fbf242db62..ec5c1acf31 100644
--- a/libartservice/service/java/com/android/server/art/prereboot/PreRebootStatsReporter.java
+++ b/libartservice/service/java/com/android/server/art/prereboot/PreRebootStatsReporter.java
@@ -16,6 +16,7 @@
 
 package com.android.server.art.prereboot;
 
+import static com.android.server.art.prereboot.PreRebootDriver.PreRebootResult;
 import static com.android.server.art.proto.PreRebootStats.JobRun;
 import static com.android.server.art.proto.PreRebootStats.JobType;
 import static com.android.server.art.proto.PreRebootStats.Status;
@@ -141,7 +142,7 @@ public class PreRebootStatsReporter {
         }
     }
 
-    public void recordJobEnded(boolean success, boolean systemRequirementCheckFailed) {
+    public void recordJobEnded(@NonNull PreRebootResult result) {
         PreRebootStats.Builder statsBuilder = load();
         if (statsBuilder.getStatus() == Status.STATUS_UNKNOWN) {
             // Failed to load, the error is already logged.
@@ -157,7 +158,7 @@ public class PreRebootStatsReporter {
                 mInjector.getCurrentTimeMillis());
 
         Status status;
-        if (success) {
+        if (result.success()) {
             // The job is cancelled if it hasn't done package scanning (total package count is 0),
             // or it's interrupted in the middle of package processing (package counts don't add up
             // to the total).
@@ -172,7 +173,7 @@ public class PreRebootStatsReporter {
                 status = Status.STATUS_CANCELLED;
             }
         } else {
-            if (systemRequirementCheckFailed) {
+            if (result.systemRequirementCheckFailed()) {
                 status = Status.STATUS_ABORTED_SYSTEM_REQUIREMENTS;
             } else {
                 status = Status.STATUS_FAILED;
diff --git a/libartservice/service/javatests/com/android/server/art/ArtManagedInstallFileHelperTest.java b/libartservice/service/javatests/com/android/server/art/ArtManagedInstallFileHelperTest.java
index 4b13716eaa..b0b52fae9e 100644
--- a/libartservice/service/javatests/com/android/server/art/ArtManagedInstallFileHelperTest.java
+++ b/libartservice/service/javatests/com/android/server/art/ArtManagedInstallFileHelperTest.java
@@ -19,10 +19,15 @@ package com.android.server.art;
 import static com.google.common.truth.Truth.assertThat;
 
 import static org.junit.Assert.assertThrows;
+import static org.mockito.Mockito.lenient;
 
 import androidx.test.filters.SmallTest;
 import androidx.test.runner.AndroidJUnit4;
 
+import com.android.server.art.testing.StaticMockitoRule;
+
+import org.junit.Before;
+import org.junit.Rule;
 import org.junit.Test;
 import org.junit.runner.RunWith;
 
@@ -31,33 +36,45 @@ import java.util.List;
 @SmallTest
 @RunWith(AndroidJUnit4.class)
 public class ArtManagedInstallFileHelperTest {
+    @Rule public StaticMockitoRule mockitoRule = new StaticMockitoRule(Constants.class);
+
+    @Before
+    public void setUp() throws Exception {
+        lenient().when(Constants.getNative64BitAbi()).thenReturn("arm64-v8a");
+        lenient().when(Constants.getNative32BitAbi()).thenReturn("armeabi-v7a");
+    }
+
     @Test
     public void testIsArtManaged() throws Exception {
         assertThat(ArtManagedInstallFileHelper.isArtManaged("/foo/bar.dm")).isTrue();
         assertThat(ArtManagedInstallFileHelper.isArtManaged("/foo/bar.prof")).isTrue();
         assertThat(ArtManagedInstallFileHelper.isArtManaged("/foo/bar.sdm")).isTrue();
+        assertThat(ArtManagedInstallFileHelper.isArtManaged("/foo/bar.arm.sdm")).isTrue();
+        assertThat(ArtManagedInstallFileHelper.isArtManaged("/foo/bar.arm64.sdm")).isTrue();
         assertThat(ArtManagedInstallFileHelper.isArtManaged("/foo/bar.abc")).isFalse();
     }
 
     @Test
     public void testFilterPathsForApk() throws Exception {
         assertThat(ArtManagedInstallFileHelper.filterPathsForApk(
-                           List.of("/foo/bar.dm", "/foo/bar.prof", "/foo/bar.sdm", "/foo/bar.abc",
-                                   "/foo/baz.dm"),
+                           List.of("/foo/bar.dm", "/foo/bar.prof", "/foo/bar.sdm",
+                                   "/foo/bar.x86_64.sdm", "/foo/bar.arm.sdm", "/foo/bar.arm64.sdm",
+                                   "/foo/bar.abc", "/foo/baz.dm"),
                            "/foo/bar.apk"))
-                .containsExactly("/foo/bar.dm", "/foo/bar.prof", "/foo/bar.sdm");
+                .containsExactly(
+                        "/foo/bar.dm", "/foo/bar.prof", "/foo/bar.arm.sdm", "/foo/bar.arm64.sdm");
 
         // Filenames don't match.
         assertThat(ArtManagedInstallFileHelper.filterPathsForApk(
-                           List.of("/foo/bar.dm", "/foo/bar.prof", "/foo/bar.sdm", "/foo/bar.abc",
-                                   "/foo/baz.dm"),
+                           List.of("/foo/bar.dm", "/foo/bar.prof", "/foo/bar.arm64.sdm",
+                                   "/foo/bar.abc", "/foo/baz.dm"),
                            "/foo/qux.apk"))
                 .isEmpty();
 
         // Directories don't match.
         assertThat(ArtManagedInstallFileHelper.filterPathsForApk(
-                           List.of("/foo/bar.dm", "/foo/bar.prof", "/foo/bar.sdm", "/foo/bar.abc",
-                                   "/foo/baz.dm"),
+                           List.of("/foo/bar.dm", "/foo/bar.prof", "/foo/bar.arm64.sdm",
+                                   "/foo/bar.abc", "/foo/baz.dm"),
                            "/quz/bar.apk"))
                 .isEmpty();
     }
@@ -70,9 +87,23 @@ public class ArtManagedInstallFileHelperTest {
         assertThat(ArtManagedInstallFileHelper.getTargetPathForApk(
                            "/foo/bar.prof", "/somewhere/base.apk"))
                 .isEqualTo("/somewhere/base.prof");
+        assertThat(ArtManagedInstallFileHelper.getTargetPathForApk(
+                           "/foo/bar.arm.sdm", "/somewhere/base.apk"))
+                .isEqualTo("/somewhere/base.arm.sdm");
+        assertThat(ArtManagedInstallFileHelper.getTargetPathForApk(
+                           "/foo/bar.arm64.sdm", "/somewhere/base.apk"))
+                .isEqualTo("/somewhere/base.arm64.sdm");
+
+        // None or invalid ISA.
         assertThat(ArtManagedInstallFileHelper.getTargetPathForApk(
                            "/foo/bar.sdm", "/somewhere/base.apk"))
-                .isEqualTo("/somewhere/base.sdm");
+                .isEqualTo("/somewhere/bar.sdm");
+        assertThat(ArtManagedInstallFileHelper.getTargetPathForApk(
+                           "/foo/bar.x86_64.sdm", "/somewhere/base.apk"))
+                .isEqualTo("/somewhere/bar.x86_64.sdm");
+        assertThat(ArtManagedInstallFileHelper.getTargetPathForApk(
+                           "/foo/bar.invalid-isa.sdm", "/somewhere/base.apk"))
+                .isEqualTo("/somewhere/bar.invalid-isa.sdm");
 
         assertThrows(IllegalArgumentException.class, () -> {
             ArtManagedInstallFileHelper.getTargetPathForApk("/foo/bar.abc", "/somewhere/base.apk");
diff --git a/libartservice/service/javatests/com/android/server/art/ArtManagerLocalTest.java b/libartservice/service/javatests/com/android/server/art/ArtManagerLocalTest.java
index 8841de7fed..754208a05f 100644
--- a/libartservice/service/javatests/com/android/server/art/ArtManagerLocalTest.java
+++ b/libartservice/service/javatests/com/android/server/art/ArtManagerLocalTest.java
@@ -16,9 +16,11 @@
 
 package com.android.server.art;
 
+import static android.app.ActivityManager.RunningAppProcessInfo;
 import static android.os.ParcelFileDescriptor.AutoCloseInputStream;
 
 import static com.android.server.art.DexUseManagerLocal.CheckedSecondaryDexInfo;
+import static com.android.server.art.ProfilePath.PrimaryCurProfilePath;
 import static com.android.server.art.model.DexoptResult.DexContainerFileDexoptResult;
 import static com.android.server.art.model.DexoptResult.PackageDexoptResult;
 import static com.android.server.art.model.DexoptStatus.DexContainerFileDexoptStatus;
@@ -37,6 +39,7 @@ import static org.mockito.Mockito.anyBoolean;
 import static org.mockito.Mockito.anyInt;
 import static org.mockito.Mockito.argThat;
 import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.inOrder;
 import static org.mockito.Mockito.isNull;
 import static org.mockito.Mockito.lenient;
 import static org.mockito.Mockito.mock;
@@ -46,6 +49,7 @@ import static org.mockito.Mockito.times;
 import static org.mockito.Mockito.verify;
 import static org.mockito.Mockito.when;
 
+import android.app.ActivityManager;
 import android.apphibernation.AppHibernationManager;
 import android.content.BroadcastReceiver;
 import android.content.Context;
@@ -58,6 +62,7 @@ import android.os.SystemProperties;
 import android.os.UserHandle;
 import android.os.UserManager;
 import android.os.storage.StorageManager;
+import android.system.OsConstants;
 
 import androidx.test.filters.SmallTest;
 
@@ -88,6 +93,7 @@ import org.junit.runners.Parameterized;
 import org.junit.runners.Parameterized.Parameter;
 import org.junit.runners.Parameterized.Parameters;
 import org.mockito.ArgumentCaptor;
+import org.mockito.InOrder;
 import org.mockito.Mock;
 
 import java.io.File;
@@ -120,6 +126,7 @@ public class ArtManagerLocalTest {
             CURRENT_TIME_MS - TimeUnit.DAYS.toMillis(INACTIVE_DAYS) + 1;
     private static final long NOT_RECENT_TIME_MS =
             CURRENT_TIME_MS - TimeUnit.DAYS.toMillis(INACTIVE_DAYS) - 1;
+    private static final int APP_ID = 1000;
 
     @Rule
     public StaticMockitoRule mockitoRule = new StaticMockitoRule(
@@ -140,6 +147,7 @@ public class ArtManagerLocalTest {
     @Mock private Context mContext;
     @Mock private PreRebootDexoptJob mPreRebootDexoptJob;
     @Mock private PreRebootStatsReporter.Injector mPreRebootStatsReporterInjector;
+    @Mock private ActivityManager mActivityManager;
     private PackageState mPkgState1;
     private AndroidPackage mPkg1;
     private CheckedSecondaryDexInfo mPkg1SecondaryDexInfo1;
@@ -188,6 +196,7 @@ public class ArtManagerLocalTest {
                 .when(mInjector.getPreRebootStatsReporter())
                 .thenAnswer(
                         invocation -> new PreRebootStatsReporter(mPreRebootStatsReporterInjector));
+        lenient().when(mInjector.getActivityManager()).thenReturn(mActivityManager);
 
         lenient().when(mArtFileManagerInjector.getArtd()).thenReturn(mArtd);
         lenient().when(mArtFileManagerInjector.getUserManager()).thenReturn(mUserManager);
@@ -324,6 +333,15 @@ public class ArtManagerLocalTest {
         verify(mArtd).deleteArtifacts(deepEq(AidlUtils.buildArtifactsPathAsInput(
                 "/data/user/0/foo/not_found.apk", "arm64", false /* isInDalvikCache */)));
 
+        verify(mArtd).deleteSdmSdcFiles(deepEq(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                "/somewhere/app/foo/base.apk", "arm64", mIsInDalvikCache)));
+        verify(mArtd).deleteSdmSdcFiles(deepEq(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                "/somewhere/app/foo/base.apk", "arm", mIsInDalvikCache)));
+        verify(mArtd).deleteSdmSdcFiles(deepEq(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                "/somewhere/app/foo/split_0.apk", "arm64", mIsInDalvikCache)));
+        verify(mArtd).deleteSdmSdcFiles(deepEq(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                "/somewhere/app/foo/split_0.apk", "arm", mIsInDalvikCache)));
+
         verify(mArtd).deleteRuntimeArtifacts(deepEq(AidlUtils.buildRuntimeArtifactsPath(
                 PKG_NAME_1, "/somewhere/app/foo/base.apk", "arm64")));
         verify(mArtd).deleteRuntimeArtifacts(deepEq(AidlUtils.buildRuntimeArtifactsPath(
@@ -335,6 +353,7 @@ public class ArtManagerLocalTest {
 
         // Verify that there are no more calls than the ones above.
         verify(mArtd, times(6)).deleteArtifacts(any());
+        verify(mArtd, times(4)).deleteSdmSdcFiles(any());
         verify(mArtd, times(4)).deleteRuntimeArtifacts(any());
     }
 
@@ -563,6 +582,15 @@ public class ArtManagerLocalTest {
         verify(mArtd).deleteArtifacts(deepEq(AidlUtils.buildArtifactsPathAsInput(
                 "/somewhere/app/foo/split_0.apk", "arm", mIsInDalvikCache)));
 
+        verify(mArtd).deleteSdmSdcFiles(deepEq(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                "/somewhere/app/foo/base.apk", "arm64", mIsInDalvikCache)));
+        verify(mArtd).deleteSdmSdcFiles(deepEq(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                "/somewhere/app/foo/base.apk", "arm", mIsInDalvikCache)));
+        verify(mArtd).deleteSdmSdcFiles(deepEq(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                "/somewhere/app/foo/split_0.apk", "arm64", mIsInDalvikCache)));
+        verify(mArtd).deleteSdmSdcFiles(deepEq(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                "/somewhere/app/foo/split_0.apk", "arm", mIsInDalvikCache)));
+
         verify(mArtd).deleteRuntimeArtifacts(deepEq(AidlUtils.buildRuntimeArtifactsPath(
                 PKG_NAME_1, "/somewhere/app/foo/base.apk", "arm64")));
         verify(mArtd).deleteRuntimeArtifacts(deepEq(AidlUtils.buildRuntimeArtifactsPath(
@@ -1259,6 +1287,7 @@ public class ArtManagerLocalTest {
                                 "arm64", true /* isInDalvikCache */)),
                 inAnyOrderDeepEquals(VdexPath.artifactsPath(AidlUtils.buildArtifactsPathAsInput(
                         "/somewhere/app/foo/split_0.apk", "arm", false /* isInDalvikCache */))),
+                inAnyOrderDeepEquals() /* sdmSdcFilesToKeep */,
                 inAnyOrderDeepEquals(AidlUtils.buildRuntimeArtifactsPath(
                                              PKG_NAME_1, "/somewhere/app/foo/split_0.apk", "arm64"),
                         AidlUtils.buildRuntimeArtifactsPath(
@@ -1266,6 +1295,60 @@ public class ArtManagerLocalTest {
                 eq(keepPreRebootStagedFiles));
     }
 
+    @Test
+    public void testCleanupDmAndSdm() throws Exception {
+        when(mPreRebootDexoptJob.hasStarted()).thenReturn(false);
+
+        // It should keep the SDM file, but not runtime images.
+        doReturn(createGetDexoptStatusResult(
+                         "speed-profile", "cloud", "location", ArtifactsLocation.SDM_NEXT_TO_DEX))
+                .when(mArtd)
+                .getDexoptStatus(eq("/somewhere/app/foo/base.apk"), eq("arm64"), any());
+
+        // It should keep the SDM file, but not runtime images.
+        doReturn(createGetDexoptStatusResult(
+                         "speed-profile", "cloud", "location", ArtifactsLocation.SDM_DALVIK_CACHE))
+                .when(mArtd)
+                .getDexoptStatus(eq("/somewhere/app/foo/base.apk"), eq("arm"), any());
+
+        // It should keep the SDM file and runtime images.
+        doReturn(createGetDexoptStatusResult(
+                         "verify", "cloud", "location", ArtifactsLocation.SDM_NEXT_TO_DEX))
+                .when(mArtd)
+                .getDexoptStatus(eq("/somewhere/app/foo/split_0.apk"), eq("arm64"), any());
+
+        // It should only keep runtime images.
+        doReturn(createGetDexoptStatusResult("verify", "vdex", "location", ArtifactsLocation.DM))
+                .when(mArtd)
+                .getDexoptStatus(eq("/somewhere/app/foo/split_0.apk"), eq("arm"), any());
+
+        // This file is uninteresting in this test.
+        doReturn(createGetDexoptStatusResult(
+                         "run-from-apk", "unknown", "unknown", ArtifactsLocation.NONE_OR_ERROR))
+                .when(mArtd)
+                .getDexoptStatus(eq("/data/user/0/foo/1.apk"), eq("arm64"), any());
+
+        when(mSnapshot.getPackageStates()).thenReturn(Map.of(PKG_NAME_1, mPkgState1));
+        mArtManagerLocal.cleanup(mSnapshot);
+
+        verify(mArtd).cleanup(any() /* profilesToKeep */,
+                inAnyOrderDeepEquals() /* artifactsToKeep */,
+                inAnyOrderDeepEquals() /* vdexFilesToKeep */,
+                inAnyOrderDeepEquals(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                                             "/somewhere/app/foo/base.apk", "arm64",
+                                             false /* isInDalvikCache */),
+                        AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                                "/somewhere/app/foo/base.apk", "arm", true /* isInDalvikCache */),
+                        AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                                "/somewhere/app/foo/split_0.apk", "arm64",
+                                false /* isInDalvikCache */)),
+                inAnyOrderDeepEquals(AidlUtils.buildRuntimeArtifactsPath(
+                                             PKG_NAME_1, "/somewhere/app/foo/split_0.apk", "arm64"),
+                        AidlUtils.buildRuntimeArtifactsPath(
+                                PKG_NAME_1, "/somewhere/app/foo/split_0.apk", "arm")),
+                eq(false) /* keepPreRebootStagedFiles */);
+    }
+
     @Test
     public void testGetArtManagedFileStatsSystem() throws Exception {
         testGetArtManagedFileStats(true /* isSystemOrRootOrShell */);
@@ -1416,6 +1499,64 @@ public class ArtManagerLocalTest {
         verify(mArtd, times(expectedGetProfileSizeCalls)).getProfileSize(any());
     }
 
+    @Test
+    public void testGetArtManagedFileStatsDmAndSdm() throws Exception {
+        // It should count the SDM file, but not runtime images.
+        doReturn(createGetDexoptStatusResult(
+                         "speed-profile", "cloud", "location", ArtifactsLocation.SDM_NEXT_TO_DEX))
+                .when(mArtd)
+                .getDexoptStatus(eq("/somewhere/app/foo/base.apk"), eq("arm64"), any());
+
+        // It should count the SDM file, but not runtime images.
+        doReturn(createGetDexoptStatusResult(
+                         "speed-profile", "cloud", "location", ArtifactsLocation.SDM_DALVIK_CACHE))
+                .when(mArtd)
+                .getDexoptStatus(eq("/somewhere/app/foo/base.apk"), eq("arm"), any());
+
+        // It should count the SDM file and runtime images.
+        doReturn(createGetDexoptStatusResult(
+                         "verify", "cloud", "location", ArtifactsLocation.SDM_NEXT_TO_DEX))
+                .when(mArtd)
+                .getDexoptStatus(eq("/somewhere/app/foo/split_0.apk"), eq("arm64"), any());
+
+        // It should only count runtime images.
+        doReturn(createGetDexoptStatusResult("verify", "vdex", "location", ArtifactsLocation.DM))
+                .when(mArtd)
+                .getDexoptStatus(eq("/somewhere/app/foo/split_0.apk"), eq("arm"), any());
+
+        // This file is uninteresting in this test.
+        doReturn(createGetDexoptStatusResult(
+                         "run-from-apk", "unknown", "unknown", ArtifactsLocation.NONE_OR_ERROR))
+                .when(mArtd)
+                .getDexoptStatus(eq("/data/user/0/foo/1.apk"), eq("arm64"), any());
+
+        // These are counted as TYPE_DEXOPT_ARTIFACT.
+        doReturn(1l << 0).when(mArtd).getSdmFileSize(
+                deepEq(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                        "/somewhere/app/foo/base.apk", "arm64", false /* isInDalvikCache */)));
+        doReturn(1l << 1).when(mArtd).getSdmFileSize(
+                deepEq(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                        "/somewhere/app/foo/base.apk", "arm", true /* isInDalvikCache */)));
+        doReturn(1l << 2).when(mArtd).getSdmFileSize(
+                deepEq(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                        "/somewhere/app/foo/split_0.apk", "arm64", false /* isInDalvikCache */)));
+        doReturn(1l << 3).when(mArtd).getRuntimeArtifactsSize(
+                deepEq(AidlUtils.buildRuntimeArtifactsPath(
+                        PKG_NAME_1, "/somewhere/app/foo/split_0.apk", "arm64")));
+        doReturn(1l << 4).when(mArtd).getRuntimeArtifactsSize(
+                deepEq(AidlUtils.buildRuntimeArtifactsPath(
+                        PKG_NAME_1, "/somewhere/app/foo/split_0.apk", "arm")));
+
+        ArtManagedFileStats stats = mArtManagerLocal.getArtManagedFileStats(mSnapshot, PKG_NAME_1);
+        assertThat(stats.getTotalSizeBytesByType(ArtManagedFileStats.TYPE_DEXOPT_ARTIFACT))
+                .isEqualTo((1l << 0) + (1l << 1) + (1l << 2) + (1l << 3) + (1l << 4));
+
+        verify(mArtd, never()).getArtifactsSize(any());
+        verify(mArtd, never()).getVdexFileSize(any());
+        verify(mArtd, times(3)).getSdmFileSize(any());
+        verify(mArtd, times(2)).getRuntimeArtifactsSize(any());
+    }
+
     @Test
     public void testCommitPreRebootStagedFiles() throws Exception {
         when(mSnapshot.getPackageStates()).thenReturn(Map.of(PKG_NAME_1, mPkgState1));
@@ -1466,6 +1607,57 @@ public class ArtManagerLocalTest {
         verify(mContext, never()).registerReceiver(any(), any());
     }
 
+    @Test
+    public void testFlushProfiles() throws Exception {
+        when(mActivityManager.getRunningAppProcesses())
+                .thenReturn(
+                        List.of(createProcessInfo(1000 /* pid */, "com.example.foo",
+                                        UserHandle.of(0).getUid(APP_ID), new String[] {PKG_NAME_1},
+                                        RunningAppProcessInfo.IMPORTANCE_FOREGROUND),
+                                createProcessInfo(1001 /* pid */, "com.example.foo:service1",
+                                        UserHandle.of(0).getUid(APP_ID), new String[] {PKG_NAME_1},
+                                        RunningAppProcessInfo.IMPORTANCE_FOREGROUND_SERVICE),
+                                createProcessInfo(1002 /* pid */, "com.example.foo:service2",
+                                        UserHandle.of(0).getUid(APP_ID), new String[] {PKG_NAME_1},
+                                        RunningAppProcessInfo.IMPORTANCE_FOREGROUND_SERVICE)),
+                        // A process goes away later.
+                        List.of(createProcessInfo(1000 /* pid */, "com.example.foo",
+                                        UserHandle.of(0).getUid(APP_ID), new String[] {PKG_NAME_1},
+                                        RunningAppProcessInfo.IMPORTANCE_FOREGROUND),
+                                createProcessInfo(1001 /* pid */, "com.example.foo:service1",
+                                        UserHandle.of(0).getUid(APP_ID), new String[] {PKG_NAME_1},
+                                        RunningAppProcessInfo.IMPORTANCE_FOREGROUND_SERVICE)));
+
+        PrimaryCurProfilePath profilePath = AidlUtils.buildPrimaryCurProfilePath(
+                0 /* userId */, PKG_NAME_1, PrimaryDexUtils.getProfileName(null /* splitName */));
+
+        var notificationForPid1000 = mock(IArtdNotification.class);
+        var notificationForPid1001 = mock(IArtdNotification.class);
+        var notificationForPid1002 = mock(IArtdNotification.class);
+        doReturn(notificationForPid1000)
+                .when(mArtd)
+                .initProfileSaveNotification(deepEq(profilePath), eq(1000) /* pid */);
+        doReturn(notificationForPid1001)
+                .when(mArtd)
+                .initProfileSaveNotification(deepEq(profilePath), eq(1001) /* pid */);
+        doReturn(notificationForPid1002)
+                .when(mArtd)
+                .initProfileSaveNotification(deepEq(profilePath), eq(1002) /* pid */);
+        when(notificationForPid1000.wait(anyInt())).thenReturn(true);
+        when(notificationForPid1001.wait(anyInt())).thenReturn(true);
+
+        assertThat(mArtManagerLocal.flushProfiles(mSnapshot, PKG_NAME_1)).isTrue();
+
+        InOrder inOrder = inOrder(mInjector, notificationForPid1000, notificationForPid1001);
+        inOrder.verify(mInjector).kill(1000 /* pid */, OsConstants.SIGUSR1);
+        inOrder.verify(notificationForPid1000).wait(1000 /* timeoutMs */);
+        inOrder.verify(mInjector).kill(1001 /* pid */, OsConstants.SIGUSR1);
+        inOrder.verify(notificationForPid1001).wait(1000 /* timeoutMs */);
+
+        verify(mInjector, never()).kill(1002 /* pid */, OsConstants.SIGUSR1);
+        verify(notificationForPid1002, never()).wait(anyInt());
+    }
+
     private AndroidPackage createPackage(boolean multiSplit) {
         AndroidPackage pkg = mock(AndroidPackage.class);
 
@@ -1507,6 +1699,7 @@ public class ArtManagerLocalTest {
         lenient().when(pkgState.getPackageName()).thenReturn(packageName);
         lenient().when(pkgState.getPrimaryCpuAbi()).thenReturn("arm64-v8a");
         lenient().when(pkgState.getSecondaryCpuAbi()).thenReturn("armeabi-v7a");
+        lenient().when(pkgState.getAppId()).thenReturn(APP_ID);
 
         AndroidPackage pkg = createPackage(multiSplit);
         lenient().when(pkgState.getAndroidPackage()).thenReturn(pkg);
@@ -1574,4 +1767,12 @@ public class ArtManagerLocalTest {
                 .when(mStorageManager.getAllocatableBytes(any()))
                 .thenReturn(ArtManagerLocal.DOWNGRADE_THRESHOLD_ABOVE_LOW_BYTES);
     }
+
+    private RunningAppProcessInfo createProcessInfo(
+            int pid, String processName, int uid, String[] pkgList, int importance) {
+        var info = new RunningAppProcessInfo(processName, pid, pkgList);
+        info.uid = uid;
+        info.importance = importance;
+        return info;
+    }
 }
diff --git a/libartservice/service/javatests/com/android/server/art/ArtShellCommandTest.java b/libartservice/service/javatests/com/android/server/art/ArtShellCommandTest.java
new file mode 100644
index 0000000000..cb8d1d57cb
--- /dev/null
+++ b/libartservice/service/javatests/com/android/server/art/ArtShellCommandTest.java
@@ -0,0 +1,640 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.android.server.art;
+
+import static com.android.server.art.PreRebootDexoptJob.JOB_ID;
+import static com.android.server.art.prereboot.PreRebootDriver.PreRebootResult;
+
+import static com.google.common.truth.Truth.assertThat;
+import static com.google.common.truth.Truth.assertWithMessage;
+
+import static org.mockito.Mockito.any;
+import static org.mockito.Mockito.anyBoolean;
+import static org.mockito.Mockito.eq;
+import static org.mockito.Mockito.isNull;
+import static org.mockito.Mockito.lenient;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.never;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+
+import android.app.job.JobInfo;
+import android.app.job.JobParameters;
+import android.app.job.JobScheduler;
+import android.os.CancellationSignal;
+import android.os.Process;
+import android.os.SystemProperties;
+import android.os.UpdateEngine;
+import android.platform.test.annotations.DisableFlags;
+import android.platform.test.annotations.EnableFlags;
+import android.platform.test.flag.junit.SetFlagsRule;
+
+import androidx.test.filters.SmallTest;
+
+import com.android.server.art.prereboot.PreRebootDriver;
+import com.android.server.art.prereboot.PreRebootStatsReporter;
+import com.android.server.art.testing.CommandExecution;
+import com.android.server.art.testing.StaticMockitoRule;
+import com.android.server.pm.PackageManagerLocal;
+
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.mockito.Mock;
+import org.mockito.junit.MockitoJUnitRunner;
+
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.TimeUnit;
+import java.util.stream.Collectors;
+import java.util.stream.Stream;
+
+@SmallTest
+@RunWith(MockitoJUnitRunner.StrictStubs.class)
+public class ArtShellCommandTest {
+    private static final long TIMEOUT_SEC = 10;
+
+    @Rule
+    public StaticMockitoRule mockitoRule = new StaticMockitoRule(
+            SystemProperties.class, BackgroundDexoptJobService.class, ArtJni.class);
+    @Rule public final SetFlagsRule mSetFlagsRule = new SetFlagsRule();
+
+    @Mock private BackgroundDexoptJobService mJobService;
+    @Mock private PreRebootDriver mPreRebootDriver;
+    @Mock private PreRebootStatsReporter mPreRebootStatsReporter;
+    @Mock private JobScheduler mJobScheduler;
+    @Mock private UpdateEngine mUpdateEngine;
+    @Mock private PreRebootDexoptJob.Injector mPreRebootDexoptJobInjector;
+    @Mock private ArtManagerLocal.Injector mArtManagerLocalInjector;
+    @Mock private PackageManagerLocal mPackageManagerLocal;
+    @Mock private ArtShellCommand.Injector mInjector;
+
+    private PreRebootDexoptJob mPreRebootDexoptJob;
+    private ArtManagerLocal mArtManagerLocal;
+    private JobInfo mJobInfo;
+    private JobParameters mJobParameters;
+
+    @Before
+    public void setUp() throws Exception {
+        lenient()
+                .when(SystemProperties.getBoolean(eq("dalvik.vm.enable_pr_dexopt"), anyBoolean()))
+                .thenReturn(true);
+
+        lenient().when(mJobScheduler.schedule(any())).thenAnswer(invocation -> {
+            mJobInfo = invocation.<JobInfo>getArgument(0);
+            mJobParameters = mock(JobParameters.class);
+            assertThat(mJobInfo.getId()).isEqualTo(JOB_ID);
+            lenient().when(mJobParameters.getExtras()).thenReturn(mJobInfo.getExtras());
+            return JobScheduler.RESULT_SUCCESS;
+        });
+
+        lenient()
+                .doAnswer(invocation -> {
+                    mJobInfo = null;
+                    mJobParameters = null;
+                    return null;
+                })
+                .when(mJobScheduler)
+                .cancel(JOB_ID);
+
+        lenient().when(mJobScheduler.getPendingJob(JOB_ID)).thenAnswer(invocation -> {
+            return mJobInfo;
+        });
+
+        lenient()
+                .when(mPreRebootDexoptJobInjector.getPreRebootDriver())
+                .thenReturn(mPreRebootDriver);
+        lenient()
+                .when(mPreRebootDexoptJobInjector.getStatsReporter())
+                .thenReturn(mPreRebootStatsReporter);
+        lenient().when(mPreRebootDexoptJobInjector.getJobScheduler()).thenReturn(mJobScheduler);
+        lenient().when(mPreRebootDexoptJobInjector.getUpdateEngine()).thenReturn(mUpdateEngine);
+        mPreRebootDexoptJob = new PreRebootDexoptJob(mPreRebootDexoptJobInjector);
+
+        lenient().when(BackgroundDexoptJobService.getJob(JOB_ID)).thenReturn(mPreRebootDexoptJob);
+
+        lenient()
+                .when(mArtManagerLocalInjector.getPreRebootDexoptJob())
+                .thenReturn(mPreRebootDexoptJob);
+        mArtManagerLocal = new ArtManagerLocal(mArtManagerLocalInjector);
+
+        lenient().when(mInjector.getArtManagerLocal()).thenReturn(mArtManagerLocal);
+        lenient().when(mInjector.getPackageManagerLocal()).thenReturn(mPackageManagerLocal);
+    }
+
+    @Test
+    public void testOnOtaStagedPermission() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.SHELL_UID);
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "on-ota-staged", "--slot", "_b")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(-1);
+            assertThat(outputs).contains("Only root can call 'on-ota-staged'");
+        }
+    }
+
+    @Test
+    @DisableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testOnOtaStagedSync() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        when(mPreRebootDriver.run(eq("_b"), eq(false) /* mapSnapshotsForOta */, any()))
+                .thenReturn(new PreRebootResult(true /* success */));
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "on-ota-staged", "--slot", "_b")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Job finished. See logs for details");
+        }
+    }
+
+    @Test
+    @DisableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testOnOtaStagedSyncFatalError() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        when(mPreRebootDriver.run(eq("_b"), eq(false) /* mapSnapshotsForOta */, any()))
+                .thenThrow(RuntimeException.class);
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "on-ota-staged", "--slot", "_b")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Job encountered a fatal error");
+        }
+    }
+
+    @Test
+    @DisableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testOnOtaStagedSyncCancelledByCommand() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        when(mPreRebootDriver.run(eq("_b"), eq(false) /* mapSnapshotsForOta */, any()))
+                .thenAnswer(invocation -> {
+                    Semaphore dexoptCancelled = new Semaphore(0 /* permits */);
+                    var cancellationSignal = invocation.<CancellationSignal>getArgument(2);
+                    cancellationSignal.setOnCancelListener(() -> dexoptCancelled.release());
+                    assertThat(dexoptCancelled.tryAcquire(TIMEOUT_SEC, TimeUnit.SECONDS)).isTrue();
+                    return new PreRebootResult(true /* success */);
+                });
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "on-ota-staged", "--slot", "_b")) {
+            assertThat(execution.getStdout().readLine()).contains("Job running...");
+
+            try (var execution2 = new CommandExecution(
+                         createHandler(), "art", "pr-dexopt-job", "--cancel")) {
+                int exitCode2 = execution2.waitAndGetExitCode();
+                String outputs2 = getOutputs(execution2);
+                assertWithMessage(outputs2).that(exitCode2).isEqualTo(0);
+                assertThat(outputs2).contains("Pre-reboot Dexopt job cancelled");
+            }
+
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Job finished. See logs for details");
+        }
+    }
+
+    @Test
+    @DisableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testOnOtaStagedSyncCancelledByBrokenPipe() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        when(mPreRebootDriver.run(eq("_b"), eq(false) /* mapSnapshotsForOta */, any()))
+                .thenAnswer(invocation -> {
+                    Semaphore dexoptCancelled = new Semaphore(0 /* permits */);
+                    var cancellationSignal = invocation.<CancellationSignal>getArgument(2);
+                    cancellationSignal.setOnCancelListener(() -> dexoptCancelled.release());
+                    assertThat(dexoptCancelled.tryAcquire(TIMEOUT_SEC, TimeUnit.SECONDS)).isTrue();
+                    return new PreRebootResult(true /* success */);
+                });
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "on-ota-staged", "--slot", "_b")) {
+            assertThat(execution.getStdout().readLine()).contains("Job running...");
+
+            execution.closeStdin();
+
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Job finished. See logs for details");
+        }
+    }
+
+    @Test
+    @EnableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testOnOtaStagedAsync() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "on-ota-staged", "--slot", "_b")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Pre-reboot Dexopt job scheduled");
+        }
+
+        when(mPreRebootDriver.run(eq("_b"), eq(false) /* mapSnapshotsForOta */, any()))
+                .thenReturn(new PreRebootResult(true /* success */));
+
+        mPreRebootDexoptJob.onStartJobImpl(mJobService, mJobParameters);
+
+        try (var execution =
+                        new CommandExecution(createHandler(), "art", "on-ota-staged", "--start")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Job finished. See logs for details");
+        }
+    }
+
+    @Test
+    @EnableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testOnOtaStagedAsyncFatalError() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "on-ota-staged", "--slot", "_b")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Pre-reboot Dexopt job scheduled");
+        }
+
+        when(mPreRebootDriver.run(eq("_b"), eq(false) /* mapSnapshotsForOta */, any()))
+                .thenThrow(RuntimeException.class);
+
+        mPreRebootDexoptJob.onStartJobImpl(mJobService, mJobParameters);
+
+        try (var execution =
+                        new CommandExecution(createHandler(), "art", "on-ota-staged", "--start")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Job encountered a fatal error");
+        }
+    }
+
+    @Test
+    @EnableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testOnOtaStagedAsyncCancelledByCommand() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "on-ota-staged", "--slot", "_b")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Pre-reboot Dexopt job scheduled");
+        }
+
+        Semaphore dexoptStarted = new Semaphore(0);
+
+        when(mPreRebootDriver.run(eq("_b"), eq(false) /* mapSnapshotsForOta */, any()))
+                .thenAnswer(invocation -> {
+                    // Step 2.
+                    dexoptStarted.release();
+
+                    Semaphore dexoptCancelled = new Semaphore(0);
+                    var cancellationSignal = invocation.<CancellationSignal>getArgument(2);
+                    cancellationSignal.setOnCancelListener(() -> dexoptCancelled.release());
+                    assertThat(dexoptCancelled.tryAcquire(TIMEOUT_SEC, TimeUnit.SECONDS)).isTrue();
+
+                    // Step 4.
+                    return new PreRebootResult(true /* success */);
+                });
+
+        mPreRebootDexoptJob.onStartJobImpl(mJobService, mJobParameters);
+
+        // Step 1.
+        try (var execution =
+                        new CommandExecution(createHandler(), "art", "on-ota-staged", "--start")) {
+            assertThat(execution.getStdout().readLine()).contains("Job running...");
+
+            assertThat(dexoptStarted.tryAcquire(TIMEOUT_SEC, TimeUnit.SECONDS)).isTrue();
+
+            // Step 3.
+            try (var execution2 = new CommandExecution(
+                         createHandler(), "art", "pr-dexopt-job", "--cancel")) {
+                int exitCode2 = execution2.waitAndGetExitCode();
+                String outputs2 = getOutputs(execution2);
+                assertWithMessage(outputs2).that(exitCode2).isEqualTo(0);
+                assertThat(outputs2).contains("Pre-reboot Dexopt job cancelled");
+            }
+
+            int exitCode = execution.waitAndGetExitCode();
+
+            // Step 5.
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Job finished. See logs for details");
+        }
+    }
+
+    @Test
+    @EnableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testOnOtaStagedAsyncCancelledByBrokenPipe() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "on-ota-staged", "--slot", "_b")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Pre-reboot Dexopt job scheduled");
+        }
+
+        Semaphore dexoptStarted = new Semaphore(0);
+
+        when(mPreRebootDriver.run(eq("_b"), eq(false) /* mapSnapshotsForOta */, any()))
+                .thenAnswer(invocation -> {
+                    // Step 2.
+                    dexoptStarted.release();
+
+                    Semaphore dexoptCancelled = new Semaphore(0);
+                    var cancellationSignal = invocation.<CancellationSignal>getArgument(2);
+                    cancellationSignal.setOnCancelListener(() -> dexoptCancelled.release());
+                    assertThat(dexoptCancelled.tryAcquire(TIMEOUT_SEC, TimeUnit.SECONDS)).isTrue();
+
+                    // Step 4.
+                    return new PreRebootResult(true /* success */);
+                });
+
+        mPreRebootDexoptJob.onStartJobImpl(mJobService, mJobParameters);
+
+        // Step 1.
+        try (var execution =
+                        new CommandExecution(createHandler(), "art", "on-ota-staged", "--start")) {
+            assertThat(execution.getStdout().readLine()).contains("Job running...");
+
+            assertThat(dexoptStarted.tryAcquire(TIMEOUT_SEC, TimeUnit.SECONDS)).isTrue();
+
+            // Step 3.
+            execution.closeStdin();
+
+            int exitCode = execution.waitAndGetExitCode();
+
+            // Step 5.
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Job finished. See logs for details");
+        }
+    }
+
+    @Test
+    @EnableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testOnOtaStagedAsyncCancelledByJobScheduler() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "on-ota-staged", "--slot", "_b")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Pre-reboot Dexopt job scheduled");
+        }
+
+        mPreRebootDexoptJob.onStartJobImpl(mJobService, mJobParameters);
+        mPreRebootDexoptJob.onStopJobImpl(mJobParameters);
+
+        mPreRebootDexoptJob.waitForRunningJob();
+        verify(mUpdateEngine).triggerPostinstall("system");
+        verify(mPreRebootDriver, never()).run(any(), anyBoolean(), any());
+    }
+
+    @Test
+    @DisableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testOnOtaStagedAsyncLegacy() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        when(SystemProperties.getBoolean(eq("dalvik.vm.pr_dexopt_async_for_ota"), anyBoolean()))
+                .thenReturn(true);
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "on-ota-staged", "--slot", "_b")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Pre-reboot Dexopt job scheduled");
+        }
+
+        when(mPreRebootDriver.run(eq("_b"), eq(true) /* mapSnapshotsForOta */, any()))
+                .thenReturn(new PreRebootResult(true /* success */));
+
+        mPreRebootDexoptJob.onStartJobImpl(mJobService, mJobParameters);
+        mPreRebootDexoptJob.waitForRunningJob();
+    }
+
+    @Test
+    public void testOnOtaStagedStartJobNotFound() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        try (var execution =
+                        new CommandExecution(createHandler(), "art", "on-ota-staged", "--start")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(1);
+            assertThat(outputs).contains("No waiting job found");
+        }
+    }
+
+    @Test
+    public void testPrDexoptJobRunMainline() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.SHELL_UID);
+
+        when(mPreRebootDriver.run(
+                     isNull() /* otaSlot */, anyBoolean() /* mapSnapshotsForOta */, any()))
+                .thenReturn(new PreRebootResult(true /* success */));
+
+        try (var execution =
+                        new CommandExecution(createHandler(), "art", "pr-dexopt-job", "--run")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Job finished. See logs for details");
+        }
+    }
+
+    @Test
+    public void testPrDexoptJobRunOtaPermission() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.SHELL_UID);
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "pr-dexopt-job", "--run", "--slot", "_b")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(-1);
+            assertThat(outputs).contains("Only root can specify '--slot'");
+        }
+    }
+
+    @Test
+    @DisableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testPrDexoptJobRunOtaLegacy() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        when(mPreRebootDriver.run(eq("_b"), eq(true) /* mapSnapshotsForOta */, any()))
+                .thenReturn(new PreRebootResult(true /* success */));
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "pr-dexopt-job", "--run", "--slot", "_b")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Job finished. See logs for details");
+        }
+    }
+
+    @Test
+    @EnableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testPrDexoptJobRunOta() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        when(mPreRebootDriver.run(eq("_b"), eq(false) /* mapSnapshotsForOta */, any()))
+                .thenReturn(new PreRebootResult(true /* success */));
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "pr-dexopt-job", "--run", "--slot", "_b")) {
+            assertThat(execution.getStdout().readLine()).contains("Job running...");
+
+            try (var execution2 = new CommandExecution(
+                         createHandler(), "art", "on-ota-staged", "--start")) {
+                int exitCode2 = execution2.waitAndGetExitCode();
+                String outputs2 = getOutputs(execution2);
+                assertWithMessage(outputs2).that(exitCode2).isEqualTo(0);
+                assertThat(outputs2).contains("Job finished. See logs for details");
+            }
+
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Job finished. See logs for details");
+        }
+    }
+
+    @Test
+    public void testPrDexoptJobScheduleMainline() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.SHELL_UID);
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "pr-dexopt-job", "--schedule")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Pre-reboot Dexopt job scheduled");
+        }
+
+        when(mPreRebootDriver.run(
+                     isNull() /* otaSlot */, anyBoolean() /* mapSnapshotsForOta */, any()))
+                .thenReturn(new PreRebootResult(true /* success */));
+
+        mPreRebootDexoptJob.onStartJobImpl(mJobService, mJobParameters);
+        mPreRebootDexoptJob.waitForRunningJob();
+    }
+
+    @Test
+    public void testPrDexoptJobScheduleOtaPermission() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.SHELL_UID);
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "pr-dexopt-job", "--schedule", "--slot", "_b")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(-1);
+            assertThat(outputs).contains("Only root can specify '--slot'");
+        }
+    }
+
+    @Test
+    @DisableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testPrDexoptJobScheduleOtaLegacy() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "pr-dexopt-job", "--schedule", "--slot", "_b")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Pre-reboot Dexopt job scheduled");
+        }
+
+        when(mPreRebootDriver.run(eq("_b"), eq(true) /* mapSnapshotsForOta */, any()))
+                .thenReturn(new PreRebootResult(true /* success */));
+
+        mPreRebootDexoptJob.onStartJobImpl(mJobService, mJobParameters);
+        mPreRebootDexoptJob.waitForRunningJob();
+    }
+
+    @Test
+    @EnableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testPrDexoptJobScheduleOta() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        try (var execution = new CommandExecution(
+                     createHandler(), "art", "pr-dexopt-job", "--schedule", "--slot", "_b")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Pre-reboot Dexopt job scheduled");
+        }
+
+        when(mPreRebootDriver.run(eq("_b"), eq(false) /* mapSnapshotsForOta */, any()))
+                .thenReturn(new PreRebootResult(true /* success */));
+
+        mPreRebootDexoptJob.onStartJobImpl(mJobService, mJobParameters);
+
+        try (var execution =
+                        new CommandExecution(createHandler(), "art", "on-ota-staged", "--start")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Job finished. See logs for details");
+        }
+
+        mPreRebootDexoptJob.waitForRunningJob();
+    }
+
+    @Test
+    public void testPrDexoptJobCancelJobNotFound() throws Exception {
+        when(mInjector.getCallingUid()).thenReturn(Process.ROOT_UID);
+
+        try (var execution =
+                        new CommandExecution(createHandler(), "art", "pr-dexopt-job", "--cancel")) {
+            int exitCode = execution.waitAndGetExitCode();
+            String outputs = getOutputs(execution);
+            assertWithMessage(outputs).that(exitCode).isEqualTo(0);
+            assertThat(outputs).contains("Pre-reboot Dexopt job cancelled");
+        }
+    }
+
+    private ArtShellCommand createHandler() {
+        return new ArtShellCommand(mInjector);
+    }
+
+    private String getOutputs(CommandExecution execution) {
+        return Stream.concat(execution.getStdout().lines(), execution.getStderr().lines())
+                .collect(Collectors.joining("\n"));
+    }
+}
diff --git a/libartservice/service/javatests/com/android/server/art/DexUseManagerTest.java b/libartservice/service/javatests/com/android/server/art/DexUseManagerTest.java
index 3ea3509b82..1627c5eeef 100644
--- a/libartservice/service/javatests/com/android/server/art/DexUseManagerTest.java
+++ b/libartservice/service/javatests/com/android/server/art/DexUseManagerTest.java
@@ -28,6 +28,7 @@ import static org.mockito.Mockito.argThat;
 import static org.mockito.Mockito.eq;
 import static org.mockito.Mockito.lenient;
 import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.never;
 import static org.mockito.Mockito.verify;
 import static org.mockito.Mockito.when;
 
@@ -36,7 +37,6 @@ import android.content.Context;
 import android.content.Intent;
 import android.os.Binder;
 import android.os.Environment;
-import android.os.Process;
 import android.os.SystemProperties;
 import android.os.UserHandle;
 import android.os.storage.StorageManager;
@@ -86,9 +86,12 @@ public class DexUseManagerTest {
     private static final String INVISIBLE_BASE_APK =
             "/somewhere/app/" + INVISIBLE_PKG_NAME + "/base.apk";
 
+    // A reduced limit to make the test run faster.
+    private static final int MAX_SECONDARY_DEX_FILES_PER_OWNER_FOR_TESTING = 50;
+
     @Rule
     public StaticMockitoRule mockitoRule = new StaticMockitoRule(
-            SystemProperties.class, Constants.class, Process.class, ArtJni.class);
+            SystemProperties.class, Constants.class, ArtJni.class);
 
     private final UserHandle mUserHandle = UserHandle.of(1);
 
@@ -125,8 +128,6 @@ public class DexUseManagerTest {
         lenient().when(Constants.getNative64BitAbi()).thenReturn("arm64-v8a");
         lenient().when(Constants.getNative32BitAbi()).thenReturn("armeabi-v7a");
 
-        lenient().when(Process.isIsolatedUid(anyInt())).thenReturn(false);
-
         // Use a LinkedHashMap so that we can control the iteration order.
         mPackageStates = new LinkedHashMap<>();
 
@@ -187,6 +188,10 @@ public class DexUseManagerTest {
         lenient().when(mInjector.getPackageManagerLocal()).thenReturn(mPackageManagerLocal);
         lenient().when(mInjector.getCallingUserHandle()).thenReturn(mUserHandle);
         lenient().when(mInjector.getCallingUid()).thenReturn(110001);
+        lenient().when(mInjector.isIsolatedUid(anyInt())).thenReturn(false);
+        lenient()
+                .when(mInjector.getMaxSecondaryDexFilesPerOwner())
+                .thenReturn(MAX_SECONDARY_DEX_FILES_PER_OWNER_FOR_TESTING);
 
         mDexUseManager = new DexUseManagerLocal(mInjector);
         mDexUseManager.systemReady();
@@ -208,7 +213,7 @@ public class DexUseManagerTest {
 
     @Test
     public void testPrimaryDexOwnedIsolated() {
-        when(Process.isIsolatedUid(anyInt())).thenReturn(true);
+        when(mInjector.isIsolatedUid(anyInt())).thenReturn(true);
         mDexUseManager.notifyDexContainersLoaded(
                 mSnapshot, OWNING_PKG_NAME, Map.of(BASE_APK, "CLC"));
 
@@ -223,7 +228,7 @@ public class DexUseManagerTest {
 
     @Test
     public void testPrimaryDexOwnedSplitIsolated() {
-        when(Process.isIsolatedUid(anyInt())).thenReturn(true);
+        when(mInjector.isIsolatedUid(anyInt())).thenReturn(true);
         mDexUseManager.notifyDexContainersLoaded(
                 mSnapshot, OWNING_PKG_NAME, Map.of(SPLIT_APK, "CLC"));
 
@@ -317,7 +322,7 @@ public class DexUseManagerTest {
         mDexUseManager.notifyDexContainersLoaded(
                 mSnapshot, LOADING_PKG_NAME, Map.of(BASE_APK, "CLC"));
 
-        when(Process.isIsolatedUid(anyInt())).thenReturn(true);
+        when(mInjector.isIsolatedUid(anyInt())).thenReturn(true);
         mDexUseManager.notifyDexContainersLoaded(
                 mSnapshot, OWNING_PKG_NAME, Map.of(BASE_APK, "CLC"));
         when(mInjector.getCurrentTimeMillis()).thenReturn(2000l);
@@ -367,7 +372,7 @@ public class DexUseManagerTest {
 
     @Test
     public void testSecondaryDexOwnedIsolated() {
-        when(Process.isIsolatedUid(anyInt())).thenReturn(true);
+        when(mInjector.isIsolatedUid(anyInt())).thenReturn(true);
         mDexUseManager.notifyDexContainersLoaded(
                 mSnapshot, OWNING_PKG_NAME, Map.of(mDeDir + "/foo.apk", "CLC"));
 
@@ -529,7 +534,7 @@ public class DexUseManagerTest {
         mDexUseManager.notifyDexContainersLoaded(mSnapshot, OWNING_PKG_NAME,
                 Map.of(mCeDir + "/baz.apk", SecondaryDexInfo.UNSUPPORTED_CLASS_LOADER_CONTEXT));
 
-        when(Process.isIsolatedUid(anyInt())).thenReturn(true);
+        when(mInjector.isIsolatedUid(anyInt())).thenReturn(true);
         mDexUseManager.notifyDexContainersLoaded(
                 mSnapshot, OWNING_PKG_NAME, Map.of(mCeDir + "/foo.apk", "CLC"));
         when(mInjector.getCurrentTimeMillis()).thenReturn(2000l);
@@ -618,7 +623,7 @@ public class DexUseManagerTest {
         mDexUseManager.notifyDexContainersLoaded(
                 mSnapshot, LOADING_PKG_NAME, Map.of(mCeDir + "/foo.apk", "CLC"));
 
-        when(Process.isIsolatedUid(anyInt())).thenReturn(true);
+        when(mInjector.isIsolatedUid(anyInt())).thenReturn(true);
         mDexUseManager.notifyDexContainersLoaded(
                 mSnapshot, OWNING_PKG_NAME, Map.of(mCeDir + "/foo.apk", "CLC"));
 
@@ -641,11 +646,11 @@ public class DexUseManagerTest {
 
     @Test
     public void testCheckedSecondaryDexNotFound() throws Exception {
-        when(mArtd.getDexFileVisibility(mCeDir + "/foo.apk")).thenReturn(FileVisibility.NOT_FOUND);
-
         mDexUseManager.notifyDexContainersLoaded(
                 mSnapshot, OWNING_PKG_NAME, Map.of(mCeDir + "/foo.apk", "CLC"));
 
+        when(mArtd.getDexFileVisibility(mCeDir + "/foo.apk")).thenReturn(FileVisibility.NOT_FOUND);
+
         assertThat(mDexUseManager.getCheckedSecondaryDexInfo(
                            OWNING_PKG_NAME, true /* excludeObsoleteDexesAndLoaders */))
                 .isEmpty();
@@ -666,7 +671,7 @@ public class DexUseManagerTest {
         mDexUseManager.notifyDexContainersLoaded(
                 mSnapshot, LOADING_PKG_NAME, Map.of(mCeDir + "/foo.apk", "CLC"));
 
-        when(Process.isIsolatedUid(anyInt())).thenReturn(true);
+        when(mInjector.isIsolatedUid(anyInt())).thenReturn(true);
         mDexUseManager.notifyDexContainersLoaded(
                 mSnapshot, OWNING_PKG_NAME, Map.of(mCeDir + "/foo.apk", "CLC"));
 
@@ -826,6 +831,18 @@ public class DexUseManagerTest {
         mDexUseManager.notifyDexContainersLoaded(mSnapshot, OWNING_PKG_NAME, map);
     }
 
+    @Test(expected = IllegalArgumentException.class)
+    public void testTooLongDexPath() throws Exception {
+        mDexUseManager.notifyDexContainersLoaded(mSnapshot, OWNING_PKG_NAME,
+                Map.of("/" + "X".repeat(DexUseManagerLocal.MAX_PATH_LENGTH), "CLC"));
+    }
+
+    @Test
+    public void testMaxLengthDexPath() throws Exception {
+        mDexUseManager.notifyDexContainersLoaded(mSnapshot, OWNING_PKG_NAME,
+                Map.of("/" + "X".repeat(DexUseManagerLocal.MAX_PATH_LENGTH - 1), "CLC"));
+    }
+
     @Test(expected = IllegalArgumentException.class)
     public void testInvalidDexPath() throws Exception {
         lenient().when(ArtJni.validateDexPath(any())).thenReturn("invalid");
@@ -833,6 +850,20 @@ public class DexUseManagerTest {
                 mSnapshot, OWNING_PKG_NAME, Map.of("/a/b.jar", "PCL[]"));
     }
 
+    @Test(expected = IllegalArgumentException.class)
+    public void testTooLongClassLoaderContext() throws Exception {
+        mDexUseManager.notifyDexContainersLoaded(mSnapshot, OWNING_PKG_NAME,
+                Map.of(mCeDir + "/foo.apk",
+                        "X".repeat(DexUseManagerLocal.MAX_CLASS_LOADER_CONTEXT_LENGTH + 1)));
+    }
+
+    @Test
+    public void testMaxLengthClassLoaderContext() throws Exception {
+        mDexUseManager.notifyDexContainersLoaded(mSnapshot, OWNING_PKG_NAME,
+                Map.of(mCeDir + "/foo.apk",
+                        "X".repeat(DexUseManagerLocal.MAX_CLASS_LOADER_CONTEXT_LENGTH)));
+    }
+
     @Test(expected = IllegalArgumentException.class)
     public void testInvalidClassLoaderContext() throws Exception {
         lenient().when(ArtJni.validateClassLoaderContext(any(), any())).thenReturn("invalid");
@@ -876,6 +907,57 @@ public class DexUseManagerTest {
                 mSnapshot, OWNING_PKG_NAME, Map.of(BASE_APK, "CLC"));
     }
 
+    @Test
+    public void testSecondaryDexPath() throws Exception {
+        mMockClock.advanceTime(DexUseManagerLocal.INTERVAL_MS); // Save.
+        long oldFileSize = mTempFile.length();
+
+        String existingDexPath = mCeDir + "/foo.apk";
+        mDexUseManager.notifyDexContainersLoaded(
+                mSnapshot, LOADING_PKG_NAME, Map.of(existingDexPath, "PCL[]"));
+
+        mMockClock.advanceTime(DexUseManagerLocal.INTERVAL_MS); // Save.
+        assertThat(mTempFile.length()).isGreaterThan(oldFileSize);
+    }
+
+    @Test
+    public void testLimitSecondaryDexFiles() throws Exception {
+        for (int n = 0; n < MAX_SECONDARY_DEX_FILES_PER_OWNER_FOR_TESTING - 1; ++n) {
+            mDexUseManager.notifyDexContainersLoaded(mSnapshot, LOADING_PKG_NAME,
+                    Map.of(String.format("%s/%04d/foo.apk", mCeDir, n), "CLC"));
+        }
+        mMockClock.advanceTime(DexUseManagerLocal.INTERVAL_MS); // Save.
+        long oldFileSize = mTempFile.length();
+
+        mDexUseManager.notifyDexContainersLoaded(
+                mSnapshot, LOADING_PKG_NAME, Map.of(mCeDir + "/9998/foo.apk", "CLC"));
+        mMockClock.advanceTime(DexUseManagerLocal.INTERVAL_MS); // Save.
+        assertThat(mTempFile.length()).isGreaterThan(oldFileSize);
+
+        oldFileSize = mTempFile.length();
+        mDexUseManager.notifyDexContainersLoaded(
+                mSnapshot, LOADING_PKG_NAME, Map.of(mCeDir + "/9999/foo.apk", "CLC"));
+        mMockClock.advanceTime(DexUseManagerLocal.INTERVAL_MS); // Save.
+        assertThat(mTempFile.length()).isEqualTo(oldFileSize);
+
+        // Can still add loading packages to existing entries after the limit is reached.
+        mDexUseManager.notifyDexContainersLoaded(
+                mSnapshot, OWNING_PKG_NAME, Map.of(mCeDir + "/9998/foo.apk", "CLC"));
+        mMockClock.advanceTime(DexUseManagerLocal.INTERVAL_MS); // Save.
+        assertThat(mTempFile.length()).isGreaterThan(oldFileSize);
+    }
+
+    @Test
+    public void testLimitSecondaryDexFilesSingleCall() throws Exception {
+        Map<String, String> clcByDexFile = new HashMap<>();
+        for (int n = 0; n < MAX_SECONDARY_DEX_FILES_PER_OWNER_FOR_TESTING + 1; ++n) {
+            clcByDexFile.put(String.format("%s/%04d/foo.apk", mCeDir, n), "CLC");
+        }
+        mDexUseManager.notifyDexContainersLoaded(mSnapshot, LOADING_PKG_NAME, clcByDexFile);
+        assertThat(mDexUseManager.getSecondaryDexInfo(OWNING_PKG_NAME))
+                .hasSize(MAX_SECONDARY_DEX_FILES_PER_OWNER_FOR_TESTING);
+    }
+
     private AndroidPackage createPackage(String packageName) {
         AndroidPackage pkg = mock(AndroidPackage.class);
         lenient().when(pkg.getStorageUuid()).thenReturn(StorageManager.UUID_DEFAULT);
diff --git a/libartservice/service/javatests/com/android/server/art/DexoptHelperTest.java b/libartservice/service/javatests/com/android/server/art/DexoptHelperTest.java
index f6ed721157..b0c67258a6 100644
--- a/libartservice/service/javatests/com/android/server/art/DexoptHelperTest.java
+++ b/libartservice/service/javatests/com/android/server/art/DexoptHelperTest.java
@@ -73,7 +73,6 @@ import java.util.concurrent.Semaphore;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.function.Consumer;
-import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
 @SmallTest
@@ -669,7 +668,7 @@ public class DexoptHelperTest {
                            .getPackageDexoptResults()
                            .stream()
                            .map(PackageDexoptResult::getPackageName)
-                           .collect(Collectors.toList()))
+                           .toList())
                 .containsExactly(PKG_NAME_FOO);
     }
 
@@ -693,8 +692,7 @@ public class DexoptHelperTest {
         progressCallbackExecutor.runAll();
 
         List<DexContainerFileDexoptResult> fileResults =
-                Stream.concat(mPrimaryResults.stream(), mSecondaryResults.stream())
-                        .collect(Collectors.toList());
+                Stream.concat(mPrimaryResults.stream(), mSecondaryResults.stream()).toList();
 
         InOrder inOrder = inOrder(progressCallback);
         inOrder.verify(progressCallback)
@@ -843,9 +841,8 @@ public class DexoptHelperTest {
         assertThat(packageResult.getPackageName()).isEqualTo(packageName);
         assertThat(packageResult.getStatus()).isEqualTo(status);
         assertThat(packageResult.getDexContainerFileDexoptResults())
-                .containsExactlyElementsIn(dexContainerFileDexoptResults.stream()
-                                                   .flatMap(r -> r.stream())
-                                                   .collect(Collectors.toList()));
+                .containsExactlyElementsIn(
+                        dexContainerFileDexoptResults.stream().flatMap(r -> r.stream()).toList());
     }
 
     /** An executor that delays execution until `runAll` is called. */
diff --git a/libartservice/service/javatests/com/android/server/art/DumpHelperTest.java b/libartservice/service/javatests/com/android/server/art/DumpHelperTest.java
index c063260f86..55224b93f7 100644
--- a/libartservice/service/javatests/com/android/server/art/DumpHelperTest.java
+++ b/libartservice/service/javatests/com/android/server/art/DumpHelperTest.java
@@ -24,19 +24,12 @@ import static com.android.server.art.model.DexoptStatus.DexContainerFileDexoptSt
 import static com.google.common.truth.Truth.assertThat;
 
 import static org.mockito.Mockito.any;
-import static org.mockito.Mockito.anyInt;
 import static org.mockito.Mockito.argThat;
-import static org.mockito.Mockito.doReturn;
-import static org.mockito.Mockito.doThrow;
 import static org.mockito.Mockito.eq;
 import static org.mockito.Mockito.lenient;
 import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
 
 import android.annotation.NonNull;
-import android.annotation.SuppressLint;
-import android.content.pm.SigningInfo;
-import android.content.pm.SigningInfoException;
 import android.os.SystemProperties;
 
 import androidx.test.filters.SmallTest;
@@ -77,8 +70,6 @@ public class DumpHelperTest {
     @Mock private ArtManagerLocal mArtManagerLocal;
     @Mock private DexUseManagerLocal mDexUseManagerLocal;
     @Mock private PackageManagerLocal.FilteredSnapshot mSnapshot;
-    @Mock private SigningInfo mSigningInfoA;
-    @Mock private SigningInfo mSigningInfoB;
 
     private DumpHelper mDumpHelper;
 
@@ -108,11 +99,6 @@ public class DumpHelperTest {
         setUpForBar();
         setUpForSdk();
 
-        lenient().when(mSigningInfoA.signersMatchExactly(mSigningInfoA)).thenReturn(true);
-        lenient().when(mSigningInfoA.signersMatchExactly(mSigningInfoB)).thenReturn(false);
-        lenient().when(mSigningInfoB.signersMatchExactly(mSigningInfoB)).thenReturn(true);
-        lenient().when(mSigningInfoB.signersMatchExactly(mSigningInfoA)).thenReturn(false);
-
         mDumpHelper = new DumpHelper(mInjector);
     }
 
@@ -161,84 +147,10 @@ public class DumpHelperTest {
                 + "Current GC: CollectorTypeCMC\n";
 
         var stringWriter = new StringWriter();
-        mDumpHelper.dump(new PrintWriter(stringWriter), mSnapshot, false /* verifySdmSignatures */);
+        mDumpHelper.dump(new PrintWriter(stringWriter), mSnapshot);
         assertThat(stringWriter.toString()).isEqualTo(expected);
     }
 
-    @Test
-    public void testDumpSdmStatusNotFound() throws Exception {
-        when(mInjector.fileExists(any())).thenReturn(false);
-
-        var stringWriter = new StringWriter();
-        mDumpHelper.dumpPackage(new PrintWriter(stringWriter), mSnapshot,
-                getPackageState(PKG_NAME_BAR), true /* verifySdmSignatures */);
-        assertThat(stringWriter.toString()).doesNotContain("sdm:");
-    }
-
-    @Test
-    public void testDumpSdmStatusInvalidSdmSignature() throws Exception {
-        when(mInjector.fileExists("/somewhere/app/bar/base.sdm")).thenReturn(true);
-        when(mInjector.getVerifiedSigningInfo(eq("/somewhere/app/bar/base.sdm"), anyInt()))
-                .thenThrow(SigningInfoException.class);
-
-        var stringWriter = new StringWriter();
-        mDumpHelper.dumpPackage(new PrintWriter(stringWriter), mSnapshot,
-                getPackageState(PKG_NAME_BAR), true /* verifySdmSignatures */);
-        assertThat(stringWriter.toString())
-                .contains("sdm: [sdm-status=pending] [sdm-signature=invalid-sdm-signature]");
-    }
-
-    @Test
-    public void testDumpSdmStatusInvalidApkSignature() throws Exception {
-        when(mInjector.fileExists("/somewhere/app/bar/base.sdm")).thenReturn(true);
-        doReturn(mSigningInfoA)
-                .when(mInjector)
-                .getVerifiedSigningInfo(eq("/somewhere/app/bar/base.sdm"), anyInt());
-        doThrow(SigningInfoException.class)
-                .when(mInjector)
-                .getVerifiedSigningInfo(eq("/somewhere/app/bar/base.apk"), anyInt());
-
-        var stringWriter = new StringWriter();
-        mDumpHelper.dumpPackage(new PrintWriter(stringWriter), mSnapshot,
-                getPackageState(PKG_NAME_BAR), true /* verifySdmSignatures */);
-        assertThat(stringWriter.toString())
-                .contains("sdm: [sdm-status=pending] [sdm-signature=invalid-apk-signature]");
-    }
-
-    @Test
-    public void testDumpSdmStatusSignersNotMatch() throws Exception {
-        when(mInjector.fileExists("/somewhere/app/bar/base.sdm")).thenReturn(true);
-        doReturn(mSigningInfoA)
-                .when(mInjector)
-                .getVerifiedSigningInfo(eq("/somewhere/app/bar/base.sdm"), anyInt());
-        doReturn(mSigningInfoB)
-                .when(mInjector)
-                .getVerifiedSigningInfo(eq("/somewhere/app/bar/base.apk"), anyInt());
-
-        var stringWriter = new StringWriter();
-        mDumpHelper.dumpPackage(new PrintWriter(stringWriter), mSnapshot,
-                getPackageState(PKG_NAME_BAR), true /* verifySdmSignatures */);
-        assertThat(stringWriter.toString())
-                .contains("sdm: [sdm-status=pending] [sdm-signature=mismatched-signers]");
-    }
-
-    @Test
-    public void testDumpSdmStatusVerified() throws Exception {
-        when(mInjector.fileExists("/somewhere/app/bar/base.sdm")).thenReturn(true);
-        doReturn(mSigningInfoA)
-                .when(mInjector)
-                .getVerifiedSigningInfo(eq("/somewhere/app/bar/base.sdm"), anyInt());
-        doReturn(mSigningInfoA)
-                .when(mInjector)
-                .getVerifiedSigningInfo(eq("/somewhere/app/bar/base.apk"), anyInt());
-
-        var stringWriter = new StringWriter();
-        mDumpHelper.dumpPackage(new PrintWriter(stringWriter), mSnapshot,
-                getPackageState(PKG_NAME_BAR), true /* verifySdmSignatures */);
-        assertThat(stringWriter.toString())
-                .contains("sdm: [sdm-status=pending] [sdm-signature=verified]");
-    }
-
     private PackageState createPackageState(@NonNull String packageName, int appId, boolean isApex,
             boolean hasPackage, @NonNull String primaryAbi, @NonNull String secondaryAbi) {
         var pkgState = mock(PackageState.class);
@@ -406,9 +318,4 @@ public class DumpHelperTest {
                         PKG_NAME_SDK, "/somewhere/app/sdk/base.apk"))
                 .thenReturn(Set.of());
     }
-
-    @SuppressLint("DirectInvocationOnMock")
-    private PackageState getPackageState(String packageName) {
-        return mSnapshot.getPackageState(packageName);
-    }
 }
diff --git a/libartservice/service/javatests/com/android/server/art/PreRebootDexoptJobTest.java b/libartservice/service/javatests/com/android/server/art/PreRebootDexoptJobTest.java
index 4aaf1d5be9..9850145702 100644
--- a/libartservice/service/javatests/com/android/server/art/PreRebootDexoptJobTest.java
+++ b/libartservice/service/javatests/com/android/server/art/PreRebootDexoptJobTest.java
@@ -17,6 +17,7 @@
 package com.android.server.art;
 
 import static com.android.server.art.PreRebootDexoptJob.JOB_ID;
+import static com.android.server.art.prereboot.PreRebootDriver.PreRebootResult;
 
 import static com.google.common.truth.Truth.assertThat;
 
@@ -36,6 +37,10 @@ import android.app.job.JobParameters;
 import android.app.job.JobScheduler;
 import android.os.CancellationSignal;
 import android.os.SystemProperties;
+import android.os.UpdateEngine;
+import android.platform.test.annotations.DisableFlags;
+import android.platform.test.annotations.EnableFlags;
+import android.platform.test.flag.junit.SetFlagsRule;
 import android.provider.DeviceConfig;
 
 import androidx.test.filters.SmallTest;
@@ -56,6 +61,7 @@ import java.io.File;
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.Semaphore;
 import java.util.concurrent.TimeUnit;
+import java.util.function.Supplier;
 
 @SmallTest
 @RunWith(MockitoJUnitRunner.StrictStubs.class)
@@ -65,11 +71,13 @@ public class PreRebootDexoptJobTest {
     @Rule
     public StaticMockitoRule mockitoRule = new StaticMockitoRule(
             SystemProperties.class, BackgroundDexoptJobService.class, ArtJni.class);
+    @Rule public final SetFlagsRule mSetFlagsRule = new SetFlagsRule();
 
     @Mock private PreRebootDexoptJob.Injector mInjector;
     @Mock private JobScheduler mJobScheduler;
     @Mock private PreRebootDriver mPreRebootDriver;
     @Mock private BackgroundDexoptJobService mJobService;
+    @Mock private UpdateEngine mUpdateEngine;
     @Mock private PreRebootStatsReporter.Injector mPreRebootStatsReporterInjector;
     private PreRebootDexoptJob mPreRebootDexoptJob;
     private JobInfo mJobInfo;
@@ -100,6 +108,7 @@ public class PreRebootDexoptJobTest {
                 .when(mInjector.getStatsReporter())
                 .thenAnswer(
                         invocation -> new PreRebootStatsReporter(mPreRebootStatsReporterInjector));
+        lenient().when(mInjector.getUpdateEngine()).thenReturn(mUpdateEngine);
 
         File tempFile = File.createTempFile("pre-reboot-stats", ".pb");
         tempFile.deleteOnExit();
@@ -130,6 +139,14 @@ public class PreRebootDexoptJobTest {
 
         mPreRebootDexoptJob = new PreRebootDexoptJob(mInjector);
         lenient().when(BackgroundDexoptJobService.getJob(JOB_ID)).thenReturn(mPreRebootDexoptJob);
+
+        lenient()
+                .doAnswer(invocation -> {
+                    CompletableFuture<?> unused = mPreRebootDexoptJob.notifyUpdateEngineReady();
+                    return null;
+                })
+                .when(mUpdateEngine)
+                .triggerPostinstall("system");
     }
 
     @Test
@@ -160,7 +177,7 @@ public class PreRebootDexoptJobTest {
                 .thenReturn(true);
 
         CompletableFuture<Void> future = mPreRebootDexoptJob.onUpdateReadyStartNow(
-                null /* otaSlot */, false /* mapSnapshotsForOta */);
+                null /* otaSlot */, true /* isUpdataEngineReady */);
 
         assertThat(future).isNull();
         verify(mPreRebootDriver, never()).run(any(), anyBoolean(), any());
@@ -183,7 +200,7 @@ public class PreRebootDexoptJobTest {
                 .thenReturn(false);
 
         CompletableFuture<Void> future = mPreRebootDexoptJob.onUpdateReadyStartNow(
-                null /* otaSlot */, false /* mapSnapshotsForOta */);
+                null /* otaSlot */, true /* isUpdataEngineReady */);
 
         assertThat(future).isNull();
         verify(mPreRebootDriver, never()).run(any(), anyBoolean(), any());
@@ -230,13 +247,13 @@ public class PreRebootDexoptJobTest {
         verify(mJobScheduler).cancel(JOB_ID);
     }
 
-    @Test
-    public void testStart() throws Exception {
+    private void checkStart(String otaSlot, Supplier<Boolean> mapSnapshotsForOtaMatcher)
+            throws Exception {
         var jobStarted = new Semaphore(0);
-        when(mPreRebootDriver.run(any(), eq(true) /* mapSnapshotsForOta */, any()))
+        when(mPreRebootDriver.run(eq(otaSlot), mapSnapshotsForOtaMatcher.get(), any()))
                 .thenAnswer(invocation -> {
                     jobStarted.release();
-                    return true;
+                    return new PreRebootResult(true /* success */);
                 });
 
         when(ArtJni.setProperty("dalvik.vm.pre-reboot.has-started", "true"))
@@ -248,7 +265,7 @@ public class PreRebootDexoptJobTest {
                 });
 
         assertThat(mPreRebootDexoptJob.hasStarted()).isFalse();
-        mPreRebootDexoptJob.onUpdateReadyImpl(null /* otaSlot */);
+        mPreRebootDexoptJob.onUpdateReadyImpl(otaSlot);
         mPreRebootDexoptJob.onStartJobImpl(mJobService, mJobParameters);
         assertThat(jobStarted.tryAcquire(TIMEOUT_SEC, TimeUnit.SECONDS)).isTrue();
         assertThat(mPreRebootDexoptJob.hasStarted()).isTrue();
@@ -257,16 +274,56 @@ public class PreRebootDexoptJobTest {
     }
 
     @Test
-    public void testSyncStart() throws Exception {
-        when(mPreRebootDriver.run(any(), eq(false) /* mapSnapshotsForOta */, any()))
-                .thenReturn(true);
+    @EnableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testStartWithUpdateEngineApi() throws Exception {
+        checkStart("_b" /* otaSlot */, () -> eq(false) /* mapSnapshotsForOtaMatcher */);
+        verify(mUpdateEngine).triggerPostinstall("system");
+    }
 
-        CompletableFuture<Void> future = mPreRebootDexoptJob.onUpdateReadyStartNow(
-                null /* otaSlot */, false /* mapSnapshotsForOta */);
+    @Test
+    @DisableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testStartWithoutUpdateEngineApi() throws Exception {
+        checkStart("_b" /* otaSlot */, () -> eq(true) /* mapSnapshotsForOtaMatcher */);
+        verify(mUpdateEngine, never()).triggerPostinstall(any());
+    }
+
+    @Test
+    public void testStartMainline() throws Exception {
+        checkStart(null /* otaSlot */, () -> anyBoolean() /* mapSnapshotsForOtaMatcher */);
+        verify(mUpdateEngine, never()).triggerPostinstall(any());
+    }
+
+    private void checkSyncStart(boolean isUpdateEngineReady, boolean expectedMapSnapshotsForOta)
+            throws Exception {
+        when(mPreRebootDriver.run(eq("_b"), eq(expectedMapSnapshotsForOta), any()))
+                .thenReturn(new PreRebootResult(true /* success */));
+
+        CompletableFuture<Void> future =
+                mPreRebootDexoptJob.onUpdateReadyStartNow("_b" /* otaSlot */, isUpdateEngineReady);
 
         Utils.getFuture(future);
     }
 
+    @Test
+    @EnableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testSyncStartWithUpdateEngineApi() throws Exception {
+        checkSyncStart(false /* isUpdataEngineReady */, false /* expectedMapSnapshotsForOta */);
+        verify(mUpdateEngine).triggerPostinstall("system");
+    }
+
+    @Test
+    @DisableFlags({android.os.Flags.FLAG_UPDATE_ENGINE_API})
+    public void testSyncStartWithoutUpdateEngineApi() throws Exception {
+        checkSyncStart(false /* isUpdataEngineReady */, true /* expectedMapSnapshotsForOta */);
+        verify(mUpdateEngine, never()).triggerPostinstall(any());
+    }
+
+    @Test
+    public void testSyncStartWithIsUpdateEngineReady() throws Exception {
+        checkSyncStart(true /* isUpdataEngineReady */, false /* expectedMapSnapshotsForOta */);
+        verify(mUpdateEngine, never()).triggerPostinstall(any());
+    }
+
     @Test
     public void testCancel() {
         Semaphore dexoptCancelled = new Semaphore(0);
@@ -276,7 +333,7 @@ public class PreRebootDexoptJobTest {
             cancellationSignal.setOnCancelListener(() -> dexoptCancelled.release());
             assertThat(dexoptCancelled.tryAcquire(TIMEOUT_SEC, TimeUnit.SECONDS)).isTrue();
             jobExited.release();
-            return true;
+            return new PreRebootResult(true /* success */);
         });
 
         mPreRebootDexoptJob.onUpdateReadyImpl(null /* otaSlot */);
@@ -297,11 +354,11 @@ public class PreRebootDexoptJobTest {
             cancellationSignal.setOnCancelListener(() -> dexoptCancelled.release());
             assertThat(dexoptCancelled.tryAcquire(TIMEOUT_SEC, TimeUnit.SECONDS)).isTrue();
             jobExited.release();
-            return true;
+            return new PreRebootResult(true /* success */);
         });
 
         CompletableFuture<Void> future = mPreRebootDexoptJob.onUpdateReadyStartNow(
-                null /* otaSlot */, false /* mapSnapshotsForOta */);
+                null /* otaSlot */, true /* isUpdataEngineReady */);
         mPreRebootDexoptJob.cancelGiven(future, false /* expectInterrupt */);
 
         // Check that `cancelGiven` is really blocking. If it wasn't, the check below might still
@@ -314,7 +371,8 @@ public class PreRebootDexoptJobTest {
         mPreRebootDexoptJob.onUpdateReadyImpl("_b" /* otaSlot */);
         mPreRebootDexoptJob.onUpdateReadyImpl(null /* otaSlot */);
 
-        when(mPreRebootDriver.run(eq("_b"), anyBoolean(), any())).thenReturn(true);
+        when(mPreRebootDriver.run(eq("_b"), anyBoolean(), any()))
+                .thenReturn(new PreRebootResult(true /* success */));
 
         mPreRebootDexoptJob.onStartJobImpl(mJobService, mJobParameters);
         mPreRebootDexoptJob.waitForRunningJob();
@@ -325,7 +383,8 @@ public class PreRebootDexoptJobTest {
         mPreRebootDexoptJob.onUpdateReadyImpl(null /* otaSlot */);
         mPreRebootDexoptJob.onUpdateReadyImpl("_a" /* otaSlot */);
 
-        when(mPreRebootDriver.run(eq("_a"), anyBoolean(), any())).thenReturn(true);
+        when(mPreRebootDriver.run(eq("_a"), anyBoolean(), any()))
+                .thenReturn(new PreRebootResult(true /* success */));
 
         mPreRebootDexoptJob.onStartJobImpl(mJobService, mJobParameters);
         mPreRebootDexoptJob.waitForRunningJob();
@@ -336,7 +395,8 @@ public class PreRebootDexoptJobTest {
         mPreRebootDexoptJob.onUpdateReadyImpl(null /* otaSlot */);
         mPreRebootDexoptJob.onUpdateReadyImpl(null /* otaSlot */);
 
-        when(mPreRebootDriver.run(isNull(), anyBoolean(), any())).thenReturn(true);
+        when(mPreRebootDriver.run(isNull(), anyBoolean(), any()))
+                .thenReturn(new PreRebootResult(true /* success */));
 
         mPreRebootDexoptJob.onStartJobImpl(mJobService, mJobParameters);
         mPreRebootDexoptJob.waitForRunningJob();
@@ -347,7 +407,8 @@ public class PreRebootDexoptJobTest {
         mPreRebootDexoptJob.onUpdateReadyImpl("_b" /* otaSlot */);
         mPreRebootDexoptJob.onUpdateReadyImpl("_b" /* otaSlot */);
 
-        when(mPreRebootDriver.run(eq("_b"), anyBoolean(), any())).thenReturn(true);
+        when(mPreRebootDriver.run(eq("_b"), anyBoolean(), any()))
+                .thenReturn(new PreRebootResult(true /* success */));
 
         mPreRebootDexoptJob.onStartJobImpl(mJobService, mJobParameters);
         mPreRebootDexoptJob.waitForRunningJob();
@@ -375,7 +436,7 @@ public class PreRebootDexoptJobTest {
         when(mPreRebootDriver.run(any(), anyBoolean(), any())).thenAnswer(invocation -> {
             // Simulate that the job takes a while to exit, no matter it's cancelled or not.
             assertThat(jobBlocker.tryAcquire(TIMEOUT_SEC, TimeUnit.SECONDS)).isTrue();
-            return true;
+            return new PreRebootResult(true /* success */);
         });
 
         // An update arrives. A job is scheduled.
@@ -461,7 +522,7 @@ public class PreRebootDexoptJobTest {
             cancellationSignal.setOnCancelListener(() -> dexoptCancelled.release());
             assertThat(dexoptCancelled.tryAcquire(TIMEOUT_SEC, TimeUnit.SECONDS)).isTrue();
             jobExited.release();
-            return true;
+            return new PreRebootResult(true /* success */);
         });
 
         // An update arrives. A job is scheduled.
@@ -475,7 +536,7 @@ public class PreRebootDexoptJobTest {
         // `onUpdateReadyStartNow`, before the job scheduler calls `onStartJob`.
         JobParameters oldParameters = mJobParameters;
         CompletableFuture<Void> future = mPreRebootDexoptJob.onUpdateReadyStartNow(
-                null /* otaSlot */, false /* mapSnapshotsForOta */);
+                null /* otaSlot */, true /* isUpdataEngineReady */);
 
         // The old job should be cancelled at this point.
         // This cannot be the new job having exited because jobs are serialized.
diff --git a/libartservice/service/javatests/com/android/server/art/PrimaryDexopterParameterizedTest.java b/libartservice/service/javatests/com/android/server/art/PrimaryDexopterParameterizedTest.java
index 10f9f59e91..9c3f87e92e 100644
--- a/libartservice/service/javatests/com/android/server/art/PrimaryDexopterParameterizedTest.java
+++ b/libartservice/service/javatests/com/android/server/art/PrimaryDexopterParameterizedTest.java
@@ -202,6 +202,7 @@ public class PrimaryDexopterParameterizedTest extends PrimaryDexopterTestBase {
         params.mIsPreReboot = true;
         params.mExpectedOutputIsPreReboot = true;
         params.mExpectedDeletesRuntimeArtifacts = false;
+        params.mExpectedDeletesSdmSdcFiles = false;
         list.add(params);
 
         params = new Params();
@@ -385,6 +386,17 @@ public class PrimaryDexopterParameterizedTest extends PrimaryDexopterTestBase {
                             PKG_NAME, "/somewhere/app/foo/split_0.apk", "arm")));
         }
 
+        if (mParams.mExpectedDeletesSdmSdcFiles) {
+            // Only delete SDM and SDC files for successful dexopt operations, namely the first one
+            // and the fourth one.
+            doReturn(1l).when(mArtd).deleteSdmSdcFiles(
+                    deepEq(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                            "/somewhere/app/foo/base.apk", "arm64", mParams.mIsInDalvikCache)));
+            doReturn(1l).when(mArtd).deleteSdmSdcFiles(
+                    deepEq(AidlUtils.buildSecureDexMetadataWithCompanionPaths(
+                            "/somewhere/app/foo/split_0.apk", "arm", mParams.mIsInDalvikCache)));
+        }
+
         assertThat(mPrimaryDexopter.dexopt())
                 .comparingElementsUsing(TestingUtils.<DexContainerFileDexoptResult>deepEquality())
                 .containsExactly(
@@ -416,6 +428,10 @@ public class PrimaryDexopterParameterizedTest extends PrimaryDexopterTestBase {
         if (!mParams.mExpectedDeletesRuntimeArtifacts) {
             verify(mArtd, times(0)).deleteRuntimeArtifacts(any());
         }
+
+        if (!mParams.mExpectedDeletesSdmSdcFiles) {
+            verify(mArtd, times(0)).deleteSdmSdcFiles(any());
+        }
     }
 
     private static class Params {
@@ -451,6 +467,7 @@ public class PrimaryDexopterParameterizedTest extends PrimaryDexopterTestBase {
         public boolean mExpectedIsHiddenApiPolicyEnabled = true;
         public boolean mExpectedOutputIsPreReboot = false;
         public boolean mExpectedDeletesRuntimeArtifacts = true;
+        public boolean mExpectedDeletesSdmSdcFiles = true;
 
         public String toString() {
             return String.format("isInDalvikCache=%b,"
@@ -477,7 +494,8 @@ public class PrimaryDexopterParameterizedTest extends PrimaryDexopterTestBase {
                             + "expectedIsDebuggable=%b,"
                             + "expectedIsHiddenApiPolicyEnabled=%b,"
                             + "expectedOutputIsPreReboot=%b,"
-                            + "expectedDeleteRuntimeArtifacts=%b",
+                            + "expectedDeletesRuntimeArtifacts=%b,"
+                            + "expectedDeletesSdmSdcFiles=%b",
                     mIsInDalvikCache, mHiddenApiEnforcementPolicy, mIsVmSafeMode, mIsDebuggable,
                     mIsSystemUi, mIsLauncher, mIsUseEmbeddedDex, mIsSanboxSdkLib,
                     mRequestedCompilerFilter, mCallbackReturnedCompilerFilter, mForce,
@@ -485,7 +503,7 @@ public class PrimaryDexopterParameterizedTest extends PrimaryDexopterTestBase {
                     mForceCompilerFilter, mAlwaysDebuggable, mExpectedCallbackInputCompilerFilter,
                     mExpectedCompilerFilter, mExpectedDexoptTrigger, mExpectedIsDebuggable,
                     mExpectedIsHiddenApiPolicyEnabled, mExpectedOutputIsPreReboot,
-                    mExpectedDeletesRuntimeArtifacts);
+                    mExpectedDeletesRuntimeArtifacts, mExpectedDeletesSdmSdcFiles);
         }
     }
 }
diff --git a/libartservice/service/javatests/com/android/server/art/PrimaryDexopterTest.java b/libartservice/service/javatests/com/android/server/art/PrimaryDexopterTest.java
index 5cc201cd37..b032538bbb 100644
--- a/libartservice/service/javatests/com/android/server/art/PrimaryDexopterTest.java
+++ b/libartservice/service/javatests/com/android/server/art/PrimaryDexopterTest.java
@@ -16,6 +16,7 @@
 
 package com.android.server.art;
 
+import static com.android.server.art.OutputArtifacts.PermissionSettings;
 import static com.android.server.art.model.DexoptResult.DexContainerFileDexoptResult;
 import static com.android.server.art.testing.TestingUtils.deepEq;
 
@@ -62,7 +63,7 @@ import java.util.concurrent.ForkJoinPool;
 import java.util.concurrent.Future;
 import java.util.concurrent.Semaphore;
 import java.util.concurrent.TimeUnit;
-import java.util.stream.Collectors;
+import java.util.function.Supplier;
 import java.util.zip.ZipFile;
 
 @SmallTest
@@ -157,50 +158,52 @@ public class PrimaryDexopterTest extends PrimaryDexopterTestBase {
         mUsedEmbeddedProfiles = new ArrayList<>();
     }
 
-    @Test
-    public void testDexoptInputVdex() throws Exception {
-        // null.
-        doReturn(dexoptIsNeeded(ArtifactsLocation.NONE_OR_ERROR))
+    private void checkDexoptInputVdex(
+            @ArtifactsLocation int location, Supplier<VdexPath> inputVdexMatcher) throws Exception {
+        doReturn(dexoptIsNeeded(location))
                 .when(mArtd)
                 .getDexoptNeeded(eq(mDexPath), eq("arm64"), any(), any(), anyInt());
-        doReturn(mArtdDexoptResult)
-                .when(mArtd)
-                .dexopt(any(), eq(mDexPath), eq("arm64"), any(), any(), any(), isNull(), any(),
-                        anyInt(), any(), any());
 
-        // ArtifactsPath, isInDalvikCache=true.
-        doReturn(dexoptIsNeeded(ArtifactsLocation.DALVIK_CACHE))
-                .when(mArtd)
-                .getDexoptNeeded(eq(mDexPath), eq("arm"), any(), any(), anyInt());
-        doReturn(mArtdDexoptResult)
-                .when(mArtd)
-                .dexopt(any(), eq(mDexPath), eq("arm"), any(), any(), any(),
-                        deepEq(VdexPath.artifactsPath(AidlUtils.buildArtifactsPathAsInput(
-                                mDexPath, "arm", true /* isInDalvikCache */))),
-                        any(), anyInt(), any(), any());
+        List<DexContainerFileDexoptResult> results = mPrimaryDexopter.dexopt();
+        verifyStatusAllOk(results);
+        verify(mArtd).dexopt(any(), eq(mDexPath), eq("arm64"), any(), any(), any(),
+                inputVdexMatcher.get(), any(), anyInt(), any(), any());
+    }
 
-        // ArtifactsPath, isInDalvikCache=false.
-        doReturn(dexoptIsNeeded(ArtifactsLocation.NEXT_TO_DEX))
-                .when(mArtd)
-                .getDexoptNeeded(eq(mSplit0DexPath), eq("arm64"), any(), any(), anyInt());
-        doReturn(mArtdDexoptResult)
-                .when(mArtd)
-                .dexopt(any(), eq(mSplit0DexPath), eq("arm64"), any(), any(), any(),
-                        deepEq(VdexPath.artifactsPath(AidlUtils.buildArtifactsPathAsInput(
-                                mSplit0DexPath, "arm64", false /* isInDalvikCache */))),
-                        any(), anyInt(), any(), any());
+    @Test
+    public void testDexoptInputVdexNoneOrError() throws Exception {
+        checkDexoptInputVdex(ArtifactsLocation.NONE_OR_ERROR, () -> isNull());
+    }
 
-        // DexMetadataPath.
-        doReturn(dexoptIsNeeded(ArtifactsLocation.DM))
-                .when(mArtd)
-                .getDexoptNeeded(eq(mSplit0DexPath), eq("arm"), any(), any(), anyInt());
-        doReturn(mArtdDexoptResult)
-                .when(mArtd)
-                .dexopt(any(), eq(mSplit0DexPath), eq("arm"), any(), any(), any(), isNull(), any(),
-                        anyInt(), any(), any());
+    @Test
+    public void testDexoptInputVdexDalvikCache() throws Exception {
+        checkDexoptInputVdex(ArtifactsLocation.DALVIK_CACHE, () -> {
+            return deepEq(VdexPath.artifactsPath(AidlUtils.buildArtifactsPathAsInput(
+                    mDexPath, "arm64", true /* isInDalvikCache */)));
+        });
+    }
 
-        List<DexContainerFileDexoptResult> results = mPrimaryDexopter.dexopt();
-        verifyStatusAllOk(results);
+    @Test
+    public void testDexoptInputVdexNextToDex() throws Exception {
+        checkDexoptInputVdex(ArtifactsLocation.NEXT_TO_DEX, () -> {
+            return deepEq(VdexPath.artifactsPath(AidlUtils.buildArtifactsPathAsInput(
+                    mDexPath, "arm64", false /* isInDalvikCache */)));
+        });
+    }
+
+    @Test
+    public void testDexoptInputVdexDm() throws Exception {
+        checkDexoptInputVdex(ArtifactsLocation.DM, () -> isNull());
+    }
+
+    @Test
+    public void testDexoptInputVdexSdmDalvikCache() throws Exception {
+        checkDexoptInputVdex(ArtifactsLocation.SDM_DALVIK_CACHE, () -> isNull());
+    }
+
+    @Test
+    public void testDexoptInputVdexSdmNextToDex() throws Exception {
+        checkDexoptInputVdex(ArtifactsLocation.SDM_NEXT_TO_DEX, () -> isNull());
     }
 
     @Test
@@ -693,7 +696,7 @@ public class PrimaryDexopterTest extends PrimaryDexopterTestBase {
         assertThat(mPrimaryDexopter.dexopt()
                            .stream()
                            .map(DexContainerFileDexoptResult::getStatus)
-                           .collect(Collectors.toList()))
+                           .toList())
                 .containsExactly(DexoptResult.DEXOPT_CANCELLED);
 
         // It shouldn't continue after being cancelled on the first file.
@@ -734,10 +737,7 @@ public class PrimaryDexopterTest extends PrimaryDexopterTestBase {
 
         mCancellationSignal.cancel();
 
-        assertThat(results.get()
-                           .stream()
-                           .map(DexContainerFileDexoptResult::getStatus)
-                           .collect(Collectors.toList()))
+        assertThat(results.get().stream().map(DexContainerFileDexoptResult::getStatus).toList())
                 .containsExactly(DexoptResult.DEXOPT_CANCELLED);
 
         // It shouldn't continue after being cancelled on the first file.
@@ -949,6 +949,52 @@ public class PrimaryDexopterTest extends PrimaryDexopterTestBase {
         }
     }
 
+    @Test
+    public void testMaybeCreateSdc() throws Exception {
+        mDexoptParams = new DexoptParams.Builder("install")
+                                .setCompilerFilter("speed-profile")
+                                .setFlags(ArtFlags.FLAG_FOR_PRIMARY_DEX)
+                                .build();
+        mPrimaryDexopter =
+                new PrimaryDexopter(mInjector, mPkgState, mPkg, mDexoptParams, mCancellationSignal);
+
+        mPrimaryDexopter.dexopt();
+
+        FsPermission dirFsPermission = AidlUtils.buildFsPermission(Process.SYSTEM_UID /* uid */,
+                Process.SYSTEM_UID /* gid */, false /* isOtherReadable */,
+                true /* isOtherExecutable */);
+        FsPermission fileFsPermission = AidlUtils.buildFsPermission(
+                Process.SYSTEM_UID /* uid */, SHARED_GID /* gid */, true /* isOtherReadable */);
+        PermissionSettings permissionSettings = AidlUtils.buildPermissionSettings(
+                dirFsPermission, fileFsPermission, null /* seContext */);
+
+        verify(mArtd).maybeCreateSdc(deepEq(AidlUtils.buildOutputSecureDexMetadataCompanion(
+                mDexPath, "arm64", false /* isInDalvikCache */, permissionSettings)));
+        verify(mArtd).maybeCreateSdc(deepEq(AidlUtils.buildOutputSecureDexMetadataCompanion(
+                mDexPath, "arm", false /* isInDalvikCache */, permissionSettings)));
+        verify(mArtd).maybeCreateSdc(deepEq(AidlUtils.buildOutputSecureDexMetadataCompanion(
+                mSplit0DexPath, "arm64", false /* isInDalvikCache */, permissionSettings)));
+        verify(mArtd).maybeCreateSdc(deepEq(AidlUtils.buildOutputSecureDexMetadataCompanion(
+                mSplit0DexPath, "arm", false /* isInDalvikCache */, permissionSettings)));
+    }
+
+    @Test
+    public void testMaybeCreateSdcCompilerFilterSkip() throws Exception {
+        mDexoptParams = new DexoptParams.Builder("install")
+                                .setCompilerFilter(DexoptParams.COMPILER_FILTER_NOOP)
+                                .setFlags(ArtFlags.FLAG_FOR_PRIMARY_DEX)
+                                .build();
+        mPrimaryDexopter =
+                new PrimaryDexopter(mInjector, mPkgState, mPkg, mDexoptParams, mCancellationSignal);
+
+        mPrimaryDexopter.dexopt();
+
+        verify(mArtd, times(4)).maybeCreateSdc(any());
+        verify(mArtd, never())
+                .dexopt(any(), any(), any(), any(), any(), any(), any(), any(), anyInt(), any(),
+                        any());
+    }
+
     private void checkDexoptWithProfile(IArtd artd, String dexPath, String isa, ProfilePath profile,
             boolean isOtherReadable) throws Exception {
         artd.dexopt(argThat(artifacts
diff --git a/libartservice/service/javatests/com/android/server/art/UtilsTest.java b/libartservice/service/javatests/com/android/server/art/UtilsTest.java
index 5dd484e021..806e4fe38f 100644
--- a/libartservice/service/javatests/com/android/server/art/UtilsTest.java
+++ b/libartservice/service/javatests/com/android/server/art/UtilsTest.java
@@ -16,6 +16,8 @@
 
 package com.android.server.art;
 
+import static android.app.ActivityManager.RunningAppProcessInfo;
+
 import static com.google.common.truth.Truth.assertThat;
 
 import static org.mockito.Mockito.eq;
@@ -23,7 +25,9 @@ import static org.mockito.Mockito.lenient;
 import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.when;
 
+import android.app.ActivityManager;
 import android.os.SystemProperties;
+import android.os.UserHandle;
 import android.util.SparseArray;
 
 import androidx.test.filters.SmallTest;
@@ -235,4 +239,64 @@ public class UtilsTest {
                 .isEqualTo("/directory/file");
         assertThat(Utils.replaceFileExtension("", ".dm")).isEqualTo(".dm");
     }
+
+    @Test
+    public void TestGetRunningProcessInfoForPackage() throws Exception {
+        String packageName1 = "com.example.foo";
+        String packageName2 = "com.example.bar";
+        PackageState pkgState1 = createPackageState(packageName1, 10000 /* appId */);
+
+        var am = mock(ActivityManager.class);
+        when(am.getRunningAppProcesses())
+                .thenReturn(List.of(createProcessInfo(1000 /* pid */, "com.example.foo",
+                                            UserHandle.of(0).getUid(10000),
+                                            new String[] {packageName1, packageName2},
+                                            RunningAppProcessInfo.IMPORTANCE_FOREGROUND),
+                        createProcessInfo(1001 /* pid */, "com.example.foo:service1",
+                                UserHandle.of(0).getUid(10000), new String[] {packageName1},
+                                RunningAppProcessInfo.IMPORTANCE_FOREGROUND_SERVICE),
+                        createProcessInfo(1002 /* pid */, "random-name",
+                                UserHandle.of(0).getUid(10000),
+                                new String[] {packageName2, packageName1},
+                                RunningAppProcessInfo.IMPORTANCE_SERVICE),
+                        // Wrong importance.
+                        createProcessInfo(1003 /* pid */, "com.example.foo",
+                                UserHandle.of(0).getUid(10000),
+                                new String[] {packageName2, packageName1},
+                                RunningAppProcessInfo.IMPORTANCE_CACHED),
+                        // User 1.
+                        createProcessInfo(1004 /* pid */, "com.example.foo",
+                                UserHandle.of(1).getUid(10000),
+                                new String[] {packageName1, packageName2},
+                                RunningAppProcessInfo.IMPORTANCE_FOREGROUND),
+                        // Wrong package (the process name doesn't matter).
+                        createProcessInfo(1005 /* pid */, "com.example.foo",
+                                UserHandle.of(0).getUid(10000), new String[] {packageName2},
+                                RunningAppProcessInfo.IMPORTANCE_FOREGROUND),
+                        // Wrong app id.
+                        createProcessInfo(1006 /* pid */, "com.example.foo",
+                                UserHandle.of(0).getUid(10001), new String[] {packageName1},
+                                RunningAppProcessInfo.IMPORTANCE_FOREGROUND)));
+
+        assertThat(Utils.getRunningProcessInfoForPackage(am, pkgState1)
+                           .stream()
+                           .map(info -> info.pid)
+                           .toList())
+                .containsExactly(1000, 1001, 1002, 1004);
+    }
+
+    private RunningAppProcessInfo createProcessInfo(
+            int pid, String processName, int uid, String[] pkgList, int importance) {
+        var info = new RunningAppProcessInfo(processName, pid, pkgList);
+        info.uid = uid;
+        info.importance = importance;
+        return info;
+    }
+
+    private PackageState createPackageState(String packageName, int appId) {
+        PackageState pkgState = mock(PackageState.class);
+        lenient().when(pkgState.getPackageName()).thenReturn(packageName);
+        lenient().when(pkgState.getAppId()).thenReturn(appId);
+        return pkgState;
+    }
 }
diff --git a/libartservice/service/javatests/com/android/server/art/model/DexoptParamsTest.java b/libartservice/service/javatests/com/android/server/art/model/DexoptParamsTest.java
index beadb314cf..1ded5da25c 100644
--- a/libartservice/service/javatests/com/android/server/art/model/DexoptParamsTest.java
+++ b/libartservice/service/javatests/com/android/server/art/model/DexoptParamsTest.java
@@ -29,7 +29,6 @@ import org.junit.runner.RunWith;
 import java.lang.reflect.Field;
 import java.lang.reflect.Modifier;
 import java.util.Arrays;
-import java.util.stream.Collectors;
 
 @SmallTest
 @RunWith(AndroidJUnit4.class)
@@ -182,7 +181,7 @@ public class DexoptParamsTest {
         assertThat(Arrays.stream(DexoptParams.class.getDeclaredFields())
                            .filter(field -> !Modifier.isStatic(field.getModifiers()))
                            .map(Field::getName)
-                           .collect(Collectors.toList()))
+                           .toList())
                 .containsExactly(
                         "mFlags", "mCompilerFilter", "mPriorityClass", "mReason", "mSplitName");
     }
diff --git a/libartservice/service/javatests/com/android/server/art/prereboot/PreRebootStatsReporterTest.java b/libartservice/service/javatests/com/android/server/art/prereboot/PreRebootStatsReporterTest.java
index 59291b7471..846a835b75 100644
--- a/libartservice/service/javatests/com/android/server/art/prereboot/PreRebootStatsReporterTest.java
+++ b/libartservice/service/javatests/com/android/server/art/prereboot/PreRebootStatsReporterTest.java
@@ -17,6 +17,7 @@
 package com.android.server.art.prereboot;
 
 import static com.android.server.art.model.DexoptStatus.DexContainerFileDexoptStatus;
+import static com.android.server.art.prereboot.PreRebootDriver.PreRebootResult;
 import static com.android.server.art.proto.PreRebootStats.JobRun;
 import static com.android.server.art.proto.PreRebootStats.JobType;
 import static com.android.server.art.proto.PreRebootStats.Status;
@@ -118,7 +119,7 @@ public class PreRebootStatsReporterTest {
                                .build());
 
             doReturn(300l).when(mInjector).getCurrentTimeMillis();
-            reporter.recordJobEnded(true /* success */, false /* systemRequirementCheckFailed */);
+            reporter.recordJobEnded(new PreRebootResult(true /* success */));
             checkProto(PreRebootStats.newBuilder()
                                .setStatus(Status.STATUS_CANCELLED)
                                .setJobType(JobType.JOB_TYPE_MAINLINE)
@@ -174,7 +175,7 @@ public class PreRebootStatsReporterTest {
                                .build());
 
             doReturn(600l).when(mInjector).getCurrentTimeMillis();
-            reporter.recordJobEnded(true /* success */, false /* systemRequirementCheckFailed */);
+            reporter.recordJobEnded(new PreRebootResult(true /* success */));
             checkProto(PreRebootStats.newBuilder()
                                .setStatus(Status.STATUS_FINISHED)
                                .setJobType(JobType.JOB_TYPE_MAINLINE)
@@ -290,7 +291,7 @@ public class PreRebootStatsReporterTest {
                                .build());
 
             doReturn(300l).when(mInjector).getCurrentTimeMillis();
-            reporter.recordJobEnded(true /* success */, false /* systemRequirementCheckFailed */);
+            reporter.recordJobEnded(new PreRebootResult(true /* success */));
             checkProto(PreRebootStats.newBuilder()
                                .setStatus(Status.STATUS_FINISHED)
                                .setJobType(JobType.JOB_TYPE_OTA)
@@ -350,7 +351,8 @@ public class PreRebootStatsReporterTest {
                                .build());
 
             doReturn(300l).when(mInjector).getCurrentTimeMillis();
-            reporter.recordJobEnded(false /* success */, systemRequirementCheckFailed);
+            reporter.recordJobEnded(
+                    new PreRebootResult(false /* success */, systemRequirementCheckFailed));
             checkProto(PreRebootStats.newBuilder()
                                .setStatus(systemRequirementCheckFailed
                                                ? Status.STATUS_ABORTED_SYSTEM_REQUIREMENTS
diff --git a/libartservice/service/javatests/com/android/server/art/testing/CommandExecution.java b/libartservice/service/javatests/com/android/server/art/testing/CommandExecution.java
new file mode 100644
index 0000000000..7391145ecb
--- /dev/null
+++ b/libartservice/service/javatests/com/android/server/art/testing/CommandExecution.java
@@ -0,0 +1,123 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.android.server.art.testing;
+
+import static org.mockito.Mockito.mock;
+
+import android.os.Binder;
+import android.os.ParcelFileDescriptor;
+
+import com.android.modules.utils.BasicShellCommandHandler;
+import com.android.server.art.Utils;
+
+import java.io.BufferedReader;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.InputStreamReader;
+import java.io.PrintWriter;
+import java.util.concurrent.CompletableFuture;
+
+/** A harness to test a {@link BasicShellCommandHandler}. */
+public class CommandExecution implements AutoCloseable {
+    private ParcelFileDescriptor[] mStdinPipe = null;
+    private ParcelFileDescriptor[] mStdoutPipe = null;
+    private ParcelFileDescriptor[] mStderrPipe = null;
+    private PrintWriter mStdinWriter = null;
+    private BufferedReader mStdoutReader = null;
+    private BufferedReader mStderrReader = null;
+    private CompletableFuture<Integer> mFuture = null;
+
+    public CommandExecution(BasicShellCommandHandler commandHandler, String... args)
+            throws Exception {
+        try {
+            mStdinPipe = ParcelFileDescriptor.createPipe();
+            mStdoutPipe = ParcelFileDescriptor.createPipe();
+            mStderrPipe = ParcelFileDescriptor.createPipe();
+            mStdinWriter = new PrintWriter(new FileOutputStream(mStdinPipe[1].getFileDescriptor()));
+            mStdoutReader = new BufferedReader(
+                    new InputStreamReader(new FileInputStream(mStdoutPipe[0].getFileDescriptor())));
+            mStderrReader = new BufferedReader(
+                    new InputStreamReader(new FileInputStream(mStderrPipe[0].getFileDescriptor())));
+            mFuture = CompletableFuture.supplyAsync(() -> exec(commandHandler, args));
+        } catch (Exception e) {
+            close();
+            throw e;
+        }
+    }
+
+    private int exec(BasicShellCommandHandler commandHandler, String... args) {
+        int exitCode = commandHandler.exec(mock(Binder.class), mStdinPipe[0].getFileDescriptor(),
+                mStdoutPipe[1].getFileDescriptor(), mStderrPipe[1].getFileDescriptor(), args);
+        try {
+            mStdinPipe[0].close();
+            mStdoutPipe[1].close();
+            mStderrPipe[1].close();
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+        return exitCode;
+    }
+
+    public int waitAndGetExitCode() {
+        return Utils.getFuture(mFuture);
+    }
+
+    public PrintWriter getStdin() {
+        return mStdinWriter;
+    }
+
+    public void closeStdin() throws Exception {
+        mStdinWriter.close();
+        mStdinPipe[1].close();
+    }
+
+    public BufferedReader getStdout() {
+        return mStdoutReader;
+    }
+
+    public BufferedReader getStderr() {
+        return mStderrReader;
+    }
+
+    @Override
+    public void close() throws Exception {
+        if (mStdinWriter != null) {
+            mStdinWriter.close();
+        }
+        if (mStdoutReader != null) {
+            mStdoutReader.close();
+        }
+        if (mStderrReader != null) {
+            mStderrReader.close();
+        }
+        // Note that we still need to close the FDs after closing the streams. See the
+        // Android-specific warning at
+        // https://developer.android.com/reference/java/io/FileInputStream#FileInputStream(java.io.FileDescriptor)
+        if (mStdinPipe != null) {
+            mStdinPipe[0].close();
+            mStdinPipe[1].close();
+        }
+        if (mStdoutPipe != null) {
+            mStdoutPipe[0].close();
+            mStdoutPipe[1].close();
+        }
+        if (mStderrPipe != null) {
+            mStderrPipe[0].close();
+            mStderrPipe[1].close();
+        }
+    }
+}
diff --git a/libartservice/service/native/service_test.cc b/libartservice/service/native/service_test.cc
index d1d429edd7..b0864b7769 100644
--- a/libartservice/service/native/service_test.cc
+++ b/libartservice/service/native/service_test.cc
@@ -30,7 +30,7 @@ using ::android::base::testing::WithMessage;
 
 using std::literals::operator""s;  // NOLINT
 
-class ArtServiceTest : public testing::Test {};
+class ArtServiceTest : public ::testing::Test {};
 
 TEST_F(ArtServiceTest, ValidatePathElementOk) {
   EXPECT_THAT(ValidatePathElement("com.android.foo", "packageName"), Ok());
@@ -110,7 +110,7 @@ TEST_F(ArtServiceTest, ValidateDexPathNul) {
 class ArtServiceGcTest : public CommonRuntimeTest {};
 
 TEST_F(ArtServiceGcTest, GetGarbageCollector) {
-  EXPECT_THAT(GetGarbageCollector(), testing::HasSubstr("CollectorType"));
+  EXPECT_THAT(GetGarbageCollector(), ::testing::HasSubstr("CollectorType"));
 }
 
 }  // namespace
diff --git a/libartservice/service/proto/dex_use.proto b/libartservice/service/proto/dex_use.proto
index 1dd962dbf4..1960882ad5 100644
--- a/libartservice/service/proto/dex_use.proto
+++ b/libartservice/service/proto/dex_use.proto
@@ -29,31 +29,31 @@ message DexUseProto {
 }
 
 message PackageDexUseProto {
-    string owning_package_name = 1;
+    string owning_package_name = 1;  // key
     repeated PrimaryDexUseProto primary_dex_use = 2;
     repeated SecondaryDexUseProto secondary_dex_use = 3;
 }
 
 message PrimaryDexUseProto {
-    string dex_file = 1;
+    string dex_file = 1;  // key
     repeated PrimaryDexUseRecordProto record = 2;
 }
 
 message PrimaryDexUseRecordProto {
-    string loading_package_name = 1;
-    bool isolated_process = 2;
+    string loading_package_name = 1;  // key
+    bool isolated_process = 2;        // key
     int64 last_used_at_ms = 3;
 }
 
 message SecondaryDexUseProto {
-    string dex_file = 1;
+    string dex_file = 1;     // key
     Int32Value user_id = 2;  // Must be explicitly set.
     repeated SecondaryDexUseRecordProto record = 3;
 }
 
 message SecondaryDexUseRecordProto {
-    string loading_package_name = 1;
-    bool isolated_process = 2;
+    string loading_package_name = 1;  // key
+    bool isolated_process = 2;        // key
     string class_loader_context = 3;
     string abi_name = 4;
     int64 last_used_at_ms = 5;
diff --git a/libarttools/art_exec_test.cc b/libarttools/art_exec_test.cc
index d6e0ba4802..c729b5dc0f 100644
--- a/libarttools/art_exec_test.cc
+++ b/libarttools/art_exec_test.cc
@@ -69,10 +69,10 @@ bool GetCap(pid_t pid, cap_flag_t flag, cap_value_t value) {
   return flag_value == CAP_SET;
 }
 
-class ArtExecTest : public testing::Test {
+class ArtExecTest : public ::testing::Test {
  protected:
   void SetUp() override {
-    testing::Test::SetUp();
+    ::testing::Test::SetUp();
     if (!kIsTargetAndroid) {
       GTEST_SKIP() << "art_exec is for device only";
     }
diff --git a/libarttools/cmdline_builder_test.cc b/libarttools/cmdline_builder_test.cc
index 1acf2e3f0b..04b746fa8b 100644
--- a/libarttools/cmdline_builder_test.cc
+++ b/libarttools/cmdline_builder_test.cc
@@ -28,7 +28,7 @@ namespace {
 using ::testing::ElementsAre;
 using ::testing::IsEmpty;
 
-class CmdlineBuilderTest : public testing::Test {
+class CmdlineBuilderTest : public ::testing::Test {
  protected:
   CmdlineBuilder args_;
 };
diff --git a/libarttools/system_properties_test.cc b/libarttools/system_properties_test.cc
index acffd9727c..6d5db58573 100644
--- a/libarttools/system_properties_test.cc
+++ b/libarttools/system_properties_test.cc
@@ -30,7 +30,7 @@ class MockSystemProperties : public SystemProperties {
   MOCK_METHOD(std::string, GetProperty, (const std::string& key), (const, override));
 };
 
-class SystemPropertiesTest : public testing::Test {
+class SystemPropertiesTest : public ::testing::Test {
  protected:
   MockSystemProperties system_properties_;
 };
diff --git a/libdexfile/dex/class_accessor-inl.h b/libdexfile/dex/class_accessor-inl.h
index 5979f86e8d..663b75cf86 100644
--- a/libdexfile/dex/class_accessor-inl.h
+++ b/libdexfile/dex/class_accessor-inl.h
@@ -69,7 +69,7 @@ inline void ClassAccessor::Method::Read() {
   code_off_ = DecodeUnsignedLeb128(&ptr_pos_);
   if (hiddenapi_ptr_pos_ != nullptr) {
     hiddenapi_flags_ = DecodeUnsignedLeb128(&hiddenapi_ptr_pos_);
-    DCHECK(hiddenapi::ApiList(hiddenapi_flags_).IsValid());
+    DCHECK(hiddenapi::ApiList::FromDexFlags(hiddenapi_flags_).IsValid());
   }
 }
 
@@ -83,7 +83,7 @@ inline void ClassAccessor::Field::Read() {
   access_flags_ = DecodeUnsignedLeb128(&ptr_pos_);
   if (hiddenapi_ptr_pos_ != nullptr) {
     hiddenapi_flags_ = DecodeUnsignedLeb128(&hiddenapi_ptr_pos_);
-    DCHECK(hiddenapi::ApiList(hiddenapi_flags_).IsValid());
+    DCHECK(hiddenapi::ApiList::FromDexFlags(hiddenapi_flags_).IsValid());
   }
 }
 
diff --git a/libdexfile/dex/class_accessor_test.cc b/libdexfile/dex/class_accessor_test.cc
index cb50096084..5c83ea5df6 100644
--- a/libdexfile/dex/class_accessor_test.cc
+++ b/libdexfile/dex/class_accessor_test.cc
@@ -31,7 +31,8 @@ TEST_F(ClassAccessorTest, TestVisiting) {
     ASSERT_GT(dex_file->NumClassDefs(), 0u);
     for (ClassAccessor accessor : dex_file->GetClasses()) {
       const dex::ClassDef& class_def = dex_file->GetClassDef(accessor.GetClassDefIndex());
-      EXPECT_EQ(accessor.GetDescriptor(), dex_file->GetTypeDescriptor(class_def.class_idx_));
+      EXPECT_EQ(accessor.GetDescriptorView(),
+                dex_file->GetTypeDescriptorView(class_def.class_idx_));
       EXPECT_EQ(class_def_idx, accessor.GetClassDefIndex());
       ++class_def_idx;
       // Check iterators against visitors.
diff --git a/libdexfile/dex/code_item_accessors.h b/libdexfile/dex/code_item_accessors.h
index 5952b2d7ea..bfdf30b066 100644
--- a/libdexfile/dex/code_item_accessors.h
+++ b/libdexfile/dex/code_item_accessors.h
@@ -30,7 +30,6 @@ struct CodeItem;
 struct TryItem;
 }  // namespace dex
 
-class ArtMethod;
 class DexFile;
 class DexInstructionIterator;
 template <typename Iter>
@@ -43,8 +42,6 @@ class CodeItemInstructionAccessor {
   ALWAYS_INLINE CodeItemInstructionAccessor(const DexFile& dex_file,
                                             const dex::CodeItem* code_item);
 
-  ALWAYS_INLINE explicit CodeItemInstructionAccessor(ArtMethod* method);
-
   ALWAYS_INLINE DexInstructionIterator begin() const;
 
   ALWAYS_INLINE DexInstructionIterator end() const;
@@ -155,8 +152,6 @@ class CodeItemDebugInfoAccessor : public CodeItemDataAccessor {
                           const dex::CodeItem* code_item,
                           uint32_t dex_method_index);
 
-  ALWAYS_INLINE explicit CodeItemDebugInfoAccessor(ArtMethod* method);
-
   uint32_t DebugInfoOffset() const {
     return debug_info_offset_;
   }
diff --git a/libdexfile/dex/code_item_accessors_test.cc b/libdexfile/dex/code_item_accessors_test.cc
index a815d06cb8..bd1ddc958d 100644
--- a/libdexfile/dex/code_item_accessors_test.cc
+++ b/libdexfile/dex/code_item_accessors_test.cc
@@ -26,7 +26,7 @@
 
 namespace art {
 
-class CodeItemAccessorsTest : public testing::Test {};
+class CodeItemAccessorsTest : public ::testing::Test {};
 
 std::unique_ptr<const DexFile> CreateFakeDex(bool compact_dex, std::vector<uint8_t>* data) {
   data->resize(MemMap::GetPageSize());
@@ -61,10 +61,6 @@ TEST(CodeItemAccessorsTest, TestDexInstructionsAccessor) {
   std::unique_ptr<const DexFile> standard_dex(CreateFakeDex(/*compact_dex=*/false,
                                                             &standard_dex_data));
   ASSERT_TRUE(standard_dex != nullptr);
-  std::vector<uint8_t> compact_dex_data;
-  std::unique_ptr<const DexFile> compact_dex(CreateFakeDex(/*compact_dex=*/true,
-                                                           &compact_dex_data));
-  ASSERT_TRUE(compact_dex != nullptr);
   static constexpr uint16_t kRegisterSize = 2;
   static constexpr uint16_t kInsSize = 1;
   static constexpr uint16_t kOutsSize = 3;
@@ -98,19 +94,6 @@ TEST(CodeItemAccessorsTest, TestDexInstructionsAccessor) {
   dex_code_item->tries_size_ = kTriesSize;
   dex_code_item->insns_size_in_code_units_ = kInsnsSizeInCodeUnits;
   verify_code_item(standard_dex.get(), dex_code_item, dex_code_item->insns_);
-
-  CompactDexFile::CodeItem* cdex_code_item =
-      reinterpret_cast<CompactDexFile::CodeItem*>(const_cast<uint8_t*>(compact_dex->Begin() +
-          CompactDexFile::CodeItem::kMaxPreHeaderSize * sizeof(uint16_t)));
-  std::vector<uint16_t> preheader;
-  cdex_code_item->Create(kRegisterSize,
-                         kInsSize,
-                         kOutsSize,
-                         kTriesSize,
-                         kInsnsSizeInCodeUnits,
-                         cdex_code_item->GetPreHeader());
-
-  verify_code_item(compact_dex.get(), cdex_code_item, cdex_code_item->insns_);
 }
 
 }  // namespace art
diff --git a/libdexfile/dex/compact_dex_file_test.cc b/libdexfile/dex/compact_dex_file_test.cc
index 799967e255..345e66b1d5 100644
--- a/libdexfile/dex/compact_dex_file_test.cc
+++ b/libdexfile/dex/compact_dex_file_test.cc
@@ -38,8 +38,8 @@ TEST(CompactDexFileTest, MagicAndVersion) {
       }
       EXPECT_EQ(valid_magic, CompactDexFile::IsMagicValid(header));
       EXPECT_EQ(valid_version, CompactDexFile::IsVersionValid(header));
-      EXPECT_EQ(valid_magic, DexFileLoader::IsMagicValid(header));
-      EXPECT_EQ(valid_magic && valid_version, DexFileLoader::IsVersionAndMagicValid(header));
+      EXPECT_FALSE(DexFileLoader::IsMagicValid(header));
+      EXPECT_FALSE(DexFileLoader::IsVersionAndMagicValid(header));
     }
   }
 }
diff --git a/libdexfile/dex/descriptors_names.cc b/libdexfile/dex/descriptors_names.cc
index f382641d72..a7cc2cbe32 100644
--- a/libdexfile/dex/descriptors_names.cc
+++ b/libdexfile/dex/descriptors_names.cc
@@ -193,21 +193,22 @@ std::string MangleForJni(const std::string& s) {
   return result;
 }
 
-std::string DotToDescriptor(const char* class_name) {
+std::string DotToDescriptor(std::string_view class_name) {
   std::string descriptor(class_name);
   std::replace(descriptor.begin(), descriptor.end(), '.', '/');
   if (descriptor.length() > 0 && descriptor[0] != '[') {
-    descriptor = "L" + descriptor + ";";
+    descriptor.insert(descriptor.begin(), 'L');
+    descriptor.insert(descriptor.end(), ';');
   }
   return descriptor;
 }
 
-std::string DescriptorToDot(const char* descriptor) {
-  size_t length = strlen(descriptor);
+std::string DescriptorToDot(std::string_view descriptor) {
+  size_t length = descriptor.length();
   if (length > 1) {
     if (descriptor[0] == 'L' && descriptor[length - 1] == ';') {
       // Descriptors have the leading 'L' and trailing ';' stripped.
-      std::string result(descriptor + 1, length - 2);
+      std::string result(descriptor.substr(1, length - 2));
       std::replace(result.begin(), result.end(), '/', '.');
       return result;
     } else {
@@ -218,16 +219,16 @@ std::string DescriptorToDot(const char* descriptor) {
     }
   }
   // Do nothing for non-class/array descriptors.
-  return descriptor;
+  return std::string(descriptor);
 }
 
-std::string DescriptorToName(const char* descriptor) {
-  size_t length = strlen(descriptor);
+std::string DescriptorToName(std::string_view descriptor) {
+  size_t length = descriptor.length();
   if (descriptor[0] == 'L' && descriptor[length - 1] == ';') {
-    std::string result(descriptor + 1, length - 2);
+    std::string result(descriptor.substr(1, length - 2));
     return result;
   }
-  return descriptor;
+  return std::string(descriptor);
 }
 
 // Helper for IsValidPartOfMemberNameUtf8(), a bit vector indicating valid low ascii.
diff --git a/libdexfile/dex/descriptors_names.h b/libdexfile/dex/descriptors_names.h
index 5ece97dddc..abaa6cc8d8 100644
--- a/libdexfile/dex/descriptors_names.h
+++ b/libdexfile/dex/descriptors_names.h
@@ -44,15 +44,15 @@ std::string MangleForJni(const std::string& s);
 std::string GetJniShortName(const std::string& class_name, const std::string& method_name);
 
 // Turn "java.lang.String" into "Ljava/lang/String;".
-std::string DotToDescriptor(const char* class_name);
+std::string DotToDescriptor(std::string_view class_name);
 
 // Turn "Ljava/lang/String;" into "java.lang.String" using the conventions of
 // java.lang.Class.getName().
-std::string DescriptorToDot(const char* descriptor);
+std::string DescriptorToDot(std::string_view descriptor);
 
 // Turn "Ljava/lang/String;" into "java/lang/String" using the opposite conventions of
 // java.lang.Class.getName().
-std::string DescriptorToName(const char* descriptor);
+std::string DescriptorToName(std::string_view descriptor);
 
 // Tests for whether 's' is a valid class name in the three common forms:
 bool IsValidBinaryClassName(const char* s);  // "java.lang.String"
diff --git a/libdexfile/dex/descriptors_names_test.cc b/libdexfile/dex/descriptors_names_test.cc
index f3ec3ed0f3..1b0654a770 100644
--- a/libdexfile/dex/descriptors_names_test.cc
+++ b/libdexfile/dex/descriptors_names_test.cc
@@ -20,7 +20,7 @@
 
 namespace art {
 
-class DescriptorsNamesTest : public testing::Test {};
+class DescriptorsNamesTest : public ::testing::Test {};
 
 TEST_F(DescriptorsNamesTest, PrettyDescriptor_ArrayReferences) {
   EXPECT_EQ("java.lang.Class[]", PrettyDescriptor("[Ljava/lang/Class;"));
diff --git a/libdexfile/dex/dex_file.h b/libdexfile/dex/dex_file.h
index 619ee0d4f5..7cf0b7f62c 100644
--- a/libdexfile/dex/dex_file.h
+++ b/libdexfile/dex/dex_file.h
@@ -591,7 +591,7 @@ class DexFile {
                              uint32_t signature_length) const;
   const dex::ProtoId* FindProtoId(dex::TypeIndex return_type_idx,
                                   const std::vector<dex::TypeIndex>& signature_type_idxs) const {
-    return FindProtoId(return_type_idx, &signature_type_idxs[0], signature_type_idxs.size());
+    return FindProtoId(return_type_idx, signature_type_idxs.data(), signature_type_idxs.size());
   }
 
   // Given a signature place the type ids into the given vector, returns true on success
diff --git a/libdexfile/dex/dex_file_loader.cc b/libdexfile/dex/dex_file_loader.cc
index e92a5ac813..df9c9c11cb 100644
--- a/libdexfile/dex/dex_file_loader.cc
+++ b/libdexfile/dex/dex_file_loader.cc
@@ -131,18 +131,11 @@ bool DexFileLoader::IsMagicValid(uint32_t magic) {
 }
 
 bool DexFileLoader::IsMagicValid(const uint8_t* magic) {
-  return StandardDexFile::IsMagicValid(magic) ||
-      CompactDexFile::IsMagicValid(magic);
+  return StandardDexFile::IsMagicValid(magic);
 }
 
 bool DexFileLoader::IsVersionAndMagicValid(const uint8_t* magic) {
-  if (StandardDexFile::IsMagicValid(magic)) {
-    return StandardDexFile::IsVersionValid(magic);
-  }
-  if (CompactDexFile::IsMagicValid(magic)) {
-    return CompactDexFile::IsVersionValid(magic);
-  }
-  return false;
+  return StandardDexFile::IsMagicValid(magic) && StandardDexFile::IsVersionValid(magic);
 }
 
 bool DexFileLoader::IsMultiDexLocation(std::string_view location) {
@@ -474,9 +467,6 @@ std::unique_ptr<DexFile> DexFileLoader::OpenCommon(std::shared_ptr<DexFileContai
   if (size >= sizeof(StandardDexFile::Header) && StandardDexFile::IsMagicValid(base)) {
     uint32_t checksum = location_checksum.value_or(header->checksum_);
     dex_file.reset(new StandardDexFile(base, location, checksum, oat_dex_file, container));
-  } else if (size >= sizeof(CompactDexFile::Header) && CompactDexFile::IsMagicValid(base)) {
-    uint32_t checksum = location_checksum.value_or(header->checksum_);
-    dex_file.reset(new CompactDexFile(base, location, checksum, oat_dex_file, container));
   } else {
     *error_msg = StringPrintf("Invalid or truncated dex file '%s'", location.c_str());
   }
@@ -489,8 +479,7 @@ std::unique_ptr<DexFile> DexFileLoader::OpenCommon(std::shared_ptr<DexFileContai
     dex_file.reset();
     return nullptr;
   }
-  // NB: Dex verifier does not understand the compact dex format.
-  if (verify && !dex_file->IsCompactDexFile()) {
+  if (verify) {
     DEXFILE_SCOPED_TRACE(std::string("Verify dex file ") + location);
     if (!dex::Verify(dex_file.get(), location.c_str(), verify_checksum, error_msg)) {
       if (error_code != nullptr) {
diff --git a/libdexfile/dex/dex_file_loader_test.cc b/libdexfile/dex/dex_file_loader_test.cc
index 3de1103256..6041ea2077 100644
--- a/libdexfile/dex/dex_file_loader_test.cc
+++ b/libdexfile/dex/dex_file_loader_test.cc
@@ -27,7 +27,7 @@
 
 namespace art {
 
-class DexFileLoaderTest : public testing::Test {};
+class DexFileLoaderTest : public ::testing::Test {};
 
 static constexpr char kLocationString[] = "/a/dex/file/location";
 
diff --git a/libdexfile/dex/dex_file_verifier.cc b/libdexfile/dex/dex_file_verifier.cc
index d840027286..3f2fd627db 100644
--- a/libdexfile/dex/dex_file_verifier.cc
+++ b/libdexfile/dex/dex_file_verifier.cc
@@ -315,8 +315,11 @@ class DexFileVerifier {
   bool CheckIntraMethodHandleItem();
   bool CheckIntraTypeList();
   // Check all fields of the given type, reading `encoded_field` entries from `ptr_`.
+  // Check instance fields against duplicates with static fields.
   template <bool kStatic>
-  bool CheckIntraClassDataItemFields(size_t count);
+  bool CheckIntraClassDataItemFields(size_t num_fields,
+                                     ClassAccessor::Field* static_fields,
+                                     size_t num_static_fields);
   // Check direct or virtual methods, reading `encoded_method` entries from `ptr_`.
   // Check virtual methods against duplicates with direct methods.
   bool CheckIntraClassDataItemMethods(size_t num_methods,
@@ -778,6 +781,29 @@ bool DexFileVerifier::CheckMap() {
                           static_cast<size_t>(item_type));
         return false;
       }
+
+      size_t alignment;
+      switch (item_type) {
+        case DexFile::kDexTypeClassDataItem:
+        case DexFile::kDexTypeStringDataItem:
+        case DexFile::kDexTypeDebugInfoItem:
+        case DexFile::kDexTypeAnnotationItem:
+        case DexFile::kDexTypeEncodedArrayItem:
+          alignment = sizeof(uint8_t);
+          break;
+        default:
+          alignment = sizeof(uint32_t);
+          break;
+      }
+
+      if (!IsAlignedParam(item->offset_, alignment)) {
+        ErrorStringPrintf("Offset(%d) should be aligned by %zu for maplist item of type %hu.",
+                          item->offset_,
+                          alignment,
+                          item->type_);
+        return false;
+      }
+
       data_items_left -= icount;
       data_item_count += icount;
     }
@@ -1522,14 +1548,23 @@ bool DexFileVerifier::CheckIntraTypeList() {
 }
 
 template <bool kStatic>
-bool DexFileVerifier::CheckIntraClassDataItemFields(size_t count) {
+bool DexFileVerifier::CheckIntraClassDataItemFields(size_t num_fields,
+                                                    ClassAccessor::Field* static_fields,
+                                                    size_t num_static_fields) {
   constexpr const char* kTypeDescr = kStatic ? "static field" : "instance field";
 
   // We cannot use ClassAccessor::Field yet as it could read beyond the end of the data section.
   const uint8_t* ptr = ptr_;
 
+  // Load the first static field for the check below.
+  size_t remaining_static_fields = num_static_fields;
+  if (remaining_static_fields != 0u) {
+    DCHECK(static_fields != nullptr);
+    static_fields->Read();
+  }
+
   uint32_t prev_index = 0;
-  for (size_t i = 0; i != count; ++i) {
+  for (size_t i = 0; i != num_fields; ++i) {
     uint32_t field_idx_diff, access_flags;
     if (UNLIKELY(!DecodeUnsignedLeb128Checked(&ptr, data_.end(), &field_idx_diff)) ||
         UNLIKELY(!DecodeUnsignedLeb128Checked(&ptr, data_.end(), &access_flags))) {
@@ -1551,6 +1586,29 @@ bool DexFileVerifier::CheckIntraClassDataItemFields(size_t count) {
       return false;
     }
 
+    // For instance fields, we cross reference the field index to make sure
+    // it doesn't match any static fields.
+    if (remaining_static_fields != 0) {
+      // The static fields are already known to be in ascending index order.
+      // So just keep up with the current index.
+      while (true) {
+        const uint32_t static_idx = static_fields->GetIndex();
+        if (static_idx > curr_index) {
+          break;
+        }
+        if (static_idx == curr_index) {
+          ErrorStringPrintf("Found instance field with same index as static field: %u",
+                            curr_index);
+          return false;
+        }
+        --remaining_static_fields;
+        if (remaining_static_fields == 0u) {
+          break;
+        }
+        static_fields->Read();
+      }
+    }
+
     prev_index = curr_index;
   }
 
@@ -1636,10 +1694,17 @@ bool DexFileVerifier::CheckIntraClassDataItem() {
   ptr_ = ptr;
 
   // Check fields.
-  if (!CheckIntraClassDataItemFields</*kStatic=*/ true>(static_fields_size)) {
+  const uint8_t* static_fields_ptr = ptr_;
+  if (!CheckIntraClassDataItemFields</*kStatic=*/ true>(static_fields_size,
+                                                        /*static_fields=*/ nullptr,
+                                                        /*num_static_fields=*/ 0u)) {
     return false;
   }
-  if (!CheckIntraClassDataItemFields</*kStatic=*/ false>(instance_fields_size)) {
+  // Static fields have been checked, so we can now use ClassAccessor::Field to read them again.
+  ClassAccessor::Field static_fields(*dex_file_, static_fields_ptr);
+  if (!CheckIntraClassDataItemFields</*kStatic=*/ false>(instance_fields_size,
+                                                         &static_fields,
+                                                         static_fields_size)) {
     return false;
   }
 
@@ -2618,7 +2683,7 @@ bool DexFileVerifier::CheckInterHiddenapiClassData() {
         failure = true;
         return;
       }
-      if (!hiddenapi::ApiList(decoded_flags).IsValid()) {
+      if (!hiddenapi::ApiList::FromDexFlags(decoded_flags).IsValid()) {
         ErrorStringPrintf("Hiddenapi class data flags invalid (%u) for %s %i",
                           decoded_flags, member_type, member.GetIndex());
         failure = true;
diff --git a/libdexfile/dex/dex_file_verifier_test.cc b/libdexfile/dex/dex_file_verifier_test.cc
index d67d9a938d..8320b94c9e 100644
--- a/libdexfile/dex/dex_file_verifier_test.cc
+++ b/libdexfile/dex/dex_file_verifier_test.cc
@@ -56,7 +56,7 @@ static void FixUpChecksum(uint8_t* dex_file) {
   header->checksum_ = adler_checksum;
 }
 
-class DexFileVerifierTest : public testing::Test {
+class DexFileVerifierTest : public ::testing::Test {
  protected:
   DexFile* GetDexFile(const uint8_t* dex_bytes, size_t length) {
     auto container = std::make_shared<MemoryDexFileContainer>(dex_bytes, length);
diff --git a/libdexfile/dex/dex_instruction.cc b/libdexfile/dex/dex_instruction.cc
index 67147bba11..4887491509 100644
--- a/libdexfile/dex/dex_instruction.cc
+++ b/libdexfile/dex/dex_instruction.cc
@@ -55,54 +55,28 @@ int32_t Instruction::GetTargetOffset() const {
   }
 }
 
-bool Instruction::CanFlowThrough() const {
-  const uint16_t* insns = reinterpret_cast<const uint16_t*>(this);
-  uint16_t insn = *insns;
-  Code opcode = static_cast<Code>(insn & 0xFF);
-  return  FlagsOf(opcode) & Instruction::kContinue;
-}
-
 size_t Instruction::SizeInCodeUnitsComplexOpcode() const {
-  const uint16_t* insns = reinterpret_cast<const uint16_t*>(this);
   // Handle special NOP encoded variable length sequences.
-  switch (*insns) {
+  uint16_t inst_data = Fetch16(0);
+  DCHECK_EQ(inst_data & 0xFF, 0) << DumpString(nullptr);
+  switch (inst_data) {
     case kPackedSwitchSignature:
-      return (4 + insns[1] * 2);
+      return (4 + Fetch16(1) * 2);
     case kSparseSwitchSignature:
-      return (2 + insns[1] * 4);
+      return (2 + Fetch16(1) * 4);
     case kArrayDataSignature: {
-      uint16_t element_size = insns[1];
-      uint32_t length = insns[2] | (((uint32_t)insns[3]) << 16);
+      uint16_t element_size = Fetch16(1);
+      uint32_t length = Fetch16(2) | ((static_cast<uint32_t>(Fetch16(3))) << 16);
       // The plus 1 is to round up for odd size and width.
-      return (4 + (element_size * length + 1) / 2);
+      uint32_t result = (4 + (element_size * length + 1) / 2);
+      // This function is used only after the `MethodVerifier` checked that the 32-bit calculation
+      // does not overflow. Let's `DCHECK()` the result against a 64-bit calculation.
+      DCHECK_EQ(result,
+                4 + (static_cast<uint64_t>(element_size) * static_cast<uint64_t>(length) + 1) / 2);
+      return result;
     }
     default:
-      if ((*insns & 0xFF) == 0) {
-        return 1;  // NOP.
-      } else {
-        LOG(FATAL) << "Unreachable: " << DumpString(nullptr);
-        UNREACHABLE();
-      }
-  }
-}
-
-size_t Instruction::CodeUnitsRequiredForSizeOfComplexOpcode() const {
-  const uint16_t* insns = reinterpret_cast<const uint16_t*>(this);
-  // Handle special NOP encoded variable length sequences.
-  switch (*insns) {
-    case kPackedSwitchSignature:
-      FALLTHROUGH_INTENDED;
-    case kSparseSwitchSignature:
-      return 2;
-    case kArrayDataSignature:
-      return 4;
-    default:
-      if ((*insns & 0xFF) == 0) {
-        return 1;  // NOP.
-      } else {
-        LOG(FATAL) << "Unreachable: " << DumpString(nullptr);
-        UNREACHABLE();
-      }
+      return 1;  // NOP.
   }
 }
 
diff --git a/libdexfile/dex/dex_instruction.h b/libdexfile/dex/dex_instruction.h
index 0f12f45acd..e47e3ebf87 100644
--- a/libdexfile/dex/dex_instruction.h
+++ b/libdexfile/dex/dex_instruction.h
@@ -197,6 +197,7 @@ class Instruction {
     kVerifyRegBCallSite       = 0x0800000,
     kVerifyRegBMethodHandle   = 0x1000000,
     kVerifyRegBPrototype      = 0x2000000,
+    kVerifyRegBFilledNewArray = 0x4000000,
   };
 
   // Collect the enums in a struct for better locality.
@@ -225,12 +226,6 @@ class Instruction {
   // Returns the size (in 2 byte code units) of the given instruction format.
   ALWAYS_INLINE static constexpr size_t SizeInCodeUnits(Format format);
 
-  // Code units required to calculate the size of the instruction.
-  size_t CodeUnitsRequiredForSizeComputation() const {
-    const int8_t result = InstructionDescriptorOf(Opcode()).size_in_code_units;
-    return UNLIKELY(result < 0) ? CodeUnitsRequiredForSizeOfComplexOpcode() : 1;
-  }
-
   // Reads an instruction out of the stream at the specified address.
   static const Instruction* At(const uint16_t* code) {
     DCHECK(code != nullptr);
@@ -558,7 +553,9 @@ class Instruction {
   int32_t GetTargetOffset() const;
 
   // Returns true if the instruction allows control flow to go to the following instruction.
-  bool CanFlowThrough() const;
+  bool CanFlowThrough() const {
+    return (FlagsOf(Opcode()) & Instruction::kContinue) != 0;
+  }
 
   // Returns true if this instruction is a switch.
   bool IsSwitch() const {
@@ -610,7 +607,8 @@ class Instruction {
         kVerifyRegBNewInstance |
         kVerifyRegBString |
         kVerifyRegBType |
-        kVerifyRegBWide;
+        kVerifyRegBWide |
+        kVerifyRegBFilledNewArray;
     return VerifyFlagsOf(opcode) & kMask;
   }
 
@@ -681,9 +679,6 @@ class Instruction {
     return kInstructionDescriptors[opcode];
   }
 
-  // Return how many code unit words are required to compute the size of the opcode.
-  size_t CodeUnitsRequiredForSizeOfComplexOpcode() const;
-
   uint32_t Fetch32(size_t offset) const {
     return (Fetch16(offset) | ((uint32_t) Fetch16(offset + 1) << 16));
   }
diff --git a/libdexfile/dex/dex_instruction_iterator.h b/libdexfile/dex/dex_instruction_iterator.h
index da494e1f08..9f4e39030c 100644
--- a/libdexfile/dex/dex_instruction_iterator.h
+++ b/libdexfile/dex/dex_instruction_iterator.h
@@ -156,83 +156,6 @@ class DexInstructionIterator : public DexInstructionIteratorBase {
   }
 };
 
-// A safe version of DexInstructionIterator that is guaranteed to not go past the end of the code
-// item.
-class SafeDexInstructionIterator : public DexInstructionIteratorBase {
- public:
-  explicit SafeDexInstructionIterator(const DexInstructionIteratorBase& start,
-                                      const DexInstructionIteratorBase& end)
-      : DexInstructionIteratorBase(&start.Inst(), start.DexPc())
-      , num_code_units_(end.DexPc()) {
-    DCHECK_EQ(start.Instructions(), end.Instructions())
-        << "start and end must be in the same code item.";
-  }
-
-  // Value after modification, does not read past the end of the allowed region. May increment past
-  // the end of the code item though.
-  SafeDexInstructionIterator& operator++() {
-    AssertValid();
-    const size_t size_code_units = Inst().CodeUnitsRequiredForSizeComputation();
-    const size_t available = NumCodeUnits() - DexPc();
-    if (UNLIKELY(size_code_units > available)) {
-      error_state_ = true;
-      return *this;
-    }
-    const size_t instruction_code_units = Inst().SizeInCodeUnits();
-    if (UNLIKELY(instruction_code_units > available)) {
-      error_state_ = true;
-      return *this;
-    }
-    data_.dex_pc_ += instruction_code_units;
-    return *this;
-  }
-
-  // Value before modification.
-  SafeDexInstructionIterator operator++(int) {
-    SafeDexInstructionIterator temp = *this;
-    ++*this;
-    return temp;
-  }
-
-  const value_type& operator*() const {
-    AssertValid();
-    return data_;
-  }
-
-  const Instruction* operator->() const {
-    AssertValid();
-    return &data_.Inst();
-  }
-
-  // Return the current instruction of the iterator.
-  ALWAYS_INLINE const Instruction& Inst() const {
-    return data_.Inst();
-  }
-
-  const uint16_t* Instructions() const {
-    return data_.Instructions();
-  }
-
-  // Returns true if the iterator is in an error state. This occurs when an instruction couldn't
-  // have its size computed without reading past the end iterator.
-  bool IsErrorState() const {
-    return error_state_;
-  }
-
- private:
-  ALWAYS_INLINE void AssertValid() const {
-    DCHECK(!IsErrorState());
-    DCHECK_LT(DexPc(), NumCodeUnits());
-  }
-
-  ALWAYS_INLINE uint32_t NumCodeUnits() const {
-    return num_code_units_;
-  }
-
-  const uint32_t num_code_units_ = 0;
-  bool error_state_ = false;
-};
-
 }  // namespace art
 
 #endif  // ART_LIBDEXFILE_DEX_DEX_INSTRUCTION_ITERATOR_H_
diff --git a/libdexfile/dex/dex_instruction_list.h b/libdexfile/dex/dex_instruction_list.h
index 0fd2a6b5bb..bbd7abd8e5 100644
--- a/libdexfile/dex/dex_instruction_list.h
+++ b/libdexfile/dex/dex_instruction_list.h
@@ -55,8 +55,8 @@
   V(0x21, ARRAY_LENGTH, "array-length", k12x, kIndexNone, kContinue | kThrow, 0, kVerifyRegA | kVerifyRegB) \
   V(0x22, NEW_INSTANCE, "new-instance", k21c, kIndexTypeRef, kContinue | kThrow, kClobber, kVerifyRegA | kVerifyRegBNewInstance) \
   V(0x23, NEW_ARRAY, "new-array", k22c, kIndexTypeRef, kContinue | kThrow, kClobber, kVerifyRegA | kVerifyRegB | kVerifyRegCNewArray) \
-  V(0x24, FILLED_NEW_ARRAY, "filled-new-array", k35c, kIndexTypeRef, kContinue | kThrow, kClobber, kVerifyRegBType | kVerifyVarArg) \
-  V(0x25, FILLED_NEW_ARRAY_RANGE, "filled-new-array/range", k3rc, kIndexTypeRef, kContinue | kThrow, kClobber, kVerifyRegBType | kVerifyVarArgRange) \
+  V(0x24, FILLED_NEW_ARRAY, "filled-new-array", k35c, kIndexTypeRef, kContinue | kThrow, kClobber, kVerifyRegBFilledNewArray | kVerifyVarArg) \
+  V(0x25, FILLED_NEW_ARRAY_RANGE, "filled-new-array/range", k3rc, kIndexTypeRef, kContinue | kThrow, kClobber, kVerifyRegBFilledNewArray | kVerifyVarArgRange) \
   V(0x26, FILL_ARRAY_DATA, "fill-array-data", k31t, kIndexNone, kContinue | kThrow, kClobber, kVerifyRegA | kVerifyArrayData) \
   V(0x27, THROW, "throw", k11x, kIndexNone, kThrow, 0, kVerifyRegA) \
   V(0x28, GOTO, "goto", k10t, kIndexNone, kBranch | kUnconditional, 0, kVerifyBranchTarget) \
diff --git a/libdexfile/dex/modifiers.h b/libdexfile/dex/modifiers.h
index 94e25e8e6e..ad6472df57 100644
--- a/libdexfile/dex/modifiers.h
+++ b/libdexfile/dex/modifiers.h
@@ -59,6 +59,9 @@ static constexpr uint32_t kAccObsoleteObject =        0x00200000;  // class (run
 // Set during boot image compilation to indicate that the class is
 // not initialized at compile time and not in the list of preloaded classes.
 static constexpr uint32_t kAccInBootImageAndNotInPreloadedClasses = 0x00400000;  // class (runtime)
+// Set after verification if at least one of the class's method has unresolved
+// type checks failures.
+static constexpr uint32_t kAccHasTypeChecksFailure = 0x00800000;  // class (runtime)
 // This is set by the class linker during LinkInterfaceMethods. It is used by a method
 // to represent that it was copied from its declaring class into another class.
 // We need copies of the original method because the method may end up in different
diff --git a/libdexfile/dex/type_lookup_table_test.cc b/libdexfile/dex/type_lookup_table_test.cc
index 4316be0bd6..27da71cf25 100644
--- a/libdexfile/dex/type_lookup_table_test.cc
+++ b/libdexfile/dex/type_lookup_table_test.cc
@@ -49,14 +49,14 @@ TEST_P(TypeLookupTableTest, Find) {
 
 INSTANTIATE_TEST_CASE_P(FindNonExistingClassWithoutCollisions,
                         TypeLookupTableTest,
-                        testing::Values(DescriptorClassDefIdxPair("LAB;", 1U)));
+                        ::testing::Values(DescriptorClassDefIdxPair("LAB;", 1U)));
 INSTANTIATE_TEST_CASE_P(FindNonExistingClassWithCollisions,
                         TypeLookupTableTest,
-                        testing::Values(DescriptorClassDefIdxPair("LDA;", dex::kDexNoIndex)));
+                        ::testing::Values(DescriptorClassDefIdxPair("LDA;", dex::kDexNoIndex)));
 INSTANTIATE_TEST_CASE_P(FindClassNoCollisions,
                         TypeLookupTableTest,
-                        testing::Values(DescriptorClassDefIdxPair("LC;", 2U)));
+                        ::testing::Values(DescriptorClassDefIdxPair("LC;", 2U)));
 INSTANTIATE_TEST_CASE_P(FindClassWithCollisions,
                         TypeLookupTableTest,
-                        testing::Values(DescriptorClassDefIdxPair("LAB;", 1U)));
+                        ::testing::Values(DescriptorClassDefIdxPair("LAB;", 1U)));
 }  // namespace art
diff --git a/libdexfile/dex/utf_test.cc b/libdexfile/dex/utf_test.cc
index 85c74d285c..e8c3313286 100644
--- a/libdexfile/dex/utf_test.cc
+++ b/libdexfile/dex/utf_test.cc
@@ -26,7 +26,7 @@
 
 namespace art {
 
-class UtfTest : public testing::Test {};
+class UtfTest : public ::testing::Test {};
 
 TEST_F(UtfTest, GetLeadingUtf16Char) {
   EXPECT_EQ(0xffff, GetLeadingUtf16Char(0xeeeeffff));
diff --git a/libelffile/elf/elf_builder.h b/libelffile/elf/elf_builder.h
index bb82b109bd..ec226567d5 100644
--- a/libelffile/elf/elf_builder.h
+++ b/libelffile/elf/elf_builder.h
@@ -37,14 +37,14 @@ namespace art {
 //   Elf_Ehdr                    - The ELF header.
 //   Elf_Phdr[]                  - Program headers for the linker.
 //   .note.gnu.build-id          - Optional build ID section (SHA-1 digest).
-//   .rodata                     - Oat metadata.
-//   .text                       - Compiled code.
-//   .bss                        - Zero-initialized writeable section.
-//   .dex                        - Reserved NOBITS space for dex-related data.
 //   .dynstr                     - Names for .dynsym.
 //   .dynsym                     - A few oat-specific dynamic symbols.
 //   .hash                       - Hash-table for .dynsym.
 //   .dynamic                    - Tags which let the linker locate .dynsym.
+//   .rodata                     - Oat metadata.
+//   .text                       - Compiled code.
+//   .bss                        - Zero-initialized writeable section.
+//   .dex                        - Reserved NOBITS space for dex-related data.
 //   .strtab                     - Names for .symtab.
 //   .symtab                     - Debug symbols.
 //   .debug_frame                - Unwind information (CFI).
@@ -57,9 +57,12 @@ namespace art {
 //
 // Some section are optional (the debug sections in particular).
 //
-// We try write the section data directly into the file without much
-// in-memory buffering.  This means we generally write sections based on the
-// dependency order (e.g. .dynamic points to .dynsym which points to .text).
+// To reduce the amount of padding necessary to page-align sections with
+// different permissions (and thus reduce disk usage), we group most read-only
+// data sections together at the start of the file. This includes .dynstr,
+// .dynsym, .hash, and .dynamic, whose contents are dependent on other sections.
+// Therefore, when building the ELF we initially just reserve space for them,
+// and write their contents later.
 //
 // In the cases where we need to buffer, we write the larger section first
 // and buffer the smaller one (e.g. .strtab is bigger than .symtab).
@@ -460,17 +463,17 @@ class ElfBuilder final {
   ElfBuilder(InstructionSet isa, OutputStream* output)
       : isa_(isa),
         stream_(output),
-        rodata_(this, ".rodata", SHT_PROGBITS, SHF_ALLOC, nullptr, 0, kElfSegmentAlignment, 0),
+        rodata_(this, ".rodata", SHT_PROGBITS, SHF_ALLOC, nullptr, 0, 4u, 0),
         text_(this, ".text", SHT_PROGBITS, SHF_ALLOC | SHF_EXECINSTR, nullptr, 0,
             kElfSegmentAlignment, 0),
         data_img_rel_ro_(this, ".data.img.rel.ro", SHT_PROGBITS, SHF_ALLOC, nullptr, 0,
             kElfSegmentAlignment, 0),
         bss_(this, ".bss", SHT_NOBITS, SHF_ALLOC, nullptr, 0, kElfSegmentAlignment, 0),
         dex_(this, ".dex", SHT_NOBITS, SHF_ALLOC, nullptr, 0, kElfSegmentAlignment, 0),
-        dynstr_(this, ".dynstr", SHF_ALLOC, kElfSegmentAlignment),
+        dynstr_(this, ".dynstr", SHF_ALLOC, 1),
         dynsym_(this, ".dynsym", SHT_DYNSYM, SHF_ALLOC, &dynstr_),
         hash_(this, ".hash", SHT_HASH, SHF_ALLOC, &dynsym_, 0, sizeof(Elf_Word), sizeof(Elf_Word)),
-        dynamic_(this, ".dynamic", SHT_DYNAMIC, SHF_ALLOC, &dynstr_, 0, kElfSegmentAlignment,
+        dynamic_(this, ".dynamic", SHT_DYNAMIC, SHF_ALLOC, &dynstr_, 0, sizeof(Elf_Addr),
             sizeof(Elf_Dyn)),
         strtab_(this, ".strtab", 0, 1),
         symtab_(this, ".symtab", SHT_SYMTAB, 0, &strtab_),
@@ -486,12 +489,14 @@ class ElfBuilder final {
         finished_(false),
         write_program_headers_(false),
         loaded_size_(0u),
-        virtual_address_(0) {
+        virtual_address_(0),
+        dynamic_sections_start_(0),
+        dynamic_sections_reserved_size_(0u) {
     text_.phdr_flags_ = PF_R | PF_X;
     data_img_rel_ro_.phdr_flags_ = PF_R | PF_W;  // Shall be made read-only at run time.
     bss_.phdr_flags_ = PF_R | PF_W;
     dex_.phdr_flags_ = PF_R;
-    dynamic_.phdr_flags_ = PF_R | PF_W;
+    dynamic_.phdr_flags_ = PF_R;
     dynamic_.phdr_type_ = PT_DYNAMIC;
     build_id_.phdr_type_ = PT_NOTE;
   }
@@ -631,6 +636,48 @@ class ElfBuilder final {
     return End();
   }
 
+  // Reserve space for: .dynstr, .dynsym, .hash and .dynamic.
+  //
+  // Dynamic section content is dependent on subsequent sections. Here, reserve enough
+  // space for it. We will write the content later (in PrepareDynamicSection).
+  void ReserveSpaceForDynamicSection(const std::string& elf_file_path) {
+    CHECK_EQ(dynamic_sections_start_, 0);
+    CHECK_EQ(dynamic_sections_reserved_size_, 0u);
+    CHECK(!rodata_.Exists());
+
+    off_t offset = stream_.Seek(0, kSeekCurrent);
+    dynamic_sections_start_ = offset;
+
+    dynstr_.AddSection();
+    // We don't expect that .dynstr section can have any alignment requirements.
+    DCHECK_EQ(dynstr_.header_.sh_addralign, 1u);
+    offset += []() consteval {
+      size_t size = 0;
+      for (size_t i = 0; i < kDynamicSymbolCount; i++) {
+        DynamicSymbol sym = static_cast<DynamicSymbol>(i);
+        size += GetDynamicSymbolName(sym).length() + 1;
+      }
+      return size;
+    }();
+    offset += GetSoname(elf_file_path).length() + 1;
+
+    dynsym_.AddSection();
+    offset = RoundUp(offset, dynsym_.header_.sh_addralign);
+    offset += kDynamicSymbolCount * sizeof(Elf_Sym);
+
+    hash_.AddSection();
+    offset = RoundUp(offset, hash_.header_.sh_addralign);
+    offset += PrepareDynamicSymbolHashtable(kDynamicSymbolCount, /*hashtable=*/ nullptr);
+
+    dynamic_.AddSection();
+    offset = RoundUp(offset, dynamic_.header_.sh_addralign);
+    offset += kDynamicEntriesCount * sizeof(Elf_Dyn);
+
+    dynamic_sections_reserved_size_ = offset - dynamic_sections_start_;
+
+    stream_.Seek(offset, kSeekSet);
+  }
+
   // The running program does not have access to section headers
   // and the loader is not supposed to use them either.
   // The dynamic sections therefore replicates some of the layout
@@ -646,13 +693,13 @@ class ElfBuilder final {
                              Elf_Word bss_methods_offset,
                              Elf_Word bss_roots_offset,
                              Elf_Word dex_size) {
-    std::string soname(elf_file_path);
-    size_t directory_separator_pos = soname.rfind('/');
-    if (directory_separator_pos != std::string::npos) {
-      soname = soname.substr(directory_separator_pos + 1);
-    }
+    CHECK_NE(dynamic_sections_reserved_size_, 0u);
+
+    // Skip over the reserved memory for dynamic sections - we prepare them later
+    // due to dependencies.
+    Elf_Addr dynamic_sections_address = virtual_address_;
+    virtual_address_ += dynamic_sections_reserved_size_;
 
-    // Allocate all pre-dynamic sections.
     rodata_.AllocateVirtualMemory(rodata_size);
     text_.AllocateVirtualMemory(text_size);
     if (data_img_rel_ro_size != 0) {
@@ -667,32 +714,33 @@ class ElfBuilder final {
 
     // Cache .dynstr, .dynsym and .hash data.
     dynstr_.Add("");  // dynstr should start with empty string.
-    Elf_Word oatdata = dynstr_.Add("oatdata");
+    Elf_Word oatdata = dynstr_.Add(GetDynamicSymbolName(DynamicSymbol::kOatData));
     dynsym_.Add(oatdata, &rodata_, rodata_.GetAddress(), rodata_size, STB_GLOBAL, STT_OBJECT);
     if (text_size != 0u) {
       // The runtime does not care about the size of this symbol (it uses the "lastword" symbol).
       // We use size 0 (meaning "unknown size" in ELF) to prevent overlap with the debug symbols.
-      Elf_Word oatexec = dynstr_.Add("oatexec");
+      Elf_Word oatexec = dynstr_.Add(GetDynamicSymbolName(DynamicSymbol::kOatExec));
       dynsym_.Add(oatexec, &text_, text_.GetAddress(), /* size= */ 0, STB_GLOBAL, STT_OBJECT);
-      Elf_Word oatlastword = dynstr_.Add("oatlastword");
+      Elf_Word oatlastword = dynstr_.Add(GetDynamicSymbolName(DynamicSymbol::kOatLastWord));
       Elf_Word oatlastword_address = text_.GetAddress() + text_size - 4;
       dynsym_.Add(oatlastword, &text_, oatlastword_address, 4, STB_GLOBAL, STT_OBJECT);
     } else if (rodata_size != 0) {
       // rodata_ can be size 0 for dwarf_test.
-      Elf_Word oatlastword = dynstr_.Add("oatlastword");
+      Elf_Word oatlastword = dynstr_.Add(GetDynamicSymbolName(DynamicSymbol::kOatLastWord));
       Elf_Word oatlastword_address = rodata_.GetAddress() + rodata_size - 4;
       dynsym_.Add(oatlastword, &rodata_, oatlastword_address, 4, STB_GLOBAL, STT_OBJECT);
     }
     DCHECK_LE(data_img_rel_ro_app_image_offset, data_img_rel_ro_size);
     if (data_img_rel_ro_size != 0u) {
-      Elf_Word oatdataimgrelro = dynstr_.Add("oatdataimgrelro");
+      Elf_Word oatdataimgrelro = dynstr_.Add(GetDynamicSymbolName(DynamicSymbol::kOatDataImgRelRo));
       dynsym_.Add(oatdataimgrelro,
                   &data_img_rel_ro_,
                   data_img_rel_ro_.GetAddress(),
                   data_img_rel_ro_size,
                   STB_GLOBAL,
                   STT_OBJECT);
-      Elf_Word oatdataimgrelrolastword = dynstr_.Add("oatdataimgrelrolastword");
+      Elf_Word oatdataimgrelrolastword =
+          dynstr_.Add(GetDynamicSymbolName(DynamicSymbol::kOatDataImgRelRoLastWord));
       dynsym_.Add(oatdataimgrelrolastword,
                   &data_img_rel_ro_,
                   data_img_rel_ro_.GetAddress() + data_img_rel_ro_size - 4,
@@ -700,7 +748,8 @@ class ElfBuilder final {
                   STB_GLOBAL,
                   STT_OBJECT);
       if (data_img_rel_ro_app_image_offset != data_img_rel_ro_size) {
-        Elf_Word oatdataimgrelroappimage = dynstr_.Add("oatdataimgrelroappimage");
+        Elf_Word oatdataimgrelroappimage =
+            dynstr_.Add(GetDynamicSymbolName(DynamicSymbol::kOatDataImgRelRoAppImage));
         dynsym_.Add(oatdataimgrelroappimage,
                     &data_img_rel_ro_,
                     data_img_rel_ro_.GetAddress() + data_img_rel_ro_app_image_offset,
@@ -711,7 +760,7 @@ class ElfBuilder final {
     }
     DCHECK_LE(bss_roots_offset, bss_size);
     if (bss_size != 0u) {
-      Elf_Word oatbss = dynstr_.Add("oatbss");
+      Elf_Word oatbss = dynstr_.Add(GetDynamicSymbolName(DynamicSymbol::kOatBss));
       dynsym_.Add(oatbss, &bss_, bss_.GetAddress(), bss_roots_offset, STB_GLOBAL, STT_OBJECT);
       DCHECK_LE(bss_methods_offset, bss_roots_offset);
       DCHECK_LE(bss_roots_offset, bss_size);
@@ -719,7 +768,7 @@ class ElfBuilder final {
       if (bss_methods_offset != bss_roots_offset) {
         Elf_Word bss_methods_address = bss_.GetAddress() + bss_methods_offset;
         Elf_Word bss_methods_size = bss_roots_offset - bss_methods_offset;
-        Elf_Word oatbssroots = dynstr_.Add("oatbssmethods");
+        Elf_Word oatbssroots = dynstr_.Add(GetDynamicSymbolName(DynamicSymbol::kOatBssMethods));
         dynsym_.Add(
             oatbssroots, &bss_, bss_methods_address, bss_methods_size, STB_GLOBAL, STT_OBJECT);
       }
@@ -727,41 +776,35 @@ class ElfBuilder final {
       if (bss_roots_offset != bss_size) {
         Elf_Word bss_roots_address = bss_.GetAddress() + bss_roots_offset;
         Elf_Word bss_roots_size = bss_size - bss_roots_offset;
-        Elf_Word oatbssroots = dynstr_.Add("oatbssroots");
+        Elf_Word oatbssroots = dynstr_.Add(GetDynamicSymbolName(DynamicSymbol::kOatBssRoots));
         dynsym_.Add(
             oatbssroots, &bss_, bss_roots_address, bss_roots_size, STB_GLOBAL, STT_OBJECT);
       }
-      Elf_Word oatbsslastword = dynstr_.Add("oatbsslastword");
+      Elf_Word oatbsslastword = dynstr_.Add(GetDynamicSymbolName(DynamicSymbol::kOatBssLastWord));
       Elf_Word bsslastword_address = bss_.GetAddress() + bss_size - 4;
       dynsym_.Add(oatbsslastword, &bss_, bsslastword_address, 4, STB_GLOBAL, STT_OBJECT);
     }
     if (dex_size != 0u) {
-      Elf_Word oatdex = dynstr_.Add("oatdex");
+      Elf_Word oatdex = dynstr_.Add(GetDynamicSymbolName(DynamicSymbol::kOatDex));
       dynsym_.Add(oatdex, &dex_, dex_.GetAddress(), /* size= */ 0, STB_GLOBAL, STT_OBJECT);
-      Elf_Word oatdexlastword = dynstr_.Add("oatdexlastword");
+      Elf_Word oatdexlastword = dynstr_.Add(GetDynamicSymbolName(DynamicSymbol::kOatDexLastWord));
       Elf_Word oatdexlastword_address = dex_.GetAddress() + dex_size - 4;
       dynsym_.Add(oatdexlastword, &dex_, oatdexlastword_address, 4, STB_GLOBAL, STT_OBJECT);
     }
 
-    Elf_Word soname_offset = dynstr_.Add(soname);
+    Elf_Word soname_offset = dynstr_.Add(GetSoname(elf_file_path));
 
     // We do not really need a hash-table since there is so few entries.
     // However, the hash-table is the only way the linker can actually
     // determine the number of symbols in .dynsym so it is required.
     int count = dynsym_.GetCacheSize() / sizeof(Elf_Sym);  // Includes NULL.
     std::vector<Elf_Word> hash;
-    hash.push_back(1);  // Number of buckets.
-    hash.push_back(count);  // Number of chains.
-    // Buckets.  Having just one makes it linear search.
-    hash.push_back(1);  // Point to first non-NULL symbol.
-    // Chains.  This creates linked list of symbols.
-    hash.push_back(0);  // Placeholder entry for the NULL symbol.
-    for (int i = 1; i < count - 1; i++) {
-      hash.push_back(i + 1);  // Each symbol points to the next one.
-    }
-    hash.push_back(0);  // Last symbol terminates the chain.
+    PrepareDynamicSymbolHashtable(count, &hash);
     hash_.Add(hash.data(), hash.size() * sizeof(hash[0]));
 
+    Elf_Addr current_virtual_address = virtual_address_;
+    virtual_address_ = dynamic_sections_address;
+
     // Allocate all remaining sections.
     dynstr_.AllocateVirtualMemory(dynstr_.GetCacheSize());
     dynsym_.AllocateVirtualMemory(dynsym_.GetCacheSize());
@@ -776,17 +819,32 @@ class ElfBuilder final {
       { .d_tag = DT_SONAME, .d_un = { .d_ptr = soname_offset }, },
       { .d_tag = DT_NULL,   .d_un = { .d_ptr = 0 }, },
     };
+    static_assert(sizeof(dyns) == kDynamicEntriesCount * sizeof(dyns[0]));
+
     dynamic_.Add(&dyns, sizeof(dyns));
     dynamic_.AllocateVirtualMemory(dynamic_.GetCacheSize());
 
+    CHECK_LE(virtual_address_, rodata_.GetAddress());
+    virtual_address_ = current_virtual_address;
+
     loaded_size_ = RoundUp(virtual_address_, kElfSegmentAlignment);
   }
 
   void WriteDynamicSection() {
+    CHECK_NE(dynamic_sections_start_, 0);
+    CHECK_NE(dynamic_sections_reserved_size_, 0u);
+
+    off_t current_offset = stream_.Seek(0, kSeekCurrent);
+    stream_.Seek(dynamic_sections_start_, kSeekSet);
+
     dynstr_.WriteCachedSection();
     dynsym_.WriteCachedSection();
     hash_.WriteCachedSection();
     dynamic_.WriteCachedSection();
+
+    DCHECK_LE(stream_.Seek(0, kSeekCurrent),
+        static_cast<off_t>(dynamic_sections_start_ + dynamic_sections_reserved_size_));
+    stream_.Seek(current_offset, kSeekSet);
   }
 
   Elf_Word GetLoadedSize() {
@@ -977,6 +1035,91 @@ class ElfBuilder final {
     return phdrs;
   }
 
+  enum class DynamicSymbol {
+    kNull,
+    kOatData,
+    kOatExec,
+    kOatLastWord,
+    kOatDataImgRelRo,
+    kOatDataImgRelRoLastWord,
+    kOatDataImgRelRoAppImage,
+    kOatBss,
+    kOatBssMethods,
+    kOatBssRoots,
+    kOatBssLastWord,
+    kOatDex,
+    kOatDexLastWord,
+    kLast = kOatDexLastWord
+  };
+
+  static constexpr size_t kDynamicSymbolCount = static_cast<size_t>(DynamicSymbol::kLast) + 1;
+  static constexpr size_t kDynamicEntriesCount = 7;
+
+  static constexpr std::string GetDynamicSymbolName(DynamicSymbol sym) {
+    switch (sym) {
+      case DynamicSymbol::kNull:
+        return "";
+      case DynamicSymbol::kOatData:
+        return "oatdata";
+      case DynamicSymbol::kOatExec:
+        return "oatexec";
+      case DynamicSymbol::kOatLastWord:
+        return "oatlastword";
+      case DynamicSymbol::kOatDataImgRelRo:
+        return "oatdataimgrelro";
+      case DynamicSymbol::kOatDataImgRelRoLastWord:
+        return "oatdataimgrelrolastword";
+      case DynamicSymbol::kOatDataImgRelRoAppImage:
+        return "oatdataimgrelroappimage";
+      case DynamicSymbol::kOatBss:
+        return "oatbss";
+      case DynamicSymbol::kOatBssMethods:
+        return "oatbssmethods";
+      case DynamicSymbol::kOatBssRoots:
+        return "oatbssroots";
+      case DynamicSymbol::kOatBssLastWord:
+        return "oatbsslastword";
+      case DynamicSymbol::kOatDex:
+        return "oatdex";
+      case DynamicSymbol::kOatDexLastWord:
+        return "oatdexlastword";
+    }
+  }
+
+  // This method builds a hashtable for dynamic symbols using `hashtable` as a storage.
+  // If `hashtable` is nullptr, it just calculate its size in bytes and returns it.
+  static size_t PrepareDynamicSymbolHashtable(size_t count, std::vector<Elf_Word> *hashtable) {
+    size_t size = 0;
+    auto write = [&size, hashtable](Elf_Word value) {
+      if (hashtable) {
+        hashtable->push_back(value);
+      }
+      size += sizeof(value);
+    };
+
+    write(1);  // Number of buckets.
+    write(count);  // Number of chains.
+    // Buckets.  Having just one makes it linear search.
+    write(1);  // Point to first non-NULL symbol.
+    // Chains.  This creates linked list of symbols.
+    write(0);  // Placeholder entry for the NULL symbol.
+    for (size_t i = 1; i < count - 1; i++) {
+      write(i + 1);  // Each symbol points to the next one.
+    }
+    write(0);  // Last symbol terminates the chain.
+
+    return size;
+  }
+
+  static std::string GetSoname(const std::string& elf_file_path) {
+    std::string soname(elf_file_path);
+    size_t directory_separator_pos = soname.rfind('/');
+    if (directory_separator_pos != std::string::npos) {
+      soname = soname.substr(directory_separator_pos + 1);
+    }
+    return soname;
+  }
+
   InstructionSet isa_;
 
   ErrorDelayingOutputStream stream_;
@@ -1014,6 +1157,12 @@ class ElfBuilder final {
   // Used for allocation of virtual address space.
   Elf_Addr virtual_address_;
 
+  // Offset in the ELF where the first dynamic section is written (.dynstr).
+  off_t dynamic_sections_start_;
+
+  // Size reserved for dynamic sections: .dynstr, .dynsym, .hash and .dynamic.
+  size_t dynamic_sections_reserved_size_;
+
   DISALLOW_COPY_AND_ASSIGN(ElfBuilder);
 };
 
diff --git a/libelffile/stream/file_output_stream.cc b/libelffile/stream/file_output_stream.cc
index bbfbdfdca8..5afe366636 100644
--- a/libelffile/stream/file_output_stream.cc
+++ b/libelffile/stream/file_output_stream.cc
@@ -16,6 +16,8 @@
 
 #include "file_output_stream.h"
 
+#include <android-base/logging.h>
+#include <sys/stat.h>
 #include <sys/types.h>
 #include <unistd.h>
 
@@ -30,6 +32,54 @@ bool FileOutputStream::WriteFully(const void* buffer, size_t byte_count) {
 }
 
 off_t FileOutputStream::Seek(off_t offset, Whence whence) {
+  static const bool allow_sparse_files = unix_file::AllowSparseFiles();
+  // If we are not allowed to generate sparse files, write zeros instead.
+  if (UNLIKELY(!allow_sparse_files)) {
+    // Check the current file size.
+    int fd = file_->Fd();
+    struct stat sb;
+    if (fstat(fd, &sb) == -1) {
+      return -1;
+    }
+    off_t file_size = sb.st_size;
+    // Calculate new desired offset.
+    switch (whence) {
+      case kSeekSet:
+        break;
+      case kSeekCurrent: {
+        off_t curr_offset = lseek(fd, 0, SEEK_CUR);
+        if (curr_offset == -1) {
+          return -1;
+        }
+        offset += curr_offset;
+        whence = kSeekSet;
+        break;
+      }
+      case kSeekEnd:
+        offset += file_size;
+        whence = kSeekSet;
+        break;
+      default:
+        LOG(FATAL) << "Unsupported seek type: " << whence;
+        UNREACHABLE();
+    }
+    // Write zeros if we are extending the file.
+    if (offset > file_size) {
+      off_t curr_offset = lseek(fd, 0, SEEK_END);
+      if (curr_offset == -1) {
+        return -1;
+      }
+      static const std::array<uint8_t, 1024> buffer{};
+      while (curr_offset < offset) {
+        size_t size = std::min<size_t>(offset - curr_offset, buffer.size());
+        ssize_t bytes_written = write(fd, buffer.data(), size);
+        if (bytes_written < 0) {
+          return -1;
+        }
+        curr_offset += bytes_written;
+      }
+    }
+  }
   return lseek(file_->Fd(), offset, static_cast<int>(whence));
 }
 
diff --git a/libnativebridge/include/nativebridge/native_bridge.h b/libnativebridge/include/nativebridge/native_bridge.h
index 491521642c..db3d1e9509 100644
--- a/libnativebridge/include/nativebridge/native_bridge.h
+++ b/libnativebridge/include/nativebridge/native_bridge.h
@@ -100,6 +100,8 @@ void* NativeBridgeGetTrampolineForFunctionPointer(const void* method,
                                                   uint32_t len,
                                                   enum JNICallType jni_call_type);
 
+bool NativeBridgeIsNativeBridgeFunctionPointer(const void* method);
+
 // True if native library paths are valid and is for an ABI that is supported by native bridge.
 // The *libpath* must point to a library.
 //
@@ -403,9 +405,10 @@ struct NativeBridgeCallbacks {
   // Get a native bridge trampoline for specified native method implementation pointer.
   //
   // Parameters:
-  //   method [IN] pointer to method implementation (ususally registered via call to
+  //   method [IN] pointer to method implementation (usually registered via call to
   //   RegisterNatives).
-  //   shorty [IN] short descriptor of native method len [IN] length of shorty
+  //   shorty [IN] short descriptor of native method
+  //   len [IN] length of shorty
   //   jni_call_type [IN] the type of JNI call
   // Returns:
   //   address of trampoline if successful, otherwise NULL
@@ -413,6 +416,18 @@ struct NativeBridgeCallbacks {
                                            const char* shorty,
                                            uint32_t len,
                                            enum JNICallType jni_call_type);
+
+  // Check if the method pointer belongs to native_bridge address space.
+  //
+  // Parameters:
+  //   method [IN] pointer to a method implementation.
+  //
+  // Returns:
+  //   true if the method is in native bridge implementation executable address
+  //   space or in other words needs a trampoline to be able to run with native bridge.
+  //
+  // Introduced in: version 8
+  bool (*isNativeBridgeFunctionPointer)(const void* method);
 };
 
 // Runtime interfaces to native bridge.
diff --git a/libnativebridge/native_bridge.cc b/libnativebridge/native_bridge.cc
index 4461f79919..af85d4ee35 100644
--- a/libnativebridge/native_bridge.cc
+++ b/libnativebridge/native_bridge.cc
@@ -133,6 +133,8 @@ enum NativeBridgeImplementationVersion {
   PRE_ZYGOTE_FORK_VERSION = 6,
   // The version with critical_native support
   CRITICAL_NATIVE_SUPPORT_VERSION = 7,
+  // The version with native bridge detection fallback for function pointers
+  IDENTIFY_NATIVELY_BRIDGED_FUNCTION_POINTERS_VERSION = 8,
 };
 
 // Whether we had an error at some point.
@@ -732,6 +734,18 @@ void* NativeBridgeLoadLibraryExt(const char* libpath, int flag, native_bridge_na
   return nullptr;
 }
 
+bool NativeBridgeIsNativeBridgeFunctionPointer(const void* method) {
+  if (NativeBridgeInitialized()) {
+    if (isCompatibleWith(IDENTIFY_NATIVELY_BRIDGED_FUNCTION_POINTERS_VERSION)) {
+      return callbacks->isNativeBridgeFunctionPointer(method);
+    } else {
+      ALOGW("not compatible with version %d, unable to call isNativeBridgeFunctionPointer",
+            IDENTIFY_NATIVELY_BRIDGED_FUNCTION_POINTERS_VERSION);
+    }
+  }
+  return false;
+}
+
 }  // extern "C"
 
 }  // namespace android
diff --git a/libnativebridge/tests/Android.bp b/libnativebridge/tests/Android.bp
index 93ef6feded..add18d12e3 100644
--- a/libnativebridge/tests/Android.bp
+++ b/libnativebridge/tests/Android.bp
@@ -85,6 +85,15 @@ cc_test_library {
     ],
 }
 
+cc_test_library {
+    name: "libnativebridge8-test-case",
+    srcs: ["NativeBridgeTestCase8.cpp"],
+    defaults: ["libnativebridge-test-case-defaults"],
+    shared_libs: [
+        "libnativebridge8IdentifyTrampolines",
+    ],
+}
+
 // A helper library to produce test-case side effect of PreZygoteForkNativeBridge.
 cc_test_library {
     name: "libnativebridge6prezygotefork",
@@ -98,6 +107,12 @@ cc_test_library {
     defaults: ["libnativebridge-test-case-defaults"],
 }
 
+cc_test_library {
+    name: "libnativebridge8IdentifyTrampolines",
+    srcs: ["NativeBridge8IdentifyTrampolines_lib.cpp"],
+    defaults: ["libnativebridge-test-case-defaults"],
+}
+
 cc_test {
     name: "libnativebridge-tests",
     defaults: [
@@ -130,6 +145,7 @@ cc_test {
         "NativeBridge3LoadLibraryExt_test.cpp",
         "NativeBridge6PreZygoteFork_test.cpp",
         "NativeBridge7CriticalNative_test.cpp",
+        "NativeBridge8IdentifyTrampolines_test.cpp",
     ],
 
     static_libs: [
@@ -140,10 +156,12 @@ cc_test {
         "liblog",
         "libnativebridge6prezygotefork",
         "libnativebridge7criticalnative",
+        "libnativebridge8IdentifyTrampolines",
     ],
     data_libs: [
         "libnativebridge6prezygotefork",
         "libnativebridge7criticalnative",
+        "libnativebridge8IdentifyTrampolines",
 
         // These are dlopen'd by libnativebridge, not libnativebridge-tests, but
         // the former is statically linked into the latter, so the linker will
@@ -153,6 +171,7 @@ cc_test {
         "libnativebridge3-test-case",
         "libnativebridge6-test-case",
         "libnativebridge7-test-case",
+        "libnativebridge8-test-case",
     ],
 
     target: {
diff --git a/libnativebridge/tests/NativeBridge8IdentifyTrampolines_lib.cpp b/libnativebridge/tests/NativeBridge8IdentifyTrampolines_lib.cpp
new file mode 100644
index 0000000000..0c4ccf03b5
--- /dev/null
+++ b/libnativebridge/tests/NativeBridge8IdentifyTrampolines_lib.cpp
@@ -0,0 +1,31 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "NativeBridge8IdentifyTrampolines_lib.h"
+
+namespace android {
+
+static const void* g_is_native_bridge_function_pointer_called_for = nullptr;
+
+void SetIsNativeBridgeFunctionPointerCalledFor(const void* ptr) {
+  g_is_native_bridge_function_pointer_called_for = ptr;
+}
+
+bool IsNativeBridgeFunctionPointerCalledFor(const void* ptr) {
+  return g_is_native_bridge_function_pointer_called_for == ptr;
+}
+
+}  // namespace android
diff --git a/libnativebridge/tests/NativeBridge8IdentifyTrampolines_lib.h b/libnativebridge/tests/NativeBridge8IdentifyTrampolines_lib.h
new file mode 100644
index 0000000000..7121e397a4
--- /dev/null
+++ b/libnativebridge/tests/NativeBridge8IdentifyTrampolines_lib.h
@@ -0,0 +1,27 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_LIBNATIVEBRIDGE_TESTS_NATIVEBRIDGE8IDENTIFYTRAMPOLINES_LIB_H_
+#define ART_LIBNATIVEBRIDGE_TESTS_NATIVEBRIDGE8IDENTIFYTRAMPOLINES_LIB_H_
+
+namespace android {
+
+void SetIsNativeBridgeFunctionPointerCalledFor(const void* ptr);
+bool IsNativeBridgeFunctionPointerCalledFor(const void* ptr);
+
+}  // namespace android
+
+#endif  // ART_LIBNATIVEBRIDGE_TESTS_NATIVEBRIDGE8IDENTIFYTRAMPOLINES_LIB_H_
diff --git a/libnativebridge/tests/NativeBridge8IdentifyTrampolines_test.cpp b/libnativebridge/tests/NativeBridge8IdentifyTrampolines_test.cpp
new file mode 100644
index 0000000000..362df54fa1
--- /dev/null
+++ b/libnativebridge/tests/NativeBridge8IdentifyTrampolines_test.cpp
@@ -0,0 +1,39 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "NativeBridge8IdentifyTrampolines_lib.h"
+#include "NativeBridgeTest.h"
+
+namespace android {
+
+TEST_F(NativeBridgeTest, V8_IdentifyTrampolines) {
+  // Init
+  ASSERT_TRUE(LoadNativeBridge(kNativeBridgeLibrary8, nullptr));
+  ASSERT_TRUE(NativeBridgeAvailable());
+  ASSERT_TRUE(PreInitializeNativeBridge(AppDataDir(), "isa"));
+  ASSERT_TRUE(NativeBridgeAvailable());
+  ASSERT_TRUE(InitializeNativeBridge(nullptr, nullptr));
+  ASSERT_TRUE(NativeBridgeAvailable());
+
+  ASSERT_EQ(NativeBridgeGetVersion(), 8U);
+
+  const void* ptr = reinterpret_cast<void*>(NativeBridgeGetVersion);
+
+  UNUSED(NativeBridgeIsNativeBridgeFunctionPointer(ptr));
+  ASSERT_TRUE(IsNativeBridgeFunctionPointerCalledFor(ptr));
+}
+
+}  // namespace android
diff --git a/libnativebridge/tests/NativeBridgeTest.h b/libnativebridge/tests/NativeBridgeTest.h
index 509973d756..1b9415db58 100644
--- a/libnativebridge/tests/NativeBridgeTest.h
+++ b/libnativebridge/tests/NativeBridgeTest.h
@@ -30,10 +30,11 @@ constexpr const char* kNativeBridgeLibrary2 = "libnativebridge2-test-case.so";
 constexpr const char* kNativeBridgeLibrary3 = "libnativebridge3-test-case.so";
 constexpr const char* kNativeBridgeLibrary6 = "libnativebridge6-test-case.so";
 constexpr const char* kNativeBridgeLibrary7 = "libnativebridge7-test-case.so";
+constexpr const char* kNativeBridgeLibrary8 = "libnativebridge8-test-case.so";
 
 namespace android {
 
-class NativeBridgeTest : public testing::Test {
+class NativeBridgeTest : public ::testing::Test {
  protected:
   NativeBridgeTest() : temp_dir_() {
     app_data_dir_ = std::string(temp_dir_.path);
diff --git a/libnativebridge/tests/NativeBridgeTestCase8.cpp b/libnativebridge/tests/NativeBridgeTestCase8.cpp
new file mode 100644
index 0000000000..c6395f526b
--- /dev/null
+++ b/libnativebridge/tests/NativeBridgeTestCase8.cpp
@@ -0,0 +1,147 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// An implementation of the native-bridge interface for testing.
+
+#include "NativeBridge8IdentifyTrampolines_lib.h"
+#include "nativebridge/native_bridge.h"
+
+// NativeBridgeCallbacks implementations
+extern "C" bool native_bridge8_initialize(
+    const android::NativeBridgeRuntimeCallbacks* /* art_cbs */,
+    const char* /* app_code_cache_dir */,
+    const char* /* isa */) {
+  return true;
+}
+
+extern "C" void* native_bridge8_loadLibrary(const char* /* libpath */, int /* flag */) {
+  return nullptr;
+}
+
+extern "C" void* native_bridge8_getTrampoline(void* /* handle */,
+                                              const char* /* name */,
+                                              const char* /* shorty */,
+                                              uint32_t /* len */) {
+  return nullptr;
+}
+
+extern "C" void* native_bridge8_getTrampoline2(void* /* handle */,
+                                               const char* /* name */,
+                                               const char* /* shorty */,
+                                               uint32_t /* len */,
+                                               android::JNICallType /* jni_call_type */) {
+  return nullptr;
+}
+
+extern "C" void* native_bridge8_getTrampolineForFunctionPointer(
+    const void* /* method */,
+    const char* /* shorty */,
+    uint32_t /* len */,
+    android::JNICallType /* jni_call_type */) {
+  return nullptr;
+}
+
+extern "C" bool native_bridge8_isSupported(const char* /* libpath */) { return false; }
+
+extern "C" const struct android::NativeBridgeRuntimeValues* native_bridge8_getAppEnv(
+    const char* /* abi */) {
+  return nullptr;
+}
+
+extern "C" bool native_bridge8_isCompatibleWith(uint32_t version) {
+  // For testing, allow 1-8, but disallow 9+.
+  return version <= 8;
+}
+
+extern "C" android::NativeBridgeSignalHandlerFn native_bridge8_getSignalHandler(int /* signal */) {
+  return nullptr;
+}
+
+extern "C" int native_bridge8_unloadLibrary(void* /* handle */) { return 0; }
+
+extern "C" const char* native_bridge8_getError() { return nullptr; }
+
+extern "C" bool native_bridge8_isPathSupported(const char* /* path */) { return true; }
+
+extern "C" android::native_bridge_namespace_t* native_bridge8_createNamespace(
+    const char* /* name */,
+    const char* /* ld_library_path */,
+    const char* /* default_library_path */,
+    uint64_t /* type */,
+    const char* /* permitted_when_isolated_path */,
+    android::native_bridge_namespace_t* /* parent_ns */) {
+  return nullptr;
+}
+
+extern "C" bool native_bridge8_linkNamespaces(android::native_bridge_namespace_t* /* from */,
+                                              android::native_bridge_namespace_t* /* to */,
+                                              const char* /* shared_libs_soname */) {
+  return true;
+}
+
+extern "C" void* native_bridge8_loadLibraryExt(const char* /* libpath */,
+                                               int /* flag */,
+                                               android::native_bridge_namespace_t* /* ns */) {
+  return nullptr;
+}
+
+extern "C" android::native_bridge_namespace_t* native_bridge8_getVendorNamespace() {
+  return nullptr;
+}
+
+extern "C" android::native_bridge_namespace_t* native_bridge8_getExportedNamespace(
+    const char* /* name */) {
+  return nullptr;
+}
+
+extern "C" bool native_bridge8_isNativeBridgeFunctionPointer(const void* ptr) {
+  android::SetIsNativeBridgeFunctionPointerCalledFor(ptr);
+  return true;
+}
+
+extern "C" void native_bridge8_preZygoteFork() {}
+
+android::NativeBridgeCallbacks NativeBridgeItf{
+    // v1
+    .version = 8,
+    .initialize = &native_bridge8_initialize,
+    .loadLibrary = &native_bridge8_loadLibrary,
+    .getTrampoline = &native_bridge8_getTrampoline,
+    .isSupported = &native_bridge8_isSupported,
+    .getAppEnv = &native_bridge8_getAppEnv,
+    // v2
+    .isCompatibleWith = &native_bridge8_isCompatibleWith,
+    .getSignalHandler = &native_bridge8_getSignalHandler,
+    // v3
+    .unloadLibrary = &native_bridge8_unloadLibrary,
+    .getError = &native_bridge8_getError,
+    .isPathSupported = &native_bridge8_isPathSupported,
+    .unused_initAnonymousNamespace = nullptr,
+    .createNamespace = &native_bridge8_createNamespace,
+    .linkNamespaces = &native_bridge8_linkNamespaces,
+    .loadLibraryExt = &native_bridge8_loadLibraryExt,
+    // v4
+    &native_bridge8_getVendorNamespace,
+    // v5
+    &native_bridge8_getExportedNamespace,
+    // v6
+    &native_bridge8_preZygoteFork,
+    // v7
+    &native_bridge8_getTrampoline2,
+    &native_bridge8_getTrampolineForFunctionPointer,
+    // v8
+    &native_bridge8_isNativeBridgeFunctionPointer,
+};
diff --git a/libnativeloader/native_loader.cpp b/libnativeloader/native_loader.cpp
index f37f661a3e..964279446f 100644
--- a/libnativeloader/native_loader.cpp
+++ b/libnativeloader/native_loader.cpp
@@ -70,6 +70,11 @@ using ::android::nativeloader::LibraryNamespaces;
 // the APEX. In that case the default namespace links to the ART namespace
 // (com_android_art) for all libraries, which means this can be used to load
 // test libraries that depend on ART internal libraries.
+//
+// There's also code in art/dalvikvm.cc to add links from com_android_art back
+// to the default namespace for NATIVELOADER_DEFAULT_NAMESPACE_LIBS, enabling
+// access in the opposite direction as well. Useful e.g. to load ART plugins in
+// NATIVELOADER_DEFAULT_NAMESPACE_LIBS.
 constexpr const char* kNativeloaderExtraLibs = "nativeloader-extra-libs";
 
 std::mutex g_namespaces_mutex;
diff --git a/libnativeloader/native_loader_api_test.cpp b/libnativeloader/native_loader_api_test.cpp
index aeda1cc2a7..6ef7d36134 100644
--- a/libnativeloader/native_loader_api_test.cpp
+++ b/libnativeloader/native_loader_api_test.cpp
@@ -38,7 +38,7 @@ using ::testing::StrEq;
 class NativeLoaderLazyTest : public ::testing::Test {
  protected:
   void SetUp() override {
-    jni_mock = std::make_unique<testing::NiceMock<MockJni>>();
+    jni_mock = std::make_unique<::testing::NiceMock<MockJni>>();
     env = std::make_unique<JNIEnv>();
     env->functions = CreateJNINativeInterface();
   }
diff --git a/libnativeloader/native_loader_test.cpp b/libnativeloader/native_loader_test.cpp
index a07551032c..90810647c3 100644
--- a/libnativeloader/native_loader_test.cpp
+++ b/libnativeloader/native_loader_test.cpp
@@ -131,7 +131,7 @@ class MockPlatform : public Platform {
     ON_CALL(*this, NativeBridgeIsSupported(_)).WillByDefault(Return(is_bridged_));
     ON_CALL(*this, NativeBridgeIsPathSupported(_)).WillByDefault(Return(is_bridged_));
     ON_CALL(*this, mock_get_exported_namespace(_, _))
-        .WillByDefault(testing::Invoke([](bool, const char* name) -> mock_namespace_handle {
+        .WillByDefault(::testing::Invoke([](bool, const char* name) -> mock_namespace_handle {
           if (namespaces.find(name) != namespaces.end()) {
             return namespaces[name];
           }
@@ -274,8 +274,8 @@ class NativeLoaderTest : public ::testing::TestWithParam<bool> {
   bool IsBridged() { return GetParam(); }
 
   void SetUp() override {
-    mock = std::make_unique<testing::NiceMock<MockPlatform>>(IsBridged());
-    jni_mock = std::make_unique<testing::NiceMock<MockJni>>();
+    mock = std::make_unique<::testing::NiceMock<MockPlatform>>(IsBridged());
+    jni_mock = std::make_unique<::testing::NiceMock<MockJni>>();
 
     env = std::make_unique<JNIEnv>();
     env->functions = CreateJNINativeInterface();
@@ -379,7 +379,7 @@ TEST_P(NativeLoaderTest, OpenNativeLibraryWithoutClassloaderAndCallerLocation) {
   EXPECT_EQ(errmsg, nullptr);
 }
 
-INSTANTIATE_TEST_SUITE_P(NativeLoaderTests, NativeLoaderTest, testing::Bool());
+INSTANTIATE_TEST_SUITE_P(NativeLoaderTests, NativeLoaderTest, ::testing::Bool());
 
 /////////////////////////////////////////////////////////////////
 
@@ -439,8 +439,8 @@ class NativeLoaderTest_Create : public NativeLoaderTest {
 
     ON_CALL(*jni_mock, JniObject_getParent(StrEq(class_loader))).WillByDefault(Return(nullptr));
 
-    EXPECT_CALL(*mock, NativeBridgeIsPathSupported(_)).Times(testing::AnyNumber());
-    EXPECT_CALL(*mock, NativeBridgeInitialized()).Times(testing::AnyNumber());
+    EXPECT_CALL(*mock, NativeBridgeIsPathSupported(_)).Times(::testing::AnyNumber());
+    EXPECT_CALL(*mock, NativeBridgeInitialized()).Times(::testing::AnyNumber());
 
     EXPECT_CALL(*mock, mock_create_namespace(
                            Eq(IsBridged()), StartsWith(expected_namespace_prefix + "-"), nullptr,
@@ -684,7 +684,7 @@ TEST_P(NativeLoaderTest_Create, TwoApks) {
   }
 }
 
-INSTANTIATE_TEST_SUITE_P(NativeLoaderTests_Create, NativeLoaderTest_Create, testing::Bool());
+INSTANTIATE_TEST_SUITE_P(NativeLoaderTests_Create, NativeLoaderTest_Create, ::testing::Bool());
 
 const std::function<Result<bool>(const struct ConfigEntry&)> always_true =
     [](const struct ConfigEntry&) -> Result<bool> { return true; };
diff --git a/libnativeloader/test/src/android/test/hostside/LibnativeloaderTest.java b/libnativeloader/test/src/android/test/hostside/LibnativeloaderTest.java
index e7207aedd7..371c458b3d 100644
--- a/libnativeloader/test/src/android/test/hostside/LibnativeloaderTest.java
+++ b/libnativeloader/test/src/android/test/hostside/LibnativeloaderTest.java
@@ -116,8 +116,10 @@ public class LibnativeloaderTest extends BaseHostJUnit4Test {
         ctx.mDevice.uninstallPackage("android.test.app.data");
 
         String cleanupPathList = testInfo.properties().get(CLEANUP_PATHS_KEY);
-        CleanupPaths cleanup = new CleanupPaths(ctx.mDevice, cleanupPathList);
-        cleanup.cleanup();
+        if (cleanupPathList != null) {
+            CleanupPaths cleanup = new CleanupPaths(ctx.mDevice, cleanupPathList);
+            cleanup.cleanup();
+        }
     }
 
     @Test
diff --git a/oatdump/oatdump.cc b/oatdump/oatdump.cc
index feba70ff3a..f7320765a7 100644
--- a/oatdump/oatdump.cc
+++ b/oatdump/oatdump.cc
@@ -157,6 +157,7 @@ class OatSymbolizer final {
     builder_.reset(new ElfBuilder<ElfTypes>(isa, output_stream.get()));
 
     builder_->Start();
+    builder_->ReserveSpaceForDynamicSection(elf_file->GetPath());
 
     auto* rodata = builder_->GetRoData();
     auto* text = builder_->GetText();
@@ -489,12 +490,11 @@ class OatDumper {
     // Print the key-value store.
     {
       os << "KEY VALUE STORE:\n";
-      size_t index = 0;
+      uint32_t offset = 0;
       const char* key;
       const char* value;
-      while (oat_header.GetStoreKeyValuePairByIndex(index, &key, &value)) {
+      while (oat_header.GetNextStoreKeyValuePair(&offset, &key, &value)) {
         os << key << " = " << value << "\n";
-        index++;
       }
       os << "\n";
     }
@@ -699,7 +699,7 @@ class OatDumper {
         return false;
       }
       for (ClassAccessor accessor : dex_file->GetClasses()) {
-        const char* descriptor = accessor.GetDescriptor();
+        std::string_view descriptor = accessor.GetDescriptorView();
         if (DescriptorToDot(descriptor).find(options_.class_filter_) == std::string::npos) {
           continue;
         }
@@ -933,7 +933,7 @@ class OatDumper {
     ScopedIndentation indent1(&vios);
     for (ClassAccessor accessor : dex_file->GetClasses()) {
       // TODO: Support regex
-      const char* descriptor = accessor.GetDescriptor();
+      std::string_view descriptor = accessor.GetDescriptorView();
       if (DescriptorToDot(descriptor).find(options_.class_filter_) == std::string::npos) {
         continue;
       }
@@ -941,12 +941,10 @@ class OatDumper {
       const uint16_t class_def_index = accessor.GetClassDefIndex();
       uint32_t oat_class_offset = oat_dex_file.GetOatClassOffset(class_def_index);
       const OatFile::OatClass oat_class = oat_dex_file.GetOatClass(class_def_index);
-      os << StringPrintf("%zd: %s (offset=0x%08zx) (type_idx=%d)",
-                         static_cast<ssize_t>(class_def_index),
-                         descriptor,
-                         AdjustOffset(oat_class_offset),
-                         accessor.GetClassIdx().index_)
-         << " (" << oat_class.GetStatus() << ")" << " (" << oat_class.GetType() << ")\n";
+      os << static_cast<ssize_t>(class_def_index) << ": " << descriptor << " (offset=0x"
+         << StringPrintf("%08zx", AdjustOffset(oat_class_offset))
+         << ") (type_idx=" << accessor.GetClassIdx().index_
+         << ") (" << oat_class.GetStatus() << ")" << " (" << oat_class.GetType() << ")\n";
       // TODO: include bitmap here if type is kOatClassSomeCompiled?
       if (options_.list_classes_) {
         continue;
@@ -2075,8 +2073,10 @@ class ImageDumper {
     if (super != nullptr) {
       DumpFields(os, obj, super);
     }
-    for (ArtField& field : klass->GetIFields()) {
-      PrintField(os, &field, obj);
+    for (ArtField& field : klass->GetFields()) {
+      if (!field.IsStatic()) {
+        PrintField(os, &field, obj);
+      }
     }
   }
 
@@ -2190,11 +2190,13 @@ class ImageDumper {
         }
       }
 
-      if (klass->NumStaticFields() != 0) {
+      if (klass->HasStaticFields()) {
         os << "STATICS:\n";
         ScopedIndentation indent2(&vios_);
-        for (ArtField& field : klass->GetSFields()) {
-          PrintField(os, &field, field.GetDeclaringClass());
+        for (ArtField& field : klass->GetFields()) {
+          if (field.IsStatic()) {
+            PrintField(os, &field, field.GetDeclaringClass());
+          }
         }
       }
     }
@@ -2927,7 +2929,7 @@ class IMTDumper {
     if (class_name[0] == 'L') {
       descriptor = class_name;
     } else {
-      descriptor = DotToDescriptor(class_name.c_str());
+      descriptor = DotToDescriptor(class_name);
     }
 
     ObjPtr<mirror::Class> klass = runtime->GetClassLinker()->FindClass(
@@ -3001,10 +3003,18 @@ class IMTDumper {
 
       for (ArtMethod& iface_method : iface->GetVirtualMethods(pointer_size)) {
         uint32_t class_hash, name_hash, signature_hash;
-        ImTable::GetImtHashComponents(&iface_method, &class_hash, &name_hash, &signature_hash);
+        ImTable::GetImtHashComponents(*iface_method.GetDexFile(),
+                                      iface_method.GetDexMethodIndex(),
+                                      &class_hash,
+                                      &name_hash,
+                                      &signature_hash);
         uint32_t imt_slot = ImTable::GetImtIndex(&iface_method);
+        // Note: For default methods we use the dex method index for calculating the slot.
+        // For abstract methods the compile-time constant `kImTableHashUseName` determines
+        // whether we use the component hashes (current behavior) or the dex method index.
         std::cerr << "    " << iface_method.PrettyMethod(true)
             << " slot=" << imt_slot
+            << " dex_method_index=" << iface_method.GetDexMethodIndex()
             << std::hex
             << " class_hash=0x" << class_hash
             << " name_hash=0x" << name_hash
@@ -3542,8 +3552,7 @@ struct OatdumpMain : public CmdlineMain<OatdumpArgs> {
                                         /*only_load_trusted_executable=*/false,
                                         ofa_context.get());
 
-    if (!oat_file_assistant.ValidateBootClassPathChecksums(*oat_file)) {
-      *error_msg = "BCP checksum check failed";
+    if (!oat_file_assistant.ValidateBootClassPathChecksums(*oat_file, error_msg)) {
       return false;
     }
 
diff --git a/oatdump/oatdump_test.cc b/oatdump/oatdump_test.cc
index fa46202327..bc25896c46 100644
--- a/oatdump/oatdump_test.cc
+++ b/oatdump/oatdump_test.cc
@@ -22,7 +22,7 @@ namespace art {
 
 INSTANTIATE_TEST_SUITE_P(DynamicOrStatic,
                          OatDumpTest,
-                         testing::Values(Flavor::kDynamic, Flavor::kStatic));
+                         ::testing::Values(Flavor::kDynamic, Flavor::kStatic));
 
 // Disable tests on arm and arm64 as they are taking too long to run. b/27824283.
 #define TEST_DISABLED_FOR_ARM_AND_ARM64() \
diff --git a/oatdump/oatdump_test.h b/oatdump/oatdump_test.h
index 9af33578db..eb713357d9 100644
--- a/oatdump/oatdump_test.h
+++ b/oatdump/oatdump_test.h
@@ -41,7 +41,7 @@ enum class Flavor {
   kStatic,   // oatdump(d)s, dex2oat(d)s
 };
 
-class OatDumpTest : public CommonRuntimeTest, public testing::WithParamInterface<Flavor> {
+class OatDumpTest : public CommonRuntimeTest, public ::testing::WithParamInterface<Flavor> {
  protected:
   virtual void SetUp() {
     CommonRuntimeTest::SetUp();
diff --git a/odrefresh/odr_metrics_record_test.cc b/odrefresh/odr_metrics_record_test.cc
index 8c24156aa4..1208e188e2 100644
--- a/odrefresh/odr_metrics_record_test.cc
+++ b/odrefresh/odr_metrics_record_test.cc
@@ -132,7 +132,7 @@ TEST_F(OdrMetricsRecordTest, HappyPath) {
 
 TEST_F(OdrMetricsRecordTest, EmptyInput) {
   OdrMetricsRecord record{};
-  ASSERT_THAT(record.ReadFromFile(file_path_), testing::Not(Ok()));
+  ASSERT_THAT(record.ReadFromFile(file_path_), ::testing::Not(Ok()));
 }
 
 TEST_F(OdrMetricsRecordTest, UnexpectedInput) {
diff --git a/odrefresh/odrefresh.cc b/odrefresh/odrefresh.cc
index bf3d62d0d0..92912b7bfa 100644
--- a/odrefresh/odrefresh.cc
+++ b/odrefresh/odrefresh.cc
@@ -870,7 +870,8 @@ static void ReportNextBootAnimationProgress(uint32_t current_compilation,
                                             uint32_t number_of_compilations) {
   // We arbitrarily show progress until 90%, expecting that our compilations take a large chunk of
   // boot time.
-  uint32_t value = (90 * current_compilation) / number_of_compilations;
+  uint32_t value =
+      number_of_compilations != 0 ? (90 * current_compilation) / number_of_compilations : 90;
   SetProperty("service.bootanim.progress", std::to_string(value));
 }
 
@@ -1761,7 +1762,6 @@ WARN_UNUSED CompilationResult OnDeviceRefresh::RunDex2oat(
     }
   }
 
-  args.Add("--oat-location=%s", artifacts.OatPath());
   std::pair<std::string, const char*> location_kind_pairs[] = {
       std::make_pair(artifacts.ImagePath(), artifacts.ImageKind()),
       std::make_pair(artifacts.OatPath(), "oat"),
@@ -1909,9 +1909,16 @@ OnDeviceRefresh::RunDex2oatForBootClasspath(const std::string& staging_dir,
                                                  preloaded_classes_file,
                                                  strerror(errno)));
     }
+    args.Add("--oat-location=%s", OdrArtifacts::ForBootImage(output_path).OatPath());
   } else {
     // Mainline extension.
     args.Add("--compiler-filter=%s", kMainlineCompilerFilter);
+    // For boot image extensions, dex2oat takes the oat location of the primary boot image and
+    // expends it with the name of the first input dex file.
+    args.Add("--oat-location=%s",
+             OdrArtifacts::ForBootImage(
+                 GetPrimaryBootImagePath(/*on_system=*/false, /*minimal=*/false, isa))
+                 .OatPath());
   }
 
   const OdrSystemProperties& system_properties = config_.GetSystemProperties();
@@ -2079,6 +2086,8 @@ WARN_UNUSED CompilationResult OnDeviceRefresh::RunDex2oatForSystemServer(
     args.Add("--class-loader-context-fds=%s", Join(fds, ':'));
   }
 
+  args.Add("--oat-location=%s", OdrArtifacts::ForSystemServer(output_path).OatPath());
+
   const OdrSystemProperties& system_properties = config_.GetSystemProperties();
   args.AddRuntimeIfNonEmpty("-Xms%s", system_properties.GetOrEmpty("dalvik.vm.dex2oat-Xms"))
       .AddRuntimeIfNonEmpty("-Xmx%s", system_properties.GetOrEmpty("dalvik.vm.dex2oat-Xmx"));
diff --git a/odrefresh/odrefresh_test.cc b/odrefresh/odrefresh_test.cc
index b19a4225b4..7f4e990ffe 100644
--- a/odrefresh/odrefresh_test.cc
+++ b/odrefresh/odrefresh_test.cc
@@ -48,12 +48,14 @@
 namespace art {
 namespace odrefresh {
 
+using ::android::base::Basename;
 using ::android::base::Split;
 using ::android::modules::sdklevel::IsAtLeastU;
 using ::testing::_;
 using ::testing::AllOf;
 using ::testing::Contains;
 using ::testing::ElementsAre;
+using ::testing::EndsWith;
 using ::testing::Not;
 using ::testing::ResultOf;
 using ::testing::Return;
@@ -336,7 +338,7 @@ TEST_F(OdRefreshTest, BootImageMainlineExtension) {
                                         FdOf(framework_jar_),
                                         FdOf(conscrypt_jar_),
                                         FdOf(framework_wifi_jar_)))),
-          Contains(Flag("--oat-location=", dalvik_cache_dir_ + "/x86_64/boot-conscrypt.oat")),
+          Contains(Flag("--oat-location=", dalvik_cache_dir_ + "/x86_64/boot.oat")),
           Not(Contains(Flag("--base=", _))),
           Contains(Flag("--boot-image=", _)),
           Contains(Flag("--cache-info-fd=", FdOf(cache_info_xml_))))))
@@ -435,11 +437,15 @@ TEST_F(OdRefreshTest, BootClasspathJarsFallback) {
 }
 
 TEST_F(OdRefreshTest, AllSystemServerJars) {
-  EXPECT_CALL(*mock_exec_utils_,
-              DoExecAndReturnCode(AllOf(Contains(Flag("--dex-file=", location_provider_jar_)),
-                                        Contains("--class-loader-context=PCL[]"),
-                                        Not(Contains(Flag("--class-loader-context-fds=", _))),
-                                        Contains(Flag("--cache-info-fd=", FdOf(cache_info_xml_))))))
+  EXPECT_CALL(
+      *mock_exec_utils_,
+      DoExecAndReturnCode(AllOf(
+          Contains(Flag("--dex-file=", location_provider_jar_)),
+          Contains("--class-loader-context=PCL[]"),
+          Not(Contains(Flag("--class-loader-context-fds=", _))),
+          Contains(Flag("--cache-info-fd=", FdOf(cache_info_xml_))),
+          Contains(Flag("--oat-location=",
+                        EndsWith("@" + Basename(location_provider_jar_) + "@classes.odex"))))))
       .WillOnce(Return(0));
   EXPECT_CALL(
       *mock_exec_utils_,
@@ -447,7 +453,9 @@ TEST_F(OdRefreshTest, AllSystemServerJars) {
           Contains(Flag("--dex-file=", services_jar_)),
           Contains(Flag("--class-loader-context=", ART_FORMAT("PCL[{}]", location_provider_jar_))),
           Contains(Flag("--class-loader-context-fds=", FdOf(location_provider_jar_))),
-          Contains(Flag("--cache-info-fd=", FdOf(cache_info_xml_))))))
+          Contains(Flag("--cache-info-fd=", FdOf(cache_info_xml_))),
+          Contains(
+              Flag("--oat-location=", EndsWith("@" + Basename(services_jar_) + "@classes.odex"))))))
       .WillOnce(Return(0));
   EXPECT_CALL(
       *mock_exec_utils_,
@@ -457,7 +465,9 @@ TEST_F(OdRefreshTest, AllSystemServerJars) {
                         ART_FORMAT("PCL[];PCL[{}:{}]", location_provider_jar_, services_jar_))),
           Contains(ListFlag("--class-loader-context-fds=",
                             ElementsAre(FdOf(location_provider_jar_), FdOf(services_jar_)))),
-          Contains(Flag("--cache-info-fd=", FdOf(cache_info_xml_))))))
+          Contains(Flag("--cache-info-fd=", FdOf(cache_info_xml_))),
+          Contains(Flag("--oat-location=",
+                        EndsWith("@" + Basename(services_foo_jar_) + "@classes.odex"))))))
       .WillOnce(Return(0));
   EXPECT_CALL(
       *mock_exec_utils_,
@@ -467,7 +477,9 @@ TEST_F(OdRefreshTest, AllSystemServerJars) {
                         ART_FORMAT("PCL[];PCL[{}:{}]", location_provider_jar_, services_jar_))),
           Contains(ListFlag("--class-loader-context-fds=",
                             ElementsAre(FdOf(location_provider_jar_), FdOf(services_jar_)))),
-          Contains(Flag("--cache-info-fd=", FdOf(cache_info_xml_))))))
+          Contains(Flag("--cache-info-fd=", FdOf(cache_info_xml_))),
+          Contains(Flag("--oat-location=",
+                        EndsWith("@" + Basename(services_bar_jar_) + "@classes.odex"))))))
       .WillOnce(Return(0));
 
   EXPECT_EQ(
diff --git a/openjdkjvmti/events.cc b/openjdkjvmti/events.cc
index 31107d08a5..a1e9a3dcf4 100644
--- a/openjdkjvmti/events.cc
+++ b/openjdkjvmti/events.cc
@@ -1251,7 +1251,7 @@ void EventHandler::HandleLocalAccessCapabilityAdded() {
           continue;
         } else if (!runtime_->GetClassLinker()->IsQuickToInterpreterBridge(code) &&
                    !runtime_->IsAsyncDeoptimizeable(&m, reinterpret_cast<uintptr_t>(code))) {
-          runtime_->GetInstrumentation()->InitializeMethodsCode(&m, /*aot_code=*/ nullptr);
+          runtime_->GetInstrumentation()->ReinitializeMethodsCode(&m);
         }
       }
       return true;
diff --git a/openjdkjvmti/ti_class.cc b/openjdkjvmti/ti_class.cc
index 5581bc2f02..40dde5755b 100644
--- a/openjdkjvmti/ti_class.cc
+++ b/openjdkjvmti/ti_class.cc
@@ -564,9 +564,8 @@ jvmtiError ClassUtil::GetClassFields(jvmtiEnv* env,
     return ERR(NULL_POINTER);
   }
 
-  art::IterationRange<art::StrideIterator<art::ArtField>> ifields = klass->GetIFields();
-  art::IterationRange<art::StrideIterator<art::ArtField>> sfields = klass->GetSFields();
-  size_t array_size = klass->NumInstanceFields() + klass->NumStaticFields();
+  art::IterationRange<art::StrideIterator<art::ArtField>> fields = klass->GetFields();
+  size_t array_size = klass->NumFields();
 
   unsigned char* out_ptr;
   jvmtiError allocError = env->Allocate(array_size * sizeof(jfieldID), &out_ptr);
@@ -576,11 +575,7 @@ jvmtiError ClassUtil::GetClassFields(jvmtiEnv* env,
   jfieldID* field_array = reinterpret_cast<jfieldID*>(out_ptr);
 
   size_t array_idx = 0;
-  for (art::ArtField& field : sfields) {
-    field_array[array_idx] = art::jni::EncodeArtField(&field);
-    ++array_idx;
-  }
-  for (art::ArtField& field : ifields) {
+  for (art::ArtField& field : fields) {
     field_array[array_idx] = art::jni::EncodeArtField(&field);
     ++array_idx;
   }
diff --git a/openjdkjvmti/ti_heap.cc b/openjdkjvmti/ti_heap.cc
index 80bfa0ff43..49a6748c28 100644
--- a/openjdkjvmti/ti_heap.cc
+++ b/openjdkjvmti/ti_heap.cc
@@ -352,43 +352,42 @@ class FieldVisitor {
 
     // Now visit fields for the current klass.
 
-    for (auto& static_field : klass->GetSFields()) {
-      if (static_field.IsPrimitiveType()) {
-        if (static_prim_visitor(obj,
-                                klass,
-                                static_field,
-                                field_index,
-                                user_data_)) {
-          return true;
-        }
-      } else {
-        if (static_ref_visitor(obj,
-                               klass,
-                               static_field,
-                               field_index,
-                               user_data_)) {
-          return true;
-        }
-      }
-      field_index++;
-    }
-
-    for (auto& instance_field : klass->GetIFields()) {
-      if (instance_field.IsPrimitiveType()) {
-        if (instance_prim_visitor(obj,
+    for (auto& field : klass->GetFields()) {
+      if (field.IsStatic()) {
+        if (field.IsPrimitiveType()) {
+          if (static_prim_visitor(obj,
                                   klass,
-                                  instance_field,
+                                  field,
                                   field_index,
                                   user_data_)) {
-          return true;
-        }
-      } else {
-        if (instance_ref_visitor(obj,
+            return true;
+          }
+        } else {
+          if (static_ref_visitor(obj,
                                  klass,
-                                 instance_field,
+                                 field,
                                  field_index,
                                  user_data_)) {
-          return true;
+            return true;
+          }
+        }
+      } else {
+        if (field.IsPrimitiveType()) {
+          if (instance_prim_visitor(obj,
+                                    klass,
+                                    field,
+                                    field_index,
+                                    user_data_)) {
+            return true;
+          }
+        } else {
+          if (instance_ref_visitor(obj,
+                                   klass,
+                                   field,
+                                   field_index,
+                                   user_data_)) {
+            return true;
+          }
         }
       }
       field_index++;
@@ -458,8 +457,7 @@ class FieldVisitor {
     auto visitor = [&count](art::ObjPtr<art::mirror::Class> inf_klass)
         REQUIRES_SHARED(art::Locks::mutator_lock_) {
       DCHECK(inf_klass->IsInterface());
-      DCHECK_EQ(0u, inf_klass->NumInstanceFields());
-      count += inf_klass->NumStaticFields();
+      count += inf_klass->NumFields();
     };
     RecursiveInterfaceVisit<decltype(visitor)>::VisitStatic(art::Thread::Current(), klass, visitor);
 
@@ -1585,6 +1583,8 @@ jvmtiError HeapExtensions::GetHeapName(jvmtiEnv* env, jint heap_id, char** heap_
   }
 }
 
+#pragma clang diagnostic push
+#pragma clang diagnostic ignored "-Wcast-function-type-mismatch"
 jvmtiError HeapExtensions::IterateThroughHeapExt(jvmtiEnv* env,
                                                  jint heap_filter,
                                                  jclass klass,
@@ -1616,6 +1616,7 @@ jvmtiError HeapExtensions::IterateThroughHeapExt(jvmtiEnv* env,
                               callbacks,
                               user_data);
 }
+#pragma clang diagnostic pop
 
 namespace {
 
diff --git a/openjdkjvmti/ti_redefine.cc b/openjdkjvmti/ti_redefine.cc
index 8e11d592e5..8e7885a726 100644
--- a/openjdkjvmti/ti_redefine.cc
+++ b/openjdkjvmti/ti_redefine.cc
@@ -2135,7 +2135,7 @@ art::ObjPtr<art::mirror::Class> Redefiner::ClassRedefinition::AllocateNewClassOb
   }
   // Finish setting up methods.
   linked_class->VisitMethods([&](art::ArtMethod* m) REQUIRES_SHARED(art::Locks::mutator_lock_) {
-    driver_->runtime_->GetInstrumentation()->InitializeMethodsCode(m, /* aot_code= */ nullptr);
+    driver_->runtime_->GetInstrumentation()->ReinitializeMethodsCode(m);
     m->SetNotIntrinsic();
     DCHECK(m->IsCopied() || m->GetDeclaringClass() == linked_class.Get())
         << m->PrettyMethod()
@@ -2573,11 +2573,10 @@ void Redefiner::ClassRedefinition::UpdateMethods(art::ObjPtr<art::mirror::Class>
     CHECK(method_id != nullptr);
     uint32_t dex_method_idx = dex_file_->GetIndexForMethodId(*method_id);
     method.SetDexMethodIndex(dex_method_idx);
-    driver_->runtime_->GetInstrumentation()->InitializeMethodsCode(&method, /*aot_code=*/ nullptr);
+    driver_->runtime_->GetInstrumentation()->ReinitializeMethodsCode(&method);
     if (method.HasCodeItem()) {
       method.SetCodeItem(
-          dex_file_->GetCodeItem(dex_file_->FindCodeItemOffset(class_def, dex_method_idx)),
-          dex_file_->IsCompactDexFile());
+          dex_file_->GetCodeItem(dex_file_->FindCodeItemOffset(class_def, dex_method_idx)));
     }
     // Clear all the intrinsics related flags.
     method.SetNotIntrinsic();
@@ -2585,21 +2584,18 @@ void Redefiner::ClassRedefinition::UpdateMethods(art::ObjPtr<art::mirror::Class>
 }
 
 void Redefiner::ClassRedefinition::UpdateFields(art::ObjPtr<art::mirror::Class> mclass) {
-  // TODO The IFields & SFields pointers should be combined like the methods_ arrays were.
-  for (auto fields_iter : {mclass->GetIFields(), mclass->GetSFields()}) {
-    for (art::ArtField& field : fields_iter) {
-      const art::dex::TypeId* new_declaring_id =
-          dex_file_->FindTypeId(field.GetDeclaringClassDescriptorView());
-      const art::dex::StringId* new_name_id = dex_file_->FindStringId(field.GetName());
-      const art::dex::TypeId* new_type_id = dex_file_->FindTypeId(field.GetTypeDescriptorView());
-      CHECK(new_name_id != nullptr && new_type_id != nullptr && new_declaring_id != nullptr);
-      const art::dex::FieldId* new_field_id =
-          dex_file_->FindFieldId(*new_declaring_id, *new_name_id, *new_type_id);
-      CHECK(new_field_id != nullptr);
-      uint32_t new_field_index = dex_file_->GetIndexForFieldId(*new_field_id);
-      // We only need to update the index since the other data in the ArtField cannot be updated.
-      field.SetDexFieldIndex(new_field_index);
-    }
+  for (art::ArtField& field : mclass->GetFields()) {
+    const art::dex::TypeId* new_declaring_id =
+        dex_file_->FindTypeId(field.GetDeclaringClassDescriptorView());
+    const art::dex::StringId* new_name_id = dex_file_->FindStringId(field.GetName());
+    const art::dex::TypeId* new_type_id = dex_file_->FindTypeId(field.GetTypeDescriptorView());
+    CHECK(new_name_id != nullptr && new_type_id != nullptr && new_declaring_id != nullptr);
+    const art::dex::FieldId* new_field_id =
+        dex_file_->FindFieldId(*new_declaring_id, *new_name_id, *new_type_id);
+    CHECK(new_field_id != nullptr);
+    uint32_t new_field_index = dex_file_->GetIndexForFieldId(*new_field_id);
+    // We only need to update the index since the other data in the ArtField cannot be updated.
+    field.SetDexFieldIndex(new_field_index);
   }
 }
 
@@ -2609,11 +2605,10 @@ void Redefiner::ClassRedefinition::CollectNewFieldAndMethodMappings(
     std::map<art::ArtField*, art::ArtField*>* field_map) {
   for (auto [new_cls, old_cls] :
        art::ZipLeft(data.GetNewClasses()->Iterate(), data.GetOldClasses()->Iterate())) {
-    for (art::ArtField& f : old_cls->GetSFields()) {
-      (*field_map)[&f] = new_cls->FindDeclaredStaticField(f.GetName(), f.GetTypeDescriptor());
-    }
-    for (art::ArtField& f : old_cls->GetIFields()) {
-      (*field_map)[&f] = new_cls->FindDeclaredInstanceField(f.GetName(), f.GetTypeDescriptor());
+    for (art::ArtField& f : old_cls->GetFields()) {
+      (*field_map)[&f] = f.IsStatic()
+          ? new_cls->FindDeclaredStaticField(f.GetName(), f.GetTypeDescriptor())
+          : new_cls->FindDeclaredInstanceField(f.GetName(), f.GetTypeDescriptor());
     }
     auto new_methods = new_cls->GetMethods(art::kRuntimePointerSize);
     for (art::ArtMethod& m : old_cls->GetMethods(art::kRuntimePointerSize)) {
@@ -2673,12 +2668,14 @@ static void CopyFields(bool is_static,
   DCHECK(!source_class->IsObjectClass() && !target_class->IsObjectClass())
       << "Should not be overriding object class fields. Target: " << target_class->PrettyClass()
       << " Source: " << source_class->PrettyClass();
-  for (art::ArtField& f : (is_static ? source_class->GetSFields() : source_class->GetIFields())) {
-    art::ArtField* new_field =
-        (is_static ? target_class->FindDeclaredStaticField(f.GetName(), f.GetTypeDescriptor())
-                   : target_class->FindDeclaredInstanceField(f.GetName(), f.GetTypeDescriptor()));
-    CHECK(new_field != nullptr) << "could not find new version of " << f.PrettyField();
-    CopyField(target, new_field, source, f);
+  for (art::ArtField& f : source_class->GetFields()) {
+    if (f.IsStatic() == is_static) {
+      art::ArtField* new_field =
+          (is_static ? target_class->FindDeclaredStaticField(f.GetName(), f.GetTypeDescriptor())
+                     : target_class->FindDeclaredInstanceField(f.GetName(), f.GetTypeDescriptor()));
+      CHECK(new_field != nullptr) << "could not find new version of " << f.PrettyField();
+      CopyField(target, new_field, source, f);
+    }
   }
   if (!is_static && !target_class->GetSuperClass()->IsObjectClass()) {
     CopyFields(
@@ -2719,8 +2716,10 @@ static void ClearFields(bool is_static,
                         art::ObjPtr<art::mirror::Class> target_class)
     REQUIRES(art::Locks::mutator_lock_) {
   DCHECK(!target_class->IsObjectClass());
-  for (art::ArtField& f : (is_static ? target_class->GetSFields() : target_class->GetIFields())) {
-    ClearField(target, f);
+  for (art::ArtField& f : target_class->GetFields()) {
+    if (f.IsStatic() == is_static) {
+      ClearField(target, f);
+    }
   }
   if (!is_static && !target_class->GetSuperClass()->IsObjectClass()) {
     ClearFields(is_static, target, target_class->GetSuperClass());
@@ -2860,27 +2859,16 @@ void Redefiner::ClassRedefinition::UpdateClassStructurally(const RedefinitionDat
           });
     } else {
       auto pred = [&](art::ArtField& f) REQUIRES(art::Locks::mutator_lock_) {
-        return std::string_view(f.GetName()) == std::string_view(field_or_method->GetName()) &&
-               std::string_view(f.GetTypeDescriptor()) ==
-                   std::string_view(field_or_method->GetTypeDescriptor());
+        return f.GetNameView() == field_or_method->GetNameView() &&
+               f.GetTypeDescriptorView() == field_or_method->GetTypeDescriptorView();
       };
-      if (field_or_method->IsStatic()) {
-        return std::any_of(
-            replacement_classes_iter.begin(),
-            replacement_classes_iter.end(),
-            [&](art::ObjPtr<art::mirror::Class> cand) REQUIRES(art::Locks::mutator_lock_) {
-              auto sfields = cand->GetSFields();
-              return std::find_if(sfields.begin(), sfields.end(), pred) != sfields.end();
-            });
-      } else {
-        return std::any_of(
+      return std::any_of(
             replacement_classes_iter.begin(),
             replacement_classes_iter.end(),
             [&](art::ObjPtr<art::mirror::Class> cand) REQUIRES(art::Locks::mutator_lock_) {
-              auto ifields = cand->GetIFields();
-              return std::find_if(ifields.begin(), ifields.end(), pred) != ifields.end();
+              auto fields = cand->GetFields();
+              return std::find_if(fields.begin(), fields.end(), pred) != fields.end();
             });
-      }
     }
   };
   // TODO Performing 2 stack-walks back to back isn't the greatest. We might want to try to combine
diff --git a/openjdkjvmti/ti_search.cc b/openjdkjvmti/ti_search.cc
index 30a889aaa5..e441010939 100644
--- a/openjdkjvmti/ti_search.cc
+++ b/openjdkjvmti/ti_search.cc
@@ -278,7 +278,7 @@ jvmtiError SearchUtil::AddToDexClassLoaderInMemory(jvmtiEnv* jvmti_env,
   // lot of code as well.
 
   // Create a memfd
-  art::File file(art::memfd_create_compat("JVMTI InMemory Added dex file", 0), /*check-usage*/true);
+  art::File file(art::memfd_create("JVMTI InMemory Added dex file", 0), /*check-usage*/true);
   if (file.Fd() < 0) {
     char* reason = strerror(errno);
     JVMTI_LOG(ERROR, jvmti_env) << "Unable to create memfd due to " << reason;
diff --git a/openjdkjvmti/ti_thread.cc b/openjdkjvmti/ti_thread.cc
index 9c71d4e3cc..f89a9d9a14 100644
--- a/openjdkjvmti/ti_thread.cc
+++ b/openjdkjvmti/ti_thread.cc
@@ -550,7 +550,7 @@ static jint GetJavaStateFromInternal(const InternalThreadState& state) {
 
 // Suspends the current thread if it has any suspend requests on it.
 void ThreadUtil::SuspendCheck(art::Thread* self) {
-  DCHECK(!self->ReadFlag(art::ThreadFlag::kSuspensionImmune));
+  DCHECK(!self->ReadFlag(art::ThreadFlag::kSuspensionImmune, std::memory_order_relaxed));
   art::ScopedObjectAccess soa(self);
   // Really this is only needed if we are in FastJNI and actually have the mutator_lock_ already.
   self->FullSuspendCheck();
diff --git a/perfetto_hprof/perfetto_hprof.cc b/perfetto_hprof/perfetto_hprof.cc
index 7379fd66c9..0602437383 100644
--- a/perfetto_hprof/perfetto_hprof.cc
+++ b/perfetto_hprof/perfetto_hprof.cc
@@ -568,8 +568,9 @@ uint64_t GetObjectId(const art::mirror::Object* obj) {
 
 template <typename F>
 void ForInstanceReferenceField(art::mirror::Class* klass, F fn) NO_THREAD_SAFETY_ANALYSIS {
-  for (art::ArtField& af : klass->GetIFields()) {
-    if (af.IsPrimitiveType() ||
+  for (art::ArtField& af : klass->GetFields()) {
+    if (af.IsStatic() ||
+        af.IsPrimitiveType() ||
         af.GetOffset().Uint32Value() == art::mirror::Object::ClassOffset().Uint32Value()) {
       continue;
     }
diff --git a/profman/boot_image_profile.cc b/profman/boot_image_profile.cc
index 9c46786df2..fb2c49c648 100644
--- a/profman/boot_image_profile.cc
+++ b/profman/boot_image_profile.cc
@@ -39,9 +39,9 @@ static constexpr char kMethodFlagStringStartup = 'S';
 static constexpr char kMethodFlagStringPostStartup = 'P';
 
 // Returns the type descriptor of the given reference.
-static std::string GetTypeDescriptor(const TypeReference& ref) {
+static std::string_view GetTypeDescriptorView(const TypeReference& ref) {
   const dex::TypeId& type_id = ref.dex_file->GetTypeId(ref.TypeIndex());
-  return ref.dex_file->GetTypeDescriptor(type_id);
+  return ref.dex_file->GetTypeDescriptorView(type_id);
 }
 
 // Returns the method representation used in the text format of the boot image profile.
@@ -49,8 +49,8 @@ static std::string BootImageRepresentation(const MethodReference& ref) {
   const DexFile* dex_file = ref.dex_file;
   const dex::MethodId& id = ref.GetMethodId();
   std::string signature_string(dex_file->GetMethodSignature(id).ToString());
-  std::string type_string(dex_file->GetTypeDescriptor(dex_file->GetTypeId(id.class_idx_)));
-  std::string method_name(dex_file->GetMethodName(id));
+  std::string type_string(dex_file->GetTypeDescriptorView(dex_file->GetTypeId(id.class_idx_)));
+  std::string method_name(dex_file->GetMethodNameView(id));
   return type_string +
         kMethodSep +
         method_name +
@@ -59,13 +59,13 @@ static std::string BootImageRepresentation(const MethodReference& ref) {
 
 // Returns the class representation used in the text format of the boot image profile.
 static std::string BootImageRepresentation(const TypeReference& ref) {
-  return GetTypeDescriptor(ref);
+  return std::string(GetTypeDescriptorView(ref));
 }
 
 // Returns the class representation used in preloaded classes.
 static std::string PreloadedClassesRepresentation(const TypeReference& ref) {
-  std::string descriptor = GetTypeDescriptor(ref);
-  return DescriptorToDot(descriptor.c_str());
+  std::string_view descriptor = GetTypeDescriptorView(ref);
+  return DescriptorToDot(descriptor);
 }
 
 // Formats the list of packages from the item metadata as a debug string.
diff --git a/profman/profile_assistant_test.cc b/profman/profile_assistant_test.cc
index 8699a021f6..15fe49db81 100644
--- a/profman/profile_assistant_test.cc
+++ b/profman/profile_assistant_test.cc
@@ -825,7 +825,7 @@ TEST_F(ProfileAssistantTest, TestProfileCreationGenerateMethods) {
   std::string expected_contents;
   for (std::string& class_name : class_names) {
     input_file_contents += class_name + std::string("\n");
-    expected_contents += DescriptorToDot(class_name.c_str()) +
+    expected_contents += DescriptorToDot(class_name) +
         std::string("\n");
   }
   std::string output_file_contents;
@@ -923,7 +923,7 @@ TEST_F(ProfileAssistantTest, TestBootImageProfile) {
   std::string input_file_contents = JoinProfileLines(input_data);
 
   ScratchFile preloaded_class_denylist;
-  std::string denylist_content = DescriptorToDot(kPreloadedDenylistedClass.c_str());
+  std::string denylist_content = DescriptorToDot(kPreloadedDenylistedClass);
   EXPECT_TRUE(preloaded_class_denylist.GetFile()->WriteFully(
       denylist_content.c_str(), denylist_content.length()));
 
@@ -940,7 +940,7 @@ TEST_F(ProfileAssistantTest, TestBootImageProfile) {
   std::string expected_profile_content = JoinProfileLines(expected_data);
 
   std::vector<std::string> expected_preloaded_data = {
-       DescriptorToDot(kDirtyClass.c_str())
+       DescriptorToDot(kDirtyClass)
   };
   std::string expected_preloaded_content = JoinProfileLines(expected_preloaded_data);
 
diff --git a/runtime/Android.bp b/runtime/Android.bp
index 1c7e9fbcb3..c0a65f27a7 100644
--- a/runtime/Android.bp
+++ b/runtime/Android.bp
@@ -397,6 +397,7 @@ cc_defaults {
         "oat/oat_file_assistant_context.cc",
         "oat/oat_file_manager.cc",
         "oat/oat_quick_method_header.cc",
+        "oat/sdc_file.cc",
         "oat/stack_map.cc",
         "object_lock.cc",
         "offsets.cc",
@@ -540,16 +541,35 @@ cc_defaults {
                 // shared between the x86 and x86_64 architectures.
                 "interpreter/mterp/nterp.cc",
                 ":libart_mterp.x86_64ng",
-                "arch/x86_64/context_x86_64.cc",
-                "arch/x86_64/entrypoints_init_x86_64.cc",
-                "arch/x86_64/jni_entrypoints_x86_64.S",
                 "arch/x86_64/memcmp16_x86_64.S",
-                "arch/x86_64/quick_entrypoints_x86_64.S",
                 "arch/x86_64/native_entrypoints_x86_64.S",
                 "arch/x86_64/thread_x86_64.cc",
                 "monitor_pool.cc",
                 "arch/x86/fault_handler_x86.cc",
-            ],
+            ] + select(soong_config_variable("art_module", "art_use_simulator"), {
+                // Quick code architecture specific sources. Only sources which relate to the quick
+                // code ISA (see definition of StackType in thread.h) should be placed in this
+                // section. This is because all simulator build configurations will always have a
+                // different quick code (target) ISA to that of the native (host machine) ISA and
+                // therefore will require quick code specific sources (e.g: assembly entrypoints)
+                // to be built for the host machine.
+                //
+                // When the simulator is enabled, i.e.: ART_USE_SIMULATOR is defined, x86_64 native
+                // code files are used with Arm64 quick code files. This ensures that the target
+                // (arm64) context and entrypoints are used even on host (x86_64).
+                true: [
+                    "arch/arm64/context_arm64.cc",
+                    "arch/arm64/entrypoints_init_arm64.cc",
+                    "arch/arm64/jni_entrypoints_arm64.S",
+                    "arch/arm64/quick_entrypoints_arm64.S",
+                ],
+                default: [
+                    "arch/x86_64/context_x86_64.cc",
+                    "arch/x86_64/entrypoints_init_x86_64.cc",
+                    "arch/x86_64/jni_entrypoints_x86_64.S",
+                    "arch/x86_64/quick_entrypoints_x86_64.S",
+                ],
+            }),
             avx: {
                 asflags: ["-DMTERP_USE_AVX"],
             },
@@ -1110,6 +1130,7 @@ art_cc_defaults {
         "native_stack_dump_test.cc",
         "oat/oat_file_assistant_test.cc",
         "oat/oat_file_test.cc",
+        "oat/sdc_file_test.cc",
         "parsed_options_test.cc",
         "prebuilt_tools_test.cc",
         "proxy_test.cc",
diff --git a/runtime/arch/arm/entrypoints_init_arm.cc b/runtime/arch/arm/entrypoints_init_arm.cc
index 4fb4774a6d..26664cd94b 100644
--- a/runtime/arch/arm/entrypoints_init_arm.cc
+++ b/runtime/arch/arm/entrypoints_init_arm.cc
@@ -28,6 +28,7 @@
 #include "entrypoints/quick/runtime_entrypoints_list.h"
 #include "entrypoints/runtime_asm_entrypoints.h"
 #include "interpreter/interpreter.h"
+#include "trace_profile.h"
 
 namespace art HIDDEN {
 
@@ -197,7 +198,7 @@ void InitEntryPoints(JniEntryPoints* jpoints,
 }
 
 void UpdateLowOverheadTraceEntrypoints([[maybe_unused]] QuickEntryPoints* qpoints,
-                                       [[maybe_unused]] bool enable) {
+                                       [[maybe_unused]] LowOverheadTraceType trace_type) {
   // This is a nop on this architecture. Low overhead tracing is only implemented for ARM64.
 }
 
diff --git a/runtime/arch/arm/native_entrypoints_arm.S b/runtime/arch/arm/native_entrypoints_arm.S
index 1666dc8d4b..1f3aae6392 100644
--- a/runtime/arch/arm/native_entrypoints_arm.S
+++ b/runtime/arch/arm/native_entrypoints_arm.S
@@ -63,7 +63,12 @@ ENTRY art_jni_dlsym_lookup_stub
     bic    ip, #TAGGED_JNI_SP_MASK                    // ArtMethod** sp
     ldr    ip, [ip]                                   // ArtMethod* method
     ldr    ip, [ip, #ART_METHOD_ACCESS_FLAGS_OFFSET]  // uint32_t access_flags
+#ifdef ART_USE_RESTRICTED_MODE
+    // Critical native methods are disabled and treated as normal native methods instead.
+    tst    ip, #(ACCESS_FLAGS_METHOD_IS_FAST_NATIVE)
+#else
     tst    ip, #(ACCESS_FLAGS_METHOD_IS_FAST_NATIVE | ACCESS_FLAGS_METHOD_IS_CRITICAL_NATIVE)
+#endif
     bne    .Llookup_stub_fast_or_critical_native
     blx    artFindNativeMethod
     b      .Llookup_stub_continue
diff --git a/runtime/arch/arm/quick_entrypoints_arm.S b/runtime/arch/arm/quick_entrypoints_arm.S
index ddfe70dc51..150780b921 100644
--- a/runtime/arch/arm/quick_entrypoints_arm.S
+++ b/runtime/arch/arm/quick_entrypoints_arm.S
@@ -1482,7 +1482,8 @@ ENTRY art_quick_generic_jni_trampoline
     vmov d0, r0, r1
 
     LOAD_RUNTIME_INSTANCE r2
-    ldrb r2, [r2,  #RUN_EXIT_HOOKS_OFFSET_FROM_RUNTIME_INSTANCE]
+    ldr r2, [r2,  #RUNTIME_INSTRUMENTATION_OFFSET]
+    ldrb r2, [r2,  #INSTRUMENTATION_RUN_EXIT_HOOKS_OFFSET]
     CFI_REMEMBER_STATE
     cbnz r2, .Lcall_method_exit_hook
 .Lcall_method_exit_hook_done:
diff --git a/runtime/arch/arm64/asm_support_arm64.S b/runtime/arch/arm64/asm_support_arm64.S
index ae965303b9..7b89707b1c 100644
--- a/runtime/arch/arm64/asm_support_arm64.S
+++ b/runtime/arch/arm64/asm_support_arm64.S
@@ -42,6 +42,30 @@
 // Implicit suspend check register.
 #define xSUSPEND x21
 
+.macro LOAD_PC_REL_ADDRESS reg, symbol
+    adr \reg, \symbol
+.endm
+
+.macro CALL_SYMBOL symbol
+    bl \symbol
+.endm
+
+.macro BRANCH_SYMBOL symbol
+    b \symbol
+.endm
+
+.macro BRANCH_SYMBOL_CBZ reg, symbol
+    cbz \reg, \symbol
+.endm
+
+.macro BRANCH_SYMBOL_NE symbol
+    b.ne \symbol
+.endm
+
+.macro BRANCH_SYMBOL_EQ symbol
+    b.eq \symbol
+.endm
+
 .macro CFI_EXPRESSION_BREG n, b, offset
     .if (-0x40 <= (\offset)) && ((\offset) < 0x40)
         CFI_EXPRESSION_BREG_1(\n, \b, \offset)
@@ -409,8 +433,8 @@
     mov x0, xSELF
 
     // Point of no return.
-    bl artDeliverPendingExceptionFromCode  // artDeliverPendingExceptionFromCode(Thread*)
-    bl art_quick_do_long_jump              // (Context*)
+    CALL_SYMBOL artDeliverPendingExceptionFromCode  // artDeliverPendingExceptionFromCode(Thread*)
+    CALL_SYMBOL art_quick_do_long_jump              // (Context*)
     brk 0  // Unreached
 .endm
 
@@ -440,7 +464,7 @@
     // Use scratch registers x8-x11 as temporaries.
     ldr    w9, [xSELF, #THREAD_ID_OFFSET]
     .if \can_be_null
-        cbz    \obj, \slow_lock
+        BRANCH_SYMBOL_CBZ \obj, \slow_lock
     .endif
                                       // Exclusive load/store has no immediate anymore.
     add    x8, \obj, #MIRROR_OBJECT_LOCK_WORD_OFFSET
@@ -459,10 +483,11 @@
 2:  // w10: original lock word, w9: thread id, w11: w10 ^ w9
                                       // Check lock word state and thread id together,
     tst    w11, #(LOCK_WORD_STATE_MASK_SHIFTED | LOCK_WORD_THIN_LOCK_OWNER_MASK_SHIFTED)
-    b.ne   \slow_lock
+    BRANCH_SYMBOL_NE \slow_lock
     add    w11, w10, #LOCK_WORD_THIN_LOCK_COUNT_ONE  // Increment the recursive lock count.
     tst    w11, #LOCK_WORD_THIN_LOCK_COUNT_MASK_SHIFTED  // Test the new thin lock count.
-    b.eq   \slow_lock                 // Zero as the new count indicates overflow, go slow path.
+    BRANCH_SYMBOL_EQ \slow_lock                 // Zero as the new count indicates overflow, go
+                                                // slow path.
     stxr   w10, w11, [x8]
     cbnz   w10, 1b                    // If the store failed, retry.
     ret
@@ -473,7 +498,7 @@
     // Use scratch registers x8-x11 as temporaries.
     ldr    w9, [xSELF, #THREAD_ID_OFFSET]
     .if \can_be_null
-        cbz    \obj, \slow_unlock
+        BRANCH_SYMBOL_CBZ \obj, \slow_unlock
     .endif
                                       // Exclusive load/store has no immediate anymore.
     add    x8, \obj, #MIRROR_OBJECT_LOCK_WORD_OFFSET
@@ -500,7 +525,7 @@
 2:
                                       // Check lock word state and thread id together.
     tst    w11, #(LOCK_WORD_STATE_MASK_SHIFTED | LOCK_WORD_THIN_LOCK_OWNER_MASK_SHIFTED)
-    b.ne   \slow_unlock
+    BRANCH_SYMBOL_NE \slow_unlock
     sub    w11, w10, #LOCK_WORD_THIN_LOCK_COUNT_ONE  // decrement count
 #ifndef USE_READ_BARRIER
     str    w11, [x8]
diff --git a/runtime/arch/arm64/entrypoints_init_arm64.cc b/runtime/arch/arm64/entrypoints_init_arm64.cc
index 3360708a4c..acc08c4fd7 100644
--- a/runtime/arch/arm64/entrypoints_init_arm64.cc
+++ b/runtime/arch/arm64/entrypoints_init_arm64.cc
@@ -29,6 +29,7 @@
 #include "entrypoints/quick/runtime_entrypoints_list.h"
 #include "entrypoints/runtime_asm_entrypoints.h"
 #include "interpreter/interpreter.h"
+#include "trace_profile.h"
 
 namespace art_flags = com::android::art::flags;
 
@@ -52,13 +53,12 @@ extern "C" mirror::Object* art_quick_read_barrier_mark_reg09(mirror::Object*);
 extern "C" mirror::Object* art_quick_read_barrier_mark_reg10(mirror::Object*);
 extern "C" mirror::Object* art_quick_read_barrier_mark_reg11(mirror::Object*);
 extern "C" mirror::Object* art_quick_read_barrier_mark_reg12(mirror::Object*);
-extern "C" mirror::Object* art_quick_read_barrier_mark_reg12(mirror::Object*);
 extern "C" mirror::Object* art_quick_read_barrier_mark_reg13(mirror::Object*);
 extern "C" mirror::Object* art_quick_read_barrier_mark_reg14(mirror::Object*);
 extern "C" mirror::Object* art_quick_read_barrier_mark_reg15(mirror::Object*);
-extern "C" mirror::Object* art_quick_read_barrier_mark_reg16(mirror::Object*);
+// extern "C" mirror::Object* art_quick_read_barrier_mark_reg16(mirror::Object*); ip0 is blocked
 extern "C" mirror::Object* art_quick_read_barrier_mark_reg17(mirror::Object*);
-extern "C" mirror::Object* art_quick_read_barrier_mark_reg18(mirror::Object*);
+// extern "C" mirror::Object* art_quick_read_barrier_mark_reg18(mirror::Object*); x18 is blocked
 extern "C" mirror::Object* art_quick_read_barrier_mark_reg19(mirror::Object*);
 extern "C" mirror::Object* art_quick_read_barrier_mark_reg20(mirror::Object*);
 extern "C" mirror::Object* art_quick_read_barrier_mark_reg21(mirror::Object*);
@@ -79,6 +79,9 @@ extern "C" mirror::Object* art_quick_read_barrier_mark_introspection_gc_roots(mi
 extern "C" void art_quick_record_entry_trace_event();
 extern "C" void art_quick_record_exit_trace_event();
 
+extern "C" void art_quick_record_long_running_entry_trace_event();
+extern "C" void art_quick_record_long_running_exit_trace_event();
+
 extern "C" void art_quick_nop_record_entry_trace_event() {
   return;
 }
@@ -126,8 +129,13 @@ void UpdateReadBarrierEntrypoints(QuickEntryPoints* qpoints, bool is_active) {
   qpoints->SetReadBarrierMarkReg28(is_active ? art_quick_read_barrier_mark_reg28 : nullptr);
   qpoints->SetReadBarrierMarkReg29(is_active ? art_quick_read_barrier_mark_reg29 : nullptr);
 
-  // Check that array switch cases are at appropriate offsets from the introspection entrypoint.
   DCHECK_ALIGNED(art_quick_read_barrier_mark_introspection, 512u);
+
+  // TODO(Simulator): the introspection entrypoints are not currently used in the simulator and
+  // they are not aligned correctly due to the veneer used in CALL_SYMBOL and BRANCH_SYMBOL.
+  // Re-enable these checks when the introspection entrypoints are used and tested.
+#ifndef ART_USE_RESTRICTED_MODE
+  // Check that array switch cases are at appropriate offsets from the introspection entrypoint.
   intptr_t array_diff =
       reinterpret_cast<intptr_t>(art_quick_read_barrier_mark_introspection_arrays) -
       reinterpret_cast<intptr_t>(art_quick_read_barrier_mark_introspection);
@@ -137,6 +145,7 @@ void UpdateReadBarrierEntrypoints(QuickEntryPoints* qpoints, bool is_active) {
       reinterpret_cast<intptr_t>(art_quick_read_barrier_mark_introspection_gc_roots) -
       reinterpret_cast<intptr_t>(art_quick_read_barrier_mark_introspection);
   DCHECK_EQ(BAKER_MARK_INTROSPECTION_GC_ROOT_ENTRYPOINT_OFFSET, gc_roots_diff);
+#endif  // ART_USE_RESTRICTED_MODE
   // The register 16, i.e. IP0, is reserved, so there is no art_quick_read_barrier_mark_reg16.
   // We're using the entry to hold a pointer to the introspection entrypoint instead.
   qpoints->SetReadBarrierMarkReg16(is_active ? art_quick_read_barrier_mark_introspection : nullptr);
@@ -213,18 +222,25 @@ void InitEntryPoints(JniEntryPoints* jpoints,
   if (art_flags::always_enable_profile_code()) {
     // These are used for always-on-tracing, currently only supported on arm64
     // devices.
-    qpoints->SetRecordEntryTraceEvent(art_quick_record_entry_trace_event);
-    qpoints->SetRecordExitTraceEvent(art_quick_record_exit_trace_event);
+    qpoints->SetRecordEntryTraceEvent(art_quick_nop_record_entry_trace_event);
+    qpoints->SetRecordExitTraceEvent(art_quick_nop_record_exit_trace_event);
   }
 }
 
-void UpdateLowOverheadTraceEntrypoints(QuickEntryPoints* qpoints, bool enable) {
-  if (enable) {
-    qpoints->SetRecordEntryTraceEvent(art_quick_record_entry_trace_event);
-    qpoints->SetRecordExitTraceEvent(art_quick_record_exit_trace_event);
-  } else {
-    qpoints->SetRecordEntryTraceEvent(art_quick_nop_record_entry_trace_event);
-    qpoints->SetRecordExitTraceEvent(art_quick_nop_record_exit_trace_event);
+void UpdateLowOverheadTraceEntrypoints(QuickEntryPoints* qpoints, LowOverheadTraceType type) {
+  switch (type) {
+    case LowOverheadTraceType::kAllMethods:
+      qpoints->SetRecordEntryTraceEvent(art_quick_record_entry_trace_event);
+      qpoints->SetRecordExitTraceEvent(art_quick_record_exit_trace_event);
+      break;
+    case LowOverheadTraceType::kLongRunningMethods:
+      qpoints->SetRecordEntryTraceEvent(art_quick_record_long_running_entry_trace_event);
+      qpoints->SetRecordExitTraceEvent(art_quick_record_long_running_exit_trace_event);
+      break;
+    case LowOverheadTraceType::kNone:
+      qpoints->SetRecordEntryTraceEvent(art_quick_nop_record_entry_trace_event);
+      qpoints->SetRecordExitTraceEvent(art_quick_nop_record_exit_trace_event);
+      break;
   }
 }
 
diff --git a/runtime/arch/arm64/jni_entrypoints_arm64.S b/runtime/arch/arm64/jni_entrypoints_arm64.S
index 2b0a214b4f..02bf4fa06d 100644
--- a/runtime/arch/arm64/jni_entrypoints_arm64.S
+++ b/runtime/arch/arm64/jni_entrypoints_arm64.S
@@ -27,7 +27,7 @@ ENTRY \name
     .ifnc \arg1, none
         mov x0, \arg1                          // Pass arg1.
     .endif
-    bl     \cxx_name                           // Call cxx_name(...).
+    CALL_SYMBOL \cxx_name                      // Call cxx_name(...).
     // Restore LR and args and return.
     ldr    lr, [sp, #(ALL_ARGS_SIZE + /*padding*/ 8)]
     .cfi_restore lr
@@ -49,7 +49,7 @@ ENTRY \name
     .ifnc \arg2, none
         mov x1, \arg2                          // Pass arg2.
     .endif
-    bl     \cxx_name                           // Call cxx_name(...).
+    CALL_SYMBOL \cxx_name                      // Call cxx_name(...).
     // Restore return registers and return.
     ldr    d0, [sp, #16]
     ldp    x0, lr, [sp], #32
@@ -68,7 +68,7 @@ ENTRY art_jni_dlsym_lookup_critical_stub
     // Note: 'tbnz' doesn't always have enough range (+/-32KB) to reach art_jni_dlsym_lookup_stub
     // so 'b' (+/-128MB) is used instead.
     tbz  x15, #0, .Lcritical_not_generic_jni
-    b art_jni_dlsym_lookup_stub
+    BRANCH_SYMBOL art_jni_dlsym_lookup_stub
 
 .Lcritical_not_generic_jni:
     // Save args, the hidden arg and caller PC. No CFI needed for args and the hidden arg.
@@ -79,7 +79,7 @@ ENTRY art_jni_dlsym_lookup_critical_stub
     // Call artCriticalNativeFrameSize(method, caller_pc)
     mov   x0, x15  // x0 := method (from hidden arg)
     mov   x1, lr   // x1 := caller_pc
-    bl    artCriticalNativeFrameSize
+    CALL_SYMBOL artCriticalNativeFrameSize
 
     // Move frame size to x14.
     mov   x14, x0
@@ -178,7 +178,7 @@ ENTRY art_jni_dlsym_lookup_critical_stub
 
     // Call artFindNativeMethodRunnable()
     mov   x0, xSELF   // pass Thread::Current()
-    bl    artFindNativeMethodRunnable
+    CALL_SYMBOL artFindNativeMethodRunnable
 
     // Store result in scratch reg.
     mov   x13, x0
@@ -347,9 +347,9 @@ ENTRY art_jni_lock_object_no_inline
     str    lr, [sp, #(ALL_ARGS_SIZE + /*padding*/ 8)]
     .cfi_rel_offset lr, ALL_ARGS_SIZE + /*padding*/ 8
     // Call `artLockObjectFromCode()`.
-    mov    x0, x15                    // Pass the object to lock.
-    mov    x1, xSELF                  // Pass Thread::Current().
-    bl     artLockObjectFromCode      // (Object* obj, Thread*)
+    mov    x0, x15                                   // Pass the object to lock.
+    mov    x1, xSELF                                 // Pass Thread::Current().
+    CALL_SYMBOL artLockObjectFromCode                // (Object* obj, Thread*)
     // Restore return address.
     ldr    lr, [sp, #(ALL_ARGS_SIZE + /*padding*/ 8)]
     .cfi_restore lr
@@ -364,9 +364,9 @@ ENTRY art_jni_lock_object_no_inline
     DECREASE_FRAME (ALL_ARGS_SIZE + /*padding*/ 8 + /*LR*/ 8)
     // Make a call to `artDeliverPendingExceptionFromCode()`.
     // Rely on the JNI transition frame constructed in the JNI stub.
-    mov    x0, xSELF                            // Pass Thread::Current().
-    bl     artDeliverPendingExceptionFromCode   // (Thread*)
-    bl     art_quick_do_long_jump               // (Context*)
+    mov    x0, xSELF                                 // Pass Thread::Current().
+    CALL_SYMBOL artDeliverPendingExceptionFromCode   // (Thread*)
+    CALL_SYMBOL art_quick_do_long_jump               // (Context*)
     brk 0  // Unreached
 END art_jni_lock_object_no_inline
 
diff --git a/runtime/arch/arm64/native_entrypoints_arm64.S b/runtime/arch/arm64/native_entrypoints_arm64.S
index 747e572c97..00558c2f98 100644
--- a/runtime/arch/arm64/native_entrypoints_arm64.S
+++ b/runtime/arch/arm64/native_entrypoints_arm64.S
@@ -62,7 +62,12 @@ ENTRY art_jni_dlsym_lookup_stub
     bic   xIP0, xIP0, #TAGGED_JNI_SP_MASK                 // ArtMethod** sp
     ldr   xIP0, [xIP0]                                    // ArtMethod* method
     ldr   xIP0, [xIP0, #ART_METHOD_ACCESS_FLAGS_OFFSET]   // uint32_t access_flags
+#ifdef ART_USE_RESTRICTED_MODE
+    // Critical native methods are disabled and treated as normal native methods instead.
+    mov   xIP1, #(ACCESS_FLAGS_METHOD_IS_FAST_NATIVE)
+#else
     mov   xIP1, #(ACCESS_FLAGS_METHOD_IS_FAST_NATIVE | ACCESS_FLAGS_METHOD_IS_CRITICAL_NATIVE)
+#endif
     tst   xIP0, xIP1
     b.ne  .Llookup_stub_fast_or_critical_native
     bl    artFindNativeMethod
diff --git a/runtime/arch/arm64/quick_entrypoints_arm64.S b/runtime/arch/arm64/quick_entrypoints_arm64.S
index 1302b5b036..98b7e969fc 100644
--- a/runtime/arch/arm64/quick_entrypoints_arm64.S
+++ b/runtime/arch/arm64/quick_entrypoints_arm64.S
@@ -202,10 +202,10 @@
   ret
 2:
   SETUP_SAVE_EVERYTHING_FRAME
-  mov x2, \is_ref                   // pass if result is a reference
-  mov x1, x0                        // pass the result
-  mov x0, xSELF                     // Thread::Current
-  bl artDeoptimizeIfNeeded
+  mov x2, \is_ref                     // pass if result is a reference
+  mov x1, x0                          // pass the result
+  mov x0, xSELF                       // Thread::Current
+  CALL_SYMBOL artDeoptimizeIfNeeded
 
   CFI_REMEMBER_STATE
   cbnz x0, 3f
@@ -217,7 +217,7 @@
 3:
   // Deoptimize.
   CFI_RESTORE_STATE_AND_DEF_CFA sp, FRAME_SIZE_SAVE_EVERYTHING
-  bl art_quick_do_long_jump         // (Context*)
+  CALL_SYMBOL art_quick_do_long_jump  // (Context*)
   brk 0  // Unreached
 .endm
 
@@ -234,7 +234,7 @@
   mov x2, \is_ref                                // pass if result is a reference
   mov x1, x0                                     // pass the result
   mov x0, xSELF                                  // Thread::Current
-  bl artDeoptimizeIfNeeded
+  CALL_SYMBOL artDeoptimizeIfNeeded
 
   CFI_REMEMBER_STATE
   cbnz x0, 3f
@@ -246,7 +246,7 @@
 3:
   // Deoptimize.
   CFI_RESTORE_STATE_AND_DEF_CFA sp, FRAME_SIZE_SAVE_EVERYTHING
-  bl art_quick_do_long_jump                      // (Context*)
+  CALL_SYMBOL art_quick_do_long_jump             // (Context*)
   brk 0  // Unreached
 .endm
 
@@ -263,10 +263,10 @@
 .macro NO_ARG_RUNTIME_EXCEPTION c_name, cxx_name
     .extern \cxx_name
 ENTRY \c_name
-    SETUP_SAVE_ALL_CALLEE_SAVES_FRAME // save all registers as basis for long jump context
-    mov x0, xSELF                     // pass Thread::Current
-    bl  \cxx_name                     // \cxx_name(Thread*)
-    bl  art_quick_do_long_jump        // (Context*)
+    SETUP_SAVE_ALL_CALLEE_SAVES_FRAME    // save all registers as basis for long jump context
+    mov x0, xSELF                        // pass Thread::Current
+    CALL_SYMBOL \cxx_name                // \cxx_name(Thread*)
+    CALL_SYMBOL art_quick_do_long_jump   // (Context*)
     brk 0  // Unreached
 END \c_name
 .endm
@@ -274,10 +274,10 @@ END \c_name
 .macro NO_ARG_RUNTIME_EXCEPTION_SAVE_EVERYTHING c_name, cxx_name
     .extern \cxx_name
 ENTRY \c_name
-    SETUP_SAVE_EVERYTHING_FRAME       // save all registers as basis for long jump context
-    mov x0, xSELF                     // pass Thread::Current
-    bl  \cxx_name                     // \cxx_name(Thread*)
-    bl  art_quick_do_long_jump        // (Context*)
+    SETUP_SAVE_EVERYTHING_FRAME          // save all registers as basis for long jump context
+    mov x0, xSELF                        // pass Thread::Current
+    CALL_SYMBOL \cxx_name                // \cxx_name(Thread*)
+    CALL_SYMBOL art_quick_do_long_jump   // (Context*)
     brk 0  // Unreached
 END \c_name
 .endm
@@ -285,10 +285,10 @@ END \c_name
 .macro ONE_ARG_RUNTIME_EXCEPTION c_name, cxx_name
     .extern \cxx_name
 ENTRY \c_name
-    SETUP_SAVE_ALL_CALLEE_SAVES_FRAME // save all registers as basis for long jump context.
-    mov x1, xSELF                     // pass Thread::Current.
-    bl  \cxx_name                     // \cxx_name(arg, Thread*).
-    bl  art_quick_do_long_jump        // (Context*)
+    SETUP_SAVE_ALL_CALLEE_SAVES_FRAME    // save all registers as basis for long jump context.
+    mov x1, xSELF                        // pass Thread::Current.
+    CALL_SYMBOL \cxx_name                // \cxx_name(arg, Thread*).
+    CALL_SYMBOL art_quick_do_long_jump   // (Context*)
     brk 0  // Unreached
 END \c_name
 .endm
@@ -296,10 +296,10 @@ END \c_name
 .macro TWO_ARG_RUNTIME_EXCEPTION_SAVE_EVERYTHING c_name, cxx_name
     .extern \cxx_name
 ENTRY \c_name
-    SETUP_SAVE_EVERYTHING_FRAME       // save all registers as basis for long jump context
-    mov x2, xSELF                     // pass Thread::Current
-    bl  \cxx_name                     // \cxx_name(arg1, arg2, Thread*)
-    bl  art_quick_do_long_jump        // (Context*)
+    SETUP_SAVE_EVERYTHING_FRAME          // save all registers as basis for long jump context
+    mov x2, xSELF                        // pass Thread::Current
+    CALL_SYMBOL \cxx_name                // \cxx_name(arg1, arg2, Thread*)
+    CALL_SYMBOL art_quick_do_long_jump   // (Context*)
     brk 0  // Unreached
 END \c_name
 .endm
@@ -331,8 +331,8 @@ ENTRY art_quick_throw_null_pointer_exception_from_signal
     SETUP_SAVE_EVERYTHING_FRAME_DECREMENTED_SP_SKIP_X29_LR
     mov x0, lr                        // pass the fault address stored in LR by the fault handler.
     mov x1, xSELF                     // pass Thread::Current.
-    bl  artThrowNullPointerExceptionFromSignal  // (arg, Thread*).
-    bl  art_quick_do_long_jump                  // (Context*)
+    CALL_SYMBOL artThrowNullPointerExceptionFromSignal  // (arg, Thread*).
+    CALL_SYMBOL art_quick_do_long_jump                  // (Context*)
     brk 0  // Unreached
 END art_quick_throw_null_pointer_exception_from_signal
 
@@ -386,7 +386,7 @@ NO_ARG_RUNTIME_EXCEPTION art_quick_throw_stack_overflow, artThrowStackOverflowFr
 
     mov    x2, xSELF                      // pass Thread::Current
     mov    x3, sp
-    bl     \cxx_name                      // (method_idx, this, Thread*, SP)
+    CALL_SYMBOL \cxx_name                 // (method_idx, this, Thread*, SP)
     mov    xIP0, x1                       // save Method*->code_
     RESTORE_SAVE_REFS_AND_ARGS_FRAME
     REFRESH_MARKING_REGISTER
@@ -801,7 +801,7 @@ ENTRY art_quick_do_long_jump
     mov x1, sp
     add x2, sp, #ARM64_LONG_JUMP_GPRS_SIZE
 
-    bl artContextCopyForLongJump  // Context* context, uintptr_t* gprs, uintptr_t* fprs
+    CALL_SYMBOL artContextCopyForLongJump  // Context* context, uintptr_t* gprs, uintptr_t* fprs
 
     add x0, sp, #ARM64_LONG_JUMP_GPRS_SIZE
 
@@ -870,9 +870,9 @@ END art_quick_lock_object
     .extern artLockObjectFromCode
 ENTRY art_quick_lock_object_no_inline
     // This is also the slow path for art_quick_lock_object.
-    SETUP_SAVE_REFS_ONLY_FRAME        // save callee saves in case we block
-    mov    x1, xSELF                  // pass Thread::Current
-    bl     artLockObjectFromCode      // (Object* obj, Thread*)
+    SETUP_SAVE_REFS_ONLY_FRAME         // save callee saves in case we block
+    mov    x1, xSELF                   // pass Thread::Current
+    CALL_SYMBOL artLockObjectFromCode  // (Object* obj, Thread*)
     RESTORE_SAVE_REFS_ONLY_FRAME
     REFRESH_MARKING_REGISTER
     RETURN_OR_DEOPT_IF_INT_RESULT_IS_ZERO_OR_DELIVER
@@ -895,9 +895,10 @@ END art_quick_unlock_object
     .extern artUnlockObjectFromCode
 ENTRY art_quick_unlock_object_no_inline
     // This is also the slow path for art_quick_unlock_object.
-    SETUP_SAVE_REFS_ONLY_FRAME        // save callee saves in case exception allocation triggers GC
-    mov    x1, xSELF                  // pass Thread::Current
-    bl     artUnlockObjectFromCode    // (Object* obj, Thread*)
+    SETUP_SAVE_REFS_ONLY_FRAME           // save callee saves in case exception allocation
+                                         // triggers GC
+    mov    x1, xSELF                     // pass Thread::Current
+    CALL_SYMBOL artUnlockObjectFromCode  // (Object* obj, Thread*)
     RESTORE_SAVE_REFS_ONLY_FRAME
     REFRESH_MARKING_REGISTER
     RETURN_OR_DEOPT_IF_INT_RESULT_IS_ZERO_OR_DELIVER
@@ -919,7 +920,7 @@ ENTRY art_quick_check_instance_of
     SAVE_REG xLR, 24
 
     // Call runtime code
-    bl artInstanceOfFromCode
+    CALL_SYMBOL artInstanceOfFromCode
 
     // Restore LR.
     RESTORE_REG xLR, 24
@@ -942,8 +943,8 @@ ENTRY art_quick_check_instance_of
 .Lthrow_class_cast_exception_for_bitstring_check:
     SETUP_SAVE_ALL_CALLEE_SAVES_FRAME // save all registers as basis for long jump context
     mov x2, xSELF                     // pass Thread::Current
-    bl artThrowClassCastExceptionForObject     // (Object*, Class*, Thread*)
-    bl art_quick_do_long_jump                  // (Context*)
+    CALL_SYMBOL artThrowClassCastExceptionForObject  // (Object*, Class*, Thread*)
+    CALL_SYMBOL art_quick_do_long_jump               // (Context*)
     brk 0  // Unreached
 END art_quick_check_instance_of
 
@@ -989,7 +990,7 @@ END art_quick_check_instance_of
     // Save LR in a register preserved by `art_quick_read_barrier_mark_regNN`
     // and unused by the `art_quick_aput_obj`.
     mov x5, lr
-    bl \mark_function
+    CALL_SYMBOL \mark_function
     mov lr, x5                                         // Restore LR.
 .endm
 #else  // USE_BAKER_READ_BARRIER
@@ -1005,7 +1006,7 @@ END art_quick_check_instance_of
         mov x1, \xObj               // pass xObj
     .endif
     mov w2, #\offset                // pass offset
-    bl artReadBarrierSlow           // artReadBarrierSlow(ref, xObj, offset)
+    CALL_SYMBOL artReadBarrierSlow  // artReadBarrierSlow(ref, xObj, offset)
     // No need to unpoison return value in w0, artReadBarrierSlow() would do the unpoisoning.
     .ifnc \wDest, w0
         mov \wDest, w0              // save return value in wDest
@@ -1064,7 +1065,7 @@ ENTRY art_quick_aput_obj
     // Call runtime code
     mov x0, x3              // Heap reference, 32b, "uncompress" = do nothing, already zero-extended
     mov x1, x4              // Heap reference, 32b, "uncompress" = do nothing, already zero-extended
-    bl artIsAssignableFromCode
+    CALL_SYMBOL artIsAssignableFromCode
 
     // Check for exception
     CFI_REMEMBER_STATE
@@ -1091,10 +1092,10 @@ ENTRY art_quick_aput_obj
     CFI_REMEMBER_STATE
 #endif  // defined(USE_READ_BARRIER) && defined(USE_BAKER_READ_BARRIER)
     SETUP_SAVE_ALL_CALLEE_SAVES_FRAME
-    mov x1, x2                      // Pass value.
-    mov x2, xSELF                   // Pass Thread::Current.
-    bl artThrowArrayStoreException  // (Object*, Object*, Thread*).
-    bl art_quick_do_long_jump       // (Context*)
+    mov x1, x2                                         // Pass value.
+    mov x2, xSELF                                      // Pass Thread::Current.
+    CALL_SYMBOL artThrowArrayStoreException            // (Object*, Object*, Thread*).
+    CALL_SYMBOL art_quick_do_long_jump                 // (Context*)
     brk 0  // Unreached
 
 #if defined(USE_READ_BARRIER) && defined(USE_BAKER_READ_BARRIER)
@@ -1134,7 +1135,7 @@ END art_quick_aput_obj
 ENTRY \name
     SETUP_SAVE_REFS_ONLY_FRAME        // save callee saves in case of GC
     mov    x1, xSELF                  // pass Thread::Current
-    bl     \entrypoint                // (uint32_t type_idx, Method* method, Thread*)
+    CALL_SYMBOL \entrypoint           // (uint32_t type_idx, Method* method, Thread*)
     RESTORE_SAVE_REFS_ONLY_FRAME
     REFRESH_MARKING_REGISTER
     \return
@@ -1147,7 +1148,7 @@ END \name
 ENTRY \name
     SETUP_SAVE_REFS_ONLY_FRAME        // save callee saves in case of GC
     mov    x2, xSELF                  // pass Thread::Current
-    bl     \entrypoint                // (uint32_t type_idx, Method* method, Thread*)
+    CALL_SYMBOL \entrypoint           // (uint32_t type_idx, Method* method, Thread*)
     RESTORE_SAVE_REFS_ONLY_FRAME
     REFRESH_MARKING_REGISTER
     \return
@@ -1160,7 +1161,7 @@ END \name
 ENTRY \name
     SETUP_SAVE_REFS_ONLY_FRAME        // save callee saves in case of GC
     mov    x3, xSELF                  // pass Thread::Current
-    bl     \entrypoint
+    CALL_SYMBOL \entrypoint
     RESTORE_SAVE_REFS_ONLY_FRAME
     REFRESH_MARKING_REGISTER
     \return
@@ -1173,7 +1174,7 @@ END \name
 ENTRY \name
     SETUP_SAVE_REFS_ONLY_FRAME        // save callee saves in case of GC
     mov    x4, xSELF                  // pass Thread::Current
-    bl     \entrypoint                //
+    CALL_SYMBOL \entrypoint           //
     RESTORE_SAVE_REFS_ONLY_FRAME
     REFRESH_MARKING_REGISTER
     \return
@@ -1190,7 +1191,7 @@ END \name
 ENTRY \name
     SETUP_SAVE_EVERYTHING_FRAME \runtime_method_offset       // save everything for stack crawl
     mov   x1, xSELF                   // pass Thread::Current
-    bl    \entrypoint                 // (int32_t index, Thread* self)
+    CALL_SYMBOL \entrypoint           // (int32_t index, Thread* self)
     cbz   w0, 1f                      // If result is null, deliver the OOME.
     DEOPT_OR_RESTORE_SAVE_EVERYTHING_FRAME_AND_RETURN_X0 x1, /* is_ref= */ 1
 1:
@@ -1360,7 +1361,7 @@ ENTRY \c_name
 .Lslow_path\c_name:
     SETUP_SAVE_REFS_ONLY_FRAME                      // save callee saves in case of GC
     mov    x1, xSELF                                // pass Thread::Current
-    bl     \cxx_name
+    CALL_SYMBOL \cxx_name
     RESTORE_SAVE_REFS_ONLY_FRAME
     REFRESH_MARKING_REGISTER
     RETURN_OR_DEOPT_IF_RESULT_IS_NON_NULL_OR_DELIVER
@@ -1410,7 +1411,7 @@ ENTRY \name
 .Lslow_path\name:
     SETUP_SAVE_REFS_ONLY_FRAME                 // Save callee saves in case of GC.
     mov    x1, xSELF                           // Pass Thread::Current.
-    bl     \entrypoint                         // (mirror::Class*, Thread*)
+    CALL_SYMBOL \entrypoint                    // (mirror::Class*, Thread*)
     RESTORE_SAVE_REFS_ONLY_FRAME
     REFRESH_MARKING_REGISTER
     RETURN_OR_DEOPT_IF_RESULT_IS_NON_NULL_OR_DELIVER
@@ -1490,7 +1491,7 @@ ENTRY \name
     // x2: Thread* self
     SETUP_SAVE_REFS_ONLY_FRAME        // save callee saves in case of GC
     mov    x2, xSELF                  // pass Thread::Current
-    bl     \entrypoint
+    CALL_SYMBOL \entrypoint
     RESTORE_SAVE_REFS_ONLY_FRAME
     REFRESH_MARKING_REGISTER
     RETURN_OR_DEOPT_IF_RESULT_IS_NON_NULL_OR_DELIVER
@@ -1589,7 +1590,7 @@ ENTRY art_quick_test_suspend
                                         // Save callee saves for stack crawl.
     SETUP_SAVE_EVERYTHING_FRAME RUNTIME_SAVE_EVERYTHING_FOR_SUSPEND_CHECK_METHOD_OFFSET
     mov    x0, xSELF
-    bl     artTestSuspendFromCode       // (Thread*)
+    CALL_SYMBOL artTestSuspendFromCode  // (Thread*)
 
     CFI_REMEMBER_STATE
     cbnz   x0, .Ltest_suspend_deoptimize
@@ -1603,7 +1604,7 @@ ENTRY art_quick_test_suspend
 .Ltest_suspend_deoptimize:
     // Deoptimize.
     CFI_RESTORE_STATE_AND_DEF_CFA sp, FRAME_SIZE_SAVE_EVERYTHING
-    bl     art_quick_do_long_jump       // (Context*)
+    CALL_SYMBOL art_quick_do_long_jump  // (Context*)
     brk 0  // Unreached
 END art_quick_test_suspend
 
@@ -1612,10 +1613,10 @@ END art_quick_test_suspend
      */
     .extern artImplicitSuspendFromCode
 ENTRY art_quick_implicit_suspend
-                                        // Save callee saves for stack crawl.
+                                           // Save callee saves for stack crawl.
     SETUP_SAVE_EVERYTHING_FRAME RUNTIME_SAVE_EVERYTHING_FOR_SUSPEND_CHECK_METHOD_OFFSET
     mov    x0, xSELF
-    bl     artImplicitSuspendFromCode   // (Thread*)
+    CALL_SYMBOL artImplicitSuspendFromCode // (Thread*)
 
     CFI_REMEMBER_STATE
     cbnz   x0, .Limplicit_suspend_deopt
@@ -1628,7 +1629,7 @@ ENTRY art_quick_implicit_suspend
 .Limplicit_suspend_deopt:
     // Deoptimize.
     CFI_RESTORE_STATE_AND_DEF_CFA sp, FRAME_SIZE_SAVE_EVERYTHING
-    bl     art_quick_do_long_jump       // (Context*)
+    CALL_SYMBOL art_quick_do_long_jump     // (Context*)
     brk 0  // Unreached
 END art_quick_implicit_suspend
 
@@ -1640,9 +1641,9 @@ END art_quick_implicit_suspend
      .extern artQuickProxyInvokeHandler
 ENTRY art_quick_proxy_invoke_handler
     SETUP_SAVE_REFS_AND_ARGS_FRAME_WITH_METHOD_IN_X0
-    mov     x2, xSELF                   // pass Thread::Current
-    mov     x3, sp                      // pass SP
-    bl      artQuickProxyInvokeHandler  // (Method* proxy method, receiver, Thread*, SP)
+    mov     x2, xSELF                            // pass Thread::Current
+    mov     x3, sp                               // pass SP
+    CALL_SYMBOL artQuickProxyInvokeHandler       // (Method* proxy method, receiver, Thread*, SP)
     ldr     x2, [xSELF, THREAD_EXCEPTION_OFFSET]
     CFI_REMEMBER_STATE
     cbnz    x2, .Lexception_in_proxy    // success if no exception is pending
@@ -1692,7 +1693,7 @@ ENTRY art_quick_resolution_trampoline
     SETUP_SAVE_REFS_AND_ARGS_FRAME
     mov x2, xSELF
     mov x3, sp
-    bl artQuickResolutionTrampoline  // (called, receiver, Thread*, SP)
+    CALL_SYMBOL artQuickResolutionTrampoline  // (called, receiver, Thread*, SP)
     CFI_REMEMBER_STATE
     cbz x0, 1f
     mov xIP0, x0            // Remember returned code pointer in xIP0.
@@ -1780,7 +1781,7 @@ ENTRY art_quick_generic_jni_trampoline
     mov x0, xSELF   // Thread*
     mov x1, x28     // SP for the managed frame.
     mov x2, sp      // reserved area for arguments and other saved data (up to managed frame)
-    bl artQuickGenericJniTrampoline  // (Thread*, sp)
+    CALL_SYMBOL artQuickGenericJniTrampoline  // (Thread*, sp)
 
     // The C call will have registered the complete save-frame on success.
     // The result of the call is:
@@ -1821,7 +1822,7 @@ ENTRY art_quick_generic_jni_trampoline
     mov x0, xSELF   // Thread register.
     fmov x2, d0     // d0 will contain floating point result, but needs to go into x2
 
-    bl artQuickGenericJniEndTrampoline
+    CALL_SYMBOL artQuickGenericJniEndTrampoline
 
     // Pending exceptions possible.
     ldr x2, [xSELF, THREAD_EXCEPTION_OFFSET]
@@ -1831,7 +1832,8 @@ ENTRY art_quick_generic_jni_trampoline
     mov sp, x28
 
     LOAD_RUNTIME_INSTANCE x1
-    ldrb w1, [x1, #RUN_EXIT_HOOKS_OFFSET_FROM_RUNTIME_INSTANCE]
+    ldr x1, [x1, #RUNTIME_INSTRUMENTATION_OFFSET]
+    ldrb w1, [x1, #INSTRUMENTATION_RUN_EXIT_HOOKS_OFFSET]
     CFI_REMEMBER_STATE
     cbnz w1, .Lcall_method_exit_hook
 .Lcall_method_exit_hook_done:
@@ -1865,14 +1867,14 @@ ENTRY art_quick_generic_jni_trampoline
     CFI_RESTORE_STATE_AND_DEF_CFA x28, FRAME_SIZE_SAVE_REFS_AND_ARGS
     fmov d0, x0
     mov x4, FRAME_SIZE_SAVE_REFS_AND_ARGS
-    bl art_quick_method_exit_hook
+    CALL_SYMBOL art_quick_method_exit_hook
     b .Lcall_method_exit_hook_done
 
 .Lexception_in_native:
     // Move to x1 then sp to please assembler.
     ldr x1, [xSELF, # THREAD_TOP_QUICK_FRAME_OFFSET]
     add sp, x1, #-1  // Remove the GenericJNI tag.
-    bl art_deliver_pending_exception
+    CALL_SYMBOL art_deliver_pending_exception
 END art_quick_generic_jni_trampoline
 
 ENTRY art_deliver_pending_exception
@@ -1895,7 +1897,7 @@ ENTRY art_quick_to_interpreter_bridge
 
     // uint64_t artQuickToInterpreterBridge(mirror::ArtMethod* method, Thread* self,
     //                                      mirror::ArtMethod** sp)
-    bl   artQuickToInterpreterBridge
+    CALL_SYMBOL artQuickToInterpreterBridge
 
     RESTORE_SAVE_REFS_AND_ARGS_FRAME       // TODO: no need to restore arguments in this case.
     REFRESH_MARKING_REGISTER
@@ -1917,9 +1919,9 @@ ONE_ARG_RUNTIME_EXCEPTION art_invoke_obsolete_method_stub, artInvokeObsoleteMeth
     .extern artDeoptimizeFromCompiledCode
 ENTRY art_quick_deoptimize_from_compiled_code
     SETUP_SAVE_EVERYTHING_FRAME
-    mov    x1, xSELF                      // Pass thread.
-    bl     artDeoptimizeFromCompiledCode  // (DeoptimizationKind, Thread*)
-    bl     art_quick_do_long_jump         // (Context*)
+    mov    x1, xSELF                           // Pass thread.
+    CALL_SYMBOL artDeoptimizeFromCompiledCode  // (DeoptimizationKind, Thread*)
+    CALL_SYMBOL art_quick_do_long_jump         // (Context*)
     brk 0  // Unreached
 END art_quick_deoptimize_from_compiled_code
 
@@ -2050,7 +2052,7 @@ ENTRY art_quick_string_builder_append
     SETUP_SAVE_REFS_ONLY_FRAME          // save callee saves in case of GC
     add    x1, sp, #(FRAME_SIZE_SAVE_REFS_ONLY + __SIZEOF_POINTER__)  // pass args
     mov    x2, xSELF                    // pass Thread::Current
-    bl     artStringBuilderAppend       // (uint32_t, const unit32_t*, Thread*)
+    CALL_SYMBOL artStringBuilderAppend  // (uint32_t, const unit32_t*, Thread*)
     RESTORE_SAVE_REFS_ONLY_FRAME
     REFRESH_MARKING_REGISTER
     RETURN_OR_DEOPT_IF_RESULT_IS_NON_NULL_OR_DELIVER
@@ -2117,7 +2119,7 @@ ENTRY \name
     .ifnc \wreg, w0
       mov   w0, \wreg                   // Pass arg1 - obj from `wreg`
     .endif
-    bl    artReadBarrierMark            // artReadBarrierMark(obj)
+    CALL_SYMBOL artReadBarrierMark      // artReadBarrierMark(obj)
     .ifnc \wreg, w0
       mov   \wreg, w0                   // Return result into `wreg`
     .endif
@@ -2251,7 +2253,7 @@ READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg29, w29, x29
 #endif
 .macro INTROSPECTION_ARRAY_LOAD index_reg
     ldr   wIP0, [xIP0, \index_reg, lsl #2]
-    b     art_quick_read_barrier_mark_introspection
+    BRANCH_SYMBOL art_quick_read_barrier_mark_introspection
 .endm
 
 .macro MOV_WIP0_TO_WREG_AND_BL_LR reg
@@ -2291,7 +2293,7 @@ READ_BARRIER_MARK_REG art_quick_read_barrier_mark_reg29, w29, x29
     stp   d30, d31, [sp, #320]
 
     mov   x0, xIP0
-    bl    artReadBarrierMark          // artReadBarrierMark(obj)
+    CALL_SYMBOL artReadBarrierMark // artReadBarrierMark(obj)
     mov   xIP0, x0
 
     // Restore core regs, except x0 and x1 as the return register switch case
@@ -2433,7 +2435,7 @@ ENTRY art_quick_invoke_polymorphic
     mov     x0, x1                      // x0 := receiver
     mov     x1, xSELF                   // x1 := Thread::Current()
     mov     x2, sp                      // x2 := SP
-    bl      artInvokePolymorphic        // artInvokePolymorphic(receiver, thread, save_area)
+    CALL_SYMBOL artInvokePolymorphic    // artInvokePolymorphic(receiver, thread, save_area)
     RESTORE_SAVE_REFS_AND_ARGS_FRAME
     REFRESH_MARKING_REGISTER
     fmov    d0, x0                      // Result is in x0. Copy to floating return register.
@@ -2453,7 +2455,7 @@ ENTRY art_quick_invoke_polymorphic_with_hidden_receiver
     SETUP_SAVE_REFS_AND_ARGS_FRAME      // Save callee saves in case allocation triggers GC.
     mov     x1, xSELF                   // x1 := Thread::Current()
     mov     x2, sp                      // x2 := SP
-    bl      artInvokePolymorphicWithHiddenReceiver // invoke with (receiver, thread, save_area)
+    CALL_SYMBOL artInvokePolymorphicWithHiddenReceiver // invoke with (receiver, thread, save_area)
     RESTORE_SAVE_REFS_AND_ARGS_FRAME
     REFRESH_MARKING_REGISTER
     fmov    d0, x0                      // Result is in x0. Copy to floating return register.
@@ -2466,7 +2468,7 @@ ENTRY art_quick_invoke_custom
                                       // x0 := call_site_idx
     mov     x1, xSELF                 // x1 := Thread::Current()
     mov     x2, sp                    // x2 := SP
-    bl      artInvokeCustom           // artInvokeCustom(call_site_idx, thread, save_area)
+    CALL_SYMBOL artInvokeCustom       // artInvokeCustom(call_site_idx, thread, save_area)
     RESTORE_SAVE_REFS_AND_ARGS_FRAME
     REFRESH_MARKING_REGISTER
     fmov    d0, x0                    // Copy result to double result register.
@@ -2538,7 +2540,7 @@ ENTRY art_quick_compile_optimized
     SETUP_SAVE_EVERYTHING_FRAME
     ldr x0, [sp, #FRAME_SIZE_SAVE_EVERYTHING] // pass ArtMethod
     mov x1, xSELF                             // pass Thread::Current
-    bl     artCompileOptimized                // (ArtMethod*, Thread*)
+    CALL_SYMBOL artCompileOptimized           // (ArtMethod*, Thread*)
     RESTORE_SAVE_EVERYTHING_FRAME
     // We don't need to restore the marking register here, as
     // artCompileOptimized doesn't allow thread suspension.
@@ -2576,6 +2578,87 @@ ENTRY art_quick_record_exit_trace_event
     ret
 END art_quick_record_exit_trace_event
 
+ENTRY art_quick_record_long_running_entry_trace_event
+    ldr xIP1, [xSELF, #TRACE_BUFFER_CURRENT_OFFSET]
+    sub xIP1, xIP1, 16
+    // xIP0 has the trace buffer pointer. This is loaded on the fast path before
+    // checking if we need to call this method. This will be still valid here.
+    cmp xIP0, xIP1
+    bhi .Lhandle_overflow
+    // Store the method pointer
+    str x0, [xIP1, 8]
+    // Store the timestamp with the last bit set to 0 to indicate method entry event
+    mrs xIP0, cntvct_el0
+    bfc xIP0, 0, 1
+    str xIP0, [xIP1]
+    str xIP1, [xSELF, #TRACE_BUFFER_CURRENT_OFFSET]
+    ret
+.Lhandle_overflow:
+    // Call runtime to flush buffer. We expect the frequency of this case to be low. For ANR it
+    // might be interesting to record the past data instead of overwriting with new entries.
+    SETUP_SAVE_EVERYTHING_FRAME
+    ldr x0, [sp, #FRAME_SIZE_SAVE_EVERYTHING] // pass ArtMethod*
+    mov x1, xSELF                             // pass Thread::Current
+    mov x2, 1                                 // set to true for entry events
+    bl  artRecordLongRunningMethodTraceEvent // (ArtMethod*, Thread*, is_entry)
+    RESTORE_SAVE_EVERYTHING_FRAME             // Note: will restore xSELF
+    REFRESH_MARKING_REGISTER
+    ret
+END art_quick_record_long_running_entry_trace_event
+
+ENTRY art_quick_record_long_running_exit_trace_event
+    ldr xIP1, [xSELF, #TRACE_BUFFER_CURRENT_OFFSET]
+    // xIP0 has the trace buffer pointer. This is loaded on the fast path before checking if we
+    // need to call this method. This will be still valid here.
+    // We just need one entry for the method exit. We only record the timestamp. Method will be
+    // available from the corresponding method entry event.
+    cmp xIP0, xIP1
+    bhs .Lhandle_overflow_exit
+    // Get the timestamp of the method exit event.
+    mrs xIP0, cntvct_el0
+    // Load the previous event.
+    ldr xIP1, [xIP1]
+    // If lsb is set for the previous event, then the previous event is an exit event. This means
+    // there was a long running method earlier on the call stack. Record the exit event for this
+    // method to construct the call chain.
+    tst xIP1, 1
+    bne .Lrecord_exit_event
+    // The previous event was an entry event. This exit corresponds to the previous method entry.
+    // Check if the method is long running by getting the current timestamp and comparing with the
+    // previous event's timestamp.
+    mrs xIP0, cntvct_el0
+    sub xIP1, xIP0, xIP1
+    cmp xIP1, #LONG_RUNNING_METHOD_THRESHOLD
+    bhs .Lrecord_exit_event
+    // This wasn't a long running method. Erase the previously recorded method entry event. We
+    // don't need to record entry / exit for this method.
+    ldr xIP1, [xSELF, #TRACE_BUFFER_CURRENT_OFFSET]
+    add xIP1, xIP1, 16
+    str xIP1, [xSELF, #TRACE_BUFFER_CURRENT_OFFSET]
+    ret
+.Lrecord_exit_event:
+    // For method exits we only record the current timestamp. We can infer the method from the
+    // corresponding method entry event.
+    ldr xIP1, [xSELF, #TRACE_BUFFER_CURRENT_OFFSET]
+    sub xIP1, xIP1, 8
+    // Set the lsb of the timestamp to 1 to indicate a method exit event.
+    orr xIP0, xIP0, #1
+    str xIP0, [xIP1]
+    str xIP1, [xSELF, #TRACE_BUFFER_CURRENT_OFFSET]
+    ret
+.Lhandle_overflow_exit:
+    // Call runtime to flush buffer. We expect the frequency of this case to be low. For ANR it
+    // might be interesting to record the past data instead of overwriting with new entries.
+    SETUP_SAVE_EVERYTHING_FRAME
+    ldr xzr, [sp, #FRAME_SIZE_SAVE_EVERYTHING] // pass nullptr. ArtMethod* isn't required for exits
+    mov x1, xSELF                              // pass Thread::Current
+    mov x2, 0                                  // set to false for exit events
+    bl  artRecordLongRunningMethodTraceEvent   // (ArtMethod*, Thread*, is_entry)
+    RESTORE_SAVE_EVERYTHING_FRAME              // Note: will restore xSELF
+    REFRESH_MARKING_REGISTER
+    ret
+END art_quick_record_long_running_exit_trace_event
+
     .extern artMethodEntryHook
 ENTRY art_quick_method_entry_hook
     SETUP_SAVE_EVERYTHING_FRAME
@@ -2583,7 +2666,7 @@ ENTRY art_quick_method_entry_hook
     ldr x0, [sp, #FRAME_SIZE_SAVE_EVERYTHING] // pass ArtMethod*
     mov x1, xSELF                             // pass Thread::Current
     mov x2, sp                                // pass SP
-    bl  artMethodEntryHook                    // (ArtMethod*, Thread*, SP)
+    CALL_SYMBOL artMethodEntryHook            // (ArtMethod*, Thread*, SP)
 
     CFI_REMEMBER_STATE
     cbnz x0, .Lentryhook_deopt
@@ -2596,7 +2679,7 @@ ENTRY art_quick_method_entry_hook
 .Lentryhook_deopt:
     // Deoptimize.
     CFI_RESTORE_STATE_AND_DEF_CFA sp, FRAME_SIZE_SAVE_EVERYTHING
-    bl  art_quick_do_long_jump                // (Context*)
+    CALL_SYMBOL art_quick_do_long_jump        // (Context*)
     brk 0  // Unreached
 END art_quick_method_entry_hook
 
@@ -2609,7 +2692,7 @@ ENTRY art_quick_method_exit_hook
     add x2, sp, #272                          // integer result ptr in kSaveEverything frame
     add x1, sp, #FRAME_SIZE_SAVE_EVERYTHING   // ArtMethod**
     mov x0, xSELF                             // Thread::Current
-    bl  artMethodExitHook                     // (Thread*, ArtMethod**, gpr_res*, fpr_res*,
+    CALL_SYMBOL artMethodExitHook             // (Thread*, ArtMethod**, gpr_res*, fpr_res*,
                                               // frame_size)
 
     CFI_REMEMBER_STATE
@@ -2623,6 +2706,6 @@ ENTRY art_quick_method_exit_hook
 .Lexithook_deopt_or_exception:
     // Deoptimize or exception thrown.
     CFI_RESTORE_STATE_AND_DEF_CFA sp, FRAME_SIZE_SAVE_EVERYTHING
-    bl  art_quick_do_long_jump                // (Context*)
+    CALL_SYMBOL art_quick_do_long_jump        // (Context*)
     brk 0  // Unreached
 END art_quick_method_exit_hook
diff --git a/runtime/arch/memcmp16_test.cc b/runtime/arch/memcmp16_test.cc
index 37aad21a40..21d3f13ab7 100644
--- a/runtime/arch/memcmp16_test.cc
+++ b/runtime/arch/memcmp16_test.cc
@@ -30,8 +30,7 @@ class RandGen {
   uint32_t val_;
 };
 
-class MemCmp16Test : public testing::Test {
-};
+class MemCmp16Test : public ::testing::Test {};
 
 // A simple implementation to compare against.
 // Note: this version is equivalent to the generic one used when no optimized version is available.
diff --git a/runtime/arch/riscv64/entrypoints_init_riscv64.cc b/runtime/arch/riscv64/entrypoints_init_riscv64.cc
index d8424cc3ee..91bf488533 100644
--- a/runtime/arch/riscv64/entrypoints_init_riscv64.cc
+++ b/runtime/arch/riscv64/entrypoints_init_riscv64.cc
@@ -19,6 +19,7 @@
 #include "entrypoints/quick/quick_default_init_entrypoints.h"
 #include "entrypoints/quick/quick_entrypoints.h"
 #include "entrypoints/quick/runtime_entrypoints_list.h"
+#include "trace_profile.h"
 
 namespace art HIDDEN {
 
@@ -144,6 +145,10 @@ void InitEntryPoints(JniEntryPoints* jpoints,
   qpoints->SetIndexOf(art_quick_indexof);
   // TODO(riscv64): More intrinsics.
 
+  // Invoke
+  qpoints->SetInvokePolymorphicWithHiddenReceiver(
+      art_quick_invoke_polymorphic_with_hidden_receiver);
+
   // Read barrier.
   UpdateReadBarrierEntrypoints(qpoints, /*is_active=*/ false);
   qpoints->SetReadBarrierSlow(artReadBarrierSlow);
@@ -151,7 +156,7 @@ void InitEntryPoints(JniEntryPoints* jpoints,
 }
 
 void UpdateLowOverheadTraceEntrypoints([[maybe_unused]] QuickEntryPoints* qpoints,
-                                       [[maybe_unused]] bool enable) {
+                                       [[maybe_unused]] LowOverheadTraceType trace_type) {
   // This is a nop on this architecture. Low overhead tracing is only implemented for ARM64.
 }
 
diff --git a/runtime/arch/riscv64/native_entrypoints_riscv64.S b/runtime/arch/riscv64/native_entrypoints_riscv64.S
index 24c8205c0f..08d5fc04ae 100644
--- a/runtime/arch/riscv64/native_entrypoints_riscv64.S
+++ b/runtime/arch/riscv64/native_entrypoints_riscv64.S
@@ -65,7 +65,12 @@ ENTRY art_jni_dlsym_lookup_stub
     andi t0, t0, ~TAGGED_JNI_SP_MASK             // ArtMethod** sp
     ld   t0, (t0)                                // ArtMethod* method
     lw   t0, ART_METHOD_ACCESS_FLAGS_OFFSET(t0)  // uint32_t access_flags
+#ifdef ART_USE_RESTRICTED_MODE
+    // Critical native methods are disabled and treated as normal native methods instead.
+    li   t1, (ACCESS_FLAGS_METHOD_IS_FAST_NATIVE)
+#else
     li   t1, (ACCESS_FLAGS_METHOD_IS_FAST_NATIVE | ACCESS_FLAGS_METHOD_IS_CRITICAL_NATIVE)
+#endif
     and  t0, t0, t1
     bnez t0, .Llookup_stub_fast_or_critical_native
     call artFindNativeMethod
diff --git a/runtime/arch/riscv64/quick_entrypoints_riscv64.S b/runtime/arch/riscv64/quick_entrypoints_riscv64.S
index 49de49fa8c..e97c786b91 100644
--- a/runtime/arch/riscv64/quick_entrypoints_riscv64.S
+++ b/runtime/arch/riscv64/quick_entrypoints_riscv64.S
@@ -357,7 +357,8 @@ ENTRY art_quick_generic_jni_trampoline
     .cfi_def_cfa_register sp
 
     LOAD_RUNTIME_INSTANCE a1
-    lb   a1, RUN_EXIT_HOOKS_OFFSET_FROM_RUNTIME_INSTANCE(a1)
+    ld   a1, RUNTIME_INSTRUMENTATION_OFFSET(a1)
+    lb   a1, INSTRUMENTATION_RUN_EXIT_HOOKS_OFFSET(a1)
     bnez a1, .Lcall_method_exit_hook
 
 .Lcall_method_exit_hook_done:
@@ -1112,6 +1113,25 @@ ENTRY art_quick_invoke_polymorphic
     RETURN_OR_DELIVER_PENDING_EXCEPTION_REG  t0
 END art_quick_invoke_polymorphic
 
+    /*
+     * Slow path for MethodHandle.invokeExact intrinsic.
+     * That intrinsic has a custom calling convention: the argument allocation doesn't start from
+     * the receiver (MethodHandle) object, but from the argument following it. That's done to match
+     * expectation of the underlying method when MethodHandle targets a method. That also affects
+     * the way arguments are spilled onto the stack.
+     */
+.extern artInvokePolymorphicWithHiddenReceiver
+ENTRY art_quick_invoke_polymorphic_with_hidden_receiver
+                                        // On entry: a0 := receiver
+    SETUP_SAVE_REFS_AND_ARGS_FRAME      // Save callee saves in case allocation triggers GC.
+    mv     a1, xSELF                    // Pass Thread::Current()
+    mv     a2, sp                       // Pass pointer to the saved frame context.
+    call   artInvokePolymorphicWithHiddenReceiver // invoke with (receiver, thread, save_area)
+    RESTORE_SAVE_REFS_AND_ARGS_FRAME
+    fmv.d.x fa0, a0                     // Copy the result also to the FP return register.
+    RETURN_OR_DELIVER_PENDING_EXCEPTION_REG  t0
+END art_quick_invoke_polymorphic_with_hidden_receiver
+
 /*
      * InvokeCustom invocation.
      * On entry:
diff --git a/runtime/arch/stub_test.cc b/runtime/arch/stub_test.cc
index 95d21e0eb7..1dd8c83499 100644
--- a/runtime/arch/stub_test.cc
+++ b/runtime/arch/stub_test.cc
@@ -1565,7 +1565,10 @@ static void TestFields(Thread* self, StubTest* test, Primitive::Type test_type)
   // Play with it...
 
   // Static fields.
-  for (ArtField& f : c->GetSFields()) {
+  for (ArtField& f : c->GetFields()) {
+    if (!f.IsStatic()) {
+      continue;
+    }
     Primitive::Type type = f.GetTypeAsPrimitiveType();
     if (test_type != type) {
      continue;
@@ -1601,7 +1604,10 @@ static void TestFields(Thread* self, StubTest* test, Primitive::Type test_type)
   }
 
   // Instance fields.
-  for (ArtField& f : c->GetIFields()) {
+  for (ArtField& f : c->GetFields()) {
+    if (f.IsStatic()) {
+      continue;
+    }
     Primitive::Type type = f.GetTypeAsPrimitiveType();
     if (test_type != type) {
       continue;
diff --git a/runtime/arch/x86/entrypoints_init_x86.cc b/runtime/arch/x86/entrypoints_init_x86.cc
index 237a55ce9d..c38accb5fe 100644
--- a/runtime/arch/x86/entrypoints_init_x86.cc
+++ b/runtime/arch/x86/entrypoints_init_x86.cc
@@ -23,6 +23,7 @@
 #include "entrypoints/quick/quick_entrypoints.h"
 #include "entrypoints/runtime_asm_entrypoints.h"
 #include "interpreter/interpreter.h"
+#include "trace_profile.h"
 
 namespace art HIDDEN {
 
@@ -129,7 +130,7 @@ void InitEntryPoints(JniEntryPoints* jpoints,
 }
 
 void UpdateLowOverheadTraceEntrypoints([[maybe_unused]] QuickEntryPoints* qpoints,
-                                       [[maybe_unused]] bool enable) {
+                                       [[maybe_unused]] LowOverheadTraceType trace_type) {
   // This is a nop on this architecture. Low overhead tracing is only implemented for ARM64.
 }
 
diff --git a/runtime/arch/x86/native_entrypoints_x86.S b/runtime/arch/x86/native_entrypoints_x86.S
index 9d1c41a069..a676b0f664 100644
--- a/runtime/arch/x86/native_entrypoints_x86.S
+++ b/runtime/arch/x86/native_entrypoints_x86.S
@@ -65,8 +65,13 @@ DEFINE_FUNCTION art_jni_dlsym_lookup_stub
     movl THREAD_TOP_QUICK_FRAME_OFFSET(%eax), %eax   // uintptr_t tagged_quick_frame
     andl LITERAL(TAGGED_JNI_SP_MASK_TOGGLED32), %eax // ArtMethod** sp
     movl (%eax), %eax                                // ArtMethod* method
+#ifdef ART_USE_RESTRICTED_MODE
+    // Critical native methods are disabled and treated as normal native methods instead.
+    testl LITERAL(ACCESS_FLAGS_METHOD_IS_FAST_NATIVE), ART_METHOD_ACCESS_FLAGS_OFFSET(%eax)
+#else
     testl LITERAL(ACCESS_FLAGS_METHOD_IS_FAST_NATIVE | ACCESS_FLAGS_METHOD_IS_CRITICAL_NATIVE), \
           ART_METHOD_ACCESS_FLAGS_OFFSET(%eax)
+#endif
     jne .Llookup_stub_fast_or_critical_native
     call SYMBOL(artFindNativeMethod)  // (Thread*)
     jmp .Llookup_stub_continue
diff --git a/runtime/arch/x86/quick_entrypoints_x86.S b/runtime/arch/x86/quick_entrypoints_x86.S
index 1078480c8c..1a3539c66d 100644
--- a/runtime/arch/x86/quick_entrypoints_x86.S
+++ b/runtime/arch/x86/quick_entrypoints_x86.S
@@ -1721,7 +1721,8 @@ DEFINE_FUNCTION art_quick_generic_jni_trampoline
     punpckldq %xmm1, %xmm0
 
     LOAD_RUNTIME_INSTANCE ebx
-    cmpb MACRO_LITERAL(0), RUN_EXIT_HOOKS_OFFSET_FROM_RUNTIME_INSTANCE(%ebx)
+    movl RUNTIME_INSTRUMENTATION_OFFSET(%ebx), %ebx
+    cmpb MACRO_LITERAL(0), INSTRUMENTATION_RUN_EXIT_HOOKS_OFFSET(%ebx)
     CFI_REMEMBER_STATE
     jne .Lcall_method_exit_hook
 .Lcall_method_exit_hook_done:
diff --git a/runtime/arch/x86_64/entrypoints_init_x86_64.cc b/runtime/arch/x86_64/entrypoints_init_x86_64.cc
index c00ecbd203..7502174821 100644
--- a/runtime/arch/x86_64/entrypoints_init_x86_64.cc
+++ b/runtime/arch/x86_64/entrypoints_init_x86_64.cc
@@ -26,6 +26,7 @@
 #include "entrypoints/quick/runtime_entrypoints_list.h"
 #include "entrypoints/runtime_asm_entrypoints.h"
 #include "interpreter/interpreter.h"
+#include "trace_profile.h"
 
 namespace art HIDDEN {
 
@@ -148,7 +149,7 @@ void InitEntryPoints(JniEntryPoints* jpoints,
 }
 
 void UpdateLowOverheadTraceEntrypoints([[maybe_unused]] QuickEntryPoints* qpoints,
-                                       [[maybe_unused]] bool enable) {
+                                       [[maybe_unused]] LowOverheadTraceType trace_tpe) {
   // This is a nop on this architecture. Low overhead tracing is only implemented for ARM64.
 }
 
diff --git a/runtime/arch/x86_64/native_entrypoints_x86_64.S b/runtime/arch/x86_64/native_entrypoints_x86_64.S
index 12194ef97c..b42981f030 100644
--- a/runtime/arch/x86_64/native_entrypoints_x86_64.S
+++ b/runtime/arch/x86_64/native_entrypoints_x86_64.S
@@ -73,8 +73,14 @@ DEFINE_FUNCTION art_jni_dlsym_lookup_stub
     movq THREAD_TOP_QUICK_FRAME_OFFSET(%rdi), %rax   // uintptr_t tagged_quick_frame
     andq LITERAL(TAGGED_JNI_SP_MASK_TOGGLED64), %rax // ArtMethod** sp
     movq (%rax), %rax                                // ArtMethod* method
+#ifdef ART_USE_RESTRICTED_MODE
+    // Critical native methods are disabled and treated as normal native methods instead.
+    testl LITERAL(ACCESS_FLAGS_METHOD_IS_FAST_NATIVE), \
+          ART_METHOD_ACCESS_FLAGS_OFFSET(%rax)
+#else
     testl LITERAL(ACCESS_FLAGS_METHOD_IS_FAST_NATIVE | ACCESS_FLAGS_METHOD_IS_CRITICAL_NATIVE), \
           ART_METHOD_ACCESS_FLAGS_OFFSET(%rax)
+#endif
     jne .Llookup_stub_fast_or_critical_native
     call SYMBOL(artFindNativeMethod)  // (Thread*)
     jmp .Llookup_stub_continue
diff --git a/runtime/arch/x86_64/quick_entrypoints_x86_64.S b/runtime/arch/x86_64/quick_entrypoints_x86_64.S
index 4d00022ff8..96c152492e 100644
--- a/runtime/arch/x86_64/quick_entrypoints_x86_64.S
+++ b/runtime/arch/x86_64/quick_entrypoints_x86_64.S
@@ -1560,7 +1560,8 @@ DEFINE_FUNCTION art_quick_generic_jni_trampoline
     movq %rax, %xmm0
 
     LOAD_RUNTIME_INSTANCE rcx
-    cmpb MACRO_LITERAL(0), RUN_EXIT_HOOKS_OFFSET_FROM_RUNTIME_INSTANCE(%rcx)
+    movq RUNTIME_INSTRUMENTATION_OFFSET(%rcx), %rcx
+    cmpb MACRO_LITERAL(0), INSTRUMENTATION_RUN_EXIT_HOOKS_OFFSET(%rcx)
     CFI_REMEMBER_STATE
     jne .Lcall_method_exit_hook
 .Lcall_method_exit_hook_done:
diff --git a/runtime/art_field-inl.h b/runtime/art_field-inl.h
index dd86e68164..8557b13ae6 100644
--- a/runtime/art_field-inl.h
+++ b/runtime/art_field-inl.h
@@ -425,19 +425,20 @@ inline ObjPtr<mirror::String> ArtField::ResolveNameString() {
 template <bool kExactOffset>
 static inline ArtField* FindFieldWithOffset(
     const IterationRange<StrideIterator<ArtField>>& fields,
-    uint32_t field_offset) REQUIRES_SHARED(Locks::mutator_lock_) {
+    uint32_t field_offset,
+    bool is_static) REQUIRES_SHARED(Locks::mutator_lock_) {
   for (ArtField& field : fields) {
-    if (kExactOffset) {
-      if (field.GetOffset().Uint32Value() == field_offset) {
-        return &field;
-      }
-    } else {
-      const uint32_t offset = field.GetOffset().Uint32Value();
-      Primitive::Type type = field.GetTypeAsPrimitiveType();
-      const size_t field_size = Primitive::ComponentSize(type);
-      DCHECK_GT(field_size, 0u);
-      if (offset <= field_offset && field_offset < offset + field_size) {
+    if (field.IsStatic() == is_static) {
+      if (kExactOffset && field.GetOffset().Uint32Value() == field_offset) {
         return &field;
+      } else {
+        const uint32_t offset = field.GetOffset().Uint32Value();
+        Primitive::Type type = field.GetTypeAsPrimitiveType();
+        const size_t field_size = Primitive::ComponentSize(type);
+        DCHECK_GT(field_size, 0u);
+        if (offset <= field_offset && field_offset < offset + field_size) {
+          return &field;
+        }
       }
     }
   }
@@ -448,7 +449,8 @@ template <bool kExactOffset, VerifyObjectFlags kVerifyFlags, ReadBarrierOption k
 inline ArtField* ArtField::FindInstanceFieldWithOffset(ObjPtr<mirror::Class> klass,
                                                        uint32_t field_offset) {
   DCHECK(klass != nullptr);
-  ArtField* field = FindFieldWithOffset<kExactOffset>(klass->GetIFields(), field_offset);
+  ArtField* field = FindFieldWithOffset<kExactOffset>(
+      klass->GetFields(), field_offset, /* is_static= */ false);
   if (field != nullptr) {
     return field;
   }
@@ -464,7 +466,7 @@ template <bool kExactOffset>
 inline ArtField* ArtField::FindStaticFieldWithOffset(ObjPtr<mirror::Class> klass,
                                                      uint32_t field_offset) {
   DCHECK(klass != nullptr);
-  return FindFieldWithOffset<kExactOffset>(klass->GetSFields(), field_offset);
+  return FindFieldWithOffset<kExactOffset>(klass->GetFields(), field_offset, /* is_static= */ true);
 }
 
 inline ObjPtr<mirror::ClassLoader> ArtField::GetClassLoader() {
diff --git a/runtime/art_method-inl.h b/runtime/art_method-inl.h
index 05d819577b..476c894cdb 100644
--- a/runtime/art_method-inl.h
+++ b/runtime/art_method-inl.h
@@ -481,6 +481,10 @@ inline const dex::ProtoId& ArtMethod::GetPrototype() {
   return dex_file->GetMethodPrototype(dex_file->GetMethodId(GetDexMethodIndex()));
 }
 
+inline const dex::ProtoIndex ArtMethod::GetProtoIndex() {
+  return GetDexFile()->GetIndexForProtoId(GetPrototype());
+}
+
 inline const dex::TypeList* ArtMethod::GetParameterTypeList() {
   DCHECK(!IsProxyMethod());
   const DexFile* dex_file = GetDexFile();
@@ -780,11 +784,6 @@ inline uint32_t ArtMethod::GetImtIndex() {
   }
 }
 
-inline void ArtMethod::CalculateAndSetImtIndex() {
-  DCHECK(IsAbstract()) << PrettyMethod();
-  imt_index_ = ImTable::GetImtIndex(this);
-}
-
 }  // namespace art
 
 #endif  // ART_RUNTIME_ART_METHOD_INL_H_
diff --git a/runtime/art_method.cc b/runtime/art_method.cc
index a03df5cc2f..856a3d6d44 100644
--- a/runtime/art_method.cc
+++ b/runtime/art_method.cc
@@ -200,17 +200,17 @@ void ArtMethod::ThrowInvocationTimeError(ObjPtr<mirror::Object> receiver) {
             ThrowIllegalAccessErrorForImplementingMethod(receiver->GetClass(), np_method, this);
             return;
           } else if (np_method->IsAbstract()) {
-            ThrowAbstractMethodError(this);
+            ThrowAbstractMethodError(this, receiver);
             return;
           }
         }
       }
       current = current->GetSuperClass();
     }
-    ThrowAbstractMethodError(this);
+    ThrowAbstractMethodError(this, receiver);
   } else {
     DCHECK(IsAbstract());
-    ThrowAbstractMethodError(this);
+    ThrowAbstractMethodError(this, receiver);
   }
 }
 
@@ -912,15 +912,6 @@ const char* ArtMethod::GetRuntimeMethodName() {
   }
 }
 
-void ArtMethod::SetCodeItem(const dex::CodeItem* code_item, bool is_compact_dex_code_item) {
-  DCHECK(HasCodeItem());
-  // We mark the lowest bit for the interpreter to know whether it's executing a
-  // method in a compact or standard dex file.
-  uintptr_t data =
-      reinterpret_cast<uintptr_t>(code_item) | (is_compact_dex_code_item ? 1 : 0);
-  SetDataPtrSize(reinterpret_cast<void*>(data), kRuntimePointerSize);
-}
-
 // AssertSharedHeld doesn't work in GetAccessFlags, so use a NO_THREAD_SAFETY_ANALYSIS helper.
 // TODO: Figure out why ASSERT_SHARED_CAPABILITY doesn't work.
 template <ReadBarrierOption kReadBarrierOption>
diff --git a/runtime/art_method.h b/runtime/art_method.h
index ee11328385..c941696c29 100644
--- a/runtime/art_method.h
+++ b/runtime/art_method.h
@@ -31,6 +31,7 @@
 #include "base/pointer_size.h"
 #include "base/runtime_debug.h"
 #include "dex/dex_file_structs.h"
+#include "dex/dex_file_types.h"
 #include "dex/modifiers.h"
 #include "dex/primitive.h"
 #include "interpreter/mterp/nterp.h"
@@ -470,12 +471,19 @@ class EXPORT ArtMethod final {
     return IsCriticalNative(GetAccessFlags());
   }
 
-  static bool IsCriticalNative(uint32_t access_flags) {
+  static bool IsCriticalNative([[maybe_unused]] uint32_t access_flags) {
+#ifdef ART_USE_RESTRICTED_MODE
+    // Return false to treat all critical native methods as normal native methods instead, i.e.:
+    // will use the generic JNI trampoline instead.
+    // TODO(Simulator): support critical native methods
+    return false;
+#else
     // The presence of the annotation is checked by ClassLinker and recorded in access flags.
     // The kAccCriticalNative flag value is used with a different meaning for non-native methods,
     // so we need to check the kAccNative flag as well.
     constexpr uint32_t mask = kAccCriticalNative | kAccNative;
     return (access_flags & mask) == mask;
+#endif
   }
 
   // Returns true if the method is managed (not native).
@@ -906,18 +914,20 @@ class EXPORT ArtMethod final {
   }
 
   bool HasCodeItem() REQUIRES_SHARED(Locks::mutator_lock_) {
-    uint32_t access_flags = GetAccessFlags();
+    return NeedsCodeItem(GetAccessFlags()) && !IsRuntimeMethod() && !IsProxyMethod();
+  }
+
+  static bool NeedsCodeItem(uint32_t access_flags) {
     return !IsNative(access_flags) &&
            !IsAbstract(access_flags) &&
-           !IsDefaultConflicting(access_flags) &&
-           !IsRuntimeMethod() &&
-           !IsProxyMethod();
+           !IsDefaultConflicting(access_flags);
   }
 
-  // We need to explicitly indicate whether the code item is obtained from the compact dex file,
-  // because in JVMTI, we obtain the code item from the standard dex file to update the method.
-  void SetCodeItem(const dex::CodeItem* code_item, bool is_compact_dex_code_item)
-      REQUIRES_SHARED(Locks::mutator_lock_);
+  void SetCodeItem(const dex::CodeItem* code_item)
+      REQUIRES_SHARED(Locks::mutator_lock_) {
+    DCHECK(HasCodeItem());
+    SetDataPtrSize(code_item, kRuntimePointerSize);
+  }
 
   // Is this a hand crafted method used for something like describing callee saves?
   bool IsCalleeSaveMethod() REQUIRES_SHARED(Locks::mutator_lock_);
@@ -966,6 +976,8 @@ class EXPORT ArtMethod final {
 
   const dex::ProtoId& GetPrototype() REQUIRES_SHARED(Locks::mutator_lock_);
 
+  const dex::ProtoIndex GetProtoIndex() REQUIRES_SHARED(Locks::mutator_lock_);
+
   const dex::TypeList* GetParameterTypeList() REQUIRES_SHARED(Locks::mutator_lock_);
 
   const char* GetDeclaringClassSourceFile() REQUIRES_SHARED(Locks::mutator_lock_);
@@ -1037,7 +1049,13 @@ class EXPORT ArtMethod final {
 
   ALWAYS_INLINE uint32_t GetImtIndex() REQUIRES_SHARED(Locks::mutator_lock_);
 
-  void CalculateAndSetImtIndex() REQUIRES_SHARED(Locks::mutator_lock_);
+  void SetImtIndex(uint16_t imt_index) REQUIRES_SHARED(Locks::mutator_lock_) {
+    imt_index_ = imt_index;
+  }
+
+  void SetHotnessCount(uint16_t hotness_count) REQUIRES_SHARED(Locks::mutator_lock_) {
+    hotness_count_ = hotness_count;
+  }
 
   static constexpr MemberOffset HotnessCountOffset() {
     return MemberOffset(OFFSETOF_MEMBER(ArtMethod, hotness_count_));
@@ -1126,7 +1144,8 @@ class EXPORT ArtMethod final {
     // Non-abstract methods: The hotness we measure for this method. Not atomic,
     // as we allow missing increments: if the method is hot, we will see it eventually.
     uint16_t hotness_count_;
-    // Abstract methods: IMT index.
+    // Abstract interface methods: IMT index.
+    // Abstract class (non-interface) methods: Unused (zero-initialized).
     uint16_t imt_index_;
   };
 
diff --git a/runtime/base/atomic_pair.h b/runtime/base/atomic_pair.h
index 8ba8faf021..21a561716c 100644
--- a/runtime/base/atomic_pair.h
+++ b/runtime/base/atomic_pair.h
@@ -23,6 +23,7 @@
 #include <type_traits>
 
 #include "base/macros.h"
+#include "base/time_utils.h"
 
 namespace art HIDDEN {
 
@@ -39,6 +40,8 @@ namespace art HIDDEN {
 static constexpr uint64_t kSeqMask = (0xFFFFFFFFull << 32);
 static constexpr uint64_t kSeqLock = (0x80000000ull << 32);
 static constexpr uint64_t kSeqIncr = (0x00000001ull << 32);
+static constexpr uint kAtomicPairMaxSpins = 10'000u;
+static constexpr uint kAtomicPairSleepNanos = 5'000u;
 
 // std::pair<> is not trivially copyable and as such it is unsuitable for atomic operations.
 template <typename IntType>
@@ -69,7 +72,7 @@ ALWAYS_INLINE static inline void AtomicPairStoreRelease(AtomicPair<IntType>* pai
 ALWAYS_INLINE static inline AtomicPair<uint64_t> AtomicPairLoadAcquire(AtomicPair<uint64_t>* pair) {
   auto* key_ptr = reinterpret_cast<std::atomic_uint64_t*>(&pair->key);
   auto* val_ptr = reinterpret_cast<std::atomic_uint64_t*>(&pair->val);
-  while (true) {
+  for (uint i = 0;; ++i) {
     uint64_t key0 = key_ptr->load(std::memory_order_acquire);
     uint64_t val = val_ptr->load(std::memory_order_acquire);
     uint64_t key1 = key_ptr->load(std::memory_order_relaxed);
@@ -77,6 +80,9 @@ ALWAYS_INLINE static inline AtomicPair<uint64_t> AtomicPairLoadAcquire(AtomicPai
     if (LIKELY((key0 & kSeqLock) == 0 && key0 == key1)) {
       return {key, val};
     }
+    if (UNLIKELY(i > kAtomicPairMaxSpins)) {
+      NanoSleep(kAtomicPairSleepNanos);
+    }
   }
 }
 
@@ -86,9 +92,15 @@ ALWAYS_INLINE static inline void AtomicPairStoreRelease(AtomicPair<uint64_t>* pa
   auto* key_ptr = reinterpret_cast<std::atomic_uint64_t*>(&pair->key);
   auto* val_ptr = reinterpret_cast<std::atomic_uint64_t*>(&pair->val);
   uint64_t key = key_ptr->load(std::memory_order_relaxed);
-  do {
+  for (uint i = 0;; ++i) {
     key &= ~kSeqLock;  // Ensure that the CAS below fails if the lock bit is already set.
-  } while (!key_ptr->compare_exchange_weak(key, key | kSeqLock));
+    if (LIKELY(key_ptr->compare_exchange_weak(key, key | kSeqLock))) {
+      break;
+    }
+    if (UNLIKELY(i > kAtomicPairMaxSpins)) {
+      NanoSleep(kAtomicPairSleepNanos);
+    }
+  }
   key = (((key & kSeqMask) + kSeqIncr) & ~kSeqLock) | (value.key & ~kSeqMask);
   val_ptr->store(value.val, std::memory_order_release);
   key_ptr->store(key, std::memory_order_release);
diff --git a/runtime/base/mutex.cc b/runtime/base/mutex.cc
index 26a665bd41..9d7b20052b 100644
--- a/runtime/base/mutex.cc
+++ b/runtime/base/mutex.cc
@@ -1023,6 +1023,7 @@ void ReaderWriterMutex::WakeupToRespondToEmptyCheckpoint() {
 
 ConditionVariable::ConditionVariable(const char* name, Mutex& guard)
     : name_(name), guard_(guard) {
+  DCHECK(name != nullptr);
 #if ART_USE_FUTEXES
   DCHECK_EQ(0, sequence_.load(std::memory_order_relaxed));
   num_waiters_ = 0;
@@ -1120,7 +1121,7 @@ void ConditionVariable::WaitHoldingLocks(Thread* self) {
     // EAGAIN == EWOULDBLK, so we let the caller try again.
     // EINTR implies a signal was sent to this thread.
     if ((errno != EINTR) && (errno != EAGAIN)) {
-      PLOG(FATAL) << "futex wait failed for " << name_;
+      PLOG(FATAL) << "futex wait failed for " << name_ << ": " << strerror(errno);
     }
   }
   SleepIfRuntimeDeleted(self);
diff --git a/runtime/base/mutex.h b/runtime/base/mutex.h
index 9185f79ab7..ca126e6454 100644
--- a/runtime/base/mutex.h
+++ b/runtime/base/mutex.h
@@ -526,9 +526,10 @@ class SCOPED_CAPABILITY MutexLock {
 
 // Pretend to acquire a mutex for checking purposes, without actually doing so. Use with
 // extreme caution when it is known the condition that the mutex would guard against cannot arise.
+template <typename T>
 class SCOPED_CAPABILITY FakeMutexLock {
  public:
-  explicit FakeMutexLock(Mutex& mu) ACQUIRE(mu) NO_THREAD_SAFETY_ANALYSIS {}
+  explicit FakeMutexLock(T& mu) ACQUIRE(mu) NO_THREAD_SAFETY_ANALYSIS {}
 
   ~FakeMutexLock() RELEASE() NO_THREAD_SAFETY_ANALYSIS {}
 
diff --git a/runtime/class_linker-inl.h b/runtime/class_linker-inl.h
index 056b26b2d2..788e88b9ed 100644
--- a/runtime/class_linker-inl.h
+++ b/runtime/class_linker-inl.h
@@ -363,7 +363,10 @@ inline ArtField* ClassLinker::ResolveField(uint32_t field_idx,
   Thread::PoisonObjectPointersIfDebug();
   ObjPtr<mirror::DexCache> dex_cache = referrer->GetDexCache();
   ArtField* resolved_field = dex_cache->GetResolvedField(field_idx);
-  if (UNLIKELY(resolved_field == nullptr)) {
+  // If `resolved_field->IsStatic()` is different than `is_static` we know that we will return
+  // nullptr. In this case we still fall into the if case below and make the call in order to throw
+  // the right exception.
+  if (UNLIKELY(resolved_field == nullptr || resolved_field->IsStatic() != is_static)) {
     StackHandleScope<2> hs(Thread::Current());
     referrer = referrer->GetInterfaceMethodIfProxy(image_pointer_size_);
     ObjPtr<mirror::Class> referring_class = referrer->GetDeclaringClass();
@@ -385,7 +388,10 @@ inline ArtField* ClassLinker::ResolveField(uint32_t field_idx,
   DCHECK(!Thread::Current()->IsExceptionPending()) << Thread::Current()->GetException()->Dump();
   ArtField* resolved = dex_cache->GetResolvedField(field_idx);
   Thread::PoisonObjectPointersIfDebug();
-  if (resolved != nullptr) {
+
+  // If `resolved->IsStatic()` is different than `is_static` we know that we will return
+  // nullptr. In this case we still continue forward in order to throw the right exception.
+  if (resolved != nullptr && resolved->IsStatic() == is_static) {
     return resolved;
   }
   const DexFile& dex_file = *dex_cache->GetDexFile();
@@ -398,7 +404,7 @@ inline ArtField* ClassLinker::ResolveField(uint32_t field_idx,
 
   // Look for the field again in case the type resolution updated the cache.
   resolved = dex_cache->GetResolvedField(field_idx);
-  if (resolved != nullptr) {
+  if (resolved != nullptr && resolved->IsStatic() == is_static) {
     return resolved;
   }
 
diff --git a/runtime/class_linker.cc b/runtime/class_linker.cc
index c2926ac263..b14eaae51b 100644
--- a/runtime/class_linker.cc
+++ b/runtime/class_linker.cc
@@ -92,11 +92,14 @@
 #include "gc/space/image_space.h"
 #include "gc/space/space-inl.h"
 #include "gc_root-inl.h"
+#include "handle.h"
 #include "handle_scope-inl.h"
 #include "hidden_api.h"
 #include "imt_conflict_table.h"
 #include "imtable-inl.h"
+#include "instrumentation-inl.h"
 #include "intern_table-inl.h"
+#include "intern_table.h"
 #include "interpreter/interpreter.h"
 #include "interpreter/mterp/nterp.h"
 #include "jit/debugger_interface.h"
@@ -228,7 +231,7 @@ static void UpdateClassAfterVerification(Handle<mirror::Class> klass,
   if (interpreter::CanRuntimeUseNterp()) {
     for (ArtMethod& m : klass->GetMethods(pointer_size)) {
       if (class_linker->IsQuickToInterpreterBridge(m.GetEntryPointFromQuickCompiledCode())) {
-        runtime->GetInstrumentation()->InitializeMethodsCode(&m, /*aot_code=*/nullptr);
+        runtime->GetInstrumentation()->ReinitializeMethodsCode(&m);
       }
     }
   }
@@ -1092,36 +1095,6 @@ void ClassLinker::FinishInit(Thread* self) {
 
   CreateStringInitBindings(self, this);
 
-  // Let the heap know some key offsets into java.lang.ref instances
-  // Note: we hard code the field indexes here rather than using FindInstanceField
-  // as the types of the field can't be resolved prior to the runtime being
-  // fully initialized
-  StackHandleScope<3> hs(self);
-  Handle<mirror::Class> java_lang_ref_Reference =
-      hs.NewHandle(GetClassRoot<mirror::Reference>(this));
-  Handle<mirror::Class> java_lang_ref_FinalizerReference =
-      hs.NewHandle(FindSystemClass(self, "Ljava/lang/ref/FinalizerReference;"));
-
-  ArtField* pendingNext = java_lang_ref_Reference->GetInstanceField(0);
-  CHECK_STREQ(pendingNext->GetName(), "pendingNext");
-  CHECK_STREQ(pendingNext->GetTypeDescriptor(), "Ljava/lang/ref/Reference;");
-
-  ArtField* queue = java_lang_ref_Reference->GetInstanceField(1);
-  CHECK_STREQ(queue->GetName(), "queue");
-  CHECK_STREQ(queue->GetTypeDescriptor(), "Ljava/lang/ref/ReferenceQueue;");
-
-  ArtField* queueNext = java_lang_ref_Reference->GetInstanceField(2);
-  CHECK_STREQ(queueNext->GetName(), "queueNext");
-  CHECK_STREQ(queueNext->GetTypeDescriptor(), "Ljava/lang/ref/Reference;");
-
-  ArtField* referent = java_lang_ref_Reference->GetInstanceField(3);
-  CHECK_STREQ(referent->GetName(), "referent");
-  CHECK_STREQ(referent->GetTypeDescriptor(), "Ljava/lang/Object;");
-
-  ArtField* zombie = java_lang_ref_FinalizerReference->GetInstanceField(2);
-  CHECK_STREQ(zombie->GetName(), "zombie");
-  CHECK_STREQ(zombie->GetTypeDescriptor(), "Ljava/lang/Object;");
-
   // ensure all class_roots_ are initialized
   for (size_t i = 0; i < static_cast<size_t>(ClassRoot::kMax); i++) {
     ClassRoot class_root = static_cast<ClassRoot>(i);
@@ -1143,6 +1116,7 @@ void ClassLinker::FinishInit(Thread* self) {
   // ensure that the class will be initialized.
   if (kMemoryToolIsAvailable && !Runtime::Current()->IsAotCompiler()) {
     ObjPtr<mirror::Class> soe_klass = FindSystemClass(self, "Ljava/lang/StackOverflowError;");
+    StackHandleScope<1> hs(self);
     if (soe_klass == nullptr || !EnsureInitialized(self, hs.NewHandle(soe_klass), true, true)) {
       // Strange, but don't crash.
       LOG(WARNING) << "Could not prepare StackOverflowError.";
@@ -1725,16 +1699,50 @@ static void VerifyInternedStringReferences(gc::space::ImageSpace* space)
   CHECK_EQ(num_recorded_refs, num_found_refs);
 }
 
+static bool PatchDexCacheLocations(Handle<mirror::ObjectArray<mirror::DexCache>> dex_caches,
+                                   InternTable* intern_table,
+                                   std::string* error_msg) REQUIRES_SHARED(Locks::mutator_lock_) {
+  // Replace the location in the dex cache in the app image (the `--dex-location` passed to
+  // dex2oat) with the actual location if needed.
+  // The actual location is computed by the logic in `OatFileBase::Setup`.
+  // This is needed when the location on device is unknown at compile-time, typically during
+  // Cloud Compilation because the compilation is done on the server and the apk is later
+  // installed on device into `/data/app/<random_string>`.
+  // This is not needed during dexpreopt because the location on device is known to be a certain
+  // location in /system, /product, etc.
+  Thread* self = Thread::Current();
+  StackHandleScope<1> hs(self);
+  MutableHandle<mirror::DexCache> dex_cache = hs.NewHandle<mirror::DexCache>(nullptr);
+  for (auto dex_cache_ptr : dex_caches.Iterate<mirror::DexCache>()) {
+    dex_cache.Assign(dex_cache_ptr);
+    std::string dex_file_location =
+        dex_cache->GetLocation(/*allow_location_mismatch=*/true)->ToModifiedUtf8();
+    const DexFile* dex_file = dex_cache->GetDexFile();
+    if (dex_file_location != dex_file->GetLocation()) {
+      ObjPtr<mirror::String> location = intern_table->InternWeak(dex_file->GetLocation().c_str());
+      if (location == nullptr) {
+        self->AssertPendingOOMException();
+        *error_msg = "Failed to intern string for dex cache location";
+        return false;
+      }
+      dex_cache->SetLocation(location);
+    }
+  }
+  return true;
+}
+
 // new_class_set is the set of classes that were read from the class table section in the image.
 // If there was no class table section, it is null.
 // Note: using a class here to avoid having to make ClassLinker internals public.
 class AppImageLoadingHelper {
  public:
-  static void Update(
+  static bool Update(
       ClassLinker* class_linker,
       gc::space::ImageSpace* space,
       Handle<mirror::ClassLoader> class_loader,
-      Handle<mirror::ObjectArray<mirror::DexCache>> dex_caches)
+      Handle<mirror::ObjectArray<mirror::DexCache>> dex_caches,
+      InternTable* intern_table,
+      std::string* error_msg)
       REQUIRES(!Locks::dex_lock_)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
@@ -1742,11 +1750,13 @@ class AppImageLoadingHelper {
       REQUIRES_SHARED(Locks::mutator_lock_);
 };
 
-void AppImageLoadingHelper::Update(
+bool AppImageLoadingHelper::Update(
     ClassLinker* class_linker,
     gc::space::ImageSpace* space,
     Handle<mirror::ClassLoader> class_loader,
-    Handle<mirror::ObjectArray<mirror::DexCache>> dex_caches)
+    Handle<mirror::ObjectArray<mirror::DexCache>> dex_caches,
+    InternTable* intern_table,
+    std::string* error_msg)
     REQUIRES(!Locks::dex_lock_)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   ScopedTrace app_image_timing("AppImage:Updating");
@@ -1756,6 +1766,9 @@ void AppImageLoadingHelper::Update(
     // the Runtime::LoadAppImageStartupCache() option.
     VerifyInternedStringReferences(space);
   }
+  if (!PatchDexCacheLocations(dex_caches, intern_table, error_msg)) {
+    return false;
+  }
   DCHECK(class_loader.Get() != nullptr);
   Thread* const self = Thread::Current();
   Runtime* const runtime = Runtime::Current();
@@ -1806,6 +1819,8 @@ void AppImageLoadingHelper::Update(
       }
     }, space->Begin(), kRuntimePointerSize);
   }
+
+  return true;
 }
 
 void AppImageLoadingHelper::HandleAppImageStrings(gc::space::ImageSpace* space) {
@@ -1959,6 +1974,13 @@ bool ClassLinker::OpenAndInitImageDexFiles(
 
   for (auto dex_cache : dex_caches.Iterate<mirror::DexCache>()) {
     std::string dex_file_location = dex_cache->GetLocation()->ToModifiedUtf8();
+    // At this point, the location in the dex cache (from `--dex-location` passed to dex2oat) is not
+    // necessarily the actual dex location on device. `OpenOatDexFile` uses the table
+    // `OatFile::oat_dex_files_` to find the dex file. For each dex file, the table contains two
+    // keys corresponding to it, one from the oat header (from `--dex-location` passed to dex2oat)
+    // and the other being the actual dex location on device, unless they are the same. The lookup
+    // is based on the former key. Later, `PatchDexCacheLocations` will replace the location in the
+    // dex cache with the actual dex location, which is the latter key in the table.
     std::unique_ptr<const DexFile> dex_file =
         OpenOatDexFile(oat_file, dex_file_location.c_str(), error_msg);
     if (dex_file == nullptr) {
@@ -2003,10 +2025,7 @@ class ImageChecker final {
       CHECK(class_class != nullptr) << "Null class class " << obj;
       if (obj_klass == class_class) {
         auto klass = obj->AsClass();
-        for (ArtField& field : klass->GetIFields()) {
-          CHECK_EQ(field.GetDeclaringClass<kWithoutReadBarrier>(), klass);
-        }
-        for (ArtField& field : klass->GetSFields()) {
+        for (ArtField& field : klass->GetFields()) {
           CHECK_EQ(field.GetDeclaringClass<kWithoutReadBarrier>(), klass);
         }
         for (ArtMethod& m : klass->GetMethods(kPointerSize)) {
@@ -2221,8 +2240,16 @@ bool ClassLinker::AddImageSpace(gc::space::ImageSpace* space,
         return false;
       }
 
+      const char* oat_apex_versions =
+          oat_header->GetStoreValueByKeyUnsafe(OatHeader::kApexVersionsKey);
+      if (oat_apex_versions == nullptr) {
+        *error_msg = StringPrintf("Missing apex versions in special root in runtime image '%s'",
+                                  space->GetImageLocation().c_str());
+        return false;
+      }
+
       // Validate the apex versions.
-      if (!gc::space::ImageSpace::ValidateApexVersions(*oat_header,
+      if (!gc::space::ImageSpace::ValidateApexVersions(oat_apex_versions,
                                                        runtime->GetApexVersions(),
                                                        space->GetImageLocation(),
                                                        error_msg)) {
@@ -2310,6 +2337,10 @@ bool ClassLinker::AddImageSpace(gc::space::ImageSpace* space,
       }, space->Begin(), image_pointer_size_);
     }
 
+    for (auto dex_cache : dex_caches.Iterate<mirror::DexCache>()) {
+      CHECK(!dex_cache->GetDexFile()->IsCompactDexFile());
+    }
+
     ScopedTrace trace("AppImage:UpdateCodeItemAndNterp");
     bool can_use_nterp = interpreter::CanRuntimeUseNterp();
     uint16_t hotness_threshold = runtime->GetJITOptions()->GetWarmupThreshold();
@@ -2319,7 +2350,7 @@ bool ClassLinker::AddImageSpace(gc::space::ImageSpace* space,
       if (method.HasCodeItem()) {
         const dex::CodeItem* code_item = method.GetDexFile()->GetCodeItem(
             reinterpret_cast32<uint32_t>(method.GetDataPtrSize(image_pointer_size_)));
-        method.SetCodeItem(code_item, method.GetDexFile()->IsCompactDexFile());
+        method.SetCodeItem(code_item);
         // The hotness counter may have changed since we compiled the image, so
         // reset it with the runtime value.
         method.ResetCounter(hotness_threshold);
@@ -2365,7 +2396,10 @@ bool ClassLinker::AddImageSpace(gc::space::ImageSpace* space,
     VLOG(image) << "Adding class table classes took " << PrettyDuration(NanoTime() - start_time2);
   }
   if (app_image) {
-    AppImageLoadingHelper::Update(this, space, class_loader, dex_caches);
+    if (!AppImageLoadingHelper::Update(
+            this, space, class_loader, dex_caches, intern_table_, error_msg)) {
+      return false;
+    }
 
     {
       ScopedTrace trace("AppImage:UpdateClassLoaders");
@@ -3859,44 +3893,6 @@ class ClassLinker::OatClassCodeIterator {
   const uint32_t num_methods_;
 };
 
-inline void ClassLinker::LinkCode(ArtMethod* method,
-                                  uint32_t class_def_method_index,
-                                  /*inout*/ OatClassCodeIterator* occi) {
-  ScopedAssertNoThreadSuspension sants(__FUNCTION__);
-  Runtime* const runtime = Runtime::Current();
-  if (runtime->IsAotCompiler()) {
-    // The following code only applies to a non-compiler runtime.
-    return;
-  }
-
-  // Method shouldn't have already been linked.
-  DCHECK_EQ(method->GetEntryPointFromQuickCompiledCode(), nullptr);
-  DCHECK(!method->GetDeclaringClass()->IsVisiblyInitialized());  // Actually ClassStatus::Idx.
-
-  if (!method->IsInvokable()) {
-    EnsureThrowsInvocationError(this, method);
-    occi->SkipAbstract(class_def_method_index);
-    return;
-  }
-
-  const void* quick_code = occi->GetAndAdvance(class_def_method_index);
-  if (method->IsNative() && quick_code == nullptr) {
-    const void* boot_jni_stub = FindBootJniStub(method);
-    if (boot_jni_stub != nullptr) {
-      // Use boot JNI stub if found.
-      quick_code = boot_jni_stub;
-    }
-  }
-  runtime->GetInstrumentation()->InitializeMethodsCode(method, quick_code);
-
-  if (method->IsNative()) {
-    // Set up the dlsym lookup stub. Do not go through `UnregisterNative()`
-    // as the extra processing for @CriticalNative is not needed yet.
-    method->SetEntryPointFromJni(
-        method->IsCriticalNative() ? GetJniDlsymLookupCriticalStub() : GetJniDlsymLookupStub());
-  }
-}
-
 void ClassLinker::SetupClass(const DexFile& dex_file,
                              const dex::ClassDef& dex_class_def,
                              Handle<mirror::Class> klass,
@@ -4000,151 +3996,108 @@ class ClassLinker::MethodAnnotationsIterator {
   const dex::MethodAnnotationsItem* const end_;
 };
 
-void ClassLinker::LoadClass(Thread* self,
-                            const DexFile& dex_file,
-                            const dex::ClassDef& dex_class_def,
-                            Handle<mirror::Class> klass) {
-  ClassAccessor accessor(dex_file,
-                         dex_class_def,
-                         /* parse_hiddenapi_class_data= */ klass->IsBootStrapClassLoaded());
-  if (!accessor.HasClassData()) {
-    return;
-  }
-  Runtime* const runtime = Runtime::Current();
-  {
-    // Note: We cannot have thread suspension until the field and method arrays are setup or else
-    // Class::VisitFieldRoots may miss some fields or methods.
-    ScopedAssertNoThreadSuspension nts(__FUNCTION__);
-    // Load static fields.
-    // We allow duplicate definitions of the same field in a class_data_item
-    // but ignore the repeated indexes here, b/21868015.
-    LinearAlloc* const allocator = GetAllocatorForClassLoader(klass->GetClassLoader());
-    LengthPrefixedArray<ArtField>* sfields = AllocArtFieldArray(self,
-                                                                allocator,
-                                                                accessor.NumStaticFields());
-    LengthPrefixedArray<ArtField>* ifields = AllocArtFieldArray(self,
-                                                                allocator,
-                                                                accessor.NumInstanceFields());
-    size_t num_sfields = 0u;
-    size_t num_ifields = 0u;
-    uint32_t last_static_field_idx = 0u;
-    uint32_t last_instance_field_idx = 0u;
-
-    // Methods
-    bool has_oat_class = false;
-    const OatFile::OatClass oat_class = (runtime->IsStarted() && !runtime->IsAotCompiler())
-        ? OatFile::FindOatClass(dex_file, klass->GetDexClassDefIndex(), &has_oat_class)
-        : OatFile::OatClass::Invalid();
-    OatClassCodeIterator occi(oat_class);
-    klass->SetMethodsPtr(
-        AllocArtMethodArray(self, allocator, accessor.NumMethods()),
-        accessor.NumDirectMethods(),
-        accessor.NumVirtualMethods());
-    size_t class_def_method_index = 0;
-    uint32_t last_dex_method_index = dex::kDexNoIndex;
-    size_t last_class_def_method_index = 0;
-
-    // Initialize separate `MethodAnnotationsIterator`s for direct and virtual methods.
-    MethodAnnotationsIterator mai_direct(dex_file, dex_file.GetAnnotationsDirectory(dex_class_def));
-    MethodAnnotationsIterator mai_virtual = mai_direct;
+class ClassLinker::LoadClassHelper {
+ public:
+  LoadClassHelper(
+      Runtime* runtime, const DexFile& dex_file, bool is_interface)
+      : runtime_(runtime),
+        dex_file_(dex_file),
+        hotness_count_(runtime->GetJITOptions()->GetWarmupThreshold()),
+        is_aot_compiler_(runtime->IsAotCompiler()),
+        is_interface_(is_interface),
+        stack_(runtime->GetArenaPool()),
+        allocator_(&stack_),
+        num_direct_methods_(0u),
+        has_finalizer_(false) {}
 
-    uint16_t hotness_threshold = runtime->GetJITOptions()->GetWarmupThreshold();
-    // Use the visitor since the ranged based loops are bit slower from seeking. Seeking to the
-    // methods needs to decode all of the fields.
-    accessor.VisitFieldsAndMethods([&](
-        const ClassAccessor::Field& field) REQUIRES_SHARED(Locks::mutator_lock_) {
-          uint32_t field_idx = field.GetIndex();
-          DCHECK_GE(field_idx, last_static_field_idx);  // Ordering enforced by DexFileVerifier.
-          if (num_sfields == 0 || LIKELY(field_idx > last_static_field_idx)) {
-            LoadField(field, klass, &sfields->At(num_sfields));
-            ++num_sfields;
-            last_static_field_idx = field_idx;
-          }
-        }, [&](const ClassAccessor::Field& field) REQUIRES_SHARED(Locks::mutator_lock_) {
-          uint32_t field_idx = field.GetIndex();
-          DCHECK_GE(field_idx, last_instance_field_idx);  // Ordering enforced by DexFileVerifier.
-          if (num_ifields == 0 || LIKELY(field_idx > last_instance_field_idx)) {
-            LoadField(field, klass, &ifields->At(num_ifields));
-            ++num_ifields;
-            last_instance_field_idx = field_idx;
-          }
-        }, [&](const ClassAccessor::Method& method) REQUIRES_SHARED(Locks::mutator_lock_) {
-          ArtMethod* art_method = klass->GetDirectMethodUnchecked(class_def_method_index,
-              image_pointer_size_);
-          LoadMethod(dex_file, method, klass.Get(), &mai_direct, art_method);
-          LinkCode(art_method, class_def_method_index, &occi);
-          uint32_t it_method_index = method.GetIndex();
-          if (last_dex_method_index == it_method_index) {
-            // duplicate case
-            art_method->SetMethodIndex(last_class_def_method_index);
-          } else {
-            art_method->SetMethodIndex(class_def_method_index);
-            last_dex_method_index = it_method_index;
-            last_class_def_method_index = class_def_method_index;
-          }
-          art_method->ResetCounter(hotness_threshold);
-          ++class_def_method_index;
-        }, [&](const ClassAccessor::Method& method) REQUIRES_SHARED(Locks::mutator_lock_) {
-          ArtMethod* art_method = klass->GetVirtualMethodUnchecked(
-              class_def_method_index - accessor.NumDirectMethods(),
-              image_pointer_size_);
-          art_method->ResetCounter(hotness_threshold);
-          LoadMethod(dex_file, method, klass.Get(), &mai_virtual, art_method);
-          LinkCode(art_method, class_def_method_index, &occi);
-          ++class_def_method_index;
-        });
+  // Note: This function can take a long time and therefore it should not be called while holding
+  // the mutator lock. Otherwise we can experience an occasional suspend request timeout.
+  void Load(const ClassAccessor& accessor,
+            const dex::ClassDef& dex_class_def,
+            const OatFile::OatClass& oat_class);
 
-    if (UNLIKELY(num_ifields + num_sfields != accessor.NumFields())) {
-      LOG(WARNING) << "Duplicate fields in class " << klass->PrettyDescriptor()
-          << " (unique static fields: " << num_sfields << "/" << accessor.NumStaticFields()
-          << ", unique instance fields: " << num_ifields << "/" << accessor.NumInstanceFields()
-          << ")";
-      // NOTE: Not shrinking the over-allocated sfields/ifields, just setting size.
-      if (sfields != nullptr) {
-        sfields->SetSize(num_sfields);
-      }
-      if (ifields != nullptr) {
-        ifields->SetSize(num_ifields);
-      }
-    }
-    // Set the field arrays.
-    klass->SetSFieldsPtr(sfields);
-    DCHECK_EQ(klass->NumStaticFields(), num_sfields);
-    klass->SetIFieldsPtr(ifields);
-    DCHECK_EQ(klass->NumInstanceFields(), num_ifields);
+  void Commit(Handle<mirror::Class> klass,
+              PointerSize pointer_size,
+              LengthPrefixedArray<ArtField>* fields,
+              LengthPrefixedArray<ArtMethod>* methods)
+      REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(Roles::uninterruptible_);
+
+  uint32_t NumFields() const {
+    return dchecked_integral_cast<uint32_t>(fields_.size());
   }
-  // Ensure that the card is marked so that remembered sets pick up native roots.
-  WriteBarrier::ForEveryFieldWrite(klass.Get());
-  self->AllowThreadSuspension();
-}
 
-void ClassLinker::LoadField(const ClassAccessor::Field& field,
-                            Handle<mirror::Class> klass,
-                            ArtField* dst) {
-  const uint32_t field_idx = field.GetIndex();
-  dst->SetDexFieldIndex(field_idx);
-  dst->SetDeclaringClass(klass.Get());
+  uint32_t NumMethods() const {
+    return dchecked_integral_cast<uint32_t>(methods_.size());
+  }
+
+ private:
+  struct ArtFieldData {
+    uint32_t access_flags;
+    uint32_t dex_field_index;
+  };
+
+  struct ArtMethodData {
+    uint32_t access_flags;
+    uint32_t dex_method_index;
+    uint16_t method_index;
+    uint16_t imt_index_or_hotness_count;
+    const void* data;
+    const void* entrypoint;
+  };
+
+  ALWAYS_INLINE
+  static void LoadField(const ClassAccessor::Field& field, /*out*/ ArtFieldData* dst);
+
+  ALWAYS_INLINE
+  void LoadMethod(const ClassAccessor::Method& method,
+                  /*inout*/ MethodAnnotationsIterator* mai,
+                  /*out*/ ArtMethodData* dst);
+
+  ALWAYS_INLINE
+  void LinkCode(ArtMethodData* method,
+                uint32_t class_def_method_index,
+                /*inout*/ OatClassCodeIterator* occi);
+
+  ALWAYS_INLINE
+  void FillFields(ObjPtr<mirror::Class> klass, /*out*/ LengthPrefixedArray<ArtField>* fields)
+      REQUIRES_SHARED(Locks::mutator_lock_);
+
+  template <PointerSize kPointerSize>
+  ALWAYS_INLINE
+  void FillMethods(ObjPtr<mirror::Class> klass, /*out*/ LengthPrefixedArray<ArtMethod>* methods)
+      REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(Roles::uninterruptible_);
+
+  Runtime* const runtime_;
+  const DexFile& dex_file_;
+  const uint16_t hotness_count_;
+  const bool is_aot_compiler_;
+  const bool is_interface_;
+
+  ArenaStack stack_;
+  ScopedArenaAllocator allocator_;
 
+  ArrayRef<ArtFieldData> fields_;
+  ArrayRef<ArtMethodData> methods_;
+  uint32_t num_direct_methods_;
+  bool has_finalizer_;
+};
+
+inline void ClassLinker::LoadClassHelper::LoadField(const ClassAccessor::Field& field,
+                                                    /*out*/ ArtFieldData* dst) {
+  dst->dex_field_index = field.GetIndex();
   // Get access flags from the DexFile and set hiddenapi runtime access flags.
-  dst->SetAccessFlags(field.GetAccessFlags() | hiddenapi::CreateRuntimeFlags(field));
+  dst->access_flags = field.GetAccessFlags() | hiddenapi::CreateRuntimeFlags(field);
 }
 
-void ClassLinker::LoadMethod(const DexFile& dex_file,
-                             const ClassAccessor::Method& method,
-                             ObjPtr<mirror::Class> klass,
-                             /*inout*/ MethodAnnotationsIterator* mai,
-                             /*out*/ ArtMethod* dst) {
-  ScopedAssertNoThreadSuspension sants(__FUNCTION__);
-
-  const uint32_t dex_method_idx = method.GetIndex();
-  const dex::MethodId& method_id = dex_file.GetMethodId(dex_method_idx);
+inline void ClassLinker::LoadClassHelper::LoadMethod(const ClassAccessor::Method& method,
+                                                     /*inout*/ MethodAnnotationsIterator* mai,
+                                                     /*out*/ ArtMethodData* dst) {
+  const uint32_t dex_method_index = method.GetIndex();
+  const dex::MethodId& method_id = dex_file_.GetMethodId(dex_method_index);
   uint32_t name_utf16_length;
-  const char* method_name = dex_file.GetStringDataAndUtf16Length(method_id.name_idx_,
-                                                                 &name_utf16_length);
-  std::string_view shorty = dex_file.GetShortyView(dex_file.GetProtoId(method_id.proto_idx_));
-
-  dst->SetDexMethodIndex(dex_method_idx);
-  dst->SetDeclaringClass(klass);
+  const char* method_name = dex_file_.GetStringDataAndUtf16Length(method_id.name_idx_,
+                                                                  &name_utf16_length);
+  std::string_view shorty = dex_file_.GetShortyView(dex_file_.GetProtoId(method_id.proto_idx_));
 
   // Get access flags from the DexFile and set hiddenapi runtime access flags.
   uint32_t access_flags = method.GetAccessFlags() | hiddenapi::CreateRuntimeFlags(method);
@@ -4161,7 +4114,7 @@ void ClassLinker::LoadMethod(const DexFile& dex_file,
     // When initializing without a boot image, `Object` and `Enum` shall have the finalizable
     // flag cleared immediately after loading these classes, see  `InitWithoutImage()`.
     if (shorty == "V") {
-      klass->SetFinalizable();
+      has_finalizer_ = true;
     }
   } else if (method_name[0] == '<') {
     // Fix broken access flags for initializers. Bug 11157540.
@@ -4170,50 +4123,49 @@ void ClassLinker::LoadMethod(const DexFile& dex_file,
            has_ascii_name("<clinit>", sizeof("<clinit>") - 1u)) << method_name;
     if (UNLIKELY((access_flags & kAccConstructor) == 0)) {
       LOG(WARNING) << method_name << " didn't have expected constructor access flag in class "
-          << klass->PrettyDescriptor() << " in dex file " << dex_file.GetLocation();
+          << PrettyDescriptor(dex_file_.GetMethodDeclaringClassDescriptor(dex_method_index))
+          << " in dex file " << dex_file_.GetLocation();
       access_flags |= kAccConstructor;
     }
   }
 
   access_flags |= GetNterpFastPathFlags(shorty, access_flags, kRuntimeQuickCodeISA);
 
+  uint16_t imt_index_or_hotness_count = hotness_count_;
+  const void* data = nullptr;
   if (UNLIKELY((access_flags & kAccNative) != 0u)) {
     // Check if the native method is annotated with @FastNative or @CriticalNative.
-    const dex::MethodAnnotationsItem* method_annotations = mai->AdvanceTo(dex_method_idx);
+    const dex::MethodAnnotationsItem* method_annotations = mai->AdvanceTo(dex_method_index);
     if (method_annotations != nullptr) {
       access_flags |=
-          annotations::GetNativeMethodAnnotationAccessFlags(dex_file, *method_annotations);
+          annotations::GetNativeMethodAnnotationAccessFlags(dex_file_, *method_annotations);
     }
-    dst->SetAccessFlags(access_flags);
-    DCHECK(!dst->IsAbstract());
-    DCHECK(!dst->HasCodeItem());
+    DCHECK(!ArtMethod::IsAbstract(access_flags));
+    DCHECK(!ArtMethod::NeedsCodeItem(access_flags));
     DCHECK_EQ(method.GetCodeItemOffset(), 0u);
-    dst->SetDataPtrSize(nullptr, image_pointer_size_);  // JNI stub/trampoline not linked yet.
+    DCHECK(data == nullptr);  // JNI stub/trampoline not linked yet.
   } else if ((access_flags & kAccAbstract) != 0u) {
-    dst->SetAccessFlags(access_flags);
-    // Must be done after SetAccessFlags since IsAbstract depends on it.
-    DCHECK(dst->IsAbstract());
-    if (klass->IsInterface()) {
-      dst->CalculateAndSetImtIndex();
-    }
-    DCHECK(!dst->HasCodeItem());
+    DCHECK(ArtMethod::IsAbstract(access_flags));
+    imt_index_or_hotness_count = is_interface_
+        ? ImTable::GetImtIndexForAbstractMethod(dex_file_, dex_method_index)
+        : /* unused */ 0u;
+    DCHECK(!ArtMethod::NeedsCodeItem(access_flags));
     DCHECK_EQ(method.GetCodeItemOffset(), 0u);
-    dst->SetDataPtrSize(nullptr, image_pointer_size_);  // Single implementation not set yet.
+    DCHECK(data == nullptr);  // Single implementation not set yet.
   } else {
-    const dex::MethodAnnotationsItem* method_annotations = mai->AdvanceTo(dex_method_idx);
+    const dex::MethodAnnotationsItem* method_annotations = mai->AdvanceTo(dex_method_index);
     if (method_annotations != nullptr &&
-        annotations::MethodIsNeverCompile(dex_file, *method_annotations)) {
+        annotations::MethodIsNeverCompile(dex_file_, *method_annotations)) {
       access_flags |= kAccCompileDontBother;
     }
-    dst->SetAccessFlags(access_flags);
-    DCHECK(!dst->IsAbstract());
-    DCHECK(dst->HasCodeItem());
+    DCHECK(!ArtMethod::IsAbstract(access_flags));
+    DCHECK(ArtMethod::NeedsCodeItem(access_flags));
     uint32_t code_item_offset = method.GetCodeItemOffset();
     DCHECK_NE(code_item_offset, 0u);
-    if (Runtime::Current()->IsAotCompiler()) {
-      dst->SetDataPtrSize(reinterpret_cast32<void*>(code_item_offset), image_pointer_size_);
+    if (is_aot_compiler_) {
+      data = reinterpret_cast32<void*>(code_item_offset);
     } else {
-      dst->SetCodeItem(dex_file.GetCodeItem(code_item_offset), dex_file.IsCompactDexFile());
+      data = dex_file_.GetCodeItem(code_item_offset);
     }
   }
 
@@ -4222,11 +4174,270 @@ void ClassLinker::LoadMethod(const DexFile& dex_file,
       !Runtime::Current()->GetJITOptions()->GetProfileSaverOptions().GetProfileBootClassPath()) {
     DCHECK(!ArtMethod::IsAbstract(access_flags));
     DCHECK(!ArtMethod::IsIntrinsic(access_flags));
-    dst->SetMemorySharedMethod();
-    dst->SetHotCounter();
+    access_flags = ArtMethod::SetMemorySharedMethod(access_flags);
+    imt_index_or_hotness_count = 0u;  // Mark the method as hot.
+  }
+
+  dst->access_flags = access_flags;
+  dst->dex_method_index = dex_method_index;
+  dst->method_index = 0u;
+  dst->imt_index_or_hotness_count  = imt_index_or_hotness_count;
+  dst->data = data;
+  dst->entrypoint = nullptr;
+}
+
+inline void ClassLinker::LoadClassHelper::LinkCode(ArtMethodData* method,
+                                                   uint32_t class_def_method_index,
+                                                   /*inout*/ OatClassCodeIterator* occi) {
+  if (is_aot_compiler_) {
+    // The following code only applies to a non-compiler runtime.
+    return;
+  }
+
+  // Method shouldn't have already been linked.
+  DCHECK_EQ(method->entrypoint, nullptr);
+
+  uint32_t access_flags = method->access_flags;
+  if (!ArtMethod::IsInvokable(access_flags)) {
+    method->entrypoint = GetQuickToInterpreterBridge();
+    occi->SkipAbstract(class_def_method_index);
+    return;
+  }
+
+  const void* quick_code = occi->GetAndAdvance(class_def_method_index);
+  if (ArtMethod::IsNative(access_flags) && quick_code == nullptr) {
+    std::string_view shorty = dex_file_.GetMethodShortyView(method->dex_method_index);
+    const void* boot_jni_stub = runtime_->GetClassLinker()->FindBootJniStub(access_flags, shorty);
+    if (boot_jni_stub != nullptr) {
+      // Use boot JNI stub if found.
+      quick_code = boot_jni_stub;
+    }
+  }
+  method->entrypoint =
+      runtime_->GetInstrumentation()->GetInitialEntrypoint(access_flags, quick_code);
+
+  if (ArtMethod::IsNative(access_flags)) {
+    // Set up the dlsym lookup stub. Do not go through `UnregisterNative()`
+    // as the extra processing for @CriticalNative is not needed yet.
+    method->data = ArtMethod::IsCriticalNative(access_flags)
+        ? GetJniDlsymLookupCriticalStub()
+        : GetJniDlsymLookupStub();
+  }
+}
+
+void ClassLinker::LoadClassHelper::Load(const ClassAccessor& accessor,
+                                        const dex::ClassDef& dex_class_def,
+                                        const OatFile::OatClass& oat_class) {
+  DCHECK(fields_.empty());
+  DCHECK(methods_.empty());
+  DCHECK_EQ(num_direct_methods_, 0u);
+  DCHECK(!has_finalizer_);
+
+  size_t num_fields = accessor.NumFields();
+  size_t num_methods = accessor.NumMethods();
+  ArrayRef<ArtFieldData> fields(allocator_.AllocArray<ArtFieldData>(num_fields), num_fields);
+  ArrayRef<ArtMethodData> methods(allocator_.AllocArray<ArtMethodData>(num_methods), num_methods);
+
+  size_t num_loaded_fields = 0u;
+  size_t num_sfields = 0u;
+  size_t num_ifields = 0u;
+  uint32_t last_static_field_idx = 0u;
+  uint32_t last_instance_field_idx = 0u;
+
+  OatClassCodeIterator occi(oat_class);
+  size_t class_def_method_index = 0;
+  uint32_t last_dex_method_index = dex::kDexNoIndex;
+  size_t last_class_def_method_index = 0;
+
+  // Initialize separate `MethodAnnotationsIterator`s for direct and virtual methods.
+  MethodAnnotationsIterator mai_direct(dex_file_, dex_file_.GetAnnotationsDirectory(dex_class_def));
+  MethodAnnotationsIterator mai_virtual = mai_direct;
+
+  // Use the visitor since the ranged based loops are bit slower from seeking. Seeking to the
+  // methods needs to decode all of the fields.
+  accessor.VisitFieldsAndMethods([&](
+      // We allow duplicate definitions of the same field in a class_data_item
+      // but ignore the repeated indexes here, b/21868015.
+      const ClassAccessor::Field& field) {
+        uint32_t field_idx = field.GetIndex();
+        DCHECK_GE(field_idx, last_static_field_idx);  // Ordering enforced by DexFileVerifier.
+        if (num_sfields == 0 || LIKELY(field_idx > last_static_field_idx)) {
+          LoadField(field, &fields[num_loaded_fields]);
+          ++num_loaded_fields;
+          ++num_sfields;
+          last_static_field_idx = field_idx;
+        }
+      }, [&](const ClassAccessor::Field& field) {
+        uint32_t field_idx = field.GetIndex();
+        DCHECK_GE(field_idx, last_instance_field_idx);  // Ordering enforced by DexFileVerifier.
+        if (num_ifields == 0 || LIKELY(field_idx > last_instance_field_idx)) {
+          LoadField(field, &fields[num_loaded_fields]);
+          ++num_loaded_fields;
+          ++num_ifields;
+          last_instance_field_idx = field_idx;
+        }
+      }, [&](const ClassAccessor::Method& method) {
+        ArtMethodData* method_data = &methods[class_def_method_index];
+        LoadMethod(method, &mai_direct, method_data);
+        LinkCode(method_data, class_def_method_index, &occi);
+        uint32_t it_method_index = method.GetIndex();
+        if (last_dex_method_index == it_method_index) {
+          // duplicate case
+          method_data->method_index = last_class_def_method_index;
+        } else {
+          method_data->method_index = class_def_method_index;
+          last_dex_method_index = it_method_index;
+          last_class_def_method_index = class_def_method_index;
+        }
+        ++class_def_method_index;
+      }, [&](const ClassAccessor::Method& method) REQUIRES_SHARED(Locks::mutator_lock_) {
+        ArtMethodData* method_data = &methods[class_def_method_index];
+        LoadMethod(method, &mai_virtual, method_data);
+        LinkCode(method_data, class_def_method_index, &occi);
+        DCHECK_EQ(method_data->method_index, 0u);  // Shall be updated in `LinkMethods()`.
+        ++class_def_method_index;
+      });
+
+  if (UNLIKELY(num_loaded_fields != num_fields)) {
+    LOG(WARNING) << "Duplicate fields in class "
+        << PrettyDescriptor(dex_file_.GetFieldDeclaringClassDescriptor(fields[0].dex_field_index))
+        << " (unique static fields: " << num_sfields << "/" << accessor.NumStaticFields()
+        << ", unique instance fields: " << num_ifields << "/" << accessor.NumInstanceFields()
+        << ")";
+    DCHECK_LT(num_loaded_fields, num_fields);
+    fields = fields.SubArray(/*pos=*/ 0u, num_loaded_fields);
+  }
+
+  // Sort the fields by dex field index to facilitate fast lookups.
+  std::sort(fields.begin(),
+            fields.end(),
+            [](ArtFieldData& lhs, ArtFieldData& rhs) {
+              return lhs.dex_field_index < rhs.dex_field_index;
+            });
+
+  fields_ = fields;
+  methods_ = methods;
+  num_direct_methods_ = accessor.NumDirectMethods();
+}
+
+void ClassLinker::LoadClassHelper::FillFields(ObjPtr<mirror::Class> klass,
+                                              LengthPrefixedArray<ArtField>* fields) {
+  DCHECK_IMPLIES(!fields_.empty(), fields != nullptr);
+  DCHECK_EQ(fields_.size(), (fields != nullptr) ? fields->size() : 0u);
+  for (size_t i = 0, size = fields_.size(); i != size; ++i) {
+    const ArtFieldData& src = fields_[i];
+    ArtField* dst = &fields->At(i);
+    dst->SetDeclaringClass(klass);
+    dst->SetAccessFlags(src.access_flags);
+    dst->SetDexFieldIndex(src.dex_field_index);
+    // The `ArtField::offset_` shall be set in `LinkFields()`.
+  }
+}
+
+template <PointerSize kPointerSize>
+void ClassLinker::LoadClassHelper::FillMethods(ObjPtr<mirror::Class> klass,
+                                               LengthPrefixedArray<ArtMethod>* methods) {
+  DCHECK_IMPLIES(!methods_.empty(), methods != nullptr);
+  DCHECK_EQ(methods_.size(), (methods != nullptr) ? methods->size() : 0u);
+  static constexpr size_t kMethodAlignment = ArtMethod::Alignment(kPointerSize);
+  static constexpr size_t kMethodSize = ArtMethod::Size(kPointerSize);
+  instrumentation::Instrumentation* instr = nullptr;
+  bool use_stubs = false;
+  if (!is_aot_compiler_) {
+    instr = runtime_->GetInstrumentation();
+    use_stubs = instr->InitialEntrypointNeedsInstrumentationStubs();
+  }
+  for (size_t i = 0, size = methods_.size(); i != size; ++i) {
+    const ArtMethodData& src = methods_[i];
+    ArtMethod* dst = &methods->At(i, kMethodSize, kMethodAlignment);
+    dst->SetDeclaringClass(klass);
+    uint32_t access_flags = src.access_flags;
+    dst->SetAccessFlags(access_flags);
+    dst->SetDexMethodIndex(src.dex_method_index);
+    dst->SetMethodIndex(src.method_index);
+    // Note: We set the appropriate field of the union (`imt_index_` or `hotness_count_`)
+    // as required by the C++ standard but we expect the C++ compiler to optimize away
+    // the condition and just copy the `imt_index_or_hotness_count` directly.
+    if (ArtMethod::IsInvokable(access_flags)) {
+      dst->SetHotnessCount(src.imt_index_or_hotness_count);
+    } else {
+      // For abstract non-interface methods, the value shall not be used.
+      DCHECK_IMPLIES(!is_interface_, src.imt_index_or_hotness_count == 0u);
+      dst->SetImtIndex(src.imt_index_or_hotness_count);
+    }
+    DCHECK_IMPLIES(dst->IsMemorySharedMethod(), !dst->IsAbstract());
+    DCHECK_IMPLIES(dst->IsMemorySharedMethod(), dst->CounterIsHot());
+    DCHECK_IMPLIES(!dst->IsAbstract() && !dst->IsMemorySharedMethod(),
+                   dst->GetCounter() == hotness_count_);
+    dst->SetDataPtrSize(src.data, kPointerSize);
+    if (instr != nullptr) {
+      DCHECK_IMPLIES(dst->IsNative(), dst->GetEntryPointFromJniPtrSize(kPointerSize) == src.data);
+      const void* entrypoint = src.entrypoint;
+      if (UNLIKELY(use_stubs)) {
+        bool is_native = ArtMethod::IsNative(access_flags);
+        entrypoint = is_native ? GetQuickGenericJniStub() : GetQuickToInterpreterBridge();
+      }
+      instr->InitializeMethodsCode(dst, entrypoint, kPointerSize);
+    }
+  }
+}
+
+void ClassLinker::LoadClassHelper::Commit(Handle<mirror::Class> klass,
+                                          PointerSize pointer_size,
+                                          LengthPrefixedArray<ArtField>* fields,
+                                          LengthPrefixedArray<ArtMethod>* methods) {
+  FillFields(klass.Get(), fields);
+  if (pointer_size == PointerSize::k64) {
+    FillMethods<PointerSize::k64>(klass.Get(), methods);
+  } else {
+    FillMethods<PointerSize::k32>(klass.Get(), methods);
+  }
+  klass->SetFieldsPtr(fields);
+  klass->SetMethodsPtr(methods, num_direct_methods_, methods_.size() - num_direct_methods_);
+  if (has_finalizer_) {
+    klass->SetFinalizable();
   }
 }
 
+void ClassLinker::LoadClass(Thread* self,
+                            const DexFile& dex_file,
+                            const dex::ClassDef& dex_class_def,
+                            Handle<mirror::Class> klass) {
+  CHECK(!dex_file.IsCompactDexFile());
+  ClassAccessor accessor(dex_file,
+                         dex_class_def,
+                         /* parse_hiddenapi_class_data= */ klass->IsBootStrapClassLoaded());
+  if (!accessor.HasClassData()) {
+    return;
+  }
+  Runtime* const runtime = Runtime::Current();
+  {
+    bool has_oat_class = false;
+    const OatFile::OatClass oat_class = (runtime->IsStarted() && !runtime->IsAotCompiler())
+        ? OatFile::FindOatClass(dex_file, klass->GetDexClassDefIndex(), &has_oat_class)
+        : OatFile::OatClass::Invalid();
+    LoadClassHelper helper(runtime, dex_file, klass->IsInterface());
+    {
+      ScopedThreadSuspension sts(self, ThreadState::kNative);
+      helper.Load(accessor, dex_class_def, oat_class);
+    }
+
+    // Note: We cannot have thread suspension until the field and method arrays are setup or else
+    // Class::VisitFieldRoots may miss some fields or methods.
+    ScopedAssertNoThreadSuspension nts(__FUNCTION__);
+
+    LinearAlloc* allocator = GetAllocatorForClassLoader(klass->GetClassLoader());
+    LengthPrefixedArray<ArtField>* fields =
+        AllocArtFieldArray(self, allocator, helper.NumFields());
+    LengthPrefixedArray<ArtMethod>* methods =
+        AllocArtMethodArray(self, allocator, helper.NumMethods());
+    helper.Commit(klass, image_pointer_size_, fields, methods);
+  }
+  // Ensure that the card is marked so that remembered sets pick up native roots.
+  WriteBarrier::ForEveryFieldWrite(klass.Get());
+  self->AllowThreadSuspension();
+}
+
 void ClassLinker::AppendToBootClassPath(Thread* self, const DexFile* dex_file) {
   ObjPtr<mirror::DexCache> dex_cache =
       AllocAndInitializeDexCache(self, *dex_file, /* class_loader= */ nullptr);
@@ -5108,6 +5319,9 @@ verifier::FailureKind ClassLinker::VerifyClass(Thread* self,
     Runtime::Current()->GetCompilerCallbacks()->UpdateClassState(
         ClassReference(&klass->GetDexFile(), klass->GetDexClassDefIndex()), klass->GetStatus());
   } else {
+    if (verifier_failure == verifier::FailureKind::kTypeChecksFailure) {
+      klass->SetHasTypeChecksFailure();
+    }
     mirror::Class::SetStatus(klass, ClassStatus::kVerified, self);
   }
 
@@ -5228,8 +5442,7 @@ void ClassLinker::ResolveMethodExceptionHandlerTypes(ArtMethod* method) {
   CHECK(method->GetDexFile()->IsInDataSection(handlers_ptr))
       << method->PrettyMethod()
       << "@" << method->GetDexFile()->GetLocation()
-      << "@" << reinterpret_cast<const void*>(handlers_ptr)
-      << " is_compact_dex=" << method->GetDexFile()->IsCompactDexFile();
+      << "@" << reinterpret_cast<const void*>(handlers_ptr);
 
   uint32_t handlers_size = DecodeUnsignedLeb128(&handlers_ptr);
   for (uint32_t idx = 0; idx < handlers_size; idx++) {
@@ -5304,18 +5517,18 @@ ObjPtr<mirror::Class> ClassLinker::CreateProxyClass(ScopedObjectAccessAlreadyRun
 
   // Instance fields are inherited, but we add a couple of static fields...
   const size_t num_fields = 2;
-  LengthPrefixedArray<ArtField>* sfields = AllocArtFieldArray(self, allocator, num_fields);
-  temp_klass->SetSFieldsPtr(sfields);
+  LengthPrefixedArray<ArtField>* fields = AllocArtFieldArray(self, allocator, num_fields);
+  temp_klass->SetFieldsPtr(fields);
 
   // 1. Create a static field 'interfaces' that holds the _declared_ interfaces implemented by
   // our proxy, so Class.getInterfaces doesn't return the flattened set.
-  ArtField& interfaces_sfield = sfields->At(0);
+  ArtField& interfaces_sfield = fields->At(0);
   interfaces_sfield.SetDexFieldIndex(0);
   interfaces_sfield.SetDeclaringClass(temp_klass.Get());
   interfaces_sfield.SetAccessFlags(kAccStatic | kAccPublic | kAccFinal);
 
   // 2. Create a static field 'throws' that holds exceptions thrown by our methods.
-  ArtField& throws_sfield = sfields->At(1);
+  ArtField& throws_sfield = fields->At(1);
   throws_sfield.SetDexFieldIndex(1);
   throws_sfield.SetDeclaringClass(temp_klass.Get());
   throws_sfield.SetAccessFlags(kAccStatic | kAccPublic | kAccFinal);
@@ -5455,7 +5668,6 @@ ObjPtr<mirror::Class> ClassLinker::CreateProxyClass(ScopedObjectAccessAlreadyRun
 
   // Consistency checks.
   if (kIsDebugBuild) {
-    CHECK(klass->GetIFieldsPtr() == nullptr);
     CheckProxyConstructor(klass->GetDirectMethod(0, image_pointer_size_));
 
     for (size_t i = 0; i < num_virtual_methods; ++i) {
@@ -5467,11 +5679,11 @@ ObjPtr<mirror::Class> ClassLinker::CreateProxyClass(ScopedObjectAccessAlreadyRun
     Handle<mirror::String> decoded_name = hs2.NewHandle(soa.Decode<mirror::String>(name));
     std::string interfaces_field_name(StringPrintf("java.lang.Class[] %s.interfaces",
                                                    decoded_name->ToModifiedUtf8().c_str()));
-    CHECK_EQ(ArtField::PrettyField(klass->GetStaticField(0)), interfaces_field_name);
+    CHECK_EQ(ArtField::PrettyField(klass->GetField(0)), interfaces_field_name);
 
     std::string throws_field_name(StringPrintf("java.lang.Class[][] %s.throws",
                                                decoded_name->ToModifiedUtf8().c_str()));
-    CHECK_EQ(ArtField::PrettyField(klass->GetStaticField(1)), throws_field_name);
+    CHECK_EQ(ArtField::PrettyField(klass->GetField(1)), throws_field_name);
 
     CHECK_EQ(klass.Get()->GetProxyInterfaces(),
              soa.Decode<mirror::ObjectArray<mirror::Class>>(interfaces));
@@ -5576,7 +5788,7 @@ bool ClassLinker::CanWeInitializeClass(ObjPtr<mirror::Class> klass,
       return false;
     }
     // Check if there are encoded static values needing initialization.
-    if (klass->NumStaticFields() != 0) {
+    if (klass->HasStaticFields()) {
       const dex::ClassDef* dex_class_def = klass->GetClassDef();
       DCHECK(dex_class_def != nullptr);
       if (dex_class_def->static_values_off_ != 0) {
@@ -5804,18 +6016,20 @@ bool ClassLinker::InitializeClass(Thread* self,
     }
   }
 
-  const size_t num_static_fields = klass->NumStaticFields();
-  if (num_static_fields > 0) {
+  if (klass->HasStaticFields()) {
     const dex::ClassDef* dex_class_def = klass->GetClassDef();
     CHECK(dex_class_def != nullptr);
-    StackHandleScope<3> hs(self);
+    StackHandleScope<2> hs(self);
     Handle<mirror::ClassLoader> class_loader(hs.NewHandle(klass->GetClassLoader()));
     Handle<mirror::DexCache> dex_cache(hs.NewHandle(klass->GetDexCache()));
 
     // Eagerly fill in static fields so that the we don't have to do as many expensive
     // Class::FindStaticField in ResolveField.
-    for (size_t i = 0; i < num_static_fields; ++i) {
-      ArtField* field = klass->GetStaticField(i);
+    for (size_t i = 0; i < klass->NumFields(); ++i) {
+      ArtField* field = klass->GetField(i);
+      if (!field->IsStatic()) {
+        continue;
+      }
       const uint32_t field_idx = field->GetDexFieldIndex();
       ArtField* resolved_field = dex_cache->GetResolvedField(field_idx);
       if (resolved_field == nullptr) {
@@ -5823,7 +6037,7 @@ bool ClassLinker::InitializeClass(Thread* self,
         DCHECK(!hiddenapi::ShouldDenyAccessToMember(
             field,
             hiddenapi::AccessContext(class_loader.Get(), dex_cache.Get()),
-            hiddenapi::AccessMethod::kNone));
+            hiddenapi::AccessMethod::kCheckWithPolicy));
         dex_cache->SetResolvedField(field_idx, field);
       } else {
         DCHECK_EQ(field, resolved_field);
@@ -6258,15 +6472,8 @@ bool ClassLinker::EnsureInitialized(Thread* self,
 
 void ClassLinker::FixupTemporaryDeclaringClass(ObjPtr<mirror::Class> temp_class,
                                                ObjPtr<mirror::Class> new_class) {
-  DCHECK_EQ(temp_class->NumInstanceFields(), 0u);
-  for (ArtField& field : new_class->GetIFields()) {
-    if (field.GetDeclaringClass() == temp_class) {
-      field.SetDeclaringClass(new_class);
-    }
-  }
-
-  DCHECK_EQ(temp_class->NumStaticFields(), 0u);
-  for (ArtField& field : new_class->GetSFields()) {
+  DCHECK_EQ(temp_class->NumFields(), 0u);
+  for (ArtField& field : new_class->GetFields()) {
     if (field.GetDeclaringClass() == temp_class) {
       field.SetDeclaringClass(new_class);
     }
@@ -6413,8 +6620,7 @@ bool ClassLinker::LinkClass(Thread* self,
     // may not see any references to the target space and clean the card for a class if another
     // class had the same array pointer.
     klass->SetMethodsPtrUnchecked(nullptr, 0, 0);
-    klass->SetSFieldsPtrUnchecked(nullptr);
-    klass->SetIFieldsPtrUnchecked(nullptr);
+    klass->SetFieldsPtrUnchecked(nullptr);
     if (UNLIKELY(h_new_class == nullptr)) {
       self->AssertPendingOOMException();
       mirror::Class::SetStatus(klass, ClassStatus::kErrorUnresolved, self);
@@ -9283,9 +9489,7 @@ bool ClassLinker::LinkFieldsHelper::LinkFields(ClassLinker* class_linker,
                                                bool is_static,
                                                size_t* class_size) {
   self->AllowThreadSuspension();
-  const size_t num_fields = is_static ? klass->NumStaticFields() : klass->NumInstanceFields();
-  LengthPrefixedArray<ArtField>* const fields = is_static ? klass->GetSFieldsPtr() :
-      klass->GetIFieldsPtr();
+  LengthPrefixedArray<ArtField>* const fields = klass->GetFieldsPtr();
 
   // Initialize field_offset
   MemberOffset field_offset(0);
@@ -9301,7 +9505,8 @@ bool ClassLinker::LinkFieldsHelper::LinkFields(ClassLinker* class_linker,
     }
   }
 
-  CHECK_EQ(num_fields == 0, fields == nullptr) << klass->PrettyClass();
+  size_t num_fields =
+      is_static ? klass->ComputeNumStaticFields() : klass->ComputeNumInstanceFields();
 
   // we want a relatively stable order so that adding new fields
   // minimizes disruption of C++ version such as Class and Method.
@@ -9341,8 +9546,11 @@ bool ClassLinker::LinkFieldsHelper::LinkFields(ClassLinker* class_linker,
   size_t num_reference_fields = 0;
   size_t primitive_fields_start = num_fields;
   DCHECK_LE(num_fields, 1u << 16);
-  for (size_t i = 0; i != num_fields; ++i) {
+  for (size_t i = 0; i != klass->NumFields(); ++i) {
     ArtField* field = &fields->At(i);
+    if (field->IsStatic() != is_static) {
+      continue;
+    }
     const char* descriptor = field->GetTypeDescriptor();
     FieldTypeOrder field_type_order = FieldTypeOrderFromFirstDescriptorCharacter(descriptor[0]);
     uint16_t field_index = dchecked_integral_cast<uint16_t>(i);
@@ -9489,10 +9697,14 @@ bool ClassLinker::LinkFieldsHelper::LinkFields(ClassLinker* class_linker,
       UNLIKELY(!class_linker->init_done_) &&
       klass->DescriptorEquals("Ljava/lang/ref/Reference;")) {
     // We know there are no non-reference fields in the Reference classes, and we know
-    // that 'referent' is alphabetically last, so this is easy...
+    // that 'referent' is alphabetically the last instance field, so this is easy...
+    // Note that we cannot use WellKnownClasses fields yet, as this is not
+    // initialized.
     CHECK_EQ(num_reference_fields, num_fields) << klass->PrettyClass();
-    CHECK_STREQ(fields->At(num_fields - 1).GetName(), "referent")
-        << klass->PrettyClass();
+    CHECK_STREQ(fields->At(klass->NumFields() - 2).GetName(), "referent");
+    CHECK(!fields->At(klass->NumFields() - 2).IsStatic());
+    CHECK_STREQ(fields->At(klass->NumFields() - 1).GetName(), "slowPathEnabled");
+    CHECK(fields->At(klass->NumFields() - 1).IsStatic());
     --num_reference_fields;
   }
 
@@ -9550,8 +9762,11 @@ bool ClassLinker::LinkFieldsHelper::LinkFields(ClassLinker* class_linker,
                                 num_reference_fields *
                                     sizeof(mirror::HeapReference<mirror::Object>));
     MemberOffset current_ref_offset = start_ref_offset;
-    for (size_t i = 0; i < num_fields; i++) {
+    for (size_t i = 0; i < klass->NumFields(); i++) {
       ArtField* field = &fields->At(i);
+      if (field->IsStatic() != is_static) {
+        continue;
+      }
       VLOG(class_linker) << "LinkFields: " << (is_static ? "static" : "instance")
           << " class=" << klass->PrettyClass() << " field=" << field->PrettyField()
           << " offset=" << field->GetOffsetDuringLinking();
@@ -10029,12 +10244,12 @@ ArtMethod* ClassLinker::FindResolvedMethod(ObjPtr<mirror::Class> klass,
   }
   DCHECK(resolved == nullptr || resolved->GetDeclaringClassUnchecked() != nullptr);
   if (resolved != nullptr &&
-      // We pass AccessMethod::kNone instead of kLinking to not warn yet on the
+      // We pass AccessMethod::kCheck instead of kLinking to not warn yet on the
       // access, as we'll be looking if the method can be accessed through an
       // interface.
       hiddenapi::ShouldDenyAccessToMember(resolved,
                                           hiddenapi::AccessContext(class_loader, dex_cache),
-                                          hiddenapi::AccessMethod::kNone)) {
+                                          hiddenapi::AccessMethod::kCheck)) {
     // The resolved method that we have found cannot be accessed due to
     // hiddenapi (typically it is declared up the hierarchy and is not an SDK
     // method). Try to find an interface method from the implemented interfaces which is
@@ -10043,11 +10258,12 @@ ArtMethod* ClassLinker::FindResolvedMethod(ObjPtr<mirror::Class> klass,
     if (itf_method == nullptr) {
       // No interface method. Call ShouldDenyAccessToMember again but this time
       // with AccessMethod::kLinking to ensure that an appropriate warning is
-      // logged.
-      hiddenapi::ShouldDenyAccessToMember(resolved,
-                                          hiddenapi::AccessContext(class_loader, dex_cache),
-                                          hiddenapi::AccessMethod::kLinking);
-      resolved = nullptr;
+      // logged and the enforcement policy is applied.
+      if (hiddenapi::ShouldDenyAccessToMember(resolved,
+                                              hiddenapi::AccessContext(class_loader, dex_cache),
+                                              hiddenapi::AccessMethod::kLinking)) {
+        resolved = nullptr;
+      }
     } else {
       // We found an interface method that is accessible, continue with the resolved method.
     }
@@ -10080,10 +10296,10 @@ static bool CheckNoSuchMethod(ArtMethod* method,
                               ObjPtr<mirror::ClassLoader> class_loader)
       REQUIRES_SHARED(Locks::mutator_lock_) {
   DCHECK(dex_cache->GetClassLoader().Ptr() == class_loader.Ptr());
-  return method == nullptr ||
-         hiddenapi::ShouldDenyAccessToMember(method,
-                                             hiddenapi::AccessContext(class_loader, dex_cache),
-                                             hiddenapi::AccessMethod::kNone);  // no warnings
+  return method == nullptr || hiddenapi::ShouldDenyAccessToMember(
+                                  method,
+                                  hiddenapi::AccessContext(class_loader, dex_cache),
+                                  hiddenapi::AccessMethod::kCheckWithPolicy);  // no warnings
 }
 
 ArtMethod* ClassLinker::FindIncompatibleMethod(ObjPtr<mirror::Class> klass,
@@ -10182,19 +10398,16 @@ ArtField* ClassLinker::FindResolvedField(ObjPtr<mirror::Class> klass,
                                          uint32_t field_idx,
                                          bool is_static) {
   DCHECK(dex_cache->GetClassLoader() == class_loader);
-  ArtField* resolved = is_static ? klass->FindStaticField(dex_cache, field_idx)
-                                 : klass->FindInstanceField(dex_cache, field_idx);
-  if (resolved != nullptr &&
+  ArtField* resolved = klass->FindField(dex_cache, field_idx);
+  if (resolved == nullptr ||
+      is_static != resolved->IsStatic() ||
       hiddenapi::ShouldDenyAccessToMember(resolved,
                                           hiddenapi::AccessContext(class_loader, dex_cache),
                                           hiddenapi::AccessMethod::kLinking)) {
-    resolved = nullptr;
-  }
-
-  if (resolved != nullptr) {
-    dex_cache->SetResolvedField(field_idx, resolved);
+    return nullptr;
   }
 
+  dex_cache->SetResolvedField(field_idx, resolved);
   return resolved;
 }
 
@@ -10348,18 +10561,13 @@ ObjPtr<mirror::MethodHandle> ClassLinker::ResolveMethodHandleForField(
   ArtField* target_field =
       ResolveField(method_handle.field_or_method_idx_, referrer, is_static);
   if (LIKELY(target_field != nullptr)) {
+    DCHECK_EQ(is_static, target_field->IsStatic()) << target_field->PrettyField();
     ObjPtr<mirror::Class> target_class = target_field->GetDeclaringClass();
     ObjPtr<mirror::Class> referring_class = referrer->GetDeclaringClass();
     if (UNLIKELY(!referring_class->CanAccessMember(target_class, target_field->GetAccessFlags()))) {
       ThrowIllegalAccessErrorField(referring_class, target_field);
       return nullptr;
     }
-    // TODO(b/364876321): ResolveField might return instance field when is_static is true and
-    // vice versa.
-    if (UNLIKELY(is_static != target_field->IsStatic())) {
-      ThrowIncompatibleClassChangeErrorField(target_field, is_static, referrer);
-      return nullptr;
-    }
     if (UNLIKELY(is_put && target_field->IsFinal())) {
       ThrowIllegalAccessErrorField(referring_class, target_field);
       return nullptr;
@@ -10369,7 +10577,7 @@ ObjPtr<mirror::MethodHandle> ClassLinker::ResolveMethodHandleForField(
     return nullptr;
   }
 
-  StackHandleScope<4> hs(self);
+  StackHandleScope<5> hs(self);
   ObjPtr<mirror::Class> array_of_class = GetClassRoot<mirror::ObjectArray<mirror::Class>>(this);
   Handle<mirror::ObjectArray<mirror::Class>> method_params(hs.NewHandle(
       mirror::ObjectArray<mirror::Class>::Alloc(self, array_of_class, num_params)));
@@ -10429,7 +10637,8 @@ ObjPtr<mirror::MethodHandle> ClassLinker::ResolveMethodHandleForField(
     return nullptr;
   }
 
-  uintptr_t target = reinterpret_cast<uintptr_t>(target_field);
+  Handle<mirror::Field> target(hs.NewHandle(
+      mirror::Field::CreateFromArtField(self, target_field, /*force_resolve=*/ true)));
   return mirror::MethodHandleImpl::Create(self, target, kind, method_type);
 }
 
diff --git a/runtime/class_linker.h b/runtime/class_linker.h
index 4ca6024442..406172ca5d 100644
--- a/runtime/class_linker.h
+++ b/runtime/class_linker.h
@@ -540,13 +540,19 @@ class ClassLinker {
       REQUIRES(!Locks::dex_lock_)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
+  // Allocating `ArtField` and `ArtMethod` arrays can lead to adding new arenas to
+  // the `LinearAlloc` but the CMC GC's `CompactionPause()` does not expect new
+  // arenas being concurrently added. Therefore we require these allocations to be
+  // done with the mutator lock held shared as this prevents concurrent execution
+  // with the `CompactionPause()` where we hold the mutator lock exclusively.
   LengthPrefixedArray<ArtField>* AllocArtFieldArray(Thread* self,
                                                     LinearAlloc* allocator,
-                                                    size_t length);
-
+                                                    size_t length)
+      REQUIRES_SHARED(Locks::mutator_lock_);
   LengthPrefixedArray<ArtMethod>* AllocArtMethodArray(Thread* self,
                                                       LinearAlloc* allocator,
-                                                      size_t length);
+                                                      size_t length)
+      REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Convenience AllocClass() overload that uses mirror::Class::InitializeClassVisitor
   // for the class initialization and uses the `java_lang_Class` from class roots
@@ -998,6 +1004,7 @@ class ClassLinker {
   class LinkFieldsHelper;
   template <PointerSize kPointerSize>
   class LinkMethodsHelper;
+  class LoadClassHelper;
   class MethodAnnotationsIterator;
   class OatClassCodeIterator;
   class VisiblyInitializedCallback;
@@ -1114,20 +1121,6 @@ class ClassLinker {
   uint32_t SizeOfClassWithoutEmbeddedTables(const DexFile& dex_file,
                                             const dex::ClassDef& dex_class_def);
 
-  void LoadField(const ClassAccessor::Field& field, Handle<mirror::Class> klass, ArtField* dst)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
-  void LoadMethod(const DexFile& dex_file,
-                  const ClassAccessor::Method& method,
-                  ObjPtr<mirror::Class> klass,
-                  /*inout*/ MethodAnnotationsIterator* mai,
-                  /*out*/ ArtMethod* dst)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
-  void LinkCode(ArtMethod* method,
-                uint32_t class_def_method_index,
-                /*inout*/ OatClassCodeIterator* occi) REQUIRES_SHARED(Locks::mutator_lock_);
-
   void FixupStaticTrampolines(Thread* self, ObjPtr<mirror::Class> klass)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
diff --git a/runtime/class_linker_test.cc b/runtime/class_linker_test.cc
index d4fcefef27..1176bb133e 100644
--- a/runtime/class_linker_test.cc
+++ b/runtime/class_linker_test.cc
@@ -110,8 +110,7 @@ class ClassLinkerTest : public CommonRuntimeTest {
     EXPECT_FALSE(primitive->IsSynthetic());
     EXPECT_EQ(0U, primitive->NumDirectMethods());
     EXPECT_EQ(0U, primitive->NumVirtualMethods());
-    EXPECT_EQ(0U, primitive->NumInstanceFields());
-    EXPECT_EQ(0U, primitive->NumStaticFields());
+    EXPECT_EQ(0U, primitive->NumFields());
     EXPECT_EQ(0U, primitive->NumDirectInterfaces());
     EXPECT_FALSE(primitive->HasVTable());
     EXPECT_EQ(0, primitive->GetIfTableCount());
@@ -156,13 +155,12 @@ class ClassLinkerTest : public CommonRuntimeTest {
     EXPECT_FALSE(JavaLangObject->IsSynthetic());
     EXPECT_EQ(4U, JavaLangObject->NumDirectMethods());
     EXPECT_EQ(11U, JavaLangObject->NumVirtualMethods());
-    EXPECT_EQ(2U, JavaLangObject->NumInstanceFields());
-    EXPECT_STREQ(JavaLangObject->GetInstanceField(0)->GetName(),
-                 "shadow$_klass_");
-    EXPECT_STREQ(JavaLangObject->GetInstanceField(1)->GetName(),
-                 "shadow$_monitor_");
+    EXPECT_EQ(2U, JavaLangObject->NumFields());
+    EXPECT_STREQ(JavaLangObject->GetField(0)->GetName(), "shadow$_klass_");
+    EXPECT_FALSE(JavaLangObject->GetField(0)->IsStatic());
+    EXPECT_STREQ(JavaLangObject->GetField(1)->GetName(), "shadow$_monitor_");
+    EXPECT_FALSE(JavaLangObject->GetField(1)->IsStatic());
 
-    EXPECT_EQ(0U, JavaLangObject->NumStaticFields());
     EXPECT_EQ(0U, JavaLangObject->NumDirectInterfaces());
 
     PointerSize pointer_size = class_linker_->GetImagePointerSize();
@@ -220,8 +218,7 @@ class ClassLinkerTest : public CommonRuntimeTest {
     EXPECT_FALSE(array->IsSynthetic());
     EXPECT_EQ(0U, array->NumDirectMethods());
     EXPECT_EQ(0U, array->NumVirtualMethods());
-    EXPECT_EQ(0U, array->NumInstanceFields());
-    EXPECT_EQ(0U, array->NumStaticFields());
+    EXPECT_EQ(0U, array->NumFields());
     EXPECT_EQ(2U, array->NumDirectInterfaces());
     EXPECT_TRUE(array->ShouldHaveImt());
     EXPECT_TRUE(array->ShouldHaveEmbeddedVTable());
@@ -343,27 +340,23 @@ class ClassLinkerTest : public CommonRuntimeTest {
           << "declaring class: " << method.GetDeclaringClass()->PrettyClass();
     }
 
-    for (size_t i = 0; i < klass->NumInstanceFields(); i++) {
-      ArtField* field = klass->GetInstanceField(i);
+    for (size_t i = 0; i < klass->NumFields(); i++) {
+      ArtField* field = klass->GetField(i);
       AssertField(klass.Get(), field);
-      EXPECT_FALSE(field->IsStatic());
-    }
-
-    for (size_t i = 0; i < klass->NumStaticFields(); i++) {
-      ArtField* field = klass->GetStaticField(i);
-      AssertField(klass.Get(), field);
-      EXPECT_TRUE(field->IsStatic());
     }
 
     // Confirm that all instances field offsets are packed together at the start.
-    EXPECT_GE(klass->NumInstanceFields(), klass->NumReferenceInstanceFields());
+    EXPECT_GE(klass->ComputeNumInstanceFields(), klass->NumReferenceInstanceFields());
     MemberOffset start_ref_offset = klass->GetFirstReferenceInstanceFieldOffset();
     MemberOffset end_ref_offset(start_ref_offset.Uint32Value() +
                                 klass->NumReferenceInstanceFields() *
                                     sizeof(mirror::HeapReference<mirror::Object>));
     MemberOffset current_ref_offset = start_ref_offset;
-    for (size_t i = 0; i < klass->NumInstanceFields(); i++) {
-      ArtField* field = klass->GetInstanceField(i);
+    for (size_t i = 0; i < klass->NumFields(); i++) {
+      ArtField* field = klass->GetField(i);
+      if (field->IsStatic()) {
+        continue;
+      }
       ObjPtr<mirror::Class> field_type = field->ResolveType();
       ASSERT_TRUE(field_type != nullptr);
       if (!field->IsPrimitiveType()) {
@@ -499,7 +492,8 @@ struct CheckOffsets {
       }
     }
 
-    size_t num_fields = is_static ? klass->NumStaticFields() : klass->NumInstanceFields();
+    size_t num_fields =
+        is_static ? klass->ComputeNumStaticFields() : klass->ComputeNumInstanceFields();
     if (offsets.size() != num_fields) {
       LOG(ERROR) << "Field count mismatch:"
          << " class=" << class_descriptor
@@ -508,17 +502,25 @@ struct CheckOffsets {
       error = true;
     }
 
+    size_t j = 0;
     for (size_t i = 0; i < offsets.size(); i++) {
-      ArtField* field = is_static ? klass->GetStaticField(i) : klass->GetInstanceField(i);
+      ArtField* field = nullptr;
+      do {
+        field = klass->GetField(j++);
+      } while (field->IsStatic() != is_static);
       std::string_view field_name(field->GetName());
       if (field_name != offsets[i].java_name) {
         error = true;
       }
     }
+    j = 0;
     if (error) {
       for (size_t i = 0; i < offsets.size(); i++) {
         CheckOffset& offset = offsets[i];
-        ArtField* field = is_static ? klass->GetStaticField(i) : klass->GetInstanceField(i);
+        ArtField* field = nullptr;
+        do {
+          field = klass->GetField(j++);
+        } while (field->IsStatic() != is_static);
         std::string_view field_name(field->GetName());
         if (field_name != offsets[i].java_name) {
           LOG(ERROR) << "JAVA FIELD ORDER MISMATCH NEXT LINE:";
@@ -530,17 +532,25 @@ struct CheckOffsets {
       }
     }
 
+    j = 0;
     for (size_t i = 0; i < offsets.size(); i++) {
       CheckOffset& offset = offsets[i];
-      ArtField* field = is_static ? klass->GetStaticField(i) : klass->GetInstanceField(i);
+      ArtField* field = nullptr;
+      do {
+        field = klass->GetField(j++);
+      } while (field->IsStatic() != is_static);
       if (field->GetOffset().Uint32Value() != offset.cpp_offset) {
         error = true;
       }
     }
+    j = 0;
     if (error) {
       for (size_t i = 0; i < offsets.size(); i++) {
         CheckOffset& offset = offsets[i];
-        ArtField* field = is_static ? klass->GetStaticField(i) : klass->GetInstanceField(i);
+        ArtField* field = nullptr;
+        do {
+          field = klass->GetField(j++);
+        } while (field->IsStatic() != is_static);
         if (field->GetOffset().Uint32Value() != offset.cpp_offset) {
           LOG(ERROR) << "OFFSET MISMATCH NEXT LINE:";
         }
@@ -583,7 +593,7 @@ struct ClassOffsets : public CheckOffsets<mirror::Class> {
     addOffset(OFFSETOF_MEMBER(mirror::Class, dex_class_def_idx_), "dexClassDefIndex");
     addOffset(OFFSETOF_MEMBER(mirror::Class, dex_type_idx_), "dexTypeIndex");
     addOffset(OFFSETOF_MEMBER(mirror::Class, ext_data_), "extData");
-    addOffset(OFFSETOF_MEMBER(mirror::Class, ifields_), "iFields");
+    addOffset(OFFSETOF_MEMBER(mirror::Class, fields_), "fields");
     addOffset(OFFSETOF_MEMBER(mirror::Class, iftable_), "ifTable");
     addOffset(OFFSETOF_MEMBER(mirror::Class, methods_), "methods");
     addOffset(OFFSETOF_MEMBER(mirror::Class, name_), "name");
@@ -597,7 +607,6 @@ struct ClassOffsets : public CheckOffsets<mirror::Class> {
     addOffset(OFFSETOF_MEMBER(mirror::Class, primitive_type_), "primitiveType");
     addOffset(OFFSETOF_MEMBER(mirror::Class, reference_instance_offsets_),
               "referenceInstanceOffsets");
-    addOffset(OFFSETOF_MEMBER(mirror::Class, sfields_), "sFields");
     addOffset(OFFSETOF_MEMBER(mirror::Class, status_), "status");
     addOffset(OFFSETOF_MEMBER(mirror::Class, super_class_), "superClass");
     addOffset(OFFSETOF_MEMBER(mirror::Class, virtual_methods_offset_), "virtualMethodsOffset");
@@ -777,6 +786,8 @@ struct MethodHandleOffsets : public CheckOffsets<mirror::MethodHandle> {
 struct MethodHandleImplOffsets : public CheckOffsets<mirror::MethodHandleImpl> {
   MethodHandleImplOffsets() : CheckOffsets<mirror::MethodHandleImpl>(
       false, "Ljava/lang/invoke/MethodHandleImpl;") {
+    addOffset(OFFSETOF_MEMBER(mirror::MethodHandleImpl, field_), "field");
+    addOffset(OFFSETOF_MEMBER(mirror::MethodHandleImpl, target_), "target");
     addOffset(OFFSETOF_MEMBER(mirror::MethodHandleImpl, target_class_or_info_),
               "targetClassOrMethodHandleInfo");
   }
@@ -973,8 +984,7 @@ TEST_F(ClassLinkerTest, FindClass) {
   EXPECT_FALSE(MyClass->IsSynthetic());
   EXPECT_EQ(1U, MyClass->NumDirectMethods());
   EXPECT_EQ(0U, MyClass->NumVirtualMethods());
-  EXPECT_EQ(0U, MyClass->NumInstanceFields());
-  EXPECT_EQ(0U, MyClass->NumStaticFields());
+  EXPECT_EQ(0U, MyClass->NumFields());
   EXPECT_EQ(0U, MyClass->NumDirectInterfaces());
 
   EXPECT_OBJ_PTR_EQ(JavaLangObject->GetClass()->GetClass(), MyClass->GetClass()->GetClass());
@@ -1135,30 +1145,6 @@ TEST_F(ClassLinkerTest, ValidatePrimitiveArrayElementsOffset) {
   // Take it as given that bytes and booleans have byte alignment
 }
 
-TEST_F(ClassLinkerTest, ValidateBoxedTypes) {
-  // Validate that the "value" field is always the 0th field in each of java.lang's box classes.
-  // This lets UnboxPrimitive avoid searching for the field by name at runtime.
-  ScopedObjectAccess soa(Thread::Current());
-  ScopedNullHandle<mirror::ClassLoader> class_loader;
-  ObjPtr<mirror::Class> c;
-  c = FindClass("Ljava/lang/Boolean;", class_loader);
-  EXPECT_STREQ("value", c->GetIFieldsPtr()->At(0).GetName());
-  c = FindClass("Ljava/lang/Byte;", class_loader);
-  EXPECT_STREQ("value", c->GetIFieldsPtr()->At(0).GetName());
-  c = FindClass("Ljava/lang/Character;", class_loader);
-  EXPECT_STREQ("value", c->GetIFieldsPtr()->At(0).GetName());
-  c = FindClass("Ljava/lang/Double;", class_loader);
-  EXPECT_STREQ("value", c->GetIFieldsPtr()->At(0).GetName());
-  c = FindClass("Ljava/lang/Float;", class_loader);
-  EXPECT_STREQ("value", c->GetIFieldsPtr()->At(0).GetName());
-  c = FindClass("Ljava/lang/Integer;", class_loader);
-  EXPECT_STREQ("value", c->GetIFieldsPtr()->At(0).GetName());
-  c = FindClass("Ljava/lang/Long;", class_loader);
-  EXPECT_STREQ("value", c->GetIFieldsPtr()->At(0).GetName());
-  c = FindClass("Ljava/lang/Short;", class_loader);
-  EXPECT_STREQ("value", c->GetIFieldsPtr()->At(0).GetName());
-}
-
 TEST_F(ClassLinkerTest, TwoClassLoadersOneClass) {
   ScopedObjectAccess soa(Thread::Current());
   StackHandleScope<3> hs(soa.Self());
@@ -1187,7 +1173,7 @@ TEST_F(ClassLinkerTest, StaticFields) {
   ArtMethod* clinit = statics->FindClassMethod("<clinit>", "()V", kRuntimePointerSize);
   EXPECT_TRUE(clinit == nullptr);
 
-  EXPECT_EQ(9U, statics->NumStaticFields());
+  EXPECT_EQ(9U, statics->NumFields());
 
   ArtField* s0 = statics->FindStaticField("s0", "Z");
   EXPECT_EQ(s0->GetTypeAsPrimitiveType(), Primitive::kPrimBoolean);
diff --git a/runtime/class_loader_context.cc b/runtime/class_loader_context.cc
index 988955c098..6ede2027dd 100644
--- a/runtime/class_loader_context.cc
+++ b/runtime/class_loader_context.cc
@@ -1119,7 +1119,8 @@ bool ClassLoaderContext::CreateInfoFromClassLoader(
   } else if (IsInMemoryDexClassLoader(class_loader)) {
     type = kInMemoryDexClassLoader;
   } else {
-    LOG(WARNING) << "Unsupported class loader";
+    LOG(WARNING) << "Unsupported class loader: "
+                 << mirror::Class::PrettyClass(class_loader->GetClass());
     return false;
   }
 
diff --git a/runtime/common_runtime_test.cc b/runtime/common_runtime_test.cc
index ab7ddbcb25..16b594d527 100644
--- a/runtime/common_runtime_test.cc
+++ b/runtime/common_runtime_test.cc
@@ -49,6 +49,7 @@
 #include "gc_root-inl.h"
 #include "gtest/gtest.h"
 #include "handle_scope-inl.h"
+#include "instrumentation.h"
 #include "interpreter/unstarted_runtime.h"
 #include "jni/java_vm_ext.h"
 #include "jni/jni_internal.h"
@@ -412,7 +413,7 @@ void CommonRuntimeTestImpl::SetUpRuntimeOptionsForFillHeap(RuntimeOptions *optio
 void CommonRuntimeTestImpl::MakeInterpreted(ObjPtr<mirror::Class> klass) {
   PointerSize pointer_size = class_linker_->GetImagePointerSize();
   for (ArtMethod& method : klass->GetMethods(pointer_size)) {
-    Runtime::Current()->GetInstrumentation()->InitializeMethodsCode(&method, /*aot_code=*/ nullptr);
+    Runtime::Current()->GetInstrumentation()->ReinitializeMethodsCode(&method);
   }
 }
 
diff --git a/runtime/common_runtime_test.h b/runtime/common_runtime_test.h
index 7c20de30fd..1866583058 100644
--- a/runtime/common_runtime_test.h
+++ b/runtime/common_runtime_test.h
@@ -244,10 +244,10 @@ class CommonRuntimeTestBase : public TestType, public CommonRuntimeTestImpl {
   }
 };
 
-using CommonRuntimeTest = CommonRuntimeTestBase<testing::Test>;
+using CommonRuntimeTest = CommonRuntimeTestBase<::testing::Test>;
 
 template <typename Param>
-using CommonRuntimeTestWithParam = CommonRuntimeTestBase<testing::TestWithParam<Param>>;
+using CommonRuntimeTestWithParam = CommonRuntimeTestBase<::testing::TestWithParam<Param>>;
 
 // Sets a CheckJni abort hook to catch failures. Note that this will cause CheckJNI to carry on
 // rather than aborting, so be careful!
diff --git a/runtime/common_throws.cc b/runtime/common_throws.cc
index c1d9a00d5d..f68488a19a 100644
--- a/runtime/common_throws.cc
+++ b/runtime/common_throws.cc
@@ -97,17 +97,27 @@ static void ThrowWrappedException(const char* exception_descriptor,
 
 // AbstractMethodError
 
-void ThrowAbstractMethodError(ArtMethod* method) {
+void ThrowAbstractMethodError(ArtMethod* method, ObjPtr<mirror::Object> receiver) {
+  std::string klass = (receiver == nullptr)
+      ? "null"
+      : mirror::Class::PrettyClass(receiver->GetClass());
   ThrowException("Ljava/lang/AbstractMethodError;", nullptr,
-                 StringPrintf("abstract method \"%s\"",
-                              ArtMethod::PrettyMethod(method).c_str()).c_str());
+                 StringPrintf("abstract method \"%s\" on receiver %s",
+                              ArtMethod::PrettyMethod(method).c_str(),
+                              klass.c_str()).c_str());
 }
 
-void ThrowAbstractMethodError(uint32_t method_idx, const DexFile& dex_file) {
+void ThrowAbstractMethodError(uint32_t method_idx,
+                              const DexFile& dex_file,
+                              ObjPtr<mirror::Object> receiver) {
+  std::string klass = (receiver == nullptr)
+      ? "null"
+      : mirror::Class::PrettyClass(receiver->GetClass());
   ThrowException("Ljava/lang/AbstractMethodError;", /* referrer= */ nullptr,
-                 StringPrintf("abstract method \"%s\"",
+                 StringPrintf("abstract method \"%s\" on receiver %s",
                               dex_file.PrettyMethod(method_idx,
-                                                    /* with_signature= */ true).c_str()).c_str());
+                                                    /* with_signature= */ true).c_str(),
+                              klass.c_str()).c_str());
 }
 
 // ArithmeticException
diff --git a/runtime/common_throws.h b/runtime/common_throws.h
index ccf31a3b8b..02a8d0bdd7 100644
--- a/runtime/common_throws.h
+++ b/runtime/common_throws.h
@@ -41,10 +41,12 @@ constexpr const char kTransactionAbortErrorDescriptor[] = "Ldalvik/system/Transa
 
 // AbstractMethodError
 
-void ThrowAbstractMethodError(ArtMethod* method)
+void ThrowAbstractMethodError(ArtMethod* method, ObjPtr<mirror::Object> receiver)
     REQUIRES_SHARED(Locks::mutator_lock_) COLD_ATTR;
 
-void ThrowAbstractMethodError(uint32_t method_idx, const DexFile& dex_file)
+void ThrowAbstractMethodError(uint32_t method_idx,
+                              const DexFile& dex_file,
+                              ObjPtr<mirror::Object> receiver)
     REQUIRES_SHARED(Locks::mutator_lock_) COLD_ATTR;
 
 // ArithmeticException
diff --git a/runtime/compiler_callbacks.h b/runtime/compiler_callbacks.h
index e196b0b921..40692af5ca 100644
--- a/runtime/compiler_callbacks.h
+++ b/runtime/compiler_callbacks.h
@@ -54,6 +54,7 @@ class CompilerCallbacks {
 
   virtual void AddUncompilableMethod(MethodReference ref) = 0;
   virtual void AddUncompilableClass(ClassReference ref) = 0;
+  virtual bool IsUncompilableMethod(MethodReference ref) = 0;
   virtual void ClassRejected(ClassReference ref) = 0;
 
   virtual verifier::VerifierDeps* GetVerifierDeps() const = 0;
diff --git a/runtime/debugger.h b/runtime/debugger.h
index fd261ab51a..b554a80f51 100644
--- a/runtime/debugger.h
+++ b/runtime/debugger.h
@@ -28,6 +28,7 @@
 #include "base/locks.h"
 #include "base/logging.h"
 #include "base/macros.h"
+#include "instrumentation.h"
 #include "jni.h"
 #include "runtime.h"
 #include "runtime_callbacks.h"
diff --git a/runtime/dex2oat_environment_test.h b/runtime/dex2oat_environment_test.h
index 7e8378aa67..53598bcfa0 100644
--- a/runtime/dex2oat_environment_test.h
+++ b/runtime/dex2oat_environment_test.h
@@ -20,13 +20,16 @@
 #include <sys/wait.h>
 
 #include <fstream>
+#include <memory>
 #include <optional>
 #include <string>
 #include <vector>
 
+#include "android-base/file.h"
 #include "android-base/result.h"
 #include "android-base/strings.h"
 #include "base/file_utils.h"
+#include "base/globals.h"
 #include "base/macros.h"
 #include "base/os.h"
 #include "base/stl_util.h"
@@ -40,6 +43,7 @@
 #include "gc/space/image_space.h"
 #include "gtest/gtest.h"
 #include "oat/oat_file_assistant.h"
+#include "oat/sdc_file.h"
 #include "runtime.h"
 #include "ziparchive/zip_writer.h"
 
@@ -256,7 +260,9 @@ class Dex2oatEnvironmentTest : public Dex2oatScratchDirs, public CommonRuntimeTe
     return WEXITSTATUS(res.status_code);
   }
 
-  void CreateDexMetadata(const std::string& vdex, const std::string& out_dm) {
+  void CreateDexMetadata(const std::string& vdex,
+                         const std::string& out_dm,
+                         bool page_aligned = false) {
     // Read the vdex bytes.
     std::unique_ptr<File> vdex_file(OS::OpenFileForReading(vdex.c_str()));
     std::vector<uint8_t> data(vdex_file->GetLength());
@@ -265,13 +271,55 @@ class Dex2oatEnvironmentTest : public Dex2oatScratchDirs, public CommonRuntimeTe
     // Zip the content.
     FILE* file = fopen(out_dm.c_str(), "wbe");
     ZipWriter writer(file);
-    writer.StartEntry("primary.vdex", ZipWriter::kAlign32);
+    writer.StartAlignedEntry(
+        "primary.vdex", /*flags=*/0, /*alignment=*/page_aligned ? kMaxPageSize : 4);
     writer.WriteBytes(data.data(), data.size());
     writer.FinishEntry();
     writer.Finish();
     fflush(file);
     fclose(file);
   }
+
+  void CreateSecureDexMetadata(const std::string& odex,
+                               const std::string& art,
+                               const std::string& out_sdm) {
+    // Zip the content.
+    std::unique_ptr<File> sdm_file(OS::CreateEmptyFileWriteOnly(out_sdm.c_str()));
+    ASSERT_NE(sdm_file, nullptr);
+    ZipWriter writer(fdopen(sdm_file->Fd(), "wb"));
+
+    std::string odex_data;
+    ASSERT_TRUE(android::base::ReadFileToString(odex, &odex_data));
+    writer.StartAlignedEntry("primary.odex", /*flags=*/0, /*alignment=*/kMaxPageSize);
+    writer.WriteBytes(odex_data.data(), odex_data.size());
+    writer.FinishEntry();
+
+    if (!art.empty()) {
+      std::string art_data;
+      ASSERT_TRUE(android::base::ReadFileToString(art, &art_data));
+      writer.StartAlignedEntry("primary.art", /*flags=*/0, /*alignment=*/kMaxPageSize);
+      writer.WriteBytes(art_data.data(), art_data.size());
+      writer.FinishEntry();
+    }
+
+    writer.Finish();
+    ASSERT_EQ(sdm_file->FlushClose(), 0);
+  }
+
+  void CreateSecureDexMetadataCompanion(const std::string& sdm,
+                                        const std::string& apex_versions,
+                                        const std::string& out_sdc) {
+    struct stat sdm_st;
+    ASSERT_EQ(stat(sdm.c_str(), &sdm_st), 0);
+
+    std::unique_ptr<File> sdc_file(OS::CreateEmptyFileWriteOnly(out_sdc.c_str()));
+    ASSERT_NE(sdc_file, nullptr);
+    SdcWriter sdc_writer(std::move(*sdc_file));
+    sdc_writer.SetSdmTimestampNs(TimeSpecToNs(sdm_st.st_mtim));
+    sdc_writer.SetApexVersions(apex_versions);
+    std::string error_msg;
+    ASSERT_TRUE(sdc_writer.Save(&error_msg)) << error_msg;
+  }
 };
 
 }  // namespace art
diff --git a/runtime/dexopt_test.cc b/runtime/dexopt_test.cc
index edb28823c7..6239f7de87 100644
--- a/runtime/dexopt_test.cc
+++ b/runtime/dexopt_test.cc
@@ -169,7 +169,7 @@ void DexoptTest::GenerateOatForTest(const std::string& dex_location,
                                         context.get(),
                                         /*load_executable=*/false);
 
-    bool match = oat_file_assistant.ValidateBootClassPathChecksums(*odex_file);
+    bool match = oat_file_assistant.ValidateBootClassPathChecksums(*odex_file, &error_msg);
     ASSERT_EQ(!with_alternate_image, match) << error_msg;
   }
 }
@@ -202,6 +202,38 @@ void DexoptTest::GenerateOatForTest(const char* dex_location, CompilerFilter::Fi
   GenerateOatForTest(dex_location, filter, /*with_alternate_image=*/false);
 }
 
+void DexoptTest::GenerateSdmDmForTest(const std::string& dex_location,
+                                      const std::string& sdm_location,
+                                      const std::string& dm_location,
+                                      CompilerFilter::Filter filter,
+                                      bool include_app_image,
+                                      const char* compilation_reason,
+                                      const std::vector<std::string>& extra_args) {
+  std::string tmp_dir = GetScratchDir() + "/sdm_tmp";
+  ASSERT_EQ(0, mkdir(tmp_dir.c_str(), 0700));
+
+  std::string odex_location = tmp_dir + "/TestDex.odex";
+  std::string vdex_location = tmp_dir + "/TestDex.vdex";
+  std::string art_location;
+
+  std::vector<std::string> extra_args_with_app_image = extra_args;
+  if (include_app_image) {
+    art_location = tmp_dir + "/TestDex.art";
+    extra_args_with_app_image.push_back("--app-image-file=" + art_location);
+  }
+
+  // Generate temporary ODEX, VDEX, and ART files in order to create the SDM and DM files from.
+  ASSERT_NO_FATAL_FAILURE(GenerateOdexForTest(
+      dex_location, odex_location, filter, compilation_reason, extra_args_with_app_image));
+
+  // Create the SDM and DM files.
+  ASSERT_NO_FATAL_FAILURE(CreateSecureDexMetadata(odex_location, art_location, sdm_location));
+  ASSERT_NO_FATAL_FAILURE(CreateDexMetadata(vdex_location, dm_location, /*page_aligned=*/true));
+
+  // Cleanup the temporary files.
+  ASSERT_NO_FATAL_FAILURE(ClearDirectory(tmp_dir.c_str()));
+}
+
 void DexoptTest::ReserveImageSpace() {
   MemMap::Init();
 
diff --git a/runtime/dexopt_test.h b/runtime/dexopt_test.h
index cf32785c0b..fa18fdd94b 100644
--- a/runtime/dexopt_test.h
+++ b/runtime/dexopt_test.h
@@ -64,6 +64,16 @@ class DexoptTest : public Dex2oatEnvironmentTest {
   // Generate a standard oat file in the oat location.
   void GenerateOatForTest(const char* dex_location, CompilerFilter::Filter filter);
 
+  // Generate sdm and dm files for the purposes of test.
+  // If `include_app_image` is true, generates an app image and includes it in the sdm file.
+  void GenerateSdmDmForTest(const std::string& dex_location,
+                            const std::string& sdm_location,
+                            const std::string& dm_location,
+                            CompilerFilter::Filter filter,
+                            bool include_app_image,
+                            const char* compilation_reason = nullptr,
+                            const std::vector<std::string>& extra_args = {});
+
   bool Dex2Oat(const std::vector<std::string>& args, std::string* error_msg);
 
  private:
diff --git a/runtime/entrypoints/entrypoint_utils-inl.h b/runtime/entrypoints/entrypoint_utils-inl.h
index 7ddf06f5ab..77bf24f80b 100644
--- a/runtime/entrypoints/entrypoint_utils-inl.h
+++ b/runtime/entrypoints/entrypoint_utils-inl.h
@@ -416,7 +416,7 @@ inline ArtField* ResolveFieldWithAccessChecks(Thread* self,
     return nullptr;
   }
 
-  if (resolve_field_type != 0u) {
+  if (resolve_field_type != 0u && caller->GetDeclaringClass()->HasTypeChecksFailure()) {
     StackArtFieldHandleScope<1> rhs(self);
     ReflectiveHandle<ArtField> field_handle(rhs.NewHandle(resolved_field));
     if (resolved_field->ResolveType().IsNull()) {
@@ -632,6 +632,13 @@ ALWAYS_INLINE ArtMethod* FindSuperMethodToCall(uint32_t method_idx,
   }
 
   if (referenced_class->IsInterface()) {
+    if (!resolved_method->GetDeclaringClass()->IsInterface()) {
+      // invoke-super from interface should not resolve to Object methods.
+      DCHECK(resolved_method->GetDeclaringClass()->IsObjectClass());
+      ThrowIncompatibleClassChangeError(
+          kSuper, resolved_method->GetInvokeType(), resolved_method, referrer);
+      return nullptr;
+    }
     // TODO We can do better than this for a (compiled) fastpath.
     ArtMethod* found_method = referenced_class->FindVirtualMethodForInterfaceSuper(
         resolved_method, linker->GetImagePointerSize());
diff --git a/runtime/entrypoints/quick/quick_jni_entrypoints.cc b/runtime/entrypoints/quick/quick_jni_entrypoints.cc
index 1359fef086..d4dc4714ba 100644
--- a/runtime/entrypoints/quick/quick_jni_entrypoints.cc
+++ b/runtime/entrypoints/quick/quick_jni_entrypoints.cc
@@ -158,7 +158,7 @@ extern uint64_t GenericJniMethodEnd(Thread* self,
   // @CriticalNative does not do a state transition. @FastNative usually does not do a state
   // transition either but it performs a suspend check that may do state transitions.
   if (LIKELY(normal_native)) {
-    if (UNLIKELY(self->ReadFlag(ThreadFlag::kMonitorJniEntryExit))) {
+    if (UNLIKELY(self->ReadFlag(ThreadFlag::kMonitorJniEntryExit, std::memory_order_relaxed))) {
       artJniMonitoredMethodEnd(self);
     } else {
       artJniMethodEnd(self);
diff --git a/runtime/entrypoints/quick/quick_thread_entrypoints.cc b/runtime/entrypoints/quick/quick_thread_entrypoints.cc
index e3511c80d5..2e79605c5e 100644
--- a/runtime/entrypoints/quick/quick_thread_entrypoints.cc
+++ b/runtime/entrypoints/quick/quick_thread_entrypoints.cc
@@ -16,6 +16,7 @@
 
 #include "arch/context.h"
 #include "callee_save_frame.h"
+#include "instrumentation.h"
 #include "jit/jit.h"
 #include "runtime.h"
 #include "thread-inl.h"
diff --git a/runtime/entrypoints/quick/quick_trampoline_entrypoints.cc b/runtime/entrypoints/quick/quick_trampoline_entrypoints.cc
index 6c817d5e16..f7f3673259 100644
--- a/runtime/entrypoints/quick/quick_trampoline_entrypoints.cc
+++ b/runtime/entrypoints/quick/quick_trampoline_entrypoints.cc
@@ -23,8 +23,8 @@
 #include "base/globals.h"
 #include "base/pointer_size.h"
 #include "callee_save_frame.h"
-#include "common_throws.h"
 #include "class_root-inl.h"
+#include "common_throws.h"
 #include "debug_print.h"
 #include "debugger.h"
 #include "dex/dex_file-inl.h"
@@ -57,12 +57,13 @@
 #include "oat/oat_quick_method_header.h"
 #include "quick_exception_handler.h"
 #include "runtime.h"
+#include "runtime_entrypoints_list.h"
 #include "scoped_thread_state_change-inl.h"
 #include "stack.h"
 #include "thread-inl.h"
+#include "trace_profile.h"
 #include "var_handles.h"
 #include "well_known_classes.h"
-#include "runtime_entrypoints_list.h"
 
 namespace art HIDDEN {
 
@@ -1929,8 +1930,17 @@ class BuildGenericJniFrameVisitor final : public QuickArgumentVisitor {
                     fsc.GetStartFprRegs(reserved_area),
                     out_args_sp);
 
+    bool uses_critical_args = critical_native;
+
+#ifdef ART_USE_RESTRICTED_MODE
+    // IsCriticalNative() always returns false so check if the method is actually a critical native
+    // method. If it is then it won't need the JNI environment or jclass arguments.
+    constexpr uint32_t mask = kAccCriticalNative | kAccNative;
+    uses_critical_args = (method->GetAccessFlags() & mask) == mask;
+#endif
+
     // First 2 parameters are always excluded for CriticalNative methods.
-    if (LIKELY(!critical_native)) {
+    if (LIKELY(!uses_critical_args)) {
       // jni environment is always first argument
       sm_.AdvancePointer(self->GetJniEnv());
 
@@ -2101,7 +2111,7 @@ extern "C" const void* artQuickGenericJniTrampoline(Thread* self,
         return nullptr;  // Report error.
       }
     }
-    if (UNLIKELY(self->ReadFlag(ThreadFlag::kMonitorJniEntryExit))) {
+    if (UNLIKELY(self->ReadFlag(ThreadFlag::kMonitorJniEntryExit, std::memory_order_relaxed))) {
       artJniMonitoredMethodStart(self);
     } else {
       artJniMethodStart(self);
@@ -2795,4 +2805,9 @@ extern "C" Context* artMethodExitHook(Thread* self,
   return nullptr;
 }
 
+extern "C" void artRecordLongRunningMethodTraceEvent(ArtMethod* method, Thread* self, bool is_entry)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  TraceProfiler::FlushBufferAndRecordTraceEvent(method, self, is_entry);
+}
+
 }  // namespace art
diff --git a/runtime/exec_utils_test.cc b/runtime/exec_utils_test.cc
index eb21652c19..636383d474 100644
--- a/runtime/exec_utils_test.cc
+++ b/runtime/exec_utils_test.cc
@@ -102,7 +102,7 @@ class NeverFallbackExecUtils : public TestingExecUtils {
   }
 };
 
-class ExecUtilsTest : public CommonRuntimeTest, public testing::WithParamInterface<bool> {
+class ExecUtilsTest : public CommonRuntimeTest, public ::testing::WithParamInterface<bool> {
  protected:
   void SetUp() override {
     CommonRuntimeTest::SetUp();
@@ -390,6 +390,6 @@ TEST_P(ExecUtilsTest, ExecNewProcessGroupFalse) {
                                    &error_msg);
 }
 
-INSTANTIATE_TEST_SUITE_P(AlwaysOrNeverFallback, ExecUtilsTest, testing::Values(true, false));
+INSTANTIATE_TEST_SUITE_P(AlwaysOrNeverFallback, ExecUtilsTest, ::testing::Values(true, false));
 
 }  // namespace art
diff --git a/runtime/fuzzer_corpus_test.cc b/runtime/fuzzer_corpus_test.cc
index 6b312d5973..800cf14cc8 100644
--- a/runtime/fuzzer_corpus_test.cc
+++ b/runtime/fuzzer_corpus_test.cc
@@ -116,12 +116,14 @@ class FuzzerCorpusTest : public CommonRuntimeTest {
 
     // Scope for the handles
     {
-      art::StackHandleScope<3> scope(soa.Self());
+      art::StackHandleScope<4> scope(soa.Self());
       art::Handle<art::mirror::ClassLoader> h_loader =
           scope.NewHandle(soa.Decode<art::mirror::ClassLoader>(class_loader));
       art::MutableHandle<art::mirror::Class> h_klass(scope.NewHandle<art::mirror::Class>(nullptr));
       art::MutableHandle<art::mirror::DexCache> h_dex_cache(
           scope.NewHandle<art::mirror::DexCache>(nullptr));
+      art::MutableHandle<art::mirror::ClassLoader> h_dex_cache_class_loader =
+          scope.NewHandle(h_loader.Get());
 
       for (art::ClassAccessor accessor : dex_file.GetClasses()) {
         h_klass.Assign(
@@ -135,13 +137,17 @@ class FuzzerCorpusTest : public CommonRuntimeTest {
           continue;
         }
         h_dex_cache.Assign(h_klass->GetDexCache());
+
+        // The class loader from the class's dex cache is different from the dex file's class loader
+        // for boot image classes e.g. java.util.AbstractCollection.
+        h_dex_cache_class_loader.Assign(h_klass->GetDexCache()->GetClassLoader());
         verifier::FailureKind failure =
             verifier::ClassVerifier::VerifyClass(soa.Self(),
                                                  /* verifier_deps= */ nullptr,
                                                  h_dex_cache->GetDexFile(),
                                                  h_klass,
                                                  h_dex_cache,
-                                                 h_loader,
+                                                 h_dex_cache_class_loader,
                                                  *h_klass->GetClassDef(),
                                                  runtime->GetCompilerCallbacks(),
                                                  verifier::HardFailLogMode::kLogWarning,
diff --git a/runtime/gc/accounting/card_table-inl.h b/runtime/gc/accounting/card_table-inl.h
index 213836e768..1b060f4cc9 100644
--- a/runtime/gc/accounting/card_table-inl.h
+++ b/runtime/gc/accounting/card_table-inl.h
@@ -51,11 +51,12 @@ static inline bool byte_cas(uint8_t old_value, uint8_t new_value, uint8_t* addre
 #endif
 }
 
-template <bool kClearCard, typename Visitor>
+template <bool kClearCard, typename Visitor, typename ModifyVisitor>
 inline size_t CardTable::Scan(ContinuousSpaceBitmap* bitmap,
                               uint8_t* const scan_begin,
                               uint8_t* const scan_end,
                               const Visitor& visitor,
+                              const ModifyVisitor& mod_visitor,
                               const uint8_t minimum_age) {
   DCHECK_GE(scan_begin, reinterpret_cast<uint8_t*>(bitmap->HeapBegin()));
   // scan_end is the byte after the last byte we scan.
@@ -69,9 +70,11 @@ inline size_t CardTable::Scan(ContinuousSpaceBitmap* bitmap,
 
   // Handle any unaligned cards at the start.
   while (!IsAligned<sizeof(intptr_t)>(card_cur) && card_cur < card_end) {
-    if (*card_cur >= minimum_age) {
+    uint8_t cur_val = *card_cur;
+    if (cur_val >= minimum_age) {
       uintptr_t start = reinterpret_cast<uintptr_t>(AddrFromCard(card_cur));
       bitmap->VisitMarkedRange(start, start + kCardSize, visitor);
+      mod_visitor(card_cur, cur_val);
       ++cards_scanned;
     }
     ++card_cur;
@@ -100,11 +103,13 @@ inline size_t CardTable::Scan(ContinuousSpaceBitmap* bitmap,
       // TODO: Investigate if processing continuous runs of dirty cards with
       // a single bitmap visit is more efficient.
       for (size_t i = 0; i < sizeof(uintptr_t); ++i) {
-        if (static_cast<uint8_t>(start_word) >= minimum_age) {
+        uint8_t cur_val = static_cast<uint8_t>(start_word);
+        if (cur_val >= minimum_age) {
           auto* card = reinterpret_cast<uint8_t*>(word_cur) + i;
           DCHECK(*card == static_cast<uint8_t>(start_word) || *card == kCardDirty)
-              << "card " << static_cast<size_t>(*card) << " intptr_t " << (start_word & 0xFF);
+              << "card " << static_cast<size_t>(*card) << " intptr_t " << cur_val;
           bitmap->VisitMarkedRange(start, start + kCardSize, visitor);
+          mod_visitor(card, cur_val);
           ++cards_scanned;
         }
         start_word >>= 8;
@@ -116,9 +121,11 @@ inline size_t CardTable::Scan(ContinuousSpaceBitmap* bitmap,
     // Handle any unaligned cards at the end.
     card_cur = reinterpret_cast<uint8_t*>(word_end);
     while (card_cur < card_end) {
-      if (*card_cur >= minimum_age) {
+      uint8_t cur_val = *card_cur;
+      if (cur_val >= minimum_age) {
         uintptr_t start = reinterpret_cast<uintptr_t>(AddrFromCard(card_cur));
         bitmap->VisitMarkedRange(start, start + kCardSize, visitor);
+        mod_visitor(card_cur, cur_val);
         ++cards_scanned;
       }
       ++card_cur;
diff --git a/runtime/gc/accounting/card_table.h b/runtime/gc/accounting/card_table.h
index 72cf57119c..98ff107baf 100644
--- a/runtime/gc/accounting/card_table.h
+++ b/runtime/gc/accounting/card_table.h
@@ -21,6 +21,7 @@
 
 #include "base/locks.h"
 #include "base/mem_map.h"
+#include "base/utils.h"
 #include "runtime_globals.h"
 
 namespace art HIDDEN {
@@ -49,8 +50,19 @@ class CardTable {
   static constexpr size_t kCardShift = 10;
   static constexpr size_t kCardSize = 1 << kCardShift;
   static constexpr uint8_t kCardClean = 0x0;
+  // Value written into the card by the write-barrier to indicate that
+  // reference(s) to some object starting in this card has been modified.
   static constexpr uint8_t kCardDirty = 0x70;
+  // Value to indicate that a dirty card is 'aged' now in the sense that it has
+  // been noticed by the GC and will be visited.
   static constexpr uint8_t kCardAged = kCardDirty - 1;
+  // Further ageing an aged card usually means clearing the card as we have
+  // already visited it when ageing it the first time. This value is used to
+  // avoid re-visiting (in the second pass of CMC marking phase) cards which
+  // contain old-to-young references and have not been dirtied since the first
+  // pass of marking. We can't simply clean these cards as they are needed later
+  // in compaction phase to update the old-to-young references.
+  static constexpr uint8_t kCardAged2 = kCardAged - 1;
 
   static CardTable* Create(const uint8_t* heap_begin, size_t heap_capacity);
   ~CardTable();
@@ -114,17 +126,33 @@ class CardTable {
                          const Visitor& visitor,
                          const ModifiedVisitor& modified);
 
-  // For every dirty at least minumum age between begin and end invoke the visitor with the
-  // specified argument. Returns how many cards the visitor was run on.
-  template <bool kClearCard, typename Visitor>
+  // For every dirty (at least minimum age) card between begin and end invoke
+  // bitmap's VisitMarkedRange() to invoke 'visitor' on every object in the
+  // card. Calls 'mod_visitor' for each such card in case the caller wants to
+  // modify the value. Returns how many cards the visitor was run on.
+  // NOTE: 'visitor' is called on one whole card at a time. Therefore,
+  // 'scan_begin' and 'scan_end' are aligned to card-size before visitor is
+  // called. Therefore visitor may get called on objects before 'scan_begin'
+  // and/or after 'scan_end'. Visitor shall detect that and act appropriately.
+  template <bool kClearCard, typename Visitor, typename ModifyVisitor>
   size_t Scan(SpaceBitmap<kObjectAlignment>* bitmap,
               uint8_t* scan_begin,
               uint8_t* scan_end,
               const Visitor& visitor,
-              const uint8_t minimum_age = kCardDirty)
-      REQUIRES(Locks::heap_bitmap_lock_)
+              const ModifyVisitor& mod_visitor,
+              const uint8_t minimum_age) REQUIRES(Locks::heap_bitmap_lock_)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
+  template <bool kClearCard, typename Visitor>
+  size_t Scan(SpaceBitmap<kObjectAlignment>* bitmap,
+              uint8_t* scan_begin,
+              uint8_t* scan_end,
+              const Visitor& visitor,
+              const uint8_t minimum_age = kCardDirty) REQUIRES(Locks::heap_bitmap_lock_)
+      REQUIRES_SHARED(Locks::mutator_lock_) {
+    return Scan<kClearCard>(bitmap, scan_begin, scan_end, visitor, VoidFunctor(), minimum_age);
+  }
+
   // Assertion used to check the given address is covered by the card table
   void CheckAddrIsInCardTable(const uint8_t* addr) const;
 
@@ -169,7 +197,8 @@ class CardTable {
 class AgeCardVisitor {
  public:
   uint8_t operator()(uint8_t card) const {
-    return (card == accounting::CardTable::kCardDirty) ? card - 1 : 0;
+    return (card == accounting::CardTable::kCardDirty) ? accounting::CardTable::kCardAged
+                                                       : accounting::CardTable::kCardClean;
   }
 };
 
diff --git a/runtime/gc/accounting/space_bitmap-inl.h b/runtime/gc/accounting/space_bitmap-inl.h
index 4f9e5a3652..702a78731d 100644
--- a/runtime/gc/accounting/space_bitmap-inl.h
+++ b/runtime/gc/accounting/space_bitmap-inl.h
@@ -32,6 +32,7 @@ namespace accounting {
 
 template<size_t kAlignment>
 inline bool SpaceBitmap<kAlignment>::AtomicTestAndSet(const mirror::Object* obj) {
+  DCHECK(obj != nullptr);
   uintptr_t addr = reinterpret_cast<uintptr_t>(obj);
   DCHECK_GE(addr, heap_begin_);
   const uintptr_t offset = addr - heap_begin_;
@@ -115,6 +116,7 @@ inline void SpaceBitmap<kAlignment>::VisitMarkedRange(uintptr_t visit_begin,
   }
 #else
   DCHECK_LE(heap_begin_, visit_begin);
+  DCHECK_LT(visit_begin, HeapLimit());
   DCHECK_LE(visit_end, HeapLimit());
 
   const uintptr_t offset_start = visit_begin - heap_begin_;
@@ -231,6 +233,7 @@ void SpaceBitmap<kAlignment>::Walk(Visitor&& visitor) {
 template<size_t kAlignment>
 template<bool kSetBit>
 inline bool SpaceBitmap<kAlignment>::Modify(const mirror::Object* obj) {
+  DCHECK(obj != nullptr);
   uintptr_t addr = reinterpret_cast<uintptr_t>(obj);
   DCHECK_GE(addr, heap_begin_);
   DCHECK(HasAddress(obj)) << obj;
diff --git a/runtime/gc/allocation_record.cc b/runtime/gc/allocation_record.cc
index 59ec9f2d1b..f89a0695d8 100644
--- a/runtime/gc/allocation_record.cc
+++ b/runtime/gc/allocation_record.cc
@@ -20,6 +20,7 @@
 #include "base/logging.h"  // For VLOG
 #include "base/pointer_size.h"
 #include "base/stl_util.h"
+#include "instrumentation.h"
 #include "obj_ptr-inl.h"
 #include "object_callbacks.h"
 #include "stack.h"
diff --git a/runtime/gc/collector/concurrent_copying.cc b/runtime/gc/collector/concurrent_copying.cc
index caaa3f6cbc..f666a8f6cc 100644
--- a/runtime/gc/collector/concurrent_copying.cc
+++ b/runtime/gc/collector/concurrent_copying.cc
@@ -155,6 +155,7 @@ ConcurrentCopying::ConcurrentCopying(Heap* heap,
     gc_freed_bytes_delta_ = metrics->YoungGcFreedBytesDelta();
     gc_duration_ = metrics->YoungGcDuration();
     gc_duration_delta_ = metrics->YoungGcDurationDelta();
+    gc_app_slow_path_during_gc_duration_delta_ = metrics->AppSlowPathDuringYoungGcDurationDelta();
   } else {
     gc_time_histogram_ = metrics->FullGcCollectionTime();
     metrics_gc_count_ = metrics->FullGcCount();
@@ -169,6 +170,7 @@ ConcurrentCopying::ConcurrentCopying(Heap* heap,
     gc_freed_bytes_delta_ = metrics->FullGcFreedBytesDelta();
     gc_duration_ = metrics->FullGcDuration();
     gc_duration_delta_ = metrics->FullGcDurationDelta();
+    gc_app_slow_path_during_gc_duration_delta_ = metrics->AppSlowPathDuringFullGcDurationDelta();
   }
 }
 
@@ -410,6 +412,7 @@ void ConcurrentCopying::InitializePhase() {
     rb_slow_path_count_.store(0, std::memory_order_relaxed);
     rb_slow_path_count_gc_.store(0, std::memory_order_relaxed);
   }
+  app_slow_path_start_time_ = 0;
 
   immune_spaces_.Reset();
   bytes_moved_.store(0, std::memory_order_relaxed);
@@ -562,6 +565,7 @@ class ConcurrentCopying::FlipCallback : public Closure {
       cc->from_space_num_bytes_at_first_pause_ = cc->region_space_->GetBytesAllocated();
     }
     cc->is_marking_ = true;
+    cc->app_slow_path_start_time_ = MilliTime();
     if (kIsDebugBuild && !cc->use_generational_cc_) {
       cc->region_space_->AssertAllRegionLiveBytesZeroOrCleared();
     }
@@ -1720,10 +1724,7 @@ class ConcurrentCopying::DisableMarkingCheckpoint : public Closure {
            thread->IsSuspended() ||
            thread->GetState() == ThreadState::kWaitingPerformingGc)
         << thread->GetState() << " thread " << thread << " self " << self;
-    // We sweep interpreter caches here so that it can be done after all
-    // reachable objects are marked and the mutators can sweep their caches
-    // without synchronization.
-    thread->SweepInterpreterCache(concurrent_copying_);
+    thread->GetInterpreterCache()->Clear(thread);
     // Disable the thread-local is_gc_marking flag.
     // Note a thread that has just started right before this checkpoint may have already this flag
     // set to false, which is ok.
@@ -1748,6 +1749,8 @@ class ConcurrentCopying::DisableMarkingCallback : public Closure {
     // to avoid a race with ThreadList::Register().
     CHECK(concurrent_copying_->is_marking_);
     concurrent_copying_->is_marking_ = false;
+    concurrent_copying_->GetCurrentIteration()->SetAppSlowPathDurationMs(
+        MilliTime() - concurrent_copying_->app_slow_path_start_time_);
     if (kUseBakerReadBarrier && kGrayDirtyImmuneObjects) {
       CHECK(concurrent_copying_->is_using_read_barrier_entrypoints_);
       concurrent_copying_->is_using_read_barrier_entrypoints_ = false;
diff --git a/runtime/gc/collector/concurrent_copying.h b/runtime/gc/collector/concurrent_copying.h
index ae94c14113..97d120e1a0 100644
--- a/runtime/gc/collector/concurrent_copying.h
+++ b/runtime/gc/collector/concurrent_copying.h
@@ -464,6 +464,7 @@ class ConcurrentCopying : public GarbageCollector {
   Atomic<uint64_t> rb_slow_path_ns_;
   Atomic<uint64_t> rb_slow_path_count_;
   Atomic<uint64_t> rb_slow_path_count_gc_;
+  uint64_t app_slow_path_start_time_;
   mutable Mutex rb_slow_path_histogram_lock_ DEFAULT_MUTEX_ACQUIRED_AFTER;
   Histogram<uint64_t> rb_slow_path_time_histogram_ GUARDED_BY(rb_slow_path_histogram_lock_);
   uint64_t rb_slow_path_count_total_ GUARDED_BY(rb_slow_path_histogram_lock_);
diff --git a/runtime/gc/collector/garbage_collector.cc b/runtime/gc/collector/garbage_collector.cc
index 798300fa29..14556c5a27 100644
--- a/runtime/gc/collector/garbage_collector.cc
+++ b/runtime/gc/collector/garbage_collector.cc
@@ -95,6 +95,7 @@ void Iteration::Reset(GcCause gc_cause, bool clear_soft_references) {
   timings_.Reset();
   pause_times_.clear();
   duration_ns_ = 0;
+  app_slow_path_duration_ms_ = 0;
   bytes_scanned_ = 0;
   clear_soft_references_ = clear_soft_references;
   gc_cause_ = gc_cause;
@@ -268,6 +269,7 @@ void GarbageCollector::Run(GcCause gc_cause, bool clear_soft_references) {
     gc_freed_bytes_delta_->Add(current_iteration->GetFreedBytes());
     gc_duration_->Add(NsToMs(current_iteration->GetDurationNs()));
     gc_duration_delta_->Add(NsToMs(current_iteration->GetDurationNs()));
+    gc_app_slow_path_during_gc_duration_delta_->Add(current_iteration->GetAppSlowPathDurationMs());
   }
 
   // Report some metrics via the ATrace interface, to surface them in Perfetto.
@@ -323,7 +325,7 @@ void GarbageCollector::SweepArray(accounting::ObjectStack* allocations,
     StackReference<mirror::Object>* out = objects;
     for (size_t i = 0; i < count; ++i) {
       mirror::Object* const obj = objects[i].AsMirrorPtr();
-      if (kUseThreadLocalAllocationStack && obj == nullptr) {
+      if (obj == nullptr) {
         continue;
       }
       if (space->HasAddress(obj)) {
@@ -444,10 +446,10 @@ const Iteration* GarbageCollector::GetCurrentIteration() const {
 
 bool GarbageCollector::ShouldEagerlyReleaseMemoryToOS() const {
   // We have seen old kernels and custom kernel features misbehave in the
-  // presence of too much usage of MADV_FREE. So always release memory eagerly
-  // while we investigate.
-  static constexpr bool kEnableLazyRelease = false;
-  if (!kEnableLazyRelease) {
+  // presence of too much usage of MADV_FREE. So only release memory eagerly
+  // on platforms we know do not have the bug.
+  static const bool gEnableLazyRelease = !kIsTargetBuild || IsKernelVersionAtLeast(6, 0);
+  if (!gEnableLazyRelease) {
     return true;
   }
   Runtime* runtime = Runtime::Current();
diff --git a/runtime/gc/collector/garbage_collector.h b/runtime/gc/collector/garbage_collector.h
index f4ee570808..1f697fd596 100644
--- a/runtime/gc/collector/garbage_collector.h
+++ b/runtime/gc/collector/garbage_collector.h
@@ -190,6 +190,7 @@ class GarbageCollector : public RootVisitor, public IsMarkedVisitor, public Mark
   metrics::MetricsBase<uint64_t>* gc_freed_bytes_delta_;
   metrics::MetricsBase<uint64_t>* gc_duration_;
   metrics::MetricsBase<uint64_t>* gc_duration_delta_;
+  metrics::MetricsBase<uint64_t>* gc_app_slow_path_during_gc_duration_delta_;
   uint64_t total_thread_cpu_time_ns_;
   uint64_t total_time_ns_;
   uint64_t total_freed_objects_;
diff --git a/runtime/gc/collector/immune_spaces.cc b/runtime/gc/collector/immune_spaces.cc
index 4128242cf7..778d47f605 100644
--- a/runtime/gc/collector/immune_spaces.cc
+++ b/runtime/gc/collector/immune_spaces.cc
@@ -51,7 +51,7 @@ void ImmuneSpaces::CreateLargestImmuneRegion() {
       space::ImageSpace* image_space = space->AsImageSpace();
       // Update the end to include the other non-heap sections.
       space_end = RoundUp(reinterpret_cast<uintptr_t>(image_space->GetImageEnd()),
-                          kElfSegmentAlignment);
+                          MemMap::GetPageSize());
       // For the app image case, GetOatFileBegin is where the oat file was mapped during image
       // creation, the actual oat file could be somewhere else.
       const OatFile* const image_oat_file = image_space->GetOatFile();
diff --git a/runtime/gc/collector/iteration.h b/runtime/gc/collector/iteration.h
index d70a30b829..086ed541b0 100644
--- a/runtime/gc/collector/iteration.h
+++ b/runtime/gc/collector/iteration.h
@@ -70,6 +70,8 @@ class Iteration {
   void SetFreedRevoke(uint64_t freed) {
     freed_bytes_revoke_ = freed;
   }
+  uint64_t GetAppSlowPathDurationMs() const { return app_slow_path_duration_ms_; }
+  void SetAppSlowPathDurationMs(uint64_t duration) { app_slow_path_duration_ms_ = duration; }
   void Reset(GcCause gc_cause, bool clear_soft_references);
   // Returns the estimated throughput of the iteration.
   uint64_t GetEstimatedThroughput() const;
@@ -91,6 +93,7 @@ class Iteration {
   GcCause gc_cause_;
   bool clear_soft_references_;
   uint64_t duration_ns_;
+  uint64_t app_slow_path_duration_ms_;
   uint64_t bytes_scanned_;
   TimingLogger timings_;
   ObjectBytePair freed_;
diff --git a/runtime/gc/collector/mark_compact-inl.h b/runtime/gc/collector/mark_compact-inl.h
index d840223720..70db85e657 100644
--- a/runtime/gc/collector/mark_compact-inl.h
+++ b/runtime/gc/collector/mark_compact-inl.h
@@ -41,7 +41,8 @@ template <size_t kAlignment>
 inline uintptr_t MarkCompact::LiveWordsBitmap<kAlignment>::SetLiveWords(uintptr_t begin,
                                                                         size_t size) {
   const uintptr_t begin_bit_idx = MemRangeBitmap::BitIndexFromAddr(begin);
-  DCHECK(!Bitmap::TestBit(begin_bit_idx));
+  DCHECK(!Bitmap::TestBit(begin_bit_idx))
+      << "begin:" << begin << " size:" << size << " begin_bit_idx:" << begin_bit_idx;
   // Range to set bit: [begin, end]
   uintptr_t end = begin + size - kAlignment;
   const uintptr_t end_bit_idx = MemRangeBitmap::BitIndexFromAddr(end);
@@ -201,10 +202,10 @@ inline bool MarkCompact::IsOnAllocStack(mirror::Object* ref) {
   return stack->Contains(ref);
 }
 
-inline void MarkCompact::UpdateRef(mirror::Object* obj,
-                                   MemberOffset offset,
-                                   uint8_t* begin,
-                                   uint8_t* end) {
+inline mirror::Object* MarkCompact::UpdateRef(mirror::Object* obj,
+                                              MemberOffset offset,
+                                              uint8_t* begin,
+                                              uint8_t* end) {
   mirror::Object* old_ref = obj->GetFieldObject<
       mirror::Object, kVerifyNone, kWithoutReadBarrier, /*kIsVolatile*/false>(offset);
   if (kIsDebugBuild) {
@@ -240,6 +241,7 @@ inline void MarkCompact::UpdateRef(mirror::Object* obj,
             offset,
             new_ref);
   }
+  return new_ref;
 }
 
 inline bool MarkCompact::VerifyRootSingleUpdate(void* root,
@@ -280,17 +282,17 @@ inline bool MarkCompact::VerifyRootSingleUpdate(void* root,
       }
     }
     DCHECK(reinterpret_cast<uint8_t*>(old_ref) >= black_allocations_begin_ ||
-           live_words_bitmap_->Test(old_ref))
+           moving_space_bitmap_->Test(old_ref))
         << "ref=" << old_ref << " <" << mirror::Object::PrettyTypeOf(old_ref) << "> RootInfo ["
         << info << "]";
   }
   return true;
 }
 
-inline void MarkCompact::UpdateRoot(mirror::CompressedReference<mirror::Object>* root,
-                                    uint8_t* begin,
-                                    uint8_t* end,
-                                    const RootInfo& info) {
+inline mirror::Object* MarkCompact::UpdateRoot(mirror::CompressedReference<mirror::Object>* root,
+                                               uint8_t* begin,
+                                               uint8_t* end,
+                                               const RootInfo& info) {
   DCHECK(!root->IsNull());
   mirror::Object* old_ref = root->AsMirrorPtr();
   if (VerifyRootSingleUpdate(root, old_ref, info)) {
@@ -298,20 +300,24 @@ inline void MarkCompact::UpdateRoot(mirror::CompressedReference<mirror::Object>*
     if (old_ref != new_ref) {
       root->Assign(new_ref);
     }
+    return new_ref;
   }
+  return nullptr;
 }
 
-inline void MarkCompact::UpdateRoot(mirror::Object** root,
-                                    uint8_t* begin,
-                                    uint8_t* end,
-                                    const RootInfo& info) {
+inline mirror::Object* MarkCompact::UpdateRoot(mirror::Object** root,
+                                               uint8_t* begin,
+                                               uint8_t* end,
+                                               const RootInfo& info) {
   mirror::Object* old_ref = *root;
   if (VerifyRootSingleUpdate(root, old_ref, info)) {
     mirror::Object* new_ref = PostCompactAddress(old_ref, begin, end);
     if (old_ref != new_ref) {
       *root = new_ref;
     }
+    return new_ref;
   }
+  return nullptr;
 }
 
 template <size_t kAlignment>
@@ -362,8 +368,6 @@ inline mirror::Object* MarkCompact::PostCompactAddressUnchecked(mirror::Object*
   }
   if (kIsDebugBuild) {
     mirror::Object* from_ref = GetFromSpaceAddr(old_ref);
-    DCHECK(live_words_bitmap_->Test(old_ref))
-         << "ref=" << old_ref;
     if (!moving_space_bitmap_->Test(old_ref)) {
       std::ostringstream oss;
       Runtime::Current()->GetHeap()->DumpSpaces(oss);
diff --git a/runtime/gc/collector/mark_compact.cc b/runtime/gc/collector/mark_compact.cc
index a7af756970..82f5e1baf5 100644
--- a/runtime/gc/collector/mark_compact.cc
+++ b/runtime/gc/collector/mark_compact.cc
@@ -48,6 +48,7 @@
 #include "gc/collector_type.h"
 #include "gc/reference_processor.h"
 #include "gc/space/bump_pointer_space.h"
+#include "gc/space/space-inl.h"
 #include "gc/task_processor.h"
 #include "gc/verification-inl.h"
 #include "jit/jit_code_cache.h"
@@ -61,14 +62,9 @@
 #ifdef ART_TARGET_ANDROID
 #include "android-modules-utils/sdk_level.h"
 #include "com_android_art.h"
+#include "com_android_art_flags.h"
 #endif
 
-#ifndef __BIONIC__
-#ifndef MREMAP_DONTUNMAP
-#define MREMAP_DONTUNMAP 4
-#endif
-#endif  // __BIONIC__
-
 // See aosp/2996596 for where these values came from.
 #ifndef UFFDIO_COPY_MODE_MMAP_TRYLOCK
 #define UFFDIO_COPY_MODE_MMAP_TRYLOCK (static_cast<uint64_t>(1) << 63)
@@ -317,6 +313,18 @@ static bool ShouldUseUserfaultfd() {
 const bool gUseUserfaultfd = ShouldUseUserfaultfd();
 const bool gUseReadBarrier = !gUseUserfaultfd;
 #endif
+#ifdef ART_TARGET_ANDROID
+bool ShouldUseGenerationalGC() {
+  if (gUseUserfaultfd && !com::android::art::flags::use_generational_cmc()) {
+    return false;
+  }
+  // Generational GC feature doesn't need a reboot. Any process (like dex2oat)
+  // can pick a different values than zygote and will be able to execute.
+  return GetBoolProperty("persist.device_config.runtime_native_boot.use_generational_gc", true);
+}
+#else
+bool ShouldUseGenerationalGC() { return true; }
+#endif
 
 namespace gc {
 namespace collector {
@@ -325,6 +333,10 @@ namespace collector {
 // significantly.
 static constexpr bool kCheckLocks = kDebugLocking;
 static constexpr bool kVerifyRootsMarked = kIsDebugBuild;
+// Verify that there are no missing card marks.
+static constexpr bool kVerifyNoMissingCardMarks = kIsDebugBuild;
+// Verify that all references in post-GC objects are valid.
+static constexpr bool kVerifyPostGCObjects = kIsDebugBuild;
 // Number of compaction buffers reserved for mutator threads in SIGBUS feature
 // case. It's extremely unlikely that we will ever have more than these number
 // of mutator threads trying to access the moving-space during one compaction
@@ -438,18 +450,51 @@ size_t MarkCompact::InitializeInfoMap(uint8_t* p, size_t moving_space_sz) {
   return total;
 }
 
+YoungMarkCompact::YoungMarkCompact(Heap* heap, MarkCompact* main)
+    : GarbageCollector(heap, "young concurrent mark compact"), main_collector_(main) {
+  // Initialize GC metrics.
+  metrics::ArtMetrics* metrics = GetMetrics();
+  gc_time_histogram_ = metrics->YoungGcCollectionTime();
+  metrics_gc_count_ = metrics->YoungGcCount();
+  metrics_gc_count_delta_ = metrics->YoungGcCountDelta();
+  gc_throughput_histogram_ = metrics->YoungGcThroughput();
+  gc_tracing_throughput_hist_ = metrics->YoungGcTracingThroughput();
+  gc_throughput_avg_ = metrics->YoungGcThroughputAvg();
+  gc_tracing_throughput_avg_ = metrics->YoungGcTracingThroughputAvg();
+  gc_scanned_bytes_ = metrics->YoungGcScannedBytes();
+  gc_scanned_bytes_delta_ = metrics->YoungGcScannedBytesDelta();
+  gc_freed_bytes_ = metrics->YoungGcFreedBytes();
+  gc_freed_bytes_delta_ = metrics->YoungGcFreedBytesDelta();
+  gc_duration_ = metrics->YoungGcDuration();
+  gc_duration_delta_ = metrics->YoungGcDurationDelta();
+  gc_app_slow_path_during_gc_duration_delta_ = metrics->AppSlowPathDuringYoungGcDurationDelta();
+  are_metrics_initialized_ = true;
+}
+
+void YoungMarkCompact::RunPhases() {
+  DCHECK(!main_collector_->young_gen_);
+  main_collector_->young_gen_ = true;
+  main_collector_->RunPhases();
+  main_collector_->young_gen_ = false;
+}
+
 MarkCompact::MarkCompact(Heap* heap)
     : GarbageCollector(heap, "concurrent mark compact"),
       gc_barrier_(0),
       lock_("mark compact lock", kGenericBottomLock),
+      sigbus_in_progress_count_{kSigbusCounterCompactionDoneMask, kSigbusCounterCompactionDoneMask},
+      mid_to_old_promo_bit_vec_(nullptr),
       bump_pointer_space_(heap->GetBumpPointerSpace()),
+      post_compact_end_(nullptr),
+      young_gen_(false),
+      use_generational_(heap->GetUseGenerational()),
+      compacting_(false),
       moving_space_bitmap_(bump_pointer_space_->GetMarkBitmap()),
       moving_space_begin_(bump_pointer_space_->Begin()),
       moving_space_end_(bump_pointer_space_->Limit()),
       black_dense_end_(moving_space_begin_),
+      mid_gen_end_(moving_space_begin_),
       uffd_(kFdUnused),
-      sigbus_in_progress_count_{kSigbusCounterCompactionDoneMask, kSigbusCounterCompactionDoneMask},
-      compacting_(false),
       marking_done_(false),
       uffd_initialized_(false),
       clamp_info_map_status_(ClampInfoStatus::kClampInfoNotDone) {
@@ -524,7 +569,6 @@ MarkCompact::MarkCompact(Heap* heap)
 
   // Initialize GC metrics.
   metrics::ArtMetrics* metrics = GetMetrics();
-  // The mark-compact collector supports only full-heap collections at the moment.
   gc_time_histogram_ = metrics->FullGcCollectionTime();
   metrics_gc_count_ = metrics->FullGcCount();
   metrics_gc_count_delta_ = metrics->FullGcCountDelta();
@@ -538,9 +582,16 @@ MarkCompact::MarkCompact(Heap* heap)
   gc_freed_bytes_delta_ = metrics->FullGcFreedBytesDelta();
   gc_duration_ = metrics->FullGcDuration();
   gc_duration_delta_ = metrics->FullGcDurationDelta();
+  gc_app_slow_path_during_gc_duration_delta_ = metrics->AppSlowPathDuringFullGcDurationDelta();
   are_metrics_initialized_ = true;
 }
 
+void MarkCompact::ResetGenerationalState() {
+  black_dense_end_ = mid_gen_end_ = moving_space_begin_;
+  post_compact_end_ = nullptr;
+  class_after_obj_map_.clear();
+}
+
 void MarkCompact::AddLinearAllocSpaceData(uint8_t* begin, size_t len) {
   DCHECK_ALIGNED_PARAM(begin, gPageSize);
   DCHECK_ALIGNED_PARAM(len, gPageSize);
@@ -610,7 +661,9 @@ void MarkCompact::MaybeClampGcStructures() {
   }
 }
 
-void MarkCompact::PrepareCardTableForMarking(bool clear_alloc_space_cards) {
+void MarkCompact::PrepareForMarking(bool pre_marking) {
+  static_assert(gc::accounting::CardTable::kCardDirty - 1 == gc::accounting::CardTable::kCardAged);
+  static_assert(gc::accounting::CardTable::kCardAged - 1 == gc::accounting::CardTable::kCardAged2);
   TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());
   accounting::CardTable* const card_table = heap_->GetCardTable();
   // immune_spaces_ is emptied in InitializePhase() before marking starts. This
@@ -642,29 +695,66 @@ void MarkCompact::PrepareCardTableForMarking(bool clear_alloc_space_cards) {
             },
             /* card modified visitor */ VoidFunctor());
       }
-    } else if (clear_alloc_space_cards) {
+    } else if (pre_marking) {
       CHECK(!space->IsZygoteSpace());
       CHECK(!space->IsImageSpace());
-      // The card-table corresponding to bump-pointer and non-moving space can
-      // be cleared, because we are going to traverse all the reachable objects
-      // in these spaces. This card-table will eventually be used to track
-      // mutations while concurrent marking is going on.
-      card_table->ClearCardRange(space->Begin(), space->Limit());
+      if (young_gen_) {
+        uint8_t* space_age_end = space->Limit();
+        // Age cards in old-gen as they contain old-to-young references.
+        if (space == bump_pointer_space_) {
+          DCHECK_ALIGNED_PARAM(old_gen_end_, gPageSize);
+          moving_space_bitmap_->ClearRange(reinterpret_cast<mirror::Object*>(old_gen_end_),
+                                           reinterpret_cast<mirror::Object*>(moving_space_end_));
+          // Clear cards in [old_gen_end_, moving_space_end_) as they are not needed.
+          card_table->ClearCardRange(old_gen_end_, space->Limit());
+          space_age_end = old_gen_end_;
+        }
+        card_table->ModifyCardsAtomic(space->Begin(),
+                                      space_age_end,
+                                      AgeCardVisitor(),
+                                      /*card modified visitor=*/VoidFunctor());
+      } else {
+        // The card-table corresponding to bump-pointer and non-moving space can
+        // be cleared, because we are going to traverse all the reachable objects
+        // in these spaces. This card-table will eventually be used to track
+        // mutations while concurrent marking is going on.
+        card_table->ClearCardRange(space->Begin(), space->Limit());
+        if (space == bump_pointer_space_) {
+          moving_space_bitmap_->Clear();
+        }
+      }
       if (space != bump_pointer_space_) {
         CHECK_EQ(space, heap_->GetNonMovingSpace());
+        if (young_gen_) {
+          space->AsContinuousMemMapAllocSpace()->BindLiveToMarkBitmap();
+        }
         non_moving_space_ = space;
         non_moving_space_bitmap_ = space->GetMarkBitmap();
       }
     } else {
-      card_table->ModifyCardsAtomic(
-          space->Begin(),
-          space->End(),
-          [](uint8_t card) {
-            return (card == gc::accounting::CardTable::kCardDirty) ?
-                       gc::accounting::CardTable::kCardAged :
-                       gc::accounting::CardTable::kCardClean;
-          },
-          /* card modified visitor */ VoidFunctor());
+      if (young_gen_) {
+        // It would be correct to retain existing aged cards and add dirty cards
+        // to that set. However, that would unecessarily need us to re-scan
+        // cards which haven't been dirtied since first-pass of marking.
+        auto card_visitor = [](uint8_t card) {
+          return (card > gc::accounting::CardTable::kCardAged2)
+                     ? card - 1
+                     : gc::accounting::CardTable::kCardClean;
+        };
+        card_table->ModifyCardsAtomic(
+            space->Begin(), space->End(), card_visitor, /*card modified visitor=*/VoidFunctor());
+      } else {
+        card_table->ModifyCardsAtomic(space->Begin(),
+                                      space->End(),
+                                      AgeCardVisitor(),
+                                      /*card modified visitor=*/VoidFunctor());
+      }
+    }
+  }
+  if (pre_marking && young_gen_) {
+    for (const auto& space : GetHeap()->GetDiscontinuousSpaces()) {
+      CHECK(space->IsLargeObjectSpace());
+      space->AsLargeObjectSpace()->CopyLiveToMarked();
     }
   }
 }
@@ -707,13 +797,16 @@ void MarkCompact::InitializePhase() {
   DCHECK_EQ(moving_space_begin_, bump_pointer_space_->Begin());
   from_space_slide_diff_ = from_space_begin_ - moving_space_begin_;
   moving_space_end_ = bump_pointer_space_->Limit();
-  if (black_dense_end_ > moving_space_begin_) {
-    moving_space_bitmap_->Clear();
+  if (use_generational_ && !young_gen_) {
+    class_after_obj_map_.clear();
   }
-  black_dense_end_ = moving_space_begin_;
   // TODO: Would it suffice to read it once in the constructor, which is called
   // in zygote process?
   pointer_size_ = Runtime::Current()->GetClassLinker()->GetImagePointerSize();
+  for (size_t i = 0; i < vector_length_; i++) {
+    DCHECK_EQ(chunk_info_vec_[i], 0u);
+  }
+  app_slow_path_start_time_ = 0;
 }
 
 class MarkCompact::ThreadFlipVisitor : public Closure {
@@ -729,7 +822,7 @@ class MarkCompact::ThreadFlipVisitor : public Closure {
     // Interpreter cache is thread-local so it needs to be swept either in a
     // flip, or a stop-the-world pause.
     CHECK(collector_->compacting_);
-    thread->SweepInterpreterCache(collector_);
+    thread->GetInterpreterCache()->Clear(thread);
     thread->AdjustTlab(collector_->black_objs_slide_diff_);
   }
 
@@ -753,8 +846,8 @@ void MarkCompact::RunPhases() {
   Thread* self = Thread::Current();
   thread_running_gc_ = self;
   Runtime* runtime = Runtime::Current();
-  InitializePhase();
   GetHeap()->PreGcVerification(this);
+  InitializePhase();
   {
     ReaderMutexLock mu(self, *Locks::mutator_lock_);
     MarkingPhase();
@@ -773,7 +866,6 @@ void MarkCompact::RunPhases() {
     ReclaimPhase();
     perform_compaction = PrepareForCompaction();
   }
-
   if (perform_compaction) {
     // Compaction pause
     ThreadFlipVisitor visitor(this);
@@ -785,8 +877,13 @@ void MarkCompact::RunPhases() {
       ReaderMutexLock mu(self, *Locks::mutator_lock_);
       CompactionPhase();
     }
+  } else {
+    if (use_generational_) {
+      DCHECK_IMPLIES(post_compact_end_ != nullptr, post_compact_end_ == black_allocations_begin_);
+    }
+    post_compact_end_ = black_allocations_begin_;
   }
-  FinishPhase();
+  FinishPhase(perform_compaction);
   GetHeap()->PostGcVerification(this);
   thread_running_gc_ = nullptr;
 }
@@ -931,6 +1028,41 @@ size_t MarkCompact::InitNonMovingFirstObjects(uintptr_t begin,
   return page_idx;
 }
 
+// Generational CMC description
+// ============================
+//
+// All allocations since last GC are considered to be in young generation.
+// Unlike other ART GCs, we promote surviving objects to old generation after
+// they survive two contiguous GCs. Objects that survive one GC are considered
+// to be in mid generation. In the next young GC, marking is performed on both
+// the young as well as mid gen objects. And then during compaction, the
+// surviving mid-gen objects are compacted and then promoted to old-gen, while
+// the surviving young gen objects are compacted and promoted to mid-gen.
+//
+// Some other important points worth explaining:
+//
+// 1. During marking-phase, 'mid_gen_end_' segregates young and mid generations.
+// Before starting compaction, in PrepareForCompaction(), we set it to the
+// corresponding post-compact addresses, aligned up to page-size. Therefore,
+// some object's beginning portion maybe in mid-gen, while the rest is in young-gen.
+// Aligning up is essential as mid_gen_end_ becomes old_gen_end_ at the end of
+// GC cycle, and the latter has to be page-aligned as old-gen pages are
+// processed differently (no compaction).
+//
+// 2. We need to maintain the mark-bitmap for the old-gen for subsequent GCs,
+// when objects are promoted to old-gen from mid-gen, their mark bits are
+// first collected in a BitVector and then later copied into mark-bitmap in
+// FinishPhase(). We can't directly set the bits in mark-bitmap as the bitmap
+// contains pre-compaction mark bits which are required during compaction.
+//
+// 3. Since we need to revisit mid-gen objects in the next GC cycle, we need to
+// dirty the cards in old-gen containing references to them. We identify these
+// references when visiting old-gen objects during compaction. However, native
+// roots are skipped at that time (they are updated separately in linear-alloc
+// space, where we don't know which object (dex-cache/class-loader/class) does
+// a native root belong to. Therefore, native roots are covered during marking
+// phase.
+
 bool MarkCompact::PrepareForCompaction() {
   TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());
   size_t chunk_info_per_page = gPageSize / kOffsetChunkSize;
@@ -938,11 +1070,13 @@ bool MarkCompact::PrepareForCompaction() {
   DCHECK_LE(vector_len, vector_length_);
   DCHECK_ALIGNED_PARAM(vector_length_, chunk_info_per_page);
   if (UNLIKELY(vector_len == 0)) {
-    // Nothing to compact.
+    // Nothing to compact. Entire heap is empty.
+    black_dense_end_ = mid_gen_end_ = moving_space_begin_;
     return false;
   }
   for (size_t i = 0; i < vector_len; i++) {
-    DCHECK_LE(chunk_info_vec_[i], kOffsetChunkSize);
+    DCHECK_LE(chunk_info_vec_[i], kOffsetChunkSize)
+        << "i:" << i << " vector_length:" << vector_len << " vector_length_:" << vector_length_;
     DCHECK_EQ(chunk_info_vec_[i], live_words_bitmap_->LiveBytesInBitmapWord(i));
   }
 
@@ -970,8 +1104,24 @@ bool MarkCompact::PrepareForCompaction() {
 
   size_t black_dense_idx = 0;
   GcCause gc_cause = GetCurrentIteration()->GetGcCause();
-  if (gc_cause != kGcCauseExplicit && gc_cause != kGcCauseCollectorTransition &&
-      !GetCurrentIteration()->GetClearSoftReferences()) {
+  if (young_gen_) {
+    DCHECK_ALIGNED_PARAM(old_gen_end_, gPageSize);
+    DCHECK_GE(mid_gen_end_, old_gen_end_);
+    DCHECK_GE(black_allocations_begin_, mid_gen_end_);
+    // old-gen's boundary was decided at the end of previous GC-cycle.
+    black_dense_idx = (old_gen_end_ - moving_space_begin_) / kOffsetChunkSize;
+    if (black_dense_idx == vector_len) {
+      // There is nothing live in young-gen.
+      DCHECK_EQ(old_gen_end_, black_allocations_begin_);
+      mid_gen_end_ = black_allocations_begin_;
+      return false;
+    }
+    InitNonMovingFirstObjects(reinterpret_cast<uintptr_t>(moving_space_begin_),
+                              reinterpret_cast<uintptr_t>(old_gen_end_),
+                              moving_space_bitmap_,
+                              first_objs_moving_space_);
+  } else if (gc_cause != kGcCauseExplicit && gc_cause != kGcCauseCollectorTransition &&
+             !GetCurrentIteration()->GetClearSoftReferences()) {
     uint64_t live_bytes = 0, total_bytes = 0;
     size_t aligned_vec_len = RoundUp(vector_len, chunk_info_per_page);
     size_t num_pages = aligned_vec_len / chunk_info_per_page;
@@ -1020,20 +1170,23 @@ bool MarkCompact::PrepareForCompaction() {
     black_dense_idx = (black_dense_end_ - moving_space_begin_) / kOffsetChunkSize;
     DCHECK_LE(black_dense_idx, vector_len);
     if (black_dense_idx == vector_len) {
-      // There is nothing to compact.
+      // There is nothing to compact. All the in-use pages are completely full.
+      mid_gen_end_ = black_allocations_begin_;
       return false;
     }
     InitNonMovingFirstObjects(reinterpret_cast<uintptr_t>(moving_space_begin_),
                               reinterpret_cast<uintptr_t>(black_dense_end_),
                               moving_space_bitmap_,
                               first_objs_moving_space_);
+  } else {
+    black_dense_end_ = moving_space_begin_;
   }
 
   InitMovingSpaceFirstObjects(vector_len, black_dense_idx / chunk_info_per_page);
   non_moving_first_objs_count_ =
       InitNonMovingFirstObjects(reinterpret_cast<uintptr_t>(non_moving_space_->Begin()),
                                 reinterpret_cast<uintptr_t>(non_moving_space_->End()),
-                                non_moving_space_->GetLiveBitmap(),
+                                non_moving_space_bitmap_,
                                 first_objs_non_moving_space_);
   // Update the vector one past the heap usage as it is required for black
   // allocated objects' post-compact address computation.
@@ -1061,14 +1214,87 @@ bool MarkCompact::PrepareForCompaction() {
   black_objs_slide_diff_ = black_allocations_begin_ - post_compact_end_;
   // We shouldn't be consuming more space after compaction than pre-compaction.
   CHECK_GE(black_objs_slide_diff_, 0);
+  for (size_t i = vector_len; i < vector_length_; i++) {
+    DCHECK_EQ(chunk_info_vec_[i], 0u);
+  }
   if (black_objs_slide_diff_ == 0) {
-    black_dense_end_ = black_allocations_begin_;
+    // Regardless of the gc-type, there are no pages to be compacted. Ensure
+    // that we don't shrink the mid-gen, which will become old-gen in
+    // FinishPhase(), thereby possibly moving some objects back to young-gen,
+    // which can cause memory corruption due to missing card marks.
+    mid_gen_end_ = std::max(mid_gen_end_, black_dense_end_);
+    mid_gen_end_ = std::min(mid_gen_end_, post_compact_end_);
     return false;
   }
-  for (size_t i = vector_len; i < vector_length_; i++) {
-    DCHECK_EQ(chunk_info_vec_[i], 0u);
+  if (use_generational_) {
+    // Current value of mid_gen_end_ represents end of 'pre-compacted' mid-gen,
+    // which was done at the end of previous GC. Compute, 'post-compacted' end of
+    // mid-gen, which will be consumed by old-gen at the end of this GC cycle.
+    DCHECK_NE(mid_gen_end_, nullptr);
+    mirror::Object* first_obj = nullptr;
+    if (mid_gen_end_ < black_allocations_begin_) {
+      ReaderMutexLock rmu(thread_running_gc_, *Locks::heap_bitmap_lock_);
+      // Find the first live object in the young-gen.
+      moving_space_bitmap_->VisitMarkedRange</*kVisitOnce=*/true>(
+          reinterpret_cast<uintptr_t>(mid_gen_end_),
+          reinterpret_cast<uintptr_t>(black_allocations_begin_),
+          [&first_obj](mirror::Object* obj) { first_obj = obj; });
+    }
+    if (first_obj != nullptr) {
+      mirror::Object* compacted_obj;
+      if (reinterpret_cast<uint8_t*>(first_obj) >= old_gen_end_) {
+        // post-compact address of the first live object in young-gen.
+        compacted_obj = PostCompactOldObjAddr(first_obj);
+        DCHECK_LT(reinterpret_cast<uint8_t*>(compacted_obj), post_compact_end_);
+      } else {
+        DCHECK(!young_gen_);
+        compacted_obj = first_obj;
+      }
+      // It's important to page-align mid-gen boundary. However, that means
+      // there could be an object overlapping that boundary. We will deal with
+      // the consequences of that at different places. Aligning up is important
+      // to ensure that we don't de-promote an object from old-gen back to
+      // young-gen. Otherwise, we may skip dirtying card for such an object if
+      // it contains native-roots to young-gen.
+      mid_gen_end_ = AlignUp(reinterpret_cast<uint8_t*>(compacted_obj), gPageSize);
+      // We need to ensure that for any object in old-gen, its class is also in
+      // there (for the same reason as mentioned above in the black-dense case).
+      // So adjust mid_gen_end_ accordingly, in the worst case all the way up
+      // to post_compact_end_.
+      auto iter = class_after_obj_map_.lower_bound(ObjReference::FromMirrorPtr(first_obj));
+      for (; iter != class_after_obj_map_.end(); iter++) {
+        // 'mid_gen_end_' is now post-compact, so need to compare with
+        // post-compact addresses.
+        compacted_obj =
+            PostCompactAddress(iter->second.AsMirrorPtr(), old_gen_end_, moving_space_end_);
+        // We cannot update the map with post-compact addresses yet as compaction-phase
+        // expects pre-compacted addresses. So we will update in FinishPhase().
+        if (reinterpret_cast<uint8_t*>(compacted_obj) < mid_gen_end_) {
+          mirror::Object* klass = iter->first.AsMirrorPtr();
+          DCHECK_LT(reinterpret_cast<uint8_t*>(klass), black_allocations_begin_);
+          klass = PostCompactAddress(klass, old_gen_end_, moving_space_end_);
+          // We only need to make sure that the class object doesn't move during
+          // compaction, which can be ensured by just making its first word be
+          // consumed in to the old-gen.
+          mid_gen_end_ =
+              std::max(mid_gen_end_, reinterpret_cast<uint8_t*>(klass) + kObjectAlignment);
+          mid_gen_end_ = AlignUp(mid_gen_end_, gPageSize);
+        }
+      }
+      CHECK_LE(mid_gen_end_, post_compact_end_);
+    } else {
+      // Young-gen is empty.
+      mid_gen_end_ = post_compact_end_;
+    }
+    DCHECK_LE(mid_gen_end_, post_compact_end_);
+    // We need this temporary bitmap only when running in generational mode.
+    if (old_gen_end_ < mid_gen_end_) {
+      mid_to_old_promo_bit_vec_.reset(
+          new BitVector((mid_gen_end_ - old_gen_end_) / kObjectAlignment,
+                        /*expandable=*/false,
+                        Allocator::GetCallocAllocator()));
+    }
   }
-
   // How do we handle compaction of heap portion used for allocations after the
   // marking-pause?
   // All allocations after the marking-pause are considered black (reachable)
@@ -1104,11 +1330,10 @@ void MarkCompact::ReMarkRoots(Runtime* runtime) {
   DCHECK_EQ(thread_running_gc_, Thread::Current());
   Locks::mutator_lock_->AssertExclusiveHeld(thread_running_gc_);
   MarkNonThreadRoots(runtime);
-  MarkConcurrentRoots(static_cast<VisitRootFlags>(kVisitRootFlagNewRoots
-                                                  | kVisitRootFlagStopLoggingNewRoots
-                                                  | kVisitRootFlagClearRootLog),
-                      runtime);
-
+  MarkConcurrentRoots(
+      static_cast<VisitRootFlags>(kVisitRootFlagNewRoots | kVisitRootFlagStopLoggingNewRoots |
+                                  kVisitRootFlagClearRootLog),
+      runtime);
   if (kVerifyRootsMarked) {
     TimingLogger::ScopedTiming t2("(Paused)VerifyRoots", GetTimings());
     VerifyRootMarkedVisitor visitor(this);
@@ -1137,6 +1362,7 @@ void MarkCompact::MarkingPause() {
         bump_pointer_space_->RevokeThreadLocalBuffers(thread);
       }
     }
+    ProcessMarkStack();
     // Fetch only the accumulated objects-allocated count as it is guaranteed to
     // be up-to-date after the TLAB revocation above.
     freed_objects_ += bump_pointer_space_->GetAccumulatedObjectsAllocated();
@@ -1193,30 +1419,48 @@ void MarkCompact::ProcessReferences(Thread* self) {
   GetHeap()->GetReferenceProcessor()->ProcessReferences(self, GetTimings());
 }
 
+void MarkCompact::SweepArray(accounting::ObjectStack* obj_arr, bool swap_bitmaps) {
+  TimingLogger::ScopedTiming t("SweepArray", GetTimings());
+  std::vector<space::ContinuousSpace*> sweep_spaces;
+  for (space::ContinuousSpace* space : heap_->GetContinuousSpaces()) {
+    if (!space->IsAllocSpace() || space == bump_pointer_space_ ||
+        immune_spaces_.ContainsSpace(space) || space->GetLiveBitmap() == nullptr) {
+      continue;
+    }
+    sweep_spaces.push_back(space);
+  }
+  GarbageCollector::SweepArray(obj_arr, swap_bitmaps, &sweep_spaces);
+}
+
 void MarkCompact::Sweep(bool swap_bitmaps) {
   TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());
-  // Ensure that nobody inserted objects in the live stack after we swapped the
-  // stacks.
-  CHECK_GE(live_stack_freeze_size_, GetHeap()->GetLiveStack()->Size());
-  {
-    TimingLogger::ScopedTiming t2("MarkAllocStackAsLive", GetTimings());
-    // Mark everything allocated since the last GC as live so that we can sweep
-    // concurrently, knowing that new allocations won't be marked as live.
-    accounting::ObjectStack* live_stack = heap_->GetLiveStack();
-    heap_->MarkAllocStackAsLive(live_stack);
-    live_stack->Reset();
-    DCHECK(mark_stack_->IsEmpty());
-  }
-  for (const auto& space : GetHeap()->GetContinuousSpaces()) {
-    if (space->IsContinuousMemMapAllocSpace() && space != bump_pointer_space_ &&
-        !immune_spaces_.ContainsSpace(space)) {
-      space::ContinuousMemMapAllocSpace* alloc_space = space->AsContinuousMemMapAllocSpace();
-      DCHECK(!alloc_space->IsZygoteSpace());
-      TimingLogger::ScopedTiming split("SweepMallocSpace", GetTimings());
-      RecordFree(alloc_space->Sweep(swap_bitmaps));
+  if (young_gen_) {
+    // Only sweep objects on the live stack.
+    SweepArray(heap_->GetLiveStack(), /*swap_bitmaps=*/false);
+  } else {
+    // Ensure that nobody inserted objects in the live stack after we swapped the
+    // stacks.
+    CHECK_GE(live_stack_freeze_size_, GetHeap()->GetLiveStack()->Size());
+    {
+      TimingLogger::ScopedTiming t2("MarkAllocStackAsLive", GetTimings());
+      // Mark everything allocated since the last GC as live so that we can sweep
+      // concurrently, knowing that new allocations won't be marked as live.
+      accounting::ObjectStack* live_stack = heap_->GetLiveStack();
+      heap_->MarkAllocStackAsLive(live_stack);
+      live_stack->Reset();
+      DCHECK(mark_stack_->IsEmpty());
+    }
+    for (const auto& space : GetHeap()->GetContinuousSpaces()) {
+      if (space->IsContinuousMemMapAllocSpace() && space != bump_pointer_space_ &&
+          !immune_spaces_.ContainsSpace(space)) {
+        space::ContinuousMemMapAllocSpace* alloc_space = space->AsContinuousMemMapAllocSpace();
+        DCHECK(!alloc_space->IsZygoteSpace());
+        TimingLogger::ScopedTiming split("SweepMallocSpace", GetTimings());
+        RecordFree(alloc_space->Sweep(swap_bitmaps));
+      }
     }
+    SweepLargeObjects(swap_bitmaps);
   }
-  SweepLargeObjects(swap_bitmaps);
 }
 
 void MarkCompact::SweepLargeObjects(bool swap_bitmaps) {
@@ -1251,27 +1495,51 @@ void MarkCompact::ReclaimPhase() {
     // Unbind the live and mark bitmaps.
     GetHeap()->UnBindBitmaps();
   }
+  // After sweeping and unbinding, we will need to use non-moving space'
+  // live-bitmap, instead of mark-bitmap.
+  non_moving_space_bitmap_ = non_moving_space_->GetLiveBitmap();
 }
 
 // We want to avoid checking for every reference if it's within the page or
 // not. This can be done if we know where in the page the holder object lies.
 // If it doesn't overlap either boundaries then we can skip the checks.
-template <bool kCheckBegin, bool kCheckEnd>
+//
+// If kDirtyOldToMid = true, then check if the object contains any references
+// into young-gen, which will be mid-gen after this GC. This is required
+// as we mark and compact mid-gen again in next GC-cycle, and hence cards
+// need to be dirtied. Note that even black-allocations (the next young-gen)
+// will also have to be checked because the pages are being compacted and hence
+// the card corresponding to the compacted page needs to be dirtied.
+template <bool kCheckBegin, bool kCheckEnd, bool kDirtyOldToMid>
 class MarkCompact::RefsUpdateVisitor {
  public:
-  explicit RefsUpdateVisitor(MarkCompact* collector,
-                             mirror::Object* obj,
-                             uint8_t* begin,
-                             uint8_t* end)
+  RefsUpdateVisitor(MarkCompact* collector,
+                    mirror::Object* obj,
+                    uint8_t* begin,
+                    uint8_t* end,
+                    accounting::CardTable* card_table = nullptr,
+                    mirror::Object* card_obj = nullptr)
+      : RefsUpdateVisitor(collector, obj, begin, end, false) {
+    DCHECK(!kCheckBegin || begin != nullptr);
+    DCHECK(!kCheckEnd || end != nullptr);
+    // We can skip checking each reference for objects whose cards are already dirty.
+    if (kDirtyOldToMid && card_obj != nullptr) {
+      dirty_card_ = card_table->IsDirty(card_obj);
+    }
+  }
+
+  RefsUpdateVisitor(
+      MarkCompact* collector, mirror::Object* obj, uint8_t* begin, uint8_t* end, bool dirty_card)
       : collector_(collector),
         moving_space_begin_(collector->black_dense_end_),
         moving_space_end_(collector->moving_space_end_),
+        young_gen_begin_(collector->mid_gen_end_),
         obj_(obj),
         begin_(begin),
-        end_(end) {
-    DCHECK(!kCheckBegin || begin != nullptr);
-    DCHECK(!kCheckEnd || end != nullptr);
-  }
+        end_(end),
+        dirty_card_(dirty_card) {}
+
+  bool ShouldDirtyCard() const { return dirty_card_; }
 
   void operator()([[maybe_unused]] mirror::Object* old,
                   MemberOffset offset,
@@ -1283,7 +1551,9 @@ class MarkCompact::RefsUpdateVisitor {
       update = (!kCheckBegin || ref >= begin_) && (!kCheckEnd || ref < end_);
     }
     if (update) {
-      collector_->UpdateRef(obj_, offset, moving_space_begin_, moving_space_end_);
+      mirror::Object* new_ref =
+          collector_->UpdateRef(obj_, offset, moving_space_begin_, moving_space_end_);
+      CheckShouldDirtyCard(new_ref);
     }
   }
 
@@ -1296,7 +1566,9 @@ class MarkCompact::RefsUpdateVisitor {
                   [[maybe_unused]] bool is_static,
                   [[maybe_unused]] bool is_obj_array) const ALWAYS_INLINE
       REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES_SHARED(Locks::heap_bitmap_lock_) {
-    collector_->UpdateRef(obj_, offset, moving_space_begin_, moving_space_end_);
+    mirror::Object* new_ref =
+        collector_->UpdateRef(obj_, offset, moving_space_begin_, moving_space_end_);
+    CheckShouldDirtyCard(new_ref);
   }
 
   void VisitRootIfNonNull(mirror::CompressedReference<mirror::Object>* root) const
@@ -1310,18 +1582,38 @@ class MarkCompact::RefsUpdateVisitor {
   void VisitRoot(mirror::CompressedReference<mirror::Object>* root) const
       ALWAYS_INLINE
       REQUIRES_SHARED(Locks::mutator_lock_) {
-    collector_->UpdateRoot(root, moving_space_begin_, moving_space_end_);
+    mirror::Object* new_ref = collector_->UpdateRoot(root, moving_space_begin_, moving_space_end_);
+    CheckShouldDirtyCard(new_ref);
   }
 
  private:
+  inline void CheckShouldDirtyCard(mirror::Object* ref) const {
+    if (kDirtyOldToMid && !dirty_card_) {
+      // moving_space_end_ is young-gen's end.
+      dirty_card_ = reinterpret_cast<uint8_t*>(ref) >= young_gen_begin_ &&
+                    reinterpret_cast<uint8_t*>(ref) < moving_space_end_;
+    }
+  }
+
   MarkCompact* const collector_;
   uint8_t* const moving_space_begin_;
   uint8_t* const moving_space_end_;
+  uint8_t* const young_gen_begin_;
   mirror::Object* const obj_;
   uint8_t* const begin_;
   uint8_t* const end_;
+  mutable bool dirty_card_;
 };
 
+inline void MarkCompact::SetBitForMidToOldPromotion(uint8_t* obj) {
+  DCHECK(use_generational_);
+  DCHECK_GE(obj, old_gen_end_);
+  DCHECK_LT(obj, mid_gen_end_);
+  // This doesn't need to be atomic as every thread only sets bits in the
+  // bit_vector words corresponding to the page it is compacting.
+  mid_to_old_promo_bit_vec_->SetBit((obj - old_gen_end_) / kObjectAlignment);
+}
+
 bool MarkCompact::IsValidObject(mirror::Object* obj) const {
   mirror::Class* klass = obj->GetClass<kVerifyNone, kWithoutReadBarrier>();
   if (!heap_->GetVerification()->IsValidHeapObjectAddress(klass)) {
@@ -1345,7 +1637,9 @@ void MarkCompact::VerifyObject(mirror::Object* ref, Callback& callback) const {
           << " post_compact_end=" << static_cast<void*>(post_compact_end_)
           << " pre_compact_klass=" << pre_compact_klass
           << " black_allocations_begin=" << static_cast<void*>(black_allocations_begin_);
-      CHECK(live_words_bitmap_->Test(pre_compact_klass));
+      if (!young_gen_) {
+        CHECK(live_words_bitmap_->Test(pre_compact_klass));
+      }
     }
     if (!IsValidObject(ref)) {
       std::ostringstream oss;
@@ -1374,10 +1668,13 @@ void MarkCompact::VerifyObject(mirror::Object* ref, Callback& callback) const {
   }
 }
 
+template <bool kSetupForGenerational>
 void MarkCompact::CompactPage(mirror::Object* obj,
                               uint32_t offset,
                               uint8_t* addr,
+                              uint8_t* to_space_addr,
                               bool needs_memset_zero) {
+  DCHECK_ALIGNED_PARAM(to_space_addr, gPageSize);
   DCHECK(moving_space_bitmap_->Test(obj)
          && live_words_bitmap_->Test(obj));
   DCHECK(live_words_bitmap_->Test(offset)) << "obj=" << obj
@@ -1387,30 +1684,42 @@ void MarkCompact::CompactPage(mirror::Object* obj,
                                            << static_cast<void*>(black_allocations_begin_)
                                            << " post_compact_addr="
                                            << static_cast<void*>(post_compact_end_);
+  accounting::CardTable* card_table = heap_->GetCardTable();
   uint8_t* const start_addr = addr;
+  // We need to find the cards in the mid-gen (which is going to be consumed
+  // into old-gen after this GC) for dirty cards (dirtied after marking-pause and
+  // until compaction pause) and dirty the corresponding post-compact cards. We
+  // could have found reference fields while updating them in RefsUpdateVisitor.
+  // But it will not catch native-roots and hence we need to directly look at the
+  // pre-compact card-table.
+  // NOTE: we may get some false-positives if the same address in post-compact
+  // heap is already allocated as TLAB and has been having write-barrers be
+  // called. But that is not harmful.
+  size_t cards_per_page = gPageSize >> accounting::CardTable::kCardShift;
+  size_t dest_cards = 0;
+  DCHECK(IsAligned<accounting::CardTable::kCardSize>(gPageSize));
+  static_assert(sizeof(dest_cards) * kBitsPerByte >=
+                kMaxPageSize / accounting::CardTable::kCardSize);
   // How many distinct live-strides do we have.
   size_t stride_count = 0;
   uint8_t* last_stride = addr;
   uint32_t last_stride_begin = 0;
-  auto verify_obj_callback = [&] (std::ostream& os) {
-                               os << " stride_count=" << stride_count
-                                  << " last_stride=" << static_cast<void*>(last_stride)
-                                  << " offset=" << offset
-                                  << " start_addr=" << static_cast<void*>(start_addr);
-                             };
-  obj = GetFromSpaceAddr(obj);
+  auto verify_obj_callback = [&](std::ostream& os) {
+    os << " stride_count=" << stride_count << " last_stride=" << static_cast<void*>(last_stride)
+       << " offset=" << offset << " start_addr=" << static_cast<void*>(start_addr);
+  };
   live_words_bitmap_->VisitLiveStrides(
       offset,
       black_allocations_begin_,
       gPageSize,
-      [&addr, &last_stride, &stride_count, &last_stride_begin, verify_obj_callback, this](
-          uint32_t stride_begin, size_t stride_size, [[maybe_unused]] bool is_last)
+      [&](uint32_t stride_begin, size_t stride_size, [[maybe_unused]] bool is_last)
           REQUIRES_SHARED(Locks::mutator_lock_) {
-            const size_t stride_in_bytes = stride_size * kAlignment;
+            size_t stride_in_bytes = stride_size * kAlignment;
+            size_t stride_begin_bytes = stride_begin * kAlignment;
             DCHECK_LE(stride_in_bytes, gPageSize);
             last_stride_begin = stride_begin;
             DCHECK(IsAligned<kAlignment>(addr));
-            memcpy(addr, from_space_begin_ + stride_begin * kAlignment, stride_in_bytes);
+            memcpy(addr, from_space_begin_ + stride_begin_bytes, stride_in_bytes);
             if (kIsDebugBuild) {
               uint8_t* space_begin = bump_pointer_space_->Begin();
               // We can interpret the first word of the stride as an
@@ -1428,43 +1737,94 @@ void MarkCompact::CompactPage(mirror::Object* obj,
               }
             }
             last_stride = addr;
-            addr += stride_in_bytes;
             stride_count++;
+            if (kSetupForGenerational) {
+              // Card idx within the gPageSize sized destination page.
+              size_t dest_card_idx = (addr - start_addr) >> accounting::CardTable::kCardShift;
+              DCHECK_LT(dest_card_idx, cards_per_page);
+              // Bytes remaining to fill in the current dest card.
+              size_t dest_bytes_remaining = accounting::CardTable::kCardSize -
+                                            (addr - start_addr) % accounting::CardTable::kCardSize;
+              // Update 'addr' for next stride before starting to modify
+              // 'stride_in_bytes' in the loops below.
+              addr += stride_in_bytes;
+              // Unconsumed bytes in the current src card.
+              size_t src_card_bytes = accounting::CardTable::kCardSize -
+                                      stride_begin_bytes % accounting::CardTable::kCardSize;
+              src_card_bytes = std::min(src_card_bytes, stride_in_bytes);
+              uint8_t* end_card = card_table->CardFromAddr(
+                  moving_space_begin_ + stride_begin_bytes + stride_in_bytes - 1);
+              for (uint8_t* card =
+                       card_table->CardFromAddr(moving_space_begin_ + stride_begin_bytes);
+                   card <= end_card;
+                   card++) {
+                if (*card == accounting::CardTable::kCardDirty) {
+                  // If the current src card will contribute to the next dest
+                  // card as well, then dirty the next one too.
+                  size_t val = dest_bytes_remaining < src_card_bytes ? 3 : 1;
+                  dest_cards |= val << dest_card_idx;
+                }
+                // Adjust destination card and its remaining bytes for next iteration.
+                if (dest_bytes_remaining <= src_card_bytes) {
+                  dest_bytes_remaining =
+                      accounting::CardTable::kCardSize - (src_card_bytes - dest_bytes_remaining);
+                  dest_card_idx++;
+                } else {
+                  dest_bytes_remaining -= src_card_bytes;
+                }
+                DCHECK_LE(dest_card_idx, cards_per_page);
+                stride_in_bytes -= src_card_bytes;
+                src_card_bytes = std::min(accounting::CardTable::kCardSize, stride_in_bytes);
+              }
+            } else {
+              addr += stride_in_bytes;
+            }
           });
   DCHECK_LT(last_stride, start_addr + gPageSize);
   DCHECK_GT(stride_count, 0u);
   size_t obj_size = 0;
-  uint32_t offset_within_obj = offset * kAlignment
-                               - (reinterpret_cast<uint8_t*>(obj) - from_space_begin_);
+  uint32_t offset_within_obj =
+      offset * kAlignment - (reinterpret_cast<uint8_t*>(obj) - moving_space_begin_);
   // First object
   if (offset_within_obj > 0) {
+    bool should_dirty_card;
     mirror::Object* to_ref = reinterpret_cast<mirror::Object*>(start_addr - offset_within_obj);
+    mirror::Object* from_obj = GetFromSpaceAddr(obj);
+    mirror::Object* post_compact_obj = nullptr;
+    if (kSetupForGenerational) {
+      post_compact_obj = PostCompactAddress(obj, black_dense_end_, moving_space_end_);
+    }
     if (stride_count > 1) {
-      RefsUpdateVisitor</*kCheckBegin*/true, /*kCheckEnd*/false> visitor(this,
-                                                                         to_ref,
-                                                                         start_addr,
-                                                                         nullptr);
-      obj_size = obj->VisitRefsForCompaction</*kFetchObjSize*/true, /*kVisitNativeRoots*/false>(
+      RefsUpdateVisitor</*kCheckBegin*/ true, /*kCheckEnd*/ false, kSetupForGenerational> visitor(
+          this, to_ref, start_addr, nullptr, card_table, post_compact_obj);
+      obj_size =
+          from_obj->VisitRefsForCompaction</*kFetchObjSize*/ true, /*kVisitNativeRoots*/ false>(
               visitor, MemberOffset(offset_within_obj), MemberOffset(-1));
+      should_dirty_card = visitor.ShouldDirtyCard();
     } else {
-      RefsUpdateVisitor</*kCheckBegin*/true, /*kCheckEnd*/true> visitor(this,
-                                                                        to_ref,
-                                                                        start_addr,
-                                                                        start_addr + gPageSize);
-      obj_size = obj->VisitRefsForCompaction</*kFetchObjSize*/true, /*kVisitNativeRoots*/false>(
-              visitor, MemberOffset(offset_within_obj), MemberOffset(offset_within_obj
-                                                                     + gPageSize));
+      RefsUpdateVisitor</*kCheckBegin*/ true, /*kCheckEnd*/ true, kSetupForGenerational> visitor(
+          this, to_ref, start_addr, start_addr + gPageSize, card_table, post_compact_obj);
+      obj_size =
+          from_obj->VisitRefsForCompaction</*kFetchObjSize*/ true, /*kVisitNativeRoots*/ false>(
+              visitor,
+              MemberOffset(offset_within_obj),
+              MemberOffset(offset_within_obj + gPageSize));
+      should_dirty_card = visitor.ShouldDirtyCard();
+    }
+    if (kSetupForGenerational && should_dirty_card) {
+      card_table->MarkCard(post_compact_obj);
     }
     obj_size = RoundUp(obj_size, kAlignment);
     DCHECK_GT(obj_size, offset_within_obj)
-        << "obj:" << obj << " class:" << obj->GetClass<kDefaultVerifyFlags, kWithFromSpaceBarrier>()
+        << "obj:" << obj
+        << " class:" << from_obj->GetClass<kDefaultVerifyFlags, kWithFromSpaceBarrier>()
         << " to_addr:" << to_ref
         << " black-allocation-begin:" << reinterpret_cast<void*>(black_allocations_begin_)
         << " post-compact-end:" << reinterpret_cast<void*>(post_compact_end_)
         << " offset:" << offset * kAlignment << " class-after-obj-iter:"
-        << (class_after_obj_iter_ != class_after_obj_map_.rend() ?
-                class_after_obj_iter_->first.AsMirrorPtr() :
-                nullptr)
+        << (class_after_obj_iter_ != class_after_obj_map_.rend()
+                ? class_after_obj_iter_->first.AsMirrorPtr()
+                : nullptr)
         << " last-reclaimed-page:" << reinterpret_cast<void*>(last_reclaimed_page_)
         << " last-checked-reclaim-page-idx:" << last_checked_reclaim_page_idx_
         << " offset-of-last-idx:"
@@ -1493,9 +1853,19 @@ void MarkCompact::CompactPage(mirror::Object* obj,
   while (bytes_to_visit > bytes_done) {
     mirror::Object* ref = reinterpret_cast<mirror::Object*>(addr + bytes_done);
     VerifyObject(ref, verify_obj_callback);
-    RefsUpdateVisitor</*kCheckBegin*/false, /*kCheckEnd*/false>
-            visitor(this, ref, nullptr, nullptr);
+    RefsUpdateVisitor</*kCheckBegin*/ false, /*kCheckEnd*/ false, kSetupForGenerational> visitor(
+        this,
+        ref,
+        nullptr,
+        nullptr,
+        dest_cards & (1 << (bytes_done >> accounting::CardTable::kCardShift)));
     obj_size = ref->VisitRefsForCompaction(visitor, MemberOffset(0), MemberOffset(-1));
+    if (kSetupForGenerational) {
+      SetBitForMidToOldPromotion(to_space_addr + bytes_done);
+      if (visitor.ShouldDirtyCard()) {
+        card_table->MarkCard(reinterpret_cast<mirror::Object*>(to_space_addr + bytes_done));
+      }
+    }
     obj_size = RoundUp(obj_size, kAlignment);
     bytes_done += obj_size;
   }
@@ -1511,11 +1881,21 @@ void MarkCompact::CompactPage(mirror::Object* obj,
     mirror::Object* ref = reinterpret_cast<mirror::Object*>(addr + bytes_done);
     obj = reinterpret_cast<mirror::Object*>(from_addr);
     VerifyObject(ref, verify_obj_callback);
-    RefsUpdateVisitor</*kCheckBegin*/false, /*kCheckEnd*/true>
-            visitor(this, ref, nullptr, start_addr + gPageSize);
+    RefsUpdateVisitor</*kCheckBegin*/ false, /*kCheckEnd*/ true, kSetupForGenerational> visitor(
+        this,
+        ref,
+        nullptr,
+        start_addr + gPageSize,
+        dest_cards & (1 << (bytes_done >> accounting::CardTable::kCardShift)));
     obj_size = obj->VisitRefsForCompaction(visitor,
                                            MemberOffset(0),
                                            MemberOffset(end_addr - (addr + bytes_done)));
+    if (kSetupForGenerational) {
+      SetBitForMidToOldPromotion(to_space_addr + bytes_done);
+      if (visitor.ShouldDirtyCard()) {
+        card_table->MarkCard(reinterpret_cast<mirror::Object*>(to_space_addr + bytes_done));
+      }
+    }
     obj_size = RoundUp(obj_size, kAlignment);
     DCHECK_GT(obj_size, 0u)
         << "from_addr:" << obj
@@ -2143,7 +2523,19 @@ void MarkCompact::CompactMovingSpace(uint8_t* page) {
         page,
         /*map_immediately=*/page == reserve_page,
         [&]() REQUIRES_SHARED(Locks::mutator_lock_) {
-          CompactPage(first_obj, pre_compact_offset_moving_space_[idx], page, kMode == kCopyMode);
+          if (use_generational_ && to_space_end < mid_gen_end_) {
+            CompactPage</*kSetupForGenerational=*/true>(first_obj,
+                                                        pre_compact_offset_moving_space_[idx],
+                                                        page,
+                                                        to_space_end,
+                                                        kMode == kCopyMode);
+          } else {
+            CompactPage</*kSetupForGenerational=*/false>(first_obj,
+                                                         pre_compact_offset_moving_space_[idx],
+                                                         page,
+                                                         to_space_end,
+                                                         kMode == kCopyMode);
+          }
         });
     if (kMode == kCopyMode && (!success || page == reserve_page) && end_idx_for_mapping - idx > 1) {
       // map the pages in the following address as they can't be mapped with the
@@ -2169,8 +2561,13 @@ void MarkCompact::CompactMovingSpace(uint8_t* page) {
           to_space_end + from_space_slide_diff_,
           /*map_immediately=*/false,
           [&]() REQUIRES_SHARED(Locks::mutator_lock_) {
-            UpdateNonMovingPage(
-                first_obj, to_space_end, from_space_slide_diff_, moving_space_bitmap_);
+            if (use_generational_) {
+              UpdateNonMovingPage</*kSetupForGenerational=*/true>(
+                  first_obj, to_space_end, from_space_slide_diff_, moving_space_bitmap_);
+            } else {
+              UpdateNonMovingPage</*kSetupForGenerational=*/false>(
+                  first_obj, to_space_end, from_space_slide_diff_, moving_space_bitmap_);
+            }
             if (kMode == kFallbackMode) {
               memcpy(to_space_end, to_space_end + from_space_slide_diff_, gPageSize);
             }
@@ -2274,55 +2671,94 @@ size_t MarkCompact::MapMovingSpacePages(size_t start_idx,
   return arr_len - start_idx;
 }
 
+template <bool kSetupForGenerational>
 void MarkCompact::UpdateNonMovingPage(mirror::Object* first,
                                       uint8_t* page,
                                       ptrdiff_t from_space_diff,
                                       accounting::ContinuousSpaceBitmap* bitmap) {
   DCHECK_LT(reinterpret_cast<uint8_t*>(first), page + gPageSize);
+  accounting::CardTable* card_table = heap_->GetCardTable();
+  mirror::Object* curr_obj = first;
+  uint8_t* from_page = page + from_space_diff;
+  uint8_t* from_page_end = from_page + gPageSize;
+  uint8_t* scan_begin =
+      std::max(reinterpret_cast<uint8_t*>(first) + mirror::kObjectHeaderSize, page);
   // For every object found in the page, visit the previous object. This ensures
   // that we can visit without checking page-end boundary.
   // Call VisitRefsForCompaction with from-space read-barrier as the klass object and
   // super-class loads require it.
   // TODO: Set kVisitNativeRoots to false once we implement concurrent
   // compaction
-  mirror::Object* curr_obj = first;
-  uint8_t* from_page = page + from_space_diff;
-  uint8_t* from_page_end = from_page + gPageSize;
-  bitmap->VisitMarkedRange(
-      reinterpret_cast<uintptr_t>(first) + mirror::kObjectHeaderSize,
-      reinterpret_cast<uintptr_t>(page + gPageSize),
-      [&](mirror::Object* next_obj) {
-        mirror::Object* from_obj = reinterpret_cast<mirror::Object*>(
-            reinterpret_cast<uint8_t*>(curr_obj) + from_space_diff);
-        if (reinterpret_cast<uint8_t*>(curr_obj) < page) {
-          RefsUpdateVisitor</*kCheckBegin*/ true, /*kCheckEnd*/ false> visitor(
-              this, from_obj, from_page, from_page_end);
-          MemberOffset begin_offset(page - reinterpret_cast<uint8_t*>(curr_obj));
-          // Native roots shouldn't be visited as they are done when this
-          // object's beginning was visited in the preceding page.
-          from_obj->VisitRefsForCompaction</*kFetchObjSize*/ false, /*kVisitNativeRoots*/ false>(
-              visitor, begin_offset, MemberOffset(-1));
-        } else {
-          RefsUpdateVisitor</*kCheckBegin*/ false, /*kCheckEnd*/ false> visitor(
-              this, from_obj, from_page, from_page_end);
-          from_obj->VisitRefsForCompaction</*kFetchObjSize*/ false>(
-              visitor, MemberOffset(0), MemberOffset(-1));
-        }
-        curr_obj = next_obj;
-      });
-
-  mirror::Object* from_obj =
-      reinterpret_cast<mirror::Object*>(reinterpret_cast<uint8_t*>(curr_obj) + from_space_diff);
-  MemberOffset end_offset(page + gPageSize - reinterpret_cast<uint8_t*>(curr_obj));
-  if (reinterpret_cast<uint8_t*>(curr_obj) < page) {
-    RefsUpdateVisitor</*kCheckBegin*/ true, /*kCheckEnd*/ true> visitor(
-        this, from_obj, from_page, from_page_end);
-    from_obj->VisitRefsForCompaction</*kFetchObjSize*/ false, /*kVisitNativeRoots*/ false>(
-        visitor, MemberOffset(page - reinterpret_cast<uint8_t*>(curr_obj)), end_offset);
+  auto obj_visitor = [&](mirror::Object* next_obj) {
+    if (curr_obj != nullptr) {
+      mirror::Object* from_obj =
+          reinterpret_cast<mirror::Object*>(reinterpret_cast<uint8_t*>(curr_obj) + from_space_diff);
+      bool should_dirty_card;
+      if (reinterpret_cast<uint8_t*>(curr_obj) < page) {
+        RefsUpdateVisitor</*kCheckBegin*/ true, /*kCheckEnd*/ false, kSetupForGenerational> visitor(
+            this, from_obj, from_page, from_page_end, card_table, curr_obj);
+        MemberOffset begin_offset(page - reinterpret_cast<uint8_t*>(curr_obj));
+        // Native roots shouldn't be visited as they are done when this
+        // object's beginning was visited in the preceding page.
+        from_obj->VisitRefsForCompaction</*kFetchObjSize*/ false, /*kVisitNativeRoots*/ false>(
+            visitor, begin_offset, MemberOffset(-1));
+        should_dirty_card = visitor.ShouldDirtyCard();
+      } else {
+        RefsUpdateVisitor</*kCheckBegin*/ false, /*kCheckEnd*/ false, kSetupForGenerational>
+            visitor(this, from_obj, from_page, from_page_end, card_table, curr_obj);
+        from_obj->VisitRefsForCompaction</*kFetchObjSize*/ false>(
+            visitor, MemberOffset(0), MemberOffset(-1));
+        should_dirty_card = visitor.ShouldDirtyCard();
+      }
+      if (kSetupForGenerational && should_dirty_card) {
+        card_table->MarkCard(curr_obj);
+      }
+    }
+    curr_obj = next_obj;
+  };
+
+  if (young_gen_) {
+    DCHECK(bitmap->Test(first));
+    // If the first-obj is covered by the same card which also covers the first
+    // word of the page, then it's important to set curr_obj to nullptr to avoid
+    // updating the references twice.
+    if (card_table->IsClean(first) ||
+        card_table->CardFromAddr(first) == card_table->CardFromAddr(scan_begin)) {
+      curr_obj = nullptr;
+    }
+    // We cannot acquire heap-bitmap-lock here as this function is called from
+    // SIGBUS handler. But it's safe as the bitmap passed to Scan function
+    // can't get modified until this GC cycle is finished.
+    FakeMutexLock mu(*Locks::heap_bitmap_lock_);
+    card_table->Scan</*kClearCard=*/false>(
+        bitmap, scan_begin, page + gPageSize, obj_visitor, accounting::CardTable::kCardAged2);
   } else {
-    RefsUpdateVisitor</*kCheckBegin*/ false, /*kCheckEnd*/ true> visitor(
-        this, from_obj, from_page, from_page_end);
-    from_obj->VisitRefsForCompaction</*kFetchObjSize*/ false>(visitor, MemberOffset(0), end_offset);
+    bitmap->VisitMarkedRange(reinterpret_cast<uintptr_t>(scan_begin),
+                             reinterpret_cast<uintptr_t>(page + gPageSize),
+                             obj_visitor);
+  }
+
+  if (curr_obj != nullptr) {
+    bool should_dirty_card;
+    mirror::Object* from_obj =
+        reinterpret_cast<mirror::Object*>(reinterpret_cast<uint8_t*>(curr_obj) + from_space_diff);
+    MemberOffset end_offset(page + gPageSize - reinterpret_cast<uint8_t*>(curr_obj));
+    if (reinterpret_cast<uint8_t*>(curr_obj) < page) {
+      RefsUpdateVisitor</*kCheckBegin*/ true, /*kCheckEnd*/ true, kSetupForGenerational> visitor(
+          this, from_obj, from_page, from_page_end, card_table, curr_obj);
+      from_obj->VisitRefsForCompaction</*kFetchObjSize*/ false, /*kVisitNativeRoots*/ false>(
+          visitor, MemberOffset(page - reinterpret_cast<uint8_t*>(curr_obj)), end_offset);
+      should_dirty_card = visitor.ShouldDirtyCard();
+    } else {
+      RefsUpdateVisitor</*kCheckBegin*/ false, /*kCheckEnd*/ true, kSetupForGenerational> visitor(
+          this, from_obj, from_page, from_page_end, card_table, curr_obj);
+      from_obj->VisitRefsForCompaction</*kFetchObjSize*/ false>(
+          visitor, MemberOffset(0), end_offset);
+      should_dirty_card = visitor.ShouldDirtyCard();
+    }
+    if (kSetupForGenerational && should_dirty_card) {
+      card_table->MarkCard(curr_obj);
+    }
   }
 }
 
@@ -2340,7 +2776,13 @@ void MarkCompact::UpdateNonMovingSpace() {
     page -= gPageSize;
     // null means there are no objects on the page to update references.
     if (obj != nullptr) {
-      UpdateNonMovingPage(obj, page, /*from_space_diff=*/0, non_moving_space_bitmap_);
+      if (use_generational_) {
+        UpdateNonMovingPage</*kSetupForGenerational=*/true>(
+            obj, page, /*from_space_diff=*/0, non_moving_space_bitmap_);
+      } else {
+        UpdateNonMovingPage</*kSetupForGenerational=*/false>(
+            obj, page, /*from_space_diff=*/0, non_moving_space_bitmap_);
+      }
     }
   }
 }
@@ -2504,12 +2946,15 @@ void MarkCompact::UpdateNonMovingSpaceBlackAllocations() {
   accounting::ObjectStack* stack = heap_->GetAllocationStack();
   const StackReference<mirror::Object>* limit = stack->End();
   uint8_t* const space_begin = non_moving_space_->Begin();
+  size_t num_pages = DivideByPageSize(non_moving_space_->Capacity());
   for (StackReference<mirror::Object>* it = stack->Begin(); it != limit; ++it) {
     mirror::Object* obj = it->AsMirrorPtr();
     if (obj != nullptr && non_moving_space_bitmap_->HasAddress(obj)) {
       non_moving_space_bitmap_->Set(obj);
-      // Clear so that we don't try to set the bit again in the next GC-cycle.
-      it->Clear();
+      if (!use_generational_) {
+        // Clear so that we don't try to set the bit again in the next GC-cycle.
+        it->Clear();
+      }
       size_t idx = DivideByPageSize(reinterpret_cast<uint8_t*>(obj) - space_begin);
       uint8_t* page_begin = AlignDown(reinterpret_cast<uint8_t*>(obj), gPageSize);
       mirror::Object* first_obj = first_objs_non_moving_space_[idx].AsMirrorPtr();
@@ -2517,7 +2962,10 @@ void MarkCompact::UpdateNonMovingSpaceBlackAllocations() {
           || (obj < first_obj && reinterpret_cast<uint8_t*>(first_obj) > page_begin)) {
         first_objs_non_moving_space_[idx].Assign(obj);
       }
-      mirror::Object* next_page_first_obj = first_objs_non_moving_space_[++idx].AsMirrorPtr();
+      if (++idx == num_pages) {
+        continue;
+      }
+      mirror::Object* next_page_first_obj = first_objs_non_moving_space_[idx].AsMirrorPtr();
       uint8_t* next_page_begin = page_begin + gPageSize;
       if (next_page_first_obj == nullptr
           || reinterpret_cast<uint8_t*>(next_page_first_obj) > next_page_begin) {
@@ -2772,7 +3220,6 @@ void MarkCompact::UpdateClassTableClasses(Runtime* runtime, bool immune_class_ta
 void MarkCompact::CompactionPause() {
   TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());
   Runtime* runtime = Runtime::Current();
-  non_moving_space_bitmap_ = non_moving_space_->GetLiveBitmap();
   if (kIsDebugBuild) {
     DCHECK_EQ(thread_running_gc_, Thread::Current());
     // TODO(Simulator): Test that this should not operate on the simulated stack when the simulator
@@ -2930,6 +3377,7 @@ void MarkCompact::CompactionPause() {
     // Release order wrt to mutator threads' SIGBUS handler load.
     sigbus_in_progress_count_[0].store(0, std::memory_order_relaxed);
     sigbus_in_progress_count_[1].store(0, std::memory_order_release);
+    app_slow_path_start_time_ = MilliTime();
     KernelPreparation();
   }
 
@@ -3114,8 +3562,7 @@ void MarkCompact::ConcurrentlyProcessMovingPage(uint8_t* fault_page,
                                                 size_t nr_moving_space_used_pages,
                                                 bool tolerate_enoent) {
   Thread* self = Thread::Current();
-  uint8_t* unused_space_begin =
-      bump_pointer_space_->Begin() + nr_moving_space_used_pages * gPageSize;
+  uint8_t* unused_space_begin = moving_space_begin_ + nr_moving_space_used_pages * gPageSize;
   DCHECK(IsAlignedParam(unused_space_begin, gPageSize));
   if (fault_page >= unused_space_begin) {
     // There is a race which allows more than one thread to install a
@@ -3124,7 +3571,7 @@ void MarkCompact::ConcurrentlyProcessMovingPage(uint8_t* fault_page,
     ZeropageIoctl(fault_page, gPageSize, /*tolerate_eexist=*/true, tolerate_enoent);
     return;
   }
-  size_t page_idx = DivideByPageSize(fault_page - bump_pointer_space_->Begin());
+  size_t page_idx = DivideByPageSize(fault_page - moving_space_begin_);
   DCHECK_LT(page_idx, moving_first_objs_count_ + black_page_count_);
   mirror::Object* first_obj = first_objs_moving_space_[page_idx].AsMirrorPtr();
   if (first_obj == nullptr) {
@@ -3177,7 +3624,13 @@ void MarkCompact::ConcurrentlyProcessMovingPage(uint8_t* fault_page,
               static_cast<uint8_t>(PageState::kMutatorProcessing),
               std::memory_order_acquire)) {
         if (fault_page < black_dense_end_) {
-          UpdateNonMovingPage(first_obj, fault_page, from_space_slide_diff_, moving_space_bitmap_);
+          if (use_generational_) {
+            UpdateNonMovingPage</*kSetupForGenerational=*/true>(
+                first_obj, fault_page, from_space_slide_diff_, moving_space_bitmap_);
+          } else {
+            UpdateNonMovingPage</*kSetupForGenerational=*/false>(
+                first_obj, fault_page, from_space_slide_diff_, moving_space_bitmap_);
+          }
           buf = fault_page + from_space_slide_diff_;
         } else {
           if (UNLIKELY(buf == nullptr)) {
@@ -3191,10 +3644,19 @@ void MarkCompact::ConcurrentlyProcessMovingPage(uint8_t* fault_page,
 
           if (fault_page < post_compact_end_) {
             // The page has to be compacted.
-            CompactPage(first_obj,
-                        pre_compact_offset_moving_space_[page_idx],
-                        buf,
-                        /*needs_memset_zero=*/true);
+            if (use_generational_ && fault_page < mid_gen_end_) {
+              CompactPage</*kSetupGenerational=*/true>(first_obj,
+                                                       pre_compact_offset_moving_space_[page_idx],
+                                                       buf,
+                                                       fault_page,
+                                                       /*needs_memset_zero=*/true);
+            } else {
+              CompactPage</*kSetupGenerational=*/false>(first_obj,
+                                                        pre_compact_offset_moving_space_[page_idx],
+                                                        buf,
+                                                        fault_page,
+                                                        /*needs_memset_zero=*/true);
+            }
           } else {
             DCHECK_NE(first_obj, nullptr);
             DCHECK_GT(pre_compact_offset_moving_space_[page_idx], 0u);
@@ -3647,6 +4109,7 @@ void MarkCompact::CompactionPhase() {
     DCHECK_EQ(data.end_ - data.begin_, static_cast<ssize_t>(data.shadow_.Size()));
     UnregisterUffd(data.begin_, data.shadow_.Size());
   }
+  GetCurrentIteration()->SetAppSlowPathDurationMs(MilliTime() - app_slow_path_start_time_);
 
   // Set compaction-done bit in the second counter to indicate that gc-thread
   // is done unregistering the spaces and therefore mutators, if in SIGBUS,
@@ -3789,16 +4252,19 @@ void MarkCompact::MarkRootsCheckpoint(Thread* self, Runtime* runtime) {
   }
   Locks::mutator_lock_->SharedLock(self);
   Locks::heap_bitmap_lock_->ExclusiveLock(self);
+  ProcessMarkStack();
 }
 
 void MarkCompact::MarkNonThreadRoots(Runtime* runtime) {
   TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());
   runtime->VisitNonThreadRoots(this);
+  ProcessMarkStack();
 }
 
 void MarkCompact::MarkConcurrentRoots(VisitRootFlags flags, Runtime* runtime) {
   TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());
   runtime->VisitConcurrentRoots(this, flags);
+  ProcessMarkStack();
 }
 
 void MarkCompact::RevokeAllThreadLocalBuffers() {
@@ -3848,10 +4314,45 @@ void MarkCompact::UpdateAndMarkModUnion() {
   }
 }
 
+void MarkCompact::ScanOldGenObjects() {
+  TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());
+  accounting::CardTable* const card_table = heap_->GetCardTable();
+  // Moving space
+  card_table->Scan</*kClearCard=*/false>(moving_space_bitmap_,
+                                         moving_space_begin_,
+                                         old_gen_end_,
+                                         ScanObjectVisitor(this),
+                                         gc::accounting::CardTable::kCardAged2);
+  ProcessMarkStack();
+  // Non-moving space
+  card_table->Scan</*kClearCard=*/false>(non_moving_space_bitmap_,
+                                         non_moving_space_->Begin(),
+                                         non_moving_space_->End(),
+                                         ScanObjectVisitor(this),
+                                         gc::accounting::CardTable::kCardAged2);
+  ProcessMarkStack();
+}
+
 void MarkCompact::MarkReachableObjects() {
   UpdateAndMarkModUnion();
   // Recursively mark all the non-image bits set in the mark bitmap.
   ProcessMarkStack();
+  if (young_gen_) {
+    // For the object overlapping on the old-gen boundary, we need to visit it
+    // to make sure that we don't miss the references in the mid-gen area, and
+    // also update the corresponding liveness info.
+    if (old_gen_end_ > moving_space_begin_) {
+      uintptr_t old_gen_end = reinterpret_cast<uintptr_t>(old_gen_end_);
+      mirror::Object* obj = moving_space_bitmap_->FindPrecedingObject(old_gen_end - kAlignment);
+      if (obj != nullptr) {
+        size_t obj_size = obj->SizeOf<kDefaultVerifyFlags>();
+        if (reinterpret_cast<uintptr_t>(obj) + RoundUp(obj_size, kAlignment) > old_gen_end) {
+          ScanObject</*kUpdateLiveWords=*/true>(obj);
+        }
+      }
+    }
+    ScanOldGenObjects();
+  }
 }
 
 void MarkCompact::ScanDirtyObjects(bool paused, uint8_t minimum_age) {
@@ -3866,18 +4367,39 @@ void MarkCompact::ScanDirtyObjects(bool paused, uint8_t minimum_age) {
       name = paused ? "(Paused)ScanGrayZygoteSpaceObjects" : "ScanGrayZygoteSpaceObjects";
       break;
     case space::kGcRetentionPolicyAlwaysCollect:
+      DCHECK(space == bump_pointer_space_ || space == non_moving_space_);
       name = paused ? "(Paused)ScanGrayAllocSpaceObjects" : "ScanGrayAllocSpaceObjects";
       break;
     }
     TimingLogger::ScopedTiming t(name, GetTimings());
-    card_table->Scan</*kClearCard*/ false>(
-        space->GetMarkBitmap(), space->Begin(), space->End(), ScanObjectVisitor(this), minimum_age);
+    if (paused && use_generational_ &&
+        space->GetGcRetentionPolicy() == space::kGcRetentionPolicyAlwaysCollect) {
+      DCHECK_EQ(minimum_age, accounting::CardTable::kCardDirty);
+      auto mod_visitor = [](uint8_t* card, uint8_t cur_val) {
+        DCHECK_EQ(cur_val, accounting::CardTable::kCardDirty);
+        *card = accounting::CardTable::kCardAged;
+      };
+
+      card_table->Scan</*kClearCard=*/false>(space->GetMarkBitmap(),
+                                             space->Begin(),
+                                             space->End(),
+                                             ScanObjectVisitor(this),
+                                             mod_visitor,
+                                             minimum_age);
+    } else {
+      card_table->Scan</*kClearCard=*/false>(space->GetMarkBitmap(),
+                                             space->Begin(),
+                                             space->End(),
+                                             ScanObjectVisitor(this),
+                                             minimum_age);
+    }
+    ProcessMarkStack();
   }
 }
 
 void MarkCompact::RecursiveMarkDirtyObjects(bool paused, uint8_t minimum_age) {
   ScanDirtyObjects(paused, minimum_age);
-  ProcessMarkStack();
+  CHECK(mark_stack_->IsEmpty());
 }
 
 void MarkCompact::MarkRoots(VisitRootFlags flags) {
@@ -3898,7 +4420,7 @@ void MarkCompact::PreCleanCards() {
   // Age the card-table before thread stack scanning checkpoint in MarkRoots()
   // as it ensures that there are no in-progress write barriers which started
   // prior to aging the card-table.
-  PrepareCardTableForMarking(/*clear_alloc_space_cards*/ false);
+  PrepareForMarking(/*pre_marking=*/false);
   MarkRoots(static_cast<VisitRootFlags>(kVisitRootFlagClearRootLog | kVisitRootFlagNewRoots));
   RecursiveMarkDirtyObjects(/*paused*/ false, accounting::CardTable::kCardDirty - 1);
 }
@@ -3920,7 +4442,7 @@ void MarkCompact::MarkingPhase() {
   DCHECK_EQ(thread_running_gc_, Thread::Current());
   WriterMutexLock mu(thread_running_gc_, *Locks::heap_bitmap_lock_);
   MaybeClampGcStructures();
-  PrepareCardTableForMarking(/*clear_alloc_space_cards*/ true);
+  PrepareForMarking(/*pre_marking=*/true);
   MarkZygoteLargeObjects();
   MarkRoots(
         static_cast<VisitRootFlags>(kVisitRootFlagAllRoots | kVisitRootFlagStartLoggingNewRoots));
@@ -3941,8 +4463,19 @@ void MarkCompact::MarkingPhase() {
 
 class MarkCompact::RefFieldsVisitor {
  public:
-  ALWAYS_INLINE explicit RefFieldsVisitor(MarkCompact* const mark_compact)
-    : mark_compact_(mark_compact) {}
+  ALWAYS_INLINE RefFieldsVisitor(MarkCompact* const mark_compact)
+      : mark_compact_(mark_compact),
+        young_gen_begin_(mark_compact->mid_gen_end_),
+        young_gen_end_(mark_compact->moving_space_end_),
+        dirty_card_(false),
+        // Ideally we should only check for objects outside young-gen. However,
+        // the boundary of young-gen can change later in PrepareForCompaction()
+        // as we need the mid-gen-end to be page-aligned. Since most of the
+        // objects don't have native-roots, it's not too costly to check all
+        // objects being visited during marking.
+        check_native_roots_to_young_gen_(mark_compact->use_generational_) {}
+
+  bool ShouldDirtyCard() const { return dirty_card_; }
 
   ALWAYS_INLINE void operator()(mirror::Object* obj,
                                 MemberOffset offset,
@@ -3952,7 +4485,8 @@ class MarkCompact::RefFieldsVisitor {
       Locks::mutator_lock_->AssertSharedHeld(Thread::Current());
       Locks::heap_bitmap_lock_->AssertExclusiveHeld(Thread::Current());
     }
-    mark_compact_->MarkObject(obj->GetFieldObject<mirror::Object>(offset), obj, offset);
+    mirror::Object* ref = obj->GetFieldObject<mirror::Object>(offset);
+    mark_compact_->MarkObject(ref, obj, offset);
   }
 
   void operator()(ObjPtr<mirror::Class> klass, ObjPtr<mirror::Reference> ref) const ALWAYS_INLINE
@@ -3974,11 +4508,20 @@ class MarkCompact::RefFieldsVisitor {
       Locks::mutator_lock_->AssertSharedHeld(Thread::Current());
       Locks::heap_bitmap_lock_->AssertExclusiveHeld(Thread::Current());
     }
-    mark_compact_->MarkObject(root->AsMirrorPtr());
+    mirror::Object* ref = root->AsMirrorPtr();
+    mark_compact_->MarkObject(ref);
+    if (check_native_roots_to_young_gen_) {
+      dirty_card_ |= reinterpret_cast<uint8_t*>(ref) >= young_gen_begin_ &&
+                     reinterpret_cast<uint8_t*>(ref) < young_gen_end_;
+    }
   }
 
  private:
   MarkCompact* const mark_compact_;
+  uint8_t* const young_gen_begin_;
+  uint8_t* const young_gen_end_;
+  mutable bool dirty_card_;
+  const bool check_native_roots_to_young_gen_;
 };
 
 template <size_t kAlignment>
@@ -3998,19 +4541,24 @@ void MarkCompact::UpdateLivenessInfo(mirror::Object* obj, size_t obj_size) {
   UpdateClassAfterObjectMap(obj);
   size_t size = RoundUp(obj_size, kAlignment);
   uintptr_t bit_index = live_words_bitmap_->SetLiveWords(obj_begin, size);
-  size_t chunk_idx = (obj_begin - live_words_bitmap_->Begin()) / kOffsetChunkSize;
+  size_t chunk_idx =
+      (obj_begin - reinterpret_cast<uintptr_t>(moving_space_begin_)) / kOffsetChunkSize;
   // Compute the bit-index within the chunk-info vector word.
   bit_index %= kBitsPerVectorWord;
   size_t first_chunk_portion = std::min(size, (kBitsPerVectorWord - bit_index) * kAlignment);
-
-  chunk_info_vec_[chunk_idx++] += first_chunk_portion;
+  chunk_info_vec_[chunk_idx] += first_chunk_portion;
+  DCHECK_LE(chunk_info_vec_[chunk_idx], kOffsetChunkSize)
+      << "first_chunk_portion:" << first_chunk_portion
+      << " obj-size:" << RoundUp(obj_size, kAlignment);
+  chunk_idx++;
   DCHECK_LE(first_chunk_portion, size);
   for (size -= first_chunk_portion; size > kOffsetChunkSize; size -= kOffsetChunkSize) {
     DCHECK_EQ(chunk_info_vec_[chunk_idx], 0u);
     chunk_info_vec_[chunk_idx++] = kOffsetChunkSize;
   }
   chunk_info_vec_[chunk_idx] += size;
-  freed_objects_--;
+  DCHECK_LE(chunk_info_vec_[chunk_idx], kOffsetChunkSize)
+      << "size:" << size << " obj-size:" << RoundUp(obj_size, kAlignment);
 }
 
 template <bool kUpdateLiveWords>
@@ -4026,21 +4574,23 @@ void MarkCompact::ScanObject(mirror::Object* obj) {
       usleep(1000);
       klass = obj->GetClass<kVerifyNone, kWithoutReadBarrier>();
       if (klass != nullptr) {
-        std::ostringstream oss;
-        klass->DumpClass(oss, mirror::Class::kDumpClassFullDetail);
-        LOG(FATAL_WITHOUT_ABORT) << "klass pointer for obj: " << obj
-                                 << " found to be null first. Reloading after " << i
-                                 << " iterations of 1ms sleep fetched klass: " << oss.str();
         break;
       }
     }
-
-    if (UNLIKELY(klass == nullptr)) {
+    if (klass == nullptr) {
       // It must be heap corruption.
-      LOG(FATAL_WITHOUT_ABORT) << "klass pointer for obj: " << obj << " found to be null.";
+      LOG(FATAL_WITHOUT_ABORT) << "klass pointer for obj: " << obj << " found to be null."
+                               << " black_dense_end: " << static_cast<void*>(black_dense_end_)
+                               << " mid_gen_end: " << static_cast<void*>(mid_gen_end_)
+                               << " prev_post_compact_end: " << prev_post_compact_end_
+                               << " prev_black_allocations_begin: " << prev_black_allocations_begin_
+                               << " prev_black_dense_end: " << prev_black_dense_end_
+                               << " prev_gc_young: " << prev_gc_young_
+                               << " prev_gc_performed_compaction: "
+                               << prev_gc_performed_compaction_;
+      heap_->GetVerification()->LogHeapCorruption(
+          obj, mirror::Object::ClassOffset(), klass, /*fatal=*/true);
     }
-    heap_->GetVerification()->LogHeapCorruption(
-        obj, mirror::Object::ClassOffset(), klass, /*fatal=*/true);
   }
   // The size of `obj` is used both here (to update `bytes_scanned_`) and in
   // `UpdateLivenessInfo`. As fetching this value can be expensive, do it once
@@ -4052,12 +4602,26 @@ void MarkCompact::ScanObject(mirror::Object* obj) {
   DCHECK(IsMarked(obj)) << "Scanning marked object " << obj << "\n" << heap_->DumpSpaces();
   if (kUpdateLiveWords && HasAddress(obj)) {
     UpdateLivenessInfo(obj, obj_size);
+    freed_objects_--;
   }
   obj->VisitReferences(visitor, visitor);
+  // old-gen cards for objects containing references to mid-gen needs to be kept
+  // dirty for re-scan in the next GC cycle. We take care of that majorly during
+  // compaction-phase as that enables us to implicitly take care of
+  // black-allocated objects as well. Unfortunately, since we don't visit
+  // native-roots during compaction, that has to be captured during marking.
+  //
+  // Note that we can't dirty the cards right away because then we will wrongly
+  // age them during re-scan of this marking-phase, and thereby may loose them
+  // by the end of the GC cycle.
+  if (visitor.ShouldDirtyCard()) {
+    dirty_cards_later_vec_.push_back(obj);
+  }
 }
 
 // Scan anything that's on the mark stack.
 void MarkCompact::ProcessMarkStack() {
+  // TODO: eventually get rid of this as we now call this function quite a few times.
   TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());
   // TODO: try prefetch like in CMS
   while (!mark_stack_->IsEmpty()) {
@@ -4101,6 +4665,12 @@ inline bool MarkCompact::MarkObjectNonNullNoPush(mirror::Object* obj,
   // We expect most of the referenes to be in bump-pointer space, so try that
   // first to keep the cost of this function minimal.
   if (LIKELY(HasAddress(obj))) {
+    // If obj is in old-gen (during young-gc) then we shouldn't add it to
+    // mark-stack to limit marking to young generation.
+    if (young_gen_ && reinterpret_cast<uint8_t*>(obj) < old_gen_end_) {
+      DCHECK(moving_space_bitmap_->Test(obj));
+      return false;
+    }
     return kParallel ? !moving_space_bitmap_->AtomicTestAndSet(obj)
                      : !moving_space_bitmap_->Set(obj);
   } else if (non_moving_space_bitmap_->HasAddress(obj)) {
@@ -4248,24 +4818,312 @@ void MarkCompact::DelayReferenceReferent(ObjPtr<mirror::Class> klass,
   heap_->GetReferenceProcessor()->DelayReferenceReferent(klass, ref, this);
 }
 
-void MarkCompact::FinishPhase() {
+template <typename Visitor>
+class MarkCompact::VisitReferencesVisitor {
+ public:
+  explicit VisitReferencesVisitor(Visitor visitor) : visitor_(visitor) {}
+
+  ALWAYS_INLINE void operator()(mirror::Object* obj,
+                                MemberOffset offset,
+                                [[maybe_unused]] bool is_static) const
+      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
+    visitor_(obj->GetFieldObject<mirror::Object>(offset));
+  }
+
+  ALWAYS_INLINE void operator()([[maybe_unused]] ObjPtr<mirror::Class> klass,
+                                ObjPtr<mirror::Reference> ref) const
+      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
+    visitor_(ref.Ptr());
+  }
+
+  void VisitRootIfNonNull(mirror::CompressedReference<mirror::Object>* root) const
+      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
+    if (!root->IsNull()) {
+      VisitRoot(root);
+    }
+  }
+
+  void VisitRoot(mirror::CompressedReference<mirror::Object>* root) const
+      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
+    visitor_(root->AsMirrorPtr());
+  }
+
+ private:
+  Visitor visitor_;
+};
+
+void MarkCompact::VerifyNoMissingCardMarks() {
+  if (kVerifyNoMissingCardMarks) {
+    accounting::CardTable* card_table = heap_->GetCardTable();
+    auto obj_visitor = [&](mirror::Object* obj) REQUIRES_SHARED(Locks::mutator_lock_) {
+      bool found = false;
+      VisitReferencesVisitor visitor(
+          [begin = old_gen_end_, end = moving_space_end_, &found](mirror::Object* ref) {
+            found |= ref >= reinterpret_cast<mirror::Object*>(begin) &&
+                     ref < reinterpret_cast<mirror::Object*>(end);
+          });
+      obj->VisitReferences</*kVisitNativeRoots=*/true>(visitor, visitor);
+      if (found) {
+        size_t obj_size = RoundUp(obj->SizeOf<kDefaultVerifyFlags>(), kAlignment);
+        if (!card_table->IsDirty(obj) &&
+            reinterpret_cast<uint8_t*>(obj) + obj_size <= old_gen_end_) {
+          std::ostringstream oss;
+          obj->DumpReferences</*kDumpNativeRoots=*/true>(oss, /*dump_type_of=*/true);
+          LOG(FATAL_WITHOUT_ABORT)
+              << "Object " << obj << " (" << obj->PrettyTypeOf()
+              << ") has references to mid-gen/young-gen:"
+              << "\n obj-size = " << obj_size
+              << "\n old-gen-end = " << static_cast<void*>(old_gen_end_)
+              << "\n mid-gen-end = " << static_cast<void*>(mid_gen_end_) << "\n references =\n"
+              << oss.str();
+          heap_->GetVerification()->LogHeapCorruption(
+              /*holder=*/nullptr, MemberOffset(0), obj, /*fatal=*/true);
+        }
+      }
+    };
+    moving_space_bitmap_->VisitMarkedRange(reinterpret_cast<uintptr_t>(moving_space_begin_),
+                                           reinterpret_cast<uintptr_t>(old_gen_end_),
+                                           obj_visitor);
+  }
+}
+
+void MarkCompact::VerifyPostGCObjects(bool performed_compaction, uint8_t* mark_bitmap_clear_end) {
+  if (kVerifyPostGCObjects) {
+    mirror::Object* last_visited_obj = nullptr;
+    auto obj_visitor =
+        [&](mirror::Object* obj, bool verify_bitmap = false) REQUIRES_SHARED(Locks::mutator_lock_) {
+          std::vector<mirror::Object*> invalid_refs;
+          if (verify_bitmap && !moving_space_bitmap_->Test(obj)) {
+            LOG(FATAL) << "Obj " << obj << " (" << obj->PrettyTypeOf()
+                       << ") doesn't have mark-bit set"
+                       << "\n prev-black-dense-end = " << static_cast<void*>(prev_black_dense_end_)
+                       << "\n old-gen-end = " << static_cast<void*>(old_gen_end_)
+                       << "\n mid-gen-end = " << static_cast<void*>(mid_gen_end_);
+          }
+          VisitReferencesVisitor visitor(
+              [verification = heap_->GetVerification(), &invalid_refs](mirror::Object* ref)
+                  REQUIRES_SHARED(Locks::mutator_lock_) {
+                    if (ref != nullptr && !verification->IsValidObject(ref)) {
+                      invalid_refs.push_back(ref);
+                    }
+                  });
+          obj->VisitReferences</*kVisitNativeRoots=*/true>(visitor, visitor);
+          if (!invalid_refs.empty()) {
+            std::ostringstream oss;
+            for (mirror::Object* ref : invalid_refs) {
+              oss << ref << " ";
+            }
+            LOG(FATAL_WITHOUT_ABORT)
+                << "Object " << obj << " (" << obj->PrettyTypeOf() << ") has invalid references:\n"
+                << oss.str() << "\ncard = " << static_cast<int>(heap_->GetCardTable()->GetCard(obj))
+                << "\n prev-black-dense-end = " << static_cast<void*>(prev_black_dense_end_)
+                << "\n old-gen-end = " << static_cast<void*>(old_gen_end_)
+                << "\n mid-gen-end = " << static_cast<void*>(mid_gen_end_)
+                << "\n black-allocations-begin = " << static_cast<void*>(black_allocations_begin_);
+
+            // Calling PrettyTypeOf() on a stale reference mostly results in segfault.
+            oss.str("");
+            obj->DumpReferences</*kDumpNativeRoots=*/true>(oss, /*dump_type_of=*/false);
+            LOG(FATAL_WITHOUT_ABORT) << "\n references =\n" << oss.str();
+
+            heap_->GetVerification()->LogHeapCorruption(
+                /*holder=*/nullptr, MemberOffset(0), obj, /*fatal=*/true);
+          }
+          last_visited_obj = obj;
+        };
+    non_moving_space_bitmap_->VisitAllMarked(obj_visitor);
+    last_visited_obj = nullptr;
+    // We should verify all objects that have survived, which means old and mid-gen
+    // Objects that were promoted to old-gen and mid-gen in this GC cycle are tightly
+    // packed, except if compaction was not performed. So we use object size to walk
+    // the heap and also verify that the mark-bit is set in the tightly packed portion.
+    moving_space_bitmap_->VisitMarkedRange(
+        reinterpret_cast<uintptr_t>(moving_space_begin_),
+        reinterpret_cast<uintptr_t>(performed_compaction ? prev_black_dense_end_
+                                                         : mark_bitmap_clear_end),
+        obj_visitor);
+    if (performed_compaction) {
+      mirror::Object* obj = last_visited_obj;
+      if (obj == nullptr || AlignUp(reinterpret_cast<uint8_t*>(obj) + obj->SizeOf(), kAlignment) <
+                                prev_black_dense_end_) {
+        obj = reinterpret_cast<mirror::Object*>(prev_black_dense_end_);
+      }
+      while (reinterpret_cast<uint8_t*>(obj) < mid_gen_end_ && obj->GetClass() != nullptr) {
+        // Objects in mid-gen will not have their corresponding mark-bits set.
+        obj_visitor(obj, reinterpret_cast<void*>(obj) < black_dense_end_);
+        uintptr_t next = reinterpret_cast<uintptr_t>(obj) + obj->SizeOf();
+        obj = reinterpret_cast<mirror::Object*>(RoundUp(next, kAlignment));
+      }
+    }
+  }
+}
+
+void MarkCompact::FinishPhase(bool performed_compaction) {
+  TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());
   GetCurrentIteration()->SetScannedBytes(bytes_scanned_);
   bool is_zygote = Runtime::Current()->IsZygote();
   compacting_ = false;
   marking_done_ = false;
+  uint8_t* mark_bitmap_clear_end = black_dense_end_;
+  LOG(DEBUG) << "ART-GC black_dense_end:" << static_cast<void*>(black_dense_end_)
+             << " mid_gen_end:" << static_cast<void*>(mid_gen_end_)
+             << " post_compact_end:" << static_cast<void*>(post_compact_end_)
+             << " black_allocations_begin:" << static_cast<void*>(black_allocations_begin_)
+             << " young:" << young_gen_ << " performed_compaction:" << performed_compaction;
+
+  // Retain values of some fields for logging in next GC cycle, in case there is
+  // a memory corruption detected.
+  prev_black_allocations_begin_ = static_cast<void*>(black_allocations_begin_);
+  prev_black_dense_end_ = static_cast<void*>(black_dense_end_);
+  prev_post_compact_end_ = static_cast<void*>(post_compact_end_);
+  prev_gc_young_ = young_gen_;
+  prev_gc_performed_compaction_ = performed_compaction;
+
+  // Whether compaction is performend or not, we always set post_compact_end_
+  // before reaching here.
+  CHECK_NE(post_compact_end_, nullptr);
+  if (use_generational_) {
+    {
+      ReaderMutexLock mu(thread_running_gc_, *Locks::mutator_lock_);
+      // We need to retain and update class-after-object map for old-gen as
+      // that won't be created in next young-gc.
+      // Jump to the first class which is getting promoted to old-gen. Since
+      // it is not compacted, references into old-gen don't need to be udated.
+      // All pairs in mid-gen will be updated with post-compact addresses and
+      // retained, as mid-gen is getting consumed into old-gen now. All pairs
+      // after mid-gen will be erased as they are not required in next GC cycle.
+      auto iter = class_after_obj_map_.lower_bound(
+          ObjReference::FromMirrorPtr(reinterpret_cast<mirror::Object*>(old_gen_end_)));
+      while (iter != class_after_obj_map_.end()) {
+        mirror::Object* klass = iter->first.AsMirrorPtr();
+        mirror::Object* obj = iter->second.AsMirrorPtr();
+        DCHECK_GT(klass, obj);
+        // Black allocations begin after marking-pause. Therefore, we cannot
+        // have a situation wherein class is allocated after the pause while its
+        // object is before.
+        if (reinterpret_cast<uint8_t*>(klass) >= black_allocations_begin_) {
+          for (auto it = iter; it != class_after_obj_map_.end(); it++) {
+            DCHECK_GE(reinterpret_cast<uint8_t*>(it->second.AsMirrorPtr()),
+                      black_allocations_begin_);
+          }
+          class_after_obj_map_.erase(iter, class_after_obj_map_.end());
+          break;
+        }
 
-  ZeroAndReleaseMemory(compaction_buffers_map_.Begin(), compaction_buffers_map_.Size());
-  info_map_.MadviseDontNeedAndZero();
-  live_words_bitmap_->ClearBitmap();
-  if (moving_space_begin_ == black_dense_end_) {
+        DCHECK(moving_space_bitmap_->Test(klass));
+        DCHECK(moving_space_bitmap_->Test(obj));
+        // As 'mid_gen_end_' is where our old-gen will end now, compute compacted
+        // addresses of <class, object> for comparisons and updating in the map.
+        mirror::Object* compacted_klass = klass;
+        mirror::Object* compacted_obj = obj;
+        if (performed_compaction) {
+          compacted_klass = PostCompactAddress(klass, old_gen_end_, moving_space_end_);
+          compacted_obj = PostCompactAddress(obj, old_gen_end_, moving_space_end_);
+          DCHECK_GT(compacted_klass, compacted_obj);
+        }
+        if (reinterpret_cast<uint8_t*>(compacted_obj) >= mid_gen_end_) {
+          iter = class_after_obj_map_.erase(iter);
+          continue;
+        } else if (mid_to_old_promo_bit_vec_.get() != nullptr) {
+          if (reinterpret_cast<uint8_t*>(compacted_klass) >= old_gen_end_) {
+            DCHECK(mid_to_old_promo_bit_vec_->IsBitSet(
+                (reinterpret_cast<uint8_t*>(compacted_obj) - old_gen_end_) / kAlignment));
+          }
+          if (reinterpret_cast<uint8_t*>(compacted_klass) < mid_gen_end_) {
+            DCHECK(mid_to_old_promo_bit_vec_->IsBitSet(
+                (reinterpret_cast<uint8_t*>(compacted_klass) - old_gen_end_) / kAlignment));
+          }
+        }
+        if (performed_compaction) {
+          auto nh = class_after_obj_map_.extract(iter++);
+          nh.key() = ObjReference::FromMirrorPtr(compacted_klass);
+          nh.mapped() = ObjReference::FromMirrorPtr(compacted_obj);
+          auto success = class_after_obj_map_.insert(iter, std::move(nh));
+          CHECK_EQ(success->first.AsMirrorPtr(), compacted_klass);
+        } else {
+          iter++;
+        }
+      }
+
+      // Dirty the cards for objects captured from native-roots during marking-phase.
+      accounting::CardTable* card_table = heap_->GetCardTable();
+      for (auto obj : dirty_cards_later_vec_) {
+        // Only moving and non-moving spaces are relevant as the remaining
+        // spaces are all immune-spaces which anyways use card-table.
+        if (HasAddress(obj)) {
+          // Objects in young-gen that refer to other young-gen objects don't
+          // need to be tracked.
+          // The vector contains pre-compact object references whereas
+          // 'mid_gen_end_' is post-compact boundary. So compare against
+          // post-compact object reference.
+          mirror::Object* compacted_obj =
+              performed_compaction ? PostCompactAddress(obj, black_dense_end_, moving_space_end_)
+                                   : obj;
+          if (reinterpret_cast<uint8_t*>(compacted_obj) < mid_gen_end_) {
+            card_table->MarkCard(compacted_obj);
+          }
+        } else if (non_moving_space_->HasAddress(obj)) {
+          card_table->MarkCard(obj);
+        }
+      }
+    }
+    dirty_cards_later_vec_.clear();
+
+    // Copy mid-gen bitmap into moving-space's mark-bitmap
+    if (mid_to_old_promo_bit_vec_.get() != nullptr) {
+      DCHECK_EQ(mid_to_old_promo_bit_vec_->GetBitSizeOf(),
+                (mid_gen_end_ - old_gen_end_) / kObjectAlignment);
+      uint32_t* bitmap_begin = reinterpret_cast<uint32_t*>(moving_space_bitmap_->Begin());
+      DCHECK(IsAligned<kObjectAlignment * BitVector::kWordBits>(gPageSize));
+      size_t index = (old_gen_end_ - moving_space_begin_) / kObjectAlignment / BitVector::kWordBits;
+      mid_to_old_promo_bit_vec_->CopyTo(&bitmap_begin[index],
+                                        mid_to_old_promo_bit_vec_->GetSizeOf());
+      mid_to_old_promo_bit_vec_.reset(nullptr);
+    } else if (!performed_compaction) {
+      // We typically only retain the mark-bitmap for the old-generation as the
+      // objects following it are expected to be contiguous. However, when
+      // compaction is not performed, we may have decided to tolerate few holes
+      // here and there. So we have to retain the bitmap for the entire
+      // 'compacted' portion of the heap, which is up to mid-gen-end.
+      DCHECK_LE(old_gen_end_, post_compact_end_);
+      mark_bitmap_clear_end = post_compact_end_;
+    }
+    // Promote all mid-gen objects to old-gen and young-gen objects to mid-gen
+    // for next GC cycle.
+    old_gen_end_ = mid_gen_end_;
+    mid_gen_end_ = post_compact_end_;
+    post_compact_end_ = nullptr;
+
+    // Verify (in debug builds) after updating mark-bitmap if class-after-object
+    // map is correct or not.
+    for (auto iter : class_after_obj_map_) {
+      DCHECK(moving_space_bitmap_->Test(iter.second.AsMirrorPtr()));
+      mirror::Object* klass = iter.first.AsMirrorPtr();
+      DCHECK_IMPLIES(!moving_space_bitmap_->Test(klass),
+                     reinterpret_cast<uint8_t*>(klass) >= old_gen_end_);
+    }
+  } else {
+    class_after_obj_map_.clear();
+    if (!performed_compaction) {
+      DCHECK_LE(old_gen_end_, post_compact_end_);
+      mark_bitmap_clear_end = post_compact_end_;
+    }
+  }
+  // Black-dense region, which requires bitmap for object-walk, could be larger
+  // than old-gen. Therefore, until next GC retain the bitmap for entire
+  // black-dense region. At the beginning of next cycle, we clear [old_gen_end_,
+  // moving_space_end_).
+  mark_bitmap_clear_end = std::max(black_dense_end_, mark_bitmap_clear_end);
+  DCHECK_ALIGNED_PARAM(mark_bitmap_clear_end, gPageSize);
+  if (moving_space_begin_ == mark_bitmap_clear_end) {
     moving_space_bitmap_->Clear();
   } else {
-    DCHECK_LT(moving_space_begin_, black_dense_end_);
-    DCHECK_LE(black_dense_end_, moving_space_end_);
-    moving_space_bitmap_->ClearRange(reinterpret_cast<mirror::Object*>(black_dense_end_),
+    DCHECK_LT(moving_space_begin_, mark_bitmap_clear_end);
+    DCHECK_LE(mark_bitmap_clear_end, moving_space_end_);
+    moving_space_bitmap_->ClearRange(reinterpret_cast<mirror::Object*>(mark_bitmap_clear_end),
                                      reinterpret_cast<mirror::Object*>(moving_space_end_));
   }
-  bump_pointer_space_->SetBlackDenseRegionSize(black_dense_end_ - moving_space_begin_);
+  bump_pointer_space_->SetBlackDenseRegionSize(mark_bitmap_clear_end - moving_space_begin_);
 
   if (UNLIKELY(is_zygote && IsValidFd(uffd_))) {
     // This unregisters all ranges as a side-effect.
@@ -4275,6 +5133,9 @@ void MarkCompact::FinishPhase() {
   }
   CHECK(mark_stack_->IsEmpty());  // Ensure that the mark stack is empty.
   mark_stack_->Reset();
+  ZeroAndReleaseMemory(compaction_buffers_map_.Begin(), compaction_buffers_map_.Size());
+  info_map_.MadviseDontNeedAndZero();
+  live_words_bitmap_->ClearBitmap();
   DCHECK_EQ(thread_running_gc_, Thread::Current());
   if (kIsDebugBuild) {
     MutexLock mu(thread_running_gc_, lock_);
@@ -4282,16 +5143,68 @@ void MarkCompact::FinishPhase() {
       updated_roots_->clear();
     }
   }
-  class_after_obj_map_.clear();
   linear_alloc_arenas_.clear();
   {
     ReaderMutexLock mu(thread_running_gc_, *Locks::mutator_lock_);
     WriterMutexLock mu2(thread_running_gc_, *Locks::heap_bitmap_lock_);
     heap_->ClearMarkedObjects();
+    if (use_generational_) {
+      if (performed_compaction) {
+        // Clear the bits set temporarily for black allocations in non-moving
+        // space in UpdateNonMovingSpaceBlackAllocations(), which is called when
+        // we perform compaction, so that objects are considered for GC in next cycle.
+        accounting::ObjectStack* stack = heap_->GetAllocationStack();
+        const StackReference<mirror::Object>* limit = stack->End();
+        for (StackReference<mirror::Object>* it = stack->Begin(); it != limit; ++it) {
+          mirror::Object* obj = it->AsMirrorPtr();
+          if (obj != nullptr && non_moving_space_bitmap_->HasAddress(obj)) {
+            non_moving_space_bitmap_->Clear(obj);
+          }
+        }
+      } else {
+        // Since we didn't perform compaction, we need to identify old objects
+        // referring to the mid-gen.
+        auto obj_visitor = [this, card_table = heap_->GetCardTable()](mirror::Object* obj) {
+          bool found = false;
+          VisitReferencesVisitor visitor(
+              [begin = old_gen_end_, end = mid_gen_end_, &found](mirror::Object* ref) {
+                found |= ref >= reinterpret_cast<mirror::Object*>(begin) &&
+                         ref < reinterpret_cast<mirror::Object*>(end);
+              });
+          uint8_t* card = card_table->CardFromAddr(obj);
+          if (*card == accounting::CardTable::kCardDirty) {
+            return;
+          }
+          // Native-roots are captured during marking and the corresponding cards are already
+          // dirtied above.
+          obj->VisitReferences</*kVisitNativeRoots=*/false>(visitor, visitor);
+          if (found) {
+            *card = accounting::CardTable::kCardDirty;
+          }
+        };
+        moving_space_bitmap_->VisitMarkedRange(reinterpret_cast<uintptr_t>(moving_space_begin_),
+                                               reinterpret_cast<uintptr_t>(old_gen_end_),
+                                               obj_visitor);
+        non_moving_space_bitmap_->VisitAllMarked(obj_visitor);
+      }
+    }
   }
   GcVisitedArenaPool* arena_pool =
       static_cast<GcVisitedArenaPool*>(Runtime::Current()->GetLinearAllocArenaPool());
   arena_pool->DeleteUnusedArenas();
+
+  if (kVerifyNoMissingCardMarks && use_generational_) {
+    // This must be done in a pause as otherwise verification between mutation
+    // and card-dirtying by a mutator will spuriosely fail.
+    ScopedPause pause(this);
+    WriterMutexLock mu(thread_running_gc_, *Locks::heap_bitmap_lock_);
+    VerifyNoMissingCardMarks();
+  }
+  if (kVerifyPostGCObjects && use_generational_) {
+    ReaderMutexLock mu(thread_running_gc_, *Locks::mutator_lock_);
+    WriterMutexLock mu2(thread_running_gc_, *Locks::heap_bitmap_lock_);
+    VerifyPostGCObjects(performed_compaction, mark_bitmap_clear_end);
+  }
 }
 
 }  // namespace collector
diff --git a/runtime/gc/collector/mark_compact.h b/runtime/gc/collector/mark_compact.h
index dd9fefb2a9..41d2ab31c2 100644
--- a/runtime/gc/collector/mark_compact.h
+++ b/runtime/gc/collector/mark_compact.h
@@ -25,6 +25,7 @@
 
 #include "barrier.h"
 #include "base/atomic.h"
+#include "base/bit_vector.h"
 #include "base/gc_visited_arena_pool.h"
 #include "base/macros.h"
 #include "base/mutex.h"
@@ -53,6 +54,64 @@ class BumpPointerSpace;
 }  // namespace space
 
 namespace collector {
+class MarkCompact;
+
+// The actual young GC code is also implemented in MarkCompact class. However,
+// using this class saves us from creating duplicate data-structures, which
+// would have happened with two instances of MarkCompact.
+class YoungMarkCompact final : public GarbageCollector {
+ public:
+  YoungMarkCompact(Heap* heap, MarkCompact* main);
+
+  void RunPhases() override REQUIRES(!Locks::mutator_lock_);
+
+  GcType GetGcType() const override { return kGcTypeSticky; }
+
+  CollectorType GetCollectorType() const override { return kCollectorTypeCMC; }
+
+  // None of the following methods are ever called as actual GC is performed by MarkCompact.
+
+  mirror::Object* MarkObject([[maybe_unused]] mirror::Object* obj) override {
+    UNIMPLEMENTED(FATAL);
+    UNREACHABLE();
+  }
+  void MarkHeapReference([[maybe_unused]] mirror::HeapReference<mirror::Object>* obj,
+                         [[maybe_unused]] bool do_atomic_update) override {
+    UNIMPLEMENTED(FATAL);
+  }
+  void VisitRoots([[maybe_unused]] mirror::Object*** roots,
+                  [[maybe_unused]] size_t count,
+                  [[maybe_unused]] const RootInfo& info) override {
+    UNIMPLEMENTED(FATAL);
+  }
+  void VisitRoots([[maybe_unused]] mirror::CompressedReference<mirror::Object>** roots,
+                  [[maybe_unused]] size_t count,
+                  [[maybe_unused]] const RootInfo& info) override {
+    UNIMPLEMENTED(FATAL);
+  }
+  bool IsNullOrMarkedHeapReference([[maybe_unused]] mirror::HeapReference<mirror::Object>* obj,
+                                   [[maybe_unused]] bool do_atomic_update) override {
+    UNIMPLEMENTED(FATAL);
+    UNREACHABLE();
+  }
+  void RevokeAllThreadLocalBuffers() override { UNIMPLEMENTED(FATAL); }
+
+  void DelayReferenceReferent([[maybe_unused]] ObjPtr<mirror::Class> klass,
+                              [[maybe_unused]] ObjPtr<mirror::Reference> reference) override {
+    UNIMPLEMENTED(FATAL);
+  }
+  mirror::Object* IsMarked([[maybe_unused]] mirror::Object* obj) override {
+    UNIMPLEMENTED(FATAL);
+    UNREACHABLE();
+  }
+  void ProcessMarkStack() override { UNIMPLEMENTED(FATAL); }
+
+ private:
+  MarkCompact* const main_collector_;
+
+  DISALLOW_IMPLICIT_CONSTRUCTORS(YoungMarkCompact);
+};
+
 class MarkCompact final : public GarbageCollector {
  public:
   using SigbusCounterType = uint32_t;
@@ -83,9 +142,7 @@ class MarkCompact final : public GarbageCollector {
   // is asserted in the function.
   bool SigbusHandler(siginfo_t* info) REQUIRES(!lock_) NO_THREAD_SAFETY_ANALYSIS;
 
-  GcType GetGcType() const override {
-    return kGcTypeFull;
-  }
+  GcType GetGcType() const override { return kGcTypePartial; }
 
   CollectorType GetCollectorType() const override {
     return kCollectorTypeCMC;
@@ -149,6 +206,11 @@ class MarkCompact final : public GarbageCollector {
   // GcVisitedArenaPool, which mostly happens only once.
   void AddLinearAllocSpaceData(uint8_t* begin, size_t len);
 
+  // Called by Heap::PreZygoteFork() to reset generational heap pointers and
+  // other data structures as the moving space gets completely evicted into new
+  // zygote-space.
+  void ResetGenerationalState();
+
   // In copy-mode of userfaultfd, we don't need to reach a 'processed' state as
   // it's given that processing thread also copies the page, thereby mapping it.
   // The order is important as we may treat them as integers. Also
@@ -171,6 +233,8 @@ class MarkCompact final : public GarbageCollector {
     kClampInfoFinished
   };
 
+  friend void YoungMarkCompact::RunPhases();
+
  private:
   using ObjReference = mirror::CompressedReference<mirror::Object>;
   static constexpr uint32_t kPageStateMask = (1 << BitSizeOf<uint8_t>()) - 1;
@@ -266,19 +330,22 @@ class MarkCompact final : public GarbageCollector {
   // mirror::Class.
   bool IsValidObject(mirror::Object* obj) const REQUIRES_SHARED(Locks::mutator_lock_);
   void InitializePhase();
-  void FinishPhase() REQUIRES(!Locks::mutator_lock_, !Locks::heap_bitmap_lock_, !lock_);
+  void FinishPhase(bool performed_compaction)
+      REQUIRES(!Locks::mutator_lock_, !Locks::heap_bitmap_lock_, !lock_);
   void MarkingPhase() REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(!Locks::heap_bitmap_lock_);
   void CompactionPhase() REQUIRES_SHARED(Locks::mutator_lock_);
 
   void SweepSystemWeaks(Thread* self, Runtime* runtime, const bool paused)
       REQUIRES_SHARED(Locks::mutator_lock_)
       REQUIRES(!Locks::heap_bitmap_lock_);
-  // Update the reference at given offset in the given object with post-compact
-  // address. [begin, end) is moving-space range.
-  ALWAYS_INLINE void UpdateRef(mirror::Object* obj,
-                               MemberOffset offset,
-                               uint8_t* begin,
-                               uint8_t* end) REQUIRES_SHARED(Locks::mutator_lock_);
+  // Update the reference at 'offset' in 'obj' with post-compact address, and
+  // return the new address. [begin, end) is a range in which compaction is
+  // happening. So post-compact address needs to be computed only for
+  // pre-compact references in this range.
+  ALWAYS_INLINE mirror::Object* UpdateRef(mirror::Object* obj,
+                                          MemberOffset offset,
+                                          uint8_t* begin,
+                                          uint8_t* end) REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Verify that the gc-root is updated only once. Returns false if the update
   // shouldn't be done.
@@ -286,20 +353,22 @@ class MarkCompact final : public GarbageCollector {
                                             mirror::Object* old_ref,
                                             const RootInfo& info)
       REQUIRES_SHARED(Locks::mutator_lock_);
-  // Update the given root with post-compact address. [begin, end) is
-  // moving-space range.
-  ALWAYS_INLINE void UpdateRoot(mirror::CompressedReference<mirror::Object>* root,
-                                uint8_t* begin,
-                                uint8_t* end,
-                                const RootInfo& info = RootInfo(RootType::kRootUnknown))
+  // Update the given root with post-compact address and return the new address. [begin, end)
+  // is a range in which compaction is happening. So post-compact address needs to be computed
+  // only for pre-compact references in this range.
+  ALWAYS_INLINE mirror::Object* UpdateRoot(mirror::CompressedReference<mirror::Object>* root,
+                                           uint8_t* begin,
+                                           uint8_t* end,
+                                           const RootInfo& info = RootInfo(RootType::kRootUnknown))
       REQUIRES_SHARED(Locks::mutator_lock_);
-  ALWAYS_INLINE void UpdateRoot(mirror::Object** root,
-                                uint8_t* begin,
-                                uint8_t* end,
-                                const RootInfo& info = RootInfo(RootType::kRootUnknown))
+  ALWAYS_INLINE mirror::Object* UpdateRoot(mirror::Object** root,
+                                           uint8_t* begin,
+                                           uint8_t* end,
+                                           const RootInfo& info = RootInfo(RootType::kRootUnknown))
       REQUIRES_SHARED(Locks::mutator_lock_);
-  // Given the pre-compact address, the function returns the post-compact
-  // address of the given object. [begin, end) is moving-space range.
+  // If the given pre-compact address (old_ref) is in [begin, end) range of moving-space,
+  // then the function returns the computed post-compact address. Otherwise, 'old_ref' is
+  // returned.
   ALWAYS_INLINE mirror::Object* PostCompactAddress(mirror::Object* old_ref,
                                                    uint8_t* begin,
                                                    uint8_t* end) const
@@ -318,8 +387,8 @@ class MarkCompact final : public GarbageCollector {
       REQUIRES_SHARED(Locks::mutator_lock_);
   // Clears (for alloc spaces in the beginning of marking phase) or ages the
   // card table. Also, identifies immune spaces and mark bitmap.
-  void PrepareCardTableForMarking(bool clear_alloc_space_cards)
-      REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(Locks::heap_bitmap_lock_);
+  void PrepareForMarking(bool pre_marking) REQUIRES_SHARED(Locks::mutator_lock_)
+      REQUIRES(Locks::heap_bitmap_lock_);
 
   // Perform one last round of marking, identifying roots from dirty cards
   // during a stop-the-world (STW) pause.
@@ -333,15 +402,20 @@ class MarkCompact final : public GarbageCollector {
   // during concurrent compaction. Also determines a black-dense region at the
   // beginning of the moving space which is not compacted. Returns false if
   // performing compaction isn't required.
-  bool PrepareForCompaction() REQUIRES_SHARED(Locks::mutator_lock_);
+  bool PrepareForCompaction() REQUIRES_SHARED(Locks::mutator_lock_)
+      REQUIRES(!Locks::heap_bitmap_lock_);
 
   // Copy gPageSize live bytes starting from 'offset' (within the moving space),
   // which must be within 'obj', into the gPageSize sized memory pointed by 'addr'.
   // Then update the references within the copied objects. The boundary objects are
   // partially updated such that only the references that lie in the page are updated.
   // This is necessary to avoid cascading userfaults.
-  void CompactPage(mirror::Object* obj, uint32_t offset, uint8_t* addr, bool needs_memset_zero)
-      REQUIRES_SHARED(Locks::mutator_lock_);
+  template <bool kSetupForGenerational>
+  void CompactPage(mirror::Object* obj,
+                   uint32_t offset,
+                   uint8_t* addr,
+                   uint8_t* to_space_addr,
+                   bool needs_memset_zero) REQUIRES_SHARED(Locks::mutator_lock_);
   // Compact the bump-pointer space. Pass page that should be used as buffer for
   // userfaultfd.
   template <int kMode>
@@ -359,6 +433,7 @@ class MarkCompact final : public GarbageCollector {
 
   // Update all the objects in the given non-moving page. 'first' object
   // could have started in some preceding page.
+  template <bool kSetupForGenerational>
   void UpdateNonMovingPage(mirror::Object* first,
                            uint8_t* page,
                            ptrdiff_t from_space_diff,
@@ -595,27 +670,44 @@ class MarkCompact final : public GarbageCollector {
   void UpdateClassTableClasses(Runtime* runtime, bool immune_class_table_only)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
+  void SweepArray(accounting::ObjectStack* obj_arr, bool swap_bitmaps)
+      REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(Locks::heap_bitmap_lock_);
+
+  // Set bit corresponding to 'obj' in 'mid_to_old_promo_bit_vec_' bit-vector.
+  // 'obj' is the post-compacted object in mid-gen, which will get promoted to
+  // old-gen and hence 'mid_to_old_promo_bit_vec_' is copied into mark-bitmap at
+  // the end of GC for next GC cycle.
+  void SetBitForMidToOldPromotion(uint8_t* obj);
+  // Scan old-gen for young GCs by looking for cards that are at least 'aged' in
+  // the card-table corresponding to moving and non-moving spaces.
+  void ScanOldGenObjects() REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_);
+
+  // Verify that cards corresponding to objects containing references to
+  // young-gen are dirty.
+  void VerifyNoMissingCardMarks() REQUIRES(Locks::heap_bitmap_lock_, Locks::mutator_lock_);
+  // Verify that post-GC objects (all objects except the ones allocated after
+  // marking pause) are valid with valid references in them. Bitmap corresponding
+  // to [moving_space_begin_, mark_bitmap_clear_end) was retained. This is used in
+  // case compaction is skipped.
+  void VerifyPostGCObjects(bool performed_compaction, uint8_t* mark_bitmap_clear_end)
+      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_);
+
   // For checkpoints
   Barrier gc_barrier_;
-  // Every object inside the immune spaces is assumed to be marked.
-  ImmuneSpaces immune_spaces_;
   // Required only when mark-stack is accessed in shared mode, which happens
   // when collecting thread-stack roots using checkpoint. Otherwise, we use it
   // to synchronize on updated_roots_ in debug-builds.
   Mutex lock_;
-  accounting::ObjectStack* mark_stack_;
-  // Special bitmap wherein all the bits corresponding to an object are set.
-  // TODO: make LiveWordsBitmap encapsulated in this class rather than a
-  // pointer. We tend to access its members in performance-sensitive
-  // code-path. Also, use a single MemMap for all the GC's data structures,
-  // which we will clear in the end. This would help in limiting the number of
-  // VMAs that get created in the kernel.
-  std::unique_ptr<LiveWordsBitmap<kAlignment>> live_words_bitmap_;
-  // Track GC-roots updated so far in a GC-cycle. This is to confirm that no
-  // GC-root is updated twice.
-  // TODO: Must be replaced with an efficient mechanism eventually. Or ensure
-  // that double updation doesn't happen in the first place.
-  std::unique_ptr<std::unordered_set<void*>> updated_roots_ GUARDED_BY(lock_);
+  // Counters to synchronize mutator threads and gc-thread at the end of
+  // compaction. Counter 0 represents the number of mutators still working on
+  // moving space pages which started before gc-thread finished compacting pages,
+  // whereas the counter 1 represents those which started afterwards but
+  // before unregistering the space from uffd. Once counter 1 reaches 0, the
+  // gc-thread madvises spaces and data structures like page-status array.
+  // Both the counters are set to 0 before compaction begins. They are or'ed
+  // with kSigbusCounterCompactionDoneMask one-by-one by gc-thread after
+  // compaction to communicate the status to future mutators.
+  std::atomic<SigbusCounterType> sigbus_in_progress_count_[2];
   MemMap from_space_map_;
   // Any array of live-bytes in logical chunks of kOffsetChunkSize size
   // in the 'to-be-compacted' space.
@@ -668,6 +760,31 @@ class MarkCompact final : public GarbageCollector {
   // either at the pair whose class is lower than the first page to be freed, or at the
   // pair whose object is not yet compacted.
   ClassAfterObjectMap::const_reverse_iterator class_after_obj_iter_;
+  // Every object inside the immune spaces is assumed to be marked.
+  ImmuneSpaces immune_spaces_;
+  // Bit-vector to store bits for objects which are promoted from mid-gen to
+  // old-gen during compaction. Later in FinishPhase() it's copied into
+  // mark-bitmap of moving-space.
+  std::unique_ptr<BitVector> mid_to_old_promo_bit_vec_;
+
+  // List of objects found to have native gc-roots into young-gen during
+  // marking. Cards corresponding to these objects are dirtied at the end of GC.
+  // These have to be captured during marking phase as we don't update
+  // native-roots during compaction.
+  std::vector<mirror::Object*> dirty_cards_later_vec_;
+  space::ContinuousSpace* non_moving_space_;
+  space::BumpPointerSpace* const bump_pointer_space_;
+  Thread* thread_running_gc_;
+  // Length of 'chunk_info_vec_' vector (defined below).
+  size_t vector_length_;
+  size_t live_stack_freeze_size_;
+  size_t non_moving_first_objs_count_;
+  // Length of first_objs_moving_space_ and pre_compact_offset_moving_space_
+  // arrays. Also the number of pages which are to be compacted.
+  size_t moving_first_objs_count_;
+  // Number of pages containing black-allocated objects, indicating number of
+  // pages to be slid.
+  size_t black_page_count_;
   // Used by FreeFromSpacePages() for maintaining markers in the moving space for
   // how far the pages have been reclaimed (madvised) and checked.
   //
@@ -688,30 +805,13 @@ class MarkCompact final : public GarbageCollector {
   // compacted contents for batching.
   uint8_t* cur_reclaimable_page_;
 
-  space::ContinuousSpace* non_moving_space_;
-  space::BumpPointerSpace* const bump_pointer_space_;
-  // The main space bitmap
-  accounting::ContinuousSpaceBitmap* const moving_space_bitmap_;
+  // Mark bits for non-moving space
   accounting::ContinuousSpaceBitmap* non_moving_space_bitmap_;
-  Thread* thread_running_gc_;
   // Array of moving-space's pages' compaction status, which is stored in the
   // least-significant byte. kProcessed entries also contain the from-space
   // offset of the page which contains the compacted contents of the ith
   // to-space page.
   Atomic<uint32_t>* moving_pages_status_;
-  size_t vector_length_;
-  size_t live_stack_freeze_size_;
-
-  uint64_t bytes_scanned_;
-
-  // For every page in the to-space (post-compact heap) we need to know the
-  // first object from which we must compact and/or update references. This is
-  // for both non-moving and moving space. Additionally, for the moving-space,
-  // we also need the offset within the object from where we need to start
-  // copying.
-  // chunk_info_vec_ holds live bytes for chunks during marking phase. After
-  // marking we perform an exclusive scan to compute offset for every chunk.
-  uint32_t* chunk_info_vec_;
   // For pages before black allocations, pre_compact_offset_moving_space_[i]
   // holds offset within the space from where the objects need to be copied in
   // the ith post-compact page.
@@ -727,70 +827,97 @@ class MarkCompact final : public GarbageCollector {
   // First object for every page. It could be greater than the page's start
   // address, or null if the page is empty.
   ObjReference* first_objs_non_moving_space_;
-  size_t non_moving_first_objs_count_;
-  // Length of first_objs_moving_space_ and pre_compact_offset_moving_space_
-  // arrays. Also the number of pages which are to be compacted.
-  size_t moving_first_objs_count_;
-  // Number of pages containing black-allocated objects, indicating number of
-  // pages to be slid.
-  size_t black_page_count_;
 
+  // Cache (from_space_begin_ - bump_pointer_space_->Begin()) so that we can
+  // compute from-space address of a given pre-comapct address efficiently.
+  ptrdiff_t from_space_slide_diff_;
   uint8_t* from_space_begin_;
+
+  // The moving space markers are ordered as follows:
+  // [moving_space_begin_, black_dense_end_, mid_gen_end_, post_compact_end_, moving_space_end_)
+
+  // End of compacted space. Used for computing post-compact address of black
+  // allocated objects. Aligned up to page size.
+  uint8_t* post_compact_end_;
+
+  // BEGIN HOT FIELDS: accessed per object
+
+  accounting::ObjectStack* mark_stack_;
+  uint64_t bytes_scanned_;
+  // Number of objects freed during this GC in moving space. It is decremented
+  // every time an object is discovered. And total-object count is added to it
+  // in MarkingPause(). It reaches the correct count only once the marking phase
+  // is completed.
+  int32_t freed_objects_;
+  // Set to true when doing young gen collection.
+  bool young_gen_;
+  const bool use_generational_;
+  // True while compacting.
+  bool compacting_;
+  // Mark bits for main space
+  accounting::ContinuousSpaceBitmap* const moving_space_bitmap_;
   // Cached values of moving-space range to optimize checking if reference
-  // belongs to moving-space or not. May get updated if and when heap is
-  // clamped.
+  // belongs to moving-space or not. May get updated if and when heap is clamped.
   uint8_t* const moving_space_begin_;
   uint8_t* moving_space_end_;
+  // In generational-mode, we maintain 3 generations: young, mid, and old.
+  // Mid generation is collected during young collections. This means objects
+  // need to survive two GCs before they get promoted to old-gen. This helps
+  // in avoiding pre-mature promotion of objects which are allocated just
+  // prior to a young collection but are short-lived.
+
   // Set to moving_space_begin_ if compacting the entire moving space.
   // Otherwise, set to a page-aligned address such that [moving_space_begin_,
   // black_dense_end_) is considered to be densely populated with reachable
-  // objects and hence is not compacted.
-  uint8_t* black_dense_end_;
+  // objects and hence is not compacted. In generational mode, old-gen is
+  // treated just like black-dense region.
+  union {
+    uint8_t* black_dense_end_;
+    uint8_t* old_gen_end_;
+  };
+  // Prior to compaction, 'mid_gen_end_' represents end of 'pre-compacted'
+  // mid-gen. During compaction, it represents 'post-compacted' end of mid-gen.
+  // This is done in PrepareForCompaction(). At the end of GC, in FinishPhase(),
+  // mid-gen gets consumed/promoted to old-gen, and young-gen becomes mid-gen,
+  // in preparation for the next GC cycle.
+  uint8_t* mid_gen_end_;
+
+  // BEGIN HOT FIELDS: accessed per reference update
+
+  // Special bitmap wherein all the bits corresponding to an object are set.
+  // TODO: make LiveWordsBitmap encapsulated in this class rather than a
+  // pointer. We tend to access its members in performance-sensitive
+  // code-path. Also, use a single MemMap for all the GC's data structures,
+  // which we will clear in the end. This would help in limiting the number of
+  // VMAs that get created in the kernel.
+  std::unique_ptr<LiveWordsBitmap<kAlignment>> live_words_bitmap_;
+  // For every page in the to-space (post-compact heap) we need to know the
+  // first object from which we must compact and/or update references. This is
+  // for both non-moving and moving space. Additionally, for the moving-space,
+  // we also need the offset within the object from where we need to start
+  // copying.
+  // chunk_info_vec_ holds live bytes for chunks during marking phase. After
+  // marking we perform an exclusive scan to compute offset for every chunk.
+  uint32_t* chunk_info_vec_;
   // moving-space's end pointer at the marking pause. All allocations beyond
   // this will be considered black in the current GC cycle. Aligned up to page
   // size.
   uint8_t* black_allocations_begin_;
-  // End of compacted space. Use for computing post-compact addr of black
-  // allocated objects. Aligned up to page size.
-  uint8_t* post_compact_end_;
   // Cache (black_allocations_begin_ - post_compact_end_) for post-compact
   // address computations.
   ptrdiff_t black_objs_slide_diff_;
-  // Cache (from_space_begin_ - bump_pointer_space_->Begin()) so that we can
-  // compute from-space address of a given pre-comapct addr efficiently.
-  ptrdiff_t from_space_slide_diff_;
 
-  // TODO: Remove once an efficient mechanism to deal with double root updation
-  // is incorporated.
-  void* stack_high_addr_;
-  void* stack_low_addr_;
+  // END HOT FIELDS: accessed per reference update
+  // END HOT FIELDS: accessed per object
 
   uint8_t* conc_compaction_termination_page_;
-
   PointerSize pointer_size_;
-  // Number of objects freed during this GC in moving space. It is decremented
-  // every time an object is discovered. And total-object count is added to it
-  // in MarkingPause(). It reaches the correct count only once the marking phase
-  // is completed.
-  int32_t freed_objects_;
   // Userfault file descriptor, accessed only by the GC itself.
   // kFallbackMode value indicates that we are in the fallback mode.
   int uffd_;
-  // Counters to synchronize mutator threads and gc-thread at the end of
-  // compaction. Counter 0 represents the number of mutators still working on
-  // moving space pages which started before gc-thread finished compacting pages,
-  // whereas the counter 1 represents those which started afterwards but
-  // before unregistering the space from uffd. Once counter 1 reaches 0, the
-  // gc-thread madvises spaces and data structures like page-status array.
-  // Both the counters are set to 0 before compaction begins. They are or'ed
-  // with kSigbusCounterCompactionDoneMask one-by-one by gc-thread after
-  // compaction to communicate the status to future mutators.
-  std::atomic<SigbusCounterType> sigbus_in_progress_count_[2];
   // When using SIGBUS feature, this counter is used by mutators to claim a page
   // out of compaction buffers to be used for the entire compaction cycle.
   std::atomic<uint16_t> compaction_buffer_counter_;
-  // True while compacting.
-  bool compacting_;
   // Set to true in MarkingPause() to indicate when allocation_stack_ should be
   // checked in IsMarked() for black allocations.
   bool marking_done_;
@@ -805,6 +932,24 @@ class MarkCompact final : public GarbageCollector {
   // is also clamped, then we set it to 'Finished'.
   ClampInfoStatus clamp_info_map_status_;
 
+  // Track GC-roots updated so far in a GC-cycle. This is to confirm that no
+  // GC-root is updated twice.
+  // TODO: Must be replaced with an efficient mechanism eventually. Or ensure
+  // that double updation doesn't happen in the first place.
+  std::unique_ptr<std::unordered_set<void*>> updated_roots_ GUARDED_BY(lock_);
+  // TODO: Remove once an efficient mechanism to deal with double root updation
+  // is incorporated.
+  void* stack_high_addr_;
+  void* stack_low_addr_;
+  // Following values for logging purposes
+  void* prev_post_compact_end_;
+  void* prev_black_dense_end_;
+  void* prev_black_allocations_begin_;
+  bool prev_gc_young_;
+  bool prev_gc_performed_compaction_;
+  // Timestamp when the read-barrier is enabled
+  uint64_t app_slow_path_start_time_;
+
   class FlipCallback;
   class ThreadFlipVisitor;
   class VerifyRootMarkedVisitor;
@@ -813,11 +958,14 @@ class MarkCompact final : public GarbageCollector {
   template <size_t kBufferSize>
   class ThreadRootsVisitor;
   class RefFieldsVisitor;
-  template <bool kCheckBegin, bool kCheckEnd> class RefsUpdateVisitor;
+  template <bool kCheckBegin, bool kCheckEnd, bool kDirtyOldToMid = false>
+  class RefsUpdateVisitor;
   class ArenaPoolPageUpdater;
   class ClassLoaderRootsUpdater;
   class LinearAllocPageUpdater;
   class ImmuneSpaceUpdateObjVisitor;
+  template <typename Visitor>
+  class VisitReferencesVisitor;
 
   DISALLOW_IMPLICIT_CONSTRUCTORS(MarkCompact);
 };
diff --git a/runtime/gc/collector/semi_space.cc b/runtime/gc/collector/semi_space.cc
index 77800c32e0..9ec44fa765 100644
--- a/runtime/gc/collector/semi_space.cc
+++ b/runtime/gc/collector/semi_space.cc
@@ -503,7 +503,7 @@ void SemiSpace::SweepSystemWeaks() {
   TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());
   Runtime* runtime = Runtime::Current();
   runtime->SweepSystemWeaks(this);
-  runtime->GetThreadList()->SweepInterpreterCaches(this);
+  runtime->GetThreadList()->ClearInterpreterCaches();
 }
 
 bool SemiSpace::ShouldSweepSpace(space::ContinuousSpace* space) const {
diff --git a/runtime/gc/collector_type.h b/runtime/gc/collector_type.h
index 3c19079c08..4267a7763c 100644
--- a/runtime/gc/collector_type.h
+++ b/runtime/gc/collector_type.h
@@ -75,6 +75,8 @@ static constexpr CollectorType kCollectorTypeDefault =
     kCollectorTypeSS
 #elif ART_DEFAULT_GC_TYPE_IS_CMS
     kCollectorTypeCMS
+#elif ART_DEFAULT_GC_TYPE_IS_MS
+    kCollectorTypeMS
 #else
 #error "ART default GC type must be set"
 #endif
diff --git a/runtime/gc/heap.cc b/runtime/gc/heap.cc
index 7ea675d4cf..dcb25a08d7 100644
--- a/runtime/gc/heap.cc
+++ b/runtime/gc/heap.cc
@@ -149,11 +149,12 @@ DEFINE_RUNTIME_DEBUG_FLAG(Heap, kStressCollectorTransition);
 // Minimum amount of remaining bytes before a concurrent GC is triggered.
 static constexpr size_t kMinConcurrentRemainingBytes = 128 * KB;
 static constexpr size_t kMaxConcurrentRemainingBytes = 512 * KB;
-// Sticky GC throughput adjustment, divided by 4. Increasing this causes sticky GC to occur more
-// relative to partial/full GC. This may be desirable since sticky GCs interfere less with mutator
-// threads (lower pauses, use less memory bandwidth).
-static double GetStickyGcThroughputAdjustment(bool use_generational_cc) {
-  return use_generational_cc ? 0.5 : 1.0;
+// Sticky GC throughput adjustment. Increasing this causes sticky GC to occur more
+// relative to partial/full GC. This may be desirable since sticky GCs interfere less
+// with mutator threads (lower pauses, use less memory bandwidth). The value
+// (1.0) for non-generational GC case is fixed and shall never change.
+static double GetStickyGcThroughputAdjustment(bool use_generational_gc) {
+  return use_generational_gc ? 0.5 : 1.0;
 }
 // Whether or not we compact the zygote in PreZygoteFork.
 static constexpr bool kCompactZygote = kMovingCollector;
@@ -307,7 +308,7 @@ Heap::Heap(size_t initial_size,
            bool gc_stress_mode,
            bool measure_gc_performance,
            bool use_homogeneous_space_compaction_for_oom,
-           bool use_generational_cc,
+           bool use_generational_gc,
            uint64_t min_interval_homogeneous_space_compaction_by_oom,
            bool dump_region_info_before_gc,
            bool dump_region_info_after_gc)
@@ -375,11 +376,10 @@ Heap::Heap(size_t initial_size,
        * verification is enabled, we limit the size of allocation stacks to speed up their
        * searching.
        */
-      max_allocation_stack_size_(kGCALotMode
-          ? kGcAlotAllocationStackSize
-          : (kVerifyObjectSupport > kVerifyObjectModeFast)
-              ? kVerifyObjectAllocationStackSize
-              : kDefaultAllocationStackSize),
+      max_allocation_stack_size_(kGCALotMode ? kGcAlotAllocationStackSize
+                                 : (kVerifyObjectSupport > kVerifyObjectModeFast)
+                                     ? kVerifyObjectAllocationStackSize
+                                     : kDefaultAllocationStackSize),
       current_allocator_(kAllocatorTypeDlMalloc),
       current_non_moving_allocator_(kAllocatorTypeNonMoving),
       bump_pointer_space_(nullptr),
@@ -408,7 +408,7 @@ Heap::Heap(size_t initial_size,
       pending_collector_transition_(nullptr),
       pending_heap_trim_(nullptr),
       use_homogeneous_space_compaction_for_oom_(use_homogeneous_space_compaction_for_oom),
-      use_generational_cc_(use_generational_cc),
+      use_generational_gc_(use_generational_gc),
       running_collection_is_blocking_(false),
       blocking_gc_count_(0U),
       blocking_gc_time_(0U),
@@ -652,7 +652,7 @@ Heap::Heap(size_t initial_size,
         space::RegionSpace::CreateMemMap(kRegionSpaceName, capacity_ * 2, request_begin);
     CHECK(region_space_mem_map.IsValid()) << "No region space mem map";
     region_space_ = space::RegionSpace::Create(
-        kRegionSpaceName, std::move(region_space_mem_map), use_generational_cc_);
+        kRegionSpaceName, std::move(region_space_mem_map), use_generational_gc_);
     AddSpace(region_space_);
   } else if (IsMovingGc(foreground_collector_type_)) {
     // Create bump pointer spaces.
@@ -778,9 +778,50 @@ Heap::Heap(size_t initial_size,
     concurrent_start_bytes_ = std::numeric_limits<size_t>::max();
   }
   CHECK_NE(target_footprint_.load(std::memory_order_relaxed), 0U);
-  // Create our garbage collectors.
+  CreateGarbageCollectors(measure_gc_performance);
+  if (!GetBootImageSpaces().empty() && non_moving_space_ != nullptr &&
+      (is_zygote || separate_non_moving_space)) {
+    // Check that there's no gap between the image space and the non moving space so that the
+    // immune region won't break (eg. due to a large object allocated in the gap). This is only
+    // required when we're the zygote.
+    // Space with smallest Begin().
+    space::ImageSpace* first_space = nullptr;
+    for (space::ImageSpace* space : boot_image_spaces_) {
+      if (first_space == nullptr || space->Begin() < first_space->Begin()) {
+        first_space = space;
+      }
+    }
+    bool no_gap = MemMap::CheckNoGaps(*first_space->GetMemMap(), *non_moving_space_->GetMemMap());
+    if (!no_gap) {
+      PrintFileToLog("/proc/self/maps", LogSeverity::ERROR);
+      MemMap::DumpMaps(LOG_STREAM(ERROR), /* terse= */ true);
+      LOG(FATAL) << "There's a gap between the image space and the non-moving space";
+    }
+  }
+  // Perfetto Java Heap Profiler Support.
+  if (runtime->IsPerfettoJavaHeapStackProfEnabled()) {
+    // Perfetto Plugin is loaded and enabled, initialize the Java Heap Profiler.
+    InitPerfettoJavaHeapProf();
+  } else {
+    // Disable the Java Heap Profiler.
+    GetHeapSampler().DisableHeapSampler();
+  }
+
+  instrumentation::Instrumentation* const instrumentation = runtime->GetInstrumentation();
+  if (gc_stress_mode_) {
+    backtrace_lock_ = new Mutex("GC complete lock");
+  }
+  if (is_running_on_memory_tool_ || gc_stress_mode_) {
+    instrumentation->InstrumentQuickAllocEntryPoints();
+  }
+  if (VLOG_IS_ON(heap) || VLOG_IS_ON(startup)) {
+    LOG(INFO) << "Heap() exiting";
+  }
+}
+
+void Heap::CreateGarbageCollectors(bool measure_gc_performance) {
   for (size_t i = 0; i < 2; ++i) {
-    const bool concurrent = i != 0;
+    const bool concurrent = (i != 0);
     if ((MayUseCollector(kCollectorTypeCMS) && concurrent) ||
         (MayUseCollector(kCollectorTypeMS) && !concurrent)) {
       garbage_collectors_.push_back(new collector::MarkSweep(this, concurrent));
@@ -798,75 +839,41 @@ Heap::Heap(size_t initial_size,
     if (MayUseCollector(kCollectorTypeCMC)) {
       mark_compact_ = new collector::MarkCompact(this);
       garbage_collectors_.push_back(mark_compact_);
+      if (use_generational_gc_) {
+        young_mark_compact_ = new collector::YoungMarkCompact(this, mark_compact_);
+        garbage_collectors_.push_back(young_mark_compact_);
+      }
     }
     if (MayUseCollector(kCollectorTypeCC)) {
       concurrent_copying_collector_ = new collector::ConcurrentCopying(this,
                                                                        /*young_gen=*/false,
-                                                                       use_generational_cc_,
+                                                                       use_generational_gc_,
                                                                        "",
                                                                        measure_gc_performance);
-      if (use_generational_cc_) {
-        young_concurrent_copying_collector_ = new collector::ConcurrentCopying(
-            this,
-            /*young_gen=*/true,
-            use_generational_cc_,
-            "young",
-            measure_gc_performance);
+      if (use_generational_gc_) {
+        young_concurrent_copying_collector_ =
+            new collector::ConcurrentCopying(this,
+                                             /*young_gen=*/true,
+                                             use_generational_gc_,
+                                             "young",
+                                             measure_gc_performance);
       }
       active_concurrent_copying_collector_.store(concurrent_copying_collector_,
                                                  std::memory_order_relaxed);
       DCHECK(region_space_ != nullptr);
       concurrent_copying_collector_->SetRegionSpace(region_space_);
-      if (use_generational_cc_) {
+      if (use_generational_gc_) {
         young_concurrent_copying_collector_->SetRegionSpace(region_space_);
         // At this point, non-moving space should be created.
         DCHECK(non_moving_space_ != nullptr);
         concurrent_copying_collector_->CreateInterRegionRefBitmaps();
       }
       garbage_collectors_.push_back(concurrent_copying_collector_);
-      if (use_generational_cc_) {
+      if (use_generational_gc_) {
         garbage_collectors_.push_back(young_concurrent_copying_collector_);
       }
     }
   }
-  if (!GetBootImageSpaces().empty() && non_moving_space_ != nullptr &&
-      (is_zygote || separate_non_moving_space)) {
-    // Check that there's no gap between the image space and the non moving space so that the
-    // immune region won't break (eg. due to a large object allocated in the gap). This is only
-    // required when we're the zygote.
-    // Space with smallest Begin().
-    space::ImageSpace* first_space = nullptr;
-    for (space::ImageSpace* space : boot_image_spaces_) {
-      if (first_space == nullptr || space->Begin() < first_space->Begin()) {
-        first_space = space;
-      }
-    }
-    bool no_gap = MemMap::CheckNoGaps(*first_space->GetMemMap(), *non_moving_space_->GetMemMap());
-    if (!no_gap) {
-      PrintFileToLog("/proc/self/maps", LogSeverity::ERROR);
-      MemMap::DumpMaps(LOG_STREAM(ERROR), /* terse= */ true);
-      LOG(FATAL) << "There's a gap between the image space and the non-moving space";
-    }
-  }
-  // Perfetto Java Heap Profiler Support.
-  if (runtime->IsPerfettoJavaHeapStackProfEnabled()) {
-    // Perfetto Plugin is loaded and enabled, initialize the Java Heap Profiler.
-    InitPerfettoJavaHeapProf();
-  } else {
-    // Disable the Java Heap Profiler.
-    GetHeapSampler().DisableHeapSampler();
-  }
-
-  instrumentation::Instrumentation* const instrumentation = runtime->GetInstrumentation();
-  if (gc_stress_mode_) {
-    backtrace_lock_ = new Mutex("GC complete lock");
-  }
-  if (is_running_on_memory_tool_ || gc_stress_mode_) {
-    instrumentation->InstrumentQuickAllocEntryPoints();
-  }
-  if (VLOG_IS_ON(heap) || VLOG_IS_ON(startup)) {
-    LOG(INFO) << "Heap() exiting";
-  }
 }
 
 MemMap Heap::MapAnonymousPreferredAddress(const char* name,
@@ -2315,7 +2322,7 @@ void Heap::ChangeCollector(CollectorType collector_type) {
     gc_plan_.clear();
     switch (collector_type_) {
       case kCollectorTypeCC: {
-        if (use_generational_cc_) {
+        if (use_generational_gc_) {
           gc_plan_.push_back(collector::kGcTypeSticky);
         }
         gc_plan_.push_back(collector::kGcTypeFull);
@@ -2327,6 +2334,9 @@ void Heap::ChangeCollector(CollectorType collector_type) {
         break;
       }
       case kCollectorTypeCMC: {
+        if (use_generational_gc_) {
+          gc_plan_.push_back(collector::kGcTypeSticky);
+        }
         gc_plan_.push_back(collector::kGcTypeFull);
         if (use_tlab_) {
           ChangeAllocator(kAllocatorTypeTLAB);
@@ -2568,6 +2578,9 @@ void Heap::PreZygoteFork() {
         region_space_->GetMarkBitmap()->Clear();
       } else {
         bump_pointer_space_->GetMemMap()->Protect(PROT_READ | PROT_WRITE);
+        if (gUseUserfaultfd && use_generational_gc_) {
+          MarkCompactCollector()->ResetGenerationalState();
+        }
       }
     }
     if (temp_space_ != nullptr) {
@@ -2675,7 +2688,7 @@ void Heap::MarkAllocStack(accounting::ContinuousSpaceBitmap* bitmap1,
   const auto* limit = stack->End();
   for (auto* it = stack->Begin(); it != limit; ++it) {
     const mirror::Object* obj = it->AsMirrorPtr();
-    if (!kUseThreadLocalAllocationStack || obj != nullptr) {
+    if (obj != nullptr) {
       if (bitmap1->HasAddress(obj)) {
         bitmap1->Set(obj);
       } else if (bitmap2->HasAddress(obj)) {
@@ -2859,10 +2872,13 @@ collector::GcType Heap::CollectGarbageInternal(collector::GcType gc_type,
           break;
         case kCollectorTypeCMC:
           collector = mark_compact_;
+          if (use_generational_gc_ && gc_type == collector::kGcTypeSticky) {
+            collector = young_mark_compact_;
+          }
           break;
         case kCollectorTypeCC:
           collector::ConcurrentCopying* active_cc_collector;
-          if (use_generational_cc_) {
+          if (use_generational_gc_) {
             // TODO: Other threads must do the flip checkpoint before they start poking at
             // active_concurrent_copying_collector_. So we should not concurrency here.
             active_cc_collector = (gc_type == collector::kGcTypeSticky) ?
@@ -3375,8 +3391,9 @@ class VerifyReferenceCardVisitor {
           if (!obj->IsObjectArray()) {
             ObjPtr<mirror::Class> klass = is_static ? obj->AsClass() : obj->GetClass();
             CHECK(klass != nullptr);
-            for (ArtField& field : (is_static ? klass->GetSFields() : klass->GetIFields())) {
-              if (field.GetOffset().Int32Value() == offset.Int32Value()) {
+            for (ArtField& field : klass->GetFields()) {
+              if (is_static == field.IsStatic() &&
+                  field.GetOffset().Int32Value() == offset.Int32Value()) {
                 LOG(ERROR) << (is_static ? "Static " : "") << "field in the live stack is "
                            << field.PrettyField();
                 break;
@@ -3800,13 +3817,13 @@ void Heap::GrowForUtilization(collector::GarbageCollector* collector_ran,
     collector::GcType non_sticky_gc_type = NonStickyGcType();
     // Find what the next non sticky collector will be.
     collector::GarbageCollector* non_sticky_collector = FindCollectorByGcType(non_sticky_gc_type);
-    if (use_generational_cc_) {
+    if (use_generational_gc_) {
       if (non_sticky_collector == nullptr) {
         non_sticky_collector = FindCollectorByGcType(collector::kGcTypePartial);
       }
       CHECK(non_sticky_collector != nullptr);
     }
-    double sticky_gc_throughput_adjustment = GetStickyGcThroughputAdjustment(use_generational_cc_);
+    double sticky_gc_throughput_adjustment = GetStickyGcThroughputAdjustment(use_generational_gc_);
 
     // If the throughput of the current sticky GC >= throughput of the non sticky collector, then
     // do another sticky collection next.
diff --git a/runtime/gc/heap.h b/runtime/gc/heap.h
index 20a74a475a..7e30c3d14e 100644
--- a/runtime/gc/heap.h
+++ b/runtime/gc/heap.h
@@ -40,6 +40,7 @@
 #include "gc/gc_cause.h"
 #include "gc/space/large_object_space.h"
 #include "gc/space/space.h"
+#include "gc/space/zygote_space.h"
 #include "handle.h"
 #include "obj_ptr.h"
 #include "offsets.h"
@@ -237,7 +238,7 @@ class Heap {
        bool gc_stress_mode,
        bool measure_gc_performance,
        bool use_homogeneous_space_compaction,
-       bool use_generational_cc,
+       bool use_generational_gc,
        uint64_t min_interval_homogeneous_space_compaction_by_oom,
        bool dump_region_info_before_gc,
        bool dump_region_info_after_gc);
@@ -570,9 +571,7 @@ class Heap {
     return num_bytes_allocated_.fetch_add(bytes, std::memory_order_relaxed);
   }
 
-  bool GetUseGenerationalCC() const {
-    return use_generational_cc_;
-  }
+  bool GetUseGenerational() const { return use_generational_gc_; }
 
   // Returns the number of objects currently allocated.
   size_t GetObjectsAllocated() const
@@ -723,8 +722,7 @@ class Heap {
   EXPORT bool ObjectIsInBootImageSpace(ObjPtr<mirror::Object> obj) const
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  bool IsInBootImageOatFile(const void* p) const
-      REQUIRES_SHARED(Locks::mutator_lock_);
+  bool IsInBootImageOatFile(const void* p) const;
 
   // Get the start address of the boot images if any; otherwise returns 0.
   uint32_t GetBootImagesStartAddress() const {
@@ -823,11 +821,16 @@ class Heap {
     return zygote_space_ != nullptr;
   }
 
+  bool IsInZygoteSpace(const mirror::Object* obj) const {
+    return zygote_space_ != nullptr && zygote_space_->Contains(obj);
+  }
+
   // Returns the active concurrent copying collector.
   collector::ConcurrentCopying* ConcurrentCopyingCollector() {
+    DCHECK(gUseReadBarrier);
     collector::ConcurrentCopying* active_collector =
             active_concurrent_copying_collector_.load(std::memory_order_relaxed);
-    if (use_generational_cc_) {
+    if (use_generational_gc_) {
       DCHECK((active_collector == concurrent_copying_collector_) ||
              (active_collector == young_concurrent_copying_collector_))
               << "active_concurrent_copying_collector: " << active_collector
@@ -1050,6 +1053,8 @@ class Heap {
   double CalculateGcWeightedAllocatedBytes(uint64_t gc_last_process_cpu_time_ns,
                                            uint64_t current_process_cpu_time) const;
 
+  // Called only from the constructor.
+  void CreateGarbageCollectors(bool measure_gc_performance);
   // Create a mem map with a preferred base address.
   static MemMap MapAnonymousPreferredAddress(const char* name,
                                              uint8_t* request_begin,
@@ -1640,10 +1645,15 @@ class Heap {
 
   std::vector<collector::GarbageCollector*> garbage_collectors_;
   collector::SemiSpace* semi_space_collector_;
-  collector::MarkCompact* mark_compact_;
   Atomic<collector::ConcurrentCopying*> active_concurrent_copying_collector_;
-  collector::ConcurrentCopying* young_concurrent_copying_collector_;
-  collector::ConcurrentCopying* concurrent_copying_collector_;
+  union {
+    collector::ConcurrentCopying* young_concurrent_copying_collector_;
+    collector::YoungMarkCompact* young_mark_compact_;
+  };
+  union {
+    collector::ConcurrentCopying* concurrent_copying_collector_;
+    collector::MarkCompact* mark_compact_;
+  };
 
   const bool is_running_on_memory_tool_;
   const bool use_tlab_;
@@ -1688,10 +1698,11 @@ class Heap {
   // Whether or not we use homogeneous space compaction to avoid OOM errors.
   bool use_homogeneous_space_compaction_for_oom_;
 
-  // If true, enable generational collection when using the Concurrent Copying
-  // (CC) collector, i.e. use sticky-bit CC for minor collections and (full) CC
-  // for major collections. Set in Heap constructor.
-  const bool use_generational_cc_;
+  // If true, enable generational collection when using a concurrent collector
+  // like Concurrent Copying (CC) or Concurrent Mark Compact (CMC) collectors,
+  // i.e. use sticky-bit for minor collections and full heap for major collections.
+  // Set in Heap constructor.
+  const bool use_generational_gc_;
 
   // True if the currently running collection has made some thread wait.
   bool running_collection_is_blocking_ GUARDED_BY(gc_complete_lock_);
diff --git a/runtime/gc/heap_test.cc b/runtime/gc/heap_test.cc
index bd8fdc6b46..0275170d98 100644
--- a/runtime/gc/heap_test.cc
+++ b/runtime/gc/heap_test.cc
@@ -45,9 +45,9 @@ class HeapTest : public CommonRuntimeTest {
                                      gc::Heap::kPreferredAllocSpaceBegin,
                                      16 * KB,
                                      PROT_READ,
-                                     /*low_4gb=*/ true,
-                                     /*reuse=*/ false,
-                                     /*reservation=*/ nullptr,
+                                     /*low_4gb=*/true,
+                                     /*reuse=*/false,
+                                     /*reservation=*/nullptr,
                                      &error_msg);
     // There is no guarantee that reserved_ will be valid (due to ASLR). See b/175018342.
     CommonRuntimeTest::SetUp();
@@ -71,16 +71,17 @@ TEST_F(HeapTest, ClearGrowthLimit) {
 TEST_F(HeapTest, GarbageCollectClassLinkerInit) {
   {
     ScopedObjectAccess soa(Thread::Current());
-    // garbage is created during ClassLinker::Init
-
+    // garbage is created during ClassLinker::Init()
+    constexpr size_t kNumArrays = 1024;
+    constexpr size_t kNumElements = 2048;
     StackHandleScope<1> hs(soa.Self());
     Handle<mirror::Class> c(
         hs.NewHandle(class_linker_->FindSystemClass(soa.Self(), "[Ljava/lang/Object;")));
-    for (size_t i = 0; i < 1024; ++i) {
+    for (size_t i = 0; i < kNumArrays; ++i) {
       StackHandleScope<1> hs2(soa.Self());
       Handle<mirror::ObjectArray<mirror::Object>> array(hs2.NewHandle(
-          mirror::ObjectArray<mirror::Object>::Alloc(soa.Self(), c.Get(), 2048)));
-      for (size_t j = 0; j < 2048; ++j) {
+          mirror::ObjectArray<mirror::Object>::Alloc(soa.Self(), c.Get(), kNumElements)));
+      for (size_t j = 0; j < kNumElements; ++j) {
         ObjPtr<mirror::String> string =
             mirror::String::AllocFromModifiedUtf8(soa.Self(), "hello, world!");
         // handle scope operator -> deferences the handle scope before running the method.
@@ -109,17 +110,29 @@ TEST_F(HeapTest, DumpGCPerformanceOnShutdown) {
 bool AnyIsFalse(bool x, bool y) { return !x || !y; }
 
 TEST_F(HeapTest, GCMetrics) {
-  // Allocate a few string objects (to be collected), then trigger garbage
-  // collection, and check that GC metrics are updated (where applicable).
+  // Allocate a lot of object arrays to be collected (to ensure the garbage collection is long
+  // enough for the timing metrics to be non-zero), then trigger garbage collection, and check that
+  // GC metrics are updated (where applicable).
   Heap* heap = Runtime::Current()->GetHeap();
   {
-    constexpr const size_t kNumObj = 128;
+    constexpr size_t kNumArrays = 32768;
+    constexpr size_t kNumElements = 4;
     ScopedObjectAccess soa(Thread::Current());
-    StackHandleScope<kNumObj> hs(soa.Self());
-    for (size_t i = 0u; i < kNumObj; ++i) {
-      Handle<mirror::String> string [[maybe_unused]] (
-          hs.NewHandle(mirror::String::AllocFromModifiedUtf8(soa.Self(), "test")));
+    StackHandleScope<kNumArrays + 1> hs(soa.Self());
+    Handle<mirror::Class> c(
+        hs.NewHandle(class_linker_->FindSystemClass(soa.Self(), "[Ljava/lang/Object;")));
+    for (size_t i = 0; i < kNumArrays; i++) {
+      MutableHandle<mirror::ObjectArray<mirror::Object>> array(hs.NewHandle(
+          mirror::ObjectArray<mirror::Object>::Alloc(soa.Self(), c.Get(), kNumElements)));
+      // Perform another allocation so that the previous object array becomes garbage,
+      // forcing all the components of the GC to be involved due to the mix of reachable
+      // and unreachable objects.
+      array.Assign(mirror::ObjectArray<mirror::Object>::Alloc(soa.Self(), c.Get(), kNumElements));
+      for (size_t j = 0; j < kNumElements; j++) {
+        array->Set<false>(j, array.Get());
+      }
     }
+
     // Do one GC while the temporary objects are reachable, forcing the GC to scan something.
     // The subsequent GC at line 127 may not scan anything but will certainly free some bytes.
     // Together the two GCs ensure success of the test.
@@ -144,6 +157,8 @@ TEST_F(HeapTest, GCMetrics) {
   metrics::MetricsBase<uint64_t>* full_gc_freed_bytes_delta = metrics->FullGcFreedBytesDelta();
   metrics::MetricsBase<uint64_t>* full_gc_duration = metrics->FullGcDuration();
   metrics::MetricsBase<uint64_t>* full_gc_duration_delta = metrics->FullGcDurationDelta();
+  metrics::MetricsBase<uint64_t>* full_gc_app_slow_path_duration_delta =
+      metrics->AppSlowPathDuringFullGcDurationDelta();
   // ART young-generation GC metrics.
   metrics::MetricsBase<int64_t>* young_gc_collection_time = metrics->YoungGcCollectionTime();
   metrics::MetricsBase<uint64_t>* young_gc_count = metrics->YoungGcCount();
@@ -160,14 +175,16 @@ TEST_F(HeapTest, GCMetrics) {
   metrics::MetricsBase<uint64_t>* young_gc_freed_bytes_delta = metrics->YoungGcFreedBytesDelta();
   metrics::MetricsBase<uint64_t>* young_gc_duration = metrics->YoungGcDuration();
   metrics::MetricsBase<uint64_t>* young_gc_duration_delta = metrics->YoungGcDurationDelta();
+  metrics::MetricsBase<uint64_t>* young_gc_app_slow_path_duration_delta =
+      metrics->AppSlowPathDuringYoungGcDurationDelta();
 
   CollectorType fg_collector_type = heap->GetForegroundCollectorType();
   if (fg_collector_type == kCollectorTypeCC || fg_collector_type == kCollectorTypeCMC) {
     // Only the Concurrent Copying and Concurrent Mark-Compact collectors enable
     // GC metrics at the moment.
-    if (heap->GetUseGenerationalCC()) {
+    if (heap->GetUseGenerational()) {
       // Check that full-heap and/or young-generation GC metrics are non-null
-      // after trigerring the collection.
+      // after triggering the collection.
       EXPECT_PRED2(
           AnyIsFalse, full_gc_collection_time->IsNull(), young_gc_collection_time->IsNull());
       EXPECT_PRED2(AnyIsFalse, full_gc_count->IsNull(), young_gc_count->IsNull());
@@ -186,18 +203,13 @@ TEST_F(HeapTest, GCMetrics) {
       EXPECT_PRED2(AnyIsFalse, full_gc_freed_bytes->IsNull(), young_gc_freed_bytes->IsNull());
       EXPECT_PRED2(
           AnyIsFalse, full_gc_freed_bytes_delta->IsNull(), young_gc_freed_bytes_delta->IsNull());
-      // We have observed that sometimes the GC duration (both for full-heap and
-      // young-generation collections) is null (b/271112044). Temporarily
-      // suspend the following checks while we investigate.
-      //
-      // TODO(b/271112044): Investigate and adjust these expectations and/or the
-      // corresponding metric logic.
-#if 0
       EXPECT_PRED2(AnyIsFalse, full_gc_duration->IsNull(), young_gc_duration->IsNull());
       EXPECT_PRED2(AnyIsFalse, full_gc_duration_delta->IsNull(), young_gc_duration_delta->IsNull());
-#endif
+      EXPECT_PRED2(AnyIsFalse,
+                   full_gc_app_slow_path_duration_delta->IsNull(),
+                   young_gc_app_slow_path_duration_delta->IsNull());
     } else {
-      // Check that only full-heap GC metrics are non-null after trigerring the collection.
+      // Check that only full-heap GC metrics are non-null after triggering the collection.
       EXPECT_FALSE(full_gc_collection_time->IsNull());
       EXPECT_FALSE(full_gc_count->IsNull());
       EXPECT_FALSE(full_gc_count_delta->IsNull());
@@ -211,6 +223,7 @@ TEST_F(HeapTest, GCMetrics) {
       EXPECT_FALSE(full_gc_freed_bytes_delta->IsNull());
       EXPECT_FALSE(full_gc_duration->IsNull());
       EXPECT_FALSE(full_gc_duration_delta->IsNull());
+      EXPECT_FALSE(full_gc_app_slow_path_duration_delta->IsNull());
 
       EXPECT_TRUE(young_gc_collection_time->IsNull());
       EXPECT_TRUE(young_gc_count->IsNull());
@@ -225,9 +238,10 @@ TEST_F(HeapTest, GCMetrics) {
       EXPECT_TRUE(young_gc_freed_bytes_delta->IsNull());
       EXPECT_TRUE(young_gc_duration->IsNull());
       EXPECT_TRUE(young_gc_duration_delta->IsNull());
+      EXPECT_TRUE(young_gc_app_slow_path_duration_delta->IsNull());
     }
   } else {
-    // Check that all metrics are null after trigerring the collection.
+    // Check that all metrics are null after triggering the collection.
     EXPECT_TRUE(full_gc_collection_time->IsNull());
     EXPECT_TRUE(full_gc_count->IsNull());
     EXPECT_TRUE(full_gc_count_delta->IsNull());
@@ -241,6 +255,7 @@ TEST_F(HeapTest, GCMetrics) {
     EXPECT_TRUE(full_gc_freed_bytes_delta->IsNull());
     EXPECT_TRUE(full_gc_duration->IsNull());
     EXPECT_TRUE(full_gc_duration_delta->IsNull());
+    EXPECT_TRUE(full_gc_app_slow_path_duration_delta->IsNull());
 
     EXPECT_TRUE(young_gc_collection_time->IsNull());
     EXPECT_TRUE(young_gc_count->IsNull());
@@ -255,6 +270,7 @@ TEST_F(HeapTest, GCMetrics) {
     EXPECT_TRUE(young_gc_freed_bytes_delta->IsNull());
     EXPECT_TRUE(young_gc_duration->IsNull());
     EXPECT_TRUE(young_gc_duration_delta->IsNull());
+    EXPECT_TRUE(young_gc_app_slow_path_duration_delta->IsNull());
   }
 }
 
diff --git a/runtime/gc/reference_processor.cc b/runtime/gc/reference_processor.cc
index cb777c895c..6420fbbff6 100644
--- a/runtime/gc/reference_processor.cc
+++ b/runtime/gc/reference_processor.cc
@@ -54,8 +54,10 @@ ReferenceProcessor::ReferenceProcessor()
 static inline MemberOffset GetSlowPathFlagOffset(ObjPtr<mirror::Class> reference_class)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   DCHECK(reference_class == GetClassRoot<mirror::Reference>());
-  // Second static field
-  ArtField* field = reference_class->GetStaticField(1);
+  // Don't use WellKnownClasses here as it may not be initialized at the point
+  // we're being called.
+  ArtField* field = reference_class->GetField(reference_class->NumFields() - 1);
+  DCHECK(field->IsStatic());
   DCHECK_STREQ(field->GetName(), "slowPathEnabled");
   return field->GetOffset();
 }
diff --git a/runtime/gc/space/bump_pointer_space.cc b/runtime/gc/space/bump_pointer_space.cc
index ad9c68da3e..75ca459843 100644
--- a/runtime/gc/space/bump_pointer_space.cc
+++ b/runtime/gc/space/bump_pointer_space.cc
@@ -80,6 +80,9 @@ void BumpPointerSpace::Clear() {
     memset(Begin(), 0, Limit() - Begin());
   }
   CHECK_NE(madvise(Begin(), Limit() - Begin(), MADV_DONTNEED), -1) << "madvise failed";
+  if (GetMarkBitmap() != nullptr) {
+    GetMarkBitmap()->Clear();
+  }
   // Reset the end of the space back to the beginning, we move the end forward as we allocate
   // objects.
   SetEnd(Begin());
@@ -90,6 +93,7 @@ void BumpPointerSpace::Clear() {
     growth_end_ = Limit();
     block_sizes_.clear();
     main_block_size_ = 0;
+    black_dense_region_size_ = 0;
   }
 }
 
diff --git a/runtime/gc/space/image_space.cc b/runtime/gc/space/image_space.cc
index 18f4c4205a..f557f3c8a9 100644
--- a/runtime/gc/space/image_space.cc
+++ b/runtime/gc/space/image_space.cc
@@ -21,10 +21,12 @@
 #include <unistd.h>
 
 #include <array>
+#include <cstddef>
 #include <memory>
 #include <optional>
 #include <random>
 #include <string>
+#include <string_view>
 #include <vector>
 
 #include "android-base/logging.h"
@@ -65,7 +67,6 @@
 #include "oat/oat_file.h"
 #include "profile/profile_compilation_info.h"
 #include "runtime.h"
-#include "runtime_globals.h"
 #include "space-inl.h"
 
 namespace art HIDDEN {
@@ -565,11 +566,6 @@ class ImageSpace::Loader {
         REQUIRES(!Locks::mutator_lock_) {
     TimingLogger logger(__PRETTY_FUNCTION__, /*precise=*/ true, VLOG_IS_ON(image));
 
-    if (gPageSize != kMinPageSize) {
-      *error_msg = "Loading app image is only supported on devices with 4K page size";
-      return nullptr;
-    }
-
     std::unique_ptr<ImageSpace> space = Init(image_filename,
                                              image_location,
                                              &logger,
@@ -670,33 +666,39 @@ class ImageSpace::Loader {
     CHECK(image_filename != nullptr);
     CHECK(image_location != nullptr);
 
-    std::unique_ptr<File> file;
+    FileWithRange file_with_range;
     {
       TimingLogger::ScopedTiming timing("OpenImageFile", logger);
-      file.reset(OS::OpenFileForReading(image_filename));
-      if (file == nullptr) {
-        *error_msg = StringPrintf("Failed to open '%s'", image_filename);
+      // Most likely, the image is compressed and doesn't really need alignment. We enforce page
+      // size alignment just in case the image is uncompressed.
+      file_with_range = OS::OpenFileDirectlyOrFromZip(
+          image_filename, OatFile::kZipSeparator, /*alignment=*/MemMap::GetPageSize(), error_msg);
+      if (file_with_range.file == nullptr) {
         return nullptr;
       }
     }
-    return Init(file.get(),
+    return Init(file_with_range.file.get(),
+                file_with_range.start,
+                file_with_range.length,
                 image_filename,
                 image_location,
-                /*profile_files=*/ {},
-                /*allow_direct_mapping=*/ true,
+                /*profile_files=*/{},
+                /*allow_direct_mapping=*/true,
                 logger,
                 image_reservation,
                 error_msg);
   }
 
   static std::unique_ptr<ImageSpace> Init(File* file,
+                                          off_t start,
+                                          size_t image_file_size,
                                           const char* image_filename,
                                           const char* image_location,
                                           const std::vector<std::string>& profile_files,
                                           bool allow_direct_mapping,
                                           TimingLogger* logger,
-                                          /*inout*/MemMap* image_reservation,
-                                          /*out*/std::string* error_msg) {
+                                          /*inout*/ MemMap* image_reservation,
+                                          /*out*/ std::string* error_msg) {
     CHECK(image_filename != nullptr);
     CHECK(image_location != nullptr);
 
@@ -705,19 +707,17 @@ class ImageSpace::Loader {
     ImageHeader image_header;
     {
       TimingLogger::ScopedTiming timing("ReadImageHeader", logger);
-      bool success = file->PreadFully(&image_header, sizeof(image_header), /*offset=*/ 0u);
+      bool success = file->PreadFully(&image_header, sizeof(image_header), start);
       if (!success || !image_header.IsValid()) {
         *error_msg = StringPrintf("Invalid image header in '%s'", image_filename);
         return nullptr;
       }
     }
     // Check that the file is larger or equal to the header size + data size.
-    const uint64_t image_file_size = static_cast<uint64_t>(file->GetLength());
     if (image_file_size < sizeof(ImageHeader) + image_header.GetDataSize()) {
-      *error_msg = StringPrintf(
-          "Image file truncated: %" PRIu64 " vs. %" PRIu64 ".",
-           image_file_size,
-           static_cast<uint64_t>(sizeof(ImageHeader) + image_header.GetDataSize()));
+      *error_msg = StringPrintf("Image file truncated: %zu vs. %" PRIu64 ".",
+                                image_file_size,
+                                sizeof(ImageHeader) + image_header.GetDataSize());
       return nullptr;
     }
 
@@ -739,10 +739,9 @@ class ImageSpace::Loader {
         RoundUp(sizeof(ImageHeader) + image_header.GetDataSize(), kElfSegmentAlignment);
     const size_t end_of_bitmap = image_bitmap_offset + bitmap_section.Size();
     if (end_of_bitmap != image_file_size) {
-      *error_msg = StringPrintf(
-          "Image file size does not equal end of bitmap: size=%" PRIu64 " vs. %zu.",
-          image_file_size,
-          end_of_bitmap);
+      *error_msg = StringPrintf("Image file size does not equal end of bitmap: size=%zu vs. %zu.",
+                                image_file_size,
+                                end_of_bitmap);
       return nullptr;
     }
 
@@ -752,15 +751,15 @@ class ImageSpace::Loader {
     // avoid reading proc maps for a mapping failure and slowing everything down.
     // For the boot image, we have already reserved the memory and we load the image
     // into the `image_reservation`.
-    MemMap map = LoadImageFile(
-        image_filename,
-        image_location,
-        image_header,
-        file->Fd(),
-        allow_direct_mapping,
-        logger,
-        image_reservation,
-        error_msg);
+    MemMap map = LoadImageFile(image_filename,
+                               image_location,
+                               image_header,
+                               file->Fd(),
+                               start,
+                               allow_direct_mapping,
+                               logger,
+                               image_reservation,
+                               error_msg);
     if (!map.IsValid()) {
       DCHECK(!error_msg->empty());
       return nullptr;
@@ -771,8 +770,8 @@ class ImageSpace::Loader {
                                               PROT_READ,
                                               MAP_PRIVATE,
                                               file->Fd(),
-                                              image_bitmap_offset,
-                                              /*low_4gb=*/ false,
+                                              start + image_bitmap_offset,
+                                              /*low_4gb=*/false,
                                               image_filename,
                                               error_msg);
     if (!image_bitmap_map.IsValid()) {
@@ -992,10 +991,11 @@ class ImageSpace::Loader {
                               const char* image_location,
                               const ImageHeader& image_header,
                               int fd,
+                              off_t start,
                               bool allow_direct_mapping,
                               TimingLogger* logger,
-                              /*inout*/MemMap* image_reservation,
-                              /*out*/std::string* error_msg) {
+                              /*inout*/ MemMap* image_reservation,
+                              /*out*/ std::string* error_msg) {
     TimingLogger::ScopedTiming timing("MapImageFile", logger);
 
     // The runtime might not be available at this point if we're running dex2oat or oatdump, in
@@ -1014,7 +1014,7 @@ class ImageSpace::Loader {
           PROT_READ | PROT_WRITE,
           MAP_PRIVATE,
           fd,
-          /*start=*/0,
+          start,
           /*low_4gb=*/true,
           image_filename,
           /*reuse=*/false,
@@ -1043,8 +1043,8 @@ class ImageSpace::Loader {
                                         PROT_READ,
                                         MAP_PRIVATE,
                                         fd,
-                                        /*start=*/ 0,
-                                        /*low_4gb=*/ false,
+                                        start,
+                                        /*low_4gb=*/false,
                                         image_filename,
                                         error_msg);
       if (!temp_map.IsValid()) {
@@ -1060,7 +1060,7 @@ class ImageSpace::Loader {
 
         Runtime::ScopedThreadPoolUsage stpu;
         ThreadPool* const pool = stpu.GetThreadPool();
-        const uint64_t start = NanoTime();
+        const uint64_t start_time = NanoTime();
         Thread* const self = Thread::Current();
         static constexpr size_t kMinBlocks = 2u;
         const bool use_parallel = pool != nullptr && image_header.GetBlockCount() >= kMinBlocks;
@@ -1091,7 +1091,7 @@ class ImageSpace::Loader {
           ScopedTrace trace("Waiting for workers");
           pool->Wait(self, true, false);
         }
-        const uint64_t time = NanoTime() - start;
+        const uint64_t time = NanoTime() - start_time;
         // Add one 1 ns to prevent possible divide by 0.
         VLOG(image) << "Decompressing image took " << PrettyDuration(time) << " ("
                     << PrettySize(static_cast<uint64_t>(map.Size()) * MsToNs(1000) / (time + 1))
@@ -1968,9 +1968,9 @@ bool ImageSpace::BootImageLayout::CompileBootclasspathElements(
   std::string art_filename = ExpandLocation(base_filename, bcp_index);
   std::string vdex_filename = ImageHeader::GetVdexLocationFromImageLocation(art_filename);
   std::string oat_filename = ImageHeader::GetOatLocationFromImageLocation(art_filename);
-  android::base::unique_fd art_fd(memfd_create_compat(art_filename.c_str(), /*flags=*/ 0));
-  android::base::unique_fd vdex_fd(memfd_create_compat(vdex_filename.c_str(), /*flags=*/ 0));
-  android::base::unique_fd oat_fd(memfd_create_compat(oat_filename.c_str(), /*flags=*/ 0));
+  android::base::unique_fd art_fd(memfd_create(art_filename.c_str(), /*flags=*/ 0));
+  android::base::unique_fd vdex_fd(memfd_create(vdex_filename.c_str(), /*flags=*/ 0));
+  android::base::unique_fd oat_fd(memfd_create(oat_filename.c_str(), /*flags=*/ 0));
   if (art_fd.get() == -1 || vdex_fd.get() == -1 || oat_fd.get() == -1) {
     *error_msg = StringPrintf("Failed to create memfd handles for compiling bootclasspath for %s",
                               boot_class_path_locations_[bcp_index].c_str());
@@ -2827,12 +2827,20 @@ class ImageSpace::BootImageLoader {
       VLOG(startup) << "Using image file " << image_filename.c_str() << " for image location "
                     << image_location << " for compiled extension";
 
-      File image_file(art_fd.release(), image_filename, /*check_usage=*/ false);
+      File image_file(art_fd.release(), image_filename, /*check_usage=*/false);
+      int64_t file_length = image_file.GetLength();
+      if (file_length < 0) {
+        *error_msg =
+            ART_FORMAT("Failed to get file length of '{}': {}", image_filename, strerror(errno));
+        return nullptr;
+      }
       std::unique_ptr<ImageSpace> result = Loader::Init(&image_file,
+                                                        /*start=*/0,
+                                                        file_length,
                                                         image_filename.c_str(),
                                                         image_location.c_str(),
                                                         profile_files,
-                                                        /*allow_direct_mapping=*/ false,
+                                                        /*allow_direct_mapping=*/false,
                                                         logger,
                                                         image_reservation,
                                                         error_msg);
@@ -2878,14 +2886,12 @@ class ImageSpace::BootImageLoader {
       TimingLogger::ScopedTiming timing("OpenOatFile", logger);
       std::string oat_filename =
           ImageHeader::GetOatLocationFromImageLocation(space->GetImageFilename());
-      std::string oat_location =
-          ImageHeader::GetOatLocationFromImageLocation(space->GetImageLocation());
 
       DCHECK_EQ(vdex_fd.get() != -1, oat_fd.get() != -1);
       if (vdex_fd.get() == -1) {
         oat_file.reset(OatFile::Open(/*zip_fd=*/-1,
                                      oat_filename,
-                                     oat_location,
+                                     oat_filename,
                                      executable_,
                                      /*low_4gb=*/false,
                                      dex_filenames,
@@ -2896,7 +2902,7 @@ class ImageSpace::BootImageLoader {
         oat_file.reset(OatFile::Open(/*zip_fd=*/-1,
                                      vdex_fd.get(),
                                      oat_fd.get(),
-                                     oat_location,
+                                     oat_filename,
                                      executable_,
                                      /*low_4gb=*/false,
                                      dex_filenames,
@@ -3249,11 +3255,6 @@ bool ImageSpace::BootImageLoader::LoadFromSystem(
     /*out*/std::string* error_msg) {
   TimingLogger logger(__PRETTY_FUNCTION__, /*precise=*/ true, VLOG_IS_ON(image));
 
-  if (gPageSize != kMinPageSize) {
-    *error_msg = "Loading boot image is only supported on devices with 4K page size";
-    return false;
-  }
-
   BootImageLayout layout(image_locations_,
                          boot_class_path_,
                          boot_class_path_locations_,
@@ -3415,31 +3416,39 @@ void ImageSpace::Dump(std::ostream& os) const {
       << ",name=\"" << GetName() << "\"]";
 }
 
-bool ImageSpace::ValidateApexVersions(const OatHeader& oat_header,
-                                      const std::string& apex_versions,
-                                      const std::string& file_location,
+bool ImageSpace::ValidateApexVersions(const OatFile& oat_file,
+                                      std::string_view runtime_apex_versions,
                                       std::string* error_msg) {
   // For a boot image, the key value store only exists in the first OAT file. Skip other OAT files.
-  if (oat_header.GetKeyValueStoreSize() == 0) {
+  if (oat_file.GetOatHeader().GetKeyValueStoreSize() == 0) {
     return true;
   }
 
-  const char* oat_apex_versions = oat_header.GetStoreValueByKey(OatHeader::kApexVersionsKey);
-  if (oat_apex_versions == nullptr) {
+  std::optional<std::string_view> oat_apex_versions = oat_file.GetApexVersions();
+  if (!oat_apex_versions.has_value()) {
     *error_msg = StringPrintf("ValidateApexVersions failed to get APEX versions from oat file '%s'",
-                              file_location.c_str());
+                              oat_file.GetLocation().c_str());
     return false;
   }
+
+  return ValidateApexVersions(
+      *oat_apex_versions, runtime_apex_versions, oat_file.GetLocation(), error_msg);
+}
+
+bool ImageSpace::ValidateApexVersions(std::string_view oat_apex_versions,
+                                      std::string_view runtime_apex_versions,
+                                      const std::string& file_location,
+                                      std::string* error_msg) {
   // For a boot image, it can be generated from a subset of the bootclasspath.
   // For an app image, some dex files get compiled with a subset of the bootclasspath.
   // For such cases, the OAT APEX versions will be a prefix of the runtime APEX versions.
-  if (!apex_versions.starts_with(oat_apex_versions)) {
-    *error_msg = StringPrintf(
-        "ValidateApexVersions found APEX versions mismatch between oat file '%s' and the runtime "
-        "(Oat file: '%s', Runtime: '%s')",
-        file_location.c_str(),
+  if (!runtime_apex_versions.starts_with(oat_apex_versions)) {
+    *error_msg = ART_FORMAT(
+        "ValidateApexVersions found APEX versions mismatch between oat file '{}' and the runtime "
+        "(Oat file: '{}', Runtime: '{}')",
+        file_location,
         oat_apex_versions,
-        apex_versions.c_str());
+        runtime_apex_versions);
     return false;
   }
   return true;
@@ -3455,10 +3464,7 @@ bool ImageSpace::ValidateOatFile(const OatFile& oat_file,
                                  ArrayRef<const std::string> dex_filenames,
                                  ArrayRef<File> dex_files,
                                  const std::string& apex_versions) {
-  if (!ValidateApexVersions(oat_file.GetOatHeader(),
-                            apex_versions,
-                            oat_file.GetLocation(),
-                            error_msg)) {
+  if (!ValidateApexVersions(oat_file, apex_versions, error_msg)) {
     return false;
   }
 
diff --git a/runtime/gc/space/image_space.h b/runtime/gc/space/image_space.h
index 266b1d5925..4d0ce8181e 100644
--- a/runtime/gc/space/image_space.h
+++ b/runtime/gc/space/image_space.h
@@ -260,9 +260,13 @@ class ImageSpace : public MemMapSpace {
       const std::string& image_location,
       bool boot_image_extension = false);
 
-  // Returns true if the APEX versions in the OAT header match the given APEX versions.
-  static bool ValidateApexVersions(const OatHeader& oat_header,
-                                   const std::string& apex_versions,
+  // Returns true if the APEX versions of the OAT file match the given APEX versions.
+  static bool ValidateApexVersions(const OatFile& oat_file,
+                                   std::string_view runtime_apex_versions,
+                                   std::string* error_msg);
+
+  static bool ValidateApexVersions(std::string_view oat_apex_versions,
+                                   std::string_view runtime_apex_versions,
                                    const std::string& file_location,
                                    std::string* error_msg);
 
diff --git a/runtime/gc/space/region_space.cc b/runtime/gc/space/region_space.cc
index e891739ec7..2aed181d71 100644
--- a/runtime/gc/space/region_space.cc
+++ b/runtime/gc/space/region_space.cc
@@ -215,7 +215,7 @@ bool RegionSpace::Region::GetUseGenerationalCC() {
   // We are retrieving the info from Heap, instead of the cached version in
   // RegionSpace, because accessing the Heap from a Region object is easier
   // than accessing the RegionSpace.
-  return art::Runtime::Current()->GetHeap()->GetUseGenerationalCC();
+  return art::Runtime::Current()->GetHeap()->GetUseGenerational();
 }
 
 inline bool RegionSpace::Region::ShouldBeEvacuated(EvacMode evac_mode) {
diff --git a/runtime/gc/space/space_create_test.cc b/runtime/gc/space/space_create_test.cc
index 83568351b3..759c5de4ef 100644
--- a/runtime/gc/space/space_create_test.cc
+++ b/runtime/gc/space/space_create_test.cc
@@ -351,10 +351,10 @@ TEST_P(SpaceCreateTest, AllocAndFreeListTestBody) {
 
 INSTANTIATE_TEST_CASE_P(CreateRosAllocSpace,
                         SpaceCreateTest,
-                        testing::Values(kMallocSpaceRosAlloc));
+                        ::testing::Values(kMallocSpaceRosAlloc));
 INSTANTIATE_TEST_CASE_P(CreateDlMallocSpace,
                         SpaceCreateTest,
-                        testing::Values(kMallocSpaceDlMalloc));
+                        ::testing::Values(kMallocSpaceDlMalloc));
 
 }  // namespace space
 }  // namespace gc
diff --git a/runtime/gc_configuration.md b/runtime/gc_configuration.md
new file mode 100644
index 0000000000..f3a5dfea20
--- /dev/null
+++ b/runtime/gc_configuration.md
@@ -0,0 +1,89 @@
+Trading off Garbage Collector Speed vs Memory Use
+-------------------------------------------------
+
+Garbage collection inherently involves a space vs. time trade-off: The less frequently we collect,
+the more memory we will use. This is true both for average heap memory use, and also maximum heap
+memory use, measured just before GC completion. (We're assuming that the application's behavior
+doesn't change depending on GC frequency, which isn't always true, but ideally should be.)
+
+Android provides primarily one knob to control this trade-off: the `HeapTargetUtilization` option.
+This is a fraction between 0 and 1, normally between 0.3 and 0.8 or so. The collector aims to
+ensure that just before a collection completes and memory is reclaimed, the ratio of live data in
+the heap to the allocated size of the heap is `HeapTargetUtilization`.  With a non-concurrent GC,
+we would trigger a GC when the heap size reaches the estimated amount of live data in the heap
+divided by `HeapTargetUtilization`. In the concurrent GC case, this is a bit more complicated, but
+the basic idea remains the same.
+
+Note that *lower* values of `HeapTargetUtilization` correspond to more memory use and less
+frequent GC. A high memory device (relative to memory requirements, which will usually depend on
+screen size, etc.) might use a value of 0.5, where a low memory device might use 0.75.
+
+This scheme is designed to keep GC cost per byte allocated roughly constant, independent of the
+heap size required by the application:
+
+GC cost is usually dominated by the cost of visiting live data in the heap.  Assume that the GC
+cost is *cL*, where *L* is the number of live bytes in the heap, which we assume remains roughly
+fixed, and *c* is the cost of processing a byte of live data. If we abbreviate
+`HeapTargetUtilization` as *u*, we will collect whenever the heap has grown to *L/u* bytes. Thus
+between collections, we can allocate *L/u - L* or *L(1/u - 1)* bytes.  This means the cost per
+byte allocated is *cL / L(1/u - 1)* or just *c / (1/u - 1)*, and thus independent of *L*, the
+amount of live data.
+
+(It's not clear whether we should really try to keep per-byte GC cost constant on a system running
+many ART instances. For a counter-argument, see [Marisa Kirisame et al, "Optimal heap limits for
+reducing browser memory use"](https://dl.acm.org/doi/10.1145/3563323). The current ART approach is
+more traditional among garbage collectors, and reasonably defensible, especially if we only have
+process-local information.)
+
+In fact GCs have some constant cost independent of heap size. With very small heaps, the above
+rule would constantly trigger GCs, and this constant cost would dominate. With a zero-sized heap,
+we would GC on every allocation. This would be slow, so a minimimum number of allocations between
+GCs, a.k.a. `HeapMinFree`, makes sense. Ideally it should be computed based on the
+heap-size-invariant GC cost, which is basically the cost of scanning GC roots, including the cost
+of "suspending" threads to capture references on thread stacks. Currently we approximate that with
+a constant, which is suboptimal. It would be better to have the GC compute this based on actual
+data, like the number of threads, and the current size of the remaining root set.
+
+`HeapMinFree` should really reflect the characteristics of the Java implementation, given data
+about the application known to ART. It is currently configurable. But we would like to move away
+from that, and have ART use more sophisticated heuristics in setting it.
+
+There used to be some argument that once application heaps get too large, we may want to limit
+their heap growth even at the expense of additional GC cost. That could be used to justify
+`HeapMaxFree`, which limits the amount of allocation between collections in applications with a
+very large heap.  Given that in most cases device memory has grown faster than our support for
+huge Java heaps, it is unclear this still has a purpose at all. We recommend it be set to
+something like the maximum heap size, so that it no longer has an effect. We may remove it, or at
+least make it ineffective, in the future. However, we still need to confirm that this is
+acceptable for very low memory devices.
+
+Favoring Latency Sensitive Processes
+------------------------------------
+
+For particularly sensitive processes, such as the foreground application or Android's system
+server, the bytes we allocate between consecutive GCs, as computed via the above controls, are
+multiplied by `ForegroundHeapGrowthMultiplier` + 1. Such applications thus use more memory
+in order to reduce GC time.
+
+When an application moves into the background, we normally trigger a GC to reduce its memory
+footprint before doing so.
+
+Interaction with Compacting Garbage Collectors
+----------------------------------------------
+ART normally uses one of two compacting garbage collectors. These also have compile-time-specified
+heap density targets. These are in effect for collections other than those that prepare the
+application for moving into the background:
+
+'kEvacuateLivePercentThreshold', currently 75% is used for the CC collector,
+`kBlackDenseRegionThreshold`, currently 95%, is used for the CMC collector. Roughly speaking, we
+don't try to compact pages or regions that already exceed these occupancy thresholds, and we
+cannot reallocate the "holes" in uncompacted memory.  This effectively prevents us from reclaiming
+some memory that remains fragmented after a GC.
+
+Some heap areas, e.g. those used for non-movable objects, are also subject to (usually very small
+amounts of) fragmentation, and are treated similarly.
+
+`HeapTargetUtilization` should thus be appreciably less than the relevant density target above, to
+ensure that, even in the presence of fragmentation, we generate enough compacted heap space to not
+instantly trigger another GC.  (In the long run, we will probably compute these density thresholds
+from `HeapTargetUtilization` instead.)
diff --git a/runtime/hidden_api.cc b/runtime/hidden_api.cc
index cb1c60d93f..0474361f1c 100644
--- a/runtime/hidden_api.cc
+++ b/runtime/hidden_api.cc
@@ -46,7 +46,7 @@ static constexpr uint64_t kHideMaxtargetsdkPHiddenApis = 149997251;
 static constexpr uint64_t kHideMaxtargetsdkQHiddenApis = 149994052;
 static constexpr uint64_t kAllowTestApiAccess = 166236554;
 
-static constexpr uint64_t kMaxLogWarnings = 100;
+static constexpr uint64_t kMaxLogAccessesToLogcat = 100;
 
 // Should be the same as dalvik.system.VMRuntime.PREVENT_META_REFLECTION_BLOCKLIST_ACCESS.
 // Corresponds to a bug id.
@@ -68,9 +68,37 @@ static const std::vector<std::string> kWarningExemptions = {
     "Lsun/misc/Unsafe;",
 };
 
+// TODO(b/377676642): Fix API annotations and delete this.
+static const std::vector<std::string> kCorePlatformApiExemptions = {
+    // Intra-core APIs that aren't also core platform APIs. These may be used by
+    // the non-updatable ICU module and hence are effectively de-facto core
+    // platform APIs.
+    "Ldalvik/annotation/compat/VersionCodes;",
+    "Ldalvik/annotation/optimization/ReachabilitySensitive;",
+    "Ldalvik/system/BlockGuard/Policy;->onNetwork",
+    "Ljava/nio/charset/CharsetEncoder;-><init>(Ljava/nio/charset/Charset;FF[BZ)V",
+    "Ljava/security/spec/ECParameterSpec;->getCurveName",
+    "Ljava/security/spec/ECParameterSpec;->setCurveName",
+    "Llibcore/api/CorePlatformApi;",
+    "Llibcore/io/AsynchronousCloseMonitor;",
+    "Llibcore/util/NonNull;",
+    "Llibcore/util/Nullable;",
+    "Lsun/security/util/DerEncoder;",
+    "Lsun/security/x509/AlgorithmId;->derEncode",
+    "Lsun/security/x509/AlgorithmId;->get",
+    // These are new system module APIs that are accessed unflagged (cf.
+    // b/400041178 and b/400041556).
+    "Ldalvik/system/VMDebug;->setCurrentProcessName",
+    "Ldalvik/system/VMDebug;->addApplication",
+    "Ldalvik/system/VMDebug;->removeApplication",
+    "Ldalvik/system/VMDebug;->setUserId",
+    "Ldalvik/system/VMDebug;->setWaitingForDebugger",
+};
+
 static inline std::ostream& operator<<(std::ostream& os, AccessMethod value) {
   switch (value) {
-    case AccessMethod::kNone:
+    case AccessMethod::kCheck:
+    case AccessMethod::kCheckWithPolicy:
       LOG(FATAL) << "Internal access to hidden API should not be logged";
       UNREACHABLE();
     case AccessMethod::kReflection:
@@ -86,6 +114,21 @@ static inline std::ostream& operator<<(std::ostream& os, AccessMethod value) {
   return os;
 }
 
+static inline std::ostream& operator<<(std::ostream& os, Domain domain) {
+  switch (domain) {
+    case Domain::kCorePlatform:
+      os << "core-platform";
+      break;
+    case Domain::kPlatform:
+      os << "platform";
+      break;
+    case Domain::kApplication:
+      os << "app";
+      break;
+  }
+  return os;
+}
+
 static inline std::ostream& operator<<(std::ostream& os, const AccessContext& value)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   if (!value.GetClass().IsNull()) {
@@ -99,14 +142,26 @@ static inline std::ostream& operator<<(std::ostream& os, const AccessContext& va
   return os;
 }
 
+static const char* FormatHiddenApiRuntimeFlags(uint32_t runtime_flags) {
+  switch (runtime_flags & kAccHiddenapiBits) {
+    case 0:
+      return "0";
+    case kAccPublicApi:
+      return "PublicApi";
+    case kAccCorePlatformApi:
+      return "CorePlatformApi";
+    default:
+      return "?";
+  }
+}
+
 static Domain DetermineDomainFromLocation(const std::string& dex_location,
                                           ObjPtr<mirror::ClassLoader> class_loader) {
   // If running with APEX, check `path` against known APEX locations.
   // These checks will be skipped on target buildbots where ANDROID_ART_ROOT
   // is set to "/system".
   if (ArtModuleRootDistinctFromAndroidRoot()) {
-    if (LocationIsOnArtModule(dex_location) || LocationIsOnConscryptModule(dex_location) ||
-        LocationIsOnI18nModule(dex_location)) {
+    if (LocationIsOnArtModule(dex_location) || LocationIsOnConscryptModule(dex_location)) {
       return Domain::kCorePlatform;
     }
 
@@ -126,7 +181,7 @@ static Domain DetermineDomainFromLocation(const std::string& dex_location,
   if (class_loader.IsNull()) {
     if (kIsTargetBuild && !kIsTargetLinux) {
       // This is unexpected only when running on Android.
-      LOG(WARNING) << "DexFile " << dex_location
+      LOG(WARNING) << "hiddenapi: DexFile " << dex_location
                    << " is in boot class path but is not in a known location";
     }
     return Domain::kPlatform;
@@ -140,7 +195,7 @@ void InitializeDexFileDomain(const DexFile& dex_file, ObjPtr<mirror::ClassLoader
 
   // Assign the domain unless a more permissive domain has already been assigned.
   // This may happen when DexFile is initialized as trusted.
-  if (IsDomainMoreTrustedThan(dex_domain, dex_file.GetHiddenapiDomain())) {
+  if (IsDomainAtLeastAsTrustedAs(dex_domain, dex_file.GetHiddenapiDomain())) {
     dex_file.SetHiddenapiDomain(dex_domain);
   }
 }
@@ -324,25 +379,33 @@ void MemberSignature::Dump(std::ostream& os) const {
   }
 }
 
-void MemberSignature::WarnAboutAccess(AccessMethod access_method,
-                                      hiddenapi::ApiList list,
-                                      bool access_denied) {
-  static std::atomic<uint64_t> log_warning_count_ = 0;
-  if (log_warning_count_ > kMaxLogWarnings) {
+void MemberSignature::LogAccessToLogcat(AccessMethod access_method,
+                                        ApiList api_list,
+                                        bool access_denied,
+                                        uint32_t runtime_flags,
+                                        const AccessContext& caller_context,
+                                        const AccessContext& callee_context,
+                                        EnforcementPolicy policy) {
+  static std::atomic<uint64_t> logged_access_count_ = 0;
+  if (logged_access_count_ > kMaxLogAccessesToLogcat) {
     return;
   }
-  LOG(WARNING) << "Accessing hidden " << (type_ == kField ? "field " : "method ")
-               << Dumpable<MemberSignature>(*this) << " (" << list << ", " << access_method
-               << (access_denied ? ", denied)" : ", allowed)");
-  if (access_denied && list.IsTestApi()) {
+  LOG(access_denied ? (policy == EnforcementPolicy::kEnabled ? ERROR : WARNING) : INFO)
+      << "hiddenapi: Accessing hidden " << (type_ == kField ? "field " : "method ")
+      << Dumpable<MemberSignature>(*this)
+      << " (runtime_flags=" << FormatHiddenApiRuntimeFlags(runtime_flags)
+      << ", domain=" << callee_context.GetDomain() << ", api=" << api_list << ") from "
+      << caller_context << " (domain=" << caller_context.GetDomain() << ") using " << access_method
+      << (access_denied ? ": denied" : ": allowed");
+  if (access_denied && api_list.IsTestApi()) {
     // see b/177047045 for more details about test api access getting denied
-    LOG(WARNING) << "If this is a platform test consider enabling "
+    LOG(WARNING) << "hiddenapi: If this is a platform test consider enabling "
                  << "VMRuntime.ALLOW_TEST_API_ACCESS change id for this package.";
   }
-  if (log_warning_count_ >= kMaxLogWarnings) {
-    LOG(WARNING) << "Reached maximum number of hidden api access warnings.";
+  if (logged_access_count_ >= kMaxLogAccessesToLogcat) {
+    LOG(WARNING) << "hiddenapi: Reached maximum number of hidden api access messages.";
   }
-  ++log_warning_count_;
+  ++logged_access_count_;
 }
 
 bool MemberSignature::Equals(const MemberSignature& other) {
@@ -358,11 +421,12 @@ void MemberSignature::LogAccessToEventLog(uint32_t sampled_value,
                                           AccessMethod access_method,
                                           bool access_denied) {
 #ifdef ART_TARGET_ANDROID
-  if (access_method == AccessMethod::kLinking || access_method == AccessMethod::kNone) {
+  if (access_method == AccessMethod::kCheck || access_method == AccessMethod::kCheckWithPolicy ||
+      access_method == AccessMethod::kLinking) {
+    // Checks do not correspond to actual accesses, so should be ignored.
     // Linking warnings come from static analysis/compilation of the bytecode
-    // and can contain false positives (i.e. code that is never run). We choose
-    // not to log these in the event log.
-    // None does not correspond to actual access, so should also be ignored.
+    // and can contain false positives (i.e. code that is never run). Hence we
+    // choose to not log those either in the event log.
     return;
   }
   Runtime* runtime = Runtime::Current();
@@ -380,13 +444,13 @@ void MemberSignature::LogAccessToEventLog(uint32_t sampled_value,
       hs.NewHandle(mirror::String::AllocFromModifiedUtf8(soa.Self(), package_name.c_str()));
   if (soa.Self()->IsExceptionPending()) {
     soa.Self()->ClearException();
-    LOG(ERROR) << "Unable to allocate string for package name which called hidden api";
+    LOG(ERROR) << "hiddenapi: Unable to allocate string for package name which called hidden api";
   }
   Handle<mirror::String> signature_jstr =
       hs.NewHandle(mirror::String::AllocFromModifiedUtf8(soa.Self(), signature_str.str().c_str()));
   if (soa.Self()->IsExceptionPending()) {
     soa.Self()->ClearException();
-    LOG(ERROR) << "Unable to allocate string for hidden api method signature";
+    LOG(ERROR) << "hiddenapi: Unable to allocate string for hidden api method signature";
   }
   WellKnownClasses::dalvik_system_VMRuntime_hiddenApiUsed
       ->InvokeStatic<'V', 'I', 'L', 'L', 'I', 'Z'>(soa.Self(),
@@ -397,7 +461,7 @@ void MemberSignature::LogAccessToEventLog(uint32_t sampled_value,
                                                    access_denied);
   if (soa.Self()->IsExceptionPending()) {
     soa.Self()->ClearException();
-    LOG(ERROR) << "Unable to report hidden api usage";
+    LOG(ERROR) << "hiddenapi: Unable to report hidden api usage";
   }
 #else
   UNUSED(sampled_value);
@@ -495,8 +559,7 @@ uint32_t GetDexFlags(T* member) REQUIRES_SHARED(Locks::mutator_lock_) {
   ObjPtr<mirror::Class> declaring_class = member->GetDeclaringClass();
   DCHECK(!declaring_class.IsNull()) << "Attempting to access a runtime method";
 
-  ApiList flags;
-  DCHECK(!flags.IsValid());
+  ApiList flags = ApiList::Invalid();
 
   // Check if the declaring class has ClassExt allocated. If it does, check if
   // the pre-JVMTI redefine dex file has been set to determine if the declaring
@@ -518,7 +581,7 @@ uint32_t GetDexFlags(T* member) REQUIRES_SHARED(Locks::mutator_lock_) {
       uint32_t member_index = GetMemberDexIndex(member);
       auto fn_visit = [&](const AccessorType& dex_member) {
         if (dex_member.GetIndex() == member_index) {
-          flags = ApiList(dex_member.GetHiddenapiFlags());
+          flags = ApiList::FromDexFlags(dex_member.GetHiddenapiFlags());
         }
       };
       VisitMembers(declaring_class->GetDexFile(), *class_def, fn_visit);
@@ -538,7 +601,7 @@ uint32_t GetDexFlags(T* member) REQUIRES_SHARED(Locks::mutator_lock_) {
       MemberSignature cur_signature(dex_member);
       if (member_signature.MemberNameAndTypeMatch(cur_signature)) {
         DCHECK(member_signature.Equals(cur_signature));
-        flags = ApiList(dex_member.GetHiddenapiFlags());
+        flags = ApiList::FromDexFlags(dex_member.GetHiddenapiFlags());
       }
     };
     VisitMembers(*original_dex, original_class_def, fn_visit);
@@ -551,16 +614,29 @@ uint32_t GetDexFlags(T* member) REQUIRES_SHARED(Locks::mutator_lock_) {
 
 template <typename T>
 bool HandleCorePlatformApiViolation(T* member,
+                                    ApiList api_list,
+                                    uint32_t runtime_flags,
                                     const AccessContext& caller_context,
+                                    const AccessContext& callee_context,
                                     AccessMethod access_method,
                                     EnforcementPolicy policy) {
   DCHECK(policy != EnforcementPolicy::kDisabled)
       << "Should never enter this function when access checks are completely disabled";
 
-  if (access_method != AccessMethod::kNone) {
-    LOG(WARNING) << "Core platform API violation: "
-                 << Dumpable<MemberSignature>(MemberSignature(member)) << " from " << caller_context
-                 << " using " << access_method;
+  if (access_method == AccessMethod::kCheck) {
+    // Always return true for internal checks, so the current enforcement policy
+    // won't affect the caller.
+    return true;
+  }
+
+  if (access_method != AccessMethod::kCheckWithPolicy) {
+    LOG(policy == EnforcementPolicy::kEnabled ? ERROR : WARNING)
+        << "hiddenapi: Core platform API violation: "
+        << Dumpable<MemberSignature>(MemberSignature(member))
+        << " (runtime_flags=" << FormatHiddenApiRuntimeFlags(runtime_flags)
+        << ", domain=" << callee_context.GetDomain() << ", api=" << api_list << ") from "
+        << caller_context << " (domain=" << caller_context.GetDomain() << ")"
+        << " using " << access_method;
 
     // If policy is set to just warn, add kAccCorePlatformApi to access flags of
     // `member` to avoid reporting the violation again next time.
@@ -574,7 +650,13 @@ bool HandleCorePlatformApiViolation(T* member,
 }
 
 template <typename T>
-bool ShouldDenyAccessToMemberImpl(T* member, ApiList api_list, AccessMethod access_method) {
+bool ShouldDenyAccessToMemberImpl(T* member,
+                                  ApiList api_list,
+                                  uint32_t runtime_flags,
+                                  const AccessContext& caller_context,
+                                  const AccessContext& callee_context,
+                                  AccessMethod access_method)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
   DCHECK(member != nullptr);
   Runtime* runtime = Runtime::Current();
   CompatFramework& compatFramework = runtime->GetCompatFramework();
@@ -616,13 +698,19 @@ bool ShouldDenyAccessToMemberImpl(T* member, ApiList api_list, AccessMethod acce
     }
   }
 
-  if (access_method != AccessMethod::kNone) {
+  if (access_method != AccessMethod::kCheck && access_method != AccessMethod::kCheckWithPolicy) {
     // Warn if blocked signature is being accessed or it is not exempted.
     if (deny_access || !member_signature.DoesPrefixMatchAny(kWarningExemptions)) {
       // Print a log message with information about this class member access.
       // We do this if we're about to deny access, or the app is debuggable.
       if (kLogAllAccesses || deny_access || runtime->IsJavaDebuggable()) {
-        member_signature.WarnAboutAccess(access_method, api_list, deny_access);
+        member_signature.LogAccessToLogcat(access_method,
+                                           api_list,
+                                           deny_access,
+                                           runtime_flags,
+                                           caller_context,
+                                           callee_context,
+                                           hiddenApiPolicy);
       }
 
       // If there is a StrictMode listener, notify it about this violation.
@@ -657,19 +745,32 @@ bool ShouldDenyAccessToMemberImpl(T* member, ApiList api_list, AccessMethod acce
 template uint32_t GetDexFlags<ArtField>(ArtField* member);
 template uint32_t GetDexFlags<ArtMethod>(ArtMethod* member);
 template bool HandleCorePlatformApiViolation(ArtField* member,
+                                             ApiList api_list,
+                                             uint32_t runtime_flags,
                                              const AccessContext& caller_context,
+                                             const AccessContext& callee_context,
                                              AccessMethod access_method,
                                              EnforcementPolicy policy);
 template bool HandleCorePlatformApiViolation(ArtMethod* member,
+                                             ApiList api_list,
+                                             uint32_t runtime_flags,
                                              const AccessContext& caller_context,
+                                             const AccessContext& callee_context,
                                              AccessMethod access_method,
                                              EnforcementPolicy policy);
 template bool ShouldDenyAccessToMemberImpl<ArtField>(ArtField* member,
                                                      ApiList api_list,
+                                                     uint32_t runtime_flags,
+                                                     const AccessContext& caller_context,
+                                                     const AccessContext& callee_context,
                                                      AccessMethod access_method);
 template bool ShouldDenyAccessToMemberImpl<ArtMethod>(ArtMethod* member,
                                                       ApiList api_list,
+                                                      uint32_t runtime_flags,
+                                                      const AccessContext& caller_context,
+                                                      const AccessContext& callee_context,
                                                       AccessMethod access_method);
+
 }  // namespace detail
 
 template <typename T>
@@ -738,11 +839,12 @@ bool ShouldDenyAccessToMember(T* member,
       // Decode hidden API access flags from the dex file.
       // This is an O(N) operation scaling with the number of fields/methods
       // in the class. Only do this on slow path and only do it once.
-      ApiList api_list(detail::GetDexFlags(member));
+      ApiList api_list = ApiList::FromDexFlags(detail::GetDexFlags(member));
       DCHECK(api_list.IsValid());
 
       // Member is hidden and caller is not exempted. Enter slow path.
-      return detail::ShouldDenyAccessToMemberImpl(member, api_list, access_method);
+      return detail::ShouldDenyAccessToMemberImpl(
+          member, api_list, runtime_flags, caller_context, callee_context, access_method);
     }
 
     case Domain::kPlatform: {
@@ -762,10 +864,36 @@ bool ShouldDenyAccessToMember(T* member,
       // If this is a proxy method, look at the interface method instead.
       member = detail::GetInterfaceMemberIfProxy(member);
 
+      // Decode hidden API access flags from the dex file. This is a slow path,
+      // like in the kApplication case above.
+      ApiList api_list = ApiList::FromDexFlags(detail::GetDexFlags(member));
+      DCHECK(api_list.IsValid());
+
+      // Max target SDK versions don't matter for platform callers, but they may
+      // still depend on unsupported APIs. Let's compare against the "max" SDK
+      // version to only allow that (and also proper SDK APIs, but they are
+      // typically combined with kCorePlatformApi already).
+      if (api_list.GetMaxAllowedSdkVersion() == SdkVersion::kMax) {
+        // Allow access and attempt to update the access flags to avoid
+        // re-examining the dex flags next time.
+        detail::MaybeUpdateAccessFlags(Runtime::Current(), member, kAccCorePlatformApi);
+        return false;
+      }
+
+      // Check for exemptions.
+      // TODO(b/377676642): Fix API annotations and delete this.
+      detail::MemberSignature member_signature(member);
+      if (member_signature.DoesPrefixMatchAny(kCorePlatformApiExemptions)) {
+        // Avoid re-examining the exemption list next time.
+        detail::MaybeUpdateAccessFlags(Runtime::Current(), member, kAccCorePlatformApi);
+        return false;
+      }
+
       // Access checks are not disabled, report the violation.
       // This may also add kAccCorePlatformApi to the access flags of `member`
       // so as to not warn again on next access.
-      return detail::HandleCorePlatformApiViolation(member, caller_context, access_method, policy);
+      return detail::HandleCorePlatformApiViolation(
+          member, api_list, runtime_flags, caller_context, callee_context, access_method, policy);
     }
 
     case Domain::kCorePlatform: {
diff --git a/runtime/hidden_api.h b/runtime/hidden_api.h
index 21cd04e2dc..f14883b03c 100644
--- a/runtime/hidden_api.h
+++ b/runtime/hidden_api.h
@@ -51,9 +51,20 @@ inline EnforcementPolicy EnforcementPolicyFromInt(int api_policy_int) {
 }
 
 // Hidden API access method
-// Thist must be kept in sync with VMRuntime.HiddenApiUsageLogger.ACCESS_METHOD_*
+// This must be kept in sync with VMRuntime.HiddenApiUsageLogger.ACCESS_METHOD_*
+// for the access methods that are logged.
 enum class AccessMethod {
-  kNone = 0,  // internal test that does not correspond to an actual access by app
+  // An internal check that does not correspond to an actual access by an app.
+  // It's not logged and the current EnforcementPolicy is not applied. The check
+  // can also be one that, if denied, will be followed by another check with one
+  // of the other methods below (except kCheckWithPolicy), which will then log
+  // and apply the policy (if that one is denied too).
+  kCheck = 0,
+
+  // Like kCheck, except the current EnforcementPolicy is applied (but it still
+  // doesn't log).
+  kCheckWithPolicy = 4,
+
   kReflection = 1,
   kJNI = 2,
   kLinking = 3,
@@ -95,7 +106,7 @@ class AccessContext {
 
   // Returns true if this domain is always allowed to access the domain of `callee`.
   bool CanAlwaysAccess(const AccessContext& callee) const {
-    return IsDomainMoreTrustedThan(domain_, callee.domain_);
+    return IsDomainAtLeastAsTrustedAs(domain_, callee.domain_);
   }
 
  private:
@@ -204,7 +215,13 @@ class MemberSignature {
 
   bool DoesPrefixMatchAny(const std::vector<std::string>& exemptions);
 
-  void WarnAboutAccess(AccessMethod access_method, ApiList list, bool access_denied);
+  void LogAccessToLogcat(AccessMethod access_method,
+                         ApiList list,
+                         bool access_denied,
+                         uint32_t runtime_flags,
+                         const AccessContext& caller_context,
+                         const AccessContext& callee_context,
+                         EnforcementPolicy policy) REQUIRES_SHARED(Locks::mutator_lock_);
 
   void LogAccessToEventLog(uint32_t sampled_value, AccessMethod access_method, bool access_denied);
 
@@ -222,16 +239,22 @@ uint32_t GetDexFlags(T* member) REQUIRES_SHARED(Locks::mutator_lock_);
 
 // Handler of detected core platform API violations. Returns true if access to
 // `member` should be denied.
-template<typename T>
+template <typename T>
 bool HandleCorePlatformApiViolation(T* member,
+                                    ApiList api_list,
+                                    uint32_t runtime_flags,
                                     const AccessContext& caller_context,
+                                    const AccessContext& callee_context,
                                     AccessMethod access_method,
-                                    EnforcementPolicy policy)
-    REQUIRES_SHARED(Locks::mutator_lock_);
+                                    EnforcementPolicy policy) REQUIRES_SHARED(Locks::mutator_lock_);
 
-template<typename T>
-bool ShouldDenyAccessToMemberImpl(T* member, ApiList api_list, AccessMethod access_method)
-    REQUIRES_SHARED(Locks::mutator_lock_);
+template <typename T>
+bool ShouldDenyAccessToMemberImpl(T* member,
+                                  ApiList api_list,
+                                  uint32_t runtime_flags,
+                                  const AccessContext& caller_context,
+                                  const AccessContext& callee_context,
+                                  AccessMethod access_method) REQUIRES_SHARED(Locks::mutator_lock_);
 
 inline ArtField* GetInterfaceMemberIfProxy(ArtField* field) { return field; }
 
@@ -244,7 +267,7 @@ inline ArtMethod* GetInterfaceMemberIfProxy(ArtMethod* method)
 ALWAYS_INLINE inline uint32_t CreateRuntimeFlags_Impl(uint32_t dex_flags) {
   uint32_t runtime_flags = 0u;
 
-  ApiList api_list(dex_flags);
+  ApiList api_list = ApiList::FromDexFlags(dex_flags);
   DCHECK(api_list.IsValid());
 
   if (api_list.Contains(ApiList::Sdk())) {
@@ -319,9 +342,9 @@ ALWAYS_INLINE inline uint32_t GetRuntimeFlags(ArtMethod* method)
       case Intrinsics::kUnsafeGetVolatile:
       case Intrinsics::kUnsafePutLongOrdered:
       case Intrinsics::kUnsafePutLongVolatile:
-      case Intrinsics::kUnsafePutObjectOrdered:
       case Intrinsics::kUnsafePutObjectVolatile:
-      case Intrinsics::kUnsafePutOrdered:
+      case Intrinsics::kUnsafePutOrderedInt:
+      case Intrinsics::kUnsafePutOrderedObject:
       case Intrinsics::kUnsafePutVolatile:
       case Intrinsics::kUnsafeLoadFence:
       case Intrinsics::kUnsafeStoreFence:
@@ -347,10 +370,10 @@ ALWAYS_INLINE inline uint32_t GetRuntimeFlags(ArtMethod* method)
       case Intrinsics::kJdkUnsafePutLongOrdered:
       case Intrinsics::kJdkUnsafePutLongVolatile:
       case Intrinsics::kJdkUnsafePutLongRelease:
-      case Intrinsics::kJdkUnsafePutObjectOrdered:
+      case Intrinsics::kJdkUnsafePutOrderedInt:
+      case Intrinsics::kJdkUnsafePutOrderedObject:
       case Intrinsics::kJdkUnsafePutReferenceVolatile:
       case Intrinsics::kJdkUnsafePutReferenceRelease:
-      case Intrinsics::kJdkUnsafePutOrdered:
       case Intrinsics::kJdkUnsafePutVolatile:
       case Intrinsics::kJdkUnsafePutRelease:
       case Intrinsics::kJdkUnsafeLoadFence:
diff --git a/runtime/hidden_api_test.cc b/runtime/hidden_api_test.cc
index b16c42f1a7..39cca7cce6 100644
--- a/runtime/hidden_api_test.cc
+++ b/runtime/hidden_api_test.cc
@@ -188,12 +188,23 @@ class HiddenApiTest : public CommonRuntimeTest {
   }
 
   bool ShouldDenyAccess(hiddenapi::ApiList list) REQUIRES_SHARED(Locks::mutator_lock_) {
-    // Choose parameters such that there are no side effects (AccessMethod::kNone)
+    // This is only used for log messages, so its state doesn't matter.
+    const hiddenapi::AccessContext placeholder_context(/* is_trusted= */ false);
+
+    // Choose parameters such that there are no side effects (AccessMethod::kCheck)
     // and that the member is not on the exemptions list (here we choose one which
     // is not even in boot class path).
     return ShouldDenyAccessToMemberImpl(/* member= */ class1_field1_,
                                         list,
-                                        /* access_method= */ hiddenapi::AccessMethod::kNone);
+                                        /* runtime_flags= */ 0,
+                                        /* caller_context= */ placeholder_context,
+                                        /* callee_context= */ placeholder_context,
+                                        hiddenapi::AccessMethod::kCheck);
+  }
+
+  bool ShouldDenyAccess(hiddenapi::ApiList list1, hiddenapi::ApiList list2)
+      REQUIRES_SHARED(Locks::mutator_lock_) {
+    return ShouldDenyAccess(hiddenapi::ApiList::Combine(list1, list2));
   }
 
   void TestLocation(const std::string& location, hiddenapi::Domain expected_domain) {
@@ -376,59 +387,63 @@ TEST_F(HiddenApiTest, CheckTestApiEnforcement) {
   runtime_->SetTargetSdkVersion(
       static_cast<uint32_t>(hiddenapi::ApiList::MaxTargetR().GetMaxAllowedSdkVersion()) + 1);
 
+  // clang-format off
+
   // Default case where all TestApis are treated like non-TestApi.
   runtime_->SetTestApiEnforcementPolicy(hiddenapi::EnforcementPolicy::kEnabled);
   SetChangeIdState(kAllowTestApiAccess, false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::Sdk()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::Sdk()), false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::Unsupported()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::Unsupported()), false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::MaxTargetR()), true);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::MaxTargetR()), true);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::MaxTargetQ()), true);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::MaxTargetQ()), true);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::MaxTargetP()), true);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::MaxTargetP()), true);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::MaxTargetO()), true);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::MaxTargetO()), true);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::Blocked()), true);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::Blocked()), true);
 
   // A case where we want to allow access to TestApis.
   runtime_->SetTestApiEnforcementPolicy(hiddenapi::EnforcementPolicy::kDisabled);
   SetChangeIdState(kAllowTestApiAccess, false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::Sdk()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::Sdk()), false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::Unsupported()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::Unsupported()), false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::MaxTargetR()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::MaxTargetR()), false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::MaxTargetQ()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::MaxTargetQ()), false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::MaxTargetP()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::MaxTargetP()), false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::MaxTargetO()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::MaxTargetO()), false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::Blocked()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::Blocked()), false);
 
   // A second case where we want to allow access to TestApis.
   runtime_->SetTestApiEnforcementPolicy(hiddenapi::EnforcementPolicy::kEnabled);
   SetChangeIdState(kAllowTestApiAccess, true);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::Sdk()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::Sdk()), false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::Unsupported()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::Unsupported()), false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::MaxTargetR()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::MaxTargetR()), false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::MaxTargetQ()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::MaxTargetQ()), false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::MaxTargetP()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::MaxTargetP()), false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::MaxTargetO()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::MaxTargetO()), false);
   ASSERT_EQ(
-      ShouldDenyAccess(hiddenapi::ApiList::TestApi() | hiddenapi::ApiList::Blocked()), false);
+      ShouldDenyAccess(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::Blocked()), false);
+
+  // clang-format on
 }
 
 TEST_F(HiddenApiTest, CheckMembersRead) {
@@ -651,14 +666,15 @@ TEST_F(HiddenApiTest, CheckMemberSignatureForProxyClass) {
 
   // Find the "interfaces" static field. This is generated for all proxies.
   ArtField* field = nullptr;
-  for (size_t i = 0; i < proxyClass->NumStaticFields(); ++i) {
-    ArtField* f = proxyClass->GetStaticField(i);
+  for (size_t i = 0; i < proxyClass->NumFields(); ++i) {
+    ArtField* f = proxyClass->GetField(i);
     if (strcmp("interfaces", f->GetName()) == 0) {
       field = f;
       break;
     }
   }
   ASSERT_TRUE(field != nullptr);
+  ASSERT_TRUE(field->IsStatic());
 
   // Test the signature. We expect the signature from the interface class.
   std::ostringstream ss_method;
diff --git a/runtime/hprof/hprof.cc b/runtime/hprof/hprof.cc
index f77370d3d4..885112c342 100644
--- a/runtime/hprof/hprof.cc
+++ b/runtime/hprof/hprof.cc
@@ -1191,7 +1191,7 @@ void Hprof::DumpHeapClass(mirror::Class* klass) {
   //       For other overhead (currently only the embedded vtable), we will generate a synthetic
   //       byte array (or field[s] in case the overhead size is of reference size or less).
 
-  const size_t num_static_fields = klass->NumStaticFields();
+  const size_t num_static_fields = klass->ComputeNumStaticFields();
 
   // Total class size:
   //   * class instance fields (including Object instance fields)
@@ -1212,10 +1212,12 @@ void Hprof::DumpHeapClass(mirror::Class* klass) {
   // Tools (ahat/Studio) will count the static fields and account for them in the class size. We
   // must thus subtract them from base_overhead_size or they will be double-counted.
   size_t class_static_fields_size = 0;
-  for (ArtField& class_static_field : klass->GetSFields()) {
-    size_t size = 0;
-    SignatureToBasicTypeAndSize(class_static_field.GetTypeDescriptor(), &size);
-    class_static_fields_size += size;
+  for (ArtField& class_field : klass->GetFields()) {
+    if (class_field.IsStatic()) {
+      size_t size = 0;
+      SignatureToBasicTypeAndSize(class_field.GetTypeDescriptor(), &size);
+      class_static_fields_size += size;
+    }
   }
 
   CHECK_GE(base_overhead_size, class_static_fields_size);
@@ -1284,8 +1286,8 @@ void Hprof::DumpHeapClass(mirror::Class* klass) {
   mirror::Class* class_class = klass->GetClass();
 
   DCHECK(class_class->GetSuperClass()->IsObjectClass());
-  const size_t static_fields_reported = class_class->NumInstanceFields()
-                                        + class_class->GetSuperClass()->NumInstanceFields()
+  const size_t static_fields_reported = class_class->ComputeNumInstanceFields()
+                                        + class_class->GetSuperClass()->ComputeNumInstanceFields()
                                         + java_heap_overhead_field_count
                                         + num_static_fields;
   __ AddU2(dchecked_integral_cast<uint16_t>(static_fields_reported));
@@ -1373,11 +1375,15 @@ void Hprof::DumpHeapClass(mirror::Class* klass) {
     auto class_instance_field_name_fn = [](ArtField& field) REQUIRES_SHARED(Locks::mutator_lock_) {
       return std::string("$class$") + field.GetName();
     };
-    for (ArtField& class_instance_field : class_class->GetIFields()) {
-      static_field_writer(class_instance_field, class_instance_field_name_fn);
+    for (ArtField& class_field : class_class->GetFields()) {
+      if (!class_field.IsStatic()) {
+        static_field_writer(class_field, class_instance_field_name_fn);
+      }
     }
-    for (ArtField& object_instance_field : class_class->GetSuperClass()->GetIFields()) {
-      static_field_writer(object_instance_field, class_instance_field_name_fn);
+    for (ArtField& object_field : class_class->GetSuperClass()->GetFields()) {
+      if (!object_field.IsStatic()) {
+        static_field_writer(object_field, class_instance_field_name_fn);
+      }
     }
   }
 
@@ -1385,13 +1391,15 @@ void Hprof::DumpHeapClass(mirror::Class* klass) {
     auto class_static_field_name_fn = [](ArtField& field) REQUIRES_SHARED(Locks::mutator_lock_) {
       return field.GetName();
     };
-    for (ArtField& class_static_field : klass->GetSFields()) {
-      static_field_writer(class_static_field, class_static_field_name_fn);
+    for (ArtField& class_field : klass->GetFields()) {
+      if (class_field.IsStatic()) {
+        static_field_writer(class_field, class_static_field_name_fn);
+      }
     }
   }
 
   // Instance fields for this class (no superclass fields)
-  int iFieldCount = klass->NumInstanceFields();
+  int iFieldCount = klass->ComputeNumInstanceFields();
   // add_internal_runtime_objects is only for classes that may retain objects live through means
   // other than fields. It is never the case for strings.
   const bool add_internal_runtime_objects = AddRuntimeInternalObjectsField(klass);
@@ -1400,8 +1408,11 @@ void Hprof::DumpHeapClass(mirror::Class* klass) {
   } else {
     __ AddU2((uint16_t)iFieldCount);
   }
-  for (int i = 0; i < iFieldCount; ++i) {
-    ArtField* f = klass->GetInstanceField(i);
+  for (uint32_t i = 0; i < klass->NumFields(); ++i) {
+    ArtField* f = klass->GetField(i);
+    if (f->IsStatic()) {
+      continue;
+    }
     __ AddStringId(LookupStringId(f->GetName()));
     HprofBasicType t = SignatureToBasicTypeAndSize(f->GetTypeDescriptor(), nullptr);
     __ AddU1(t);
@@ -1487,9 +1498,11 @@ void Hprof::DumpHeapInstanceObject(mirror::Object* obj,
 
   // Write the instance data;  fields for this class, followed by super class fields, and so on.
   do {
-    const size_t instance_fields = klass->NumInstanceFields();
-    for (size_t i = 0; i < instance_fields; ++i) {
-      ArtField* f = klass->GetInstanceField(i);
+    for (size_t i = 0; i < klass->NumFields(); ++i) {
+      ArtField* f = klass->GetField(i);
+      if (f->IsStatic()) {
+        continue;
+      }
       size_t size;
       HprofBasicType t = SignatureToBasicTypeAndSize(f->GetTypeDescriptor(), &size);
       switch (t) {
diff --git a/runtime/imtable-inl.h b/runtime/imtable-inl.h
index 9be56cb8f8..8af1d89f60 100644
--- a/runtime/imtable-inl.h
+++ b/runtime/imtable-inl.h
@@ -33,65 +33,52 @@ static constexpr uint32_t kImTableHashCoefficientClass = 427;
 static constexpr uint32_t kImTableHashCoefficientName = 16;
 static constexpr uint32_t kImTableHashCoefficientSignature = 14;
 
-inline void ImTable::GetImtHashComponents(ArtMethod* method,
+inline void ImTable::GetImtHashComponents(const DexFile& dex_file,
+                                          uint32_t dex_method_index,
                                           uint32_t* class_hash,
                                           uint32_t* name_hash,
                                           uint32_t* signature_hash) {
   if (kImTableHashUseName) {
-    if (method->IsProxyMethod()) {
-      *class_hash = 0;
-      *name_hash = 0;
-      *signature_hash = 0;
-      return;
-    }
-
-    const DexFile* dex_file = method->GetDexFile();
-    const dex::MethodId& method_id = dex_file->GetMethodId(method->GetDexMethodIndex());
+    const dex::MethodId& method_id = dex_file.GetMethodId(dex_method_index);
 
     // Class descriptor for the class component.
-    *class_hash = ComputeModifiedUtf8Hash(dex_file->GetMethodDeclaringClassDescriptor(method_id));
+    *class_hash = ComputeModifiedUtf8Hash(dex_file.GetMethodDeclaringClassDescriptor(method_id));
 
     // Method name for the method component.
-    *name_hash = ComputeModifiedUtf8Hash(dex_file->GetMethodName(method_id));
+    *name_hash = ComputeModifiedUtf8Hash(dex_file.GetMethodName(method_id));
 
-    const dex::ProtoId& proto_id = dex_file->GetMethodPrototype(method_id);
+    const dex::ProtoId& proto_id = dex_file.GetMethodPrototype(method_id);
 
     // Read the proto for the signature component.
     uint32_t tmp = ComputeModifiedUtf8Hash(
-        dex_file->GetTypeDescriptor(dex_file->GetTypeId(proto_id.return_type_idx_)));
+        dex_file.GetTypeDescriptor(dex_file.GetTypeId(proto_id.return_type_idx_)));
 
     // Mix in the argument types.
     // Note: we could consider just using the shorty. This would be faster, at the price of
     //       potential collisions.
-    const dex::TypeList* param_types = dex_file->GetProtoParameters(proto_id);
+    const dex::TypeList* param_types = dex_file.GetProtoParameters(proto_id);
     if (param_types != nullptr) {
       for (size_t i = 0; i != param_types->Size(); ++i) {
         const dex::TypeItem& type = param_types->GetTypeItem(i);
         tmp = 31 * tmp + ComputeModifiedUtf8Hash(
-            dex_file->GetTypeDescriptor(dex_file->GetTypeId(type.type_idx_)));
+            dex_file.GetTypeDescriptor(dex_file.GetTypeId(type.type_idx_)));
       }
     }
 
     *signature_hash = tmp;
     return;
   } else {
-    *class_hash = method->GetDexMethodIndex();
+    *class_hash = dex_method_index;
     *name_hash = 0;
     *signature_hash = 0;
     return;
   }
 }
 
-inline uint32_t ImTable::GetImtIndex(ArtMethod* method) {
-  DCHECK(!method->IsCopied());
-  if (!method->IsAbstract()) {
-    // For default methods, where we cannot store the imt_index, we use the
-    // method_index instead. We mask it with the closest power of two to
-    // simplify the interpreter.
-    return method->GetMethodIndex() & (ImTable::kSizeTruncToPowerOfTwo - 1);
-  }
+inline uint32_t ImTable::GetImtIndexForAbstractMethod(const DexFile& dex_file,
+                                                      uint32_t dex_method_index) {
   uint32_t class_hash, name_hash, signature_hash;
-  GetImtHashComponents(method, &class_hash, &name_hash, &signature_hash);
+  GetImtHashComponents(dex_file, dex_method_index, &class_hash, &name_hash, &signature_hash);
 
   uint32_t mixed_hash;
   if (!kImTableHashUseCoefficients) {
@@ -105,6 +92,18 @@ inline uint32_t ImTable::GetImtIndex(ArtMethod* method) {
   return mixed_hash % ImTable::kSize;
 }
 
+inline uint32_t ImTable::GetImtIndex(ArtMethod* method) {
+  DCHECK(!method->IsCopied());
+  DCHECK(!method->IsProxyMethod());
+  if (!method->IsAbstract()) {
+    // For default methods, where we cannot store the imt_index, we use the
+    // method_index instead. We mask it with the closest power of two to
+    // simplify the interpreter.
+    return method->GetMethodIndex() & (ImTable::kSizeTruncToPowerOfTwo - 1);
+  }
+  return GetImtIndexForAbstractMethod(*method->GetDexFile(), method->GetDexMethodIndex());
+}
+
 }  // namespace art
 
 #endif  // ART_RUNTIME_IMTABLE_INL_H_
diff --git a/runtime/imtable.h b/runtime/imtable.h
index 0d604ca244..f7e9066c78 100644
--- a/runtime/imtable.h
+++ b/runtime/imtable.h
@@ -81,11 +81,14 @@ class ImTable {
   }
 
   // Converts a method to the base hash components used in GetImtIndex.
-  ALWAYS_INLINE static inline void GetImtHashComponents(ArtMethod* method,
+  ALWAYS_INLINE static inline void GetImtHashComponents(const DexFile& dex_file,
+                                                        uint32_t dex_method_index,
                                                         uint32_t* class_hash,
                                                         uint32_t* name_hash,
-                                                        uint32_t* signature_hash)
-      REQUIRES_SHARED(Locks::mutator_lock_);
+                                                        uint32_t* signature_hash);
+
+  ALWAYS_INLINE static inline uint32_t GetImtIndexForAbstractMethod(const DexFile& dex_file,
+                                                                    uint32_t dex_method_index);
 
   // The (complete) hashing scheme to map an ArtMethod to a slot in the Interface Method Table
   // (IMT).
diff --git a/runtime/instrumentation-inl.h b/runtime/instrumentation-inl.h
new file mode 100644
index 0000000000..3b035ea71b
--- /dev/null
+++ b/runtime/instrumentation-inl.h
@@ -0,0 +1,129 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_RUNTIME_INSTRUMENTATION_INL_H_
+#define ART_RUNTIME_INSTRUMENTATION_INL_H_
+
+#include "instrumentation.h"
+
+#include "art_method-inl.h"
+#include "entrypoints/runtime_asm_entrypoints.h"
+#include "gc/heap.h"
+#include "jit/jit.h"
+#include "runtime.h"
+
+namespace art HIDDEN {
+namespace instrumentation {
+
+inline bool Instrumentation::CanUseAotCode(const void* quick_code) {
+  if (quick_code == nullptr) {
+    return false;
+  }
+  Runtime* runtime = Runtime::Current();
+  // For simplicity, we never use AOT code for debuggable.
+  if (runtime->IsJavaDebuggable()) {
+    return false;
+  }
+
+  if (runtime->IsNativeDebuggable()) {
+    DCHECK(runtime->UseJitCompilation() && runtime->GetJit()->JitAtFirstUse());
+    // If we are doing native debugging, ignore application's AOT code,
+    // since we want to JIT it (at first use) with extra stackmaps for native
+    // debugging. We keep however all AOT code from the boot image,
+    // since the JIT-at-first-use is blocking and would result in non-negligible
+    // startup performance impact.
+    return runtime->GetHeap()->IsInBootImageOatFile(quick_code);
+  }
+
+  return true;
+}
+
+inline const void* Instrumentation::GetInitialEntrypoint(uint32_t method_access_flags,
+                                                         const void* aot_code) {
+  if (!ArtMethod::IsInvokable(method_access_flags)) {
+    return GetQuickToInterpreterBridge();
+  }
+
+  // Special case if we need an initialization check.
+  if (ArtMethod::NeedsClinitCheckBeforeCall(method_access_flags)) {
+    // If we have code but the method needs a class initialization check before calling that code,
+    // install the resolution stub that will perform the check. It will be replaced by the proper
+    // entry point by `ClassLinker::FixupStaticTrampolines()` after initializing class.
+    // Note: This mimics the logic in image_writer.cc that installs the resolution stub only
+    // if we have compiled code or we can execute nterp, and the method needs a class
+    // initialization check.
+    return (aot_code != nullptr || ArtMethod::IsNative(method_access_flags))
+        ? GetQuickResolutionStub()
+        : GetQuickToInterpreterBridge();
+  }
+
+  // Use the provided AOT code if possible.
+  if (CanUseAotCode(aot_code)) {
+    return aot_code;
+  }
+
+  // Use default entrypoints.
+  return ArtMethod::IsNative(method_access_flags) ? GetQuickGenericJniStub()
+                                                  : GetQuickToInterpreterBridge();
+}
+
+
+inline bool Instrumentation::InitialEntrypointNeedsInstrumentationStubs() {
+  return IsForcedInterpretOnly() || EntryExitStubsInstalled();
+}
+
+inline void Instrumentation::InitializeMethodsCode(ArtMethod* method,
+                                                   const void* entrypoint,
+                                                   PointerSize pointer_size) {
+  if (kIsDebugBuild) {
+    // Entrypoint should be uninitialized.
+    CHECK(method->GetEntryPointFromQuickCompiledCodePtrSize(pointer_size) == nullptr)
+        << method->PrettyMethod();
+    // We initialize the entrypoint while loading the class, well before the class
+    // is verified and Nterp entrypoint is allowed. We prefer to check for resolved
+    // because a verified class may lose its "verified" status (by becoming erroneous)
+    // but the resolved status is always kept (as "resolved erroneous" if needed).
+    CHECK(!method->GetDeclaringClass()->IsResolved());
+    CHECK_NE(entrypoint, interpreter::GetNterpEntryPoint()) << method->PrettyMethod();
+    if (InitialEntrypointNeedsInstrumentationStubs()) {
+      const void* expected =
+          method->IsNative() ? GetQuickGenericJniStub() : GetQuickToInterpreterBridge();
+      CHECK_EQ(entrypoint, expected) << method->PrettyMethod() << " " << method->IsNative();
+    } else if (method->NeedsClinitCheckBeforeCall()) {
+      if (method->IsNative()) {
+        CHECK_EQ(entrypoint, GetQuickResolutionStub());
+      } else {
+        // We do not have the original `aot_code` to determine which entrypoint to expect.
+        CHECK(entrypoint == GetQuickResolutionStub() ||
+              entrypoint == GetQuickToInterpreterBridge());
+      }
+    } else {
+      bool is_stub = (entrypoint == GetQuickToInterpreterBridge()) ||
+                     (entrypoint == GetQuickGenericJniStub()) ||
+                     (entrypoint == GetQuickResolutionStub());
+      const void* aot_code = is_stub ? nullptr : entrypoint;
+      const void* initial = GetInitialEntrypoint(method->GetAccessFlags(), aot_code);
+      CHECK_EQ(initial, entrypoint)
+          << method->PrettyMethod() << " 0x" << std::hex << method->GetAccessFlags();
+    }
+  }
+  method->SetEntryPointFromQuickCompiledCodePtrSize(entrypoint, pointer_size);
+}
+
+}  // namespace instrumentation
+}  // namespace art
+
+#endif  // ART_RUNTIME_INSTRUMENTATION_INL_H_
diff --git a/runtime/instrumentation.cc b/runtime/instrumentation.cc
index ce740da1a8..28c4f35db3 100644
--- a/runtime/instrumentation.cc
+++ b/runtime/instrumentation.cc
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-#include "instrumentation.h"
+#include "instrumentation-inl.h"
 
 #include <functional>
 #include <optional>
@@ -315,37 +315,13 @@ bool Instrumentation::InterpretOnly(ArtMethod* method) REQUIRES_SHARED(Locks::mu
   return InterpretOnly() || IsDeoptimized(method);
 }
 
-static bool CanUseAotCode(const void* quick_code)
-    REQUIRES_SHARED(Locks::mutator_lock_) {
-  if (quick_code == nullptr) {
-    return false;
-  }
-  Runtime* runtime = Runtime::Current();
-  // For simplicity, we never use AOT code for debuggable.
-  if (runtime->IsJavaDebuggable()) {
-    return false;
-  }
-
-  if (runtime->IsNativeDebuggable()) {
-    DCHECK(runtime->UseJitCompilation() && runtime->GetJit()->JitAtFirstUse());
-    // If we are doing native debugging, ignore application's AOT code,
-    // since we want to JIT it (at first use) with extra stackmaps for native
-    // debugging. We keep however all AOT code from the boot image,
-    // since the JIT-at-first-use is blocking and would result in non-negligible
-    // startup performance impact.
-    return runtime->GetHeap()->IsInBootImageOatFile(quick_code);
-  }
-
-  return true;
-}
-
 static bool CanUseNterp(ArtMethod* method) REQUIRES_SHARED(Locks::mutator_lock_) {
   return interpreter::CanRuntimeUseNterp() &&
       CanMethodUseNterp(method) &&
       method->IsDeclaringClassVerifiedMayBeDead();
 }
 
-static const void* GetOptimizedCodeFor(ArtMethod* method) REQUIRES_SHARED(Locks::mutator_lock_) {
+const void* Instrumentation::GetOptimizedCodeFor(ArtMethod* method) {
   DCHECK(!Runtime::Current()->GetInstrumentation()->InterpretOnly(method));
   CHECK(method->IsInvokable()) << method->PrettyMethod();
   if (method->IsProxyMethod()) {
@@ -378,8 +354,7 @@ static const void* GetOptimizedCodeFor(ArtMethod* method) REQUIRES_SHARED(Locks:
   return method->IsNative() ? GetQuickGenericJniStub() : GetQuickToInterpreterBridge();
 }
 
-void Instrumentation::InitializeMethodsCode(ArtMethod* method, const void* aot_code)
-    REQUIRES_SHARED(Locks::mutator_lock_) {
+void Instrumentation::ReinitializeMethodsCode(ArtMethod* method) {
   if (!method->IsInvokable()) {
     DCHECK(method->GetEntryPointFromQuickCompiledCode() == nullptr ||
            Runtime::Current()->GetClassLinker()->IsQuickToInterpreterBridge(
@@ -405,7 +380,7 @@ void Instrumentation::InitializeMethodsCode(ArtMethod* method, const void* aot_c
     // Note: this mimics the logic in image_writer.cc that installs the resolution
     // stub only if we have compiled code or we can execute nterp, and the method needs a class
     // initialization check.
-    if (aot_code != nullptr || method->IsNative() || CanUseNterp(method)) {
+    if (method->IsNative() || CanUseNterp(method)) {
       if (kIsDebugBuild && CanUseNterp(method)) {
         // Adds some test coverage for the nterp clinit entrypoint.
         UpdateEntryPoints(method, interpreter::GetNterpWithClinitEntryPoint());
@@ -418,12 +393,6 @@ void Instrumentation::InitializeMethodsCode(ArtMethod* method, const void* aot_c
     return;
   }
 
-  // Use the provided AOT code if possible.
-  if (CanUseAotCode(aot_code)) {
-    UpdateEntryPoints(method, aot_code);
-    return;
-  }
-
   // We check if the class is verified as we need the slow interpreter for lock verification.
   // If the class is not verified, This will be updated in
   // ClassLinker::UpdateClassAfterVerification.
diff --git a/runtime/instrumentation.h b/runtime/instrumentation.h
index 59e6b29b90..5112f0e789 100644
--- a/runtime/instrumentation.h
+++ b/runtime/instrumentation.h
@@ -322,8 +322,22 @@ class Instrumentation {
   // Returns a string representation of the given entry point.
   static std::string EntryPointString(const void* code);
 
-  // Initialize the entrypoint of the method .`aot_code` is the AOT code.
-  EXPORT void InitializeMethodsCode(ArtMethod* method, const void* aot_code)
+  // Return the best initial entrypoint of a method, assuming that stubs are not in use.
+  // This function can be called while the thread is suspended.
+  const void* GetInitialEntrypoint(uint32_t method_access_flags, const void* aot_code);
+
+  // Check if the best initial entrypoint needs to be overridden with stubs.
+  bool InitialEntrypointNeedsInstrumentationStubs()
+      REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(Roles::uninterruptible_);
+
+  // Initialize the method's entrypoint with aot code or runtime stub.
+  // The caller must check and apply `InitialEntrypointNeedsInstrumentationStubs()`
+  // in the same `Roles::uninterruptible_` section of code.
+  void InitializeMethodsCode(ArtMethod* method, const void* entrypoint, PointerSize pointer_size)
+      REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(Roles::uninterruptible_);
+
+  // Reinitialize the entrypoint of the method.
+  EXPORT void ReinitializeMethodsCode(ArtMethod* method)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Update the code of a method respecting any installed stubs.
@@ -601,6 +615,9 @@ class Instrumentation {
       REQUIRES_SHARED(Locks::mutator_lock_);
 
  private:
+  static bool CanUseAotCode(const void* quick_code);
+  static const void* GetOptimizedCodeFor(ArtMethod* method) REQUIRES_SHARED(Locks::mutator_lock_);
+
   // Update the current instrumentation_level_.
   void UpdateInstrumentationLevel(InstrumentationLevel level);
 
diff --git a/runtime/interpreter/interpreter_cache.cc b/runtime/interpreter/interpreter_cache.cc
index a272d14bba..6b335e9cf1 100644
--- a/runtime/interpreter/interpreter_cache.cc
+++ b/runtime/interpreter/interpreter_cache.cc
@@ -21,7 +21,8 @@ namespace art HIDDEN {
 
 void InterpreterCache::Clear(Thread* owning_thread) {
   DCHECK(owning_thread->GetInterpreterCache() == this);
-  DCHECK(owning_thread == Thread::Current() || owning_thread->IsSuspended());
+  DCHECK(owning_thread == Thread::Current() || owning_thread->IsSuspended() ||
+         owning_thread->ReadFlag(ThreadFlag::kRunningFlipFunction, std::memory_order_relaxed));
   // Avoid using std::fill (or its variant) as there could be a concurrent sweep
   // happening by the GC thread and these functions may clear partially.
   for (Entry& entry : data_) {
diff --git a/runtime/interpreter/interpreter_common.cc b/runtime/interpreter/interpreter_common.cc
index 5024b16ba2..5deb0e1881 100644
--- a/runtime/interpreter/interpreter_common.cc
+++ b/runtime/interpreter/interpreter_common.cc
@@ -1419,14 +1419,14 @@ bool DoFilledNewArray(const Instruction* inst,
   ObjPtr<mirror::Class> component_class = array_class->GetComponentType();
   const bool is_primitive_int_component = component_class->IsPrimitiveInt();
   if (UNLIKELY(component_class->IsPrimitive() && !is_primitive_int_component)) {
-    if (component_class->IsPrimitiveLong() || component_class->IsPrimitiveDouble()) {
-      ThrowRuntimeException("Bad filled array request for type %s",
-                            component_class->PrettyDescriptor().c_str());
-    } else {
-      self->ThrowNewExceptionF("Ljava/lang/InternalError;",
-                               "Found type %s; filled-new-array not implemented for anything but 'int'",
-                               component_class->PrettyDescriptor().c_str());
-    }
+    // Verifier rejects `filled-new-array/-range` with descriptors `[J` and `[D`.
+    // These are forbidden, see https://source.android.com/docs/core/runtime/dalvik-bytecode .
+    DCHECK(!component_class->IsPrimitiveLong());
+    DCHECK(!component_class->IsPrimitiveDouble());
+    self->ThrowNewExceptionF(
+        "Ljava/lang/InternalError;",
+        "Found type %s; filled-new-array not implemented for anything but 'int'",
+        component_class->PrettyDescriptor().c_str());
     return false;
   }
   ObjPtr<mirror::Object> new_array = mirror::Array::Alloc(
diff --git a/runtime/interpreter/mterp/nterp.cc b/runtime/interpreter/mterp/nterp.cc
index b929444fc6..95cfe7fb7d 100644
--- a/runtime/interpreter/mterp/nterp.cc
+++ b/runtime/interpreter/mterp/nterp.cc
@@ -35,6 +35,12 @@ namespace art HIDDEN {
 namespace interpreter {
 
 bool IsNterpSupported() {
+#ifdef ART_USE_RESTRICTED_MODE
+  // TODO(Simulator): Support Nterp.
+  // Nterp uses the native stack and quick stack frame layout; this will be a complication
+  // for the simulator mode. We should use switch interpreter only for now.
+  return false;
+#else
   switch (kRuntimeQuickCodeISA) {
     case InstructionSet::kArm:
     case InstructionSet::kThumb2:
@@ -48,6 +54,7 @@ bool IsNterpSupported() {
     default:
       return false;
   }
+#endif  // #ifdef ART_USE_RESTRICTED_MODE
 }
 
 bool CanRuntimeUseNterp() REQUIRES_SHARED(Locks::mutator_lock_) {
@@ -384,33 +391,66 @@ extern "C" size_t NterpGetMethod(Thread* self, ArtMethod* caller, const uint16_t
   }
 }
 
+ALWAYS_INLINE FLATTEN
+static ArtField* FindFieldFast(ArtMethod* caller, uint16_t field_index)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  if (caller->IsObsolete()) {
+    return nullptr;
+  }
+
+  ObjPtr<mirror::Class> cls = caller->GetDeclaringClass();
+  const dex::FieldId& field_id = cls->GetDexFile().GetFieldId(field_index);
+  if (cls->GetDexTypeIndex() == field_id.class_idx_) {
+    // Field is in the same class as the caller, no need to do access checks.
+    return cls->FindDeclaredField(field_index);
+  }
+
+  return nullptr;
+}
+
+NO_INLINE
+static ArtField* FindFieldSlow(Thread* self,
+                               ArtMethod* caller,
+                               uint16_t field_index,
+                               bool is_static,
+                               bool is_put)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  return ResolveFieldWithAccessChecks(
+      self,
+      Runtime::Current()->GetClassLinker(),
+      field_index,
+      caller,
+      is_static,
+      /*is_put=*/ is_put,
+      /*resolve_field_type=*/ 0);
+}
+
 LIBART_PROTECTED
 extern "C" size_t NterpGetStaticField(Thread* self,
                                       ArtMethod* caller,
                                       const uint16_t* dex_pc_ptr,
                                       size_t resolve_field_type)  // Resolve if not zero
     REQUIRES_SHARED(Locks::mutator_lock_) {
-  UpdateHotness(caller);
   const Instruction* inst = Instruction::At(dex_pc_ptr);
   uint16_t field_index = inst->VRegB_21c();
-  ClassLinker* const class_linker = Runtime::Current()->GetClassLinker();
   Instruction::Code opcode = inst->Opcode();
-  ArtField* resolved_field = ResolveFieldWithAccessChecks(
-      self,
-      class_linker,
-      field_index,
-      caller,
-      /*is_static=*/ true,
-      /*is_put=*/ IsInstructionSPut(opcode),
-      resolve_field_type);
 
-  if (resolved_field == nullptr) {
-    DCHECK(self->IsExceptionPending());
-    return 0;
+  ArtField* resolved_field = FindFieldFast(caller, field_index);
+  if (resolved_field == nullptr || !resolved_field->IsStatic()) {
+    resolved_field = FindFieldSlow(
+        self, caller, field_index, /*is_static=*/ true, IsInstructionSPut(opcode));
+    if (resolved_field == nullptr) {
+      DCHECK(self->IsExceptionPending());
+      return 0;
+    }
+    // Only update hotness for slow lookups.
+    UpdateHotness(caller);
   }
+
   if (UNLIKELY(!resolved_field->GetDeclaringClass()->IsVisiblyInitialized())) {
     StackHandleScope<1> hs(self);
     Handle<mirror::Class> h_class(hs.NewHandle(resolved_field->GetDeclaringClass()));
+    ClassLinker* const class_linker = Runtime::Current()->GetClassLinker();
     if (UNLIKELY(!class_linker->EnsureInitialized(
                       self, h_class, /*can_init_fields=*/ true, /*can_init_parents=*/ true))) {
       DCHECK(self->IsExceptionPending());
@@ -418,26 +458,34 @@ extern "C" size_t NterpGetStaticField(Thread* self,
     }
     DCHECK(h_class->IsInitializing());
   }
+
+  // For sput-object, try to resolve the field type even if we were not requested to.
+  // Only if the field type is successfully resolved can we update the cache. If we
+  // fail to resolve the type, we clear the exception to keep interpreter
+  // semantics of not throwing when null is stored.
+  bool update_cache = true;
+  if (opcode == Instruction::SPUT_OBJECT &&
+      caller->GetDeclaringClass()->HasTypeChecksFailure() &&
+      resolved_field->ResolveType() == nullptr) {
+    DCHECK(self->IsExceptionPending());
+    if (resolve_field_type) {
+      return 0;
+    }
+    self->ClearException();
+    update_cache = false;
+  }
+
   if (resolved_field->IsVolatile()) {
     // Or the result with 1 to notify to nterp this is a volatile field. We
     // also don't cache the result as we don't want nterp to have its fast path always
     // check for it.
     return reinterpret_cast<size_t>(resolved_field) | 1;
-  } else {
-    // For sput-object, try to resolve the field type even if we were not requested to.
-    // Only if the field type is successfully resolved can we update the cache. If we
-    // fail to resolve the type, we clear the exception to keep interpreter
-    // semantics of not throwing when null is stored.
-    if (opcode == Instruction::SPUT_OBJECT &&
-        resolve_field_type == 0 &&
-        resolved_field->ResolveType() == nullptr) {
-      DCHECK(self->IsExceptionPending());
-      self->ClearException();
-    } else {
-      UpdateCache(self, dex_pc_ptr, resolved_field);
-    }
-    return reinterpret_cast<size_t>(resolved_field);
   }
+
+  if (update_cache) {
+    UpdateCache(self, dex_pc_ptr, resolved_field);
+  }
+  return reinterpret_cast<size_t>(resolved_field);
 }
 
 LIBART_PROTECTED
@@ -446,38 +494,44 @@ extern "C" uint32_t NterpGetInstanceFieldOffset(Thread* self,
                                                 const uint16_t* dex_pc_ptr,
                                                 size_t resolve_field_type)  // Resolve if not zero
     REQUIRES_SHARED(Locks::mutator_lock_) {
-  UpdateHotness(caller);
   const Instruction* inst = Instruction::At(dex_pc_ptr);
   uint16_t field_index = inst->VRegC_22c();
-  ClassLinker* const class_linker = Runtime::Current()->GetClassLinker();
   Instruction::Code opcode = inst->Opcode();
-  ArtField* resolved_field = ResolveFieldWithAccessChecks(
-      self,
-      class_linker,
-      field_index,
-      caller,
-      /*is_static=*/ false,
-      /*is_put=*/ IsInstructionIPut(opcode),
-      resolve_field_type);
-  if (resolved_field == nullptr) {
-    DCHECK(self->IsExceptionPending());
-    return 0;
-  }
-  if (resolved_field->IsVolatile()) {
-    // Don't cache for a volatile field, and return a negative offset as marker
-    // of volatile.
-    return -resolved_field->GetOffset().Uint32Value();
+
+  ArtField* resolved_field = FindFieldFast(caller, field_index);
+  if (resolved_field == nullptr || resolved_field->IsStatic()) {
+    resolved_field = FindFieldSlow(
+        self, caller, field_index, /*is_static=*/ false, IsInstructionIPut(opcode));
+    if (resolved_field == nullptr) {
+      DCHECK(self->IsExceptionPending());
+      return 0;
+    }
+    // Only update hotness for slow lookups.
+    UpdateHotness(caller);
   }
+
   // For iput-object, try to resolve the field type even if we were not requested to.
   // Only if the field type is successfully resolved can we update the cache. If we
   // fail to resolve the type, we clear the exception to keep interpreter
   // semantics of not throwing when null is stored.
+  bool update_cache = true;
   if (opcode == Instruction::IPUT_OBJECT &&
-      resolve_field_type == 0 &&
+      caller->GetDeclaringClass()->HasTypeChecksFailure() &&
       resolved_field->ResolveType() == nullptr) {
     DCHECK(self->IsExceptionPending());
+    if (resolve_field_type != 0u) {
+      return 0;
+    }
     self->ClearException();
-  } else {
+    update_cache = false;
+  }
+
+  if (resolved_field->IsVolatile()) {
+    // Don't cache for a volatile field, and return a negative offset as marker
+    // of volatile.
+    return -resolved_field->GetOffset().Uint32Value();
+  }
+  if (update_cache) {
     UpdateCache(self, dex_pc_ptr, resolved_field->GetOffset().Uint32Value());
   }
   return resolved_field->GetOffset().Uint32Value();
diff --git a/runtime/interpreter/shadow_frame.h b/runtime/interpreter/shadow_frame.h
index 6b3ec915df..0c542f0173 100644
--- a/runtime/interpreter/shadow_frame.h
+++ b/runtime/interpreter/shadow_frame.h
@@ -108,30 +108,9 @@ class ShadowFrame {
     return number_of_vregs_;
   }
 
-  uint32_t GetDexPC() const {
-    return (dex_pc_ptr_ == nullptr) ? dex_pc_ : dex_pc_ptr_ - dex_instructions_;
-  }
-
-  int16_t GetCachedHotnessCountdown() const {
-    return cached_hotness_countdown_;
-  }
-
-  void SetCachedHotnessCountdown(int16_t cached_hotness_countdown) {
-    cached_hotness_countdown_ = cached_hotness_countdown;
-  }
-
-  int16_t GetHotnessCountdown() const {
-    return hotness_countdown_;
-  }
-
-  void SetHotnessCountdown(int16_t hotness_countdown) {
-    hotness_countdown_ = hotness_countdown;
-  }
+  uint32_t GetDexPC() const { return dex_pc_; }
 
-  void SetDexPC(uint32_t dex_pc) {
-    dex_pc_ = dex_pc;
-    dex_pc_ptr_ = nullptr;
-  }
+  void SetDexPC(uint32_t dex_pc) { dex_pc_ = dex_pc; }
 
   ShadowFrame* GetLink() const {
     return link_;
@@ -167,10 +146,6 @@ class ShadowFrame {
     return &vregs_[i + NumberOfVRegs()];
   }
 
-  const uint16_t* GetDexInstructions() const {
-    return dex_instructions_;
-  }
-
   float GetVRegFloat(size_t i) const {
     DCHECK_LT(i, NumberOfVRegs());
     // NOTE: Strict-aliasing?
@@ -304,22 +279,6 @@ class ShadowFrame {
     return OFFSETOF_MEMBER(ShadowFrame, vregs_);
   }
 
-  static constexpr size_t DexPCPtrOffset() {
-    return OFFSETOF_MEMBER(ShadowFrame, dex_pc_ptr_);
-  }
-
-  static constexpr size_t DexInstructionsOffset() {
-    return OFFSETOF_MEMBER(ShadowFrame, dex_instructions_);
-  }
-
-  static constexpr size_t CachedHotnessCountdownOffset() {
-    return OFFSETOF_MEMBER(ShadowFrame, cached_hotness_countdown_);
-  }
-
-  static constexpr size_t HotnessCountdownOffset() {
-    return OFFSETOF_MEMBER(ShadowFrame, hotness_countdown_);
-  }
-
   // Create ShadowFrame for interpreter using provided memory.
   static ShadowFrame* CreateShadowFrameImpl(uint32_t num_vregs,
                                             ArtMethod* method,
@@ -328,14 +287,6 @@ class ShadowFrame {
     return new (memory) ShadowFrame(num_vregs, method, dex_pc);
   }
 
-  const uint16_t* GetDexPCPtr() {
-    return dex_pc_ptr_;
-  }
-
-  void SetDexPCPtr(uint16_t* dex_pc_ptr) {
-    dex_pc_ptr_ = dex_pc_ptr;
-  }
-
   bool NeedsNotifyPop() const {
     return GetFrameFlag(FrameFlags::kNotifyFramePop);
   }
@@ -407,12 +358,8 @@ class ShadowFrame {
   ShadowFrame(uint32_t num_vregs, ArtMethod* method, uint32_t dex_pc)
       : link_(nullptr),
         method_(method),
-        dex_pc_ptr_(nullptr),
-        dex_instructions_(nullptr),
         number_of_vregs_(num_vregs),
         dex_pc_(dex_pc),
-        cached_hotness_countdown_(0),
-        hotness_countdown_(0),
         frame_flags_(0) {
     memset(vregs_, 0, num_vregs * (sizeof(uint32_t) + sizeof(StackReference<mirror::Object>)));
   }
@@ -442,14 +389,9 @@ class ShadowFrame {
   // Link to previous shadow frame or null.
   ShadowFrame* link_;
   ArtMethod* method_;
-  const uint16_t* dex_pc_ptr_;
-  // Dex instruction base of the code item.
-  const uint16_t* dex_instructions_;
   LockCountData lock_count_data_;  // This may contain GC roots when lock counting is active.
   const uint32_t number_of_vregs_;
   uint32_t dex_pc_;
-  int16_t cached_hotness_countdown_;
-  int16_t hotness_countdown_;
 
   // This is a set of ShadowFrame::FrameFlags which denote special states this frame is in.
   // NB alignment requires that this field takes 4 bytes no matter its size. Only 7 bits are
diff --git a/runtime/interpreter/unstarted_runtime.cc b/runtime/interpreter/unstarted_runtime.cc
index 9149f3a6c2..246f81b014 100644
--- a/runtime/interpreter/unstarted_runtime.cc
+++ b/runtime/interpreter/unstarted_runtime.cc
@@ -139,7 +139,7 @@ static void UnstartedRuntimeFindClass(Thread* self,
                                       bool initialize_class)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   CHECK(className != nullptr);
-  std::string descriptor(DotToDescriptor(className->ToModifiedUtf8().c_str()));
+  std::string descriptor = DotToDescriptor(className->ToModifiedUtf8());
   ClassLinker* class_linker = Runtime::Current()->GetClassLinker();
 
   ObjPtr<mirror::Class> found =
@@ -356,20 +356,12 @@ void UnstartedRuntime::UnstartedClassGetDeclaredField(
   ObjPtr<mirror::Class> klass = shadow_frame->GetVRegReference(arg_offset)->AsClass();
   ObjPtr<mirror::String> name2 = shadow_frame->GetVRegReference(arg_offset + 1)->AsString();
   ArtField* found = nullptr;
-  for (ArtField& field : klass->GetIFields()) {
+  for (ArtField& field : klass->GetFields()) {
     if (name2->Equals(field.GetName())) {
       found = &field;
       break;
     }
   }
-  if (found == nullptr) {
-    for (ArtField& field : klass->GetSFields()) {
-      if (name2->Equals(field.GetName())) {
-        found = &field;
-        break;
-      }
-    }
-  }
   if (found != nullptr && ShouldDenyAccessToMember(found, shadow_frame)) {
     found = nullptr;
   }
diff --git a/runtime/intrinsics_list.h b/runtime/intrinsics_list.h
index ccb94645f2..ebd424da52 100644
--- a/runtime/intrinsics_list.h
+++ b/runtime/intrinsics_list.h
@@ -248,10 +248,10 @@
   V(UnsafeGetByte, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Lsun/misc/Unsafe;", "getByte", "(Ljava/lang/Object;J)B") \
   V(UnsafePut, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Lsun/misc/Unsafe;", "putInt", "(Ljava/lang/Object;JI)V") \
   V(UnsafePutAbsolute, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Lsun/misc/Unsafe;", "putInt", "(JI)V") \
-  V(UnsafePutOrdered, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Lsun/misc/Unsafe;", "putOrderedInt", "(Ljava/lang/Object;JI)V") \
+  V(UnsafePutOrderedInt, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Lsun/misc/Unsafe;", "putOrderedInt", "(Ljava/lang/Object;JI)V") \
   V(UnsafePutVolatile, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Lsun/misc/Unsafe;", "putIntVolatile", "(Ljava/lang/Object;JI)V") \
   V(UnsafePutObject, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Lsun/misc/Unsafe;", "putObject", "(Ljava/lang/Object;JLjava/lang/Object;)V") \
-  V(UnsafePutObjectOrdered, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Lsun/misc/Unsafe;", "putOrderedObject", "(Ljava/lang/Object;JLjava/lang/Object;)V") \
+  V(UnsafePutOrderedObject, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Lsun/misc/Unsafe;", "putOrderedObject", "(Ljava/lang/Object;JLjava/lang/Object;)V") \
   V(UnsafePutObjectVolatile, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Lsun/misc/Unsafe;", "putObjectVolatile", "(Ljava/lang/Object;JLjava/lang/Object;)V") \
   V(UnsafePutLong, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Lsun/misc/Unsafe;", "putLong", "(Ljava/lang/Object;JJ)V") \
   V(UnsafePutLongOrdered, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Lsun/misc/Unsafe;", "putOrderedLong", "(Ljava/lang/Object;JJ)V") \
@@ -282,11 +282,11 @@
   V(JdkUnsafeGetByte, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Ljdk/internal/misc/Unsafe;", "getByte", "(Ljava/lang/Object;J)B") \
   V(JdkUnsafePut, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Ljdk/internal/misc/Unsafe;", "putInt", "(Ljava/lang/Object;JI)V") \
   V(JdkUnsafePutAbsolute, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Ljdk/internal/misc/Unsafe;", "putInt", "(JI)V") \
-  V(JdkUnsafePutOrdered, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Ljdk/internal/misc/Unsafe;", "putOrderedInt", "(Ljava/lang/Object;JI)V") \
+  V(JdkUnsafePutOrderedInt, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Ljdk/internal/misc/Unsafe;", "putOrderedInt", "(Ljava/lang/Object;JI)V") \
   V(JdkUnsafePutRelease, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Ljdk/internal/misc/Unsafe;", "putIntRelease", "(Ljava/lang/Object;JI)V") \
   V(JdkUnsafePutVolatile, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Ljdk/internal/misc/Unsafe;", "putIntVolatile", "(Ljava/lang/Object;JI)V") \
   V(JdkUnsafePutReference, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Ljdk/internal/misc/Unsafe;", "putReference", "(Ljava/lang/Object;JLjava/lang/Object;)V") \
-  V(JdkUnsafePutObjectOrdered, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Ljdk/internal/misc/Unsafe;", "putOrderedObject", "(Ljava/lang/Object;JLjava/lang/Object;)V") \
+  V(JdkUnsafePutOrderedObject, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Ljdk/internal/misc/Unsafe;", "putOrderedObject", "(Ljava/lang/Object;JLjava/lang/Object;)V") \
   V(JdkUnsafePutReferenceVolatile, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Ljdk/internal/misc/Unsafe;", "putReferenceVolatile", "(Ljava/lang/Object;JLjava/lang/Object;)V") \
   V(JdkUnsafePutReferenceRelease, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Ljdk/internal/misc/Unsafe;", "putReferenceRelease", "(Ljava/lang/Object;JLjava/lang/Object;)V") \
   V(JdkUnsafePutLong, kVirtual, kNeedsEnvironment, kAllSideEffects, kCanThrow, "Ljdk/internal/misc/Unsafe;", "putLong", "(Ljava/lang/Object;JJ)V") \
diff --git a/runtime/jit/debugger_interface.cc b/runtime/jit/debugger_interface.cc
index d367bbf5cd..674e1e1b23 100644
--- a/runtime/jit/debugger_interface.cc
+++ b/runtime/jit/debugger_interface.cc
@@ -428,11 +428,7 @@ void AddNativeDebugInfoForDex(Thread* self, const DexFile* dexfile) {
   DCHECK(dexfile != nullptr);
   // Container dex files (v41) may store data past the size defined in the header.
   uint32_t size = dexfile->SizeIncludingSharedData();
-  if (dexfile->IsCompactDexFile()) {
-    // Compact dex files may store data past the size defined in the header.
-    const DexFile::Header& header = dexfile->GetHeader();
-    size = std::max(size, header.data_off_ + header.data_size_);
-  }
+  CHECK(!dexfile->IsCompactDexFile());
   const ArrayRef<const uint8_t> symfile(dexfile->Begin(), size);
   CreateJITCodeEntryInternal<DexNativeInfo>(symfile);
 }
diff --git a/runtime/jit/jit_code_cache-inl.h b/runtime/jit/jit_code_cache-inl.h
index fab2073e55..b4553a1ec3 100644
--- a/runtime/jit/jit_code_cache-inl.h
+++ b/runtime/jit/jit_code_cache-inl.h
@@ -38,7 +38,7 @@ EXPORT void JitCodeCache::VisitRootTables(ArtMethod* method, RootVisitorType& vi
 
   Thread* self = Thread::Current();
   ScopedDebugDisallowReadBarriers sddrb(self);
-  MutexLock mu(self, *Locks::jit_lock_);
+  ReaderMutexLock mu(self, *Locks::jit_mutator_lock_);
 
   auto code_ptrs_it = method_code_map_reversed_.find(method);
   if (code_ptrs_it == method_code_map_reversed_.end()) {
diff --git a/runtime/jit/jit_code_cache.cc b/runtime/jit/jit_code_cache.cc
index a8a03d4fdd..022532a54d 100644
--- a/runtime/jit/jit_code_cache.cc
+++ b/runtime/jit/jit_code_cache.cc
@@ -871,7 +871,7 @@ bool JitCodeCache::RemoveMethod(ArtMethod* method, bool release_memory) {
     return false;
   }
 
-  Runtime::Current()->GetInstrumentation()->InitializeMethodsCode(method, /*aot_code=*/ nullptr);
+  Runtime::Current()->GetInstrumentation()->ReinitializeMethodsCode(method);
   return true;
 }
 
@@ -1779,7 +1779,7 @@ void JitCodeCache::InvalidateAllCompiledCode() {
           OatQuickMethodHeader::FromCodePointer(data.GetCode());
       for (ArtMethod* method : data.GetMethods()) {
         if (method->GetEntryPointFromQuickCompiledCode() == method_header->GetEntryPoint()) {
-          instr->InitializeMethodsCode(method, /*aot_code=*/ nullptr);
+          instr->ReinitializeMethodsCode(method);
         }
       }
     }
@@ -1789,7 +1789,7 @@ void JitCodeCache::InvalidateAllCompiledCode() {
       if (UNLIKELY(meth->IsObsolete())) {
         linker->SetEntryPointsForObsoleteMethod(meth);
       } else {
-        instr->InitializeMethodsCode(meth, /*aot_code=*/ nullptr);
+        instr->ReinitializeMethodsCode(meth);
       }
     }
     osr_code_map_.clear();
@@ -1803,7 +1803,7 @@ void JitCodeCache::InvalidateAllCompiledCode() {
     if (entry.method->IsPreCompiled()) {
       entry.method->ClearPreCompiled();
     }
-    instr->InitializeMethodsCode(entry.method, /*aot_code=*/nullptr);
+    instr->ReinitializeMethodsCode(entry.method);
   }
 }
 
@@ -1816,7 +1816,7 @@ void JitCodeCache::InvalidateCompiledCodeFor(ArtMethod* method,
   // the future.
   if (method_entrypoint == header->GetEntryPoint()) {
     // The entrypoint is the one to invalidate, so we just update it to the interpreter entry point.
-    Runtime::Current()->GetInstrumentation()->InitializeMethodsCode(method, /*aot_code=*/ nullptr);
+    Runtime::Current()->GetInstrumentation()->ReinitializeMethodsCode(method);
   } else {
     Thread* self = Thread::Current();
     ScopedDebugDisallowReadBarriers sddrb(self);
diff --git a/runtime/jit/jit_code_cache.h b/runtime/jit/jit_code_cache.h
index 2d2f841360..84b56f1247 100644
--- a/runtime/jit/jit_code_cache.h
+++ b/runtime/jit/jit_code_cache.h
@@ -359,8 +359,10 @@ class JitCodeCache {
 
   // Visit GC roots (except j.l.Class and j.l.String) held by JIT-ed code.
   template<typename RootVisitorType>
-  EXPORT void VisitRootTables(ArtMethod* method,
-                              RootVisitorType& visitor) NO_THREAD_SAFETY_ANALYSIS;
+  EXPORT void VisitRootTables(ArtMethod* method, RootVisitorType& visitor)
+      REQUIRES(Locks::heap_bitmap_lock_)
+      REQUIRES(!Locks::jit_mutator_lock_)
+      REQUIRES_SHARED(Locks::mutator_lock_);
 
   void SweepRootTables(IsMarkedVisitor* visitor)
       REQUIRES(!Locks::jit_lock_)
diff --git a/runtime/jit/jit_memory_region_test.cc b/runtime/jit/jit_memory_region_test.cc
index 449255a7f7..b17e01fff9 100644
--- a/runtime/jit/jit_memory_region_test.cc
+++ b/runtime/jit/jit_memory_region_test.cc
@@ -52,7 +52,7 @@ static void registerSignalHandler() {
   sigaction(SIGSEGV, &sa, nullptr);
 }
 
-class TestZygoteMemory : public testing::Test {
+class TestZygoteMemory : public ::testing::Test {
  public:
   void BasicTest() {
     // Zygote JIT memory only works on kernels that don't segfault on flush.
diff --git a/runtime/jni/jni_id_manager.cc b/runtime/jni/jni_id_manager.cc
index f295d29640..789265a3e5 100644
--- a/runtime/jni/jni_id_manager.cc
+++ b/runtime/jni/jni_id_manager.cc
@@ -460,19 +460,12 @@ void JniIdManager::VisitReflectiveTargets(ReflectiveValueVisitor* rvv) {
               !old_ext_data->HasStaticFieldPointerIdMarker())
             << old_class->PrettyClass();
         // Clear the old field mapping.
-        if (old_field->IsStatic()) {
-          size_t old_off = ArraySlice<ArtField>(old_class->GetSFieldsPtr()).OffsetOf(old_field);
-          ObjPtr<mirror::PointerArray> old_statics(old_ext_data->GetStaticJFieldIDsPointerArray());
-          if (!old_statics.IsNull()) {
-            old_statics->SetElementPtrSize(old_off, 0, kRuntimePointerSize);
-          }
-        } else {
-          size_t old_off = ArraySlice<ArtField>(old_class->GetIFieldsPtr()).OffsetOf(old_field);
-          ObjPtr<mirror::PointerArray> old_instances(
-              old_ext_data->GetInstanceJFieldIDsPointerArray());
-          if (!old_instances.IsNull()) {
-            old_instances->SetElementPtrSize(old_off, 0, kRuntimePointerSize);
-          }
+        size_t old_off = ArraySlice<ArtField>(old_class->GetFieldsPtr()).OffsetOf(old_field);
+        ObjPtr<mirror::PointerArray> array(old_field->IsStatic()
+            ? old_ext_data->GetStaticJFieldIDsPointerArray()
+            : old_ext_data->GetInstanceJFieldIDsPointerArray());
+        if (!array.IsNull()) {
+          array->SetElementPtrSize(old_off, 0, kRuntimePointerSize);
         }
       }
       if (!new_ext_data.IsNull()) {
@@ -480,19 +473,12 @@ void JniIdManager::VisitReflectiveTargets(ReflectiveValueVisitor* rvv) {
               !new_ext_data->HasStaticFieldPointerIdMarker())
             << new_class->PrettyClass();
         // Set the new field mapping.
-        if (new_field->IsStatic()) {
-          size_t new_off = ArraySlice<ArtField>(new_class->GetSFieldsPtr()).OffsetOf(new_field);
-          ObjPtr<mirror::PointerArray> new_statics(new_ext_data->GetStaticJFieldIDsPointerArray());
-          if (!new_statics.IsNull()) {
-            new_statics->SetElementPtrSize(new_off, id, kRuntimePointerSize);
-          }
-        } else {
-          size_t new_off = ArraySlice<ArtField>(new_class->GetIFieldsPtr()).OffsetOf(new_field);
-          ObjPtr<mirror::PointerArray> new_instances(
-              new_ext_data->GetInstanceJFieldIDsPointerArray());
-          if (!new_instances.IsNull()) {
-            new_instances->SetElementPtrSize(new_off, id, kRuntimePointerSize);
-          }
+        size_t new_off = ArraySlice<ArtField>(new_class->GetFieldsPtr()).OffsetOf(new_field);
+        ObjPtr<mirror::PointerArray> array(new_field->IsStatic()
+            ? new_ext_data->GetStaticJFieldIDsPointerArray()
+            : new_ext_data->GetInstanceJFieldIDsPointerArray());
+        if (!array.IsNull()) {
+          array->SetElementPtrSize(new_off, id, kRuntimePointerSize);
         }
       }
     }
diff --git a/runtime/jni/jni_internal.cc b/runtime/jni/jni_internal.cc
index f1ea88da4e..1dde2de741 100644
--- a/runtime/jni/jni_internal.cc
+++ b/runtime/jni/jni_internal.cc
@@ -43,6 +43,7 @@
 #include "handle_scope.h"
 #include "hidden_api.h"
 #include "indirect_reference_table-inl.h"
+#include "instrumentation.h"
 #include "interpreter/interpreter.h"
 #include "java_vm_ext.h"
 #include "jni_env_ext.h"
@@ -494,7 +495,7 @@ ArtMethod* FindMethodJNI(const ScopedObjectAccess& soa,
     method = c->FindClassMethod(name, sig, pointer_size);
   }
   if (method != nullptr &&
-      ShouldDenyAccessToMember(method, soa.Self(), hiddenapi::AccessMethod::kNone)) {
+      ShouldDenyAccessToMember(method, soa.Self(), hiddenapi::AccessMethod::kCheckWithPolicy)) {
     // The resolved method that we have found cannot be accessed due to
     // hiddenapi (typically it is declared up the hierarchy and is not an SDK
     // method). Try to find an interface method from the implemented interfaces which is
@@ -2701,7 +2702,13 @@ class JNI {
         // TODO: make this a hard register error in the future.
       }
 
-      if (is_class_loader_namespace_natively_bridged) {
+      // It is possible to link a class with native methods from a library loaded by
+      // a different classloader. In this case IsClassLoaderNamespaceNativelyBridged
+      // fails detect if native bridge is enabled and may return false.
+      // For this reason we always check method with native bridge (see b/393035780
+      // for details).
+      if (is_class_loader_namespace_natively_bridged ||
+          android::NativeBridgeIsNativeBridgeFunctionPointer(fnPtr)) {
         fnPtr = GenerateNativeBridgeTrampoline(fnPtr, m);
       }
       const void* final_function_ptr = class_linker->RegisterNative(soa.Self(), m, fnPtr);
diff --git a/runtime/method_handles.cc b/runtime/method_handles.cc
index 161fa2cd01..ea7b3d39e1 100644
--- a/runtime/method_handles.cc
+++ b/runtime/method_handles.cc
@@ -100,12 +100,12 @@ bool GetUnboxedTypeAndValue(ObjPtr<mirror::Object> o, Primitive::Type* type, JVa
     REQUIRES_SHARED(Locks::mutator_lock_) {
   ScopedAssertNoThreadSuspension ants(__FUNCTION__);
   ObjPtr<mirror::Class> klass = o->GetClass();
-  ArtField* primitive_field = &klass->GetIFieldsPtr()->At(0);
-#define CASE_PRIMITIVE(primitive, abbrev, _, shorthand)         \
-  if (klass == GetBoxedPrimitiveClass(primitive)) {             \
-    *type = primitive;                                          \
-    value->Set ## shorthand(primitive_field->Get ## abbrev(o)); \
-    return true;                                                \
+#define CASE_PRIMITIVE(primitive, abbrev, java_name, shorthand)                 \
+  if (klass == GetBoxedPrimitiveClass(primitive)) {                             \
+    *type = primitive;                                                          \
+    value->Set ## shorthand(                                                    \
+        WellKnownClasses::java_lang_ ## java_name ## _value->Get ## abbrev(o)); \
+    return true;                                                                \
   }
   PRIMITIVES_LIST(CASE_PRIMITIVE)
 #undef CASE_PRIMITIVE
@@ -782,7 +782,8 @@ static bool DoMethodHandleInvokeMethod(Thread* self,
     if (called_method->IsDefaultConflicting()) {
       ThrowIncompatibleClassChangeErrorForMethodConflict(called_method);
     } else {
-      ThrowAbstractMethodError(called_method);
+      ThrowAbstractMethodError(called_method,
+                               shadow_frame.GetVRegReference(operands->GetOperand(0)));
     }
     return false;
   }
diff --git a/runtime/metrics/reporter_test.cc b/runtime/metrics/reporter_test.cc
index 039e43ca48..b3a0c025ca 100644
--- a/runtime/metrics/reporter_test.cc
+++ b/runtime/metrics/reporter_test.cc
@@ -399,7 +399,7 @@ TEST_F(MetricsReporterTest, CompilerFilter) {
 }
 
 // Test class for period spec parsing
-class ReportingPeriodSpecTest : public testing::Test {
+class ReportingPeriodSpecTest : public ::testing::Test {
  public:
   void VerifyFalse(const std::string& spec_str) {
     Verify(spec_str, false, false, false, {});
diff --git a/runtime/metrics/statsd.cc b/runtime/metrics/statsd.cc
index 605dadbc9b..26f6c20664 100644
--- a/runtime/metrics/statsd.cc
+++ b/runtime/metrics/statsd.cc
@@ -128,25 +128,27 @@ constexpr std::optional<int32_t> EncodeDatumId(DatumId datum_id) {
           statsd::ART_DATUM_DELTA_REPORTED__KIND__ART_DATUM_DELTA_GC_TOTAL_COLLECTION_TIME_MS);
     case DatumId::kYoungGcThroughputAvg:
       return std::make_optional(
-          statsd::ART_DATUM_REPORTED__KIND__ART_DATUM_GC_YOUNG_GENERATION_COLLECTION_THROUGHPUT_AVG_MB_PER_SEC);
+          statsd::
+              ART_DATUM_REPORTED__KIND__ART_DATUM_GC_YOUNG_GENERATION_COLLECTION_THROUGHPUT_AVG_MB_PER_SEC);
     case DatumId::kFullGcThroughputAvg:
       return std::make_optional(
-          statsd::ART_DATUM_REPORTED__KIND__ART_DATUM_GC_FULL_HEAP_COLLECTION_THROUGHPUT_AVG_MB_PER_SEC);
+          statsd::
+              ART_DATUM_REPORTED__KIND__ART_DATUM_GC_FULL_HEAP_COLLECTION_THROUGHPUT_AVG_MB_PER_SEC);
     case DatumId::kYoungGcTracingThroughputAvg:
       return std::make_optional(
-          statsd::ART_DATUM_REPORTED__KIND__ART_DATUM_GC_YOUNG_GENERATION_TRACING_THROUGHPUT_AVG_MB_PER_SEC);
+          statsd::
+              ART_DATUM_REPORTED__KIND__ART_DATUM_GC_YOUNG_GENERATION_TRACING_THROUGHPUT_AVG_MB_PER_SEC);
     case DatumId::kFullGcTracingThroughputAvg:
       return std::make_optional(
-          statsd::ART_DATUM_REPORTED__KIND__ART_DATUM_GC_FULL_HEAP_TRACING_THROUGHPUT_AVG_MB_PER_SEC);
+          statsd::
+              ART_DATUM_REPORTED__KIND__ART_DATUM_GC_FULL_HEAP_TRACING_THROUGHPUT_AVG_MB_PER_SEC);
     case DatumId::kGcWorldStopTime:
-      return std::make_optional(
-          statsd::ART_DATUM_REPORTED__KIND__ART_DATUM_GC_WORLD_STOP_TIME_US);
+      return std::make_optional(statsd::ART_DATUM_REPORTED__KIND__ART_DATUM_GC_WORLD_STOP_TIME_US);
     case DatumId::kGcWorldStopTimeDelta:
       return std::make_optional(
           statsd::ART_DATUM_DELTA_REPORTED__KIND__ART_DATUM_DELTA_GC_WORLD_STOP_TIME_US);
     case DatumId::kGcWorldStopCount:
-      return std::make_optional(
-          statsd::ART_DATUM_REPORTED__KIND__ART_DATUM_GC_WORLD_STOP_COUNT);
+      return std::make_optional(statsd::ART_DATUM_REPORTED__KIND__ART_DATUM_GC_WORLD_STOP_COUNT);
     case DatumId::kGcWorldStopCountDelta:
       return std::make_optional(
           statsd::ART_DATUM_DELTA_REPORTED__KIND__ART_DATUM_DELTA_GC_WORLD_STOP_COUNT);
@@ -195,6 +197,14 @@ constexpr std::optional<int32_t> EncodeDatumId(DatumId datum_id) {
     case DatumId::kTimeElapsedDelta:
       return std::make_optional(
           statsd::ART_DATUM_DELTA_REPORTED__KIND__ART_DATUM_DELTA_TIME_ELAPSED_MS);
+    case DatumId::kAppSlowPathDuringYoungGcDurationDelta:
+      return std::make_optional(
+          statsd::
+              ART_DATUM_DELTA_REPORTED__KIND__ART_DATUM_DELTA_GC_APP_SLOW_PATH_DURING_YOUNG_GENERATION_COLLECTION_DURATION_MILLIS);
+    case DatumId::kAppSlowPathDuringFullGcDurationDelta:
+      return std::make_optional(
+          statsd::
+              ART_DATUM_DELTA_REPORTED__KIND__ART_DATUM_DELTA_GC_APP_SLOW_PATH_DURING_FULL_HEAP_COLLECTION_DURATION_MILLIS);
   }
 }
 
@@ -215,18 +225,18 @@ constexpr int32_t EncodeCompileFilter(CompilerFilterReporting filter) {
     case CompilerFilterReporting::kSpeed:
       return statsd::ART_DATUM_REPORTED__COMPILE_FILTER__ART_COMPILATION_FILTER_SPEED;
     case CompilerFilterReporting::kEverythingProfile:
-      return statsd::
-          ART_DATUM_REPORTED__COMPILE_FILTER__ART_COMPILATION_FILTER_EVERYTHING_PROFILE;
+      return statsd::ART_DATUM_REPORTED__COMPILE_FILTER__ART_COMPILATION_FILTER_EVERYTHING_PROFILE;
     case CompilerFilterReporting::kEverything:
       return statsd::ART_DATUM_REPORTED__COMPILE_FILTER__ART_COMPILATION_FILTER_EVERYTHING;
     case CompilerFilterReporting::kError:
       return statsd::ART_DATUM_REPORTED__COMPILE_FILTER__ART_COMPILATION_FILTER_ERROR;
     case CompilerFilterReporting::kUnknown:
-       return statsd::ART_DATUM_REPORTED__COMPILE_FILTER__ART_COMPILATION_FILTER_UNKNOWN;
+      return statsd::ART_DATUM_REPORTED__COMPILE_FILTER__ART_COMPILATION_FILTER_UNKNOWN;
     case CompilerFilterReporting::kRunFromApk:
-       return statsd::ART_DATUM_REPORTED__COMPILE_FILTER__ART_COMPILATION_FILTER_FAKE_RUN_FROM_APK;
+      return statsd::ART_DATUM_REPORTED__COMPILE_FILTER__ART_COMPILATION_FILTER_FAKE_RUN_FROM_APK;
     case CompilerFilterReporting::kRunFromApkFallback:
-       return statsd::ART_DATUM_REPORTED__COMPILE_FILTER__ART_COMPILATION_FILTER_FAKE_RUN_FROM_APK_FALLBACK;
+      return statsd::
+          ART_DATUM_REPORTED__COMPILE_FILTER__ART_COMPILATION_FILTER_FAKE_RUN_FROM_APK_FALLBACK;
   }
 }
 
diff --git a/runtime/mirror/array-inl.h b/runtime/mirror/array-inl.h
index 1898aca12f..36113d679f 100644
--- a/runtime/mirror/array-inl.h
+++ b/runtime/mirror/array-inl.h
@@ -209,23 +209,21 @@ inline void PrimitiveArray<T>::Memcpy(int32_t dst_pos,
 
   // Note for non-byte copies we can't rely on standard libc functions like memcpy(3) and memmove(3)
   // in our implementation, because they may copy byte-by-byte.
+  static_assert(sizeof(T) == sizeof(uint8_t) || sizeof(T) == sizeof(uint16_t) ||
+                sizeof(T) == sizeof(uint32_t) || sizeof(T) == sizeof(uint64_t));
   void* dst_raw = GetRawData(sizeof(T), dst_pos);
   const void* src_raw = src->GetRawData(sizeof(T), src_pos);
   if (sizeof(T) == sizeof(uint8_t)) {
     memcpy(dst_raw, src_raw, count);
-  } else if (sizeof(T) == sizeof(uint16_t)) {
-    uint16_t* d = reinterpret_cast<uint16_t*>(dst_raw);
-    const uint16_t* s = reinterpret_cast<const uint16_t*>(src_raw);
-    ArrayForwardCopy<uint16_t>(d, s, count);
   } else if (sizeof(T) == sizeof(uint32_t)) {
+    // b/392789466 Avoids copy using float registers on aarch64 for better performance.
     uint32_t* d = reinterpret_cast<uint32_t*>(dst_raw);
     const uint32_t* s = reinterpret_cast<const uint32_t*>(src_raw);
     ArrayForwardCopy<uint32_t>(d, s, count);
   } else {
-    DCHECK_EQ(sizeof(T), sizeof(uint64_t));
-    uint64_t* d = reinterpret_cast<uint64_t*>(dst_raw);
-    const uint64_t* s = reinterpret_cast<const uint64_t*>(src_raw);
-    ArrayForwardCopy<uint64_t>(d, s, count);
+    T* d = reinterpret_cast<T*>(dst_raw);
+    const T* s = reinterpret_cast<const T*>(src_raw);
+    ArrayForwardCopy<T>(d, s, count);
   }
 }
 
diff --git a/runtime/mirror/class-inl.h b/runtime/mirror/class-inl.h
index 3d3c71759b..06b5cf5e9f 100644
--- a/runtime/mirror/class-inl.h
+++ b/runtime/mirror/class-inl.h
@@ -29,6 +29,7 @@
 #include "class_linker.h"
 #include "class_loader.h"
 #include "common_throws.h"
+#include "dex/class_accessor-inl.h"
 #include "dex/dex_file-inl.h"
 #include "dex/invoke_type.h"
 #include "dex_cache.h"
@@ -421,7 +422,7 @@ inline bool Class::IsDiscoverable(bool public_only,
   }
 
   return !hiddenapi::ShouldDenyAccessToMember(
-      member, access_context, hiddenapi::AccessMethod::kNone);
+      member, access_context, hiddenapi::AccessMethod::kCheckWithPolicy);
 }
 
 // Determine whether "this" is assignable from "src", where both of these
@@ -643,9 +644,9 @@ inline void Class::SetIfTable(ObjPtr<IfTable> new_iftable) {
       IfTableOffset(), new_iftable);
 }
 
-inline LengthPrefixedArray<ArtField>* Class::GetIFieldsPtr() {
+inline LengthPrefixedArray<ArtField>* Class::GetFieldsPtr() {
   DCHECK(IsLoaded() || IsErroneous()) << GetStatus();
-  return GetFieldPtr<LengthPrefixedArray<ArtField>*>(OFFSET_OF_OBJECT_MEMBER(Class, ifields_));
+  return GetFieldPtr<LengthPrefixedArray<ArtField>*>(OFFSET_OF_OBJECT_MEMBER(Class, fields_));
 }
 
 template<VerifyObjectFlags kVerifyFlags, ReadBarrierOption kReadBarrierOption>
@@ -680,44 +681,21 @@ inline MemberOffset Class::GetFirstReferenceStaticFieldOffsetDuringLinking(
   return MemberOffset(base);
 }
 
-inline void Class::SetIFieldsPtr(LengthPrefixedArray<ArtField>* new_ifields) {
-  DCHECK(GetIFieldsPtrUnchecked() == nullptr);
-  return SetFieldPtr<false>(OFFSET_OF_OBJECT_MEMBER(Class, ifields_), new_ifields);
+inline void Class::SetFieldsPtr(LengthPrefixedArray<ArtField>* new_fields) {
+  DCHECK(GetFieldsPtrUnchecked() == nullptr);
+  return SetFieldPtr<false>(OFFSET_OF_OBJECT_MEMBER(Class, fields_), new_fields);
 }
 
-inline void Class::SetIFieldsPtrUnchecked(LengthPrefixedArray<ArtField>* new_ifields) {
-  SetFieldPtr<false, true, kVerifyNone>(OFFSET_OF_OBJECT_MEMBER(Class, ifields_), new_ifields);
+inline void Class::SetFieldsPtrUnchecked(LengthPrefixedArray<ArtField>* new_fields) {
+  SetFieldPtr<false, true, kVerifyNone>(OFFSET_OF_OBJECT_MEMBER(Class, fields_), new_fields);
 }
 
-inline LengthPrefixedArray<ArtField>* Class::GetSFieldsPtrUnchecked() {
-  return GetFieldPtr<LengthPrefixedArray<ArtField>*>(OFFSET_OF_OBJECT_MEMBER(Class, sfields_));
+inline LengthPrefixedArray<ArtField>* Class::GetFieldsPtrUnchecked() {
+  return GetFieldPtr<LengthPrefixedArray<ArtField>*>(OFFSET_OF_OBJECT_MEMBER(Class, fields_));
 }
 
-inline LengthPrefixedArray<ArtField>* Class::GetIFieldsPtrUnchecked() {
-  return GetFieldPtr<LengthPrefixedArray<ArtField>*>(OFFSET_OF_OBJECT_MEMBER(Class, ifields_));
-}
-
-inline LengthPrefixedArray<ArtField>* Class::GetSFieldsPtr() {
-  DCHECK(IsLoaded() || IsErroneous()) << GetStatus();
-  return GetSFieldsPtrUnchecked();
-}
-
-inline void Class::SetSFieldsPtr(LengthPrefixedArray<ArtField>* new_sfields) {
-  DCHECK((IsRetired() && new_sfields == nullptr) ||
-         GetFieldPtr<ArtField*>(OFFSET_OF_OBJECT_MEMBER(Class, sfields_)) == nullptr);
-  SetFieldPtr<false>(OFFSET_OF_OBJECT_MEMBER(Class, sfields_), new_sfields);
-}
-
-inline void Class::SetSFieldsPtrUnchecked(LengthPrefixedArray<ArtField>* new_sfields) {
-  SetFieldPtr<false, true, kVerifyNone>(OFFSET_OF_OBJECT_MEMBER(Class, sfields_), new_sfields);
-}
-
-inline ArtField* Class::GetStaticField(uint32_t i) {
-  return &GetSFieldsPtr()->At(i);
-}
-
-inline ArtField* Class::GetInstanceField(uint32_t i) {
-  return &GetIFieldsPtr()->At(i);
+inline ArtField* Class::GetField(uint32_t i) {
+  return &GetFieldsPtr()->At(i);
 }
 
 template<VerifyObjectFlags kVerifyFlags>
@@ -1020,8 +998,8 @@ inline void Class::AssertInitializedOrInitializingInThread(Thread* self) {
 
 inline ObjPtr<ObjectArray<Class>> Class::GetProxyInterfaces() {
   CHECK(IsProxyClass());
-  // First static field.
-  ArtField* field = GetStaticField(0);
+  // First field.
+  ArtField* field = GetField(0);
   DCHECK_STREQ(field->GetName(), "interfaces");
   MemberOffset field_offset = field->GetOffset();
   return GetFieldObject<ObjectArray<Class>>(field_offset);
@@ -1029,8 +1007,8 @@ inline ObjPtr<ObjectArray<Class>> Class::GetProxyInterfaces() {
 
 inline ObjPtr<ObjectArray<ObjectArray<Class>>> Class::GetProxyThrows() {
   CHECK(IsProxyClass());
-  // Second static field.
-  ArtField* field = GetStaticField(1);
+  // Second field.
+  ArtField* field = GetField(1);
   DCHECK_STREQ(field->GetName(), "throws");
   MemberOffset field_offset = field->GetOffset();
   return GetFieldObject<ObjectArray<ObjectArray<Class>>>(field_offset);
@@ -1124,20 +1102,12 @@ inline ArraySlice<ArtMethod> Class::GetMethods(PointerSize pointer_size) {
   return GetMethodsSliceRangeUnchecked(methods, pointer_size, 0u, NumMethods(methods));
 }
 
-inline IterationRange<StrideIterator<ArtField>> Class::GetIFields() {
-  return MakeIterationRangeFromLengthPrefixedArray(GetIFieldsPtr());
+inline IterationRange<StrideIterator<ArtField>> Class::GetFields() {
+  return MakeIterationRangeFromLengthPrefixedArray(GetFieldsPtr());
 }
 
-inline IterationRange<StrideIterator<ArtField>> Class::GetSFields() {
-  return MakeIterationRangeFromLengthPrefixedArray(GetSFieldsPtr());
-}
-
-inline IterationRange<StrideIterator<ArtField>> Class::GetIFieldsUnchecked() {
-  return MakeIterationRangeFromLengthPrefixedArray(GetIFieldsPtrUnchecked());
-}
-
-inline IterationRange<StrideIterator<ArtField>> Class::GetSFieldsUnchecked() {
-  return MakeIterationRangeFromLengthPrefixedArray(GetSFieldsPtrUnchecked());
+inline IterationRange<StrideIterator<ArtField>> Class::GetFieldsUnchecked() {
+  return MakeIterationRangeFromLengthPrefixedArray(GetFieldsPtrUnchecked());
 }
 
 inline void Class::CheckPointerSize(PointerSize pointer_size) {
@@ -1234,14 +1204,37 @@ inline uint32_t Class::NumVirtualMethods() {
   return NumMethods() - GetVirtualMethodsStartOffset();
 }
 
-inline uint32_t Class::NumInstanceFields() {
-  LengthPrefixedArray<ArtField>* arr = GetIFieldsPtrUnchecked();
+inline uint32_t Class::NumFields() {
+  LengthPrefixedArray<ArtField>* arr = GetFieldsPtrUnchecked();
   return arr != nullptr ? arr->size() : 0u;
 }
 
-inline uint32_t Class::NumStaticFields() {
-  LengthPrefixedArray<ArtField>* arr = GetSFieldsPtrUnchecked();
-  return arr != nullptr ? arr->size() : 0u;
+inline bool Class::HasStaticFields() {
+  if (IsArrayClass() || IsPrimitive()) {
+    return false;
+  }
+  ClassAccessor accessor(GetDexFile(), GetDexClassDefIndex());
+  return accessor.NumStaticFields() != 0u;
+}
+
+inline uint32_t Class::ComputeNumStaticFields() {
+  uint32_t num = 0;
+  for (ArtField& field : GetFields()) {
+    if (field.IsStatic()) {
+      ++num;
+    }
+  }
+  return num;
+}
+
+inline uint32_t Class::ComputeNumInstanceFields() {
+  uint32_t num = 0;
+  for (ArtField& field : GetFields()) {
+    if (!field.IsStatic()) {
+      ++num;
+    }
+  }
+  return num;
 }
 
 template <typename T, VerifyObjectFlags kVerifyFlags, typename Visitor>
@@ -1262,11 +1255,9 @@ template <VerifyObjectFlags kVerifyFlags, typename Visitor>
 inline void Class::FixupNativePointers(Class* dest,
                                        PointerSize pointer_size,
                                        const Visitor& visitor) {
-  // Update the field arrays.
-  FixupNativePointer<LengthPrefixedArray<ArtField>*, kVerifyFlags>(
-      dest, pointer_size, visitor, OFFSET_OF_OBJECT_MEMBER(Class, sfields_));
+  // Update the field array.
   FixupNativePointer<LengthPrefixedArray<ArtField>*, kVerifyFlags>(
-      dest, pointer_size, visitor, OFFSET_OF_OBJECT_MEMBER(Class, ifields_));
+      dest, pointer_size, visitor, OFFSET_OF_OBJECT_MEMBER(Class, fields_));
   // Update method array.
   FixupNativePointer<LengthPrefixedArray<ArtMethod>*, kVerifyFlags>(
       dest, pointer_size, visitor, OFFSET_OF_OBJECT_MEMBER(Class, methods_));
@@ -1345,6 +1336,16 @@ inline void Class::SetHasDefaultMethods() {
   SetAccessFlagsDuringLinking(flags | kAccHasDefaultMethod);
 }
 
+inline void Class::SetHasTypeChecksFailure() {
+  uint32_t flags = GetField32(OFFSET_OF_OBJECT_MEMBER(Class, access_flags_));
+  SetAccessFlags(flags | kAccHasTypeChecksFailure);
+}
+
+inline bool Class::HasTypeChecksFailure() {
+  uint32_t flags = GetField32(OFFSET_OF_OBJECT_MEMBER(Class, access_flags_));
+  return (flags & kAccHasTypeChecksFailure) != 0u;
+}
+
 inline void Class::ClearFinalizable() {
   // We're clearing the finalizable flag only for `Object` and `Enum`
   // during early setup without the boot image.
@@ -1365,6 +1366,38 @@ inline ImTable* Class::FindSuperImt(PointerSize pointer_size) {
   return nullptr;
 }
 
+ALWAYS_INLINE FLATTEN inline ArtField* Class::FindDeclaredField(uint32_t dex_field_idx) {
+  size_t num_fields = NumFields();
+  if (num_fields > 0) {
+    // The field array is an ordered list of fields where there may be missing
+    // indices. For example, it could be [40, 42], but in 90% of cases cases we have
+    // [40, 41, 42]. The latter is the case we are optimizing for, where for
+    // example `dex_field_idx` is 41, and we can just substract it with the
+    // first field index (40) and directly access the array with that index (1).
+    uint32_t index = dex_field_idx - GetField(0)->GetDexFieldIndex();
+    if (index < num_fields) {
+      ArtField* field = GetField(index);
+      if (field->GetDexFieldIndex() == dex_field_idx) {
+        return field;
+      }
+    } else {
+      index = num_fields;
+    }
+    // If there is a field, it's down the array. The array is ordered by field
+    // index, so we know we can stop the search if `dex_field_idx` is greater
+    // than the current field's index.
+    for (; index > 0; --index) {
+      ArtField* field = GetField(index - 1);
+      if (field->GetDexFieldIndex() == dex_field_idx) {
+        return field;
+      } else if (field->GetDexFieldIndex() < dex_field_idx) {
+        break;
+      }
+    }
+  }
+  return nullptr;
+}
+
 }  // namespace mirror
 }  // namespace art
 
diff --git a/runtime/mirror/class-refvisitor-inl.h b/runtime/mirror/class-refvisitor-inl.h
index 96c218b175..8de27de485 100644
--- a/runtime/mirror/class-refvisitor-inl.h
+++ b/runtime/mirror/class-refvisitor-inl.h
@@ -120,11 +120,8 @@ void Class::VisitMethods(Visitor visitor, PointerSize pointer_size) {
 
 template<ReadBarrierOption kReadBarrierOption, class Visitor>
 void Class::VisitFields(Visitor visitor) {
-  for (ArtField& sfield : GetSFieldsUnchecked()) {
-    visitor(&sfield);
-  }
-  for (ArtField& ifield : GetIFieldsUnchecked()) {
-    visitor(&ifield);
+  for (ArtField& field : GetFieldsUnchecked()) {
+    visitor(&field);
   }
 }
 
diff --git a/runtime/mirror/class.cc b/runtime/mirror/class.cc
index a122393171..01f8fb3201 100644
--- a/runtime/mirror/class.cc
+++ b/runtime/mirror/class.cc
@@ -426,22 +426,15 @@ void Class::DumpClass(std::ostream& os, int flags) {
       os << StringPrintf("    %2zd: %s\n", i, ArtMethod::PrettyMethod(
           GetDirectMethod(i, image_pointer_size)).c_str());
     }
-    if (NumStaticFields() > 0) {
-      os << "  static fields (" << NumStaticFields() << " entries):\n";
+    if (NumFields() > 0) {
+      os << "  fields (" << NumFields() << " entries):\n";
       if (IsResolved()) {
-        for (size_t i = 0; i < NumStaticFields(); ++i) {
-          os << StringPrintf("    %2zd: %s\n", i, ArtField::PrettyField(GetStaticField(i)).c_str());
-        }
-      } else {
-        os << "    <not yet available>";
-      }
-    }
-    if (NumInstanceFields() > 0) {
-      os << "  instance fields (" << NumInstanceFields() << " entries):\n";
-      if (IsResolved()) {
-        for (size_t i = 0; i < NumInstanceFields(); ++i) {
-          os << StringPrintf("    %2zd: %s\n", i,
-                             ArtField::PrettyField(GetInstanceField(i)).c_str());
+        for (size_t i = 0; i < NumFields(); ++i) {
+          ArtField* field = GetField(i);
+          os << StringPrintf("    %2zd: %s %s\n",
+                             i,
+                             field->IsStatic() ? "static" : "instance",
+                             ArtField::PrettyField(field).c_str());
         }
       } else {
         os << "    <not yet available>";
@@ -1107,25 +1100,30 @@ static std::tuple<bool, ArtField*> FindFieldByNameAndType(const DexFile& dex_fil
   return {false, nullptr};
 }
 
-ArtField* Class::FindDeclaredInstanceField(std::string_view name, std::string_view type) {
+ArtField* Class::FindDeclaredField(std::string_view name, std::string_view type) {
   // Binary search by name. Interfaces are not relevant because they can't contain instance fields.
-  LengthPrefixedArray<ArtField>* ifields = GetIFieldsPtr();
-  if (ifields == nullptr) {
+  LengthPrefixedArray<ArtField>* fields = GetFieldsPtr();
+  if (fields == nullptr) {
     return nullptr;
   }
   DCHECK(!IsProxyClass());
-  auto [success, field] = FindFieldByNameAndType(GetDexFile(), ifields, name, type);
+  auto [success, field] = FindFieldByNameAndType(GetDexFile(), fields, name, type);
   DCHECK_EQ(success, field != nullptr);
   return field;
 }
 
+ArtField* Class::FindDeclaredInstanceField(std::string_view name, std::string_view type) {
+  ArtField* f = FindDeclaredField(name, type);
+  if (f != nullptr && !f->IsStatic()) {
+    return f;
+  }
+  return nullptr;
+}
+
 ArtField* Class::FindDeclaredInstanceField(ObjPtr<DexCache> dex_cache, uint32_t dex_field_idx) {
-  if (GetDexCache() == dex_cache) {
-    for (ArtField& field : GetIFields()) {
-      if (field.GetDexFieldIndex() == dex_field_idx) {
-        return &field;
-      }
-    }
+  ArtField* f = FindDeclaredField(dex_cache, dex_field_idx);
+  if (f != nullptr && !f->IsStatic()) {
+    return f;
   }
   return nullptr;
 }
@@ -1144,39 +1142,44 @@ ArtField* Class::FindInstanceField(std::string_view name, std::string_view type)
 
 ArtField* Class::FindDeclaredStaticField(std::string_view name, std::string_view type) {
   DCHECK(!type.empty());
-  LengthPrefixedArray<ArtField>* sfields = GetSFieldsPtr();
-  if (sfields == nullptr) {
+  LengthPrefixedArray<ArtField>* fields = GetFieldsPtr();
+  if (fields == nullptr) {
     return nullptr;
   }
   if (UNLIKELY(IsProxyClass())) {
     // Proxy fields do not have appropriate dex field indexes required by
     // `FindFieldByNameAndType()`. However, each proxy class has exactly
     // the same artificial fields created by the `ClassLinker`.
-    DCHECK_EQ(sfields->size(), 2u);
-    DCHECK_EQ(strcmp(sfields->At(0).GetName(), "interfaces"), 0);
-    DCHECK_EQ(strcmp(sfields->At(0).GetTypeDescriptor(), "[Ljava/lang/Class;"), 0);
-    DCHECK_EQ(strcmp(sfields->At(1).GetName(), "throws"), 0);
-    DCHECK_EQ(strcmp(sfields->At(1).GetTypeDescriptor(), "[[Ljava/lang/Class;"), 0);
+    DCHECK_EQ(fields->size(), 2u);
+    DCHECK_EQ(strcmp(fields->At(0).GetName(), "interfaces"), 0);
+    DCHECK_EQ(strcmp(fields->At(0).GetTypeDescriptor(), "[Ljava/lang/Class;"), 0);
+    DCHECK(fields->At(0).IsStatic());
+    DCHECK_EQ(strcmp(fields->At(1).GetName(), "throws"), 0);
+    DCHECK_EQ(strcmp(fields->At(1).GetTypeDescriptor(), "[[Ljava/lang/Class;"), 0);
+    DCHECK(fields->At(1).IsStatic());
     if (name == "interfaces") {
-      return (type == "[Ljava/lang/Class;") ? &sfields->At(0) : nullptr;
+      return (type == "[Ljava/lang/Class;") ? &fields->At(0) : nullptr;
     } else if (name == "throws") {
-      return (type == "[[Ljava/lang/Class;") ? &sfields->At(1) : nullptr;
+      return (type == "[[Ljava/lang/Class;") ? &fields->At(1) : nullptr;
     } else {
       return nullptr;
     }
   }
-  auto [success, field] = FindFieldByNameAndType(GetDexFile(), sfields, name, type);
-  DCHECK_EQ(success, field != nullptr);
-  return field;
+  ArtField* f = FindDeclaredField(name, type);
+  if (f != nullptr && f->IsStatic()) {
+    return f;
+  }
+  return nullptr;
+}
+
+ArtField* Class::FindDeclaredField(ObjPtr<DexCache> dex_cache, uint32_t dex_field_idx) {
+  return (dex_cache == GetDexCache()) ? FindDeclaredField(dex_field_idx) : nullptr;
 }
 
 ArtField* Class::FindDeclaredStaticField(ObjPtr<DexCache> dex_cache, uint32_t dex_field_idx) {
-  if (dex_cache == GetDexCache()) {
-    for (ArtField& field : GetSFields()) {
-      if (field.GetDexFieldIndex() == dex_field_idx) {
-        return &field;
-      }
-    }
+  ArtField* f = FindDeclaredField(dex_cache, dex_field_idx);
+  if (f != nullptr && f->IsStatic()) {
+    return f;
   }
   return nullptr;
 }
@@ -1190,17 +1193,11 @@ ObjPtr<mirror::ObjectArray<mirror::Field>> Class::GetDeclaredFields(
     return nullptr;
   }
   StackHandleScope<1> hs(self);
-  IterationRange<StrideIterator<ArtField>> ifields = GetIFields();
-  IterationRange<StrideIterator<ArtField>> sfields = GetSFields();
-  size_t array_size = NumInstanceFields() + NumStaticFields();
+  IterationRange<StrideIterator<ArtField>> fields = GetFields();
+  size_t array_size = NumFields();
   auto hiddenapi_context = hiddenapi::GetReflectionCallerAccessContext(self);
   // Lets go subtract all the non discoverable fields.
-  for (ArtField& field : ifields) {
-    if (!IsDiscoverable(public_only, hiddenapi_context, &field)) {
-      --array_size;
-    }
-  }
-  for (ArtField& field : sfields) {
+  for (ArtField& field : fields) {
     if (!IsDiscoverable(public_only, hiddenapi_context, &field)) {
       --array_size;
     }
@@ -1211,7 +1208,7 @@ ObjPtr<mirror::ObjectArray<mirror::Field>> Class::GetDeclaredFields(
   if (object_array == nullptr) {
     return nullptr;
   }
-  for (ArtField& field : ifields) {
+  for (ArtField& field : fields) {
     if (IsDiscoverable(public_only, hiddenapi_context, &field)) {
       ObjPtr<mirror::Field> reflect_field =
           mirror::Field::CreateFromArtField(self, &field, force_resolve);
@@ -1229,23 +1226,6 @@ ObjPtr<mirror::ObjectArray<mirror::Field>> Class::GetDeclaredFields(
                                          array_idx++, reflect_field);
     }
   }
-  for (ArtField& field : sfields) {
-    if (IsDiscoverable(public_only, hiddenapi_context, &field)) {
-      ObjPtr<mirror::Field> reflect_field =
-          mirror::Field::CreateFromArtField(self, &field, force_resolve);
-      if (reflect_field == nullptr) {
-        if (kIsDebugBuild) {
-          self->AssertPendingException();
-        }
-        return nullptr;
-      }
-      // We're initializing a newly allocated object, so we do not need to record that under
-      // a transaction. If the transaction is aborted, the whole object shall be unreachable.
-      object_array->SetWithoutChecks</*kTransactionActive=*/ false,
-                                     /*kCheckTransaction=*/ false>(
-                                         array_idx++, reflect_field);
-    }
-  }
   DCHECK_EQ(array_idx, array_size);
   return object_array.Get();
 }
@@ -1274,17 +1254,15 @@ ArtField* Class::FindStaticField(std::string_view name, std::string_view type) {
 }
 
 // Find a field using the JLS field resolution order.
-// Template arguments can be used to limit the search to either static or instance fields.
+// Template arguments can be used to extend the search to static fields of interfaces.
 // The search should be limited only if we know that a full search would yield a field of
 // the right type or no field at all. This can be known for field references in a method
 // if we have previously verified that method and did not find a field type mismatch.
-template <bool kSearchInstanceFields, bool kSearchStaticFields>
+template <bool kSearchStaticFieldsInInterfaces>
 ALWAYS_INLINE
 ArtField* FindFieldImpl(ObjPtr<mirror::Class> klass,
                         ObjPtr<mirror::DexCache> dex_cache,
                         uint32_t field_idx) REQUIRES_SHARED(Locks::mutator_lock_) {
-  static_assert(kSearchInstanceFields || kSearchStaticFields);
-
   // FIXME: Hijacking a proxy class by a custom class loader can break this assumption.
   DCHECK(!klass->IsProxyClass());
 
@@ -1296,12 +1274,7 @@ ArtField* FindFieldImpl(ObjPtr<mirror::Class> klass,
     // Lookup is always performed in the class referenced by the FieldId.
     DCHECK_EQ(klass->GetDexTypeIndex(),
               klass_dex_cache->GetDexFile()->GetFieldId(field_idx).class_idx_);
-    ArtField* f =  kSearchInstanceFields
-        ? klass->FindDeclaredInstanceField(klass_dex_cache, field_idx)
-        : nullptr;
-    if (kSearchStaticFields && f == nullptr) {
-      f = klass->FindDeclaredStaticField(klass_dex_cache, field_idx);
-    }
+    ArtField* f = klass->FindDeclaredField(klass_dex_cache, field_idx);
     if (f != nullptr) {
       return f;
     }
@@ -1346,27 +1319,14 @@ ArtField* FindFieldImpl(ObjPtr<mirror::Class> klass,
 
   auto find_field_by_name_and_type = [&](ObjPtr<mirror::Class> k, ObjPtr<DexCache> k_dex_cache)
       REQUIRES_SHARED(Locks::mutator_lock_) -> std::tuple<bool, ArtField*> {
-    if ((!kSearchInstanceFields || k->GetIFieldsPtr() == nullptr) &&
-        (!kSearchStaticFields || k->GetSFieldsPtr() == nullptr)) {
+    if (k->GetFieldsPtr() == nullptr) {
       return {false, nullptr};
     }
     ensure_name_and_type_initialized();
     const DexFile& k_dex_file = *k_dex_cache->GetDexFile();
-    if (kSearchInstanceFields && k->GetIFieldsPtr() != nullptr) {
-      auto [success, field] = FindFieldByNameAndType(k_dex_file, k->GetIFieldsPtr(), name, type);
-      DCHECK_EQ(success, field != nullptr);
-      if (success) {
-        return {true, field};
-      }
-    }
-    if (kSearchStaticFields && k->GetSFieldsPtr() != nullptr) {
-      auto [success, field] = FindFieldByNameAndType(k_dex_file, k->GetSFieldsPtr(), name, type);
-      DCHECK_EQ(success, field != nullptr);
-      if (success) {
-        return {true, field};
-      }
-    }
-    return {false, nullptr};
+    auto [success, field] = FindFieldByNameAndType(k_dex_file, k->GetFieldsPtr(), name, type);
+    DCHECK_EQ(success, field != nullptr);
+    return {success, field};
   };
 
   // If we had a dex cache mismatch, search declared fields by name and type.
@@ -1379,7 +1339,7 @@ ArtField* FindFieldImpl(ObjPtr<mirror::Class> klass,
   }
 
   // Search direct interfaces for static fields.
-  if (kSearchStaticFields) {
+  if (kSearchStaticFieldsInInterfaces) {
     ArtField* f = search_direct_interfaces(klass);
     if (f != nullptr) {
       return f;
@@ -1393,22 +1353,11 @@ ArtField* FindFieldImpl(ObjPtr<mirror::Class> klass,
     if (k_dex_cache == dex_cache) {
       // Matching dex_cache. We cannot compare the `field_idx` anymore because
       // the type index differs, so compare the name index and type index.
-      if (kSearchInstanceFields) {
-        for (ArtField& field : k->GetIFields()) {
-          const dex::FieldId& other_field_id = dex_file.GetFieldId(field.GetDexFieldIndex());
-          if (other_field_id.name_idx_ == field_id.name_idx_ &&
-              other_field_id.type_idx_ == field_id.type_idx_) {
-            return &field;
-          }
-        }
-      }
-      if (kSearchStaticFields) {
-        for (ArtField& field : k->GetSFields()) {
-          const dex::FieldId& other_field_id = dex_file.GetFieldId(field.GetDexFieldIndex());
-           if (other_field_id.name_idx_ == field_id.name_idx_ &&
-              other_field_id.type_idx_ == field_id.type_idx_) {
-            return &field;
-          }
+      for (ArtField& field : k->GetFields()) {
+        const dex::FieldId& other_field_id = dex_file.GetFieldId(field.GetDexFieldIndex());
+        if (other_field_id.name_idx_ == field_id.name_idx_ &&
+            other_field_id.type_idx_ == field_id.type_idx_) {
+          return &field;
         }
       }
     } else {
@@ -1418,7 +1367,7 @@ ArtField* FindFieldImpl(ObjPtr<mirror::Class> klass,
         return field;
       }
     }
-    if (kSearchStaticFields) {
+    if (kSearchStaticFieldsInInterfaces) {
       // Is this field in any of this class' interfaces?
       ArtField* f = search_direct_interfaces(k);
       if (f != nullptr) {
@@ -1431,20 +1380,17 @@ ArtField* FindFieldImpl(ObjPtr<mirror::Class> klass,
 
 FLATTEN
 ArtField* Class::FindField(ObjPtr<mirror::DexCache> dex_cache, uint32_t field_idx) {
-  return FindFieldImpl</*kSearchInstanceFields=*/ true,
-                       /*kSearchStaticFields*/ true>(this, dex_cache, field_idx);
+  return FindFieldImpl</*kSearchStaticFieldsInInterfaces*/ true>(this, dex_cache, field_idx);
 }
 
 FLATTEN
 ArtField* Class::FindInstanceField(ObjPtr<mirror::DexCache> dex_cache, uint32_t field_idx) {
-  return FindFieldImpl</*kSearchInstanceFields=*/ true,
-                       /*kSearchStaticFields*/ false>(this, dex_cache, field_idx);
+  return FindFieldImpl</*kSearchStaticFieldsInInterfaces*/ false>(this, dex_cache, field_idx);
 }
 
 FLATTEN
 ArtField* Class::FindStaticField(ObjPtr<mirror::DexCache> dex_cache, uint32_t field_idx) {
-  return FindFieldImpl</*kSearchInstanceFields=*/ false,
-                       /*kSearchStaticFields*/ true>(this, dex_cache, field_idx);
+  return FindFieldImpl</*kSearchStaticFieldsInInterfaces*/ true>(this, dex_cache, field_idx);
 }
 
 void Class::ClearSkipAccessChecksFlagOnAllMethods(PointerSize pointer_size) {
@@ -1501,7 +1447,7 @@ const char* Class::GetDescriptor(std::string* storage) {
     // the contents of the String are also constant. See ReadBarrierOption.
     ObjPtr<mirror::String> name = klass->GetName<kVerifyNone, kWithoutReadBarrier>();
     DCHECK(name != nullptr);
-    *storage = DotToDescriptor(name->ToModifiedUtf8().c_str());
+    *storage = DotToDescriptor(name->ToModifiedUtf8());
   } else {
     const char* descriptor;
     if (klass->IsPrimitive()) {
@@ -1876,7 +1822,7 @@ bool Class::ProxyDescriptorEquals(ObjPtr<mirror::Class> match) {
   }
 
   // Note: Proxy descriptor should never match a non-proxy descriptor but ART does not enforce that.
-  std::string descriptor = DotToDescriptor(name->ToModifiedUtf8().c_str());
+  std::string descriptor = DotToDescriptor(name->ToModifiedUtf8());
   std::string_view match_descriptor =
       match->GetDexFile().GetTypeDescriptorView(match->GetDexTypeIndex());
   return descriptor == match_descriptor;
@@ -2009,7 +1955,7 @@ ObjPtr<Method> Class::GetDeclaredMethodInternal(
   }
   auto h_args = hs.NewHandle(args);
   Handle<Class> h_klass = hs.NewHandle(klass);
-  constexpr hiddenapi::AccessMethod access_method = hiddenapi::AccessMethod::kNone;
+  constexpr hiddenapi::AccessMethod access_method = hiddenapi::AccessMethod::kCheckWithPolicy;
   ArtMethod* result = nullptr;
   bool result_hidden = false;
   for (auto& m : h_klass->GetDeclaredVirtualMethods(kPointerSize)) {
@@ -2247,7 +2193,7 @@ bool Class::EnsureStaticFieldIds(Handle<Class> h_this) {
     self->AssertPendingOOMException();
     return false;
   }
-  return ext->EnsureStaticJFieldIDsArrayPresent(h_this->NumStaticFields());
+  return ext->EnsureStaticJFieldIDsArrayPresent(h_this->NumFields());
 }
 ObjPtr<Object> Class::GetInstanceFieldIds() {
   ObjPtr<ClassExt> ext(GetExtData());
@@ -2265,42 +2211,42 @@ bool Class::EnsureInstanceFieldIds(Handle<Class> h_this) {
     self->AssertPendingOOMException();
     return false;
   }
-  return ext->EnsureInstanceJFieldIDsArrayPresent(h_this->NumInstanceFields());
+  return ext->EnsureInstanceJFieldIDsArrayPresent(h_this->NumFields());
 }
 
 size_t Class::GetStaticFieldIdOffset(ArtField* field) {
   DCHECK_LT(reinterpret_cast<uintptr_t>(field),
-            reinterpret_cast<uintptr_t>(&*GetSFieldsPtr()->end()))
+            reinterpret_cast<uintptr_t>(&*GetFieldsPtr()->end()))
       << "field not part of the current class. " << field->PrettyField() << " class is "
       << PrettyClass();
   DCHECK_GE(reinterpret_cast<uintptr_t>(field),
-            reinterpret_cast<uintptr_t>(&*GetSFieldsPtr()->begin()))
+            reinterpret_cast<uintptr_t>(&*GetFieldsPtr()->begin()))
       << "field not part of the current class. " << field->PrettyField() << " class is "
       << PrettyClass();
-  uintptr_t start = reinterpret_cast<uintptr_t>(&GetSFieldsPtr()->At(0));
+  uintptr_t start = reinterpret_cast<uintptr_t>(&GetFieldsPtr()->At(0));
   uintptr_t fld = reinterpret_cast<uintptr_t>(field);
   size_t res = (fld - start) / sizeof(ArtField);
-  DCHECK_EQ(&GetSFieldsPtr()->At(res), field)
+  DCHECK_EQ(&GetFieldsPtr()->At(res), field)
       << "Incorrect field computation expected: " << field->PrettyField()
-      << " got: " << GetSFieldsPtr()->At(res).PrettyField();
+      << " got: " << GetFieldsPtr()->At(res).PrettyField();
   return res;
 }
 
 size_t Class::GetInstanceFieldIdOffset(ArtField* field) {
   DCHECK_LT(reinterpret_cast<uintptr_t>(field),
-            reinterpret_cast<uintptr_t>(&*GetIFieldsPtr()->end()))
+            reinterpret_cast<uintptr_t>(&*GetFieldsPtr()->end()))
       << "field not part of the current class. " << field->PrettyField() << " class is "
       << PrettyClass();
   DCHECK_GE(reinterpret_cast<uintptr_t>(field),
-            reinterpret_cast<uintptr_t>(&*GetIFieldsPtr()->begin()))
+            reinterpret_cast<uintptr_t>(&*GetFieldsPtr()->begin()))
       << "field not part of the current class. " << field->PrettyField() << " class is "
       << PrettyClass();
-  uintptr_t start = reinterpret_cast<uintptr_t>(&GetIFieldsPtr()->At(0));
+  uintptr_t start = reinterpret_cast<uintptr_t>(&GetFieldsPtr()->At(0));
   uintptr_t fld = reinterpret_cast<uintptr_t>(field);
   size_t res = (fld - start) / sizeof(ArtField);
-  DCHECK_EQ(&GetIFieldsPtr()->At(res), field)
+  DCHECK_EQ(&GetFieldsPtr()->At(res), field)
       << "Incorrect field computation expected: " << field->PrettyField()
-      << " got: " << GetIFieldsPtr()->At(res).PrettyField();
+      << " got: " << GetFieldsPtr()->At(res).PrettyField();
   return res;
 }
 
@@ -2345,7 +2291,8 @@ static bool IsInterfaceMethodAccessible(ArtMethod* interface_method)
     REQUIRES_SHARED(Locks::mutator_lock_) {
   // If the interface method is part of the public SDK, return it.
   if ((hiddenapi::GetRuntimeFlags(interface_method) & kAccPublicApi) != 0) {
-    hiddenapi::ApiList api_list(hiddenapi::detail::GetDexFlags(interface_method));
+    hiddenapi::ApiList api_list =
+        hiddenapi::ApiList::FromDexFlags(hiddenapi::detail::GetDexFlags(interface_method));
     // The kAccPublicApi flag is also used as an optimization to avoid
     // other hiddenapi checks to always go on the slow path. Therefore, we
     // need to check here if the method is in the SDK list.
diff --git a/runtime/mirror/class.h b/runtime/mirror/class.h
index 9560f985ac..c8b234b789 100644
--- a/runtime/mirror/class.h
+++ b/runtime/mirror/class.h
@@ -288,6 +288,9 @@ class EXPORT MANAGED Class final : public Object {
 
   ALWAYS_INLINE void SetHasDefaultMethods() REQUIRES_SHARED(Locks::mutator_lock_);
 
+  ALWAYS_INLINE void SetHasTypeChecksFailure() REQUIRES_SHARED(Locks::mutator_lock_);
+  ALWAYS_INLINE bool HasTypeChecksFailure() REQUIRES_SHARED(Locks::mutator_lock_);
+
   ALWAYS_INLINE void SetFinalizable() REQUIRES_SHARED(Locks::mutator_lock_) {
     uint32_t flags = GetField32(OFFSET_OF_OBJECT_MEMBER(Class, access_flags_));
     SetAccessFlagsDuringLinking(flags | kAccClassIsFinalizable);
@@ -1032,21 +1035,24 @@ class EXPORT MANAGED Class final : public Object {
   ALWAYS_INLINE void SetIfTable(ObjPtr<IfTable> new_iftable)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  // Get instance fields of the class (See also GetSFields).
-  LengthPrefixedArray<ArtField>* GetIFieldsPtr() REQUIRES_SHARED(Locks::mutator_lock_);
+  // Get fields of the class.
+  LengthPrefixedArray<ArtField>* GetFieldsPtr() REQUIRES_SHARED(Locks::mutator_lock_);
 
-  ALWAYS_INLINE IterationRange<StrideIterator<ArtField>> GetIFields()
+  ALWAYS_INLINE IterationRange<StrideIterator<ArtField>> GetFields()
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  void SetIFieldsPtr(LengthPrefixedArray<ArtField>* new_ifields)
+  void SetFieldsPtr(LengthPrefixedArray<ArtField>* new_fields)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Unchecked edition has no verification flags.
-  void SetIFieldsPtrUnchecked(LengthPrefixedArray<ArtField>* new_sfields)
+  void SetFieldsPtrUnchecked(LengthPrefixedArray<ArtField>* new_fields)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  uint32_t NumInstanceFields() REQUIRES_SHARED(Locks::mutator_lock_);
-  ArtField* GetInstanceField(uint32_t i) REQUIRES_SHARED(Locks::mutator_lock_);
+  ArtField* GetField(uint32_t i) REQUIRES_SHARED(Locks::mutator_lock_);
+  uint32_t NumFields() REQUIRES_SHARED(Locks::mutator_lock_);
+  bool HasStaticFields() REQUIRES_SHARED(Locks::mutator_lock_);
+  uint32_t ComputeNumStaticFields() REQUIRES_SHARED(Locks::mutator_lock_);
+  uint32_t ComputeNumInstanceFields() REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Returns the number of instance fields containing reference types. Does not count fields in any
   // super classes.
@@ -1104,23 +1110,6 @@ class EXPORT MANAGED Class final : public Object {
   MemberOffset GetFirstReferenceStaticFieldOffsetDuringLinking(PointerSize pointer_size)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  // Gets the static fields of the class.
-  LengthPrefixedArray<ArtField>* GetSFieldsPtr() REQUIRES_SHARED(Locks::mutator_lock_);
-  ALWAYS_INLINE IterationRange<StrideIterator<ArtField>> GetSFields()
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
-  void SetSFieldsPtr(LengthPrefixedArray<ArtField>* new_sfields)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
-  // Unchecked edition has no verification flags.
-  void SetSFieldsPtrUnchecked(LengthPrefixedArray<ArtField>* new_sfields)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
-  uint32_t NumStaticFields() REQUIRES_SHARED(Locks::mutator_lock_);
-
-  // TODO: uint16_t
-  ArtField* GetStaticField(uint32_t i) REQUIRES_SHARED(Locks::mutator_lock_);
-
   // Find a static or instance field using the JLS resolution order
   ArtField* FindField(ObjPtr<mirror::DexCache> dex_cache, uint32_t field_idx)
       REQUIRES_SHARED(Locks::mutator_lock_);
@@ -1134,6 +1123,15 @@ class EXPORT MANAGED Class final : public Object {
   ArtField* FindInstanceField(ObjPtr<DexCache> dex_cache, uint32_t dex_field_idx)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
+  ArtField* FindDeclaredField(ObjPtr<DexCache> dex_cache, uint32_t dex_field_idx)
+      REQUIRES_SHARED(Locks::mutator_lock_);
+
+  ArtField* FindDeclaredField(uint32_t dex_field_idx)
+      REQUIRES_SHARED(Locks::mutator_lock_);
+
+  ArtField* FindDeclaredField(std::string_view name, std::string_view type)
+      REQUIRES_SHARED(Locks::mutator_lock_);
+
   ArtField* FindDeclaredInstanceField(std::string_view name, std::string_view type)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
@@ -1384,7 +1382,7 @@ class EXPORT MANAGED Class final : public Object {
       REQUIRES_SHARED(Locks::mutator_lock_);
   ObjPtr<Object> GetInstanceFieldIds() REQUIRES_SHARED(Locks::mutator_lock_);
 
-  // Calculate the index in the ifields_, methods_ or sfields_ arrays a method is located at. This
+  // Calculate the index in the fields_ or methods_ arrays a method is located at. This
   // is to be used with the above Get{,OrCreate}...Ids functions.
   size_t GetStaticFieldIdOffset(ArtField* field)
       REQUIRES_SHARED(Locks::mutator_lock_);
@@ -1426,11 +1424,8 @@ class EXPORT MANAGED Class final : public Object {
   void CheckObjectAlloc() REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Unchecked editions is for root visiting.
-  LengthPrefixedArray<ArtField>* GetSFieldsPtrUnchecked() REQUIRES_SHARED(Locks::mutator_lock_);
-  IterationRange<StrideIterator<ArtField>> GetSFieldsUnchecked()
-      REQUIRES_SHARED(Locks::mutator_lock_);
-  LengthPrefixedArray<ArtField>* GetIFieldsPtrUnchecked() REQUIRES_SHARED(Locks::mutator_lock_);
-  IterationRange<StrideIterator<ArtField>> GetIFieldsUnchecked()
+  LengthPrefixedArray<ArtField>* GetFieldsPtrUnchecked() REQUIRES_SHARED(Locks::mutator_lock_);
+  IterationRange<StrideIterator<ArtField>> GetFieldsUnchecked()
       REQUIRES_SHARED(Locks::mutator_lock_);
 
   // The index in the methods_ array where the first declared virtual method is.
@@ -1514,16 +1509,16 @@ class EXPORT MANAGED Class final : public Object {
   // virtual_ methods_ for miranda methods.
   HeapReference<PointerArray> vtable_;
 
-  // instance fields
+  // instance and static fields
   //
   // These describe the layout of the contents of an Object.
   // Note that only the fields directly declared by this class are
-  // listed in ifields; fields declared by a superclass are listed in
-  // the superclass's Class.ifields.
+  // listed in `fields_`; fields declared by a superclass are listed in
+  // the superclass's `Class.fields_`.
   //
   // ArtFields are allocated as a length prefixed ArtField array, and not an array of pointers to
   // ArtFields.
-  uint64_t ifields_;
+  uint64_t fields_;
 
   // Pointer to an ArtMethod length-prefixed array. All the methods where this class is the place
   // where they are logically defined. This includes all private, static, final and virtual methods
@@ -1542,9 +1537,6 @@ class EXPORT MANAGED Class final : public Object {
   // Note that this field is used by the native debugger as the unique identifier for the type.
   uint64_t methods_;
 
-  // Static fields length-prefixed array.
-  uint64_t sfields_;
-
   // Access flags; low 16 bits are defined by VM spec.
   uint32_t access_flags_;
 
@@ -1587,7 +1579,7 @@ class EXPORT MANAGED Class final : public Object {
   // bits contains the size shift of the primitive type.
   uint32_t primitive_type_;
 
-  // Bitmap of offsets of ifields.
+  // Bitmap of offsets of instance fields.
   uint32_t reference_instance_offsets_;
 
   // See the real definition in subtype_check_bits_and_status.h
@@ -1611,7 +1603,7 @@ class EXPORT MANAGED Class final : public Object {
   // VTableEntry embedded_vtable_[0];
   // Static fields, variable size.
   // uint32_t fields_[0];
-  // Embedded bitmap of offsets of ifields, for classes that need more than 31
+  // Embedded bitmap of offsets of instance fields, for classes that need more than 31
   // reference-offset bits. 'reference_instance_offsets_' stores the number of
   // 32-bit entries that hold the entire bitmap. We compute the offset of first
   // entry by subtracting this number from class_size_.
diff --git a/runtime/mirror/dex_cache-inl.h b/runtime/mirror/dex_cache-inl.h
index 4ac5131958..71ada5db9e 100644
--- a/runtime/mirror/dex_cache-inl.h
+++ b/runtime/mirror/dex_cache-inl.h
@@ -19,12 +19,14 @@
 
 #include "dex_cache.h"
 
-#include <android-base/logging.h>
+#include <atomic>
 
+#include "android-base/logging.h"
 #include "art_field.h"
 #include "art_method.h"
 #include "base/atomic_pair.h"
 #include "base/casts.h"
+#include "base/globals.h"
 #include "base/pointer_size.h"
 #include "class_linker.h"
 #include "dex/dex_file.h"
@@ -38,8 +40,6 @@
 #include "runtime.h"
 #include "write_barrier-inl.h"
 
-#include <atomic>
-
 namespace art HIDDEN {
 namespace mirror {
 
@@ -346,9 +346,17 @@ inline void DexCache::VisitNativeRoots(const Visitor& visitor) {
 }
 
 template <VerifyObjectFlags kVerifyFlags, ReadBarrierOption kReadBarrierOption>
-inline ObjPtr<String> DexCache::GetLocation() {
-  return GetFieldObject<String, kVerifyFlags, kReadBarrierOption>(
+inline ObjPtr<String> DexCache::GetLocation(bool allow_location_mismatch) {
+  ObjPtr<String> location = GetFieldObject<String, kVerifyFlags, kReadBarrierOption>(
       OFFSET_OF_OBJECT_MEMBER(DexCache, location_));
+  // At runtime, if the DexCache is from an app image or dynamically created, then its location must
+  // match the DexFile location.
+  // TODO(jiakaiz): Remove the AOT compiler and boot classpath checks?
+  if (kIsDebugBuild && !allow_location_mismatch && !Runtime::Current()->IsAotCompiler() &&
+      GetDexFile() != nullptr && !ClassLinker::IsBootClassLoader(GetClassLoader())) {
+    DCHECK_EQ(location->ToModifiedUtf8(), GetDexFile()->GetLocation());
+  }
+  return location;
 }
 
 }  // namespace mirror
diff --git a/runtime/mirror/dex_cache.cc b/runtime/mirror/dex_cache.cc
index b981f08d97..7dc407c737 100644
--- a/runtime/mirror/dex_cache.cc
+++ b/runtime/mirror/dex_cache.cc
@@ -245,10 +245,7 @@ void DexCache::SetResolvedType(dex::TypeIndex type_idx, ObjPtr<Class> resolved)
     }
     auto* resolved_fields = GetResolvedFieldsArray();
     if (resolved_fields != nullptr) {
-      for (ArtField& current_field : resolved->GetSFields()) {
-        resolved_fields->Set(current_field.GetDexFieldIndex(), &current_field);
-      }
-      for (ArtField& current_field : resolved->GetIFields()) {
+      for (ArtField& current_field : resolved->GetFields()) {
         resolved_fields->Set(current_field.GetDexFieldIndex(), &current_field);
       }
     }
diff --git a/runtime/mirror/dex_cache.h b/runtime/mirror/dex_cache.h
index c1d8cd8335..5cfb37a5b3 100644
--- a/runtime/mirror/dex_cache.h
+++ b/runtime/mirror/dex_cache.h
@@ -308,9 +308,10 @@ class MANAGED DexCache final : public Object {
   // WARNING: This does not free the memory since it is in LinearAlloc.
   EXPORT void ResetNativeArrays() REQUIRES_SHARED(Locks::mutator_lock_);
 
-  template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags,
-           ReadBarrierOption kReadBarrierOption = kWithReadBarrier>
-  ObjPtr<String> GetLocation() REQUIRES_SHARED(Locks::mutator_lock_);
+  template <VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags,
+            ReadBarrierOption kReadBarrierOption = kWithReadBarrier>
+  ObjPtr<String> GetLocation(bool allow_location_mismatch = false)
+      REQUIRES_SHARED(Locks::mutator_lock_);
 
   String* GetResolvedString(dex::StringIndex string_idx) ALWAYS_INLINE
       REQUIRES_SHARED(Locks::mutator_lock_);
diff --git a/runtime/mirror/dex_cache_test.cc b/runtime/mirror/dex_cache_test.cc
index 1429bb3e04..f2cfcbf050 100644
--- a/runtime/mirror/dex_cache_test.cc
+++ b/runtime/mirror/dex_cache_test.cc
@@ -85,14 +85,16 @@ TEST_F(DexCacheTest, TestResolvedFieldAccess) {
   ASSERT_TRUE(klass2 != nullptr);
   EXPECT_OBJ_PTR_EQ(klass1->GetDexCache(), klass2->GetDexCache());
 
-  EXPECT_NE(klass1->NumStaticFields(), 0u);
-  for (ArtField& field : klass2->GetSFields()) {
-    EXPECT_FALSE(
-        klass1->ResolvedFieldAccessTest</*throw_on_failure=*/ false>(
-            klass2.Get(),
-            &field,
-            klass1->GetDexCache(),
-            field.GetDexFieldIndex()));
+  EXPECT_NE(klass1->ComputeNumStaticFields(), 0u);
+  for (ArtField& field : klass2->GetFields()) {
+    if (field.IsStatic()) {
+      EXPECT_FALSE(
+          klass1->ResolvedFieldAccessTest</*throw_on_failure=*/ false>(
+              klass2.Get(),
+              &field,
+              klass1->GetDexCache(),
+              field.GetDexFieldIndex()));
+    }
   }
 }
 
diff --git a/runtime/mirror/field.cc b/runtime/mirror/field.cc
index 5ad59fadf4..40af891f01 100644
--- a/runtime/mirror/field.cc
+++ b/runtime/mirror/field.cc
@@ -32,8 +32,7 @@ void Field::VisitTarget(ReflectiveValueVisitor* v) {
   if (orig != new_value) {
     SetOffset<false>(new_value->GetOffset().Int32Value());
     SetDeclaringClass<false>(new_value->GetDeclaringClass());
-    auto new_range =
-        IsStatic() ? GetDeclaringClass()->GetSFields() : GetDeclaringClass()->GetIFields();
+    auto new_range = GetDeclaringClass()->GetFields();
     auto position = std::find_if(
         new_range.begin(), new_range.end(), [&](const auto& f) { return &f == new_value; });
     DCHECK(position != new_range.end());
@@ -45,13 +44,7 @@ void Field::VisitTarget(ReflectiveValueVisitor* v) {
 
 ArtField* Field::GetArtField() {
   ObjPtr<mirror::Class> declaring_class = GetDeclaringClass();
-  if (IsStatic()) {
-    DCHECK_LT(GetArtFieldIndex(), declaring_class->NumStaticFields());
-    return declaring_class->GetStaticField(GetArtFieldIndex());
-  } else {
-    DCHECK_LT(GetArtFieldIndex(), declaring_class->NumInstanceFields());
-    return declaring_class->GetInstanceField(GetArtFieldIndex());
-  }
+  return declaring_class->GetField(GetArtFieldIndex());
 }
 
 ObjPtr<mirror::Field> Field::CreateFromArtField(Thread* self,
@@ -86,8 +79,7 @@ ObjPtr<mirror::Field> Field::CreateFromArtField(Thread* self,
       field->GetDeclaringClass());
   ret->SetAccessFlags</*kTransactionActive=*/ false, /*kCheckTransaction=*/ false>(
       field->GetAccessFlags());
-  auto iter_range = field->IsStatic() ? field->GetDeclaringClass()->GetSFields()
-                                      : field->GetDeclaringClass()->GetIFields();
+  auto iter_range = field->GetDeclaringClass()->GetFields();
   auto position = std::find_if(
       iter_range.begin(), iter_range.end(), [&](const auto& f) { return &f == field; });
   DCHECK(position != iter_range.end());
diff --git a/runtime/mirror/method_handle_impl.cc b/runtime/mirror/method_handle_impl.cc
index 5e80fb0925..0190c4bb4a 100644
--- a/runtime/mirror/method_handle_impl.cc
+++ b/runtime/mirror/method_handle_impl.cc
@@ -58,6 +58,19 @@ ObjPtr<mirror::MethodHandleImpl> MethodHandleImpl::Create(Thread* const self,
   return mh.Get();
 }
 
+ObjPtr<mirror::MethodHandleImpl> MethodHandleImpl::Create(Thread* const self,
+                                                          Handle<Field> field,
+                                                          MethodHandle::Kind kind,
+                                                          Handle<MethodType> method_type)
+    REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(!Roles::uninterruptible_) {
+  StackHandleScope<1> hs(self);
+  Handle<mirror::MethodHandleImpl> mh(hs.NewHandle(ObjPtr<MethodHandleImpl>::DownCast(
+      WellKnownClasses::java_lang_invoke_MethodHandleImpl_fieldInit->NewObject<'L', 'I', 'L'>(
+          self, field, static_cast<uint32_t>(kind), method_type))));
+
+  return mh.Get();
+}
+
 void MethodHandle::VisitTarget(ReflectiveValueVisitor* v) {
   void* target = GetTargetField();
   void* result;
diff --git a/runtime/mirror/method_handle_impl.h b/runtime/mirror/method_handle_impl.h
index 427a20013e..a4a6e67a5a 100644
--- a/runtime/mirror/method_handle_impl.h
+++ b/runtime/mirror/method_handle_impl.h
@@ -22,6 +22,7 @@
 #include "base/macros.h"
 #include "class.h"
 #include "method_type.h"
+#include "mirror/field.h"
 #include "obj_ptr.h"
 #include "object.h"
 
@@ -127,8 +128,20 @@ class MANAGED MethodHandleImpl : public MethodHandle {
                                                         Handle<MethodType> method_type)
       REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(!Roles::uninterruptible_);
 
+  EXPORT static ObjPtr<mirror::MethodHandleImpl> Create(Thread* const self,
+                                                        Handle<Field> field,
+                                                        MethodHandle::Kind kind,
+                                                        Handle<MethodType> method_type)
+      REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(!Roles::uninterruptible_);
+
+  static MemberOffset TargetOffset() {
+    return MemberOffset(OFFSETOF_MEMBER(MethodHandleImpl, target_));
+  }
+
  private:
+  HeapReference<mirror::Field> field_;
   HeapReference<mirror::Object> target_class_or_info_;  // Unused by the runtime.
+  uint64_t target_;
 
   friend struct art::MethodHandleImplOffsets;  // for verifying offset information
   DISALLOW_IMPLICIT_CONSTRUCTORS(MethodHandleImpl);
diff --git a/runtime/mirror/object-inl.h b/runtime/mirror/object-inl.h
index 9e807d1c5b..b94a57d3c6 100644
--- a/runtime/mirror/object-inl.h
+++ b/runtime/mirror/object-inl.h
@@ -939,6 +939,62 @@ inline void Object::VerifyTransaction() {
   }
 }
 
+class Object::DumpRefsVisitor {
+ public:
+  explicit DumpRefsVisitor(std::ostream& os, bool dump_type_of)
+      : os_(os), dump_type_of_(dump_type_of) {}
+
+  ALWAYS_INLINE void operator()(mirror::Object* obj,
+                                MemberOffset offset,
+                                [[maybe_unused]] bool is_static) const
+      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
+    mirror::Object* ref = obj->GetFieldObject<mirror::Object>(offset);
+    if (ref != nullptr) {
+      os_ << "\nref[" << offset << "] = " << ref;
+      if (dump_type_of_) {
+        os_ << " (" << ref->PrettyTypeOf() << ")";
+      }
+    }
+  }
+
+  ALWAYS_INLINE void operator()([[maybe_unused]] ObjPtr<mirror::Class> klass,
+                                ObjPtr<mirror::Reference> ref) const
+      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
+    if (!ref.IsNull()) {
+      os_ << "\nreferant[" << mirror::Reference::ReferentOffset() << "] = " << ref.Ptr() << " (";
+      if (dump_type_of_) {
+        os_ << " (" << ref->PrettyTypeOf() << ")";
+      }
+    }
+  }
+
+  void VisitRootIfNonNull(mirror::CompressedReference<mirror::Object>* root) const
+      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
+    if (!root->IsNull()) {
+      VisitRoot(root);
+    }
+  }
+
+  void VisitRoot(mirror::CompressedReference<mirror::Object>* root) const
+      REQUIRES(Locks::heap_bitmap_lock_) REQUIRES_SHARED(Locks::mutator_lock_) {
+    mirror::Object* ref = root->AsMirrorPtr();
+    os_ << "\nroot[" << root << "] = " << ref;
+    if (dump_type_of_) {
+      os_ << " (" << ref->PrettyTypeOf() << ")\n";
+    }
+  }
+
+ private:
+  std::ostream& os_;
+  bool dump_type_of_;
+};
+
+template <bool kDumpNativeRoots>
+void Object::DumpReferences(std::ostream& os, bool dump_type_of) {
+  DumpRefsVisitor visitor(os, dump_type_of);
+  VisitReferences<kDumpNativeRoots>(visitor, visitor);
+}
+
 }  // namespace mirror
 }  // namespace art
 
diff --git a/runtime/mirror/object.cc b/runtime/mirror/object.cc
index b28978603c..3fa71c66d1 100644
--- a/runtime/mirror/object.cc
+++ b/runtime/mirror/object.cc
@@ -248,8 +248,8 @@ void Object::CheckFieldAssignmentImpl(MemberOffset field_offset, ObjPtr<Object>
     return;
   }
   for (ObjPtr<Class> cur = c; cur != nullptr; cur = cur->GetSuperClass()) {
-    for (ArtField& field : cur->GetIFields()) {
-      if (field.GetOffset().Int32Value() == field_offset.Int32Value()) {
+    for (ArtField& field : cur->GetFields()) {
+      if (!field.IsStatic() && field.GetOffset().Int32Value() == field_offset.Int32Value()) {
         CHECK_NE(field.GetTypeAsPrimitiveType(), Primitive::kPrimNot);
         // TODO: resolve the field type for moving GC.
         ObjPtr<mirror::Class> field_type =
@@ -266,8 +266,8 @@ void Object::CheckFieldAssignmentImpl(MemberOffset field_offset, ObjPtr<Object>
     return;
   }
   if (IsClass()) {
-    for (ArtField& field : AsClass()->GetSFields()) {
-      if (field.GetOffset().Int32Value() == field_offset.Int32Value()) {
+    for (ArtField& field : AsClass()->GetFields()) {
+      if (field.IsStatic() && field.GetOffset().Int32Value() == field_offset.Int32Value()) {
         CHECK_NE(field.GetTypeAsPrimitiveType(), Primitive::kPrimNot);
         // TODO: resolve the field type for moving GC.
         ObjPtr<mirror::Class> field_type =
diff --git a/runtime/mirror/object.h b/runtime/mirror/object.h
index 2b04a55c73..0fd9003e33 100644
--- a/runtime/mirror/object.h
+++ b/runtime/mirror/object.h
@@ -690,6 +690,9 @@ class EXPORT MANAGED LOCKABLE Object {
   std::string PrettyTypeOf()
       REQUIRES_SHARED(Locks::mutator_lock_);
 
+  // Dump non-null references and their type.
+  template <bool kDumpNativeRoots>
+  void DumpReferences(std::ostream& osi, bool dump_type_of);
   // A utility function that does a raw copy of `src`'s data into the buffer `dst_bytes`.
   // Skips the object header.
   static void CopyRawObjectData(uint8_t* dst_bytes,
@@ -802,6 +805,8 @@ class EXPORT MANAGED LOCKABLE Object {
   // Monitor and hash code information.
   uint32_t monitor_;
 
+  class DumpRefsVisitor;
+
   friend class art::Monitor;
   friend struct art::ObjectOffsets;  // for verifying offset information
   friend class CopyObjectVisitor;  // for CopyObject().
diff --git a/runtime/monitor.cc b/runtime/monitor.cc
index e03b470805..7aba2a4504 100644
--- a/runtime/monitor.cc
+++ b/runtime/monitor.cc
@@ -1694,9 +1694,16 @@ size_t MonitorList::Size() {
 
 class MonitorDeflateVisitor : public IsMarkedVisitor {
  public:
-  MonitorDeflateVisitor() : self_(Thread::Current()), deflate_count_(0) {}
+  MonitorDeflateVisitor()
+      : self_(Thread::Current()), deflate_count_(0), heap_(Runtime::Current()->GetHeap()) {}
 
   mirror::Object* IsMarked(mirror::Object* object) override REQUIRES(Locks::mutator_lock_) {
+    // Avoid deflating monitors in zygote/image spaces because that could
+    // end up dirtying otherwise shared/clean memory.
+    if (heap_->IsInZygoteSpace(object) || heap_->ObjectIsInBootImageSpace(object)) {
+      return object;  // Monitor was not deflated.
+    }
+
     if (Monitor::Deflate(self_, object)) {
       DCHECK_NE(object->GetLockWord(true).GetState(), LockWord::kFatLocked);
       ++deflate_count_;
@@ -1708,6 +1715,7 @@ class MonitorDeflateVisitor : public IsMarkedVisitor {
 
   Thread* const self_;
   size_t deflate_count_;
+  gc::Heap* heap_;
 };
 
 size_t MonitorList::DeflateMonitors() {
diff --git a/runtime/mutator_gc_coord.md b/runtime/mutator_gc_coord.md
index 01e3ef025d..35996a5f8e 100644
--- a/runtime/mutator_gc_coord.md
+++ b/runtime/mutator_gc_coord.md
@@ -308,6 +308,125 @@ to prevent a single thread suspension of a thread currently between
 it will complete before it can be affected by suspension requests from other
 threads.
 
+
+Thread state transitions, flags, and memory ordering
+----------------------------------------------------
+
+Logically when a state transitions to state `kRunnable`, it acquires the mutator
+lock (in shared mode). When it changes its state back to suspended, it releases
+that lock. These must enforce proper happens-before ordering with respect to a
+thread acquiring the mutator lock in exclusive mode. Thus changing the thread
+state to suspended normally requires a release operation, to ensure visibility
+to a thread wishing to "suspend". Conversely, when we change the state back to
+`kRunnable` in `TransitionFromSuspendedToRunnable` we do so with an acquire
+compare-exchange operation to check that no suspension request remains.
+
+Any suspending thread should clear `kSuspendRequest` with a release
+operation. This, together with the acquire compare-exchange when returning to
+runnable state, ensure that suspend-count decrements happen-before a thread
+becomes runnable again.
+
+Flags are often set and tested with relaxed ordering, even when they may
+communicate a request to another thread. The reason this is safe varies.
+Generally the request is conformed by some other better synchronized
+interaction, which establishes the necessary ordering. In particular:
+
+`kCheckPointRequest`
+: See below. The call happens-before checkpoint execution since
+`checkpoint_function` accesses are guarded by a lock, as are updates to the
+flag. Checkpoint completion ordering is ensured by waiting for both
+`ThreadList::RunCheckpoint()` to complete, and for a barrier indicating
+completion in "runnable" threads.
+
+`kEmptyCheckpointRequest`
+: Currently (12/2024) in flux. See below and b/382722942 . We currently use
+acquire/release ordering to access this flag in some places to partially
+mitigate memory ordering issues here.
+
+`kActiveSuspendBarrier`
+: Changes are protected by `thread_suspend_count_lock_`, and
+`PassActiveSuspendBarriers` rechecks it while holding that lock.
+
+`kPendingFlipFunction`
+: Set while holding `thread_list_lock_`, but this lock is not consistently held
+while checking the flag, notably in `EnsureFlipFunctionStarted()`. We need
+acquire/release ordering to make sure that the requestor happens-before flip
+function execution, and the flip function is properly visible, among other
+things. We still commonly read the flag using a relaxed operation, but we
+confirm it with an acquire compare-exchange operation in
+`EnsureFlipFunctionStarted()` before actiong on it.
+
+`kRunningFlipFunction`
+: Cleared with release ordering, and read with acquire ordering by
+`WaitForFlipFunction` and its callers, thus ensuring that flip function
+execution happens-before completion of WaitForFlipFunction.
+
+`kSuspensionImmune`
+: Guarded by `thread_suspend_count_lock_`.
+
+### Checkpoint-specific considerations
+
+Checkpoints expose additional tricky issues, since they are also used to ensure
+that certain global state changes have propagated to all running Java threads.
+
+`ThreadList::RunCheckpoint()` guarantees that "if at point _X_ `RunCheckpoint()`
+has returned, and all checkpoints have been properly observed to have completed,
+then every thread has executed a code sequence _S_ during which it remained in
+a suspended state, such that the call to `RunCheckpoint` happens-before the end
+of _S_, and the beginning of _S_ happened before _X_."
+
+For each thread _T_, we attempt to atomically install the `kCheckpointRequest`
+flag while ensuring that _T_ is runnable.  If this succeeds, the fact that
+`tlsPtr_.checkpoint_function` is protected by `thread_suspend_count_lock_`, and
+`checkpoint_function` is written by the requestor, and read by _T_ at a suspend
+point, ensures that the call happens-before _S_. Normally a barrier ensures that
+_S_ happens-before _X_ .
+
+If we are unable to do so for a thread _T_ because we find it suspended, we run
+`checkpoint_function` ourselves. Before running it we set `kSuspendRequest`
+(with a release store) while _T_ is in _S_, preventing _T_ from terminating _S_
+by becoming runnable, after an acquire load of the flags. This ensures that the
+`RunCheckpoint()` call happens-before the end of _S_. Since we learned of _T_'s
+suspended state via an acquire load of its state, which it stored with a release
+store, we know that the beginning of _S_ happens-before we return from
+`RunCheckpoint()`, and hence before _X_ .
+
+The case of empty checkpoints is worth highlighting. We have traditionally
+optimized that by avoiding ever suspending the affected threads. This appears
+correct if all operations are sequentially consistent. But once memory model
+issues are considered, it appears more expensive to do fully correctly than it
+is to forego the optimization, as we explain below:
+
+We need to ensure that if Thread _A_ performs some update _U_ and then calls
+`RunEmptyCheckpoint()` then, when `RunEmptyCheckpoint()` returns, every Thread
+_B_ will have passed a suspend point at which _U_ became guaranteed visible to
+that thread. This could be ensured in different ways, depending on the observed
+state of Thread _B_:
+
+Runnable
+: Use acquire/release ordering when setting and detecting the
+`kEmptyCheckpointRequest` flag, and then use a barrier to observe when thread _B_
+has done so. In this case, Thread _B_ actually executes code at a suspend point.
+The acquire release ordering on `kEmptyCheckpointRequest` ensures that _U_
+happens-before the suspend point, and the barrier ensures that the suspend point
+happens-before the `RunEmptyCheckpoint()` return.
+
+Suspended
+: In this case, we have no mechanism for the thread to execute synchronization
+operations, and things become trickier. Thread _B_ is suspended, but it may
+restart at any time. Thread _A_ wants to conclude that Thread _B_ is already
+suspended at a suspend point, and thus it is safe to ignore Thread _B_.
+Effectively, it wants to perform the update _U_, read _B_'s state, and continue,
+while _B_ may change its state back to runnable, and then read the result of
+_U_. Both threads effectively perform a store (either _U_ or to the state of
+_B_) and then read the other value. With only acquire/release ordering, this can
+result in _A_ seeing the old suspended state, and _B_ failing to see the update
+_U_, which is an incorrect outcome.
+: The canonical fix for this kind of `store; load` reordering problem is to make
+all operations sequentially consistent. There does not appear to be an easy way
+to limit this overhead to just empty checkpoint execution. Thus it appears to be
+better to forego this "optimization".
+
 [^1]: In the most recent versions of ART, compiler-generated code loads through
     the address at `tlsPtr_.suspend_trigger`. A thread suspension is requested
     by setting this to null, triggering a `SIGSEGV`, causing that thread to
diff --git a/runtime/native/dalvik_system_DexFile.cc b/runtime/native/dalvik_system_DexFile.cc
index a1d4f16d26..da5fe5f0f2 100644
--- a/runtime/native/dalvik_system_DexFile.cc
+++ b/runtime/native/dalvik_system_DexFile.cc
@@ -487,7 +487,7 @@ static jclass DexFile_defineClassNative(JNIEnv* env,
     VLOG(class_linker) << "Failed to find class_name";
     return nullptr;
   }
-  const std::string descriptor(DotToDescriptor(class_name.c_str()));
+  const std::string descriptor = DotToDescriptor(class_name);
   const size_t hash = ComputeModifiedUtf8Hash(descriptor);
   for (auto& dex_file : dex_files) {
     const dex::ClassDef* dex_class_def = OatDexFile::FindClassDef(*dex_file, descriptor, hash);
@@ -628,38 +628,6 @@ static jint GetDexOptNeeded(JNIEnv* env,
                                             downgrade);
 }
 
-static jstring DexFile_getDexFileStatus(JNIEnv* env,
-                                        jclass,
-                                        jstring javaFilename,
-                                        jstring javaInstructionSet) {
-  ScopedUtfChars filename(env, javaFilename);
-  if (env->ExceptionCheck()) {
-    return nullptr;
-  }
-
-  ScopedUtfChars instruction_set(env, javaInstructionSet);
-  if (env->ExceptionCheck()) {
-    return nullptr;
-  }
-
-  const InstructionSet target_instruction_set = GetInstructionSetFromString(
-      instruction_set.c_str());
-  if (target_instruction_set == InstructionSet::kNone) {
-    ScopedLocalRef<jclass> iae(env, env->FindClass("java/lang/IllegalArgumentException"));
-    std::string message(StringPrintf("Instruction set %s is invalid.", instruction_set.c_str()));
-    env->ThrowNew(iae.get(), message.c_str());
-    return nullptr;
-  }
-
-  // The API doesn't support passing a class loader context, so skip the class loader context check
-  // and assume that it's OK.
-  OatFileAssistant oat_file_assistant(filename.c_str(),
-                                      target_instruction_set,
-                                      /* context= */ nullptr,
-                                      /* load_executable= */ false);
-  return env->NewStringUTF(oat_file_assistant.GetStatusDump().c_str());
-}
-
 // Return an array specifying the optimization status of the given file.
 // The array specification is [compiler_filter, compiler_reason].
 static jobjectArray DexFile_getDexFileOptimizationStatus(JNIEnv* env,
@@ -915,8 +883,7 @@ static jobjectArray DexFile_getDexFileOutputPaths(JNIEnv* env,
       for (const OatDexFile* oat_dex_file : oat_dex_files) {
         if (DexFileLoader::GetBaseLocation(oat_dex_file->GetDexFileLocation()) ==
                 filename.c_str()) {
-          oat_filename = GetSystemImageFilename(oat_file->GetLocation().c_str(),
-                                                target_instruction_set);
+          oat_filename = oat_file->GetLocation();
           is_vdex_only = oat_file->IsBackedByVdexOnly();
           break;
         }
@@ -1039,8 +1006,6 @@ static JNINativeMethod gMethods[] = {
         DexFile, getNonProfileGuidedCompilerFilter, "(Ljava/lang/String;)Ljava/lang/String;"),
     NATIVE_METHOD(DexFile, getSafeModeCompilerFilter, "(Ljava/lang/String;)Ljava/lang/String;"),
     NATIVE_METHOD(DexFile, isBackedByOatFile, "(Ljava/lang/Object;)Z"),
-    NATIVE_METHOD(
-        DexFile, getDexFileStatus, "(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;"),
     NATIVE_METHOD(DexFile,
                   getDexFileOutputPaths,
                   "(Ljava/lang/String;Ljava/lang/String;)[Ljava/lang/String;"),
diff --git a/runtime/native/dalvik_system_VMDebug.cc b/runtime/native/dalvik_system_VMDebug.cc
index 9440610f36..c3be52106c 100644
--- a/runtime/native/dalvik_system_VMDebug.cc
+++ b/runtime/native/dalvik_system_VMDebug.cc
@@ -188,10 +188,16 @@ static void VMDebug_dumpLowOverheadTraceFdImpl(JNIEnv*, jclass, jint originalFd)
   TraceProfiler::Dump(fd);
 }
 
-static void VMDebug_startLowOverheadTraceImpl(JNIEnv*, jclass) {
+static void VMDebug_startLowOverheadTraceForAllMethodsImpl(JNIEnv*, jclass) {
   TraceProfiler::Start();
 }
 
+static void VMDebug_startLowOverheadTraceForLongRunningMethodsImpl(JNIEnv*,
+                                                                   jclass,
+                                                                   jlong traceDuration) {
+  TraceProfiler::StartTraceLongRunningMethods(traceDuration);
+}
+
 static jboolean VMDebug_isDebuggerConnected(JNIEnv*, jclass) {
   // This function will be replaced by the debugger when it's connected. See
   // external/oj-libjdwp/src/share/vmDebug.c for implementation when debugger is connected.
@@ -312,9 +318,9 @@ static jlong VMDebug_countInstancesOfClass(JNIEnv* env,
 
 static jobject VMDebug_getExecutableMethodFileOffsetsNative(JNIEnv* env,
                                                             jclass,
-                                                            jobject javaMethod) {
+                                                            jobject javaExecutable) {
   ScopedObjectAccess soa(env);
-  ObjPtr<mirror::Executable> m = soa.Decode<mirror::Executable>(javaMethod);
+  ObjPtr<mirror::Executable> m = soa.Decode<mirror::Executable>(javaExecutable);
   if (m == nullptr) {
     soa.Self()->ThrowNewExceptionF("Ljava/lang/RuntimeException;",
                                    "Could not find mirror::Executable for supplied jobject");
@@ -698,14 +704,15 @@ static JNINativeMethod gMethods[] = {
     NATIVE_METHOD(VMDebug, addApplication, "(Ljava/lang/String;)V"),
     NATIVE_METHOD(VMDebug, removeApplication, "(Ljava/lang/String;)V"),
     NATIVE_METHOD(VMDebug, setUserId, "(I)V"),
-    NATIVE_METHOD(VMDebug, startLowOverheadTraceImpl, "()V"),
+    NATIVE_METHOD(VMDebug, startLowOverheadTraceForAllMethodsImpl, "()V"),
+    NATIVE_METHOD(VMDebug, startLowOverheadTraceForLongRunningMethodsImpl, "(J)V"),
     NATIVE_METHOD(VMDebug, stopLowOverheadTraceImpl, "()V"),
     NATIVE_METHOD(VMDebug, dumpLowOverheadTraceImpl, "(Ljava/lang/String;)V"),
     NATIVE_METHOD(VMDebug, dumpLowOverheadTraceFdImpl, "(I)V"),
     NATIVE_METHOD(
         VMDebug,
         getExecutableMethodFileOffsetsNative,
-        "(Ljava/lang/reflect/Method;)Ldalvik/system/VMDebug$ExecutableMethodFileOffsets;"),
+        "(Ljava/lang/reflect/Executable;)Ldalvik/system/VMDebug$ExecutableMethodFileOffsets;"),
 };
 
 void register_dalvik_system_VMDebug(JNIEnv* env) {
diff --git a/runtime/native/dalvik_system_VMRuntime.cc b/runtime/native/dalvik_system_VMRuntime.cc
index 0e9660aaac..52a00c957c 100644
--- a/runtime/native/dalvik_system_VMRuntime.cc
+++ b/runtime/native/dalvik_system_VMRuntime.cc
@@ -22,14 +22,13 @@
 extern "C" void android_set_application_target_sdk_version(uint32_t version);
 #endif
 #include <inttypes.h>
-#include <limits>
 #include <limits.h>
-#include "nativehelper/scoped_utf_chars.h"
 
-#include <android-base/stringprintf.h>
-#include <android-base/strings.h>
+#include <limits>
 
 #include "android-base/properties.h"
+#include "android-base/stringprintf.h"
+#include "android-base/strings.h"
 #include "arch/instruction_set.h"
 #include "art_method-inl.h"
 #include "base/pointer_size.h"
@@ -51,6 +50,7 @@ extern "C" void android_set_application_target_sdk_version(uint32_t version);
 #include "jit/jit.h"
 #include "jni/java_vm_ext.h"
 #include "jni/jni_internal.h"
+#include "metrics/statsd.h"
 #include "mirror/array-alloc-inl.h"
 #include "mirror/class-inl.h"
 #include "mirror/dex_cache-inl.h"
@@ -58,6 +58,7 @@ extern "C" void android_set_application_target_sdk_version(uint32_t version);
 #include "native_util.h"
 #include "nativehelper/jni_macros.h"
 #include "nativehelper/scoped_local_ref.h"
+#include "nativehelper/scoped_utf_chars.h"
 #include "runtime.h"
 #include "scoped_fast_native_object_access-inl.h"
 #include "scoped_thread_state_change-inl.h"
@@ -244,6 +245,14 @@ static jint VMRuntime_getSdkVersionNative([[maybe_unused]] JNIEnv* env,
                                        default_sdk_version);
 }
 
+static jint VMRuntime_getIntSystemProperty([[maybe_unused]] JNIEnv* env,
+                                           [[maybe_unused]] jclass klass,
+                                           jstring attribute_name,
+                                           jint default_value) {
+  return android::base::GetIntProperty(std::string(ScopedUtfChars(env, attribute_name)),
+                                       default_value);
+}
+
 static void VMRuntime_setTargetSdkVersionNative(JNIEnv*, jobject, jint target_sdk_version) {
   // This is the target SDK version of the app we're about to run. It is intended that this a place
   // where workarounds can be enabled.
@@ -460,6 +469,10 @@ static void VMRuntime_bootCompleted([[maybe_unused]] JNIEnv* env, [[maybe_unused
   if (jit != nullptr) {
     jit->BootCompleted();
   }
+
+  if (Runtime::Current()->IsSystemServer()) {
+    metrics::SetupCallbackForDeviceStatus();
+  }
 }
 
 class ClearJitCountersVisitor : public ClassVisitor {
@@ -542,57 +555,59 @@ static jlong VMRuntime_getFullGcCount([[maybe_unused]] JNIEnv* env, [[maybe_unus
 }
 
 static JNINativeMethod gMethods[] = {
-  FAST_NATIVE_METHOD(VMRuntime, addressOf, "(Ljava/lang/Object;)J"),
-  NATIVE_METHOD(VMRuntime, bootClassPath, "()Ljava/lang/String;"),
-  NATIVE_METHOD(VMRuntime, clampGrowthLimit, "()V"),
-  NATIVE_METHOD(VMRuntime, classPath, "()Ljava/lang/String;"),
-  NATIVE_METHOD(VMRuntime, clearGrowthLimit, "()V"),
-  NATIVE_METHOD(VMRuntime, setHiddenApiExemptions, "([Ljava/lang/String;)V"),
-  NATIVE_METHOD(VMRuntime, setHiddenApiAccessLogSamplingRate, "(I)V"),
-  NATIVE_METHOD(VMRuntime, getTargetHeapUtilization, "()F"),
-  FAST_NATIVE_METHOD(VMRuntime, isNativeDebuggable, "()Z"),
-  NATIVE_METHOD(VMRuntime, isJavaDebuggable, "()Z"),
-  NATIVE_METHOD(VMRuntime, nativeSetTargetHeapUtilization, "(F)V"),
-  FAST_NATIVE_METHOD(VMRuntime, newNonMovableArray, "(Ljava/lang/Class;I)Ljava/lang/Object;"),
-  FAST_NATIVE_METHOD(VMRuntime, newUnpaddedArray, "(Ljava/lang/Class;I)Ljava/lang/Object;"),
-  NATIVE_METHOD(VMRuntime, properties, "()[Ljava/lang/String;"),
-  NATIVE_METHOD(VMRuntime, getSdkVersionNative, "(I)I"),
-  NATIVE_METHOD(VMRuntime, setTargetSdkVersionNative, "(I)V"),
-  NATIVE_METHOD(VMRuntime, setDisabledCompatChangesNative, "([J)V"),
-  NATIVE_METHOD(VMRuntime, registerNativeAllocation, "(J)V"),
-  NATIVE_METHOD(VMRuntime, registerNativeFree, "(J)V"),
-  NATIVE_METHOD(VMRuntime, getNotifyNativeInterval, "()I"),
-  NATIVE_METHOD(VMRuntime, getFinalizerTimeoutMs, "()J"),
-  NATIVE_METHOD(VMRuntime, notifyNativeAllocationsInternal, "()V"),
-  NATIVE_METHOD(VMRuntime, notifyStartupCompleted, "()V"),
-  NATIVE_METHOD(VMRuntime, registerSensitiveThread, "()V"),
-  NATIVE_METHOD(VMRuntime, requestConcurrentGC, "()V"),
-  NATIVE_METHOD(VMRuntime, requestHeapTrim, "()V"),
-  NATIVE_METHOD(VMRuntime, runHeapTasks, "()V"),
-  NATIVE_METHOD(VMRuntime, updateProcessState, "(I)V"),
-  NATIVE_METHOD(VMRuntime, startHeapTaskProcessor, "()V"),
-  NATIVE_METHOD(VMRuntime, stopHeapTaskProcessor, "()V"),
-  NATIVE_METHOD(VMRuntime, trimHeap, "()V"),
-  NATIVE_METHOD(VMRuntime, vmVersion, "()Ljava/lang/String;"),
-  NATIVE_METHOD(VMRuntime, vmLibrary, "()Ljava/lang/String;"),
-  NATIVE_METHOD(VMRuntime, vmInstructionSet, "()Ljava/lang/String;"),
-  FAST_NATIVE_METHOD(VMRuntime, is64Bit, "()Z"),
-  FAST_NATIVE_METHOD(VMRuntime, isCheckJniEnabled, "()Z"),
-  NATIVE_METHOD(VMRuntime, preloadDexCaches, "()V"),
-  NATIVE_METHOD(VMRuntime, registerAppInfo,
-      "(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;[Ljava/lang/String;I)V"),
-  NATIVE_METHOD(VMRuntime, isBootClassPathOnDisk, "(Ljava/lang/String;)Z"),
-  NATIVE_METHOD(VMRuntime, getCurrentInstructionSet, "()Ljava/lang/String;"),
-  NATIVE_METHOD(VMRuntime, setSystemDaemonThreadPriority, "()V"),
-  NATIVE_METHOD(VMRuntime, setDedupeHiddenApiWarnings, "(Z)V"),
-  NATIVE_METHOD(VMRuntime, setProcessPackageName, "(Ljava/lang/String;)V"),
-  NATIVE_METHOD(VMRuntime, setProcessDataDirectory, "(Ljava/lang/String;)V"),
-  NATIVE_METHOD(VMRuntime, bootCompleted, "()V"),
-  NATIVE_METHOD(VMRuntime, resetJitCounters, "()V"),
-  NATIVE_METHOD(VMRuntime, isValidClassLoaderContext, "(Ljava/lang/String;)Z"),
-  NATIVE_METHOD(VMRuntime, getBaseApkOptimizationInfo,
-      "()Ldalvik/system/DexFile$OptimizationInfo;"),
-  NATIVE_METHOD(VMRuntime, getFullGcCount, "()J"),
+    FAST_NATIVE_METHOD(VMRuntime, addressOf, "(Ljava/lang/Object;)J"),
+    NATIVE_METHOD(VMRuntime, bootClassPath, "()Ljava/lang/String;"),
+    NATIVE_METHOD(VMRuntime, clampGrowthLimit, "()V"),
+    NATIVE_METHOD(VMRuntime, classPath, "()Ljava/lang/String;"),
+    NATIVE_METHOD(VMRuntime, clearGrowthLimit, "()V"),
+    NATIVE_METHOD(VMRuntime, setHiddenApiExemptions, "([Ljava/lang/String;)V"),
+    NATIVE_METHOD(VMRuntime, setHiddenApiAccessLogSamplingRate, "(I)V"),
+    NATIVE_METHOD(VMRuntime, getTargetHeapUtilization, "()F"),
+    FAST_NATIVE_METHOD(VMRuntime, isNativeDebuggable, "()Z"),
+    NATIVE_METHOD(VMRuntime, isJavaDebuggable, "()Z"),
+    NATIVE_METHOD(VMRuntime, nativeSetTargetHeapUtilization, "(F)V"),
+    FAST_NATIVE_METHOD(VMRuntime, newNonMovableArray, "(Ljava/lang/Class;I)Ljava/lang/Object;"),
+    FAST_NATIVE_METHOD(VMRuntime, newUnpaddedArray, "(Ljava/lang/Class;I)Ljava/lang/Object;"),
+    NATIVE_METHOD(VMRuntime, properties, "()[Ljava/lang/String;"),
+    NATIVE_METHOD(VMRuntime, getSdkVersionNative, "(I)I"),
+    FAST_NATIVE_METHOD(VMRuntime, getIntSystemProperty, "(Ljava/lang/String;I)I"),
+    NATIVE_METHOD(VMRuntime, setTargetSdkVersionNative, "(I)V"),
+    NATIVE_METHOD(VMRuntime, setDisabledCompatChangesNative, "([J)V"),
+    NATIVE_METHOD(VMRuntime, registerNativeAllocation, "(J)V"),
+    NATIVE_METHOD(VMRuntime, registerNativeFree, "(J)V"),
+    NATIVE_METHOD(VMRuntime, getNotifyNativeInterval, "()I"),
+    NATIVE_METHOD(VMRuntime, getFinalizerTimeoutMs, "()J"),
+    NATIVE_METHOD(VMRuntime, notifyNativeAllocationsInternal, "()V"),
+    NATIVE_METHOD(VMRuntime, notifyStartupCompleted, "()V"),
+    NATIVE_METHOD(VMRuntime, registerSensitiveThread, "()V"),
+    NATIVE_METHOD(VMRuntime, requestConcurrentGC, "()V"),
+    NATIVE_METHOD(VMRuntime, requestHeapTrim, "()V"),
+    NATIVE_METHOD(VMRuntime, runHeapTasks, "()V"),
+    NATIVE_METHOD(VMRuntime, updateProcessState, "(I)V"),
+    NATIVE_METHOD(VMRuntime, startHeapTaskProcessor, "()V"),
+    NATIVE_METHOD(VMRuntime, stopHeapTaskProcessor, "()V"),
+    NATIVE_METHOD(VMRuntime, trimHeap, "()V"),
+    NATIVE_METHOD(VMRuntime, vmVersion, "()Ljava/lang/String;"),
+    NATIVE_METHOD(VMRuntime, vmLibrary, "()Ljava/lang/String;"),
+    NATIVE_METHOD(VMRuntime, vmInstructionSet, "()Ljava/lang/String;"),
+    FAST_NATIVE_METHOD(VMRuntime, is64Bit, "()Z"),
+    FAST_NATIVE_METHOD(VMRuntime, isCheckJniEnabled, "()Z"),
+    NATIVE_METHOD(VMRuntime, preloadDexCaches, "()V"),
+    NATIVE_METHOD(VMRuntime,
+                  registerAppInfo,
+                  "(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;[Ljava/lang/String;I)V"),
+    NATIVE_METHOD(VMRuntime, isBootClassPathOnDisk, "(Ljava/lang/String;)Z"),
+    NATIVE_METHOD(VMRuntime, getCurrentInstructionSet, "()Ljava/lang/String;"),
+    NATIVE_METHOD(VMRuntime, setSystemDaemonThreadPriority, "()V"),
+    NATIVE_METHOD(VMRuntime, setDedupeHiddenApiWarnings, "(Z)V"),
+    NATIVE_METHOD(VMRuntime, setProcessPackageName, "(Ljava/lang/String;)V"),
+    NATIVE_METHOD(VMRuntime, setProcessDataDirectory, "(Ljava/lang/String;)V"),
+    NATIVE_METHOD(VMRuntime, bootCompleted, "()V"),
+    NATIVE_METHOD(VMRuntime, resetJitCounters, "()V"),
+    NATIVE_METHOD(VMRuntime, isValidClassLoaderContext, "(Ljava/lang/String;)Z"),
+    NATIVE_METHOD(
+        VMRuntime, getBaseApkOptimizationInfo, "()Ldalvik/system/DexFile$OptimizationInfo;"),
+    NATIVE_METHOD(VMRuntime, getFullGcCount, "()J"),
 };
 
 void register_dalvik_system_VMRuntime(JNIEnv* env) {
diff --git a/runtime/native/java_lang_Class.cc b/runtime/native/java_lang_Class.cc
index c31c9790bd..efd52918af 100644
--- a/runtime/native/java_lang_Class.cc
+++ b/runtime/native/java_lang_Class.cc
@@ -107,7 +107,7 @@ static jclass Class_classForName(JNIEnv* env, jclass, jstring javaName, jboolean
     return nullptr;
   }
 
-  std::string descriptor(DotToDescriptor(name.c_str()));
+  std::string descriptor = DotToDescriptor(name);
   Handle<mirror::ClassLoader> class_loader(
       hs.NewHandle(soa.Decode<mirror::ClassLoader>(javaLoader)));
   ClassLinker* class_linker = Runtime::Current()->GetClassLinker();
@@ -290,11 +290,7 @@ ALWAYS_INLINE static inline ObjPtr<mirror::Field> GetDeclaredField(Thread* self,
     ThrowRuntimeException("Obsolete Object!");
     return nullptr;
   }
-  ArtField* art_field = FindFieldByName(name, c->GetIFieldsPtr());
-  if (art_field != nullptr) {
-    return mirror::Field::CreateFromArtField(self, art_field, true);
-  }
-  art_field = FindFieldByName(name, c->GetSFieldsPtr());
+  ArtField* art_field = FindFieldByName(name, c->GetFieldsPtr());
   if (art_field != nullptr) {
     return mirror::Field::CreateFromArtField(self, art_field, true);
   }
diff --git a/runtime/native/java_lang_VMClassLoader.cc b/runtime/native/java_lang_VMClassLoader.cc
index eeae51c5e7..c723a1291a 100644
--- a/runtime/native/java_lang_VMClassLoader.cc
+++ b/runtime/native/java_lang_VMClassLoader.cc
@@ -84,7 +84,7 @@ static jclass VMClassLoader_findLoadedClass(JNIEnv* env, jclass, jobject javaLoa
   ClassLinker* cl = Runtime::Current()->GetClassLinker();
 
   // Compute hash once.
-  std::string descriptor(DotToDescriptor(name.c_str()));
+  std::string descriptor = DotToDescriptor(name);
   const size_t descriptor_hash = ComputeModifiedUtf8Hash(descriptor);
 
   ObjPtr<mirror::Class> c = VMClassLoader::LookupClass(cl,
diff --git a/runtime/native/jdk_internal_misc_Unsafe.cc b/runtime/native/jdk_internal_misc_Unsafe.cc
index e9d6f3a83f..a74dc37ef4 100644
--- a/runtime/native/jdk_internal_misc_Unsafe.cc
+++ b/runtime/native/jdk_internal_misc_Unsafe.cc
@@ -22,6 +22,7 @@
 #include <cstring>
 #include <atomic>
 
+#include "jni.h"
 #include "nativehelper/jni_macros.h"
 
 #include "base/quasi_atomic.h"
@@ -607,6 +608,10 @@ static void Unsafe_unpark(JNIEnv* env, jobject, jobject jthread) {
   }
 }
 
+static jobject Unsafe_allocateInstance(JNIEnv* env, jobject, jclass cls) {
+  return env->AllocObject(cls);
+}
+
 static JNINativeMethod gMethods[] = {
     FAST_NATIVE_METHOD(Unsafe, compareAndSwapInt, "(Ljava/lang/Object;JII)Z"),
     FAST_NATIVE_METHOD(Unsafe, compareAndSwapLong, "(Ljava/lang/Object;JJJ)Z"),
@@ -654,6 +659,7 @@ static JNINativeMethod gMethods[] = {
     FAST_NATIVE_METHOD(Unsafe, setMemory, "(JJB)V"),
     FAST_NATIVE_METHOD(Unsafe, copyMemory0, "(Ljava/lang/Object;JLjava/lang/Object;JJ)V"),
     FAST_NATIVE_METHOD(Unsafe, getBoolean, "(Ljava/lang/Object;J)Z"),
+    NATIVE_METHOD(Unsafe, allocateInstance, "(Ljava/lang/Class;)Ljava/lang/Object;"),
 
     FAST_NATIVE_METHOD(Unsafe, getByte, "(Ljava/lang/Object;J)B"),
     FAST_NATIVE_METHOD(Unsafe, getChar, "(Ljava/lang/Object;J)C"),
diff --git a/runtime/native_gc_triggering.md b/runtime/native_gc_triggering.md
index 8adbb94bf3..1dc9e5d675 100644
--- a/runtime/native_gc_triggering.md
+++ b/runtime/native_gc_triggering.md
@@ -45,6 +45,15 @@ difficult to approximate, even to within an order of magnitude.
 The triggering heuristic
 ------------------------
 
+We compute the total amount of native memory allocated as the sum of
+
+1. memory allocated, but not yet deallocated, by the system memory allocator, as reported by
+   `mallinfo()`, and
+
+2. the number of bytes registered via `VMRuntime.registerNativeAllocation()` and not yet
+   unregistered via `VMRuntime.registerNativeFree()`. This includes non-malloc-allocated objects
+   allocated via `NativeAllocationRegistry`.
+
 Though we use mallinfo() to track native allocation, this call itself can be expensive, and thus
 we perform this check fairly rarely. More precisely, we do so only after the application has
 called `NativeAllocationRegistry.registerNativeAllocation()` a certain number of times or
@@ -59,7 +68,7 @@ The actual computation for triggering a native-allocation-GC is performed by
 1. An adjusted heap size for GC triggering. This consists of the Java heap size at which we would
    normally trigger a GC plus an allowance for native heap size. This allowance currently consists
    of one half (background processes) or three halves (foreground processes) of
-   `NativeAllocationGcWatermark()`. The latter is HeapMaxFree (typically 32MB) plus 1/8 of the
+   `NativeAllocationGcWatermark()`. The latter is `HeapMaxFree` (typically 32MB) plus 1/8 of the
    currently targeted heap size. For a foreground process, this allowance would typically be in
    the 50-100 MB range for something other than a low-end device.
 
@@ -88,3 +97,12 @@ is solid with scudo and jemalloc on Android, and minimally usable for testing el
 
 (Some of this assumes typical current (May 2024) configuration constants, and may need to be
 updated.)
+
+Workaround for excessive native GCs
+-----------------------------------
+
+If an application routinely allocates and deallocates large amounts of memory without requiring
+the GC, it may be preferable to bypass the system allocator, for example by using mmap directly.
+This will avoid unnecessary GC triggering. This is clearly much more convenient for a small number
+of large allocations than for small allocations. It could also be addressed with a new ART API,
+but so far we have not found this necessary.
diff --git a/runtime/noop_compiler_callbacks.h b/runtime/noop_compiler_callbacks.h
index 2a4a45af17..9432c539a5 100644
--- a/runtime/noop_compiler_callbacks.h
+++ b/runtime/noop_compiler_callbacks.h
@@ -35,6 +35,7 @@ class NoopCompilerCallbacks final : public CompilerCallbacks {
 
   void AddUncompilableMethod([[maybe_unused]] MethodReference ref) override {}
   void AddUncompilableClass([[maybe_unused]] ClassReference ref) override {}
+  bool IsUncompilableMethod([[maybe_unused]] MethodReference ref) override { return false; }
   void ClassRejected([[maybe_unused]] ClassReference ref) override {}
 
   verifier::VerifierDeps* GetVerifierDeps() const override { return nullptr; }
diff --git a/runtime/nterp_helpers.cc b/runtime/nterp_helpers.cc
index 0a306e2f56..90edb54b3f 100644
--- a/runtime/nterp_helpers.cc
+++ b/runtime/nterp_helpers.cc
@@ -236,9 +236,6 @@ bool CanMethodUseNterp(ArtMethod* method, InstructionSet isa) {
       method->IsProxyMethod()) {
     return false;
   }
-  if (isa == InstructionSet::kRiscv64 && method->GetDexFile()->IsCompactDexFile()) {
-    return false;  // Riscv64 nterp does not support compact dex yet.
-  }
   // There is no need to add the alignment padding size for comparison with aligned limit.
   size_t frame_size_without_padding = NterpGetFrameSizeWithoutPadding(method, isa);
   DCHECK_EQ(NterpGetFrameSize(method, isa), RoundUp(frame_size_without_padding, kStackAlignment));
diff --git a/runtime/oat/elf_file.cc b/runtime/oat/elf_file.cc
index 5fb8053856..6db09f2d1e 100644
--- a/runtime/oat/elf_file.cc
+++ b/runtime/oat/elf_file.cc
@@ -21,14 +21,14 @@
 #include <sys/types.h>
 #include <unistd.h>
 
-#include "android-base/stringprintf.h"
-#include "android-base/strings.h"
+#include <cstddef>
+#include <memory>
 
+#include "android-base/stringprintf.h"
 #include "arch/instruction_set.h"
-#include "base/leb128.h"
-#include "base/stl_util.h"
+#include "base/casts.h"
+#include "base/os.h"
 #include "base/unix_file/fd_file.h"
-#include "base/utils.h"
 #include "elf/elf_utils.h"
 #include "elf_file_impl.h"
 
@@ -36,318 +36,90 @@ namespace art HIDDEN {
 
 using android::base::StringPrintf;
 
-template <typename ElfTypes>
-ElfFileImpl<ElfTypes>::ElfFileImpl(File* file, bool writable, bool program_header_only)
-  : writable_(writable),
-    program_header_only_(program_header_only),
-    header_(nullptr),
-    base_address_(nullptr),
-    program_headers_start_(nullptr),
-    section_headers_start_(nullptr),
-    dynamic_program_header_(nullptr),
-    dynamic_section_start_(nullptr),
-    symtab_section_start_(nullptr),
-    dynsym_section_start_(nullptr),
-    strtab_section_start_(nullptr),
-    dynstr_section_start_(nullptr),
-    hash_section_start_(nullptr),
-    symtab_symbol_table_(nullptr),
-    dynsym_symbol_table_(nullptr) {
-  CHECK(file != nullptr);
-}
-
 template <typename ElfTypes>
 ElfFileImpl<ElfTypes>* ElfFileImpl<ElfTypes>::Open(File* file,
-                                                   bool writable,
-                                                   bool program_header_only,
+                                                   off_t start,
+                                                   size_t file_length,
+                                                   const std::string& file_location,
                                                    bool low_4gb,
                                                    std::string* error_msg) {
   std::unique_ptr<ElfFileImpl<ElfTypes>> elf_file(
-      new ElfFileImpl<ElfTypes>(file, writable, program_header_only));
-  int prot;
-  int flags;
-  if (writable) {
-    prot = PROT_READ | PROT_WRITE;
-    flags = MAP_SHARED;
-  } else {
-    prot = PROT_READ;
-    flags = MAP_PRIVATE;
-  }
-  if (!elf_file->Setup(file, prot, flags, low_4gb, error_msg)) {
+      new ElfFileImpl<ElfTypes>(file, start, file_length, file_location));
+  if (!elf_file->Setup(low_4gb, error_msg)) {
     return nullptr;
   }
   return elf_file.release();
 }
 
 template <typename ElfTypes>
-ElfFileImpl<ElfTypes>* ElfFileImpl<ElfTypes>::Open(File* file,
-                                                   int prot,
-                                                   int flags,
-                                                   bool low_4gb,
-                                                   std::string* error_msg) {
-  std::unique_ptr<ElfFileImpl<ElfTypes>> elf_file(
-      new ElfFileImpl<ElfTypes>(file, (prot & PROT_WRITE) != 0, /* program_header_only= */ false));
-  if (!elf_file->Setup(file, prot, flags, low_4gb, error_msg)) {
-    return nullptr;
+bool ElfFileImpl<ElfTypes>::Setup(bool low_4gb, std::string* error_msg) {
+  if (file_length_ < sizeof(Elf_Ehdr)) {
+    *error_msg = StringPrintf(
+        "File size of %zd bytes not large enough to contain ELF header of "
+        "%zd bytes: '%s'",
+        file_length_,
+        sizeof(Elf_Ehdr),
+        file_location_.c_str());
+    return false;
   }
-  return elf_file.release();
-}
 
-template <typename ElfTypes>
-bool ElfFileImpl<ElfTypes>::Setup(File* file,
-                                  int prot,
-                                  int flags,
-                                  bool low_4gb,
-                                  std::string* error_msg) {
-  int64_t temp_file_length = file->GetLength();
-  if (temp_file_length < 0) {
-    errno = -temp_file_length;
-    *error_msg = StringPrintf("Failed to get length of file: '%s' fd=%d: %s",
-                              file->GetPath().c_str(), file->Fd(), strerror(errno));
+  int prot = PROT_READ;
+  int flags = MAP_PRIVATE;
+
+  // first just map ELF header to get program header size information
+  size_t elf_header_size = sizeof(Elf_Ehdr);
+  if (!SetMap(MemMap::MapFile(elf_header_size,
+                              prot,
+                              flags,
+                              file_->Fd(),
+                              start_,
+                              low_4gb,
+                              file_location_.c_str(),
+                              error_msg),
+              error_msg)) {
     return false;
   }
-  size_t file_length = static_cast<size_t>(temp_file_length);
-  if (file_length < sizeof(Elf_Ehdr)) {
-    *error_msg = StringPrintf("File size of %zd bytes not large enough to contain ELF header of "
-                              "%zd bytes: '%s'", file_length, sizeof(Elf_Ehdr),
-                              file->GetPath().c_str());
+  // then remap to cover program header
+  size_t program_header_size = header_->e_phoff + (header_->e_phentsize * header_->e_phnum);
+  if (file_length_ < program_header_size) {
+    *error_msg = StringPrintf(
+        "File size of %zd bytes not large enough to contain ELF program header of %zd bytes: '%s'",
+        file_length_,
+        sizeof(Elf_Ehdr),
+        file_location_.c_str());
     return false;
   }
-
-  if (program_header_only_) {
-    // first just map ELF header to get program header size information
-    size_t elf_header_size = sizeof(Elf_Ehdr);
-    if (!SetMap(file,
-                MemMap::MapFile(elf_header_size,
-                                prot,
-                                flags,
-                                file->Fd(),
-                                0,
-                                low_4gb,
-                                file->GetPath().c_str(),
-                                error_msg),
-                error_msg)) {
-      return false;
-    }
-    // then remap to cover program header
-    size_t program_header_size = header_->e_phoff + (header_->e_phentsize * header_->e_phnum);
-    if (file_length < program_header_size) {
-      *error_msg = StringPrintf("File size of %zd bytes not large enough to contain ELF program "
-                                "header of %zd bytes: '%s'", file_length,
-                                sizeof(Elf_Ehdr), file->GetPath().c_str());
-      return false;
-    }
-    if (!SetMap(file,
-                MemMap::MapFile(program_header_size,
-                                prot,
-                                flags,
-                                file->Fd(),
-                                0,
-                                low_4gb,
-                                file->GetPath().c_str(),
-                                error_msg),
-                error_msg)) {
-      *error_msg = StringPrintf("Failed to map ELF program headers: %s", error_msg->c_str());
-      return false;
-    }
-  } else {
-    // otherwise map entire file
-    if (!SetMap(file,
-                MemMap::MapFile(file->GetLength(),
-                                prot,
-                                flags,
-                                file->Fd(),
-                                0,
-                                low_4gb,
-                                file->GetPath().c_str(),
-                                error_msg),
-                error_msg)) {
-      *error_msg = StringPrintf("Failed to map ELF file: %s", error_msg->c_str());
-      return false;
-    }
-  }
-
-  if (program_header_only_) {
-    program_headers_start_ = Begin() + GetHeader().e_phoff;
-  } else {
-    if (!CheckAndSet(GetHeader().e_phoff, "program headers", &program_headers_start_, error_msg)) {
-      return false;
-    }
-
-    // Setup section headers.
-    if (!CheckAndSet(GetHeader().e_shoff, "section headers", &section_headers_start_, error_msg)) {
-      return false;
-    }
-
-    // Find shstrtab.
-    Elf_Shdr* shstrtab_section_header = GetSectionNameStringSection();
-    if (shstrtab_section_header == nullptr) {
-      *error_msg = StringPrintf("Failed to find shstrtab section header in ELF file: '%s'",
-                                file->GetPath().c_str());
-      return false;
-    }
-
-    // Find .dynamic section info from program header
-    dynamic_program_header_ = FindProgamHeaderByType(PT_DYNAMIC);
-    if (dynamic_program_header_ == nullptr) {
-      *error_msg = StringPrintf("Failed to find PT_DYNAMIC program header in ELF file: '%s'",
-                                file->GetPath().c_str());
-      return false;
-    }
-
-    if (!CheckAndSet(GetDynamicProgramHeader().p_offset, "dynamic section",
-                     reinterpret_cast<uint8_t**>(&dynamic_section_start_), error_msg)) {
-      return false;
-    }
-
-    // Find other sections from section headers
-    for (Elf_Word i = 0; i < GetSectionHeaderNum(); i++) {
-      Elf_Shdr* section_header = GetSectionHeader(i);
-      if (section_header == nullptr) {
-        *error_msg = StringPrintf("Failed to find section header for section %d in ELF file: '%s'",
-                                  i, file->GetPath().c_str());
-        return false;
-      }
-      switch (section_header->sh_type) {
-        case SHT_SYMTAB: {
-          if (!CheckAndSet(section_header->sh_offset, "symtab",
-                           reinterpret_cast<uint8_t**>(&symtab_section_start_), error_msg)) {
-            return false;
-          }
-          break;
-        }
-        case SHT_DYNSYM: {
-          if (!CheckAndSet(section_header->sh_offset, "dynsym",
-                           reinterpret_cast<uint8_t**>(&dynsym_section_start_), error_msg)) {
-            return false;
-          }
-          break;
-        }
-        case SHT_STRTAB: {
-          // TODO: base these off of sh_link from .symtab and .dynsym above
-          if ((section_header->sh_flags & SHF_ALLOC) != 0) {
-            // Check that this is named ".dynstr" and ignore otherwise.
-            const char* header_name = GetString(*shstrtab_section_header, section_header->sh_name);
-            if (strncmp(".dynstr", header_name, 8) == 0) {
-              if (!CheckAndSet(section_header->sh_offset, "dynstr",
-                               reinterpret_cast<uint8_t**>(&dynstr_section_start_), error_msg)) {
-                return false;
-              }
-            }
-          } else {
-            // Check that this is named ".strtab" and ignore otherwise.
-            const char* header_name = GetString(*shstrtab_section_header, section_header->sh_name);
-            if (strncmp(".strtab", header_name, 8) == 0) {
-              if (!CheckAndSet(section_header->sh_offset, "strtab",
-                               reinterpret_cast<uint8_t**>(&strtab_section_start_), error_msg)) {
-                return false;
-              }
-            }
-          }
-          break;
-        }
-        case SHT_DYNAMIC: {
-          if (reinterpret_cast<uint8_t*>(dynamic_section_start_) !=
-              Begin() + section_header->sh_offset) {
-            LOG(WARNING) << "Failed to find matching SHT_DYNAMIC for PT_DYNAMIC in "
-                         << file->GetPath() << ": " << std::hex
-                         << reinterpret_cast<void*>(dynamic_section_start_)
-                         << " != " << reinterpret_cast<void*>(Begin() + section_header->sh_offset);
-            return false;
-          }
-          break;
-        }
-        case SHT_HASH: {
-          if (!CheckAndSet(section_header->sh_offset, "hash section",
-                           reinterpret_cast<uint8_t**>(&hash_section_start_), error_msg)) {
-            return false;
-          }
-          break;
-        }
-      }
-    }
-
-    // Check for the existence of some sections.
-    if (!CheckSectionsExist(file, error_msg)) {
-      return false;
-    }
-  }
-
-  return true;
-}
-
-template <typename ElfTypes>
-ElfFileImpl<ElfTypes>::~ElfFileImpl() {
-  delete symtab_symbol_table_;
-  delete dynsym_symbol_table_;
-}
-
-template <typename ElfTypes>
-bool ElfFileImpl<ElfTypes>::CheckAndSet(Elf32_Off offset, const char* label,
-                                        uint8_t** target, std::string* error_msg) {
-  if (Begin() + offset >= End()) {
-    *error_msg = StringPrintf("Offset %d is out of range for %s in ELF file: '%s'", offset, label,
-                              file_path_.c_str());
+  if (!SetMap(MemMap::MapFile(program_header_size,
+                              prot,
+                              flags,
+                              file_->Fd(),
+                              start_,
+                              low_4gb,
+                              file_location_.c_str(),
+                              error_msg),
+              error_msg)) {
+    *error_msg = StringPrintf("Failed to map ELF program headers: %s", error_msg->c_str());
     return false;
   }
-  *target = Begin() + offset;
-  return true;
-}
-
-template <typename ElfTypes>
-bool ElfFileImpl<ElfTypes>::CheckSectionsLinked(const uint8_t* source,
-                                                const uint8_t* target) const {
-  // Only works in whole-program mode, as we need to iterate over the sections.
-  // Note that we normally can't search by type, as duplicates are allowed for most section types.
-  if (program_header_only_) {
-    return true;
-  }
 
-  Elf_Shdr* source_section = nullptr;
-  Elf_Word target_index = 0;
-  bool target_found = false;
-  for (Elf_Word i = 0; i < GetSectionHeaderNum(); i++) {
-    Elf_Shdr* section_header = GetSectionHeader(i);
-
-    if (Begin() + section_header->sh_offset == source) {
-      // Found the source.
-      source_section = section_header;
-      if (target_index) {
-        break;
-      }
-    } else if (Begin() + section_header->sh_offset == target) {
-      target_index = i;
-      target_found = true;
-      if (source_section != nullptr) {
-        break;
-      }
-    }
-  }
+  program_headers_start_ = Begin() + GetHeader().e_phoff;
 
-  return target_found && source_section != nullptr && source_section->sh_link == target_index;
+  return true;
 }
 
 template <typename ElfTypes>
-  bool ElfFileImpl<ElfTypes>::CheckSectionsExist(File* file, std::string* error_msg) const {
-  if (!program_header_only_) {
-    // If in full mode, need section headers.
-    if (section_headers_start_ == nullptr) {
-      *error_msg = StringPrintf("No section headers in ELF file: '%s'", file->GetPath().c_str());
-      return false;
-    }
-  }
-
+bool ElfFileImpl<ElfTypes>::CheckSectionsExist(std::string* error_msg) const {
   // This is redundant, but defensive.
   if (dynamic_program_header_ == nullptr) {
     *error_msg = StringPrintf("Failed to find PT_DYNAMIC program header in ELF file: '%s'",
-                              file->GetPath().c_str());
+                              file_location_.c_str());
     return false;
   }
 
   // Need a dynamic section. This is redundant, but defensive.
   if (dynamic_section_start_ == nullptr) {
-    *error_msg = StringPrintf("Failed to find dynamic section in ELF file: '%s'",
-                              file->GetPath().c_str());
+    *error_msg =
+        StringPrintf("Failed to find dynamic section in ELF file: '%s'", file_location_.c_str());
     return false;
   }
 
@@ -356,71 +128,51 @@ template <typename ElfTypes>
   if (symtab_section_start_ != nullptr) {
     // When there's a symtab, there should be a strtab.
     if (strtab_section_start_ == nullptr) {
-      *error_msg = StringPrintf("No strtab for symtab in ELF file: '%s'", file->GetPath().c_str());
-      return false;
-    }
-
-    // The symtab should link to the strtab.
-    if (!CheckSectionsLinked(reinterpret_cast<const uint8_t*>(symtab_section_start_),
-                             reinterpret_cast<const uint8_t*>(strtab_section_start_))) {
-      *error_msg = StringPrintf("Symtab is not linked to the strtab in ELF file: '%s'",
-                                file->GetPath().c_str());
+      *error_msg = StringPrintf("No strtab for symtab in ELF file: '%s'", file_location_.c_str());
       return false;
     }
   }
 
   // We always need a dynstr & dynsym.
   if (dynstr_section_start_ == nullptr) {
-    *error_msg = StringPrintf("No dynstr in ELF file: '%s'", file->GetPath().c_str());
+    *error_msg = StringPrintf("No dynstr in ELF file: '%s'", file_location_.c_str());
     return false;
   }
   if (dynsym_section_start_ == nullptr) {
-    *error_msg = StringPrintf("No dynsym in ELF file: '%s'", file->GetPath().c_str());
+    *error_msg = StringPrintf("No dynsym in ELF file: '%s'", file_location_.c_str());
     return false;
   }
 
   // Need a hash section for dynamic symbol lookup.
   if (hash_section_start_ == nullptr) {
-    *error_msg = StringPrintf("Failed to find hash section in ELF file: '%s'",
-                              file->GetPath().c_str());
+    *error_msg =
+        StringPrintf("Failed to find hash section in ELF file: '%s'", file_location_.c_str());
     return false;
   }
 
-  // And the hash section should be linking to the dynsym.
-  if (!CheckSectionsLinked(reinterpret_cast<const uint8_t*>(hash_section_start_),
-                           reinterpret_cast<const uint8_t*>(dynsym_section_start_))) {
-    *error_msg = StringPrintf("Hash section is not linked to the dynstr in ELF file: '%s'",
-                              file->GetPath().c_str());
+  // We'd also like to confirm a shstrtab. This is usually the last in an oat file, and a good
+  // indicator of whether writing was successful (or the process crashed and left garbage).
+  // It might not be mapped, but we can compare against the file size.
+  size_t offset = GetHeader().e_shoff + (GetHeader().e_shstrndx * GetHeader().e_shentsize);
+  if (offset >= file_length_) {
+    *error_msg =
+        StringPrintf("Shstrtab is not in the mapped ELF file: '%s'", file_location_.c_str());
     return false;
   }
 
-  // We'd also like to confirm a shstrtab in program_header_only_ mode (else Open() does this for
-  // us). This is usually the last in an oat file, and a good indicator of whether writing was
-  // successful (or the process crashed and left garbage).
-  if (program_header_only_) {
-    // It might not be mapped, but we can compare against the file size.
-    int64_t offset = static_cast<int64_t>(GetHeader().e_shoff +
-                                          (GetHeader().e_shstrndx * GetHeader().e_shentsize));
-    if (offset >= file->GetLength()) {
-      *error_msg = StringPrintf("Shstrtab is not in the mapped ELF file: '%s'",
-                                file->GetPath().c_str());
-      return false;
-    }
-  }
-
   return true;
 }
 
 template <typename ElfTypes>
-bool ElfFileImpl<ElfTypes>::SetMap(File* file, MemMap&& map, std::string* error_msg) {
+bool ElfFileImpl<ElfTypes>::SetMap(MemMap&& map, std::string* error_msg) {
   if (!map.IsValid()) {
     // MemMap::Open should have already set an error.
     DCHECK(!error_msg->empty());
     return false;
   }
   map_ = std::move(map);
-  CHECK(map_.IsValid()) << file->GetPath();
-  CHECK(map_.Begin() != nullptr) << file->GetPath();
+  CHECK(map_.IsValid()) << file_location_;
+  CHECK(map_.Begin() != nullptr) << file_location_;
 
   header_ = reinterpret_cast<Elf_Ehdr*>(map_.Begin());
   if ((ELFMAG0 != header_->e_ident[EI_MAG0])
@@ -428,8 +180,11 @@ bool ElfFileImpl<ElfTypes>::SetMap(File* file, MemMap&& map, std::string* error_
       || (ELFMAG2 != header_->e_ident[EI_MAG2])
       || (ELFMAG3 != header_->e_ident[EI_MAG3])) {
     *error_msg = StringPrintf("Failed to find ELF magic value %d %d %d %d in %s, found %d %d %d %d",
-                              ELFMAG0, ELFMAG1, ELFMAG2, ELFMAG3,
-                              file->GetPath().c_str(),
+                              ELFMAG0,
+                              ELFMAG1,
+                              ELFMAG2,
+                              ELFMAG3,
+                              file_location_.c_str(),
                               header_->e_ident[EI_MAG0],
                               header_->e_ident[EI_MAG1],
                               header_->e_ident[EI_MAG2],
@@ -440,109 +195,92 @@ bool ElfFileImpl<ElfTypes>::SetMap(File* file, MemMap&& map, std::string* error_
   if (elf_class != header_->e_ident[EI_CLASS]) {
     *error_msg = StringPrintf("Failed to find expected EI_CLASS value %d in %s, found %d",
                               elf_class,
-                              file->GetPath().c_str(),
+                              file_location_.c_str(),
                               header_->e_ident[EI_CLASS]);
     return false;
   }
   if (ELFDATA2LSB != header_->e_ident[EI_DATA]) {
     *error_msg = StringPrintf("Failed to find expected EI_DATA value %d in %s, found %d",
                               ELFDATA2LSB,
-                              file->GetPath().c_str(),
+                              file_location_.c_str(),
                               header_->e_ident[EI_CLASS]);
     return false;
   }
   if (EV_CURRENT != header_->e_ident[EI_VERSION]) {
     *error_msg = StringPrintf("Failed to find expected EI_VERSION value %d in %s, found %d",
                               EV_CURRENT,
-                              file->GetPath().c_str(),
+                              file_location_.c_str(),
                               header_->e_ident[EI_CLASS]);
     return false;
   }
   if (ET_DYN != header_->e_type) {
     *error_msg = StringPrintf("Failed to find expected e_type value %d in %s, found %d",
                               ET_DYN,
-                              file->GetPath().c_str(),
+                              file_location_.c_str(),
                               header_->e_type);
     return false;
   }
   if (EV_CURRENT != header_->e_version) {
     *error_msg = StringPrintf("Failed to find expected e_version value %d in %s, found %d",
                               EV_CURRENT,
-                              file->GetPath().c_str(),
+                              file_location_.c_str(),
                               header_->e_version);
     return false;
   }
   if (0 != header_->e_entry) {
     *error_msg = StringPrintf("Failed to find expected e_entry value %d in %s, found %d",
                               0,
-                              file->GetPath().c_str(),
+                              file_location_.c_str(),
                               static_cast<int32_t>(header_->e_entry));
     return false;
   }
   if (0 == header_->e_phoff) {
-    *error_msg = StringPrintf("Failed to find non-zero e_phoff value in %s",
-                              file->GetPath().c_str());
+    *error_msg =
+        StringPrintf("Failed to find non-zero e_phoff value in %s", file_location_.c_str());
     return false;
   }
   if (0 == header_->e_shoff) {
-    *error_msg = StringPrintf("Failed to find non-zero e_shoff value in %s",
-                              file->GetPath().c_str());
+    *error_msg =
+        StringPrintf("Failed to find non-zero e_shoff value in %s", file_location_.c_str());
     return false;
   }
   if (0 == header_->e_ehsize) {
-    *error_msg = StringPrintf("Failed to find non-zero e_ehsize value in %s",
-                              file->GetPath().c_str());
+    *error_msg =
+        StringPrintf("Failed to find non-zero e_ehsize value in %s", file_location_.c_str());
     return false;
   }
   if (0 == header_->e_phentsize) {
-    *error_msg = StringPrintf("Failed to find non-zero e_phentsize value in %s",
-                              file->GetPath().c_str());
+    *error_msg =
+        StringPrintf("Failed to find non-zero e_phentsize value in %s", file_location_.c_str());
     return false;
   }
   if (0 == header_->e_phnum) {
-    *error_msg = StringPrintf("Failed to find non-zero e_phnum value in %s",
-                              file->GetPath().c_str());
+    *error_msg =
+        StringPrintf("Failed to find non-zero e_phnum value in %s", file_location_.c_str());
     return false;
   }
   if (0 == header_->e_shentsize) {
-    *error_msg = StringPrintf("Failed to find non-zero e_shentsize value in %s",
-                              file->GetPath().c_str());
+    *error_msg =
+        StringPrintf("Failed to find non-zero e_shentsize value in %s", file_location_.c_str());
     return false;
   }
   if (0 == header_->e_shnum) {
-    *error_msg = StringPrintf("Failed to find non-zero e_shnum value in %s",
-                              file->GetPath().c_str());
+    *error_msg =
+        StringPrintf("Failed to find non-zero e_shnum value in %s", file_location_.c_str());
     return false;
   }
   if (0 == header_->e_shstrndx) {
-    *error_msg = StringPrintf("Failed to find non-zero e_shstrndx value in %s",
-                              file->GetPath().c_str());
+    *error_msg =
+        StringPrintf("Failed to find non-zero e_shstrndx value in %s", file_location_.c_str());
     return false;
   }
   if (header_->e_shstrndx >= header_->e_shnum) {
     *error_msg = StringPrintf("Failed to find e_shnum value %d less than %d in %s",
                               header_->e_shstrndx,
                               header_->e_shnum,
-                              file->GetPath().c_str());
+                              file_location_.c_str());
     return false;
   }
-
-  if (!program_header_only_) {
-    if (header_->e_phoff >= Size()) {
-      *error_msg = StringPrintf("Failed to find e_phoff value %" PRIu64 " less than %zd in %s",
-                                static_cast<uint64_t>(header_->e_phoff),
-                                Size(),
-                                file->GetPath().c_str());
-      return false;
-    }
-    if (header_->e_shoff >= Size()) {
-      *error_msg = StringPrintf("Failed to find e_shoff value %" PRIu64 " less than %zd in %s",
-                                static_cast<uint64_t>(header_->e_shoff),
-                                Size(),
-                                file->GetPath().c_str());
-      return false;
-    }
-  }
   return true;
 }
 
@@ -558,13 +296,6 @@ uint8_t* ElfFileImpl<ElfTypes>::GetProgramHeadersStart() const {
   return program_headers_start_;
 }
 
-template <typename ElfTypes>
-uint8_t* ElfFileImpl<ElfTypes>::GetSectionHeadersStart() const {
-  CHECK(!program_header_only_);              // Only used in "full" mode.
-  CHECK(section_headers_start_ != nullptr);  // Is checked in CheckSectionsExist
-  return section_headers_start_;
-}
-
 template <typename ElfTypes>
 typename ElfTypes::Phdr& ElfFileImpl<ElfTypes>::GetDynamicProgramHeader() const {
   CHECK(dynamic_program_header_ != nullptr);  // Is checked in CheckSectionsExist
@@ -580,7 +311,7 @@ typename ElfTypes::Dyn* ElfFileImpl<ElfTypes>::GetDynamicSectionStart() const {
 template <typename ElfTypes>
 typename ElfTypes::Sym* ElfFileImpl<ElfTypes>::GetSymbolSectionStart(
     Elf_Word section_type) const {
-  CHECK(IsSymbolSectionType(section_type)) << file_path_ << " " << section_type;
+  CHECK(IsSymbolSectionType(section_type)) << file_location_ << " " << section_type;
   switch (section_type) {
     case SHT_SYMTAB: {
       return symtab_section_start_;
@@ -600,7 +331,7 @@ typename ElfTypes::Sym* ElfFileImpl<ElfTypes>::GetSymbolSectionStart(
 template <typename ElfTypes>
 const char* ElfFileImpl<ElfTypes>::GetStringSectionStart(
     Elf_Word section_type) const {
-  CHECK(IsSymbolSectionType(section_type)) << file_path_ << " " << section_type;
+  CHECK(IsSymbolSectionType(section_type)) << file_location_ << " " << section_type;
   switch (section_type) {
     case SHT_SYMTAB: {
       return strtab_section_start_;
@@ -618,7 +349,7 @@ const char* ElfFileImpl<ElfTypes>::GetStringSectionStart(
 template <typename ElfTypes>
 const char* ElfFileImpl<ElfTypes>::GetString(Elf_Word section_type,
                                              Elf_Word i) const {
-  CHECK(IsSymbolSectionType(section_type)) << file_path_ << " " << section_type;
+  CHECK(IsSymbolSectionType(section_type)) << file_location_ << " " << section_type;
   if (i == 0) {
     return nullptr;
   }
@@ -676,7 +407,7 @@ typename ElfTypes::Word ElfFileImpl<ElfTypes>::GetProgramHeaderNum() const {
 
 template <typename ElfTypes>
 typename ElfTypes::Phdr* ElfFileImpl<ElfTypes>::GetProgramHeader(Elf_Word i) const {
-  CHECK_LT(i, GetProgramHeaderNum()) << file_path_;  // Validity check for caller.
+  CHECK_LT(i, GetProgramHeaderNum()) << file_location_;  // Validity check for caller.
   uint8_t* program_header = GetProgramHeadersStart() + (i * GetHeader().e_phentsize);
   CHECK_LT(program_header, End());
   return reinterpret_cast<Elf_Phdr*>(program_header);
@@ -698,35 +429,6 @@ typename ElfTypes::Word ElfFileImpl<ElfTypes>::GetSectionHeaderNum() const {
   return GetHeader().e_shnum;
 }
 
-template <typename ElfTypes>
-typename ElfTypes::Shdr* ElfFileImpl<ElfTypes>::GetSectionHeader(Elf_Word i) const {
-  // Can only access arbitrary sections when we have the whole file, not just program header.
-  // Even if we Load(), it doesn't bring in all the sections.
-  CHECK(!program_header_only_) << file_path_;
-  if (i >= GetSectionHeaderNum()) {
-    return nullptr;  // Failure condition.
-  }
-  uint8_t* section_header = GetSectionHeadersStart() + (i * GetHeader().e_shentsize);
-  if (section_header >= End()) {
-    return nullptr;  // Failure condition.
-  }
-  return reinterpret_cast<Elf_Shdr*>(section_header);
-}
-
-template <typename ElfTypes>
-typename ElfTypes::Shdr* ElfFileImpl<ElfTypes>::FindSectionByType(Elf_Word type) const {
-  // Can only access arbitrary sections when we have the whole file, not just program header.
-  // We could change this to switch on known types if they were detected during loading.
-  CHECK(!program_header_only_) << file_path_;
-  for (Elf_Word i = 0; i < GetSectionHeaderNum(); i++) {
-    Elf_Shdr* section_header = GetSectionHeader(i);
-    if (section_header->sh_type == type) {
-      return section_header;
-    }
-  }
-  return nullptr;
-}
-
 // from bionic
 static unsigned elfhash(const char *_name) {
   const unsigned char *name = (const unsigned char *) _name;
@@ -741,11 +443,6 @@ static unsigned elfhash(const char *_name) {
   return h;
 }
 
-template <typename ElfTypes>
-typename ElfTypes::Shdr* ElfFileImpl<ElfTypes>::GetSectionNameStringSection() const {
-  return GetSectionHeader(GetHeader().e_shstrndx);
-}
-
 template <typename ElfTypes>
 const uint8_t* ElfFileImpl<ElfTypes>::FindDynamicSymbolAddress(
     const std::string& symbol_name) const {
@@ -803,8 +500,8 @@ bool ElfFileImpl<ElfTypes>::IsSymbolSectionType(Elf_Word section_type) {
 template <typename ElfTypes>
 typename ElfTypes::Word ElfFileImpl<ElfTypes>::GetSymbolNum(Elf_Shdr& section_header) const {
   CHECK(IsSymbolSectionType(section_header.sh_type))
-      << file_path_ << " " << section_header.sh_type;
-  CHECK_NE(0U, section_header.sh_entsize) << file_path_;
+      << file_location_ << " " << section_header.sh_type;
+  CHECK_NE(0U, section_header.sh_entsize) << file_location_;
   return section_header.sh_size / section_header.sh_entsize;
 }
 
@@ -817,137 +514,6 @@ typename ElfTypes::Sym* ElfFileImpl<ElfTypes>::GetSymbol(Elf_Word section_type,
   return sym_start + i;
 }
 
-template <typename ElfTypes>
-typename ElfFileImpl<ElfTypes>::SymbolTable**
-ElfFileImpl<ElfTypes>::GetSymbolTable(Elf_Word section_type) {
-  CHECK(IsSymbolSectionType(section_type)) << file_path_ << " " << section_type;
-  switch (section_type) {
-    case SHT_SYMTAB: {
-      return &symtab_symbol_table_;
-    }
-    case SHT_DYNSYM: {
-      return &dynsym_symbol_table_;
-    }
-    default: {
-      LOG(FATAL) << section_type;
-      return nullptr;
-    }
-  }
-}
-
-template <typename ElfTypes>
-typename ElfTypes::Sym* ElfFileImpl<ElfTypes>::FindSymbolByName(
-    Elf_Word section_type, const std::string& symbol_name, bool build_map) {
-  CHECK(!program_header_only_) << file_path_;
-  CHECK(IsSymbolSectionType(section_type)) << file_path_ << " " << section_type;
-
-  SymbolTable** symbol_table = GetSymbolTable(section_type);
-  if (*symbol_table != nullptr || build_map) {
-    if (*symbol_table == nullptr) {
-      DCHECK(build_map);
-      *symbol_table = new SymbolTable;
-      Elf_Shdr* symbol_section = FindSectionByType(section_type);
-      if (symbol_section == nullptr) {
-        return nullptr;  // Failure condition.
-      }
-      Elf_Shdr* string_section = GetSectionHeader(symbol_section->sh_link);
-      if (string_section == nullptr) {
-        return nullptr;  // Failure condition.
-      }
-      for (uint32_t i = 0; i < GetSymbolNum(*symbol_section); i++) {
-        Elf_Sym* symbol = GetSymbol(section_type, i);
-        if (symbol == nullptr) {
-          return nullptr;  // Failure condition.
-        }
-        unsigned char type = (sizeof(Elf_Addr) == sizeof(Elf64_Addr))
-                             ? ELF64_ST_TYPE(symbol->st_info)
-                             : ELF32_ST_TYPE(symbol->st_info);
-        if (type == STT_NOTYPE) {
-          continue;
-        }
-        const char* name = GetString(*string_section, symbol->st_name);
-        if (name == nullptr) {
-          continue;
-        }
-        std::pair<typename SymbolTable::iterator, bool> result =
-            (*symbol_table)->insert(std::make_pair(name, symbol));
-        if (!result.second) {
-          // If a duplicate, make sure it has the same logical value. Seen on x86.
-          if ((symbol->st_value != result.first->second->st_value) ||
-              (symbol->st_size != result.first->second->st_size) ||
-              (symbol->st_info != result.first->second->st_info) ||
-              (symbol->st_other != result.first->second->st_other) ||
-              (symbol->st_shndx != result.first->second->st_shndx)) {
-            return nullptr;  // Failure condition.
-          }
-        }
-      }
-    }
-    CHECK(*symbol_table != nullptr);
-    typename SymbolTable::const_iterator it = (*symbol_table)->find(symbol_name);
-    if (it == (*symbol_table)->end()) {
-      return nullptr;
-    }
-    return it->second;
-  }
-
-  // Fall back to linear search
-  Elf_Shdr* symbol_section = FindSectionByType(section_type);
-  if (symbol_section == nullptr) {
-    return nullptr;
-  }
-  Elf_Shdr* string_section = GetSectionHeader(symbol_section->sh_link);
-  if (string_section == nullptr) {
-    return nullptr;
-  }
-  for (uint32_t i = 0; i < GetSymbolNum(*symbol_section); i++) {
-    Elf_Sym* symbol = GetSymbol(section_type, i);
-    if (symbol == nullptr) {
-      return nullptr;  // Failure condition.
-    }
-    const char* name = GetString(*string_section, symbol->st_name);
-    if (name == nullptr) {
-      continue;
-    }
-    if (symbol_name == name) {
-      return symbol;
-    }
-  }
-  return nullptr;
-}
-
-template <typename ElfTypes>
-typename ElfTypes::Addr ElfFileImpl<ElfTypes>::FindSymbolAddress(
-    Elf_Word section_type, const std::string& symbol_name, bool build_map) {
-  Elf_Sym* symbol = FindSymbolByName(section_type, symbol_name, build_map);
-  if (symbol == nullptr) {
-    return 0;
-  }
-  return symbol->st_value;
-}
-
-template <typename ElfTypes>
-const char* ElfFileImpl<ElfTypes>::GetString(Elf_Shdr& string_section,
-                                             Elf_Word i) const {
-  CHECK(!program_header_only_) << file_path_;
-  // TODO: remove this static_cast from enum when using -std=gnu++0x
-  if (static_cast<Elf_Word>(SHT_STRTAB) != string_section.sh_type) {
-    return nullptr;  // Failure condition.
-  }
-  if (i >= string_section.sh_size) {
-    return nullptr;
-  }
-  if (i == 0) {
-    return nullptr;
-  }
-  uint8_t* strings = Begin() + string_section.sh_offset;
-  uint8_t* string = strings + i;
-  if (string >= End()) {
-    return nullptr;
-  }
-  return reinterpret_cast<const char*>(string);
-}
-
 template <typename ElfTypes>
 typename ElfTypes::Word ElfFileImpl<ElfTypes>::GetDynamicNum() const {
   return GetDynamicProgramHeader().p_filesz / sizeof(Elf_Dyn);
@@ -955,70 +521,10 @@ typename ElfTypes::Word ElfFileImpl<ElfTypes>::GetDynamicNum() const {
 
 template <typename ElfTypes>
 typename ElfTypes::Dyn& ElfFileImpl<ElfTypes>::GetDynamic(Elf_Word i) const {
-  CHECK_LT(i, GetDynamicNum()) << file_path_;
+  CHECK_LT(i, GetDynamicNum()) << file_location_;
   return *(GetDynamicSectionStart() + i);
 }
 
-template <typename ElfTypes>
-typename ElfTypes::Dyn* ElfFileImpl<ElfTypes>::FindDynamicByType(Elf_Sword type) const {
-  for (Elf_Word i = 0; i < GetDynamicNum(); i++) {
-    Elf_Dyn* dyn = &GetDynamic(i);
-    if (dyn->d_tag == type) {
-      return dyn;
-    }
-  }
-  return nullptr;
-}
-
-template <typename ElfTypes>
-typename ElfTypes::Word ElfFileImpl<ElfTypes>::FindDynamicValueByType(Elf_Sword type) const {
-  Elf_Dyn* dyn = FindDynamicByType(type);
-  if (dyn == nullptr) {
-    return 0;
-  } else {
-    return dyn->d_un.d_val;
-  }
-}
-
-template <typename ElfTypes>
-typename ElfTypes::Rel* ElfFileImpl<ElfTypes>::GetRelSectionStart(Elf_Shdr& section_header) const {
-  CHECK(SHT_REL == section_header.sh_type) << file_path_ << " " << section_header.sh_type;
-  return reinterpret_cast<Elf_Rel*>(Begin() + section_header.sh_offset);
-}
-
-template <typename ElfTypes>
-typename ElfTypes::Word ElfFileImpl<ElfTypes>::GetRelNum(Elf_Shdr& section_header) const {
-  CHECK(SHT_REL == section_header.sh_type) << file_path_ << " " << section_header.sh_type;
-  CHECK_NE(0U, section_header.sh_entsize) << file_path_;
-  return section_header.sh_size / section_header.sh_entsize;
-}
-
-template <typename ElfTypes>
-typename ElfTypes::Rel& ElfFileImpl<ElfTypes>::GetRel(Elf_Shdr& section_header, Elf_Word i) const {
-  CHECK(SHT_REL == section_header.sh_type) << file_path_ << " " << section_header.sh_type;
-  CHECK_LT(i, GetRelNum(section_header)) << file_path_;
-  return *(GetRelSectionStart(section_header) + i);
-}
-
-template <typename ElfTypes>
-typename ElfTypes::Rela* ElfFileImpl<ElfTypes>::GetRelaSectionStart(Elf_Shdr& section_header) const {
-  CHECK(SHT_RELA == section_header.sh_type) << file_path_ << " " << section_header.sh_type;
-  return reinterpret_cast<Elf_Rela*>(Begin() + section_header.sh_offset);
-}
-
-template <typename ElfTypes>
-typename ElfTypes::Word ElfFileImpl<ElfTypes>::GetRelaNum(Elf_Shdr& section_header) const {
-  CHECK(SHT_RELA == section_header.sh_type) << file_path_ << " " << section_header.sh_type;
-  return section_header.sh_size / section_header.sh_entsize;
-}
-
-template <typename ElfTypes>
-typename ElfTypes::Rela& ElfFileImpl<ElfTypes>::GetRela(Elf_Shdr& section_header, Elf_Word i) const {
-  CHECK(SHT_RELA == section_header.sh_type) << file_path_ << " " << section_header.sh_type;
-  CHECK_LT(i, GetRelaNum(section_header)) << file_path_;
-  return *(GetRelaSectionStart(section_header) + i);
-}
-
 template <typename ElfTypes>
 bool ElfFileImpl<ElfTypes>::GetLoadedSize(size_t* size, std::string* error_msg) const {
   uint8_t* vaddr_begin;
@@ -1035,7 +541,7 @@ size_t ElfFileImpl<ElfTypes>::GetElfSegmentAlignmentFromFile() const {
     }
     return program_header->p_align;
   }
-  LOG(ERROR) << "No loadable segment found in ELF file " << file_path_;
+  LOG(ERROR) << "No loadable segment found in ELF file " << file_location_;
   return 0;
 }
 
@@ -1060,7 +566,7 @@ bool ElfFileImpl<ElfTypes>::GetLoadedAddressRange(/*out*/uint8_t** vaddr_begin,
       std::ostringstream oss;
       oss << "Program header #" << i << " has overflow in p_vaddr+p_memsz: 0x" << std::hex
           << program_header->p_vaddr << "+0x" << program_header->p_memsz << "=0x" << end_vaddr
-          << " in ELF file \"" << file_path_ << "\"";
+          << " in ELF file \"" << file_location_ << "\"";
       *error_msg = oss.str();
       *vaddr_begin = nullptr;
       *vaddr_size = static_cast<size_t>(-1);
@@ -1072,13 +578,13 @@ bool ElfFileImpl<ElfTypes>::GetLoadedAddressRange(/*out*/uint8_t** vaddr_begin,
   }
   min_vaddr = RoundDown(min_vaddr, kElfSegmentAlignment);
   max_vaddr = RoundUp(max_vaddr, kElfSegmentAlignment);
-  CHECK_LT(min_vaddr, max_vaddr) << file_path_;
+  CHECK_LT(min_vaddr, max_vaddr) << file_location_;
   // Check that the range fits into the runtime address space.
   if (UNLIKELY(max_vaddr - 1u > std::numeric_limits<size_t>::max())) {
     std::ostringstream oss;
     oss << "Loaded range is 0x" << std::hex << min_vaddr << "-0x" << max_vaddr
-        << " but maximum size_t is 0x" << std::numeric_limits<size_t>::max()
-        << " for ELF file \"" << file_path_ << "\"";
+        << " but maximum size_t is 0x" << std::numeric_limits<size_t>::max() << " for ELF file \""
+        << file_location_ << "\"";
     *error_msg = oss.str();
     *vaddr_begin = nullptr;
     *vaddr_size = static_cast<size_t>(-1);
@@ -1107,13 +613,10 @@ static InstructionSet GetInstructionSetFromELF(uint16_t e_machine,
 }
 
 template <typename ElfTypes>
-bool ElfFileImpl<ElfTypes>::Load(File* file,
-                                 bool executable,
+bool ElfFileImpl<ElfTypes>::Load(bool executable,
                                  bool low_4gb,
-                                 /*inout*/MemMap* reservation,
-                                 /*out*/std::string* error_msg) {
-  CHECK(program_header_only_) << file->GetPath();
-
+                                 /*inout*/ MemMap* reservation,
+                                 /*out*/ std::string* error_msg) {
   if (executable) {
     InstructionSet elf_ISA = GetInstructionSetFromELF(GetHeader().e_machine, GetHeader().e_flags);
     if (elf_ISA != kRuntimeQuickCodeISA) {
@@ -1148,14 +651,6 @@ bool ElfFileImpl<ElfTypes>::Load(File* file,
     // non-zero, the segments require the specific address specified,
     // which either was specified in the file because we already set
     // base_address_ after the first zero segment).
-    int64_t temp_file_length = file->GetLength();
-    if (temp_file_length < 0) {
-      errno = -temp_file_length;
-      *error_msg = StringPrintf("Failed to get length of file: '%s' fd=%d: %s",
-                                file->GetPath().c_str(), file->Fd(), strerror(errno));
-      return false;
-    }
-    size_t file_length = static_cast<size_t>(temp_file_length);
     if (!reserved) {
       uint8_t* vaddr_begin;
       size_t vaddr_size;
@@ -1163,16 +658,16 @@ bool ElfFileImpl<ElfTypes>::Load(File* file,
         DCHECK(!error_msg->empty());
         return false;
       }
-      std::string reservation_name = "ElfFile reservation for " + file->GetPath();
-      MemMap local_reservation = MemMap::MapAnonymous(
-          reservation_name.c_str(),
-          (reservation != nullptr) ? reservation->Begin() : nullptr,
-          vaddr_size,
-          PROT_NONE,
-          low_4gb,
-          /* reuse= */ false,
-          reservation,
-          error_msg);
+      std::string reservation_name = "ElfFile reservation for " + file_location_;
+      MemMap local_reservation =
+          MemMap::MapAnonymous(reservation_name.c_str(),
+                               (reservation != nullptr) ? reservation->Begin() : nullptr,
+                               vaddr_size,
+                               PROT_NONE,
+                               low_4gb,
+                               /*reuse=*/false,
+                               reservation,
+                               error_msg);
       if (!local_reservation.IsValid()) {
         *error_msg = StringPrintf("Failed to allocate %s: %s",
                                   reservation_name.c_str(),
@@ -1206,82 +701,90 @@ bool ElfFileImpl<ElfTypes>::Load(File* file,
     if ((program_header->p_flags & PF_R) != 0) {
       prot |= PROT_READ;
     }
-    int flags = 0;
-    if (writable_) {
-      prot |= PROT_WRITE;
-      flags |= MAP_SHARED;
-    } else {
-      flags |= MAP_PRIVATE;
-    }
     if (program_header->p_filesz > program_header->p_memsz) {
       *error_msg = StringPrintf("Invalid p_filesz > p_memsz (%" PRIu64 " > %" PRIu64 "): %s",
                                 static_cast<uint64_t>(program_header->p_filesz),
                                 static_cast<uint64_t>(program_header->p_memsz),
-                                file->GetPath().c_str());
+                                file_location_.c_str());
       return false;
     }
     if (program_header->p_filesz < program_header->p_memsz &&
         !IsAligned<kElfSegmentAlignment>(program_header->p_filesz)) {
-      *error_msg = StringPrintf("Unsupported unaligned p_filesz < p_memsz (%" PRIu64
-                                " < %" PRIu64 "): %s",
-                                static_cast<uint64_t>(program_header->p_filesz),
-                                static_cast<uint64_t>(program_header->p_memsz),
-                                file->GetPath().c_str());
+      *error_msg =
+          StringPrintf("Unsupported unaligned p_filesz < p_memsz (%" PRIu64 " < %" PRIu64 "): %s",
+                       static_cast<uint64_t>(program_header->p_filesz),
+                       static_cast<uint64_t>(program_header->p_memsz),
+                       file_location_.c_str());
       return false;
     }
-    if (file_length < (program_header->p_offset + program_header->p_filesz)) {
-      *error_msg = StringPrintf("File size of %zd bytes not large enough to contain ELF segment "
-                                "%d of %" PRIu64 " bytes: '%s'", file_length, i,
-                                static_cast<uint64_t>(program_header->p_offset + program_header->p_filesz),
-                                file->GetPath().c_str());
+    if (file_length_ < (program_header->p_offset + program_header->p_filesz)) {
+      *error_msg = StringPrintf(
+          "File size of %zd bytes not large enough to contain ELF segment "
+          "%d of %" PRIu64 " bytes: '%s'",
+          file_length_,
+          i,
+          static_cast<uint64_t>(program_header->p_offset + program_header->p_filesz),
+          file_location_.c_str());
       return false;
     }
     if (program_header->p_filesz != 0u) {
-      MemMap segment =
-          MemMap::MapFileAtAddress(p_vaddr,
-                                   program_header->p_filesz,
-                                   prot,
-                                   flags,
-                                   file->Fd(),
-                                   program_header->p_offset,
-                                   /* low_4gb= */ false,
-                                   file->GetPath().c_str(),
-                                   /* reuse= */ true,  // implies MAP_FIXED
-                                   /* reservation= */ nullptr,
-                                   error_msg);
+      MemMap segment = MemMap::MapFileAtAddress(p_vaddr,
+                                                program_header->p_filesz,
+                                                prot,
+                                                MAP_PRIVATE,
+                                                file_->Fd(),
+                                                start_ + program_header->p_offset,
+                                                /*low_4gb=*/false,
+                                                file_location_.c_str(),
+                                                /*reuse=*/true,  // implies MAP_FIXED
+                                                /*reservation=*/nullptr,
+                                                error_msg);
       if (!segment.IsValid()) {
         *error_msg = StringPrintf("Failed to map ELF file segment %d from %s: %s",
-                                  i, file->GetPath().c_str(), error_msg->c_str());
+                                  i,
+                                  file_location_.c_str(),
+                                  error_msg->c_str());
         return false;
       }
       if (segment.Begin() != p_vaddr) {
-        *error_msg = StringPrintf("Failed to map ELF file segment %d from %s at expected address %p, "
-                                  "instead mapped to %p",
-                                  i, file->GetPath().c_str(), p_vaddr, segment.Begin());
+        *error_msg = StringPrintf(
+            "Failed to map ELF file segment %d from %s at expected address %p, "
+            "instead mapped to %p",
+            i,
+            file_location_.c_str(),
+            p_vaddr,
+            segment.Begin());
         return false;
       }
       segments_.push_back(std::move(segment));
     }
     if (program_header->p_filesz < program_header->p_memsz) {
       std::string name = StringPrintf("Zero-initialized segment %" PRIu64 " of ELF file %s",
-                                      static_cast<uint64_t>(i), file->GetPath().c_str());
+                                      static_cast<uint64_t>(i),
+                                      file_location_.c_str());
       MemMap segment = MemMap::MapAnonymous(name.c_str(),
                                             p_vaddr + program_header->p_filesz,
                                             program_header->p_memsz - program_header->p_filesz,
                                             prot,
-                                            /* low_4gb= */ false,
-                                            /* reuse= */ true,
-                                            /* reservation= */ nullptr,
+                                            /*low_4gb=*/false,
+                                            /*reuse=*/true,
+                                            /*reservation=*/nullptr,
                                             error_msg);
       if (!segment.IsValid()) {
         *error_msg = StringPrintf("Failed to map zero-initialized ELF file segment %d from %s: %s",
-                                  i, file->GetPath().c_str(), error_msg->c_str());
+                                  i,
+                                  file_location_.c_str(),
+                                  error_msg->c_str());
         return false;
       }
       if (segment.Begin() != p_vaddr) {
-        *error_msg = StringPrintf("Failed to map zero-initialized ELF file segment %d from %s "
-                                  "at expected address %p, instead mapped to %p",
-                                  i, file->GetPath().c_str(), p_vaddr, segment.Begin());
+        *error_msg = StringPrintf(
+            "Failed to map zero-initialized ELF file segment %d from %s "
+            "at expected address %p, instead mapped to %p",
+            i,
+            file_location_.c_str(),
+            p_vaddr,
+            segment.Begin());
         return false;
       }
       segments_.push_back(std::move(segment));
@@ -1291,8 +794,8 @@ bool ElfFileImpl<ElfTypes>::Load(File* file,
   // Now that we are done loading, .dynamic should be in memory to find .dynstr, .dynsym, .hash
   uint8_t* dsptr = base_address_ + GetDynamicProgramHeader().p_vaddr;
   if ((dsptr < Begin() || dsptr >= End()) && !ValidPointer(dsptr)) {
-    *error_msg = StringPrintf("dynamic section address invalid in ELF file %s",
-                              file->GetPath().c_str());
+    *error_msg =
+        StringPrintf("dynamic section address invalid in ELF file %s", file_location_.c_str());
     return false;
   }
   dynamic_section_start_ = reinterpret_cast<Elf_Dyn*>(dsptr);
@@ -1304,7 +807,8 @@ bool ElfFileImpl<ElfTypes>::Load(File* file,
       case DT_HASH: {
         if (!ValidPointer(d_ptr)) {
           *error_msg = StringPrintf("DT_HASH value %p does not refer to a loaded ELF segment of %s",
-                                    d_ptr, file->GetPath().c_str());
+                                    d_ptr,
+                                    file_location_.c_str());
           return false;
         }
         hash_section_start_ = reinterpret_cast<Elf_Word*>(d_ptr);
@@ -1313,7 +817,8 @@ bool ElfFileImpl<ElfTypes>::Load(File* file,
       case DT_STRTAB: {
         if (!ValidPointer(d_ptr)) {
           *error_msg = StringPrintf("DT_HASH value %p does not refer to a loaded ELF segment of %s",
-                                    d_ptr, file->GetPath().c_str());
+                                    d_ptr,
+                                    file_location_.c_str());
           return false;
         }
         dynstr_section_start_ = reinterpret_cast<char*>(d_ptr);
@@ -1322,7 +827,8 @@ bool ElfFileImpl<ElfTypes>::Load(File* file,
       case DT_SYMTAB: {
         if (!ValidPointer(d_ptr)) {
           *error_msg = StringPrintf("DT_HASH value %p does not refer to a loaded ELF segment of %s",
-                                    d_ptr, file->GetPath().c_str());
+                                    d_ptr,
+                                    file_location_.c_str());
           return false;
         }
         dynsym_section_start_ = reinterpret_cast<Elf_Sym*>(d_ptr);
@@ -1330,9 +836,12 @@ bool ElfFileImpl<ElfTypes>::Load(File* file,
       }
       case DT_NULL: {
         if (GetDynamicNum() != i+1) {
-          *error_msg = StringPrintf("DT_NULL found after %d .dynamic entries, "
-                                    "expected %d as implied by size of PT_DYNAMIC segment in %s",
-                                    i + 1, GetDynamicNum(), file->GetPath().c_str());
+          *error_msg = StringPrintf(
+              "DT_NULL found after %d .dynamic entries, "
+              "expected %d as implied by size of PT_DYNAMIC segment in %s",
+              i + 1,
+              GetDynamicNum(),
+              file_location_.c_str());
           return false;
         }
         break;
@@ -1341,7 +850,7 @@ bool ElfFileImpl<ElfTypes>::Load(File* file,
   }
 
   // Check for the existence of some sections.
-  if (!CheckSectionsExist(file, error_msg)) {
+  if (!CheckSectionsExist(error_msg)) {
     return false;
   }
 
@@ -1358,349 +867,56 @@ bool ElfFileImpl<ElfTypes>::ValidPointer(const uint8_t* start) const {
   return false;
 }
 
-
-template <typename ElfTypes>
-typename ElfTypes::Shdr* ElfFileImpl<ElfTypes>::FindSectionByName(
-    const std::string& name) const {
-  CHECK(!program_header_only_);
-  Elf_Shdr* shstrtab_sec = GetSectionNameStringSection();
-  if (shstrtab_sec == nullptr) {
-    return nullptr;
-  }
-  for (uint32_t i = 0; i < GetSectionHeaderNum(); i++) {
-    Elf_Shdr* shdr = GetSectionHeader(i);
-    if (shdr == nullptr) {
-      return nullptr;
-    }
-    const char* sec_name = GetString(*shstrtab_sec, shdr->sh_name);
-    if (sec_name == nullptr) {
-      continue;
-    }
-    if (name == sec_name) {
-      return shdr;
-    }
-  }
-  return nullptr;
-}
-
-template <typename ElfTypes>
-bool ElfFileImpl<ElfTypes>::Strip(File* file, std::string* error_msg) {
-  // ELF files produced by MCLinker look roughly like this
-  //
-  // +------------+
-  // | Elf_Ehdr   | contains number of Elf_Shdr and offset to first
-  // +------------+
-  // | Elf_Phdr   | program headers
-  // | Elf_Phdr   |
-  // | ...        |
-  // | Elf_Phdr   |
-  // +------------+
-  // | section    | mixture of needed and unneeded sections
-  // +------------+
-  // | section    |
-  // +------------+
-  // | ...        |
-  // +------------+
-  // | section    |
-  // +------------+
-  // | Elf_Shdr   | section headers
-  // | Elf_Shdr   |
-  // | ...        | contains offset to section start
-  // | Elf_Shdr   |
-  // +------------+
-  //
-  // To strip:
-  // - leave the Elf_Ehdr and Elf_Phdr values in place.
-  // - walk the sections making a new set of Elf_Shdr section headers for what we want to keep
-  // - move the sections are keeping up to fill in gaps of sections we want to strip
-  // - write new Elf_Shdr section headers to end of file, updating Elf_Ehdr
-  // - truncate rest of file
-  //
-
-  std::vector<Elf_Shdr> section_headers;
-  std::vector<Elf_Word> section_headers_original_indexes;
-  section_headers.reserve(GetSectionHeaderNum());
-
-
-  Elf_Shdr* string_section = GetSectionNameStringSection();
-  CHECK(string_section != nullptr);
-  for (Elf_Word i = 0; i < GetSectionHeaderNum(); i++) {
-    Elf_Shdr* sh = GetSectionHeader(i);
-    CHECK(sh != nullptr);
-    const char* name = GetString(*string_section, sh->sh_name);
-    if (name == nullptr) {
-      CHECK_EQ(0U, i);
-      section_headers.push_back(*sh);
-      section_headers_original_indexes.push_back(0);
-      continue;
-    }
-    std::string_view name_sv(name);
-    if (name_sv.starts_with(".debug") || (name_sv == ".strtab") || (name_sv == ".symtab")) {
-      continue;
-    }
-    section_headers.push_back(*sh);
-    section_headers_original_indexes.push_back(i);
-  }
-  CHECK_NE(0U, section_headers.size());
-  CHECK_EQ(section_headers.size(), section_headers_original_indexes.size());
-
-  // section 0 is the null section, sections start at offset of first section
-  CHECK(GetSectionHeader(1) != nullptr);
-  Elf_Off offset = GetSectionHeader(1)->sh_offset;
-  for (size_t i = 1; i < section_headers.size(); i++) {
-    Elf_Shdr& new_sh = section_headers[i];
-    Elf_Shdr* old_sh = GetSectionHeader(section_headers_original_indexes[i]);
-    CHECK(old_sh != nullptr);
-    CHECK_EQ(new_sh.sh_name, old_sh->sh_name);
-    if (old_sh->sh_addralign > 1) {
-      offset = RoundUp(offset, old_sh->sh_addralign);
-    }
-    if (old_sh->sh_offset == offset) {
-      // already in place
-      offset += old_sh->sh_size;
-      continue;
-    }
-    // shift section earlier
-    memmove(Begin() + offset,
-            Begin() + old_sh->sh_offset,
-            old_sh->sh_size);
-    new_sh.sh_offset = offset;
-    offset += old_sh->sh_size;
-  }
-
-  Elf_Off shoff = offset;
-  size_t section_headers_size_in_bytes = section_headers.size() * sizeof(Elf_Shdr);
-  memcpy(Begin() + offset, &section_headers[0], section_headers_size_in_bytes);
-  offset += section_headers_size_in_bytes;
-
-  GetHeader().e_shnum = section_headers.size();
-  GetHeader().e_shoff = shoff;
-  int result = ftruncate(file->Fd(), offset);
-  if (result != 0) {
-    *error_msg = StringPrintf("Failed to truncate while stripping ELF file: '%s': %s",
-                              file->GetPath().c_str(), strerror(errno));
-    return false;
-  }
-  return true;
-}
-
 // Explicit instantiations
 template class ElfFileImpl<ElfTypes32>;
 template class ElfFileImpl<ElfTypes64>;
 
-ElfFile::ElfFile(ElfFileImpl32* elf32) : elf32_(elf32), elf64_(nullptr) {
-}
-
-ElfFile::ElfFile(ElfFileImpl64* elf64) : elf32_(nullptr), elf64_(elf64) {
-}
-
-ElfFile::~ElfFile() {
-  // Should never have 32 and 64-bit impls.
-  CHECK_NE(elf32_.get() == nullptr, elf64_.get() == nullptr);
-}
-
 ElfFile* ElfFile::Open(File* file,
-                       bool writable,
-                       bool program_header_only,
+                       off_t start,
+                       size_t file_length,
+                       const std::string& file_location,
                        bool low_4gb,
-                       /*out*/std::string* error_msg) {
-  if (file->GetLength() < EI_NIDENT) {
-    *error_msg = StringPrintf("File %s is too short to be a valid ELF file",
-                              file->GetPath().c_str());
+                       /*out*/ std::string* error_msg) {
+  if (file_length < EI_NIDENT) {
+    *error_msg = StringPrintf("File %s is too short to be a valid ELF file", file_location.c_str());
     return nullptr;
   }
   MemMap map = MemMap::MapFile(EI_NIDENT,
                                PROT_READ,
                                MAP_PRIVATE,
                                file->Fd(),
-                               0,
+                               start,
                                low_4gb,
-                               file->GetPath().c_str(),
+                               file_location.c_str(),
                                error_msg);
   if (!map.IsValid() || map.Size() != EI_NIDENT) {
     return nullptr;
   }
   uint8_t* header = map.Begin();
   if (header[EI_CLASS] == ELFCLASS64) {
-    ElfFileImpl64* elf_file_impl = ElfFileImpl64::Open(file,
-                                                       writable,
-                                                       program_header_only,
-                                                       low_4gb,
-                                                       error_msg);
-    if (elf_file_impl == nullptr) {
-      return nullptr;
-    }
-    return new ElfFile(elf_file_impl);
+    return ElfFileImpl64::Open(file, start, file_length, file_location, low_4gb, error_msg);
   } else if (header[EI_CLASS] == ELFCLASS32) {
-    ElfFileImpl32* elf_file_impl = ElfFileImpl32::Open(file,
-                                                       writable,
-                                                       program_header_only,
-                                                       low_4gb,
-                                                       error_msg);
-    if (elf_file_impl == nullptr) {
-      return nullptr;
-    }
-    return new ElfFile(elf_file_impl);
+    return ElfFileImpl32::Open(file, start, file_length, file_location, low_4gb, error_msg);
   } else {
     *error_msg = StringPrintf("Failed to find expected EI_CLASS value %d or %d in %s, found %d",
-                              ELFCLASS32, ELFCLASS64,
-                              file->GetPath().c_str(),
+                              ELFCLASS32,
+                              ELFCLASS64,
+                              file_location.c_str(),
                               header[EI_CLASS]);
     return nullptr;
   }
 }
 
-ElfFile* ElfFile::Open(File* file, int mmap_prot, int mmap_flags, /*out*/std::string* error_msg) {
-  // low_4gb support not required for this path.
-  constexpr bool low_4gb = false;
-  if (file->GetLength() < EI_NIDENT) {
-    *error_msg = StringPrintf("File %s is too short to be a valid ELF file",
-                              file->GetPath().c_str());
-    return nullptr;
-  }
-  MemMap map = MemMap::MapFile(EI_NIDENT,
-                               PROT_READ,
-                               MAP_PRIVATE,
-                               file->Fd(),
-                               /* start= */ 0,
-                               low_4gb,
-                               file->GetPath().c_str(),
-                               error_msg);
-  if (!map.IsValid() || map.Size() != EI_NIDENT) {
-    return nullptr;
-  }
-  uint8_t* header = map.Begin();
-  if (header[EI_CLASS] == ELFCLASS64) {
-    ElfFileImpl64* elf_file_impl = ElfFileImpl64::Open(file,
-                                                       mmap_prot,
-                                                       mmap_flags,
-                                                       low_4gb,
-                                                       error_msg);
-    if (elf_file_impl == nullptr) {
-      return nullptr;
-    }
-    return new ElfFile(elf_file_impl);
-  } else if (header[EI_CLASS] == ELFCLASS32) {
-    ElfFileImpl32* elf_file_impl = ElfFileImpl32::Open(file,
-                                                       mmap_prot,
-                                                       mmap_flags,
-                                                       low_4gb,
-                                                       error_msg);
-    if (elf_file_impl == nullptr) {
-      return nullptr;
-    }
-    return new ElfFile(elf_file_impl);
-  } else {
-    *error_msg = StringPrintf("Failed to find expected EI_CLASS value %d or %d in %s, found %d",
-                              ELFCLASS32, ELFCLASS64,
-                              file->GetPath().c_str(),
-                              header[EI_CLASS]);
+ElfFile* ElfFile::Open(File* file,
+                       bool low_4gb,
+                       /*out*/ std::string* error_msg) {
+  int64_t file_length = file->GetLength();
+  if (file_length < 0) {
+    *error_msg =
+        ART_FORMAT("Failed to get file length of '{}': {}", file->GetPath(), strerror(errno));
     return nullptr;
   }
-}
-
-#define DELEGATE_TO_IMPL(func, ...) \
-  if (elf64_.get() != nullptr) { \
-    return elf64_->func(__VA_ARGS__); \
-  } else { \
-    DCHECK(elf32_.get() != nullptr); \
-    return elf32_->func(__VA_ARGS__); \
-  }
-
-bool ElfFile::Load(File* file,
-                   bool executable,
-                   bool low_4gb,
-                   /*inout*/MemMap* reservation,
-                   /*out*/std::string* error_msg) {
-  DELEGATE_TO_IMPL(Load, file, executable, low_4gb, reservation, error_msg);
-}
-
-const uint8_t* ElfFile::FindDynamicSymbolAddress(const std::string& symbol_name) const {
-  DELEGATE_TO_IMPL(FindDynamicSymbolAddress, symbol_name);
-}
-
-size_t ElfFile::Size() const {
-  DELEGATE_TO_IMPL(Size);
-}
-
-uint8_t* ElfFile::Begin() const {
-  DELEGATE_TO_IMPL(Begin);
-}
-
-uint8_t* ElfFile::End() const {
-  DELEGATE_TO_IMPL(End);
-}
-
-const std::string& ElfFile::GetFilePath() const {
-  DELEGATE_TO_IMPL(GetFilePath);
-}
-
-bool ElfFile::GetSectionOffsetAndSize(const char* section_name, uint64_t* offset,
-                                      uint64_t* size) const {
-  if (elf32_.get() == nullptr) {
-    CHECK(elf64_.get() != nullptr);
-
-    Elf64_Shdr *shdr = elf64_->FindSectionByName(section_name);
-    if (shdr == nullptr) {
-      return false;
-    }
-    if (offset != nullptr) {
-      *offset = shdr->sh_offset;
-    }
-    if (size != nullptr) {
-      *size = shdr->sh_size;
-    }
-    return true;
-  } else {
-    Elf32_Shdr *shdr = elf32_->FindSectionByName(section_name);
-    if (shdr == nullptr) {
-      return false;
-    }
-    if (offset != nullptr) {
-      *offset = shdr->sh_offset;
-    }
-    if (size != nullptr) {
-      *size = shdr->sh_size;
-    }
-    return true;
-  }
-}
-
-bool ElfFile::HasSection(const std::string& name) const {
-  if (elf64_.get() != nullptr) {
-    return elf64_->FindSectionByName(name) != nullptr;
-  } else {
-    return elf32_->FindSectionByName(name) != nullptr;
-  }
-}
-
-uint64_t ElfFile::FindSymbolAddress(unsigned section_type,
-                                    const std::string& symbol_name,
-                                    bool build_map) {
-  DELEGATE_TO_IMPL(FindSymbolAddress, section_type, symbol_name, build_map);
-}
-
-bool ElfFile::GetLoadedSize(size_t* size, std::string* error_msg) const {
-  DELEGATE_TO_IMPL(GetLoadedSize, size, error_msg);
-}
-
-size_t ElfFile::GetElfSegmentAlignmentFromFile() const {
-  DELEGATE_TO_IMPL(GetElfSegmentAlignmentFromFile);
-}
-
-const uint8_t* ElfFile::GetBaseAddress() const { DELEGATE_TO_IMPL(GetBaseAddress); }
-
-bool ElfFile::Strip(File* file, std::string* error_msg) {
-  std::unique_ptr<ElfFile> elf_file(ElfFile::Open(file, true, false, /*low_4gb=*/false, error_msg));
-  if (elf_file.get() == nullptr) {
-    return false;
-  }
-
-  if (elf_file->elf64_.get() != nullptr) {
-    return elf_file->elf64_->Strip(file, error_msg);
-  } else {
-    return elf_file->elf32_->Strip(file, error_msg);
-  }
+  return Open(file, /*start=*/0, file_length, file->GetPath(), low_4gb, error_msg);
 }
 
 }  // namespace art
diff --git a/runtime/oat/elf_file.h b/runtime/oat/elf_file.h
index 90e6c61817..d86f4fb4e4 100644
--- a/runtime/oat/elf_file.h
+++ b/runtime/oat/elf_file.h
@@ -17,17 +17,18 @@
 #ifndef ART_RUNTIME_OAT_ELF_FILE_H_
 #define ART_RUNTIME_OAT_ELF_FILE_H_
 
-#include <memory>
+#include <cstddef>
 #include <string>
+#include <vector>
 
+#include "android-base/logging.h"
 #include "base/macros.h"
+#include "base/mem_map.h"
 #include "base/os.h"
 #include "elf/elf_utils.h"
 
 namespace art HIDDEN {
 
-class MemMap;
-
 template <typename ElfTypes>
 class ElfFileImpl;
 
@@ -40,75 +41,72 @@ using ElfFileImpl64 = ElfFileImpl<ElfTypes64>;
 // ELFObjectFile.
 class ElfFile {
  public:
+  // Loads the program headers.
+  // Does not take the ownership of the file. It must stay alive during the `Load` call.
   static ElfFile* Open(File* file,
-                       bool writable,
-                       bool program_header_only,
+                       off_t start,
+                       size_t file_length,
+                       const std::string& file_location,
                        bool low_4gb,
-                       /*out*/std::string* error_msg);
-  // Open with specific mmap flags, Always maps in the whole file, not just the
-  // program header sections.
-  static ElfFile* Open(File* file,
-                       int mmap_prot,
-                       int mmap_flags,
-                       /*out*/std::string* error_msg);
-  ~ElfFile();
-
-  // Load segments into memory based on PT_LOAD program headers
-  bool Load(File* file,
-            bool executable,
-            bool low_4gb,
-            /*inout*/MemMap* reservation,
-            /*out*/std::string* error_msg);
+                       /*out*/ std::string* error_msg);
 
-  const uint8_t* FindDynamicSymbolAddress(const std::string& symbol_name) const;
+  static ElfFile* Open(File* file,
+                       bool low_4gb,
+                       /*out*/ std::string* error_msg);
 
-  size_t Size() const;
+  virtual ~ElfFile() = default;
 
-  // The start of the memory map address range for this ELF file.
-  uint8_t* Begin() const;
+  // Load segments into memory based on PT_LOAD program headers.
+  virtual bool Load(bool executable,
+                    bool low_4gb,
+                    /*inout*/ MemMap* reservation,
+                    /*out*/ std::string* error_msg) = 0;
 
-  // The end of the memory map address range for this ELF file.
-  uint8_t* End() const;
+  virtual const uint8_t* FindDynamicSymbolAddress(const std::string& symbol_name) const = 0;
 
-  const std::string& GetFilePath() const;
+  // Returns the location of the ELF file, for debugging purposes only.
+  // Note that the location is not necessarily a path to a file on disk. It can also be a zip entry
+  // inside a zip file.
+  const std::string& GetFileLocation() const { return file_location_; }
 
-  bool GetSectionOffsetAndSize(const char* section_name, uint64_t* offset, uint64_t* size) const;
+  uint8_t* GetBaseAddress() const { return base_address_; }
 
-  bool HasSection(const std::string& name) const;
+  uint8_t* Begin() const { return map_.Begin(); }
 
-  uint64_t FindSymbolAddress(unsigned section_type,
-                             const std::string& symbol_name,
-                             bool build_map);
+  uint8_t* End() const { return map_.End(); }
 
-  bool GetLoadedSize(size_t* size, std::string* error_msg) const;
+  size_t Size() const { return map_.Size(); }
 
-  size_t GetElfSegmentAlignmentFromFile() const;
+  virtual bool GetLoadedSize(size_t* size, std::string* error_msg) const = 0;
 
-  const uint8_t* GetBaseAddress() const;
+  virtual size_t GetElfSegmentAlignmentFromFile() const = 0;
 
-  // Strip an ELF file of unneeded debugging information.
-  // Returns true on success, false on failure.
-  static bool Strip(File* file, std::string* error_msg);
+  virtual bool Is64Bit() const = 0;
 
-  bool Is64Bit() const {
-    return elf64_.get() != nullptr;
+ protected:
+  ElfFile(File* file, off_t start, size_t file_length, const std::string& file_location)
+      : file_(file), start_(start), file_length_(file_length), file_location_(file_location) {
+    CHECK(file != nullptr);
   }
 
-  ElfFileImpl32* GetImpl32() const {
-    return elf32_.get();
-  }
+  File* const file_;
+  const off_t start_;
+  const size_t file_length_;
+  const std::string file_location_;
 
-  ElfFileImpl64* GetImpl64() const {
-    return elf64_.get();
-  }
+  // ELF header mapping. If program_header_only_ is false, will
+  // actually point to the entire elf file.
+  MemMap map_;
+  std::vector<MemMap> segments_;
 
- private:
-  explicit ElfFile(ElfFileImpl32* elf32);
-  explicit ElfFile(ElfFileImpl64* elf64);
+  // Pointer to start of first PT_LOAD program segment after Load()
+  // when program_header_only_ is true.
+  uint8_t* base_address_ = nullptr;
 
-  const std::unique_ptr<ElfFileImpl32> elf32_;
-  const std::unique_ptr<ElfFileImpl64> elf64_;
+  // The program header should always available but use GetProgramHeadersStart() to be sure.
+  uint8_t* program_headers_start_ = nullptr;
 
+ private:
   DISALLOW_COPY_AND_ASSIGN(ElfFile);
 };
 
diff --git a/runtime/oat/elf_file_impl.h b/runtime/oat/elf_file_impl.h
index 67c60e906c..6db86d011f 100644
--- a/runtime/oat/elf_file_impl.h
+++ b/runtime/oat/elf_file_impl.h
@@ -17,19 +17,18 @@
 #ifndef ART_RUNTIME_OAT_ELF_FILE_IMPL_H_
 #define ART_RUNTIME_OAT_ELF_FILE_IMPL_H_
 
-#include <map>
-#include <memory>
 #include <type_traits>
 #include <vector>
 
 #include "base/macros.h"
 #include "base/mem_map.h"
-#include "elf/elf_utils.h"
+#include "base/os.h"
+#include "elf_file.h"
 
 namespace art HIDDEN {
 
 template <typename ElfTypes>
-class ElfFileImpl {
+class ElfFileImpl : public ElfFile {
  public:
   using Elf_Addr = typename ElfTypes::Addr;
   using Elf_Off = typename ElfTypes::Off;
@@ -45,36 +44,11 @@ class ElfFileImpl {
   using Elf_Dyn = typename ElfTypes::Dyn;
 
   static ElfFileImpl* Open(File* file,
-                           bool writable,
-                           bool program_header_only,
+                           off_t start,
+                           size_t file_length,
+                           const std::string& file_location,
                            bool low_4gb,
-                           /*out*/std::string* error_msg);
-  static ElfFileImpl* Open(File* file,
-                           int mmap_prot,
-                           int mmap_flags,
-                           bool low_4gb,
-                           /*out*/std::string* error_msg);
-  ~ElfFileImpl();
-
-  const std::string& GetFilePath() const {
-    return file_path_;
-  }
-
-  uint8_t* GetBaseAddress() const {
-    return base_address_;
-  }
-
-  uint8_t* Begin() const {
-    return map_.Begin();
-  }
-
-  uint8_t* End() const {
-    return map_.End();
-  }
-
-  size_t Size() const {
-    return map_.Size();
-  }
+                           /*out*/ std::string* error_msg);
 
   Elf_Ehdr& GetHeader() const;
 
@@ -82,143 +56,79 @@ class ElfFileImpl {
   Elf_Phdr* GetProgramHeader(Elf_Word) const;
 
   Elf_Word GetSectionHeaderNum() const;
-  Elf_Shdr* GetSectionHeader(Elf_Word) const;
   Elf_Shdr* FindSectionByType(Elf_Word type) const;
-  Elf_Shdr* FindSectionByName(const std::string& name) const;
-
-  Elf_Shdr* GetSectionNameStringSection() const;
 
   // Find .dynsym using .hash for more efficient lookup than FindSymbolAddress.
-  const uint8_t* FindDynamicSymbolAddress(const std::string& symbol_name) const;
+  const uint8_t* FindDynamicSymbolAddress(const std::string& symbol_name) const override;
 
   static bool IsSymbolSectionType(Elf_Word section_type);
   Elf_Word GetSymbolNum(Elf_Shdr&) const;
   Elf_Sym* GetSymbol(Elf_Word section_type, Elf_Word i) const;
 
-  // Find address of symbol in specified table, returning 0 if it is
-  // not found. See FindSymbolByName for an explanation of build_map.
-  Elf_Addr FindSymbolAddress(Elf_Word section_type,
-                             const std::string& symbol_name,
-                             bool build_map);
-
-  // Lookup a string given string section and offset. Returns null for special 0 offset.
-  const char* GetString(Elf_Shdr&, Elf_Word) const;
-
   Elf_Word GetDynamicNum() const;
   Elf_Dyn& GetDynamic(Elf_Word) const;
 
-  Elf_Word GetRelNum(Elf_Shdr&) const;
-  Elf_Rel& GetRel(Elf_Shdr&, Elf_Word) const;
-
-  Elf_Word GetRelaNum(Elf_Shdr&) const;
-  Elf_Rela& GetRela(Elf_Shdr&, Elf_Word) const;
-
   // Retrieves the expected size when the file is loaded at runtime. Returns true if successful.
-  bool GetLoadedSize(size_t* size, std::string* error_msg) const;
+  bool GetLoadedSize(size_t* size, std::string* error_msg) const override;
 
   // Get the alignment of the first loadable program segment. Return 0 if no loadable segment found.
-  size_t GetElfSegmentAlignmentFromFile() const;
+  size_t GetElfSegmentAlignmentFromFile() const override;
 
   // Load segments into memory based on PT_LOAD program headers.
   // executable is true at run time, false at compile time.
-  bool Load(File* file,
-            bool executable,
+  bool Load(bool executable,
             bool low_4gb,
-            /*inout*/MemMap* reservation,
-            /*out*/std::string* error_msg);
+            /*inout*/ MemMap* reservation,
+            /*out*/ std::string* error_msg) override;
 
-  bool Strip(File* file, std::string* error_msg);
+  bool Is64Bit() const override { return std::is_same_v<ElfTypes, ElfTypes64>; }
 
  private:
-  ElfFileImpl(File* file, bool writable, bool program_header_only);
+  ElfFileImpl(File* file, off_t start, size_t file_length, const std::string& file_location)
+      : ElfFile(file, start, file_length, file_location) {}
 
   bool GetLoadedAddressRange(/*out*/uint8_t** vaddr_begin,
                              /*out*/size_t* vaddr_size,
                              /*out*/std::string* error_msg) const;
 
-  bool Setup(File* file, int prot, int flags, bool low_4gb, std::string* error_msg);
+  bool Setup(bool low_4gb, std::string* error_msg);
 
-  bool SetMap(File* file, MemMap&& map, std::string* error_msg);
+  bool SetMap(MemMap&& map, std::string* error_msg);
 
   uint8_t* GetProgramHeadersStart() const;
-  uint8_t* GetSectionHeadersStart() const;
   Elf_Phdr& GetDynamicProgramHeader() const;
   Elf_Dyn* GetDynamicSectionStart() const;
   Elf_Sym* GetSymbolSectionStart(Elf_Word section_type) const;
   const char* GetStringSectionStart(Elf_Word section_type) const;
-  Elf_Rel* GetRelSectionStart(Elf_Shdr&) const;
-  Elf_Rela* GetRelaSectionStart(Elf_Shdr&) const;
   Elf_Word* GetHashSectionStart() const;
   Elf_Word GetHashBucketNum() const;
   Elf_Word GetHashChainNum() const;
   Elf_Word GetHashBucket(size_t i, bool* ok) const;
   Elf_Word GetHashChain(size_t i, bool* ok) const;
 
-  using SymbolTable = std::map<std::string, Elf_Sym*>;
-  SymbolTable** GetSymbolTable(Elf_Word section_type);
-
   bool ValidPointer(const uint8_t* start) const;
 
   const Elf_Sym* FindDynamicSymbol(const std::string& symbol_name) const;
 
   // Check that certain sections and their dependencies exist.
-  bool CheckSectionsExist(File* file, std::string* error_msg) const;
-
-  // Check that the link of the first section links to the second section.
-  bool CheckSectionsLinked(const uint8_t* source, const uint8_t* target) const;
-
-  // Check whether the offset is in range, and set to target to Begin() + offset if OK.
-  bool CheckAndSet(Elf32_Off offset, const char* label, uint8_t** target, std::string* error_msg);
-
-  // Find symbol in specified table, returning null if it is not found.
-  //
-  // If build_map is true, builds a map to speed repeated access. The
-  // map does not included untyped symbol values (aka STT_NOTYPE)
-  // since they can contain duplicates. If build_map is false, the map
-  // will be used if it was already created. Typically build_map
-  // should be set unless only a small number of symbols will be
-  // looked up.
-  Elf_Sym* FindSymbolByName(Elf_Word section_type,
-                            const std::string& symbol_name,
-                            bool build_map);
+  bool CheckSectionsExist(std::string* error_msg) const;
 
   Elf_Phdr* FindProgamHeaderByType(Elf_Word type) const;
 
-  Elf_Dyn* FindDynamicByType(Elf_Sword type) const;
-  Elf_Word FindDynamicValueByType(Elf_Sword type) const;
-
   // Lookup a string by section type. Returns null for special 0 offset.
   const char* GetString(Elf_Word section_type, Elf_Word) const;
 
-  const std::string file_path_;
-  const bool writable_;
-  const bool program_header_only_;
-
-  // ELF header mapping. If program_header_only_ is false, will
-  // actually point to the entire elf file.
-  MemMap map_;
-  Elf_Ehdr* header_;
-  std::vector<MemMap> segments_;
-
-  // Pointer to start of first PT_LOAD program segment after Load()
-  // when program_header_only_ is true.
-  uint8_t* base_address_;
-
-  // The program header should always available but use GetProgramHeadersStart() to be sure.
-  uint8_t* program_headers_start_;
+  Elf_Ehdr* header_ = nullptr;
 
   // Conditionally available values. Use accessors to ensure they exist if they are required.
-  uint8_t* section_headers_start_;
-  Elf_Phdr* dynamic_program_header_;
-  Elf_Dyn* dynamic_section_start_;
-  Elf_Sym* symtab_section_start_;
-  Elf_Sym* dynsym_section_start_;
-  char* strtab_section_start_;
-  char* dynstr_section_start_;
-  Elf_Word* hash_section_start_;
-
-  SymbolTable* symtab_symbol_table_;
-  SymbolTable* dynsym_symbol_table_;
+  uint8_t* section_headers_start_ = nullptr;
+  Elf_Phdr* dynamic_program_header_ = nullptr;
+  Elf_Dyn* dynamic_section_start_ = nullptr;
+  Elf_Sym* symtab_section_start_ = nullptr;
+  Elf_Sym* dynsym_section_start_ = nullptr;
+  char* strtab_section_start_ = nullptr;
+  char* dynstr_section_start_ = nullptr;
+  Elf_Word* hash_section_start_ = nullptr;
 
   DISALLOW_COPY_AND_ASSIGN(ElfFileImpl);
 };
diff --git a/runtime/oat/image.cc b/runtime/oat/image.cc
index b62a4e0621..309f36edbc 100644
--- a/runtime/oat/image.cc
+++ b/runtime/oat/image.cc
@@ -30,6 +30,7 @@
 #include "mirror/object-inl.h"
 #include "mirror/object_array-inl.h"
 #include "mirror/object_array.h"
+#include "oat.h"
 
 namespace art HIDDEN {
 
@@ -72,7 +73,7 @@ ImageHeader::ImageHeader(uint32_t image_reservation_size,
   CHECK_EQ(image_begin, RoundUp(image_begin, kElfSegmentAlignment));
   if (oat_checksum != 0u) {
     CHECK_EQ(oat_file_begin, RoundUp(oat_file_begin, kElfSegmentAlignment));
-    CHECK_EQ(oat_data_begin, RoundUp(oat_data_begin, kElfSegmentAlignment));
+    CHECK_EQ(oat_data_begin, RoundUp(oat_data_begin, alignof(OatHeader)));
     CHECK_LT(image_roots, oat_file_begin);
     CHECK_LE(oat_file_begin, oat_data_begin);
     CHECK_LT(oat_data_begin, oat_data_end);
diff --git a/runtime/oat/oat.cc b/runtime/oat/oat.cc
index 0cb22065b0..9882b0fa51 100644
--- a/runtime/oat/oat.cc
+++ b/runtime/oat/oat.cc
@@ -17,9 +17,10 @@
 #include "oat.h"
 
 #include <string.h>
+#include <zlib.h>
 
+#include "android-base/logging.h"
 #include "android-base/stringprintf.h"
-
 #include "arch/instruction_set.h"
 #include "arch/instruction_set_features.h"
 #include "base/bit_utils.h"
@@ -37,6 +38,13 @@ static size_t ComputeOatHeaderSize(const SafeMap<std::string, std::string>* vari
     for ( ; it != end; ++it) {
       estimate += it->first.length() + 1;
       estimate += it->second.length() + 1;
+
+      size_t non_deterministic_field_length = OatHeader::GetNonDeterministicFieldLength(it->first);
+      if (non_deterministic_field_length > 0u) {
+        DCHECK_LE(it->second.length(), non_deterministic_field_length);
+        size_t padding = non_deterministic_field_length - it->second.length();
+        estimate += padding;
+      }
     }
   }
   return sizeof(OatHeader) + estimate;
@@ -45,7 +53,8 @@ static size_t ComputeOatHeaderSize(const SafeMap<std::string, std::string>* vari
 OatHeader* OatHeader::Create(InstructionSet instruction_set,
                              const InstructionSetFeatures* instruction_set_features,
                              uint32_t dex_file_count,
-                             const SafeMap<std::string, std::string>* variable_data) {
+                             const SafeMap<std::string, std::string>* variable_data,
+                             uint32_t base_oat_offset) {
   // Estimate size of optional data.
   size_t needed_size = ComputeOatHeaderSize(variable_data);
 
@@ -56,19 +65,30 @@ OatHeader* OatHeader::Create(InstructionSet instruction_set,
   return new (memory) OatHeader(instruction_set,
                                 instruction_set_features,
                                 dex_file_count,
-                                variable_data);
+                                variable_data,
+                                base_oat_offset);
+}
+
+void OatHeader::Delete(OatHeader* header) {
+  if (header != nullptr) {
+    size_t size = header->GetHeaderSize();
+    header->~OatHeader();
+    operator delete(header, size);
+  }
 }
 
 OatHeader::OatHeader(InstructionSet instruction_set,
                      const InstructionSetFeatures* instruction_set_features,
                      uint32_t dex_file_count,
-                     const SafeMap<std::string, std::string>* variable_data)
+                     const SafeMap<std::string, std::string>* variable_data,
+                     uint32_t base_oat_offset)
     : oat_checksum_(0u),
       instruction_set_(instruction_set),
       instruction_set_features_bitmap_(instruction_set_features->AsBitmap()),
       dex_file_count_(dex_file_count),
       oat_dex_files_offset_(0),
       bcp_bss_info_offset_(0),
+      base_oat_offset_(base_oat_offset),
       executable_offset_(0),
       jni_dlsym_lookup_trampoline_offset_(0),
       jni_dlsym_lookup_critical_trampoline_offset_(0),
@@ -100,7 +120,9 @@ bool OatHeader::IsValid() const {
   if (version_ != kOatVersion) {
     return false;
   }
-  if (!IsAligned<kElfSegmentAlignment>(executable_offset_)) {
+  // Only check the offset is valid after it has been set.
+  if (executable_offset_ != 0u &&
+      !IsAligned<kElfSegmentAlignment>(executable_offset_ + base_oat_offset_)) {
     return false;
   }
   if (!IsValidInstructionSet(instruction_set_)) {
@@ -122,7 +144,9 @@ std::string OatHeader::GetValidationErrorMessage() const {
                         kOatVersion[0], kOatVersion[1], kOatVersion[2], kOatVersion[3],
                         version_[0], version_[1], version_[2], version_[3]);
   }
-  if (!IsAligned<kElfSegmentAlignment>(executable_offset_)) {
+  // Only check the offset is valid after it has been set.
+  if (executable_offset_ != 0u &&
+      !IsAligned<kElfSegmentAlignment>(executable_offset_ + base_oat_offset_)) {
     return "Executable offset not properly aligned.";
   }
   if (!IsValidInstructionSet(instruction_set_)) {
@@ -199,13 +223,13 @@ void OatHeader::SetBcpBssInfoOffset(uint32_t bcp_info_offset) {
 
 uint32_t OatHeader::GetExecutableOffset() const {
   DCHECK(IsValid());
-  DCHECK_ALIGNED(executable_offset_, kElfSegmentAlignment);
+  DCHECK_ALIGNED(executable_offset_ + base_oat_offset_, kElfSegmentAlignment);
   CHECK_GT(executable_offset_, sizeof(OatHeader));
   return executable_offset_;
 }
 
 void OatHeader::SetExecutableOffset(uint32_t executable_offset) {
-  DCHECK_ALIGNED(executable_offset, kElfSegmentAlignment);
+  DCHECK_ALIGNED(executable_offset + base_oat_offset_, kElfSegmentAlignment);
   CHECK_GT(executable_offset, sizeof(OatHeader));
   DCHECK(IsValid());
   DCHECK_EQ(executable_offset_, 0U);
@@ -349,69 +373,81 @@ const uint8_t* OatHeader::GetKeyValueStore() const {
   return key_value_store_;
 }
 
-const char* OatHeader::GetStoreValueByKey(const char* key) const {
+const char* OatHeader::GetStoreValueByKeyUnsafe(const char* key) const {
   std::string_view key_view(key);
-  const char* ptr = reinterpret_cast<const char*>(&key_value_store_);
-  const char* end = ptr + key_value_store_size_;
-
-  while (ptr < end) {
-    // Scan for a closing zero.
-    const char* str_end = reinterpret_cast<const char*>(memchr(ptr, 0, end - ptr));
-    if (UNLIKELY(str_end == nullptr)) {
-      LOG(WARNING) << "OatHeader: Unterminated key in key value store.";
-      return nullptr;
-    }
-    const char* value_start = str_end + 1;
-    const char* value_end =
-        reinterpret_cast<const char*>(memchr(value_start, 0, end - value_start));
-    if (UNLIKELY(value_end == nullptr)) {
-      LOG(WARNING) << "OatHeader: Unterminated value in key value store.";
-      return nullptr;
-    }
-    if (key_view == std::string_view(ptr, str_end - ptr)) {
+
+  uint32_t offset = 0;
+  const char* current_key;
+  const char* value;
+  while (GetNextStoreKeyValuePair(&offset, &current_key, &value)) {
+    if (key_view == current_key) {
       // Same as key.
-      return value_start;
+      return value;
     }
-    // Different from key. Advance over the value.
-    ptr = value_end + 1;
   }
+
   // Not found.
   return nullptr;
 }
 
-bool OatHeader::GetStoreKeyValuePairByIndex(size_t index,
-                                            const char** key,
-                                            const char** value) const {
-  const char* ptr = reinterpret_cast<const char*>(&key_value_store_);
-  const char* end = ptr + key_value_store_size_;
-  size_t counter = index;
+bool OatHeader::GetNextStoreKeyValuePair(/*inout*/ uint32_t* offset,
+                                         /*out*/ const char** key,
+                                         /*out*/ const char** value) const {
+  if (*offset >= key_value_store_size_) {
+    return false;
+  }
 
-  while (ptr < end) {
-    // Scan for a closing zero.
-    const char* str_end = reinterpret_cast<const char*>(memchr(ptr, 0, end - ptr));
-    if (UNLIKELY(str_end == nullptr)) {
-      LOG(WARNING) << "OatHeader: Unterminated key in key value store.";
-      return false;
-    }
-    const char* value_start = str_end + 1;
-    const char* value_end =
-        reinterpret_cast<const char*>(memchr(value_start, 0, end - value_start));
-    if (UNLIKELY(value_end == nullptr)) {
-      LOG(WARNING) << "OatHeader: Unterminated value in key value store.";
+  const char* start = reinterpret_cast<const char*>(&key_value_store_);
+  const char* ptr = start + *offset;
+  const char* end = start + key_value_store_size_;
+
+  // Scan for a closing zero.
+  const char* str_end = reinterpret_cast<const char*>(memchr(ptr, 0, end - ptr));
+  if (UNLIKELY(str_end == nullptr)) {
+    LOG(WARNING) << "OatHeader: Unterminated key in key value store.";
+    return false;
+  }
+  const char* value_start = str_end + 1;
+  const char* value_end = reinterpret_cast<const char*>(memchr(value_start, 0, end - value_start));
+  if (UNLIKELY(value_end == nullptr)) {
+    LOG(WARNING) << "OatHeader: Unterminated value in key value store.";
+    return false;
+  }
+
+  *key = ptr;
+  *value = value_start;
+
+  // Advance over the value.
+  size_t key_len = str_end - ptr;
+  size_t value_len = value_end - value_start;
+  size_t non_deterministic_field_length = GetNonDeterministicFieldLength(*key);
+  if (non_deterministic_field_length > 0u) {
+    if (UNLIKELY(value_len > non_deterministic_field_length)) {
+      LOG(WARNING) << "OatHeader: Non-deterministic field too long in key value store.";
       return false;
     }
-    if (counter == 0) {
-      *key = ptr;
-      *value = value_start;
-      return true;
-    } else {
-      --counter;
+    *offset += key_len + 1 + non_deterministic_field_length + 1;
+  } else {
+    *offset += key_len + 1 + value_len + 1;
+  }
+
+  return true;
+}
+
+void OatHeader::ComputeChecksum(/*inout*/ uint32_t* checksum) const {
+  *checksum = adler32(*checksum, reinterpret_cast<const uint8_t*>(this), sizeof(OatHeader));
+
+  uint32_t last_offset = 0;
+  uint32_t offset = 0;
+  const char* key;
+  const char* value;
+  while (GetNextStoreKeyValuePair(&offset, &key, &value)) {
+    if (IsDeterministicField(key)) {
+      // Update the checksum.
+      *checksum = adler32(*checksum, GetKeyValueStore() + last_offset, offset - last_offset);
     }
-    // Advance over the value.
-    ptr = value_end + 1;
+    last_offset = offset;
   }
-  // Not found.
-  return false;
 }
 
 size_t OatHeader::GetHeaderSize() const {
@@ -462,6 +498,14 @@ void OatHeader::Flatten(const SafeMap<std::string, std::string>* key_value_store
       data_ptr += it->first.length() + 1;
       strlcpy(data_ptr, it->second.c_str(), it->second.length() + 1);
       data_ptr += it->second.length() + 1;
+
+      size_t non_deterministic_field_length = GetNonDeterministicFieldLength(it->first);
+      if (non_deterministic_field_length > 0u) {
+        DCHECK_LE(it->second.length(), non_deterministic_field_length);
+        size_t padding = non_deterministic_field_length - it->second.length();
+        memset(data_ptr, 0, padding);
+        data_ptr += padding;
+      }
     }
   }
   key_value_store_size_ = data_ptr - reinterpret_cast<char*>(&key_value_store_);
diff --git a/runtime/oat/oat.h b/runtime/oat/oat.h
index 2b5c21e129..2069b569e3 100644
--- a/runtime/oat/oat.h
+++ b/runtime/oat/oat.h
@@ -18,6 +18,9 @@
 #define ART_RUNTIME_OAT_OAT_H_
 
 #include <array>
+#include <cstddef>
+#include <string_view>
+#include <utility>
 #include <vector>
 
 #include "base/compiler_filter.h"
@@ -44,8 +47,8 @@ std::ostream& operator<<(std::ostream& stream, StubType stub_type);
 class EXPORT PACKED(4) OatHeader {
  public:
   static constexpr std::array<uint8_t, 4> kOatMagic { { 'o', 'a', 't', '\n' } };
-  // Last oat version changed reason: reland "arm64: Store resolved MethodType-s in .bss."
-  static constexpr std::array<uint8_t, 4> kOatVersion{{'2', '5', '4', '\0'}};
+  // Last oat version changed reason: Ensure oat checksum determinism across hosts and devices.
+  static constexpr std::array<uint8_t, 4> kOatVersion{{'2', '5', '9', '\0'}};
 
   static constexpr const char* kDex2OatCmdLineKey = "dex2oat-cmdline";
   static constexpr const char* kDebuggableKey = "debuggable";
@@ -59,6 +62,34 @@ class EXPORT PACKED(4) OatHeader {
   static constexpr const char* kCompilationReasonKey = "compilation-reason";
   static constexpr const char* kRequiresImage = "requires-image";
 
+  // Fields listed here are key value store fields that are deterministic across hosts and devices,
+  // meaning they should have exactly the same value when the oat file is generated on different
+  // hosts and devices for the same app / boot image and for the same device model with the same
+  // compiler options. If you are adding a new field that doesn't hold this property, put it in
+  // `kNonDeterministicFieldsAndLengths` and assign a length limit.
+  //
+  // When writing the oat header, the non-deterministic fields are padded to their length limits and
+  // excluded from the oat checksum computation. This makes the oat checksum deterministic across
+  // hosts and devices, which is important for Cloud Compilation, where we generate an oat file on a
+  // host and use it on a device.
+  static constexpr std::array<std::string_view, 9> kDeterministicFields{
+      kDebuggableKey,
+      kNativeDebuggableKey,
+      kCompilerFilter,
+      kClassPathKey,
+      kBootClassPathKey,
+      kBootClassPathChecksumsKey,
+      kConcurrentCopying,
+      kCompilationReasonKey,
+      kRequiresImage,
+  };
+
+  static constexpr std::array<std::pair<std::string_view, size_t>, 2>
+      kNonDeterministicFieldsAndLengths{
+          std::make_pair(kDex2OatCmdLineKey, 2048),
+          std::make_pair(kApexVersionsKey, 1024),
+      };
+
   static constexpr const char kTrueValue[] = "true";
   static constexpr const char kFalseValue[] = "false";
 
@@ -66,7 +97,27 @@ class EXPORT PACKED(4) OatHeader {
   static OatHeader* Create(InstructionSet instruction_set,
                            const InstructionSetFeatures* instruction_set_features,
                            uint32_t dex_file_count,
-                           const SafeMap<std::string, std::string>* variable_data);
+                           const SafeMap<std::string, std::string>* variable_data,
+                           uint32_t base_oat_offset = 0u);
+  static void Delete(OatHeader* header);
+
+  static constexpr bool IsDeterministicField(std::string_view key) {
+    for (std::string_view field : kDeterministicFields) {
+      if (field == key) {
+        return true;
+      }
+    }
+    return false;
+  }
+
+  static constexpr size_t GetNonDeterministicFieldLength(std::string_view key) {
+    for (auto [field, length] : kNonDeterministicFieldsAndLengths) {
+      if (field == key) {
+        return length;
+      }
+    }
+    return 0;
+  }
 
   bool IsValid() const;
   std::string GetValidationErrorMessage() const;
@@ -113,8 +164,21 @@ class EXPORT PACKED(4) OatHeader {
 
   uint32_t GetKeyValueStoreSize() const;
   const uint8_t* GetKeyValueStore() const;
-  const char* GetStoreValueByKey(const char* key) const;
-  bool GetStoreKeyValuePairByIndex(size_t index, const char** key, const char** value) const;
+  const char* GetStoreValueByKeyUnsafe(const char* key) const;
+
+  const char* GetStoreValueByKey(const char* key) const {
+    // Do not get apex versions from the oat header directly. Use `OatFile::GetApexVersions`
+    // instead.
+    DCHECK_NE(std::string_view(key), kApexVersionsKey);
+    return GetStoreValueByKeyUnsafe(key);
+  }
+
+  // Returns the next key-value pair, at the given offset. On success, updates `offset`.
+  // The expected use case is to start the iteration with an offset initialized to zero and
+  // repeatedly call this function with the same offset pointer, until the function returns false.
+  bool GetNextStoreKeyValuePair(/*inout*/ uint32_t* offset,
+                                /*out*/ const char** key,
+                                /*out*/ const char** value) const;
 
   size_t GetHeaderSize() const;
   bool IsDebuggable() const;
@@ -125,13 +189,16 @@ class EXPORT PACKED(4) OatHeader {
 
   const uint8_t* GetOatAddress(StubType type) const;
 
+  void ComputeChecksum(/*inout*/ uint32_t* checksum) const;
+
  private:
   bool KeyHasValue(const char* key, const char* value, size_t value_size) const;
 
   OatHeader(InstructionSet instruction_set,
             const InstructionSetFeatures* instruction_set_features,
             uint32_t dex_file_count,
-            const SafeMap<std::string, std::string>* variable_data);
+            const SafeMap<std::string, std::string>* variable_data,
+            uint32_t base_oat_offset);
 
   // Returns true if the value of the given key is "true", false otherwise.
   bool IsKeyEnabled(const char* key) const;
@@ -147,6 +214,10 @@ class EXPORT PACKED(4) OatHeader {
   uint32_t dex_file_count_;
   uint32_t oat_dex_files_offset_;
   uint32_t bcp_bss_info_offset_;
+  // Offset of the oat header (i.e. start of the oat data) in the ELF file.
+  // It is used to additional validation of the oat header as it is not
+  // page-aligned in the memory.
+  uint32_t base_oat_offset_;
   uint32_t executable_offset_;
   uint32_t jni_dlsym_lookup_trampoline_offset_;
   uint32_t jni_dlsym_lookup_critical_trampoline_offset_;
diff --git a/runtime/oat/oat_file.cc b/runtime/oat/oat_file.cc
index 3e298e5590..ec845cd0c1 100644
--- a/runtime/oat/oat_file.cc
+++ b/runtime/oat/oat_file.cc
@@ -20,26 +20,37 @@
 #include <sys/stat.h>
 #include <unistd.h>
 
+#include <cstddef>
 #include <cstdint>
+#include <cstdio>
 #include <cstdlib>
 #include <cstring>
+#include <memory>
+#include <optional>
 #include <sstream>
+#include <string>
 #include <type_traits>
 
+#include "android-base/file.h"
 #include "android-base/logging.h"
 #include "android-base/stringprintf.h"
 #include "arch/instruction_set_features.h"
 #include "art_method.h"
+#include "base/array_ref.h"
 #include "base/bit_vector.h"
 #include "base/file_utils.h"
+#include "base/globals.h"
 #include "base/logging.h"  // For VLOG_IS_ON.
+#include "base/macros.h"
 #include "base/mem_map.h"
 #include "base/os.h"
 #include "base/pointer_size.h"
 #include "base/stl_util.h"
 #include "base/systrace.h"
+#include "base/time_utils.h"
 #include "base/unix_file/fd_file.h"
 #include "base/utils.h"
+#include "base/zip_archive.h"
 #include "class_loader_context.h"
 #include "dex/art_dex_file_loader.h"
 #include "dex/dex_file.h"
@@ -57,6 +68,7 @@
 #include "mirror/class.h"
 #include "mirror/object-inl.h"
 #include "oat.h"
+#include "oat/sdc_file.h"
 #include "oat_file-inl.h"
 #include "oat_file_manager.h"
 #include "runtime-inl.h"
@@ -67,6 +79,21 @@
 #include <link.h>  // for dl_iterate_phdr.
 #endif
 
+#ifdef __GLIBC__
+#include <gnu/libc-version.h>  // for gnu_get_libc_version.
+// strverscmp is part of the GNU/Linux extension, so define _GNU_SOURCE before including
+// string.h, and undefine it afterward if it is not already defined.
+#ifndef _GNU_SOURCE
+#define _GNU_SOURCE
+#define DEFINED_GNU_SOURCE
+#endif
+#include <string.h>  // for strverscmp
+#ifdef DEFINED_GNU_SOURCE
+#undef _GNU_SOURCE
+#undef DEFINED_GNU_SOURCE
+#endif
+#endif
+
 // dlopen_ext support from bionic.
 #ifdef ART_TARGET_ANDROID
 #include "android/dlext.h"
@@ -90,6 +117,32 @@ static constexpr bool kUseDlopenOnHost = true;
 // For debugging, Open will print DlOpen error message if set to true.
 static constexpr bool kPrintDlOpenErrorMessage = false;
 
+// Returns whether dlopen can load dynamic shared objects with a read-only .dynamic section.
+// According to the ELF spec whether .dynamic is writable or not is determined by the operating
+// system and processor (Book I, part 1 "Object Files", "Special sections"). Bionic and glibc
+// >= 2.35 support read-only .dynamic. Older glibc versions have a bug that causes a crash if
+// this section is read-only: https://sourceware.org/bugzilla/show_bug.cgi?id=28340.
+bool IsReadOnlyDynamicSupportedByDlOpen() {
+  // The following lambda will be executed only once as a part of a static
+  // variable initialization.
+#ifdef __GLIBC__
+  static bool is_ro_dynamic_supported = []() {
+    // libc version has the following format:
+    //   "X.Y"
+    // where:
+    //   X - major version in the decimal format.
+    //   Y - minor version in the decimal format.
+    // for example:
+    //    "2.34"
+    const char* libc_version = gnu_get_libc_version();
+    return strverscmp(libc_version, "2.35") >= 0;
+  }();
+  return is_ro_dynamic_supported;
+#else
+  return true;
+#endif
+}
+
 // Note for OatFileBase and descendents:
 //
 // These are used in OatFile::Open to try all our loaders.
@@ -111,7 +164,6 @@ class OatFileBase : public OatFile {
                                   const std::string& vdex_filename,
                                   const std::string& elf_filename,
                                   const std::string& location,
-                                  bool writable,
                                   bool executable,
                                   bool low_4gb,
                                   ArrayRef<const std::string> dex_filenames,
@@ -125,7 +177,6 @@ class OatFileBase : public OatFile {
                                   int oat_fd,
                                   const std::string& vdex_filename,
                                   const std::string& oat_filename,
-                                  bool writable,
                                   bool executable,
                                   bool low_4gb,
                                   ArrayRef<const std::string> dex_filenames,
@@ -133,6 +184,14 @@ class OatFileBase : public OatFile {
                                   /*inout*/ MemMap* reservation,  // Where to load if not null.
                                   /*out*/ std::string* error_msg);
 
+  template <typename kOatFileBaseSubType>
+  static OatFileBase* OpenOatFileFromSdm(const std::string& sdm_filename,
+                                         const std::string& sdc_filename,
+                                         const std::string& dm_filename,
+                                         const std::string& dex_filename,
+                                         bool executable,
+                                         /*out*/ std::string* error_msg);
+
  protected:
   OatFileBase(const std::string& filename, bool executable) : OatFile(filename, executable) {}
 
@@ -141,30 +200,24 @@ class OatFileBase : public OatFile {
 
   virtual void PreLoad() = 0;
 
-  bool LoadVdex(const std::string& vdex_filename,
-                bool writable,
-                bool low_4gb,
-                std::string* error_msg);
+  bool LoadVdex(const std::string& vdex_filename, bool low_4gb, std::string* error_msg);
 
   bool LoadVdex(int vdex_fd,
                 const std::string& vdex_filename,
-                bool writable,
                 bool low_4gb,
                 std::string* error_msg);
 
   virtual bool Load(const std::string& elf_filename,
-                    bool writable,
                     bool executable,
                     bool low_4gb,
-                    /*inout*/MemMap* reservation,  // Where to load if not null.
-                    /*out*/std::string* error_msg) = 0;
+                    /*inout*/ MemMap* reservation,  // Where to load if not null.
+                    /*out*/ std::string* error_msg) = 0;
 
   virtual bool Load(int oat_fd,
-                    bool writable,
                     bool executable,
                     bool low_4gb,
-                    /*inout*/MemMap* reservation,  // Where to load if not null.
-                    /*out*/std::string* error_msg) = 0;
+                    /*inout*/ MemMap* reservation,  // Where to load if not null.
+                    /*out*/ std::string* error_msg) = 0;
 
   bool ComputeFields(const std::string& file_path, std::string* error_msg);
 
@@ -215,7 +268,6 @@ OatFileBase* OatFileBase::OpenOatFile(int zip_fd,
                                       const std::string& vdex_filename,
                                       const std::string& elf_filename,
                                       const std::string& location,
-                                      bool writable,
                                       bool executable,
                                       bool low_4gb,
                                       ArrayRef<const std::string> dex_filenames,
@@ -226,12 +278,7 @@ OatFileBase* OatFileBase::OpenOatFile(int zip_fd,
 
   ret->PreLoad();
 
-  if (!ret->Load(elf_filename,
-                 writable,
-                 executable,
-                 low_4gb,
-                 reservation,
-                 error_msg)) {
+  if (!ret->Load(elf_filename, executable, low_4gb, reservation, error_msg)) {
     return nullptr;
   }
 
@@ -241,7 +288,7 @@ OatFileBase* OatFileBase::OpenOatFile(int zip_fd,
 
   ret->PreSetup(elf_filename);
 
-  if (!ret->LoadVdex(vdex_filename, writable, low_4gb, error_msg)) {
+  if (!ret->LoadVdex(vdex_filename, low_4gb, error_msg)) {
     return nullptr;
   }
 
@@ -258,7 +305,6 @@ OatFileBase* OatFileBase::OpenOatFile(int zip_fd,
                                       int oat_fd,
                                       const std::string& vdex_location,
                                       const std::string& oat_location,
-                                      bool writable,
                                       bool executable,
                                       bool low_4gb,
                                       ArrayRef<const std::string> dex_filenames,
@@ -267,12 +313,7 @@ OatFileBase* OatFileBase::OpenOatFile(int zip_fd,
                                       /*out*/ std::string* error_msg) {
   std::unique_ptr<OatFileBase> ret(new kOatFileBaseSubType(oat_location, executable));
 
-  if (!ret->Load(oat_fd,
-                 writable,
-                 executable,
-                 low_4gb,
-                 reservation,
-                 error_msg)) {
+  if (!ret->Load(oat_fd, executable, low_4gb, reservation, error_msg)) {
     return nullptr;
   }
 
@@ -282,7 +323,7 @@ OatFileBase* OatFileBase::OpenOatFile(int zip_fd,
 
   ret->PreSetup(oat_location);
 
-  if (!ret->LoadVdex(vdex_fd, vdex_location, writable, low_4gb, error_msg)) {
+  if (!ret->LoadVdex(vdex_fd, vdex_location, low_4gb, error_msg)) {
     return nullptr;
   }
 
@@ -293,15 +334,66 @@ OatFileBase* OatFileBase::OpenOatFile(int zip_fd,
   return ret.release();
 }
 
-bool OatFileBase::LoadVdex(const std::string& vdex_filename,
-                           bool writable,
-                           bool low_4gb,
-                           std::string* error_msg) {
+template <typename kOatFileBaseSubType>
+OatFileBase* OatFileBase::OpenOatFileFromSdm(const std::string& sdm_filename,
+                                             const std::string& sdc_filename,
+                                             const std::string& dm_filename,
+                                             const std::string& dex_filename,
+                                             bool executable,
+                                             /*out*/ std::string* error_msg) {
+  std::string elf_filename = sdm_filename + kZipSeparator + "primary.odex";
+  std::unique_ptr<OatFileBase> ret(new kOatFileBaseSubType(elf_filename, executable));
+
+  struct stat sdm_st;
+  if (stat(sdm_filename.c_str(), &sdm_st) != 0) {
+    *error_msg = ART_FORMAT("Failed to stat sdm file '{}': {}", sdm_filename, strerror(errno));
+    return nullptr;
+  }
+
+  std::unique_ptr<SdcReader> sdc_reader = SdcReader::Load(sdc_filename, error_msg);
+  if (sdc_reader == nullptr) {
+    return nullptr;
+  }
+  if (sdc_reader->GetSdmTimestampNs() != TimeSpecToNs(sdm_st.st_mtim)) {
+    // The sdm file had been replaced after the sdc file was created.
+    *error_msg = ART_FORMAT("Obsolete sdc file '{}'", sdc_filename);
+    return nullptr;
+  }
+  // The apex-versions value in the sdc file, written by ART Service, is the value of
+  // `Runtime::GetApexVersions` at the time where the sdm file was first seen on device. We use it
+  // to override the APEX versions in the oat header. This is for detecting samegrade placebos.
+  ret->override_apex_versions_ = sdc_reader->GetApexVersions();
+
+  if (!ret->Load(elf_filename, executable, /*low_4gb=*/false, /*reservation=*/nullptr, error_msg)) {
+    return nullptr;
+  }
+
+  if (!ret->ComputeFields(elf_filename, error_msg)) {
+    return nullptr;
+  }
+
+  ret->PreSetup(elf_filename);
+
+  ret->vdex_ = VdexFile::OpenFromDm(dm_filename, ret->vdex_begin_, ret->vdex_end_, error_msg);
+  if (ret->vdex_ == nullptr) {
+    return nullptr;
+  }
+
+  if (!ret->Setup(/*zip_fd=*/-1,
+                  ArrayRef<const std::string>(&dex_filename, /*size=*/1u),
+                  /*dex_files=*/{},
+                  error_msg)) {
+    return nullptr;
+  }
+
+  return ret.release();
+}
+
+bool OatFileBase::LoadVdex(const std::string& vdex_filename, bool low_4gb, std::string* error_msg) {
   vdex_ = VdexFile::OpenAtAddress(vdex_begin_,
                                   vdex_end_ - vdex_begin_,
                                   /*mmap_reuse=*/vdex_begin_ != nullptr,
                                   vdex_filename,
-                                  writable,
                                   low_4gb,
                                   error_msg);
   if (vdex_.get() == nullptr) {
@@ -315,7 +407,6 @@ bool OatFileBase::LoadVdex(const std::string& vdex_filename,
 
 bool OatFileBase::LoadVdex(int vdex_fd,
                            const std::string& vdex_filename,
-                           bool writable,
                            bool low_4gb,
                            std::string* error_msg) {
   if (vdex_fd != -1) {
@@ -328,9 +419,9 @@ bool OatFileBase::LoadVdex(int vdex_fd,
                                       vdex_end_ - vdex_begin_,
                                       /*mmap_reuse=*/vdex_begin_ != nullptr,
                                       vdex_fd,
+                                      /*start=*/0,
                                       s.st_size,
                                       vdex_filename,
-                                      writable,
                                       low_4gb,
                                       error_msg);
       if (vdex_.get() == nullptr) {
@@ -767,9 +858,14 @@ bool OatFileBase::Setup(int zip_fd,
     std::string dex_file_name = dex_file_location;
     if (!dex_filenames.empty()) {
       dex_file_name.replace(/*pos*/ 0u, primary_location.size(), primary_location_replacement);
-      // If the location does not contain path and matches the file name component,
-      // use the provided file name also as the location.
-      // TODO: Do we need this for anything other than tests?
+      // If the location (the `--dex-location` passed to dex2oat) only contains the basename and
+      // matches the basename in the provided file name, use the provided file name also as the
+      // location.
+      // This is needed when the location on device is unknown at compile-time, typically during
+      // Cloud Compilation because the compilation is done on the server and the apk is later
+      // installed on device into `/data/app/<random_string>`.
+      // This is not needed during dexpreopt because the location on device is known to be a certain
+      // location in /system, /product, etc.
       if (dex_file_location.find('/') == std::string::npos &&
           dex_file_name.size() > dex_file_location.size() &&
           dex_file_name[dex_file_name.size() - dex_file_location.size() - 1u] == '/' &&
@@ -1152,14 +1248,12 @@ class DlOpenOatFile final : public OatFileBase {
   void PreLoad() override;
 
   bool Load(const std::string& elf_filename,
-            bool writable,
             bool executable,
             bool low_4gb,
-            /*inout*/MemMap* reservation,  // Where to load if not null.
-            /*out*/std::string* error_msg) override;
+            /*inout*/ MemMap* reservation,  // Where to load if not null.
+            /*out*/ std::string* error_msg) override;
 
   bool Load([[maybe_unused]] int oat_fd,
-            [[maybe_unused]] bool writable,
             [[maybe_unused]] bool executable,
             [[maybe_unused]] bool low_4gb,
             [[maybe_unused]] /*inout*/ MemMap* reservation,
@@ -1235,11 +1329,10 @@ void DlOpenOatFile::PreLoad() {
 }
 
 bool DlOpenOatFile::Load(const std::string& elf_filename,
-                         bool writable,
                          bool executable,
                          bool low_4gb,
-                         /*inout*/MemMap* reservation,  // Where to load if not null.
-                         /*out*/std::string* error_msg) {
+                         /*inout*/ MemMap* reservation,  // Where to load if not null.
+                         /*out*/ std::string* error_msg) {
   // Use dlopen only when flagged to do so, and when it's OK to load things executable.
   // TODO: Also try when not executable? The issue here could be re-mapping as writable (as
   //       !executable is a sign that we may want to patch), which may not be allowed for
@@ -1252,15 +1345,16 @@ bool DlOpenOatFile::Load(const std::string& elf_filename,
     *error_msg = "DlOpen does not support low 4gb loading.";
     return false;
   }
-  if (writable) {
-    *error_msg = "DlOpen does not support writable loading.";
-    return false;
-  }
   if (!executable) {
     *error_msg = "DlOpen does not support non-executable loading.";
     return false;
   }
 
+  if (!IsReadOnlyDynamicSupportedByDlOpen()) {
+    *error_msg = "DlOpen does not support read-only .dynamic section.";
+    return false;
+  }
+
   // dlopen always returns the same library if it is already opened on the host. For this reason
   // we only use dlopen if we are the target or we do not already have the dex file opened. Having
   // the same library loaded multiple times at different addresses is required for class unloading
@@ -1309,11 +1403,11 @@ bool DlOpenOatFile::Dlopen(const std::string& elf_filename,
   return false;
 #else
   {
-    UniqueCPtr<char> absolute_path(realpath(elf_filename.c_str(), nullptr));
-    if (absolute_path == nullptr) {
-      *error_msg = StringPrintf("Failed to find absolute path for '%s'", elf_filename.c_str());
-      return false;
-    }
+    // `elf_filename` is in the format of `/path/to/oat` or `/path/to/zip!/primary.odex`. We can
+    // reuse `GetDexCanonicalLocation` to resolve the real path of the part before "!" even though
+    // `elf_filename` does not refer to a dex file.
+    static_assert(std::string_view(kZipSeparator).starts_with(DexFileLoader::kMultiDexSeparator));
+    std::string absolute_path = DexFileLoader::GetDexCanonicalLocation(elf_filename.c_str());
 #ifdef ART_TARGET_ANDROID
     android_dlextinfo extinfo = {};
     extinfo.flags = ANDROID_DLEXT_FORCE_LOAD;   // Force-load, don't reuse handle
@@ -1329,9 +1423,9 @@ bool DlOpenOatFile::Dlopen(const std::string& elf_filename,
     }
 
     if (strncmp(kAndroidArtApexDefaultPath,
-                absolute_path.get(),
+                absolute_path.c_str(),
                 sizeof(kAndroidArtApexDefaultPath) - 1) != 0 ||
-        absolute_path.get()[sizeof(kAndroidArtApexDefaultPath) - 1] != '/') {
+        absolute_path.c_str()[sizeof(kAndroidArtApexDefaultPath) - 1] != '/') {
       // Use the system namespace for OAT files outside the ART APEX. Search
       // paths and links don't matter here, but permitted paths do, and the
       // system namespace is configured to allow loading from all appropriate
@@ -1340,7 +1434,7 @@ bool DlOpenOatFile::Dlopen(const std::string& elf_filename,
       extinfo.library_namespace = GetSystemLinkerNamespace();
     }
 
-    dlopen_handle_ = android_dlopen_ext(absolute_path.get(), RTLD_NOW, &extinfo);
+    dlopen_handle_ = android_dlopen_ext(absolute_path.c_str(), RTLD_NOW, &extinfo);
     if (reservation != nullptr && dlopen_handle_ != nullptr) {
       // Find used pages from the reservation.
       struct dl_iterate_context {
@@ -1414,7 +1508,7 @@ bool DlOpenOatFile::Dlopen(const std::string& elf_filename,
       return false;
     }
     MutexLock mu(Thread::Current(), *Locks::host_dlopen_handles_lock_);
-    dlopen_handle_ = dlopen(absolute_path.get(), RTLD_NOW);
+    dlopen_handle_ = dlopen(absolute_path.c_str(), RTLD_NOW);
     if (dlopen_handle_ != nullptr) {
       if (!host_dlopen_handles_.insert(dlopen_handle_).second) {
         dlclose(dlopen_handle_);
@@ -1576,12 +1670,6 @@ class ElfOatFile final : public OatFileBase {
  public:
   ElfOatFile(const std::string& filename, bool executable) : OatFileBase(filename, executable) {}
 
-  bool InitializeFromElfFile(int zip_fd,
-                             ElfFile* elf_file,
-                             VdexFile* vdex_file,
-                             ArrayRef<const std::string> dex_filenames,
-                             std::string* error_msg);
-
  protected:
   const uint8_t* FindDynamicSymbolAddress(const std::string& symbol_name,
                                           std::string* error_msg) const override {
@@ -1596,18 +1684,16 @@ class ElfOatFile final : public OatFileBase {
   }
 
   bool Load(const std::string& elf_filename,
-            bool writable,
             bool executable,
             bool low_4gb,
-            /*inout*/MemMap* reservation,  // Where to load if not null.
-            /*out*/std::string* error_msg) override;
+            /*inout*/ MemMap* reservation,  // Where to load if not null.
+            /*out*/ std::string* error_msg) override;
 
   bool Load(int oat_fd,
-            bool writable,
             bool executable,
             bool low_4gb,
-            /*inout*/MemMap* reservation,  // Where to load if not null.
-            /*out*/std::string* error_msg) override;
+            /*inout*/ MemMap* reservation,  // Where to load if not null.
+            /*out*/ std::string* error_msg) override;
 
   void PreSetup([[maybe_unused]] const std::string& elf_filename) override {}
 
@@ -1617,11 +1703,13 @@ class ElfOatFile final : public OatFileBase {
 
  private:
   bool ElfFileOpen(File* file,
-                   bool writable,
+                   off_t start,
+                   size_t file_length,
+                   const std::string& file_location,
                    bool executable,
                    bool low_4gb,
-                   /*inout*/MemMap* reservation,  // Where to load if not null.
-                   /*out*/std::string* error_msg);
+                   /*inout*/ MemMap* reservation,  // Where to load if not null.
+                   /*out*/ std::string* error_msg);
 
  private:
   // Backing memory map for oat file during cross compilation.
@@ -1630,64 +1718,48 @@ class ElfOatFile final : public OatFileBase {
   DISALLOW_COPY_AND_ASSIGN(ElfOatFile);
 };
 
-bool ElfOatFile::InitializeFromElfFile(int zip_fd,
-                                       ElfFile* elf_file,
-                                       VdexFile* vdex_file,
-                                       ArrayRef<const std::string> dex_filenames,
-                                       std::string* error_msg) {
-  ScopedTrace trace(__PRETTY_FUNCTION__);
-  if (IsExecutable()) {
-    *error_msg = "Cannot initialize from elf file in executable mode.";
-    return false;
-  }
-  elf_file_.reset(elf_file);
-  SetVdex(vdex_file);
-  uint64_t offset, size;
-  bool has_section = elf_file->GetSectionOffsetAndSize(".rodata", &offset, &size);
-  CHECK(has_section);
-  SetBegin(elf_file->Begin() + offset);
-  SetEnd(elf_file->Begin() + size + offset);
-  // Ignore the optional .bss section when opening non-executable.
-  return Setup(zip_fd, dex_filenames, /*dex_files=*/{}, error_msg);
-}
-
 bool ElfOatFile::Load(const std::string& elf_filename,
-                      bool writable,
                       bool executable,
                       bool low_4gb,
-                      /*inout*/MemMap* reservation,
-                      /*out*/std::string* error_msg) {
+                      /*inout*/ MemMap* reservation,
+                      /*out*/ std::string* error_msg) {
   ScopedTrace trace(__PRETTY_FUNCTION__);
-  std::unique_ptr<File> file(OS::OpenFileForReading(elf_filename.c_str()));
+
+  // Mirrors the alignment in the Bionic's dlopen. Actually, ART's MemMap only requires 4096 byte
+  // alignment, but we want to be more strict here, to reflect what the Bionic's dlopen would be
+  // able to load.
+  auto [file, start, length] = OS::OpenFileDirectlyOrFromZip(
+      elf_filename, kZipSeparator, /*alignment=*/MemMap::GetPageSize(), error_msg);
   if (file == nullptr) {
-    *error_msg = StringPrintf("Failed to open oat filename for reading: %s", strerror(errno));
     return false;
   }
-  return ElfOatFile::ElfFileOpen(file.get(),
-                                 writable,
-                                 executable,
-                                 low_4gb,
-                                 reservation,
-                                 error_msg);
+
+  return ElfOatFile::ElfFileOpen(
+      file.get(), start, length, elf_filename, executable, low_4gb, reservation, error_msg);
 }
 
 bool ElfOatFile::Load(int oat_fd,
-                      bool writable,
                       bool executable,
                       bool low_4gb,
-                      /*inout*/MemMap* reservation,
-                      /*out*/std::string* error_msg) {
+                      /*inout*/ MemMap* reservation,
+                      /*out*/ std::string* error_msg) {
   ScopedTrace trace(__PRETTY_FUNCTION__);
   if (oat_fd != -1) {
     int duped_fd = DupCloexec(oat_fd);
     std::unique_ptr<File> file = std::make_unique<File>(duped_fd, false);
     if (file == nullptr) {
-      *error_msg = StringPrintf("Failed to open oat filename for reading: %s",
-                                strerror(errno));
+      *error_msg = StringPrintf("Failed to open oat file for reading: %s", strerror(errno));
+      return false;
+    }
+    int64_t file_length = file->GetLength();
+    if (file_length < 0) {
+      *error_msg = StringPrintf("Failed to get file length of oat file: %s", strerror(errno));
       return false;
     }
     return ElfOatFile::ElfFileOpen(file.get(),
-                                   writable,
+                                   /*start=*/0,
+                                   file_length,
+                                   file->GetPath(),
                                    executable,
                                    low_4gb,
                                    reservation,
@@ -1697,22 +1769,20 @@ bool ElfOatFile::Load(int oat_fd,
 }
 
 bool ElfOatFile::ElfFileOpen(File* file,
-                             bool writable,
+                             off_t start,
+                             size_t file_length,
+                             const std::string& file_location,
                              bool executable,
                              bool low_4gb,
-                             /*inout*/MemMap* reservation,
-                             /*out*/std::string* error_msg) {
+                             /*inout*/ MemMap* reservation,
+                             /*out*/ std::string* error_msg) {
   ScopedTrace trace(__PRETTY_FUNCTION__);
-  elf_file_.reset(ElfFile::Open(file,
-                                writable,
-                                /*program_header_only=*/true,
-                                low_4gb,
-                                error_msg));
+  elf_file_.reset(ElfFile::Open(file, start, file_length, file_location, low_4gb, error_msg));
   if (elf_file_ == nullptr) {
     DCHECK(!error_msg->empty());
     return false;
   }
-  bool loaded = elf_file_->Load(file, executable, low_4gb, reservation, error_msg);
+  bool loaded = elf_file_->Load(executable, low_4gb, reservation, error_msg);
   DCHECK(loaded || !error_msg->empty());
   return loaded;
 }
@@ -1720,7 +1790,12 @@ bool ElfOatFile::ElfFileOpen(File* file,
 class OatFileBackedByVdex final : public OatFileBase {
  public:
   explicit OatFileBackedByVdex(const std::string& filename)
-      : OatFileBase(filename, /*executable=*/false) {}
+      : OatFileBase(filename, /*executable=*/false),
+        oat_header_(nullptr) {}
+
+  ~OatFileBackedByVdex() {
+    OatHeader::Delete(oat_header_);
+  }
 
   static OatFileBackedByVdex* Open(const std::vector<const DexFile*>& dex_files,
                                    std::unique_ptr<VdexFile>&& vdex_file,
@@ -1862,11 +1937,11 @@ class OatFileBackedByVdex final : public OatFileBase {
       store.Put(OatHeader::kClassPathKey, context->EncodeContextForOatFile(""));
     }
 
-    oat_header_.reset(OatHeader::Create(kRuntimeQuickCodeISA,
-                                        isa_features.get(),
-                                        number_of_dex_files,
-                                        &store));
-    const uint8_t* begin = reinterpret_cast<const uint8_t*>(oat_header_.get());
+    oat_header_ = OatHeader::Create(kRuntimeQuickCodeISA,
+                                    isa_features.get(),
+                                    number_of_dex_files,
+                                    &store);
+    const uint8_t* begin = reinterpret_cast<const uint8_t*>(oat_header_);
     SetBegin(begin);
     SetEnd(begin + oat_header_->GetHeaderSize());
   }
@@ -1875,7 +1950,6 @@ class OatFileBackedByVdex final : public OatFileBase {
   void PreLoad() override {}
 
   bool Load([[maybe_unused]] const std::string& elf_filename,
-            [[maybe_unused]] bool writable,
             [[maybe_unused]] bool executable,
             [[maybe_unused]] bool low_4gb,
             [[maybe_unused]] MemMap* reservation,
@@ -1885,7 +1959,6 @@ class OatFileBackedByVdex final : public OatFileBase {
   }
 
   bool Load([[maybe_unused]] int oat_fd,
-            [[maybe_unused]] bool writable,
             [[maybe_unused]] bool executable,
             [[maybe_unused]] bool low_4gb,
             [[maybe_unused]] MemMap* reservation,
@@ -1909,7 +1982,7 @@ class OatFileBackedByVdex final : public OatFileBase {
   }
 
  private:
-  std::unique_ptr<OatHeader> oat_header_;
+  OatHeader* oat_header_;
 
   DISALLOW_COPY_AND_ASSIGN(OatFileBackedByVdex);
 };
@@ -1951,7 +2024,6 @@ OatFile* OatFile::Open(int zip_fd,
                                                                  vdex_filename,
                                                                  oat_filename,
                                                                  oat_location,
-                                                                 /*writable=*/false,
                                                                  executable,
                                                                  low_4gb,
                                                                  dex_filenames,
@@ -1981,7 +2053,6 @@ OatFile* OatFile::Open(int zip_fd,
                                                                 vdex_filename,
                                                                 oat_filename,
                                                                 oat_location,
-                                                                /*writable=*/false,
                                                                 executable,
                                                                 low_4gb,
                                                                 dex_filenames,
@@ -2010,7 +2081,6 @@ OatFile* OatFile::Open(int zip_fd,
                                                                 oat_fd,
                                                                 vdex_location,
                                                                 oat_location,
-                                                                /*writable=*/false,
                                                                 executable,
                                                                 low_4gb,
                                                                 dex_filenames,
@@ -2037,6 +2107,38 @@ OatFile* OatFile::OpenFromVdex(int zip_fd,
   return OatFileBackedByVdex::Open(zip_fd, std::move(vdex_file), location, context, error_msg);
 }
 
+OatFile* OatFile::OpenFromSdm(const std::string& sdm_filename,
+                              const std::string& sdc_filename,
+                              const std::string& dm_filename,
+                              const std::string& dex_filename,
+                              bool executable,
+                              /*out*/ std::string* error_msg) {
+  ScopedTrace trace("Open sdm file " + sdm_filename);
+  CHECK(!sdm_filename.empty());
+  CHECK(!sdc_filename.empty());
+  CHECK(!dm_filename.empty());
+  CHECK(!dex_filename.empty());
+
+  // Check if the dm file exists, to fail fast. The dm file contains the vdex that is essential for
+  // using the odex in the sdm file.
+  if (!OS::FileExists(dm_filename.c_str())) {
+    *error_msg =
+        ART_FORMAT("Not loading sdm file because dm file '{}' does not exist", dm_filename);
+    return nullptr;
+  }
+
+  // Try dlopen first, as it is required for native debuggability. This will fail fast if dlopen is
+  // disabled.
+  OatFile* with_dlopen = OatFileBase::OpenOatFileFromSdm<DlOpenOatFile>(
+      sdm_filename, sdc_filename, dm_filename, dex_filename, executable, error_msg);
+  if (with_dlopen != nullptr) {
+    return with_dlopen;
+  }
+
+  return OatFileBase::OpenOatFileFromSdm<ElfOatFile>(
+      sdm_filename, sdc_filename, dm_filename, dex_filename, executable, error_msg);
+}
+
 OatFile::OatFile(const std::string& location, bool is_executable)
     : location_(location),
       vdex_(nullptr),
@@ -2589,4 +2691,13 @@ bool OatFile::IsBackedByVdexOnly() const {
   return oat_dex_files_storage_.size() >= 1 && oat_dex_files_storage_[0]->IsBackedByVdexOnly();
 }
 
+std::optional<std::string_view> OatFile::GetApexVersions() const {
+  if (override_apex_versions_.has_value()) {
+    return override_apex_versions_;
+  }
+  const char* oat_apex_versions =
+      GetOatHeader().GetStoreValueByKeyUnsafe(OatHeader::kApexVersionsKey);
+  return oat_apex_versions != nullptr ? std::make_optional(oat_apex_versions) : std::nullopt;
+}
+
 }  // namespace art
diff --git a/runtime/oat/oat_file.h b/runtime/oat/oat_file.h
index 39304f6a8c..33645fa4ba 100644
--- a/runtime/oat/oat_file.h
+++ b/runtime/oat/oat_file.h
@@ -19,6 +19,7 @@
 
 #include <list>
 #include <memory>
+#include <optional>
 #include <string>
 #include <string_view>
 #include <vector>
@@ -102,6 +103,11 @@ class PACKED(4) OatMethodOffsets {
 
 class OatFile {
  public:
+  // The zip separator. This has to be the one that Bionic's dlopen recognizes because oat files are
+  // opened through dlopen in `DlOpenOatFile`. This is different from the ART's zip separator for
+  // MultiDex.
+  static constexpr const char* kZipSeparator = "!/";
+
   // Open an oat file. Returns null on failure.
   // The `dex_filenames` argument, if provided, overrides the dex locations
   // from oat file when opening the dex files if they are not embedded in the
@@ -181,6 +187,13 @@ class OatFile {
                                ClassLoaderContext* context,
                                std::string* error_msg);
 
+  static OatFile* OpenFromSdm(const std::string& sdm_filename,
+                              const std::string& sdc_filename,
+                              const std::string& dm_filename,
+                              const std::string& dex_filename,
+                              bool executable,
+                              /*out*/ std::string* error_msg);
+
   // Set the start of the app image.
   // Needed for initializing app image relocations in the .data.img.rel.ro section.
   void SetAppImageBegin(uint8_t* app_image_begin) const {
@@ -421,6 +434,8 @@ class OatFile {
   // Returns the mapping info of `dex_file` if found in the BcpBssInfo, or nullptr otherwise.
   const BssMappingInfo* FindBcpMappingInfo(const DexFile* dex_file) const;
 
+  std::optional<std::string_view> GetApexVersions() const;
+
  protected:
   OatFile(const std::string& filename, bool executable);
 
@@ -513,6 +528,9 @@ class OatFile {
   // by the `dex_filenames` parameter, in case the OatFile does not embed the dex code.
   std::vector<std::unique_ptr<const DexFile>> external_dex_files_;
 
+  // If set, overrides the APEX versions in the header.
+  std::optional<std::string> override_apex_versions_ = std::nullopt;
+
   friend class gc::collector::FakeOatFile;  // For modifying begin_ and end_.
   friend class OatClass;
   friend class art::OatDexFile;
diff --git a/runtime/oat/oat_file_assistant.cc b/runtime/oat/oat_file_assistant.cc
index 7ee65db1a5..53f412908d 100644
--- a/runtime/oat/oat_file_assistant.cc
+++ b/runtime/oat/oat_file_assistant.cc
@@ -21,6 +21,8 @@
 #include <memory>
 #include <optional>
 #include <sstream>
+#include <string_view>
+#include <utility>
 #include <vector>
 
 #include "android-base/file.h"
@@ -49,9 +51,9 @@
 #include "gc/space/image_space.h"
 #include "image.h"
 #include "oat.h"
+#include "oat/oat_file.h"
 #include "oat_file_assistant_context.h"
 #include "runtime.h"
-#include "runtime_globals.h"
 #include "scoped_thread_state_change-inl.h"
 #include "vdex_file.h"
 #include "zlib.h"
@@ -114,12 +116,6 @@ OatFileAssistant::OatFileAssistant(const char* dex_location,
       isa_(isa),
       load_executable_(load_executable),
       only_load_trusted_executable_(only_load_trusted_executable),
-      odex_(this, /*is_oat_location=*/false),
-      oat_(this, /*is_oat_location=*/true),
-      vdex_for_odex_(this, /*is_oat_location=*/false),
-      vdex_for_oat_(this, /*is_oat_location=*/true),
-      dm_for_odex_(this, /*is_oat_location=*/false),
-      dm_for_oat_(this, /*is_oat_location=*/true),
       zip_fd_(zip_fd) {
   CHECK(dex_location != nullptr) << "OatFileAssistant: null dex location";
   CHECK_IMPLIES(load_executable, context != nullptr) << "Loading executable without a context";
@@ -166,66 +162,93 @@ OatFileAssistant::OatFileAssistant(const char* dex_location,
   // Get the odex filename.
   std::string error_msg;
   std::string odex_file_name;
-  if (DexLocationToOdexFilename(dex_location_, isa_, &odex_file_name, &error_msg)) {
-    odex_.Reset(odex_file_name, UseFdToReadFiles(), zip_fd, vdex_fd, oat_fd);
-    std::string vdex_file_name = GetVdexFilename(odex_file_name);
-    // We dup FDs as the odex_ will claim ownership.
-    vdex_for_odex_.Reset(vdex_file_name,
-                         UseFdToReadFiles(),
-                         DupCloexec(zip_fd),
-                         DupCloexec(vdex_fd),
-                         DupCloexec(oat_fd));
-
-    std::string dm_file_name = GetDmFilename(dex_location_);
-    dm_for_odex_.Reset(dm_file_name,
-                       UseFdToReadFiles(),
-                       DupCloexec(zip_fd),
-                       DupCloexec(vdex_fd),
-                       DupCloexec(oat_fd));
-  } else {
+  if (!DexLocationToOdexFilename(dex_location_, isa_, &odex_file_name, &error_msg)) {
     LOG(WARNING) << "Failed to determine odex file name: " << error_msg;
   }
 
+  // Get the oat filename.
+  std::string oat_file_name;
   if (!UseFdToReadFiles()) {
-    // Get the oat filename.
-    std::string oat_file_name;
-    if (DexLocationToOatFilename(dex_location_,
-                                 isa_,
-                                 GetRuntimeOptions().deny_art_apex_data_files,
-                                 &oat_file_name,
-                                 &error_msg)) {
-      oat_.Reset(oat_file_name, /*use_fd=*/false);
-      std::string vdex_file_name = GetVdexFilename(oat_file_name);
-      vdex_for_oat_.Reset(vdex_file_name, UseFdToReadFiles(), zip_fd, vdex_fd, oat_fd);
-      std::string dm_file_name = GetDmFilename(dex_location);
-      dm_for_oat_.Reset(dm_file_name, UseFdToReadFiles(), zip_fd, vdex_fd, oat_fd);
-    } else if (kIsTargetAndroid) {
-      // No need to warn on host. We are probably in oatdump, where we only need OatFileAssistant to
-      // validate BCP checksums.
-      LOG(WARNING) << "Failed to determine oat file name for dex location " << dex_location_ << ": "
-                   << error_msg;
+    if (!DexLocationToOatFilename(dex_location_,
+                                  isa_,
+                                  GetRuntimeOptions().deny_art_apex_data_files,
+                                  &oat_file_name,
+                                  &error_msg)) {
+      if (kIsTargetAndroid) {
+        // No need to warn on host. We are probably in oatdump, where we only need OatFileAssistant
+        // to validate BCP checksums.
+        LOG(WARNING) << "Failed to determine oat file name for dex location " << dex_location_
+                     << ": " << error_msg;
+      }
     }
   }
 
-  // Check if the dex directory is writable.
-  // This will be needed in most uses of OatFileAssistant and so it's OK to
-  // compute it eagerly. (the only use which will not make use of it is
-  // OatFileAssistant::GetStatusDump())
-  size_t pos = dex_location_.rfind('/');
-  if (pos == std::string::npos) {
-    LOG(WARNING) << "Failed to determine dex file parent directory: " << dex_location_;
-  } else if (!UseFdToReadFiles()) {
-    // We cannot test for parent access when using file descriptors. That's ok
-    // because in this case we will always pick the odex file anyway.
-    std::string parent = dex_location_.substr(0, pos);
-    if (access(parent.c_str(), W_OK) == 0) {
-      dex_parent_writable_ = true;
-    } else {
-      VLOG(oat) << "Dex parent of " << dex_location_ << " is not writable: " << strerror(errno);
-    }
+  if (!oat_file_name.empty() && !UseFdToReadFiles()) {
+    // The oat location. This is for apps on readonly filesystems (typically, system apps and
+    // incremental apps). This must be prioritized over the odex location, because the odex location
+    // probably has the dexpreopt artifacts for such apps.
+    info_list_.push_back(std::make_unique<OatFileInfoBackedByOat>(this,
+                                                                  oat_file_name,
+                                                                  /*is_oat_location=*/true,
+                                                                  /*use_fd=*/false));
+    info_list_.push_back(
+        std::make_unique<OatFileInfoBackedBySdm>(this,
+                                                 GetSdmFilename(dex_location_, isa),
+                                                 /*is_oat_location=*/true,
+                                                 GetDmFilename(dex_location_),
+                                                 GetSdcFilename(oat_file_name)));
+  }
+
+  if (!odex_file_name.empty()) {
+    // The odex location, which is the most common.
+    info_list_.push_back(std::make_unique<OatFileInfoBackedByOat>(this,
+                                                                  odex_file_name,
+                                                                  /*is_oat_location=*/false,
+                                                                  UseFdToReadFiles(),
+                                                                  zip_fd,
+                                                                  vdex_fd,
+                                                                  oat_fd));
+    info_list_.push_back(
+        std::make_unique<OatFileInfoBackedBySdm>(this,
+                                                 GetSdmFilename(dex_location_, isa),
+                                                 /*is_oat_location=*/false,
+                                                 GetDmFilename(dex_location_),
+                                                 GetSdcFilename(odex_file_name)));
+  }
+
+  // When there is no odex/oat available (e.g., they are both out of date), we look for a useable
+  // vdex file.
+
+  if (!oat_file_name.empty() && !UseFdToReadFiles()) {
+    // The vdex-only file next to 'oat_`.
+    info_list_.push_back(std::make_unique<OatFileInfoBackedByVdex>(this,
+                                                                   GetVdexFilename(oat_file_name),
+                                                                   /*is_oat_location=*/true,
+                                                                   /*use_fd=*/false));
+  }
+
+  if (!odex_file_name.empty()) {
+    // The vdex-only file next to `odex_`.
+    // We dup FDs as the odex_ will claim ownership.
+    info_list_.push_back(std::make_unique<OatFileInfoBackedByVdex>(this,
+                                                                   GetVdexFilename(odex_file_name),
+                                                                   /*is_oat_location=*/false,
+                                                                   UseFdToReadFiles(),
+                                                                   DupCloexec(zip_fd),
+                                                                   DupCloexec(vdex_fd)));
+  }
+
+  if (!UseFdToReadFiles()) {
+    // A .dm file may be available, look for it.
+    info_list_.push_back(
+        std::make_unique<OatFileInfoBackedByDm>(this, GetDmFilename(dex_location_)));
   }
 }
 
+// Must be defined outside of the class, to prevent inlining, which causes callers to access hidden
+// symbols used by the destructor. `NOINLINE` doesn't work.
+OatFileAssistant::~OatFileAssistant() = default;
+
 std::unique_ptr<OatFileAssistant> OatFileAssistant::Create(
     const std::string& filename,
     const std::string& isa_str,
@@ -311,13 +334,10 @@ int OatFileAssistant::GetDexOptNeeded(CompilerFilter::Filter target_compiler_fil
                                       bool profile_changed,
                                       bool downgrade) {
   OatFileInfo& info = GetBestInfo();
-  if (info.CheckDisableCompactDex()) {  // TODO(b/256664509): Clean this up.
-    VLOG(oat) << "Should recompile: disable cdex";
-    return kDex2OatFromScratch;
-  }
   DexOptNeeded dexopt_needed = info.GetDexOptNeeded(
       target_compiler_filter, GetDexOptTrigger(target_compiler_filter, profile_changed, downgrade));
-  if (dexopt_needed != kNoDexOptNeeded && (&info == &dm_for_oat_ || &info == &dm_for_odex_)) {
+  if (dexopt_needed != kNoDexOptNeeded &&
+      (info.GetType() == OatFileType::kDm || info.GetType() == OatFileType::kSdm)) {
     // The usable vdex file is in the DM file. This information cannot be encoded in the integer.
     // Return kDex2OatFromScratch so that neither the vdex in the "oat" location nor the vdex in the
     // "odex" location will be picked by installd.
@@ -333,10 +353,6 @@ bool OatFileAssistant::GetDexOptNeeded(CompilerFilter::Filter target_compiler_fi
                                        DexOptTrigger dexopt_trigger,
                                        /*out*/ DexOptStatus* dexopt_status) {
   OatFileInfo& info = GetBestInfo();
-  if (info.CheckDisableCompactDex()) {  // TODO(b/256664509): Clean this up.
-    dexopt_status->location_ = kLocationNoneOrError;
-    return true;
-  }
   DexOptNeeded dexopt_needed = info.GetDexOptNeeded(target_compiler_filter, dexopt_trigger);
   dexopt_status->location_ = GetLocation(info);
   return dexopt_needed != kNoDexOptNeeded;
@@ -348,53 +364,6 @@ std::unique_ptr<OatFile> OatFileAssistant::GetBestOatFile() {
   return GetBestInfo().ReleaseFileForUse();
 }
 
-std::string OatFileAssistant::GetStatusDump() {
-  std::ostringstream status;
-  bool oat_file_exists = false;
-  bool odex_file_exists = false;
-  if (oat_.Status() != kOatCannotOpen) {
-    // If we can open the file, Filename should not return null.
-    CHECK(oat_.Filename() != nullptr);
-
-    oat_file_exists = true;
-    status << *oat_.Filename() << "[status=" << oat_.Status() << ", ";
-    const OatFile* file = oat_.GetFile();
-    if (file == nullptr) {
-      // If the file is null even though the status is not kOatCannotOpen, it
-      // means we must have a vdex file with no corresponding oat file. In
-      // this case we cannot determine the compilation filter. Indicate that
-      // we have only the vdex file instead.
-      status << "vdex-only";
-    } else {
-      status << "compilation_filter=" << CompilerFilter::NameOfFilter(file->GetCompilerFilter());
-    }
-  }
-
-  if (odex_.Status() != kOatCannotOpen) {
-    // If we can open the file, Filename should not return null.
-    CHECK(odex_.Filename() != nullptr);
-
-    odex_file_exists = true;
-    if (oat_file_exists) {
-      status << "] ";
-    }
-    status << *odex_.Filename() << "[status=" << odex_.Status() << ", ";
-    const OatFile* file = odex_.GetFile();
-    if (file == nullptr) {
-      status << "vdex-only";
-    } else {
-      status << "compilation_filter=" << CompilerFilter::NameOfFilter(file->GetCompilerFilter());
-    }
-  }
-
-  if (!oat_file_exists && !odex_file_exists) {
-    status << "invalid[";
-  }
-
-  status << "]";
-  return status.str();
-}
-
 std::vector<std::unique_ptr<const DexFile>> OatFileAssistant::LoadDexFiles(
     const OatFile& oat_file, const char* dex_location) {
   std::vector<std::unique_ptr<const DexFile>> dex_files;
@@ -451,9 +420,23 @@ std::optional<bool> OatFileAssistant::HasDexFiles(std::string* error_msg) {
   return checksum.has_value();
 }
 
-OatFileAssistant::OatStatus OatFileAssistant::OdexFileStatus() { return odex_.Status(); }
+OatFileAssistant::OatStatus OatFileAssistant::OdexFileStatus() {
+  for (const std::unique_ptr<OatFileInfo>& info : info_list_) {
+    if (info->GetType() == OatFileType::kOat && !info->IsOatLocation()) {
+      return info->Status();
+    }
+  }
+  return kOatCannotOpen;
+}
 
-OatFileAssistant::OatStatus OatFileAssistant::OatFileStatus() { return oat_.Status(); }
+OatFileAssistant::OatStatus OatFileAssistant::OatFileStatus() {
+  for (const std::unique_ptr<OatFileInfo>& info : info_list_) {
+    if (info->GetType() == OatFileType::kOat && info->IsOatLocation()) {
+      return info->Status();
+    }
+  }
+  return kOatCannotOpen;
+}
 
 bool OatFileAssistant::DexChecksumUpToDate(const OatFile& file, std::string* error_msg) {
   if (!file.ContainsDexCode()) {
@@ -494,55 +477,48 @@ bool OatFileAssistant::DexChecksumUpToDate(const OatFile& file, std::string* err
   return true;
 }
 
-OatFileAssistant::OatStatus OatFileAssistant::GivenOatFileStatus(const OatFile& file) {
+OatFileAssistant::OatStatus OatFileAssistant::GivenOatFileStatus(const OatFile& file,
+                                                                 /*out*/ std::string* error_msg) {
   // Verify the ART_USE_READ_BARRIER state.
   // TODO: Don't fully reject files due to read barrier state. If they contain
   // compiled code and are otherwise okay, we should return something like
   // kOatRelocationOutOfDate. If they don't contain compiled code, the read
   // barrier state doesn't matter.
   if (file.GetOatHeader().IsConcurrentCopying() != gUseReadBarrier) {
+    *error_msg = "Read barrier state mismatch";
     return kOatCannotOpen;
   }
 
   // Verify the dex checksum.
-  std::string error_msg;
-  if (!DexChecksumUpToDate(file, &error_msg)) {
-    LOG(ERROR) << error_msg;
+  if (!DexChecksumUpToDate(file, error_msg)) {
+    LOG(ERROR) << *error_msg;
     return kOatDexOutOfDate;
   }
 
   CompilerFilter::Filter current_compiler_filter = file.GetCompilerFilter();
 
   // Verify the image checksum
-  if (file.IsBackedByVdexOnly()) {
-    VLOG(oat) << "Image checksum test skipped for vdex file " << file.GetLocation();
-  } else if (CompilerFilter::DependsOnImageChecksum(current_compiler_filter)) {
-    if (!ValidateBootClassPathChecksums(file)) {
-      VLOG(oat) << "Oat image checksum does not match image checksum.";
+  if (!file.IsBackedByVdexOnly() &&
+      CompilerFilter::DependsOnImageChecksum(current_compiler_filter)) {
+    if (!ValidateBootClassPathChecksums(file, error_msg)) {
       return kOatBootImageOutOfDate;
     }
     if (!gc::space::ImageSpace::ValidateApexVersions(
-            file.GetOatHeader(),
-            GetOatFileAssistantContext()->GetApexVersions(),
-            file.GetLocation(),
-            &error_msg)) {
-      VLOG(oat) << error_msg;
+            file, GetOatFileAssistantContext()->GetApexVersions(), error_msg)) {
       return kOatBootImageOutOfDate;
     }
-  } else {
-    VLOG(oat) << "Image checksum test skipped for compiler filter " << current_compiler_filter;
   }
 
   // The constraint is only enforced if the zip has uncompressed dex code.
   if (only_load_trusted_executable_ &&
       !LocationIsTrusted(file.GetLocation(), !GetRuntimeOptions().deny_art_apex_data_files) &&
       file.ContainsDexCode() && ZipFileOnlyContainsUncompressedDex()) {
-    LOG(ERROR) << "Not loading " << dex_location_
-               << ": oat file has dex code, but APK has uncompressed dex code";
+    *error_msg = "Oat file has dex code, but APK has uncompressed dex code";
+    LOG(ERROR) << "Not loading " << dex_location_ << ": " << *error_msg;
     return kOatDexOutOfDate;
   }
 
-  if (!ClassLoaderContextIsOkay(file)) {
+  if (!ClassLoaderContextIsOkay(file, error_msg)) {
     return kOatContextOutOfDate;
   }
 
@@ -836,29 +812,23 @@ bool OatFileAssistant::ValidateBootClassPathChecksums(OatFileAssistantContext* o
   return true;
 }
 
-bool OatFileAssistant::ValidateBootClassPathChecksums(const OatFile& oat_file) {
+bool OatFileAssistant::ValidateBootClassPathChecksums(const OatFile& oat_file,
+                                                      /*out*/ std::string* error_msg) {
   // Get the checksums and the BCP from the oat file.
   const char* oat_boot_class_path_checksums =
       oat_file.GetOatHeader().GetStoreValueByKey(OatHeader::kBootClassPathChecksumsKey);
   const char* oat_boot_class_path =
       oat_file.GetOatHeader().GetStoreValueByKey(OatHeader::kBootClassPathKey);
   if (oat_boot_class_path_checksums == nullptr || oat_boot_class_path == nullptr) {
+    *error_msg = "Missing boot image information from oat file";
     return false;
   }
 
-  std::string error_msg;
-  bool result = ValidateBootClassPathChecksums(GetOatFileAssistantContext(),
-                                               isa_,
-                                               oat_boot_class_path_checksums,
-                                               oat_boot_class_path,
-                                               &error_msg);
-  if (!result) {
-    VLOG(oat) << "Failed to verify checksums of oat file " << oat_file.GetLocation()
-              << " error: " << error_msg;
-    return false;
-  }
-
-  return true;
+  return ValidateBootClassPathChecksums(GetOatFileAssistantContext(),
+                                        isa_,
+                                        oat_boot_class_path_checksums,
+                                        oat_boot_class_path,
+                                        error_msg);
 }
 
 bool OatFileAssistant::IsPrimaryBootImageUsable() {
@@ -867,80 +837,40 @@ bool OatFileAssistant::IsPrimaryBootImageUsable() {
 
 OatFileAssistant::OatFileInfo& OatFileAssistant::GetBestInfo() {
   ScopedTrace trace("GetBestInfo");
-  // TODO(calin): Document the side effects of class loading when
-  // running dalvikvm command line.
-  if (dex_parent_writable_ || UseFdToReadFiles()) {
-    // If the parent of the dex file is writable it means that we can
-    // create the odex file. In this case we unconditionally pick the odex
-    // as the best oat file. This corresponds to the regular use case when
-    // apps gets installed or when they load private, secondary dex file.
-    // For apps on the system partition the odex location will not be
-    // writable and thus the oat location might be more up to date.
-
-    // If the odex is not useable, and we have a useable vdex, return the vdex
-    // instead.
-    VLOG(oat) << ART_FORMAT("GetBestInfo checking odex next to the dex file ({})",
-                            odex_.DisplayFilename());
-    if (!odex_.IsUseable()) {
-      VLOG(oat) << ART_FORMAT("GetBestInfo checking vdex next to the dex file ({})",
-                              vdex_for_odex_.DisplayFilename());
-      if (vdex_for_odex_.IsUseable()) {
-        return vdex_for_odex_;
+
+  for (const std::unique_ptr<OatFileInfo>& info : info_list_) {
+    if (VLOG_IS_ON(oat) && info->FileExists()) {
+      std::string error_msg;
+      OatStatus status = info->Status(&error_msg);
+      std::string message = ART_FORMAT("GetBestInfo: {} ({}) is {}",
+                                       info->GetLocationDebugString(),
+                                       info->DisplayFilename(),
+                                       fmt::streamed(status));
+      const OatFile* file = info->GetFile();
+      if (file != nullptr) {
+        message += ART_FORMAT(" with filter '{}' executable '{}'",
+                              fmt::streamed(file->GetCompilerFilter()),
+                              file->IsExecutable());
       }
-      VLOG(oat) << ART_FORMAT("GetBestInfo checking dm ({})", dm_for_odex_.DisplayFilename());
-      if (dm_for_odex_.IsUseable()) {
-        return dm_for_odex_;
+      if (!info->IsUseable()) {
+        message += ": " + error_msg;
       }
+      VLOG(oat) << message;
     }
-    return odex_;
-  }
-
-  // We cannot write to the odex location. This must be a system app.
-
-  // If the oat location is useable take it.
-  VLOG(oat) << ART_FORMAT("GetBestInfo checking odex in dalvik-cache ({})", oat_.DisplayFilename());
-  if (oat_.IsUseable()) {
-    return oat_;
-  }
 
-  // The oat file is not useable but the odex file might be up to date.
-  // This is an indication that we are dealing with an up to date prebuilt
-  // (that doesn't need relocation).
-  VLOG(oat) << ART_FORMAT("GetBestInfo checking odex next to the dex file ({})",
-                          odex_.DisplayFilename());
-  if (odex_.IsUseable()) {
-    return odex_;
+    if (info->IsUseable()) {
+      return *info;
+    }
   }
 
-  // Look for a useable vdex file.
-  VLOG(oat) << ART_FORMAT("GetBestInfo checking vdex in dalvik-cache ({})",
-                          vdex_for_oat_.DisplayFilename());
-  if (vdex_for_oat_.IsUseable()) {
-    return vdex_for_oat_;
-  }
-  VLOG(oat) << ART_FORMAT("GetBestInfo checking vdex next to the dex file ({})",
-                          vdex_for_odex_.DisplayFilename());
-  if (vdex_for_odex_.IsUseable()) {
-    return vdex_for_odex_;
-  }
-  VLOG(oat) << ART_FORMAT("GetBestInfo checking dm ({})", dm_for_oat_.DisplayFilename());
-  if (dm_for_oat_.IsUseable()) {
-    return dm_for_oat_;
-  }
-  // TODO(jiakaiz): Is this the same as above?
-  VLOG(oat) << ART_FORMAT("GetBestInfo checking dm ({})", dm_for_odex_.DisplayFilename());
-  if (dm_for_odex_.IsUseable()) {
-    return dm_for_odex_;
+  // No usable artifact. Pick the oat or odex if they exist, or empty info if not.
+  VLOG(oat) << ART_FORMAT("GetBestInfo: {} has no usable artifacts", dex_location_);
+  for (const std::unique_ptr<OatFileInfo>& info : info_list_) {
+    if (info->GetType() == OatFileType::kOat && info->Status() != kOatCannotOpen) {
+      return *info;
+    }
   }
-
-  // We got into the worst situation here:
-  // - the oat location is not useable
-  // - the prebuild odex location is not up to date
-  // - the vdex-only file is not useable
-  // - and we don't have the original dex file anymore (stripped).
-  // Pick the odex if it exists, or the oat if not.
-  VLOG(oat) << "GetBestInfo no usable artifacts";
-  return (odex_.Status() == kOatCannotOpen) ? oat_ : odex_;
+  return empty_info_;
 }
 
 std::unique_ptr<gc::space::ImageSpace> OatFileAssistant::OpenImageSpace(const OatFile* oat_file) {
@@ -958,18 +888,12 @@ std::unique_ptr<gc::space::ImageSpace> OatFileAssistant::OpenImageSpace(const Oa
   return ret;
 }
 
-OatFileAssistant::OatFileInfo::OatFileInfo(OatFileAssistant* oat_file_assistant,
-                                           bool is_oat_location)
-    : oat_file_assistant_(oat_file_assistant), is_oat_location_(is_oat_location) {}
+bool OatFileAssistant::OatFileInfo::IsOatLocation() const { return is_oat_location_; }
 
-bool OatFileAssistant::OatFileInfo::IsOatLocation() { return is_oat_location_; }
+const std::string* OatFileAssistant::OatFileInfo::Filename() const { return &filename_; }
 
-const std::string* OatFileAssistant::OatFileInfo::Filename() {
-  return filename_provided_ ? &filename_ : nullptr;
-}
-
-const char* OatFileAssistant::OatFileInfo::DisplayFilename() {
-  return filename_provided_ ? filename_.c_str() : "unknown";
+const char* OatFileAssistant::OatFileInfo::DisplayFilename() const {
+  return !filename_.empty() ? filename_.c_str() : "unknown";
 }
 
 bool OatFileAssistant::OatFileInfo::IsUseable() {
@@ -986,20 +910,22 @@ bool OatFileAssistant::OatFileInfo::IsUseable() {
   }
 }
 
-OatFileAssistant::OatStatus OatFileAssistant::OatFileInfo::Status() {
+OatFileAssistant::OatStatus OatFileAssistant::OatFileInfo::Status(/*out*/ std::string* error_msg) {
   ScopedTrace trace("Status");
-  if (!status_attempted_) {
-    status_attempted_ = true;
-    const OatFile* file = GetFile();
+  if (!status_.has_value()) {
+    std::string temp_error_msg;
+    const OatFile* file = GetFile(&temp_error_msg);
     if (file == nullptr) {
-      status_ = kOatCannotOpen;
+      status_ = std::make_pair(kOatCannotOpen, std::move(temp_error_msg));
     } else {
-      status_ = oat_file_assistant_->GivenOatFileStatus(*file);
-      VLOG(oat) << file->GetLocation() << " is " << status_ << " with filter "
-                << file->GetCompilerFilter();
+      status_ = std::make_pair(oat_file_assistant_->GivenOatFileStatus(*file, &temp_error_msg),
+                               std::move(temp_error_msg));
     }
   }
-  return status_;
+  if (error_msg != nullptr) {
+    *error_msg = status_->second;
+  }
+  return status_->first;
 }
 
 OatFileAssistant::DexOptNeeded OatFileAssistant::OatFileInfo::GetDexOptNeeded(
@@ -1035,115 +961,143 @@ OatFileAssistant::DexOptNeeded OatFileAssistant::OatFileInfo::GetDexOptNeeded(
   }
 }
 
-const OatFile* OatFileAssistant::OatFileInfo::GetFile() {
+bool OatFileAssistant::OatFileInfo::FileExists() const {
+  return !filename_.empty() && OS::FileExists(filename_.c_str());
+}
+
+bool OatFileAssistant::OatFileInfoBackedByOat::FileExists() const {
+  return use_fd_ || OatFileInfo::FileExists();
+}
+
+bool OatFileAssistant::OatFileInfoBackedBySdm::FileExists() const {
+  return OatFileInfo::FileExists() && OS::FileExists(sdc_filename_.c_str());
+}
+
+bool OatFileAssistant::OatFileInfoBackedByVdex::FileExists() const {
+  return use_fd_ || OatFileInfo::FileExists();
+}
+
+const OatFile* OatFileAssistant::OatFileInfo::GetFile(/*out*/ std::string* error_msg) {
   CHECK(!file_released_) << "GetFile called after oat file released.";
-  if (load_attempted_) {
-    return file_.get();
-  }
-  load_attempted_ = true;
-  if (!filename_provided_) {
-    return nullptr;
+
+  if (!file_.has_value()) {
+    if (LocationIsOnArtApexData(filename_) &&
+        oat_file_assistant_->GetRuntimeOptions().deny_art_apex_data_files) {
+      file_ = std::make_pair(nullptr, "ART apexdata is untrusted");
+      LOG(WARNING) << "OatFileAssistant rejected file " << filename_ << ": " << file_->second;
+    } else {
+      std::string temp_error_msg;
+      file_ = std::make_pair(LoadFile(&temp_error_msg), std::move(temp_error_msg));
+    }
   }
 
-  if (LocationIsOnArtApexData(filename_) &&
-      oat_file_assistant_->GetRuntimeOptions().deny_art_apex_data_files) {
-    LOG(WARNING) << "OatFileAssistant rejected file " << filename_
-                 << ": ART apexdata is untrusted.";
-    return nullptr;
+  if (error_msg != nullptr) {
+    *error_msg = file_->second;
   }
+  return file_->first.get();
+}
 
-  std::string error_msg;
+std::unique_ptr<OatFile> OatFileAssistant::OatFileInfoBackedByOat::LoadFile(
+    std::string* error_msg) const {
   bool executable = oat_file_assistant_->load_executable_;
-  if (filename_.ends_with(kVdexExtension)) {
-    executable = false;
-    // Check to see if there is a vdex file we can make use of.
-    std::unique_ptr<VdexFile> vdex;
-    if (use_fd_) {
-      if (vdex_fd_ >= 0) {
-        struct stat s;
-        int rc = TEMP_FAILURE_RETRY(fstat(vdex_fd_, &s));
-        if (rc == -1) {
-          error_msg = StringPrintf("Failed getting length of the vdex file %s.", strerror(errno));
-        } else {
-          vdex = VdexFile::Open(vdex_fd_,
-                                s.st_size,
-                                filename_,
-                                /*writable=*/false,
-                                /*low_4gb=*/false,
-                                &error_msg);
-        }
-      }
-    } else {
-      vdex = VdexFile::Open(filename_,
-                            /*writable=*/false,
-                            /*low_4gb=*/false,
-                            &error_msg);
-    }
-    if (vdex == nullptr) {
-      VLOG(oat) << "unable to open vdex file " << filename_ << ": " << error_msg;
-    } else {
-      file_.reset(OatFile::OpenFromVdex(zip_fd_,
-                                        std::move(vdex),
-                                        oat_file_assistant_->dex_location_,
-                                        oat_file_assistant_->context_,
-                                        &error_msg));
-    }
-  } else if (filename_.ends_with(kDmExtension)) {
-    executable = false;
-    // Check to see if there is a vdex file we can make use of.
-    std::unique_ptr<ZipArchive> dm_file(ZipArchive::Open(filename_.c_str(), &error_msg));
-    if (dm_file != nullptr) {
-      std::unique_ptr<VdexFile> vdex(VdexFile::OpenFromDm(filename_, *dm_file));
-      if (vdex != nullptr) {
-        file_.reset(OatFile::OpenFromVdex(zip_fd_,
-                                          std::move(vdex),
-                                          oat_file_assistant_->dex_location_,
-                                          oat_file_assistant_->context_,
-                                          &error_msg));
-      }
+  if (executable && oat_file_assistant_->only_load_trusted_executable_) {
+    executable = LocationIsTrusted(filename_, /*trust_art_apex_data_files=*/true);
+  }
+
+  if (use_fd_) {
+    if (oat_fd_ < 0 || vdex_fd_ < 0) {
+      *error_msg = "oat_fd or vdex_fd not provided";
+      return nullptr;
     }
+    ArrayRef<const std::string> dex_locations(&oat_file_assistant_->dex_location_,
+                                              /*size=*/1u);
+    return std::unique_ptr<OatFile>(OatFile::Open(zip_fd_,
+                                                  vdex_fd_,
+                                                  oat_fd_,
+                                                  filename_,
+                                                  executable,
+                                                  /*low_4gb=*/false,
+                                                  dex_locations,
+                                                  /*dex_files=*/{},
+                                                  /*reservation=*/nullptr,
+                                                  error_msg));
   } else {
-    if (executable && oat_file_assistant_->only_load_trusted_executable_) {
-      executable = LocationIsTrusted(filename_, /*trust_art_apex_data_files=*/true);
-    }
-    VLOG(oat) << "Loading " << filename_ << " with executable: " << executable;
+    return std::unique_ptr<OatFile>(OatFile::Open(/*zip_fd=*/-1,
+                                                  filename_,
+                                                  filename_,
+                                                  executable,
+                                                  /*low_4gb=*/false,
+                                                  oat_file_assistant_->dex_location_,
+                                                  error_msg));
+  }
+}
+
+std::unique_ptr<OatFile> OatFileAssistant::OatFileInfoBackedBySdm::LoadFile(
+    std::string* error_msg) const {
+  bool executable = oat_file_assistant_->load_executable_;
+  if (executable && oat_file_assistant_->only_load_trusted_executable_) {
+    executable = LocationIsTrusted(filename_, /*trust_art_apex_data_files=*/true);
+  }
+
+  return std::unique_ptr<OatFile>(OatFile::OpenFromSdm(filename_,
+                                                       sdc_filename_,
+                                                       dm_filename_,
+                                                       oat_file_assistant_->dex_location_,
+                                                       executable,
+                                                       error_msg));
+}
 
-    if (gPageSize != kMinPageSize) {
-      LOG(WARNING) << "Loading odex files is only supported on devices with 4K page size";
+std::unique_ptr<OatFile> OatFileAssistant::OatFileInfoBackedByVdex::LoadFile(
+    std::string* error_msg) const {
+  // Check to see if there is a vdex file we can make use of.
+  std::unique_ptr<VdexFile> vdex;
+  if (use_fd_) {
+    if (vdex_fd_ < 0) {
+      *error_msg = "vdex_fd not provided";
       return nullptr;
     }
-
-    if (use_fd_) {
-      if (oat_fd_ >= 0 && vdex_fd_ >= 0) {
-        ArrayRef<const std::string> dex_locations(&oat_file_assistant_->dex_location_,
-                                                  /*size=*/1u);
-        file_.reset(OatFile::Open(zip_fd_,
-                                  vdex_fd_,
-                                  oat_fd_,
-                                  filename_,
-                                  executable,
-                                  /*low_4gb=*/false,
-                                  dex_locations,
-                                  /*dex_files=*/{},
-                                  /*reservation=*/nullptr,
-                                  &error_msg));
-      }
-    } else {
-      file_.reset(OatFile::Open(/*zip_fd=*/-1,
-                                filename_,
-                                filename_,
-                                executable,
-                                /*low_4gb=*/false,
-                                oat_file_assistant_->dex_location_,
-                                &error_msg));
+    struct stat s;
+    if (fstat(vdex_fd_, &s) < 0) {
+      *error_msg = ART_FORMAT("Failed getting length of the vdex file: {}", strerror(errno));
+      return nullptr;
     }
-  }
-  if (file_.get() == nullptr) {
-    VLOG(oat) << "OatFileAssistant test for existing oat file " << filename_ << ": " << error_msg;
+    vdex = VdexFile::Open(vdex_fd_,
+                          s.st_size,
+                          filename_,
+                          /*low_4gb=*/false,
+                          error_msg);
   } else {
-    VLOG(oat) << "Successfully loaded " << filename_ << " with executable: " << executable;
+    vdex = VdexFile::Open(filename_,
+                          /*low_4gb=*/false,
+                          error_msg);
+  }
+  if (vdex == nullptr) {
+    *error_msg = ART_FORMAT("Unable to open vdex file: {}", *error_msg);
+    return nullptr;
+  }
+  return std::unique_ptr<OatFile>(OatFile::OpenFromVdex(zip_fd_,
+                                                        std::move(vdex),
+                                                        oat_file_assistant_->dex_location_,
+                                                        oat_file_assistant_->context_,
+                                                        error_msg));
+}
+
+std::unique_ptr<OatFile> OatFileAssistant::OatFileInfoBackedByDm::LoadFile(
+    std::string* error_msg) const {
+  // Check to see if there is a vdex file we can make use of.
+  std::unique_ptr<ZipArchive> dm_file(ZipArchive::Open(filename_.c_str(), error_msg));
+  if (dm_file == nullptr) {
+    return nullptr;
   }
-  return file_.get();
+  std::unique_ptr<VdexFile> vdex(VdexFile::OpenFromDm(filename_, *dm_file, error_msg));
+  if (vdex == nullptr) {
+    return nullptr;
+  }
+  return std::unique_ptr<OatFile>(OatFile::OpenFromVdex(/*zip_fd=*/-1,
+                                                        std::move(vdex),
+                                                        oat_file_assistant_->dex_location_,
+                                                        oat_file_assistant_->context_,
+                                                        error_msg));
 }
 
 bool OatFileAssistant::OatFileInfo::ShouldRecompileForFilter(CompilerFilter::Filter target,
@@ -1151,13 +1105,6 @@ bool OatFileAssistant::OatFileInfo::ShouldRecompileForFilter(CompilerFilter::Fil
   const OatFile* file = GetFile();
   DCHECK(file != nullptr);
 
-  if (CompilerFilter::IsBetter(target, CompilerFilter::kVerify) && gPageSize != kMinPageSize) {
-    // Prevent infinite recompilations during background dexopt on 16K page devices.
-    VLOG(oat) << "Adjusting target filter to 'verify' because loading odex files is only supported "
-                 "on devices with 4K page size";
-    target = CompilerFilter::kVerify;
-  }
-
   CompilerFilter::Filter current = file->GetCompilerFilter();
   if (dexopt_trigger.targetFilterIsBetter && CompilerFilter::IsBetter(target, current)) {
     VLOG(oat) << ART_FORMAT("Should recompile: targetFilterIsBetter (current: {}, target: {})",
@@ -1214,7 +1161,8 @@ bool OatFileAssistant::OatFileInfo::ShouldRecompileForFilter(CompilerFilter::Fil
   return false;
 }
 
-bool OatFileAssistant::ClassLoaderContextIsOkay(const OatFile& oat_file) const {
+bool OatFileAssistant::ClassLoaderContextIsOkay(const OatFile& oat_file,
+                                                /*out*/ std::string* error_msg) const {
   if (context_ == nullptr) {
     // The caller requests to skip the check.
     return true;
@@ -1236,9 +1184,10 @@ bool OatFileAssistant::ClassLoaderContextIsOkay(const OatFile& oat_file) const {
                                               /*verify_names=*/true,
                                               /*verify_checksums=*/true);
   if (matches == ClassLoaderContext::VerificationResult::kMismatch) {
-    VLOG(oat) << "ClassLoaderContext check failed. Context was " << oat_file.GetClassLoaderContext()
-              << ". The expected context is "
-              << context_->EncodeContextForOatFile(android::base::Dirname(dex_location_));
+    *error_msg =
+        ART_FORMAT("ClassLoaderContext check failed. Context was {}. The expected context is {}",
+                   oat_file.GetClassLoaderContext(),
+                   context_->EncodeContextForOatFile(android::base::Dirname(dex_location_)));
     return false;
   }
   return true;
@@ -1249,26 +1198,9 @@ bool OatFileAssistant::OatFileInfo::IsExecutable() {
   return (file != nullptr && file->IsExecutable());
 }
 
-void OatFileAssistant::OatFileInfo::Reset() {
-  load_attempted_ = false;
-  file_.reset();
-  status_attempted_ = false;
-}
-
-void OatFileAssistant::OatFileInfo::Reset(
-    const std::string& filename, bool use_fd, int zip_fd, int vdex_fd, int oat_fd) {
-  filename_provided_ = true;
-  filename_ = filename;
-  use_fd_ = use_fd;
-  zip_fd_ = zip_fd;
-  vdex_fd_ = vdex_fd;
-  oat_fd_ = oat_fd;
-  Reset();
-}
-
 std::unique_ptr<OatFile> OatFileAssistant::OatFileInfo::ReleaseFile() {
   file_released_ = true;
-  return std::move(file_);
+  return std::move(file_->first);
 }
 
 std::unique_ptr<OatFile> OatFileAssistant::OatFileInfo::ReleaseFileForUse() {
@@ -1280,19 +1212,6 @@ std::unique_ptr<OatFile> OatFileAssistant::OatFileInfo::ReleaseFileForUse() {
   return std::unique_ptr<OatFile>();
 }
 
-// Check if we should reject vdex containing cdex code as part of the cdex
-// deprecation.
-// TODO(b/256664509): Clean this up.
-bool OatFileAssistant::OatFileInfo::CheckDisableCompactDex() {
-  const OatFile* oat_file = GetFile();
-  if (oat_file == nullptr) {
-    return false;
-  }
-  const VdexFile* vdex_file = oat_file->GetVdexFile();
-  return vdex_file != nullptr && vdex_file->HasDexSection() &&
-         !vdex_file->HasOnlyStandardDexFiles();
-}
-
 // TODO(calin): we could provide a more refined status here
 // (e.g. run from uncompressed apk, run with vdex but not oat etc). It will allow us to
 // track more experiments but adds extra complexity.
@@ -1402,7 +1321,13 @@ bool OatFileAssistant::ZipFileOnlyContainsUncompressedDex() {
 
 OatFileAssistant::Location OatFileAssistant::GetLocation(OatFileInfo& info) {
   if (info.IsUseable()) {
-    if (&info == &dm_for_oat_ || &info == &dm_for_odex_) {
+    if (info.GetType() == OatFileType::kSdm) {
+      if (info.IsOatLocation()) {
+        return kLocationSdmOat;
+      } else {
+        return kLocationSdmOdex;
+      }
+    } else if (info.GetType() == OatFileType::kDm) {
       return kLocationDm;
     } else if (info.IsOatLocation()) {
       return kLocationOat;
diff --git a/runtime/oat/oat_file_assistant.h b/runtime/oat/oat_file_assistant.h
index 2c9b8ab204..80eabba34b 100644
--- a/runtime/oat/oat_file_assistant.h
+++ b/runtime/oat/oat_file_assistant.h
@@ -23,7 +23,9 @@
 #include <sstream>
 #include <string>
 #include <string_view>
+#include <utility>
 #include <variant>
+#include <vector>
 
 #include "arch/instruction_set.h"
 #include "base/compiler_filter.h"
@@ -121,8 +123,14 @@ class OatFileAssistant {
     kLocationOat = 1,
     // In the "oat" folder next to the dex file.
     kLocationOdex = 2,
-    // In the DM file. This means the only usable file is the vdex file.
+    // In the dm file. This means the only usable file is the vdex file.
     kLocationDm = 3,
+    // The oat and art files are in the sdm file next to the dex file. The vdex file is in the dm
+    // file next to the dex file. The sdc file is in the global "dalvik-cache" folder.
+    kLocationSdmOat = 4,
+    // The oat and art files are in the sdm file next to the dex file. The vdex file is in the dm
+    // file next to the dex file. The sdc file is next to the dex file.
+    kLocationSdmOdex = 5,
   };
 
   // Represents the status of the current oat file and/or vdex file.
@@ -182,6 +190,8 @@ class OatFileAssistant {
                           int oat_fd,
                           int zip_fd);
 
+  EXPORT ~OatFileAssistant();
+
   // A convenient factory function that accepts ISA, class loader context, and compiler filter in
   // strings. Returns the created instance and ClassLoaderContext on success, or returns nullptr and
   // outputs an error message if it fails to parse the input strings.
@@ -238,10 +248,6 @@ class OatFileAssistant {
   // the OatFileAssistant object.
   std::unique_ptr<OatFile> GetBestOatFile();
 
-  // Returns a human readable description of the status of the code for the
-  // dex file. The returned description is for debugging purposes only.
-  std::string GetStatusDump();
-
   // Computes the optimization status of the given dex file. The result is
   // returned via the two output parameters.
   //   - out_odex_location: the location of the (best) odex that will be used
@@ -299,6 +305,8 @@ class OatFileAssistant {
   // ASLR. The odex file is treated as if it were read-only.
   //
   // Returns the status of the odex file for the dex location.
+  //
+  // For testing purposes only.
   OatStatus OdexFileStatus();
 
   // When the dex files is compiled on the target device, the oat file is the
@@ -306,6 +314,8 @@ class OatFileAssistant {
   // (possibly-out-of-date) offset for ASLR.
   //
   // Returns the status of the oat file for the dex location.
+  //
+  // For testing purposes only.
   OatStatus OatFileStatus();
 
   OatStatus GetBestStatus() {
@@ -360,10 +370,11 @@ class OatFileAssistant {
   // anonymous dex file(s) created by AnonymousDexVdexLocation.
   EXPORT static bool IsAnonymousVdexBasename(const std::string& basename);
 
-  bool ClassLoaderContextIsOkay(const OatFile& oat_file) const;
+  bool ClassLoaderContextIsOkay(const OatFile& oat_file, /*out*/ std::string* error_msg) const;
 
   // Validates the boot class path checksum of an OatFile.
-  EXPORT bool ValidateBootClassPathChecksums(const OatFile& oat_file);
+  EXPORT bool ValidateBootClassPathChecksums(const OatFile& oat_file,
+                                             /*out*/ std::string* error_msg);
 
   // Validates the given bootclasspath and bootclasspath checksums found in an oat header.
   static bool ValidateBootClassPathChecksums(OatFileAssistantContext* ofa_context,
@@ -373,20 +384,35 @@ class OatFileAssistant {
                                              /*out*/ std::string* error_msg);
 
  private:
+  enum class OatFileType {
+    kNone,
+    kOat,
+    kSdm,
+    kVdex,
+    kDm,
+  };
+
   class OatFileInfo {
    public:
-    // Initially the info is for no file in particular. It will treat the
-    // file as out of date until Reset is called with a real filename to use
-    // the cache for.
-    // Pass true for is_oat_location if the information associated with this
-    // OatFileInfo is for the oat location, as opposed to the odex location.
-    OatFileInfo(OatFileAssistant* oat_file_assistant, bool is_oat_location);
+    // Empty info. Treated as kOatCannotOpen.
+    // Use constructors in subclasses to construct a real instance.
+    explicit OatFileInfo(OatFileAssistant* oat_file_assistant)
+        : oat_file_assistant_(oat_file_assistant), filename_(""), is_oat_location_(false) {}
+
+    virtual ~OatFileInfo() = default;
+
+    // ART code is compiled with `-fno-rtti`, so we need a virtual function to return type
+    // information.
+    virtual OatFileType GetType() { return OatFileType::kNone; }
 
-    bool IsOatLocation();
+    // Returns a string indicating the location of the oat file, for debugging purposes only.
+    virtual const char* GetLocationDebugString() { return "none"; }
 
-    const std::string* Filename();
+    bool IsOatLocation() const;
 
-    const char* DisplayFilename();
+    const std::string* Filename() const;
+
+    const char* DisplayFilename() const;
 
     // Returns true if this oat file can be used for running code. The oat
     // file can be used for running code as long as it is not out of date with
@@ -397,33 +423,26 @@ class OatFileAssistant {
     bool IsUseable();
 
     // Returns the status of this oat file.
-    OatStatus Status();
+    // Optionally, returns `error_msg` showing why the status is not `kOatUpToDate`.
+    OatStatus Status(/*out*/ std::string* error_msg = nullptr);
 
     // Return the DexOptNeeded value for this oat file with respect to the given target compilation
     // filter and dexopt trigger.
     DexOptNeeded GetDexOptNeeded(CompilerFilter::Filter target_compiler_filter,
                                  const DexOptTrigger dexopt_trigger);
 
+    // Returns true if the file exists.
+    virtual bool FileExists() const;
+
     // Returns the loaded file.
     // Loads the file if needed. Returns null if the file failed to load.
     // The caller shouldn't clean up or free the returned pointer.
-    const OatFile* GetFile();
+    // Optionally, returns `error_msg` showing why the file failed to load.
+    const OatFile* GetFile(/*out*/ std::string* error_msg = nullptr);
 
     // Returns true if the file is opened executable.
     bool IsExecutable();
 
-    // Clear any cached information about the file that depends on the
-    // contents of the file. This does not reset the provided filename.
-    void Reset();
-
-    // Clear any cached information and switch to getting info about the oat
-    // file with the given filename.
-    void Reset(const std::string& filename,
-               bool use_fd,
-               int zip_fd = -1,
-               int vdex_fd = -1,
-               int oat_fd = -1);
-
     // Release the loaded oat file for runtime use.
     // Returns null if the oat file hasn't been loaded or is out of date.
     // Ensures the returned file is not loaded executable if it has unuseable
@@ -434,10 +453,25 @@ class OatFileAssistant {
     // the OatFileInfo object.
     std::unique_ptr<OatFile> ReleaseFileForUse();
 
-    // Check if we should reject vdex containing cdex code as part of the cdex
-    // deprecation.
-    // TODO(b/256664509): Clean this up.
-    bool CheckDisableCompactDex();
+   protected:
+    // Constructs a real instance.
+    // Pass true for is_oat_location if the information associated with this
+    // OatFileInfo is for the oat location, as opposed to the odex location.
+    OatFileInfo(OatFileAssistant* oat_file_assistant,
+                const std::string& filename,
+                bool is_oat_location)
+        : oat_file_assistant_(oat_file_assistant),
+          filename_(filename),
+          is_oat_location_(is_oat_location) {}
+
+    // Loads the file.
+    virtual std::unique_ptr<OatFile> LoadFile(std::string* error_msg) const {
+      *error_msg = "Not implemented";
+      return nullptr;
+    }
+
+    OatFileAssistant* const oat_file_assistant_;
+    const std::string filename_;
 
    private:
     // Returns true if the oat file is usable but at least one dexopt trigger is matched. This
@@ -453,22 +487,15 @@ class OatFileAssistant {
     // the OatFileInfo object.
     std::unique_ptr<OatFile> ReleaseFile();
 
-    OatFileAssistant* oat_file_assistant_;
     const bool is_oat_location_;
 
-    bool filename_provided_ = false;
-    std::string filename_;
-
-    int zip_fd_ = -1;
-    int oat_fd_ = -1;
-    int vdex_fd_ = -1;
-    bool use_fd_ = false;
-
-    bool load_attempted_ = false;
-    std::unique_ptr<OatFile> file_;
+    // A pair of the loaded file and the error message, if `GetFile` has been attempted.
+    // `std::nullopt` if `GetFile` has not been attempted.
+    std::optional<std::pair<std::unique_ptr<OatFile>, std::string>> file_ = std::nullopt;
 
-    bool status_attempted_ = false;
-    OatStatus status_ = OatStatus::kOatCannotOpen;
+    // A pair of the oat status and the error message, if `Status` has been attempted.
+    // `std::nullopt` if `Status` has not been attempted.
+    std::optional<std::pair<OatStatus, std::string>> status_ = std::nullopt;
 
     // For debugging only.
     // If this flag is set, the file has been released to the user and the
@@ -476,6 +503,109 @@ class OatFileAssistant {
     bool file_released_ = false;
   };
 
+  class OatFileInfoBackedByOat : public OatFileInfo {
+   public:
+    OatFileInfoBackedByOat(OatFileAssistant* oat_file_assistant,
+                           const std::string& filename,
+                           bool is_oat_location,
+                           bool use_fd,
+                           int zip_fd = -1,
+                           int vdex_fd = -1,
+                           int oat_fd = -1)
+        : OatFileInfo(oat_file_assistant, filename, is_oat_location),
+          use_fd_(use_fd),
+          zip_fd_(zip_fd),
+          vdex_fd_(vdex_fd),
+          oat_fd_(oat_fd) {}
+
+    OatFileType GetType() override { return OatFileType::kOat; }
+
+    const char* GetLocationDebugString() override {
+      return IsOatLocation() ? "odex in dalvik-cache" : "odex next to the dex file";
+    }
+
+    bool FileExists() const override;
+
+   protected:
+    std::unique_ptr<OatFile> LoadFile(std::string* error_msg) const override;
+
+   private:
+    const bool use_fd_;
+    const int zip_fd_;
+    const int vdex_fd_;
+    const int oat_fd_;
+  };
+
+  class OatFileInfoBackedBySdm : public OatFileInfo {
+   public:
+    OatFileInfoBackedBySdm(OatFileAssistant* oat_file_assistant,
+                           const std::string& sdm_filename,
+                           bool is_oat_location,
+                           const std::string& dm_filename,
+                           const std::string& sdc_filename)
+        : OatFileInfo(oat_file_assistant, sdm_filename, is_oat_location),
+          dm_filename_(dm_filename),
+          sdc_filename_(sdc_filename) {}
+
+    OatFileType GetType() override { return OatFileType::kSdm; }
+
+    const char* GetLocationDebugString() override {
+      return IsOatLocation() ? "sdm with sdc in dalvik-cache" : "sdm with sdc next to the dex file";
+    }
+
+    bool FileExists() const override;
+
+   protected:
+    std::unique_ptr<OatFile> LoadFile(std::string* error_msg) const override;
+
+   private:
+    const std::string dm_filename_;
+    const std::string sdc_filename_;
+  };
+
+  class OatFileInfoBackedByVdex : public OatFileInfo {
+   public:
+    OatFileInfoBackedByVdex(OatFileAssistant* oat_file_assistant,
+                            const std::string& filename,
+                            bool is_oat_location,
+                            bool use_fd,
+                            int zip_fd = -1,
+                            int vdex_fd = -1)
+        : OatFileInfo(oat_file_assistant, filename, is_oat_location),
+          use_fd_(use_fd),
+          zip_fd_(zip_fd),
+          vdex_fd_(vdex_fd) {}
+
+    OatFileType GetType() override { return OatFileType::kVdex; }
+
+    const char* GetLocationDebugString() override {
+      return IsOatLocation() ? "vdex in dalvik-cache" : "vdex next to the dex file";
+    }
+
+    bool FileExists() const override;
+
+   protected:
+    std::unique_ptr<OatFile> LoadFile(std::string* error_msg) const override;
+
+   private:
+    const bool use_fd_;
+    const int zip_fd_;
+    const int vdex_fd_;
+  };
+
+  class OatFileInfoBackedByDm : public OatFileInfo {
+   public:
+    OatFileInfoBackedByDm(OatFileAssistant* oat_file_assistant, const std::string& filename)
+        : OatFileInfo(oat_file_assistant, filename, /*is_oat_location=*/false) {}
+
+    OatFileType GetType() override { return OatFileType::kDm; }
+
+    const char* GetLocationDebugString() override { return "dm"; }
+
+   protected:
+    std::unique_ptr<OatFile> LoadFile(std::string* error_msg) const override;
+  };
+
   // Return info for the best oat file.
   OatFileInfo& GetBestInfo();
 
@@ -492,7 +622,7 @@ class OatFileAssistant {
 
   // Return the status for a given opened oat file with respect to the dex
   // location.
-  OatStatus GivenOatFileStatus(const OatFile& file);
+  OatStatus GivenOatFileStatus(const OatFile& file, /*out*/ std::string* error_msg);
 
   // Gets the dex checksum required for an up-to-date oat file.
   // Returns cached result from GetMultiDexChecksum.
@@ -537,9 +667,6 @@ class OatFileAssistant {
   // skipped.
   ClassLoaderContext* context_;
 
-  // Whether or not the parent directory of the dex file is writable.
-  bool dex_parent_writable_ = false;
-
   // In a properly constructed OatFileAssistant object, isa_ should be either
   // the 32 or 64 bit variant for the current device.
   const InstructionSet isa_ = InstructionSet::kNone;
@@ -560,22 +687,11 @@ class OatFileAssistant {
   std::optional<std::string> cached_required_dex_checksums_error_;
   bool required_dex_checksums_attempted_ = false;
 
-  // The AOT-compiled file of an app when the APK of the app is in /data.
-  OatFileInfo odex_;
-  // The AOT-compiled file of an app when the APK of the app is on a read-only partition
-  // (for example /system).
-  OatFileInfo oat_;
-
-  // The vdex-only file next to `odex_` when `odex_' cannot be used (for example
-  // it is out of date).
-  OatFileInfo vdex_for_odex_;
-  // The vdex-only file next to 'oat_` when `oat_' cannot be used (for example
-  // it is out of date).
-  OatFileInfo vdex_for_oat_;
-
-  // The vdex-only file next to the apk.
-  OatFileInfo dm_for_odex_;
-  OatFileInfo dm_for_oat_;
+  // Empty oat file info, used as a placeholder.
+  OatFileInfo empty_info_ = OatFileInfo(this);
+
+  // Oat file info candidates, ordered by precedence.
+  std::vector<std::unique_ptr<OatFileInfo>> info_list_;
 
   // File descriptor corresponding to apk, dex file, or zip.
   int zip_fd_;
diff --git a/runtime/oat/oat_file_assistant_context.h b/runtime/oat/oat_file_assistant_context.h
index 82b79edef0..28d5b49dc2 100644
--- a/runtime/oat/oat_file_assistant_context.h
+++ b/runtime/oat/oat_file_assistant_context.h
@@ -76,7 +76,7 @@ class OatFileAssistantContext {
   const std::vector<std::string>* GetBcpChecksums(size_t bcp_index, std::string* error_msg);
   // Returns a string that represents the apex versions of boot classpath jars. See
   // `Runtime::apex_versions_` for the encoding format.
-  const std::string& GetApexVersions();
+  EXPORT const std::string& GetApexVersions();
 
  private:
   std::unique_ptr<RuntimeOptions> runtime_options_;
diff --git a/runtime/oat/oat_file_assistant_test.cc b/runtime/oat/oat_file_assistant_test.cc
index a3b370470f..c3e2f63e24 100644
--- a/runtime/oat/oat_file_assistant_test.cc
+++ b/runtime/oat/oat_file_assistant_test.cc
@@ -26,12 +26,14 @@
 #include <optional>
 #include <string>
 #include <type_traits>
+#include <unordered_map>
 #include <vector>
 
 #include "android-base/scopeguard.h"
 #include "android-base/strings.h"
 #include "arch/instruction_set.h"
 #include "art_field-inl.h"
+#include "base/file_utils.h"
 #include "base/os.h"
 #include "base/utils.h"
 #include "class_linker.h"
@@ -42,6 +44,7 @@
 #include "oat_file.h"
 #include "oat_file_assistant_context.h"
 #include "oat_file_manager.h"
+#include "obj_ptr.h"
 #include "scoped_thread_state_change.h"
 #include "thread.h"
 
@@ -50,7 +53,7 @@ namespace art HIDDEN {
 class OatFileAssistantBaseTest : public DexoptTest {};
 
 class OatFileAssistantTest : public OatFileAssistantBaseTest,
-                             public testing::WithParamInterface<bool> {
+                             public ::testing::WithParamInterface<bool> {
  public:
   void SetUp() override {
     DexoptTest::SetUp();
@@ -238,39 +241,6 @@ class OatFileAssistantTest : public OatFileAssistantBaseTest,
   std::vector<std::unique_ptr<const DexFile>> opened_dex_files_;
 };
 
-class ScopedNonWritable {
- public:
-  explicit ScopedNonWritable(const std::string& dex_location) {
-    is_valid_ = false;
-    size_t pos = dex_location.rfind('/');
-    if (pos != std::string::npos) {
-      is_valid_ = true;
-      dex_parent_ = dex_location.substr(0, pos);
-      if (chmod(dex_parent_.c_str(), 0555) != 0)  {
-        PLOG(ERROR) << "Could not change permissions on " << dex_parent_;
-      }
-    }
-  }
-
-  bool IsSuccessful() { return is_valid_ && (access(dex_parent_.c_str(), W_OK) != 0); }
-
-  ~ScopedNonWritable() {
-    if (is_valid_) {
-      if (chmod(dex_parent_.c_str(), 0777) != 0) {
-        PLOG(ERROR) << "Could not restore permissions on " << dex_parent_;
-      }
-    }
-  }
-
- private:
-  std::string dex_parent_;
-  bool is_valid_;
-};
-
-static bool IsExecutedAsRoot() {
-  return geteuid() == 0;
-}
-
 // Case: We have a MultiDEX file and up-to-date ODEX file for it with relative
 // encoded dex locations.
 // Expect: The oat file status is kNoDexOptNeeded.
@@ -586,20 +556,10 @@ TEST_P(OatFileAssistantTest, OdexUpToDateSymLink) {
 // Case: We have a DEX file and up-to-date OAT file for it.
 // Expect: The status is kNoDexOptNeeded.
 TEST_P(OatFileAssistantTest, OatUpToDate) {
-  if (IsExecutedAsRoot()) {
-    // We cannot simulate non writable locations when executed as root: b/38000545.
-    LOG(ERROR) << "Test skipped because it's running as root";
-    return;
-  }
-
   std::string dex_location = GetScratchDir() + "/OatUpToDate.jar";
   Copy(GetDexSrc1(), dex_location);
   GenerateOatForTest(dex_location.c_str(), CompilerFilter::kSpeed);
 
-  // Force the use of oat location by making the dex parent not writable.
-  ScopedNonWritable scoped_non_writable(dex_location);
-  ASSERT_TRUE(scoped_non_writable.IsSuccessful());
-
   auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
 
   OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str());
@@ -828,10 +788,6 @@ TEST_P(OatFileAssistantTest, VdexUpToDateNoOdex) {
                                /*expected_location=*/OatFileAssistant::kLocationOdex,
                                /*expected_legacy_result=*/-OatFileAssistant::kDex2OatForFilter);
 
-  // Make sure we don't crash in this case when we dump the status. We don't
-  // care what the actual dumped value is.
-  oat_file_assistant.GetStatusDump();
-
   VerifyOptimizationStatus(dex_location,
                            default_context_.get(),
                            "verify",
@@ -864,12 +820,6 @@ TEST_P(OatFileAssistantTest, EmptyVdexOdex) {
 // Case: We have a DEX file and up-to-date (OAT) VDEX file for it, but no OAT
 // file.
 TEST_P(OatFileAssistantTest, VdexUpToDateNoOat) {
-  if (IsExecutedAsRoot()) {
-    // We cannot simulate non writable locations when executed as root: b/38000545.
-    LOG(ERROR) << "Test skipped because it's running as root";
-    return;
-  }
-
   std::string dex_location = GetScratchDir() + "/VdexUpToDateNoOat.jar";
   std::string oat_location;
   std::string error_msg;
@@ -881,9 +831,6 @@ TEST_P(OatFileAssistantTest, VdexUpToDateNoOat) {
   GenerateOatForTest(dex_location.c_str(), CompilerFilter::kSpeed);
   ASSERT_EQ(0, unlink(oat_location.c_str()));
 
-  ScopedNonWritable scoped_non_writable(dex_location);
-  ASSERT_TRUE(scoped_non_writable.IsSuccessful());
-
   auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
 
   OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str());
@@ -900,19 +847,10 @@ TEST_P(OatFileAssistantTest, VdexUpToDateNoOat) {
 // Expect: The status is kNoDexOptNeeded if the profile hasn't changed, but
 // kDex2Oat if the profile has changed.
 TEST_P(OatFileAssistantTest, ProfileOatUpToDate) {
-  if (IsExecutedAsRoot()) {
-    // We cannot simulate non writable locations when executed as root: b/38000545.
-    LOG(ERROR) << "Test skipped because it's running as root";
-    return;
-  }
-
   std::string dex_location = GetScratchDir() + "/ProfileOatUpToDate.jar";
   Copy(GetDexSrc1(), dex_location);
   GenerateOatForTest(dex_location.c_str(), CompilerFilter::kSpeedProfile);
 
-  ScopedNonWritable scoped_non_writable(dex_location);
-  ASSERT_TRUE(scoped_non_writable.IsSuccessful());
-
   auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
 
   OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str());
@@ -969,19 +907,10 @@ TEST_P(OatFileAssistantTest, ProfileOatUpToDate) {
 // Case: We have a MultiDEX file and up-to-date OAT file for it.
 // Expect: The status is kNoDexOptNeeded and we load all dex files.
 TEST_P(OatFileAssistantTest, MultiDexOatUpToDate) {
-  if (IsExecutedAsRoot()) {
-    // We cannot simulate non writable locations when executed as root: b/38000545.
-    LOG(ERROR) << "Test skipped because it's running as root";
-    return;
-  }
-
   std::string dex_location = GetScratchDir() + "/MultiDexOatUpToDate.jar";
   Copy(GetMultiDexSrc1(), dex_location);
   GenerateOatForTest(dex_location.c_str(), CompilerFilter::kSpeed);
 
-  ScopedNonWritable scoped_non_writable(dex_location);
-  ASSERT_TRUE(scoped_non_writable.IsSuccessful());
-
   auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
 
   OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str(),
@@ -1009,12 +938,6 @@ TEST_P(OatFileAssistantTest, MultiDexOatUpToDate) {
 // Case: We have a MultiDEX file where the non-main multdex entry is out of date.
 // Expect: The status is kDex2OatNeeded.
 TEST_P(OatFileAssistantTest, MultiDexNonMainOutOfDate) {
-  if (IsExecutedAsRoot()) {
-    // We cannot simulate non writable locations when executed as root: b/38000545.
-    LOG(ERROR) << "Test skipped because it's running as root";
-    return;
-  }
-
   std::string dex_location = GetScratchDir() + "/MultiDexNonMainOutOfDate.jar";
 
   // Compile code for GetMultiDexSrc1.
@@ -1025,9 +948,6 @@ TEST_P(OatFileAssistantTest, MultiDexNonMainOutOfDate) {
   // is out of date.
   Copy(GetMultiDexSrc2(), dex_location);
 
-  ScopedNonWritable scoped_non_writable(dex_location);
-  ASSERT_TRUE(scoped_non_writable.IsSuccessful());
-
   auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
 
   OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str());
@@ -1043,12 +963,6 @@ TEST_P(OatFileAssistantTest, MultiDexNonMainOutOfDate) {
 // Case: We have a DEX file and an OAT file out of date with respect to the
 // dex checksum.
 TEST_P(OatFileAssistantTest, OatDexOutOfDate) {
-  if (IsExecutedAsRoot()) {
-    // We cannot simulate non writable locations when executed as root: b/38000545.
-    LOG(ERROR) << "Test skipped because it's running as root";
-    return;
-  }
-
   std::string dex_location = GetScratchDir() + "/OatDexOutOfDate.jar";
 
   // We create a dex, generate an oat for it, then overwrite the dex with a
@@ -1057,9 +971,6 @@ TEST_P(OatFileAssistantTest, OatDexOutOfDate) {
   GenerateOatForTest(dex_location.c_str(), CompilerFilter::kSpeed);
   Copy(GetDexSrc2(), dex_location);
 
-  ScopedNonWritable scoped_non_writable(dex_location);
-  ASSERT_TRUE(scoped_non_writable.IsSuccessful());
-
   auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
 
   OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str());
@@ -1132,12 +1043,6 @@ TEST_P(OatFileAssistantTest, VdexMultiDexNonMainOutOfDate) {
 // Case: We have a DEX file and an OAT file out of date with respect to the
 // boot image.
 TEST_P(OatFileAssistantTest, OatImageOutOfDate) {
-  if (IsExecutedAsRoot()) {
-    // We cannot simulate non writable locations when executed as root: b/38000545.
-    LOG(ERROR) << "Test skipped because it's running as root";
-    return;
-  }
-
   std::string dex_location = GetScratchDir() + "/OatImageOutOfDate.jar";
 
   Copy(GetDexSrc1(), dex_location);
@@ -1145,9 +1050,6 @@ TEST_P(OatFileAssistantTest, OatImageOutOfDate) {
                      CompilerFilter::kSpeed,
                      /* with_alternate_image= */ true);
 
-  ScopedNonWritable scoped_non_writable(dex_location);
-  ASSERT_TRUE(scoped_non_writable.IsSuccessful());
-
   auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
 
   OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str());
@@ -1292,7 +1194,7 @@ TEST_P(OatFileAssistantTest, ResourceOnlyDex) {
 }
 
 // Case: We have a DEX file, an ODEX file and an OAT file.
-// Expect: It shouldn't crash. We should load the odex file executable.
+// Expect: It shouldn't crash. We should load the oat file executable.
 TEST_P(OatFileAssistantTest, OdexOatOverlap) {
   std::string dex_location = GetScratchDir() + "/OdexOatOverlap.jar";
   std::string odex_location = GetOdexDir() + "/OdexOatOverlap.odex";
@@ -1313,7 +1215,7 @@ TEST_P(OatFileAssistantTest, OdexOatOverlap) {
                                CompilerFilter::kSpeed,
                                /*expected_dexopt_needed=*/false,
                                /*expected_is_vdex_usable=*/true,
-                               /*expected_location=*/OatFileAssistant::kLocationOdex,
+                               /*expected_location=*/OatFileAssistant::kLocationOat,
                                /*expected_legacy_result=*/OatFileAssistant::kNoDexOptNeeded);
 
   EXPECT_FALSE(oat_file_assistant.IsInBootClassPath());
@@ -1335,20 +1237,11 @@ TEST_P(OatFileAssistantTest, OdexOatOverlap) {
 // Case: We have a DEX file and up-to-date OAT file for it.
 // Expect: We should load an executable dex file.
 TEST_P(OatFileAssistantTest, LoadOatUpToDate) {
-  if (IsExecutedAsRoot()) {
-    // We cannot simulate non writable locations when executed as root: b/38000545.
-    LOG(ERROR) << "Test skipped because it's running as root";
-    return;
-  }
-
   std::string dex_location = GetScratchDir() + "/LoadOatUpToDate.jar";
 
   Copy(GetDexSrc1(), dex_location);
   GenerateOatForTest(dex_location.c_str(), CompilerFilter::kSpeed);
 
-  ScopedNonWritable scoped_non_writable(dex_location);
-  ASSERT_TRUE(scoped_non_writable.IsSuccessful());
-
   auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
 
   // Load the oat using an oat file assistant.
@@ -1369,20 +1262,11 @@ TEST_P(OatFileAssistantTest, LoadOatUpToDate) {
 // Case: We have a DEX file and up-to-date quicken OAT file for it.
 // Expect: We should still load the oat file as executable.
 TEST_P(OatFileAssistantTest, LoadExecInterpretOnlyOatUpToDate) {
-  if (IsExecutedAsRoot()) {
-    // We cannot simulate non writable locations when executed as root: b/38000545.
-    LOG(ERROR) << "Test skipped because it's running as root";
-    return;
-  }
-
   std::string dex_location = GetScratchDir() + "/LoadExecInterpretOnlyOatUpToDate.jar";
 
   Copy(GetDexSrc1(), dex_location);
   GenerateOatForTest(dex_location.c_str(), CompilerFilter::kVerify);
 
-  ScopedNonWritable scoped_non_writable(dex_location);
-  ASSERT_TRUE(scoped_non_writable.IsSuccessful());
-
   auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
 
   // Load the oat using an oat file assistant.
@@ -1403,19 +1287,10 @@ TEST_P(OatFileAssistantTest, LoadExecInterpretOnlyOatUpToDate) {
 // Case: We have a DEX file and up-to-date OAT file for it.
 // Expect: Loading non-executable should load the oat non-executable.
 TEST_P(OatFileAssistantTest, LoadNoExecOatUpToDate) {
-  if (IsExecutedAsRoot()) {
-    // We cannot simulate non writable locations when executed as root: b/38000545.
-    LOG(ERROR) << "Test skipped because it's running as root";
-    return;
-  }
-
   std::string dex_location = GetScratchDir() + "/LoadNoExecOatUpToDate.jar";
 
   Copy(GetDexSrc1(), dex_location);
 
-  ScopedNonWritable scoped_non_writable(dex_location);
-  ASSERT_TRUE(scoped_non_writable.IsSuccessful());
-
   GenerateOatForTest(dex_location.c_str(), CompilerFilter::kSpeed);
 
   auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
@@ -2309,10 +2184,330 @@ TEST_P(OatFileAssistantTest, ShouldRecompileForImageFromSpeedProfile) {
             oat_file_assistant.GetDexOptNeeded(CompilerFilter::kVerify));
 }
 
-// Test that GetLocation of a dex file is the same whether the dex
-// filed is backed by an oat file or not.
+// Case: We have SDM, DM, and SDC files for an uncompressed DEX file, and the SDC file is in odex
+// location.
+// Expect: The best artifact location should be kLocationSdmOdex. Dexopt should be performed only if
+// the compiler filter is better than "speed-profile".
+//
+// The legacy version should return kDex2OatFromScratch if the target compiler filter is better than
+// "verify".
+TEST_P(OatFileAssistantTest, SdmUpToDate) {
+  std::string dex_location = GetScratchDir() + "/TestDex.jar";
+  std::string sdm_location =
+      GetScratchDir() + ART_FORMAT("/TestDex.{}.sdm", GetInstructionSetString(kRuntimeISA));
+  std::string dm_location = GetScratchDir() + "/TestDex.dm";
+  std::string sdc_location = GetOdexDir() + "/TestDex.sdc";
+  Copy(GetMultiDexUncompressedAlignedSrc1(), dex_location);
+
+  ASSERT_NO_FATAL_FAILURE(GenerateSdmDmForTest(dex_location,
+                                               sdm_location,
+                                               dm_location,
+                                               CompilerFilter::kSpeedProfile,
+                                               /*include_app_image=*/true,
+                                               /*compilation_reason=*/"cloud"));
+  ASSERT_NO_FATAL_FAILURE(
+      CreateSecureDexMetadataCompanion(sdm_location, runtime_->GetApexVersions(), sdc_location));
+
+  auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
+
+  OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str());
+
+  VerifyOptimizationStatusWithInstance(&oat_file_assistant,
+                                       "speed-profile",
+                                       "cloud",
+                                       "up-to-date",
+                                       OatFileAssistant::kLocationSdmOdex);
+
+  VerifyGetDexOptNeeded(&oat_file_assistant,
+                        CompilerFilter::kSpeed,
+                        default_trigger_,
+                        /*expected_dexopt_needed=*/true,
+                        /*expected_is_vdex_usable=*/true,
+                        /*expected_location=*/OatFileAssistant::kLocationSdmOdex);
+  EXPECT_EQ(OatFileAssistant::kDex2OatFromScratch,
+            oat_file_assistant.GetDexOptNeeded(CompilerFilter::kSpeed));
+
+  VerifyGetDexOptNeeded(&oat_file_assistant,
+                        CompilerFilter::kSpeedProfile,
+                        default_trigger_,
+                        /*expected_dexopt_needed=*/false,
+                        /*expected_is_vdex_usable=*/true,
+                        /*expected_location=*/OatFileAssistant::kLocationSdmOdex);
+  EXPECT_EQ(OatFileAssistant::kNoDexOptNeeded,
+            oat_file_assistant.GetDexOptNeeded(CompilerFilter::kSpeedProfile));
+
+  VerifyGetDexOptNeeded(&oat_file_assistant,
+                        CompilerFilter::kVerify,
+                        default_trigger_,
+                        /*expected_dexopt_needed=*/false,
+                        /*expected_is_vdex_usable=*/true,
+                        /*expected_location=*/OatFileAssistant::kLocationSdmOdex);
+  EXPECT_EQ(OatFileAssistant::kNoDexOptNeeded,
+            oat_file_assistant.GetDexOptNeeded(CompilerFilter::kVerify));
+}
+
+// Case: We have SDM, DM, and SDC files for an uncompressed DEX file, and the SDC file is in oat
+// location.
+// Expect: The best artifact location should be kLocationSdmOat.
+TEST_P(OatFileAssistantTest, SdmUpToDateSdcInOatLocation) {
+  std::string dex_location = GetScratchDir() + "/TestDex.jar";
+  std::string sdm_location =
+      GetScratchDir() + ART_FORMAT("/TestDex.{}.sdm", GetInstructionSetString(kRuntimeISA));
+  std::string dm_location = GetScratchDir() + "/TestDex.dm";
+  Copy(GetMultiDexUncompressedAlignedSrc1(), dex_location);
+
+  std::string oat_location;
+  std::string error_msg;
+  ASSERT_TRUE(OatFileAssistant::DexLocationToOatFilename(
+      dex_location, kRuntimeISA, &oat_location, &error_msg))
+      << error_msg;
+  std::string sdc_location = GetSdcFilename(oat_location);
+
+  ASSERT_NO_FATAL_FAILURE(GenerateSdmDmForTest(dex_location,
+                                               sdm_location,
+                                               dm_location,
+                                               CompilerFilter::kSpeedProfile,
+                                               /*include_app_image=*/true,
+                                               /*compilation_reason=*/"cloud"));
+  ASSERT_NO_FATAL_FAILURE(
+      CreateSecureDexMetadataCompanion(sdm_location, runtime_->GetApexVersions(), sdc_location));
+
+  auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
+
+  OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str());
+
+  VerifyOptimizationStatusWithInstance(&oat_file_assistant,
+                                       "speed-profile",
+                                       "cloud",
+                                       "up-to-date",
+                                       OatFileAssistant::kLocationSdmOat);
+}
+
+// Case: We have SDM, DM, and SDC files for an uncompressed DEX file, and the SDM file contains no
+// ART file.
+// Expect: The best artifact location should be kLocationSdmOdex.
+TEST_P(OatFileAssistantTest, SdmUpToDateNoArt) {
+  std::string dex_location = GetScratchDir() + "/TestDex.jar";
+  std::string sdm_location =
+      GetScratchDir() + ART_FORMAT("/TestDex.{}.sdm", GetInstructionSetString(kRuntimeISA));
+  std::string dm_location = GetScratchDir() + "/TestDex.dm";
+  std::string sdc_location = GetOdexDir() + "/TestDex.sdc";
+  Copy(GetMultiDexUncompressedAlignedSrc1(), dex_location);
+
+  ASSERT_NO_FATAL_FAILURE(GenerateSdmDmForTest(dex_location,
+                                               sdm_location,
+                                               dm_location,
+                                               CompilerFilter::kSpeedProfile,
+                                               /*include_app_image=*/false,
+                                               /*compilation_reason=*/"cloud"));
+  ASSERT_NO_FATAL_FAILURE(
+      CreateSecureDexMetadataCompanion(sdm_location, runtime_->GetApexVersions(), sdc_location));
+
+  auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
+
+  OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str());
+
+  VerifyOptimizationStatusWithInstance(&oat_file_assistant,
+                                       "speed-profile",
+                                       "cloud",
+                                       "up-to-date",
+                                       OatFileAssistant::kLocationSdmOdex);
+}
+
+// Case: We have SDM, DM, and SDC files for an uncompressed DEX file. Meanwhile, we have an ODEX
+// file that is also up to date.
+// Expect: The ODEX file is preferred over the SDM file. The best artifact location should be
+// kLocationOdex.
+TEST_P(OatFileAssistantTest, SdmAndOdexUpToDate) {
+  std::string dex_location = GetScratchDir() + "/TestDex.jar";
+  std::string sdm_location =
+      GetScratchDir() + ART_FORMAT("/TestDex.{}.sdm", GetInstructionSetString(kRuntimeISA));
+  std::string dm_location = GetScratchDir() + "/TestDex.dm";
+  std::string sdc_location = GetOdexDir() + "/TestDex.sdc";
+  std::string odex_location = GetOdexDir() + "/TestDex.odex";
+  Copy(GetMultiDexUncompressedAlignedSrc1(), dex_location);
+
+  ASSERT_NO_FATAL_FAILURE(GenerateSdmDmForTest(dex_location,
+                                               sdm_location,
+                                               dm_location,
+                                               CompilerFilter::kSpeedProfile,
+                                               /*include_app_image=*/true,
+                                               /*compilation_reason=*/"cloud"));
+  ASSERT_NO_FATAL_FAILURE(
+      CreateSecureDexMetadataCompanion(sdm_location, runtime_->GetApexVersions(), sdc_location));
+
+  ASSERT_NO_FATAL_FAILURE(GenerateOdexForTest(dex_location,
+                                              odex_location,
+                                              CompilerFilter::kSpeedProfile,
+                                              /*compilation_reason=*/"bg-dexopt"));
+
+  auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
+
+  OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str());
+
+  VerifyOptimizationStatusWithInstance(&oat_file_assistant,
+                                       "speed-profile",
+                                       "bg-dexopt",
+                                       "up-to-date",
+                                       OatFileAssistant::kLocationOdex);
+}
+
+// Case: We have SDM, DM, and SDC files for an uncompressed DEX file. Meanwhile, we have a VDEX
+// file that is also up to date.
+// Expect: The SDM file is preferred over the VDEX file. The best artifact location should be
+// kLocationSdmOdex.
+TEST_P(OatFileAssistantTest, SdmAndVdexUpToDate) {
+  std::string dex_location = GetScratchDir() + "/TestDex.jar";
+  std::string sdm_location =
+      GetScratchDir() + ART_FORMAT("/TestDex.{}.sdm", GetInstructionSetString(kRuntimeISA));
+  std::string dm_location = GetScratchDir() + "/TestDex.dm";
+  std::string sdc_location = GetOdexDir() + "/TestDex.sdc";
+  std::string odex_location = GetOdexDir() + "/TestDex.odex";
+  Copy(GetMultiDexUncompressedAlignedSrc1(), dex_location);
+
+  ASSERT_NO_FATAL_FAILURE(GenerateSdmDmForTest(dex_location,
+                                               sdm_location,
+                                               dm_location,
+                                               CompilerFilter::kSpeedProfile,
+                                               /*include_app_image=*/true,
+                                               /*compilation_reason=*/"cloud"));
+  ASSERT_NO_FATAL_FAILURE(
+      CreateSecureDexMetadataCompanion(sdm_location, runtime_->GetApexVersions(), sdc_location));
+
+  ASSERT_NO_FATAL_FAILURE(GenerateOdexForTest(dex_location,
+                                              odex_location,
+                                              CompilerFilter::kSpeedProfile,
+                                              /*compilation_reason=*/"bg-dexopt"));
+  ASSERT_EQ(0, unlink(odex_location.c_str()));
+
+  auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
+
+  OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str());
+
+  VerifyOptimizationStatusWithInstance(&oat_file_assistant,
+                                       "speed-profile",
+                                       "cloud",
+                                       "up-to-date",
+                                       OatFileAssistant::kLocationSdmOdex);
+}
+
+// Case: We have SDM, DM, and SDC files for a compressed DEX file.
+// Expect: The SDM file is still picked. Dexopt should be performed if the compiler filter is
+// "speed-profile" or above.
+TEST_P(OatFileAssistantTest, SdmUpToDateCompressedDex) {
+  std::string dex_location = GetScratchDir() + "/TestDex.jar";
+  std::string sdm_location =
+      GetScratchDir() + ART_FORMAT("/TestDex.{}.sdm", GetInstructionSetString(kRuntimeISA));
+  std::string dm_location = GetScratchDir() + "/TestDex.dm";
+  std::string sdc_location = GetOdexDir() + "/TestDex.sdc";
+  Copy(GetMultiDexSrc1(), dex_location);
+
+  ASSERT_NO_FATAL_FAILURE(GenerateSdmDmForTest(dex_location,
+                                               sdm_location,
+                                               dm_location,
+                                               CompilerFilter::kSpeedProfile,
+                                               /*include_app_image=*/true,
+                                               /*compilation_reason=*/"cloud",
+                                               /*extra_args=*/{"--copy-dex-files=false"}));
+  ASSERT_NO_FATAL_FAILURE(
+      CreateSecureDexMetadataCompanion(sdm_location, runtime_->GetApexVersions(), sdc_location));
+
+  auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
+
+  OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str());
+
+  VerifyOptimizationStatusWithInstance(&oat_file_assistant,
+                                       "speed-profile",
+                                       "cloud",
+                                       "up-to-date",
+                                       OatFileAssistant::kLocationSdmOdex);
+
+  VerifyGetDexOptNeeded(&oat_file_assistant,
+                        CompilerFilter::kSpeedProfile,
+                        default_trigger_,
+                        /*expected_dexopt_needed=*/true,
+                        /*expected_is_vdex_usable=*/true,
+                        /*expected_location=*/OatFileAssistant::kLocationSdmOdex);
+  EXPECT_EQ(OatFileAssistant::kDex2OatFromScratch,
+            oat_file_assistant.GetDexOptNeeded(CompilerFilter::kSpeedProfile));
+
+  VerifyGetDexOptNeeded(&oat_file_assistant,
+                        CompilerFilter::kVerify,
+                        default_trigger_,
+                        /*expected_dexopt_needed=*/false,
+                        /*expected_is_vdex_usable=*/true,
+                        /*expected_location=*/OatFileAssistant::kLocationSdmOdex);
+  EXPECT_EQ(OatFileAssistant::kNoDexOptNeeded,
+            oat_file_assistant.GetDexOptNeeded(CompilerFilter::kVerify));
+}
+
+// Case: We have SDM, DM, and SDC files for an uncompressed DEX file, but the SDC file contains the
+// wrong APEX versions.
+// Expect: The SDM file is rejected, while the DM file is still picked. Dexopt should be performed
+// if the compiler filter is better than "verify".
+TEST_P(OatFileAssistantTest, SdmApexVersionMismatch) {
+  std::string dex_location = GetScratchDir() + "/TestDex.jar";
+  std::string sdm_location =
+      GetScratchDir() + ART_FORMAT("/TestDex.{}.sdm", GetInstructionSetString(kRuntimeISA));
+  std::string dm_location = GetScratchDir() + "/TestDex.dm";
+  std::string sdc_location = GetOdexDir() + "/TestDex.sdc";
+  Copy(GetMultiDexUncompressedAlignedSrc1(), dex_location);
+
+  ASSERT_NO_FATAL_FAILURE(GenerateSdmDmForTest(dex_location,
+                                               sdm_location,
+                                               dm_location,
+                                               CompilerFilter::kSpeedProfile,
+                                               /*include_app_image=*/true,
+                                               /*compilation_reason=*/"cloud"));
+  ASSERT_NO_FATAL_FAILURE(
+      CreateSecureDexMetadataCompanion(sdm_location, "wrong-apex-version", sdc_location));
+
+  auto scoped_maybe_without_runtime = ScopedMaybeWithoutRuntime();
+
+  OatFileAssistant oat_file_assistant = CreateOatFileAssistant(dex_location.c_str());
+
+  VerifyOptimizationStatusWithInstance(
+      &oat_file_assistant, "verify", "vdex", "up-to-date", OatFileAssistant::kLocationDm);
+
+  VerifyGetDexOptNeeded(&oat_file_assistant,
+                        CompilerFilter::kSpaceProfile,
+                        default_trigger_,
+                        /*expected_dexopt_needed=*/true,
+                        /*expected_is_vdex_usable=*/true,
+                        /*expected_location=*/OatFileAssistant::kLocationDm);
+  EXPECT_EQ(OatFileAssistant::kDex2OatFromScratch,
+            oat_file_assistant.GetDexOptNeeded(CompilerFilter::kSpaceProfile));
+
+  VerifyGetDexOptNeeded(&oat_file_assistant,
+                        CompilerFilter::kVerify,
+                        default_trigger_,
+                        /*expected_dexopt_needed=*/false,
+                        /*expected_is_vdex_usable=*/true,
+                        /*expected_location=*/OatFileAssistant::kLocationDm);
+  EXPECT_EQ(OatFileAssistant::kNoDexOptNeeded,
+            oat_file_assistant.GetDexOptNeeded(CompilerFilter::kVerify));
+}
+
+class CollectDexCacheVisitor : public DexCacheVisitor {
+ public:
+  explicit CollectDexCacheVisitor(
+      std::unordered_map<std::string, ObjPtr<mirror::DexCache>>& dex_caches)
+      : dex_caches_(dex_caches) {}
+
+  void Visit(ObjPtr<mirror::DexCache> dex_cache)
+      REQUIRES_SHARED(Locks::dex_lock_, Locks::mutator_lock_) override {
+    dex_caches_[dex_cache->GetDexFile()->GetLocation()] = dex_cache;
+  }
+
+ private:
+  std::unordered_map<std::string, ObjPtr<mirror::DexCache>>& dex_caches_;
+};
+
+// Test that, no matter the dex file is backed by an oat file or not, the location fields in
+// DexFile, OatDexFile, and DexCache are the same as the actual dex location.
 TEST_F(OatFileAssistantBaseTest, GetDexLocation) {
   std::string dex_location = GetScratchDir() + "/TestDex.jar";
+  std::string dex_location_multidex = dex_location + "!classes2.dex";
   std::string oat_location = GetOdexDir() + "/TestDex.odex";
   std::string art_location = GetOdexDir() + "/TestDex.art";
 
@@ -2320,7 +2515,7 @@ TEST_F(OatFileAssistantBaseTest, GetDexLocation) {
   Thread::Current()->TransitionFromSuspendedToRunnable();
   runtime_->Start();
 
-  Copy(GetDexSrc1(), dex_location);
+  Copy(GetMultiDexSrc1(), dex_location);
 
   std::vector<std::unique_ptr<const DexFile>> dex_files;
   std::vector<std::string> error_msgs;
@@ -2332,9 +2527,11 @@ TEST_F(OatFileAssistantBaseTest, GetDexLocation) {
       /*dex_elements=*/nullptr,
       &oat_file,
       &error_msgs);
-  ASSERT_EQ(dex_files.size(), 1u) << android::base::Join(error_msgs, "\n");
+  ASSERT_EQ(dex_files.size(), 2u) << android::base::Join(error_msgs, "\n");
   EXPECT_EQ(oat_file, nullptr);
-  std::string stored_dex_location = dex_files[0]->GetLocation();
+  EXPECT_EQ(dex_files[0]->GetLocation(), dex_location);
+  EXPECT_EQ(dex_files[1]->GetLocation(), dex_location_multidex);
+
   {
     // Create the oat file.
     std::vector<std::string> args;
@@ -2351,10 +2548,24 @@ TEST_F(OatFileAssistantBaseTest, GetDexLocation) {
       /*dex_elements=*/nullptr,
       &oat_file,
       &error_msgs);
-  ASSERT_EQ(dex_files.size(), 1u) << android::base::Join(error_msgs, "\n");
+  ASSERT_EQ(dex_files.size(), 2u) << android::base::Join(error_msgs, "\n");
   ASSERT_NE(oat_file, nullptr);
-  std::string oat_stored_dex_location = dex_files[0]->GetLocation();
-  EXPECT_EQ(oat_stored_dex_location, stored_dex_location);
+  EXPECT_EQ(dex_files[0]->GetLocation(), dex_location);
+  EXPECT_EQ(dex_files[1]->GetLocation(), dex_location_multidex);
+  EXPECT_EQ(oat_file->GetOatDexFiles()[0]->GetLocation(), dex_location);
+  EXPECT_EQ(oat_file->GetOatDexFiles()[1]->GetLocation(), dex_location_multidex);
+
+  std::unordered_map<std::string, ObjPtr<mirror::DexCache>> dex_caches;
+  CollectDexCacheVisitor visitor(dex_caches);
+  ClassLinker* class_linker = Runtime::Current()->GetClassLinker();
+  {
+    ScopedObjectAccess soa(Thread::Current());
+    ReaderMutexLock mu(Thread::Current(), *Locks::dex_lock_);
+    class_linker->VisitDexCaches(&visitor);
+    EXPECT_EQ(dex_caches[dex_location]->GetLocation()->ToModifiedUtf8(), dex_location);
+    EXPECT_EQ(dex_caches[dex_location_multidex]->GetLocation()->ToModifiedUtf8(),
+              dex_location_multidex);
+  }
 }
 
 // Test that a dex file on the platform location gets the right hiddenapi domain,
@@ -2687,6 +2898,8 @@ TEST_P(OatFileAssistantTest, ValidateBootClassPathChecksums) {
 //    - Oat file corrupted after status check, before reload unexecutable
 //    because it's unrelocated and no dex2oat
 
-INSTANTIATE_TEST_SUITE_P(WithOrWithoutRuntime, OatFileAssistantTest, testing::Values(true, false));
+INSTANTIATE_TEST_SUITE_P(WithOrWithoutRuntime,
+                         OatFileAssistantTest,
+                         ::testing::Values(true, false));
 
 }  // namespace art
diff --git a/runtime/oat/oat_file_manager.cc b/runtime/oat/oat_file_manager.cc
index 320961755f..cf54837d44 100644
--- a/runtime/oat/oat_file_manager.cc
+++ b/runtime/oat/oat_file_manager.cc
@@ -555,8 +555,7 @@ std::vector<std::unique_ptr<const DexFile>> OatFileManager::OpenDexFilesFromOat_
   std::unique_ptr<VdexFile> vdex_file = nullptr;
   if (has_vdex && OS::FileExists(vdex_path.c_str())) {
     vdex_file = VdexFile::Open(vdex_path,
-                               /* writable= */ false,
-                               /* low_4gb= */ false,
+                               /*low_4gb=*/false,
                                &error_msg);
     if (vdex_file == nullptr) {
       LOG(WARNING) << "Failed to open vdex " << vdex_path << ": " << error_msg;
diff --git a/runtime/oat/oat_file_test.cc b/runtime/oat/oat_file_test.cc
index 12a26f1cc1..f79c4cac26 100644
--- a/runtime/oat/oat_file_test.cc
+++ b/runtime/oat/oat_file_test.cc
@@ -16,6 +16,8 @@
 
 #include "oat_file.h"
 
+#include <dlfcn.h>
+
 #include <string>
 
 #include "common_runtime_test.h"
@@ -93,4 +95,56 @@ TEST_F(OatFileTest, ChangingMultiDexUncompressed) {
       << error_msg;
 }
 
+TEST_F(OatFileTest, DlOpenLoad) {
+  std::string dex_location = GetScratchDir() + "/LoadOat.jar";
+
+  Copy(GetDexSrc1(), dex_location);
+  GenerateOatForTest(dex_location.c_str(), CompilerFilter::kSpeed);
+
+  std::string oat_location;
+  std::string error_msg;
+  ASSERT_TRUE(OatFileAssistant::DexLocationToOatFilename(
+      dex_location, kRuntimeISA, &oat_location, &error_msg))
+      << error_msg;
+
+  // Clear previous errors if any.
+  dlerror();
+  error_msg.clear();
+  std::unique_ptr<OatFile> odex_file(OatFile::Open(/*zip_fd=*/-1,
+                                                   oat_location,
+                                                   oat_location,
+                                                   /*executable=*/true,
+                                                   /*low_4gb=*/false,
+                                                   dex_location,
+                                                   &error_msg));
+  ASSERT_NE(odex_file.get(), nullptr) << error_msg;
+
+#ifdef __GLIBC__
+  if (!error_msg.empty()) {
+    // If a valid oat file was returned but there was an error message, then dlopen failed
+    // but the backup ART ELF loader successfully loaded the oat file.
+    // The only expected reason for this is a bug in glibc that prevents loading dynamic
+    // shared objects with a read-only dynamic section:
+    // https://sourceware.org/bugzilla/show_bug.cgi?id=28340.
+    ASSERT_TRUE(error_msg == "DlOpen does not support read-only .dynamic section.") << error_msg;
+    GTEST_SKIP() << error_msg;
+  }
+#else
+  // If a valid oat file was returned with no error message, then dlopen was successful.
+  ASSERT_TRUE(error_msg.empty()) << error_msg;
+#endif
+
+  const char *dlerror_msg = dlerror();
+  ASSERT_EQ(dlerror_msg, nullptr) << dlerror_msg;
+
+  // Ensure that the oat file is loaded with dlopen by requesting information about it
+  // using dladdr.
+  Dl_info info;
+  ASSERT_NE(dladdr(odex_file->Begin(), &info), 0);
+  EXPECT_STREQ(info.dli_fname, oat_location.c_str())
+      << "dli_fname: " << info.dli_fname
+      << ", location: " << oat_location;
+  EXPECT_STREQ(info.dli_sname, "oatdata") << info.dli_sname;
+}
+
 }  // namespace art
diff --git a/runtime/oat/sdc_file.cc b/runtime/oat/sdc_file.cc
new file mode 100644
index 0000000000..e141aea85e
--- /dev/null
+++ b/runtime/oat/sdc_file.cc
@@ -0,0 +1,114 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "sdc_file.h"
+
+#include <cstdint>
+#include <memory>
+#include <regex>
+#include <string>
+#include <string_view>
+#include <unordered_map>
+#include <vector>
+
+#include "android-base/file.h"
+#include "android-base/parseint.h"
+#include "android-base/scopeguard.h"
+#include "base/macros.h"
+#include "base/utils.h"
+
+namespace art HIDDEN {
+
+using ::android::base::ParseInt;
+using ::android::base::ReadFileToString;
+using ::android::base::WriteStringToFd;
+
+std::unique_ptr<SdcReader> SdcReader::Load(const std::string& filename, std::string* error_msg) {
+  std::unique_ptr<SdcReader> reader(new SdcReader());
+
+  // The sdc file is supposed to be small, so read fully into memory for simplicity.
+  if (!ReadFileToString(filename, &reader->content_)) {
+    *error_msg = ART_FORMAT("Failed to load sdc file '{}': {}", filename, strerror(errno));
+    return nullptr;
+  }
+
+  std::vector<std::string_view> lines;
+  Split(reader->content_, '\n', &lines);
+  std::unordered_map<std::string_view, std::string_view> map;
+  for (std::string_view line : lines) {
+    size_t pos = line.find('=');
+    if (pos == std::string_view::npos || pos == 0) {
+      *error_msg = ART_FORMAT("Malformed line '{}' in sdc file '{}'", line, filename);
+      return nullptr;
+    }
+    if (!map.try_emplace(line.substr(0, pos), line.substr(pos + 1)).second) {
+      *error_msg = ART_FORMAT("Duplicate key '{}' in sdc file '{}'", line.substr(0, pos), filename);
+      return nullptr;
+    }
+  }
+
+  decltype(map)::iterator it;
+  if ((it = map.find("sdm-timestamp-ns")) == map.end()) {
+    *error_msg = ART_FORMAT("Missing key 'sdm-timestamp-ns' in sdc file '{}'", filename);
+    return nullptr;
+  }
+  if (!ParseInt(std::string(it->second), &reader->sdm_timestamp_ns_, /*min=*/INT64_C(1))) {
+    *error_msg = ART_FORMAT("Invalid 'sdm-timestamp-ns' {}", it->second);
+    return nullptr;
+  }
+
+  if ((it = map.find("apex-versions")) == map.end()) {
+    *error_msg = ART_FORMAT("Missing key 'apex-versions' in sdc file '{}'", filename);
+    return nullptr;
+  }
+  if (!std::regex_match(it->second.begin(), it->second.end(), std::regex("[0-9/]*"))) {
+    *error_msg = ART_FORMAT("Invalid 'apex-versions' {}", it->second);
+    return nullptr;
+  }
+  reader->apex_versions_ = it->second;
+
+  if (map.size() > 2) {
+    *error_msg = ART_FORMAT("Malformed sdc file '{}'. Unrecognized keys", filename);
+    return nullptr;
+  }
+
+  return reader;
+}
+
+bool SdcWriter::Save(std::string* error_msg) {
+  auto cleanup = android::base::make_scope_guard([this] { (void)file_.FlushClose(); });
+  if (sdm_timestamp_ns_ <= 0) {
+    *error_msg = ART_FORMAT("Invalid 'sdm-timestamp-ns' {}", sdm_timestamp_ns_);
+    return false;
+  }
+  DCHECK_EQ(file_.GetLength(), 0);
+  std::string content =
+      ART_FORMAT("sdm-timestamp-ns={}\napex-versions={}\n", sdm_timestamp_ns_, apex_versions_);
+  if (!WriteStringToFd(content, file_.Fd())) {
+    *error_msg = ART_FORMAT("Failed to write sdc file '{}': {}", file_.GetPath(), strerror(errno));
+    return false;
+  }
+  int res = file_.FlushClose();
+  if (res != 0) {
+    *error_msg =
+        ART_FORMAT("Failed to flush close sdc file '{}': {}", file_.GetPath(), strerror(-res));
+    return false;
+  }
+  cleanup.Disable();
+  return true;
+}
+
+}  // namespace art
diff --git a/runtime/oat/sdc_file.h b/runtime/oat/sdc_file.h
new file mode 100644
index 0000000000..f4d7a5d116
--- /dev/null
+++ b/runtime/oat/sdc_file.h
@@ -0,0 +1,102 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_RUNTIME_OAT_SDC_FILE_H_
+#define ART_RUNTIME_OAT_SDC_FILE_H_
+
+#include <memory>
+#include <string>
+#include <string_view>
+#include <utility>
+
+#include "base/macros.h"
+#include "base/os.h"
+
+namespace art HIDDEN {
+
+// A helper class to read a secure dex metadata companion (SDC) file.
+//
+// Secure dex metadata companion (SDC) file is a file type that augments a secure dex metadata (SDM)
+// file with additional metadata.
+//
+// 1. There may be exactly one SDC file accompanying each SDM file. An SDC file without a
+//    corresponding SDM file, or with a mismatching SDM timestamp, is garbage.
+// 2. They are always local on device.
+// 3. They are only read and written by the ART module.
+// 4. A later version of the ART module must be able to understand the contents.
+//
+// It is a text file in the format of:
+//   key1=value1\n
+//   key2=value2\n
+//   ...
+// Repeated keys are not allowed. This is an extensible format, so versioning is not needed.
+//
+// In principle, ART Service generates an SDC file for an SDM file during installation.
+// Specifically, during dexopt, which typically takes place during installation, if there is an SDM
+// file while the corresponding SDC file is missing (meaning the SDM file is newly installed) or
+// stale (meaning the SDM file is newly replaced), ART Service will generate a new SDC file. This
+// means an SDM file without a corresponding SDC file is a transient state and is valid from ART
+// Service's perspective.
+//
+// From the runtime's perspective, an SDM file without a corresponding SDC file is incomplete. That
+// means:
+// - At app execution time, the runtime ignores an SDM file without a corresponding SDC.
+// - ART Service's file GC, which uses the runtime's judgement, considers an SDM file without a
+//   corresponding SDC invalid and may clean it up. This may race with a package installation before
+//   the SDC is created, but it's rare and the effect is recoverable, so it's considered acceptable.
+class EXPORT SdcReader {
+ public:
+  static std::unique_ptr<SdcReader> Load(const std::string& filename, std::string* error_msg);
+
+  // The mtime of the SDM file on device, in nanoseconds.
+  // This is for detecting obsolete SDC files.
+  int64_t GetSdmTimestampNs() const { return sdm_timestamp_ns_; }
+
+  // The value of `Runtime::GetApexVersions` at the time where the SDM file was first seen on
+  // device. This is for detecting samegrade placebos.
+  std::string_view GetApexVersions() const { return apex_versions_; }
+
+ private:
+  SdcReader() = default;
+
+  std::string content_;
+  int64_t sdm_timestamp_ns_;
+  std::string_view apex_versions_;
+};
+
+// A helper class to write a secure dex metadata companion (SDC) file.
+class EXPORT SdcWriter {
+ public:
+  // Takes ownership of the file.
+  explicit SdcWriter(File&& file) : file_(std::move(file)) {}
+
+  // See `SdcReader::GetSdmTimestampNs`.
+  void SetSdmTimestampNs(int64_t value) { sdm_timestamp_ns_ = value; }
+
+  // See `SdcReader::GetApexVersions`.
+  void SetApexVersions(std::string_view value) { apex_versions_ = value; }
+
+  bool Save(std::string* error_msg);
+
+ private:
+  File file_;
+  int64_t sdm_timestamp_ns_ = 0;
+  std::string apex_versions_;
+};
+
+}  // namespace art
+
+#endif  // ART_RUNTIME_OAT_SDC_FILE_H_
diff --git a/runtime/oat/sdc_file_test.cc b/runtime/oat/sdc_file_test.cc
new file mode 100644
index 0000000000..eb1d3d6cb2
--- /dev/null
+++ b/runtime/oat/sdc_file_test.cc
@@ -0,0 +1,169 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "sdc_file.h"
+
+#include <cstdint>
+#include <memory>
+#include <string>
+
+#include "android-base/file.h"
+#include "base/common_art_test.h"
+#include "base/macros.h"
+#include "base/os.h"
+#include "gmock/gmock.h"
+#include "gtest/gtest.h"
+
+namespace art HIDDEN {
+
+using ::android::base::ReadFileToString;
+using ::android::base::WriteStringToFile;
+using ::testing::HasSubstr;
+using ::testing::StartsWith;
+
+class SdcFileTestBase : public CommonArtTest {
+ protected:
+  void SetUp() override {
+    CommonArtTest::SetUp();
+
+    scratch_dir_ = std::make_unique<ScratchDir>();
+    test_file_ = scratch_dir_->GetPath() + "test.sdc";
+  }
+
+  void TearDown() override {
+    scratch_dir_.reset();
+    CommonArtTest::TearDown();
+  }
+
+  std::unique_ptr<ScratchDir> scratch_dir_;
+  std::string test_file_;
+};
+
+class SdcReaderTest : public SdcFileTestBase {};
+
+TEST_F(SdcReaderTest, Success) {
+  ASSERT_TRUE(WriteStringToFile(
+      "sdm-timestamp-ns=987654321000000003\napex-versions=/12345678/12345679\n", test_file_));
+
+  std::string error_msg;
+  std::unique_ptr<SdcReader> reader = SdcReader::Load(test_file_, &error_msg);
+  ASSERT_NE(reader, nullptr) << error_msg;
+
+  EXPECT_EQ(reader->GetApexVersions(), "/12345678/12345679");
+  EXPECT_EQ(reader->GetSdmTimestampNs(), INT64_C(987654321000000003));
+}
+
+TEST_F(SdcReaderTest, NotFound) {
+  std::string error_msg;
+  std::unique_ptr<SdcReader> reader = SdcReader::Load(test_file_, &error_msg);
+  ASSERT_EQ(reader, nullptr);
+
+  EXPECT_THAT(error_msg, StartsWith("Failed to load sdc file"));
+}
+
+TEST_F(SdcReaderTest, MissingApexVersions) {
+  ASSERT_TRUE(WriteStringToFile("sdm-timestamp-ns=987654321\n", test_file_));
+
+  std::string error_msg;
+  std::unique_ptr<SdcReader> reader = SdcReader::Load(test_file_, &error_msg);
+  ASSERT_EQ(reader, nullptr);
+
+  EXPECT_THAT(error_msg, StartsWith("Missing key 'apex-versions' in sdc file"));
+}
+
+TEST_F(SdcReaderTest, InvalidSdmTimestamp) {
+  ASSERT_TRUE(
+      WriteStringToFile("sdm-timestamp-ns=0\napex-versions=/12345678/12345679\n", test_file_));
+
+  std::string error_msg;
+  std::unique_ptr<SdcReader> reader = SdcReader::Load(test_file_, &error_msg);
+  ASSERT_EQ(reader, nullptr);
+
+  EXPECT_THAT(error_msg, HasSubstr("Invalid 'sdm-timestamp-ns'"));
+}
+
+TEST_F(SdcReaderTest, InvalidApexVersions) {
+  ASSERT_TRUE(WriteStringToFile("sdm-timestamp-ns=987654321\napex-versions=abc\n", test_file_));
+
+  std::string error_msg;
+  std::unique_ptr<SdcReader> reader = SdcReader::Load(test_file_, &error_msg);
+  ASSERT_EQ(reader, nullptr);
+
+  EXPECT_THAT(error_msg, HasSubstr("Invalid 'apex-versions'"));
+}
+
+TEST_F(SdcReaderTest, UnrecognizedKey) {
+  ASSERT_TRUE(WriteStringToFile(
+      "sdm-timestamp-ns=987654321\napex-versions=/12345678/12345679\nwrong-key=12345678\n",
+      test_file_));
+
+  std::string error_msg;
+  std::unique_ptr<SdcReader> reader = SdcReader::Load(test_file_, &error_msg);
+  ASSERT_EQ(reader, nullptr);
+
+  EXPECT_THAT(error_msg, HasSubstr("Unrecognized keys"));
+}
+
+class SdcWriterTest : public SdcFileTestBase {};
+
+TEST_F(SdcWriterTest, Success) {
+  std::unique_ptr<File> file(OS::CreateEmptyFileWriteOnly(test_file_.c_str()));
+  ASSERT_NE(file, nullptr);
+  SdcWriter writer(std::move(*file));
+
+  writer.SetApexVersions("/12345678/12345679");
+  writer.SetSdmTimestampNs(987654321l);
+
+  std::string error_msg;
+  ASSERT_TRUE(writer.Save(&error_msg)) << error_msg;
+
+  std::string content;
+  ASSERT_TRUE(ReadFileToString(test_file_, &content));
+
+  EXPECT_EQ(content, "sdm-timestamp-ns=987654321\napex-versions=/12345678/12345679\n");
+}
+
+TEST_F(SdcWriterTest, SaveFailed) {
+  ASSERT_TRUE(WriteStringToFile("", test_file_));
+
+  std::unique_ptr<File> file(OS::OpenFileForReading(test_file_.c_str()));
+  ASSERT_NE(file, nullptr);
+  SdcWriter writer(
+      File(file->Release(), file->GetPath(), /*check_usage=*/false, /*read_only_mode=*/false));
+
+  writer.SetApexVersions("/12345678/12345679");
+  writer.SetSdmTimestampNs(987654321l);
+
+  std::string error_msg;
+  EXPECT_FALSE(writer.Save(&error_msg));
+
+  EXPECT_THAT(error_msg, StartsWith("Failed to write sdc file"));
+}
+
+TEST_F(SdcWriterTest, InvalidSdmTimestamp) {
+  std::unique_ptr<File> file(OS::CreateEmptyFileWriteOnly(test_file_.c_str()));
+  ASSERT_NE(file, nullptr);
+  SdcWriter writer(std::move(*file));
+
+  writer.SetApexVersions("/12345678/12345679");
+
+  std::string error_msg;
+  EXPECT_FALSE(writer.Save(&error_msg));
+
+  EXPECT_THAT(error_msg, StartsWith("Invalid 'sdm-timestamp-ns'"));
+}
+
+}  // namespace art
diff --git a/runtime/parsed_options_test.cc b/runtime/parsed_options_test.cc
index 973adb5a53..110754405a 100644
--- a/runtime/parsed_options_test.cc
+++ b/runtime/parsed_options_test.cc
@@ -149,7 +149,7 @@ TEST_F(ParsedOptionsTest, ParsedOptionsGenerationalCC) {
   EXPECT_TRUE(map.Exists(Opt::GcOption));
 
   XGcOption xgc = map.GetOrDefault(Opt::GcOption);
-  ASSERT_TRUE(xgc.generational_cc);
+  ASSERT_TRUE(xgc.generational_gc);
 }
 
 TEST_F(ParsedOptionsTest, ParsedOptionsInstructionSet) {
diff --git a/runtime/proxy_test.cc b/runtime/proxy_test.cc
index f9bf31bedd..d0e40d86bb 100644
--- a/runtime/proxy_test.cc
+++ b/runtime/proxy_test.cc
@@ -93,11 +93,10 @@ TEST_F(ProxyTest, ProxyFieldHelper) {
   ASSERT_TRUE(proxyClass->IsProxyClass());
   ASSERT_TRUE(proxyClass->IsInitialized());
 
-  EXPECT_TRUE(proxyClass->GetIFieldsPtr() == nullptr);
-
-  LengthPrefixedArray<ArtField>* static_fields = proxyClass->GetSFieldsPtr();
-  ASSERT_TRUE(static_fields != nullptr);
-  ASSERT_EQ(2u, proxyClass->NumStaticFields());
+  LengthPrefixedArray<ArtField>* fields = proxyClass->GetFieldsPtr();
+  ASSERT_TRUE(fields != nullptr);
+  ASSERT_EQ(2u, proxyClass->NumFields());
+  ASSERT_EQ(0u, proxyClass->ComputeNumInstanceFields());
 
   Handle<mirror::Class> interfacesFieldClass(
       hs.NewHandle(class_linker_->FindSystemClass(soa.Self(), "[Ljava/lang/Class;")));
@@ -107,7 +106,7 @@ TEST_F(ProxyTest, ProxyFieldHelper) {
   ASSERT_TRUE(throwsFieldClass != nullptr);
 
   // Test "Class[] interfaces" field.
-  ArtField* field = &static_fields->At(0);
+  ArtField* field = &fields->At(0);
   EXPECT_STREQ("interfaces", field->GetName());
   EXPECT_STREQ("[Ljava/lang/Class;", field->GetTypeDescriptor());
   EXPECT_EQ("[Ljava/lang/Class;", field->GetTypeDescriptorView());
@@ -117,7 +116,7 @@ TEST_F(ProxyTest, ProxyFieldHelper) {
   EXPECT_FALSE(field->IsPrimitiveType());
 
   // Test "Class[][] throws" field.
-  field = &static_fields->At(1);
+  field = &fields->At(1);
   EXPECT_STREQ("throws", field->GetName());
   EXPECT_STREQ("[[Ljava/lang/Class;", field->GetTypeDescriptor());
   EXPECT_EQ("[[Ljava/lang/Class;", field->GetTypeDescriptorView());
@@ -149,10 +148,10 @@ TEST_F(ProxyTest, CheckArtMirrorFieldsOfProxyStaticFields) {
   ASSERT_TRUE(proxyClass1->IsProxyClass());
   ASSERT_TRUE(proxyClass1->IsInitialized());
 
-  LengthPrefixedArray<ArtField>* static_fields0 = proxyClass0->GetSFieldsPtr();
+  LengthPrefixedArray<ArtField>* static_fields0 = proxyClass0->GetFieldsPtr();
   ASSERT_TRUE(static_fields0 != nullptr);
   ASSERT_EQ(2u, static_fields0->size());
-  LengthPrefixedArray<ArtField>* static_fields1 = proxyClass1->GetSFieldsPtr();
+  LengthPrefixedArray<ArtField>* static_fields1 = proxyClass1->GetFieldsPtr();
   ASSERT_TRUE(static_fields1 != nullptr);
   ASSERT_EQ(2u, static_fields1->size());
 
diff --git a/runtime/quick_exception_handler.cc b/runtime/quick_exception_handler.cc
index 09bbb2ab9e..8180d0a5b3 100644
--- a/runtime/quick_exception_handler.cc
+++ b/runtime/quick_exception_handler.cc
@@ -34,6 +34,7 @@
 #include "entrypoints/quick/quick_entrypoints_enum.h"
 #include "entrypoints/runtime_asm_entrypoints.h"
 #include "handle_scope-inl.h"
+#include "instrumentation.h"
 #include "interpreter/shadow_frame-inl.h"
 #include "jit/jit.h"
 #include "jit/jit_code_cache.h"
@@ -745,8 +746,7 @@ void QuickExceptionHandler::DeoptimizeSingleFrame(DeoptimizationKind kind) {
     runtime->GetJit()->GetCodeCache()->InvalidateCompiledCodeFor(
         deopt_method, visitor.GetSingleFrameDeoptQuickMethodHeader());
   } else {
-    runtime->GetInstrumentation()->InitializeMethodsCode(
-        deopt_method, /*aot_code=*/ nullptr);
+    runtime->GetInstrumentation()->ReinitializeMethodsCode(deopt_method);
   }
 
   // If the deoptimization is due to an inline cache, update it with the type
diff --git a/runtime/reflection.cc b/runtime/reflection.cc
index 8ebbeddcb1..c6dcf59ca1 100644
--- a/runtime/reflection.cc
+++ b/runtime/reflection.cc
@@ -262,13 +262,13 @@ class ArgArray {
 #define DO_FIRST_ARG(boxed, get_fn, append) { \
           if (LIKELY(arg != nullptr && \
                      arg->GetClass() == WellKnownClasses::java_lang_##boxed)) { \
-            ArtField* primitive_field = arg->GetClass()->GetInstanceField(0); \
+            ArtField* primitive_field = WellKnownClasses::java_lang_##boxed##_value; \
             append(primitive_field-> get_fn(arg.Get()));
 
 #define DO_ARG(boxed, get_fn, append) \
           } else if (LIKELY(arg != nullptr && \
                             arg->GetClass() == WellKnownClasses::java_lang_##boxed)) { \
-            ArtField* primitive_field = arg->GetClass()->GetInstanceField(0); \
+            ArtField* primitive_field = WellKnownClasses::java_lang_##boxed##_value; \
             append(primitive_field-> get_fn(arg.Get()));
 
 #define DO_FAIL(expected) \
@@ -952,31 +952,30 @@ static bool UnboxPrimitive(ObjPtr<mirror::Object> o,
   JValue boxed_value;
   ObjPtr<mirror::Class> klass = o->GetClass();
   Primitive::Type primitive_type;
-  ArtField* primitive_field = &klass->GetIFieldsPtr()->At(0);
   if (klass == WellKnownClasses::java_lang_Boolean) {
     primitive_type = Primitive::kPrimBoolean;
-    boxed_value.SetZ(primitive_field->GetBoolean(o));
+    boxed_value.SetZ(WellKnownClasses::java_lang_Boolean_value->GetBoolean(o));
   } else if (klass == WellKnownClasses::java_lang_Byte) {
     primitive_type = Primitive::kPrimByte;
-    boxed_value.SetB(primitive_field->GetByte(o));
+    boxed_value.SetB(WellKnownClasses::java_lang_Byte_value->GetByte(o));
   } else if (klass == WellKnownClasses::java_lang_Character) {
     primitive_type = Primitive::kPrimChar;
-    boxed_value.SetC(primitive_field->GetChar(o));
+    boxed_value.SetC(WellKnownClasses::java_lang_Character_value->GetChar(o));
   } else if (klass == WellKnownClasses::java_lang_Float) {
     primitive_type = Primitive::kPrimFloat;
-    boxed_value.SetF(primitive_field->GetFloat(o));
+    boxed_value.SetF(WellKnownClasses::java_lang_Float_value->GetFloat(o));
   } else if (klass == WellKnownClasses::java_lang_Double) {
     primitive_type = Primitive::kPrimDouble;
-    boxed_value.SetD(primitive_field->GetDouble(o));
+    boxed_value.SetD(WellKnownClasses::java_lang_Double_value->GetDouble(o));
   } else if (klass == WellKnownClasses::java_lang_Integer) {
     primitive_type = Primitive::kPrimInt;
-    boxed_value.SetI(primitive_field->GetInt(o));
+    boxed_value.SetI(WellKnownClasses::java_lang_Integer_value->GetInt(o));
   } else if (klass == WellKnownClasses::java_lang_Long) {
     primitive_type = Primitive::kPrimLong;
-    boxed_value.SetJ(primitive_field->GetLong(o));
+    boxed_value.SetJ(WellKnownClasses::java_lang_Long_value->GetLong(o));
   } else if (klass == WellKnownClasses::java_lang_Short) {
     primitive_type = Primitive::kPrimShort;
-    boxed_value.SetS(primitive_field->GetShort(o));
+    boxed_value.SetS(WellKnownClasses::java_lang_Short_value->GetShort(o));
   } else {
     std::string temp;
     ThrowIllegalArgumentException(
diff --git a/runtime/runtime.cc b/runtime/runtime.cc
index b1e71d3bb1..d6c11628fe 100644
--- a/runtime/runtime.cc
+++ b/runtime/runtime.cc
@@ -221,6 +221,8 @@ struct TraceConfig {
   TraceClockSource clock_source;
 };
 
+extern bool ShouldUseGenerationalGC();
+
 namespace {
 
 #ifdef __APPLE__
@@ -280,7 +282,7 @@ Runtime::Runtime()
       abort_(nullptr),
       stats_enabled_(false),
       is_running_on_memory_tool_(kRunningOnMemoryTool),
-      instrumentation_(),
+      instrumentation_(new instrumentation::Instrumentation()),
       main_thread_group_(nullptr),
       system_thread_group_(nullptr),
       system_class_loader_(nullptr),
@@ -1323,7 +1325,6 @@ void Runtime::InitNonZygoteOrPostFork(
     if (!odrefresh::UploadStatsIfAvailable(&err)) {
       LOG(WARNING) << "Failed to upload odrefresh metrics: " << err;
     }
-    metrics::SetupCallbackForDeviceStatus();
     metrics::ReportDeviceMetrics();
   }
 
@@ -1439,6 +1440,40 @@ static inline void CreatePreAllocatedException(Thread* self,
   detailMessageField->SetObject</* kTransactionActive= */ false>(exception->Read(), message);
 }
 
+inline void Runtime::CreatePreAllocatedExceptions(Thread* self) {
+  // Pre-allocate an OutOfMemoryError for the case when we fail to
+  // allocate the exception to be thrown.
+  CreatePreAllocatedException(self,
+                              this,
+                              &pre_allocated_OutOfMemoryError_when_throwing_exception_,
+                              "Ljava/lang/OutOfMemoryError;",
+                              "OutOfMemoryError thrown while trying to throw an exception; "
+                              "no stack trace available");
+  // Pre-allocate an OutOfMemoryError for the double-OOME case.
+  CreatePreAllocatedException(self,
+                              this,
+                              &pre_allocated_OutOfMemoryError_when_throwing_oome_,
+                              "Ljava/lang/OutOfMemoryError;",
+                              "OutOfMemoryError thrown while trying to throw OutOfMemoryError; "
+                              "no stack trace available");
+  // Pre-allocate an OutOfMemoryError for the case when we fail to
+  // allocate while handling a stack overflow.
+  CreatePreAllocatedException(self,
+                              this,
+                              &pre_allocated_OutOfMemoryError_when_handling_stack_overflow_,
+                              "Ljava/lang/OutOfMemoryError;",
+                              "OutOfMemoryError thrown while trying to handle a stack overflow; "
+                              "no stack trace available");
+  // Pre-allocate a NoClassDefFoundError for the common case of failing to find a system class
+  // ahead of checking the application's class loader.
+  CreatePreAllocatedException(self,
+                              this,
+                              &pre_allocated_NoClassDefFoundError_,
+                              "Ljava/lang/NoClassDefFoundError;",
+                              "Class not found using the boot class loader; "
+                              "no stack trace available");
+}
+
 std::string Runtime::GetApexVersions(ArrayRef<const std::string> boot_class_path_locations) {
   std::vector<std::string_view> bcp_apexes;
   for (std::string_view jar : boot_class_path_locations) {
@@ -1743,7 +1778,8 @@ bool Runtime::Init(RuntimeArgumentMap&& runtime_options_in) {
   XGcOption xgc_option = runtime_options.GetOrDefault(Opt::GcOption);
 
   // Generational CC collection is currently only compatible with Baker read barriers.
-  bool use_generational_cc = kUseBakerReadBarrier && xgc_option.generational_cc;
+  bool use_generational_gc = (kUseBakerReadBarrier || gUseUserfaultfd) &&
+                             xgc_option.generational_gc && ShouldUseGenerationalGC();
 
   // Cache the apex versions.
   InitializeApexVersions();
@@ -1792,7 +1828,7 @@ bool Runtime::Init(RuntimeArgumentMap&& runtime_options_in) {
                        xgc_option.gcstress_,
                        xgc_option.measure_,
                        runtime_options.GetOrDefault(Opt::EnableHSpaceCompactForOOM),
-                       use_generational_cc,
+                       use_generational_gc,
                        runtime_options.GetOrDefault(Opt::HSpaceCompactForOOMMinIntervalsMs),
                        runtime_options.Exists(Opt::DumpRegionInfoBeforeGC),
                        runtime_options.Exists(Opt::DumpRegionInfoAfterGC));
@@ -1889,6 +1925,12 @@ bool Runtime::Init(RuntimeArgumentMap&& runtime_options_in) {
       break;
   }
 
+#ifdef ART_USE_RESTRICTED_MODE
+  // TODO(Simulator): support signal handling and implicit checks.
+  implicit_suspend_checks_ = false;
+  implicit_null_checks_ = false;
+#endif  // ART_USE_RESTRICTED_MODE
+
   fault_manager.Init(!no_sig_chain_);
   if (!no_sig_chain_) {
     if (HandlesSignalsInCompiledCode()) {
@@ -2073,38 +2115,7 @@ bool Runtime::Init(RuntimeArgumentMap&& runtime_options_in) {
     DCHECK(pre_allocated_NoClassDefFoundError_.Read()->GetClass()
                ->DescriptorEquals("Ljava/lang/NoClassDefFoundError;"));
   } else {
-    // Pre-allocate an OutOfMemoryError for the case when we fail to
-    // allocate the exception to be thrown.
-    CreatePreAllocatedException(self,
-                                this,
-                                &pre_allocated_OutOfMemoryError_when_throwing_exception_,
-                                "Ljava/lang/OutOfMemoryError;",
-                                "OutOfMemoryError thrown while trying to throw an exception; "
-                                    "no stack trace available");
-    // Pre-allocate an OutOfMemoryError for the double-OOME case.
-    CreatePreAllocatedException(self,
-                                this,
-                                &pre_allocated_OutOfMemoryError_when_throwing_oome_,
-                                "Ljava/lang/OutOfMemoryError;",
-                                "OutOfMemoryError thrown while trying to throw OutOfMemoryError; "
-                                    "no stack trace available");
-    // Pre-allocate an OutOfMemoryError for the case when we fail to
-    // allocate while handling a stack overflow.
-    CreatePreAllocatedException(self,
-                                this,
-                                &pre_allocated_OutOfMemoryError_when_handling_stack_overflow_,
-                                "Ljava/lang/OutOfMemoryError;",
-                                "OutOfMemoryError thrown while trying to handle a stack overflow; "
-                                    "no stack trace available");
-
-    // Pre-allocate a NoClassDefFoundError for the common case of failing to find a system class
-    // ahead of checking the application's class loader.
-    CreatePreAllocatedException(self,
-                                this,
-                                &pre_allocated_NoClassDefFoundError_,
-                                "Ljava/lang/NoClassDefFoundError;",
-                                "Class not found using the boot class loader; "
-                                    "no stack trace available");
+    CreatePreAllocatedExceptions(self);
   }
 
   // Class-roots are setup, we can now finish initializing the JniIdManager.
@@ -2447,8 +2458,8 @@ void Runtime::DumpDeoptimizations(std::ostream& os) {
   }
 }
 
-std::optional<uint64_t> Runtime::SiqQuitNanoTime() const {
-  return signal_catcher_ != nullptr ? signal_catcher_->SiqQuitNanoTime() : std::nullopt;
+std::optional<uint64_t> Runtime::SigQuitNanoTime() const {
+  return signal_catcher_ != nullptr ? signal_catcher_->SigQuitNanoTime() : std::nullopt;
 }
 
 void Runtime::DumpForSigQuit(std::ostream& os) {
@@ -3210,21 +3221,21 @@ class DeoptimizeBootImageClassVisitor : public ClassVisitor {
       if (Runtime::Current()->GetHeap()->IsInBootImageOatFile(code) &&
           (!m.IsNative() || deoptimize_native_methods) &&
           !m.IsProxyMethod()) {
-        instrumentation_->InitializeMethodsCode(&m, /*aot_code=*/ nullptr);
+        instrumentation_->ReinitializeMethodsCode(&m);
       }
 
       if (Runtime::Current()->GetJit() != nullptr &&
           Runtime::Current()->GetJit()->GetCodeCache()->IsInZygoteExecSpace(code) &&
           (!m.IsNative() || deoptimize_native_methods)) {
         DCHECK(!m.IsProxyMethod());
-        instrumentation_->InitializeMethodsCode(&m, /*aot_code=*/ nullptr);
+        instrumentation_->ReinitializeMethodsCode(&m);
       }
 
       if (m.IsPreCompiled()) {
         // Precompilation is incompatible with debuggable, so clear the flag
         // and update the entrypoint in case it has been compiled.
         m.ClearPreCompiled();
-        instrumentation_->InitializeMethodsCode(&m, /*aot_code=*/ nullptr);
+        instrumentation_->ReinitializeMethodsCode(&m);
       }
 
       // Clear MemorySharedAccessFlags so the boot class methods can be JITed better.
@@ -3505,10 +3516,4 @@ void Runtime::DCheckNoTransactionCheckAllowed() {
   }
 }
 
-NO_INLINE void Runtime::AllowPageSizeAccess() {
-#ifdef ART_PAGE_SIZE_AGNOSTIC
-  gPageSize.AllowAccess();
-#endif
-}
-
 }  // namespace art
diff --git a/runtime/runtime.h b/runtime/runtime.h
index 97eac64fa1..8f781cff34 100644
--- a/runtime/runtime.h
+++ b/runtime/runtime.h
@@ -40,7 +40,6 @@
 #include "dex/dex_file_types.h"
 #include "experimental_flags.h"
 #include "gc_root.h"
-#include "instrumentation.h"
 #include "jdwp_provider.h"
 #include "jni/jni_id_manager.h"
 #include "jni_id_type.h"
@@ -63,6 +62,10 @@ namespace hiddenapi {
 enum class EnforcementPolicy;
 }  // namespace hiddenapi
 
+namespace instrumentation {
+class Instrumentation;
+}  // namespace instrumentation
+
 namespace jit {
 class Jit;
 class JitCodeCache;
@@ -314,7 +317,7 @@ class Runtime {
   void DetachCurrentThread(bool should_run_callbacks = true) REQUIRES(!Locks::mutator_lock_);
 
   // If we are handling SIQQUIT return the time when we received it.
-  std::optional<uint64_t> SiqQuitNanoTime() const;
+  std::optional<uint64_t> SigQuitNanoTime() const;
 
   void DumpDeoptimizations(std::ostream& os);
   void DumpForSigQuit(std::ostream& os);
@@ -607,11 +610,11 @@ class Runtime {
       bool profile_system_server = false);
 
   const instrumentation::Instrumentation* GetInstrumentation() const {
-    return &instrumentation_;
+    return instrumentation_.get();
   }
 
   instrumentation::Instrumentation* GetInstrumentation() {
-    return &instrumentation_;
+    return instrumentation_.get();
   }
 
   void RegisterAppInfo(const std::string& package_name,
@@ -1115,6 +1118,8 @@ class Runtime {
   // See Flags::ReloadAllFlags as well.
   static void ReloadAllFlags(const std::string& caller);
 
+  inline void CreatePreAllocatedExceptions(Thread* self) REQUIRES_SHARED(Locks::mutator_lock_);
+
   // Parses /apex/apex-info-list.xml to build a string containing apex versions of boot classpath
   // jars, which is encoded into .oat files.
   static std::string GetApexVersions(ArrayRef<const std::string> boot_class_path_locations);
@@ -1134,9 +1139,6 @@ class Runtime {
 
   bool AreMetricsInitialized() const { return metrics_reporter_ != nullptr; }
 
-  // For `artd` only.
-  EXPORT static void AllowPageSizeAccess();
-
  private:
   static void InitPlatformSignalHandlers();
 
@@ -1334,7 +1336,7 @@ class Runtime {
 
   std::unique_ptr<TraceConfig> trace_config_;
 
-  instrumentation::Instrumentation instrumentation_;
+  std::unique_ptr<instrumentation::Instrumentation> instrumentation_;
 
   jobject main_thread_group_;
   jobject system_thread_group_;
diff --git a/runtime/runtime_callbacks_test.cc b/runtime/runtime_callbacks_test.cc
index 053d4eaaf5..0fd9efa65e 100644
--- a/runtime/runtime_callbacks_test.cc
+++ b/runtime/runtime_callbacks_test.cc
@@ -35,6 +35,7 @@
 #include "dex/class_reference.h"
 #include "handle.h"
 #include "handle_scope-inl.h"
+#include "instrumentation.h"
 #include "mirror/class-alloc-inl.h"
 #include "mirror/class_loader.h"
 #include "monitor-inl.h"
@@ -80,7 +81,7 @@ class RuntimeCallbacksTest : public CommonRuntimeTest {
     PointerSize pointer_size = class_linker_->GetImagePointerSize();
     for (auto& m : klass->GetMethods(pointer_size)) {
       if (!m.IsAbstract()) {
-        Runtime::Current()->GetInstrumentation()->InitializeMethodsCode(&m, /*aot_code=*/ nullptr);
+        Runtime::Current()->GetInstrumentation()->ReinitializeMethodsCode(&m);
       }
     }
   }
diff --git a/runtime/runtime_globals.h b/runtime/runtime_globals.h
index dc69063b97..cdab9e0deb 100644
--- a/runtime/runtime_globals.h
+++ b/runtime/runtime_globals.h
@@ -110,7 +110,12 @@ static constexpr ALWAYS_INLINE size_t ModuloPageSize(size_t num) {
 // Returns whether the given memory offset can be used for generating
 // an implicit null check.
 static inline bool CanDoImplicitNullCheckOn(uintptr_t offset) {
+#ifdef ART_USE_RESTRICTED_MODE
+  UNUSED(offset);
+  return false;
+#else
   return offset < gPageSize;
+#endif  // ART_USE_RESTRICTED_MODE
 }
 
 // Required object alignment
@@ -122,24 +127,24 @@ static constexpr bool kMovingCollector = true;
 static constexpr bool kMarkCompactSupport = false && kMovingCollector;
 // True if we allow moving classes.
 static constexpr bool kMovingClasses = !kMarkCompactSupport;
-// When using the Concurrent Copying (CC) collector, if
-// `ART_USE_GENERATIONAL_CC` is true, enable generational collection by default,
-// i.e. use sticky-bit CC for minor collections and (full) CC for major
+// When using the Concurrent Collectors (CC or CMC), if
+// `ART_USE_GENERATIONAL_GC` is true, enable generational collection by default,
+// i.e. use sticky-bit CC/CMC for minor collections and (full) CC/CMC for major
 // collections.
 // This default value can be overridden with the runtime option
-// `-Xgc:[no]generational_cc`.
+// `-Xgc:[no]generational_gc`.
 //
 // TODO(b/67628039): Consider either:
 // - renaming this to a better descriptive name (e.g.
-//   `ART_USE_GENERATIONAL_CC_BY_DEFAULT`); or
-// - removing `ART_USE_GENERATIONAL_CC` and having a fixed default value.
+//   `ART_USE_GENERATIONAL_GC_BY_DEFAULT`); or
+// - removing `ART_USE_GENERATIONAL_GC` and having a fixed default value.
 // Any of these changes will require adjusting users of this preprocessor
 // directive and the corresponding build system environment variable (e.g. in
 // ART's continuous testing).
-#ifdef ART_USE_GENERATIONAL_CC
-static constexpr bool kEnableGenerationalCCByDefault = true;
+#ifdef ART_USE_GENERATIONAL_GC
+static constexpr bool kEnableGenerationalGCByDefault = true;
 #else
-static constexpr bool kEnableGenerationalCCByDefault = false;
+static constexpr bool kEnableGenerationalGCByDefault = false;
 #endif
 
 // If true, enable the tlab allocator by default.
diff --git a/runtime/runtime_image.cc b/runtime/runtime_image.cc
index c73810de59..28f311a6f9 100644
--- a/runtime/runtime_image.cc
+++ b/runtime/runtime_image.cc
@@ -860,27 +860,23 @@ class RuntimeImageHelper {
 
   void CopyFieldArrays(ObjPtr<mirror::Class> cls, uint32_t class_image_address)
       REQUIRES_SHARED(Locks::mutator_lock_) {
-    LengthPrefixedArray<ArtField>* fields[] = {
-        cls->GetSFieldsPtr(), cls->GetIFieldsPtr(),
-    };
-    for (LengthPrefixedArray<ArtField>* cur_fields : fields) {
-      if (cur_fields != nullptr) {
-        // Copy the array.
-        size_t number_of_fields = cur_fields->size();
-        size_t size = LengthPrefixedArray<ArtField>::ComputeSize(number_of_fields);
-        size_t offset = art_fields_.size();
-        art_fields_.resize(offset + size);
-        auto* dest_array =
-            reinterpret_cast<LengthPrefixedArray<ArtField>*>(art_fields_.data() + offset);
-        memcpy(dest_array, cur_fields, size);
-        native_relocations_.Put(cur_fields,
-                                std::make_pair(NativeRelocationKind::kArtFieldArray, offset));
-
-        // Update the class pointer of individual fields.
-        for (size_t i = 0; i != number_of_fields; ++i) {
-          dest_array->At(i).GetDeclaringClassAddressWithoutBarrier()->Assign(
-              reinterpret_cast<mirror::Class*>(class_image_address));
-        }
+    LengthPrefixedArray<ArtField>* cur_fields = cls->GetFieldsPtr();
+    if (cur_fields != nullptr) {
+      // Copy the array.
+      size_t number_of_fields = cur_fields->size();
+      size_t size = LengthPrefixedArray<ArtField>::ComputeSize(number_of_fields);
+      size_t offset = art_fields_.size();
+      art_fields_.resize(offset + size);
+      auto* dest_array =
+          reinterpret_cast<LengthPrefixedArray<ArtField>*>(art_fields_.data() + offset);
+      memcpy(dest_array, cur_fields, size);
+      native_relocations_.Put(cur_fields,
+                              std::make_pair(NativeRelocationKind::kArtFieldArray, offset));
+
+      // Update the class pointer of individual fields.
+      for (size_t i = 0; i != number_of_fields; ++i) {
+        dest_array->At(i).GetDeclaringClassAddressWithoutBarrier()->Assign(
+            reinterpret_cast<mirror::Class*>(class_image_address));
       }
     }
   }
@@ -1142,11 +1138,12 @@ class RuntimeImageHelper {
 
     std::unique_ptr<const InstructionSetFeatures> isa_features =
         InstructionSetFeatures::FromCppDefines();
-    std::unique_ptr<OatHeader> oat_header(
+    std::unique_ptr<OatHeader, decltype(&OatHeader::Delete)> oat_header(
         OatHeader::Create(kRuntimeQuickCodeISA,
                           isa_features.get(),
                           number_of_dex_files,
-                          &key_value_store));
+                          &key_value_store),
+        &OatHeader::Delete);
 
     // Create the byte array containing the oat header and dex checksums.
     uint32_t checksums_size = checksums.size() * sizeof(uint32_t);
@@ -1429,7 +1426,7 @@ class RuntimeImageHelper {
     }
 
     // Trivial case: no static fields.
-    if (cls->NumStaticFields() == 0u) {
+    if (!cls->HasStaticFields()) {
       return true;
     }
 
@@ -1872,11 +1869,6 @@ static bool EnsureDirectoryExists(const std::string& directory, std::string* err
 }
 
 bool RuntimeImage::WriteImageToDisk(std::string* error_msg) {
-  if (gPageSize != kMinPageSize) {
-    *error_msg = "Writing runtime image is only supported on devices with 4K page size";
-    return false;
-  }
-
   gc::Heap* heap = Runtime::Current()->GetHeap();
   if (!heap->HasBootImageSpace()) {
     *error_msg = "Cannot generate an app image without a boot image";
diff --git a/runtime/runtime_test.cc b/runtime/runtime_test.cc
index 182a992434..3360b3b558 100644
--- a/runtime/runtime_test.cc
+++ b/runtime/runtime_test.cc
@@ -102,8 +102,6 @@ TEST_F(RuntimeTest, ElfAlignmentMismatch) {
 
   std::string error_msg;
   std::unique_ptr<ElfFile> elf_file(ElfFile::Open(core_oat_file.get(),
-                                                  /*writable=*/false,
-                                                  /*program_header_only=*/true,
                                                   /*low_4gb=*/false,
                                                   &error_msg));
   ASSERT_TRUE(elf_file != nullptr) << error_msg;
diff --git a/runtime/signal_catcher.cc b/runtime/signal_catcher.cc
index 09a4b2d85e..c7e297f3b7 100644
--- a/runtime/signal_catcher.cc
+++ b/runtime/signal_catcher.cc
@@ -37,6 +37,7 @@
 #include "base/time_utils.h"
 #include "base/utils.h"
 #include "class_linker.h"
+#include "com_android_art_flags.h"
 #include "gc/heap.h"
 #include "jit/profile_saver.h"
 #include "palette/palette.h"
@@ -45,6 +46,9 @@
 #include "signal_set.h"
 #include "thread.h"
 #include "thread_list.h"
+#include "trace_profile.h"
+
+namespace art_flags = com::android::art::flags;
 
 namespace art HIDDEN {
 
@@ -138,6 +142,10 @@ void SignalCatcher::HandleSigQuit() {
 
   os << "Debug Store: " << DebugStoreGetString() << "\n";
 
+  if (art_flags::always_enable_profile_code()) {
+    os << "LongRunningMethods: " << TraceProfiler::GetLongRunningMethodsString() << "\n";
+  }
+
   runtime->DumpForSigQuit(os);
 
   if ((false)) {
diff --git a/runtime/signal_catcher.h b/runtime/signal_catcher.h
index 79014ea022..4d5d71ede8 100644
--- a/runtime/signal_catcher.h
+++ b/runtime/signal_catcher.h
@@ -42,7 +42,7 @@ class SignalCatcher {
   void HandleSigQuit() REQUIRES(!Locks::mutator_lock_, !Locks::thread_list_lock_,
                                 !Locks::thread_suspend_count_lock_);
 
-  std::optional<uint64_t> SiqQuitNanoTime() const { return sigquit_nanotime_; }
+  std::optional<uint64_t> SigQuitNanoTime() const { return sigquit_nanotime_; }
 
  private:
   // NO_THREAD_SAFETY_ANALYSIS for static function calling into member function with excludes lock.
diff --git a/runtime/thread-inl.h b/runtime/thread-inl.h
index 432453e311..431d66d7ff 100644
--- a/runtime/thread-inl.h
+++ b/runtime/thread-inl.h
@@ -83,7 +83,11 @@ inline void Thread::AllowThreadSuspension() {
 inline void Thread::CheckSuspend(bool implicit) {
   DCHECK_EQ(Thread::Current(), this);
   while (true) {
-    StateAndFlags state_and_flags = GetStateAndFlags(std::memory_order_relaxed);
+    // Memory_order_relaxed should be OK, since RunCheckpointFunction shares a lock with the
+    // requestor, and FullSuspendCheck() re-checks later. But we currently need memory_order_acquire
+    // for the empty checkpoint path.
+    // TODO (b/382722942): Revisit after we fix RunEmptyCheckpoint().
+    StateAndFlags state_and_flags = GetStateAndFlags(std::memory_order_acquire);
     if (LIKELY(!state_and_flags.IsAnyOfFlagsSet(SuspendOrCheckpointRequestFlags()))) {
       break;
     } else if (state_and_flags.IsFlagSet(ThreadFlag::kCheckpointRequest)) {
@@ -110,7 +114,8 @@ inline void Thread::CheckEmptyCheckpointFromWeakRefAccess(BaseMutex* cond_var_mu
   Thread* self = Thread::Current();
   DCHECK_EQ(self, this);
   for (;;) {
-    if (ReadFlag(ThreadFlag::kEmptyCheckpointRequest)) {
+    // TODO (b/382722942): Revisit memory ordering after we fix RunEmptyCheckpoint().
+    if (ReadFlag(ThreadFlag::kEmptyCheckpointRequest, std::memory_order_acquire)) {
       RunEmptyCheckpoint();
       // Check we hold only an expected mutex when accessing weak ref.
       if (kIsDebugBuild) {
@@ -135,7 +140,8 @@ inline void Thread::CheckEmptyCheckpointFromWeakRefAccess(BaseMutex* cond_var_mu
 inline void Thread::CheckEmptyCheckpointFromMutex() {
   DCHECK_EQ(Thread::Current(), this);
   for (;;) {
-    if (ReadFlag(ThreadFlag::kEmptyCheckpointRequest)) {
+    // TODO (b/382722942): Revisit memory ordering after we fix RunEmptyCheckpoint().
+    if (ReadFlag(ThreadFlag::kEmptyCheckpointRequest, std::memory_order_acquire)) {
       RunEmptyCheckpoint();
     } else {
       break;
@@ -234,7 +240,11 @@ inline void Thread::AssertThreadSuspensionIsAllowable(bool check_locks) const {
 inline void Thread::TransitionToSuspendedAndRunCheckpoints(ThreadState new_state) {
   DCHECK_NE(new_state, ThreadState::kRunnable);
   while (true) {
-    StateAndFlags old_state_and_flags = GetStateAndFlags(std::memory_order_relaxed);
+    // memory_order_relaxed is OK for ordinary checkpoints, which enforce ordering via
+    // thread_suspend_count_lock_ . It is not currently OK for empty checkpoints.
+    // TODO (b/382722942): Consider changing back to memory_order_relaxed after fixing empty
+    // checkpoints.
+    StateAndFlags old_state_and_flags = GetStateAndFlags(std::memory_order_acquire);
     DCHECK_EQ(old_state_and_flags.GetState(), ThreadState::kRunnable);
     if (UNLIKELY(old_state_and_flags.IsFlagSet(ThreadFlag::kCheckpointRequest))) {
       IncrementStatsCounter(&checkpoint_count_);
@@ -265,6 +275,8 @@ inline void Thread::TransitionToSuspendedAndRunCheckpoints(ThreadState new_state
 inline void Thread::CheckActiveSuspendBarriers() {
   DCHECK_NE(GetState(), ThreadState::kRunnable);
   while (true) {
+    // memory_order_relaxed is OK here, since PassActiveSuspendBarriers() rechecks with
+    // thread_suspend_count_lock_ .
     StateAndFlags state_and_flags = GetStateAndFlags(std::memory_order_relaxed);
     if (LIKELY(!state_and_flags.IsFlagSet(ThreadFlag::kCheckpointRequest) &&
                !state_and_flags.IsFlagSet(ThreadFlag::kEmptyCheckpointRequest) &&
@@ -540,7 +552,7 @@ inline void Thread::IncrementSuspendCount(Thread* self) {
 }
 
 inline void Thread::DecrementSuspendCount(Thread* self, bool for_user_code) {
-  DCHECK(ReadFlag(ThreadFlag::kSuspendRequest));
+  DCHECK(ReadFlag(ThreadFlag::kSuspendRequest, std::memory_order_relaxed));
   Locks::thread_suspend_count_lock_->AssertHeld(self);
   if (UNLIKELY(tls32_.suspend_count <= 0)) {
     UnsafeLogFatalForSuspendCount(self, this);
diff --git a/runtime/thread.cc b/runtime/thread.cc
index 5bd97eb59a..8d37d4006a 100644
--- a/runtime/thread.cc
+++ b/runtime/thread.cc
@@ -166,10 +166,10 @@ void InitEntryPoints(JniEntryPoints* jpoints,
                      QuickEntryPoints* qpoints,
                      bool monitor_jni_entry_exit);
 void UpdateReadBarrierEntrypoints(QuickEntryPoints* qpoints, bool is_active);
-void UpdateLowOverheadTraceEntrypoints(QuickEntryPoints* qpoints, bool enable);
+void UpdateLowOverheadTraceEntrypoints(QuickEntryPoints* qpoints, LowOverheadTraceType trace_type);
 
-void Thread::UpdateTlsLowOverheadTraceEntrypoints(bool enable) {
-  UpdateLowOverheadTraceEntrypoints(&tlsPtr_.quick_entrypoints, enable);
+void Thread::UpdateTlsLowOverheadTraceEntrypoints(LowOverheadTraceType trace_type) {
+  UpdateLowOverheadTraceEntrypoints(&tlsPtr_.quick_entrypoints, trace_type);
 }
 
 void Thread::SetIsGcMarkingAndUpdateEntrypoints(bool is_marking) {
@@ -1072,7 +1072,7 @@ bool Thread::Init(ThreadList* thread_list, JavaVMExt* java_vm, JNIEnvExt* jni_en
   ScopedTrace trace3("ThreadList::Register");
   thread_list->Register(this);
   if (art_flags::always_enable_profile_code()) {
-    UpdateTlsLowOverheadTraceEntrypoints(!Trace::IsTracingEnabled());
+    UpdateTlsLowOverheadTraceEntrypoints(TraceProfiler::GetTraceType());
   }
   return true;
 }
@@ -1484,13 +1484,22 @@ void Thread::GetThreadName(std::string& name) const {
 }
 
 uint64_t Thread::GetCpuMicroTime() const {
+#if defined(__linux__)
+  return Thread::GetCpuNanoTime() / 1000;
+#else  // __APPLE__
+  UNIMPLEMENTED(WARNING);
+  return -1;
+#endif
+}
+
+uint64_t Thread::GetCpuNanoTime() const {
 #if defined(__linux__)
   clockid_t cpu_clock_id;
   pthread_getcpuclockid(tlsPtr_.pthread_self, &cpu_clock_id);
   timespec now;
   clock_gettime(cpu_clock_id, &now);
-  return static_cast<uint64_t>(now.tv_sec) * UINT64_C(1000000) +
-         static_cast<uint64_t>(now.tv_nsec) / UINT64_C(1000);
+  return static_cast<uint64_t>(now.tv_sec) * UINT64_C(1000000000) +
+         static_cast<uint64_t>(now.tv_nsec);
 #else  // __APPLE__
   UNIMPLEMENTED(WARNING);
   return -1;
@@ -1530,7 +1539,7 @@ bool Thread::PassActiveSuspendBarriers() {
   std::vector<AtomicInteger*> pass_barriers{};
   {
     MutexLock mu(this, *Locks::thread_suspend_count_lock_);
-    if (!ReadFlag(ThreadFlag::kActiveSuspendBarrier)) {
+    if (!ReadFlag(ThreadFlag::kActiveSuspendBarrier, std::memory_order_relaxed)) {
       // Quick exit test: The barriers have already been claimed - this is possible as there may
       // be a race to claim and it doesn't matter who wins.  All of the callers of this function
       // (except SuspendAllInternal) will first test the kActiveSuspendBarrier flag without the
@@ -1604,7 +1613,8 @@ void Thread::RunEmptyCheckpoint() {
   // Note: Empty checkpoint does not access the thread's stack,
   // so we do not need to check for the flip function.
   DCHECK_EQ(Thread::Current(), this);
-  AtomicClearFlag(ThreadFlag::kEmptyCheckpointRequest);
+  // See mutator_gc_coord.md and b/382722942 for memory ordering discussion.
+  AtomicClearFlag(ThreadFlag::kEmptyCheckpointRequest, std::memory_order_release);
   Runtime::Current()->GetThreadList()->EmptyCheckpointBarrier()->Pass(this);
 }
 
@@ -1626,7 +1636,7 @@ bool Thread::RequestCheckpoint(Closure* function) {
   } else {
     checkpoint_overflow_.push_back(function);
   }
-  DCHECK(ReadFlag(ThreadFlag::kCheckpointRequest));
+  DCHECK(ReadFlag(ThreadFlag::kCheckpointRequest, std::memory_order_relaxed));
   TriggerSuspend();
   return true;
 }
@@ -1790,7 +1800,8 @@ bool Thread::RequestSynchronousCheckpoint(Closure* function, ThreadState wait_st
     // Since we're runnable, and kPendingFlipFunction is set with all threads suspended, it
     // cannot be set again here. Thus kRunningFlipFunction is either already set after the
     // EnsureFlipFunctionStarted call, or will not be set before we call Run().
-    if (ReadFlag(ThreadFlag::kRunningFlipFunction)) {
+    // See mutator_gc_coord.md for a discussion of memory ordering for thread flags.
+    if (ReadFlag(ThreadFlag::kRunningFlipFunction, std::memory_order_acquire)) {
       WaitForFlipFunction(self);
     }
     function->Run(this);
@@ -1831,7 +1842,8 @@ bool Thread::EnsureFlipFunctionStarted(Thread* self,
   DCHECK(self == Current());
   bool check_exited = (tef != nullptr);
   // Check that the thread can't unexpectedly exit while we are running.
-  DCHECK(self == target || check_exited || target->ReadFlag(ThreadFlag::kSuspendRequest) ||
+  DCHECK(self == target || check_exited ||
+         target->ReadFlag(ThreadFlag::kSuspendRequest, std::memory_order_relaxed) ||
          Locks::thread_list_lock_->IsExclusiveHeld(self))
       << *target;
   bool become_runnable;
@@ -1857,6 +1869,8 @@ bool Thread::EnsureFlipFunctionStarted(Thread* self,
   target->VerifyState();
   if (old_state_and_flags.GetValue() == 0) {
     become_runnable = false;
+    // Memory_order_relaxed is OK here, since we re-check with memory_order_acquire below before
+    // acting on a pending flip function.
     old_state_and_flags = target->GetStateAndFlags(std::memory_order_relaxed);
   } else {
     become_runnable = true;
@@ -1868,8 +1882,12 @@ bool Thread::EnsureFlipFunctionStarted(Thread* self,
   while (true) {
     DCHECK(!check_exited || (Locks::thread_list_lock_->IsExclusiveHeld(self) && !tef->HasExited()));
     if (!old_state_and_flags.IsFlagSet(ThreadFlag::kPendingFlipFunction)) {
+      // Re-read kRunningFlipFunction flag with acquire ordering to ensure that if we claim
+      // flip function has run then its execution happened-before our return.
+      bool running_flip =
+          target->ReadFlag(ThreadFlag::kRunningFlipFunction, std::memory_order_acquire);
       maybe_release();
-      set_finished(!old_state_and_flags.IsFlagSet(ThreadFlag::kRunningFlipFunction));
+      set_finished(!running_flip);
       return false;
     }
     DCHECK(!old_state_and_flags.IsFlagSet(ThreadFlag::kRunningFlipFunction));
@@ -1900,7 +1918,8 @@ bool Thread::EnsureFlipFunctionStarted(Thread* self,
       // Let caller retry.
       return false;
     }
-    old_state_and_flags = target->GetStateAndFlags(std::memory_order_acquire);
+    // Again, we re-read with memory_order_acquire before acting on the flags.
+    old_state_and_flags = target->GetStateAndFlags(std::memory_order_relaxed);
   }
   // Unreachable.
 }
@@ -1908,14 +1927,14 @@ bool Thread::EnsureFlipFunctionStarted(Thread* self,
 void Thread::RunFlipFunction(Thread* self) {
   // This function is called either by the thread running `ThreadList::FlipThreadRoots()` or when
   // a thread becomes runnable, after we've successfully set the kRunningFlipFunction ThreadFlag.
-  DCHECK(ReadFlag(ThreadFlag::kRunningFlipFunction));
+  DCHECK(ReadFlag(ThreadFlag::kRunningFlipFunction, std::memory_order_relaxed));
 
   Closure* flip_function = GetFlipFunction();
   tlsPtr_.flip_function.store(nullptr, std::memory_order_relaxed);
   DCHECK(flip_function != nullptr);
   VerifyState();
   flip_function->Run(this);
-  DCHECK(!ReadFlag(ThreadFlag::kPendingFlipFunction));
+  DCHECK(!ReadFlag(ThreadFlag::kPendingFlipFunction, std::memory_order_relaxed));
   VerifyState();
   AtomicClearFlag(ThreadFlag::kRunningFlipFunction, std::memory_order_release);
   // From here on this thread may go away, and it is no longer safe to access.
@@ -1933,12 +1952,12 @@ void Thread::WaitForFlipFunction(Thread* self) const {
   // Repeat the check after waiting to guard against spurious wakeups (and because
   // we share the `thread_suspend_count_lock_` and `resume_cond_` with other code).
   // Check that the thread can't unexpectedly exit while we are running.
-  DCHECK(self == this || ReadFlag(ThreadFlag::kSuspendRequest) ||
+  DCHECK(self == this || ReadFlag(ThreadFlag::kSuspendRequest, std::memory_order_relaxed) ||
          Locks::thread_list_lock_->IsExclusiveHeld(self));
   MutexLock mu(self, *Locks::thread_suspend_count_lock_);
   while (true) {
-    StateAndFlags old_state_and_flags = GetStateAndFlags(std::memory_order_acquire);
-    if (!old_state_and_flags.IsFlagSet(ThreadFlag::kRunningFlipFunction)) {
+    // See mutator_gc_coord.md for a discussion of memory ordering for thread flags.
+    if (!ReadFlag(ThreadFlag::kRunningFlipFunction, std::memory_order_acquire)) {
       return;
     }
     // We sometimes hold mutator lock here. OK since the flip function must complete quickly.
@@ -1958,9 +1977,10 @@ void Thread::WaitForFlipFunctionTestingExited(Thread* self, ThreadExitFlag* tef)
   // complicated locking dance.
   MutexLock mu(self, *Locks::thread_suspend_count_lock_);
   while (true) {
-    StateAndFlags old_state_and_flags = GetStateAndFlags(std::memory_order_acquire);
+    // See mutator_gc_coord.md for a discussion of memory ordering for thread flags.
+    bool running_flip = ReadFlag(ThreadFlag::kRunningFlipFunction, std::memory_order_acquire);
     Locks::thread_list_lock_->Unlock(self);  // So we can wait or return.
-    if (!old_state_and_flags.IsFlagSet(ThreadFlag::kRunningFlipFunction)) {
+    if (!running_flip) {
       return;
     }
     resume_cond_->WaitHoldingLocks(self);
@@ -1976,7 +1996,7 @@ void Thread::WaitForFlipFunctionTestingExited(Thread* self, ThreadExitFlag* tef)
 
 void Thread::FullSuspendCheck(bool implicit) {
   ScopedTrace trace(__FUNCTION__);
-  DCHECK(!ReadFlag(ThreadFlag::kSuspensionImmune));
+  DCHECK(!ReadFlag(ThreadFlag::kSuspensionImmune, std::memory_order_relaxed));
   DCHECK(this == Thread::Current());
   VLOG(threads) << this << " self-suspending";
   // Make thread appear suspended to other threads, release mutator_lock_.
@@ -2419,7 +2439,7 @@ Thread::DumpOrder Thread::DumpStack(std::ostream& os,
                                /*check_suspended=*/ !force_dump_stack,
                                /*dump_locks=*/ !force_dump_stack);
     Runtime* runtime = Runtime::Current();
-    std::optional<uint64_t> start = runtime != nullptr ? runtime->SiqQuitNanoTime() : std::nullopt;
+    std::optional<uint64_t> start = runtime != nullptr ? runtime->SigQuitNanoTime() : std::nullopt;
     if (start.has_value()) {
       os << "DumpLatencyMs: " << static_cast<float>(nanotime - start.value()) / 1000000.0 << "\n";
     }
@@ -2707,9 +2727,9 @@ Thread::~Thread() {
     tlsPtr_.jni_env = nullptr;
   }
   CHECK_NE(GetState(), ThreadState::kRunnable);
-  CHECK(!ReadFlag(ThreadFlag::kCheckpointRequest));
-  CHECK(!ReadFlag(ThreadFlag::kEmptyCheckpointRequest));
-  CHECK(!ReadFlag(ThreadFlag::kSuspensionImmune));
+  CHECK(!ReadFlag(ThreadFlag::kCheckpointRequest, std::memory_order_relaxed));
+  CHECK(!ReadFlag(ThreadFlag::kEmptyCheckpointRequest, std::memory_order_relaxed));
+  CHECK(!ReadFlag(ThreadFlag::kSuspensionImmune, std::memory_order_relaxed));
   CHECK(tlsPtr_.checkpoint_function == nullptr);
   CHECK_EQ(checkpoint_overflow_.size(), 0u);
   // A pending flip function request is OK. FlipThreadRoots will have been notified that we
@@ -3317,8 +3337,7 @@ jobjectArray Thread::InternalStackTraceToStackTraceElementArray(
     return nullptr;
   }
 
-  dex::ProtoIndex proto_idx =
-      method->GetDexFile()->GetIndexForProtoId(interface_method->GetPrototype());
+  dex::ProtoIndex proto_idx = interface_method->GetProtoIndex();
   Handle<mirror::MethodType> method_type_object(hs.NewHandle<mirror::MethodType>(
       class_linker->ResolveMethodType(soa.Self(), proto_idx, interface_method)));
   if (method_type_object == nullptr) {
@@ -4818,11 +4837,11 @@ mirror::Object* Thread::GetPeerFromOtherThread() {
   // mutator lock in exclusive mode, and we should not have a pending flip function.
   if (kIsDebugBuild && Locks::thread_list_lock_->IsExclusiveHeld(self)) {
     Locks::mutator_lock_->AssertExclusiveHeld(self);
-    CHECK(!ReadFlag(ThreadFlag::kPendingFlipFunction));
+    CHECK(!ReadFlag(ThreadFlag::kPendingFlipFunction, std::memory_order_relaxed));
   }
   // Ensure that opeer is not obsolete.
   EnsureFlipFunctionStarted(self, this);
-  if (ReadFlag(ThreadFlag::kRunningFlipFunction)) {
+  if (ReadFlag(ThreadFlag::kRunningFlipFunction, std::memory_order_acquire)) {
     // Does not release mutator lock. Hence no new flip requests can be issued.
     WaitForFlipFunction(self);
   }
@@ -4833,7 +4852,8 @@ mirror::Object* Thread::LockedGetPeerFromOtherThread(ThreadExitFlag* tef) {
   DCHECK(tlsPtr_.jpeer == nullptr);
   Thread* self = Thread::Current();
   Locks::thread_list_lock_->AssertHeld(self);
-  if (ReadFlag(ThreadFlag::kPendingFlipFunction)) {
+  // memory_order_relaxed is OK here, because we recheck it later with acquire order.
+  if (ReadFlag(ThreadFlag::kPendingFlipFunction, std::memory_order_relaxed)) {
     // It is unsafe to call EnsureFlipFunctionStarted with thread_list_lock_. Thus we temporarily
     // release it, taking care to handle the case in which "this" thread disapppears while we no
     // longer hold it.
@@ -4844,7 +4864,7 @@ mirror::Object* Thread::LockedGetPeerFromOtherThread(ThreadExitFlag* tef) {
       return nullptr;
     }
   }
-  if (ReadFlag(ThreadFlag::kRunningFlipFunction)) {
+  if (ReadFlag(ThreadFlag::kRunningFlipFunction, std::memory_order_acquire)) {
     // Does not release mutator lock. Hence no new flip requests can be issued.
     WaitForFlipFunction(self);
   }
diff --git a/runtime/thread.h b/runtime/thread.h
index 8ce9854212..4a675d4c99 100644
--- a/runtime/thread.h
+++ b/runtime/thread.h
@@ -105,6 +105,7 @@ class StackedShadowFrameRecord;
 class Thread;
 class ThreadList;
 enum VisitRootFlags : uint8_t;
+enum class LowOverheadTraceType;
 
 // A piece of data that can be held in the CustomTls. The destructor will be called during thread
 // shutdown. The thread the destructor is called on is not necessarily the same thread it was stored
@@ -127,6 +128,7 @@ enum class ThreadFlag : uint32_t {
   kSuspendRequest = 1u << 0,
 
   // Request that the thread do some checkpoint work and then continue.
+  // Only modified while holding thread_suspend_count_lock_ .
   kCheckpointRequest = 1u << 1,
 
   // Request that the thread do empty checkpoint and then continue.
@@ -158,7 +160,7 @@ enum class ThreadFlag : uint32_t {
   // in any case not check for such requests; other clients of SuspendAll might.
   // Prevents a situation in which we are asked to suspend just before we suspend all
   // other threads, and then notice the suspension request and suspend ourselves,
-  // leading to deadlock. Guarded by suspend_count_lock_ .
+  // leading to deadlock. Guarded by thread_suspend_count_lock_ .
   // Should not ever be set when we try to transition to kRunnable.
   // TODO(b/296639267): Generalize use to prevent SuspendAll from blocking
   // in-progress GC.
@@ -641,6 +643,9 @@ class EXPORT Thread {
   // Returns the thread-specific CPU-time clock in microseconds or -1 if unavailable.
   uint64_t GetCpuMicroTime() const;
 
+  // Returns the thread-specific CPU-time clock in nanoseconds or -1 if unavailable.
+  uint64_t GetCpuNanoTime() const;
+
   mirror::Object* GetPeer() const REQUIRES_SHARED(Locks::mutator_lock_) {
     DCHECK(Thread::Current() == this) << "Use GetPeerFromOtherThread instead";
     CHECK(tlsPtr_.jpeer == nullptr);
@@ -1426,7 +1431,7 @@ class EXPORT Thread {
     }
   }
 
-  void UpdateTlsLowOverheadTraceEntrypoints(bool enable);
+  void UpdateTlsLowOverheadTraceEntrypoints(LowOverheadTraceType type);
 
   uint64_t GetTraceClockBase() const {
     return tls64_.trace_clock_base;
@@ -1451,8 +1456,10 @@ class EXPORT Thread {
   // Undo the effect of the previous call. Again only invoked by the thread itself.
   void AllowPreMonitorMutexes();
 
-  bool ReadFlag(ThreadFlag flag) const {
-    return GetStateAndFlags(std::memory_order_relaxed).IsFlagSet(flag);
+  // Read a flag with the given memory order. See mutator_gc_coord.md for memory ordering
+  // considerations.
+  bool ReadFlag(ThreadFlag flag, std::memory_order order) const {
+    return GetStateAndFlags(order).IsFlagSet(flag);
   }
 
   void AtomicSetFlag(ThreadFlag flag, std::memory_order order = std::memory_order_seq_cst) {
diff --git a/runtime/thread_list.cc b/runtime/thread_list.cc
index 3665b34cd5..60e3d727f3 100644
--- a/runtime/thread_list.cc
+++ b/runtime/thread_list.cc
@@ -28,6 +28,7 @@
 #include <tuple>
 #include <vector>
 
+#include "android-base/properties.h"
 #include "android-base/stringprintf.h"
 #include "art_field-inl.h"
 #include "base/aborting.h"
@@ -253,13 +254,16 @@ void ThreadList::Dump(std::ostream& os, bool dump_native_stack) {
     os << "DALVIK THREADS (" << list_.size() << "):\n";
   }
   if (self != nullptr) {
+    // Dump() can be called in any mutator lock state.
+    bool mutator_lock_held = Locks::mutator_lock_->IsSharedHeld(self);
     DumpCheckpoint checkpoint(dump_native_stack);
     // Acquire mutator lock separately for each thread, to avoid long runnable code sequence
     // without suspend checks.
-    size_t threads_running_checkpoint = RunCheckpoint(&checkpoint,
-                                                      nullptr,
-                                                      true,
-                                                      /* acquire_mutator_lock= */ true);
+    size_t threads_running_checkpoint =
+        RunCheckpoint(&checkpoint,
+                      nullptr,
+                      true,
+                      /* acquire_mutator_lock= */ !mutator_lock_held);
     if (threads_running_checkpoint != 0) {
       checkpoint.WaitForThreadsToRunThroughCheckpoint(threads_running_checkpoint);
     }
@@ -557,7 +561,7 @@ void ThreadList::RunEmptyCheckpoint() {
                 std::find(runnable_thread_ids.begin(), runnable_thread_ids.end(), tid) !=
                 runnable_thread_ids.end();
             if (is_in_runnable_thread_ids &&
-                thread->ReadFlag(ThreadFlag::kEmptyCheckpointRequest)) {
+                thread->ReadFlag(ThreadFlag::kEmptyCheckpointRequest, std::memory_order_relaxed)) {
               // Found a runnable thread that hasn't responded to the empty checkpoint request.
               // Assume it's stuck and safe to dump its stack.
               thread->Dump(LOG_STREAM(FATAL_WITHOUT_ABORT),
@@ -717,7 +721,7 @@ static constexpr unsigned kSuspendBarrierIters = kShortSuspendTimeouts ? 5 : 20;
 
 #if ART_USE_FUTEXES
 
-// Returns true if it timed out.
+// Returns true if it timed out. Times out after timeout_ns/kSuspendBarrierIters nsecs
 static bool WaitOnceForSuspendBarrier(AtomicInteger* barrier,
                                       int32_t cur_val,
                                       uint64_t timeout_ns) {
@@ -762,13 +766,21 @@ static bool WaitOnceForSuspendBarrier(AtomicInteger* barrier,
 std::optional<std::string> ThreadList::WaitForSuspendBarrier(AtomicInteger* barrier,
                                                              pid_t t,
                                                              int attempt_of_4) {
-  // Only fail after kIter timeouts, to make us robust against app freezing.
 #if ART_USE_FUTEXES
   const uint64_t start_time = NanoTime();
 #endif
   uint64_t timeout_ns =
       attempt_of_4 == 0 ? thread_suspend_timeout_ns_ : thread_suspend_timeout_ns_ / 4;
-
+  static bool is_user_build = (android::base::GetProperty("ro.build.type", "") == "user");
+  // Significantly increase timeouts in user builds, since they result in crashes.
+  // Many of these are likely to turn into ANRs, which are less informative for the developer, but
+  // friendlier to the user. We do not completely suppress timeouts, so that we avoid invisible
+  // problems for cases not covered by ANR detection, e.g. a problem in a clean-up daemon.
+  if (is_user_build) {
+    static constexpr int USER_MULTIPLIER = 2;  // Start out small, perhaps increase later if we
+                                               // still have an issue?
+    timeout_ns *= USER_MULTIPLIER;
+  }
   uint64_t avg_wait_multiplier = 1;
   uint64_t wait_multiplier = 1;
   if (attempt_of_4 != 1) {
@@ -801,10 +813,24 @@ std::optional<std::string> ThreadList::WaitForSuspendBarrier(AtomicInteger* barr
     return std::nullopt;
   }
 
+  // Extra timeout to compensate for concurrent thread dumps, so that we are less likely to time
+  // out during ANR dumps.
+  uint64_t dump_adjustment_ns = 0;
+  // Total timeout increment if we see a concurrent thread dump. Distributed evenly across
+  // remaining iterations.
+  static constexpr uint64_t kDumpWaitNSecs = 30'000'000'000ull;  // 30 seconds
+  // Replacement timeout if thread is stopped for tracing, probably by a debugger.
+  static constexpr uint64_t kTracingWaitNSecs = 7'200'000'000'000ull;  // wait a bit < 2 hours;
+
   // Long wait; gather information in case of timeout.
   std::string sampled_state = collect_state ? GetOsThreadStatQuick(t) : "";
+  if (collect_state && GetStateFromStatString(sampled_state) == 't') {
+    LOG(WARNING) << "Thread suspension nearly timed out due to Tracing stop (debugger attached?)";
+    timeout_ns = kTracingWaitNSecs;
+  }
+  // Only fail after kSuspendBarrierIters timeouts, to make us robust against app freezing.
   while (i < kSuspendBarrierIters) {
-    if (WaitOnceForSuspendBarrier(barrier, cur_val, timeout_ns)) {
+    if (WaitOnceForSuspendBarrier(barrier, cur_val, timeout_ns + dump_adjustment_ns)) {
       ++i;
 #if ART_USE_FUTEXES
       if (!kShortSuspendTimeouts) {
@@ -817,6 +843,15 @@ std::optional<std::string> ThreadList::WaitForSuspendBarrier(AtomicInteger* barr
       DCHECK_EQ(cur_val, 0);
       return std::nullopt;
     }
+    std::optional<uint64_t> last_sigquit_nanotime = Runtime::Current()->SigQuitNanoTime();
+    if (last_sigquit_nanotime.has_value() && i < kSuspendBarrierIters) {
+      // Adjust dump_adjustment_ns to reflect the number of iterations we have left and how long
+      // ago we started dumping threads.
+      uint64_t new_unscaled_adj = kDumpWaitNSecs + last_sigquit_nanotime.value() - NanoTime();
+      // Scale by the fraction of iterations still remaining.
+      dump_adjustment_ns = new_unscaled_adj * kSuspendBarrierIters / kSuspendBarrierIters - i;
+    }
+    // Keep the old dump_adjustment_ns if SigQuitNanoTime() was cleared.
   }
   uint64_t final_wait_time = NanoTime() - start_time;
   uint64_t total_wait_time = attempt_of_4 == 0 ?
@@ -877,8 +912,8 @@ void ThreadList::SuspendAll(const char* cause, bool long_suspend) {
   }
 
   // SuspendAllInternal blocks if we are in the middle of a flip.
-  DCHECK(!self->ReadFlag(ThreadFlag::kPendingFlipFunction));
-  DCHECK(!self->ReadFlag(ThreadFlag::kRunningFlipFunction));
+  DCHECK(!self->ReadFlag(ThreadFlag::kPendingFlipFunction, std::memory_order_relaxed));
+  DCHECK(!self->ReadFlag(ThreadFlag::kRunningFlipFunction, std::memory_order_relaxed));
 
   ATraceBegin((std::string("Mutator threads suspended for ") + cause).c_str());
 
@@ -1657,6 +1692,15 @@ void ThreadList::SweepInterpreterCaches(IsMarkedVisitor* visitor) const {
   }
 }
 
+void ThreadList::ClearInterpreterCaches() const {
+  Thread* self = Thread::Current();
+  Locks::mutator_lock_->AssertExclusiveHeld(self);
+  MutexLock mu(self, *Locks::thread_list_lock_);
+  for (const auto& thread : list_) {
+    thread->GetInterpreterCache()->Clear(thread);
+  }
+}
+
 uint32_t ThreadList::AllocThreadId(Thread* self) {
   MutexLock mu(self, *Locks::allocated_thread_ids_lock_);
   for (size_t i = 0; i < allocated_ids_.size(); ++i) {
diff --git a/runtime/thread_list.h b/runtime/thread_list.h
index 1cf28989fb..a1f823572a 100644
--- a/runtime/thread_list.h
+++ b/runtime/thread_list.h
@@ -62,11 +62,14 @@ class ThreadList {
 
   void ShutDown();
 
-  void DumpForSigQuit(std::ostream& os)
-      REQUIRES(!Locks::thread_list_lock_, !Locks::mutator_lock_);
-  // For thread suspend timeout dumps.
+  // Dump stacks for all threads.
+  // This version includes some additional data.
+  void DumpForSigQuit(std::ostream& os) REQUIRES(!Locks::thread_list_lock_, !Locks::mutator_lock_);
+
+  // This version is less jank-prone if mutator_lock_ is not held.
   EXPORT void Dump(std::ostream& os, bool dump_native_stack = true)
       REQUIRES(!Locks::thread_list_lock_, !Locks::thread_suspend_count_lock_);
+
   pid_t GetLockOwner();  // For SignalCatcher.
 
   // Thread suspension support.
@@ -138,6 +141,13 @@ class ThreadList {
   // threads must act on that. It is possible that on return there will be threads which have not,
   // and will not, run the checkpoint_function, and neither have/will any of their ancestors.
   //
+  // We guarantee that if a thread calls RunCheckpoint() then, if at point X RunCheckpoint() has
+  // returned, and all checkpoints have been properly observed to have completed (usually via a
+  // barrier), then every thread has executed a code sequence S during which it remained in a
+  // suspended state, such that the call to `RunCheckpoint` happens-before the end of S, and the
+  // beginning of S happened-before X.  Thus after a RunCheckpoint() call, no preexisting
+  // thread can still be relying on global information it caches between suspend points.
+  //
   // TODO: Is it possible to simplify mutator_lock handling here? Should this wait for completion?
   EXPORT size_t RunCheckpoint(Closure* checkpoint_function,
                               Closure* callback = nullptr,
@@ -157,6 +167,10 @@ class ThreadList {
   // in-flight mutator heap access (eg. a read barrier.) Runnable threads will respond by
   // decrementing the empty checkpoint barrier count. This works even when the weak ref access is
   // disabled. Only one concurrent use is currently supported.
+  // TODO(b/382722942): This is intended to guarantee the analogous memory ordering property to
+  // RunCheckpoint(). It over-optimizes by always avoiding thread suspension and hence does not in
+  // fact guarantee this. (See the discussion in `mutator_gc_coord.md`.) Fix this by implementing
+  // this with RunCheckpoint() instead.
   void RunEmptyCheckpoint()
       REQUIRES(!Locks::thread_list_lock_, !Locks::thread_suspend_count_lock_);
 
@@ -213,6 +227,7 @@ class ThreadList {
   EXPORT void SweepInterpreterCaches(IsMarkedVisitor* visitor) const
       REQUIRES(Locks::mutator_lock_, !Locks::thread_list_lock_);
 
+  void ClearInterpreterCaches() const REQUIRES(Locks::mutator_lock_, !Locks::thread_list_lock_);
   // Return a copy of the thread list.
   std::list<Thread*> GetList() REQUIRES(Locks::thread_list_lock_) {
     return list_;
diff --git a/runtime/trace.cc b/runtime/trace.cc
index 0fc41b6886..e003c6b3a5 100644
--- a/runtime/trace.cc
+++ b/runtime/trace.cc
@@ -32,8 +32,8 @@
 #include "base/unix_file/fd_file.h"
 #include "base/utils.h"
 #include "class_linker.h"
-#include "common_throws.h"
 #include "com_android_art_flags.h"
+#include "common_throws.h"
 #include "debugger.h"
 #include "dex/descriptors_names.h"
 #include "dex/dex_file-inl.h"
@@ -51,6 +51,7 @@
 #include "stack.h"
 #include "thread.h"
 #include "thread_list.h"
+#include "trace_common.h"
 #include "trace_profile.h"
 
 namespace art_flags = com::android::art::flags;
@@ -60,8 +61,8 @@ namespace art HIDDEN {
 struct MethodTraceRecord {
   ArtMethod* method;
   TraceAction action;
-  uint32_t wall_clock_time;
-  uint32_t thread_cpu_time;
+  uint64_t wall_clock_time;
+  uint64_t thread_cpu_time;
 };
 
 using android::base::StringPrintf;
@@ -94,6 +95,8 @@ static constexpr size_t kScalingFactorEncodedEntries = 6;
 // The key identifying the tracer to update instrumentation.
 static constexpr const char* kTracerInstrumentationKey = "Tracer";
 
+double TimestampCounter::tsc_to_nanosec_scaling_factor = -1;
+
 Trace* Trace::the_trace_ = nullptr;
 pthread_t Trace::sampling_pthread_ = 0U;
 std::unique_ptr<std::vector<ArtMethod*>> Trace::temp_stack_trace_;
@@ -104,131 +107,6 @@ static TraceAction DecodeTraceAction(uint32_t tmid) {
 }
 
 namespace {
-// Scaling factor to convert timestamp counter into wall clock time reported in micro seconds.
-// This is initialized at the start of tracing using the timestamp counter update frequency.
-// See InitializeTimestampCounters for more details.
-double tsc_to_microsec_scaling_factor = -1.0;
-
-uint64_t GetTimestamp() {
-  uint64_t t = 0;
-#if defined(__arm__)
-  // On ARM 32 bit, we don't always have access to the timestamp counters from user space. There is
-  // no easy way to check if it is safe to read the timestamp counters. There is HWCAP_EVTSTRM which
-  // is set when generic timer is available but not necessarily from the user space. Kernel disables
-  // access to generic timer when there are known problems on the target CPUs. Sometimes access is
-  // disabled only for 32-bit processes even when 64-bit processes can accesses the timer from user
-  // space. These are not reflected in the HWCAP_EVTSTRM capability.So just fallback to
-  // clock_gettime on these processes. See b/289178149 for more discussion.
-  t = MicroTime();
-#elif defined(__aarch64__)
-  // See Arm Architecture Registers  Armv8 section System Registers
-  asm volatile("mrs %0, cntvct_el0" : "=r"(t));
-#elif defined(__i386__) || defined(__x86_64__)
-  // rdtsc returns two 32-bit values in rax and rdx even on 64-bit architectures.
-  unsigned int lo, hi;
-  asm volatile("rdtsc" : "=a"(lo), "=d"(hi));
-  t = (static_cast<uint64_t>(hi) << 32) | lo;
-#elif defined(__riscv)
-  asm volatile("rdtime %0" : "=r"(t));
-#else
-  t = MicroTime();
-#endif
-  return t;
-}
-
-#if defined(__i386__) || defined(__x86_64__) || defined(__aarch64__)
-// Here we compute the scaling factor by sleeping for a millisecond. Alternatively, we could
-// generate raw timestamp counter and also time using clock_gettime at the start and the end of the
-// trace. We can compute the frequency of timestamp counter upadtes in the post processing step
-// using these two samples. However, that would require a change in Android Studio which is the main
-// consumer of these profiles. For now, just compute the frequency of tsc updates here.
-double computeScalingFactor() {
-  uint64_t start = MicroTime();
-  uint64_t start_tsc = GetTimestamp();
-  // Sleep for one millisecond.
-  usleep(1000);
-  uint64_t diff_tsc = GetTimestamp() - start_tsc;
-  uint64_t diff_time = MicroTime() - start;
-  double scaling_factor = static_cast<double>(diff_time) / diff_tsc;
-  DCHECK(scaling_factor > 0.0) << scaling_factor;
-  return scaling_factor;
-}
-#endif
-
-#if defined(__i386__) || defined(__x86_64__)
-double GetScalingFactorForX86() {
-  uint32_t eax, ebx, ecx;
-  asm volatile("cpuid" : "=a"(eax), "=b"(ebx), "=c"(ecx) : "a"(0x0), "c"(0));
-  if (eax < 0x15) {
-    // There is no 15H - Timestamp counter and core crystal clock information
-    // leaf. Just compute the frequency.
-    return computeScalingFactor();
-  }
-
-  // From Intel architecture-instruction-set-extensions-programming-reference:
-  // EBX[31:0]/EAX[31:0] indicates the ratio of the TSC frequency and the
-  // core crystal clock frequency.
-  // If EBX[31:0] is 0, the TSC and "core crystal clock" ratio is not enumerated.
-  // If ECX is 0, the nominal core crystal clock frequency is not enumerated.
-  // "TSC frequency" = "core crystal clock frequency" * EBX/EAX.
-  // The core crystal clock may differ from the reference clock, bus clock, or core clock
-  // frequencies.
-  // EAX Bits 31 - 00: An unsigned integer which is the denominator of the
-  //                   TSC/"core crystal clock" ratio.
-  // EBX Bits 31 - 00: An unsigned integer which is the numerator of the
-  //                   TSC/"core crystal clock" ratio.
-  // ECX Bits 31 - 00: An unsigned integer which is the nominal frequency of the core
-  //                   crystal clock in Hz.
-  // EDX Bits 31 - 00: Reserved = 0.
-  asm volatile("cpuid" : "=a"(eax), "=b"(ebx), "=c"(ecx) : "a"(0x15), "c"(0));
-  if (ebx == 0 || ecx == 0) {
-    return computeScalingFactor();
-  }
-  double coreCrystalFreq = ecx;
-  // frequency = coreCrystalFreq * (ebx / eax)
-  // scaling_factor = seconds_to_microseconds / frequency
-  //                = seconds_to_microseconds * eax / (coreCrystalFreq * ebx)
-  double seconds_to_microseconds = 1000 * 1000;
-  double scaling_factor = (seconds_to_microseconds * eax) / (coreCrystalFreq * ebx);
-  return scaling_factor;
-}
-#endif
-
-void InitializeTimestampCounters() {
-  // It is sufficient to initialize this once for the entire execution. Just return if it is
-  // already initialized.
-  if (tsc_to_microsec_scaling_factor > 0.0) {
-    return;
-  }
-
-#if defined(__arm__)
-  // On ARM 32 bit, we don't always have access to the timestamp counters from
-  // user space. Seem comment in GetTimestamp for more details.
-  tsc_to_microsec_scaling_factor = 1.0;
-#elif defined(__aarch64__)
-  double seconds_to_microseconds = 1000 * 1000;
-  uint64_t freq = 0;
-  // See Arm Architecture Registers  Armv8 section System Registers
-  asm volatile("mrs %0,  cntfrq_el0" : "=r"(freq));
-  if (freq == 0) {
-    // It is expected that cntfrq_el0 is correctly setup during system initialization but some
-    // devices don't do this. In such cases fall back to computing the frequency. See b/315139000.
-    tsc_to_microsec_scaling_factor = computeScalingFactor();
-  } else {
-    tsc_to_microsec_scaling_factor = seconds_to_microseconds / static_cast<double>(freq);
-  }
-#elif defined(__i386__) || defined(__x86_64__)
-  tsc_to_microsec_scaling_factor = GetScalingFactorForX86();
-#else
-  tsc_to_microsec_scaling_factor = 1.0;
-#endif
-}
-
-ALWAYS_INLINE uint64_t GetMicroTime(uint64_t counter) {
-  DCHECK(tsc_to_microsec_scaling_factor > 0.0) << tsc_to_microsec_scaling_factor;
-  return tsc_to_microsec_scaling_factor * counter;
-}
-
 TraceClockSource GetClockSourceFromFlags(int flags) {
   bool need_wall = flags & Trace::TraceFlag::kTraceClockSourceWallClock;
   bool need_thread_cpu = flags & Trace::TraceFlag::kTraceClockSourceThreadCpu;
@@ -378,8 +256,8 @@ static uint16_t GetRecordSize(TraceClockSource clock_source, int version) {
     return (clock_source == TraceClockSource::kDual) ? kTraceRecordSizeDualClock :
                                                        kTraceRecordSizeSingleClock;
   } else {
-    return (clock_source == TraceClockSource::kDual) ? kTraceRecordSizeDualClockV2 :
-                                                       kTraceRecordSizeSingleClockV2;
+    return (clock_source == TraceClockSource::kDual) ? kMaxTraceRecordSizeDualClockV2
+                                                     : kMaxTraceRecordSizeSingleClockV2;
   }
 }
 
@@ -402,7 +280,7 @@ bool UseFastTraceListeners(TraceClockSource clock_source) {
   bool is_fast_trace = !UseThreadCpuClock(clock_source);
 #if defined(__arm__)
   // On ARM 32 bit, we don't always have access to the timestamp counters from
-  // user space. See comment in GetTimestamp for more details.
+  // user space. See comment in TimestampCounter::GetTimestamp for more details.
   is_fast_trace = false;
 #endif
   return is_fast_trace;
@@ -410,19 +288,20 @@ bool UseFastTraceListeners(TraceClockSource clock_source) {
 
 void Trace::MeasureClockOverhead() {
   if (UseThreadCpuClock(clock_source_)) {
-    Thread::Current()->GetCpuMicroTime();
+    Thread::Current()->GetCpuNanoTime();
   }
   if (UseWallClock(clock_source_)) {
-    GetTimestamp();
+    TimestampCounter::GetTimestamp();
   }
 }
 
 // Compute an average time taken to measure clocks.
-uint32_t Trace::GetClockOverheadNanoSeconds() {
+uint64_t Trace::GetClockOverheadNanoSeconds() {
   Thread* self = Thread::Current();
-  uint64_t start = self->GetCpuMicroTime();
+  uint64_t start = self->GetCpuNanoTime();
 
-  for (int i = 4000; i > 0; i--) {
+  const uint64_t numIter = 4000;
+  for (int i = numIter; i > 0; i--) {
     MeasureClockOverhead();
     MeasureClockOverhead();
     MeasureClockOverhead();
@@ -433,8 +312,8 @@ uint32_t Trace::GetClockOverheadNanoSeconds() {
     MeasureClockOverhead();
   }
 
-  uint64_t elapsed_us = self->GetCpuMicroTime() - start;
-  return static_cast<uint32_t>(elapsed_us / 32);
+  uint64_t elapsed_ns = self->GetCpuNanoTime() - start;
+  return elapsed_ns / (numIter * 8);
 }
 
 static void GetSample(Thread* thread, void* arg) REQUIRES_SHARED(Locks::mutator_lock_) {
@@ -469,7 +348,7 @@ void Trace::CompareAndUpdateStackTrace(Thread* thread,
   // Update the thread's stack trace sample.
   thread->SetStackTraceSample(stack_trace);
   // Read timer clocks to use for all events in this trace.
-  uint32_t thread_clock_diff = 0;
+  uint64_t thread_clock_diff = 0;
   uint64_t timestamp_counter = 0;
   ReadClocks(thread, &thread_clock_diff, &timestamp_counter);
   if (old_stack_trace == nullptr) {
@@ -766,7 +645,7 @@ void Trace::Start(std::unique_ptr<File>&& trace_file_in,
 
   // Initialize the frequency of timestamp counter updates here. This is needed
   // to get wallclock time from timestamp counter values.
-  InitializeTimestampCounters();
+  TimestampCounter::InitializeTimestampCounters();
 
   Runtime* runtime = Runtime::Current();
 
@@ -838,14 +717,6 @@ void Trace::Start(std::unique_ptr<File>&& trace_file_in,
                                                          the_trace_,
                                                          /*needs_interpreter=*/false);
     }
-
-    if (art_flags::always_enable_profile_code()) {
-      // Reset the trace low overhead trace entry points to be a nop.
-      MutexLock thread_list_mutex(self, *Locks::thread_list_lock_);
-      for (Thread* thread : Runtime::Current()->GetThreadList()->GetList()) {
-        thread->UpdateTlsLowOverheadTraceEntrypoints(/*enable= */ false);
-      }
-    }
   }
 
   // Can't call this when holding the mutator lock.
@@ -925,10 +796,6 @@ void Trace::StopTracing(bool flush_entries) {
           the_trace->trace_writer_->FlushBuffer(
               thread, /* is_sync= */ false, /* free_buffer= */ true);
         }
-
-        if (art_flags::always_enable_profile_code()) {
-          thread->UpdateTlsLowOverheadTraceEntrypoints(/*enable= */ true);
-        }
       }
       the_trace_ = nullptr;
       sampling_pthread_ = 0U;
@@ -1030,14 +897,14 @@ TraceWriter::TraceWriter(File* trace_file,
                          size_t buffer_size,
                          int num_trace_buffers,
                          int trace_format_version,
-                         uint32_t clock_overhead_ns)
+                         uint64_t clock_overhead_ns)
     : trace_file_(trace_file),
       trace_output_mode_(output_mode),
       clock_source_(clock_source),
       buf_(new uint8_t[std::max(kMinBufSize, buffer_size)]()),
       buffer_size_(std::max(kMinBufSize, buffer_size)),
       trace_format_version_(trace_format_version),
-      start_time_(GetMicroTime(GetTimestamp())),
+      start_time_(TimestampCounter::GetNanoTime(TimestampCounter::GetTimestamp())),
       overflow_(false),
       num_records_(0),
       clock_overhead_ns_(clock_overhead_ns),
@@ -1053,7 +920,8 @@ TraceWriter::TraceWriter(File* trace_file,
   // We record monotonic time at the start of the trace, because Android Studio
   // fetches the monotonic timer from other places and matches these times to
   // construct a cpu profile. See b/318052824 for more context.
-  uint64_t start_time_monotonic = start_time_ + (MicroTime() - GetMicroTime(GetTimestamp()));
+  uint64_t start_time_monotonic =
+      start_time_ + (NanoTime() - TimestampCounter::GetNanoTime(TimestampCounter::GetTimestamp()));
   uint16_t trace_version = GetTraceVersion(clock_source_, trace_format_version_);
   if (output_mode == TraceOutputMode::kStreaming) {
     trace_version |= 0xF0U;
@@ -1065,7 +933,8 @@ TraceWriter::TraceWriter(File* trace_file,
     Append4LE(buf_.get(), kTraceMagicValue);
     Append2LE(buf_.get() + 4, trace_version);
     Append2LE(buf_.get() + 6, kTraceHeaderLength);
-    Append8LE(buf_.get() + 8, start_time_monotonic);
+    // Use microsecond precision for V1 format.
+    Append8LE(buf_.get() + 8, (start_time_monotonic / 1000));
     if (trace_version >= kTraceVersionDualClock) {
       uint16_t record_size = GetRecordSize(clock_source_, trace_format_version_);
       Append2LE(buf_.get() + 16, record_size);
@@ -1136,7 +1005,7 @@ Trace::Trace(File* trace_file,
 std::string TraceWriter::CreateSummary(int flags) {
   std::ostringstream os;
   // Compute elapsed time.
-  uint64_t elapsed = GetMicroTime(GetTimestamp()) - start_time_;
+  uint64_t elapsed = TimestampCounter::GetNanoTime(TimestampCounter::GetTimestamp()) - start_time_;
   os << StringPrintf("%cversion\n", kTraceTokenChar);
   os << StringPrintf("%d\n", GetTraceVersion(clock_source_, trace_format_version_));
   os << StringPrintf("data-file-overflow=%s\n", overflow_ ? "true" : "false");
@@ -1149,11 +1018,15 @@ std::string TraceWriter::CreateSummary(int flags) {
   } else {
     os << StringPrintf("clock=wall\n");
   }
-  os << StringPrintf("elapsed-time-usec=%" PRIu64 "\n", elapsed);
+  if (trace_format_version_ == Trace::kFormatV1) {
+    os << StringPrintf("elapsed-time-usec=%" PRIu64 "\n", elapsed / 1000);
+  } else {
+    os << StringPrintf("elapsed-time-nsec=%" PRIu64 "\n", elapsed);
+  }
   if (trace_output_mode_ != TraceOutputMode::kStreaming) {
     os << StringPrintf("num-method-calls=%zd\n", num_records_);
   }
-  os << StringPrintf("clock-call-overhead-nsec=%d\n", clock_overhead_ns_);
+  os << StringPrintf("clock-call-overhead-nsec=%" PRIu64 "\n", clock_overhead_ns_);
   os << StringPrintf("vm=art\n");
   os << StringPrintf("pid=%d\n", getpid());
   if ((flags & Trace::kTraceCountAllocs) != 0) {
@@ -1305,7 +1178,7 @@ void Trace::FieldWritten([[maybe_unused]] Thread* thread,
 }
 
 void Trace::MethodEntered(Thread* thread, ArtMethod* method) {
-  uint32_t thread_clock_diff = 0;
+  uint64_t thread_clock_diff = 0;
   uint64_t timestamp_counter = 0;
   ReadClocks(thread, &thread_clock_diff, &timestamp_counter);
   LogMethodTraceEvent(thread, method, kTraceMethodEnter, thread_clock_diff, timestamp_counter);
@@ -1315,14 +1188,14 @@ void Trace::MethodExited(Thread* thread,
                          ArtMethod* method,
                          [[maybe_unused]] instrumentation::OptionalFrame frame,
                          [[maybe_unused]] JValue& return_value) {
-  uint32_t thread_clock_diff = 0;
+  uint64_t thread_clock_diff = 0;
   uint64_t timestamp_counter = 0;
   ReadClocks(thread, &thread_clock_diff, &timestamp_counter);
   LogMethodTraceEvent(thread, method, kTraceMethodExit, thread_clock_diff, timestamp_counter);
 }
 
 void Trace::MethodUnwind(Thread* thread, ArtMethod* method, [[maybe_unused]] uint32_t dex_pc) {
-  uint32_t thread_clock_diff = 0;
+  uint64_t thread_clock_diff = 0;
   uint64_t timestamp_counter = 0;
   ReadClocks(thread, &thread_clock_diff, &timestamp_counter);
   LogMethodTraceEvent(thread, method, kTraceUnroll, thread_clock_diff, timestamp_counter);
@@ -1351,19 +1224,19 @@ void Trace::WatchedFramePop([[maybe_unused]] Thread* self,
   LOG(ERROR) << "Unexpected WatchedFramePop event in tracing";
 }
 
-void Trace::ReadClocks(Thread* thread, uint32_t* thread_clock_diff, uint64_t* timestamp_counter) {
+void Trace::ReadClocks(Thread* thread, uint64_t* thread_clock_diff, uint64_t* timestamp_counter) {
   if (UseThreadCpuClock(clock_source_)) {
     uint64_t clock_base = thread->GetTraceClockBase();
     if (UNLIKELY(clock_base == 0)) {
       // First event, record the base time in the map.
-      uint64_t time = thread->GetCpuMicroTime();
+      uint64_t time = thread->GetCpuNanoTime();
       thread->SetTraceClockBase(time);
     } else {
-      *thread_clock_diff = thread->GetCpuMicroTime() - clock_base;
+      *thread_clock_diff = thread->GetCpuNanoTime() - clock_base;
     }
   }
   if (UseWallClock(clock_source_)) {
-    *timestamp_counter = GetTimestamp();
+    *timestamp_counter = TimestampCounter::GetTimestamp();
   }
 }
 
@@ -1371,15 +1244,6 @@ std::string TraceWriter::GetMethodLine(const std::string& method_line, uint32_t
   return StringPrintf("%#x\t%s", (method_index << TraceActionBits), method_line.c_str());
 }
 
-std::string TraceWriter::GetMethodInfoLine(ArtMethod* method) {
-  method = method->GetInterfaceMethodIfProxy(kRuntimePointerSize);
-  return StringPrintf("%s\t%s\t%s\t%s\n",
-                      PrettyDescriptor(method->GetDeclaringClassDescriptor()).c_str(),
-                      method->GetName(),
-                      method->GetSignature().ToString().c_str(),
-                      method->GetDeclaringClassSourceFile());
-}
-
 void TraceWriter::RecordThreadInfo(Thread* thread) {
   // This is the first event from this thread, so first record information about the thread.
   std::string thread_name;
@@ -1683,6 +1547,11 @@ void TraceWriter::ReadValuesFromRecord(uintptr_t* method_trace_entries,
   record.wall_clock_time = 0;
   if (has_thread_cpu_clock) {
     record.thread_cpu_time = method_trace_entries[record_index++];
+    if (art::kRuntimePointerSize == PointerSize::k32) {
+      // On 32-bit architectures threadcputime is stored as two 32-bit values.
+      uint64_t high_bits = method_trace_entries[record_index++];
+      record.thread_cpu_time = (high_bits << 32 | record.thread_cpu_time);
+    }
   }
   if (has_wall_clock) {
     uint64_t timestamp = method_trace_entries[record_index++];
@@ -1691,23 +1560,39 @@ void TraceWriter::ReadValuesFromRecord(uintptr_t* method_trace_entries,
       uint64_t high_timestamp = method_trace_entries[record_index++];
       timestamp = (high_timestamp << 32 | timestamp);
     }
-    record.wall_clock_time = GetMicroTime(timestamp) - start_time_;
+    record.wall_clock_time = TimestampCounter::GetNanoTime(timestamp) - start_time_;
   }
 }
 
-void TraceWriter::FlushEntriesFormatV1(
+size_t TraceWriter::FlushEntriesFormatV1(
     uintptr_t* method_trace_entries,
     size_t tid,
     const std::unordered_map<ArtMethod*, std::string>& method_infos,
     size_t end_offset,
-    size_t* current_index,
-    uint8_t* buffer_ptr) {
+    size_t num_records) {
+  size_t buffer_index = 0;
+  uint8_t* buffer_ptr = buf_.get();
+
+  const size_t record_size = GetRecordSize(clock_source_, trace_format_version_);
+  DCHECK_LT(record_size, kPerThreadBufSize);
+  if (trace_output_mode_ != TraceOutputMode::kStreaming) {
+    // In non-streaming mode we only flush to file at the end, so retain the earlier data. If the
+    // buffer is full we don't process any more entries.
+    buffer_index = cur_offset_;
+
+    // Check if there is sufficient space in the buffer for non-streaming case. If not return early.
+    // In FormatV1, the encoding of events is fixed size, so we can determine the amount of buffer
+    // space required.
+    if (cur_offset_ + record_size * num_records >= buffer_size_) {
+      overflow_ = true;
+      return 0;
+    }
+  }
+
   uint16_t thread_id = GetThreadEncoding(tid);
   bool has_thread_cpu_clock = UseThreadCpuClock(clock_source_);
   bool has_wall_clock = UseWallClock(clock_source_);
-  size_t buffer_index = *current_index;
   size_t num_entries = GetNumEntries(clock_source_);
-  const size_t record_size = GetRecordSize(clock_source_, trace_format_version_);
 
   for (size_t entry_index = kPerThreadBufSize; entry_index != end_offset;) {
     entry_index -= num_entries;
@@ -1730,62 +1615,96 @@ void TraceWriter::FlushEntriesFormatV1(
                      record.wall_clock_time);
     buffer_index += record_size;
   }
-  *current_index = buffer_index;
+
+  if (trace_output_mode_ == TraceOutputMode::kStreaming) {
+    // Flush the contents of buffer to file.
+    if (!trace_file_->WriteFully(buffer_ptr, buffer_index)) {
+      PLOG(WARNING) << "Failed streaming a tracing event.";
+    }
+  } else {
+    // In non-streaming mode, we keep the data in the buffer and write to the
+    // file when tracing has stopped. Just update the offset of the buffer.
+    cur_offset_ = buffer_index;
+  }
+  return num_records;
 }
 
-void TraceWriter::FlushEntriesFormatV2(
-    uintptr_t* method_trace_entries,
-    size_t tid,
-    size_t num_records,
-    size_t* current_index,
-    uint8_t* init_buffer_ptr) {
-  uint8_t* current_buffer_ptr = init_buffer_ptr;
+size_t TraceWriter::FlushEntriesFormatV2(uintptr_t* method_trace_entries,
+                                         size_t tid,
+                                         size_t num_records) {
+  uint8_t* init_buffer_ptr = buf_.get();
+  uint8_t* end_buffer_ptr = buf_.get() + buffer_size_;
 
-  EncodeEventBlockHeader(current_buffer_ptr, tid, num_records);
-  current_buffer_ptr += kEntryHeaderSizeV2;
+  if (trace_output_mode_ != TraceOutputMode::kStreaming) {
+    // In non-streaming mode we only flush to file at the end, so retain the earlier data. If the
+    // buffer is full we don't process any more entries.
+    init_buffer_ptr = buf_.get() + cur_offset_;
+  }
 
+  uint8_t* current_buffer_ptr = init_buffer_ptr;
   bool has_thread_cpu_clock = UseThreadCpuClock(clock_source_);
   bool has_wall_clock = UseWallClock(clock_source_);
   size_t num_entries = GetNumEntries(clock_source_);
-  uint32_t prev_wall_timestamp = 0;
-  uint32_t prev_thread_timestamp = 0;
+  uint64_t prev_wall_timestamp = 0;
+  uint64_t prev_thread_timestamp = 0;
   uint64_t prev_method_action_encoding = 0;
   size_t entry_index = kPerThreadBufSize;
-  for (size_t i = 0; i < num_records; i++) {
-    entry_index -= num_entries;
+  size_t curr_record_index = 0;
+  const int max_record_size = GetRecordSize(clock_source_, trace_format_version_);
+
+  while (curr_record_index < num_records) {
+    current_buffer_ptr = init_buffer_ptr + kEntryHeaderSizeV2;
+    for (; curr_record_index < num_records; curr_record_index++) {
+      // Don't process more entries if the buffer doesn't have sufficient space.
+      if (end_buffer_ptr - current_buffer_ptr < max_record_size) {
+        break;
+      }
 
-    MethodTraceRecord record;
-    ReadValuesFromRecord(
-        method_trace_entries, entry_index, record, has_thread_cpu_clock, has_wall_clock);
+      entry_index -= num_entries;
+      MethodTraceRecord record;
+      ReadValuesFromRecord(
+          method_trace_entries, entry_index, record, has_thread_cpu_clock, has_wall_clock);
 
-    // TODO(mythria): Explore the possibility of using method pointer instead of having an encoding.
-    // On 64-bit this means method ids would use 8 bytes but that is okay since we only encode the
-    // full method id in the header and then encode the diff against the method id in the header.
-    // The diff is usually expected to be small.
-    uint64_t method_id = reinterpret_cast<uintptr_t>(record.method);
-    uint64_t method_action_encoding = method_id | record.action;
-
-    int64_t method_diff = method_action_encoding - prev_method_action_encoding;
-    current_buffer_ptr = EncodeSignedLeb128(current_buffer_ptr, method_diff);
-    prev_method_action_encoding = method_action_encoding;
-
-    if (has_wall_clock) {
-      current_buffer_ptr =
-          EncodeUnsignedLeb128(current_buffer_ptr, (record.wall_clock_time - prev_wall_timestamp));
-      prev_wall_timestamp = record.wall_clock_time;
+      uint64_t method_id = reinterpret_cast<uintptr_t>(record.method);
+      uint64_t method_action_encoding = method_id | record.action;
+
+      int64_t method_diff = method_action_encoding - prev_method_action_encoding;
+      current_buffer_ptr = EncodeSignedLeb128(current_buffer_ptr, method_diff);
+      prev_method_action_encoding = method_action_encoding;
+
+      if (has_wall_clock) {
+        current_buffer_ptr = EncodeUnsignedLeb128(current_buffer_ptr,
+                                                  (record.wall_clock_time - prev_wall_timestamp));
+        prev_wall_timestamp = record.wall_clock_time;
+      }
+
+      if (has_thread_cpu_clock) {
+        current_buffer_ptr = EncodeUnsignedLeb128(current_buffer_ptr,
+                                                  (record.thread_cpu_time - prev_thread_timestamp));
+        prev_thread_timestamp = record.thread_cpu_time;
+      }
     }
 
-    if (has_thread_cpu_clock) {
-      current_buffer_ptr = EncodeUnsignedLeb128(current_buffer_ptr,
-                                                (record.thread_cpu_time - prev_thread_timestamp));
-      prev_thread_timestamp = record.thread_cpu_time;
+    uint32_t size = current_buffer_ptr - (init_buffer_ptr + kEntryHeaderSizeV2);
+    EncodeEventBlockHeader(init_buffer_ptr, tid, curr_record_index, size);
+
+    if (trace_output_mode_ != TraceOutputMode::kStreaming) {
+      if (curr_record_index < num_records) {
+        overflow_ = true;
+      }
+      // In non-streaming mode, we keep the data in the buffer and write to the
+      // file when tracing has stopped. Just update the offset of the buffer.
+      cur_offset_ += (current_buffer_ptr - init_buffer_ptr);
+      return curr_record_index;
+    } else {
+      // Flush the contents of the buffer to the file.
+      if (!trace_file_->WriteFully(init_buffer_ptr, current_buffer_ptr - init_buffer_ptr)) {
+        PLOG(WARNING) << "Failed streaming a tracing event.";
+      }
     }
   }
 
-  // Update the total size of the block excluding header size.
-  uint8_t* total_size_loc = init_buffer_ptr + kEntryHeaderSizeV2 - 4;
-  Append4LE(total_size_loc, current_buffer_ptr - (init_buffer_ptr + kEntryHeaderSizeV2));
-  *current_index += current_buffer_ptr - init_buffer_ptr;
+  return num_records;
 }
 
 void TraceWriter::FlushBuffer(uintptr_t* method_trace_entries,
@@ -1803,48 +1722,22 @@ void TraceWriter::FlushBuffer(uintptr_t* method_trace_entries,
   size_t num_entries = GetNumEntries(clock_source_);
   size_t num_records = (kPerThreadBufSize - current_offset) / num_entries;
   DCHECK_EQ((kPerThreadBufSize - current_offset) % num_entries, 0u);
-  const size_t record_size = GetRecordSize(clock_source_, trace_format_version_);
-  DCHECK_LT(record_size, kPerThreadBufSize);
-
-  if (trace_output_mode_ != TraceOutputMode::kStreaming) {
-    // In non-streaming mode we only flush to file at the end, so retain the earlier data. If the
-    // buffer is full we don't process any more entries.
-    current_index = cur_offset_;
-
-    // Check if there is sufficient place in the buffer for non-streaming case. If not return early.
-    if (cur_offset_ + record_size * num_records >= buffer_size) {
-      overflow_ = true;
-      return;
-    }
-  }
-  num_records_ += num_records;
 
-  DCHECK_GT(buffer_size_, record_size * num_entries);
+  int num_records_written = 0;
   if (trace_format_version_ == Trace::kFormatV1) {
-    FlushEntriesFormatV1(
-        method_trace_entries, tid, method_infos, current_offset, &current_index, buffer_ptr);
+    num_records_written =
+        FlushEntriesFormatV1(method_trace_entries, tid, method_infos, current_offset, num_records);
   } else {
-    FlushEntriesFormatV2(
-        method_trace_entries, tid, num_records, &current_index, buffer_ptr + current_index);
-  }
-
-  if (trace_output_mode_ == TraceOutputMode::kStreaming) {
-    // Flush the contents of buffer to file.
-    if (!trace_file_->WriteFully(buffer_ptr, current_index)) {
-      PLOG(WARNING) << "Failed streaming a tracing event.";
-    }
-  } else {
-    // In non-streaming mode, we keep the data in the buffer and write to the
-    // file when tracing has stopped. Just updated the offset of the buffer.
-    cur_offset_ = current_index;
+    num_records_written = FlushEntriesFormatV2(method_trace_entries, tid, num_records);
   }
+  num_records_ += num_records_written;
   return;
 }
 
 void Trace::LogMethodTraceEvent(Thread* thread,
                                 ArtMethod* method,
                                 TraceAction action,
-                                uint32_t thread_clock_diff,
+                                uint64_t thread_clock_diff,
                                 uint64_t timestamp_counter) {
   // This method is called in both tracing modes (method and sampling). In sampling mode, this
   // method is only called by the sampling thread. In method tracing mode, it can be called
@@ -1887,7 +1780,13 @@ void Trace::LogMethodTraceEvent(Thread* thread,
   method = method->GetNonObsoleteMethod();
   current_entry[entry_index++] = reinterpret_cast<uintptr_t>(method) | action;
   if (UseThreadCpuClock(clock_source_)) {
-    current_entry[entry_index++] = thread_clock_diff;
+    if (art::kRuntimePointerSize == PointerSize::k32) {
+      // On 32-bit architectures store threadcputimer as two 32-bit values.
+      current_entry[entry_index++] = static_cast<uint32_t>(thread_clock_diff);
+      current_entry[entry_index++] = thread_clock_diff >> 32;
+    } else {
+      current_entry[entry_index++] = thread_clock_diff;
+    }
   }
   if (UseWallClock(clock_source_)) {
     if (art::kRuntimePointerSize == PointerSize::k32) {
@@ -1904,8 +1803,8 @@ void TraceWriter::EncodeEventEntry(uint8_t* ptr,
                                    uint16_t thread_id,
                                    uint32_t method_index,
                                    TraceAction action,
-                                   uint32_t thread_clock_diff,
-                                   uint32_t wall_clock_diff) {
+                                   uint64_t thread_clock_diff,
+                                   uint64_t wall_clock_diff) {
   static constexpr size_t kPacketSize = 14U;  // The maximum size of data in a packet.
   DCHECK(method_index < (1 << (32 - TraceActionBits)));
   uint32_t method_value = (method_index << TraceActionBits) | action;
@@ -1913,22 +1812,29 @@ void TraceWriter::EncodeEventEntry(uint8_t* ptr,
   Append4LE(ptr + 2, method_value);
   ptr += 6;
 
+  static constexpr uint64_t ns_to_us = 1000;
+  uint32_t thread_clock_diff_us = thread_clock_diff / ns_to_us;
+  uint32_t wall_clock_diff_us = wall_clock_diff / ns_to_us;
   if (UseThreadCpuClock(clock_source_)) {
-    Append4LE(ptr, thread_clock_diff);
+    Append4LE(ptr, thread_clock_diff_us);
     ptr += 4;
   }
   if (UseWallClock(clock_source_)) {
-    Append4LE(ptr, wall_clock_diff);
+    Append4LE(ptr, wall_clock_diff_us);
   }
   static_assert(kPacketSize == 2 + 4 + 4 + 4, "Packet size incorrect.");
 }
 
-void TraceWriter::EncodeEventBlockHeader(uint8_t* ptr, uint32_t thread_id, uint32_t num_records) {
+void TraceWriter::EncodeEventBlockHeader(uint8_t* ptr,
+                                         uint32_t thread_id,
+                                         uint32_t num_records,
+                                         uint32_t size) {
   ptr[0] = kEntryHeaderV2;
   Append4LE(ptr + 1, thread_id);
   // This specifies the total number of records encoded in the block using lebs.
   DCHECK_LT(num_records, 1u << 24);
   Append3LE(ptr + 5, num_records);
+  Append4LE(ptr + 8, size);
 }
 
 void TraceWriter::EnsureSpace(uint8_t* buffer,
diff --git a/runtime/trace.h b/runtime/trace.h
index ffd70aa8e6..ca728e93f9 100644
--- a/runtime/trace.h
+++ b/runtime/trace.h
@@ -32,15 +32,12 @@
 #include "base/mutex.h"
 #include "base/os.h"
 #include "base/safe_map.h"
+#include "base/unix_file/fd_file.h"
 #include "class_linker.h"
 #include "instrumentation.h"
 #include "runtime_globals.h"
 #include "thread_pool.h"
 
-namespace unix_file {
-class FdFile;
-}  // namespace unix_file
-
 namespace art HIDDEN {
 
 class ArtField;
@@ -111,7 +108,10 @@ enum class TraceOutputMode {
 // We need 3 entries to store 64-bit timestamp counter as two 32-bit values on 32-bit architectures.
 static constexpr uint32_t kNumEntriesForWallClock =
     (kRuntimePointerSize == PointerSize::k64) ? 2 : 3;
-static constexpr uint32_t kNumEntriesForDualClock = kNumEntriesForWallClock + 1;
+// Timestamps are stored as two 32-bit balues on 32-bit architectures.
+static constexpr uint32_t kNumEntriesForDualClock = (kRuntimePointerSize == PointerSize::k64)
+                                                        ? kNumEntriesForWallClock + 1
+                                                        : kNumEntriesForWallClock + 2;
 
 // These define offsets in bytes for the individual fields of a trace entry. These are used by the
 // JITed code when storing a trace entry.
@@ -132,8 +132,11 @@ static constexpr int kSummaryHeaderV2 = 3;
 
 // Packet sizes for the new method tracing format.
 static constexpr uint16_t kTraceHeaderLengthV2 = 32;
-static constexpr uint16_t kTraceRecordSizeSingleClockV2 = 6;
-static constexpr uint16_t kTraceRecordSizeDualClockV2 = kTraceRecordSizeSingleClockV2 + 2;
+// We have 2 entries (method pointer and timestamp) which are uleb encoded. Each
+// of them is a maximum of 64 bits which would need 10 bytes at the maximum.
+static constexpr uint16_t kMaxTraceRecordSizeSingleClockV2 = 20;
+// We will have one more timestamp of 64 bits if we use a dual clock source.
+static constexpr uint16_t kMaxTraceRecordSizeDualClockV2 = kMaxTraceRecordSizeSingleClockV2 + 10;
 static constexpr uint16_t kEntryHeaderSizeV2 = 12;
 
 static constexpr uint16_t kTraceVersionSingleClockV2 = 4;
@@ -200,7 +203,7 @@ class TraceWriter {
               size_t buffer_size,
               int num_trace_buffers,
               int trace_format_version,
-              uint32_t clock_overhead_ns);
+              uint64_t clock_overhead_ns);
 
   // This encodes all the events in the per-thread trace buffer and writes it to the trace file /
   // buffer. This acquires streaming lock to prevent any other threads writing concurrently. It is
@@ -306,18 +309,14 @@ class TraceWriter {
                             bool has_thread_cpu_clock,
                             bool has_wall_clock);
 
-  void FlushEntriesFormatV2(uintptr_t* method_trace_entries,
-                            size_t tid,
-                            size_t num_records,
-                            size_t* current_index,
-                            uint8_t* init_buffer_ptr) REQUIRES(trace_writer_lock_);
-
-  void FlushEntriesFormatV1(uintptr_t* method_trace_entries,
-                            size_t tid,
-                            const std::unordered_map<ArtMethod*, std::string>& method_infos,
-                            size_t end_offset,
-                            size_t* current_index,
-                            uint8_t* buffer_ptr) REQUIRES(trace_writer_lock_);
+  size_t FlushEntriesFormatV2(uintptr_t* method_trace_entries, size_t tid, size_t num_records)
+      REQUIRES(trace_writer_lock_);
+
+  size_t FlushEntriesFormatV1(uintptr_t* method_trace_entries,
+                              size_t tid,
+                              const std::unordered_map<ArtMethod*, std::string>& method_infos,
+                              size_t end_offset,
+                              size_t num_records) REQUIRES(trace_writer_lock_);
   // Get a 32-bit id for the method and specify if the method hasn't been seen before. If this is
   // the first time we see this method record information (like method name, declaring class etc.,)
   // about the method.
@@ -330,7 +329,6 @@ class TraceWriter {
 
   // Get the information about the method.
   std::string GetMethodLine(const std::string& method_line, uint32_t method_id);
-  std::string GetMethodInfoLine(ArtMethod* method) REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Helper function to record method information when processing the events. These are used by
   // streaming output mode. Non-streaming modes dump the methods and threads list at the end of
@@ -343,12 +341,12 @@ class TraceWriter {
                         uint16_t thread_id,
                         uint32_t method_index,
                         TraceAction action,
-                        uint32_t thread_clock_diff,
-                        uint32_t wall_clock_diff) REQUIRES(trace_writer_lock_);
+                        uint64_t thread_clock_diff,
+                        uint64_t wall_clock_diff) REQUIRES(trace_writer_lock_);
 
   // Encodes the header for the events block. This assumes that there is enough space reserved to
   // encode the entry.
-  void EncodeEventBlockHeader(uint8_t* ptr, uint32_t thread_id, uint32_t num_records)
+  void EncodeEventBlockHeader(uint8_t* ptr, uint32_t thread_id, uint32_t num_records, uint32_t size)
       REQUIRES(trace_writer_lock_);
 
   // Ensures there is sufficient space in the buffer to record the requested_size. If there is not
@@ -418,7 +416,7 @@ class TraceWriter {
   size_t num_records_;
 
   // Clock overhead.
-  const uint32_t clock_overhead_ns_;
+  const uint64_t clock_overhead_ns_;
 
   std::vector<std::atomic<size_t>> owner_tids_;
   std::unique_ptr<uintptr_t[]> trace_buffer_;
@@ -520,7 +518,7 @@ class Trace final : public instrumentation::InstrumentationListener, public Clas
   static void RemoveListeners() REQUIRES(Locks::mutator_lock_);
 
   void MeasureClockOverhead();
-  uint32_t GetClockOverheadNanoSeconds();
+  uint64_t GetClockOverheadNanoSeconds();
 
   void CompareAndUpdateStackTrace(Thread* thread, std::vector<ArtMethod*>* stack_trace)
       REQUIRES_SHARED(Locks::mutator_lock_);
@@ -606,12 +604,12 @@ class Trace final : public instrumentation::InstrumentationListener, public Clas
       // how to annotate this.
       NO_THREAD_SAFETY_ANALYSIS;
 
-  void ReadClocks(Thread* thread, uint32_t* thread_clock_diff, uint64_t* timestamp_counter);
+  void ReadClocks(Thread* thread, uint64_t* thread_clock_diff, uint64_t* timestamp_counter);
 
   void LogMethodTraceEvent(Thread* thread,
                            ArtMethod* method,
                            TraceAction action,
-                           uint32_t thread_clock_diff,
+                           uint64_t thread_clock_diff,
                            uint64_t timestamp_counter) REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Singleton instance of the Trace or null when no method tracing is active.
diff --git a/runtime/trace_common.h b/runtime/trace_common.h
new file mode 100644
index 0000000000..f29b347462
--- /dev/null
+++ b/runtime/trace_common.h
@@ -0,0 +1,169 @@
+/*
+ * Copyright (C) 2024 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_RUNTIME_TRACE_COMMON_H_
+#define ART_RUNTIME_TRACE_COMMON_H_
+
+#include "android-base/stringprintf.h"
+#include "art_method-inl.h"
+#include "dex/descriptors_names.h"
+#include "oat/oat_quick_method_header.h"
+
+using android::base::StringPrintf;
+
+namespace art HIDDEN {
+
+static std::string GetMethodInfoLine(ArtMethod* method) REQUIRES_SHARED(Locks::mutator_lock_) {
+  method = method->GetInterfaceMethodIfProxy(kRuntimePointerSize);
+  return StringPrintf("%s\t%s\t%s\t%s\n",
+                      PrettyDescriptor(method->GetDeclaringClassDescriptor()).c_str(),
+                      method->GetName(),
+                      method->GetSignature().ToString().c_str(),
+                      method->GetDeclaringClassSourceFile());
+}
+
+class TimestampCounter {
+ public:
+  static uint64_t GetTimestamp() {
+    uint64_t t = 0;
+#if defined(__arm__)
+    // On ARM 32 bit, we don't always have access to the timestamp counters from user space. There
+    // is no easy way to check if it is safe to read the timestamp counters. There is HWCAP_EVTSTRM
+    // which is set when generic timer is available but not necessarily from the user space. Kernel
+    // disables access to generic timer when there are known problems on the target CPUs. Sometimes
+    // access is disabled only for 32-bit processes even when 64-bit processes can accesses the
+    // timer from user space. These are not reflected in the HWCAP_EVTSTRM capability.So just
+    // fallback to clock_gettime on these processes. See b/289178149 for more discussion.
+    t = NanoTime();
+#elif defined(__aarch64__)
+    // See Arm Architecture Registers  Armv8 section System Registers
+    asm volatile("mrs %0, cntvct_el0" : "=r"(t));
+#elif defined(__i386__) || defined(__x86_64__)
+    // rdtsc returns two 32-bit values in rax and rdx even on 64-bit architectures.
+    unsigned int lo, hi;
+    asm volatile("rdtsc" : "=a"(lo), "=d"(hi));
+    t = (static_cast<uint64_t>(hi) << 32) | lo;
+#elif defined(__riscv)
+    asm volatile("rdtime %0" : "=r"(t));
+#else
+    t = NanoTime();
+#endif
+    return t;
+  }
+
+  static void InitializeTimestampCounters() {
+    // It is sufficient to initialize this once for the entire execution. Just return if it is
+    // already initialized.
+    if (tsc_to_nanosec_scaling_factor > 0.0) {
+      return;
+    }
+
+#if defined(__arm__)
+    // On ARM 32 bit, we don't always have access to the timestamp counters from
+    // user space. Seem comment in GetTimestamp for more details.
+    tsc_to_nanosec_scaling_factor = 1.0;
+#elif defined(__aarch64__)
+    double seconds_to_nanoseconds = 1000 * 1000;
+    uint64_t freq = 0;
+    // See Arm Architecture Registers  Armv8 section System Registers
+    asm volatile("mrs %0,  cntfrq_el0" : "=r"(freq));
+    if (freq == 0) {
+      // It is expected that cntfrq_el0 is correctly setup during system initialization but some
+      // devices don't do this. In such cases fall back to computing the frequency. See b/315139000.
+      tsc_to_nanosec_scaling_factor = computeScalingFactor();
+    } else {
+      tsc_to_nanosec_scaling_factor = seconds_to_nanoseconds / static_cast<double>(freq);
+    }
+#elif defined(__i386__) || defined(__x86_64__)
+    tsc_to_nanosec_scaling_factor = GetScalingFactorForX86();
+#else
+    tsc_to_nanosec_scaling_factor = 1.0;
+#endif
+  }
+
+  static ALWAYS_INLINE uint64_t GetNanoTime(uint64_t counter) {
+    DCHECK(tsc_to_nanosec_scaling_factor > 0.0) << tsc_to_nanosec_scaling_factor;
+    return tsc_to_nanosec_scaling_factor * counter;
+  }
+
+ private:
+#if defined(__i386__) || defined(__x86_64__) || defined(__aarch64__)
+  // Here we compute the scaling factor by sleeping for a millisecond. Alternatively, we could
+  // generate raw timestamp counter and also time using clock_gettime at the start and the end of
+  // the trace. We can compute the frequency of timestamp counter upadtes in the post processing
+  // step using these two samples. However, that would require a change in Android Studio which is
+  // the main consumer of these profiles. For now, just compute the frequency of tsc updates here.
+  static double computeScalingFactor() {
+    uint64_t start = NanoTime();
+    uint64_t start_tsc = GetTimestamp();
+    // Sleep for one millisecond.
+    usleep(1000);
+    uint64_t diff_tsc = GetTimestamp() - start_tsc;
+    uint64_t diff_time = NanoTime() - start;
+    double scaling_factor = static_cast<double>(diff_time) / diff_tsc;
+    DCHECK(scaling_factor > 0.0) << scaling_factor;
+    return scaling_factor;
+  }
+#endif
+
+#if defined(__i386__) || defined(__x86_64__)
+  static double GetScalingFactorForX86() {
+    uint32_t eax, ebx, ecx;
+    asm volatile("cpuid" : "=a"(eax), "=b"(ebx), "=c"(ecx) : "a"(0x0), "c"(0));
+    if (eax < 0x15) {
+      // There is no 15H - Timestamp counter and core crystal clock information
+      // leaf. Just compute the frequency.
+      return computeScalingFactor();
+    }
+
+    // From Intel architecture-instruction-set-extensions-programming-reference:
+    // EBX[31:0]/EAX[31:0] indicates the ratio of the TSC frequency and the
+    // core crystal clock frequency.
+    // If EBX[31:0] is 0, the TSC and "core crystal clock" ratio is not enumerated.
+    // If ECX is 0, the nominal core crystal clock frequency is not enumerated.
+    // "TSC frequency" = "core crystal clock frequency" * EBX/EAX.
+    // The core crystal clock may differ from the reference clock, bus clock, or core clock
+    // frequencies.
+    // EAX Bits 31 - 00: An unsigned integer which is the denominator of the
+    //                   TSC/"core crystal clock" ratio.
+    // EBX Bits 31 - 00: An unsigned integer which is the numerator of the
+    //                   TSC/"core crystal clock" ratio.
+    // ECX Bits 31 - 00: An unsigned integer which is the nominal frequency of the core
+    //                   crystal clock in Hz.
+    // EDX Bits 31 - 00: Reserved = 0.
+    asm volatile("cpuid" : "=a"(eax), "=b"(ebx), "=c"(ecx) : "a"(0x15), "c"(0));
+    if (ebx == 0 || ecx == 0) {
+      return computeScalingFactor();
+    }
+    double coreCrystalFreq = ecx;
+    // frequency = coreCrystalFreq * (ebx / eax)
+    // scaling_factor = seconds_to_nanoseconds / frequency
+    //                = seconds_to_nanoseconds * eax / (coreCrystalFreq * ebx)
+    double seconds_to_nanoseconds = 1000 * 1000;
+    double scaling_factor = (seconds_to_nanoseconds * eax) / (coreCrystalFreq * ebx);
+    return scaling_factor;
+  }
+#endif
+
+  // Scaling factor to convert timestamp counter into wall clock time reported in nano seconds.
+  // This is initialized at the start of tracing using the timestamp counter update frequency.
+  // See InitializeTimestampCounters for more details.
+  static double tsc_to_nanosec_scaling_factor;
+};
+
+}  // namespace art
+
+#endif  // ART_RUNTIME_TRACE_COMMON_H_
diff --git a/runtime/trace_profile.cc b/runtime/trace_profile.cc
index 6907e28972..6eff2cdfe1 100644
--- a/runtime/trace_profile.cc
+++ b/runtime/trace_profile.cc
@@ -17,17 +17,22 @@
 #include "trace_profile.h"
 
 #include "android-base/stringprintf.h"
+#include "arch/context.h"
 #include "art_method-inl.h"
 #include "base/leb128.h"
 #include "base/mutex.h"
 #include "base/unix_file/fd_file.h"
 #include "com_android_art_flags.h"
 #include "dex/descriptors_names.h"
+#include "gc/task_processor.h"
+#include "oat/oat_quick_method_header.h"
 #include "runtime.h"
+#include "stack.h"
 #include "thread-current-inl.h"
 #include "thread.h"
 #include "thread_list.h"
 #include "trace.h"
+#include "trace_common.h"
 
 namespace art_flags = com::android::art::flags;
 
@@ -40,6 +45,8 @@ using android::base::StringPrintf;
 // sizeof(uintptr_t).
 static constexpr size_t kMaxBytesPerTraceEntry = sizeof(uintptr_t);
 
+static constexpr size_t kMaxEntriesAfterFlush = kAlwaysOnTraceBufSize / 2;
+
 // We don't handle buffer overflows when processing the raw trace entries. We have a maximum of
 // kAlwaysOnTraceBufSize raw entries and we need a maximum of kMaxBytesPerTraceEntry to encode
 // each entry. To avoid overflow, we ensure that there are at least kMinBufSizeForEncodedData
@@ -51,10 +58,58 @@ static constexpr size_t kProfileMagicValue = 0x4C4F4D54;
 // TODO(mythria): 10 is a randomly chosen value. Tune it if required.
 static constexpr size_t kBufSizeForEncodedData = kMinBufSizeForEncodedData * 10;
 
-static constexpr size_t kAlwaysOnTraceHeaderSize = 8;
+static constexpr size_t kAlwaysOnTraceHeaderSize = 12;
+static constexpr size_t kAlwaysOnMethodInfoHeaderSize = 11;
+static constexpr size_t kAlwaysOnThreadInfoHeaderSize = 7;
 
 bool TraceProfiler::profile_in_progress_ = false;
 
+TraceData* TraceProfiler::trace_data_ = nullptr;
+
+void TraceData::AddTracedThread(Thread* thread) {
+  MutexLock mu(Thread::Current(), trace_data_lock_);
+  size_t thread_id = thread->GetTid();
+  if (traced_threads_.find(thread_id) != traced_threads_.end()) {
+    return;
+  }
+
+  std::string thread_name;
+  thread->GetThreadName(thread_name);
+  traced_threads_.emplace(thread_id, thread_name);
+}
+
+void TraceData::MaybeWaitForTraceDumpToFinish() {
+  if (!trace_dump_in_progress_) {
+    return;
+  }
+  trace_dump_condition_.Wait(Thread::Current());
+}
+
+void TraceData::SignalTraceDumpComplete() {
+  trace_dump_in_progress_ = false;
+  trace_dump_condition_.Broadcast(Thread::Current());
+}
+
+void TraceData::AppendToLongRunningMethods(const uint8_t* buffer, size_t size) {
+  MutexLock mu(Thread::Current(), trace_data_lock_);
+  if (curr_buffer_ == nullptr) {
+    curr_buffer_.reset(new uint8_t[kBufSizeForEncodedData]);
+    curr_index_ = 0;
+  }
+  if (curr_index_ + size <= kBufSizeForEncodedData) {
+    memcpy(curr_buffer_.get() + curr_index_, buffer, size);
+    curr_index_ += size;
+  } else {
+    size_t remaining_bytes = kBufSizeForEncodedData - curr_index_;
+    if (remaining_bytes != 0) {
+      memcpy(curr_buffer_.get() + curr_index_, buffer, remaining_bytes);
+    }
+    overflow_buffers_.push_back(std::move(curr_buffer_));
+    curr_buffer_.reset(new uint8_t[kBufSizeForEncodedData]);
+    memcpy(curr_buffer_.get(), buffer + remaining_bytes, size - remaining_bytes);
+  }
+}
+
 void TraceProfiler::AllocateBuffer(Thread* thread) {
   if (!art_flags::always_enable_profile_code()) {
     return;
@@ -67,37 +122,202 @@ void TraceProfiler::AllocateBuffer(Thread* thread) {
   }
 
   auto buffer = new uintptr_t[kAlwaysOnTraceBufSize];
-  memset(buffer, 0, kAlwaysOnTraceBufSize * sizeof(uintptr_t));
-  thread->SetMethodTraceBuffer(buffer, kAlwaysOnTraceBufSize);
+  size_t index = kAlwaysOnTraceBufSize;
+  if (trace_data_->GetTraceType() == LowOverheadTraceType::kAllMethods) {
+    memset(buffer, 0, kAlwaysOnTraceBufSize * sizeof(uintptr_t));
+  } else {
+    DCHECK(trace_data_->GetTraceType() == LowOverheadTraceType::kLongRunningMethods);
+    // For long running methods add a placeholder method exit entry. This avoids
+    // additional checks on method exits to see if the previous entry is valid.
+    index--;
+    buffer[index] = 0x1;
+  }
+  thread->SetMethodTraceBuffer(buffer, index);
 }
 
-void TraceProfiler::Start() {
-  if (!art_flags::always_enable_profile_code()) {
-    LOG(ERROR) << "Feature not supported. Please build with ART_ALWAYS_ENABLE_PROFILE_CODE.";
-    return;
+LowOverheadTraceType TraceProfiler::GetTraceType() {
+  MutexLock mu(Thread::Current(), *Locks::trace_lock_);
+  // LowOverhead trace entry points are configured based on the trace type. When trace_data_ is null
+  // then there is no low overhead tracing running, so we use nop entry points.
+  if (trace_data_ == nullptr) {
+    return LowOverheadTraceType::kNone;
   }
 
-  Thread* self = Thread::Current();
-  MutexLock mu(self, *Locks::trace_lock_);
-  if (profile_in_progress_) {
-    LOG(ERROR) << "Profile already in progress. Ignoring this request";
-    return;
+  return trace_data_->GetTraceType();
+}
+
+namespace {
+void RecordMethodsOnThreadStack(Thread* thread, uintptr_t* method_trace_buffer)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  struct MethodEntryStackVisitor final : public StackVisitor {
+    MethodEntryStackVisitor(Thread* thread_in, Context* context)
+        : StackVisitor(thread_in, context, StackVisitor::StackWalkKind::kSkipInlinedFrames) {}
+
+    bool VisitFrame() override REQUIRES_SHARED(Locks::mutator_lock_) {
+      ArtMethod* m = GetMethod();
+      if (m != nullptr && !m->IsRuntimeMethod()) {
+        if (GetCurrentShadowFrame() != nullptr) {
+          // TODO(mythria): Support low-overhead tracing for the switch interpreter.
+        } else {
+          const OatQuickMethodHeader* method_header = GetCurrentOatQuickMethodHeader();
+          if (method_header == nullptr) {
+            // TODO(mythria): Consider low-overhead tracing support for the GenericJni stubs.
+          } else {
+            // Ignore nterp methods. We don't support recording trace events in nterp.
+            if (!method_header->IsNterpMethodHeader()) {
+              stack_methods_.push_back(m);
+            }
+          }
+        }
+      }
+      return true;
+    }
+
+    std::vector<ArtMethod*> stack_methods_;
+  };
+
+  std::unique_ptr<Context> context(Context::Create());
+  MethodEntryStackVisitor visitor(thread, context.get());
+  visitor.WalkStack(true);
+
+  // Create method entry events for all methods currently on the thread's stack.
+  uint64_t init_ts = TimestampCounter::GetTimestamp();
+  // Set the lsb to 0 to indicate method entry.
+  init_ts = init_ts & ~1;
+  size_t index = kAlwaysOnTraceBufSize - 1;
+  for (auto smi = visitor.stack_methods_.rbegin(); smi != visitor.stack_methods_.rend(); smi++) {
+    method_trace_buffer[index--] = reinterpret_cast<uintptr_t>(*smi);
+    method_trace_buffer[index--] = init_ts;
+
+    if (index < kMaxEntriesAfterFlush) {
+      // To keep the implementation simple, ignore methods deep down the stack. If the call stack
+      // unwinds beyond this point then we will see method exits without corresponding method
+      // entries.
+      break;
+    }
   }
 
-  if (Trace::IsTracingEnabledLocked()) {
-    LOG(ERROR) << "Cannot start a profile when method tracing is in progress";
-    return;
+  // Record a placeholder method exit event into the buffer so we record method exits for the
+  // methods that are currently on stack.
+  method_trace_buffer[index] = 0x1;
+  thread->SetMethodTraceBuffer(method_trace_buffer, index);
+}
+
+// Records the thread and method info.
+void DumpThreadMethodInfo(const std::unordered_map<size_t, std::string>& traced_threads,
+                          const std::unordered_set<ArtMethod*>& traced_methods,
+                          std::ostringstream& os) REQUIRES_SHARED(Locks::mutator_lock_) {
+  // Dump data about thread information.
+  for (const auto& it : traced_threads) {
+    uint8_t thread_header[kAlwaysOnThreadInfoHeaderSize];
+    thread_header[0] = kThreadInfoHeaderV2;
+    Append4LE(thread_header + 1, it.first);
+    Append2LE(thread_header + 5, it.second.length());
+    os.write(reinterpret_cast<char*>(thread_header), kAlwaysOnThreadInfoHeaderSize);
+    os.write(it.second.c_str(), it.second.length());
   }
 
-  profile_in_progress_ = true;
+  // Dump data about method information.
+  for (ArtMethod* method : traced_methods) {
+    std::string method_line = GetMethodInfoLine(method);
+    uint16_t method_line_length = static_cast<uint16_t>(method_line.length());
+    uint8_t method_header[kAlwaysOnMethodInfoHeaderSize];
+    method_header[0] = kMethodInfoHeaderV2;
+    Append8LE(method_header + 1, reinterpret_cast<uint64_t>(method));
+    Append2LE(method_header + 9, method_line_length);
+    os.write(reinterpret_cast<char*>(method_header), kAlwaysOnMethodInfoHeaderSize);
+    os.write(method_line.c_str(), method_line_length);
+  }
+}
+}  // namespace
+
+class TraceStopTask : public gc::HeapTask {
+ public:
+  explicit TraceStopTask(uint64_t target_run_time) : gc::HeapTask(target_run_time) {}
+
+  void Run([[maybe_unused]] Thread* self) override { TraceProfiler::TraceTimeElapsed(); }
+};
+
+static class LongRunningMethodsTraceStartCheckpoint final : public Closure {
+ public:
+  void Run(Thread* thread) override REQUIRES_SHARED(Locks::mutator_lock_) {
+    auto buffer = new uintptr_t[kAlwaysOnTraceBufSize];
+    // Record methods that are currently on stack.
+    RecordMethodsOnThreadStack(thread, buffer);
+    thread->UpdateTlsLowOverheadTraceEntrypoints(LowOverheadTraceType::kLongRunningMethods);
+  }
+} long_running_methods_checkpoint_;
 
-  ScopedSuspendAll ssa(__FUNCTION__);
-  MutexLock tl(self, *Locks::thread_list_lock_);
-  for (Thread* thread : Runtime::Current()->GetThreadList()->GetList()) {
+static class AllMethodsTraceStartCheckpoint final : public Closure {
+ public:
+  void Run(Thread* thread) override {
     auto buffer = new uintptr_t[kAlwaysOnTraceBufSize];
     memset(buffer, 0, kAlwaysOnTraceBufSize * sizeof(uintptr_t));
+    thread->UpdateTlsLowOverheadTraceEntrypoints(LowOverheadTraceType::kAllMethods);
     thread->SetMethodTraceBuffer(buffer, kAlwaysOnTraceBufSize);
   }
+} all_methods_checkpoint_;
+
+void TraceProfiler::Start(LowOverheadTraceType trace_type, uint64_t trace_duration_ns) {
+  if (!art_flags::always_enable_profile_code()) {
+    LOG(ERROR) << "Feature not supported. Please build with ART_ALWAYS_ENABLE_PROFILE_CODE.";
+    return;
+  }
+
+  TimestampCounter::InitializeTimestampCounters();
+
+  Runtime* runtime = Runtime::Current();
+  Thread* self = Thread::Current();
+  uint64_t new_end_time = 0;
+  bool add_trace_end_task = false;
+  {
+    MutexLock mu(self, *Locks::trace_lock_);
+    if (Trace::IsTracingEnabledLocked()) {
+      LOG(ERROR) << "Cannot start a low-overehad trace when regular tracing is in progress";
+      return;
+    }
+
+    if (profile_in_progress_) {
+      // We allow overlapping starts only when collecting long running methods.
+      // If a trace of different type is in progress we ignore the request.
+      if (trace_type == LowOverheadTraceType::kAllMethods ||
+          trace_data_->GetTraceType() != trace_type) {
+        LOG(ERROR) << "Profile already in progress. Ignoring this request";
+        return;
+      }
+
+      // For long running methods, just update the end time if there's a trace already in progress.
+      new_end_time = NanoTime() + trace_duration_ns;
+      if (trace_data_->GetTraceEndTime() < new_end_time) {
+        trace_data_->SetTraceEndTime(new_end_time);
+        add_trace_end_task = true;
+      }
+    } else {
+      profile_in_progress_ = true;
+      trace_data_ = new TraceData(trace_type);
+
+      if (trace_type == LowOverheadTraceType::kAllMethods) {
+        runtime->GetThreadList()->RunCheckpoint(&all_methods_checkpoint_);
+      } else {
+        runtime->GetThreadList()->RunCheckpoint(&long_running_methods_checkpoint_);
+      }
+
+      if (trace_type == LowOverheadTraceType::kLongRunningMethods) {
+        new_end_time = NanoTime() + trace_duration_ns;
+        add_trace_end_task = true;
+        trace_data_->SetTraceEndTime(new_end_time);
+      }
+    }
+  }
+
+  if (add_trace_end_task) {
+    // Add a Task that stops the tracing after trace_duration.
+    runtime->GetHeap()->AddHeapTask(new TraceStopTask(new_end_time));
+  }
+}
+
+void TraceProfiler::Start() {
+  TraceProfiler::Start(LowOverheadTraceType::kAllMethods, /* trace_duration_ns= */ 0);
 }
 
 void TraceProfiler::Stop() {
@@ -108,28 +328,39 @@ void TraceProfiler::Stop() {
 
   Thread* self = Thread::Current();
   MutexLock mu(self, *Locks::trace_lock_);
+  TraceProfiler::StopLocked();
+}
+
+void TraceProfiler::StopLocked() {
   if (!profile_in_progress_) {
     LOG(ERROR) << "No Profile in progress but a stop was requested";
     return;
   }
 
-  ScopedSuspendAll ssa(__FUNCTION__);
-  MutexLock tl(self, *Locks::thread_list_lock_);
-  for (Thread* thread : Runtime::Current()->GetThreadList()->GetList()) {
+  // We should not delete trace_data_ when there is an ongoing trace dump. So
+  // wait for any in progress trace dump to finish.
+  trace_data_->MaybeWaitForTraceDumpToFinish();
+
+  static FunctionClosure reset_buffer([](Thread* thread) {
     auto buffer = thread->GetMethodTraceBuffer();
     if (buffer != nullptr) {
       delete[] buffer;
       thread->SetMethodTraceBuffer(/* buffer= */ nullptr, /* offset= */ 0);
     }
-  }
+    thread->UpdateTlsLowOverheadTraceEntrypoints(LowOverheadTraceType::kNone);
+  });
 
+  Runtime::Current()->GetThreadList()->RunCheckpoint(&reset_buffer);
   profile_in_progress_ = false;
+  DCHECK_NE(trace_data_, nullptr);
+  delete trace_data_;
+  trace_data_ = nullptr;
 }
 
-uint8_t* TraceProfiler::DumpBuffer(uint32_t thread_id,
-                                   uintptr_t* method_trace_entries,
-                                   uint8_t* buffer,
-                                   std::unordered_set<ArtMethod*>& methods) {
+size_t TraceProfiler::DumpBuffer(uint32_t thread_id,
+                                 uintptr_t* method_trace_entries,
+                                 uint8_t* buffer,
+                                 std::unordered_set<ArtMethod*>& methods) {
   // Encode header at the end once we compute the number of records.
   uint8_t* curr_buffer_ptr = buffer + kAlwaysOnTraceHeaderSize;
 
@@ -173,7 +404,7 @@ uint8_t* TraceProfiler::DumpBuffer(uint32_t thread_id,
   buffer[0] = kEntryHeaderV2;
   Append4LE(buffer + 1, thread_id);
   Append3LE(buffer + 5, num_records);
-  return curr_buffer_ptr;
+  return curr_buffer_ptr - buffer;
 }
 
 void TraceProfiler::Dump(int fd) {
@@ -183,15 +414,8 @@ void TraceProfiler::Dump(int fd) {
   }
 
   std::unique_ptr<File> trace_file(new File(fd, /*check_usage=*/true));
-  Dump(std::move(trace_file));
-}
-
-std::string TraceProfiler::GetMethodInfoLine(ArtMethod* method) {
-  return StringPrintf("%s\t%s\t%s\t%s\n",
-                      PrettyDescriptor(method->GetDeclaringClassDescriptor()).c_str(),
-                      method->GetName(),
-                      method->GetSignature().ToString().c_str(),
-                      method->GetDeclaringClassSourceFile());
+  std::ostringstream os;
+  Dump(std::move(trace_file), os);
 }
 
 void TraceProfiler::Dump(const char* filename) {
@@ -206,102 +430,306 @@ void TraceProfiler::Dump(const char* filename) {
     return;
   }
 
-  Dump(std::move(trace_file));
+  std::ostringstream os;
+  Dump(std::move(trace_file), os);
 }
 
-void TraceProfiler::Dump(std::unique_ptr<File>&& trace_file) {
+void TraceProfiler::Dump(std::unique_ptr<File>&& trace_file, std::ostringstream& os) {
   Thread* self = Thread::Current();
-  std::unordered_set<ArtMethod*> traced_methods;
-  std::unordered_map<size_t, std::string> traced_threads;
-  MutexLock mu(self, *Locks::trace_lock_);
-  if (!profile_in_progress_) {
-    LOG(ERROR) << "No Profile in progress. Nothing to dump.";
+  Runtime* runtime = Runtime::Current();
+
+  size_t threads_running_checkpoint = 0;
+  std::unique_ptr<TraceDumpCheckpoint> checkpoint;
+  {
+    MutexLock mu(self, *Locks::trace_lock_);
+    if (!profile_in_progress_ || trace_data_->IsTraceDumpInProgress()) {
+      if (trace_file != nullptr && !trace_file->Close()) {
+        PLOG(WARNING) << "Failed to close file.";
+      }
+      return;
+    }
+
+    trace_data_->SetTraceDumpInProgress();
+
+    // Collect long running methods from all the threads;
+    checkpoint.reset(new TraceDumpCheckpoint(trace_data_, trace_file));
+    threads_running_checkpoint = runtime->GetThreadList()->RunCheckpoint(checkpoint.get());
+  }
+
+  // Wait for all threads to dump their data.
+  if (threads_running_checkpoint != 0) {
+    checkpoint->WaitForThreadsToRunThroughCheckpoint(threads_running_checkpoint);
+  }
+  checkpoint->FinishTraceDump(os);
+
+  if (trace_file != nullptr) {
+    std::string info = os.str();
+    if (!trace_file->WriteFully(info.c_str(), info.length())) {
+      PLOG(WARNING) << "Failed writing information to file";
+    }
+
+    if (!trace_file->Close()) {
+      PLOG(WARNING) << "Failed to close file.";
+    }
+  }
+}
+
+void TraceProfiler::ReleaseThreadBuffer(Thread* self) {
+  if (!IsTraceProfileInProgress()) {
     return;
   }
+  // TODO(mythria): Maybe it's good to cache these and dump them when requested. For now just
+  // relese the buffer when a thread is exiting.
+  auto buffer = self->GetMethodTraceBuffer();
+  delete[] buffer;
+  self->SetMethodTraceBuffer(nullptr, 0);
+}
 
-  uint8_t* buffer_ptr = new uint8_t[kBufSizeForEncodedData];
-  uint8_t* curr_buffer_ptr = buffer_ptr;
+bool TraceProfiler::IsTraceProfileInProgress() {
+  return profile_in_progress_;
+}
+
+void TraceProfiler::StartTraceLongRunningMethods(uint64_t trace_duration_ns) {
+  TraceProfiler::Start(LowOverheadTraceType::kLongRunningMethods, trace_duration_ns);
+}
+
+void TraceProfiler::TraceTimeElapsed() {
+  MutexLock mu(Thread::Current(), *Locks::trace_lock_);
+  DCHECK_IMPLIES(!profile_in_progress_, trace_data_ != nullptr);
+  if (!profile_in_progress_ || trace_data_->GetTraceEndTime() > NanoTime()) {
+    // The end duration was extended by another start, so just ignore this task.
+    return;
+  }
+  TraceProfiler::StopLocked();
+}
 
-  // Add a header for the trace: 4-bits of magic value and 2-bits for the version.
-  Append4LE(curr_buffer_ptr, kProfileMagicValue);
-  Append2LE(curr_buffer_ptr + 4, /*trace_version=*/ 1);
-  curr_buffer_ptr += 6;
+size_t TraceProfiler::DumpLongRunningMethodBuffer(uint32_t thread_id,
+                                                  uintptr_t* method_trace_entries,
+                                                  uintptr_t* end_trace_entries,
+                                                  uint8_t* buffer,
+                                                  std::unordered_set<ArtMethod*>& methods) {
+  // Encode header at the end once we compute the number of records.
+  uint8_t* curr_buffer_ptr = buffer + kAlwaysOnTraceHeaderSize;
 
-  ScopedSuspendAll ssa(__FUNCTION__);
-  MutexLock tl(self, *Locks::thread_list_lock_);
-  for (Thread* thread : Runtime::Current()->GetThreadList()->GetList()) {
-    auto method_trace_entries = thread->GetMethodTraceBuffer();
-    if (method_trace_entries == nullptr) {
+  int num_records = 0;
+  uintptr_t prev_time_action_encoding = 0;
+  uintptr_t prev_method_ptr = 0;
+  int64_t end_index = end_trace_entries - method_trace_entries;
+  for (int64_t i = kAlwaysOnTraceBufSize; i > end_index;) {
+    uintptr_t event = method_trace_entries[--i];
+    if (event == 0x1) {
+      // This is a placeholder event. Ignore this event.
       continue;
     }
 
-    std::string thread_name;
-    thread->GetThreadName(thread_name);
-    traced_threads.emplace(thread->GetThreadId(), thread_name);
+    bool is_method_exit = event & 0x1;
+    uint64_t event_time;
+    uintptr_t method_ptr;
+    if (is_method_exit) {
+      // Method exit. We only have timestamp here.
+      event_time = TimestampCounter::GetNanoTime(event & ~0x1);
+    } else {
+      // method entry
+      method_ptr = event;
+      event_time = TimestampCounter::GetNanoTime(method_trace_entries[--i] & ~0x1);
+    }
 
-    size_t offset = curr_buffer_ptr - buffer_ptr;
-    if (offset >= kMinBufSizeForEncodedData) {
-      if (!trace_file->WriteFully(buffer_ptr, offset)) {
-        PLOG(WARNING) << "Failed streaming a tracing event.";
-      }
-      curr_buffer_ptr = buffer_ptr;
+    uint64_t time_action_encoding = event_time << 1;
+    if (is_method_exit) {
+      time_action_encoding |= 1;
     }
-    curr_buffer_ptr =
-        DumpBuffer(thread->GetTid(), method_trace_entries, curr_buffer_ptr, traced_methods);
-    // Reset the buffer and continue profiling. We need to set the buffer to
-    // zeroes, since we use a circular buffer and detect empty entries by
-    // checking for zeroes.
-    memset(method_trace_entries, 0, kAlwaysOnTraceBufSize * sizeof(uintptr_t));
-    // Reset the current pointer.
-    thread->SetMethodTraceBufferCurrentEntry(kAlwaysOnTraceBufSize);
-  }
-
-  // Write any remaining data to file and close the file.
-  if (curr_buffer_ptr != buffer_ptr) {
-    if (!trace_file->WriteFully(buffer_ptr, curr_buffer_ptr - buffer_ptr)) {
-      PLOG(WARNING) << "Failed streaming a tracing event.";
+    int64_t time_action_diff = time_action_encoding - prev_time_action_encoding;
+    curr_buffer_ptr = EncodeSignedLeb128(curr_buffer_ptr, time_action_diff);
+    prev_time_action_encoding = time_action_encoding;
+
+    if (!is_method_exit) {
+      int64_t method_diff = method_ptr - prev_method_ptr;
+      ArtMethod* method = reinterpret_cast<ArtMethod*>(method_ptr);
+      methods.insert(method);
+      prev_method_ptr = method_ptr;
+      curr_buffer_ptr = EncodeSignedLeb128(curr_buffer_ptr, method_diff);
     }
+    num_records++;
   }
 
-  std::ostringstream os;
-  // Dump data about thread information.
-  os << "\n*threads\n";
-  for (const auto& it : traced_threads) {
-    os << it.first << "\t" << it.second << "\n";
-  }
+  // Fill in header information:
+  // 1 byte of header identifier
+  // 4 bytes of thread_id
+  // 3 bytes of number of records
+  // 4 bytes the size of the data
+  buffer[0] = kEntryHeaderV2;
+  Append4LE(buffer + 1, thread_id);
+  Append3LE(buffer + 5, num_records);
+  size_t size = curr_buffer_ptr - buffer;
+  Append4LE(buffer + 8, size - kAlwaysOnTraceHeaderSize);
+  return curr_buffer_ptr - buffer;
+}
 
-  // Dump data about method information.
-  os << "*methods\n";
-  for (ArtMethod* method : traced_methods) {
-    uint64_t method_id = reinterpret_cast<uint64_t>(method);
-    os << method_id << "\t" << GetMethodInfoLine(method);
+void TraceProfiler::FlushBufferAndRecordTraceEvent(ArtMethod* method,
+                                                   Thread* thread,
+                                                   bool is_entry) {
+  uint64_t timestamp = TimestampCounter::GetTimestamp();
+  std::unordered_set<ArtMethod*> traced_methods;
+  uintptr_t* method_trace_entries = thread->GetMethodTraceBuffer();
+  DCHECK(method_trace_entries != nullptr);
+  uintptr_t** method_trace_curr_ptr = thread->GetTraceBufferCurrEntryPtr();
+
+  // Find the last method exit event. We can flush all the entries before this event. We cannot
+  // flush remaining events because we haven't determined if they are long running or not.
+  uintptr_t* processed_events_ptr = nullptr;
+  for (uintptr_t* ptr = *method_trace_curr_ptr;
+       ptr < method_trace_entries + kAlwaysOnTraceBufSize;) {
+    if (*ptr & 0x1) {
+      // Method exit. We need to keep events until (including this method exit) here.
+      processed_events_ptr = ptr + 1;
+      break;
+    }
+    ptr += 2;
   }
 
-  os << "*end";
+  size_t num_occupied_entries = (processed_events_ptr - *method_trace_curr_ptr);
+  size_t index = kAlwaysOnTraceBufSize;
+
+  std::unique_ptr<uint8_t[]> buffer_ptr(new uint8_t[kBufSizeForEncodedData]);
+  size_t num_bytes;
+  if (num_occupied_entries > kMaxEntriesAfterFlush) {
+    // If we don't have sufficient space just record a placeholder exit and flush all the existing
+    // events. We have accurate timestamps to filter out these events in a post-processing step.
+    // This would happen only when we have very deeply (~1024) nested code.
+    num_bytes = DumpLongRunningMethodBuffer(thread->GetTid(),
+                                            method_trace_entries,
+                                            *method_trace_curr_ptr,
+                                            buffer_ptr.get(),
+                                            traced_methods);
+
+    // Encode a placeholder exit event. This will be ignored when dumping the methods.
+    method_trace_entries[--index] = 0x1;
+  } else {
+    // Flush all the entries till the method exit event.
+    num_bytes = DumpLongRunningMethodBuffer(thread->GetTid(),
+                                            method_trace_entries,
+                                            processed_events_ptr,
+                                            buffer_ptr.get(),
+                                            traced_methods);
+
+    // Move the remaining events to the start of the buffer.
+    for (uintptr_t* ptr = processed_events_ptr - 1; ptr >= *method_trace_curr_ptr; ptr--) {
+      method_trace_entries[--index] = *ptr;
+    }
+  }
 
-  std::string info = os.str();
-  if (!trace_file->WriteFully(info.c_str(), info.length())) {
-    PLOG(WARNING) << "Failed writing information to file";
+  // Record new entry
+  if (is_entry) {
+    method_trace_entries[--index] = reinterpret_cast<uintptr_t>(method);
+    method_trace_entries[--index] = timestamp & ~1;
+  } else {
+    if (method_trace_entries[index] & 0x1) {
+      method_trace_entries[--index] = timestamp | 1;
+    } else {
+      size_t prev_timestamp = method_trace_entries[index];
+      if (timestamp - prev_timestamp < kLongRunningMethodThreshold) {
+        index += 2;
+        DCHECK_LT(index, kAlwaysOnTraceBufSize);
+      } else {
+        method_trace_entries[--index] = timestamp | 1;
+      }
+    }
   }
+  *method_trace_curr_ptr = method_trace_entries + index;
 
-  if (!trace_file->Close()) {
-    PLOG(WARNING) << "Failed to close file.";
+  MutexLock mu(Thread::Current(), *Locks::trace_lock_);
+  trace_data_->AppendToLongRunningMethods(buffer_ptr.get(), num_bytes);
+  trace_data_->AddTracedMethods(traced_methods);
+  trace_data_->AddTracedThread(thread);
+}
+
+std::string TraceProfiler::GetLongRunningMethodsString() {
+  if (!art_flags::always_enable_profile_code()) {
+    return std::string();
   }
+
+  std::ostringstream os;
+  Dump(std::unique_ptr<File>(), os);
+  return os.str();
 }
 
-void TraceProfiler::ReleaseThreadBuffer(Thread* self) {
-  if (!IsTraceProfileInProgress()) {
-    return;
+void TraceDumpCheckpoint::Run(Thread* thread) {
+  auto method_trace_entries = thread->GetMethodTraceBuffer();
+  if (method_trace_entries != nullptr) {
+    std::unordered_set<ArtMethod*> traced_methods;
+    if (trace_data_->GetTraceType() == LowOverheadTraceType::kLongRunningMethods) {
+      uintptr_t* method_trace_curr_ptr = *(thread->GetTraceBufferCurrEntryPtr());
+      std::unique_ptr<uint8_t[]> buffer_ptr(new uint8_t[kBufSizeForEncodedData]);
+      size_t num_bytes = TraceProfiler::DumpLongRunningMethodBuffer(thread->GetTid(),
+                                                                    method_trace_entries,
+                                                                    method_trace_curr_ptr,
+                                                                    buffer_ptr.get(),
+                                                                    traced_methods);
+      MutexLock mu(Thread::Current(), trace_file_lock_);
+      if (trace_file_ != nullptr) {
+        if (!trace_file_->WriteFully(buffer_ptr.get(), num_bytes)) {
+          PLOG(WARNING) << "Failed streaming a tracing event.";
+        }
+      } else {
+        trace_data_->AppendToLongRunningMethods(buffer_ptr.get(), num_bytes);
+      }
+    } else {
+      std::unique_ptr<uint8_t> buffer_ptr(new uint8_t[kBufSizeForEncodedData]);
+      size_t num_bytes = TraceProfiler::DumpBuffer(
+          thread->GetTid(), method_trace_entries, buffer_ptr.get(), traced_methods);
+      MutexLock mu(Thread::Current(), trace_file_lock_);
+      if (!trace_file_->WriteFully(buffer_ptr.get(), num_bytes)) {
+        PLOG(WARNING) << "Failed streaming a tracing event.";
+      }
+    }
+    trace_data_->AddTracedThread(thread);
+    trace_data_->AddTracedMethods(traced_methods);
   }
-  // TODO(mythria): Maybe it's good to cache these and dump them when requested. For now just
-  // relese the buffer when a thread is exiting.
-  auto buffer = self->GetMethodTraceBuffer();
-  delete[] buffer;
-  self->SetMethodTraceBuffer(nullptr, 0);
+  barrier_.Pass(Thread::Current());
 }
 
-bool TraceProfiler::IsTraceProfileInProgress() {
-  return profile_in_progress_;
+void TraceDumpCheckpoint::WaitForThreadsToRunThroughCheckpoint(size_t threads_running_checkpoint) {
+  Thread* self = Thread::Current();
+  ScopedThreadStateChange tsc(self, ThreadState::kWaitingForCheckPointsToRun);
+  barrier_.Increment(self, threads_running_checkpoint);
+}
+
+void TraceDumpCheckpoint::FinishTraceDump(std::ostringstream& os) {
+  // Dump all the data.
+  trace_data_->DumpData(os);
+
+  // Any trace stop requests will be blocked while a dump is in progress. So
+  // broadcast the completion condition for any waiting requests.
+  MutexLock mu(Thread::Current(), *Locks::trace_lock_);
+  trace_data_->SignalTraceDumpComplete();
+}
+
+void TraceData::DumpData(std::ostringstream& os) {
+  std::unordered_set<ArtMethod*> methods;
+  std::unordered_map<size_t, std::string> threads;
+  {
+    // We cannot dump method information while holding trace_lock_, since we have to also
+    // acquire a mutator lock. Take a snapshot of thread and method information.
+    MutexLock mu(Thread::Current(), trace_data_lock_);
+    if (curr_buffer_ != nullptr) {
+      for (size_t i = 0; i < overflow_buffers_.size(); i++) {
+        os.write(reinterpret_cast<char*>(overflow_buffers_[i].get()), kBufSizeForEncodedData);
+      }
+
+      os.write(reinterpret_cast<char*>(curr_buffer_.get()), curr_index_);
+    }
+
+    methods = traced_methods_;
+    if (trace_type_ != LowOverheadTraceType::kLongRunningMethods) {
+      threads = traced_threads_;
+    }
+  }
+
+  // Dump the information about traced_methods and threads
+  {
+    ScopedObjectAccess soa(Thread::Current());
+    DumpThreadMethodInfo(threads, methods, os);
+  }
 }
 
 }  // namespace art
diff --git a/runtime/trace_profile.h b/runtime/trace_profile.h
index 694eb7ac99..cd6fdc01ea 100644
--- a/runtime/trace_profile.h
+++ b/runtime/trace_profile.h
@@ -22,6 +22,8 @@
 #include "base/locks.h"
 #include "base/macros.h"
 #include "base/os.h"
+#include "thread.h"
+#include "thread_pool.h"
 
 namespace art HIDDEN {
 
@@ -31,6 +33,136 @@ class ArtMethod;
 // entries required in the buffer.
 static constexpr size_t kAlwaysOnTraceBufSize = 2048;
 
+// The typical frequency at which the timestamp counters are updated is 24576000.
+// 2^23 (8388608) corresponds to about 341ms at that frequency.
+static constexpr size_t kLongRunningMethodThreshold = 1 << 23;
+
+enum class LowOverheadTraceType {
+  kLongRunningMethods,
+  kAllMethods,
+  kNone
+};
+
+class TraceData {
+ public:
+  explicit TraceData(LowOverheadTraceType trace_type)
+      : curr_buffer_(nullptr),
+        curr_index_(0),
+        trace_type_(trace_type),
+        trace_end_time_(0),
+        trace_dump_in_progress_(false),
+        trace_dump_condition_("trace dump condition", *Locks::trace_lock_),
+        trace_data_lock_("Trace Data lock", LockLevel::kGenericBottomLock) {}
+
+  LowOverheadTraceType GetTraceType() const {
+    return trace_type_;
+  }
+
+  uint64_t GetTraceEndTime() const {
+    return trace_end_time_;
+  }
+
+  void SetTraceEndTime(uint64_t end_time) {
+    trace_end_time_ = end_time;
+  }
+
+  // Dumps events collected in the buffers and the information about threads and methods into the
+  // output stream.
+  void DumpData(std::ostringstream& os);
+
+  void AppendToLongRunningMethods(const uint8_t* buffer, size_t size);
+
+  void AddTracedMethods(std::unordered_set<ArtMethod*>& methods) {
+    MutexLock mu(Thread::Current(), trace_data_lock_);
+    traced_methods_.merge(methods);
+  }
+
+  void AddTracedMethod(ArtMethod* method) {
+    MutexLock mu(Thread::Current(), trace_data_lock_);
+    traced_methods_.insert(method);
+  }
+
+  void AddTracedThread(Thread* thread);
+
+  // If there is no trace dump in progress this returns immediately. Otherwise
+  // it waits on a condition variable waiting for the trace dump to finish.
+  void MaybeWaitForTraceDumpToFinish() REQUIRES(Locks::trace_lock_);
+
+  // Called when a trace dump is finished to notify any waiting requests. This
+  // also resets the trace_dump_in_progress_ to false.
+  void SignalTraceDumpComplete() REQUIRES(Locks::trace_lock_);
+
+  void SetTraceDumpInProgress() REQUIRES(Locks::trace_lock_) {
+    trace_dump_in_progress_ = true;
+  }
+
+  bool IsTraceDumpInProgress() const REQUIRES(Locks::trace_lock_) {
+    return trace_dump_in_progress_;
+  }
+
+ private:
+  // This is used to hold the long running methods when the per-thread buffer overflows.
+  std::unique_ptr<uint8_t> curr_buffer_ GUARDED_BY(trace_data_lock_);
+
+  // The index of the next free space in the curr_buffer_
+  size_t curr_index_ GUARDED_BY(trace_data_lock_);
+
+  // When the curr_buffer_ becomes full, we store it in this list and allocate a new buffer.
+  std::vector<std::unique_ptr<uint8_t>> overflow_buffers_ GUARDED_BY(trace_data_lock_);
+
+  LowOverheadTraceType trace_type_;
+
+  uint64_t trace_end_time_;
+
+  // These hold the methods and threads see so far. These are used to generate information about
+  // the methods and threads.
+  std::unordered_set<ArtMethod*> traced_methods_ GUARDED_BY(trace_data_lock_);
+
+  // Threads might exit before we dump the data, so record thread id and name when we see a new
+  // thread.
+  std::unordered_map<size_t, std::string> traced_threads_ GUARDED_BY(trace_data_lock_);
+
+  // This specifies if a trace dump is in progress. We release the trace_lock_
+  // when waiting for the checkpoints to finish. We shouldn't delete trace data
+  // when a dump is in progress. trace_dump_in_progress_ and
+  // trace_dump_condition_ are used to make sure we wait for any in progress
+  // trace dumps to finish before deleting the trace data.
+  bool trace_dump_in_progress_ GUARDED_BY(Locks::trace_lock_);
+  ConditionVariable trace_dump_condition_ GUARDED_BY(Locks::trace_lock_);
+
+  // Lock to synchronize access to traced_methods_, traced_threads_ and curr_buffer_ which
+  // can be accessed simultaneously by multiple threads when running TraceDumpCheckpoint.
+  Mutex trace_data_lock_;
+};
+
+class TraceDumpCheckpoint final : public Closure {
+ public:
+  TraceDumpCheckpoint(TraceData* trace_data, const std::unique_ptr<File>& trace_file)
+      : barrier_(0),
+        trace_data_(trace_data),
+        trace_file_(trace_file),
+        trace_file_lock_("trace file lock", LockLevel::kGenericBottomLock) {}
+
+  void Run(Thread* thread) override REQUIRES_SHARED(Locks::mutator_lock_);
+  void WaitForThreadsToRunThroughCheckpoint(size_t threads_running_checkpoint);
+  void FinishTraceDump(std::ostringstream& os);
+
+ private:
+  // The barrier to be passed through and for the requestor to wait upon.
+  Barrier barrier_;
+
+  // Trace data to record the data from each thread.
+  TraceData* trace_data_;
+
+  // Trace file to flush the data. If the trace_file_ is empty then the data is recorded in the
+  // trace_data_.
+  const std::unique_ptr<File>& trace_file_ GUARDED_BY(trace_file_lock_);
+
+  // Lock to synchronize access to trace_file_. We need to write the data of
+  // each thread as a block so we hold a lock while flushing the data.
+  Mutex trace_file_lock_;
+};
+
 // This class implements low-overhead tracing. This feature is available only when
 // always_enable_profile_code is enabled which is a build time flag defined in
 // build/flags/art-flags.aconfig. When this flag is enabled, AOT and JITed code can record events
@@ -42,6 +174,10 @@ class TraceProfiler {
   // Starts profiling by allocating a per-thread buffer for all the threads.
   static void Start();
 
+  // Starts recording long running methods. A long running method means any
+  // method that executes for more than kLongRunningMethodDuration.
+  static void StartTraceLongRunningMethods(uint64_t trace_duration_ns);
+
   // Releases all the buffers.
   static void Stop();
 
@@ -49,6 +185,10 @@ class TraceProfiler {
   static void Dump(int fd);
   static void Dump(const char* trace_filename);
 
+  // Get the long running methods as a string. This is used in the sigquit handler to record
+  // information about long running methods.
+  static std::string GetLongRunningMethodsString();
+
   // Called when thread is exiting to release the allocated buffer.
   static void ReleaseThreadBuffer(Thread* self) REQUIRES(Locks::trace_lock_);
 
@@ -57,23 +197,52 @@ class TraceProfiler {
   // Allocates a buffer for the specified thread.
   static void AllocateBuffer(Thread* thread);
 
+  // Used to flush the long running method buffer when it is full. This method flushes all methods
+  // that have already seen an exit and records them into a string. If we don't have sufficient free
+  // entries after this processing (for example: if we have a really deep call stack) then we record
+  // a placeholder method exit event and flush all events.
+  static void FlushBufferAndRecordTraceEvent(ArtMethod* method, Thread* thread, bool is_entry);
+
+  static LowOverheadTraceType GetTraceType();
+
+  // Callback that is run when the specified duration for the long running trace has elapsed. If the
+  // trace is still running then tracing is stopped and all buffers are released. If the trace
+  // has already stopped then this request is ignored.
+  static void TraceTimeElapsed();
+
  private:
-  // Dumps the events from all threads into the trace_file.
-  static void Dump(std::unique_ptr<File>&& trace_file);
+  // Starts tracing.
+  static void Start(LowOverheadTraceType trace_type, uint64_t trace_duration_ns);
+
+  // Dumps the tracing data into the specified trace_file
+  static void Dump(std::unique_ptr<File>&& trace_file, std::ostringstream& os);
+
+  // Stops tracing.
+  static void StopLocked() REQUIRES(Locks::trace_lock_);
 
   // This method goes over all the events in the thread_buffer and stores the encoded event in the
-  // buffer. It returns the pointer to the next free entry in the buffer.
+  // buffer. It returns the number of bytes written into the buffer.
   // This also records the ArtMethods from the events in the thread_buffer in a set. This set is
   // used to dump the information about the methods once buffers from all threads have been
   // processed.
-  static uint8_t* DumpBuffer(uint32_t thread_id,
-                             uintptr_t* thread_buffer,
-                             uint8_t* buffer /* out */,
-                             std::unordered_set<ArtMethod*>& methods /* out */);
-
-  static std::string GetMethodInfoLine(ArtMethod* method) REQUIRES(Locks::mutator_lock_);
+  static size_t DumpBuffer(uint32_t thread_id,
+                           uintptr_t* thread_buffer,
+                           uint8_t* buffer /* out */,
+                           std::unordered_set<ArtMethod*>& methods /* out */);
+
+  // Dumps all the trace events from the thread into the buffer. Also records the ArtMethods from
+  // the events which is then used to record information about these methods.
+  static size_t DumpLongRunningMethodBuffer(uint32_t thread_id,
+                                            uintptr_t* method_trace_entries,
+                                            uintptr_t* end_trace_entries,
+                                            uint8_t* buffer,
+                                            std::unordered_set<ArtMethod*>& methods);
 
   static bool profile_in_progress_ GUARDED_BY(Locks::trace_lock_);
+
+  static TraceData* trace_data_ GUARDED_BY(Locks::trace_lock_);
+
+  friend class TraceDumpCheckpoint;
   DISALLOW_COPY_AND_ASSIGN(TraceProfiler);
 };
 
diff --git a/runtime/vdex_file.cc b/runtime/vdex_file.cc
index db2a976c22..64ce9e84e1 100644
--- a/runtime/vdex_file.cc
+++ b/runtime/vdex_file.cc
@@ -18,16 +18,16 @@
 
 #include <sys/mman.h>  // For the PROT_* and MAP_* constants.
 #include <sys/stat.h>  // for mkdir()
+#include <sys/types.h>
 
 #include <memory>
 #include <unordered_set>
 
-#include <android-base/logging.h>
-#include <android-base/stringprintf.h>
-#include <log/log.h>
-
+#include "android-base/logging.h"
+#include "android-base/stringprintf.h"
 #include "base/bit_utils.h"
 #include "base/leb128.h"
+#include "base/macros.h"
 #include "base/stl_util.h"
 #include "base/systrace.h"
 #include "base/unix_file/fd_file.h"
@@ -39,8 +39,9 @@
 #include "dex/dex_file_loader.h"
 #include "gc/heap.h"
 #include "gc/space/image_space.h"
-#include "mirror/class-inl.h"
 #include "handle_scope-inl.h"
+#include "log/log.h"
+#include "mirror/class-inl.h"
 #include "runtime.h"
 #include "verifier/verifier_deps.h"
 
@@ -68,7 +69,6 @@ std::unique_ptr<VdexFile> VdexFile::OpenAtAddress(uint8_t* mmap_addr,
                                                   size_t mmap_size,
                                                   bool mmap_reuse,
                                                   const std::string& vdex_filename,
-                                                  bool writable,
                                                   bool low_4gb,
                                                   std::string* error_msg) {
   ScopedTrace trace(("VdexFile::OpenAtAddress " + vdex_filename).c_str());
@@ -77,15 +77,9 @@ std::unique_ptr<VdexFile> VdexFile::OpenAtAddress(uint8_t* mmap_addr,
     return nullptr;
   }
 
-  std::unique_ptr<File> vdex_file;
-  if (writable) {
-    vdex_file.reset(OS::OpenFileReadWrite(vdex_filename.c_str()));
-  } else {
-    vdex_file.reset(OS::OpenFileForReading(vdex_filename.c_str()));
-  }
+  std::unique_ptr<File> vdex_file(OS::OpenFileForReading(vdex_filename.c_str()));
   if (vdex_file == nullptr) {
-    *error_msg = "Could not open file " + vdex_filename +
-                 (writable ? " for read/write" : "for reading");
+    *error_msg = "Could not open file for reading";
     return nullptr;
   }
 
@@ -99,9 +93,9 @@ std::unique_ptr<VdexFile> VdexFile::OpenAtAddress(uint8_t* mmap_addr,
                        mmap_size,
                        mmap_reuse,
                        vdex_file->Fd(),
+                       /*start=*/0,
                        vdex_length,
                        vdex_filename,
-                       writable,
                        low_4gb,
                        error_msg);
 }
@@ -110,9 +104,9 @@ std::unique_ptr<VdexFile> VdexFile::OpenAtAddress(uint8_t* mmap_addr,
                                                   size_t mmap_size,
                                                   bool mmap_reuse,
                                                   int file_fd,
+                                                  off_t start,
                                                   size_t vdex_length,
                                                   const std::string& vdex_filename,
-                                                  bool writable,
                                                   bool low_4gb,
                                                   std::string* error_msg) {
   if (mmap_addr != nullptr && mmap_size < vdex_length) {
@@ -123,18 +117,17 @@ std::unique_ptr<VdexFile> VdexFile::OpenAtAddress(uint8_t* mmap_addr,
   }
   CHECK_IMPLIES(mmap_reuse, mmap_addr != nullptr);
   // Start as PROT_WRITE so we can mprotect back to it if we want to.
-  MemMap mmap = MemMap::MapFileAtAddress(
-      mmap_addr,
-      vdex_length,
-      PROT_READ | PROT_WRITE,
-      writable ? MAP_SHARED : MAP_PRIVATE,
-      file_fd,
-      /* start= */ 0u,
-      low_4gb,
-      vdex_filename.c_str(),
-      mmap_reuse,
-      /* reservation= */ nullptr,
-      error_msg);
+  MemMap mmap = MemMap::MapFileAtAddress(mmap_addr,
+                                         vdex_length,
+                                         PROT_READ | PROT_WRITE,
+                                         MAP_PRIVATE,
+                                         file_fd,
+                                         start,
+                                         low_4gb,
+                                         vdex_filename.c_str(),
+                                         mmap_reuse,
+                                         /*reservation=*/nullptr,
+                                         error_msg);
   if (!mmap.IsValid()) {
     *error_msg = "Failed to mmap file " + vdex_filename + " : " + *error_msg;
     return nullptr;
@@ -150,36 +143,84 @@ std::unique_ptr<VdexFile> VdexFile::OpenAtAddress(uint8_t* mmap_addr,
 }
 
 std::unique_ptr<VdexFile> VdexFile::OpenFromDm(const std::string& filename,
-                                               const ZipArchive& archive) {
-  std::string error_msg;
-  std::unique_ptr<ZipEntry> zip_entry(archive.Find(VdexFile::kVdexNameInDmFile, &error_msg));
+                                               const ZipArchive& archive,
+                                               std::string* error_msg) {
+  std::unique_ptr<ZipEntry> zip_entry(archive.Find(VdexFile::kVdexNameInDmFile, error_msg));
   if (zip_entry == nullptr) {
-    LOG(INFO) << "No " << VdexFile::kVdexNameInDmFile << " file in DexMetadata archive. "
-              << "Not doing fast verification.";
+    *error_msg = ART_FORMAT("No {} file in DexMetadata archive. Not doing fast verification: {}",
+                            VdexFile::kVdexNameInDmFile,
+                            *error_msg);
     return nullptr;
   }
   MemMap input_file = zip_entry->MapDirectlyOrExtract(
-      filename.c_str(),
-      VdexFile::kVdexNameInDmFile,
-      &error_msg,
-      alignof(VdexFile));
+      filename.c_str(), VdexFile::kVdexNameInDmFile, error_msg, alignof(VdexFile));
   if (!input_file.IsValid()) {
-    LOG(WARNING) << "Could not open vdex file in DexMetadata archive: " << error_msg;
+    *error_msg = "Could not open vdex file in DexMetadata archive: " + *error_msg;
     return nullptr;
   }
   std::unique_ptr<VdexFile> vdex_file = std::make_unique<VdexFile>(std::move(input_file));
   if (!vdex_file->IsValid()) {
-    LOG(WARNING) << "The dex metadata .vdex is not valid. Ignoring it.";
+    *error_msg = "The dex metadata .vdex is not valid. Ignoring it.";
     return nullptr;
   }
   if (vdex_file->HasDexSection()) {
-    LOG(ERROR) << "The dex metadata is not allowed to contain dex files";
+    *error_msg = "The dex metadata is not allowed to contain dex files";
     android_errorWriteLog(0x534e4554, "178055795");  // Report to SafetyNet.
     return nullptr;
   }
   return vdex_file;
 }
 
+std::unique_ptr<VdexFile> VdexFile::OpenFromDm(const std::string& filename,
+                                               uint8_t* vdex_begin_,
+                                               uint8_t* vdex_end_,
+                                               std::string* error_msg) {
+  std::string vdex_filename = filename + OatFile::kZipSeparator + kVdexNameInDmFile;
+  // This overload of `OpenFromDm` is for loading both odex and vdex. We need to map the vdex at the
+  // address required by the odex, so the vdex must be uncompressed and page-aligned.
+  // To load vdex only, use the other overload.
+  FileWithRange vdex_file_with_range = OS::OpenFileDirectlyOrFromZip(
+      vdex_filename, OatFile::kZipSeparator, /*alignment=*/MemMap::GetPageSize(), error_msg);
+  if (vdex_file_with_range.file == nullptr) {
+    return nullptr;
+  }
+  std::unique_ptr<VdexFile> vdex_file =
+      VdexFile::OpenAtAddress(vdex_begin_,
+                              vdex_end_ - vdex_begin_,
+                              /*mmap_reuse=*/vdex_begin_ != nullptr,
+                              vdex_file_with_range.file->Fd(),
+                              vdex_file_with_range.start,
+                              vdex_file_with_range.length,
+                              vdex_filename,
+                              /*low_4gb=*/false,
+                              error_msg);
+  if (vdex_file == nullptr) {
+    return nullptr;
+  }
+  if (vdex_file->HasDexSection()) {
+    *error_msg = "The dex metadata is not allowed to contain dex files";
+    return nullptr;
+  }
+  return vdex_file;
+}
+
+bool VdexFile::IsValid() const {
+  if (mmap_.Size() < sizeof(VdexFileHeader) || !GetVdexFileHeader().IsValid()) {
+    return false;
+  }
+
+  // Invalidate vdex files that contain dex files in the no longer supported
+  // compact dex format. Revert this whenever the vdex version is bumped.
+  size_t i = 0;
+  for (const uint8_t* dex_file_start = GetNextDexFileData(nullptr, i); dex_file_start != nullptr;
+       dex_file_start = GetNextDexFileData(dex_file_start, ++i)) {
+    if (!DexFileLoader::IsMagicValid(dex_file_start)) {
+      return false;
+    }
+  }
+  return true;
+}
+
 const uint8_t* VdexFile::GetNextDexFileData(const uint8_t* cursor, uint32_t dex_file_index) const {
   DCHECK(cursor == nullptr || (cursor > Begin() && cursor <= End()));
   if (cursor == nullptr) {
@@ -384,12 +425,6 @@ bool VdexFile::MatchesDexFileChecksums(const std::vector<const DexFile::Header*>
   return true;
 }
 
-bool VdexFile::HasOnlyStandardDexFiles() const {
-  // All are the same so it's enough to check the first one.
-  const uint8_t* dex_file_start = GetNextDexFileData(nullptr, 0);
-  return dex_file_start == nullptr || StandardDexFile::IsMagicValid(dex_file_start);
-}
-
 static ObjPtr<mirror::Class> FindClassAndClearException(ClassLinker* class_linker,
                                                         Thread* self,
                                                         const char* descriptor,
@@ -528,6 +563,7 @@ ClassStatus VdexFile::ComputeClassStatus(Thread* self, Handle<mirror::Class> cls
         class_linker, self, source_desc, source_desc_length, class_loader));
 
     if (destination == nullptr || source == nullptr) {
+      cls->SetHasTypeChecksFailure();
       // The interpreter / compiler can handle a missing class.
       continue;
     }
diff --git a/runtime/vdex_file.h b/runtime/vdex_file.h
index 4a1665b34d..f192b7f466 100644
--- a/runtime/vdex_file.h
+++ b/runtime/vdex_file.h
@@ -18,6 +18,8 @@
 #define ART_RUNTIME_VDEX_FILE_H_
 
 #include <stdint.h>
+#include <sys/types.h>
+
 #include <string>
 
 #include "base/array_ref.h"
@@ -122,6 +124,7 @@ class VdexFile {
     static constexpr uint8_t kVdexMagic[] = { 'v', 'd', 'e', 'x' };
 
     // The format version of the verifier deps header and the verifier deps.
+    // TODO: Revert the dex header checks in VdexFile::IsValid when this is bumped.
     // Last update: Introduce vdex sections.
     static constexpr uint8_t kVdexVersion[] = { '0', '2', '7', '\0' };
 
@@ -191,7 +194,6 @@ class VdexFile {
                                                         size_t mmap_size,
                                                         bool mmap_reuse,
                                                         const std::string& vdex_filename,
-                                                        bool writable,
                                                         bool low_4gb,
                                                         std::string* error_msg);
 
@@ -201,46 +203,37 @@ class VdexFile {
                                                         size_t mmap_size,
                                                         bool mmap_reuse,
                                                         int file_fd,
+                                                        off_t start,
                                                         size_t vdex_length,
                                                         const std::string& vdex_filename,
-                                                        bool writable,
                                                         bool low_4gb,
                                                         std::string* error_msg);
 
   // Returns nullptr if the vdex file cannot be opened or is not valid.
   static std::unique_ptr<VdexFile> Open(const std::string& vdex_filename,
-                                        bool writable,
                                         bool low_4gb,
                                         std::string* error_msg) {
-    return OpenAtAddress(nullptr,
-                         0,
-                         false,
-                         vdex_filename,
-                         writable,
-                         low_4gb,
-                         error_msg);
+    return OpenAtAddress(nullptr, 0, false, vdex_filename, low_4gb, error_msg);
   }
 
   // Returns nullptr if the vdex file cannot be opened or is not valid.
   static std::unique_ptr<VdexFile> Open(int file_fd,
                                         size_t vdex_length,
                                         const std::string& vdex_filename,
-                                        bool writable,
                                         bool low_4gb,
                                         std::string* error_msg) {
-    return OpenAtAddress(nullptr,
-                         0,
-                         false,
-                         file_fd,
-                         vdex_length,
-                         vdex_filename,
-                         writable,
-                         low_4gb,
-                         error_msg);
+    return OpenAtAddress(
+        nullptr, 0, false, file_fd, /*start=*/0, vdex_length, vdex_filename, low_4gb, error_msg);
   }
 
   EXPORT static std::unique_ptr<VdexFile> OpenFromDm(const std::string& filename,
-                                                     const ZipArchive& archive);
+                                                     const ZipArchive& archive,
+                                                     std::string* error_msg);
+
+  static std::unique_ptr<VdexFile> OpenFromDm(const std::string& filename,
+                                              uint8_t* vdex_begin_,
+                                              uint8_t* vdex_end_,
+                                              std::string* error_msg);
 
   const uint8_t* Begin() const { return mmap_.Begin(); }
   const uint8_t* End() const { return mmap_.End(); }
@@ -259,9 +252,7 @@ class VdexFile {
         GetSectionHeader(VdexSection::kVerifierDepsSection).section_size);
   }
 
-  bool IsValid() const {
-    return mmap_.Size() >= sizeof(VdexFileHeader) && GetVdexFileHeader().IsValid();
-  }
+  EXPORT bool IsValid() const;
 
   // This method is for iterating over the dex files in the vdex. If `cursor` is null,
   // the first dex file is returned. If `cursor` is not null, it must point to a dex
@@ -295,10 +286,6 @@ class VdexFile {
   // order must match too.
   bool MatchesDexFileChecksums(const std::vector<const DexFile::Header*>& dex_headers) const;
 
-  // Returns true if all dex files are standard dex rather than compact dex.
-  // Also returns true if there are no dex files at all.
-  bool HasOnlyStandardDexFiles() const;
-
   ClassStatus ComputeClassStatus(Thread* self, Handle<mirror::Class> cls) const
       REQUIRES_SHARED(Locks::mutator_lock_);
 
@@ -309,8 +296,6 @@ class VdexFile {
   }
 
  private:
-  bool ContainsDexFile(const DexFile& dex_file) const;
-
   const uint8_t* DexBegin() const {
     DCHECK(HasDexSection());
     return Begin() + GetSectionHeader(VdexSection::kDexFileSection).section_offset;
diff --git a/runtime/vdex_file_test.cc b/runtime/vdex_file_test.cc
index 4c359e38c6..fb0c03aa6a 100644
--- a/runtime/vdex_file_test.cc
+++ b/runtime/vdex_file_test.cc
@@ -33,13 +33,11 @@ TEST_F(VdexFileTest, OpenEmptyVdex) {
   std::unique_ptr<VdexFile> vdex = VdexFile::Open(tmp.GetFd(),
                                                   0,
                                                   tmp.GetFilename(),
-                                                  /*writable=*/false,
                                                   /*low_4gb=*/false,
                                                   &error_msg);
   EXPECT_TRUE(vdex == nullptr);
 
-  vdex = VdexFile::Open(
-      tmp.GetFilename(), /*writable=*/false, /*low_4gb=*/false, &error_msg);
+  vdex = VdexFile::Open(tmp.GetFilename(), /*low_4gb=*/false, &error_msg);
   EXPECT_TRUE(vdex == nullptr);
 }
 
diff --git a/runtime/verifier/method_verifier-inl.h b/runtime/verifier/method_verifier-inl.h
index a13a58eede..762f1d6b85 100644
--- a/runtime/verifier/method_verifier-inl.h
+++ b/runtime/verifier/method_verifier-inl.h
@@ -35,7 +35,7 @@ inline MethodReference MethodVerifier::GetMethodReference() const {
 }
 
 inline bool MethodVerifier::HasFailures() const {
-  return !failure_messages_.empty();
+  return !failures_.empty();
 }
 
 }  // namespace verifier
diff --git a/runtime/verifier/method_verifier.cc b/runtime/verifier/method_verifier.cc
index 74ea53b3f3..91e46365cf 100644
--- a/runtime/verifier/method_verifier.cc
+++ b/runtime/verifier/method_verifier.cc
@@ -65,6 +65,7 @@
 #include "stack.h"
 #include "vdex_file.h"
 #include "verifier/method_verifier.h"
+#include "verifier_compiler_binding.h"
 #include "verifier_deps.h"
 
 namespace art HIDDEN {
@@ -81,13 +82,12 @@ void PcToRegisterLineTable::Init(InstructionFlags* flags,
                                  uint32_t insns_size,
                                  uint16_t registers_size,
                                  ArenaAllocator& allocator,
-                                 RegTypeCache* reg_types,
                                  uint32_t interesting_dex_pc) {
   DCHECK_GT(insns_size, 0U);
   register_lines_.resize(insns_size);
   for (uint32_t i = 0; i < insns_size; i++) {
     if ((i == interesting_dex_pc) || flags[i].IsBranchTarget()) {
-      register_lines_[i].reset(RegisterLine::Create(registers_size, allocator, reg_types));
+      register_lines_[i].reset(RegisterLine::Create(registers_size, allocator));
     }
   }
 }
@@ -120,36 +120,33 @@ constexpr bool IsCompatThrow(Instruction::Code opcode) {
   return opcode == Instruction::Code::RETURN_OBJECT || opcode == Instruction::Code::MOVE_EXCEPTION;
 }
 
-template <bool kVerifierDebug>
-class MethodVerifier final : public ::art::verifier::MethodVerifier {
+class MethodVerifierImpl : public ::art::verifier::MethodVerifier {
  public:
   bool IsInstanceConstructor() const {
     return IsConstructor() && !IsStatic();
   }
 
-  void FindLocksAtDexPc() REQUIRES_SHARED(Locks::mutator_lock_);
-
- private:
-  MethodVerifier(Thread* self,
-                 ArenaPool* arena_pool,
-                 RegTypeCache* reg_types,
-                 VerifierDeps* verifier_deps,
-                 const dex::CodeItem* code_item,
-                 uint32_t method_idx,
-                 bool aot_mode,
-                 Handle<mirror::DexCache> dex_cache,
-                 const dex::ClassDef& class_def,
-                 uint32_t access_flags,
-                 bool verify_to_dump,
-                 uint32_t api_level) REQUIRES_SHARED(Locks::mutator_lock_)
-     : art::verifier::MethodVerifier(self,
-                                     arena_pool,
-                                     reg_types,
-                                     verifier_deps,
-                                     class_def,
-                                     code_item,
-                                     method_idx,
-                                     aot_mode),
+ protected:
+  MethodVerifierImpl(Thread* self,
+                     ArenaPool* arena_pool,
+                     RegTypeCache* reg_types,
+                     VerifierDeps* verifier_deps,
+                     const dex::CodeItem* code_item,
+                     uint32_t method_idx,
+                     bool aot_mode,
+                     Handle<mirror::DexCache> dex_cache,
+                     const dex::ClassDef& class_def,
+                     uint32_t access_flags,
+                     bool verify_to_dump,
+                     uint32_t api_level) REQUIRES_SHARED(Locks::mutator_lock_)
+     : ::art::verifier::MethodVerifier(self,
+                                       arena_pool,
+                                       reg_types,
+                                       verifier_deps,
+                                       class_def,
+                                       code_item,
+                                       method_idx,
+                                       aot_mode),
        method_access_flags_(access_flags),
        return_type_(nullptr),
        dex_cache_(dex_cache),
@@ -201,20 +198,15 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
 
   // Adds the given string to the beginning of the last failure message.
   void PrependToLastFailMessage(std::string prepend) {
-    size_t failure_num = failure_messages_.size();
-    DCHECK_NE(failure_num, 0U);
-    std::ostringstream* last_fail_message = failure_messages_[failure_num - 1];
-    prepend += last_fail_message->str();
-    failure_messages_[failure_num - 1] = new std::ostringstream(prepend, std::ostringstream::ate);
-    delete last_fail_message;
+    MessageOStream* last_fail_message = &LastFailureMessageStream();
+    prepend += last_fail_message->view();
+    last_fail_message->str(std::move(prepend));
   }
 
-  // Adds the given string to the end of the last failure message.
-  void AppendToLastFailMessage(const std::string& append) {
-    size_t failure_num = failure_messages_.size();
-    DCHECK_NE(failure_num, 0U);
-    std::ostringstream* last_fail_message = failure_messages_[failure_num - 1];
-    (*last_fail_message) << append;
+  // Return the last failure message stream for appending.
+  MessageOStream& LastFailureMessageStream() {
+    DCHECK(!failures_.empty());
+    return failures_.back().message;
   }
 
   /*
@@ -286,8 +278,9 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
    *   instruction
    */
   template <Instruction::Code kDispatchOpcode>
-  ALWAYS_INLINE bool VerifyInstruction(const Instruction* inst,
-                                       uint32_t code_offset,
+  ALWAYS_INLINE bool VerifyInstruction(uint32_t dex_pc,
+                                       uint32_t end_dex_pc,
+                                       const Instruction* inst,
                                        uint16_t inst_data);
 
   /* Ensure that the register index is valid for this code item. */
@@ -430,25 +423,18 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
   // Perform static checks on an instruction referencing a constant method handle. All we do here
   // is ensure that the method index is in the valid range.
   bool CheckMethodHandleIndex(uint32_t idx) {
-    uint32_t limit = dex_file_->NumMethodHandles();
-    if (UNLIKELY(idx >= limit)) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "bad method handle index " << idx << " (max "
-                                        << limit << ")";
+    if (UNLIKELY(idx >= dex_file_->NumMethodHandles())) {
+      FailBadMethodHandleIndex(idx);
       return false;
     }
     return true;
   }
 
-  // Perform static checks on a "new-instance" instruction. Specifically, make sure the class
-  // reference isn't for an array class.
-  bool CheckNewInstance(dex::TypeIndex idx);
-
   // Perform static checks on a prototype indexing instruction. All we do here is ensure that the
   // prototype index is in the valid range.
   bool CheckPrototypeIndex(uint32_t idx) {
-    if (UNLIKELY(idx >= dex_file_->GetHeader().proto_ids_size_)) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "bad prototype index " << idx << " (max "
-                                        << dex_file_->GetHeader().proto_ids_size_ << ")";
+    if (UNLIKELY(idx >= dex_file_->NumProtoIds())) {
+      FailBadPrototypeIndex(idx);
       return false;
     }
     return true;
@@ -456,9 +442,8 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
 
   /* Ensure that the string index is in the valid range. */
   bool CheckStringIndex(uint32_t idx) {
-    if (UNLIKELY(idx >= dex_file_->GetHeader().string_ids_size_)) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "bad string index " << idx << " (max "
-                                        << dex_file_->GetHeader().string_ids_size_ << ")";
+    if (UNLIKELY(idx >= dex_file_->NumStringIds())) {
+      FailBadStringIndex(idx);
       return false;
     }
     return true;
@@ -468,30 +453,59 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
   // index is in the valid range.
   bool CheckTypeIndex(dex::TypeIndex idx) {
     if (UNLIKELY(idx.index_ >= dex_file_->GetHeader().type_ids_size_)) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "bad type index " << idx.index_ << " (max "
-                                        << dex_file_->GetHeader().type_ids_size_ << ")";
+      FailBadTypeIndex(idx);
       return false;
     }
     return true;
   }
 
-  // Perform static checks on a "new-array" instruction. Specifically, make sure they aren't
-  // creating an array of arrays that causes the number of dimensions to exceed 255.
+  // Perform static checks on a `new-instance` instruction. Specifically, make sure the class
+  // reference isn't for an array class.
+  bool CheckNewInstance(dex::TypeIndex idx);
+
+  // Perform static checks on a `*new-array*` instruction. Specifically, make sure it
+  // references an array class with the number of dimensions not exceeding 255.
+  // For `filled-new-array*`, check for a valid component type; `I` is accepted, `J` and `D`
+  // are rejected in line with the specification and other primitive component types are marked
+  // for interpreting (throws `InternalError` in interpreter and the compiler cannot handle them).
+  template <bool kFilled>
   bool CheckNewArray(dex::TypeIndex idx);
 
-  // Verify an array data table. "cur_offset" is the offset of the fill-array-data instruction.
-  bool CheckArrayData(uint32_t cur_offset);
+  // Determine if the relative `offset` targets a valid dex pc.
+  // The `offset` should be inside the range `[-dex_pc, end_dex_pc - dex_pc)`.
+  ALWAYS_INLINE
+  static bool IsOffsetInRange(uint32_t dex_pc, uint32_t end_dex_pc, int32_t offset) {
+    DCHECK_LT(dex_pc, end_dex_pc);
+    if (offset >= 0) {
+      return static_cast<uint32_t>(offset) < end_dex_pc - dex_pc;
+    } else {
+      // Use well-defined unsigned arithmetic for the lower bound check.
+      return dex_pc >= -static_cast<uint32_t>(offset);
+    }
+  }
+
+  // Verify an array data table.
+  bool CheckArrayData(uint32_t dex_pc, uint32_t end_dex_pc, const Instruction* inst);
 
   // Verify that the target of a branch instruction is valid. We don't expect code to jump directly
   // into an exception handler, but it's valid to do so as long as the target isn't a
   // "move-exception" instruction. We verify that in a later stage.
-  // The dex format forbids certain instructions from branching to themselves.
+  // The dex format forbids instructions other than `goto/32` from branching to themselves.
   // Updates "insn_flags_", setting the "branch target" flag.
-  bool CheckBranchTarget(uint32_t cur_offset);
-
-  // Verify a switch table. "cur_offset" is the offset of the switch instruction.
+  template <Instruction::Format kFormat>
+  ALWAYS_INLINE
+  bool CheckAndMarkBranchTarget(uint32_t dex_pc,
+                                uint32_t end_dex_pc,
+                                const Instruction* inst,
+                                uint16_t inst_data);
+
+  // Verify a switch table.
   // Updates "insn_flags_", setting the "branch target" flag.
-  bool CheckSwitchTargets(uint32_t cur_offset);
+  ALWAYS_INLINE
+  bool CheckAndMarkSwitchTargets(uint32_t dex_pc,
+                                 uint32_t end_dex_pc,
+                                 const Instruction* inst,
+                                 uint16_t inst_data);
 
   // Check the register indices used in a "vararg" instruction, such as invoke-virtual or
   // filled-new-array.
@@ -547,76 +561,12 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
   bool GetBranchOffset(uint32_t cur_offset, int32_t* pOffset, bool* pConditional,
                        bool* selfOkay);
 
-  /* Perform detailed code-flow analysis on a single method. */
-  bool VerifyCodeFlow() REQUIRES_SHARED(Locks::mutator_lock_);
-
   // Set the register types for the first instruction in the method based on the method signature.
   // This has the side-effect of validating the signature.
   bool SetTypesFromSignature() REQUIRES_SHARED(Locks::mutator_lock_);
 
-  /*
-   * Perform code flow on a method.
-   *
-   * The basic strategy is as outlined in v3 4.11.1.2: set the "changed" bit on the first
-   * instruction, process it (setting additional "changed" bits), and repeat until there are no
-   * more.
-   *
-   * v3 4.11.1.1
-   * - (N/A) operand stack is always the same size
-   * - operand stack [registers] contain the correct types of values
-   * - local variables [registers] contain the correct types of values
-   * - methods are invoked with the appropriate arguments
-   * - fields are assigned using values of appropriate types
-   * - opcodes have the correct type values in operand registers
-   * - there is never an uninitialized class instance in a local variable in code protected by an
-   *   exception handler (operand stack is okay, because the operand stack is discarded when an
-   *   exception is thrown) [can't know what's a local var w/o the debug info -- should fall out of
-   *   register typing]
-   *
-   * v3 4.11.1.2
-   * - execution cannot fall off the end of the code
-   *
-   * (We also do many of the items described in the "static checks" sections, because it's easier to
-   * do them here.)
-   *
-   * We need an array of RegType values, one per register, for every instruction. If the method uses
-   * monitor-enter, we need extra data for every register, and a stack for every "interesting"
-   * instruction. In theory this could become quite large -- up to several megabytes for a monster
-   * function.
-   *
-   * NOTE:
-   * The spec forbids backward branches when there's an uninitialized reference in a register. The
-   * idea is to prevent something like this:
-   *   loop:
-   *     move r1, r0
-   *     new-instance r0, MyClass
-   *     ...
-   *     if-eq rN, loop  // once
-   *   initialize r0
-   *
-   * This leaves us with two different instances, both allocated by the same instruction, but only
-   * one is initialized. The scheme outlined in v3 4.11.1.4 wouldn't catch this, so they work around
-   * it by preventing backward branches. We achieve identical results without restricting code
-   * reordering by specifying that you can't execute the new-instance instruction if a register
-   * contains an uninitialized instance created by that same instruction.
-   */
-  template <bool kMonitorDexPCs>
-  bool CodeFlowVerifyMethod() REQUIRES_SHARED(Locks::mutator_lock_);
-
-  /*
-   * Perform verification for a single instruction.
-   *
-   * This requires fully decoding the instruction to determine the effect it has on registers.
-   *
-   * Finds zero or more following instructions and sets the "changed" flag if execution at that
-   * point needs to be (re-)evaluated. Register changes are merged into "reg_types_" at the target
-   * addresses. Does not set or clear any other flags in "insn_flags_".
-   */
-  bool CodeFlowVerifyInstruction(uint32_t* start_guess)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
-  // Perform verification of a new array instruction
-  void VerifyNewArray(const Instruction* inst, bool is_filled, bool is_range)
+  // Perform verification of a `filled-new-array/-range` instruction.
+  bool VerifyFilledNewArray(const Instruction* inst, bool is_range)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Helper to perform verification on puts of primitive type.
@@ -655,14 +605,6 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
   const RegType& ResolveClass(dex::TypeIndex class_idx)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  /*
-   * For the "move-exception" instruction at "work_insn_idx_", which must be at an exception handler
-   * address, determine the Join of all exceptions that can land here. Fails if no matching
-   * exception handler can be found or if the Join of exception types fails.
-   */
-  const RegType& GetCaughtExceptionType()
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
   /*
    * Resolves a method based on an index and performs access checks to ensure
    * the referrer can access the resolved method.
@@ -713,53 +655,6 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
    */
   bool CheckCallSite(uint32_t call_site_idx);
 
-  /*
-   * Verify that the target instruction is not "move-exception". It's important that the only way
-   * to execute a move-exception is as the first instruction of an exception handler.
-   * Returns "true" if all is well, "false" if the target instruction is move-exception.
-   */
-  bool CheckNotMoveException(const uint16_t* insns, int insn_idx) {
-    if ((insns[insn_idx] & 0xff) == Instruction::MOVE_EXCEPTION) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid use of move-exception";
-      return false;
-    }
-    return true;
-  }
-
-  /*
-   * Verify that the target instruction is not "move-result". It is important that we cannot
-   * branch to move-result instructions, but we have to make this a distinct check instead of
-   * adding it to CheckNotMoveException, because it is legal to continue into "move-result"
-   * instructions - as long as the previous instruction was an invoke, which is checked elsewhere.
-   */
-  bool CheckNotMoveResult(const uint16_t* insns, int insn_idx) {
-    if (((insns[insn_idx] & 0xff) >= Instruction::MOVE_RESULT) &&
-        ((insns[insn_idx] & 0xff) <= Instruction::MOVE_RESULT_OBJECT)) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid use of move-result*";
-      return false;
-    }
-    return true;
-  }
-
-  /*
-   * Verify that the target instruction is not "move-result" or "move-exception". This is to
-   * be used when checking branch and switch instructions, but not instructions that can
-   * continue.
-   */
-  bool CheckNotMoveExceptionOrMoveResult(const uint16_t* insns, int insn_idx) {
-    return (CheckNotMoveException(insns, insn_idx) && CheckNotMoveResult(insns, insn_idx));
-  }
-
-  /*
-  * Control can transfer to "next_insn". Merge the registers from merge_line into the table at
-  * next_insn, and set the changed flag on the target address if any of the registers were changed.
-  * In the case of fall-through, update the merge line on a change as its the working line for the
-  * next instruction.
-  * Returns "false" if an error is encountered.
-  */
-  bool UpdateRegisters(uint32_t next_insn, RegisterLine* merge_line, bool update_merge_line)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
   // Return the register type for the method.
   const RegType& GetMethodReturnType() REQUIRES_SHARED(Locks::mutator_lock_);
 
@@ -804,6 +699,19 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
     }
   }
 
+  ALWAYS_INLINE static bool IsMoveResult(Instruction::Code opcode) {
+    static_assert(Instruction::MOVE_RESULT + 1 == Instruction::MOVE_RESULT_WIDE);
+    static_assert(Instruction::MOVE_RESULT_WIDE + 1 == Instruction::MOVE_RESULT_OBJECT);
+    return Instruction::MOVE_RESULT <= opcode && opcode <= Instruction::MOVE_RESULT_OBJECT;
+  }
+
+  ALWAYS_INLINE static bool IsMoveResultOrMoveException(Instruction::Code opcode) {
+    static_assert(Instruction::MOVE_RESULT + 1 == Instruction::MOVE_RESULT_WIDE);
+    static_assert(Instruction::MOVE_RESULT_WIDE + 1 == Instruction::MOVE_RESULT_OBJECT);
+    static_assert(Instruction::MOVE_RESULT_OBJECT + 1 == Instruction::MOVE_EXCEPTION);
+    return Instruction::MOVE_RESULT <= opcode && opcode <= Instruction::MOVE_EXCEPTION;
+  }
+
   NO_INLINE void FailInvalidArgCount(const Instruction* inst, uint32_t arg_count) {
     Fail(VERIFY_ERROR_BAD_CLASS_HARD)
         << "invalid arg count (" << arg_count << ") in " << inst->Name();
@@ -823,6 +731,165 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
         << "bad method index " << method_idx << " (max " << dex_file_->NumMethodIds() << ")";
   }
 
+  NO_INLINE void FailBadMethodHandleIndex(uint32_t idx) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "bad method handle index " << idx << " (max " << dex_file_->NumMethodHandles() << ")";
+  }
+
+  NO_INLINE void FailBadPrototypeIndex(uint32_t idx) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "bad prototype index " << idx << " (max " << dex_file_->NumProtoIds() << ")";
+  }
+
+  NO_INLINE void FailBadStringIndex(uint32_t idx) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "bad string index " << idx << " (max " << dex_file_->NumStringIds() << ")";
+  }
+
+  NO_INLINE void FailBadTypeIndex(dex::TypeIndex idx) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "bad type index " << idx.index_ << " (max " << dex_file_->NumTypeIds() << ")";
+  }
+
+  NO_INLINE void FailBadNewArrayNotArray(const char* descriptor) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "can't new-array class '" << descriptor << "' (not an array)";
+  }
+
+  NO_INLINE void FailBadNewArrayTooManyDimensions(const char* descriptor) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "can't new-array class '" << descriptor << "' (exceeds limit)";
+  }
+
+  NO_INLINE void FailBadFilledNewArray(const char* descriptor) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "can't fill-new-array class '" << descriptor << "' (wide component type)";
+  }
+
+  NO_INLINE void FailBranchOffsetZero(uint32_t dex_pc) {
+    work_insn_idx_ = dex_pc;  // Let `Fail()` record the dex PC of the failing instruction.
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "branch offset of zero not allowed.";
+  }
+
+  NO_INLINE void FailTargetOffsetOutOfRange(uint32_t dex_pc, uint32_t end_dex_pc, int32_t offset) {
+    work_insn_idx_ = dex_pc;  // Let `Fail()` record the dex PC of the failing instruction.
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "invalid target offset " << offset
+        << " (end " << reinterpret_cast<void*>(end_dex_pc) << ")";
+  }
+
+  NO_INLINE void FailTargetMidInstruction(uint32_t dex_pc, uint32_t target_dex_pc) {
+    work_insn_idx_ = dex_pc;  // Let `Fail()` record the dex PC of the failing instruction.
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "target dex pc " << reinterpret_cast<void*>(target_dex_pc)
+        << " is not at instruction start.";
+  }
+
+  NO_INLINE void FailBranchTargetIsMoveResultOrMoveException(uint32_t dex_pc,
+                                                             uint32_t target_dex_pc,
+                                                             Instruction::Code target_opcode) {
+    DCHECK(IsMoveResultOrMoveException(target_opcode));
+    work_insn_idx_ = dex_pc;  // Let `Fail()` record the dex PC of the failing instruction.
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "invalid use of " << target_opcode << " as branch target at "
+        << reinterpret_cast<void*>(target_dex_pc);
+  }
+
+  NO_INLINE void FailUnalignedTableDexPc(uint32_t dex_pc, uint32_t table_dex_pc) {
+    work_insn_idx_ = dex_pc;  // Let `Fail()` record the dex PC of the failing instruction.
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "unaligned table at " << table_dex_pc;
+  }
+
+  NO_INLINE void FailBadArrayDataSignature(uint32_t dex_pc, uint32_t array_data_dex_pc) {
+    work_insn_idx_ = dex_pc;  // Let `Fail()` record the dex PC of the failing instruction.
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "invalid magic for array-data at " << reinterpret_cast<void*>(array_data_dex_pc);
+  }
+
+  NO_INLINE void FailBadSwitchPayloadSignature(uint32_t dex_pc,
+                                               uint32_t switch_payload_dex_pc,
+                                               uint16_t signature,
+                                               uint16_t expected_signature) {
+    work_insn_idx_ = dex_pc;  // Let `Fail()` record the dex PC of the failing instruction.
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "wrong signature for switch payload at "
+        << reinterpret_cast<void*>(switch_payload_dex_pc)
+        << " (0x" << std::hex << signature << ", wanted 0x" << expected_signature << ")";
+  }
+
+  NO_INLINE void FailPackedSwitchKeyOverflow(uint32_t dex_pc,
+                                             uint32_t switch_payload_dex_pc,
+                                             int32_t first_key,
+                                             uint32_t switch_count) {
+    work_insn_idx_ = dex_pc;  // Let `Fail()` record the dex PC of the failing instruction.
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "invalid packed switch payload at "
+        << reinterpret_cast<void*>(switch_payload_dex_pc)
+        << ", key overflow: first_key=" << first_key << ", switch_count=" << switch_count;
+  }
+
+  NO_INLINE void FailSparseSwitchPayloadKeyOrder(uint32_t dex_pc,
+                                                 uint32_t switch_payload_dex_pc,
+                                                 int32_t previous_key,
+                                                 int32_t current_key) {
+    work_insn_idx_ = dex_pc;  // Let `Fail()` record the dex PC of the failing instruction.
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "invalid sparse switch payload at "
+        << reinterpret_cast<void*>(switch_payload_dex_pc)
+        << ", unordered keys: previous=" << previous_key << ", current=" << current_key;
+  }
+
+  NO_INLINE void FailSwitchTargetOffsetOutOfRange(uint32_t dex_pc,
+                                                  uint32_t end_dex_pc,
+                                                  uint32_t switch_payload_dex_pc,
+                                                  int32_t offset,
+                                                  uint32_t target_index) {
+    FailTargetOffsetOutOfRange(dex_pc, end_dex_pc, offset);
+    LastFailureMessageStream()
+        << " in switch payload at " << reinterpret_cast<void*>(switch_payload_dex_pc)
+        << ", target index " << target_index;
+  }
+
+  NO_INLINE void FailSwitchTargetMidInstruction(uint32_t dex_pc,
+                                                uint32_t target_dex_pc,
+                                                uint32_t switch_payload_dex_pc,
+                                                uint32_t target_index) {
+    FailTargetMidInstruction(dex_pc, target_dex_pc);
+    LastFailureMessageStream()
+        << " in switch payload at " << reinterpret_cast<void*>(switch_payload_dex_pc)
+        << ", target index " << target_index;
+  }
+
+  NO_INLINE void FailSwitchTargetIsMoveResultOrMoveException(uint32_t dex_pc,
+                                                             uint32_t target_dex_pc,
+                                                             Instruction::Code target_opcode,
+                                                             uint32_t switch_payload_dex_pc,
+                                                             uint32_t target_index) {
+    FailBranchTargetIsMoveResultOrMoveException(dex_pc, target_dex_pc, target_opcode);
+    LastFailureMessageStream()
+        << " in switch payload at " << reinterpret_cast<void*>(switch_payload_dex_pc)
+        << ", target index " << target_index;
+  }
+
+  NO_INLINE void FailForCopyReference(uint32_t vdst, uint32_t vsrc, const RegType& type)
+      REQUIRES_SHARED(Locks::mutator_lock_) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "copy-reference v" << vdst << "<-v" << vsrc << " type=" << type;
+  }
+
+  NO_INLINE void FailForCopyCat1(uint32_t vdst, uint32_t vsrc, const RegType& type)
+      REQUIRES_SHARED(Locks::mutator_lock_) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "copy-cat1 v" << vdst << "<-v" << vsrc << " type=" << type;
+  }
+
+  NO_INLINE void FailForCopyCat2(
+      uint32_t vdst, uint32_t vsrc, const RegType& type_l, const RegType& type_h)
+      REQUIRES_SHARED(Locks::mutator_lock_) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+        << "copy-cat2 v" << vdst << "<-v" << vsrc << " type=" << type_l << "/" << type_h;
+  }
+
   NO_INLINE void FailForRegisterType(uint32_t vsrc,
                                      const RegType& check_type,
                                      const RegType& src_type,
@@ -856,6 +923,60 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
         vsrc, reg_types_.GetFromId(src_type_id), reg_types_.GetFromId(src_type_id_h));
   }
 
+  ALWAYS_INLINE inline bool VerifyCopyReference(uint32_t vdst, uint32_t vsrc)
+      REQUIRES_SHARED(Locks::mutator_lock_) {
+    const RegType& type = work_line_->GetRegisterType(this, vsrc);
+    // Allow conflicts to be copied around.
+    if (UNLIKELY(!type.IsConflict() && !type.IsReferenceTypes())) {
+      FailForCopyReference(vdst, vsrc, type);
+      return false;
+    }
+    work_line_->CopyReference(vdst, vsrc, type);
+    return true;
+  }
+
+  ALWAYS_INLINE inline bool VerifyCopyCat1(uint32_t vdst, uint32_t vsrc)
+      REQUIRES_SHARED(Locks::mutator_lock_) {
+    uint16_t src_type_id = work_line_->GetRegisterTypeId(vsrc);
+    if (UNLIKELY(src_type_id >= RegTypeCache::NumberOfRegKindCacheIds()) ||
+        UNLIKELY(RegTypeCache::RegKindForId(src_type_id) != RegType::kConflict &&
+                 !RegType::IsCategory1Types(RegTypeCache::RegKindForId(src_type_id)))) {
+      const RegType& type = reg_types_.GetFromId(src_type_id);
+      DCHECK(!type.IsConflict() && !type.IsCategory1Types()) << type;
+      FailForCopyCat1(vdst, vsrc, type);
+      return false;
+    }
+    RegType::Kind kind = RegTypeCache::RegKindForId(src_type_id);
+    DCHECK(kind == RegType::kConflict || RegType::IsCategory1Types(kind)) << kind;
+    work_line_->SetRegisterType(vdst, kind);
+    return true;
+  }
+
+  ALWAYS_INLINE inline bool VerifyCopyCat2(uint32_t vdst, uint32_t vsrc)
+      REQUIRES_SHARED(Locks::mutator_lock_) {
+    uint16_t src_type_id_l = work_line_->GetRegisterTypeId(vsrc);
+    uint16_t src_type_id_h = work_line_->GetRegisterTypeId(vsrc + 1);
+    auto to_high_id = [](uint16_t low_id) ALWAYS_INLINE {
+      RegType::Kind low_kind = RegTypeCache::RegKindForId(low_id);
+      DCHECK(RegType::IsLowHalf(low_kind));
+      return RegTypeCache::IdForRegKind(RegType::ToHighHalf(low_kind));
+    };
+    if (UNLIKELY(src_type_id_l >= RegTypeCache::NumberOfRegKindCacheIds()) ||
+        UNLIKELY(!RegType::IsLowHalf(RegTypeCache::RegKindForId(src_type_id_l))) ||
+        UNLIKELY(src_type_id_h != to_high_id(src_type_id_l))) {
+      const RegType& type_l = reg_types_.GetFromId(src_type_id_l);
+      const RegType& type_h = reg_types_.GetFromId(src_type_id_h);
+      DCHECK(!type_l.CheckWidePair(type_h));
+      FailForCopyCat2(vdst, vsrc, type_l, type_h);
+      return false;
+    }
+    DCHECK(reg_types_.GetFromId(src_type_id_l).CheckWidePair(reg_types_.GetFromId(src_type_id_h)));
+    work_line_->SetRegisterTypeWide(vdst,
+                                    RegTypeCache::RegKindForId(src_type_id_l),
+                                    RegTypeCache::RegKindForId(src_type_id_h));
+    return true;
+  }
+
   ALWAYS_INLINE inline bool VerifyRegisterType(uint32_t vsrc, const RegType& check_type)
       REQUIRES_SHARED(Locks::mutator_lock_) {
     // Verify the src register type against the check type refining the type of the register
@@ -874,7 +995,7 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
         fail_type = VERIFY_ERROR_BAD_CLASS_HARD;
       }
       FailForRegisterType(vsrc, check_type, src_type, fail_type);
-      return false;
+      return fail_type != VERIFY_ERROR_BAD_CLASS_HARD;
     }
     if (check_type.IsLowHalf()) {
       const RegType& src_type_h = work_line_->GetRegisterType(this, vsrc + 1);
@@ -943,37 +1064,61 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
    * Verify types for a simple two-register instruction (e.g. "neg-int").
    * "dst_type" is stored into vA, and "src_type" is verified against vB.
    */
-  void CheckUnaryOp(const Instruction* inst, RegType::Kind dst_kind, RegType::Kind src_kind)
+  ALWAYS_INLINE
+  bool CheckUnaryOp(const Instruction* inst,
+                    uint16_t inst_data,
+                    RegType::Kind dst_kind,
+                    RegType::Kind src_kind)
       REQUIRES_SHARED(Locks::mutator_lock_) {
-    if (VerifyRegisterType(inst->VRegB_12x(), src_kind)) {
-      work_line_->SetRegisterType(inst->VRegA_12x(), dst_kind);
+    if (VerifyRegisterType(inst->VRegB_12x(inst_data), src_kind)) {
+      work_line_->SetRegisterType(inst->VRegA_12x(inst_data), dst_kind);
+      return true;
+    } else {
+      return false;
     }
   }
 
-  void CheckUnaryOpWide(const Instruction* inst,
+  ALWAYS_INLINE
+  bool CheckUnaryOpWide(const Instruction* inst,
+                        uint16_t inst_data,
                         RegType::Kind dst_kind,
                         RegType::Kind src_kind)
       REQUIRES_SHARED(Locks::mutator_lock_) {
-    if (VerifyRegisterTypeWide(inst->VRegB_12x(), src_kind)) {
-      work_line_->SetRegisterTypeWide(inst->VRegA_12x(), dst_kind, RegType::ToHighHalf(dst_kind));
+    if (VerifyRegisterTypeWide(inst->VRegB_12x(inst_data), src_kind)) {
+      work_line_->SetRegisterTypeWide(
+          inst->VRegA_12x(inst_data), dst_kind, RegType::ToHighHalf(dst_kind));
+      return true;
+    } else {
+      return false;
     }
   }
 
-  void CheckUnaryOpToWide(const Instruction* inst,
+  ALWAYS_INLINE
+  bool CheckUnaryOpToWide(const Instruction* inst,
+                          uint16_t inst_data,
                           RegType::Kind dst_kind,
                           RegType::Kind src_kind)
       REQUIRES_SHARED(Locks::mutator_lock_) {
-    if (VerifyRegisterType(inst->VRegB_12x(), src_kind)) {
-      work_line_->SetRegisterTypeWide(inst->VRegA_12x(), dst_kind, RegType::ToHighHalf(dst_kind));
+    if (VerifyRegisterType(inst->VRegB_12x(inst_data), src_kind)) {
+      work_line_->SetRegisterTypeWide(
+          inst->VRegA_12x(inst_data), dst_kind, RegType::ToHighHalf(dst_kind));
+      return true;
+    } else {
+      return false;
     }
   }
 
-  void CheckUnaryOpFromWide(const Instruction* inst,
+  ALWAYS_INLINE
+  bool CheckUnaryOpFromWide(const Instruction* inst,
+                            uint16_t inst_data,
                             RegType::Kind dst_kind,
                             RegType::Kind src_kind)
       REQUIRES_SHARED(Locks::mutator_lock_) {
-    if (VerifyRegisterTypeWide(inst->VRegB_12x(), src_kind)) {
-      work_line_->SetRegisterType(inst->VRegA_12x(), dst_kind);
+    if (VerifyRegisterTypeWide(inst->VRegB_12x(inst_data), src_kind)) {
+      work_line_->SetRegisterType(inst->VRegA_12x(inst_data), dst_kind);
+      return true;
+    } else {
+      return false;
     }
   }
 
@@ -982,13 +1127,15 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
    * "dst_type" is stored into vA, and "src_type1"/"src_type2" are verified
    * against vB/vC.
    */
-  void CheckBinaryOp(const Instruction* inst,
+  ALWAYS_INLINE
+  bool CheckBinaryOp(const Instruction* inst,
+                     uint16_t inst_data,
                      RegType::Kind dst_kind,
                      RegType::Kind src_kind1,
                      RegType::Kind src_kind2,
                      bool check_boolean_op)
       REQUIRES_SHARED(Locks::mutator_lock_) {
-    const uint32_t vregA = inst->VRegA_23x();
+    const uint32_t vregA = inst->VRegA_23x(inst_data);
     const uint32_t vregB = inst->VRegB_23x();
     const uint32_t vregC = inst->VRegC_23x();
     if (VerifyRegisterType(vregB, src_kind1) &&
@@ -999,44 +1146,62 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
                 RegTypeCache::RegKindForId(work_line_->GetRegisterTypeId(vregB))) &&
             RegType::IsBooleanTypes(
                 RegTypeCache::RegKindForId(work_line_->GetRegisterTypeId(vregC)))) {
-          work_line_->SetRegisterType(vregA, RegType::Kind::kBoolean);
-          return;
+          dst_kind = RegType::Kind::kBoolean;
         }
       }
       work_line_->SetRegisterType(vregA, dst_kind);
+      return true;
+    } else {
+      return false;
     }
   }
 
-  void CheckBinaryOpWide(const Instruction* inst,
+  ALWAYS_INLINE
+  bool CheckBinaryOpWide(const Instruction* inst,
+                         uint16_t inst_data,
                          RegType::Kind dst_kind,
                          RegType::Kind src_kind1,
                          RegType::Kind src_kind2)
       REQUIRES_SHARED(Locks::mutator_lock_) {
     if (VerifyRegisterTypeWide(inst->VRegB_23x(), src_kind1) &&
         VerifyRegisterTypeWide(inst->VRegC_23x(), src_kind2)) {
-      work_line_->SetRegisterTypeWide(inst->VRegA_23x(), dst_kind, RegType::ToHighHalf(dst_kind));
+      work_line_->SetRegisterTypeWide(
+          inst->VRegA_23x(inst_data), dst_kind, RegType::ToHighHalf(dst_kind));
+      return true;
+    } else {
+      return false;
     }
   }
 
-  void CheckBinaryOpWideCmp(const Instruction* inst,
+  ALWAYS_INLINE
+  bool CheckBinaryOpWideCmp(const Instruction* inst,
+                            uint16_t inst_data,
                             RegType::Kind dst_kind,
                             RegType::Kind src_kind1,
                             RegType::Kind src_kind2)
       REQUIRES_SHARED(Locks::mutator_lock_) {
     if (VerifyRegisterTypeWide(inst->VRegB_23x(), src_kind1) &&
         VerifyRegisterTypeWide(inst->VRegC_23x(), src_kind2)) {
-      work_line_->SetRegisterType(inst->VRegA_23x(), dst_kind);
+      work_line_->SetRegisterType(inst->VRegA_23x(inst_data), dst_kind);
+      return true;
+    } else {
+      return false;
     }
   }
 
-  void CheckBinaryOpWideShift(const Instruction* inst,
+  ALWAYS_INLINE
+  bool CheckBinaryOpWideShift(const Instruction* inst,
+                              uint16_t inst_data,
                               RegType::Kind long_lo_kind,
                               RegType::Kind int_kind)
       REQUIRES_SHARED(Locks::mutator_lock_) {
     if (VerifyRegisterTypeWide(inst->VRegB_23x(), long_lo_kind) &&
         VerifyRegisterType(inst->VRegC_23x(), int_kind)) {
       RegType::Kind long_hi_kind = RegType::ToHighHalf(long_lo_kind);
-      work_line_->SetRegisterTypeWide(inst->VRegA_23x(), long_lo_kind, long_hi_kind);
+      work_line_->SetRegisterTypeWide(inst->VRegA_23x(inst_data), long_lo_kind, long_hi_kind);
+      return true;
+    } else {
+      return false;
     }
   }
 
@@ -1044,14 +1209,16 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
    * Verify types for a binary "2addr" operation. "src_type1"/"src_type2"
    * are verified against vA/vB, then "dst_type" is stored into vA.
    */
-  void CheckBinaryOp2addr(const Instruction* inst,
+  ALWAYS_INLINE
+  bool CheckBinaryOp2addr(const Instruction* inst,
+                          uint16_t inst_data,
                           RegType::Kind dst_kind,
                           RegType::Kind src_kind1,
                           RegType::Kind src_kind2,
                           bool check_boolean_op)
       REQUIRES_SHARED(Locks::mutator_lock_) {
-    const uint32_t vregA = inst->VRegA_12x();
-    const uint32_t vregB = inst->VRegB_12x();
+    const uint32_t vregA = inst->VRegA_12x(inst_data);
+    const uint32_t vregB = inst->VRegB_12x(inst_data);
     if (VerifyRegisterType(vregA, src_kind1) &&
         VerifyRegisterType(vregB, src_kind2)) {
       if (check_boolean_op) {
@@ -1060,37 +1227,49 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
                 RegTypeCache::RegKindForId(work_line_->GetRegisterTypeId(vregA))) &&
             RegType::IsBooleanTypes(
                 RegTypeCache::RegKindForId(work_line_->GetRegisterTypeId(vregB)))) {
-          work_line_->SetRegisterType(vregA, RegType::Kind::kBoolean);
-          return;
+          dst_kind = RegType::Kind::kBoolean;
         }
       }
       work_line_->SetRegisterType(vregA, dst_kind);
+      return true;
+    } else {
+      return false;
     }
   }
 
-  void CheckBinaryOp2addrWide(const Instruction* inst,
+  ALWAYS_INLINE
+  bool CheckBinaryOp2addrWide(const Instruction* inst,
+                              uint16_t inst_data,
                               RegType::Kind dst_kind,
                               RegType::Kind src_kind1,
                               RegType::Kind src_kind2)
       REQUIRES_SHARED(Locks::mutator_lock_) {
-    const uint32_t vregA = inst->VRegA_12x();
-    const uint32_t vregB = inst->VRegB_12x();
+    const uint32_t vregA = inst->VRegA_12x(inst_data);
+    const uint32_t vregB = inst->VRegB_12x(inst_data);
     if (VerifyRegisterTypeWide(vregA, src_kind1) &&
         VerifyRegisterTypeWide(vregB, src_kind2)) {
       work_line_->SetRegisterTypeWide(vregA, dst_kind, RegType::ToHighHalf(dst_kind));
+      return true;
+    } else {
+      return false;
     }
   }
 
-  void CheckBinaryOp2addrWideShift(const Instruction* inst,
+  ALWAYS_INLINE
+  bool CheckBinaryOp2addrWideShift(const Instruction* inst,
+                                   uint16_t inst_data,
                                    RegType::Kind long_lo_kind,
                                    RegType::Kind int_kind)
       REQUIRES_SHARED(Locks::mutator_lock_) {
-    const uint32_t vregA = inst->VRegA_12x();
-    const uint32_t vregB = inst->VRegB_12x();
+    const uint32_t vregA = inst->VRegA_12x(inst_data);
+    const uint32_t vregB = inst->VRegB_12x(inst_data);
     if (VerifyRegisterTypeWide(vregA, long_lo_kind) &&
         VerifyRegisterType(vregB, int_kind)) {
       RegType::Kind long_hi_kind = RegType::ToHighHalf(long_lo_kind);
       work_line_->SetRegisterTypeWide(vregA, long_lo_kind, long_hi_kind);
+      return true;
+    } else {
+      return false;
     }
   }
 
@@ -1100,25 +1279,29 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
    *
    * If "check_boolean_op" is set, we use the constant value in vC.
    */
-  void CheckLiteralOp(const Instruction* inst,
+  template <bool kIsLit16>
+  ALWAYS_INLINE
+  bool CheckLiteralOp(const Instruction* inst,
+                      uint16_t inst_data,
                       RegType::Kind dst_kind,
                       RegType::Kind src_kind,
-                      bool check_boolean_op,
-                      bool is_lit16)
+                      bool check_boolean_op)
       REQUIRES_SHARED(Locks::mutator_lock_) {
-    const uint32_t vregA = is_lit16 ? inst->VRegA_22s() : inst->VRegA_22b();
-    const uint32_t vregB = is_lit16 ? inst->VRegB_22s() : inst->VRegB_22b();
+    const uint32_t vregA = kIsLit16 ? inst->VRegA_22s(inst_data) : inst->VRegA_22b(inst_data);
+    const uint32_t vregB = kIsLit16 ? inst->VRegB_22s(inst_data) : inst->VRegB_22b();
     if (VerifyRegisterType(vregB, src_kind)) {
       if (check_boolean_op) {
         DCHECK_EQ(dst_kind, RegType::Kind::kInteger);
         /* check vB with the call, then check the constant manually */
-        const uint32_t val = is_lit16 ? inst->VRegC_22s() : inst->VRegC_22b();
+        const uint32_t val = kIsLit16 ? inst->VRegC_22s() : inst->VRegC_22b();
         if (work_line_->GetRegisterType(this, vregB).IsBooleanTypes() && (val == 0 || val == 1)) {
-          work_line_->SetRegisterType(vregA, RegType::Kind::kBoolean);
-          return;
+          dst_kind = RegType::Kind::kBoolean;
         }
       }
       work_line_->SetRegisterType(vregA, dst_kind);
+      return true;
+    } else {
+      return false;
     }
   }
 
@@ -1129,15 +1312,12 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
   RegType::Kind DetermineCat1Constant(int32_t value)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  ALWAYS_INLINE bool FailOrAbort(bool condition, const char* error_msg, uint32_t work_insn_idx);
-
   ALWAYS_INLINE InstructionFlags& GetModifiableInstructionFlags(size_t index) {
     return insn_flags_[index];
   }
 
   // Returns the method index of an invoke instruction.
-  static uint16_t GetMethodIdxOfInvoke(const Instruction* inst)
-      REQUIRES_SHARED(Locks::mutator_lock_) {
+  static uint16_t GetMethodIdxOfInvoke(const Instruction* inst) {
     // Note: This is compiled to a single load in release mode.
     Instruction::Code opcode = inst->Opcode();
     if (opcode == Instruction::INVOKE_VIRTUAL ||
@@ -1161,30 +1341,27 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
       return inst->VRegB_4rcc();
     }
   }
+
   // Returns the field index of a field access instruction.
-  uint16_t GetFieldIdxOfFieldAccess(const Instruction* inst, bool is_static)
-      REQUIRES_SHARED(Locks::mutator_lock_) {
-    if (is_static) {
+  ALWAYS_INLINE static uint16_t GetFieldIdxOfFieldAccess(const Instruction* inst) {
+    // Note: This is compiled to a single load in release mode.
+    Instruction::Code opcode = inst->Opcode();
+    if (IsInstructionSGet(opcode) || IsInstructionSPut(opcode)) {
       return inst->VRegB_21c();
     } else {
+      DCHECK(IsInstructionIGet(opcode) || IsInstructionIPut(opcode));
       return inst->VRegC_22c();
     }
   }
 
-  // Run verification on the method. Returns true if verification completes and false if the input
-  // has an irrecoverable corruption.
-  bool Verify() override REQUIRES_SHARED(Locks::mutator_lock_);
-
   // For app-compatibility, code after a runtime throw is treated as dead code
   // for apps targeting <= S.
-  // Returns whether the current instruction was marked as throwing.
-  bool PotentiallyMarkRuntimeThrow() override;
+  void PotentiallyMarkRuntimeThrow() override;
 
   // Dump the failures encountered by the verifier.
   std::ostream& DumpFailures(std::ostream& os) {
-    DCHECK_EQ(failures_.size(), failure_messages_.size());
-    for (const auto* stream : failure_messages_) {
-        os << stream->str() << "\n";
+    for (const VerifyErrorAndMessage& veam : failures_) {
+        os << veam.message.view() << "\n";
     }
     return os;
   }
@@ -1197,7 +1374,12 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
   }
   void Dump(VariableIndentationOutputStream* vios) REQUIRES_SHARED(Locks::mutator_lock_);
 
-  bool HandleMoveException(const Instruction* inst) REQUIRES_SHARED(Locks::mutator_lock_);
+  struct HandleMoveExceptionResult {
+    bool success;
+    bool skip_verification_of_exception_handler;
+  };
+  HandleMoveExceptionResult HandleMoveException(const Instruction* inst)
+      REQUIRES_SHARED(Locks::mutator_lock_);
 
   const uint32_t method_access_flags_;  // Method's access flags.
   const RegType* return_type_;  // Lazily computed return type of the method.
@@ -1234,40 +1416,98 @@ class MethodVerifier final : public ::art::verifier::MethodVerifier {
   // Instead, unset level should correspond to max().
   const uint32_t api_level_;
 
-  friend class ::art::verifier::MethodVerifier;
-
-  DISALLOW_COPY_AND_ASSIGN(MethodVerifier);
+  DISALLOW_COPY_AND_ASSIGN(MethodVerifierImpl);
 };
 
-// Note: returns true on failure.
 template <bool kVerifierDebug>
-inline bool MethodVerifier<kVerifierDebug>::FailOrAbort(bool condition,
-                                                        const char* error_msg,
-                                                        uint32_t work_insn_idx) {
-  if (kIsDebugBuild) {
-    // In a debug build, abort if the error condition is wrong. Only warn if
-    // we are already aborting (as this verification is likely run to print
-    // lock information).
-    if (LIKELY(gAborting == 0)) {
-      DCHECK(condition) << error_msg << work_insn_idx << " "
-                        << dex_file_->PrettyMethod(dex_method_idx_);
-    } else {
-      if (!condition) {
-        LOG(ERROR) << error_msg << work_insn_idx;
-        Fail(VERIFY_ERROR_BAD_CLASS_HARD) << error_msg << work_insn_idx;
-        return true;
-      }
-    }
-  } else {
-    // In a non-debug build, just fail the class.
-    if (!condition) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << error_msg << work_insn_idx;
-      return true;
-    }
-  }
+class MethodVerifier final : public MethodVerifierImpl {
+ public:
+  void FindLocksAtDexPc() REQUIRES_SHARED(Locks::mutator_lock_);
 
-  return false;
-}
+ private:
+  using MethodVerifierImpl::MethodVerifierImpl;
+
+  /* Perform detailed code-flow analysis on a single method. */
+  bool VerifyCodeFlow() REQUIRES_SHARED(Locks::mutator_lock_);
+
+  /*
+   * Perform code flow on a method.
+   *
+   * The basic strategy is as outlined in v3 4.11.1.2: set the "changed" bit on the first
+   * instruction, process it (setting additional "changed" bits), and repeat until there are no
+   * more.
+   *
+   * v3 4.11.1.1
+   * - (N/A) operand stack is always the same size
+   * - operand stack [registers] contain the correct types of values
+   * - local variables [registers] contain the correct types of values
+   * - methods are invoked with the appropriate arguments
+   * - fields are assigned using values of appropriate types
+   * - opcodes have the correct type values in operand registers
+   * - there is never an uninitialized class instance in a local variable in code protected by an
+   *   exception handler (operand stack is okay, because the operand stack is discarded when an
+   *   exception is thrown) [can't know what's a local var w/o the debug info -- should fall out of
+   *   register typing]
+   *
+   * v3 4.11.1.2
+   * - execution cannot fall off the end of the code
+   *
+   * (We also do many of the items described in the "static checks" sections, because it's easier to
+   * do them here.)
+   *
+   * We need an array of RegType values, one per register, for every instruction. If the method uses
+   * monitor-enter, we need extra data for every register, and a stack for every "interesting"
+   * instruction. In theory this could become quite large -- up to several megabytes for a monster
+   * function.
+   *
+   * NOTE:
+   * The spec forbids backward branches when there's an uninitialized reference in a register. The
+   * idea is to prevent something like this:
+   *   loop:
+   *     move r1, r0
+   *     new-instance r0, MyClass
+   *     ...
+   *     if-eq rN, loop  // once
+   *   initialize r0
+   *
+   * This leaves us with two different instances, both allocated by the same instruction, but only
+   * one is initialized. The scheme outlined in v3 4.11.1.4 wouldn't catch this, so they work around
+   * it by preventing backward branches. We achieve identical results without restricting code
+   * reordering by specifying that you can't execute the new-instance instruction if a register
+   * contains an uninitialized instance created by that same instruction.
+   */
+  template <bool kMonitorDexPCs>
+  bool CodeFlowVerifyMethod() REQUIRES_SHARED(Locks::mutator_lock_);
+
+  /*
+   * Perform verification for a single instruction.
+   *
+   * This requires fully decoding the instruction to determine the effect it has on registers.
+   *
+   * Finds zero or more following instructions and sets the "changed" flag if execution at that
+   * point needs to be (re-)evaluated. Register changes are merged into "reg_types_" at the target
+   * addresses. Does not set or clear any other flags in "insn_flags_".
+   */
+  bool CodeFlowVerifyInstruction(uint32_t* start_guess)
+      REQUIRES_SHARED(Locks::mutator_lock_);
+
+  /*
+  * Control can transfer to "next_insn". Merge the registers from merge_line into the table at
+  * next_insn, and set the changed flag on the target address if any of the registers were changed.
+  * In the case of fall-through, update the merge line on a change as it's the working line for the
+  * next instruction.
+  */
+  void UpdateRegisters(uint32_t next_insn, RegisterLine* merge_line, bool update_merge_line)
+      REQUIRES_SHARED(Locks::mutator_lock_);
+
+  // Run verification on the method. Returns true if verification completes and false if the input
+  // has an irrecoverable corruption.
+  bool Verify() override REQUIRES_SHARED(Locks::mutator_lock_);
+
+  friend class ::art::verifier::MethodVerifier;
+
+  DISALLOW_COPY_AND_ASSIGN(MethodVerifier);
+};
 
 static bool IsLargeMethod(const CodeItemDataAccessor& accessor) {
   if (!accessor.HasCodeItem()) {
@@ -1461,6 +1701,10 @@ bool MethodVerifier<kVerifierDebug>::Verify() {
     return false;
   }
 
+  if (code_item_accessor_.InsnsSizeInCodeUnits() == 0u) {
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "code item has no opcode";
+    return false;
+  }
   // Allocate and initialize an array to hold instruction data.
   insn_flags_.reset(allocator_.AllocArray<InstructionFlags>(
       code_item_accessor_.InsnsSizeInCodeUnits()));
@@ -1483,38 +1727,82 @@ bool MethodVerifier<kVerifierDebug>::Verify() {
   return result;
 }
 
-template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::ComputeWidthsAndCountOps() {
+bool MethodVerifierImpl::ComputeWidthsAndCountOps() {
   // We can't assume the instruction is well formed, handle the case where calculating the size
   // goes past the end of the code item.
-  SafeDexInstructionIterator it(code_item_accessor_.begin(), code_item_accessor_.end());
-  if (it == code_item_accessor_.end()) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "code item has no opcode";
-    return false;
-  }
-  for ( ; !it.IsErrorState() && it < code_item_accessor_.end(); ++it) {
-    // In case the instruction goes past the end of the code item, make sure to not process it.
-    SafeDexInstructionIterator next = it;
-    ++next;
-    if (next.IsErrorState()) {
-      break;
+  const uint32_t insns_size = code_item_accessor_.InsnsSizeInCodeUnits();
+  const Instruction* inst = &code_item_accessor_.InstructionAt(0u);
+  uint32_t dex_pc = 0u;
+  while (dex_pc != insns_size) {
+    const uint32_t remaining_code_units = insns_size - dex_pc;
+    const uint16_t inst_data = inst->Fetch16(0);
+    const Instruction::Code opcode = inst->Opcode(inst_data);
+    uint32_t instruction_size = 0u;
+    bool ok;
+    if (opcode == Instruction::NOP) {
+      auto check_switch = [&](uint32_t base_size, uint32_t entry_size) ALWAYS_INLINE {
+        if (UNLIKELY(base_size > remaining_code_units)) {
+          return false;
+        }
+        // This 32-bit calculation cannot overflow because `num_entries` starts as 16-bit.
+        uint32_t num_entries = inst->Fetch16(1);
+        instruction_size = base_size + num_entries * entry_size;
+        if (UNLIKELY(instruction_size > remaining_code_units)) {
+          return false;
+        }
+        return true;
+      };
+      switch (inst_data) {
+        case Instruction::kPackedSwitchSignature:
+          ok = check_switch(4u, 2u);
+          break;
+        case Instruction::kSparseSwitchSignature:
+          ok = check_switch(2u, 4u);
+          break;
+        case Instruction::kArrayDataSignature:
+          if (UNLIKELY(remaining_code_units < 4u)) {
+            ok = false;
+          } else {
+            uint16_t element_size = inst->Fetch16(1);
+            uint32_t length = inst->Fetch16(2) | (((uint32_t)inst->Fetch16(3)) << 16);
+            // Use 64-bit calculation to avoid arithmetic overflow.
+            uint64_t bytes = static_cast<uint64_t>(element_size) * static_cast<uint64_t>(length);
+            uint64_t code_units = UINT64_C(4) + (bytes + /* round up */ UINT64_C(1)) / UINT64_C(2);
+            if (UNLIKELY(code_units > remaining_code_units)) {
+              ok = false;
+            } else {
+              instruction_size = dchecked_integral_cast<uint32_t>(code_units);
+              ok = true;
+            }
+          }
+          break;
+        default:
+          instruction_size = 1u;
+          ok = true;
+          break;
+      }
+    } else {
+      instruction_size = Instruction::SizeInCodeUnits(Instruction::FormatOf(opcode));
+      DCHECK_EQ(instruction_size, inst->SizeInCodeUnits());
+      ok = LIKELY(instruction_size <= remaining_code_units);
     }
-    GetModifiableInstructionFlags(it.DexPc()).SetIsOpcode();
-  }
-
-  if (it != code_item_accessor_.end()) {
-    const size_t insns_size = code_item_accessor_.InsnsSizeInCodeUnits();
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "code did not end where expected ("
-                                      << it.DexPc() << " vs. " << insns_size << ")";
-    return false;
+    if (!ok) {
+      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "code did not end where expected ("
+                                        << dex_pc << " vs. " << insns_size << ")";
+      return false;
+    }
+    GetModifiableInstructionFlags(dex_pc).SetIsOpcode();
+    DCHECK_NE(instruction_size, 0u);
+    DCHECK_EQ(instruction_size, inst->SizeInCodeUnits());
+    DCHECK_LE(instruction_size, remaining_code_units);
+    dex_pc += instruction_size;
+    inst = inst->RelativeAt(instruction_size);
   }
   DCHECK(GetInstructionFlags(0).IsOpcode());
-
   return true;
 }
 
-template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::ScanTryCatchBlocks() {
+bool MethodVerifierImpl::ScanTryCatchBlocks() {
   const uint32_t tries_size = code_item_accessor_.TriesSize();
   if (tries_size == 0) {
     return true;
@@ -1546,14 +1834,16 @@ bool MethodVerifier<kVerifierDebug>::ScanTryCatchBlocks() {
     CatchHandlerIterator iterator(handlers_ptr);
     for (; iterator.HasNext(); iterator.Next()) {
       uint32_t dex_pc = iterator.GetHandlerAddress();
+      // `DexFileVerifier` checks that the `dex_pc` is in range.
+      DCHECK_LT(dex_pc, code_item_accessor_.InsnsSizeInCodeUnits());
       if (!GetInstructionFlags(dex_pc).IsOpcode()) {
-        Fail(VERIFY_ERROR_BAD_CLASS_HARD)
-            << "exception handler starts at bad address (" << dex_pc << ")";
+        work_insn_idx_ = dex_pc;  // Let `Fail()` record the dex PC of the failing instruction.
+        Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "exception handler starts at bad address";
         return false;
       }
-      if (!CheckNotMoveResult(code_item_accessor_.Insns(), dex_pc)) {
-        Fail(VERIFY_ERROR_BAD_CLASS_HARD)
-            << "exception handler begins with move-result* (" << dex_pc << ")";
+      if (UNLIKELY(IsMoveResult(code_item_accessor_.InstructionAt(dex_pc).Opcode()))) {
+        work_insn_idx_ = dex_pc;  // Let `Fail()` record the dex PC of the failing instruction.
+        Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "exception handler begins with move-result*";
         return false;
       }
       GetModifiableInstructionFlags(dex_pc).SetBranchTarget();
@@ -1573,8 +1863,7 @@ bool MethodVerifier<kVerifierDebug>::ScanTryCatchBlocks() {
   return true;
 }
 
-template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::VerifyInstructions() {
+bool MethodVerifierImpl::VerifyInstructions() {
   // Flag the start of the method as a branch target.
   GetModifiableInstructionFlags(0).SetBranchTarget();
   const Instruction* inst = Instruction::At(code_item_accessor_.Insns());
@@ -1623,7 +1912,7 @@ bool MethodVerifier<kVerifierDebug>::VerifyInstructions() {
 #define DEFINE_CASE(opcode, c, p, format, index, flags, eflags, vflags)             \
       case opcode: {                                                                \
         constexpr Instruction::Code kOpcode = enum_cast<Instruction::Code>(opcode); \
-        if (!VerifyInstruction<kOpcode>(inst, dex_pc, inst_data)) {                 \
+        if (!VerifyInstruction<kOpcode>(dex_pc, end_dex_pc, inst, inst_data)) {     \
           DCHECK_NE(failures_.size(), 0U);                                          \
           return false;                                                             \
         }                                                                           \
@@ -1649,11 +1938,11 @@ bool MethodVerifier<kVerifierDebug>::VerifyInstructions() {
   return true;
 }
 
-template <bool kVerifierDebug>
 template <Instruction::Code kDispatchOpcode>
-inline bool MethodVerifier<kVerifierDebug>::VerifyInstruction(const Instruction* inst,
-                                                              uint32_t code_offset,
-                                                              uint16_t inst_data) {
+inline bool MethodVerifierImpl::VerifyInstruction(uint32_t dex_pc,
+                                                  uint32_t end_dex_pc,
+                                                  const Instruction* inst,
+                                                  uint16_t inst_data) {
   // The `kDispatchOpcode` may differ from the actual opcode but it shall have the
   // same verification flags and format. We explicitly `DCHECK` these below and
   // the format is also `DCHECK`ed in VReg getters that take it as an argument.
@@ -1706,6 +1995,10 @@ inline bool MethodVerifier<kVerifierDebug>::VerifyInstruction(const Instruction*
     case Instruction::kVerifyRegBPrototype:
       result = result && CheckPrototypeIndex(inst->VRegB(kFormat, inst_data));
       break;
+    case Instruction::kVerifyRegBFilledNewArray:
+      result = result &&
+               CheckNewArray</*kFilled=*/ true>(dex::TypeIndex(inst->VRegB(kFormat, inst_data)));
+      break;
     case Instruction::kVerifyNothing:
       break;
   }
@@ -1719,7 +2012,7 @@ inline bool MethodVerifier<kVerifierDebug>::VerifyInstruction(const Instruction*
       result = result && CheckFieldIndex(inst, inst_data, inst->VRegC(kFormat));
       break;
     case Instruction::kVerifyRegCNewArray:
-      result = result && CheckNewArray(dex::TypeIndex(inst->VRegC(kFormat)));
+      result = result && CheckNewArray</*kFilled=*/ false>(dex::TypeIndex(inst->VRegC(kFormat)));
       break;
     case Instruction::kVerifyRegCType:
       result = result && CheckTypeIndex(dex::TypeIndex(inst->VRegC(kFormat)));
@@ -1743,13 +2036,13 @@ inline bool MethodVerifier<kVerifierDebug>::VerifyInstruction(const Instruction*
   DCHECK_EQ(kVerifyExtra, inst->GetVerifyExtraFlags());
   switch (kVerifyExtra) {
     case Instruction::kVerifyArrayData:
-      result = result && CheckArrayData(code_offset);
+      result = result && CheckArrayData(dex_pc, end_dex_pc, inst);
       break;
     case Instruction::kVerifyBranchTarget:
-      result = result && CheckBranchTarget(code_offset);
+      result = result && CheckAndMarkBranchTarget<kFormat>(dex_pc, end_dex_pc, inst, inst_data);
       break;
     case Instruction::kVerifySwitchTargets:
-      result = result && CheckSwitchTargets(code_offset);
+      result = result && CheckAndMarkSwitchTargets(dex_pc, end_dex_pc, inst, inst_data);
       break;
     case Instruction::kVerifyVarArgNonZero:
       // Fall-through.
@@ -1786,11 +2079,8 @@ inline bool MethodVerifier<kVerifierDebug>::VerifyInstruction(const Instruction*
   return result;
 }
 
-template <bool kVerifierDebug>
-inline bool MethodVerifier<kVerifierDebug>::CheckNewInstance(dex::TypeIndex idx) {
-  if (UNLIKELY(idx.index_ >= dex_file_->GetHeader().type_ids_size_)) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "bad type index " << idx.index_ << " (max "
-                                      << dex_file_->GetHeader().type_ids_size_ << ")";
+inline bool MethodVerifierImpl::CheckNewInstance(dex::TypeIndex idx) {
+  if (!CheckTypeIndex(idx)) {
     return false;
   }
   // We don't need the actual class, just a pointer to the class name.
@@ -1799,125 +2089,122 @@ inline bool MethodVerifier<kVerifierDebug>::CheckNewInstance(dex::TypeIndex idx)
     Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "can't call new-instance on type '" << descriptor << "'";
     return false;
   } else if (UNLIKELY(descriptor == "Ljava/lang/Class;")) {
-    // An unlikely new instance on Class is not allowed. Fall back to interpreter to ensure an
-    // exception is thrown when this statement is executed (compiled code would not do that).
+    // An unlikely new instance on Class is not allowed.
     Fail(VERIFY_ERROR_INSTANTIATION);
   }
   return true;
 }
 
-template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::CheckNewArray(dex::TypeIndex idx) {
-  if (UNLIKELY(idx.index_ >= dex_file_->GetHeader().type_ids_size_)) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "bad type index " << idx.index_ << " (max "
-                                      << dex_file_->GetHeader().type_ids_size_ << ")";
+template <bool kFilled>
+inline bool MethodVerifierImpl::CheckNewArray(dex::TypeIndex idx) {
+  if (!CheckTypeIndex(idx)) {
     return false;
   }
-  int bracket_count = 0;
   const char* descriptor = dex_file_->GetTypeDescriptor(idx);
   const char* cp = descriptor;
-  while (*cp++ == '[') {
-    bracket_count++;
+  while (*cp == '[') {
+    ++cp;
   }
-  if (UNLIKELY(bracket_count == 0)) {
+  size_t bracket_count = static_cast<size_t>(cp - descriptor);
+  if (UNLIKELY(bracket_count == 0u)) {
     /* The given class must be an array type. */
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
-        << "can't new-array class '" << descriptor << "' (not an array)";
+    FailBadNewArrayNotArray(descriptor);
     return false;
-  } else if (UNLIKELY(bracket_count > 255)) {
+  } else if (UNLIKELY(bracket_count > 255u)) {
     /* It is illegal to create an array of more than 255 dimensions. */
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
-        << "can't new-array class '" << descriptor << "' (exceeds limit)";
+    FailBadNewArrayTooManyDimensions(descriptor);
     return false;
   }
+  if (kFilled && bracket_count == 1u && UNLIKELY(*cp != 'I' && *cp != 'L')) {
+    if (UNLIKELY(*cp == 'J') || UNLIKELY(*cp == 'D')) {
+      // Forbidden, see https://source.android.com/docs/core/runtime/dalvik-bytecode .
+      FailBadFilledNewArray(descriptor);
+      return false;
+    } else {
+      // Fall back to interpreter to throw `InternalError`. Compiler does not handle this case.
+      Fail(VERIFY_ERROR_FILLED_NEW_ARRAY);
+    }
+  }
   return true;
 }
 
-template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::CheckArrayData(uint32_t cur_offset) {
-  const uint32_t insn_count = code_item_accessor_.InsnsSizeInCodeUnits();
-  const uint16_t* insns = code_item_accessor_.Insns() + cur_offset;
-  const uint16_t* array_data;
-  int32_t array_data_offset;
-
-  DCHECK_LT(cur_offset, insn_count);
-  /* make sure the start of the array data table is in range */
-  array_data_offset = insns[1] | (static_cast<int32_t>(insns[2]) << 16);
-  if (UNLIKELY(static_cast<int32_t>(cur_offset) + array_data_offset < 0 ||
-               cur_offset + array_data_offset + 2 >= insn_count)) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid array data start: at " << cur_offset
-                                      << ", data offset " << array_data_offset
-                                      << ", count " << insn_count;
+bool MethodVerifierImpl::CheckArrayData(uint32_t dex_pc,
+                                        uint32_t end_dex_pc,
+                                        const Instruction* inst) {
+  int32_t array_data_offset = inst->VRegB_31t();
+  /* Make sure the start of the array data table is in range. */
+  if (!IsOffsetInRange(dex_pc, end_dex_pc, array_data_offset)) {
+    FailTargetOffsetOutOfRange(dex_pc, end_dex_pc, array_data_offset);
     return false;
   }
-  /* offset to array data table is a relative branch-style offset */
-  array_data = insns + array_data_offset;
-  // Make sure the table is at an even dex pc, that is, 32-bit aligned.
-  if (UNLIKELY(!IsAligned<4>(array_data))) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "unaligned array data table: at " << cur_offset
-                                      << ", data offset " << array_data_offset;
+  // Make sure the array-data is marked as an opcode.
+  // This ensures that it was reached when traversing the code in `ComputeWidthsAndCountOps()`.
+  uint32_t array_data_dex_pc = dex_pc + array_data_offset;
+  if (UNLIKELY(!GetInstructionFlags(array_data_dex_pc).IsOpcode())) {
+    FailTargetMidInstruction(dex_pc, array_data_dex_pc);
     return false;
   }
-  // Make sure the array-data is marked as an opcode. This ensures that it was reached when
-  // traversing the code item linearly. It is an approximation for a by-spec padding value.
-  if (UNLIKELY(!GetInstructionFlags(cur_offset + array_data_offset).IsOpcode())) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "array data table at " << cur_offset
-                                      << ", data offset " << array_data_offset
-                                      << " not correctly visited, probably bad padding.";
+  // Make sure the table is at an even dex pc, that is, 32-bit aligned.
+  if (UNLIKELY(!IsAligned<2>(array_data_dex_pc))) {
+    FailUnalignedTableDexPc(dex_pc, array_data_dex_pc);
     return false;
   }
-
-  uint32_t value_width = array_data[1];
-  uint32_t value_count = *reinterpret_cast<const uint32_t*>(&array_data[2]);
-  uint32_t table_size = 4 + (value_width * value_count + 1) / 2;
-  /* make sure the end of the switch is in range */
-  if (UNLIKELY(cur_offset + array_data_offset + table_size > insn_count)) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid array data end: at " << cur_offset
-                                      << ", data offset " << array_data_offset << ", end "
-                                      << cur_offset + array_data_offset + table_size
-                                      << ", count " << insn_count;
+  const Instruction* array_data = inst->RelativeAt(array_data_offset);
+  DCHECK_EQ(array_data, &code_item_accessor_.InstructionAt(array_data_dex_pc));
+  DCHECK_ALIGNED(array_data, 4u);
+  // Make sure the array data has the correct signature.
+  if (UNLIKELY(array_data->Fetch16(0) != Instruction::kArrayDataSignature)) {
+    FailBadArrayDataSignature(dex_pc, array_data_dex_pc);
     return false;
   }
+  // The length of the array data has been verified by `ComputeWidthsAndCountOps()`.
+  DCHECK_LT(array_data_dex_pc, end_dex_pc);
+  DCHECK_LE(array_data->SizeInCodeUnits(), end_dex_pc - array_data_dex_pc);
   return true;
 }
 
-template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::CheckBranchTarget(uint32_t cur_offset) {
+template <Instruction::Format kFormat>
+bool MethodVerifierImpl::CheckAndMarkBranchTarget(uint32_t dex_pc,
+                                                  uint32_t end_dex_pc,
+                                                  const Instruction* inst,
+                                                  uint16_t inst_data) {
   int32_t offset;
-  bool isConditional, selfOkay;
-  if (!GetBranchOffset(cur_offset, &offset, &isConditional, &selfOkay)) {
+  if constexpr (kFormat == Instruction::k22t) {  // if-<cond>?
+    offset = inst->VRegC(kFormat);
+  } else if constexpr (kFormat == Instruction::k21t) {  // if-<cond>z?
+    offset = inst->VRegB(kFormat, /*unused*/ inst_data);
+  } else {  // goto
+    offset = inst->VRegA(kFormat, inst_data);
+  }
+  // Only `goto/32` instruction can target itself. For other instructions `offset` must not be 0.
+  DCHECK_EQ(kFormat == Instruction::k30t,
+            code_item_accessor_.InstructionAt(dex_pc).Opcode() == Instruction::GOTO_32);
+  if (kFormat != Instruction::k30t && UNLIKELY(offset == 0)) {
+    FailBranchOffsetZero(dex_pc);
     return false;
   }
-  if (UNLIKELY(!selfOkay && offset == 0)) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "branch offset of zero not allowed at"
-                                      << reinterpret_cast<void*>(cur_offset);
+  if (!IsOffsetInRange(dex_pc, end_dex_pc, offset)) {
+    FailTargetOffsetOutOfRange(dex_pc, end_dex_pc, offset);
     return false;
   }
-  // Check for 32-bit overflow. This isn't strictly necessary if we can depend on the runtime
-  // to have identical "wrap-around" behavior, but it's unwise to depend on that.
-  if (UNLIKELY(((int64_t) cur_offset + (int64_t) offset) != (int64_t) (cur_offset + offset))) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "branch target overflow "
-                                      << reinterpret_cast<void*>(cur_offset) << " +" << offset;
+  uint32_t target_dex_pc = dex_pc + offset;
+  if (UNLIKELY(!GetInstructionFlags(target_dex_pc).IsOpcode())) {
+    FailTargetMidInstruction(dex_pc, target_dex_pc);
     return false;
   }
-  int32_t abs_offset = cur_offset + offset;
-  if (UNLIKELY(abs_offset < 0 ||
-               (uint32_t) abs_offset >= code_item_accessor_.InsnsSizeInCodeUnits()  ||
-               !GetInstructionFlags(abs_offset).IsOpcode())) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid branch target " << offset << " (-> "
-                                      << reinterpret_cast<void*>(abs_offset) << ") at "
-                                      << reinterpret_cast<void*>(cur_offset);
+  Instruction::Code target_opcode = inst->RelativeAt(offset)->Opcode();
+  if (UNLIKELY(IsMoveResultOrMoveException(target_opcode))) {
+    FailBranchTargetIsMoveResultOrMoveException(dex_pc, target_dex_pc, target_opcode);
     return false;
   }
-  GetModifiableInstructionFlags(abs_offset).SetBranchTarget();
+  GetModifiableInstructionFlags(target_dex_pc).SetBranchTarget();
   return true;
 }
 
-template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::GetBranchOffset(uint32_t cur_offset,
-                                                     int32_t* pOffset,
-                                                     bool* pConditional,
-                                                     bool* selfOkay) {
+bool MethodVerifierImpl::GetBranchOffset(uint32_t cur_offset,
+                                         int32_t* pOffset,
+                                         bool* pConditional,
+                                         bool* selfOkay) {
   const uint16_t* insns = code_item_accessor_.Insns() + cur_offset;
   *pConditional = false;
   *selfOkay = false;
@@ -1953,38 +2240,37 @@ bool MethodVerifier<kVerifierDebug>::GetBranchOffset(uint32_t cur_offset,
   return true;
 }
 
-template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::CheckSwitchTargets(uint32_t cur_offset) {
-  const uint32_t insn_count = code_item_accessor_.InsnsSizeInCodeUnits();
-  DCHECK_LT(cur_offset, insn_count);
-  const uint16_t* insns = code_item_accessor_.Insns() + cur_offset;
-  /* make sure the start of the switch is in range */
-  int32_t switch_offset = insns[1] | (static_cast<int32_t>(insns[2]) << 16);
-  if (UNLIKELY(static_cast<int32_t>(cur_offset) + switch_offset < 0 ||
-               cur_offset + switch_offset + 2 > insn_count)) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid switch start: at " << cur_offset
-                                      << ", switch offset " << switch_offset
-                                      << ", count " << insn_count;
+bool MethodVerifierImpl::CheckAndMarkSwitchTargets(uint32_t dex_pc,
+                                                   uint32_t end_dex_pc,
+                                                   const Instruction* inst,
+                                                   uint16_t inst_data) {
+  int32_t switch_payload_offset = inst->VRegB_31t();
+  /* Make sure the start of the switch data is in range. */
+  if (!IsOffsetInRange(dex_pc, end_dex_pc, switch_payload_offset)) {
+    FailTargetOffsetOutOfRange(dex_pc, end_dex_pc, switch_payload_offset);
     return false;
   }
-  /* offset to switch table is a relative branch-style offset */
-  const uint16_t* switch_insns = insns + switch_offset;
-  // Make sure the table is at an even dex pc, that is, 32-bit aligned.
-  if (UNLIKELY(!IsAligned<4>(switch_insns))) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "unaligned switch table: at " << cur_offset
-                                      << ", switch offset " << switch_offset;
+  // Make sure the switch data is marked as an opcode.
+  // This ensures that it was reached when traversing the code in `ComputeWidthsAndCountOps()`.
+  uint32_t switch_payload_dex_pc = dex_pc + switch_payload_offset;
+  if (UNLIKELY(!GetInstructionFlags(switch_payload_dex_pc).IsOpcode())) {
+    FailTargetMidInstruction(dex_pc, switch_payload_dex_pc);
     return false;
   }
-  // Make sure the switch data is marked as an opcode. This ensures that it was reached when
-  // traversing the code item linearly. It is an approximation for a by-spec padding value.
-  if (UNLIKELY(!GetInstructionFlags(cur_offset + switch_offset).IsOpcode())) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "switch table at " << cur_offset
-                                      << ", switch offset " << switch_offset
-                                      << " not correctly visited, probably bad padding.";
+  // Make sure the switch data is at an even dex pc, that is, 32-bit aligned.
+  if (UNLIKELY(!IsAligned<2>(switch_payload_dex_pc))) {
+    FailUnalignedTableDexPc(dex_pc, switch_payload_dex_pc);
     return false;
   }
 
-  bool is_packed_switch = (*insns & 0xff) == Instruction::PACKED_SWITCH;
+  /* offset to switch table is a relative branch-style offset */
+  const Instruction* payload = inst->RelativeAt(switch_payload_offset);
+  DCHECK_EQ(payload, &code_item_accessor_.InstructionAt(switch_payload_dex_pc));
+  DCHECK_ALIGNED(payload, 4u);
+  const uint16_t* switch_insns = reinterpret_cast<const uint16_t*>(payload);
+
+  bool is_packed_switch = inst->Opcode(inst_data) == Instruction::PACKED_SWITCH;
+  DCHECK_IMPLIES(!is_packed_switch, inst->Opcode(inst_data) == Instruction::SPARSE_SWITCH);
 
   uint32_t switch_count = switch_insns[1];
   int32_t targets_offset;
@@ -1998,21 +2284,15 @@ bool MethodVerifier<kVerifierDebug>::CheckSwitchTargets(uint32_t cur_offset) {
     targets_offset = 2 + 2 * switch_count;
     expected_signature = Instruction::kSparseSwitchSignature;
   }
-  uint32_t table_size = targets_offset + switch_count * 2;
-  if (UNLIKELY(switch_insns[0] != expected_signature)) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD)
-        << StringPrintf("wrong signature for switch table (%x, wanted %x)",
-                        switch_insns[0], expected_signature);
-    return false;
-  }
-  /* make sure the end of the switch is in range */
-  if (UNLIKELY(cur_offset + switch_offset + table_size > (uint32_t) insn_count)) {
-    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid switch end: at " << cur_offset
-                                      << ", switch offset " << switch_offset
-                                      << ", end " << (cur_offset + switch_offset + table_size)
-                                      << ", count " << insn_count;
+  uint16_t signature = switch_insns[0];
+  if (UNLIKELY(signature != expected_signature)) {
+    FailBadSwitchPayloadSignature(dex_pc, switch_payload_dex_pc, signature, expected_signature);
     return false;
   }
+  // The table size has been verified in `ComputeWidthsAndCountOps()`.
+  uint32_t table_size = targets_offset + switch_count * 2;
+  DCHECK_LT(switch_payload_dex_pc, end_dex_pc);
+  DCHECK_LE(table_size, end_dex_pc - switch_payload_dex_pc);
 
   constexpr int32_t keys_offset = 2;
   if (switch_count > 1) {
@@ -2022,8 +2302,7 @@ bool MethodVerifier<kVerifierDebug>::CheckSwitchTargets(uint32_t cur_offset) {
       int32_t max_first_key =
           std::numeric_limits<int32_t>::max() - (static_cast<int32_t>(switch_count) - 1);
       if (UNLIKELY(first_key > max_first_key)) {
-        Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid packed switch: first_key=" << first_key
-                                          << ", switch_count=" << switch_count;
+        FailPackedSwitchKeyOverflow(dex_pc, switch_payload_dex_pc, first_key, switch_count);
         return false;
       }
     } else {
@@ -2034,8 +2313,7 @@ bool MethodVerifier<kVerifierDebug>::CheckSwitchTargets(uint32_t cur_offset) {
             static_cast<int32_t>(switch_insns[keys_offset + targ * 2]) |
             static_cast<int32_t>(switch_insns[keys_offset + targ * 2 + 1] << 16);
         if (UNLIKELY(key <= last_key)) {
-          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid sparse switch: last key=" << last_key
-                                            << ", this=" << key;
+          FailSparseSwitchPayloadKeyOrder(dex_pc, switch_payload_dex_pc, last_key, key);
           return false;
         }
         last_key = key;
@@ -2046,17 +2324,22 @@ bool MethodVerifier<kVerifierDebug>::CheckSwitchTargets(uint32_t cur_offset) {
   for (uint32_t targ = 0; targ < switch_count; targ++) {
     int32_t offset = static_cast<int32_t>(switch_insns[targets_offset + targ * 2]) |
                      static_cast<int32_t>(switch_insns[targets_offset + targ * 2 + 1] << 16);
-    int32_t abs_offset = cur_offset + offset;
-    if (UNLIKELY(abs_offset < 0 ||
-                 abs_offset >= static_cast<int32_t>(insn_count) ||
-                 !GetInstructionFlags(abs_offset).IsOpcode())) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid switch target " << offset
-                                        << " (-> " << reinterpret_cast<void*>(abs_offset) << ") at "
-                                        << reinterpret_cast<void*>(cur_offset)
-                                        << "[" << targ << "]";
+    if (!IsOffsetInRange(dex_pc, end_dex_pc, offset)) {
+      FailSwitchTargetOffsetOutOfRange(dex_pc, end_dex_pc, switch_payload_dex_pc, offset, targ);
+      return false;
+    }
+    uint32_t target_dex_pc = dex_pc + offset;
+    if (UNLIKELY(!GetInstructionFlags(target_dex_pc).IsOpcode())) {
+      FailSwitchTargetMidInstruction(dex_pc, target_dex_pc, switch_payload_dex_pc, targ);
       return false;
     }
-    GetModifiableInstructionFlags(abs_offset).SetBranchTarget();
+    Instruction::Code target_opcode = inst->RelativeAt(offset)->Opcode();
+    if (UNLIKELY(IsMoveResultOrMoveException(target_opcode))) {
+      FailSwitchTargetIsMoveResultOrMoveException(
+          dex_pc, target_dex_pc, target_opcode, switch_payload_dex_pc, targ);
+      return false;
+    }
+    GetModifiableInstructionFlags(target_dex_pc).SetBranchTarget();
   }
   return true;
 }
@@ -2070,11 +2353,10 @@ bool MethodVerifier<kVerifierDebug>::VerifyCodeFlow() {
                   code_item_accessor_.InsnsSizeInCodeUnits(),
                   registers_size,
                   allocator_,
-                  GetRegTypeCache(),
                   interesting_dex_pc_);
 
-  work_line_.reset(RegisterLine::Create(registers_size, allocator_, GetRegTypeCache()));
-  saved_line_.reset(RegisterLine::Create(registers_size, allocator_, GetRegTypeCache()));
+  work_line_.reset(RegisterLine::Create(registers_size, allocator_));
+  saved_line_.reset(RegisterLine::Create(registers_size, allocator_));
 
   /* Initialize register types of method arguments. */
   if (!SetTypesFromSignature()) {
@@ -2098,8 +2380,7 @@ bool MethodVerifier<kVerifierDebug>::VerifyCodeFlow() {
   return true;
 }
 
-template <bool kVerifierDebug>
-void MethodVerifier<kVerifierDebug>::Dump(VariableIndentationOutputStream* vios) {
+void MethodVerifierImpl::Dump(VariableIndentationOutputStream* vios) {
   if (!code_item_accessor_.HasCodeItem()) {
     vios->Stream() << "Native method\n";
     return;
@@ -2133,8 +2414,7 @@ void MethodVerifier<kVerifierDebug>::Dump(VariableIndentationOutputStream* vios)
   }
 }
 
-template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::SetTypesFromSignature() {
+bool MethodVerifierImpl::SetTypesFromSignature() {
   RegisterLine* reg_line = reg_table_.GetLine(0);
 
   // Should have been verified earlier.
@@ -2230,16 +2510,16 @@ bool MethodVerifier<kVerifierDebug>::SetTypesFromSignature() {
           return false;
         }
 
-        const RegType* lo_half;
-        const RegType* hi_half;
+        RegType::Kind lo_half;
+        RegType::Kind hi_half;
         if (descriptor[0] == 'J') {
-          lo_half = &reg_types_.LongLo();
-          hi_half = &reg_types_.LongHi();
+          lo_half = RegType::kLongLo;
+          hi_half = RegType::kLongHi;
         } else {
-          lo_half = &reg_types_.DoubleLo();
-          hi_half = &reg_types_.DoubleHi();
+          lo_half = RegType::kDoubleLo;
+          hi_half = RegType::kDoubleHi;
         }
-        reg_line->SetRegisterTypeWide(arg_start + cur_arg, *lo_half, *hi_half);
+        reg_line->SetRegisterTypeWide(arg_start + cur_arg, lo_half, hi_half);
         cur_arg++;
         break;
       }
@@ -2331,7 +2611,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyMethod() {
       if (register_line != nullptr) {
         if (work_line_->CompareLine(register_line) != 0) {
           Dump(LOG_STREAM(FATAL_WITHOUT_ABORT));
-          LOG(FATAL_WITHOUT_ABORT) << InfoMessages().str();
+          LOG(FATAL_WITHOUT_ABORT) << InfoMessages().view();
           LOG(FATAL) << "work_line diverged in " << dex_file_->PrettyMethod(dex_method_idx_)
                      << "@" << reinterpret_cast<void*>(work_insn_idx_) << "\n"
                      << " work_line=" << work_line_->Dump(this) << "\n"
@@ -2348,13 +2628,31 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyMethod() {
       HandleMonitorDexPcsWorkLine(monitor_enter_dex_pcs_, work_line_.get());
     }
 
-    if (!CodeFlowVerifyInstruction(&start_guess)) {
+    if (UNLIKELY(!CodeFlowVerifyInstruction(&start_guess))) {
+      DCHECK(flags_.have_pending_hard_failure_);
+      if (IsAotMode()) {
+        /* When AOT compiling, check that the last failure is a hard failure */
+        DCHECK(!failures_.empty());
+        if (failures_.back().error != VERIFY_ERROR_BAD_CLASS_HARD) {
+          LOG(ERROR) << "Pending failures:";
+          for (const VerifyErrorAndMessage& veam : failures_) {
+            LOG(ERROR) << veam.error << " " << veam.message.view();
+          }
+          LOG(FATAL) << "Pending hard failure, but last failure not hard.";
+        }
+      }
+      if (kVerifierDebug) {
+        InfoMessages() << "Rejecting opcode "
+                       << code_item_accessor_.InstructionAt(work_insn_idx_).DumpString(dex_file_);
+      }
+
       std::string prepend(dex_file_->PrettyMethod(dex_method_idx_));
       prepend += " failed to verify: ";
       PrependToLastFailMessage(prepend);
       return false;
     }
     /* Clear "changed" and mark as visited. */
+    DCHECK(!flags_.have_pending_hard_failure_);
     GetModifiableInstructionFlags(insn_idx).SetVisited();
     GetModifiableInstructionFlags(insn_idx).ClearChanged();
   }
@@ -2404,43 +2702,12 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyMethod() {
     // To dump the state of the verify after a method, do something like:
     // if (dex_file_->PrettyMethod(dex_method_idx_) ==
     //     "boolean java.lang.String.equals(java.lang.Object)") {
-    //   LOG(INFO) << InfoMessages().str();
+    //   LOG(INFO) << InfoMessages().view();
     // }
   }
   return true;
 }
 
-// Setup a register line for the given return instruction.
-template <bool kVerifierDebug>
-static void AdjustReturnLine(MethodVerifier<kVerifierDebug>* verifier,
-                             const Instruction* ret_inst,
-                             RegisterLine* line) {
-  Instruction::Code opcode = ret_inst->Opcode();
-
-  switch (opcode) {
-    case Instruction::RETURN_VOID:
-      if (verifier->IsInstanceConstructor()) {
-        // Before we mark all regs as conflicts, check that we don't have an uninitialized this.
-        line->CheckConstructorReturn(verifier);
-      }
-      line->MarkAllRegistersAsConflicts(verifier);
-      break;
-
-    case Instruction::RETURN:
-    case Instruction::RETURN_OBJECT:
-      line->MarkAllRegistersAsConflictsExcept(verifier, ret_inst->VRegA_11x());
-      break;
-
-    case Instruction::RETURN_WIDE:
-      line->MarkAllRegistersAsConflictsExceptWide(verifier, ret_inst->VRegA_11x());
-      break;
-
-    default:
-      LOG(FATAL) << "Unknown return opcode " << opcode;
-      UNREACHABLE();
-  }
-}
-
 template <bool kVerifierDebug>
 bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_guess) {
   /*
@@ -2488,7 +2755,6 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
   }
   // Per-instruction flag, should not be set here.
   DCHECK(!flags_.have_pending_runtime_throw_failure_);
-  bool exc_handler_unreachable = false;
 
 
   // We need to ensure the work line is consistent while performing validation. When we spot a
@@ -2498,44 +2764,65 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
   RegisterLineArenaUniquePtr fallthrough_line;
 
   using enum RegType::Kind;
-  switch (inst->Opcode()) {
+  uint16_t inst_data = inst->Fetch16(0);
+  Instruction::Code opcode = inst->Opcode(inst_data);
+  switch (opcode) {
     case Instruction::NOP:
       /*
        * A "pure" NOP has no effect on anything. Data tables start with
        * a signature that looks like a NOP; if we see one of these in
        * the course of executing code then we have a problem.
        */
-      if (inst->VRegA_10x() != 0) {
+      if (inst->VRegA_10x(inst_data) != 0) {
         Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "encountered data table in instruction stream";
+        return false;
       }
       break;
 
     case Instruction::MOVE:
-      work_line_->CopyRegister1(this, inst->VRegA_12x(), inst->VRegB_12x(), kTypeCategory1nr);
+      if (!VerifyCopyCat1(inst->VRegA_12x(inst_data), inst->VRegB_12x(inst_data))) {
+        return false;
+      }
       break;
     case Instruction::MOVE_FROM16:
-      work_line_->CopyRegister1(this, inst->VRegA_22x(), inst->VRegB_22x(), kTypeCategory1nr);
+      if (!VerifyCopyCat1(inst->VRegA_22x(inst_data), inst->VRegB_22x())) {
+        return false;
+      }
       break;
     case Instruction::MOVE_16:
-      work_line_->CopyRegister1(this, inst->VRegA_32x(), inst->VRegB_32x(), kTypeCategory1nr);
+      if (!VerifyCopyCat1(inst->VRegA_32x(), inst->VRegB_32x())) {
+        return false;
+      }
       break;
     case Instruction::MOVE_WIDE:
-      work_line_->CopyRegister2(this, inst->VRegA_12x(), inst->VRegB_12x());
+      if (!VerifyCopyCat2(inst->VRegA_12x(inst_data), inst->VRegB_12x(inst_data))) {
+        return false;
+      }
       break;
     case Instruction::MOVE_WIDE_FROM16:
-      work_line_->CopyRegister2(this, inst->VRegA_22x(), inst->VRegB_22x());
+      if (!VerifyCopyCat2(inst->VRegA_22x(inst_data), inst->VRegB_22x())) {
+        return false;
+      }
       break;
     case Instruction::MOVE_WIDE_16:
-      work_line_->CopyRegister2(this, inst->VRegA_32x(), inst->VRegB_32x());
+      if (!VerifyCopyCat2(inst->VRegA_32x(), inst->VRegB_32x())) {
+        return false;
+      }
       break;
     case Instruction::MOVE_OBJECT:
-      work_line_->CopyRegister1(this, inst->VRegA_12x(), inst->VRegB_12x(), kTypeCategoryRef);
+      if (!VerifyCopyReference(inst->VRegA_12x(inst_data), inst->VRegB_12x(inst_data))) {
+        return false;
+      }
       break;
     case Instruction::MOVE_OBJECT_FROM16:
-      work_line_->CopyRegister1(this, inst->VRegA_22x(), inst->VRegB_22x(), kTypeCategoryRef);
+      if (!VerifyCopyReference(inst->VRegA_22x(inst_data), inst->VRegB_22x())) {
+        return false;
+      }
       break;
     case Instruction::MOVE_OBJECT_16:
-      work_line_->CopyRegister1(this, inst->VRegA_32x(), inst->VRegB_32x(), kTypeCategoryRef);
+      if (!VerifyCopyReference(inst->VRegA_32x(), inst->VRegB_32x())) {
+        return false;
+      }
       break;
 
     /*
@@ -2550,160 +2837,168 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
      * easier to read in some cases.)
      */
     case Instruction::MOVE_RESULT:
-      work_line_->CopyResultRegister1(this, inst->VRegA_11x(), false);
+      work_line_->CopyResultRegister1(this, inst->VRegA_11x(inst_data), false);
       break;
     case Instruction::MOVE_RESULT_WIDE:
-      work_line_->CopyResultRegister2(this, inst->VRegA_11x());
+      work_line_->CopyResultRegister2(this, inst->VRegA_11x(inst_data));
       break;
     case Instruction::MOVE_RESULT_OBJECT:
-      work_line_->CopyResultRegister1(this, inst->VRegA_11x(), true);
+      work_line_->CopyResultRegister1(this, inst->VRegA_11x(inst_data), true);
       break;
 
-    case Instruction::MOVE_EXCEPTION:
-      if (!HandleMoveException(inst)) {
-        exc_handler_unreachable = true;
+    case Instruction::MOVE_EXCEPTION: {
+      auto result = HandleMoveException(inst);
+      if (!result.success) {
+        return false;
+      }
+      DCHECK_NE(opcode_flags & Instruction::kContinue, 0);
+      if (UNLIKELY(result.skip_verification_of_exception_handler)) {
+        // Avoid verification of the following exception handler instructions.
+        opcode_flags &= ~Instruction::kContinue;
       }
       break;
+    }
 
     case Instruction::RETURN_VOID:
-      if (!IsInstanceConstructor() || work_line_->CheckConstructorReturn(this)) {
-        if (!GetMethodReturnType().IsConflict()) {
-          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "return-void not expected";
-        }
+      if (IsInstanceConstructor() && UNLIKELY(!work_line_->CheckConstructorReturn(this))) {
+        return false;
+      }
+      if (!GetMethodReturnType().IsConflict()) {
+        Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "return-void not expected";
+        return false;
       }
       break;
-    case Instruction::RETURN:
-      if (!IsInstanceConstructor() || work_line_->CheckConstructorReturn(this)) {
-        /* check the method signature */
-        const RegType& return_type = GetMethodReturnType();
-        if (!return_type.IsCategory1Types()) {
-          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "unexpected non-category 1 return type "
-                                            << return_type;
-        } else {
-          // Compilers may generate synthetic functions that write byte values into boolean fields.
-          // Also, it may use integer values for boolean, byte, short, and character return types.
-          const uint32_t vregA = inst->VRegA_11x();
-          const RegType& src_type = work_line_->GetRegisterType(this, vregA);
-          bool use_src = ((return_type.IsBoolean() && src_type.IsByte()) ||
-                          ((return_type.IsBoolean() || return_type.IsByte() ||
-                           return_type.IsShort() || return_type.IsChar()) &&
-                           src_type.IsInteger()));
-          /* check the register contents */
-          bool success = VerifyRegisterType(vregA, use_src ? src_type : return_type);
-          if (!success) {
-            AppendToLastFailMessage(StringPrintf(" return-1nr on invalid register v%d", vregA));
-          }
+    case Instruction::RETURN: {
+      if (IsInstanceConstructor() && UNLIKELY(!work_line_->CheckConstructorReturn(this))) {
+        return false;
+      }
+      /* check the method signature */
+      const RegType& return_type = GetMethodReturnType();
+      if (!return_type.IsCategory1Types()) {
+        Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "unexpected non-category 1 return type "
+                                          << return_type;
+        return false;
+      } else {
+        // Compilers may generate synthetic functions that write byte values into boolean fields.
+        // Also, it may use integer values for boolean, byte, short, and character return types.
+        const uint32_t vregA = inst->VRegA_11x(inst_data);
+        const RegType& src_type = work_line_->GetRegisterType(this, vregA);
+        bool use_src = ((return_type.IsBoolean() && src_type.IsByte()) ||
+                        ((return_type.IsBoolean() || return_type.IsByte() ||
+                         return_type.IsShort() || return_type.IsChar()) &&
+                         src_type.IsInteger()));
+        /* check the register contents */
+        bool success = VerifyRegisterType(vregA, use_src ? src_type : return_type);
+        if (!success) {
+          LastFailureMessageStream() << " return-1nr on invalid register v" << vregA;
         }
       }
-      break;
-    case Instruction::RETURN_WIDE:
-      if (!IsInstanceConstructor() || work_line_->CheckConstructorReturn(this)) {
-        /* check the method signature */
-        const RegType& return_type = GetMethodReturnType();
-        if (!return_type.IsCategory2Types()) {
-          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "return-wide not expected";
-        } else {
-          /* check the register contents */
-          const uint32_t vregA = inst->VRegA_11x();
-          bool success = VerifyRegisterTypeWide(vregA, return_type.GetKind());
-          if (!success) {
-            AppendToLastFailMessage(StringPrintf(" return-wide on invalid register v%d", vregA));
-          }
+      break;
+    }
+    case Instruction::RETURN_WIDE: {
+      if (IsInstanceConstructor() && UNLIKELY(!work_line_->CheckConstructorReturn(this))) {
+        return false;
+      }
+      /* check the method signature */
+      const RegType& return_type = GetMethodReturnType();
+      if (!return_type.IsCategory2Types()) {
+        Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "return-wide not expected";
+        return false;
+      } else {
+        /* check the register contents */
+        const uint32_t vregA = inst->VRegA_11x(inst_data);
+        bool success = VerifyRegisterTypeWide(vregA, return_type.GetKind());
+        if (!success) {
+          LastFailureMessageStream() << " return-wide on invalid register v" << vregA;
         }
       }
       break;
-    case Instruction::RETURN_OBJECT:
-      if (!IsInstanceConstructor() || work_line_->CheckConstructorReturn(this)) {
-        const RegType& return_type = GetMethodReturnType();
-        if (!return_type.IsReferenceTypes()) {
-          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "return-object not expected";
-        } else {
-          /* return_type is the *expected* return type, not register value */
-          DCHECK(!return_type.IsZeroOrNull());
-          DCHECK(!return_type.IsUninitializedReference());
-          const uint32_t vregA = inst->VRegA_11x();
-          const RegType& reg_type = work_line_->GetRegisterType(this, vregA);
-          // Disallow returning undefined, conflict & uninitialized values and verify that the
-          // reference in vAA is an instance of the "return_type."
-          if (reg_type.IsUndefined()) {
-            Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "returning undefined register";
-          } else if (reg_type.IsConflict()) {
-            Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "returning register with conflict";
-          } else if (reg_type.IsUninitializedTypes()) {
-            Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "returning uninitialized object '"
-                                              << reg_type << "'";
-          } else if (!reg_type.IsReferenceTypes()) {
-            // We really do expect a reference here.
-            Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "return-object returns a non-reference type "
-                                              << reg_type;
-          } else if (!IsAssignableFrom(return_type, reg_type)) {
-            if (reg_type.IsUnresolvedTypes() || return_type.IsUnresolvedTypes()) {
-              Fail(VERIFY_ERROR_UNRESOLVED_TYPE_CHECK)
-                  << " can't resolve returned type '" << return_type << "' or '" << reg_type << "'";
-            } else {
-              Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "returning '" << reg_type
-                  << "', but expected from declaration '" << return_type << "'";
-            }
+    }
+    case Instruction::RETURN_OBJECT: {
+      if (IsInstanceConstructor() && UNLIKELY(!work_line_->CheckConstructorReturn(this))) {
+        return false;
+      }
+      const RegType& return_type = GetMethodReturnType();
+      if (!return_type.IsReferenceTypes()) {
+        Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "return-object not expected";
+        return false;
+      } else {
+        /* return_type is the *expected* return type, not register value */
+        DCHECK(!return_type.IsZeroOrNull());
+        DCHECK(!return_type.IsUninitializedReference());
+        const uint32_t vregA = inst->VRegA_11x(inst_data);
+        const RegType& reg_type = work_line_->GetRegisterType(this, vregA);
+        // Disallow returning undefined, conflict & uninitialized values and verify that the
+        // reference in vAA is an instance of the "return_type."
+        if (reg_type.IsUndefined()) {
+          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "returning undefined register";
+          return false;
+        } else if (reg_type.IsConflict()) {
+          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "returning register with conflict";
+          return false;
+        } else if (reg_type.IsUninitializedTypes()) {
+          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "returning uninitialized object '"
+                                            << reg_type << "'";
+          return false;
+        } else if (!reg_type.IsReferenceTypes()) {
+          // We really do expect a reference here.
+          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "return-object returns a non-reference type "
+                                            << reg_type;
+          return false;
+        } else if (!IsAssignableFrom(return_type, reg_type)) {
+          if (reg_type.IsUnresolvedTypes() || return_type.IsUnresolvedTypes()) {
+            Fail(VERIFY_ERROR_UNRESOLVED_TYPE_CHECK)
+                << " can't resolve returned type '" << return_type << "' or '" << reg_type << "'";
+          } else {
+            Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "returning '" << reg_type
+                << "', but expected from declaration '" << return_type << "'";
+            return false;
           }
         }
       }
       break;
-
+    }
       /* could be boolean, int, float, or a null reference */
     case Instruction::CONST_4: {
-      int32_t val = static_cast<int32_t>(inst->VRegB_11n() << 28) >> 28;
-      work_line_->SetRegisterType(inst->VRegA_11n(), DetermineCat1Constant(val));
+      int32_t val = static_cast<int32_t>(inst->VRegB_11n(inst_data) << 28) >> 28;
+      work_line_->SetRegisterType(inst->VRegA_11n(inst_data), DetermineCat1Constant(val));
       break;
     }
     case Instruction::CONST_16: {
       int16_t val = static_cast<int16_t>(inst->VRegB_21s());
-      work_line_->SetRegisterType(inst->VRegA_21s(), DetermineCat1Constant(val));
+      work_line_->SetRegisterType(inst->VRegA_21s(inst_data), DetermineCat1Constant(val));
       break;
     }
     case Instruction::CONST: {
       int32_t val = inst->VRegB_31i();
-      work_line_->SetRegisterType(inst->VRegA_31i(), DetermineCat1Constant(val));
+      work_line_->SetRegisterType(inst->VRegA_31i(inst_data), DetermineCat1Constant(val));
       break;
     }
     case Instruction::CONST_HIGH16: {
       int32_t val = static_cast<int32_t>(inst->VRegB_21h() << 16);
-      work_line_->SetRegisterType(inst->VRegA_21h(), DetermineCat1Constant(val));
+      work_line_->SetRegisterType(inst->VRegA_21h(inst_data), DetermineCat1Constant(val));
       break;
     }
       /* could be long or double; resolved upon use */
-    case Instruction::CONST_WIDE_16: {
-      int64_t val = static_cast<int16_t>(inst->VRegB_21s());
-      const RegType& lo = reg_types_.ConstantLo();
-      const RegType& hi = reg_types_.ConstantHi();
-      work_line_->SetRegisterTypeWide(inst->VRegA_21s(), lo, hi);
+    case Instruction::CONST_WIDE_16:
+      work_line_->SetRegisterTypeWide(inst->VRegA_21s(inst_data), kConstantLo, kConstantHi);
       break;
-    }
-    case Instruction::CONST_WIDE_32: {
-      int64_t val = static_cast<int32_t>(inst->VRegB_31i());
-      const RegType& lo = reg_types_.ConstantLo();
-      const RegType& hi = reg_types_.ConstantHi();
-      work_line_->SetRegisterTypeWide(inst->VRegA_31i(), lo, hi);
+    case Instruction::CONST_WIDE_32:
+      work_line_->SetRegisterTypeWide(inst->VRegA_31i(inst_data), kConstantLo, kConstantHi);
       break;
-    }
-    case Instruction::CONST_WIDE: {
-      int64_t val = inst->VRegB_51l();
-      const RegType& lo = reg_types_.ConstantLo();
-      const RegType& hi = reg_types_.ConstantHi();
-      work_line_->SetRegisterTypeWide(inst->VRegA_51l(), lo, hi);
+    case Instruction::CONST_WIDE:
+      work_line_->SetRegisterTypeWide(inst->VRegA_51l(inst_data), kConstantLo, kConstantHi);
       break;
-    }
-    case Instruction::CONST_WIDE_HIGH16: {
-      int64_t val = static_cast<uint64_t>(inst->VRegB_21h()) << 48;
-      const RegType& lo = reg_types_.ConstantLo();
-      const RegType& hi = reg_types_.ConstantHi();
-      work_line_->SetRegisterTypeWide(inst->VRegA_21h(), lo, hi);
+    case Instruction::CONST_WIDE_HIGH16:
+      work_line_->SetRegisterTypeWide(inst->VRegA_21h(inst_data), kConstantLo, kConstantHi);
       break;
-    }
     case Instruction::CONST_STRING:
-      work_line_->SetRegisterType<LockOp::kClear>(inst->VRegA_21c(), reg_types_.JavaLangString());
+      work_line_->SetRegisterType<LockOp::kClear>(
+          inst->VRegA_21c(inst_data), reg_types_.JavaLangString());
       break;
     case Instruction::CONST_STRING_JUMBO:
-      work_line_->SetRegisterType<LockOp::kClear>(inst->VRegA_31c(), reg_types_.JavaLangString());
+      work_line_->SetRegisterType<LockOp::kClear>(
+          inst->VRegA_31c(inst_data), reg_types_.JavaLangString());
       break;
     case Instruction::CONST_CLASS: {
       // Get type from instruction if unresolved then we need an access check
@@ -2711,20 +3006,26 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
       const RegType& res_type = ResolveClass<CheckAccess::kYes>(dex::TypeIndex(inst->VRegB_21c()));
       // Register holds class, ie its type is class, on error it will hold Conflict.
       work_line_->SetRegisterType<LockOp::kClear>(
-          inst->VRegA_21c(),
+          inst->VRegA_21c(inst_data),
           res_type.IsConflict() ? res_type : reg_types_.JavaLangClass());
       break;
     }
     case Instruction::CONST_METHOD_HANDLE:
       work_line_->SetRegisterType<LockOp::kClear>(
-          inst->VRegA_21c(), reg_types_.JavaLangInvokeMethodHandle());
+          inst->VRegA_21c(inst_data), reg_types_.JavaLangInvokeMethodHandle());
       break;
     case Instruction::CONST_METHOD_TYPE:
       work_line_->SetRegisterType<LockOp::kClear>(
-          inst->VRegA_21c(), reg_types_.JavaLangInvokeMethodType());
+          inst->VRegA_21c(inst_data), reg_types_.JavaLangInvokeMethodType());
       break;
-    case Instruction::MONITOR_ENTER:
-      work_line_->PushMonitor(this, inst->VRegA_11x(), work_insn_idx_);
+    case Instruction::MONITOR_ENTER: {
+      uint32_t vreg = inst->VRegA_11x(inst_data);
+      const RegType& reg_type = work_line_->GetRegisterType(this, vreg);
+      if (!reg_type.IsReferenceTypes()) {
+        Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "monitor-enter on non-object (" << reg_type << ")";
+        return false;
+      }
+      work_line_->PushMonitor(this, vreg, reg_type, work_insn_idx_);
       // Check whether the previous instruction is a move-object with vAA as a source, creating
       // untracked lock aliasing.
       if (0 != work_insn_idx_ && !GetInstructionFlags(work_insn_idx_).IsBranchTarget()) {
@@ -2737,13 +3038,10 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
           case Instruction::MOVE_OBJECT:
           case Instruction::MOVE_OBJECT_16:
           case Instruction::MOVE_OBJECT_FROM16:
-            if (prev_inst.VRegB() == inst->VRegA_11x()) {
+            if (static_cast<uint32_t>(prev_inst.VRegB()) == vreg) {
               // Redo the copy. This won't change the register types, but update the lock status
               // for the aliased register.
-              work_line_->CopyRegister1(this,
-                                        prev_inst.VRegA(),
-                                        prev_inst.VRegB(),
-                                        kTypeCategoryRef);
+              work_line_->CopyReference(prev_inst.VRegA(), vreg, reg_type);
             }
             break;
 
@@ -2772,16 +3070,12 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
             }
 
             // Update the lock status for the aliased register.
-            if (prev_inst.VRegA() == inst->VRegA_11x()) {
-              work_line_->CopyRegister1(this,
-                                        prev2_inst.VRegA(),
-                                        inst->VRegA_11x(),
-                                        kTypeCategoryRef);
-            } else if (prev2_inst.VRegA() == inst->VRegA_11x()) {
-              work_line_->CopyRegister1(this,
-                                        prev_inst.VRegA(),
-                                        inst->VRegA_11x(),
-                                        kTypeCategoryRef);
+            uint32_t prev_inst_vregA = prev_inst.VRegA_21c(prev_inst.Fetch16(0));
+            uint32_t prev2_inst_vregA = prev2_inst.VRegA_21c(prev2_inst.Fetch16(0));
+            if (prev_inst_vregA == vreg) {
+              work_line_->CopyReference(prev2_inst_vregA, vreg, reg_type);
+            } else if (prev2_inst_vregA == vreg) {
+              work_line_->CopyReference(prev_inst_vregA, vreg, reg_type);
             }
             break;
           }
@@ -2791,7 +3085,8 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
         }
       }
       break;
-    case Instruction::MONITOR_EXIT:
+    }
+    case Instruction::MONITOR_EXIT: {
       /*
        * monitor-exit instructions are odd. They can throw exceptions,
        * but when they do they act as if they succeeded and the PC is
@@ -2813,8 +3108,15 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
        * "live" so we still need to check it.
        */
       opcode_flags &= ~Instruction::kThrow;
-      work_line_->PopMonitor(this, inst->VRegA_11x());
+      uint32_t vreg = inst->VRegA_11x(inst_data);
+      const RegType& reg_type = work_line_->GetRegisterType(this, vreg);
+      if (!reg_type.IsReferenceTypes()) {
+        Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "monitor-exit on non-object (" << reg_type << ")";
+        return false;
+      }
+      work_line_->PopMonitor(this, vreg, reg_type);
       break;
+    }
     case Instruction::CHECK_CAST:
     case Instruction::INSTANCE_OF: {
       /*
@@ -2835,59 +3137,46 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
           Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "using primitive type "
               << dex_file_->GetTypeDescriptorView(type_idx) << " in instanceof in "
               << GetDeclaringClass();
-          break;
+          return false;
         }
 
         DCHECK_NE(failures_.size(), 0U);
         if (!is_checkcast) {
-          work_line_->SetRegisterType(inst->VRegA_22c(), kBoolean);
+          work_line_->SetRegisterType(inst->VRegA_22c(inst_data), kBoolean);
         }
         break;  // bad class
       }
       // TODO: check Compiler::CanAccessTypeWithoutChecks returns false when res_type is unresolved
-      uint32_t orig_type_reg = (is_checkcast) ? inst->VRegA_21c() : inst->VRegB_22c();
+      uint32_t orig_type_reg =
+          (is_checkcast) ? inst->VRegA_21c(inst_data) : inst->VRegB_22c(inst_data);
       const RegType& orig_type = work_line_->GetRegisterType(this, orig_type_reg);
       if (!res_type.IsNonZeroReferenceTypes()) {
-        if (is_checkcast) {
-          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "check-cast on unexpected class " << res_type;
-        } else {
-          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "instance-of on unexpected class " << res_type;
-        }
+        Fail(VERIFY_ERROR_BAD_CLASS_HARD) << opcode << " on unexpected class " << res_type;
+        return false;
       } else if (!orig_type.IsReferenceTypes()) {
-        if (is_checkcast) {
-          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "check-cast on non-reference in v" << orig_type_reg;
-        } else {
-          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "instance-of on non-reference in v" << orig_type_reg;
-        }
+        Fail(VERIFY_ERROR_BAD_CLASS_HARD) << opcode << " on non-reference in v" << orig_type_reg;
+        return false;
       } else if (orig_type.IsUninitializedTypes()) {
-        if (is_checkcast) {
-          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "check-cast on uninitialized reference in v"
-                                            << orig_type_reg;
-        } else {
-          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "instance-of on uninitialized reference in v"
-                                            << orig_type_reg;
-        }
+        Fail(VERIFY_ERROR_BAD_CLASS_HARD) << opcode << " on uninitialized reference in v"
+                                          << orig_type_reg;
+        return false;
       } else {
         if (is_checkcast) {
-          work_line_->SetRegisterType<LockOp::kKeep>(inst->VRegA_21c(), res_type);
+          work_line_->SetRegisterType<LockOp::kKeep>(inst->VRegA_21c(inst_data), res_type);
         } else {
-          work_line_->SetRegisterType(inst->VRegA_22c(), kBoolean);
+          work_line_->SetRegisterType(inst->VRegA_22c(inst_data), kBoolean);
         }
       }
       break;
     }
     case Instruction::ARRAY_LENGTH: {
-      const RegType& res_type = work_line_->GetRegisterType(this, inst->VRegB_12x());
-      if (res_type.IsReferenceTypes()) {
-        if (!res_type.IsArrayTypes() && !res_type.IsZeroOrNull()) {
-          // ie not an array or null
-          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "array-length on non-array " << res_type;
-        } else {
-          work_line_->SetRegisterType(inst->VRegA_12x(), kInteger);
-        }
-      } else {
+      const RegType& res_type = work_line_->GetRegisterType(this, inst->VRegB_12x(inst_data));
+      if (!res_type.IsReferenceTypes() || (!res_type.IsArrayTypes() && !res_type.IsZeroOrNull())) {
+        // ie not an array or null
         Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "array-length on non-array " << res_type;
+        return false;
       }
+      work_line_->SetRegisterType(inst->VRegA_12x(inst_data), kInteger);
       break;
     }
     case Instruction::NEW_INSTANCE: {
@@ -2907,44 +3196,70 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
       }
       const RegType& uninit_type = reg_types_.Uninitialized(res_type);
       // Add the new uninitialized reference to the register state and record the allocation dex pc.
-      uint32_t vA = inst->VRegA_21c();
+      uint32_t vA = inst->VRegA_21c(inst_data);
       work_line_->DCheckUniqueNewInstanceDexPc(this, work_insn_idx_);
       work_line_->SetRegisterTypeForNewInstance(vA, uninit_type, work_insn_idx_);
       break;
     }
-    case Instruction::NEW_ARRAY:
-      VerifyNewArray(inst, false, false);
+    case Instruction::NEW_ARRAY: {
+      // Make sure the "size" register has a valid type.
+      if (!VerifyRegisterType(inst->VRegB_22c(), RegType::Kind::kInteger)) {
+        return false;
+      }
+      // Dex file verifier ensures that all valid type indexes reference valid descriptors and the
+      // `CheckNewArray()` ensures that the descriptor starts with an `[` before we get to the
+      // code flow verification. So, we should see only array types here.
+      const RegType& res_type = ResolveClass<CheckAccess::kYes>(dex::TypeIndex(inst->VRegC_22c()));
+      DCHECK(res_type.IsArrayTypes());
+      // Set the register type to the array class.
+      work_line_->SetRegisterType<LockOp::kClear>(inst->VRegA_22c(), res_type);
       break;
+    }
     case Instruction::FILLED_NEW_ARRAY:
-      VerifyNewArray(inst, true, false);
+      if (!VerifyFilledNewArray(inst, /*is_range=*/ false)) {
+        return false;
+      }
       just_set_result = true;  // Filled new array sets result register
       break;
     case Instruction::FILLED_NEW_ARRAY_RANGE:
-      VerifyNewArray(inst, true, true);
+      if (!VerifyFilledNewArray(inst, /*is_range=*/ true)) {
+        return false;
+      }
       just_set_result = true;  // Filled new array range sets result register
       break;
     case Instruction::CMPL_FLOAT:
     case Instruction::CMPG_FLOAT:
-      CheckBinaryOp(inst, kInteger, kFloat, kFloat, /*check_boolean_op=*/ false);
+      if (!CheckBinaryOp(inst, inst_data, kInteger, kFloat, kFloat, /*check_boolean_op=*/ false)) {
+        return false;
+      }
       break;
     case Instruction::CMPL_DOUBLE:
     case Instruction::CMPG_DOUBLE:
-      CheckBinaryOpWideCmp(inst, kInteger, kDoubleLo, kDoubleLo);
+      if (!CheckBinaryOpWideCmp(inst, inst_data, kInteger, kDoubleLo, kDoubleLo)) {
+        return false;
+      }
       break;
     case Instruction::CMP_LONG:
-      CheckBinaryOpWideCmp(inst, kInteger, kLongLo, kLongLo);
+      if (!CheckBinaryOpWideCmp(inst, inst_data, kInteger, kLongLo, kLongLo)) {
+        return false;
+      }
       break;
     case Instruction::THROW: {
-      const RegType& res_type = work_line_->GetRegisterType(this, inst->VRegA_11x());
+      const RegType& res_type = work_line_->GetRegisterType(this, inst->VRegA_11x(inst_data));
       if (!IsAssignableFrom(reg_types_.JavaLangThrowable(), res_type)) {
         if (res_type.IsUninitializedTypes()) {
           Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "thrown exception not initialized";
+          return false;
         } else if (!res_type.IsReferenceTypes()) {
           Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "thrown value of non-reference type " << res_type;
+          return false;
         } else {
-          Fail(res_type.IsUnresolvedTypes()
-                  ? VERIFY_ERROR_UNRESOLVED_TYPE_CHECK : VERIFY_ERROR_BAD_CLASS_HARD)
+          bool unresolved = res_type.IsUnresolvedTypes();
+          Fail(unresolved ? VERIFY_ERROR_UNRESOLVED_TYPE_CHECK : VERIFY_ERROR_BAD_CLASS_HARD)
                 << "thrown class " << res_type << " not instanceof Throwable";
+          if (!unresolved) {
+            return false;
+          }
         }
       }
       break;
@@ -2958,42 +3273,43 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
     case Instruction::PACKED_SWITCH:
     case Instruction::SPARSE_SWITCH:
       /* verify that vAA is an integer, or can be converted to one */
-      VerifyRegisterType(inst->VRegA_31t(), kInteger);
+      VerifyRegisterType(inst->VRegA_31t(inst_data), kInteger);
       break;
 
     case Instruction::FILL_ARRAY_DATA: {
       /* Similar to the verification done for APUT */
-      const RegType& array_type = work_line_->GetRegisterType(this, inst->VRegA_31t());
+      const RegType& array_type = work_line_->GetRegisterType(this, inst->VRegA_31t(inst_data));
       /* array_type can be null if the reg type is Zero */
       if (!array_type.IsZeroOrNull()) {
         if (!array_type.IsArrayTypes()) {
           Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid fill-array-data with array type "
                                             << array_type;
+          return false;
         } else if (array_type.IsUnresolvedTypes()) {
           // If it's an unresolved array type, it must be non-primitive.
           Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid fill-array-data for array of type "
                                             << array_type;
+          return false;
         } else {
           const RegType& component_type = reg_types_.GetComponentType(array_type);
           DCHECK(!component_type.IsConflict());
           if (component_type.IsNonZeroReferenceTypes()) {
             Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid fill-array-data with component type "
                                               << component_type;
+            return false;
           } else {
             // Now verify if the element width in the table matches the element width declared in
-            // the array
+            // the array. The signature has been verified by `CheckArrayData()`.
             const uint16_t* array_data =
                 insns + (insns[1] | (static_cast<int32_t>(insns[2]) << 16));
-            if (array_data[0] != Instruction::kArrayDataSignature) {
-              Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invalid magic for array-data";
-            } else {
-              size_t elem_width = Primitive::ComponentSize(component_type.GetPrimitiveType());
-              // Since we don't compress the data in Dex, expect to see equal width of data stored
-              // in the table and expected from the array class.
-              if (array_data[1] != elem_width) {
-                Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "array-data size mismatch (" << array_data[1]
-                                                  << " vs " << elem_width << ")";
-              }
+            DCHECK_EQ(array_data[0], Instruction::kArrayDataSignature);
+            size_t elem_width = Primitive::ComponentSize(component_type.GetPrimitiveType());
+            // Since we don't compress the data in Dex, expect to see equal width of data stored
+            // in the table and expected from the array class.
+            if (array_data[1] != elem_width) {
+              Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "array-data size mismatch (" << array_data[1]
+                                                << " vs " << elem_width << ")";
+              return false;
             }
           }
         }
@@ -3002,8 +3318,8 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
     }
     case Instruction::IF_EQ:
     case Instruction::IF_NE: {
-      const RegType& reg_type1 = work_line_->GetRegisterType(this, inst->VRegA_22t());
-      const RegType& reg_type2 = work_line_->GetRegisterType(this, inst->VRegB_22t());
+      const RegType& reg_type1 = work_line_->GetRegisterType(this, inst->VRegA_22t(inst_data));
+      const RegType& reg_type2 = work_line_->GetRegisterType(this, inst->VRegB_22t(inst_data));
       bool mismatch = false;
       if (reg_type1.IsZeroOrNull()) {  // zero then integral or reference expected
         mismatch = !reg_type2.IsReferenceTypes() && !reg_type2.IsIntegralTypes();
@@ -3015,6 +3331,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
       if (mismatch) {
         Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "args to if-eq/if-ne (" << reg_type1 << ","
                                           << reg_type2 << ") must both be references or integral";
+        return false;
       }
       break;
     }
@@ -3022,20 +3339,22 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
     case Instruction::IF_GE:
     case Instruction::IF_GT:
     case Instruction::IF_LE: {
-      const RegType& reg_type1 = work_line_->GetRegisterType(this, inst->VRegA_22t());
-      const RegType& reg_type2 = work_line_->GetRegisterType(this, inst->VRegB_22t());
+      const RegType& reg_type1 = work_line_->GetRegisterType(this, inst->VRegA_22t(inst_data));
+      const RegType& reg_type2 = work_line_->GetRegisterType(this, inst->VRegB_22t(inst_data));
       if (!reg_type1.IsIntegralTypes() || !reg_type2.IsIntegralTypes()) {
         Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "args to 'if' (" << reg_type1 << ","
                                           << reg_type2 << ") must be integral";
+        return false;
       }
       break;
     }
     case Instruction::IF_EQZ:
     case Instruction::IF_NEZ: {
-      const RegType& reg_type = work_line_->GetRegisterType(this, inst->VRegA_21t());
+      const RegType& reg_type = work_line_->GetRegisterType(this, inst->VRegA_21t(inst_data));
       if (!reg_type.IsReferenceTypes() && !reg_type.IsIntegralTypes()) {
         Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "type " << reg_type
                                           << " unexpected as arg to if-eqz/if-nez";
+        return false;
       }
 
       // Find previous instruction - its existence is a precondition to peephole optimization.
@@ -3065,7 +3384,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
        */
       if (!CurrentInsnFlags()->IsBranchTarget() &&
           (Instruction::INSTANCE_OF == instance_of_inst.Opcode()) &&
-          (inst->VRegA_21t() == instance_of_inst.VRegA_22c()) &&
+          (inst->VRegA_21t(inst_data) == instance_of_inst.VRegA_22c()) &&
           (instance_of_inst.VRegA_22c() != instance_of_inst.VRegB_22c())) {
         // Check the type of the instance-of is different than that of registers type, as if they
         // are the same there is no work to be done here. Check that the conversion is not to or
@@ -3091,8 +3410,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
             !orig_type.IsZeroOrNull() &&
             IsStrictlyAssignableFrom(orig_type, cast_type.Merge(orig_type, &reg_types_, this))) {
           RegisterLine* update_line = RegisterLine::Create(code_item_accessor_.RegistersSize(),
-                                                           allocator_,
-                                                           GetRegTypeCache());
+                                                           allocator_);
           if (inst->Opcode() == Instruction::IF_EQZ) {
             fallthrough_line.reset(update_line);
           } else {
@@ -3143,10 +3461,11 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
     case Instruction::IF_GEZ:
     case Instruction::IF_GTZ:
     case Instruction::IF_LEZ: {
-      const RegType& reg_type = work_line_->GetRegisterType(this, inst->VRegA_21t());
+      const RegType& reg_type = work_line_->GetRegisterType(this, inst->VRegA_21t(inst_data));
       if (!reg_type.IsIntegralTypes()) {
         Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "type " << reg_type
                                           << " unexpected as arg to if-ltz/if-gez/if-gtz/if-lez";
+        return false;
       }
       break;
     }
@@ -3300,7 +3619,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
                          dex_file_->GetTypeDescriptorView(return_type_idx));
       const RegType& return_type = reg_types_.FromTypeIndex(return_type_idx);
       if (!return_type.IsLowHalf()) {
-        work_line_->SetResultRegisterType(this, return_type);
+        work_line_->SetResultRegisterType(return_type);
       } else {
         work_line_->SetResultRegisterTypeWide(return_type, return_type.HighHalf(&reg_types_));
       }
@@ -3329,20 +3648,21 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
          * this method (which implies that we're in a constructor ourselves).
          */
         const RegType& this_type = GetInvocationThis(inst);
-        if (this_type.IsConflict())  // failure.
-          break;
+        if (this_type.IsConflict()) {  // failure.
+          return false;
+        }
 
         /* no null refs allowed (?) */
         if (this_type.IsZeroOrNull()) {
           Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "unable to initialize null ref";
-          break;
+          return false;
         }
 
         /* arg must be an uninitialized reference */
         if (!this_type.IsUninitializedTypes()) {
           Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "Expected initialization on uninitialized reference "
               << this_type;
-          break;
+          return false;
         }
 
         // Note: According to JLS, constructors are never inherited. Therefore the target
@@ -3363,7 +3683,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
       }
       const RegType& return_type = reg_types_.FromTypeIndex(return_type_idx);
       if (!return_type.IsLowHalf()) {
-        work_line_->SetResultRegisterType(this, return_type);
+        work_line_->SetResultRegisterType(return_type);
       } else {
         work_line_->SetResultRegisterTypeWide(return_type, return_type.HighHalf(&reg_types_));
       }
@@ -3382,7 +3702,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
                          dex_file_->GetTypeDescriptorView(return_type_idx));
       const RegType& return_type = reg_types_.FromTypeIndex(return_type_idx);
       if (!return_type.IsLowHalf()) {
-        work_line_->SetResultRegisterType(this, return_type);
+        work_line_->SetResultRegisterType(return_type);
       } else {
         work_line_->SetResultRegisterTypeWide(return_type, return_type.HighHalf(&reg_types_));
       }
@@ -3411,7 +3731,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
         if (this_type.IsUninitializedTypes()) {
           Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "interface call on uninitialized object "
               << this_type;
-          break;
+          return false;
         }
         // In the past we have tried to assert that "called_interface" is assignable
         // from "this_type.GetClass()", however, as we do an imprecise Join
@@ -3433,7 +3753,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
                          dex_file_->GetTypeDescriptorView(return_type_idx));
       const RegType& return_type = reg_types_.FromTypeIndex(return_type_idx);
       if (!return_type.IsLowHalf()) {
-        work_line_->SetResultRegisterType(this, return_type);
+        work_line_->SetResultRegisterType(return_type);
       } else {
         work_line_->SetResultRegisterTypeWide(return_type, return_type.HighHalf(&reg_types_));
       }
@@ -3446,13 +3766,12 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
       ArtMethod* called_method = VerifyInvocationArgs(inst, METHOD_POLYMORPHIC, is_range);
       if (called_method == nullptr) {
         // Convert potential soft failures in VerifyInvocationArgs() to hard errors.
-        if (failure_messages_.size() > 0) {
-          std::string message = failure_messages_.back()->str();
-          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << message;
-        } else {
-          Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "invoke-polymorphic verification failure.";
-        }
-        break;
+        std::string_view message = failures_.empty() ? "invoke-polymorphic verification failure."
+                                                     : failures_.back().message.view();
+        // Note: Adding another failure to `failures_` does not invalidate the view of
+        // the previous message (if any) -  the list node holding it is not even moved.
+        Fail(VERIFY_ERROR_BAD_CLASS_HARD) << message;
+        return false;
       }
       if (!CheckSignaturePolymorphicMethod(called_method) ||
           !CheckSignaturePolymorphicReceiver(inst)) {
@@ -3463,7 +3782,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
       const RegType& return_type =
           reg_types_.FromTypeIndex(dex_file_->GetProtoId(proto_idx).return_type_idx_);
       if (!return_type.IsLowHalf()) {
-        work_line_->SetResultRegisterType(this, return_type);
+        work_line_->SetResultRegisterType(return_type);
       } else {
         work_line_->SetResultRegisterTypeWide(return_type, return_type.HighHalf(&reg_types_));
       }
@@ -3494,7 +3813,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
       // Step 3. Propagate return type information
       const RegType& return_type = reg_types_.FromTypeIndex(proto_id.return_type_idx_);
       if (!return_type.IsLowHalf()) {
-        work_line_->SetResultRegisterType(this, return_type);
+        work_line_->SetResultRegisterType(return_type);
       } else {
         work_line_->SetResultRegisterTypeWide(return_type, return_type.HighHalf(&reg_types_));
       }
@@ -3503,62 +3822,100 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
     }
     case Instruction::NEG_INT:
     case Instruction::NOT_INT:
-      CheckUnaryOp(inst, kInteger, kInteger);
+      if (!CheckUnaryOp(inst, inst_data, kInteger, kInteger)) {
+        return false;
+      }
       break;
     case Instruction::NEG_LONG:
     case Instruction::NOT_LONG:
-      CheckUnaryOpWide(inst, kLongLo, kLongLo);
+      if (!CheckUnaryOpWide(inst, inst_data, kLongLo, kLongLo)) {
+        return false;
+      }
       break;
     case Instruction::NEG_FLOAT:
-      CheckUnaryOp(inst, kFloat, kFloat);
+      if (!CheckUnaryOp(inst, inst_data, kFloat, kFloat)) {
+        return false;
+      }
       break;
     case Instruction::NEG_DOUBLE:
-      CheckUnaryOpWide(inst, kDoubleLo, kDoubleLo);
+      if (!CheckUnaryOpWide(inst, inst_data, kDoubleLo, kDoubleLo)) {
+        return false;
+      }
       break;
     case Instruction::INT_TO_LONG:
-      CheckUnaryOpToWide(inst, kLongLo, kInteger);
+      if (!CheckUnaryOpToWide(inst, inst_data, kLongLo, kInteger)) {
+        return false;
+      }
       break;
     case Instruction::INT_TO_FLOAT:
-      CheckUnaryOp(inst, kFloat, kInteger);
+      if (!CheckUnaryOp(inst, inst_data, kFloat, kInteger)) {
+        return false;
+      }
       break;
     case Instruction::INT_TO_DOUBLE:
-      CheckUnaryOpToWide(inst, kDoubleLo, kInteger);
+      if (!CheckUnaryOpToWide(inst, inst_data, kDoubleLo, kInteger)) {
+        return false;
+      }
       break;
     case Instruction::LONG_TO_INT:
-      CheckUnaryOpFromWide(inst, kInteger, kLongLo);
+      if (!CheckUnaryOpFromWide(inst, inst_data, kInteger, kLongLo)) {
+        return false;
+      }
       break;
     case Instruction::LONG_TO_FLOAT:
-      CheckUnaryOpFromWide(inst, kFloat, kLongLo);
+      if (!CheckUnaryOpFromWide(inst, inst_data, kFloat, kLongLo)) {
+        return false;
+      }
       break;
     case Instruction::LONG_TO_DOUBLE:
-      CheckUnaryOpWide(inst, kDoubleLo, kLongLo);
+      if (!CheckUnaryOpWide(inst, inst_data, kDoubleLo, kLongLo)) {
+        return false;
+      }
       break;
     case Instruction::FLOAT_TO_INT:
-      CheckUnaryOp(inst, kInteger, kFloat);
+      if (!CheckUnaryOp(inst, inst_data, kInteger, kFloat)) {
+        return false;
+      }
       break;
     case Instruction::FLOAT_TO_LONG:
-      CheckUnaryOpToWide(inst, kLongLo, kFloat);
+      if (!CheckUnaryOpToWide(inst, inst_data, kLongLo, kFloat)) {
+        return false;
+      }
       break;
     case Instruction::FLOAT_TO_DOUBLE:
-      CheckUnaryOpToWide(inst, kDoubleLo, kFloat);
+      if (!CheckUnaryOpToWide(inst, inst_data, kDoubleLo, kFloat)) {
+        return false;
+      }
       break;
     case Instruction::DOUBLE_TO_INT:
-      CheckUnaryOpFromWide(inst, kInteger, kDoubleLo);
+      if (!CheckUnaryOpFromWide(inst, inst_data, kInteger, kDoubleLo)) {
+        return false;
+      }
       break;
     case Instruction::DOUBLE_TO_LONG:
-      CheckUnaryOpWide(inst, kLongLo, kDoubleLo);
+      if (!CheckUnaryOpWide(inst, inst_data, kLongLo, kDoubleLo)) {
+        return false;
+      }
       break;
     case Instruction::DOUBLE_TO_FLOAT:
-      CheckUnaryOpFromWide(inst, kFloat, kDoubleLo);
+      if (!CheckUnaryOpFromWide(inst, inst_data, kFloat, kDoubleLo)) {
+        return false;
+      }
       break;
     case Instruction::INT_TO_BYTE:
-      CheckUnaryOp(inst, kByte, kInteger);
+      if (!CheckUnaryOp(inst, inst_data, kByte, kInteger)) {
+        return false;
+      }
       break;
     case Instruction::INT_TO_CHAR:
-      CheckUnaryOp(inst, kChar, kInteger);
+      if (!CheckUnaryOp(inst, inst_data, kChar, kInteger)) {
+        return false;
+      }
       break;
     case Instruction::INT_TO_SHORT:
-      CheckUnaryOp(inst, kShort, kInteger);
+      if (!CheckUnaryOp(inst, inst_data, kShort, kInteger)) {
+        return false;
+      }
       break;
 
     case Instruction::ADD_INT:
@@ -3569,12 +3926,18 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
     case Instruction::SHL_INT:
     case Instruction::SHR_INT:
     case Instruction::USHR_INT:
-      CheckBinaryOp(inst, kInteger, kInteger, kInteger, /*check_boolean_op=*/ false);
+      if (!CheckBinaryOp(
+               inst, inst_data, kInteger, kInteger, kInteger, /*check_boolean_op=*/ false)) {
+        return false;
+      }
       break;
     case Instruction::AND_INT:
     case Instruction::OR_INT:
     case Instruction::XOR_INT:
-      CheckBinaryOp(inst, kInteger, kInteger, kInteger, /*check_boolean_op=*/ true);
+      if (!CheckBinaryOp(
+               inst, inst_data, kInteger, kInteger, kInteger, /*check_boolean_op=*/ true)) {
+        return false;
+      }
       break;
     case Instruction::ADD_LONG:
     case Instruction::SUB_LONG:
@@ -3584,27 +3947,35 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
     case Instruction::AND_LONG:
     case Instruction::OR_LONG:
     case Instruction::XOR_LONG:
-      CheckBinaryOpWide(inst, kLongLo, kLongLo, kLongLo);
+      if (!CheckBinaryOpWide(inst, inst_data, kLongLo, kLongLo, kLongLo)) {
+        return false;
+      }
       break;
     case Instruction::SHL_LONG:
     case Instruction::SHR_LONG:
     case Instruction::USHR_LONG:
       /* shift distance is Int, making these different from other binary operations */
-      CheckBinaryOpWideShift(inst, kLongLo, kInteger);
+      if (!CheckBinaryOpWideShift(inst, inst_data, kLongLo, kInteger)) {
+        return false;
+      }
       break;
     case Instruction::ADD_FLOAT:
     case Instruction::SUB_FLOAT:
     case Instruction::MUL_FLOAT:
     case Instruction::DIV_FLOAT:
     case Instruction::REM_FLOAT:
-      CheckBinaryOp(inst, kFloat, kFloat, kFloat, /*check_boolean_op=*/ false);
+      if (!CheckBinaryOp(inst, inst_data, kFloat, kFloat, kFloat, /*check_boolean_op=*/ false)) {
+        return false;
+      }
       break;
     case Instruction::ADD_DOUBLE:
     case Instruction::SUB_DOUBLE:
     case Instruction::MUL_DOUBLE:
     case Instruction::DIV_DOUBLE:
     case Instruction::REM_DOUBLE:
-      CheckBinaryOpWide(inst, kDoubleLo, kDoubleLo, kDoubleLo);
+      if (!CheckBinaryOpWide(inst, inst_data, kDoubleLo, kDoubleLo, kDoubleLo)) {
+        return false;
+      }
       break;
     case Instruction::ADD_INT_2ADDR:
     case Instruction::SUB_INT_2ADDR:
@@ -3613,15 +3984,24 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
     case Instruction::SHL_INT_2ADDR:
     case Instruction::SHR_INT_2ADDR:
     case Instruction::USHR_INT_2ADDR:
-      CheckBinaryOp2addr(inst, kInteger, kInteger, kInteger, /*check_boolean_op=*/ false);
+      if (!CheckBinaryOp2addr(
+               inst, inst_data, kInteger, kInteger, kInteger, /*check_boolean_op=*/ false)) {
+        return false;
+      }
       break;
     case Instruction::AND_INT_2ADDR:
     case Instruction::OR_INT_2ADDR:
     case Instruction::XOR_INT_2ADDR:
-      CheckBinaryOp2addr(inst, kInteger, kInteger, kInteger, /*check_boolean_op=*/ true);
+      if (!CheckBinaryOp2addr(
+               inst, inst_data, kInteger, kInteger, kInteger, /*check_boolean_op=*/ true)) {
+        return false;
+      }
       break;
     case Instruction::DIV_INT_2ADDR:
-      CheckBinaryOp2addr(inst, kInteger, kInteger, kInteger, /*check_boolean_op=*/ false);
+      if (!CheckBinaryOp2addr(
+               inst, inst_data, kInteger, kInteger, kInteger, /*check_boolean_op=*/ false)) {
+        return false;
+      }
       break;
     case Instruction::ADD_LONG_2ADDR:
     case Instruction::SUB_LONG_2ADDR:
@@ -3631,38 +4011,53 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
     case Instruction::AND_LONG_2ADDR:
     case Instruction::OR_LONG_2ADDR:
     case Instruction::XOR_LONG_2ADDR:
-      CheckBinaryOp2addrWide(inst, kLongLo, kLongLo, kLongLo);
+      if (!CheckBinaryOp2addrWide(inst, inst_data, kLongLo, kLongLo, kLongLo)) {
+        return false;
+      }
       break;
     case Instruction::SHL_LONG_2ADDR:
     case Instruction::SHR_LONG_2ADDR:
     case Instruction::USHR_LONG_2ADDR:
-      CheckBinaryOp2addrWideShift(inst, kLongLo, kInteger);
+      if (!CheckBinaryOp2addrWideShift(inst, inst_data, kLongLo, kInteger)) {
+        return false;
+      }
       break;
     case Instruction::ADD_FLOAT_2ADDR:
     case Instruction::SUB_FLOAT_2ADDR:
     case Instruction::MUL_FLOAT_2ADDR:
     case Instruction::DIV_FLOAT_2ADDR:
     case Instruction::REM_FLOAT_2ADDR:
-      CheckBinaryOp2addr(inst, kFloat, kFloat, kFloat, /*check_boolean_op=*/ false);
+      if (!CheckBinaryOp2addr(
+               inst, inst_data, kFloat, kFloat, kFloat, /*check_boolean_op=*/ false)) {
+        return false;
+      }
       break;
     case Instruction::ADD_DOUBLE_2ADDR:
     case Instruction::SUB_DOUBLE_2ADDR:
     case Instruction::MUL_DOUBLE_2ADDR:
     case Instruction::DIV_DOUBLE_2ADDR:
     case Instruction::REM_DOUBLE_2ADDR:
-      CheckBinaryOp2addrWide(inst, kDoubleLo, kDoubleLo, kDoubleLo);
+      if (!CheckBinaryOp2addrWide(inst, inst_data, kDoubleLo, kDoubleLo, kDoubleLo)) {
+        return false;
+      }
       break;
     case Instruction::ADD_INT_LIT16:
     case Instruction::RSUB_INT_LIT16:
     case Instruction::MUL_INT_LIT16:
     case Instruction::DIV_INT_LIT16:
     case Instruction::REM_INT_LIT16:
-      CheckLiteralOp(inst, kInteger, kInteger, /*check_boolean_op=*/ false, /*is_lit16=*/ true);
+      if (!CheckLiteralOp</*kIsLit16=*/ true>(
+               inst, inst_data, kInteger, kInteger, /*check_boolean_op=*/ false)) {
+        return false;
+      }
       break;
     case Instruction::AND_INT_LIT16:
     case Instruction::OR_INT_LIT16:
     case Instruction::XOR_INT_LIT16:
-      CheckLiteralOp(inst, kInteger, kInteger, /*check_boolean_op=*/ true, /*is_lit16=*/ true);
+      if (!CheckLiteralOp</*kIsLit16=*/ true>(
+               inst, inst_data, kInteger, kInteger, /*check_boolean_op=*/ true)) {
+        return false;
+      }
       break;
     case Instruction::ADD_INT_LIT8:
     case Instruction::RSUB_INT_LIT8:
@@ -3672,12 +4067,18 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
     case Instruction::SHL_INT_LIT8:
     case Instruction::SHR_INT_LIT8:
     case Instruction::USHR_INT_LIT8:
-      CheckLiteralOp(inst, kInteger, kInteger, /*check_boolean_op=*/ false, /*is_lit16=*/ false);
+      if (!CheckLiteralOp</*kIsLit16=*/ false>(
+               inst, inst_data, kInteger, kInteger, /*check_boolean_op=*/ false)) {
+        return false;
+      }
       break;
     case Instruction::AND_INT_LIT8:
     case Instruction::OR_INT_LIT8:
     case Instruction::XOR_INT_LIT8:
-      CheckLiteralOp(inst, kInteger, kInteger, /*check_boolean_op=*/ true, /*is_lit16=*/ false);
+      if (!CheckLiteralOp</*kIsLit16=*/ false>(
+               inst, inst_data, kInteger, kInteger, /*check_boolean_op=*/ true)) {
+        return false;
+      }
       break;
 
     /* These should never appear during verification. */
@@ -3687,7 +4088,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
     case Instruction::UNUSED_79:
     case Instruction::UNUSED_7A:
       Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "Unexpected opcode " << inst->DumpString(dex_file_);
-      break;
+      return false;
 
     /*
      * DO NOT add a "default" clause here. Without it the compiler will
@@ -3696,21 +4097,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
   }  // end - switch (dec_insn.opcode)
 
   if (flags_.have_pending_hard_failure_) {
-    if (IsAotMode()) {
-      /* When AOT compiling, check that the last failure is a hard failure */
-      if (failures_[failures_.size() - 1] != VERIFY_ERROR_BAD_CLASS_HARD) {
-        LOG(ERROR) << "Pending failures:";
-        for (auto& error : failures_) {
-          LOG(ERROR) << error;
-        }
-        for (auto& error_msg : failure_messages_) {
-          LOG(ERROR) << error_msg->str();
-        }
-        LOG(FATAL) << "Pending hard failure, but last failure not hard.";
-      }
-    }
     /* immediate failure, reject class */
-    InfoMessages() << "Rejecting opcode " << inst->DumpString(dex_file_);
     return false;
   } else if (flags_.have_pending_runtime_throw_failure_) {
     LogVerifyInfo() << "Elevating opcode flags from " << opcode_flags << " to Throw";
@@ -3726,7 +4113,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
    * not expensive and it makes our debugging output cleaner.)
    */
   if (!just_set_result) {
-    work_line_->SetResultTypeToUnknown(GetRegTypeCache());
+    work_line_->SetResultTypeToUnknown();
   }
 
   /*
@@ -3749,19 +4136,12 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
       return false;
     }
     DCHECK_EQ(isConditional, (opcode_flags & Instruction::kContinue) != 0);
-    if (!CheckNotMoveExceptionOrMoveResult(code_item_accessor_.Insns(),
-                                           work_insn_idx_ + branch_target)) {
-      return false;
-    }
+    DCHECK(!IsMoveResultOrMoveException(inst->RelativeAt(branch_target)->Opcode()));
     /* update branch target, set "changed" if appropriate */
     if (nullptr != branch_line) {
-      if (!UpdateRegisters(work_insn_idx_ + branch_target, branch_line.get(), false)) {
-        return false;
-      }
+      UpdateRegisters(work_insn_idx_ + branch_target, branch_line.get(), false);
     } else {
-      if (!UpdateRegisters(work_insn_idx_ + branch_target, work_line_.get(), false)) {
-        return false;
-      }
+      UpdateRegisters(work_insn_idx_ + branch_target, work_line_.get(), false);
     }
   }
 
@@ -3796,12 +4176,8 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
          (static_cast<int32_t>(switch_insns[offset_to_targets + targ * 2 + 1]) << 16);
       abs_offset = work_insn_idx_ + offset;
       DCHECK_LT(abs_offset, code_item_accessor_.InsnsSizeInCodeUnits());
-      if (!CheckNotMoveExceptionOrMoveResult(code_item_accessor_.Insns(), abs_offset)) {
-        return false;
-      }
-      if (!UpdateRegisters(abs_offset, work_line_.get(), false)) {
-        return false;
-      }
+      DCHECK(!IsMoveResultOrMoveException(inst->RelativeAt(offset)->Opcode()));
+      UpdateRegisters(abs_offset, work_line_.get(), false);
     }
   }
 
@@ -3845,9 +4221,7 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
         LogVerifyInfo() << "Updating exception handler 0x"
                         << std::hex << iterator.GetHandlerAddress();
       }
-      if (!UpdateRegisters(iterator.GetHandlerAddress(), saved_line_.get(), false)) {
-        return false;
-      }
+      UpdateRegisters(iterator.GetHandlerAddress(), saved_line_.get(), false);
     }
 
     /*
@@ -3873,35 +4247,30 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
    *        because it changes work_line_ when performing peephole optimization
    *        and this change should not be used in those cases.
    */
-  if ((opcode_flags & Instruction::kContinue) != 0 && !exc_handler_unreachable) {
+  if ((opcode_flags & Instruction::kContinue) != 0) {
     DCHECK_EQ(&code_item_accessor_.InstructionAt(work_insn_idx_), inst);
     uint32_t next_insn_idx = work_insn_idx_ + inst->SizeInCodeUnits();
     if (next_insn_idx >= code_item_accessor_.InsnsSizeInCodeUnits()) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "Execution can walk off end of code area";
+      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "Can flow through to end of code area";
       return false;
     }
     // The only way to get to a move-exception instruction is to get thrown there. Make sure the
     // next instruction isn't one.
-    if (!CheckNotMoveException(code_item_accessor_.Insns(), next_insn_idx)) {
+    Instruction::Code next_opcode = code_item_accessor_.InstructionAt(next_insn_idx).Opcode();
+    if (UNLIKELY(next_opcode == Instruction::MOVE_EXCEPTION)) {
+      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "Can flow through to move-exception";
       return false;
     }
     if (nullptr != fallthrough_line) {
       // Make workline consistent with fallthrough computed from peephole optimization.
       work_line_->CopyFromLine(fallthrough_line.get());
     }
-    if (GetInstructionFlags(next_insn_idx).IsReturn()) {
-      // For returns we only care about the operand to the return, all other registers are dead.
-      const Instruction* ret_inst = &code_item_accessor_.InstructionAt(next_insn_idx);
-      AdjustReturnLine(this, ret_inst, work_line_.get());
-    }
     RegisterLine* next_line = reg_table_.GetLine(next_insn_idx);
     if (next_line != nullptr) {
       // Merge registers into what we have for the next instruction, and set the "changed" flag if
       // needed. If the merge changes the state of the registers then the work line will be
       // updated.
-      if (!UpdateRegisters(next_insn_idx, work_line_.get(), true)) {
-        return false;
-      }
+      UpdateRegisters(next_insn_idx, work_line_.get(), true);
     } else {
       /*
        * We're not recording register data for the next instruction, so we don't know what the
@@ -3942,9 +4311,8 @@ bool MethodVerifier<kVerifierDebug>::CodeFlowVerifyInstruction(uint32_t* start_g
   return true;
 }  // NOLINT(readability/fn_size)
 
-template <bool kVerifierDebug>
 template <CheckAccess C>
-const RegType& MethodVerifier<kVerifierDebug>::ResolveClass(dex::TypeIndex class_idx) {
+const RegType& MethodVerifierImpl::ResolveClass(dex::TypeIndex class_idx) {
   // FIXME: `RegTypeCache` can currently return a few fundamental classes such as j.l.Object
   // or j.l.Class without resolving them using the current class loader and recording them
   // in the corresponding `ClassTable`. The subsequent method and field lookup by callers of
@@ -3997,101 +4365,106 @@ const RegType& MethodVerifier<kVerifierDebug>::ResolveClass(dex::TypeIndex class
   return result;
 }
 
-template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::HandleMoveException(const Instruction* inst)  {
+MethodVerifierImpl::HandleMoveExceptionResult
+MethodVerifierImpl::HandleMoveException(const Instruction* inst)  {
   // We do not allow MOVE_EXCEPTION as the first instruction in a method. This is a simple case
   // where one entrypoint to the catch block is not actually an exception path.
   if (work_insn_idx_ == 0) {
     Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "move-exception at pc 0x0";
-    return true;
+    return {false, false};
   }
   /*
    * This statement can only appear as the first instruction in an exception handler. We verify
    * that as part of extracting the exception type from the catch block list.
    */
-  auto caught_exc_type_fn = [&]() REQUIRES_SHARED(Locks::mutator_lock_) ->
-      std::pair<bool, const RegType*> {
-    const RegType* common_super = nullptr;
-    if (code_item_accessor_.TriesSize() != 0) {
-      const uint8_t* handlers_ptr = code_item_accessor_.GetCatchHandlerData();
-      uint32_t handlers_size = DecodeUnsignedLeb128(&handlers_ptr);
-      const RegType* unresolved = nullptr;
-      for (uint32_t i = 0; i < handlers_size; i++) {
-        CatchHandlerIterator iterator(handlers_ptr);
-        for (; iterator.HasNext(); iterator.Next()) {
-          if (iterator.GetHandlerAddress() == (uint32_t) work_insn_idx_) {
-            if (!iterator.GetHandlerTypeIndex().IsValid()) {
-              common_super = &reg_types_.JavaLangThrowable();
-            } else {
-              // Do access checks only on resolved exception classes.
-              const RegType& exception =
-                  ResolveClass<CheckAccess::kOnResolvedClass>(iterator.GetHandlerTypeIndex());
-              if (!IsAssignableFrom(reg_types_.JavaLangThrowable(), exception)) {
-                DCHECK(!exception.IsUninitializedTypes());  // Comes from dex, shouldn't be uninit.
-                if (exception.IsUnresolvedTypes()) {
-                  if (unresolved == nullptr) {
-                    unresolved = &exception;
-                  } else {
-                    unresolved = &unresolved->SafeMerge(exception, &reg_types_, this);
-                  }
+  const RegType* common_super = nullptr;
+  const RegType* unresolved = nullptr;
+  if (code_item_accessor_.TriesSize() != 0) {
+    const uint8_t* handlers_ptr = code_item_accessor_.GetCatchHandlerData();
+    uint32_t handlers_size = DecodeUnsignedLeb128(&handlers_ptr);
+    for (uint32_t i = 0; i < handlers_size; i++) {
+      CatchHandlerIterator iterator(handlers_ptr);
+      for (; iterator.HasNext(); iterator.Next()) {
+        if (iterator.GetHandlerAddress() == (uint32_t) work_insn_idx_) {
+          if (!iterator.GetHandlerTypeIndex().IsValid()) {
+            common_super = &reg_types_.JavaLangThrowable();
+          } else {
+            // Do access checks only on resolved exception classes.
+            const RegType& exception =
+                ResolveClass<CheckAccess::kOnResolvedClass>(iterator.GetHandlerTypeIndex());
+            if (!IsAssignableFrom(reg_types_.JavaLangThrowable(), exception)) {
+              DCHECK(!exception.IsUninitializedTypes());  // Comes from dex, shouldn't be uninit.
+              if (exception.IsUnresolvedTypes()) {
+                if (unresolved == nullptr) {
+                  unresolved = &exception;
                 } else {
-                  Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "unexpected non-throwable class "
-                                                    << exception;
-                  return std::make_pair(true, &reg_types_.Conflict());
+                  unresolved = &unresolved->SafeMerge(exception, &reg_types_, this);
                 }
-              } else if (common_super == nullptr) {
-                common_super = &exception;
-              } else if (common_super->Equals(exception)) {
-                // odd case, but nothing to do
               } else {
-                common_super = &common_super->Merge(exception, &reg_types_, this);
-                if (FailOrAbort(IsAssignableFrom(reg_types_.JavaLangThrowable(), *common_super),
-                                "java.lang.Throwable is not assignable-from common_super at ",
-                                work_insn_idx_)) {
-                  break;
-                }
+                Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "unexpected non-throwable class "
+                                                  << exception;
+                return {false, false};
+              }
+            } else if (common_super == nullptr) {
+              common_super = &exception;
+            } else if (common_super->Equals(exception)) {
+              // odd case, but nothing to do
+            } else {
+              common_super = &common_super->Merge(exception, &reg_types_, this);
+              if (UNLIKELY(!IsAssignableFrom(reg_types_.JavaLangThrowable(), *common_super))) {
+                Fail(VERIFY_ERROR_BAD_CLASS_HARD)
+                    << "java.lang.Throwable is not assignable-from common_super";
+                return {false, false};
               }
             }
           }
         }
-        handlers_ptr = iterator.EndDataPointer();
-      }
-      if (unresolved != nullptr) {
-        // Soft-fail, but do not handle this with a synthetic throw.
-        Fail(VERIFY_ERROR_UNRESOLVED_TYPE_CHECK, /*pending_exc=*/ false)
-            << "Unresolved catch handler";
-        bool should_continue = true;
-        if (common_super != nullptr) {
-          unresolved = &unresolved->Merge(*common_super, &reg_types_, this);
-        } else {
-          should_continue = !PotentiallyMarkRuntimeThrow();
-        }
-        return std::make_pair(should_continue, unresolved);
+      }
+      handlers_ptr = iterator.EndDataPointer();
+    }
+  }
+  const RegType* reg_type = nullptr;
+  bool skip_verification_of_exception_handler = false;
+  if (unresolved != nullptr) {
+    // Soft-fail, but do not handle this with a synthetic throw.
+    Fail(VERIFY_ERROR_UNRESOLVED_TYPE_CHECK, /*pending_exc=*/ false)
+        << "Unresolved catch handler";
+    bool should_continue = true;
+    if (common_super != nullptr) {
+      reg_type = &unresolved->Merge(*common_super, &reg_types_, this);
+    } else {
+      reg_type = unresolved;
+      if (!IsAotMode() && !IsSdkVersionSetAndAtLeast(api_level_, SdkVersion::kS_V2)) {
+        // This is an unreachable handler at runtime. For older API levels, we avoid the
+        // verification of the entire handler for compatibility reasons. The instruction
+        // doesn't throw, but we mark the method as having a pending runtime throw failure
+        // so that the JIT compiler does not try to compile it - the compiler expects all
+        // instructions to be properly verified and may crash otherwise.
+        Fail(VERIFY_ERROR_RUNTIME_THROW, /* pending_exc= */ false);
+        skip_verification_of_exception_handler = true;
       }
     }
-    if (common_super == nullptr) {
-      /* No catch block */
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "unable to find exception handler";
-      return std::make_pair(true, &reg_types_.Conflict());
-    }
+  } else if (common_super == nullptr) {
+    /* No catch block */
+    Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "unable to find exception handler";
+    return {false, false};
+  } else {
     DCHECK(common_super->HasClass());
     CheckForFinalAbstractClass(common_super->GetClass());
-    return std::make_pair(true, common_super);
-  };
-  auto result = caught_exc_type_fn();
-  work_line_->SetRegisterType<LockOp::kClear>(inst->VRegA_11x(), *result.second);
-  return result.first;
+    reg_type = common_super;
+  }
+  DCHECK(reg_type != nullptr);
+  work_line_->SetRegisterType<LockOp::kClear>(inst->VRegA_11x(), *reg_type);
+  return {true, skip_verification_of_exception_handler};
 }
 
-template <bool kVerifierDebug>
-ArtMethod* MethodVerifier<kVerifierDebug>::ResolveMethodAndCheckAccess(
+ArtMethod* MethodVerifierImpl::ResolveMethodAndCheckAccess(
     uint32_t dex_method_idx, MethodType method_type) {
   const dex::MethodId& method_id = dex_file_->GetMethodId(dex_method_idx);
   const RegType& klass_type = ResolveClass<CheckAccess::kYes>(method_id.class_idx_);
   if (klass_type.IsConflict()) {
-    std::string append(" in attempt to access method ");
-    append += dex_file_->GetMethodName(method_id);
-    AppendToLastFailMessage(append);
+    LastFailureMessageStream()
+        << " in attempt to access method " << dex_file_->GetMethodName(method_id);
     return nullptr;
   }
   if (klass_type.IsUnresolvedTypes()) {
@@ -4160,6 +4533,16 @@ ArtMethod* MethodVerifier<kVerifierDebug>::ResolveMethodAndCheckAccess(
           << " is in an interface class " << klass->PrettyClass();
       return nullptr;
     }
+    if (method_type == METHOD_SUPER &&
+        res_method->GetDeclaringClass()->IsObjectClass()) {
+      Fail(VERIFY_ERROR_NO_METHOD) << "invoke-super " << klass->PrettyDescriptor() << "."
+                                   << dex_file_->GetMethodName(method_id) << " "
+                                   << dex_file_->GetMethodSignature(method_id) << " resolved to "
+                                   << "object method " << res_method->PrettyMethod() << " "
+                                   << "but Object methods are excluded from super "
+                                   << "method resolution on interfaces.";
+      return nullptr;
+    }
   } else {
     if (method_type == METHOD_INTERFACE) {
       Fail(VERIFY_ERROR_CLASS_CHANGE)
@@ -4221,9 +4604,8 @@ ArtMethod* MethodVerifier<kVerifierDebug>::ResolveMethodAndCheckAccess(
   return res_method;
 }
 
-template <bool kVerifierDebug>
 template <class T>
-ArtMethod* MethodVerifier<kVerifierDebug>::VerifyInvocationArgsFromIterator(
+ArtMethod* MethodVerifierImpl::VerifyInvocationArgsFromIterator(
     T* it, const Instruction* inst, MethodType method_type, bool is_range, ArtMethod* res_method) {
   DCHECK_EQ(!is_range, inst->HasVarArgs());
 
@@ -4362,10 +4744,9 @@ ArtMethod* MethodVerifier<kVerifierDebug>::VerifyInvocationArgsFromIterator(
   return res_method;
 }
 
-template <bool kVerifierDebug>
-void MethodVerifier<kVerifierDebug>::VerifyInvocationArgsUnresolvedMethod(const Instruction* inst,
-                                                                          MethodType method_type,
-                                                                          bool is_range) {
+void MethodVerifierImpl::VerifyInvocationArgsUnresolvedMethod(const Instruction* inst,
+                                                              MethodType method_type,
+                                                              bool is_range) {
   // As the method may not have been resolved, make this static check against what we expect.
   // The main reason for this code block is to fail hard when we find an illegal use, e.g.,
   // wrong number of arguments or wrong primitive types, even if the method could not be resolved.
@@ -4375,8 +4756,7 @@ void MethodVerifier<kVerifierDebug>::VerifyInvocationArgsUnresolvedMethod(const
   VerifyInvocationArgsFromIterator(&it, inst, method_type, is_range, nullptr);
 }
 
-template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::CheckCallSite(uint32_t call_site_idx) {
+bool MethodVerifierImpl::CheckCallSite(uint32_t call_site_idx) {
   if (call_site_idx >= dex_file_->NumCallSiteIds()) {
     Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "Bad call site id #" << call_site_idx
                                       << " >= " << dex_file_->NumCallSiteIds();
@@ -4416,7 +4796,11 @@ bool MethodVerifier<kVerifierDebug>::CheckCallSite(uint32_t call_site_idx) {
                                         << index[i] << " >= " << type_and_max[i].second;
       return false;
     }
-    it.Next();
+
+    // Don't increase if we are going to read past the item.
+    if (i != kRequiredArguments - 1) {
+      it.Next();
+    }
   }
 
   // Check method handle kind is valid.
@@ -4430,8 +4814,7 @@ bool MethodVerifier<kVerifierDebug>::CheckCallSite(uint32_t call_site_idx) {
   return true;
 }
 
-template <bool kVerifierDebug>
-ArtMethod* MethodVerifier<kVerifierDebug>::VerifyInvocationArgs(
+ArtMethod* MethodVerifierImpl::VerifyInvocationArgs(
     const Instruction* inst, MethodType method_type, bool is_range) {
   // Resolve the method. This could be an abstract or concrete method depending on what sort of call
   // we're making.
@@ -4523,8 +4906,7 @@ ArtMethod* MethodVerifier<kVerifierDebug>::VerifyInvocationArgs(
   return verified_method;
 }
 
-template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::CheckSignaturePolymorphicMethod(ArtMethod* method) {
+bool MethodVerifierImpl::CheckSignaturePolymorphicMethod(ArtMethod* method) {
   ObjPtr<mirror::Class> klass = method->GetDeclaringClass();
   const char* method_name = method->GetName();
 
@@ -4572,8 +4954,7 @@ bool MethodVerifier<kVerifierDebug>::CheckSignaturePolymorphicMethod(ArtMethod*
   return true;
 }
 
-template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::CheckSignaturePolymorphicReceiver(const Instruction* inst) {
+bool MethodVerifierImpl::CheckSignaturePolymorphicReceiver(const Instruction* inst) {
   const RegType& this_type = GetInvocationThis(inst);
   if (this_type.IsZeroOrNull()) {
     /* null pointer always passes (and always fails at run time) */
@@ -4606,61 +4987,46 @@ bool MethodVerifier<kVerifierDebug>::CheckSignaturePolymorphicReceiver(const Ins
   return true;
 }
 
-template <bool kVerifierDebug>
-void MethodVerifier<kVerifierDebug>::VerifyNewArray(const Instruction* inst,
-                                                    bool is_filled,
-                                                    bool is_range) {
+bool MethodVerifierImpl::VerifyFilledNewArray(const Instruction* inst, bool is_range) {
   dex::TypeIndex type_idx;
-  if (!is_filled) {
-    DCHECK_EQ(inst->Opcode(), Instruction::NEW_ARRAY);
-    type_idx = dex::TypeIndex(inst->VRegC_22c());
-  } else if (!is_range) {
+  if (!is_range) {
     DCHECK_EQ(inst->Opcode(), Instruction::FILLED_NEW_ARRAY);
     type_idx = dex::TypeIndex(inst->VRegB_35c());
   } else {
     DCHECK_EQ(inst->Opcode(), Instruction::FILLED_NEW_ARRAY_RANGE);
     type_idx = dex::TypeIndex(inst->VRegB_3rc());
   }
+  // Dex file verifier ensures that all valid type indexes reference valid descriptors and the
+  // `CheckNewArray()` ensures that the descriptor starts with an `[` before we get to the
+  // code flow verification. So, we should see only array types here.
   const RegType& res_type = ResolveClass<CheckAccess::kYes>(type_idx);
-  if (res_type.IsConflict()) {  // bad class
-    DCHECK_NE(failures_.size(), 0U);
-  } else {
-    // TODO: check Compiler::CanAccessTypeWithoutChecks returns false when res_type is unresolved
-    if (!res_type.IsArrayTypes()) {
-      Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "new-array on non-array class " << res_type;
-    } else if (!is_filled) {
-      /* make sure "size" register is valid type */
-      VerifyRegisterType(inst->VRegB_22c(), RegType::Kind::kInteger);
-      /* set register type to array class */
-      work_line_->SetRegisterType<LockOp::kClear>(inst->VRegA_22c(), res_type);
-    } else {
-      DCHECK(!res_type.IsUnresolvedMergedReference());
-      // Verify each register. If "arg_count" is bad, VerifyRegisterType() will run off the end of
-      // the list and fail. It's legal, if silly, for arg_count to be zero.
-      const RegType& expected_type = reg_types_.GetComponentType(res_type);
-      uint32_t arg_count = (is_range) ? inst->VRegA_3rc() : inst->VRegA_35c();
-      uint32_t arg[5];
-      if (!is_range) {
-        inst->GetVarArgs(arg);
-      }
-      for (size_t ui = 0; ui < arg_count; ui++) {
-        uint32_t get_reg = is_range ? inst->VRegC_3rc() + ui : arg[ui];
-        VerifyRegisterType(get_reg, expected_type);
-        if (flags_.have_pending_hard_failure_) {
-          // Don't continue on hard failures.
-          return;
-        }
-      }
-      // filled-array result goes into "result" register
-      work_line_->SetResultRegisterType(this, res_type);
+  DCHECK(res_type.IsArrayTypes());
+  // TODO: check Compiler::CanAccessTypeWithoutChecks returns false when res_type is unresolved
+  DCHECK(!res_type.IsUnresolvedMergedReference());
+  // Verify each input register. It's legal, if silly, for arg_count to be zero.
+  const RegType& expected_type = reg_types_.GetComponentType(res_type);
+  uint32_t arg_count = (is_range) ? inst->VRegA_3rc() : inst->VRegA_35c();
+  uint32_t arg[5];
+  if (!is_range) {
+    inst->GetVarArgs(arg);
+  }
+  for (size_t ui = 0; ui < arg_count; ui++) {
+    uint32_t get_reg = is_range ? inst->VRegC_3rc() + ui : arg[ui];
+    if (!VerifyRegisterType(get_reg, expected_type)) {
+      // Don't continue on hard failures.
+      DCHECK(flags_.have_pending_hard_failure_);
+      return false;
     }
+    DCHECK(!flags_.have_pending_hard_failure_);
   }
+  // filled-array result goes into "result" register
+  work_line_->SetResultRegisterType(res_type);
+  return true;
 }
 
-template <bool kVerifierDebug>
-void MethodVerifier<kVerifierDebug>::VerifyAGet(const Instruction* inst,
-                                                const RegType& insn_type,
-                                                bool is_primitive) {
+void MethodVerifierImpl::VerifyAGet(const Instruction* inst,
+                                    const RegType& insn_type,
+                                    bool is_primitive) {
   const RegType& index_type = work_line_->GetRegisterType(this, inst->VRegC_23x());
   if (!index_type.IsArrayIndexTypes()) {
     Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "Invalid reg type for array index (" << index_type << ")";
@@ -4682,8 +5048,8 @@ void MethodVerifier<kVerifierDebug>::VerifyAGet(const Instruction* inst,
       } else {
         // Category 2
         work_line_->SetRegisterTypeWide(inst->VRegA_23x(),
-                                        reg_types_.ConstantLo(),
-                                        reg_types_.ConstantHi());
+                                        RegType::kConstantLo,
+                                        RegType::kConstantHi);
       }
     } else if (!array_type.IsArrayTypes()) {
       Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "not array type " << array_type << " with aget";
@@ -4727,9 +5093,7 @@ void MethodVerifier<kVerifierDebug>::VerifyAGet(const Instruction* inst,
   }
 }
 
-template <bool kVerifierDebug>
-void MethodVerifier<kVerifierDebug>::VerifyPrimitivePut(const RegType& target_type,
-                                                        uint32_t vregA) {
+void MethodVerifierImpl::VerifyPrimitivePut(const RegType& target_type, uint32_t vregA) {
   // Primitive assignability rules are weaker than regular assignability rules.
   bool value_compatible;
   const RegType& value_type = work_line_->GetRegisterType(this, vregA);
@@ -4755,10 +5119,9 @@ void MethodVerifier<kVerifierDebug>::VerifyPrimitivePut(const RegType& target_ty
   }
 }
 
-template <bool kVerifierDebug>
-void MethodVerifier<kVerifierDebug>::VerifyAPut(const Instruction* inst,
-                                                const RegType& insn_type,
-                                                bool is_primitive) {
+void MethodVerifierImpl::VerifyAPut(const Instruction* inst,
+                                    const RegType& insn_type,
+                                    bool is_primitive) {
   const RegType& index_type = work_line_->GetRegisterType(this, inst->VRegC_23x());
   if (!index_type.IsArrayIndexTypes()) {
     Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "Invalid reg type for array index (" << index_type << ")";
@@ -4836,8 +5199,7 @@ void MethodVerifier<kVerifierDebug>::VerifyAPut(const Instruction* inst,
   }
 }
 
-template <bool kVerifierDebug>
-ArtField* MethodVerifier<kVerifierDebug>::GetStaticField(uint32_t field_idx, bool is_put) {
+ArtField* MethodVerifierImpl::GetStaticField(uint32_t field_idx, bool is_put) {
   const dex::FieldId& field_id = dex_file_->GetFieldId(field_idx);
   // Check access to class
   const RegType& klass_type = ResolveClass<CheckAccess::kYes>(field_id.class_idx_);
@@ -4872,10 +5234,7 @@ ArtField* MethodVerifier<kVerifierDebug>::GetStaticField(uint32_t field_idx, boo
   return GetISFieldCommon(field, is_put);
 }
 
-template <bool kVerifierDebug>
-ArtField* MethodVerifier<kVerifierDebug>::GetInstanceField(uint32_t vregB,
-                                                           uint32_t field_idx,
-                                                           bool is_put) {
+ArtField* MethodVerifierImpl::GetInstanceField(uint32_t vregB, uint32_t field_idx, bool is_put) {
   const RegType& obj_type = work_line_->GetRegisterType(this, vregB);
   if (!obj_type.IsReferenceTypes()) {
     // Trying to read a field from something that isn't a reference.
@@ -4993,8 +5352,7 @@ ArtField* MethodVerifier<kVerifierDebug>::GetInstanceField(uint32_t vregB,
   return GetISFieldCommon(field, is_put);
 }
 
-template <bool kVerifierDebug>
-ArtField* MethodVerifier<kVerifierDebug>::GetISFieldCommon(ArtField* field, bool is_put) {
+ArtField* MethodVerifierImpl::GetISFieldCommon(ArtField* field, bool is_put) {
   DCHECK(field != nullptr);
   if (!CanAccessMember(field->GetDeclaringClass(), field->GetAccessFlags())) {
     Fail(VERIFY_ERROR_ACCESS_FIELD)
@@ -5012,12 +5370,11 @@ ArtField* MethodVerifier<kVerifierDebug>::GetISFieldCommon(ArtField* field, bool
   return field;
 }
 
-template <bool kVerifierDebug>
 template <FieldAccessType kAccType>
-void MethodVerifier<kVerifierDebug>::VerifyISFieldAccess(const Instruction* inst,
-                                                         bool is_primitive,
-                                                         bool is_static) {
-  uint32_t field_idx = GetFieldIdxOfFieldAccess(inst, is_static);
+void MethodVerifierImpl::VerifyISFieldAccess(const Instruction* inst,
+                                             bool is_primitive,
+                                             bool is_static) {
+  uint32_t field_idx = GetFieldIdxOfFieldAccess(inst);
   DCHECK(!flags_.have_pending_hard_failure_);
   ArtField* field;
   if (is_static) {
@@ -5056,9 +5413,10 @@ void MethodVerifier<kVerifierDebug>::VerifyISFieldAccess(const Instruction* inst
 }
 
 template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::UpdateRegisters(uint32_t next_insn,
+void MethodVerifier<kVerifierDebug>::UpdateRegisters(uint32_t next_insn,
                                                      RegisterLine* merge_line,
                                                      bool update_merge_line) {
+  DCHECK(!flags_.have_pending_hard_failure_);
   bool changed = true;
   RegisterLine* target_line = reg_table_.GetLine(next_insn);
   if (!GetInstructionFlags(next_insn).IsVisitedOrChanged()) {
@@ -5068,29 +5426,13 @@ bool MethodVerifier<kVerifierDebug>::UpdateRegisters(uint32_t next_insn,
      * only way a register can transition out of "unknown", so this is not just an optimization.)
      */
     target_line->CopyFromLine(merge_line);
-    if (GetInstructionFlags(next_insn).IsReturn()) {
-      // Verify that the monitor stack is empty on return.
-      merge_line->VerifyMonitorStackEmpty(this);
-
-      // For returns we only care about the operand to the return, all other registers are dead.
-      // Initialize them as conflicts so they don't add to GC and deoptimization information.
-      const Instruction* ret_inst = &code_item_accessor_.InstructionAt(next_insn);
-      AdjustReturnLine(this, ret_inst, target_line);
-      // Directly bail if a hard failure was found.
-      if (flags_.have_pending_hard_failure_) {
-        return false;
-      }
-    }
   } else {
     RegisterLineArenaUniquePtr copy;
     if (kVerifierDebug) {
-      copy.reset(RegisterLine::Create(target_line->NumRegs(), allocator_, GetRegTypeCache()));
+      copy.reset(RegisterLine::Create(target_line->NumRegs(), allocator_));
       copy->CopyFromLine(target_line);
     }
     changed = target_line->MergeRegisters(this, merge_line);
-    if (flags_.have_pending_hard_failure_) {
-      return false;
-    }
     if (kVerifierDebug && changed) {
       LogVerifyInfo() << "Merging at [" << reinterpret_cast<void*>(work_insn_idx_) << "]"
                       << " to [" << reinterpret_cast<void*>(next_insn) << "]: " << "\n"
@@ -5105,11 +5447,10 @@ bool MethodVerifier<kVerifierDebug>::UpdateRegisters(uint32_t next_insn,
   if (changed) {
     GetModifiableInstructionFlags(next_insn).SetChanged();
   }
-  return true;
+  DCHECK(!flags_.have_pending_hard_failure_);
 }
 
-template <bool kVerifierDebug>
-const RegType& MethodVerifier<kVerifierDebug>::GetMethodReturnType() {
+const RegType& MethodVerifierImpl::GetMethodReturnType() {
   if (return_type_ == nullptr) {
     const dex::MethodId& method_id = dex_file_->GetMethodId(dex_method_idx_);
     const dex::ProtoId& proto_id = dex_file_->GetMethodPrototype(method_id);
@@ -5118,8 +5459,7 @@ const RegType& MethodVerifier<kVerifierDebug>::GetMethodReturnType() {
   return *return_type_;
 }
 
-template <bool kVerifierDebug>
-RegType::Kind MethodVerifier<kVerifierDebug>::DetermineCat1Constant(int32_t value) {
+RegType::Kind MethodVerifierImpl::DetermineCat1Constant(int32_t value) {
   // Imprecise constant type.
   if (value < -32768) {
     return RegType::Kind::kIntegerConstant;
@@ -5142,10 +5482,9 @@ RegType::Kind MethodVerifier<kVerifierDebug>::DetermineCat1Constant(int32_t valu
   }
 }
 
-template <bool kVerifierDebug>
-bool MethodVerifier<kVerifierDebug>::PotentiallyMarkRuntimeThrow() {
+void MethodVerifierImpl::PotentiallyMarkRuntimeThrow() {
   if (IsAotMode() || IsSdkVersionSetAndAtLeast(api_level_, SdkVersion::kS_V2)) {
-    return false;
+    return;
   }
   // Compatibility mode: we treat the following code unreachable and the verifier
   // will not analyze it.
@@ -5154,13 +5493,7 @@ bool MethodVerifier<kVerifierDebug>::PotentiallyMarkRuntimeThrow() {
   if (work_insn_idx_ < dex::kDexNoIndex) {
     const Instruction& inst = code_item_accessor_.InstructionAt(work_insn_idx_);
     Instruction::Code opcode = inst.Opcode();
-    if (opcode == Instruction::MOVE_EXCEPTION) {
-      // This is an unreachable handler. The instruction doesn't throw, but we
-      // mark the method as having a pending runtime throw failure so that
-      // the compiler does not try to compile it.
-      Fail(VERIFY_ERROR_RUNTIME_THROW, /* pending_exc= */ false);
-      return true;
-    }
+    DCHECK_NE(opcode, Instruction::MOVE_EXCEPTION);
     // How to handle runtime failures for instructions that are not flagged kThrow.
     if ((Instruction::FlagsOf(opcode) & Instruction::kThrow) == 0 &&
         !impl::IsCompatThrow(opcode) &&
@@ -5176,7 +5509,6 @@ bool MethodVerifier<kVerifierDebug>::PotentiallyMarkRuntimeThrow() {
     }
   }
   flags_.have_pending_runtime_throw_failure_ = true;
-  return true;
 }
 
 }  // namespace
@@ -5203,6 +5535,7 @@ MethodVerifier::MethodVerifier(Thread* self,
       dex_file_(reg_types->GetDexFile()),
       class_def_(class_def),
       code_item_accessor_(*dex_file_, code_item),
+      failures_(allocator_.Adapter(kArenaAllocVerifier)),
       flags_{ .have_pending_hard_failure_ = false, .have_pending_runtime_throw_failure_ = false },
       const_flags_{ .aot_mode_ = aot_mode, .can_load_classes_ = reg_types->CanLoadClasses() },
       encountered_failure_types_(0),
@@ -5211,10 +5544,6 @@ MethodVerifier::MethodVerifier(Thread* self,
       link_(nullptr) {
 }
 
-MethodVerifier::~MethodVerifier() {
-  STLDeleteElements(&failure_messages_);
-}
-
 MethodVerifier::FailureData MethodVerifier::VerifyMethod(Thread* self,
                                                          ArenaPool* arena_pool,
                                                          RegTypeCache* reg_types,
@@ -5259,24 +5588,6 @@ MethodVerifier::FailureData MethodVerifier::VerifyMethod(Thread* self,
   }
 }
 
-// Return whether the runtime knows how to execute a method without needing to
-// re-verify it at runtime (and therefore save on first use of the class).
-// The AOT/JIT compiled code is not affected.
-static inline bool CanRuntimeHandleVerificationFailure(uint32_t encountered_failure_types) {
-  constexpr uint32_t unresolved_mask =
-      verifier::VerifyError::VERIFY_ERROR_UNRESOLVED_TYPE_CHECK |
-      verifier::VerifyError::VERIFY_ERROR_NO_CLASS |
-      verifier::VerifyError::VERIFY_ERROR_CLASS_CHANGE |
-      verifier::VerifyError::VERIFY_ERROR_INSTANTIATION |
-      verifier::VerifyError::VERIFY_ERROR_ACCESS_CLASS |
-      verifier::VerifyError::VERIFY_ERROR_ACCESS_FIELD |
-      verifier::VerifyError::VERIFY_ERROR_NO_METHOD |
-      verifier::VerifyError::VERIFY_ERROR_NO_FIELD |
-      verifier::VerifyError::VERIFY_ERROR_ACCESS_METHOD |
-      verifier::VerifyError::VERIFY_ERROR_RUNTIME_THROW;
-  return (encountered_failure_types & (~unresolved_mask)) == 0;
-}
-
 template <bool kVerifierDebug>
 MethodVerifier::FailureData MethodVerifier::VerifyMethod(Thread* self,
                                                          ArenaPool* arena_pool,
@@ -5318,16 +5629,19 @@ MethodVerifier::FailureData MethodVerifier::VerifyMethod(Thread* self,
             << reg_types->GetDexFile()->PrettyMethod(method_idx) << "\n");
       }
       if (kVerifierDebug) {
-        LOG(INFO) << verifier.InfoMessages().str();
+        LOG(INFO) << verifier.InfoMessages().view();
         verifier.Dump(LOG_STREAM(INFO));
       }
-      if (CanRuntimeHandleVerificationFailure(verifier.encountered_failure_types_)) {
+      if (CanCompilerHandleVerificationFailure(verifier.encountered_failure_types_)) {
         if (verifier.encountered_failure_types_ & VERIFY_ERROR_UNRESOLVED_TYPE_CHECK) {
           result.kind = FailureKind::kTypeChecksFailure;
         } else {
           result.kind = FailureKind::kAccessChecksFailure;
         }
       } else {
+        // If the compiler cannot handle the failure, force a soft failure to
+        // ensure the class will be re-verified at runtime and the method marked
+        // as not compilable.
         result.kind = FailureKind::kSoftFailure;
       }
     }
@@ -5359,24 +5673,23 @@ MethodVerifier::FailureData MethodVerifier::VerifyMethod(Thread* self,
           << reg_types->GetDexFile()->PrettyMethod(method_idx) << "\n");
     }
     if (hard_failure_msg != nullptr) {
-      CHECK(!verifier.failure_messages_.empty());
-      *hard_failure_msg =
-          verifier.failure_messages_[verifier.failure_messages_.size() - 1]->str();
+      CHECK(!verifier.failures_.empty());
+      *hard_failure_msg = verifier.failures_.back().message.view();
     }
     result.kind = FailureKind::kHardFailure;
 
     if (kVerifierDebug || VLOG_IS_ON(verifier)) {
-      LOG(ERROR) << verifier.InfoMessages().str();
+      LOG(ERROR) << verifier.InfoMessages().view();
       verifier.Dump(LOG_STREAM(ERROR));
     }
     // Under verifier-debug, dump the complete log into the error message.
     if (kVerifierDebug && hard_failure_msg != nullptr) {
       hard_failure_msg->append("\n");
-      hard_failure_msg->append(verifier.InfoMessages().str());
+      hard_failure_msg->append(verifier.InfoMessages().view());
       hard_failure_msg->append("\n");
       std::ostringstream oss;
       verifier.Dump(oss);
-      hard_failure_msg->append(oss.str());
+      hard_failure_msg->append(oss.view());
     }
   }
   if (kTimeVerifyMethod) {
@@ -5422,7 +5735,7 @@ MethodVerifier* MethodVerifier::CalculateVerificationInfo(
   verifier->Verify();
   if (VLOG_IS_ON(verifier)) {
     verifier->DumpFailures(VLOG_STREAM(verifier));
-    VLOG(verifier) << verifier->InfoMessages().str();
+    VLOG(verifier) << verifier->InfoMessages().view();
     verifier->Dump(VLOG_STREAM(verifier));
   }
   if (verifier->flags_.have_pending_hard_failure_) {
@@ -5461,7 +5774,7 @@ void MethodVerifier::VerifyMethodAndDump(Thread* self,
       api_level);
   verifier.Verify();
   verifier.DumpFailures(vios->Stream());
-  vios->Stream() << verifier.InfoMessages().str();
+  vios->Stream() << verifier.InfoMessages().view();
   // Only dump if no hard failures. Otherwise the verifier may be not fully initialized
   // and querying any info is dangerous/can abort.
   if (!verifier.flags_.have_pending_hard_failure_) {
@@ -5543,6 +5856,7 @@ std::ostream& MethodVerifier::Fail(VerifyError error, bool pending_exc) {
       case VERIFY_ERROR_ACCESS_FIELD:
       case VERIFY_ERROR_ACCESS_METHOD:
       case VERIFY_ERROR_INSTANTIATION:
+      case VERIFY_ERROR_FILLED_NEW_ARRAY:
       case VERIFY_ERROR_CLASS_CHANGE: {
         PotentiallyMarkRuntimeThrow();
         break;
@@ -5568,12 +5882,10 @@ std::ostream& MethodVerifier::Fail(VerifyError error, bool pending_exc) {
     CHECK_NE(error, VERIFY_ERROR_BAD_CLASS_HARD);
   }
 
-  failures_.push_back(error);
-  std::string location(StringPrintf("%s: [0x%X] ", dex_file_->PrettyMethod(dex_method_idx_).c_str(),
-                                    work_insn_idx_));
-  std::ostringstream* failure_message = new std::ostringstream(location, std::ostringstream::ate);
-  failure_messages_.push_back(failure_message);
-  return *failure_message;
+  std::string location =
+      StringPrintf("%s: [0x%X] ", dex_file_->PrettyMethod(dex_method_idx_).c_str(), work_insn_idx_);
+  failures_.emplace_back(error, location, failures_.get_allocator());
+  return failures_.back().message;
 }
 
 ScopedNewLine MethodVerifier::LogVerifyInfo() {
diff --git a/runtime/verifier/method_verifier.h b/runtime/verifier/method_verifier.h
index 13631850b7..b9fd4a4bf6 100644
--- a/runtime/verifier/method_verifier.h
+++ b/runtime/verifier/method_verifier.h
@@ -76,7 +76,6 @@ class PcToRegisterLineTable {
             uint32_t insns_size,
             uint16_t registers_size,
             ArenaAllocator& allocator,
-            RegTypeCache* reg_types,
             uint32_t interesting_dex_pc);
 
   bool IsInitialized() const {
@@ -156,7 +155,7 @@ class MethodVerifier {
                                uint32_t api_level)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  virtual ~MethodVerifier();
+  virtual ~MethodVerifier() {}
 
   const CodeItemDataAccessor& CodeItem() const {
     return code_item_accessor_;
@@ -293,7 +292,7 @@ class MethodVerifier {
                                         uint32_t api_level)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  virtual bool PotentiallyMarkRuntimeThrow() = 0;
+  virtual void PotentiallyMarkRuntimeThrow() = 0;
 
   std::ostringstream& InfoMessages() {
     if (!info_messages_.has_value()) {
@@ -331,10 +330,16 @@ class MethodVerifier {
   // Owned, but not unique_ptr since insn_flags_ are allocated in arenas.
   ArenaUniquePtr<InstructionFlags[]> insn_flags_;
 
-  // The types of any error that occurs.
-  std::vector<VerifyError> failures_;
-  // Error messages associated with failures.
-  std::vector<std::ostringstream*> failure_messages_;
+  // The types of any error that occurs and associated error messages.
+  using MessageOStream =
+      std::basic_ostringstream<char, std::char_traits<char>, ArenaAllocatorAdapter<char>>;
+  struct VerifyErrorAndMessage {
+    VerifyErrorAndMessage(VerifyError e, const std::string& location, ArenaAllocatorAdapter<char> a)
+        : error(e), message(location, std::ios_base::ate, a) {}
+    VerifyError error;
+    MessageOStream message;
+  };
+  ArenaList<VerifyErrorAndMessage> failures_;
 
   struct {
     // Is there a pending hard failure?
diff --git a/runtime/verifier/reg_type.cc b/runtime/verifier/reg_type.cc
index a2b52f4f87..9b4cc94416 100644
--- a/runtime/verifier/reg_type.cc
+++ b/runtime/verifier/reg_type.cc
@@ -193,7 +193,7 @@ bool RegType::IsObjectArrayTypes() const {
     return down_cast<const UnresolvedMergedReferenceType&>(*this).IsObjectArrayTypesImpl();
   } else if (IsUnresolvedTypes()) {
     // Primitive arrays will always resolve.
-    DCHECK(descriptor_[1] == 'L' || descriptor_[1] == '[');
+    DCHECK_IMPLIES(descriptor_[0] == '[', descriptor_[1] == 'L' || descriptor_[1] == '[');
     return descriptor_[0] == '[';
   } else if (HasClass()) {
     ObjPtr<mirror::Class> type = GetClass();
diff --git a/runtime/verifier/reg_type.h b/runtime/verifier/reg_type.h
index a8f31c523c..58dd88215d 100644
--- a/runtime/verifier/reg_type.h
+++ b/runtime/verifier/reg_type.h
@@ -170,21 +170,36 @@ class RegType {
   constexpr bool IsZeroOrNull() const {
     return IsZero() || IsNull();
   }
-  bool IsCategory1Types() const {
-    return IsIntegralTypes() || IsFloat();
+  static constexpr bool IsCategory1Types(Kind kind) {
+    return IsIntegralTypes(kind) || kind == kFloat;
   }
-  bool IsCategory2Types() const {
+  constexpr bool IsCategory1Types() const {
+    return IsCategory1Types(GetKind());
+  }
+  constexpr bool IsCategory2Types() const {
     return IsLowHalf();  // Don't expect explicit testing of high halves
   }
   static constexpr bool IsBooleanTypes(Kind kind) {
     return kind == Kind::kBoolean || kind == Kind::kZero || kind == Kind::kBooleanConstant;
   }
   constexpr bool IsBooleanTypes() const { return IsBooleanTypes(GetKind()); }
+  static constexpr bool IsByteTypes(Kind kind) {
+    return kind == kByte ||
+           kind == kPositiveByteConstant ||
+           kind == kByteConstant ||
+           IsBooleanTypes(kind);
+  }
   constexpr bool IsByteTypes() const {
-    return IsByte() || IsPositiveByteConstant() || IsByteConstant() || IsBooleanTypes();
+    return IsByteTypes(GetKind());
+  }
+  static constexpr bool IsShortTypes(Kind kind) {
+    return kind == kShort ||
+           kind == kPositiveShortConstant ||
+           kind == kShortConstant ||
+           IsByteTypes(kind);
   }
   constexpr bool IsShortTypes() const {
-    return IsShort() || IsPositiveShortConstant() || IsShortConstant() || IsByteTypes();
+    return IsShortTypes(GetKind());
   }
   constexpr bool IsCharTypes() const {
     return IsChar() ||
@@ -193,8 +208,15 @@ class RegType {
            IsPositiveByteConstant() ||
            IsBooleanTypes();
   }
+  static constexpr bool IsIntegralTypes(Kind kind) {
+    return kind == kInteger ||
+           kind == kIntegerConstant ||
+           kind == kChar ||
+           kind == kCharConstant ||
+           IsShortTypes(kind);
+  }
   constexpr bool IsIntegralTypes() const {
-    return IsInteger() || IsIntegerConstant() || IsChar() || IsCharConstant() || IsShortTypes();
+    return IsIntegralTypes(GetKind());
   }
   // Give the constant value encoded, but this shouldn't be called in the
   // general case.
diff --git a/runtime/verifier/register_line-inl.h b/runtime/verifier/register_line-inl.h
index a775ac5561..2bf48596f3 100644
--- a/runtime/verifier/register_line-inl.h
+++ b/runtime/verifier/register_line-inl.h
@@ -66,8 +66,8 @@ template <LockOp kLockOp>
 inline void RegisterLine::SetRegisterType(uint32_t vdst, const RegType& new_type) {
   DCHECK(!new_type.IsLowHalf());
   DCHECK(!new_type.IsHighHalf());
-  // Should only keep locks for reference types.
-  DCHECK_IMPLIES(kLockOp == LockOp::kKeep, new_type.IsReferenceTypes());
+  // Should only keep locks for reference types, or when copying a conflict with `move-object`.
+  DCHECK_IMPLIES(kLockOp == LockOp::kKeep, new_type.IsReferenceTypes() || new_type.IsConflict());
   SetRegisterTypeImpl<kLockOp>(vdst, new_type.GetId());
 }
 
@@ -97,16 +97,16 @@ inline void RegisterLine::SetRegisterTypeWide(uint32_t vdst,
   SetRegisterTypeWideImpl(vdst, new_type1.GetId(), new_type2.GetId());
 }
 
-inline void RegisterLine::SetResultTypeToUnknown(RegTypeCache* reg_types) {
-  result_[0] = reg_types->Undefined().GetId();
-  result_[1] = result_[0];
+inline void RegisterLine::SetResultTypeToUnknown() {
+  result_[0] = RegTypeCache::kUndefinedCacheId;
+  result_[1] = RegTypeCache::kUndefinedCacheId;
 }
 
-inline void RegisterLine::SetResultRegisterType(MethodVerifier* verifier, const RegType& new_type) {
+inline void RegisterLine::SetResultRegisterType(const RegType& new_type) {
   DCHECK(!new_type.IsLowHalf());
   DCHECK(!new_type.IsHighHalf());
   result_[0] = new_type.GetId();
-  result_[1] = verifier->GetRegTypeCache()->Undefined().GetId();
+  result_[1] = RegTypeCache::kUndefinedCacheId;
 }
 
 inline void RegisterLine::SetResultRegisterTypeWide(const RegType& new_type1,
@@ -126,41 +126,14 @@ inline void RegisterLine::SetRegisterTypeForNewInstance(uint32_t vdst,
   allocation_dex_pcs_[vdst] = dex_pc;
 }
 
-inline void RegisterLine::CopyRegister1(MethodVerifier* verifier, uint32_t vdst, uint32_t vsrc,
-                                 TypeCategory cat) {
-  DCHECK(cat == kTypeCategory1nr || cat == kTypeCategoryRef);
-  const RegType& type = GetRegisterType(verifier, vsrc);
-  if (type.IsLowHalf() || type.IsHighHalf()) {
-    verifier->Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "Expected category1 register type not '"
-        << type << "'";
-    return;
-  }
-  // FIXME: If `vdst == vsrc`, we clear locking information before we try to copy it below. Adding
-  // `move-object v1, v1` to the middle of `OK.runStraightLine()` in run-test 088 makes it fail.
-  SetRegisterType<LockOp::kClear>(vdst, type);
-  if (!type.IsConflict() &&                                  // Allow conflicts to be copied around.
-      ((cat == kTypeCategory1nr && !type.IsCategory1Types()) ||
-       (cat == kTypeCategoryRef && !type.IsReferenceTypes()))) {
-    verifier->Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "copy1 v" << vdst << "<-v" << vsrc << " type=" << type
-                                                 << " cat=" << static_cast<int>(cat);
-  } else if (cat == kTypeCategoryRef) {
-    CopyRegToLockDepth(vdst, vsrc);
-    if (allocation_dex_pcs_ != nullptr) {
-      // Copy allocation dex pc for uninitialized types. (Copy unused value for other types.)
-      allocation_dex_pcs_[vdst] = allocation_dex_pcs_[vsrc];
-    }
-  }
-}
-
-inline void RegisterLine::CopyRegister2(MethodVerifier* verifier, uint32_t vdst, uint32_t vsrc) {
-  const RegType& type_l = GetRegisterType(verifier, vsrc);
-  const RegType& type_h = GetRegisterType(verifier, vsrc + 1);
-
-  if (!type_l.CheckWidePair(type_h)) {
-    verifier->Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "copy2 v" << vdst << "<-v" << vsrc
-                                                 << " type=" << type_l << "/" << type_h;
-  } else {
-    SetRegisterTypeWide(vdst, type_l, type_h);
+inline void RegisterLine::CopyReference(uint32_t vdst, uint32_t vsrc, const RegType& type) {
+  DCHECK_EQ(type.GetId(), GetRegisterTypeId(vsrc));
+  DCHECK(type.IsConflict() || type.IsReferenceTypes());
+  SetRegisterType<LockOp::kKeep>(vdst, type);
+  CopyRegToLockDepth(vdst, vsrc);
+  if (allocation_dex_pcs_ != nullptr) {
+    // Copy allocation dex pc for uninitialized types. (Copy unused value for other types.)
+    allocation_dex_pcs_[vdst] = allocation_dex_pcs_[vsrc];
   }
 }
 
@@ -206,16 +179,12 @@ inline size_t RegisterLine::ComputeSize(size_t num_regs) {
   return OFFSETOF_MEMBER(RegisterLine, line_) + num_regs * sizeof(uint16_t);
 }
 
-inline RegisterLine* RegisterLine::Create(size_t num_regs,
-                                          ArenaAllocator& allocator,
-                                          RegTypeCache* reg_types) {
+inline RegisterLine* RegisterLine::Create(size_t num_regs, ArenaAllocator& allocator) {
   void* memory = allocator.Alloc(ComputeSize(num_regs));
-  return new (memory) RegisterLine(num_regs, allocator, reg_types);
+  return new (memory) RegisterLine(num_regs, allocator);
 }
 
-inline RegisterLine::RegisterLine(size_t num_regs,
-                                  ArenaAllocator& allocator,
-                                  RegTypeCache* reg_types)
+inline RegisterLine::RegisterLine(size_t num_regs, ArenaAllocator& allocator)
     : num_regs_(num_regs),
       allocation_dex_pcs_(nullptr),
       monitors_(allocator.Adapter(kArenaAllocVerifier)),
@@ -227,7 +196,7 @@ inline RegisterLine::RegisterLine(size_t num_regs,
   DCHECK(std::all_of(line_,
                      line_ + num_regs_,
                      [](auto id) { return id == RegTypeCache::kUndefinedCacheId;}));
-  SetResultTypeToUnknown(reg_types);
+  SetResultTypeToUnknown();
 }
 
 inline void RegisterLine::ClearRegToLockDepth(size_t reg, size_t depth) {
diff --git a/runtime/verifier/register_line.cc b/runtime/verifier/register_line.cc
index 6c4227cf8e..bcf1341048 100644
--- a/runtime/verifier/register_line.cc
+++ b/runtime/verifier/register_line.cc
@@ -91,31 +91,6 @@ void RegisterLine::MarkRefsAsInitialized(MethodVerifier* verifier, uint32_t vsrc
   DCHECK_GT(changed, 0u);
 }
 
-void RegisterLine::MarkAllRegistersAsConflicts(MethodVerifier* verifier) {
-  uint16_t conflict_type_id = verifier->GetRegTypeCache()->Conflict().GetId();
-  for (uint32_t i = 0; i < num_regs_; i++) {
-    line_[i] = conflict_type_id;
-  }
-}
-
-void RegisterLine::MarkAllRegistersAsConflictsExcept(MethodVerifier* verifier, uint32_t vsrc) {
-  uint16_t conflict_type_id = verifier->GetRegTypeCache()->Conflict().GetId();
-  for (uint32_t i = 0; i < num_regs_; i++) {
-    if (i != vsrc) {
-      line_[i] = conflict_type_id;
-    }
-  }
-}
-
-void RegisterLine::MarkAllRegistersAsConflictsExceptWide(MethodVerifier* verifier, uint32_t vsrc) {
-  uint16_t conflict_type_id = verifier->GetRegTypeCache()->Conflict().GetId();
-  for (uint32_t i = 0; i < num_regs_; i++) {
-    if ((i != vsrc) && (i != (vsrc + 1))) {
-      line_[i] = conflict_type_id;
-    }
-  }
-}
-
 std::string RegisterLine::Dump(MethodVerifier* verifier) const {
   std::string result;
   for (size_t i = 0; i < num_regs_; i++) {
@@ -141,9 +116,9 @@ void RegisterLine::CopyResultRegister1(MethodVerifier* verifier, uint32_t vdst,
     verifier->Fail(VERIFY_ERROR_BAD_CLASS_HARD)
         << "copyRes1 v" << vdst << "<- result0"  << " type=" << type;
   } else {
-    DCHECK(verifier->GetRegTypeCache()->GetFromId(result_[1]).IsUndefined());
+    DCHECK_EQ(result_[1], RegTypeCache::kUndefinedCacheId);
     SetRegisterType<LockOp::kClear>(vdst, type);
-    result_[0] = verifier->GetRegTypeCache()->Undefined().GetId();
+    result_[0] = RegTypeCache::kUndefinedCacheId;
   }
 }
 
@@ -160,26 +135,24 @@ void RegisterLine::CopyResultRegister2(MethodVerifier* verifier, uint32_t vdst)
   } else {
     DCHECK(type_l.CheckWidePair(type_h));  // Set should never allow this case
     SetRegisterTypeWide(vdst, type_l, type_h);  // also sets the high
-    result_[0] = verifier->GetRegTypeCache()->Undefined().GetId();
-    result_[1] = verifier->GetRegTypeCache()->Undefined().GetId();
+    result_[0] = RegTypeCache::kUndefinedCacheId;
+    result_[1] = RegTypeCache::kUndefinedCacheId;
   }
 }
 
 static constexpr uint32_t kVirtualNullRegister = std::numeric_limits<uint32_t>::max();
 
-void RegisterLine::PushMonitor(MethodVerifier* verifier, uint32_t reg_idx, int32_t insn_idx) {
-  const RegType& reg_type = GetRegisterType(verifier, reg_idx);
-  if (!reg_type.IsReferenceTypes()) {
-    verifier->Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "monitor-enter on non-object ("
-        << reg_type << ")";
-  } else if (monitors_.size() >= kMaxMonitorStackDepth) {
+void RegisterLine::PushMonitor(
+    MethodVerifier* verifier, uint32_t vreg, const RegType& reg_type, int32_t insn_idx) {
+  DCHECK_EQ(reg_type.GetId(), GetRegisterTypeId(vreg));
+  if (monitors_.size() >= kMaxMonitorStackDepth) {
     verifier->Fail(VERIFY_ERROR_LOCKING);
     if (kDumpLockFailures) {
       VLOG(verifier) << "monitor-enter stack overflow while verifying "
                      << verifier->GetMethodReference().PrettyMethod();
     }
   } else {
-    if (SetRegToLockDepth(reg_idx, monitors_.size())) {
+    if (SetRegToLockDepth(vreg, monitors_.size())) {
       // Null literals can establish aliases that we can't easily track. As such, handle the zero
       // case as the 2^32-1 register (which isn't available in dex bytecode).
       if (reg_type.IsZero()) {
@@ -190,18 +163,16 @@ void RegisterLine::PushMonitor(MethodVerifier* verifier, uint32_t reg_idx, int32
     } else {
       verifier->Fail(VERIFY_ERROR_LOCKING);
       if (kDumpLockFailures) {
-        VLOG(verifier) << "unexpected monitor-enter on register v" <<  reg_idx << " in "
+        VLOG(verifier) << "unexpected monitor-enter on register v" <<  vreg << " in "
                        << verifier->GetMethodReference().PrettyMethod();
       }
     }
   }
 }
 
-void RegisterLine::PopMonitor(MethodVerifier* verifier, uint32_t reg_idx) {
-  const RegType& reg_type = GetRegisterType(verifier, reg_idx);
-  if (!reg_type.IsReferenceTypes()) {
-    verifier->Fail(VERIFY_ERROR_BAD_CLASS_HARD) << "monitor-exit on non-object (" << reg_type << ")";
-  } else if (monitors_.empty()) {
+void RegisterLine::PopMonitor(MethodVerifier* verifier, uint32_t vreg, const RegType& reg_type) {
+  DCHECK_EQ(reg_type.GetId(), GetRegisterTypeId(vreg));
+  if (monitors_.empty()) {
     verifier->Fail(VERIFY_ERROR_LOCKING);
     if (kDumpLockFailures) {
       VLOG(verifier) << "monitor-exit stack underflow while verifying "
@@ -210,14 +181,14 @@ void RegisterLine::PopMonitor(MethodVerifier* verifier, uint32_t reg_idx) {
   } else {
     monitors_.pop_back();
 
-    bool success = IsSetLockDepth(reg_idx, monitors_.size());
+    bool success = IsSetLockDepth(vreg, monitors_.size());
 
     if (!success && reg_type.IsZero()) {
       // Null literals can establish aliases that we can't easily track. As such, handle the zero
       // case as the 2^32-1 register (which isn't available in dex bytecode).
       success = IsSetLockDepth(kVirtualNullRegister, monitors_.size());
       if (success) {
-        reg_idx = kVirtualNullRegister;
+        vreg = kVirtualNullRegister;
       }
     }
 
@@ -230,7 +201,7 @@ void RegisterLine::PopMonitor(MethodVerifier* verifier, uint32_t reg_idx) {
     } else {
       // Record the register was unlocked. This clears all aliases, thus it will also clear the
       // null lock, if necessary.
-      ClearRegToLockDepth(reg_idx, monitors_.size());
+      ClearRegToLockDepth(vreg, monitors_.size());
     }
   }
 }
@@ -283,7 +254,7 @@ bool RegisterLine::MergeRegisters(MethodVerifier* verifier, const RegisterLine*
           incoming_line->allocation_dex_pcs_ != nullptr &&
           allocation_dex_pcs_[idx] != incoming_line->allocation_dex_pcs_[idx] &&
           needs_allocation_dex_pc()) {
-        line_[idx] = verifier->GetRegTypeCache()->Conflict().GetId();
+        line_[idx] = RegTypeCache::kConflictCacheId;
       }
     }
   }
diff --git a/runtime/verifier/register_line.h b/runtime/verifier/register_line.h
index 19c2e2084f..c1aceaead4 100644
--- a/runtime/verifier/register_line.h
+++ b/runtime/verifier/register_line.h
@@ -76,15 +76,10 @@ class RegisterLine {
       std::numeric_limits<RegisterStackMask>::digits;
 
   // Create a register line of num_regs registers.
-  static RegisterLine* Create(size_t num_regs, ArenaAllocator& allocator, RegTypeCache* reg_types);
+  static RegisterLine* Create(size_t num_regs, ArenaAllocator& allocator);
 
-  // Implement category-1 "move" instructions. Copy a 32-bit value from "vsrc" to "vdst".
-  void CopyRegister1(MethodVerifier* verifier, uint32_t vdst, uint32_t vsrc, TypeCategory cat)
-      REQUIRES_SHARED(Locks::mutator_lock_);
-
-  // Implement category-2 "move" instructions. Copy a 64-bit value from "vsrc" to "vdst". This
-  // copies both halves of the register.
-  void CopyRegister2(MethodVerifier* verifier, uint32_t vdst, uint32_t vsrc)
+  // Copy reference (or conflict) register.
+  void CopyReference(uint32_t vdst, uint32_t vsrc, const RegType& type)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Implement "move-result". Copy the category-1 value from the result register to another
@@ -98,7 +93,7 @@ class RegisterLine {
       REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Set the invisible result register to unknown
-  void SetResultTypeToUnknown(RegTypeCache* reg_types) REQUIRES_SHARED(Locks::mutator_lock_);
+  void SetResultTypeToUnknown() REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Set the type of register N, verifying that the register is valid.  If "newType" is the "Lo"
   // part of a 64-bit value, register N+1 will be set to "newType+1".
@@ -123,7 +118,7 @@ class RegisterLine {
       REQUIRES_SHARED(Locks::mutator_lock_);
 
   /* Set the type of the "result" register. */
-  void SetResultRegisterType(MethodVerifier* verifier, const RegType& new_type)
+  void SetResultRegisterType(const RegType& new_type)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
   void SetResultRegisterTypeWide(const RegType& new_type1, const RegType& new_type2)
@@ -170,13 +165,6 @@ class RegisterLine {
   void MarkRefsAsInitialized(MethodVerifier* verifier, uint32_t vsrc)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
-  /*
-   * Update all registers to be Conflict except vsrc.
-   */
-  void MarkAllRegistersAsConflicts(MethodVerifier* verifier);
-  void MarkAllRegistersAsConflictsExcept(MethodVerifier* verifier, uint32_t vsrc);
-  void MarkAllRegistersAsConflictsExceptWide(MethodVerifier* verifier, uint32_t vsrc);
-
   void SetThisInitialized() {
     this_initialized_ = true;
   }
@@ -212,11 +200,12 @@ class RegisterLine {
   ALWAYS_INLINE static size_t ComputeSize(size_t num_regs);
 
   // Verify/push monitor onto the monitor stack, locking the value in reg_idx at location insn_idx.
-  void PushMonitor(MethodVerifier* verifier, uint32_t reg_idx, int32_t insn_idx)
+  void PushMonitor(
+      MethodVerifier* verifier, uint32_t vreg, const RegType& reg_type, int32_t insn_idx)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Verify/pop monitor from monitor stack ensuring that we believe the monitor is locked
-  void PopMonitor(MethodVerifier* verifier, uint32_t reg_idx)
+  void PopMonitor(MethodVerifier* verifier, uint32_t vreg, const RegType& reg_type)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
   // Stack of currently held monitors and where they were locked
@@ -270,9 +259,13 @@ class RegisterLine {
       REQUIRES_SHARED(Locks::mutator_lock_);
 
   void CopyRegToLockDepth(size_t dst, size_t src) {
+    // Note: We do not clear the entry for `dst` before copying, so we need to `Overwrite()`
+    // or `erase()`. This preserves the lock depths in the unlikely case that `dst == src`.
     auto it = reg_to_lock_depths_.find(src);
     if (it != reg_to_lock_depths_.end()) {
-      reg_to_lock_depths_.Put(dst, it->second);
+      reg_to_lock_depths_.Overwrite(dst, it->second);
+    } else {
+      reg_to_lock_depths_.erase(dst);
     }
   }
 
@@ -305,7 +298,7 @@ class RegisterLine {
     reg_to_lock_depths_.erase(reg);
   }
 
-  RegisterLine(size_t num_regs, ArenaAllocator& allocator, RegTypeCache* reg_types);
+  RegisterLine(size_t num_regs, ArenaAllocator& allocator);
 
   static constexpr uint32_t kNoDexPc = static_cast<uint32_t>(-1);
 
diff --git a/runtime/verifier/register_line_test.cc b/runtime/verifier/register_line_test.cc
index 4d0f0c20cd..0490683a86 100644
--- a/runtime/verifier/register_line_test.cc
+++ b/runtime/verifier/register_line_test.cc
@@ -120,8 +120,8 @@ TEST_F(RegisterLineTest, NewInstanceDexPcsMerging) {
   constexpr size_t kNumRegs = 1u;
   constexpr uint32_t kVReg = 0u;
   ArenaAllocator& allocator = GetArenaAllocator(verifier.get());
-  RegisterLineArenaUniquePtr line1(RegisterLine::Create(kNumRegs, allocator, &reg_types));
-  RegisterLineArenaUniquePtr line2(RegisterLine::Create(kNumRegs, allocator, &reg_types));
+  RegisterLineArenaUniquePtr line1(RegisterLine::Create(kNumRegs, allocator));
+  RegisterLineArenaUniquePtr line2(RegisterLine::Create(kNumRegs, allocator));
   for (const TestCase& test_case : test_cases) {
     ASSERT_TRUE(test_case.reg_type1.IsUninitializedTypes() ||
                 test_case.reg_type2.IsUninitializedTypes());
diff --git a/runtime/verifier/verifier_compiler_binding.h b/runtime/verifier/verifier_compiler_binding.h
index dd96a75975..d492751e64 100644
--- a/runtime/verifier/verifier_compiler_binding.h
+++ b/runtime/verifier/verifier_compiler_binding.h
@@ -27,17 +27,14 @@ namespace verifier {
 
 ALWAYS_INLINE
 static inline bool CanCompilerHandleVerificationFailure(uint32_t encountered_failure_types) {
-  constexpr uint32_t unresolved_mask =
-      verifier::VerifyError::VERIFY_ERROR_NO_CLASS |
-      verifier::VerifyError::VERIFY_ERROR_UNRESOLVED_TYPE_CHECK |
-      verifier::VerifyError::VERIFY_ERROR_CLASS_CHANGE |
-      verifier::VerifyError::VERIFY_ERROR_NO_METHOD |
-      verifier::VerifyError::VERIFY_ERROR_NO_FIELD |
-      verifier::VerifyError::VERIFY_ERROR_INSTANTIATION |
-      verifier::VerifyError::VERIFY_ERROR_ACCESS_CLASS |
-      verifier::VerifyError::VERIFY_ERROR_ACCESS_FIELD |
-      verifier::VerifyError::VERIFY_ERROR_ACCESS_METHOD;
-  return (encountered_failure_types & (~unresolved_mask)) == 0;
+  // These are and should remain the only two reasons a verified method cannot
+  // be compiled. The vdex file will mark classes where those methods are defined
+  // as verify-at-runtime and we should ideally not break that format in adding
+  // a new kind of failure.
+  constexpr uint32_t errors_needing_reverification =
+      verifier::VerifyError::VERIFY_ERROR_RUNTIME_THROW |
+      verifier::VerifyError::VERIFY_ERROR_LOCKING;
+  return (encountered_failure_types & errors_needing_reverification) == 0;
 }
 
 }  // namespace verifier
diff --git a/runtime/verifier/verifier_deps.cc b/runtime/verifier/verifier_deps.cc
index ed7629f805..5cb36db679 100644
--- a/runtime/verifier/verifier_deps.cc
+++ b/runtime/verifier/verifier_deps.cc
@@ -746,6 +746,7 @@ bool VerifierDeps::VerifyDexFileAndUpdateStatus(
           class_linker, self, source_desc, source_desc_length, class_loader));
 
       if (destination == nullptr || source == nullptr) {
+        deps.verified_classes_[class_def_index] = false;
         // We currently don't use assignability information for unresolved
         // types, as the status of the class using unresolved types will be soft
         // fail in the vdex.
diff --git a/runtime/verifier/verifier_enums.h b/runtime/verifier/verifier_enums.h
index 86775ff56d..1c90520155 100644
--- a/runtime/verifier/verifier_enums.h
+++ b/runtime/verifier/verifier_enums.h
@@ -81,9 +81,11 @@ enum VerifyError : uint32_t {
   VERIFY_ERROR_ACCESS_METHOD =     1 << 7,   // IllegalAccessError.
   VERIFY_ERROR_CLASS_CHANGE =      1 << 8,   // IncompatibleClassChangeError.
   VERIFY_ERROR_INSTANTIATION =     1 << 9,   // InstantiationError.
-  VERIFY_ERROR_LOCKING =           1 << 10,  // Could not guarantee balanced locking. This should
+  VERIFY_ERROR_FILLED_NEW_ARRAY =  1 << 10,  // Unsupported `filled-new-array/-range` component
+                                             // type. Interpreter throws `InternalError.
+  VERIFY_ERROR_LOCKING =           1 << 11,  // Could not guarantee balanced locking. This should
                                              // be punted to the interpreter with access checks.
-  VERIFY_ERROR_RUNTIME_THROW =     1 << 11,  // The interpreter found an instruction that will
+  VERIFY_ERROR_RUNTIME_THROW =     1 << 12,  // The interpreter found an instruction that will
                                              // throw. Used for app compatibility for apps < T.
 };
 std::ostream& operator<<(std::ostream& os, VerifyError rhs);
diff --git a/runtime/well_known_classes.cc b/runtime/well_known_classes.cc
index 5c1d9e9da0..14b0feecb6 100644
--- a/runtime/well_known_classes.cc
+++ b/runtime/well_known_classes.cc
@@ -95,6 +95,7 @@ ArtMethod* WellKnownClasses::java_lang_ThreadGroup_add;
 ArtMethod* WellKnownClasses::java_lang_ThreadGroup_threadTerminated;
 ArtMethod* WellKnownClasses::java_lang_invoke_MethodHandle_asType;
 ArtMethod* WellKnownClasses::java_lang_invoke_MethodHandle_invokeExact;
+ArtMethod* WellKnownClasses::java_lang_invoke_MethodHandleImpl_fieldInit;
 ArtMethod* WellKnownClasses::java_lang_invoke_MethodHandleImpl_init;
 ArtMethod* WellKnownClasses::java_lang_invoke_MethodHandles_lookup;
 ArtMethod* WellKnownClasses::java_lang_invoke_MethodHandles_makeIdentity;
@@ -126,7 +127,10 @@ ArtField* WellKnownClasses::dalvik_system_DexPathList_dexElements;
 ArtField* WellKnownClasses::dalvik_system_DexPathList__Element_dexFile;
 ArtField* WellKnownClasses::dalvik_system_VMRuntime_nonSdkApiUsageConsumer;
 ArtField* WellKnownClasses::java_io_FileDescriptor_descriptor;
+ArtField* WellKnownClasses::java_lang_ref_Reference_disableIntrinsic;
+ArtField* WellKnownClasses::java_lang_ref_Reference_slowPathEnabled;
 ArtField* WellKnownClasses::java_lang_ClassLoader_parent;
+ArtField* WellKnownClasses::java_lang_Object_shadowKlass;
 ArtField* WellKnownClasses::java_lang_String_EMPTY;
 ArtField* WellKnownClasses::java_lang_Thread_parkBlocker;
 ArtField* WellKnownClasses::java_lang_Thread_daemon;
@@ -172,6 +176,9 @@ ArtField* WellKnownClasses::java_lang_Short_ShortCache_cache;
 ArtField* WellKnownClasses::java_lang_Integer_IntegerCache_cache;
 ArtField* WellKnownClasses::java_lang_Long_LongCache_cache;
 
+ArtField* WellKnownClasses::java_lang_Boolean_value;
+ArtField* WellKnownClasses::java_lang_Float_value;
+ArtField* WellKnownClasses::java_lang_Double_value;
 ArtField* WellKnownClasses::java_lang_Byte_value;
 ArtField* WellKnownClasses::java_lang_Character_value;
 ArtField* WellKnownClasses::java_lang_Short_value;
@@ -407,6 +414,12 @@ void WellKnownClasses::InitFieldsAndMethodsOnly(JNIEnv* env) {
   java_lang_Long_LongCache_cache = CacheBoxingCacheField(
       class_linker, self, "Ljava/lang/Long$LongCache;", "[Ljava/lang/Long;");
 
+  java_lang_Boolean_value = CacheValueInBoxField(
+      class_linker, self, "Ljava/lang/Boolean;", "Z");
+  java_lang_Float_value = CacheValueInBoxField(
+      class_linker, self, "Ljava/lang/Float;", "F");
+  java_lang_Double_value = CacheValueInBoxField(
+      class_linker, self, "Ljava/lang/Double;", "D");
   java_lang_Byte_value = CacheValueInBoxField(
       class_linker, self, "Ljava/lang/Byte;", "B");
   java_lang_Character_value = CacheValueInBoxField(
@@ -633,6 +646,12 @@ void WellKnownClasses::InitFieldsAndMethodsOnly(JNIEnv* env) {
       "invokeExact",
       "([Ljava/lang/Object;)Ljava/lang/Object;",
       pointer_size);
+  java_lang_invoke_MethodHandleImpl_fieldInit = CacheMethod(
+    j_l_i_MethodHandleImpl.Get(),
+    /*is_static=*/ false,
+    "<init>",
+    "(Ljava/lang/reflect/Field;ILjava/lang/invoke/MethodType;)V",
+    pointer_size);
   java_lang_invoke_MethodHandleImpl_init = CacheMethod(
       j_l_i_MethodHandleImpl.Get(),
       /*is_static=*/ false,
@@ -737,6 +756,16 @@ void WellKnownClasses::InitFieldsAndMethodsOnly(JNIEnv* env) {
       "(I[BII)Lorg/apache/harmony/dalvik/ddmc/Chunk;",
       pointer_size);
 
+  ObjPtr<mirror::Class> j_l_Object = GetClassRoot<mirror::Object>(class_linker);
+  java_lang_Object_shadowKlass = CacheField(
+      j_l_Object, /*is_static=*/ false, "shadow$_klass_", "Ljava/lang/Class;");
+
+  ObjPtr<mirror::Class> j_l_r_Reference = GetClassRoot<mirror::Reference>(class_linker);
+  java_lang_ref_Reference_disableIntrinsic = CacheField(
+      j_l_r_Reference, /*is_static=*/ true, "disableIntrinsic", "Z");
+  java_lang_ref_Reference_slowPathEnabled = CacheField(
+      j_l_r_Reference, /*is_static=*/ true, "slowPathEnabled", "Z");
+
   dalvik_system_BaseDexClassLoader_pathList = CacheField(
       d_s_bdcl.Get(), /*is_static=*/ false, "pathList", "Ldalvik/system/DexPathList;");
   dalvik_system_BaseDexClassLoader_sharedLibraryLoaders = CacheField(
@@ -933,6 +962,7 @@ void WellKnownClasses::Clear() {
   java_lang_invoke_MethodHandle_asType = nullptr;
   java_lang_invoke_MethodHandle_invokeExact = nullptr;
   java_lang_invoke_MethodHandleImpl_init = nullptr;
+  java_lang_invoke_MethodHandleImpl_fieldInit = nullptr;
   java_lang_invoke_MethodHandles_lookup = nullptr;
   java_lang_invoke_MethodHandles_makeIdentity = nullptr;
   java_lang_invoke_MethodHandles_Lookup_findConstructor = nullptr;
@@ -959,7 +989,10 @@ void WellKnownClasses::Clear() {
   dalvik_system_DexPathList_dexElements = nullptr;
   dalvik_system_DexPathList__Element_dexFile = nullptr;
   dalvik_system_VMRuntime_nonSdkApiUsageConsumer = nullptr;
+  java_lang_ref_Reference_disableIntrinsic = nullptr;
+  java_lang_ref_Reference_slowPathEnabled = nullptr;
   java_lang_ClassLoader_parent = nullptr;
+  java_lang_Object_shadowKlass = nullptr;
   java_lang_String_EMPTY = nullptr;
   java_lang_Thread_parkBlocker = nullptr;
   java_lang_Thread_daemon = nullptr;
diff --git a/runtime/well_known_classes.h b/runtime/well_known_classes.h
index 6d5c9781ef..8798744448 100644
--- a/runtime/well_known_classes.h
+++ b/runtime/well_known_classes.h
@@ -141,6 +141,7 @@ struct EXPORT WellKnownClasses {
   static ArtMethod* java_lang_ThreadGroup_threadTerminated;
   static ArtMethod* java_lang_invoke_MethodHandle_asType;
   static ArtMethod* java_lang_invoke_MethodHandle_invokeExact;
+  static ArtMethod* java_lang_invoke_MethodHandleImpl_fieldInit;
   static ArtMethod* java_lang_invoke_MethodHandleImpl_init;
   static ArtMethod* java_lang_invoke_MethodHandles_lookup;
   static ArtMethod* java_lang_invoke_MethodHandles_makeIdentity;
@@ -172,7 +173,10 @@ struct EXPORT WellKnownClasses {
   static ArtField* dalvik_system_DexPathList__Element_dexFile;
   static ArtField* dalvik_system_VMRuntime_nonSdkApiUsageConsumer;
   static ArtField* java_io_FileDescriptor_descriptor;
+  static ArtField* java_lang_ref_Reference_disableIntrinsic;
+  static ArtField* java_lang_ref_Reference_slowPathEnabled;
   static ArtField* java_lang_ClassLoader_parent;
+  static ArtField* java_lang_Object_shadowKlass;
   static ArtField* java_lang_String_EMPTY;
   static ArtField* java_lang_Thread_parkBlocker;
   static ArtField* java_lang_Thread_daemon;
@@ -218,6 +222,9 @@ struct EXPORT WellKnownClasses {
   static ArtField* java_lang_Integer_IntegerCache_cache;
   static ArtField* java_lang_Long_LongCache_cache;
 
+  static ArtField* java_lang_Boolean_value;
+  static ArtField* java_lang_Float_value;
+  static ArtField* java_lang_Double_value;
   static ArtField* java_lang_Byte_value;
   static ArtField* java_lang_Character_value;
   static ArtField* java_lang_Short_value;
diff --git a/sigchainlib/sigchain_test.cc b/sigchainlib/sigchain_test.cc
index 3a68381117..238aea3152 100644
--- a/sigchainlib/sigchain_test.cc
+++ b/sigchainlib/sigchain_test.cc
@@ -104,7 +104,8 @@ static void TestSignalBlocking(const std::function<void()>& fn) {
 
   fn();
 
-  if (testing::Test::HasFatalFailure()) return;
+  if (::testing::Test::HasFatalFailure())
+    return;
   ASSERT_EQ(0, RealSigprocmask(SIG_SETMASK, nullptr, &mask));
   ASSERT_FALSE(sigismember64(&mask, SIGSEGV));
 }
@@ -266,7 +267,7 @@ DISABLE_HWASAN void fault_address_tag_impl() {
 
   auto* tagged_null = reinterpret_cast<int*>(0x2bULL << 56);
   EXPECT_EXIT(
-      { [[maybe_unused]] volatile int load = *tagged_null; }, testing::ExitedWithCode(0), "");
+      { [[maybe_unused]] volatile int load = *tagged_null; }, ::testing::ExitedWithCode(0), "");
 
   // Our sigaction implementation always implements the "clear unknown bits"
   // semantics for oldact.sa_flags regardless of kernel version so we rely on it
@@ -275,9 +276,10 @@ DISABLE_HWASAN void fault_address_tag_impl() {
   ASSERT_EQ(0, sigaction(SIGSEGV, &action, nullptr));
   ASSERT_EQ(0, sigaction(SIGSEGV, nullptr, &action));
   if (action.sa_flags & SA_EXPOSE_TAGBITS) {
-      EXPECT_EXIT({ [[maybe_unused]] volatile int load = *tagged_null; },
-                  testing::ExitedWithCode(0x2b),
-                  "");
+    EXPECT_EXIT(
+        { [[maybe_unused]] volatile int load = *tagged_null; },
+        ::testing::ExitedWithCode(0x2b),
+        "");
   }
 }
 #endif
diff --git a/simulator/code_simulator_arm64.cc b/simulator/code_simulator_arm64.cc
index 08a5deae1f..b9a0ce2a30 100644
--- a/simulator/code_simulator_arm64.cc
+++ b/simulator/code_simulator_arm64.cc
@@ -21,6 +21,10 @@
 using namespace vixl::aarch64;  // NOLINT(build/namespaces)
 
 namespace art {
+
+// Enable the simulator debugger, disabled by default.
+static constexpr bool kSimDebuggerEnabled = false;
+
 namespace arm64 {
 
 // VIXL has not been tested on 32bit architectures, so Simulator is not always
@@ -48,6 +52,8 @@ CodeSimulatorArm64::CodeSimulatorArm64()
 
   simulator_ = new Simulator(decoder_, stdout, std::move(stack));
   simulator_->SetVectorLengthInBits(kArm64DefaultSVEVectorLength);
+  simulator_->DisableGCSCheck();
+  simulator_->SetDebuggerEnabled(kSimDebuggerEnabled);
 }
 
 CodeSimulatorArm64::~CodeSimulatorArm64() {
diff --git a/simulator/code_simulator_arm64.h b/simulator/code_simulator_arm64.h
index dee6b32c34..a5f5d0fa64 100644
--- a/simulator/code_simulator_arm64.h
+++ b/simulator/code_simulator_arm64.h
@@ -19,12 +19,7 @@
 
 #include "memory"
 
-// TODO(VIXL): Make VIXL compile cleanly with -Wshadow, -Wdeprecated-declarations.
-#pragma GCC diagnostic push
-#pragma GCC diagnostic ignored "-Wshadow"
-#pragma GCC diagnostic ignored "-Wdeprecated-declarations"
 #include "aarch64/simulator-aarch64.h"
-#pragma GCC diagnostic pop
 
 #include "arch/instruction_set.h"
 #include "code_simulator.h"
diff --git a/test/055-enum-performance/Android.bp b/test/055-enum-performance/Android.bp
index b56c8d7db7..bb8e11d2ab 100644
--- a/test/055-enum-performance/Android.bp
+++ b/test/055-enum-performance/Android.bp
@@ -15,15 +15,12 @@ package {
 java_test {
     name: "art-run-test-055-enum-performance",
     defaults: ["art-run-test-defaults"],
-    test_config_template: ":art-run-test-target-template",
+    test_config_template: ":art-run-test-target-no-test-suite-tag-template",
     srcs: ["src/**/*.java"],
     data: [
         ":art-run-test-055-enum-performance-expected-stdout",
         ":art-run-test-055-enum-performance-expected-stderr",
     ],
-    test_suites: [
-        "mts-art",
-    ],
 }
 
 // Test's expected standard output.
diff --git a/test/088-monitor-verification/smali/OK.smali b/test/088-monitor-verification/smali/OK.smali
index a43ecb0704..042ea37d7f 100644
--- a/test/088-monitor-verification/smali/OK.smali
+++ b/test/088-monitor-verification/smali/OK.smali
@@ -9,6 +9,8 @@
 
    invoke-static {v1, v2}, LOK;->runStraightLine(Ljava/lang/Object;Ljava/lang/Object;)V
 
+   invoke-static {v1, v2}, LOK;->runStraightLine2(Ljava/lang/Object;Ljava/lang/Object;)V
+
    invoke-static {v1, v2}, LOK;->runBalancedJoin(Ljava/lang/Object;Ljava/lang/Object;)V
 
    return-void
@@ -41,6 +43,24 @@
 
 .end method
 
+.method public static runStraightLine2(Ljava/lang/Object;Ljava/lang/Object;)V
+   .registers 3
+
+   invoke-static {}, LMain;->assertIsManaged()V
+
+   monitor-enter v1      # 1
+   monitor-enter v2      # 2
+
+   # No-op move should not invalidate locking information.
+   move-object v2, v2
+
+   monitor-exit v2       # 2
+   monitor-exit v1       # 1
+
+   return-void
+
+.end method
+
 .method public static runBalancedJoin(Ljava/lang/Object;Ljava/lang/Object;)V
    .registers 3
 
diff --git a/test/088-monitor-verification/src/Main.java b/test/088-monitor-verification/src/Main.java
index 3ed1939a08..a7c24960b4 100644
--- a/test/088-monitor-verification/src/Main.java
+++ b/test/088-monitor-verification/src/Main.java
@@ -42,6 +42,7 @@ public class Main {
         ensureJitCompiled(TwoPath.class, "twoPath");
         ensureJitCompiled(Class.forName("OK"), "runNoMonitors");
         ensureJitCompiled(Class.forName("OK"), "runStraightLine");
+        ensureJitCompiled(Class.forName("OK"), "runStraightLine2");
         ensureJitCompiled(Class.forName("OK"), "runBalancedJoin");
         ensureJitCompiled(Class.forName("NullLocks"), "run");
 
diff --git a/test/1336-short-finalizer-timeout/src/Main.java b/test/1336-short-finalizer-timeout/src/Main.java
index 3f024b15d6..0441574c9a 100644
--- a/test/1336-short-finalizer-timeout/src/Main.java
+++ b/test/1336-short-finalizer-timeout/src/Main.java
@@ -48,6 +48,12 @@ public class Main {
         // the new timeout plus the 5 seconds we wait to dump thread stacks before actually
         // exiting.
         snooze(9800);
+        if (System.getenv("ART_TEST_ON_VM") != null) {
+          // Under emulation we sometimes seem to just not shut down fast enough.  If the process is
+          // still running at this point, sleep again for longer. This makes the test much less
+          // useful, but not 100% useless.
+          snooze(20_000);
+        }
 
         // We should not get here, since it should only take 5.5 seconds for the timed out
         // finalizer to kill the process.
diff --git a/test/150-loadlibrary/src/Main.java b/test/150-loadlibrary/src/Main.java
index ec96221d1e..7e7802bb64 100644
--- a/test/150-loadlibrary/src/Main.java
+++ b/test/150-loadlibrary/src/Main.java
@@ -26,30 +26,17 @@ public class Main {
       throw new IllegalStateException("Expected non-null classloader for Object");
     }
 
-    // Try to load libarttest(d) with the BootClassLoader. First construct the filename.
+    // Try to load libarttest(d) with the BootClassLoader. First construct the
+    // filename. It's in NATIVELOADER_DEFAULT_NAMESPACE_LIBS, so it's accessible
+    // simply through the file name.
     String libName = System.mapLibraryName(args[0]);
-    Method libPathsMethod = Runtime.class.getDeclaredMethod("getLibPaths");
-    libPathsMethod.setAccessible(true);
-    String[] libPaths = (String[])libPathsMethod.invoke(Runtime.getRuntime());
-    String fileName = null;
-    for (String p : libPaths) {
-      String candidate = p + libName;
-      if (new File(candidate).exists()) {
-          fileName = candidate;
-          break;
-      }
-    }
-    if (fileName == null) {
-      throw new IllegalStateException("Didn't find " + libName + " in " +
-          Arrays.toString(libPaths));
-    }
 
     // Then call an internal function that accepts the classloader. Do not use load(), as it
     // is deprecated and only there for backwards compatibility, and prints a warning to the
     // log that we'd have to strip (it contains the pid).
     Method m = Runtime.class.getDeclaredMethod("nativeLoad", String.class, ClassLoader.class);
     m.setAccessible(true);
-    Object result = m.invoke(Runtime.getRuntime(), fileName, bootClassLoader);
+    Object result = m.invoke(Runtime.getRuntime(), libName, bootClassLoader);
     if (result != null) {
       throw new IllegalStateException(result.toString());
     }
diff --git a/test/153-reference-stress/src/Main.java b/test/153-reference-stress/src/Main.java
index fc6f9ccb35..5f2f72b4e6 100644
--- a/test/153-reference-stress/src/Main.java
+++ b/test/153-reference-stress/src/Main.java
@@ -17,7 +17,7 @@
 import java.lang.ref.WeakReference;
 
 public class Main {
-    static final int numWeakReferences = 16 * 1024;
+    static final int numWeakReferences = 16 * 1024 / (isVm() ? 4 : 1);
     static WeakReference[] weakReferences = new WeakReference[numWeakReferences];
     static volatile boolean done = false;
     static Object keepAlive;
@@ -70,4 +70,8 @@ public class Main {
             }
         }
     }
+
+    private static boolean isVm() {
+        return System.getenv("ART_TEST_ON_VM") != null;
+    }
 }
diff --git a/test/1963-add-to-dex-classloader-in-memory/check_memfd_create.cc b/test/1963-add-to-dex-classloader-in-memory/check_memfd_create.cc
deleted file mode 100644
index 70a64d71ee..0000000000
--- a/test/1963-add-to-dex-classloader-in-memory/check_memfd_create.cc
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Copyright (C) 2019 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-#include <string>
-#include <iostream>
-#include <sstream>
-
-#include "jvmti.h"
-
-#include "base/logging.h"
-#include "base/globals.h"
-#include "base/memfd.h"
-
-#ifdef __linux__
-#include <sys/utsname.h>
-#endif
-
-namespace art {
-namespace Test1963AddToDexClassLoaderInMemory {
-
-extern "C" JNIEXPORT jboolean JNICALL Java_Main_hasWorkingMemfdCreate(JNIEnv*, jclass) {
-  // We should always have a working version if we're on normal buildbots.
-  if (!art::kIsTargetBuild) {
-    return true;
-  }
-#ifdef __linux__
-  struct utsname name;
-  if (uname(&name) >= 0) {
-    std::istringstream version(name.release);
-    std::string major_str;
-    std::string minor_str;
-    std::getline(version, major_str, '.');
-    std::getline(version, minor_str, '.');
-    int major = std::stoi(major_str);
-    int minor = std::stoi(minor_str);
-    if (major >= 4 || (major == 3 && minor >= 17)) {
-      // memfd_create syscall was added in 3.17
-      return true;
-    }
-  }
-#endif
-  int res = memfd_create_compat("TEST THAT MEMFD CREATE WORKS", 0);
-  if (res < 0) {
-    PLOG(ERROR) << "Unable to call memfd_create_compat successfully!";
-    return false;
-  } else {
-    close(res);
-    return true;
-  }
-}
-
-}  // namespace Test1963AddToDexClassLoaderInMemory
-}  // namespace art
diff --git a/test/1963-add-to-dex-classloader-in-memory/run.py b/test/1963-add-to-dex-classloader-in-memory/run.py
index ad7c1aed1b..4796039801 100644
--- a/test/1963-add-to-dex-classloader-in-memory/run.py
+++ b/test/1963-add-to-dex-classloader-in-memory/run.py
@@ -17,11 +17,3 @@
 
 def run(ctx, args):
   ctx.default_run(args, jvmti=True)
-
-  # Some of our test devices are so old that they don't have memfd_create and are setup in such a way
-  # that tmpfile() doesn't work. In these cases this test cannot complete successfully.
-  # If we see this in stdout, make the expected stdout identical.
-  ctx.run(
-      fr"grep -q -- '---NO memfd_create---' '{args.stdout_file}' &&"
-      fr" echo '---NO memfd_create---' > expected-stdout.txt",
-      check=False)
diff --git a/test/1963-add-to-dex-classloader-in-memory/src/Main.java b/test/1963-add-to-dex-classloader-in-memory/src/Main.java
index 1825e4faab..9c4cf57739 100644
--- a/test/1963-add-to-dex-classloader-in-memory/src/Main.java
+++ b/test/1963-add-to-dex-classloader-in-memory/src/Main.java
@@ -17,16 +17,10 @@
 public class Main {
   public static void main(String[] args) throws Exception {
     try {
-      if (!hasWorkingMemfdCreate()) {
-        System.out.println("---NO memfd_create---");
-      }
       art.Test1963.run();
     } catch (Throwable t) {
       System.out.println(t);
       t.printStackTrace(System.out);
-      return;
     }
   }
-
-  public static native boolean hasWorkingMemfdCreate();
 }
diff --git a/test/2011-stack-walk-concurrent-instrument/stack_walk_concurrent.cc b/test/2011-stack-walk-concurrent-instrument/stack_walk_concurrent.cc
index 9ae1bedd23..a6ce0521e4 100644
--- a/test/2011-stack-walk-concurrent-instrument/stack_walk_concurrent.cc
+++ b/test/2011-stack-walk-concurrent-instrument/stack_walk_concurrent.cc
@@ -19,6 +19,7 @@
 
 #include "arch/context.h"
 #include "art_method-inl.h"
+#include "instrumentation.h"
 #include "jni.h"
 #include "scoped_thread_state_change.h"
 #include "stack.h"
diff --git a/test/2038-hiddenapi-jvmti-ext/run.py b/test/2038-hiddenapi-jvmti-ext/run.py
index 4796039801..49a7ef365a 100644
--- a/test/2038-hiddenapi-jvmti-ext/run.py
+++ b/test/2038-hiddenapi-jvmti-ext/run.py
@@ -17,3 +17,6 @@
 
 def run(ctx, args):
   ctx.default_run(args, jvmti=True)
+
+  # Ignore hiddenapi's denial errors which go to stderr on host and qemu (but not on device).
+  ctx.run(fr"sed -i -E '/ E dalvikvm.* hiddenapi: /d' '{args.stderr_file}'")
diff --git a/test/2040-huge-native-alloc/src/Main.java b/test/2040-huge-native-alloc/src/Main.java
index 3c8ae23c82..8149c1ead4 100644
--- a/test/2040-huge-native-alloc/src/Main.java
+++ b/test/2040-huge-native-alloc/src/Main.java
@@ -104,7 +104,7 @@ public class Main {
     if (startingGcNum == getGcNum()) {
       System.out.println("No gc completed");
     }
-    if (actualTime > 500_000_000) {
+    if (actualTime > 500_000_000 * (isVm() ? 4 : 1)) {
       System.out.println("Notifications ran too slowly; excessive blocking? msec = "
           + (actualTime / 1_000_000));
     } else if (actualTime < minBlockingTime) {
@@ -135,4 +135,8 @@ public class Main {
   private static native ByteBuffer getHugeNativeBuffer();
   private static native void deleteHugeNativeBuffer(ByteBuffer buf);
   private static native int getGcNum();
+
+  private static boolean isVm() {
+    return System.getenv("ART_TEST_ON_VM") != null;
+  }
 }
diff --git a/test/2246-trace-v2/dump_trace.cc b/test/2246-trace-v2/dump_trace.cc
index 4dfc42ef4b..fbfa0256af 100644
--- a/test/2246-trace-v2/dump_trace.cc
+++ b/test/2246-trace-v2/dump_trace.cc
@@ -68,18 +68,16 @@ bool ProcessThreadOrMethodInfo(std::unique_ptr<File>& file,
   uint64_t id = ReadNumber(num_bytes_for_id, header);
   int length = ReadNumber(2, header + num_bytes_for_id);
 
-  char* name = new char[length];
-  if (!file->ReadFully(name, length)) {
-    delete[] name;
+  std::unique_ptr<char[]> name(new char[length]);
+  if (!file->ReadFully(name.get(), length)) {
     return false;
   }
-  std::string str(name, length);
+  std::string str(name.get(), length);
   std::replace(str.begin(), str.end(), '\t', ' ');
   if (str[str.length() - 1] == '\n') {
     str.erase(str.length() - 1);
   }
   name_map.emplace(id, str);
-  delete[] name;
   return true;
 }
 
@@ -158,9 +156,8 @@ bool ProcessTraceEntries(std::unique_ptr<File>& file,
   int num_records = ReadNumber(3, header + offset);
   offset += 3;
   int total_size = ReadNumber(4, header + offset);
-  uint8_t* buffer = new uint8_t[total_size];
-  if (!file->ReadFully(buffer, total_size)) {
-    delete[] buffer;
+  std::unique_ptr<uint8_t[]> buffer(new uint8_t[total_size]);
+  if (!file->ReadFully(buffer.get(), total_size)) {
     return false;
   }
 
@@ -181,11 +178,12 @@ bool ProcessTraceEntries(std::unique_ptr<File>& file,
   std::string thread_name = thread_map[thread_id];
   bool print_thread_events = (thread_name.compare(thread_name_filter) == 0);
 
-  const uint8_t* current_buffer_ptr = buffer;
+  const uint8_t* current_buffer_ptr = buffer.get();
   int64_t prev_method_value = 0;
   for (int i = 0; i < num_records; i++) {
     int64_t diff = 0;
-    if (!DecodeSignedLeb128Checked(&current_buffer_ptr, buffer + total_size - 1, &diff)) {
+    if (!DecodeSignedLeb128Checked<int64_t>(
+            &current_buffer_ptr, buffer.get() + total_size - 1, &diff)) {
       LOG(FATAL) << "Reading past the buffer???";
     }
     int64_t curr_method_value = prev_method_value + diff;
@@ -204,9 +202,9 @@ bool ProcessTraceEntries(std::unique_ptr<File>& file,
                       &ignored_method_depth);
     }
     // Read timestamps
-    DecodeUnsignedLeb128(&current_buffer_ptr);
+    DecodeUnsignedLeb128<uint64_t>(&current_buffer_ptr);
     if (is_dual_clock) {
-      DecodeUnsignedLeb128(&current_buffer_ptr);
+      DecodeUnsignedLeb128<uint64_t>(&current_buffer_ptr);
     }
   }
   current_depth_map[thread_id] = current_depth;
diff --git a/test/2255-checker-branch-redirection/src/Main.java b/test/2255-checker-branch-redirection/src/Main.java
index bfc6381942..a2c04ef6e0 100644
--- a/test/2255-checker-branch-redirection/src/Main.java
+++ b/test/2255-checker-branch-redirection/src/Main.java
@@ -32,6 +32,15 @@ public class Main {
         assertEquals(30, $noinline$testEliminateIfParameterOppositeCondition(20, 10, 20 < 10));
         assertEquals(40, $noinline$testEliminateIfParameterOppositeCondition_2(20, 40, 20 < 40));
         assertEquals(30, $noinline$testEliminateIfParameterOppositeCondition_2(20, 10, 20 < 10));
+
+        assertEquals(2, $noinline$testEliminateIfFp(20.0, 40.0));
+        assertEquals(3, $noinline$testEliminateIfFp(20.0, 10.0));
+        assertEquals(3, $noinline$testEliminateIfFp(20.0, Double.NaN));
+        assertEquals(3, $noinline$testEliminateIfFp(Double.NaN, 10.0));
+        assertEquals(3, $noinline$testDoNotEliminateIfOppositeCondFpWrongBias(20.0, 40.0));
+        assertEquals(2, $noinline$testDoNotEliminateIfOppositeCondFpWrongBias(20.0, 10.0));
+        assertEquals(3, $noinline$testDoNotEliminateIfOppositeCondFpWrongBias(20.0, Double.NaN));
+        assertEquals(3, $noinline$testDoNotEliminateIfOppositeCondFpWrongBias(Double.NaN, 10.0));
     }
 
     private static int $noinline$emptyMethod(int a) {
@@ -274,6 +283,50 @@ public class Main {
         return result;
     }
 
+    /// CHECK-START: int Main.$noinline$testEliminateIfFp(double, double) dead_code_elimination$after_gvn (before)
+    /// CHECK:     If
+    /// CHECK:     If
+
+    /// CHECK-START: int Main.$noinline$testEliminateIfFp(double, double) dead_code_elimination$after_gvn (after)
+    /// CHECK:     If
+    /// CHECK-NOT: If
+    private static int $noinline$testEliminateIfFp(double a, double b) {
+        int result = 0;
+        if (a < b) {
+            $noinline$emptyMethod(0);
+        } else {
+            $noinline$emptyMethod(1);
+        }
+        if (a < b) {
+            result += $noinline$emptyMethod(2);
+        } else {
+            result += $noinline$emptyMethod(3);
+        }
+        return result;
+    }
+
+    /// CHECK-START: int Main.$noinline$testDoNotEliminateIfOppositeCondFpWrongBias(double, double) dead_code_elimination$initial (before)
+    /// CHECK:     If
+    /// CHECK:     If
+
+    /// CHECK-START: int Main.$noinline$testDoNotEliminateIfOppositeCondFpWrongBias(double, double) dead_code_elimination$initial (after)
+    /// CHECK:     If
+    /// CHECK:     If
+    private static int $noinline$testDoNotEliminateIfOppositeCondFpWrongBias(double a, double b) {
+        int result = 0;
+        if (a < b) {
+            $noinline$emptyMethod(0);
+        } else {
+            $noinline$emptyMethod(1);
+        }
+        if (a >= b) {
+            result += $noinline$emptyMethod(2);
+        } else {
+            result += $noinline$emptyMethod(3);
+        }
+        return result;
+    }
+
     public static void assertEquals(int expected, int result) {
         if (expected != result) {
             throw new Error("Expected: " + expected + ", found: " + result);
diff --git a/test/2265-checker-select-binary-unary/src/Main.java b/test/2265-checker-select-binary-unary/src/Main.java
index 33734fc4d6..61f4b053aa 100644
--- a/test/2265-checker-select-binary-unary/src/Main.java
+++ b/test/2265-checker-select-binary-unary/src/Main.java
@@ -31,7 +31,7 @@ public class Main {
         assertIntEquals(12, $noinline$testLongToInt(1, 0));
     }
 
-    /// CHECK-START: long Main.$noinline$testIntToLong(int, int) select_generator (after)
+    /// CHECK-START: long Main.$noinline$testIntToLong(int, int) control_flow_simplifier (after)
     /// CHECK:     <<Const10:j\d+>> LongConstant 10
     /// CHECK:     <<Const1:i\d+>>  IntConstant 1
     /// CHECK:     <<Const2:i\d+>>  IntConstant 2
@@ -54,7 +54,7 @@ public class Main {
         return result + (a < b ? c : d);
     }
 
-    /// CHECK-START: float Main.$noinline$testIntToFloat(int, int) select_generator (after)
+    /// CHECK-START: float Main.$noinline$testIntToFloat(int, int) control_flow_simplifier (after)
     /// CHECK:     <<Const10:f\d+>> FloatConstant 10
     /// CHECK:     <<Const1:i\d+>>  IntConstant 1
     /// CHECK:     <<Const2:i\d+>>  IntConstant 2
@@ -77,7 +77,7 @@ public class Main {
         return result + (a < b ? c : d);
     }
 
-    /// CHECK-START: byte Main.$noinline$testIntToByte(int, int) select_generator (after)
+    /// CHECK-START: byte Main.$noinline$testIntToByte(int, int) control_flow_simplifier (after)
     /// CHECK:     <<Const10:i\d+>>  IntConstant 10
     /// CHECK:     <<Const257:i\d+>> IntConstant 257
     /// CHECK:     <<Const258:i\d+>> IntConstant 258
@@ -102,7 +102,7 @@ public class Main {
         return (byte) (result + (byte) (a < b ? c : d));
     }
 
-    /// CHECK-START: int Main.$noinline$testLongToInt(int, int) select_generator (after)
+    /// CHECK-START: int Main.$noinline$testLongToInt(int, int) control_flow_simplifier (after)
     /// CHECK:     <<Const10:i\d+>> IntConstant 10
     /// CHECK:     <<Const1:j\d+>>  LongConstant 4294967297
     /// CHECK:     <<Const2:j\d+>>  LongConstant 4294967298
diff --git a/test/2279-aconfig-flags/src/Main.java b/test/2279-aconfig-flags/src/Main.java
index 4a282db04d..6d600b8136 100644
--- a/test/2279-aconfig-flags/src/Main.java
+++ b/test/2279-aconfig-flags/src/Main.java
@@ -14,19 +14,28 @@
  * limitations under the License.
  */
 
-import com.android.libcore.Flags;
-
 public class Main {
     public static void main(String[] args) {
+        // Test a flag in libcore/libcore.aconfig.
         if (!isVTrunkStableFlagEnabled()) {
             throw new AssertionError(
                     "The value of com.android.libcore.v_apis flag is expected to be true.");
         }
+
+        // Test a flag in art/build/flags/art-flags.aconfig.
+        if (!isArtTestFlagEnabled()) {
+            throw new AssertionError(
+                    "The value of com.android.art.flags.test flag is expected to be true.");
+        }
     }
 
     private static boolean isVTrunkStableFlagEnabled() {
         // The Flags class definition is expected to be in core-libart.jar.
-        return Flags.vApis();
+        return com.android.libcore.Flags.vApis();
     }
 
+    private static boolean isArtTestFlagEnabled() {
+        // The Flags class definition is expected to be in core-libart.jar.
+        return com.android.art.flags.Flags.test();
+    }
 }
diff --git a/test/2286-invokevirtual-invokeexact/expected-stderr.txt b/test/2286-invokevirtual-invokeexact/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/2286-invokevirtual-invokeexact/expected-stdout.txt b/test/2286-invokevirtual-invokeexact/expected-stdout.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/2286-invokevirtual-invokeexact/info.txt b/test/2286-invokevirtual-invokeexact/info.txt
new file mode 100644
index 0000000000..40c068f5a5
--- /dev/null
+++ b/test/2286-invokevirtual-invokeexact/info.txt
@@ -0,0 +1,2 @@
+Ensure that invoke-virtual targeting MethodHandle::invoke or MethodHandle::invokeExact
+does not crash the runtime.
diff --git a/test/2286-invokevirtual-invokeexact/smali/Test.smali b/test/2286-invokevirtual-invokeexact/smali/Test.smali
new file mode 100644
index 0000000000..0b04aa453f
--- /dev/null
+++ b/test/2286-invokevirtual-invokeexact/smali/Test.smali
@@ -0,0 +1,26 @@
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+.class public LTest;
+
+.super Ljava/lang/Object;
+
+.method public static test(Ljava/lang/invoke/MethodHandle;[Ljava/lang/Object;)Ljava/lang/Object;
+    .registers 2
+    # invoke-polymorphic is the right way to execute invokeExact.
+    # invoke-polymorphic {p0, p1}, Ljava/lang/invoke/MethodHandle;->invokeExact([Ljava/lang/Object;)Ljava/lang/Object;, ([Ljava/lang/Object;)Ljava/lang/Object;
+    invoke-virtual {p0, p1}, Ljava/lang/invoke/MethodHandle;->invokeExact([Ljava/lang/Object;)Ljava/lang/Object;
+    move-result-object v0
+    return-object v0
+.end method
diff --git a/test/2286-invokevirtual-invokeexact/src/Main.java b/test/2286-invokevirtual-invokeexact/src/Main.java
new file mode 100644
index 0000000000..68f8269292
--- /dev/null
+++ b/test/2286-invokevirtual-invokeexact/src/Main.java
@@ -0,0 +1,45 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static java.lang.invoke.MethodType.methodType;
+
+import java.lang.invoke.MethodHandle;
+import java.lang.invoke.MethodHandles;
+import java.lang.invoke.MethodType;
+import java.lang.reflect.Method;
+
+public class Main {
+    public static void main(String[] args) throws Throwable {
+        MethodHandle mh = MethodHandles.lookup()
+            .findStatic(Main.class, "toString", methodType(Object.class, Object[].class));
+
+        Object[] objects = new Object[2];
+        objects[0] = "111";
+        objects[1] = 10;
+
+        Class testClass = Class.forName("Test");
+
+        Method m = testClass.getDeclaredMethod("test", MethodHandle.class, Object[].class);
+        try {
+            Object o = m.invoke(null, mh, objects);
+            throw new AssertionError("unreachable");
+        } catch (Exception expected) {}
+    }
+
+    static Object toString(Object[] objects) {
+        return "objects";
+    }
+}
diff --git a/test/2286-method-tracing-aot-code/expected-stderr.txt b/test/2286-method-tracing-aot-code/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/2286-method-tracing-aot-code/expected-stdout.txt b/test/2286-method-tracing-aot-code/expected-stdout.txt
new file mode 100644
index 0000000000..6a5618ebc6
--- /dev/null
+++ b/test/2286-method-tracing-aot-code/expected-stdout.txt
@@ -0,0 +1 @@
+JNI_OnLoad called
diff --git a/test/2286-method-tracing-aot-code/info.txt b/test/2286-method-tracing-aot-code/info.txt
new file mode 100644
index 0000000000..2c5b5a42f7
--- /dev/null
+++ b/test/2286-method-tracing-aot-code/info.txt
@@ -0,0 +1 @@
+Tests that AOT code is used if available after tracing has stopped.
diff --git a/test/2286-method-tracing-aot-code/src/Main.java b/test/2286-method-tracing-aot-code/src/Main.java
new file mode 100644
index 0000000000..138f6fd11c
--- /dev/null
+++ b/test/2286-method-tracing-aot-code/src/Main.java
@@ -0,0 +1,124 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.FileDescriptor;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.lang.reflect.Method;
+
+public class Main {
+    private static final String TEMP_FILE_NAME_PREFIX = "test";
+    private static final String TEMP_FILE_NAME_SUFFIX = ".trace";
+    private static File file;
+
+    public static void main(String[] args) throws Exception {
+        System.loadLibrary(args[0]);
+        String name = System.getProperty("java.vm.name");
+        if (!"Dalvik".equals(name)) {
+            System.out.println("This test is not supported on " + name);
+            return;
+        }
+
+        if (VMDebug.getMethodTracingMode() != 0) {
+            VMDebug.$noinline$stopMethodTracing();
+        }
+
+        Main m = new Main();
+        boolean isAot_main_before = isAotCompiled(Main.class, "main");
+        boolean isAot_callOuter_before = isAotCompiled(Main.class, "callOuterFunction");
+
+        file = createTempFile();
+        FileOutputStream out_file = new FileOutputStream(file);
+        VMDebug.startMethodTracing(file.getPath(), out_file.getFD(), /*buffer_size=*/0, /*flags=*/0,
+                /*sampling=*/false, /*sampling_interval*/ 0, /*streaming=*/true);
+        m.$noinline$doSomeWork();
+
+        VMDebug.$noinline$stopMethodTracing();
+        out_file.close();
+        file.delete();
+
+        boolean isAot_main_after = isAotCompiled(Main.class, "main");
+        boolean isAot_callOuter_after = isAotCompiled(Main.class, "callOuterFunction");
+        if (isAot_main_before != isAot_main_after) {
+            throw new Exception("AOT code for main not restored after method tracing? "
+                    + isAot_main_before + " " + isAot_main_after);
+        }
+
+        if (isAot_callOuter_before != isAot_callOuter_after) {
+            throw new Exception("AOT code for callOuter not restored after method tracing? "
+                    + isAot_callOuter_before + " " + isAot_callOuter_after);
+        }
+    }
+
+    private static File createTempFile() throws Exception {
+        try {
+            return File.createTempFile(TEMP_FILE_NAME_PREFIX, TEMP_FILE_NAME_SUFFIX);
+        } catch (IOException e) {
+            System.setProperty("java.io.tmpdir", "/data/local/tmp");
+            try {
+                return File.createTempFile(TEMP_FILE_NAME_PREFIX, TEMP_FILE_NAME_SUFFIX);
+            } catch (IOException e2) {
+                System.setProperty("java.io.tmpdir", "/sdcard");
+                return File.createTempFile(TEMP_FILE_NAME_PREFIX, TEMP_FILE_NAME_SUFFIX);
+            }
+        }
+    }
+
+    public void callOuterFunction() {
+        callLeafFunction();
+    }
+
+    public void callLeafFunction() {}
+
+    public void $noinline$doSomeWork() {
+        callOuterFunction();
+        callLeafFunction();
+    }
+
+    private static class VMDebug {
+        private static final Method startMethodTracingMethod;
+        private static final Method stopMethodTracingMethod;
+        private static final Method getMethodTracingModeMethod;
+        static {
+            try {
+                Class<?> c = Class.forName("dalvik.system.VMDebug");
+                startMethodTracingMethod = c.getDeclaredMethod("startMethodTracing", String.class,
+                        FileDescriptor.class, Integer.TYPE, Integer.TYPE, Boolean.TYPE,
+                        Integer.TYPE, Boolean.TYPE);
+                stopMethodTracingMethod = c.getDeclaredMethod("stopMethodTracing");
+                getMethodTracingModeMethod = c.getDeclaredMethod("getMethodTracingMode");
+            } catch (Exception e) {
+                throw new RuntimeException(e);
+            }
+        }
+
+        public static void startMethodTracing(String filename, FileDescriptor fd, int bufferSize,
+                int flags, boolean samplingEnabled, int intervalUs, boolean streaming)
+                throws Exception {
+            startMethodTracingMethod.invoke(
+                    null, filename, fd, bufferSize, flags, samplingEnabled, intervalUs, streaming);
+        }
+        public static void $noinline$stopMethodTracing() throws Exception {
+            stopMethodTracingMethod.invoke(null);
+        }
+        public static int getMethodTracingMode() throws Exception {
+            return (int) getMethodTracingModeMethod.invoke(null);
+        }
+    }
+
+    private native static boolean isAotCompiled(Class<?> cls, String methodName);
+}
diff --git a/test/412-new-array/smali/fill_array_data.smali b/test/412-new-array/smali/FillArrayData.smali
similarity index 71%
rename from test/412-new-array/smali/fill_array_data.smali
rename to test/412-new-array/smali/FillArrayData.smali
index f163084432..324ebcf916 100644
--- a/test/412-new-array/smali/fill_array_data.smali
+++ b/test/412-new-array/smali/FillArrayData.smali
@@ -1,3 +1,19 @@
+# /*
+#  * Copyright 2014 The Android Open Source Project
+#  *
+#  * Licensed under the Apache License, Version 2.0 (the "License");
+#  * you may not use this file except in compliance with the License.
+#  * You may obtain a copy of the License at
+#  *
+#  *      http://www.apache.org/licenses/LICENSE-2.0
+#  *
+#  * Unless required by applicable law or agreed to in writing, software
+#  * distributed under the License is distributed on an "AS IS" BASIS,
+#  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  * See the License for the specific language governing permissions and
+#  * limitations under the License.
+#  */
+
 .class public LFillArrayData;
 
 .super Ljava/lang/Object;
diff --git a/test/412-new-array/smali/filled_new_array.smali b/test/412-new-array/smali/FilledNewArray.smali
similarity index 63%
rename from test/412-new-array/smali/filled_new_array.smali
rename to test/412-new-array/smali/FilledNewArray.smali
index ed8683a14b..19c8487aa9 100644
--- a/test/412-new-array/smali/filled_new_array.smali
+++ b/test/412-new-array/smali/FilledNewArray.smali
@@ -1,3 +1,19 @@
+# /*
+#  * Copyright 2014 The Android Open Source Project
+#  *
+#  * Licensed under the Apache License, Version 2.0 (the "License");
+#  * you may not use this file except in compliance with the License.
+#  * You may obtain a copy of the License at
+#  *
+#  *      http://www.apache.org/licenses/LICENSE-2.0
+#  *
+#  * Unless required by applicable law or agreed to in writing, software
+#  * distributed under the License is distributed on an "AS IS" BASIS,
+#  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  * See the License for the specific language governing permissions and
+#  * limitations under the License.
+#  */
+
 .class public LFilledNewArray;
 
 .super Ljava/lang/Object;
diff --git a/test/412-new-array/smali/FilledNewArrayInternalErrorB.smali b/test/412-new-array/smali/FilledNewArrayInternalErrorB.smali
new file mode 100644
index 0000000000..b5cf0e2a72
--- /dev/null
+++ b/test/412-new-array/smali/FilledNewArrayInternalErrorB.smali
@@ -0,0 +1,26 @@
+# /*
+#  * Copyright 2025 The Android Open Source Project
+#  *
+#  * Licensed under the Apache License, Version 2.0 (the "License");
+#  * you may not use this file except in compliance with the License.
+#  * You may obtain a copy of the License at
+#  *
+#  *      http://www.apache.org/licenses/LICENSE-2.0
+#  *
+#  * Unless required by applicable law or agreed to in writing, software
+#  * distributed under the License is distributed on an "AS IS" BASIS,
+#  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  * See the License for the specific language governing permissions and
+#  * limitations under the License.
+#  */
+
+.class public LFilledNewArrayInternalErrorB;
+
+.super Ljava/lang/Object;
+
+.method public static fail()[B
+   .registers 1
+   filled-new-array {}, [B
+   move-result-object v0
+   return-object v0
+.end method
diff --git a/test/412-new-array/smali/FilledNewArrayInternalErrorC.smali b/test/412-new-array/smali/FilledNewArrayInternalErrorC.smali
new file mode 100644
index 0000000000..768fa9ff40
--- /dev/null
+++ b/test/412-new-array/smali/FilledNewArrayInternalErrorC.smali
@@ -0,0 +1,26 @@
+# /*
+#  * Copyright 2025 The Android Open Source Project
+#  *
+#  * Licensed under the Apache License, Version 2.0 (the "License");
+#  * you may not use this file except in compliance with the License.
+#  * You may obtain a copy of the License at
+#  *
+#  *      http://www.apache.org/licenses/LICENSE-2.0
+#  *
+#  * Unless required by applicable law or agreed to in writing, software
+#  * distributed under the License is distributed on an "AS IS" BASIS,
+#  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  * See the License for the specific language governing permissions and
+#  * limitations under the License.
+#  */
+
+.class public LFilledNewArrayInternalErrorC;
+
+.super Ljava/lang/Object;
+
+.method public static fail()[C
+   .registers 1
+   filled-new-array {}, [C
+   move-result-object v0
+   return-object v0
+.end method
diff --git a/test/412-new-array/smali/FilledNewArrayInternalErrorF.smali b/test/412-new-array/smali/FilledNewArrayInternalErrorF.smali
new file mode 100644
index 0000000000..31056510bd
--- /dev/null
+++ b/test/412-new-array/smali/FilledNewArrayInternalErrorF.smali
@@ -0,0 +1,26 @@
+# /*
+#  * Copyright 2025 The Android Open Source Project
+#  *
+#  * Licensed under the Apache License, Version 2.0 (the "License");
+#  * you may not use this file except in compliance with the License.
+#  * You may obtain a copy of the License at
+#  *
+#  *      http://www.apache.org/licenses/LICENSE-2.0
+#  *
+#  * Unless required by applicable law or agreed to in writing, software
+#  * distributed under the License is distributed on an "AS IS" BASIS,
+#  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  * See the License for the specific language governing permissions and
+#  * limitations under the License.
+#  */
+
+.class public LFilledNewArrayInternalErrorF;
+
+.super Ljava/lang/Object;
+
+.method public static fail()[F
+   .registers 1
+   filled-new-array {}, [F
+   move-result-object v0
+   return-object v0
+.end method
diff --git a/test/412-new-array/smali/FilledNewArrayInternalErrorS.smali b/test/412-new-array/smali/FilledNewArrayInternalErrorS.smali
new file mode 100644
index 0000000000..8394a8f1b9
--- /dev/null
+++ b/test/412-new-array/smali/FilledNewArrayInternalErrorS.smali
@@ -0,0 +1,26 @@
+# /*
+#  * Copyright 2025 The Android Open Source Project
+#  *
+#  * Licensed under the Apache License, Version 2.0 (the "License");
+#  * you may not use this file except in compliance with the License.
+#  * You may obtain a copy of the License at
+#  *
+#  *      http://www.apache.org/licenses/LICENSE-2.0
+#  *
+#  * Unless required by applicable law or agreed to in writing, software
+#  * distributed under the License is distributed on an "AS IS" BASIS,
+#  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  * See the License for the specific language governing permissions and
+#  * limitations under the License.
+#  */
+
+.class public LFilledNewArrayInternalErrorS;
+
+.super Ljava/lang/Object;
+
+.method public static fail()[S
+   .registers 1
+   filled-new-array {}, [S
+   move-result-object v0
+   return-object v0
+.end method
diff --git a/test/412-new-array/smali/FilledNewArrayInternalErrorZ.smali b/test/412-new-array/smali/FilledNewArrayInternalErrorZ.smali
new file mode 100644
index 0000000000..2e7ae7e368
--- /dev/null
+++ b/test/412-new-array/smali/FilledNewArrayInternalErrorZ.smali
@@ -0,0 +1,26 @@
+# /*
+#  * Copyright 2025 The Android Open Source Project
+#  *
+#  * Licensed under the Apache License, Version 2.0 (the "License");
+#  * you may not use this file except in compliance with the License.
+#  * You may obtain a copy of the License at
+#  *
+#  *      http://www.apache.org/licenses/LICENSE-2.0
+#  *
+#  * Unless required by applicable law or agreed to in writing, software
+#  * distributed under the License is distributed on an "AS IS" BASIS,
+#  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  * See the License for the specific language governing permissions and
+#  * limitations under the License.
+#  */
+
+.class public LFilledNewArrayInternalErrorZ;
+
+.super Ljava/lang/Object;
+
+.method public static fail()[Z
+   .registers 1
+   filled-new-array {}, [Z
+   move-result-object v0
+   return-object v0
+.end method
diff --git a/test/412-new-array/smali/FilledNewArrayVerifyError.smali b/test/412-new-array/smali/FilledNewArrayVerifyError.smali
new file mode 100644
index 0000000000..78b8076d9d
--- /dev/null
+++ b/test/412-new-array/smali/FilledNewArrayVerifyError.smali
@@ -0,0 +1,26 @@
+# /*
+#  * Copyright 2014 The Android Open Source Project
+#  *
+#  * Licensed under the Apache License, Version 2.0 (the "License");
+#  * you may not use this file except in compliance with the License.
+#  * You may obtain a copy of the License at
+#  *
+#  *      http://www.apache.org/licenses/LICENSE-2.0
+#  *
+#  * Unless required by applicable law or agreed to in writing, software
+#  * distributed under the License is distributed on an "AS IS" BASIS,
+#  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  * See the License for the specific language governing permissions and
+#  * limitations under the License.
+#  */
+
+.class public LFilledNewArrayVerifyError;
+
+.super Ljava/lang/Object;
+
+.method public static newRef(Ljava/lang/Object;Ljava/lang/Object;)[Ljava/lang/Object;
+   .registers 3
+   filled-new-array {v1, v2}, [Ljava/lang/Integer;
+   move-result-object v0
+   return-object v0
+.end method
diff --git a/test/412-new-array/smali/FilledNewArrayVerifyErrorD.smali b/test/412-new-array/smali/FilledNewArrayVerifyErrorD.smali
new file mode 100644
index 0000000000..4d44fdbbb2
--- /dev/null
+++ b/test/412-new-array/smali/FilledNewArrayVerifyErrorD.smali
@@ -0,0 +1,26 @@
+# /*
+#  * Copyright 2025 The Android Open Source Project
+#  *
+#  * Licensed under the Apache License, Version 2.0 (the "License");
+#  * you may not use this file except in compliance with the License.
+#  * You may obtain a copy of the License at
+#  *
+#  *      http://www.apache.org/licenses/LICENSE-2.0
+#  *
+#  * Unless required by applicable law or agreed to in writing, software
+#  * distributed under the License is distributed on an "AS IS" BASIS,
+#  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  * See the License for the specific language governing permissions and
+#  * limitations under the License.
+#  */
+
+.class public LFilledNewArrayVerifyErrorD;
+
+.super Ljava/lang/Object;
+
+.method public static fail()[D
+   .registers 1
+   filled-new-array {}, [D
+   move-result-object v0
+   return-object v0
+.end method
diff --git a/test/412-new-array/smali/FilledNewArrayVerifyErrorJ.smali b/test/412-new-array/smali/FilledNewArrayVerifyErrorJ.smali
new file mode 100644
index 0000000000..7e8e2e5244
--- /dev/null
+++ b/test/412-new-array/smali/FilledNewArrayVerifyErrorJ.smali
@@ -0,0 +1,26 @@
+# /*
+#  * Copyright 2025 The Android Open Source Project
+#  *
+#  * Licensed under the Apache License, Version 2.0 (the "License");
+#  * you may not use this file except in compliance with the License.
+#  * You may obtain a copy of the License at
+#  *
+#  *      http://www.apache.org/licenses/LICENSE-2.0
+#  *
+#  * Unless required by applicable law or agreed to in writing, software
+#  * distributed under the License is distributed on an "AS IS" BASIS,
+#  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  * See the License for the specific language governing permissions and
+#  * limitations under the License.
+#  */
+
+.class public LFilledNewArrayVerifyErrorJ;
+
+.super Ljava/lang/Object;
+
+.method public static fail()[J
+   .registers 1
+   filled-new-array {}, [J
+   move-result-object v0
+   return-object v0
+.end method
diff --git a/test/412-new-array/smali/filled_new_array_verify_error.smali b/test/412-new-array/smali/filled_new_array_verify_error.smali
deleted file mode 100644
index b1470ec612..0000000000
--- a/test/412-new-array/smali/filled_new_array_verify_error.smali
+++ /dev/null
@@ -1,10 +0,0 @@
-.class public LFilledNewArrayVerifyError;
-
-.super Ljava/lang/Object;
-
-.method public static newRef(Ljava/lang/Object;Ljava/lang/Object;)[Ljava/lang/Object;
-   .registers 3
-   filled-new-array {v1, v2}, [Ljava/lang/Integer;
-   move-result-object v0
-   return-object v0
-.end method
diff --git a/test/412-new-array/src/Main.java b/test/412-new-array/src/Main.java
index fb348ba9c9..c2f82908d7 100644
--- a/test/412-new-array/src/Main.java
+++ b/test/412-new-array/src/Main.java
@@ -29,7 +29,21 @@ public class Main extends TestCase {
     testNegativeArraySize();
     testSmaliFilledNewArray();
     testSmaliFillArrayData();
-    testSmaliVerifyError();
+
+    // Ensure the elements in filled-new-array must be assignable
+    // to the array component type.
+    testSmaliVerifyError("FilledNewArrayVerifyError");
+
+    // Ensure invalid array types `[J` and `[D` are rejected.
+    testSmaliVerifyError("FilledNewArrayVerifyErrorJ");
+    testSmaliVerifyError("FilledNewArrayVerifyErrorD");
+
+    // Test that `filled-new-array` with `[Z`, `B`, `[C`, `[S` or `[F` throws `InternalError`.
+    testSmaliInternalError("FilledNewArrayInternalErrorZ");
+    testSmaliInternalError("FilledNewArrayInternalErrorB");
+    testSmaliInternalError("FilledNewArrayInternalErrorC");
+    testSmaliInternalError("FilledNewArrayInternalErrorS");
+    testSmaliInternalError("FilledNewArrayInternalErrorF");
   }
 
   static void $opt$TestAllocations() {
@@ -205,18 +219,29 @@ public class Main extends TestCase {
     }
   }
 
-  public static void testSmaliVerifyError() throws Exception {
+  public static void testSmaliVerifyError(String name) throws Exception {
     Error error = null;
-    // Ensure the elements in filled-new-array must be assignable
-    // to the array component type.
     try {
-      Class.forName("FilledNewArrayVerifyError");
+      Class.forName(name);
     } catch (VerifyError e) {
       error = e;
     }
     assertNotNull(error);
   }
 
+  public static void testSmaliInternalError(String name) throws Exception {
+    Throwable t = null;
+    try {
+      Class<?> c = Class.forName(name);
+      Method m = c.getMethod("fail");
+      m.invoke(null);
+    } catch (InvocationTargetException e) {
+      t = e.getCause();
+      assertTrue(t instanceof InternalError);
+    }
+    assertNotNull(t);
+  }
+
   public static void testSmaliFillArrayData() throws Exception {
     Class<?> c = Class.forName("FillArrayData");
     {
diff --git a/test/458-checker-instruct-simplification/src/Main.java b/test/458-checker-instruct-simplification/src/Main.java
index 784acf7949..f65d339dae 100644
--- a/test/458-checker-instruct-simplification/src/Main.java
+++ b/test/458-checker-instruct-simplification/src/Main.java
@@ -1231,7 +1231,7 @@ public class Main {
   /// CHECK-DAG:      <<Phi:i\d+>>      Phi [<<Const13>>,<<Const54>>]
   /// CHECK-DAG:                        Return [<<Phi>>]
 
-  /// CHECK-START: int Main.$noinline$booleanFieldNotEqualOne() select_generator (after)
+  /// CHECK-START: int Main.$noinline$booleanFieldNotEqualOne() control_flow_simplifier (after)
   /// CHECK-DAG:      <<Field:z\d+>>    StaticFieldGet
   /// CHECK-DAG:      <<Const13:i\d+>>  IntConstant 13
   /// CHECK-DAG:      <<Const54:i\d+>>  IntConstant 54
@@ -1252,7 +1252,7 @@ public class Main {
   /// CHECK-DAG:      <<Phi:i\d+>>      Phi [<<Const13>>,<<Const54>>]
   /// CHECK-DAG:                        Return [<<Phi>>]
 
-  /// CHECK-START: int Main.$noinline$booleanFieldEqualZero() select_generator (after)
+  /// CHECK-START: int Main.$noinline$booleanFieldEqualZero() control_flow_simplifier (after)
   /// CHECK-DAG:      <<Field:z\d+>>    StaticFieldGet
   /// CHECK-DAG:      <<Const13:i\d+>>  IntConstant 13
   /// CHECK-DAG:      <<Const54:i\d+>>  IntConstant 54
@@ -1278,7 +1278,7 @@ public class Main {
   /// CHECK-DAG:      <<Phi2:i\d+>>     Phi [<<Const13>>,<<Const54>>]
   /// CHECK-DAG:                        Return [<<Phi2>>]
 
-  /// CHECK-START: int Main.$noinline$intConditionNotEqualOne(int) select_generator (after)
+  /// CHECK-START: int Main.$noinline$intConditionNotEqualOne(int) control_flow_simplifier (after)
   /// CHECK-DAG:      <<Arg:i\d+>>      ParameterValue
   /// CHECK-DAG:      <<Const13:i\d+>>  IntConstant 13
   /// CHECK-DAG:      <<Const42:i\d+>>  IntConstant 42
@@ -1308,7 +1308,7 @@ public class Main {
   /// CHECK-DAG:      <<Phi2:i\d+>>     Phi [<<Const13>>,<<Const54>>]
   /// CHECK-DAG:                        Return [<<Phi2>>]
 
-  /// CHECK-START: int Main.$noinline$intConditionEqualZero(int) select_generator (after)
+  /// CHECK-START: int Main.$noinline$intConditionEqualZero(int) control_flow_simplifier (after)
   /// CHECK-DAG:      <<Arg:i\d+>>      ParameterValue
   /// CHECK-DAG:      <<Const13:i\d+>>  IntConstant 13
   /// CHECK-DAG:      <<Const42:i\d+>>  IntConstant 42
diff --git a/test/463-checker-boolean-simplifier/smali/Main2.smali b/test/463-checker-boolean-simplifier/smali/Main2.smali
index e8ebb2380c..f52700f34c 100644
--- a/test/463-checker-boolean-simplifier/smali/Main2.smali
+++ b/test/463-checker-boolean-simplifier/smali/Main2.smali
@@ -31,7 +31,7 @@
 # Elementary test negating a boolean. Verifies that blocks are merged and
 # empty branches removed.
 
-## CHECK-START: boolean Main2.BooleanNot(boolean) select_generator (before)
+## CHECK-START: boolean Main2.BooleanNot(boolean) control_flow_simplifier (before)
 ## CHECK-DAG:     <<Param:z\d+>>    ParameterValue
 ## CHECK-DAG:     <<Const0:i\d+>>   IntConstant 0
 ## CHECK-DAG:     <<Const1:i\d+>>   IntConstant 1
@@ -39,24 +39,24 @@
 ## CHECK-DAG:     <<Phi:i\d+>>      Phi [<<Const1>>,<<Const0>>]
 ## CHECK-DAG:                       Return [<<Phi>>]
 
-## CHECK-START: boolean Main2.BooleanNot(boolean) select_generator (before)
+## CHECK-START: boolean Main2.BooleanNot(boolean) control_flow_simplifier (before)
 ## CHECK:                           Goto
 ## CHECK:                           Goto
 ## CHECK:                           Goto
 ## CHECK-NOT:                       Goto
 
-## CHECK-START: boolean Main2.BooleanNot(boolean) select_generator (after)
+## CHECK-START: boolean Main2.BooleanNot(boolean) control_flow_simplifier (after)
 ## CHECK-DAG:     <<Param:z\d+>>    ParameterValue
 ## CHECK-DAG:     <<Const0:i\d+>>   IntConstant 0
 ## CHECK-DAG:     <<Const1:i\d+>>   IntConstant 1
 ## CHECK-DAG:     <<NotParam:i\d+>> Select [<<Const1>>,<<Const0>>,<<Param>>]
 ## CHECK-DAG:                       Return [<<NotParam>>]
 
-## CHECK-START: boolean Main2.BooleanNot(boolean) select_generator (after)
+## CHECK-START: boolean Main2.BooleanNot(boolean) control_flow_simplifier (after)
 ## CHECK-NOT:                       If
 ## CHECK-NOT:                       Phi
 
-## CHECK-START: boolean Main2.BooleanNot(boolean) select_generator (after)
+## CHECK-START: boolean Main2.BooleanNot(boolean) control_flow_simplifier (after)
 ## CHECK:                           Goto
 ## CHECK-NOT:                       Goto
 
@@ -86,7 +86,7 @@
 # Program which further uses negated conditions.
 # Note that Phis are discovered retrospectively.
 
-## CHECK-START: boolean Main2.ValuesOrdered(int, int, int) select_generator (before)
+## CHECK-START: boolean Main2.ValuesOrdered(int, int, int) control_flow_simplifier (before)
 ## CHECK-DAG:     <<ParamX:i\d+>>   ParameterValue
 ## CHECK-DAG:     <<ParamY:i\d+>>   ParameterValue
 ## CHECK-DAG:     <<ParamZ:i\d+>>   ParameterValue
@@ -103,7 +103,7 @@
 ## CHECK-DAG:     <<PhiYZ>>         Phi [<<Const1>>,<<Const0>>]
 ## CHECK-DAG:     <<PhiXYZ>>        Phi [<<Const1>>,<<Const0>>]
 
-## CHECK-START: boolean Main2.ValuesOrdered(int, int, int) select_generator (after)
+## CHECK-START: boolean Main2.ValuesOrdered(int, int, int) control_flow_simplifier (after)
 ## CHECK-DAG:     <<ParamX:i\d+>>   ParameterValue
 ## CHECK-DAG:     <<ParamY:i\d+>>   ParameterValue
 ## CHECK-DAG:     <<ParamZ:i\d+>>   ParameterValue
@@ -164,7 +164,7 @@
     goto :goto_a
 .end method
 
-## CHECK-START: int Main2.NegatedCondition(boolean) select_generator (before)
+## CHECK-START: int Main2.NegatedCondition(boolean) control_flow_simplifier (before)
 ## CHECK-DAG:     <<Param:z\d+>>    ParameterValue
 ## CHECK-DAG:     <<Const42:i\d+>>  IntConstant 42
 ## CHECK-DAG:     <<Const43:i\d+>>  IntConstant 43
@@ -172,14 +172,14 @@
 ## CHECK-DAG:     <<Phi:i\d+>>      Phi [<<Const42>>,<<Const43>>]
 ## CHECK-DAG:                       Return [<<Phi>>]
 
-## CHECK-START: int Main2.NegatedCondition(boolean) select_generator (after)
+## CHECK-START: int Main2.NegatedCondition(boolean) control_flow_simplifier (after)
 ## CHECK-DAG:     <<Param:z\d+>>    ParameterValue
 ## CHECK-DAG:     <<Const42:i\d+>>  IntConstant 42
 ## CHECK-DAG:     <<Const43:i\d+>>  IntConstant 43
 ## CHECK-DAG:     <<Select:i\d+>>   Select [<<Const43>>,<<Const42>>,<<Param>>]
 ## CHECK-DAG:                       Return [<<Select>>]
 
-## CHECK-START: int Main2.NegatedCondition(boolean) select_generator (after)
+## CHECK-START: int Main2.NegatedCondition(boolean) control_flow_simplifier (after)
 ## CHECK-NOT:                       BooleanNot
 
 # The original java source of this method:
@@ -214,7 +214,7 @@
 # This test currently checks that we don't perform select generation due to
 # having multiple phis.
 
-## CHECK-START: int Main2.MultiplePhis() select_generator (before)
+## CHECK-START: int Main2.MultiplePhis() control_flow_simplifier (before)
 ## CHECK-DAG:     <<Const0:i\d+>>   IntConstant 0
 ## CHECK-DAG:     <<Const1:i\d+>>   IntConstant 1
 ## CHECK-DAG:     <<Const13:i\d+>>  IntConstant 13
@@ -226,16 +226,16 @@
 ## CHECK-DAG:                       If [<<Cond>>]
 ## CHECK-DAG:                       Return [<<PhiX>>]
 
-## CHECK-START: int Main2.MultiplePhis() select_generator (after)
+## CHECK-START: int Main2.MultiplePhis() control_flow_simplifier (after)
 ## CHECK-DAG:     <<Const0:i\d+>>   IntConstant 0
 ## CHECK-DAG:     <<Const1:i\d+>>   IntConstant 1
 ## CHECK-DAG:     <<Const13:i\d+>>  IntConstant 13
 ## CHECK-DAG:     <<Const42:i\d+>>  IntConstant 42
-## CHECK-DAG:     <<PhiX:i\d+>>     Phi [<<Const0>>,<<Const13>>,<<Const42>>]
-## CHECK-DAG:     <<PhiY:i\d+>>     Phi [<<Const1>>,<<Add:i\d+>>,<<Add>>]
+## CHECK-DAG:     <<PhiX:i\d+>>     Phi [<<Const0>>,<<Select:i\d+>>]
+## CHECK-DAG:     <<PhiY:i\d+>>     Phi [<<Const1>>,<<Add:i\d+>>]
 ## CHECK-DAG:     <<Add>>           Add [<<PhiY>>,<<Const1>>]
 ## CHECK-DAG:     <<Cond:z\d+>>     LessThanOrEqual [<<Add>>,<<Const1>>]
-## CHECK-DAG:                       If [<<Cond>>]
+## CHECK-DAG:     <<Select>>        Select [<<Const13>>,<<Const42>>,<<Cond>>]
 ## CHECK-DAG:                       Return [<<PhiX>>]
 
 # The original java source of this method:
diff --git a/test/463-checker-boolean-simplifier/src-art/Main.java b/test/463-checker-boolean-simplifier/src-art/Main.java
index 2c759ed6f9..a76966d167 100644
--- a/test/463-checker-boolean-simplifier/src-art/Main.java
+++ b/test/463-checker-boolean-simplifier/src-art/Main.java
@@ -39,7 +39,7 @@ public class Main {
    * and 0 when False.
    */
 
-  /// CHECK-START: boolean Main.GreaterThan(int, int) select_generator (before)
+  /// CHECK-START: boolean Main.GreaterThan(int, int) control_flow_simplifier (before)
   /// CHECK-DAG:     <<ParamX:i\d+>>   ParameterValue
   /// CHECK-DAG:     <<ParamY:i\d+>>   ParameterValue
   /// CHECK-DAG:     <<Const0:i\d+>>   IntConstant 0
@@ -49,7 +49,7 @@ public class Main {
   /// CHECK-DAG:     <<Phi:i\d+>>      Phi [<<Const0>>,<<Const1>>]
   /// CHECK-DAG:                       Return [<<Phi>>]
 
-  /// CHECK-START: boolean Main.GreaterThan(int, int) select_generator (after)
+  /// CHECK-START: boolean Main.GreaterThan(int, int) control_flow_simplifier (after)
   /// CHECK-DAG:     <<ParamX:i\d+>>   ParameterValue
   /// CHECK-DAG:     <<ParamY:i\d+>>   ParameterValue
   /// CHECK-DAG:     <<Const0:i\d+>>   IntConstant 0
@@ -67,7 +67,7 @@ public class Main {
    * and 1 when False.
    */
 
-  /// CHECK-START: boolean Main.LessThan(int, int) select_generator (before)
+  /// CHECK-START: boolean Main.LessThan(int, int) control_flow_simplifier (before)
   /// CHECK-DAG:     <<ParamX:i\d+>>   ParameterValue
   /// CHECK-DAG:     <<ParamY:i\d+>>   ParameterValue
   /// CHECK-DAG:     <<Const0:i\d+>>   IntConstant 0
@@ -77,7 +77,7 @@ public class Main {
   /// CHECK-DAG:     <<Phi:i\d+>>      Phi [<<Const1>>,<<Const0>>]
   /// CHECK-DAG:                       Return [<<Phi>>]
 
-  /// CHECK-START: boolean Main.LessThan(int, int) select_generator (after)
+  /// CHECK-START: boolean Main.LessThan(int, int) control_flow_simplifier (after)
   /// CHECK-DAG:     <<ParamX:i\d+>>   ParameterValue
   /// CHECK-DAG:     <<ParamY:i\d+>>   ParameterValue
   /// CHECK-DAG:     <<Const0:i\d+>>   IntConstant 0
@@ -90,7 +90,7 @@ public class Main {
     return (x < y) ? true : false;
   }
 
-  /// CHECK-START: int Main.SimpleTrueBlock(boolean, int) select_generator (after)
+  /// CHECK-START: int Main.SimpleTrueBlock(boolean, int) control_flow_simplifier (after)
   /// CHECK-DAG:     <<ParamX:z\d+>>   ParameterValue
   /// CHECK-DAG:     <<ParamY:i\d+>>   ParameterValue
   /// CHECK-DAG:     <<Const42:i\d+>>  IntConstant 42
@@ -99,14 +99,14 @@ public class Main {
   /// CHECK-DAG:     <<Select:i\d+>>   Select [<<Const43>>,<<Add>>,<<ParamX>>]
   /// CHECK-DAG:                       Return [<<Select>>]
 
-  /// CHECK-START: int Main.SimpleTrueBlock(boolean, int) select_generator (after)
+  /// CHECK-START: int Main.SimpleTrueBlock(boolean, int) control_flow_simplifier (after)
   /// CHECK-NOT:     If
 
   public static int SimpleTrueBlock(boolean x, int y) {
     return x ? y + 42 : 43;
   }
 
-  /// CHECK-START: int Main.SimpleFalseBlock(boolean, int) select_generator (after)
+  /// CHECK-START: int Main.SimpleFalseBlock(boolean, int) control_flow_simplifier (after)
   /// CHECK-DAG:     <<ParamX:z\d+>>   ParameterValue
   /// CHECK-DAG:     <<ParamY:i\d+>>   ParameterValue
   /// CHECK-DAG:     <<Const42:i\d+>>  IntConstant 42
@@ -115,14 +115,14 @@ public class Main {
   /// CHECK-DAG:     <<Select:i\d+>>   Select [<<Add>>,<<Const42>>,<<ParamX>>]
   /// CHECK-DAG:                       Return [<<Select>>]
 
-  /// CHECK-START: int Main.SimpleFalseBlock(boolean, int) select_generator (after)
+  /// CHECK-START: int Main.SimpleFalseBlock(boolean, int) control_flow_simplifier (after)
   /// CHECK-NOT:     If
 
   public static int SimpleFalseBlock(boolean x, int y) {
     return x ? 42 : y + 43;
   }
 
-  /// CHECK-START: int Main.SimpleBothBlocks(boolean, int, int) select_generator (after)
+  /// CHECK-START: int Main.SimpleBothBlocks(boolean, int, int) control_flow_simplifier (after)
   /// CHECK-DAG:     <<ParamX:z\d+>>   ParameterValue
   /// CHECK-DAG:     <<ParamY:i\d+>>   ParameterValue
   /// CHECK-DAG:     <<ParamZ:i\d+>>   ParameterValue
@@ -133,14 +133,14 @@ public class Main {
   /// CHECK-DAG:     <<Select:i\d+>>   Select [<<AddFalse>>,<<AddTrue>>,<<ParamX>>]
   /// CHECK-DAG:                       Return [<<Select>>]
 
-  /// CHECK-START: int Main.SimpleBothBlocks(boolean, int, int) select_generator (after)
+  /// CHECK-START: int Main.SimpleBothBlocks(boolean, int, int) control_flow_simplifier (after)
   /// CHECK-NOT:     If
 
   public static int SimpleBothBlocks(boolean x, int y, int z) {
     return x ? y + 42 : z + 43;
   }
 
-  /// CHECK-START: int Main.ThreeBlocks(boolean, boolean) select_generator (after)
+  /// CHECK-START: int Main.ThreeBlocks(boolean, boolean) control_flow_simplifier (after)
   /// CHECK-DAG:     <<ParamX:z\d+>>    ParameterValue
   /// CHECK-DAG:     <<ParamY:z\d+>>    ParameterValue
   /// CHECK-DAG:     <<Const1:i\d+>>    IntConstant 1
@@ -160,7 +160,7 @@ public class Main {
     }
   }
 
-  /// CHECK-START: int Main.TrueBlockWithTooManyInstructions(boolean) select_generator (before)
+  /// CHECK-START: int Main.TrueBlockWithTooManyInstructions(boolean) control_flow_simplifier (before)
   /// CHECK-DAG:     <<This:l\d+>>    ParameterValue
   /// CHECK-DAG:     <<Cond:z\d+>>    ParameterValue
   /// CHECK-DAG:     <<Const2:i\d+>>  IntConstant 2
@@ -170,14 +170,14 @@ public class Main {
   /// CHECK-DAG:     <<Add:i\d+>>     Add [<<Iget>>,<<Const2>>]
   /// CHECK-DAG:                      Phi [<<Add>>,<<Const43>>]
 
-  /// CHECK-START: int Main.TrueBlockWithTooManyInstructions(boolean) select_generator (after)
+  /// CHECK-START: int Main.TrueBlockWithTooManyInstructions(boolean) control_flow_simplifier (after)
   /// CHECK-NOT:     Select
 
   public int TrueBlockWithTooManyInstructions(boolean x) {
     return x ? (read_field + 2) : 43;
   }
 
-  /// CHECK-START: int Main.FalseBlockWithTooManyInstructions(boolean) select_generator (before)
+  /// CHECK-START: int Main.FalseBlockWithTooManyInstructions(boolean) control_flow_simplifier (before)
   /// CHECK-DAG:     <<This:l\d+>>    ParameterValue
   /// CHECK-DAG:     <<Cond:z\d+>>    ParameterValue
   /// CHECK-DAG:     <<Const3:i\d+>>  IntConstant 3
@@ -187,14 +187,14 @@ public class Main {
   /// CHECK-DAG:     <<Add:i\d+>>     Add [<<Iget>>,<<Const3>>]
   /// CHECK-DAG:                      Phi [<<Const42>>,<<Add>>]
 
-  /// CHECK-START: int Main.FalseBlockWithTooManyInstructions(boolean) select_generator (after)
+  /// CHECK-START: int Main.FalseBlockWithTooManyInstructions(boolean) control_flow_simplifier (after)
   /// CHECK-NOT:     Select
 
   public int FalseBlockWithTooManyInstructions(boolean x) {
     return x ? 42 : (read_field + 3);
   }
 
-  /// CHECK-START: int Main.TrueBlockWithSideEffects(boolean) select_generator (before)
+  /// CHECK-START: int Main.TrueBlockWithSideEffects(boolean) control_flow_simplifier (before)
   /// CHECK-DAG:     <<This:l\d+>>    ParameterValue
   /// CHECK-DAG:     <<Cond:z\d+>>    ParameterValue
   /// CHECK-DAG:     <<Const42:i\d+>> IntConstant 42
@@ -203,14 +203,14 @@ public class Main {
   /// CHECK-DAG:                      InstanceFieldSet [<<This>>,<<Const42>>]
   /// CHECK-DAG:                      Phi [<<Const42>>,<<Const43>>]
 
-  /// CHECK-START: int Main.TrueBlockWithSideEffects(boolean) select_generator (after)
+  /// CHECK-START: int Main.TrueBlockWithSideEffects(boolean) control_flow_simplifier (after)
   /// CHECK-NOT:     Select
 
   public int TrueBlockWithSideEffects(boolean x) {
     return x ? (write_field = 42) : 43;
   }
 
-  /// CHECK-START: int Main.FalseBlockWithSideEffects(boolean) select_generator (before)
+  /// CHECK-START: int Main.FalseBlockWithSideEffects(boolean) control_flow_simplifier (before)
   /// CHECK-DAG:     <<This:l\d+>>    ParameterValue
   /// CHECK-DAG:     <<Cond:z\d+>>    ParameterValue
   /// CHECK-DAG:     <<Const42:i\d+>> IntConstant 42
@@ -219,7 +219,7 @@ public class Main {
   /// CHECK-DAG:                      InstanceFieldSet [<<This>>,<<Const43>>]
   /// CHECK-DAG:                      Phi [<<Const42>>,<<Const43>>]
 
-  /// CHECK-START: int Main.FalseBlockWithSideEffects(boolean) select_generator (after)
+  /// CHECK-START: int Main.FalseBlockWithSideEffects(boolean) control_flow_simplifier (after)
   /// CHECK-NOT:     Select
 
   public int FalseBlockWithSideEffects(boolean x) {
diff --git a/test/468-checker-bool-simplif-regression/smali/TestCase.smali b/test/468-checker-bool-simplif-regression/smali/TestCase.smali
index 87ad21ead4..10f2a4abe7 100644
--- a/test/468-checker-bool-simplif-regression/smali/TestCase.smali
+++ b/test/468-checker-bool-simplif-regression/smali/TestCase.smali
@@ -18,7 +18,7 @@
 
 .field public static value:Z
 
-## CHECK-START: boolean TestCase.testCase() select_generator (before)
+## CHECK-START: boolean TestCase.testCase() control_flow_simplifier (before)
 ## CHECK-DAG:     <<Const0:i\d+>>   IntConstant 0
 ## CHECK-DAG:     <<Const1:i\d+>>   IntConstant 1
 ## CHECK-DAG:     <<Value:z\d+>>    StaticFieldGet
@@ -26,7 +26,7 @@
 ## CHECK-DAG:     <<Phi:i\d+>>      Phi [<<Const1>>,<<Const0>>]
 ## CHECK-DAG:                       Return [<<Phi>>]
 
-## CHECK-START: boolean TestCase.testCase() select_generator (after)
+## CHECK-START: boolean TestCase.testCase() control_flow_simplifier (after)
 ## CHECK-DAG:     <<Const0:i\d+>>   IntConstant 0
 ## CHECK-DAG:     <<Const1:i\d+>>   IntConstant 1
 ## CHECK-DAG:     <<Value:z\d+>>    StaticFieldGet
diff --git a/test/474-checker-boolean-input/src/Main.java b/test/474-checker-boolean-input/src/Main.java
index fbc28d8d52..6799f677bb 100644
--- a/test/474-checker-boolean-input/src/Main.java
+++ b/test/474-checker-boolean-input/src/Main.java
@@ -27,7 +27,7 @@ public class Main {
    * we implement a suitable type analysis.
    */
 
-  /// CHECK-START: boolean Main.TestPhiAsBoolean(int) select_generator (after)
+  /// CHECK-START: boolean Main.TestPhiAsBoolean(int) control_flow_simplifier (after)
   /// CHECK-DAG:     <<Phi:i\d+>>     Phi
   /// CHECK-DAG:                      Select [{{i\d+}},{{i\d+}},<<Phi>>]
 
@@ -47,7 +47,7 @@ public class Main {
    * we implement a suitable type analysis.
    */
 
-  /// CHECK-START: boolean Main.TestAndAsBoolean(boolean, boolean) select_generator (after)
+  /// CHECK-START: boolean Main.TestAndAsBoolean(boolean, boolean) control_flow_simplifier (after)
   /// CHECK-DAG:     <<And:i\d+>>     And
   /// CHECK-DAG:                      Select [{{i\d+}},{{i\d+}},<<And>>]
 
@@ -64,7 +64,7 @@ public class Main {
    * we implement a suitable type analysis.
    */
 
-  /// CHECK-START: boolean Main.TestOrAsBoolean(boolean, boolean) select_generator (after)
+  /// CHECK-START: boolean Main.TestOrAsBoolean(boolean, boolean) control_flow_simplifier (after)
   /// CHECK-DAG:     <<Or:i\d+>>      Or
   /// CHECK-DAG:                      Select [{{i\d+}},{{i\d+}},<<Or>>]
 
@@ -81,7 +81,7 @@ public class Main {
    * we implement a suitable type analysis.
    */
 
-  /// CHECK-START: boolean Main.TestXorAsBoolean(boolean, boolean) select_generator (after)
+  /// CHECK-START: boolean Main.TestXorAsBoolean(boolean, boolean) control_flow_simplifier (after)
   /// CHECK-DAG:     <<Xor:i\d+>>     Xor
   /// CHECK-DAG:                      Select [{{i\d+}},{{i\d+}},<<Xor>>]
 
diff --git a/test/485-checker-dce-loop-update/smali/TestCase.smali b/test/485-checker-dce-loop-update/smali/TestCase.smali
index 3e7bca93c9..efc60d5470 100644
--- a/test/485-checker-dce-loop-update/smali/TestCase.smali
+++ b/test/485-checker-dce-loop-update/smali/TestCase.smali
@@ -162,7 +162,7 @@
 ## CHECK-START: int TestCase.testExitPredecessors(int, boolean, boolean) dead_code_elimination$after_inlining (after)
 ## CHECK-NOT:                    IntConstant 5
 
-## CHECK-START: int TestCase.testExitPredecessors(int, boolean, boolean) select_generator (after)
+## CHECK-START: int TestCase.testExitPredecessors(int, boolean, boolean) control_flow_simplifier (after)
 ## CHECK-DAG:     <<ArgX:i\d+>>  ParameterValue
 ## CHECK-DAG:     <<ArgY:z\d+>>  ParameterValue
 ## CHECK-DAG:     <<ArgZ:z\d+>>  ParameterValue
diff --git a/test/530-checker-lse/src/Main.java b/test/530-checker-lse/src/Main.java
index 2516129c6c..2a6a244679 100644
--- a/test/530-checker-lse/src/Main.java
+++ b/test/530-checker-lse/src/Main.java
@@ -4507,5 +4507,7 @@ public class Main {
     assertLongEquals(testOverlapLoop(50), 7778742049l);
     assertIntEquals($noinline$testPartialEscape1(new TestClass(), true), 1);
     assertIntEquals($noinline$testPartialEscape1(new TestClass(), false), 0);
+
+    TypeConversions.main();
   }
 }
diff --git a/test/530-checker-lse/src/TypeConversions.java b/test/530-checker-lse/src/TypeConversions.java
new file mode 100644
index 0000000000..8901c7e282
--- /dev/null
+++ b/test/530-checker-lse/src/TypeConversions.java
@@ -0,0 +1,276 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TypeConversions {
+  static byte static_byte;
+  static char static_char;
+  static int static_int;
+  static int unrelated_static_int;
+
+  public static void assertIntEquals(int expected, int result) {
+    if (expected != result) {
+      throw new Error("Expected: " + expected + ", found: " + result);
+    }
+  }
+
+  /// CHECK-START: int TypeConversions.$noinline$loopPhiStoreLoadConversionInt8(int) load_store_elimination (before)
+  /// CHECK-DAG: <<Value:i\d+>> ParameterValue
+  // The type conversion has already been eliminated.
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<Value>>] field_name:TypeConversions.static_byte
+  /// CHECK-DAG:                StaticFieldSet field_name:TypeConversions.unrelated_static_int loop:B{{\d+}}
+  /// CHECK-DAG: <<GetB:b\d+>>  StaticFieldGet field_name:TypeConversions.static_byte
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<GetB>>] field_name:TypeConversions.static_int
+  /// CHECK-DAG: <<GetI:i\d+>>  StaticFieldGet field_name:TypeConversions.static_int field_type:Int32
+  /// CHECK-DAG:                Return [<<GetI>>]
+
+  /// CHECK-START: int TypeConversions.$noinline$loopPhiStoreLoadConversionInt8(int) load_store_elimination (after)
+  /// CHECK-DAG: <<Value:i\d+>> ParameterValue
+  /// CHECK-DAG: <<Conv:b\d+>>  TypeConversion [<<Value>>]
+  /// CHECK-DAG:                Return [<<Conv>>]
+
+  /// CHECK-START: int TypeConversions.$noinline$loopPhiStoreLoadConversionInt8(int) load_store_elimination (after)
+  /// CHECK-NOT: StaticFieldGet
+  public static int $noinline$loopPhiStoreLoadConversionInt8(int value) {
+    static_byte = (byte) value;
+    // Irrelevant code but needed to make LSE use loop Phi placeholders.
+    for (int q = 1; q < 12; q++) {
+      unrelated_static_int = 24;
+    }
+    static_int = static_byte;
+    return static_int;
+  }
+
+  /// CHECK-START: int TypeConversions.$noinline$loopPhiStoreLoadConversionUint8(int) load_store_elimination (before)
+  /// CHECK-DAG: <<Value:i\d+>> ParameterValue
+  // The type conversion has already been eliminated.
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<Value>>] field_name:TypeConversions.static_byte
+  /// CHECK-DAG:                StaticFieldSet field_name:TypeConversions.unrelated_static_int loop:B{{\d+}}
+  // The `& 0xff` has already been merged with the load.
+  /// CHECK-DAG: <<GetA:a\d+>>  StaticFieldGet field_name:TypeConversions.static_byte
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<GetA>>] field_name:TypeConversions.static_int
+  /// CHECK-DAG: <<GetI:i\d+>>  StaticFieldGet field_name:TypeConversions.static_int field_type:Int32
+  /// CHECK-DAG:                Return [<<GetI>>]
+
+  /// CHECK-START: int TypeConversions.$noinline$loopPhiStoreLoadConversionUint8(int) load_store_elimination (after)
+  /// CHECK-DAG: <<Value:i\d+>> ParameterValue
+  /// CHECK-DAG: <<Conv:a\d+>>  TypeConversion [<<Value>>]
+  /// CHECK-DAG:                Return [<<Conv>>]
+
+  /// CHECK-START: int TypeConversions.$noinline$loopPhiStoreLoadConversionUint8(int) load_store_elimination (after)
+  /// CHECK-NOT: StaticFieldGet
+  public static int $noinline$loopPhiStoreLoadConversionUint8(int value) {
+    static_byte = (byte) value;
+    // Irrelevant code but needed to make LSE use loop Phi placeholders.
+    for (int q = 1; q < 12; q++) {
+        unrelated_static_int = 24;
+    }
+    static_int = static_byte & 0xff;
+    return static_int;
+  }
+
+  /// CHECK-START: int TypeConversions.$noinline$loopPhiTwoStoreLoadConversions(int) load_store_elimination (before)
+  /// CHECK-DAG: <<Value:i\d+>> ParameterValue
+  // The type conversion has already been eliminated.
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<Value>>] field_name:TypeConversions.static_byte
+  /// CHECK-DAG:                StaticFieldSet field_name:TypeConversions.unrelated_static_int loop:B{{\d+}}
+  /// CHECK-DAG: <<GetB:b\d+>>  StaticFieldGet field_name:TypeConversions.static_byte
+  // The type conversion has already been eliminated.
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<GetB>>] field_name:TypeConversions.static_int
+  /// CHECK-DAG: <<GetI1:i\d+>> StaticFieldGet field_name:TypeConversions.static_int field_type:Int32
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<GetI1>>] field_name:TypeConversions.static_char
+  /// CHECK-DAG: <<GetC:c\d+>>  StaticFieldGet field_name:TypeConversions.static_char field_type:Uint16
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<GetC>>] field_name:TypeConversions.static_int
+  /// CHECK-DAG: <<GetI2:i\d+>> StaticFieldGet field_name:TypeConversions.static_int field_type:Int32
+  /// CHECK-DAG:                Return [<<GetI2>>]
+
+  /// CHECK-START: int TypeConversions.$noinline$loopPhiTwoStoreLoadConversions(int) load_store_elimination (after)
+  /// CHECK-DAG: <<Value:i\d+>> ParameterValue
+  /// CHECK-DAG: <<ConvB:b\d+>> TypeConversion [<<Value>>]
+  /// CHECK-DAG: <<ConvC:c\d+>> TypeConversion [<<ConvB>>]
+  /// CHECK-DAG:                Return [<<ConvC>>]
+
+  /// CHECK-START: int TypeConversions.$noinline$loopPhiTwoStoreLoadConversions(int) load_store_elimination (after)
+  /// CHECK-NOT: StaticFieldGet
+  public static int $noinline$loopPhiTwoStoreLoadConversions(int value) {
+      static_byte = (byte) value;
+      // Irrelevant code but needed to make LSE use loop Phi placeholders.
+      for (int q = 1; q < 12; q++) {
+          unrelated_static_int = 24;
+      }
+      // Note: We need to go through `static_int` so that the instruction
+      // simplifier eliminates the type conversion to `char`.
+      // TODO: Improve the instruction simplifier to eliminate the conversion
+      // for `static_char = (char) static_byte`.
+      static_int = static_byte;
+      static_char = (char) static_int;
+      static_int = static_char;
+      return static_int;
+  }
+
+  /// CHECK-START: int TypeConversions.$noinline$conditionalStoreLoadConversionInt8InLoop(int, boolean) load_store_elimination (before)
+  /// CHECK-DAG: <<Value:i\d+>> ParameterValue
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<Value>>] field_name:TypeConversions.static_int
+  /// CHECK-DAG: <<GetI:i\d+>>  StaticFieldGet field_name:TypeConversions.static_int field_type:Int32 loop:<<Loop:B\d+>>
+  // The type conversion has already been eliminated.
+  /// CHECK-DAG:                StaticFieldSet field_name:TypeConversions.static_byte loop:<<Loop>>
+  /// CHECK-DAG: <<GetB:b\d+>>  StaticFieldGet field_name:TypeConversions.static_byte loop:<<Loop>>
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<GetB>>] field_name:TypeConversions.static_int loop:<<Loop>>
+  /// CHECK-DAG: <<GetI2:i\d+>> StaticFieldGet field_name:TypeConversions.static_int field_type:Int32 loop:none
+  /// CHECK-DAG:                Return [<<GetI2>>]
+
+  /// CHECK-START: int TypeConversions.$noinline$conditionalStoreLoadConversionInt8InLoop(int, boolean) load_store_elimination (after)
+  /// CHECK-DAG: <<Value:i\d+>> ParameterValue
+  /// CHECK-DAG: <<Phi1:i\d+>>  Phi [<<Value>>,<<Phi2:i\d+>>] loop:<<Loop:B\d+>>
+  /// CHECK-DAG: <<Conv:b\d+>>  TypeConversion [<<Phi1>>] loop:<<Loop>>
+  /// CHECK-DAG: <<Phi2>>       Phi [<<Phi1>>,<<Conv>>] loop:<<Loop>>
+  /// CHECK-DAG:                Return [<<Phi1>>]
+
+  /// CHECK-START: int TypeConversions.$noinline$conditionalStoreLoadConversionInt8InLoop(int, boolean) load_store_elimination (after)
+  /// CHECK-NOT: StaticFieldGet
+  public static int $noinline$conditionalStoreLoadConversionInt8InLoop(int value, boolean cond) {
+      static_int = value;
+      for (int q = 1; q < 12; q++) {
+          if (cond) {
+              static_byte = (byte) static_int;
+              static_int = static_byte;
+          }
+      }
+      return static_int;
+  }
+
+  /// CHECK-START: int TypeConversions.$noinline$conditionalStoreLoadConversionUint8InLoop(int, boolean) load_store_elimination (before)
+  /// CHECK-DAG: <<Value:i\d+>> ParameterValue
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<Value>>] field_name:TypeConversions.static_int
+  /// CHECK-DAG: <<GetI1:i\d+>> StaticFieldGet field_name:TypeConversions.static_int field_type:Int32 loop:<<Loop:B\d+>>
+  // The type conversion has already been eliminated.
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<GetI1>>] field_name:TypeConversions.static_byte loop:<<Loop>>
+  /// CHECK-DAG: <<GetA:a\d+>>  StaticFieldGet field_name:TypeConversions.static_byte loop:<<Loop>>
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<GetA>>] field_name:TypeConversions.static_int loop:<<Loop>>
+  /// CHECK-DAG: <<GetI2:i\d+>> StaticFieldGet field_name:TypeConversions.static_int field_type:Int32 loop:none
+  /// CHECK-DAG:                Return [<<GetI2>>]
+
+  /// CHECK-START: int TypeConversions.$noinline$conditionalStoreLoadConversionUint8InLoop(int, boolean) load_store_elimination (after)
+  /// CHECK-DAG: <<Value:i\d+>> ParameterValue
+  /// CHECK-DAG: <<Phi1:i\d+>>  Phi [<<Value>>,<<Phi2:i\d+>>] loop:<<Loop:B\d+>>
+  /// CHECK-DAG: <<Conv:a\d+>>  TypeConversion [<<Phi1>>] loop:<<Loop>>
+  /// CHECK-DAG: <<Phi2>>       Phi [<<Phi1>>,<<Conv>>] loop:<<Loop>>
+  /// CHECK-DAG:                Return [<<Phi1>>]
+
+  /// CHECK-START: int TypeConversions.$noinline$conditionalStoreLoadConversionUint8InLoop(int, boolean) load_store_elimination (after)
+  /// CHECK-NOT: StaticFieldGet
+  public static int $noinline$conditionalStoreLoadConversionUint8InLoop(int value, boolean cond) {
+      static_int = value;
+      for (int q = 1; q < 12; q++) {
+          if (cond) {
+              static_byte = (byte) static_int;
+              static_int = static_byte & 0xff;
+          }
+      }
+      return static_int;
+  }
+
+  /// CHECK-START: int TypeConversions.$noinline$twoConditionalStoreLoadConversionsInLoop(int, boolean) load_store_elimination (before)
+  /// CHECK-DAG: <<Value:i\d+>> ParameterValue
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<Value>>] field_name:TypeConversions.static_int
+  /// CHECK-DAG: <<GetI1:i\d+>> StaticFieldGet field_name:TypeConversions.static_int field_type:Int32 loop:<<Loop:B\d+>>
+  // The type conversion has already been eliminated.
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<GetI1>>] field_name:TypeConversions.static_byte loop:<<Loop>>
+  /// CHECK-DAG: <<GetB:b\d+>>  StaticFieldGet field_name:TypeConversions.static_byte loop:<<Loop>>
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<GetB>>] field_name:TypeConversions.static_int loop:<<Loop>>
+  /// CHECK-DAG: <<GetI2:i\d+>> StaticFieldGet field_name:TypeConversions.static_int field_type:Int32 loop:<<Loop>>
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<GetI2>>] field_name:TypeConversions.static_char loop:<<Loop>>
+  /// CHECK-DAG: <<GetC:c\d+>>  StaticFieldGet field_name:TypeConversions.static_char loop:<<Loop>>
+  /// CHECK-DAG:                StaticFieldSet [{{l\d+}},<<GetC>>] field_name:TypeConversions.static_int loop:<<Loop>>
+  /// CHECK-DAG: <<GetI3:i\d+>> StaticFieldGet field_name:TypeConversions.static_int field_type:Int32 loop:none
+  /// CHECK-DAG:                Return [<<GetI3>>]
+
+  /// CHECK-START: int TypeConversions.$noinline$twoConditionalStoreLoadConversionsInLoop(int, boolean) load_store_elimination (after)
+  /// CHECK-DAG: <<Value:i\d+>> ParameterValue
+  /// CHECK-DAG: <<Phi1:i\d+>>  Phi [<<Value>>,<<Phi2:i\d+>>] loop:<<Loop:B\d+>>
+  /// CHECK-DAG: <<Conv1:b\d+>> TypeConversion [<<Phi1>>] loop:<<Loop>>
+  /// CHECK-DAG: <<Conv2:c\d+>> TypeConversion [<<Conv1>>] loop:<<Loop>>
+  /// CHECK-DAG: <<Phi2>>       Phi [<<Phi1>>,<<Conv2>>] loop:<<Loop>>
+  /// CHECK-DAG:                Return [<<Phi1>>]
+
+  /// CHECK-START: int TypeConversions.$noinline$twoConditionalStoreLoadConversionsInLoop(int, boolean) load_store_elimination (after)
+  /// CHECK-NOT: StaticFieldGet
+  public static int $noinline$twoConditionalStoreLoadConversionsInLoop(int value, boolean cond) {
+      static_int = value;
+      for (int q = 1; q < 12; q++) {
+          if (cond) {
+              static_byte = (byte) static_int;
+              // Note: We need to go through `static_int` so that the instruction
+              // simplifier eliminates the type conversion to `char`.
+              // TODO: Improve the instruction simplifier to eliminate the conversion
+              // for `static_char = (char) static_byte`.
+              static_int = static_byte;
+              static_char = (char) static_int;
+              static_int = static_char;
+          }
+      }
+      return static_int;
+  }
+
+  public static void main() {
+    assertIntEquals(42, $noinline$loopPhiStoreLoadConversionInt8(42));
+    assertIntEquals(-42, $noinline$loopPhiStoreLoadConversionInt8(-42));
+    assertIntEquals(-128, $noinline$loopPhiStoreLoadConversionInt8(128));
+    assertIntEquals(127, $noinline$loopPhiStoreLoadConversionInt8(-129));
+
+    assertIntEquals(42, $noinline$loopPhiStoreLoadConversionUint8(42));
+    assertIntEquals(214, $noinline$loopPhiStoreLoadConversionUint8(-42));
+    assertIntEquals(128, $noinline$loopPhiStoreLoadConversionUint8(128));
+    assertIntEquals(127, $noinline$loopPhiStoreLoadConversionUint8(-129));
+
+    assertIntEquals(42, $noinline$loopPhiTwoStoreLoadConversions(42));
+    assertIntEquals(65494, $noinline$loopPhiTwoStoreLoadConversions(-42));
+    assertIntEquals(65408, $noinline$loopPhiTwoStoreLoadConversions(128));
+    assertIntEquals(127, $noinline$loopPhiTwoStoreLoadConversions(-129));
+    assertIntEquals(0, $noinline$loopPhiTwoStoreLoadConversions(256));
+    assertIntEquals(65535, $noinline$loopPhiTwoStoreLoadConversions(-257));
+
+    assertIntEquals(42, $noinline$conditionalStoreLoadConversionInt8InLoop(42, false));
+    assertIntEquals(42, $noinline$conditionalStoreLoadConversionInt8InLoop(42, true));
+    assertIntEquals(-42, $noinline$conditionalStoreLoadConversionInt8InLoop(-42, false));
+    assertIntEquals(-42, $noinline$conditionalStoreLoadConversionInt8InLoop(-42, true));
+    assertIntEquals(128, $noinline$conditionalStoreLoadConversionInt8InLoop(128, false));
+    assertIntEquals(-128, $noinline$conditionalStoreLoadConversionInt8InLoop(128, true));
+    assertIntEquals(-129, $noinline$conditionalStoreLoadConversionInt8InLoop(-129, false));
+    assertIntEquals(127, $noinline$conditionalStoreLoadConversionInt8InLoop(-129, true));
+
+    assertIntEquals(42, $noinline$conditionalStoreLoadConversionUint8InLoop(42, false));
+    assertIntEquals(42, $noinline$conditionalStoreLoadConversionUint8InLoop(42, true));
+    assertIntEquals(-42, $noinline$conditionalStoreLoadConversionUint8InLoop(-42, false));
+    assertIntEquals(214, $noinline$conditionalStoreLoadConversionUint8InLoop(-42, true));
+    assertIntEquals(128, $noinline$conditionalStoreLoadConversionUint8InLoop(128, false));
+    assertIntEquals(128, $noinline$conditionalStoreLoadConversionUint8InLoop(128, true));
+    assertIntEquals(-129, $noinline$conditionalStoreLoadConversionUint8InLoop(-129, false));
+    assertIntEquals(127, $noinline$conditionalStoreLoadConversionUint8InLoop(-129, true));
+
+    assertIntEquals(42, $noinline$twoConditionalStoreLoadConversionsInLoop(42, false));
+    assertIntEquals(42, $noinline$twoConditionalStoreLoadConversionsInLoop(42, true));
+    assertIntEquals(-42, $noinline$twoConditionalStoreLoadConversionsInLoop(-42, false));
+    assertIntEquals(65494, $noinline$twoConditionalStoreLoadConversionsInLoop(-42, true));
+    assertIntEquals(128, $noinline$twoConditionalStoreLoadConversionsInLoop(128, false));
+    assertIntEquals(65408, $noinline$twoConditionalStoreLoadConversionsInLoop(128, true));
+    assertIntEquals(-129, $noinline$twoConditionalStoreLoadConversionsInLoop(-129, false));
+    assertIntEquals(127, $noinline$twoConditionalStoreLoadConversionsInLoop(-129, true));
+    assertIntEquals(256, $noinline$twoConditionalStoreLoadConversionsInLoop(256, false));
+    assertIntEquals(0, $noinline$twoConditionalStoreLoadConversionsInLoop(256, true));
+    assertIntEquals(-257, $noinline$twoConditionalStoreLoadConversionsInLoop(-257, false));
+    assertIntEquals(65535, $noinline$twoConditionalStoreLoadConversionsInLoop(-257, true));
+  }
+}
diff --git a/test/552-checker-sharpening/run.py b/test/552-checker-sharpening/run.py
index 1a893fbb1c..4c1c0aeaea 100644
--- a/test/552-checker-sharpening/run.py
+++ b/test/552-checker-sharpening/run.py
@@ -16,5 +16,9 @@
 
 
 def run(ctx, args):
-  # Use a profile to put specific classes in the app image.
-  ctx.default_run(args, profile=True)
+  # Use a profile to put specific classes in the app image. Also run tests with different
+  # dex2oat options to cover cases with varying .rodata offsets.
+  # Since a build ID section appears before .rodata in an oat file, .rodata offset depends on
+  # presence of build id section in the file.
+  ctx.default_run(args, profile=True, compiler_only_option=["--generate-build-id"])
+  ctx.default_run(args, profile=True, compiler_only_option=["--no-generate-build-id"])
diff --git a/test/567-checker-builder-intrinsics/src/TestCompare.java b/test/567-checker-builder-intrinsics/src/TestCompare.java
index 1feb249d5d..eb51cfe1c2 100644
--- a/test/567-checker-builder-intrinsics/src/TestCompare.java
+++ b/test/567-checker-builder-intrinsics/src/TestCompare.java
@@ -37,7 +37,7 @@ public class TestCompare {
     }
   }
 
-  /// CHECK-START: int TestCompare.compareBooleans(boolean, boolean) select_generator (after)
+  /// CHECK-START: int TestCompare.compareBooleans(boolean, boolean) control_flow_simplifier (after)
   /// CHECK-NOT:                     Phi
 
   /// CHECK-START: int TestCompare.compareBooleans(boolean, boolean) instruction_simplifier$before_codegen (after)
@@ -64,7 +64,7 @@ public class TestCompare {
   ///  CHECK-START: int TestCompare.compareBooleans2(boolean, boolean) builder (after)
   ///  CHECK-NOT:                     InvokeStaticOrDirect
 
-  ///  CHECK-START: int TestCompare.compareBooleans2(boolean, boolean) select_generator (after)
+  ///  CHECK-START: int TestCompare.compareBooleans2(boolean, boolean) control_flow_simplifier (after)
   ///  CHECK:         <<ArgX:z\d+>>   ParameterValue
   ///  CHECK:         <<ArgY:z\d+>>   ParameterValue
   ///  CHECK-DAG:     <<Zero:i\d+>>   IntConstant 0
@@ -74,7 +74,7 @@ public class TestCompare {
   ///  CHECK-DAG:     <<Result:i\d+>> Compare [<<SelX>>,<<SelY>>]
   ///  CHECK-DAG:                     Return [<<Result>>]
 
-  ///  CHECK-START: int TestCompare.compareBooleans2(boolean, boolean) select_generator (after)
+  ///  CHECK-START: int TestCompare.compareBooleans2(boolean, boolean) control_flow_simplifier (after)
   ///  CHECK-NOT:                     Phi
 
   ///  CHECK-START: int TestCompare.compareBooleans2(boolean, boolean) instruction_simplifier$before_codegen (after)
diff --git a/test/567-checker-builder-intrinsics/src/TestMinMax.java b/test/567-checker-builder-intrinsics/src/TestMinMax.java
index 7207006784..af7c343d55 100644
--- a/test/567-checker-builder-intrinsics/src/TestMinMax.java
+++ b/test/567-checker-builder-intrinsics/src/TestMinMax.java
@@ -564,7 +564,7 @@ public class TestMinMax {
     return x;
   }
 
-  /// CHECK-START: int TestMinMax.minmax3(int) select_generator (after)
+  /// CHECK-START: int TestMinMax.minmax3(int) control_flow_simplifier (after)
   /// CHECK-DAG: <<Par:i\d+>>  ParameterValue
   /// CHECK-DAG: <<P100:i\d+>> IntConstant 100
   /// CHECK-DAG: <<M100:i\d+>> IntConstant -100
@@ -588,7 +588,7 @@ public class TestMinMax {
     return (x > 100) ? 100 : ((x < -100) ? -100 : x);
   }
 
-  /// CHECK-START: int TestMinMax.minmax4(int) select_generator (after)
+  /// CHECK-START: int TestMinMax.minmax4(int) control_flow_simplifier (after)
   /// CHECK-DAG: <<Par:i\d+>>  ParameterValue
   /// CHECK-DAG: <<P100:i\d+>> IntConstant 100
   /// CHECK-DAG: <<M100:i\d+>> IntConstant -100
@@ -612,7 +612,7 @@ public class TestMinMax {
     return (x < -100) ? -100 : ((x > 100) ? 100 : x);
   }
 
-  /// CHECK-START: int TestMinMax.minmaxCSEScalar(int, int) select_generator (after)
+  /// CHECK-START: int TestMinMax.minmaxCSEScalar(int, int) control_flow_simplifier (after)
   /// CHECK-DAG: <<Par1:i\d+>> ParameterValue
   /// CHECK-DAG: <<Par2:i\d+>> ParameterValue
   /// CHECK-DAG: <<Cnd1:z\d+>> LessThanOrEqual    [<<Par1>>,<<Par2>>]
@@ -648,7 +648,7 @@ public class TestMinMax {
     return t1 + t2 + t3 + t4 + t5 + t6;
   }
 
-  /// CHECK-START: int TestMinMax.minmaxCSEArray(int[], int[]) select_generator (after)
+  /// CHECK-START: int TestMinMax.minmaxCSEArray(int[], int[]) control_flow_simplifier (after)
   /// CHECK-DAG: <<Arr1:i\d+>> ArrayGet
   /// CHECK-DAG: <<Arr2:i\d+>> ArrayGet
   /// CHECK-DAG: <<Cnd1:z\d+>> LessThanOrEqual    [<<Arr1>>,<<Arr2>>]
diff --git a/test/567-checker-builder-intrinsics/src/TestRotate.java b/test/567-checker-builder-intrinsics/src/TestRotate.java
index 40abb1b1d1..322c55ac56 100644
--- a/test/567-checker-builder-intrinsics/src/TestRotate.java
+++ b/test/567-checker-builder-intrinsics/src/TestRotate.java
@@ -281,7 +281,7 @@ public class TestRotate {
   /// CHECK-START: int TestRotate.$inline$rotateLeftBoolean(boolean, int) builder (after)
   /// CHECK-NOT:                      InvokeStaticOrDirect
 
-  /// CHECK-START: int TestRotate.$inline$rotateLeftBoolean(boolean, int) select_generator (after)
+  /// CHECK-START: int TestRotate.$inline$rotateLeftBoolean(boolean, int) control_flow_simplifier (after)
   /// CHECK:         <<ArgVal:z\d+>>  ParameterValue
   /// CHECK:         <<ArgDist:i\d+>> ParameterValue
   /// CHECK-DAG:     <<Zero:i\d+>>    IntConstant 0
@@ -290,7 +290,7 @@ public class TestRotate {
   /// CHECK-DAG:     <<Result:i\d+>>  Rol [<<SelVal>>,<<ArgDist>>]
   /// CHECK-DAG:                      Return [<<Result>>]
 
-  /// CHECK-START: int TestRotate.$inline$rotateLeftBoolean(boolean, int) select_generator (after)
+  /// CHECK-START: int TestRotate.$inline$rotateLeftBoolean(boolean, int) control_flow_simplifier (after)
   /// CHECK-NOT:                      Phi
 
   /// CHECK-START: int TestRotate.$inline$rotateLeftBoolean(boolean, int) instruction_simplifier$before_codegen (after)
@@ -522,7 +522,7 @@ public class TestRotate {
   /// CHECK-START: int TestRotate.rotateRightBoolean(boolean, int) builder (after)
   /// CHECK-NOT:                      InvokeStaticOrDirect
 
-  /// CHECK-START: int TestRotate.rotateRightBoolean(boolean, int) select_generator (after)
+  /// CHECK-START: int TestRotate.rotateRightBoolean(boolean, int) control_flow_simplifier (after)
   /// CHECK:         <<ArgVal:z\d+>>  ParameterValue
   /// CHECK:         <<ArgDist:i\d+>> ParameterValue
   /// CHECK-DAG:     <<Zero:i\d+>>    IntConstant 0
@@ -531,7 +531,7 @@ public class TestRotate {
   /// CHECK-DAG:     <<Result:i\d+>>  Ror [<<SelVal>>,<<ArgDist>>]
   /// CHECK-DAG:                      Return [<<Result>>]
 
-  /// CHECK-START: int TestRotate.rotateRightBoolean(boolean, int) select_generator (after)
+  /// CHECK-START: int TestRotate.rotateRightBoolean(boolean, int) control_flow_simplifier (after)
   /// CHECK-NOT:                     Phi
 
   /// CHECK-START: int TestRotate.rotateRightBoolean(boolean, int) instruction_simplifier$before_codegen (after)
diff --git a/test/567-checker-builder-intrinsics/src/TestSignum.java b/test/567-checker-builder-intrinsics/src/TestSignum.java
index 818c940dac..bc146beb9d 100644
--- a/test/567-checker-builder-intrinsics/src/TestSignum.java
+++ b/test/567-checker-builder-intrinsics/src/TestSignum.java
@@ -81,7 +81,7 @@ public class TestSignum {
   /// CHECK-START: int TestSignum.signBoolean(boolean) builder (after)
   /// CHECK-NOT:                     InvokeStaticOrDirect
 
-  /// CHECK-START: int TestSignum.signBoolean(boolean) select_generator (after)
+  /// CHECK-START: int TestSignum.signBoolean(boolean) control_flow_simplifier (after)
   /// CHECK-DAG:     <<Arg:z\d+>>    ParameterValue
   /// CHECK-DAG:     <<Zero:i\d+>>   IntConstant 0
   /// CHECK-DAG:     <<One:i\d+>>    IntConstant 1
@@ -89,7 +89,7 @@ public class TestSignum {
   /// CHECK-DAG:     <<Result:i\d+>> Compare [<<Sel>>,<<Zero>>]
   /// CHECK-DAG:                     Return [<<Result>>]
 
-  /// CHECK-START: int TestSignum.signBoolean(boolean) select_generator (after)
+  /// CHECK-START: int TestSignum.signBoolean(boolean) control_flow_simplifier (after)
   /// CHECK-NOT:                     Phi
 
   /// CHECK-START: int TestSignum.signBoolean(boolean) instruction_simplifier$after_gvn (after)
diff --git a/test/592-checker-regression-bool-input/smali/TestCase.smali b/test/592-checker-regression-bool-input/smali/TestCase.smali
index ad4e902724..8e9ad809c0 100644
--- a/test/592-checker-regression-bool-input/smali/TestCase.smali
+++ b/test/592-checker-regression-bool-input/smali/TestCase.smali
@@ -16,7 +16,7 @@
 
 .super Ljava/lang/Object;
 
-## CHECK-START: boolean TestCase.testCase() select_generator (after)
+## CHECK-START: boolean TestCase.testCase() control_flow_simplifier (after)
 ## CHECK-DAG:     <<Select:i\d+>>          Select
 ## CHECK-DAG:                              Return [<<Select>>]
 
diff --git a/test/593-checker-boolean-2-integral-conv/smali/SmaliTests.smali b/test/593-checker-boolean-2-integral-conv/smali/SmaliTests.smali
index 8e84d054a0..1673fccdd8 100644
--- a/test/593-checker-boolean-2-integral-conv/smali/SmaliTests.smali
+++ b/test/593-checker-boolean-2-integral-conv/smali/SmaliTests.smali
@@ -40,7 +40,7 @@
 ##  CHECK-DAG:     <<IToS:b\d+>>          TypeConversion [<<Phi>>]
 ##  CHECK-DAG:                            Return [<<IToS>>]
 
-##  CHECK-START: byte SmaliTests.booleanToByte(boolean) select_generator (after)
+##  CHECK-START: byte SmaliTests.booleanToByte(boolean) control_flow_simplifier (after)
 ##  CHECK:         <<Arg:z\d+>>           ParameterValue
 ##  CHECK-DAG:     <<Zero:i\d+>>          IntConstant 0
 ##  CHECK-DAG:     <<One:i\d+>>           IntConstant 1
@@ -75,7 +75,7 @@
 ##  CHECK-DAG:     <<IToS:s\d+>>          TypeConversion [<<Phi>>]
 ##  CHECK-DAG:                            Return [<<IToS>>]
 
-##  CHECK-START: short SmaliTests.booleanToShort(boolean) select_generator (after)
+##  CHECK-START: short SmaliTests.booleanToShort(boolean) control_flow_simplifier (after)
 ##  CHECK:         <<Arg:z\d+>>           ParameterValue
 ##  CHECK-DAG:     <<Zero:i\d+>>          IntConstant 0
 ##  CHECK-DAG:     <<One:i\d+>>           IntConstant 1
@@ -110,7 +110,7 @@
 ##  CHECK-DAG:     <<IToC:c\d+>>          TypeConversion [<<Phi>>]
 ##  CHECK-DAG:                            Return [<<IToC>>]
 
-##  CHECK-START: char SmaliTests.booleanToChar(boolean) select_generator (after)
+##  CHECK-START: char SmaliTests.booleanToChar(boolean) control_flow_simplifier (after)
 ##  CHECK:         <<Arg:z\d+>>           ParameterValue
 ##  CHECK-DAG:     <<Zero:i\d+>>          IntConstant 0
 ##  CHECK-DAG:     <<One:i\d+>>           IntConstant 1
@@ -144,7 +144,7 @@
 ##  CHECK-DAG:     <<Phi:i\d+>>           Phi [<<One>>,<<Zero>>]
 ##  CHECK-DAG:                            Return [<<Phi>>]
 
-##  CHECK-START: int SmaliTests.booleanToInt(boolean) select_generator (after)
+##  CHECK-START: int SmaliTests.booleanToInt(boolean) control_flow_simplifier (after)
 ##  CHECK:         <<Arg:z\d+>>           ParameterValue
 ##  CHECK-DAG:     <<Zero:i\d+>>          IntConstant 0
 ##  CHECK-DAG:     <<One:i\d+>>           IntConstant 1
@@ -177,7 +177,7 @@
 ## CHECK-DAG:     <<IToJ:j\d+>>          TypeConversion [<<Phi>>]
 ## CHECK-DAG:                            Return [<<IToJ>>]
 
-## CHECK-START: long SmaliTests.booleanToLong(boolean) select_generator (after)
+## CHECK-START: long SmaliTests.booleanToLong(boolean) control_flow_simplifier (after)
 ## CHECK-DAG:     <<Arg:z\d+>>           ParameterValue
 ## CHECK-DAG:     <<Zero:i\d+>>          IntConstant 0
 ## CHECK-DAG:     <<One:i\d+>>           IntConstant 1
@@ -227,7 +227,7 @@
 ## CHECK-DAG:     <<JToI:i\d+>>          TypeConversion [<<IToJ>>]
 ## CHECK-DAG:                            Return [<<JToI>>]
 
-## CHECK-START: int SmaliTests.longToIntOfBoolean() select_generator (after)
+## CHECK-START: int SmaliTests.longToIntOfBoolean() control_flow_simplifier (after)
 ## CHECK-DAG:     <<Zero:i\d+>>          IntConstant 0
 ## CHECK-DAG:     <<One:i\d+>>           IntConstant 1
 ## CHECK-DAG:     <<Sget:z\d+>>          StaticFieldGet
diff --git a/test/593-checker-boolean-2-integral-conv/src/Main.java b/test/593-checker-boolean-2-integral-conv/src/Main.java
index 545f16d27e..0911db580c 100644
--- a/test/593-checker-boolean-2-integral-conv/src/Main.java
+++ b/test/593-checker-boolean-2-integral-conv/src/Main.java
@@ -74,13 +74,13 @@ public class Main {
   /// CHECK-DAG:     <<Phi:j\d+>>           Phi [<<One>>,<<Zero>>]
   /// CHECK-DAG:                            Return [<<Phi>>]
 
-  /// CHECK-START: long Main.booleanToLong(boolean) select_generator (after)
+  /// CHECK-START: long Main.booleanToLong(boolean) control_flow_simplifier (after)
   /// CHECK-NOT:                            IntConstant
   /// CHECK-NOT:                            Equal
   /// CHECK-NOT:                            If
   /// CHECK-NOT:                            Phi
 
-  /// CHECK-START: long Main.booleanToLong(boolean) select_generator (after)
+  /// CHECK-START: long Main.booleanToLong(boolean) control_flow_simplifier (after)
   /// CHECK:         <<Arg:z\d+>>           ParameterValue
   /// CHECK-DAG:     <<Zero:j\d+>>          LongConstant 0
   /// CHECK-DAG:     <<One:j\d+>>           LongConstant 1
@@ -114,13 +114,13 @@ public class Main {
   /// CHECK-DAG:     <<JToI:i\d+>>          TypeConversion [<<Phi>>]
   /// CHECK-DAG:                            Return [<<JToI>>]
 
-  /// CHECK-START: long Main.booleanToLong(boolean) select_generator (after)
+  /// CHECK-START: long Main.booleanToLong(boolean) control_flow_simplifier (after)
   /// CHECK-NOT:                            IntConstant
   /// CHECK-NOT:                            Equal
   /// CHECK-NOT:                            If
   /// CHECK-NOT:                            Phi
 
-  /// CHECK-START: int Main.longToIntOfBoolean() select_generator (after)
+  /// CHECK-START: int Main.longToIntOfBoolean() control_flow_simplifier (after)
   /// CHECK-DAG:     <<Zero:j\d+>>          LongConstant 0
   /// CHECK-DAG:     <<One:j\d+>>           LongConstant 1
   /// CHECK-DAG:     <<Sget:z\d+>>          StaticFieldGet
diff --git a/test/597-deopt-new-string/deopt.cc b/test/597-deopt-new-string/deopt.cc
index b8828157e0..d14257f307 100644
--- a/test/597-deopt-new-string/deopt.cc
+++ b/test/597-deopt-new-string/deopt.cc
@@ -18,6 +18,7 @@
 
 #include "gc/gc_cause.h"
 #include "gc/scoped_gc_critical_section.h"
+#include "instrumentation.h"
 #include "mirror/class-inl.h"
 #include "runtime.h"
 #include "scoped_thread_state_change-inl.h"
diff --git a/test/656-annotation-lookup-generic-jni/expected-stdout.txt b/test/656-annotation-lookup-generic-jni/expected-stdout.txt
index 4519c7e442..1ba8aa29bc 100644
--- a/test/656-annotation-lookup-generic-jni/expected-stdout.txt
+++ b/test/656-annotation-lookup-generic-jni/expected-stdout.txt
@@ -1,3 +1,3 @@
-JNI_OnLoad called
+JNI_OnLoad in libarttest_external.cc called
 Java_Test_nativeMethodWithAnnotation
 passed
diff --git a/test/656-annotation-lookup-generic-jni/run.py b/test/656-annotation-lookup-generic-jni/run.py
index 0ebb768cc9..d555938083 100644
--- a/test/656-annotation-lookup-generic-jni/run.py
+++ b/test/656-annotation-lookup-generic-jni/run.py
@@ -16,6 +16,7 @@
 
 
 def run(ctx, args):
+  args.testlib += [args.testlib[0] + "_external"]
   ctx.default_run(args)
 
   # On gcstress configurations, an extra "JNI_OnUnload called" line may
diff --git a/test/656-annotation-lookup-generic-jni/src-art/Main.java b/test/656-annotation-lookup-generic-jni/src-art/Main.java
index ed375064f1..ed1aab97e1 100644
--- a/test/656-annotation-lookup-generic-jni/src-art/Main.java
+++ b/test/656-annotation-lookup-generic-jni/src-art/Main.java
@@ -42,7 +42,7 @@ public class Main {
     // Load and initialize the Test class.
     Class<?> testClass = classLoader.loadClass("Test");
     Method initialize = testClass.getMethod("initialize", String.class);
-    initialize.invoke(null, args[0]);
+    initialize.invoke(null, args[1]);
 
     // Invoke Test.nativeMethodWithAnnotation().
     Method nativeMethodWithAnnotation = testClass.getMethod("nativeMethodWithAnnotation");
diff --git a/test/656-annotation-lookup-generic-jni/test.cc b/test/656-annotation-lookup-generic-jni/test.cc
index 172e04669f..26d9f9678e 100644
--- a/test/656-annotation-lookup-generic-jni/test.cc
+++ b/test/656-annotation-lookup-generic-jni/test.cc
@@ -20,6 +20,11 @@
 
 namespace art {
 
+// The JNI entrypoint below ends up in libarttest(d).so, while the test loads
+// libarttest(d)_external.so instead. That lib depends on libarttest(d).so, so
+// its exported symbols become visible directly in it. Hence we don't need to
+// create a wrapper for the JNI method in libarttest(d)_external.so.
+
 // Native method annotated with `SampleAnnotation` in Java source.
 extern "C" JNIEXPORT void JNICALL Java_Test_nativeMethodWithAnnotation(JNIEnv*, jclass) {
   std::cout << "Java_Test_nativeMethodWithAnnotation" << std::endl;
diff --git a/test/657-branches/src/Main.java b/test/657-branches/src/Main.java
index 2b62c5faa1..138e507139 100644
--- a/test/657-branches/src/Main.java
+++ b/test/657-branches/src/Main.java
@@ -19,11 +19,11 @@ public class Main {
   public static void foo(float f) {
     // The reason this used to break:
     // 1) We inline the 'foo' call, so blocks now only contain HLoadClass instructions.
-    // 2) We then run the select_generator pass, which cannot change the
+    // 2) We then run the control_flow_simplifier pass, which cannot change the
     //    if/else because blocks contain instructions.
     // 3) We run GVN which will remove the HLoadClass instructions in the blocks.
     // 4) At code generation, we are in the unlikely situation that a diamond shape
-    //    contains no instruction (usually removed by select_generator). This used
+    //    contains no instruction (usually removed by control_flow_simplifier). This used
     //    to trip the ARM code generators.
     if (f < 1.2f) {
       foo(Main.class, Object.class);
diff --git a/test/663-checker-select-generator/src/Main.java b/test/663-checker-select-generator/src/Main.java
index 1a185fbc62..baf57e6b22 100644
--- a/test/663-checker-select-generator/src/Main.java
+++ b/test/663-checker-select-generator/src/Main.java
@@ -21,13 +21,13 @@ public class Main {
   /// CHECK-START: int Main.$noinline$testSimpleDiamondSameValue(boolean) builder (after)
   /// CHECK-NOT: Phi
 
-  /// CHECK-START: int Main.$noinline$testSimpleDiamondSameValue(boolean) select_generator (before)
+  /// CHECK-START: int Main.$noinline$testSimpleDiamondSameValue(boolean) control_flow_simplifier (before)
   /// CHECK-NOT: Phi
 
-  /// CHECK-START: int Main.$noinline$testSimpleDiamondSameValue(boolean) select_generator (after)
+  /// CHECK-START: int Main.$noinline$testSimpleDiamondSameValue(boolean) control_flow_simplifier (after)
   /// CHECK-NOT: Phi
 
-  /// CHECK-START: int Main.$noinline$testSimpleDiamondSameValue(boolean) select_generator (after)
+  /// CHECK-START: int Main.$noinline$testSimpleDiamondSameValue(boolean) control_flow_simplifier (after)
   /// CHECK-NOT: Select
   private static int $noinline$testSimpleDiamondSameValue(boolean bool_param) {
     int return_value;
@@ -41,14 +41,14 @@ public class Main {
 
   // Check that we generate a select for a simple diamond pattern, with different values.
 
-  /// CHECK-START: int Main.$noinline$testSimpleDiamondDifferentValue(boolean) select_generator (before)
+  /// CHECK-START: int Main.$noinline$testSimpleDiamondDifferentValue(boolean) control_flow_simplifier (before)
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
   /// CHECK-DAG:   <<Const20:i\d+>> IntConstant 20
   /// CHECK-DAG:   <<Phi:i\d+>>     Phi [<<Arg1:i\d+>>,<<Arg2:i\d+>>]
   /// CHECK-DAG:                    Return [<<Phi>>]
   /// CHECK-EVAL:  set(["<<Arg1>>","<<Arg2>>"]) == set(["<<Const10>>","<<Const20>>"])
 
-  /// CHECK-START: int Main.$noinline$testSimpleDiamondDifferentValue(boolean) select_generator (after)
+  /// CHECK-START: int Main.$noinline$testSimpleDiamondDifferentValue(boolean) control_flow_simplifier (after)
   /// CHECK-DAG:   <<Bool:z\d+>>    ParameterValue
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
   /// CHECK-DAG:   <<Const20:i\d+>> IntConstant 20
@@ -70,13 +70,13 @@ public class Main {
   /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValue(boolean, boolean) builder (after)
   /// CHECK-NOT: Phi
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValue(boolean, boolean) select_generator (before)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValue(boolean, boolean) control_flow_simplifier (before)
   /// CHECK-NOT: Phi
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValue(boolean, boolean) select_generator (after)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValue(boolean, boolean) control_flow_simplifier (after)
   /// CHECK-NOT: Phi
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValue(boolean, boolean) select_generator (after)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValue(boolean, boolean) control_flow_simplifier (after)
   /// CHECK-NOT: Select
   private static int $noinline$testDoubleDiamondSameValue(boolean bool_param_1, boolean bool_param_2) {
       int return_value;
@@ -94,21 +94,20 @@ public class Main {
 
   // Check that we generate a select for a double diamond pattern, with a different value in the outer branch.
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllOuter(boolean, boolean) select_generator (before)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllOuter(boolean, boolean) control_flow_simplifier (before)
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
   /// CHECK-DAG:   <<Const20:i\d+>> IntConstant 20
   /// CHECK-DAG:   <<Phi:i\d+>>     Phi [<<Arg1:i\d+>>,<<Arg2:i\d+>>,<<Arg3:i\d+>>]
   /// CHECK-DAG:                    Return [<<Phi>>]
   /// CHECK-EVAL:  set(["<<Arg1>>","<<Arg2>>","<<Arg3>>"]) == set(["<<Const10>>","<<Const20>>","<<Const20>>"])
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllOuter(boolean, boolean) select_generator (after)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllOuter(boolean, boolean) control_flow_simplifier (after)
   /// CHECK-DAG:   <<Bool1:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Bool2:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
   /// CHECK-DAG:   <<Const20:i\d+>> IntConstant 20
-  /// CHECK-DAG:   <<Select:i\d+>>  Select [<<Const20>>,<<Const20>>,<<Bool2>>]
-  /// CHECK-DAG:   <<Select2:i\d+>> Select [<<Select>>,<<Const10>>,<<Bool1>>]
-  /// CHECK-DAG:                    Return [<<Select2>>]
+  /// CHECK-DAG:   <<Select:i\d+>>  Select [<<Const20>>,<<Const10>>,<<Bool1>>]
+  /// CHECK-DAG:                    Return [<<Select>>]
   private static int $noinline$testDoubleDiamondSameValueButNotAllOuter(boolean bool_param_1, boolean bool_param_2) {
       int return_value;
     if (bool_param_1) {
@@ -125,14 +124,14 @@ public class Main {
 
   // Check that we generate a select for a double diamond pattern, with a different value in the inner branch.
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllInner(boolean, boolean) select_generator (before)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllInner(boolean, boolean) control_flow_simplifier (before)
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
   /// CHECK-DAG:   <<Const20:i\d+>> IntConstant 20
   /// CHECK-DAG:   <<Phi:i\d+>>     Phi [<<Arg1:i\d+>>,<<Arg2:i\d+>>,<<Arg3:i\d+>>]
   /// CHECK-DAG:                    Return [<<Phi>>]
   /// CHECK-EVAL:  set(["<<Arg1>>","<<Arg2>>","<<Arg3>>"]) == set(["<<Const10>>","<<Const20>>","<<Const20>>"])
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllInner(boolean, boolean) select_generator (after)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllInner(boolean, boolean) control_flow_simplifier (after)
   /// CHECK-DAG:   <<Bool1:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Bool2:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
@@ -156,7 +155,7 @@ public class Main {
 
   // Check that we generate a select for a double diamond pattern, with a all different values.
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondDifferentValue(boolean, boolean) select_generator (before)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondDifferentValue(boolean, boolean) control_flow_simplifier (before)
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
   /// CHECK-DAG:   <<Const20:i\d+>> IntConstant 20
   /// CHECK-DAG:   <<Const30:i\d+>> IntConstant 30
@@ -164,7 +163,7 @@ public class Main {
   /// CHECK-DAG:                    Return [<<Phi>>]
   /// CHECK-EVAL:  set(["<<Arg1>>","<<Arg2>>","<<Arg3>>"]) == set(["<<Const10>>","<<Const20>>","<<Const30>>"])
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondDifferentValue(boolean, boolean) select_generator (after)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondDifferentValue(boolean, boolean) control_flow_simplifier (after)
   /// CHECK-DAG:   <<Bool1:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Bool2:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
@@ -200,7 +199,7 @@ public class Main {
   /// CHECK:       Return [<<Const10>>]
   /// CHECK:       Return [<<Const10>>]
 
-  /// CHECK-START: int Main.$noinline$testSimpleDiamondSameValueWithReturn(boolean) select_generator (after)
+  /// CHECK-START: int Main.$noinline$testSimpleDiamondSameValueWithReturn(boolean) control_flow_simplifier (after)
   /// CHECK-DAG:   <<Bool:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
   /// CHECK-DAG:   <<Select:i\d+>>  Select [<<Const10>>,<<Const10>>,<<Bool>>]
@@ -222,13 +221,13 @@ public class Main {
 
   // Same as testSimpleDiamondDifferentValue, but branches return.
 
-  /// CHECK-START: int Main.$noinline$testSimpleDiamondDifferentValueWithReturn(boolean) select_generator (before)
+  /// CHECK-START: int Main.$noinline$testSimpleDiamondDifferentValueWithReturn(boolean) control_flow_simplifier (before)
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
   /// CHECK-DAG:   <<Const20:i\d+>> IntConstant 20
   /// CHECK-DAG:                    Return [<<Const10>>]
   /// CHECK-DAG:                    Return [<<Const20>>]
 
-  /// CHECK-START: int Main.$noinline$testSimpleDiamondDifferentValueWithReturn(boolean) select_generator (after)
+  /// CHECK-START: int Main.$noinline$testSimpleDiamondDifferentValueWithReturn(boolean) control_flow_simplifier (after)
   /// CHECK-DAG:   <<Bool:z\d+>>    ParameterValue
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
   /// CHECK-DAG:   <<Const20:i\d+>> IntConstant 20
@@ -249,7 +248,7 @@ public class Main {
   /// CHECK:       Return [<<Const10>>]
   /// CHECK:       Return [<<Const10>>]
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueWithReturn(boolean, boolean) select_generator (after)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueWithReturn(boolean, boolean) control_flow_simplifier (after)
   /// CHECK-DAG:   <<Bool1:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Bool2:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
@@ -278,7 +277,7 @@ public class Main {
 
   // Same as testDoubleDiamondSameValueButNotAllOuter, but branches return.
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllOuterWithReturn(boolean, boolean) select_generator (before)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllOuterWithReturn(boolean, boolean) control_flow_simplifier (before)
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
   /// CHECK-DAG:   <<Const20:i\d+>> IntConstant 20
   /// CHECK-DAG:                    Return [<<Const10>>]
@@ -286,12 +285,12 @@ public class Main {
   /// CHECK-DAG:                    Return [<<Const20>>]
 
   // Note that we have 3 returns as D8 only merges when the line positions are equal.
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllOuterWithReturn(boolean, boolean) select_generator (before)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllOuterWithReturn(boolean, boolean) control_flow_simplifier (before)
   /// CHECK:                    Return
   /// CHECK:                    Return
   /// CHECK:                    Return
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllOuterWithReturn(boolean, boolean) select_generator (after)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllOuterWithReturn(boolean, boolean) control_flow_simplifier (after)
   /// CHECK-DAG:   <<Bool1:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Bool2:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
@@ -313,14 +312,14 @@ public class Main {
 
   // Same as testDoubleDiamondSameValueButNotAllInner, but branches return.
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllInnerWithReturn(boolean, boolean) select_generator (before)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllInnerWithReturn(boolean, boolean) control_flow_simplifier (before)
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
   /// CHECK-DAG:   <<Const20:i\d+>> IntConstant 20
   /// CHECK-DAG:                    Return [<<Const10>>]
   /// CHECK-DAG:                    Return [<<Const20>>]
   /// CHECK-DAG:                    Return [<<Const20>>]
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllInnerWithReturn(boolean, boolean) select_generator (after)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondSameValueButNotAllInnerWithReturn(boolean, boolean) control_flow_simplifier (after)
   /// CHECK-DAG:   <<Bool1:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Bool2:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
@@ -342,7 +341,7 @@ public class Main {
 
   // Same as testDoubleDiamondDifferentValue, but branches return.
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondDifferentValueWithReturn(boolean, boolean) select_generator (before)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondDifferentValueWithReturn(boolean, boolean) control_flow_simplifier (before)
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
   /// CHECK-DAG:   <<Const20:i\d+>> IntConstant 20
   /// CHECK-DAG:   <<Const30:i\d+>> IntConstant 30
@@ -350,7 +349,7 @@ public class Main {
   /// CHECK-DAG:                    Return [<<Const20>>]
   /// CHECK-DAG:                    Return [<<Const30>>]
 
-  /// CHECK-START: int Main.$noinline$testDoubleDiamondDifferentValueWithReturn(boolean, boolean) select_generator (after)
+  /// CHECK-START: int Main.$noinline$testDoubleDiamondDifferentValueWithReturn(boolean, boolean) control_flow_simplifier (after)
   /// CHECK-DAG:   <<Bool1:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Bool2:z\d+>>   ParameterValue
   /// CHECK-DAG:   <<Const10:i\d+>> IntConstant 10
diff --git a/test/674-hiddenapi/build.py b/test/674-hiddenapi/build.py
index c3ebb50de5..24ad35b3c5 100644
--- a/test/674-hiddenapi/build.py
+++ b/test/674-hiddenapi/build.py
@@ -26,7 +26,7 @@ import os
 def build(ctx):
   if ctx.jvm:
     return  # The test does not build on JVM
-  ctx.default_build(use_hiddenapi=True)
+  ctx.default_build(use_hiddenapi=True, delete_srcs=False)
 
   # Move the jar file into the resource folder to be bundled with the test.
   os.mkdir(ctx.test_dir / "res")
diff --git a/test/674-hiddenapi/hiddenapi.cc b/test/674-hiddenapi/hiddenapi.cc
index f1b0c18c27..a851e3f425 100644
--- a/test/674-hiddenapi/hiddenapi.cc
+++ b/test/674-hiddenapi/hiddenapi.cc
@@ -14,6 +14,8 @@
  * limitations under the License.
  */
 
+#include <dlfcn.h>
+
 #include "base/sdk_version.h"
 #include "dex/art_dex_file_loader.h"
 #include "hidden_api.h"
@@ -21,6 +23,10 @@
 #include "runtime.h"
 #include "ti-agent/scoped_utf_chars.h"
 
+#ifdef ART_TARGET_ANDROID
+#include "nativeloader/dlext_namespaces.h"
+#endif
+
 namespace art {
 namespace Test674HiddenApi {
 
@@ -29,6 +35,30 @@ static constexpr uint64_t kPreventMetaReflectionBlocklistAccess = 142365358;
 
 std::vector<std::vector<std::unique_ptr<const DexFile>>> opened_dex_files;
 
+// The JNI entrypoints below end up in libarttest(d).so, while the test makes
+// copies of libarttest(d)_external.so and loads them instead. Those libs depend
+// on libarttest(d).so, so its exported symbols become visible directly in them.
+// Hence we don't need to create wrappers for the JNI methods in
+// libarttest(d)_external.so.
+
+extern "C" JNIEXPORT void JNICALL
+Java_Main_addDefaultNamespaceLibsLinkToSystemLinkerNamespace(JNIEnv*, jclass) {
+#ifdef ART_TARGET_ANDROID
+  const char* links = getenv("NATIVELOADER_DEFAULT_NAMESPACE_LIBS");
+  if (links == nullptr || *links == 0) {
+    LOG(FATAL) << "Expected NATIVELOADER_DEFAULT_NAMESPACE_LIBS to be set";
+  }
+  struct android_namespace_t* system_ns = android_get_exported_namespace("system");
+  if (system_ns == nullptr) {
+    LOG(FATAL) << "Failed to retrieve system namespace";
+  }
+  if (!android_link_namespaces(system_ns, nullptr, links)) {
+    LOG(FATAL) << "Error adding linker namespace link from system to default for " << links << ": "
+               << dlerror();
+  }
+#endif
+}
+
 extern "C" JNIEXPORT void JNICALL Java_Main_init(JNIEnv*, jclass) {
   Runtime* runtime = Runtime::Current();
   runtime->SetHiddenApiEnforcementPolicy(hiddenapi::EnforcementPolicy::kEnabled);
diff --git a/test/674-hiddenapi/run.py b/test/674-hiddenapi/run.py
index 1e364faf15..7967a46047 100644
--- a/test/674-hiddenapi/run.py
+++ b/test/674-hiddenapi/run.py
@@ -1,5 +1,3 @@
-#!/bin/bash
-#
 # Copyright (C) 2019 The Android Open Source Project
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -16,6 +14,8 @@
 
 
 def run(ctx, args):
+  args.testlib += [args.testlib[0] + "_external"]
+
   # Make verification soft fail so that we can re-verify boot classpath
   # methods at runtime.
   #
@@ -24,4 +24,6 @@ def run(ctx, args):
   ctx.default_run(args, verify_soft_fail=True, secondary_compilation=False)
 
   ctx.run(fr"sed -i -E '/(JNI_OnLoad|JNI_OnUnload)/d' '{args.stdout_file}'")
-  ctx.run(fr"sed -i -E '/^dalvikvm(32|64) E [^]]+]/d' '{args.stderr_file}'")
+
+  # Ignore hiddenapi's denial errors which go to stderr on host and qemu (but not on device).
+  ctx.run(fr"sed -i -E '/ E dalvikvm.* hiddenapi: /d' '{args.stderr_file}'")
diff --git a/test/674-hiddenapi/src-art/Main.java b/test/674-hiddenapi/src-art/Main.java
index 5a1b89b0eb..f215e592d2 100644
--- a/test/674-hiddenapi/src-art/Main.java
+++ b/test/674-hiddenapi/src-art/Main.java
@@ -15,6 +15,7 @@
  */
 
 import dalvik.system.PathClassLoader;
+
 import java.io.File;
 import java.lang.reflect.Constructor;
 import java.lang.reflect.Method;
@@ -31,7 +32,7 @@ public class Main {
 
   public static void main(String[] args) throws Exception {
     System.loadLibrary(args[0]);
-    prepareNativeLibFileName(args[0]);
+    prepareNativeLibFileName(args[1]);
 
     // Enable hidden API checks in case they are disabled by default.
     init();
@@ -75,6 +76,16 @@ public class Main {
     doTest(DexDomain.CorePlatform, DexDomain.Application, true);
     doUnloading();
 
+    // The following tests use the boot class loader to load ChildClass, and
+    // that class loader uses the "system" namespace in the native linker
+    // namespace config rather than the usual "clns-XXX" namespaces created for
+    // class loaders by libnativeloader. Hence we need to add links to the libs
+    // in NATIVELOADER_DEFAULT_NAMESPACE_LIBS (in particular libarttest(d).so)
+    // to the "system" namespace, so that the tests below can load the copy of
+    // libarttest(d)_external.so (which depends on libarttest(d).so). Note that
+    // this cannot be undone.
+    addDefaultNamespaceLibsLinkToSystemLinkerNamespace();
+
     // Append child to boot class path, first as a platform dex file.
     // It should not be allowed to access non-public, non-core platform API members.
     int childIdx = appendToBootClassLoader(DEX_CHILD, /* isCorePlatform */ false);
@@ -127,7 +138,7 @@ public class Main {
         addAllApisToSdk);
   }
 
-  // Routine which tries to figure out the absolute path of our native library.
+  // Routine which tries to figure out the absolute path of our native libarttest(d)_external.so.
   private static void prepareNativeLibFileName(String arg) throws Exception {
     String libName = System.mapLibraryName(arg);
     Method libPathsMethod = Runtime.class.getDeclaredMethod("getLibPaths");
@@ -179,6 +190,7 @@ public class Main {
 
   private static ClassLoader BOOT_CLASS_LOADER = Object.class.getClassLoader();
 
+  private static native void addDefaultNamespaceLibsLinkToSystemLinkerNamespace();
   private static native int appendToBootClassLoader(String dexPath, boolean isCorePlatform);
   private static native void setDexDomain(int index, boolean isCorePlatform);
   private static native void init();
diff --git a/test/674-hiddenapi/src-ex/ChildClass.java b/test/674-hiddenapi/src-ex/ChildClass.java
index 128d7017a8..5ffc9be271 100644
--- a/test/674-hiddenapi/src-ex/ChildClass.java
+++ b/test/674-hiddenapi/src-ex/ChildClass.java
@@ -121,8 +121,10 @@ public class ChildClass {
         expected = Behaviour.Granted;
         invokesMemberCallback = false;
       } else if (parentDomain == DexDomain.CorePlatform && childDomain == DexDomain.Platform) {
-        expected = (hiddenness == Hiddenness.BlocklistAndCorePlatformApi)
-            ? Behaviour.Granted : Behaviour.Denied;
+        expected = (hiddenness == Hiddenness.Unsupported
+                           || hiddenness == Hiddenness.BlocklistAndCorePlatformApi)
+                ? Behaviour.Granted
+                : Behaviour.Denied;
         invokesMemberCallback = false;
       } else if (isSameBoot) {
         expected = Behaviour.Granted;
diff --git a/test/677-fsi/expected-stderr.txt b/test/677-fsi/expected-stderr.txt
index 35c39182a5..2295afcf0a 100644
--- a/test/677-fsi/expected-stderr.txt
+++ b/test/677-fsi/expected-stderr.txt
@@ -1,2 +1,2 @@
-oat file has dex code, but APK has uncompressed dex code
-oat file has dex code, but APK has uncompressed dex code
+Oat file has dex code, but APK has uncompressed dex code
+Oat file has dex code, but APK has uncompressed dex code
diff --git a/test/677-fsi/run.py b/test/677-fsi/run.py
index 357692f74e..e124eff4df 100644
--- a/test/677-fsi/run.py
+++ b/test/677-fsi/run.py
@@ -26,7 +26,7 @@ def run(ctx, args):
   # Only keep the lines we're interested in.
   ctx.run(fr"sed -i '/Hello World/!d' '{args.stdout_file}'")
   ctx.run(
-      fr"sed -i '/^.*: oat file has dex code, but APK has uncompressed dex code/!d' '{args.stderr_file}'"
+      fr"sed -i '/^.*: Oat file has dex code, but APK has uncompressed dex code/!d' '{args.stderr_file}'"
   )
 
   # Remove part of message containing filename.
diff --git a/test/733-icce/expected-stderr.txt b/test/733-icce/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/733-icce/expected-stdout.txt b/test/733-icce/expected-stdout.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/733-icce/info.txt b/test/733-icce/info.txt
new file mode 100644
index 0000000000..de28c6d51d
--- /dev/null
+++ b/test/733-icce/info.txt
@@ -0,0 +1 @@
+Regression test for a missing incompatible class change check in the compiler.
diff --git a/test/733-icce/smali/Cls.smali b/test/733-icce/smali/Cls.smali
new file mode 100644
index 0000000000..af35234543
--- /dev/null
+++ b/test/733-icce/smali/Cls.smali
@@ -0,0 +1,29 @@
+#
+# Copyright (C) 2024 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+.class public LCls;
+.super LMissing;
+
+.method public constructor <init>()V
+.registers 1
+    invoke-direct {v0}, Ljava/lang/Object;-><init>()V
+    return-void
+.end method
+
+.method public callVirtualAsStatic()V
+.registers 1
+    invoke-static {}, LOther;->virtualMethod()V
+    return-void
+.end method
diff --git a/test/733-icce/smali/Other.smali b/test/733-icce/smali/Other.smali
new file mode 100644
index 0000000000..e8a7ba70b4
--- /dev/null
+++ b/test/733-icce/smali/Other.smali
@@ -0,0 +1,28 @@
+#
+# Copyright (C) 2024 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+.class public LOther;
+.super Ljava/lang/Object;
+
+.method public constructor <init>()V
+.registers 1
+    invoke-direct {v0}, Ljava/lang/Object;-><init>()V
+    return-void
+.end method
+
+.method public virtualMethod()V
+.registers 1
+    return-void
+.end method
diff --git a/test/733-icce/src/Main.java b/test/733-icce/src/Main.java
new file mode 100644
index 0000000000..8e254b2c2a
--- /dev/null
+++ b/test/733-icce/src/Main.java
@@ -0,0 +1,21 @@
+/*
+ * Copyright (C) 2024 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class Main {
+
+  public static void main(String[] args) {
+  }
+}
diff --git a/test/734-duplicate-fields/build.py b/test/734-duplicate-fields/build.py
new file mode 100644
index 0000000000..d531de7a5c
--- /dev/null
+++ b/test/734-duplicate-fields/build.py
@@ -0,0 +1,20 @@
+#
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+def build(ctx):
+  if ctx.jvm:
+    return  # The test does not build on JVM
+  ctx.default_build()
diff --git a/test/734-duplicate-fields/expected-stderr.txt b/test/734-duplicate-fields/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/734-duplicate-fields/expected-stdout.txt b/test/734-duplicate-fields/expected-stdout.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/734-duplicate-fields/info.txt b/test/734-duplicate-fields/info.txt
new file mode 100644
index 0000000000..94fefae9e7
--- /dev/null
+++ b/test/734-duplicate-fields/info.txt
@@ -0,0 +1,3 @@
+Regression test for duplicate static/instance fields, where the runtime would
+have non-deterministic behavior. We now fail dex verification in the presence of
+duplicate instance/static fields.
diff --git a/test/734-duplicate-fields/run.py b/test/734-duplicate-fields/run.py
new file mode 100644
index 0000000000..d98d701b6a
--- /dev/null
+++ b/test/734-duplicate-fields/run.py
@@ -0,0 +1,18 @@
+# Copyright 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+def run(ctx, args):
+  # Disable prebuild to avoid dex2oat failing with dex file verification.
+  ctx.default_run(args, prebuild=False)
diff --git a/test/734-duplicate-fields/smali-ex/Cls.smali b/test/734-duplicate-fields/smali-ex/Cls.smali
new file mode 100644
index 0000000000..f9c76f3146
--- /dev/null
+++ b/test/734-duplicate-fields/smali-ex/Cls.smali
@@ -0,0 +1,28 @@
+#
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+.class public LCls;
+.super Ljava/lang/Object;
+
+.method public constructor <init>()V
+.registers 2
+    invoke-direct {p0}, Ljava/lang/Object;-><init>()V
+    const/16 v0, 0x42
+    iput v0, p0, LCls;->myField:I
+    return-void
+.end method
+
+.field public myField:I
+.field public static myField:I = 0x1
diff --git a/test/734-duplicate-fields/src/Main.java b/test/734-duplicate-fields/src/Main.java
new file mode 100644
index 0000000000..0d97d02744
--- /dev/null
+++ b/test/734-duplicate-fields/src/Main.java
@@ -0,0 +1,34 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import dalvik.system.PathClassLoader;
+
+public class Main {
+  static final String TEST_NAME = "734-duplicate-fields";
+
+  static final String SECONDARY_NAME = TEST_NAME + "-ex";
+  static final String SECONDARY_DEX_FILE =
+    System.getenv("DEX_LOCATION") + "/" + SECONDARY_NAME + ".jar";
+
+  public static void main(String[] args) throws Exception {
+    PathClassLoader pcl = new PathClassLoader(SECONDARY_DEX_FILE, Main.class.getClassLoader());
+    try {
+      Class.forName("Cls", true, pcl);
+      throw new Error("Expected ClassNotFoundException");
+    } catch (ClassNotFoundException expected) {
+    }
+  }
+}
diff --git a/test/735-checker-condition-merging/expected-stderr.txt b/test/735-checker-condition-merging/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/735-checker-condition-merging/expected-stdout.txt b/test/735-checker-condition-merging/expected-stdout.txt
new file mode 100644
index 0000000000..4071f36257
--- /dev/null
+++ b/test/735-checker-condition-merging/expected-stdout.txt
@@ -0,0 +1,20 @@
+IfXLtzAElseB(-7): A
+IfXLtzAElseB(42): B
+IfXLtzAElseB_Move(-7): A
+IfXLtzAElseB_Move(42): B
+IfXLtzAElseB_EnvUse(-7): A
+IfXLtzAElseB_EnvUse(42): B
+IfXNullAElseB(null): A
+IfXNullAElseB(new Object()): B
+IfXNullAElseB_Move(null): A
+IfXNullAElseB_Move(new Object()): B
+IfXNullAElseB_EnvUse(null): A
+IfXNullAElseB_EnvUse(new Object()): B
+IfXNullAElseB_RefNoEnvInBlock(null, true): A
+IfXNullAElseB_RefNoEnvInBlock(new Object(), true): B
+IfXNullAElseB_RefNoEnvInBlock(null, false): C
+IfXNullAElseB_RefNoEnvInBlock(new Object(), false): C
+IfLt7_0AElseB_86LoadFromConstantTable(2.0, true): A
+IfLt7_0AElseB_86LoadFromConstantTable(10.0, true): B
+IfLt7_0AElseB_86LoadFromConstantTable(2.0, false): C
+IfLt7_0AElseB_86LoadFromConstantTable(10.0, false): C
diff --git a/test/735-checker-condition-merging/info.txt b/test/735-checker-condition-merging/info.txt
new file mode 100644
index 0000000000..30790ab52f
--- /dev/null
+++ b/test/735-checker-condition-merging/info.txt
@@ -0,0 +1 @@
+Test for merging `HCondition` to the user with the "emited at use site" approach.
diff --git a/test/735-checker-condition-merging/src/Main.java b/test/735-checker-condition-merging/src/Main.java
new file mode 100644
index 0000000000..98e9c4cdfe
--- /dev/null
+++ b/test/735-checker-condition-merging/src/Main.java
@@ -0,0 +1,285 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class Main {
+    private int intField;
+
+    public static void main(String[] args) {
+        System.out.print("IfXLtzAElseB(-7): ");
+        $noinline$IfXLtzAElseB(-7);
+        System.out.print("IfXLtzAElseB(42): ");
+        $noinline$IfXLtzAElseB(42);
+
+        System.out.print("IfXLtzAElseB_Move(-7): ");
+        new Main().$noinline$IfXLtzAElseB_Move(-7);
+        System.out.print("IfXLtzAElseB_Move(42): ");
+        new Main().$noinline$IfXLtzAElseB_Move(42);
+
+        System.out.print("IfXLtzAElseB_EnvUse(-7): ");
+        $noinline$IfXLtzAElseB_EnvUse(-7);
+        System.out.print("IfXLtzAElseB_EnvUse(42): ");
+        $noinline$IfXLtzAElseB_EnvUse(42);
+
+        System.out.print("IfXNullAElseB(null): ");
+        $noinline$IfXNullAElseB(null);
+        System.out.print("IfXNullAElseB(new Object()): ");
+        $noinline$IfXNullAElseB(new Object());
+
+        System.out.print("IfXNullAElseB_Move(null): ");
+        new Main().$noinline$IfXNullAElseB_Move(null);
+        System.out.print("IfXNullAElseB_Move(new Object()): ");
+        new Main().$noinline$IfXNullAElseB_Move(new Object());
+
+        System.out.print("IfXNullAElseB_EnvUse(null): ");
+        new Main().$noinline$IfXNullAElseB_EnvUse(null);
+        System.out.print("IfXNullAElseB_EnvUse(new Object()): ");
+        new Main().$noinline$IfXNullAElseB_EnvUse(new Object());
+
+        System.out.print("IfXNullAElseB_RefNoEnvInBlock(null, true): ");
+        new Main().$noinline$IfXNullAElseB_RefNoEnvInBlock(null, true);
+        System.out.print("IfXNullAElseB_RefNoEnvInBlock(new Object(), true): ");
+        new Main().$noinline$IfXNullAElseB_RefNoEnvInBlock(new Object(), true);
+        System.out.print("IfXNullAElseB_RefNoEnvInBlock(null, false): ");
+        new Main().$noinline$IfXNullAElseB_RefNoEnvInBlock(null, false);
+        System.out.print("IfXNullAElseB_RefNoEnvInBlock(new Object(), false): ");
+        new Main().$noinline$IfXNullAElseB_RefNoEnvInBlock(new Object(), false);
+
+        System.out.print("IfLt7_0AElseB_86LoadFromConstantTable(2.0, true): ");
+        new Main().$noinline$IfLt7_0AElseB_86LoadFromConstantTable(2.0, true);
+        System.out.print("IfLt7_0AElseB_86LoadFromConstantTable(10.0, true): ");
+        new Main().$noinline$IfLt7_0AElseB_86LoadFromConstantTable(10.0, true);
+        System.out.print("IfLt7_0AElseB_86LoadFromConstantTable(2.0, false): ");
+        new Main().$noinline$IfLt7_0AElseB_86LoadFromConstantTable(2.0, false);
+        System.out.print("IfLt7_0AElseB_86LoadFromConstantTable(10.0, false): ");
+        new Main().$noinline$IfLt7_0AElseB_86LoadFromConstantTable(10.0, false);
+
+        // Note: We do not test the code paths where `ConditionMoveWouldExtendReferenceLifetime()`
+        // in the "prepare_for_register_allocation" pass finds an instruction with environment
+        // between the `HCondition` and its user in this run-test. These are difficult to create
+        // from Java code and changes to other passes can easily invalidate such tests. Therefore
+        // we defer to using gtests for these cases.
+    }
+
+    private static void $noinline$A() {
+        System.out.println("A");
+    }
+
+    private static void $noinline$B() {
+        System.out.println("B");
+    }
+
+    private static void $noinline$C() {
+        System.out.println("C");
+    }
+
+    private static boolean $inline$XLtz(int x) {
+        // After inlining, this shall be turned to a `HSelect` and then simplified as `HLessThan`.
+        return x < 0;
+    }
+
+    private static boolean $inline$XNull(Object x) {
+        // After inlining, this shall be turned to a `HSelect` and then simplified as `HEqual`.
+        return x == null;
+    }
+
+    private static boolean $inline$XLt7_0(double x) {
+        return x < 7.0;
+    }
+
+    private static void $noinline$ignore(int ignored) {}
+
+    /// CHECK-START: void Main.$noinline$IfXLtzAElseB(int) prepare_for_register_allocation (before)
+    /// CHECK:      <<Cond:z\d+>> {{GreaterThanOrEqual|LessThan}} emitted_at_use_site:false
+    /// CHECK-NEXT:               If [<<Cond>>]
+
+    /// CHECK-START: void Main.$noinline$IfXLtzAElseB(int) prepare_for_register_allocation (after)
+    /// CHECK:      <<Cond:z\d+>> {{GreaterThanOrEqual|LessThan}} emitted_at_use_site:true
+    /// CHECK-NEXT:               If [<<Cond>>]
+
+    public static void $noinline$IfXLtzAElseB(int x) {
+        if (x < 0) {
+            $noinline$A();
+        } else {
+            $noinline$B();
+        }
+    }
+
+    /// CHECK-START: void Main.$noinline$IfXLtzAElseB_Move(int) prepare_for_register_allocation (before)
+    /// CHECK:      <<Cond:z\d+>> LessThan emitted_at_use_site:false
+    /// CHECK-NEXT:               InstanceFieldGet
+    // On X86, there can be also X86ComputeBaseMethodAddress here.
+    /// CHECK:                    If [<<Cond>>]
+
+    /// CHECK-START: void Main.$noinline$IfXLtzAElseB_Move(int) prepare_for_register_allocation (after)
+    /// CHECK:                    InstanceFieldGet
+    // On X86, there can be also X86ComputeBaseMethodAddress here.
+    /// CHECK:      <<Cond:z\d+>> LessThan emitted_at_use_site:true
+    /// CHECK-NEXT:               If [<<Cond>>]
+
+    public void $noinline$IfXLtzAElseB_Move(int x) {
+        boolean cond = $inline$XLtz(x);
+
+        int value = intField;
+        if (cond) {
+            cond = false;  // Avoid environment use below.
+            $noinline$A();
+        } else {
+            cond = false;  // Avoid environment use below.
+            $noinline$B();
+        }
+        $noinline$ignore(value);
+    }
+
+    /// CHECK-START: void Main.$noinline$IfXLtzAElseB_EnvUse(int) prepare_for_register_allocation (before)
+    /// CHECK:                    LessThan emitted_at_use_site:false
+
+    /// CHECK-START: void Main.$noinline$IfXLtzAElseB_EnvUse(int) prepare_for_register_allocation (after)
+    /// CHECK-DAG:  <<Cond:z\d+>> LessThan emitted_at_use_site:false
+    // Match an environment use. Use the fact that the <<Cond>> is in vreg 0. Otherwise we'd
+    // need to add a regex to match the earlier vregs which is difficult due to a regex eagerly
+    // consuming as much as possible but it could be curtailed by using the fact that there
+    // are no other boolean (`z`) values in the graph, for example with `{{([^z,]+,)*}}`. This
+    // would be much easier if we could put a variable inside the regex and make the entire
+    // env uses a single regex, `env:[[{{([^,]+,)*<<Cond>>(,[^,\]]+)*}}]]`.
+    /// CHECK-DAG:                InvokeStaticOrDirect env:[[<<Cond>>{{(,[^,\]]+)*}}]]
+
+    public static void $noinline$IfXLtzAElseB_EnvUse(int x) {
+        boolean cond = $inline$XLtz(x);
+        if (cond) {
+            $noinline$A();
+        } else {
+            $noinline$B();
+        }
+    }
+
+    /// CHECK-START: void Main.$noinline$IfXNullAElseB(java.lang.Object) prepare_for_register_allocation (before)
+    /// CHECK:      <<Cond:z\d+>> {{Equal|NotEqual}} emitted_at_use_site:false
+    /// CHECK-NEXT:               If [<<Cond>>]
+
+    /// CHECK-START: void Main.$noinline$IfXNullAElseB(java.lang.Object) prepare_for_register_allocation (after)
+    /// CHECK:      <<Cond:z\d+>> {{Equal|NotEqual}} emitted_at_use_site:true
+    /// CHECK-NEXT:               If [<<Cond>>]
+
+    public static void $noinline$IfXNullAElseB(Object x) {
+        if (x == null) {
+            $noinline$A();
+        } else {
+            $noinline$B();
+        }
+    }
+
+    /// CHECK-START: void Main.$noinline$IfXNullAElseB_Move(java.lang.Object) prepare_for_register_allocation (before)
+    /// CHECK:      <<Cond:z\d+>> Equal emitted_at_use_site:false
+    /// CHECK-NEXT:               InstanceFieldGet
+    // On X86, there can be also X86ComputeBaseMethodAddress here.
+    /// CHECK:                    If [<<Cond>>]
+
+    /// CHECK-START: void Main.$noinline$IfXNullAElseB_Move(java.lang.Object) prepare_for_register_allocation (after)
+    /// CHECK:                    InstanceFieldGet
+    // On X86, there can be also X86ComputeBaseMethodAddress here.
+    /// CHECK:      <<Cond:z\d+>> Equal emitted_at_use_site:true
+    /// CHECK-NEXT:               If [<<Cond>>]
+
+    public void $noinline$IfXNullAElseB_Move(Object x) {
+        boolean cond = $inline$XNull(x);
+
+        int value = intField;
+        if (cond) {
+            cond = false;  // Avoid environment use below.
+            $noinline$A();
+        } else {
+            cond = false;  // Avoid environment use below.
+            $noinline$B();
+        }
+        $noinline$ignore(value);
+    }
+
+    /// CHECK-START: void Main.$noinline$IfXNullAElseB_EnvUse(java.lang.Object) prepare_for_register_allocation (before)
+    /// CHECK:                    Equal emitted_at_use_site:false
+
+    /// CHECK-START: void Main.$noinline$IfXNullAElseB_EnvUse(java.lang.Object) prepare_for_register_allocation (after)
+    /// CHECK:                    Equal emitted_at_use_site:false
+
+    public static void $noinline$IfXNullAElseB_EnvUse(Object x) {
+        boolean cond = $inline$XNull(x);
+        if (cond) {
+            $noinline$A();
+        } else {
+            $noinline$B();
+        }
+    }
+
+    /// CHECK-START: void Main.$noinline$IfXNullAElseB_RefNoEnvInBlock(java.lang.Object, boolean) prepare_for_register_allocation (before)
+    /// CHECK:      <<Cond:z\d+>> {{Equal|NotEqual}} emitted_at_use_site:false
+    /// CHECK:                    If [<<Cond>>]
+
+    /// CHECK-START: void Main.$noinline$IfXNullAElseB_RefNoEnvInBlock(java.lang.Object, boolean) prepare_for_register_allocation (after)
+    /// CHECK:      <<Cond:z\d+>> {{Equal|NotEqual}} emitted_at_use_site:false
+    /// CHECK:                    If [<<Cond>>]
+
+    public static void $noinline$IfXNullAElseB_RefNoEnvInBlock(Object x, boolean otherCond) {
+        boolean cond = $inline$XNull(x);
+        if (otherCond) {
+            if (cond) {
+                cond = false;  // Avoid environment use below.
+                $noinline$A();
+            } else {
+                cond = false;  // Avoid environment use below.
+                $noinline$B();
+            }
+        } else {
+            cond = false;  // Avoid environment use below.
+            $noinline$C();
+        }
+    }
+
+    /// CHECK-START: void Main.$noinline$IfLt7_0AElseB_86LoadFromConstantTable(double, boolean) prepare_for_register_allocation (before)
+    /// CHECK:      <<Cond:z\d+>> {{LessThan|GreaterThanOrEqual}} emitted_at_use_site:false
+    /// CHECK:                    If [<<Cond>>]
+
+    /// CHECK-START: void Main.$noinline$IfLt7_0AElseB_86LoadFromConstantTable(double, boolean) prepare_for_register_allocation (after)
+    /// CHECK:      <<Cond:z\d+>> {{LessThan|GreaterThanOrEqual}} emitted_at_use_site:true
+    /// CHECK-NEXT:               If [<<Cond>>]
+
+    /// CHECK-START-X86: void Main.$noinline$IfLt7_0AElseB_86LoadFromConstantTable(double, boolean) prepare_for_register_allocation (after)
+    /// CHECK:                    X86ComputeBaseMethodAddress
+    // Note: X86ComputeBaseMethodAddress is not moved before X86LoadFromConstantTable because
+    // it has additional uses in all the `$noinline$` invokes.
+    /// CHECK:                    X86LoadFromConstantTable
+    /// CHECK-NEXT: <<Cond:z\d+>> {{LessThan|GreaterThanOrEqual}} emitted_at_use_site:true
+    /// CHECK-NEXT:               If [<<Cond>>]
+
+    /// CHECK-START-X86: void Main.$noinline$IfLt7_0AElseB_86LoadFromConstantTable(double, boolean) prepare_for_register_allocation (after)
+    /// CHECK-DAG: <<MA:i\d+>>    X86ComputeBaseMethodAddress
+    /// CHECK-DAG:                InvokeStaticOrDirect [<<MA>>]
+
+    public static void $noinline$IfLt7_0AElseB_86LoadFromConstantTable(
+            double x, boolean otherCond) {
+        boolean cond = $inline$XLt7_0(x);
+        if (otherCond) {
+            if (cond) {
+                cond = false;  // Avoid environment use below.
+                $noinline$A();
+            } else {
+                cond = false;  // Avoid environment use below.
+                $noinline$B();
+            }
+        } else {
+            cond = false;  // Avoid environment use below.
+            $noinline$C();
+        }
+    }
+}
diff --git a/test/735-interface-clone/expected-stderr.txt b/test/735-interface-clone/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/735-interface-clone/expected-stdout.txt b/test/735-interface-clone/expected-stdout.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/735-interface-clone/info.txt b/test/735-interface-clone/info.txt
new file mode 100644
index 0000000000..6a4ac1de2e
--- /dev/null
+++ b/test/735-interface-clone/info.txt
@@ -0,0 +1 @@
+Regression test calling super.clone from an interface method.
diff --git a/test/735-interface-clone/smali/Cls.smali b/test/735-interface-clone/smali/Cls.smali
new file mode 100644
index 0000000000..df8a0c2cd1
--- /dev/null
+++ b/test/735-interface-clone/smali/Cls.smali
@@ -0,0 +1,23 @@
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+.class  LCls;
+.super  Ljava/lang/Object;
+.implements LItf;
+
+.method public constructor <init>()V
+.registers 2
+       invoke-direct {v1}, Ljava/lang/Object;-><init>()V
+       return-void
+.end method
diff --git a/test/735-interface-clone/smali/Itf.smali b/test/735-interface-clone/smali/Itf.smali
new file mode 100644
index 0000000000..891a56195a
--- /dev/null
+++ b/test/735-interface-clone/smali/Itf.smali
@@ -0,0 +1,22 @@
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+.class public abstract interface LItf;
+.super Ljava/lang/Object;
+
+.method public doClone()V
+.registers 1
+       invoke-super {p0}, LItf;->clone()Ljava/lang/Object;
+       return-void
+.end method
diff --git a/test/735-interface-clone/src/Main.java b/test/735-interface-clone/src/Main.java
new file mode 100644
index 0000000000..44c01ca489
--- /dev/null
+++ b/test/735-interface-clone/src/Main.java
@@ -0,0 +1,32 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.lang.reflect.InvocationTargetException;
+
+public class Main {
+
+  public static void main(String[] args) throws Exception {
+    Class<?> cls = Class.forName("Cls");
+    try {
+      cls.getMethod("doClone").invoke(cls.newInstance(), null);
+      throw new Error("Expected IncompatibleClassChangeError");
+    } catch (InvocationTargetException e) {
+      if (!(e.getCause() instanceof IncompatibleClassChangeError)) {
+        throw new Error("Expected IncompatibleClassChangeError, got " + e.getCause());
+      }
+    }
+  }
+}
diff --git a/test/736-interface-super-Object/expected-stderr.txt b/test/736-interface-super-Object/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/736-interface-super-Object/expected-stdout.txt b/test/736-interface-super-Object/expected-stdout.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/736-interface-super-Object/info.txt b/test/736-interface-super-Object/info.txt
new file mode 100644
index 0000000000..48708f7a99
--- /dev/null
+++ b/test/736-interface-super-Object/info.txt
@@ -0,0 +1 @@
+Regression test calling super.toString from an interface method.
diff --git a/test/736-interface-super-Object/smali/Cls.smali b/test/736-interface-super-Object/smali/Cls.smali
new file mode 100644
index 0000000000..df8a0c2cd1
--- /dev/null
+++ b/test/736-interface-super-Object/smali/Cls.smali
@@ -0,0 +1,23 @@
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+.class  LCls;
+.super  Ljava/lang/Object;
+.implements LItf;
+
+.method public constructor <init>()V
+.registers 2
+       invoke-direct {v1}, Ljava/lang/Object;-><init>()V
+       return-void
+.end method
diff --git a/test/736-interface-super-Object/smali/Itf.smali b/test/736-interface-super-Object/smali/Itf.smali
new file mode 100644
index 0000000000..dc6eefb480
--- /dev/null
+++ b/test/736-interface-super-Object/smali/Itf.smali
@@ -0,0 +1,22 @@
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+.class public abstract interface LItf;
+.super Ljava/lang/Object;
+
+.method public doToString()V
+.registers 1
+       invoke-super {p0}, LItf;->toString()Ljava/lang/String;
+       return-void
+.end method
diff --git a/test/736-interface-super-Object/src/Main.java b/test/736-interface-super-Object/src/Main.java
new file mode 100644
index 0000000000..27806217d3
--- /dev/null
+++ b/test/736-interface-super-Object/src/Main.java
@@ -0,0 +1,32 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.lang.reflect.InvocationTargetException;
+
+public class Main {
+
+  public static void main(String[] args) throws Exception {
+    Class<?> cls = Class.forName("Cls");
+    try {
+      cls.getMethod("doToString").invoke(cls.newInstance(), null);
+      throw new Error("Expected IncompatibleClassChangeError");
+    } catch (InvocationTargetException e) {
+      if (!(e.getCause() instanceof IncompatibleClassChangeError)) {
+        throw new Error("Expected IncompatibleClassChangeError, got " + e.getCause());
+      }
+    }
+  }
+}
diff --git a/test/817-hiddenapi/build.py b/test/817-hiddenapi/build.py
index 37b8cd5bb7..e08a351d99 100644
--- a/test/817-hiddenapi/build.py
+++ b/test/817-hiddenapi/build.py
@@ -27,7 +27,7 @@ def build(ctx):
   # hidden API access flags in dex files. DexFileVerifier is not invoked on boot
   # class path dex files, so the boot jar loads fine in the latter case.
 
-  ctx.default_build(use_hiddenapi=True)
+  ctx.default_build(use_hiddenapi=True, delete_srcs=False)
 
   # Move the jar file into the resource folder to be bundled with the test.
   os.mkdir(ctx.test_dir / "res")
diff --git a/test/817-hiddenapi/expected-stdout.txt b/test/817-hiddenapi/expected-stdout.txt
index 8db7853696..3fc3bce6bd 100644
--- a/test/817-hiddenapi/expected-stdout.txt
+++ b/test/817-hiddenapi/expected-stdout.txt
@@ -1,2 +1,2 @@
 JNI_OnLoad called
-JNI_OnLoad called
+JNI_OnLoad in libarttest_external.cc called
diff --git a/test/817-hiddenapi/libarttest_api.cc b/test/817-hiddenapi/libarttest_api.cc
new file mode 100644
index 0000000000..35995f2dfe
--- /dev/null
+++ b/test/817-hiddenapi/libarttest_api.cc
@@ -0,0 +1,29 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "libarttest_api.h"
+
+#include "runtime.h"
+
+namespace art {
+namespace test_817_hiddenapi {
+
+void SetDedupeHiddenApiWarnings(bool value) {
+  Runtime::Current()->SetDedupeHiddenApiWarnings(value);
+}
+
+}  // namespace test_817_hiddenapi
+}  // namespace art
diff --git a/test/817-hiddenapi/libarttest_api.h b/test/817-hiddenapi/libarttest_api.h
new file mode 100644
index 0000000000..cb0d5b4311
--- /dev/null
+++ b/test/817-hiddenapi/libarttest_api.h
@@ -0,0 +1,28 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ART_TEST_817_HIDDENAPI_LIBARTTEST_API_H_
+#define ART_TEST_817_HIDDENAPI_LIBARTTEST_API_H_
+
+namespace art {
+namespace test_817_hiddenapi {
+
+void SetDedupeHiddenApiWarnings(bool value);
+
+}  // namespace test_817_hiddenapi
+}  // namespace art
+
+#endif  // ART_TEST_817_HIDDENAPI_LIBARTTEST_API_H_
diff --git a/test/817-hiddenapi/run.py b/test/817-hiddenapi/run.py
index 0ebb768cc9..a394a4ea29 100644
--- a/test/817-hiddenapi/run.py
+++ b/test/817-hiddenapi/run.py
@@ -1,5 +1,3 @@
-#!/bin/bash
-#
 # Copyright 2022 The Android Open Source Project
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -16,8 +14,12 @@
 
 
 def run(ctx, args):
+  args.testlib += [args.testlib[0] + "_external"]
   ctx.default_run(args)
 
   # On gcstress configurations, an extra "JNI_OnUnload called" line may
   # be emitted. If so, remove it.
   ctx.run(fr"sed -i '/^JNI_OnUnload called$/d' '{args.stdout_file}'")
+
+  # Ignore hiddenapi's denial errors which go to stderr on host and qemu (but not on device).
+  ctx.run(fr"sed -i -E '/ E dalvikvm.* hiddenapi: /d' '{args.stderr_file}'")
diff --git a/test/817-hiddenapi/src-art/Main.java b/test/817-hiddenapi/src-art/Main.java
index 6c46deadb1..77af8b15cc 100644
--- a/test/817-hiddenapi/src-art/Main.java
+++ b/test/817-hiddenapi/src-art/Main.java
@@ -39,7 +39,7 @@ public class Main {
     m.invoke(null);
 
     // Create a new native library which 'childLoader' can load.
-    String absoluteLibraryPath = getNativeLibFileName(args[0]);
+    String absoluteLibraryPath = getNativeLibFileName(args[1]);
 
     // Do the test for JNI code.
     m = cls.getDeclaredMethod("testNative", String.class);
diff --git a/test/817-hiddenapi/test_native.cc b/test/817-hiddenapi/test_native.cc
index d99fb06220..9d15ffd668 100644
--- a/test/817-hiddenapi/test_native.cc
+++ b/test/817-hiddenapi/test_native.cc
@@ -14,12 +14,13 @@
  * limitations under the License.
  */
 
-#include "jni.h"
-
 #include <android-base/logging.h>
+#include <jni.h>
+#include <nativehelper/ScopedUtfChars.h>
+
+#include <iostream>
 
-#include "nativehelper/ScopedUtfChars.h"
-#include "runtime.h"
+#include "libarttest_api.h"
 
 namespace art {
 
@@ -52,7 +53,7 @@ extern "C" JNIEXPORT jboolean JNICALL Java_TestCase_testAccessInternal(JNIEnv* e
 }
 
 extern "C" JNIEXPORT void JNICALL Java_TestCase_dedupeHiddenApiWarnings(JNIEnv*, jclass) {
-  Runtime::Current()->SetDedupeHiddenApiWarnings(true);
+  art::test_817_hiddenapi::SetDedupeHiddenApiWarnings(true);
 }
 
 }  // namespace art
diff --git a/test/850-checker-branches/smali/TestCase.smali b/test/850-checker-branches/smali/TestCase.smali
index 1c9dc50b0b..8ee82b3ff3 100644
--- a/test/850-checker-branches/smali/TestCase.smali
+++ b/test/850-checker-branches/smali/TestCase.smali
@@ -16,7 +16,7 @@
 
 .super Ljava/lang/Object;
 
-## CHECK-START: int TestCase.withBranch(boolean) select_generator (before)
+## CHECK-START: int TestCase.withBranch(boolean) control_flow_simplifier (before)
 ## CHECK: If true_count:2 false_count:1
 .method public static withBranch(Z)I
   .registers 2
diff --git a/test/860-vdex-failure/expected-stderr.txt b/test/860-vdex-failure/expected-stderr.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/860-vdex-failure/expected-stdout.txt b/test/860-vdex-failure/expected-stdout.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/test/860-vdex-failure/info.txt b/test/860-vdex-failure/info.txt
new file mode 100644
index 0000000000..aeab3b8aee
--- /dev/null
+++ b/test/860-vdex-failure/info.txt
@@ -0,0 +1,2 @@
+Regression test for the optimizing compiler, which used to try to inline a
+method which failed verification.
diff --git a/test/860-vdex-failure/run.py b/test/860-vdex-failure/run.py
new file mode 100644
index 0000000000..91e6651059
--- /dev/null
+++ b/test/860-vdex-failure/run.py
@@ -0,0 +1,20 @@
+#!/bin/bash
+#
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+def run(ctx, args):
+  # Pass speed as filter to ensure compilation.
+  ctx.default_run(args, vdex=True, vdex_filter="speed")
diff --git a/test/860-vdex-failure/smali/Caller.smali b/test/860-vdex-failure/smali/Caller.smali
new file mode 100644
index 0000000000..ca85cb6126
--- /dev/null
+++ b/test/860-vdex-failure/smali/Caller.smali
@@ -0,0 +1,22 @@
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+.class public LCaller;
+.super Ljava/lang/Object;
+
+.method public static test()V
+  .registers 0
+  invoke-static {}, LTestCase;->test()V
+  return-void
+.end method
diff --git a/test/860-vdex-failure/smali/TestCase.smali b/test/860-vdex-failure/smali/TestCase.smali
new file mode 100644
index 0000000000..51b4132606
--- /dev/null
+++ b/test/860-vdex-failure/smali/TestCase.smali
@@ -0,0 +1,22 @@
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+.class public LTestCase;
+.super Ljava/lang/Object;
+
+.method public static test()V
+  .registers 1
+  move-result-object v0
+  return-void
+.end method
diff --git a/test/860-vdex-failure/src/Main.java b/test/860-vdex-failure/src/Main.java
new file mode 100644
index 0000000000..99a300f838
--- /dev/null
+++ b/test/860-vdex-failure/src/Main.java
@@ -0,0 +1,30 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.lang.reflect.InvocationTargetException;
+
+public class Main {
+  public static void main(String[] args) throws Exception {
+    try {
+      Class.forName("Caller").getDeclaredMethod("test").invoke(null);
+      throw new Error("Expected InvocationTargetException");
+    } catch (InvocationTargetException e) {
+      if (!(e.getCause() instanceof VerifyError)) {
+        throw new Error("Expected VerifyError, got " + e.getCause());
+      }
+    }
+  }
+}
diff --git a/test/900-hello-plugin/run.py b/test/900-hello-plugin/run.py
index 607a0e5d40..f56ee8a532 100644
--- a/test/900-hello-plugin/run.py
+++ b/test/900-hello-plugin/run.py
@@ -18,18 +18,6 @@
 def run(ctx, args):
   plugin = "libartagent.so" if args.O else "libartagentd.so"
 
-  # Adjust the agent path when running on device.
-  if not args.host:
-    for i, opt in enumerate(args.runtime_option):
-      if opt.startswith("-Djava.library.path="):
-        libpath = opt.split("=")[-1]
-        assert libpath.startswith("/data/nativetest"), libpath
-
-        # The linker configuration used for dalvikvm(64) in the ART APEX requires us
-        # to pass the full path to the agent to the runtime when running on device.
-        plugin = f"{libpath}/{plugin}"
-        break
-
   ctx.default_run(
       args,
       runtime_option=[
diff --git a/test/906-iterate-heap/expected-stdout.txt b/test/906-iterate-heap/expected-stdout.txt
index 73b7129bba..125e65132e 100644
--- a/test/906-iterate-heap/expected-stdout.txt
+++ b/test/906-iterate-heap/expected-stdout.txt
@@ -19,27 +19,27 @@
 1@0 (32, 2xD '0000000000000000000000000000f03f')
 2
 doTestPrimitiveFieldsClasses
-10000@0 (static, int, index=3) 0000000000000000
+10000@0 (static, int, index=9) 0000000000000000
 10001
-10000@0 (static, int, index=11) 0000000000000000
+10000@0 (static, int, index=14) 0000000000000000
 10001
 10001
 10001
 doTestPrimitiveFieldsIntegral
 10000@0 (instance, int, index=2) 0000000000000000
+10001@0 (instance, byte, index=3) 0000000000000001
+10002@0 (instance, char, index=4) 0000000000000061
+10003@0 (instance, int, index=5) 0000000000000003
+10004@0 (instance, long, index=6) 0000000000000004
+10005@0 (instance, short, index=8) 0000000000000002
+10006
+doTestPrimitiveFieldsFloat
+10000@0 (instance, int, index=3) 0000000000000000
 10001@0 (instance, byte, index=4) 0000000000000001
 10002@0 (instance, char, index=5) 0000000000000061
 10003@0 (instance, int, index=6) 0000000000000003
 10004@0 (instance, long, index=7) 0000000000000004
 10005@0 (instance, short, index=9) 0000000000000002
-10006
-doTestPrimitiveFieldsFloat
-10000@0 (instance, int, index=3) 0000000000000000
-10001@0 (instance, byte, index=5) 0000000000000001
-10002@0 (instance, char, index=6) 0000000000000061
-10003@0 (instance, int, index=7) 0000000000000003
-10004@0 (instance, long, index=8) 0000000000000004
-10005@0 (instance, short, index=10) 0000000000000002
-10006@0 (instance, double, index=12) 3ff3ae147ae147ae
-10007@0 (instance, float, index=13) 000000003f9d70a4
+10006@0 (instance, double, index=11) 3ff3ae147ae147ae
+10007@0 (instance, float, index=12) 000000003f9d70a4
 10008
diff --git a/test/913-heaps/expected-stdout.txt b/test/913-heaps/expected-stdout.txt
index 7d3622305a..87ad2d669d 100644
--- a/test/913-heaps/expected-stdout.txt
+++ b/test/913-heaps/expected-stdout.txt
@@ -128,27 +128,27 @@ root@root --(thread)--> 3000@0 [size=120, length=-1]
 4@0 (18, 3xS '010002000300')
 1@0 (14, 2xZ '0001')
 23456789
-10000@0 (static, int, index=3) 0000000000000000
+10000@0 (static, int, index=9) 0000000000000000
 10001
-10000@0 (static, int, index=11) 0000000000000000
+10000@0 (static, int, index=14) 0000000000000000
 10001
 10001
 10001
 10000@0 (instance, int, index=2) 0000000000000000
+10001@0 (instance, byte, index=3) 0000000000000001
+10002@0 (instance, char, index=4) 0000000000000061
+10003@0 (instance, int, index=5) 0000000000000003
+10004@0 (instance, long, index=6) 0000000000000004
+10005@0 (instance, short, index=8) 0000000000000002
+10006
+10000@0 (instance, int, index=3) 0000000000000000
 10001@0 (instance, byte, index=4) 0000000000000001
 10002@0 (instance, char, index=5) 0000000000000061
 10003@0 (instance, int, index=6) 0000000000000003
 10004@0 (instance, long, index=7) 0000000000000004
 10005@0 (instance, short, index=9) 0000000000000002
-10006
-10000@0 (instance, int, index=3) 0000000000000000
-10001@0 (instance, byte, index=5) 0000000000000001
-10002@0 (instance, char, index=6) 0000000000000061
-10003@0 (instance, int, index=7) 0000000000000003
-10004@0 (instance, long, index=8) 0000000000000004
-10005@0 (instance, short, index=10) 0000000000000002
-10006@0 (instance, double, index=12) 3ff3ae147ae147ae
-10007@0 (instance, float, index=13) 000000003f9d70a4
+10006@0 (instance, double, index=11) 3ff3ae147ae147ae
+10007@0 (instance, float, index=12) 000000003f9d70a4
 10008
 --- klass ---
 root@root --(stack-local[id=1,tag=3000,depth=2,method=doFollowReferencesTestNonRoot,vreg=0,location= 31])--> 1@1000 [size=16, length=-1]
diff --git a/test/913-heaps/heaps.cc b/test/913-heaps/heaps.cc
index 197230f01e..e857c4b57c 100644
--- a/test/913-heaps/heaps.cc
+++ b/test/913-heaps/heaps.cc
@@ -1119,6 +1119,8 @@ static jint JNICALL HeapIterationExtCallback([[maybe_unused]] jlong class_tag,
   return 0;
 }
 
+#pragma clang diagnostic push
+#pragma clang diagnostic ignored "-Wcast-function-type-mismatch"
 extern "C" JNIEXPORT void JNICALL Java_art_Test913_iterateThroughHeapExt(
     JNIEnv* env, [[maybe_unused]] jclass klass) {
   CHECK(gIterateThroughHeapExt != nullptr);
@@ -1132,6 +1134,7 @@ extern "C" JNIEXPORT void JNICALL Java_art_Test913_iterateThroughHeapExt(
   JvmtiErrorToException(env, jvmti_env, ret);
   CHECK(gFoundExt);
 }
+#pragma clang diagnostic pop
 
 extern "C" JNIEXPORT jboolean JNICALL Java_art_Test913_checkInitialized(JNIEnv* env, jclass, jclass c) {
   jint status;
diff --git a/test/999-redefine-hiddenapi/run.py b/test/999-redefine-hiddenapi/run.py
index 4796039801..49a7ef365a 100644
--- a/test/999-redefine-hiddenapi/run.py
+++ b/test/999-redefine-hiddenapi/run.py
@@ -17,3 +17,6 @@
 
 def run(ctx, args):
   ctx.default_run(args, jvmti=True)
+
+  # Ignore hiddenapi's denial errors which go to stderr on host and qemu (but not on device).
+  ctx.run(fr"sed -i -E '/ E dalvikvm.* hiddenapi: /d' '{args.stderr_file}'")
diff --git a/test/Android.bp b/test/Android.bp
index a0e2f5980d..a447d4aa37 100644
--- a/test/Android.bp
+++ b/test/Android.bp
@@ -86,11 +86,6 @@ cc_defaults {
             relative_install_path: "art/x86_64",
         },
     },
-    // Tests aren't generally included in any APEX, but this is necessary to
-    // avoid duplicate install rules for them by making them unavailable to platform.
-    apex_available: [
-        "com.android.art.debug",
-    ],
 }
 
 // Variant of art_test_defaults for test libraries that installs them in a
@@ -368,6 +363,10 @@ art_cc_defaults {
         },
     },
     static_libs: [
+        // This dependency links the whole runtime statically into the test. Note that the boot
+        // classpath is not (normally) bundled with the test, so if the runtime is used to actually
+        // start a VM it may load the boot classpath from the device. Depending on the test
+        // configuration, that may not be in sync with the statically linked runtime.
         "libart-gtest",
     ],
     version_script: ":art-standalone-gtest-version",
@@ -538,31 +537,6 @@ java_defaults {
     ],
 }
 
-art_cc_test_library {
-    name: "libarttest",
-    defaults: ["libarttest-defaults"],
-    shared_libs: [
-        "libart",
-        "libdexfile#impl",
-        "libprofile",
-        "libartbase",
-    ],
-}
-
-art_cc_test_library {
-    name: "libarttestd",
-    defaults: [
-        "art_debug_defaults",
-        "libarttest-defaults",
-    ],
-    shared_libs: [
-        "libartd",
-        "libdexfiled#impl",
-        "libprofiled",
-        "libartbased",
-    ],
-}
-
 art_cc_defaults {
     name: "libnativebridgetest-defaults",
     defaults: ["art_test_defaults"],
@@ -755,7 +729,6 @@ art_cc_defaults {
         "1959-redefine-object-instrument/fake_redef_object.cc",
         "1960-obsolete-jit-multithread-native/native_say_hi.cc",
         "1964-add-to-dex-classloader-file/add_to_loader.cc",
-        "1963-add-to-dex-classloader-in-memory/check_memfd_create.cc",
         "2012-structural-redefinition-failures-jni-id/set-jni-id-used.cc",
         "2031-zygote-compiled-frame-deopt/native-wait.cc",
         "2038-hiddenapi-jvmti-ext/hiddenapi_ext.cc",
@@ -944,7 +917,7 @@ cc_defaults {
         "692-vdex-inmem-loader/vdex_inmem_loader.cc",
         "720-thread-priority/thread_priority.cc",
         "800-smali/jni.cc",
-        "817-hiddenapi/test_native.cc",
+        "817-hiddenapi/libarttest_api.cc",
         "855-native/throws_exception.cc",
         "909-attach-agent/disallow_debugging.cc",
         "993-breakpoints-non-debuggable/native_attach_agent.cc",
@@ -974,6 +947,71 @@ cc_defaults {
         "libnativehelper",
         "libunwindstack",
     ],
+    target: {
+        android: {
+            header_libs: ["libnativeloader-headers"],
+            shared_libs: ["libdl_android"],
+        },
+    },
+}
+
+art_cc_test_library {
+    name: "libarttest",
+    defaults: ["libarttest-defaults"],
+    shared_libs: [
+        "libart",
+        "libdexfile#impl",
+        "libprofile",
+        "libartbase",
+    ],
+}
+
+art_cc_test_library {
+    name: "libarttestd",
+    defaults: [
+        "art_debug_defaults",
+        "libarttest-defaults",
+    ],
+    shared_libs: [
+        "libartd",
+        "libdexfiled#impl",
+        "libprofiled",
+        "libartbased",
+    ],
+}
+
+// "External" native code for run tests. Unlike libarttest(d), this library is
+// not installed in the com_android_art namespace, so it cannot access ART
+// internals. It's instead installed in a location that will be available as
+// java.library.path in run tests, and it can call functions in
+// libarttest(d).so.
+cc_defaults {
+    name: "libarttest_external-defaults",
+    defaults: ["art_test_defaults"],
+    srcs: [
+        "817-hiddenapi/test_native.cc",
+        "common/libarttest_external.cc",
+    ],
+    shared_libs: [
+        "libbase",
+        "liblog",
+        "libnativehelper",
+    ],
+}
+
+art_cc_test_library {
+    name: "libarttest_external",
+    defaults: ["libarttest_external-defaults"],
+    shared_libs: ["libarttest"],
+}
+
+art_cc_test_library {
+    name: "libarttestd_external",
+    defaults: [
+        "art_debug_defaults",
+        "libarttest_external-defaults",
+    ],
+    shared_libs: ["libarttestd"],
 }
 
 java_library {
@@ -1354,6 +1392,18 @@ art_cc_test {
     test_config: "art-gtests-target-chroot.xml",
 }
 
+art_cc_test {
+    name: "ArtTest",
+    data: [
+        // We need the ART testing apex, which contains debug binaries and other utilities.
+        ":com.android.art.testing",
+        // Archive which contains all run-test data and test-specific bash scripts to run them.
+        ":art-test-target-fg",
+    ],
+    test_suites: ["general-tests"],
+    test_config: "ArtTest.xml",
+}
+
 csuite_test {
     name: "csuite-app-compile-launch",
     test_config_template: "csuite-app-compile-launch.xml",
diff --git a/test/Android.run-test.bp b/test/Android.run-test.bp
index f119ec75b4..24b8612c05 100644
--- a/test/Android.run-test.bp
+++ b/test/Android.run-test.bp
@@ -2675,8 +2675,8 @@ genrule_defaults {
 }
 
 java_genrule {
-    name: "art-run-test-host-data-merged-tmp",
-    out: ["art-run-test-host-data-merged.tgz"],
+    name: "art-test-host-tmp",
+    out: ["art-test-host.tgz"],
     srcs: [
         ":art-run-test-host-data-shard00-tmp",
         ":art-run-test-host-data-shard01-tmp",
@@ -2787,14 +2787,14 @@ java_genrule {
 // This filegroup is so that the host prebuilt etc can depend on a device genrule,
 // as prebuilt_etc doesn't have the equivalent of device_common_srcs.
 filegroup {
-    name: "art-run-test-host-data-merged-fg",
-    device_common_srcs: [":art-run-test-host-data-merged-tmp"],
+    name: "art-test-host-fg",
+    device_common_srcs: [":art-test-host-tmp"],
 }
 
 // Install in the output directory to make it accessible for tests.
 prebuilt_etc_host {
-    name: "art-run-test-host-data-merged",
-    src: ":art-run-test-host-data-merged-fg",
+    name: "art-test-host",
+    src: ":art-test-host-fg",
     required: [
         "art-run-test-host-data-shard00",
         "art-run-test-host-data-shard01",
@@ -2899,7 +2899,7 @@ prebuilt_etc_host {
         "art-run-test-host-data-shardHiddenApi",
     ],
     sub_dir: "art",
-    filename: "art-run-test-host-data-merged.tgz",
+    filename: "art-test-host.tgz",
 }
 
 // Phony target used to build all shards
@@ -5795,8 +5795,8 @@ genrule_defaults {
 }
 
 java_genrule {
-    name: "art-run-test-target-data-merged-tmp",
-    out: ["art-run-test-target-data-merged.tgz"],
+    name: "art-test-target-tmp",
+    out: ["art-test-target.tgz"],
     srcs: [
         ":art-run-test-target-data-shard00-tmp",
         ":art-run-test-target-data-shard01-tmp",
@@ -5907,14 +5907,14 @@ java_genrule {
 // This filegroup is so that the host prebuilt etc can depend on a device genrule,
 // as prebuilt_etc doesn't have the equivalent of device_common_srcs.
 filegroup {
-    name: "art-run-test-target-data-merged-fg",
-    device_common_srcs: [":art-run-test-target-data-merged-tmp"],
+    name: "art-test-target-fg",
+    device_common_srcs: [":art-test-target-tmp"],
 }
 
 // Install in the output directory to make it accessible for tests.
 prebuilt_etc_host {
-    name: "art-run-test-target-data-merged",
-    src: ":art-run-test-target-data-merged-fg",
+    name: "art-test-target",
+    src: ":art-test-target-fg",
     required: [
         "art-run-test-target-data-shard00",
         "art-run-test-target-data-shard01",
@@ -6019,7 +6019,7 @@ prebuilt_etc_host {
         "art-run-test-target-data-shardHiddenApi",
     ],
     sub_dir: "art",
-    filename: "art-run-test-target-data-merged.tgz",
+    filename: "art-test-target.tgz",
 }
 
 // Phony target used to build all shards
@@ -8915,8 +8915,8 @@ genrule_defaults {
 }
 
 java_genrule {
-    name: "art-run-test-jvm-data-merged-tmp",
-    out: ["art-run-test-jvm-data-merged.tgz"],
+    name: "art-test-jvm-tmp",
+    out: ["art-test-jvm.tgz"],
     srcs: [
         ":art-run-test-jvm-data-shard00-tmp",
         ":art-run-test-jvm-data-shard01-tmp",
@@ -9027,14 +9027,14 @@ java_genrule {
 // This filegroup is so that the host prebuilt etc can depend on a device genrule,
 // as prebuilt_etc doesn't have the equivalent of device_common_srcs.
 filegroup {
-    name: "art-run-test-jvm-data-merged-fg",
-    device_common_srcs: [":art-run-test-jvm-data-merged-tmp"],
+    name: "art-test-jvm-fg",
+    device_common_srcs: [":art-test-jvm-tmp"],
 }
 
 // Install in the output directory to make it accessible for tests.
 prebuilt_etc_host {
-    name: "art-run-test-jvm-data-merged",
-    src: ":art-run-test-jvm-data-merged-fg",
+    name: "art-test-jvm",
+    src: ":art-test-jvm-fg",
     required: [
         "art-run-test-jvm-data-shard00",
         "art-run-test-jvm-data-shard01",
@@ -9139,7 +9139,7 @@ prebuilt_etc_host {
         "art-run-test-jvm-data-shardHiddenApi",
     ],
     sub_dir: "art",
-    filename: "art-run-test-jvm-data-merged.tgz",
+    filename: "art-test-jvm.tgz",
 }
 
 // Phony target used to build all shards
diff --git a/test/Android.run-test.bp.py b/test/Android.run-test.bp.py
index e208672b11..aec0b8eb46 100755
--- a/test/Android.run-test.bp.py
+++ b/test/Android.run-test.bp.py
@@ -147,7 +147,7 @@ def main():
         }}
         """))
 
-      name = "art-run-test-{mode}-data-merged".format(mode=mode)
+      name = "art-test-{mode}".format(mode=mode)
       srcs = ("\n"+" "*16).join('":{}-tmp",'.format(n) for n in names)
       deps = ("\n"+" "*16).join('"{}",'.format(n) for n in names)
       f.write(textwrap.dedent(f"""
diff --git a/test/Android.run-test.mk b/test/Android.run-test.mk
index f4fdd10ef0..5c1624181b 100644
--- a/test/Android.run-test.mk
+++ b/test/Android.run-test.mk
@@ -30,23 +30,15 @@ TEST_ART_RUN_TEST_DEPENDENCIES := \
 # images (all images as we sync only once).
 ART_TEST_TARGET_RUN_TEST_DEPENDENCIES := $(TESTING_ART_APEX) $(TARGET_CORE_IMG_OUTS)
 
-# Also need libartagent.
-ART_TEST_TARGET_RUN_TEST_DEPENDENCIES += libartagent-target libartagentd-target
-
-# Also need libtiagent.
-ART_TEST_TARGET_RUN_TEST_DEPENDENCIES += libtiagent-target libtiagentd-target
-
-# Also need libtistress.
-ART_TEST_TARGET_RUN_TEST_DEPENDENCIES += libtistress-target libtistressd-target
-
-# Also need libarttest.
-ART_TEST_TARGET_RUN_TEST_DEPENDENCIES += libarttest-target libarttestd-target
-
-# Also need libnativebridgetest.
-ART_TEST_TARGET_RUN_TEST_DEPENDENCIES += libnativebridgetest-target libnativebridgetestd-target
-
-# Also need signal_dumper.
-ART_TEST_TARGET_RUN_TEST_DEPENDENCIES += signal_dumper-target
+# Also need these other libs and binaries.
+ART_TEST_TARGET_RUN_TEST_DEPENDENCIES += \
+  libartagent-target libartagentd-target \
+  libtiagent-target libtiagentd-target \
+  libtistress-target libtistressd-target \
+  libarttest-target libarttestd-target \
+  libarttest_external-target libarttestd_external-target \
+  libnativebridgetest-target libnativebridgetestd-target \
+  signal_dumper-target
 
 # All tests require the host executables. The tests also depend on the core images, but on
 # specific version depending on the compiler.
@@ -62,6 +54,8 @@ ART_TEST_HOST_RUN_TEST_DEPENDENCIES := \
   $(ART_TEST_LIST_host_$(ART_HOST_ARCH)_libartagentd) \
   $(ART_TEST_LIST_host_$(ART_HOST_ARCH)_libarttest) \
   $(ART_TEST_LIST_host_$(ART_HOST_ARCH)_libarttestd) \
+  $(ART_TEST_LIST_host_$(ART_HOST_ARCH)_libarttest_external) \
+  $(ART_TEST_LIST_host_$(ART_HOST_ARCH)_libarttestd_external) \
   $(ART_TEST_LIST_host_$(ART_HOST_ARCH)_libnativebridgetest) \
   $(ART_TEST_LIST_host_$(ART_HOST_ARCH)_libnativebridgetestd) \
   $(ART_HOST_OUT_SHARED_LIBRARIES)/libicu_jni$(ART_HOST_SHLIB_EXTENSION) \
@@ -84,6 +78,8 @@ ART_TEST_HOST_RUN_TEST_DEPENDENCIES += \
   $(ART_TEST_LIST_host_$(2ND_ART_HOST_ARCH)_libartagentd) \
   $(ART_TEST_LIST_host_$(2ND_ART_HOST_ARCH)_libarttest) \
   $(ART_TEST_LIST_host_$(2ND_ART_HOST_ARCH)_libarttestd) \
+  $(ART_TEST_LIST_host_$(2ND_ART_HOST_ARCH)_libarttest_external) \
+  $(ART_TEST_LIST_host_$(2ND_ART_HOST_ARCH)_libarttestd_external) \
   $(ART_TEST_LIST_host_$(2ND_ART_HOST_ARCH)_libnativebridgetest) \
   $(ART_TEST_LIST_host_$(2ND_ART_HOST_ARCH)_libnativebridgetestd) \
   $(2ND_ART_HOST_OUT_SHARED_LIBRARIES)/libicu_jni$(ART_HOST_SHLIB_EXTENSION) \
diff --git a/tools/ahat/ahat-tests.xml b/test/ArtTest.xml
similarity index 59%
rename from tools/ahat/ahat-tests.xml
rename to test/ArtTest.xml
index b07905a9a7..05d7e96d68 100644
--- a/tools/ahat/ahat-tests.xml
+++ b/test/ArtTest.xml
@@ -1,5 +1,5 @@
 <?xml version="1.0" encoding="utf-8"?>
-<!-- Copyright (C) 2018 The Android Open Source Project
+<!-- Copyright (C) 2025 The Android Open Source Project
 
      Licensed under the Apache License, Version 2.0 (the "License");
      you may not use this file except in compliance with the License.
@@ -13,11 +13,15 @@
      See the License for the specific language governing permissions and
      limitations under the License.
 -->
-<configuration description="Runs the ahat unit tests">
-    <option name="test-suite-tag" value="ahat" />
-    <option name="test-suite-tag" value="art-tools" />
-    <option name="null-device" value="true" />
-    <test class="com.android.tradefed.testtype.HostTest" >
-        <option name="class" value="com.android.ahat.AhatTestSuite" />
+<configuration description="ART generic test runner">
+    <option name="test-suite-tag" value="ArtTest" />
+
+    <target_preparer class="com.android.tradefed.targetprep.InstallApexModuleTargetPreparer" >
+        <option name="test-file-name" value="com.android.art.testing.apex" />
+    </target_preparer>
+
+    <!--
+    <test class="com.android.tradefed.testtype.ArtTest">
     </test>
+    -->
 </configuration>
diff --git a/test/common/libarttest_external.cc b/test/common/libarttest_external.cc
new file mode 100644
index 0000000000..21bd080c60
--- /dev/null
+++ b/test/common/libarttest_external.cc
@@ -0,0 +1,32 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <jni.h>
+
+#include <iostream>
+
+namespace art {
+
+// Override JNI_OnLoad in libarttest(d).so.
+extern "C" JNIEXPORT jint JNI_OnLoad(JavaVM*, void*) {
+  std::cout << "JNI_OnLoad in libarttest_external.cc called" << std::endl;
+  return JNI_VERSION_1_6;
+}
+
+// Override JNI_OnUnload in libarttest(d).so.
+extern "C" JNIEXPORT void JNI_OnUnload(JavaVM*, void*) {}
+
+}  // namespace art
diff --git a/test/common/runtime_state.cc b/test/common/runtime_state.cc
index c046e26755..465e2f6850 100644
--- a/test/common/runtime_state.cc
+++ b/test/common/runtime_state.cc
@@ -164,6 +164,19 @@ extern "C" JNIEXPORT jboolean JNICALL Java_Main_compiledWithOptimizing(JNIEnv* e
   return JNI_TRUE;
 }
 
+static ArtMethod* GetMethod(ScopedObjectAccess& soa, jclass cls, const ScopedUtfChars& chars)
+    REQUIRES_SHARED(Locks::mutator_lock_) {
+  CHECK(chars.c_str() != nullptr);
+  ArtMethod* method = soa.Decode<mirror::Class>(cls)->FindDeclaredDirectMethodByName(
+      chars.c_str(), kRuntimePointerSize);
+  if (method == nullptr) {
+    method = soa.Decode<mirror::Class>(cls)->FindDeclaredVirtualMethodByName(chars.c_str(),
+                                                                             kRuntimePointerSize);
+  }
+  DCHECK(method != nullptr) << "Unable to find method called " << chars.c_str();
+  return method;
+}
+
 extern "C" JNIEXPORT jboolean JNICALL Java_Main_isAotCompiled(JNIEnv* env,
                                                               jclass,
                                                               jclass cls,
@@ -172,8 +185,7 @@ extern "C" JNIEXPORT jboolean JNICALL Java_Main_isAotCompiled(JNIEnv* env,
   ScopedObjectAccess soa(self);
   ScopedUtfChars chars(env, method_name);
   CHECK(chars.c_str() != nullptr);
-  ArtMethod* method = soa.Decode<mirror::Class>(cls)->FindDeclaredDirectMethodByName(
-        chars.c_str(), kRuntimePointerSize);
+  ArtMethod* method = GetMethod(soa, cls, chars);
   const void* oat_code = method->GetOatMethodQuickCode(kRuntimePointerSize);
   if (oat_code == nullptr) {
     return false;
@@ -182,19 +194,6 @@ extern "C" JNIEXPORT jboolean JNICALL Java_Main_isAotCompiled(JNIEnv* env,
   return actual_code == oat_code;
 }
 
-static ArtMethod* GetMethod(ScopedObjectAccess& soa, jclass cls, const ScopedUtfChars& chars)
-    REQUIRES_SHARED(Locks::mutator_lock_) {
-  CHECK(chars.c_str() != nullptr);
-  ArtMethod* method = soa.Decode<mirror::Class>(cls)->FindDeclaredDirectMethodByName(
-        chars.c_str(), kRuntimePointerSize);
-  if (method == nullptr) {
-    method = soa.Decode<mirror::Class>(cls)->FindDeclaredVirtualMethodByName(
-        chars.c_str(), kRuntimePointerSize);
-  }
-  DCHECK(method != nullptr) << "Unable to find method called " << chars.c_str();
-  return method;
-}
-
 extern "C" JNIEXPORT jboolean JNICALL Java_Main_hasJitCompiledEntrypoint(JNIEnv* env,
                                                                          jclass,
                                                                          jclass cls,
@@ -458,7 +457,7 @@ extern "C" JNIEXPORT void JNICALL Java_Main_deoptimizeNativeMethod(JNIEnv* env,
   ScopedUtfChars chars(env, method_name);
   ArtMethod* method = GetMethod(soa, cls, chars);
   CHECK(method->IsNative());
-  Runtime::Current()->GetInstrumentation()->InitializeMethodsCode(method, /*aot_code=*/ nullptr);
+  Runtime::Current()->GetInstrumentation()->ReinitializeMethodsCode(method);
 }
 
 extern "C" JNIEXPORT jboolean JNICALL Java_Main_isDebuggable(JNIEnv*, jclass) {
diff --git a/test/default_run.py b/test/default_run.py
index fa6341a31d..21673f79a3 100755
--- a/test/default_run.py
+++ b/test/default_run.py
@@ -920,19 +920,11 @@ def default_run(ctx, args, **kwargs):
   if SIMPLEPERF:
     dalvikvm_cmdline = f"simpleperf record {dalvikvm_cmdline} && simpleperf report"
 
-  def sanitize_dex2oat_cmdline(cmdline: str) -> str:
-    args = []
-    for arg in cmdline.split(" "):
-      if arg == "--class-loader-context=&":
-        arg = "--class-loader-context=\&"
-      args.append(arg)
-    return " ".join(args)
-
   # Remove whitespace.
-  dex2oat_cmdline = sanitize_dex2oat_cmdline(dex2oat_cmdline)
+  dex2oat_cmdline = re.sub(" +", " ", dex2oat_cmdline)
   dalvikvm_cmdline = re.sub(" +", " ", dalvikvm_cmdline)
   dm_cmdline = re.sub(" +", " ", dm_cmdline)
-  vdex_cmdline = sanitize_dex2oat_cmdline(vdex_cmdline)
+  vdex_cmdline = re.sub(" +", " ", vdex_cmdline)
   profman_cmdline = re.sub(" +", " ", profman_cmdline)
 
   # Use an empty ASAN_OPTIONS to enable defaults.
@@ -978,10 +970,11 @@ def default_run(ctx, args, **kwargs):
       # installation.
       LD_LIBRARY_PATH = f"{ANDROID_ROOT}/{LIBRARY_DIRECTORY}"
 
-    # This adds libarttest(d).so to the default linker namespace when dalvikvm
-    # is run from /apex/com.android.art/bin. Since that namespace is essentially
-    # an alias for the com_android_art namespace, that gives libarttest(d).so
-    # full access to the internal ART libraries.
+    # This adds libarttest(d).so and various other test libraries to the default
+    # linker namespace when dalvikvm is run from /apex/com.android.art/bin.
+    # Since that namespace is essentially an alias for the com_android_art
+    # namespace, that gives libarttest(d).so full access to the internal ART
+    # libraries.
     LD_LIBRARY_PATH = f"/data/{TEST_DIRECTORY}/com.android.art/lib{SUFFIX64}:{LD_LIBRARY_PATH}"
     dlib = ("" if TEST_IS_NDEBUG else "d")
     art_test_internal_libraries = [
diff --git a/test/generate-boot-image/Android.bp b/test/generate-boot-image/Android.bp
index 60aac13e91..e555e2b9d3 100644
--- a/test/generate-boot-image/Android.bp
+++ b/test/generate-boot-image/Android.bp
@@ -44,11 +44,13 @@ art_cc_binary {
     srcs: ["generate-boot-image.cc"],
     shared_libs: [
         "liblog",
+        "libz", // For "libartbase".
     ],
     static_libs: [
         "libartbase",
         "libartbase-testing",
         "libbase",
+        "libziparchive", // For "libartbase".
     ],
     stl: "c++_static",
     tidy: true,
diff --git a/test/generate-boot-image/generate-boot-image.cc b/test/generate-boot-image/generate-boot-image.cc
index 3cd992f0c6..539670fa85 100644
--- a/test/generate-boot-image/generate-boot-image.cc
+++ b/test/generate-boot-image/generate-boot-image.cc
@@ -46,7 +46,6 @@ using ::android::base::ParseBool;
 using ::android::base::ParseBoolResult;
 using ::android::base::StringPrintf;
 using ::art::testing::GetLibCoreDexFileNames;
-using ::art::testing::GetLibCoreDexLocations;
 
 constexpr const char* kUsage = R"(
 A commandline tool to generate a primary boot image for testing.
@@ -80,6 +79,9 @@ Supported options:
       host. The default on target is based on the ISA of this binary.
   --core-only=true|false: If true, only compile ART jars. Otherwise, also compile core-icu4j and
       conscrypt. Default: false
+  --android-root-for-location=true|false: If true, use --android-root as a prefix to the dex
+      locations. This allows non-device paths to the bootclasspath jars to be used, for example: to
+      generate a boot image on host that can be used on host. Default: false
   --: Arguments following '--' are directly passed to dex2oat.
 )";
 
@@ -94,6 +96,7 @@ struct Options {
   std::string profile_file = "";
   std::string instruction_set = "";
   bool core_only = false;
+  bool android_root_for_location = false;
   std::vector<std::string> dex2oat_options;
 };
 
@@ -133,7 +136,12 @@ int GenerateBootImage(const Options& options) {
 
   std::vector<std::string> dex_files =
       GetLibCoreDexFileNames(options.android_root, options.core_only);
-  std::vector<std::string> dex_locations = GetLibCoreDexLocations(options.core_only);
+  std::vector<std::string> dex_locations;
+  if (options.android_root_for_location) {
+    dex_locations = dex_files;
+  } else {
+    dex_locations = GetLibCoreDexFileNames(/*prefix=*/"", options.core_only);
+  }
   args.push_back("--runtime-arg");
   args.push_back("-Xbootclasspath:" + Join(dex_files, ":"));
   args.push_back("--runtime-arg");
@@ -209,6 +217,12 @@ int Main(int argc, char** argv) {
         Usage(ART_FORMAT("Unrecognized --core-only value: '{}'", arg));
       }
       options.core_only = result == ParseBoolResult::kTrue;
+    } else if (ConsumePrefix(&arg, "--android-root-for-location=")) {
+      ParseBoolResult result = ParseBool(arg);
+      if (result == ParseBoolResult::kError) {
+        Usage(ART_FORMAT("Unrecognized --android-root-for-location value: '{}'", arg));
+      }
+      options.android_root_for_location = result == ParseBoolResult::kTrue;
     } else if (arg == "--") {
       for (i++; i < argc; i++) {
         options.dex2oat_options.push_back(argv[i]);
diff --git a/test/knownfailures.json b/test/knownfailures.json
index 1a3a44ee91..4d2d8ae654 100644
--- a/test/knownfailures.json
+++ b/test/knownfailures.json
@@ -1064,7 +1064,8 @@
           "2264-throwing-systemcleaner",
           "2267-class-implements-itself",
           "2276-const-method-type-gc-cleanup",
-          "2281-method-handle-invoke-static-class-unload"
+          "2281-method-handle-invoke-static-class-unload",
+          "2286-invokevirtual-invokeexact"
         ],
         "variant": "jvm",
         "bug": "b/73888836",
@@ -1140,6 +1141,9 @@
                   "692-vdex-inmem-loader",
                   "693-vdex-inmem-loader-evict",
                   "723-string-init-range",
+                  "734-duplicate-fields",
+                  "735-interface-clone",
+                  "736-interface-super-Object",
                   "808-checker-invoke-super",
                   "809-checker-invoke-super-bss",
                   "810-checker-invoke-super-default",
@@ -1164,6 +1168,7 @@
                   "848-pattern-match",
                   "854-image-inlining",
                   "855-native",
+                  "860-vdex-failure",
                   "999-redefine-hiddenapi",
                   "1000-non-moving-space-stress",
                   "1001-app-image-regions",
@@ -1224,7 +1229,8 @@
                   "2263-method-trace-jit",
                   "2270-mh-internal-hiddenapi-use",
                   "2271-profile-inline-cache",
-                  "2279-aconfig-flags"],
+                  "2279-aconfig-flags",
+                  "2286-method-tracing-aot-code"],
         "variant": "jvm",
         "description": ["Doesn't run on RI."]
     },
@@ -1435,16 +1441,6 @@
         "env_vars": {"ART_USE_READ_BARRIER": "false", "ART_DEFAULT_GC_TYPE": "CMS"},
         "description": ["Uses the low-ram flag which does not work with CMS"]
     },
-    {
-        "tests": ["150-loadlibrary",
-                  "656-annotation-lookup-generic-jni",
-                  "674-hiddenapi",
-                  "817-hiddenapi",
-                  "900-hello-plugin"],
-        "variant": "target",
-        "bug": "b/186654484",
-        "description": ["Disabled after the switch to avoid allow_all_shared_libs from the ART namespace to system."]
-    },
     {
         "tests": ["2001-virtual-structural-multithread"],
         "env_vars": {"SANITIZE_HOST": "address"},
@@ -1627,5 +1623,19 @@
                         "The ability to destroy a thread group and the concept of a destroyed ",
                         "thread group no longer exists. A thread group is eligible to be GC'ed ",
                         "when there are no live threads in the group and it is otherwise unreachable."]
+    },
+    {
+        "tests": ["004-StackWalk",
+                  "141-class-unload",
+                  "178-app-image-native-method",
+                  "597-deopt-busy-loop",
+                  "629-vdex-speed",
+                  "638-checker-inline-cache-intrinsic",
+                  "661-oat-writer-layout",
+                  "692-vdex-secondary-loader",
+                  "850-checker-branches",
+                  "2042-reference-processing"],
+        "env_vars": {"ART_USE_RESTRICTED_MODE": "true"},
+        "description": ["Test failures when using the restricted mode for simulator."]
     }
 ]
diff --git a/test/run-test b/test/run-test
index cc31077818..8cca59fddf 100755
--- a/test/run-test
+++ b/test/run-test
@@ -759,6 +759,11 @@ if True:
       if prebuild_mode:
         run_checker = True
 
+        if os.environ.get("ART_USE_RESTRICTED_MODE") == "true":
+          # TODO(Simulator): support checker runs.
+          run_checker = False
+          cfg_output_dir = tmp_dir
+
         if not target_mode:
           cfg_output_dir = tmp_dir
           checker_args = f"--arch={host_arch_name.upper()}"
@@ -870,8 +875,8 @@ if True:
     assert run_optimizing, "The CFG can be dumped only in optimizing mode"
     if target_mode:
       if ON_VM:
-        run(f'{SCP_CMD} "{SSH_USER}@${SSH_HOST}:{CHROOT}/{cfg_output_dir}/'
-            f'{cfg_output} {dump_cfg_path}"')
+        run(f'{SCP_CMD} {SSH_USER}@{SSH_HOST}:{CHROOT}/{cfg_output_dir}/'
+            f'{cfg_output} {dump_cfg_path}')
       else:
         run(f"adb pull {chroot}/{cfg_output_dir}/{cfg_output} {dump_cfg_path}")
     else:
@@ -903,7 +908,7 @@ if True:
     os.makedirs(dst.parent, exist_ok=True)
     txt = runner.read_text()
     txt = txt.replace(Path(tmp_dir).name, "${TMP_DIR}")  # Make it deterministic.
-    txt = re.sub('\[run-test:\d+\]', '[run-test:(line-number)]', txt)
+    txt = re.sub(r'\[run-test:\d+\]', '[run-test:(line-number)]', txt)
     dst.write_text(txt)
 
   if args.create_runner or save_runner_dir:
diff --git a/test/run_test_build.py b/test/run_test_build.py
index a93950f02f..3b6ac73a33 100755
--- a/test/run_test_build.py
+++ b/test/run_test_build.py
@@ -150,7 +150,7 @@ class BuildTestContext:
                        stderr=subprocess.STDOUT,
                        stdout=subprocess.PIPE)
     if REPORT_SLOW_COMMANDS:
-      m = re.search("([0-9\.]+)user", p.stdout)
+      m = re.search(r"([0-9\.]+)user", p.stdout)
       assert m, p.stdout
       t = float(m.group(1))
       if t > 1.0:
@@ -253,7 +253,8 @@ class BuildTestContext:
       use_smali=True,
       use_jasmin=True,
       javac_source_arg="1.8",
-      javac_target_arg="1.8"
+      javac_target_arg="1.8",
+      delete_srcs=True,
     ):
     javac_classpath = javac_classpath.copy()  # Do not modify default value.
 
@@ -505,6 +506,15 @@ class BuildTestContext:
       else:
         make_hiddenapi(Path("classes.dex"))
 
+    # Clean up intermediate files.
+    if self.target or self.host:
+      for f in self.test_dir.glob("**/*.class"):
+        f.unlink()
+    if delete_srcs and "-checker-" not in self.test_name:
+      for ext in ["java", "smali", "j"]:
+        for f in self.test_dir.glob(f"**/*.{ext}"):
+          f.unlink()
+
     # Create a single dex jar with two dex files for multidex.
     if need_dex:
       if Path("classes2.dex").exists():
@@ -571,19 +581,24 @@ def create_ci_runner_scripts(out, mode, test_names):
   ]
   run([python, script] + args + test_names, env=envs, check=True)
   tests = {
-    "setup": {
-      "adb push": [[str(setup.relative_to(out)), "/data/local/tmp/art/setup.sh"]],
-      "adb shell": [["sh", "/data/local/tmp/art/setup.sh"]],
+    "setup#compile-boot-image": {
+      "adb push": [
+        [str(setup.relative_to(out)), "/data/local/tmp/art/setup.sh"]
+      ],
+      "adb shell": [
+        ["rm", "-rf", "/data/local/tmp/art/test"],
+        ["sh", "/data/local/tmp/art/setup.sh"],
+      ],
     },
   }
   for runner in Path(out).glob("*/*.sh"):
     test_name = runner.parent.name
     test_hash = runner.stem
     target_dir = f"/data/local/tmp/art/test/{test_hash}"
-    tests[f"{test_name}-{test_hash}"] = {
-      "dependencies": ["setup"],
+    tests[f"{test_name}#{test_hash}"] = {
+      "dependencies": ["setup#compile-boot-image"],
       "adb push": [
-        [f"../{mode}/{test_name}/", f"{target_dir}/"],
+        [f"../{mode}/{test_name}", f"{target_dir}"],
         [str(runner.relative_to(out)), f"{target_dir}/run.sh"]
       ],
       "adb shell": [["sh", f"{target_dir}/run.sh"]],
diff --git a/test/standalone_test_lib_check.cc b/test/standalone_test_lib_check.cc
index 426a302f54..72c835a658 100644
--- a/test/standalone_test_lib_check.cc
+++ b/test/standalone_test_lib_check.cc
@@ -180,6 +180,6 @@ TEST(StandaloneTestAllowedLibDeps, test) {
     disallowed_libs.push_back(dyn_lib_dep);
   }
 
-  EXPECT_THAT(disallowed_libs, testing::IsEmpty())
+  EXPECT_THAT(disallowed_libs, ::testing::IsEmpty())
       << path_to_self.value() << " has disallowed shared library dependencies.";
 }
diff --git a/test/testing/art_fake_com.android.os.statsd/Android.bp b/test/testing/art_fake_com.android.os.statsd/Android.bp
index a9cdf4421e..5073a98604 100644
--- a/test/testing/art_fake_com.android.os.statsd/Android.bp
+++ b/test/testing/art_fake_com.android.os.statsd/Android.bp
@@ -45,7 +45,7 @@ cc_library_shared {
 apex {
     name: "art_fake_com.android.os.statsd",
     defaults: [
-        "s-launched-apex-module",
+        "r-launched-apex-module",
     ],
     compile_multilib: "both",
     native_shared_libs: [
@@ -55,6 +55,5 @@ apex {
     key: "com.android.art.key",
     certificate: ":com.android.art.certificate",
     file_contexts: ":com.android.os.statsd-file_contexts",
-    min_sdk_version: "30",
     visibility: ["//visibility:private"],
 }
diff --git a/test/testrunner/env.py b/test/testrunner/env.py
index 3aa27938f3..d0380318a3 100644
--- a/test/testrunner/env.py
+++ b/test/testrunner/env.py
@@ -164,4 +164,4 @@ ART_CHROOT_CMD = _env.get('ART_CHROOT_CMD', "unshare --user --map-root-user chro
 if ART_TEST_ON_VM:
   ART_TEST_CHROOT = _env.get('ART_TEST_CHROOT', f"/home/{ART_TEST_SSH_USER}/art-test-chroot")
 else:
-  ART_TEST_CHROOT = _env.get('ART_TEST_CHROOT', "/data/local/art-test-chroot")
+  ART_TEST_CHROOT = _env.get('ART_TEST_CHROOT')
diff --git a/test/testrunner/testrunner.py b/test/testrunner/testrunner.py
index 5892345067..447bdd2494 100755
--- a/test/testrunner/testrunner.py
+++ b/test/testrunner/testrunner.py
@@ -317,8 +317,6 @@ def setup_test_env():
       # Use only part of the cores since fully loading the device tends to lead to timeouts.
       fraction = 1.0 if env.ART_TEST_ON_VM else 0.75
       n_thread = max(1, int(get_target_cpu_count() * fraction))
-      if device_name == 'fugu':
-        n_thread = 1
   else:
     device_name = "host"
     if n_thread == 0:
@@ -1211,7 +1209,7 @@ def main():
   if build:
     build_targets = []
     # Build only the needed shards (depending on the selected tests).
-    shards = set(re.search("(\d\d)-", t).group(1) for t in tests)
+    shards = set(re.search(r"(\d\d)-", t).group(1) for t in tests)
     if any("hiddenapi" in t for t in tests):
       shards.add("HiddenApi")  # Include special HiddenApi shard.
     for mode in ['host', 'target', 'jvm']:
diff --git a/test/ti-agent/jni_binder.cc b/test/ti-agent/jni_binder.cc
index a115c22930..e85dc83e44 100644
--- a/test/ti-agent/jni_binder.cc
+++ b/test/ti-agent/jni_binder.cc
@@ -123,12 +123,12 @@ static void BindMethod(jvmtiEnv* jvmti_env, JNIEnv* env, jclass klass, jmethodID
   LOG(FATAL) << "Could not find " << mangled_names[0];
 }
 
-static std::string DescriptorToDot(const char* descriptor) {
-  size_t length = strlen(descriptor);
+static std::string DescriptorToDot(std::string_view descriptor) {
+  size_t length = descriptor.length();
   if (length > 1) {
     if (descriptor[0] == 'L' && descriptor[length - 1] == ';') {
       // Descriptors have the leading 'L' and trailing ';' stripped.
-      std::string result(descriptor + 1, length - 2);
+      std::string result(descriptor.substr(1, length - 2));
       std::replace(result.begin(), result.end(), '/', '.');
       return result;
     } else {
@@ -139,7 +139,7 @@ static std::string DescriptorToDot(const char* descriptor) {
     }
   }
   // Do nothing for non-class/array descriptors.
-  return descriptor;
+  return std::string(descriptor);
 }
 
 static jobject GetSystemClassLoader(JNIEnv* env) {
@@ -155,7 +155,7 @@ static jobject GetSystemClassLoader(JNIEnv* env) {
 static jclass FindClassWithClassLoader(JNIEnv* env, const char* class_name, jobject class_loader) {
   // Create a String of the name.
   std::string descriptor = android::base::StringPrintf("L%s;", class_name);
-  std::string dot_name = DescriptorToDot(descriptor.c_str());
+  std::string dot_name = DescriptorToDot(descriptor);
   ScopedLocalRef<jstring> name_str(env, env->NewStringUTF(dot_name.c_str()));
 
   // Call Class.forName with it.
diff --git a/test/utils/regen-test-files b/test/utils/regen-test-files
index c7c32c401a..4e5a9723c2 100755
--- a/test/utils/regen-test-files
+++ b/test/utils/regen-test-files
@@ -97,7 +97,6 @@ LINT_BASELINE_FILENAME = "lint-baseline.xml"
 # when ignoring their `run` script), even if not exactly as they would
 # with the original ART run-test harness.
 runnable_test_exceptions = frozenset([
-  "055-enum-performance",
   "059-finalizer-throw",
   "080-oom-throw",
   "133-static-invoke-super",
@@ -133,6 +132,8 @@ known_failing_tests = frozenset([
   "004-SignalTest",
   "004-UnsafeTest",
   "051-thread",
+  # 055-enum-performance is time sensitive and disabled in test.py
+  "055-enum-performance",
   "086-null-super",
   "087-gc-after-link",
   "136-daemon-jni-shutdown",
diff --git a/tools/ahat/Android.bp b/tools/ahat/Android.bp
index e1c7314605..29fc18f5e4 100644
--- a/tools/ahat/Android.bp
+++ b/tools/ahat/Android.bp
@@ -13,11 +13,6 @@
 // limitations under the License.
 
 package {
-    // See: http://go/android-license-faq
-    // A large-scale-change added 'default_applicable_licenses' to import
-    // all of the 'license_kinds' from "art_license"
-    // to get the below license kinds:
-    //   SPDX-license-identifier-Apache-2.0
     default_applicable_licenses: ["art_license"],
 }
 
@@ -34,9 +29,6 @@ java_binary_host {
     // Use a relaxed version to allow distribution against older runtimes.
     java_version: "11",
     javacflags: ["-Xdoclint:all/protected"],
-    static_libs: [
-        "guava",
-    ],
 }
 
 // --- ahat-test-dump.jar --------------
@@ -59,9 +51,52 @@ java_test_helper_library {
     srcs: ["src/ri-test-dump/**/*.java"],
 }
 
+java_binary_host {
+    name: "ahat-ri-test-dump-bin",
+    static_libs: ["ahat-ri-test-dump"],
+    main_class: "Main",
+}
+
 cc_library_shared {
     name: "libahat-test-jni",
     srcs: ["src/test/jni/**/*.cpp"],
     header_libs: ["jni_headers"],
     host_supported: true,
 }
+
+java_genrule {
+    name: "ahat-test-dump-gen",
+    srcs: [
+        "ahat-test-dump-gen.sh.in",
+        ":ahat-test-dump",
+    ],
+    out: ["ahat-test-dump-gen.sh"],
+    cmd: "sed -e s=@AHAT_TEST_DUMP_JAR@=$(location :ahat-test-dump)= $(location ahat-test-dump-gen.sh.in) > $(out)",
+}
+
+// Run ahat-ri-test-dump to generate ri-test-dump.hprof
+genrule {
+    name: "ahat_ri_test_dump_hprof",
+    out: ["ri-test-dump.hprof"],
+    tools: ["ahat-ri-test-dump-bin"],
+    cmd: "$(location ahat-ri-test-dump-bin) $(out)",
+}
+
+// To run these tests, use: atest ahat-tests --host
+java_test_host {
+    name: "ahat-tests",
+    srcs: ["src/test/**/*.java"],
+    manifest: "etc/ahat-tests.mf",
+    java_resources: [
+        ":ahat_ri_test_dump_hprof",
+        ":ahat-tests-res",
+    ],
+    static_libs: [
+        "ahat",
+        "junit-host",
+    ],
+    test_options: {
+        unit_test: true,
+    },
+    test_suites: ["general-tests"],
+}
diff --git a/tools/ahat/Android.mk b/tools/ahat/Android.mk
deleted file mode 100644
index 5db78e2765..0000000000
--- a/tools/ahat/Android.mk
+++ /dev/null
@@ -1,143 +0,0 @@
-#
-# Copyright (C) 2015 The Android Open Source Project
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-LOCAL_PATH := $(call my-dir)
-
-include art/build/Android.common_path.mk
-
-# The ahat tests rely on running ART to generate a heap dump for test, but ART
-# doesn't run on darwin. Only build and run the tests for linux.
-# There are also issues with running under instrumentation.
-ifeq ($(HOST_OS),linux)
-ifneq ($(EMMA_INSTRUMENT),true)
-
-# Determine the location of the test-dump.jar, test-dump.hprof, and proguard
-AHAT_TEST_DUMP_JAR := $(call intermediates-dir-for,JAVA_LIBRARIES,ahat-test-dump)/javalib.jar
-AHAT_TEST_DUMP_COMMON := $(call intermediates-dir-for,JAVA_LIBRARIES,ahat-test-dump,,COMMON)
-AHAT_TEST_DUMP_JNI := $(ART_HOST_OUT_SHARED_LIBRARIES)/libahat-test-jni$(ART_HOST_SHLIB_EXTENSION)
-AHAT_TEST_DUMP_HPROF := $(AHAT_TEST_DUMP_COMMON)/test-dump.hprof
-AHAT_TEST_DUMP_BASE_HPROF := $(AHAT_TEST_DUMP_COMMON)/test-dump-base.hprof
-AHAT_TEST_DUMP_PROGUARD_MAP := $(AHAT_TEST_DUMP_COMMON)/test-dump.map
-AHAT_TEST_DUMP_PROGUARD_DICTIONARY := $(AHAT_TEST_DUMP_COMMON)/proguard_dictionary
-
-# Directories to use for ANDROID_DATA when generating the test dumps to
-# ensure we don't pollute the source tree with any artifacts from running
-# dalvikvm.
-AHAT_TEST_DUMP_ANDROID_DATA := $(AHAT_TEST_DUMP_COMMON)/test-dump-android_data
-AHAT_TEST_DUMP_BASE_ANDROID_DATA := $(AHAT_TEST_DUMP_COMMON)/test-dump-base-android_data
-
-# Generate the proguard map in the desired location by copying it from
-# wherever the build system generates it by default.
-$(AHAT_TEST_DUMP_PROGUARD_MAP): PRIVATE_AHAT_SOURCE_PROGUARD_MAP := $(AHAT_TEST_DUMP_PROGUARD_DICTIONARY)
-$(AHAT_TEST_DUMP_PROGUARD_MAP): $(AHAT_TEST_DUMP_PROGUARD_DICTIONARY)
-	cp $(PRIVATE_AHAT_SOURCE_PROGUARD_MAP) $@
-
-ifeq (true,$(HOST_PREFER_32_BIT))
-  AHAT_TEST_DALVIKVM_DEP := $(HOST_OUT_EXECUTABLES)/dalvikvm32
-  AHAT_TEST_DALVIKVM_ARG := --32
-else
-  AHAT_TEST_DALVIKVM_DEP := $(HOST_OUT_EXECUTABLES)/dalvikvm64
-  AHAT_TEST_DALVIKVM_ARG := --64
-endif
-
-# Run ahat-test-dump.jar to generate test-dump.hprof and test-dump-base.hprof
-# The scripts below are run with --no-compile to avoid dependency on dex2oat.
-AHAT_TEST_DUMP_DEPENDENCIES := \
-  $(AHAT_TEST_DALVIKVM_DEP) \
-  $(AHAT_TEST_DUMP_JNI) \
-  $(ART_HOST_SHARED_LIBRARY_DEPENDENCIES) \
-  $(ART_HOST_SHARED_LIBRARY_DEBUG_DEPENDENCIES) \
-  $(ART_HOST_DEX_DEPENDENCIES) \
-  $(HOST_OUT_EXECUTABLES)/art \
-  $(HOST_CORE_IMG_OUTS)
-
-$(AHAT_TEST_DUMP_HPROF): PRIVATE_AHAT_TEST_ART := $(HOST_OUT_EXECUTABLES)/art
-$(AHAT_TEST_DUMP_HPROF): PRIVATE_AHAT_TEST_DUMP_JAR := $(AHAT_TEST_DUMP_JAR)
-$(AHAT_TEST_DUMP_HPROF): PRIVATE_AHAT_TEST_ANDROID_DATA := $(AHAT_TEST_DUMP_ANDROID_DATA)
-$(AHAT_TEST_DUMP_HPROF): PRIVATE_AHAT_TEST_DALVIKVM_ARG := $(AHAT_TEST_DALVIKVM_ARG)
-$(AHAT_TEST_DUMP_HPROF): $(AHAT_TEST_DUMP_JAR) $(AHAT_TEST_DUMP_DEPENDENCIES)
-	rm -rf $(PRIVATE_AHAT_TEST_ANDROID_DATA)
-	mkdir -p $(PRIVATE_AHAT_TEST_ANDROID_DATA)
-	ANDROID_DATA=$(PRIVATE_AHAT_TEST_ANDROID_DATA) \
-	  $(PRIVATE_AHAT_TEST_ART) --no-compile -d $(PRIVATE_AHAT_TEST_DALVIKVM_ARG) \
-	  -cp $(PRIVATE_AHAT_TEST_DUMP_JAR) Main $@
-
-$(AHAT_TEST_DUMP_BASE_HPROF): PRIVATE_AHAT_TEST_ART := $(HOST_OUT_EXECUTABLES)/art
-$(AHAT_TEST_DUMP_BASE_HPROF): PRIVATE_AHAT_TEST_DUMP_JAR := $(AHAT_TEST_DUMP_JAR)
-$(AHAT_TEST_DUMP_BASE_HPROF): PRIVATE_AHAT_TEST_ANDROID_DATA := $(AHAT_TEST_DUMP_BASE_ANDROID_DATA)
-$(AHAT_TEST_DUMP_BASE_HPROF): PRIVATE_AHAT_TEST_DALVIKVM_ARG := $(AHAT_TEST_DALVIKVM_ARG)
-$(AHAT_TEST_DUMP_BASE_HPROF): $(AHAT_TEST_DUMP_JAR) $(AHAT_TEST_DUMP_DEPENDENCIES)
-	rm -rf $(PRIVATE_AHAT_TEST_ANDROID_DATA)
-	mkdir -p $(PRIVATE_AHAT_TEST_ANDROID_DATA)
-	ANDROID_DATA=$(PRIVATE_AHAT_TEST_ANDROID_DATA) \
-	  $(PRIVATE_AHAT_TEST_ART) --no-compile -d $(PRIVATE_AHAT_TEST_DALVIKVM_ARG) \
-	  -cp $(PRIVATE_AHAT_TEST_DUMP_JAR) Main $@ --base
-
-
-# Determine the location of the ri-test-dump.jar and ri-test-dump.hprof.
-AHAT_RI_TEST_DUMP_JAR := $(call intermediates-dir-for,JAVA_LIBRARIES,ahat-ri-test-dump,HOST)/javalib.jar
-AHAT_RI_TEST_DUMP_COMMON := $(call intermediates-dir-for,JAVA_LIBRARIES,ahat-ri-test-dump,HOST,COMMON)
-AHAT_RI_TEST_DUMP_HPROF := $(AHAT_RI_TEST_DUMP_COMMON)/ri-test-dump.hprof
-
-# Run ahat-ri-test-dump.jar to generate ri-test-dump.hprof
-$(AHAT_RI_TEST_DUMP_HPROF): PRIVATE_AHAT_RI_TEST_DUMP_JAR := $(AHAT_RI_TEST_DUMP_JAR)
-$(AHAT_RI_TEST_DUMP_HPROF): $(AHAT_RI_TEST_DUMP_JAR)
-	rm -rf $@
-	java -cp $(PRIVATE_AHAT_RI_TEST_DUMP_JAR) Main $@
-
-# --- ahat-tests.jar --------------
-# To run these tests, use: atest ahat-tests --host
-include $(CLEAR_VARS)
-LOCAL_SRC_FILES := $(call all-java-files-under, src/test)
-LOCAL_JAR_MANIFEST := etc/ahat-tests.mf
-LOCAL_JAVA_RESOURCE_FILES := \
-  $(AHAT_TEST_DUMP_HPROF) \
-  $(AHAT_TEST_DUMP_BASE_HPROF) \
-  $(AHAT_TEST_DUMP_PROGUARD_MAP) \
-  $(AHAT_RI_TEST_DUMP_HPROF) \
-  $(LOCAL_PATH)/etc/L.hprof \
-  $(LOCAL_PATH)/etc/O.hprof \
-  $(LOCAL_PATH)/etc/RI.hprof
-LOCAL_STATIC_JAVA_LIBRARIES := ahat junit-host
-LOCAL_IS_HOST_MODULE := true
-LOCAL_MODULE_TAGS := tests
-LOCAL_MODULE := ahat-tests
-LOCAL_LICENSE_KINDS := SPDX-license-identifier-Apache-2.0
-LOCAL_LICENSE_CONDITIONS := notice
-LOCAL_NOTICE_FILE := $(LOCAL_PATH)/../../NOTICE
-LOCAL_TEST_CONFIG := ahat-tests.xml
-LOCAL_COMPATIBILITY_SUITE := general-tests
-include $(BUILD_HOST_JAVA_LIBRARY)
-AHAT_TEST_JAR := $(LOCAL_BUILT_MODULE)
-
-endif # EMMA_INSTRUMENT
-endif # linux
-
-# Clean up local variables.
-AHAT_TEST_JAR :=
-AHAT_TEST_DUMP_JAR :=
-AHAT_TEST_DUMP_JNI :=
-AHAT_TEST_DUMP_COMMON :=
-AHAT_TEST_DUMP_HPROF :=
-AHAT_TEST_DUMP_BASE_HPROF :=
-AHAT_TEST_DUMP_PROGUARD_MAP :=
-AHAT_TEST_DUMP_DEPENDENCIES :=
-AHAT_TEST_DUMP_ANDROID_DATA :=
-AHAT_TEST_DUMP_BASE_ANDROID_DATA :=
-
-AHAT_RI_TEST_DUMP_JAR :=
-AHAT_RI_TEST_DUMP_COMMON :=
-AHAT_RI_TEST_DUMP_HPROF :=
diff --git a/tools/ahat/OWNERS b/tools/ahat/OWNERS
new file mode 100644
index 0000000000..778633842d
--- /dev/null
+++ b/tools/ahat/OWNERS
@@ -0,0 +1,3 @@
+# Bug component: 1707588
+ruhler@google.com
+shayba@google.com
diff --git a/tools/ahat/README.txt b/tools/ahat/README.txt
index b799e481e5..526a64666a 100644
--- a/tools/ahat/README.txt
+++ b/tools/ahat/README.txt
@@ -55,6 +55,12 @@ Known Issues:
  * Line number decoding for allocations in proguarded classes.
 
 Release History:
+ 1.8 January 02, 2025
+   Show string values of byte[] instances.
+   Fix accounting for cleaned native registrations
+   Add option to download the contents of a byte[] as a file.
+   Show retained size in allocations view.
+
  1.7.3 June 27, 2024
    Add support to display bitmaps included in heapdump. To use this
    functionality, collect the heapdump with `adb shell am dumpheap -b <fmt>`,
diff --git a/tools/ahat/ahat-test-dump-gen.sh.in b/tools/ahat/ahat-test-dump-gen.sh.in
new file mode 100755
index 0000000000..ea6b3b4481
--- /dev/null
+++ b/tools/ahat/ahat-test-dump-gen.sh.in
@@ -0,0 +1,46 @@
+#!/bin/bash
+#
+# Copyright (C) 2025 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# ahat-test-dump-gen.sh.in is an input template for a script to re-generate
+# the test dump hprof files. The files should be regenerated whenever there
+# are changes to src/test-dump/*.
+#
+# To regenerate the test dump files:
+# $ m ahat-test-dump-gen
+# $ croot
+# $ bash out/soong/.intermediates/art/tools/ahat/ahat-test-dump-gen/android_common/gen/ahat-test-dump-gen.sh
+#
+# The outputs are placed in the etc/ directory where they can be checked in to
+# be used by the ahat tests. Note: You'll see a lot of error messages from
+# running the script that should be safe to ignore, as long as you see
+# etc/test-dump.hprof and etc/test-dump-base.hprof being generated.
+
+AHAT_ETC_DIR=${ANDROID_BUILD_TOP}/art/tools/ahat/etc
+AHAT_TEST_DUMP_JAR=@AHAT_TEST_DUMP_JAR@
+AHAT_TEST_DUMP_MAP=$(dirname ${AHAT_TEST_DUMP_JAR})/../proguard_dictionary
+
+# Build required dependencies.
+m build-art-host libahat-test-jni
+
+
+# test-dump.hprof
+art --no-compile -cp ${AHAT_TEST_DUMP_JAR} Main ${AHAT_ETC_DIR}/test-dump.hprof
+
+# test-dump-base.hprof
+art --no-compile -cp ${AHAT_TEST_DUMP_JAR} Main ${AHAT_ETC_DIR}/test-dump-base.hprof --base
+
+# test-dump.map
+cp ${AHAT_TEST_DUMP_MAP} ${AHAT_ETC_DIR}/test-dump.map
diff --git a/tools/ahat/etc/Android.bp b/tools/ahat/etc/Android.bp
new file mode 100644
index 0000000000..dd38518a36
--- /dev/null
+++ b/tools/ahat/etc/Android.bp
@@ -0,0 +1,30 @@
+//
+// Copyright (C) 2025 The Android Open Source Project
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package {
+    default_applicable_licenses: ["art_license"],
+}
+
+filegroup {
+    name: "ahat-tests-res",
+    srcs: [
+        "test-dump.hprof",
+        "test-dump-base.hprof",
+        "test-dump.map",
+        "L.hprof",
+        "O.hprof",
+        "RI.hprof",
+    ],
+}
diff --git a/tools/ahat/etc/README.txt b/tools/ahat/etc/README.txt
index e9b5b22dae..837c7ecf90 100644
--- a/tools/ahat/etc/README.txt
+++ b/tools/ahat/etc/README.txt
@@ -7,3 +7,8 @@ O.hprof
 
 RI.hprof
   A version of the test-dump hprof generated on the reference implementation.
+
+test-dump.hprof, test-dump-base.hprof, test-dump.map
+  Recent versions of the test-dump generated using ahat-test-dump-gen. See
+  comments in ahat-test-dump-gen.sh.in for more details. These will need to be
+  regenerated manually any time the test-dump source code is modified.
diff --git a/tools/ahat/etc/ahat.mf b/tools/ahat/etc/ahat.mf
index 43bc17db37..5f3de53aa7 100644
--- a/tools/ahat/etc/ahat.mf
+++ b/tools/ahat/etc/ahat.mf
@@ -1,4 +1,4 @@
 Name: ahat/
 Implementation-Title: ahat
-Implementation-Version: 1.7.3
+Implementation-Version: 1.8
 Main-Class: com.android.ahat.Main
diff --git a/tools/ahat/etc/hprofdump.py b/tools/ahat/etc/hprofdump.py
index 1c4f21b159..324157aef3 100644
--- a/tools/ahat/etc/hprofdump.py
+++ b/tools/ahat/etc/hprofdump.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright (C) 2018 The Android Open Source Project
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -17,6 +18,7 @@
 #   dumps and heap dump viewers.
 
 import time
+
 import struct
 import sys
 
@@ -24,27 +26,27 @@ filename = sys.argv[1]
 hprof = open(filename, "rb")
 
 def readu1(hprof):
-  return struct.unpack('!B', hprof.read(1))[0]
+    return struct.unpack('!B', hprof.read(1))[0]
 
 def readu2(hprof):
-  return struct.unpack('!H', hprof.read(2))[0]
+    return struct.unpack('!H', hprof.read(2))[0]
 
 def readu4(hprof):
-  return struct.unpack('!I', hprof.read(4))[0]
+    return struct.unpack('!I', hprof.read(4))[0]
 
 def readu8(hprof):
-  return struct.unpack('!Q', hprof.read(8))[0]
+    return struct.unpack('!Q', hprof.read(8))[0]
 
 def readN(n, hprof):
-  if n == 1:
-    return readu1(hprof)
-  if n == 2:
-    return readu2(hprof)
-  if n == 4:
-    return readu4(hprof)
-  if n == 8:
-    return readu8(hprof)
-  raise Exception("Unsupported size of readN: %d" % n)
+    if n == 1:
+        return readu1(hprof)
+    if n == 2:
+        return readu2(hprof)
+    if n == 4:
+        return readu4(hprof)
+    if n == 8:
+        return readu8(hprof)
+    raise Exception("Unsupported size of readN: %d" % n)
 
 TY_OBJECT = 2
 TY_BOOLEAN = 4
@@ -57,275 +59,332 @@ TY_INT = 10
 TY_LONG = 11
 
 def showty(ty):
-  if ty == TY_OBJECT:
-    return "Object"
-  if ty == TY_BOOLEAN:
-    return "boolean"
-  if ty == TY_CHAR:
-    return "char"
-  if ty == TY_FLOAT:
-    return "float"
-  if ty == TY_DOUBLE:
-    return "double"
-  if ty == TY_BYTE:
-    return "byte"
-  if ty == TY_SHORT:
-    return "short"
-  if ty == TY_INT:
-    return "int"
-  if ty == TY_LONG:
-    return "long"
-  raise Exception("Unsupported type %d" % ty)
+    if ty == TY_OBJECT:
+        return "Object"
+    if ty == TY_BOOLEAN:
+        return "boolean"
+    if ty == TY_CHAR:
+        return "char"
+    if ty == TY_FLOAT:
+        return "float"
+    if ty == TY_DOUBLE:
+        return "double"
+    if ty == TY_BYTE:
+        return "byte"
+    if ty == TY_SHORT:
+        return "short"
+    if ty == TY_INT:
+        return "int"
+    if ty == TY_LONG:
+        return "long"
+    raise Exception("Unsupported type %d" % ty)
+
+strs = {}
 
-strs = { }
 def showstr(id):
-  if id in strs:
-    return strs[id]
-  return "STR[@%x]" % id
+    if id in strs:
+        return strs[id]
+    return "STR[@%x]" % id
+
+loaded = {}
 
-loaded = { }
 def showloaded(serial):
-  if serial in loaded:
-    return showstr(loaded[serial])
-  return "SERIAL[@%x]" % serial
+    if serial in loaded:
+        return showstr(loaded[serial])
+    return "SERIAL[@%x]" % serial
 
-classobjs = { }
-def showclassobj(id):
-  if id in classobjs:
-    return "%s @%x" % (showstr(classobjs[id]), id)
-  return "@%x" % id
+classobjs = {}
 
+def showclassobj(id):
+    if id in classobjs:
+        return "%s @%x" % (showstr(classobjs[id]), id)
+    return "@%x" % id
 
-# [u1]* An initial NULL terminate series of bytes representing the format name
-# and version.
 version = ""
 c = hprof.read(1)
-while (c != '\0'):
-  version += c
-  c = hprof.read(1)
-print "Version: %s" % version
+while (c != b'\0'):
+    version += c.decode('ascii')
+    c = hprof.read(1)
+print("Version: %s" % version)
 
-# [u4] size of identifiers.
 idsize = readu4(hprof)
-print "ID Size: %d bytes" % idsize
+print("ID Size: %d bytes" % idsize)
+
 def readID(hprof):
-  return readN(idsize, hprof)
+    return readN(idsize, hprof)
 
 def valsize(ty):
-  if ty == TY_OBJECT:
-    return idsize
-  if ty == TY_BOOLEAN:
-    return 1
-  if ty == TY_CHAR:
-    return 2
-  if ty == TY_FLOAT:
-    return 4
-  if ty == TY_DOUBLE:
-    return 8
-  if ty == TY_BYTE:
-    return 1
-  if ty == TY_SHORT:
-    return 2
-  if ty == TY_INT:
-    return 4
-  if ty == TY_LONG:
-    return 8
-  raise Exception("Unsupported type %d" % ty)
+    if ty == TY_OBJECT:
+        return idsize
+    if ty == TY_BOOLEAN:
+        return 1
+    if ty == TY_CHAR:
+        return 2
+    if ty == TY_FLOAT:
+        return 4
+    if ty == TY_DOUBLE:
+        return 8
+    if ty == TY_BYTE:
+        return 1
+    if ty == TY_SHORT:
+        return 2
+    if ty == TY_INT:
+        return 4
+    if ty == TY_LONG:
+        return 8
+    raise Exception("Unsupported type %d" % ty)
 
 def readval(ty, hprof):
-  return readN(valsize(ty), hprof)
+    return readN(valsize(ty), hprof)
 
-# [u4] high word of number of ms since 0:00 GMT, 1/1/70
-# [u4] low word of number of ms since 0:00 GMT, 1/1/70
 timestamp = (readu4(hprof) << 32) | readu4(hprof)
 s, ms = divmod(timestamp, 1000)
-print "Date: %s.%03d" % (time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(s)), ms)
+print("Date: %s.%03d" % (time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(s)), ms))
 
 while hprof.read(1):
-  hprof.seek(-1,1)
-  pos = hprof.tell()
-  tag = readu1(hprof)
-  time = readu4(hprof)
-  length = readu4(hprof)
-  if tag == 0x01:
-    id = readID(hprof)
-    string = hprof.read(length - idsize)
-    print "%d: STRING %x %s" % (pos, id, repr(string))
-    strs[id] = string
-  elif tag == 0x02:
-    serial = readu4(hprof)
-    classobj = readID(hprof)
-    stack = readu4(hprof)
-    classname = readID(hprof)
-    loaded[serial] = classname
-    classobjs[classobj] = classname
-    print "LOAD CLASS #%d %s @%x stack=@%x" % (serial, showstr(classname), classobj, stack)
-  elif tag == 0x04:
-    id = readID(hprof)
-    method = readID(hprof)
-    sig = readID(hprof)
-    file = readID(hprof)
-    serial = readu4(hprof)
-    line = readu4(hprof);
-    print "STACK FRAME %d '%s' '%s' '%s' line=%d classserial=%d" % (id, showstr(method), showstr(sig), showstr(file), line, serial)
-  elif tag == 0x05:
-    serial = readu4(hprof)
-    print "STACK TRACE %d" % serial
-    thread = readu4(hprof)
-    frames = readu4(hprof)
-    hprof.read(idsize * frames)
-  elif tag == 0x06:
-    print "ALLOC SITES"
-    flags = readu2(hprof)
-    cutoff_ratio = readu4(hprof)
-    live_bytes = readu4(hprof)
-    live_insts = readu4(hprof)
-    alloc_bytes = readu8(hprof)
-    alloc_insts = readu8(hprof)
-    numsites = readu4(hprof)
-    while numsites > 0:
-      indicator = readu1(hprof)
-      class_serial = readu4(hprof)
-      stack = readu4(hprof)
-      live_bytes = readu4(hprof)
-      live_insts = readu4(hprof)
-      alloc_bytes = readu4(hprof)
-      alloc_insts = readu4(hprof)
-      numsites -= 1
-  elif tag == 0x0A:
-    thread = readu4(hprof)
-    object = readID(hprof)
-    stack = readu4(hprof)
-    name = readID(hprof)
-    group_name = readID(hprof)
-    pgroup_name = readID(hprof)
-    print "START THREAD serial=%d" % thread
-  elif tag == 0x0B:
-    thread = readu4(hprof)
-    print "END THREAD"
-  elif tag == 0x0C or tag == 0x1C:
-    if tag == 0x0C:
-      print "HEAP DUMP"
-    else:
-      print "HEAP DUMP SEGMENT"
-
-    while (length > 0):
-      subtag = readu1(hprof) ; length -= 1
-      if subtag == 0xFF:
-        print " ROOT UNKNOWN"
-        objid = readID(hprof) ; length -= idsize
-      elif subtag == 0x01:
-        print " ROOT JNI GLOBAL"
-        objid = readID(hprof) ; length -= idsize
-        ref = readID(hprof) ; length -= idsize
-      elif subtag == 0x02:
-        print " ROOT JNI LOCAL"
-        objid = readID(hprof) ; length -= idsize
-        thread = readu4(hprof) ; length -= 4
-        frame = readu4(hprof) ; length -= 4
-      elif subtag == 0x03:
-        print " ROOT JAVA FRAME"
-        objid = readID(hprof) ; length -= idsize
-        serial = readu4(hprof) ; length -= 4
-        frame = readu4(hprof) ; length -= 4
-      elif subtag == 0x04:
-        objid = readID(hprof) ; length -= idsize
-        serial = readu4(hprof) ; length -= 4
-        print " ROOT NATIVE STACK serial=%d" % serial
-      elif subtag == 0x05:
-        print " ROOT STICKY CLASS"
-        objid = readID(hprof) ; length -= idsize
-      elif subtag == 0x06:
-        print " ROOT THREAD BLOCK"
-        objid = readID(hprof) ; length -= idsize
-        thread = readu4(hprof) ; length -= 4
-      elif subtag == 0x07:
-        print " ROOT MONITOR USED"
-        objid = readID(hprof) ; length -= idsize
-      elif subtag == 0x08:
-        threadid = readID(hprof) ; length -= idsize
-        serial = readu4(hprof) ; length -= 4
-        stack = readu4(hprof) ; length -= 4
-        print " ROOT THREAD OBJECT threadid=@%x serial=%d" % (threadid, serial)
-      elif subtag == 0x20:
-        print " CLASS DUMP"
-        print "  class class object ID: %s" % showclassobj(readID(hprof)) ; length -= idsize
-        print "  stack trace serial number: #%d" % readu4(hprof) ; length -= 4
-        print "  super class object ID: @%x" % readID(hprof) ; length -= idsize
-        print "  class loader object ID: @%x" % readID(hprof) ; length -= idsize
-        print "  signers object ID: @%x" % readID(hprof) ; length -= idsize
-        print "  protection domain object ID: @%x" % readID(hprof) ; length -= idsize
-        print "  reserved: @%x" % readID(hprof) ; length -= idsize
-        print "  reserved: @%x" % readID(hprof) ; length -= idsize
-        print "  instance size (in bytes): %d" % readu4(hprof) ; length -= 4
-        print "  constant pool:"
-        poolsize = readu2(hprof) ; length -= 2
-        while poolsize > 0:
-          poolsize -= 1
-          idx = readu2(hprof) ; length -= 2
-          ty = readu1(hprof) ; length -= 1
-          val = readval(ty, hprof) ; length -= valsize(ty)
-          print "   %d %s 0x%x" % (idx, showty(ty), val)
-        numstatic = readu2(hprof) ; length -= 2
-        print "  static fields:"
-        while numstatic > 0:
-          numstatic -= 1
-          nameid = readID(hprof) ; length -= idsize
-          ty = readu1(hprof) ; length -= 1
-          val = readval(ty, hprof) ; length -= valsize(ty)
-          print "   %s %s 0x%x" % (showstr(nameid), showty(ty), val)
-        numinst = readu2(hprof) ; length -= 2
-        print "  instance fields:"
-        while numinst > 0:
-          numinst -= 1
-          nameid = readID(hprof) ; length -= idsize
-          ty = readu1(hprof) ; length -= 1
-          print "   %s %s" % (showstr(nameid), showty(ty))
-      elif subtag == 0x21:
-        print " INSTANCE DUMP:"
-        print "  object ID: @%x" % readID(hprof) ; length -= idsize
-        stack = readu4(hprof) ; length -= 4
-        print "  stack: %s" % stack
-        print "  class object ID: %s" % showclassobj(readID(hprof)) ; length -= idsize
-        datalen = readu4(hprof) ; length -= 4
-        print "  %d bytes of instance data" % datalen
-        data = hprof.read(datalen) ; length -= datalen
-      elif subtag == 0x22:
-        print " OBJECT ARRAY DUMP:"
-        print "  array object ID: @%x" % readID(hprof) ; length -= idsize
-        stack = readu4(hprof) ; length -= 4
-        print "  stack: %s" % stack
-        count = readu4(hprof) ; length -= 4
-        print "  array class object ID: %s" % showclassobj(readID(hprof)) ; length -= idsize
-        hprof.read(idsize * count) ; length -= (idsize * count)
-      elif subtag == 0x23:
-        print " PRIMITIVE ARRAY DUMP:"
-        print "  array object ID: @%x" % readID(hprof) ; length -= idsize
-        stack = readu4(hprof) ; length -= 4
-        count = readu4(hprof) ; length -= 4
-        ty = readu1(hprof) ; length -= 1
-        hprof.read(valsize(ty)*count) ; length -= (valsize(ty)*count)
-      elif subtag == 0x89:
-        print " HPROF_ROOT_INTERNED_STRING"
-        objid = readID(hprof) ; length -= idsize
-      elif subtag == 0x8b:
-        objid = readID(hprof) ; length -= idsize
-        print " HPROF ROOT DEBUGGER @%x (at offset %d)" % (objid, hprof.tell() - (idsize + 1))
-      elif subtag == 0x8d:
-        objid = readID(hprof) ; length -= idsize
-        print " HPROF ROOT VM INTERNAL @%x" % objid
-      elif subtag == 0xfe:
-        hty = readu4(hprof) ; length -= 4
-        hnameid = readID(hprof) ; length -= idsize
-        print " HPROF_HEAP_DUMP_INFO %s" % showstr(hnameid)
-      else:
-        raise Exception("TODO: subtag %x" % subtag)
-  elif tag == 0x0E:
-    flags = readu4(hprof)
-    depth = readu2(hprof)
-    print "CONTROL SETTINGS %x %d" % (flags, depth)
-  elif tag == 0x2C:
-    print "HEAP DUMP END"
-  else:
-    raise Exception("TODO: TAG %x" % tag)
+    hprof.seek(-1, 1)
+    pos = hprof.tell()
+    tag = readu1(hprof)
+    time = readu4(hprof)
+    length = readu4(hprof)
+    if tag == 0x01:
+        id = readID(hprof)
+        string = hprof.read(length - idsize)
+        print("%d: STRING %x %s" % (pos, id, repr(string)))
+        strs[id] = string.decode('latin-1')
+    elif tag == 0x02:
+        serial = readu4(hprof)
+        classobj = readID(hprof)
+        stack = readu4(hprof)
+        classname = readID(hprof)
+        loaded[serial] = classname
+        classobjs[classobj] = classname
+        print("LOAD CLASS #%d %s @%x stack=@%x" % (serial, showstr(classname), classobj, stack))
+    elif tag == 0x04:
+        id = readID(hprof)
+        method = readID(hprof)
+        sig = readID(hprof)
+        file = readID(hprof)
+        serial = readu4(hprof)
+        line = readu4(hprof)
+        print("STACK FRAME %d '%s' '%s' '%s' line=%d classserial=%d" % (
+        id, showstr(method), showstr(sig), showstr(file), line, serial))
+    elif tag == 0x05:
+        serial = readu4(hprof)
+        print("STACK TRACE %d" % serial)
+        thread = readu4(hprof)
+        frames = readu4(hprof)
+        hprof.read(idsize * frames)
+    elif tag == 0x06:
+        print("ALLOC SITES")
+        flags = readu2(hprof)
+        cutoff_ratio = readu4(hprof)
+        live_bytes = readu4(hprof)
+        live_insts = readu4(hprof)
+        alloc_bytes = readu8(hprof)
+        alloc_insts = readu8(hprof)
+        numsites = readu4(hprof)
+        while numsites > 0:
+            indicator = readu1(hprof)
+            class_serial = readu4(hprof)
+            stack = readu4(hprof)
+            live_bytes = readu4(hprof)
+            live_insts = readu4(hprof)
+            alloc_bytes = readu4(hprof)
+            alloc_insts = readu4(hprof)
+            numsites -= 1
+    elif tag == 0x0A:
+        thread = readu4(hprof)
+        object = readID(hprof)
+        stack = readu4(hprof)
+        name = readID(hprof)
+        group_name = readID(hprof)
+        pgroup_name = readID(hprof)
+        print("START THREAD serial=%d" % thread)
+    elif tag == 0x0B:
+        thread = readu4(hprof)
+        print("END THREAD")
+    elif tag == 0x0C or tag == 0x1C:
+        if tag == 0x0C:
+            print("HEAP DUMP")
+        else:
+            print("HEAP DUMP SEGMENT")
 
+        while (length > 0):
+            subtag = readu1(hprof);
+            length -= 1
+            if subtag == 0xFF:
+                print(" ROOT UNKNOWN")
+                objid = readID(hprof);
+                length -= idsize
+            elif subtag == 0x01:
+                print(" ROOT JNI GLOBAL")
+                objid = readID(hprof);
+                length -= idsize
+                ref = readID(hprof);
+                length -= idsize
+            elif subtag == 0x02:
+                print(" ROOT JNI LOCAL")
+                objid = readID(hprof);
+                length -= idsize
+                thread = readu4(hprof);
+                length -= 4
+                frame = readu4(hprof);
+                length -= 4
+            elif subtag == 0x03:
+                print(" ROOT JAVA FRAME")
+                objid = readID(hprof);
+                length -= idsize
+                serial = readu4(hprof);
+                length -= 4
+                frame = readu4(hprof);
+                length -= 4
+            elif subtag == 0x04:
+                objid = readID(hprof);
+                length -= idsize
+                serial = readu4(hprof);
+                length -= 4
+                print(" ROOT NATIVE STACK serial=%d" % serial)
+            elif subtag == 0x05:
+                print(" ROOT STICKY CLASS")
+                objid = readID(hprof);
+                length -= idsize
+            elif subtag == 0x06:
+                print(" ROOT THREAD BLOCK")
+                objid = readID(hprof);
+                length -= idsize
+                thread = readu4(hprof);
+                length -= 4
+            elif subtag == 0x07:
+                print(" ROOT MONITOR USED")
+                objid = readID(hprof);
+                length -= idsize
+            elif subtag == 0x08:
+                threadid = readID(hprof);
+                length -= idsize
+                serial = readu4(hprof);
+                length -= 4
+                stack = readu4(hprof);
+                length -= 4
+                print(" ROOT THREAD OBJECT threadid=@%x serial=%d" % (threadid, serial))
+            elif subtag == 0x20:
+                print(" CLASS DUMP")
+                print("  class class object ID: %s" % showclassobj(readID(hprof)));
+                length -= idsize
+                print("  stack trace serial number: #%d" % readu4(hprof));
+                length -= 4
+                print("  super class object ID: @%x" % readID(hprof));
+                length -= idsize
+                print("  class loader object ID: @%x" % readID(hprof));
+                length -= idsize
+                print("  signers object ID: @%x" % readID(hprof));
+                length -= idsize
+                print("  protection domain object ID: @%x" % readID(hprof));
+                length -= idsize
+                print("  reserved: @%x" % readID(hprof));
+                length -= idsize
+                print("  reserved: @%x" % readID(hprof));
+                length -= idsize
+                print("  instance size (in bytes): %d" % readu4(hprof));
+                length -= 4
+                print("  constant pool:")
+                poolsize = readu2(hprof);
+                length -= 2
+                while poolsize > 0:
+                    poolsize -= 1
+                    idx = readu2(hprof);
+                    length -= 2
+                    ty = readu1(hprof);
+                    length -= 1
+                    val = readval(ty, hprof);
+                    length -= valsize(ty)
+                    print("   %d %s 0x%x" % (idx, showty(ty), val))
+                numstatic = readu2(hprof);
+                length -= 2
+                print("  static fields:")
+                while numstatic > 0:
+                    numstatic -= 1
+                    nameid = readID(hprof);
+                    length -= idsize
+                    ty = readu1(hprof);
+                    length -= 1
+                    val = readval(ty, hprof);
+                    length -= valsize(ty)
+                    print("   %s %s 0x%x" % (showstr(nameid), showty(ty), val))
+                numinst = readu2(hprof);
+                length -= 2
+                print("  instance fields:")
+                while numinst > 0:
+                    numinst -= 1
+                    nameid = readID(hprof);
+                    length -= idsize
+                    ty = readu1(hprof);
+                    length -= 1
+                    print("   %s %s" % (showstr(nameid), showty(ty)))
+            elif subtag == 0x21:
+                print(" INSTANCE DUMP:")
+                print("  object ID: @%x" % readID(hprof));
+                length -= idsize
+                stack = readu4(hprof);
+                length -= 4
+                print("  stack: %s" % stack)
+                print("  class object ID: %s" % showclassobj(readID(hprof)));
+                length -= idsize
+                datalen = readu4(hprof);
+                length -= 4
+                print("  %d bytes of instance data" % datalen)
+                data = hprof.read(datalen);
+                length -= datalen
+            elif subtag == 0x22:
+                print(" OBJECT ARRAY DUMP:")
+                print("  array object ID: @%x" % readID(hprof));
+                length -= idsize
+                stack = readu4(hprof);
+                length -= 4
+                print("  stack: %s" % stack)
+                count = readu4(hprof);
+                length -= 4
+                print("  array class object ID: %s" % showclassobj(readID(hprof)));
+                length -= idsize
+                hprof.read(idsize * count);
+                length -= (idsize * count)
+            elif subtag == 0x23:
+                print(" PRIMITIVE ARRAY DUMP:")
+                print("  array object ID: @%x" % readID(hprof));
+                length -= idsize
+                stack = readu4(hprof);
+                length -= 4
+                count = readu4(hprof);
+                length -= 4
+                ty = readu1(hprof);
+                length -= 1
+                hprof.read(valsize(ty) * count);
+                length -= (valsize(ty) * count)
+            elif subtag == 0x89:
+                print(" HPROF_ROOT_INTERNED_STRING")
+                objid = readID(hprof);
+                length -= idsize
+            elif subtag == 0x8b:
+                objid = readID(hprof);
+                length -= idsize
+                print(" HPROF ROOT DEBUGGER @%x (at offset %d)" % (objid, hprof.tell() - (idsize + 1)))
+            elif subtag == 0x8d:
+                objid = readID(hprof);
+                length -= idsize
+                print(" HPROF ROOT VM INTERNAL @%x" % objid)
+            elif subtag == 0xfe:
+                hty = readu4(hprof);
+                length -= 4
+                hnameid = readID(hprof);
+                length -= idsize
+                print(" HPROF_HEAP_DUMP_INFO %s" % showstr(hnameid))
+            else:
+                raise Exception("TODO: subtag %x" % subtag)
+    elif tag == 0x0E:
+        flags = readu4(hprof)
+        depth = readu2(hprof)
+        print("CONTROL SETTINGS %x %d" % (flags, depth))
+    elif tag == 0x2C:
+        print("HEAP DUMP END")
+    else:
+        raise Exception("TODO: TAG %x" % tag)
diff --git a/tools/ahat/etc/test-dump-base.hprof b/tools/ahat/etc/test-dump-base.hprof
new file mode 100644
index 0000000000..87049c2ec6
Binary files /dev/null and b/tools/ahat/etc/test-dump-base.hprof differ
diff --git a/tools/ahat/etc/test-dump.hprof b/tools/ahat/etc/test-dump.hprof
new file mode 100644
index 0000000000..e539e16e00
Binary files /dev/null and b/tools/ahat/etc/test-dump.hprof differ
diff --git a/tools/ahat/etc/test-dump.map b/tools/ahat/etc/test-dump.map
new file mode 100644
index 0000000000..c956de19e8
--- /dev/null
+++ b/tools/ahat/etc/test-dump.map
@@ -0,0 +1,250 @@
+# compiler: R8
+# compiler_version: 8.10.9-dev
+# min_api: 35
+# compiler_hash: a7ad18a70460b799d0482e497c109a75bf7f91de
+# common_typos_disable
+# {"id":"com.android.tools.r8.mapping","version":"2.2"}
+# pg_map_id: edbbcc48c7b01d8c2f908b650b78ba301895bb3a321ccf9468aaf6b721e19d39
+# pg_map_hash: SHA-256 edbbcc48c7b01d8c2f908b650b78ba301895bb3a321ccf9468aaf6b721e19d39
+DumpedStuff -> DumpedStuff:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    java.lang.ref.WeakReference aShortWeakPathToSamplePathObject -> A
+    java.lang.ref.WeakReference aWeakRefToGcRoot -> B
+    java.lang.ref.SoftReference aSoftChain -> C
+    java.lang.Object[] basicStringRef -> D
+    DumpedStuff$AddedObject addedObject -> E
+      # {"id":"com.android.tools.r8.residualsignature","signature":"La;"}
+    DumpedStuff$UnchangedObject unchangedObject -> F
+      # {"id":"com.android.tools.r8.residualsignature","signature":"Lp;"}
+    DumpedStuff$RemovedObject removedObject -> G
+      # {"id":"com.android.tools.r8.residualsignature","signature":"Ln;"}
+    DumpedStuff$ModifiedObject modifiedObject -> H
+      # {"id":"com.android.tools.r8.residualsignature","signature":"Lk;"}
+    DumpedStuff$StackSmasher stackSmasher -> I
+      # {"id":"com.android.tools.r8.residualsignature","signature":"Lo;"}
+    DumpedStuff$StackSmasher stackSmasherAdded -> J
+      # {"id":"com.android.tools.r8.residualsignature","signature":"Lo;"}
+    int[] modifiedArray -> K
+    java.lang.Object objectAllocatedAtKnownSite -> L
+    java.lang.Object objectAllocatedAtKnownSubSite -> M
+    android.os.IBinder correctBinderProxy -> N
+      # {"id":"com.android.tools.r8.residualsignature","signature":"La/c;"}
+    android.os.IBinder imposedBinderProxy -> O
+      # {"id":"com.android.tools.r8.residualsignature","signature":"La/c;"}
+    android.os.IBinder carriedBinderProxy -> P
+      # {"id":"com.android.tools.r8.residualsignature","signature":"La/c;"}
+    java.lang.Object correctBinderProxyObject -> Q
+    java.lang.Object impostorBinderProxyObject -> R
+    java.lang.Object carrierBinderProxyObject -> S
+    java.lang.Object binderService -> T
+    java.lang.Object fakeBinderService -> U
+    java.lang.Object binderToken -> V
+    java.lang.Object namedBinderToken -> W
+    java.lang.Object unreachableAnchor -> X
+    java.lang.String modifiedStaticField -> Y
+    java.lang.String basicString -> d
+    java.lang.String nonAscii -> e
+    java.lang.String embeddedZero -> f
+    char[] charArray -> g
+    byte[] byteString -> h
+    byte[] byteNotString -> i
+    byte[] byteEmpty -> j
+    java.lang.String nullString -> k
+    java.lang.Object anObject -> l
+    java.lang.Object aCleanedObject -> m
+    java.lang.Runnable aCleanerThunk -> n
+    DumpedStuff$Reference aReference -> o
+      # {"id":"com.android.tools.r8.residualsignature","signature":"Lm;"}
+    java.lang.ref.ReferenceQueue referenceQueue -> p
+    java.lang.ref.PhantomReference aPhantomReference -> q
+    java.lang.ref.WeakReference aWeakReference -> r
+    java.lang.ref.WeakReference aNullReferentReference -> s
+    java.lang.ref.SoftReference aSoftReference -> t
+    DumpedStuff$Reference reachabilityReferenceChain -> u
+      # {"id":"com.android.tools.r8.residualsignature","signature":"Lm;"}
+    byte[] bigArray -> v
+    android.graphics.Bitmap bitmapOne -> w
+      # {"id":"com.android.tools.r8.residualsignature","signature":"Landroid/graphics/b;"}
+    android.graphics.Bitmap bitmapTwo -> x
+      # {"id":"com.android.tools.r8.residualsignature","signature":"Landroid/graphics/b;"}
+    DumpedStuff$ObjectTree[] gcPathArray -> y
+      # {"id":"com.android.tools.r8.residualsignature","signature":"[Ll;"}
+    DumpedStuff$Reference aLongStrongPathToSamplePathObject -> z
+      # {"id":"com.android.tools.r8.residualsignature","signature":"Lm;"}
+    1:6:void <clinit>():256:256 -> <clinit>
+    1:1:void <init>(boolean):45:45 -> <init>
+    2:11:void <init>(boolean):187:196 -> <init>
+    12:17:void <init>(boolean):198:203 -> <init>
+    18:20:void <init>(boolean):206:208 -> <init>
+    21:22:void <init>(boolean):215:216 -> <init>
+    23:23:void <init>(boolean):219:219 -> <init>
+    24:29:void <init>(boolean):228:233 -> <init>
+    30:33:void <init>(boolean):235:238 -> <init>
+    34:34:void <init>(boolean):240:240 -> <init>
+    35:35:void <init>(boolean):46:46 -> <init>
+    36:36:void <init>(boolean):49:49 -> <init>
+    37:37:void <init>(boolean):51:51 -> <init>
+    38:40:void <init>(boolean):55:57 -> <init>
+    41:42:void <init>(boolean):60:61 -> <init>
+    43:45:void <init>(boolean):65:67 -> <init>
+    46:53:void <init>(boolean):70:77 -> <init>
+    54:56:void <init>(boolean):82:84 -> <init>
+    57:59:void <init>(boolean):87:89 -> <init>
+    60:60:void <init>(boolean):93:93 -> <init>
+    61:62:void <init>(boolean):95:96 -> <init>
+    1:8:void allocateObjectAtOverriddenSite():42:42 -> b
+    1:7:void allocateObjectAtKnownSite():30:30 -> c
+    8:10:void allocateObjectAtKnownSite():31:31 -> c
+    11:13:void allocateObjectAtKnownSite():32:32 -> c
+    14:16:void allocateObjectAtKnownSite():33:33 -> c
+    17:20:void allocateObjectAtKnownSite():34:34 -> c
+    1:8:void allocateObjectAtKnownSubSite():38:38 -> d
+    1:39:void shouldNotGc():245:245 -> e
+    40:47:void shouldNotGc():252:252 -> e
+DumpedStuff$AddedObject -> a:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    1:4:void <init>():109:109 -> <init>
+DumpedStuff$BinderProxyCarrier -> b:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    android.os.IBinder mRemote -> a
+      # {"id":"com.android.tools.r8.residualsignature","signature":"La/c;"}
+    1:3:void <init>(android.os.IBinder):164:164 -> <init>
+      # {"id":"com.android.tools.r8.residualsignature","signature":"(La/c;)V"}
+    4:6:void <init>(android.os.IBinder):165:165 -> <init>
+DumpedStuff$BinderService -> c:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    1:1:void <init>(DumpedStuff-IA):0:0 -> <init>
+      # {"id":"com.android.tools.r8.synthesized"}
+      # {"id":"com.android.tools.r8.residualsignature","signature":"(Lr;)V"}
+    2:2:void <init>():169:169 -> <init>
+DumpedStuff$FakeBinderService -> d:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    1:1:void <init>(DumpedStuff-IA):0:0 -> <init>
+      # {"id":"com.android.tools.r8.synthesized"}
+      # {"id":"com.android.tools.r8.residualsignature","signature":"(Lr;)V"}
+    2:2:void <init>():173:173 -> <init>
+DumpedStuff$IBinderInterfaceImpostor -> g:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+DumpedStuff$IBinderInterfaceImpostor$Stub -> f:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    1:4:void <init>():152:152 -> <init>
+DumpedStuff$IBinderInterfaceImpostor$Stub$Proxy -> e:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    android.os.IBinder mFakeRemote -> a
+      # {"id":"com.android.tools.r8.residualsignature","signature":"La/c;"}
+    1:3:void <init>(android.os.IBinder):155:155 -> <init>
+      # {"id":"com.android.tools.r8.residualsignature","signature":"(La/c;)V"}
+    4:8:void <init>(android.os.IBinder):154:154 -> <init>
+    9:11:void <init>(android.os.IBinder):156:156 -> <init>
+DumpedStuff$IDumpedManager -> j:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+DumpedStuff$IDumpedManager$Stub -> i:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    java.lang.String DESCRIPTOR -> b
+    1:6:void <init>():140:140 -> <init>
+DumpedStuff$IDumpedManager$Stub$Proxy -> h:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    android.os.IBinder mRemote -> a
+      # {"id":"com.android.tools.r8.residualsignature","signature":"La/c;"}
+    1:3:void <init>(android.os.IBinder):144:144 -> <init>
+      # {"id":"com.android.tools.r8.residualsignature","signature":"(La/c;)V"}
+    4:6:void <init>(android.os.IBinder):145:145 -> <init>
+DumpedStuff$ModifiedObject -> k:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    int value -> a
+    java.lang.String modifiedRefField -> b
+    java.lang.String unmodifiedRefField -> c
+    1:4:void <init>():118:118 -> <init>
+DumpedStuff$ObjectTree -> l:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    DumpedStuff$ObjectTree left -> a
+      # {"id":"com.android.tools.r8.residualsignature","signature":"Ll;"}
+    DumpedStuff$ObjectTree right -> b
+      # {"id":"com.android.tools.r8.residualsignature","signature":"Ll;"}
+    1:3:void <init>(DumpedStuff$ObjectTree,DumpedStuff$ObjectTree):103:103 -> <init>
+      # {"id":"com.android.tools.r8.residualsignature","signature":"(Ll;Ll;)V"}
+    4:5:void <init>(DumpedStuff$ObjectTree,DumpedStuff$ObjectTree):104:104 -> <init>
+    6:8:void <init>(DumpedStuff$ObjectTree,DumpedStuff$ObjectTree):105:105 -> <init>
+DumpedStuff$Reference -> m:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    java.lang.Object referent -> a
+    1:3:void <init>(java.lang.Object):131:131 -> <init>
+    4:6:void <init>(java.lang.Object):132:132 -> <init>
+DumpedStuff$RemovedObject -> n:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    1:4:void <init>():112:112 -> <init>
+DumpedStuff$StackSmasher -> o:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    DumpedStuff$StackSmasher child -> a
+      # {"id":"com.android.tools.r8.residualsignature","signature":"Lo;"}
+    1:4:void <init>():124:124 -> <init>
+DumpedStuff$UnchangedObject -> p:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    1:4:void <init>():115:115 -> <init>
+DumpedStuff$Unreachable -> q:
+# {"id":"sourceFile","fileName":"DumpedStuff.java"}
+    java.lang.Object anchor -> a
+    java.lang.Object self -> b
+    1:3:void <init>(java.lang.Object):181:181 -> <init>
+    4:5:void <init>(java.lang.Object):182:182 -> <init>
+    6:8:void <init>(java.lang.Object):183:183 -> <init>
+DumpedStuff-IA -> r:
+# {"id":"sourceFile","fileName":"R8$$SyntheticClass"}
+# {"id":"com.android.tools.r8.synthesized"}
+Main -> Main:
+# {"id":"sourceFile","fileName":"Main.java"}
+    DumpedStuff stuff -> a
+    1:4:void <init>():24:24 -> <init>
+    1:2:void main(java.lang.String[]):30:31 -> main
+    3:3:void main(java.lang.String[]):34:34 -> main
+    4:4:void main(java.lang.String[]):38:38 -> main
+    5:5:void main(java.lang.String[]):41:41 -> main
+    6:6:void main(java.lang.String[]):44:44 -> main
+    7:7:void main(java.lang.String[]):48:48 -> main
+    8:8:void main(java.lang.String[]):53:53 -> main
+    9:9:void main(java.lang.String[]):56:56 -> main
+    10:11:void main(java.lang.String[]):59:60 -> main
+SuperDumpedStuff -> SuperDumpedStuff:
+# {"id":"sourceFile","fileName":"SuperDumpedStuff.java"}
+    java.lang.Object objectAllocatedAtObfSuperSite -> a
+    java.lang.Object objectAllocatedAtUnObfSuperSite -> b
+    java.lang.Object objectAllocatedAtOverriddenSite -> c
+    1:4:void <init>():19:19 -> <init>
+    1:8:void allocateObjectAtObfSuperSite():22:22 -> a
+    1:8:void allocateObjectAtUnObfSuperSite():26:26 -> allocateObjectAtUnObfSuperSite
+    1:8:void allocateObjectAtOverriddenSite():30:30 -> b
+android.graphics.Bitmap -> android.graphics.b:
+# {"id":"sourceFile","fileName":"Bitmap.java"}
+    long mNativePtr -> a
+    int mWidth -> b
+    int mHeight -> c
+    android.graphics.Bitmap$DumpData dumpData -> d
+      # {"id":"com.android.tools.r8.residualsignature","signature":"Landroid/graphics/a;"}
+    1:11:void <clinit>():59:59 -> <clinit>
+    1:5:void <init>(int,int,long,byte[]):29:33 -> <init>
+android.graphics.Bitmap$DumpData -> android.graphics.a:
+# {"id":"sourceFile","fileName":"Bitmap.java"}
+    int format -> a
+    long[] natives -> b
+    byte[][] buffers -> c
+    int max -> d
+    int count -> e
+    1:3:void <init>(int,int):43:43 -> <init>
+    4:5:void <init>(int,int):44:44 -> <init>
+    6:7:void <init>(int,int):45:45 -> <init>
+    8:11:void <init>(int,int):46:46 -> <init>
+    12:16:void <init>(int,int):47:47 -> <init>
+    17:19:void <init>(int,int):48:48 -> <init>
+    1:6:void add(long,byte[]):52:52 -> a
+    7:10:void add(long,byte[]):53:53 -> a
+    11:20:void add(long,byte[]):54:54 -> a
+android.os.Binder -> a.a:
+# {"id":"sourceFile","fileName":"Binder.java"}
+    java.lang.String mDescriptor -> a
+    1:2:void <init>():37:38 -> <init>
+    3:4:void <init>(java.lang.String):41:42 -> <init>
+android.os.BinderProxy -> a.b:
+# {"id":"sourceFile","fileName":"BinderProxy.java"}
+    1:4:void <init>():20:20 -> <init>
+android.os.IBinder -> a.c:
+# {"id":"sourceFile","fileName":"IBinder.java"}
diff --git a/tools/ahat/src/main/com/android/ahat/ArrayHandler.java b/tools/ahat/src/main/com/android/ahat/ArrayHandler.java
new file mode 100644
index 0000000000..4a9f379d57
--- /dev/null
+++ b/tools/ahat/src/main/com/android/ahat/ArrayHandler.java
@@ -0,0 +1,66 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.android.ahat;
+
+import com.android.ahat.heapdump.AhatInstance;
+import com.android.ahat.heapdump.AhatSnapshot;
+import com.sun.net.httpserver.HttpExchange;
+import com.sun.net.httpserver.HttpHandler;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.PrintStream;
+
+class ArrayHandler implements HttpHandler {
+  private AhatSnapshot mSnapshot;
+
+  public ArrayHandler(AhatSnapshot snapshot) {
+    mSnapshot = snapshot;
+  }
+
+  @Override
+  public void handle(HttpExchange exchange) throws IOException {
+    try {
+      Query query = new Query(exchange.getRequestURI());
+      long id = query.getLong("id", 0);
+      AhatInstance inst = mSnapshot.findInstance(id);
+      byte[] bytes = inst.asByteArray();
+
+      if (bytes == null) {
+          exchange.getResponseHeaders().add("Content-Type", "text/html");
+          exchange.sendResponseHeaders(404, 0);
+          PrintStream ps = new PrintStream(exchange.getResponseBody());
+          HtmlDoc doc = new HtmlDoc(ps, DocString.text("ahat"), DocString.uri("style.css"));
+          doc.big(DocString.text("No byte[] found for the given request."));
+          doc.close();
+          return;
+      }
+
+      exchange.getResponseHeaders().add("Content-Disposition",
+          String.format("attachment; filename=\"array-0x%08x.bin\"", id));
+      exchange.sendResponseHeaders(200, 0);
+      OutputStream os = exchange.getResponseBody();
+      os.write(bytes);
+      os.close();
+    } catch (RuntimeException e) {
+      // Print runtime exceptions to standard error for debugging purposes,
+      // because otherwise they are swallowed and not reported.
+      System.err.println("Exception when handling " + exchange.getRequestURI() + ": ");
+      e.printStackTrace();
+      throw e;
+    }
+  }
+}
diff --git a/tools/ahat/src/main/com/android/ahat/Main.java b/tools/ahat/src/main/com/android/ahat/Main.java
index 586f95ea8d..329f69e700 100644
--- a/tools/ahat/src/main/com/android/ahat/Main.java
+++ b/tools/ahat/src/main/com/android/ahat/Main.java
@@ -188,6 +188,7 @@ public class Main {
     server.createContext("/objects", new AhatHttpHandler(new ObjectsHandler(ahat)));
     server.createContext("/site", new AhatHttpHandler(new SiteHandler(ahat)));
     server.createContext("/bitmap", new BitmapHandler(ahat));
+    server.createContext("/array", new ArrayHandler(ahat));
     server.createContext("/style.css", new StaticHandler("etc/style.css", "text/css"));
     server.setExecutor(Executors.newFixedThreadPool(1));
     System.out.println("Server started on http://localhost:" + port);
diff --git a/tools/ahat/src/main/com/android/ahat/ObjectHandler.java b/tools/ahat/src/main/com/android/ahat/ObjectHandler.java
index 4d85cc0c84..c8677cf40b 100644
--- a/tools/ahat/src/main/com/android/ahat/ObjectHandler.java
+++ b/tools/ahat/src/main/com/android/ahat/ObjectHandler.java
@@ -67,9 +67,7 @@ class ObjectHandler implements AhatHandler {
 
     printAllocationSite(doc, query, inst);
 
-    if (!inst.isUnreachable()) {
-      printGcRootPath(doc, query, inst);
-    }
+    printSamplePath(doc, query, inst);
 
     doc.section("Object Info");
     AhatClassObj cls = inst.getClassObj();
@@ -128,6 +126,12 @@ class ObjectHandler implements AhatHandler {
         new Column("Value"),
         new Column("", Column.Align.LEFT, diff));
 
+    if (array.hasByteArray()) {
+      doc.println(
+        DocString.link(DocString.uri("array?id=" + array.getId()),
+        DocString.text(" Download bytes")));
+    }
+
     List<Value> elements = array.getValues();
     SubsetSelector<Value> selector = new SubsetSelector(query, ARRAY_ELEMENTS_ID, elements);
     int i = 0;
@@ -252,13 +256,17 @@ class ObjectHandler implements AhatHandler {
     }
   }
 
-  private void printGcRootPath(Doc doc, Query query, AhatInstance inst) {
-    doc.section("Sample Path from GC Root");
-    List<PathElement> path = inst.getPathFromGcRoot();
+  private void printSamplePath(Doc doc, Query query, AhatInstance inst) {
+    List<PathElement> path = inst.getSamplePath();
 
     // Add a fake PathElement as a marker for the root.
     final PathElement root = new PathElement(null, null);
-    path.add(0, root);
+    if (inst.isUnreachable()) {
+      doc.section("Sample Path");
+    } else {
+      doc.section("Sample Path from GC Root");
+      path.add(0, root);
+    }
 
     HeapTable.TableConfig<PathElement> table = new HeapTable.TableConfig<PathElement>() {
       public String getHeapsDescription() {
diff --git a/tools/ahat/src/main/com/android/ahat/ObjectsHandler.java b/tools/ahat/src/main/com/android/ahat/ObjectsHandler.java
index 4cdbaf4270..aaa16ce29d 100644
--- a/tools/ahat/src/main/com/android/ahat/ObjectsHandler.java
+++ b/tools/ahat/src/main/com/android/ahat/ObjectsHandler.java
@@ -125,7 +125,8 @@ class ObjectsHandler implements AhatHandler {
       SubsetSelector<AhatInstance> selector = new SubsetSelector(query, OBJECTS_ID, insts);
       for (AhatInstance inst : selector.selected()) {
         AhatInstance base = inst.getBaseline();
-        SizeTable.row(doc, inst.getSize(), base.getSize(),
+        SizeTable.row(doc,
+            inst.getTotalRetainedSize(), base.getTotalRetainedSize(),
             DocString.text(inst.getHeap().getName()),
             Summarizer.summarize(inst));
       }
@@ -134,4 +135,3 @@ class ObjectsHandler implements AhatHandler {
     }
   }
 }
-
diff --git a/tools/ahat/src/main/com/android/ahat/heapdump/AhatArrayInstance.java b/tools/ahat/src/main/com/android/ahat/heapdump/AhatArrayInstance.java
index dbf4c7e70e..1c8907a2d8 100644
--- a/tools/ahat/src/main/com/android/ahat/heapdump/AhatArrayInstance.java
+++ b/tools/ahat/src/main/com/android/ahat/heapdump/AhatArrayInstance.java
@@ -400,7 +400,8 @@ public class AhatArrayInstance extends AhatInstance {
     return String.format("%s[%d]@%08x", className, mValues.size(), getId());
   }
 
-  byte[] asByteArray() {
+  @Override
+  public byte[] asByteArray() {
     return mByteArray;
   }
 }
diff --git a/tools/ahat/src/main/com/android/ahat/heapdump/AhatBitmapInstance.java b/tools/ahat/src/main/com/android/ahat/heapdump/AhatBitmapInstance.java
index 30631784ab..c8a48dd08a 100644
--- a/tools/ahat/src/main/com/android/ahat/heapdump/AhatBitmapInstance.java
+++ b/tools/ahat/src/main/com/android/ahat/heapdump/AhatBitmapInstance.java
@@ -17,16 +17,15 @@
 package com.android.ahat.heapdump;
 
 import java.awt.image.BufferedImage;
-import java.util.Arrays;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
 import java.util.Set;
-import java.util.stream.Collectors;
-import com.google.common.collect.TreeMultimap;
+import java.util.TreeMap;
 
 /**
  * A java object that has `android.graphics.Bitmap` as its base class.
@@ -66,14 +65,14 @@ public class AhatBitmapInstance extends AhatClassInstance implements Comparable<
     private int format;
     private Map<Long, byte[]> buffers;
     private Set<Long> referenced;
-    private TreeMultimap<BitmapInfo, AhatBitmapInstance> instances;
+    private Map<BitmapInfo, List<AhatBitmapInstance>> instances;
 
     BitmapDumpData(int count, int format) {
       this.count = count;
       this.format = format;
       this.buffers = new HashMap<Long, byte[]>(count);
       this.referenced = new HashSet<Long>(count);
-      this.instances = TreeMultimap.create();
+      this.instances = new TreeMap<>();
     }
   };
 
@@ -124,16 +123,15 @@ public class AhatBitmapInstance extends AhatClassInstance implements Comparable<
       AhatBitmapInstance bmp = obj.asBitmapInstance();
       if (bmp != null) {
         BitmapInfo info = bmp.getBitmapInfo(result);
-        if (info != null) {
-          result.instances.put(info, bmp);
+
+        // Avoid adding instances referenced from BitmapDumpData. These
+        // instances shall *not* be counted.
+        if (info != null && !result.referenced.contains(bmp.getId())) {
+          result.instances.computeIfAbsent(info, k -> new ArrayList<>()).add(bmp);
         }
       }
     }
 
-    /* remove all instances referenced from BitmapDumpData,
-     * these instances shall *not* be counted
-     */
-    instances.removeIf(i -> { return result.referenced.contains(i.getId()); });
     return result;
   }
 
@@ -182,10 +180,13 @@ public class AhatBitmapInstance extends AhatClassInstance implements Comparable<
    */
   public static List<List<AhatBitmapInstance>> findDuplicates(BitmapDumpData bitmapDumpData) {
     if (bitmapDumpData != null) {
-      return bitmapDumpData.instances.keySet().stream()
-          .filter(k -> bitmapDumpData.instances.get(k).size() > 1)
-          .map(k -> new ArrayList<>(bitmapDumpData.instances.get(k)))
-          .collect(Collectors.toList());
+      List<List<AhatBitmapInstance>> duplicates = new ArrayList<>();
+      for (List<AhatBitmapInstance> values : bitmapDumpData.instances.values()) {
+        if (values.size() > 1) {
+          duplicates.add(new ArrayList<>(values));
+        }
+      }
+      return duplicates;
     }
     return null;
   }
diff --git a/tools/ahat/src/main/com/android/ahat/heapdump/AhatInstance.java b/tools/ahat/src/main/com/android/ahat/heapdump/AhatInstance.java
index 8358553722..e42453e41f 100644
--- a/tools/ahat/src/main/com/android/ahat/heapdump/AhatInstance.java
+++ b/tools/ahat/src/main/com/android/ahat/heapdump/AhatInstance.java
@@ -616,10 +616,10 @@ public abstract class AhatInstance implements Diffable<AhatInstance> {
 
   /**
    * Returns a sample path from a GC root to this instance. The first element
-   * of the returned path is a GC root object. This instance is included as
-   * the last element of the path with an empty field description.
+   * of the returned path is a GC root object. The last element of the
+   * returned path is 'this' with an empty field description.
    * <p>
-   * If the instance is strongly reachable, a path of string references will
+   * If the instance is strongly reachable, a path of strong references will
    * be returned. If the instance is weakly reachable, the returned path will
    * include a soft/weak/phantom/finalizer reference somewhere along it.
    * Returns null if this instance is not reachable.
@@ -631,7 +631,18 @@ public abstract class AhatInstance implements Diffable<AhatInstance> {
     if (isUnreachable()) {
       return null;
     }
+    return getSamplePath();
+  }
 
+  /**
+   * Returns a sample path to this instance.
+   * If the instance is reachable, this returns a path from a GC root.
+   * Otherwise this returns an arbitrary path leading to the instance.
+   *
+   * @return sample path to this instance
+   * @see PathElement
+   */
+  public List<PathElement> getSamplePath() {
     List<PathElement> path = new ArrayList<PathElement>();
 
     AhatInstance dom = this;
@@ -653,7 +664,7 @@ public abstract class AhatInstance implements Diffable<AhatInstance> {
    * Returns null if the given instance has no next instance to the gc root.
    */
   private static PathElement getNextPathElementToGcRoot(AhatInstance inst) {
-    if (inst.isRoot()) {
+    if (inst.isRoot() || inst.mNextInstanceToGcRoot == null) {
       return null;
     }
     return new PathElement(inst.mNextInstanceToGcRoot, inst.mNextInstanceToGcRootField);
@@ -675,10 +686,17 @@ public abstract class AhatInstance implements Diffable<AhatInstance> {
    * Read the byte[] value from an hprof Instance.
    * Returns null if the instance is not a byte array.
    */
-  byte[] asByteArray() {
+  public byte[] asByteArray() {
     return null;
   }
 
+  /**
+   * Whether this array instance has an underlying byte array.
+   */
+  public boolean hasByteArray() {
+    return asByteArray() != null;
+  }
+
   void setBaseline(AhatInstance baseline) {
     mBaseline = baseline;
   }
@@ -707,19 +725,20 @@ public abstract class AhatInstance implements Diffable<AhatInstance> {
   }
 
   /**
-   * Determine the reachability of the all instances reachable from the given
-   * root instance. Initializes the following fields:
+   * Determine the reachability of instances.
+   * Initializes the following fields:
    *   mReachability
    *   mNextInstanceToGcRoot
    *   mNextInstanceToGcRootField
    *   mReverseReferences
    *
+   * @param root root used for determining which instances are reachable.
+   * @param insts the list of all instances.
    * @param progress used to track progress of the traversal.
-   * @param numInsts upper bound on the total number of instances reachable
-   *                 from the root, solely used for the purposes of tracking
-   *                 progress.
+   * @param numInsts the number of instances, for tracking progress.
    */
-  static void computeReachability(SuperRoot root, Progress progress, long numInsts) {
+  static void computeReachability(
+      SuperRoot root, Iterable<AhatInstance> insts, Progress progress, long numInsts) {
     // Start by doing a breadth first search through strong references.
     // Then continue the breadth first through each weaker kind of reference.
     progress.start("Computing reachability", numInsts);
@@ -761,6 +780,30 @@ public abstract class AhatInstance implements Diffable<AhatInstance> {
         }
       }
     }
+
+    // Initialize reachability related fields for unreachable instances,
+    // just in case people want to explore more about where unreachable
+    // instances come from.
+    for (AhatInstance inst : insts) {
+      if (inst.isUnreachable()) {
+        progress.advance();
+        for (Reference ref : inst.getReferences()) {
+          if (ref.ref.mReverseReferences == null) {
+            ref.ref.mReverseReferences = new ArrayList<AhatInstance>();
+          }
+          ref.ref.mReverseReferences.add(ref.src);
+
+          // An unreachable instance doesn't have a path to GC root, but it's
+          // still useful to see a sample path of who is referencing the
+          // object. To avoid introducing cycles in the sample path, we force
+          // the sample paths to have objects in increasing id order.
+          if (ref.ref.mNextInstanceToGcRoot == null && ref.src.mId < ref.ref.mId) {
+            ref.ref.mNextInstanceToGcRoot = ref.src;
+            ref.ref.mNextInstanceToGcRootField = ref.field;
+          }
+        }
+      }
+    }
     progress.done();
   }
 
diff --git a/tools/ahat/src/main/com/android/ahat/heapdump/AhatSnapshot.java b/tools/ahat/src/main/com/android/ahat/heapdump/AhatSnapshot.java
index 1c36448337..63b3774b9a 100644
--- a/tools/ahat/src/main/com/android/ahat/heapdump/AhatSnapshot.java
+++ b/tools/ahat/src/main/com/android/ahat/heapdump/AhatSnapshot.java
@@ -50,7 +50,7 @@ public class AhatSnapshot implements Diffable<AhatSnapshot> {
     mHeaps = heaps;
     mRootSite = rootSite;
 
-    AhatInstance.computeReachability(mSuperRoot, progress, mInstances.size());
+    AhatInstance.computeReachability(mSuperRoot, mInstances, progress, mInstances.size());
 
     mBitmapDumpData = AhatBitmapInstance.findBitmapDumpData(mSuperRoot, mInstances);
 
@@ -65,7 +65,9 @@ public class AhatSnapshot implements Diffable<AhatSnapshot> {
       }
 
       if (retained == Reachability.UNREACHABLE && inst.isUnreachable()) {
-        mSuperRoot.addRoot(inst);
+        if (inst.getSamplePath().size() == 1) {
+          mSuperRoot.addRoot(inst);
+        }
       }
     }
 
diff --git a/tools/ahat/src/test-dump/DumpedStuff.java b/tools/ahat/src/test-dump/DumpedStuff.java
index e1f97fa895..19f6524122 100644
--- a/tools/ahat/src/test-dump/DumpedStuff.java
+++ b/tools/ahat/src/test-dump/DumpedStuff.java
@@ -174,6 +174,16 @@ public class DumpedStuff extends SuperDumpedStuff {
     // Intentionally empty
   };
 
+  private static class Unreachable {
+    public Object anchor;
+    public Object self;
+
+    public Unreachable(Object anchor) {
+      this.self = this;
+      this.anchor = anchor;
+    }
+  }
+
   public String basicString = "hello, world";
   public String nonAscii = "Sigma () is not ASCII";
   public String embeddedZero = "embedded\0...";  // Non-ASCII for string compression purposes.
@@ -227,6 +237,8 @@ public class DumpedStuff extends SuperDumpedStuff {
   Object binderToken = new android.os.Binder();
   Object namedBinderToken = new android.os.Binder("awesomeToken");
 
+  Object unreachableAnchor = new Object();
+
   // Allocate those objects that we need to not be GC'd before taking the heap
   // dump.
   public void shouldNotGc() {
@@ -236,6 +248,8 @@ public class DumpedStuff extends SuperDumpedStuff {
         new WeakReference(
         new SoftReference(
         new PhantomReference(new Object(), referenceQueue))))));
+
+    new Unreachable(unreachableAnchor);
   }
 
   static {
diff --git a/tools/ahat/src/test/com/android/ahat/InstanceTest.java b/tools/ahat/src/test/com/android/ahat/InstanceTest.java
index b3b970eb66..bc6324fa21 100644
--- a/tools/ahat/src/test/com/android/ahat/InstanceTest.java
+++ b/tools/ahat/src/test/com/android/ahat/InstanceTest.java
@@ -29,10 +29,11 @@ import java.io.IOException;
 import java.util.List;
 import org.junit.Test;
 
-import static org.junit.Assert.assertNotEquals;
+import static org.junit.Assert.assertArrayEquals;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertNotEquals;
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertTrue;
 
@@ -216,6 +217,30 @@ public class InstanceTest {
     assertNull(obj.asString());
   }
 
+  @Test
+  public void asByteArrayNotByteArray() throws IOException {
+    TestDump dump = TestDump.getTestDump();
+    AhatInstance obj = dump.getDumpedAhatInstance("anObject");
+    assertNotNull(obj);
+    assertNull(obj.asByteArray());
+  }
+
+  @Test
+  public void asByteArrayIsEmptyByteArray() throws IOException {
+    TestDump dump = TestDump.getTestDump();
+    AhatInstance obj = dump.getDumpedAhatInstance("byteEmpty");
+    assertNotNull(obj);
+    assertArrayEquals(obj.asByteArray(), new byte[] {});
+  }
+
+  @Test
+  public void asByteArrayIsSomeByteArray() throws IOException {
+    TestDump dump = TestDump.getTestDump();
+    AhatInstance obj = dump.getDumpedAhatInstance("byteNotString");
+    assertNotNull(obj);
+    assertArrayEquals(obj.asByteArray(), new byte[] {0, 1, 2, 3, 4, 5});
+  }
+
   @Test
   public void basicReference() throws IOException {
     TestDump dump = TestDump.getTestDump();
diff --git a/tools/ahat/src/test/com/android/ahat/ObjectHandlerTest.java b/tools/ahat/src/test/com/android/ahat/ObjectHandlerTest.java
index 1b8a781e0c..b29f9716bc 100644
--- a/tools/ahat/src/test/com/android/ahat/ObjectHandlerTest.java
+++ b/tools/ahat/src/test/com/android/ahat/ObjectHandlerTest.java
@@ -16,12 +16,19 @@
 
 package com.android.ahat;
 
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertTrue;
+
 import com.android.ahat.heapdump.AhatInstance;
 import com.android.ahat.heapdump.AhatSnapshot;
-import java.io.IOException;
+import com.android.ahat.heapdump.Site;
+
 import org.junit.Test;
 
-import static org.junit.Assert.assertNotNull;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
 
 public class ObjectHandlerTest {
   @Test
@@ -71,4 +78,31 @@ public class ObjectHandlerTest {
     AhatHandler handler = new ObjectHandler(dump.getAhatSnapshot());
     TestHandler.testNoCrash(handler, "http://localhost:7100/object?id=" + object.getId());
   }
+
+  @Test
+  public void noCrashUnreachable() throws IOException {
+    // Exercise the case where the object is unreachable.
+    // We had bugs in the past where trying to print the sample path for any
+    // unreachable instance would lead to an infinite loop or null pointer
+    // exception.
+
+    // Our unreachable object should be the only reference to the
+    // unreachableAnchor instance, aside from dumpedStuff.
+    TestDump dump = TestDump.getTestDump();
+
+    AhatInstance anchor = dump.getDumpedAhatInstance("unreachableAnchor");
+    assertNotNull(anchor);
+
+    List<AhatInstance> reverse = anchor.getReverseReferences();
+    assertEquals(2, reverse.size());
+
+    AhatInstance unreachable = reverse.get(0);
+    if (!unreachable.isUnreachable()) {
+      unreachable = reverse.get(1);
+    }
+    assertTrue(unreachable.isUnreachable());
+
+    AhatHandler handler = new ObjectHandler(dump.getAhatSnapshot());
+    TestHandler.testNoCrash(handler, "http://localhost:7100/object?id=" + unreachable.getId());
+  }
 }
diff --git a/tools/art b/tools/art
index 47724de9f9..f25ac49492 100755
--- a/tools/art
+++ b/tools/art
@@ -127,34 +127,6 @@ find_cp_in_args() {
   done
 }
 
-# Delete the 'oat' directories relative to the classpath's dex files.
-# e.g. (foo/classes.dex bar/classes.dex) would delete (foo/oat bar/oat) directories.
-cleanup_oat_directory() {
-  local classpath
-  classpath=("$@")
-
-  local dirpath
-
-  for path in "${classpath[@]}"; do
-    dirpath="$(dirname "$path")"
-    [[ -d "$dirpath" ]] && verbose_run rm -rf "$dirpath/oat"
-  done
-}
-
-# Parse -cp <CP>, -classpath <CP>, and $CLASSPATH to find the dex files.
-# Each dex file's directory will have an 'oat' file directory, delete it.
-# Input: Command line arguments to the art script.
-# e.g. -cp foo/classes.dex:bar/classes.dex would delete (foo/oat bar/oat) directories.
-cleanup_oat_directory_for_classpath() {
-  if [ "$CLEAN_OAT_FILES" = "yes" ]; then
-    # First try: Use $CLASSPATH environment variable.
-    parse_classpath "$CLASSPATH"
-    # Second try: Look for latest -cp or -classpath arg which will take precedence.
-    find_cp_in_args "$@"
-
-    cleanup_oat_directory "${PARSE_CLASSPATH_RESULT[@]}"
-  fi
-}
 
 # Attempt to find $ANDROID_ROOT/framework/<isa>/core.art' without knowing what <isa> is.
 function check_if_boot_image_file_exists() {
@@ -198,11 +170,18 @@ function run_dex2oat() {
     done
     # Create oat file directory.
     verbose_run mkdir -p $(dirname "$dex_file")/oat/$ISA
-    local oat_file=$(basename "$dex_file")
-    local oat_file=$(dirname "$dex_file")/oat/$ISA/${oat_file%.*}.odex
+    local oat_file="$(realpath $dex_file)"
+    local oat_file="${oat_file:1}"
+    local oat_file="${oat_file//\//@}"
+    if [[ $dex_file != *.dex ]]; then
+      local oat_file=$ANDROID_DATA/dalvik-cache/$ISA/${oat_file}@classes.dex
+    else
+      local oat_file=$ANDROID_DATA/dalvik-cache/$ISA/${oat_file}
+    fi
     if [ "$GENERATE_APP_IMAGE" = "yes" ]; then
-      local art_file=$(basename "$dex_file")
-      local art_file=$(dirname "$dex_file")/oat/$ISA/${art_file%.*}.art
+      local art_file="${dex_file:1}"
+      local art_file="${art_file//\//@}"
+      local art_file=$ANDROID_DATA/dalvik-cache/$ISA/${art_file%.*}.art
       DEX2OAT_FLAGS+=("--app-image-file=$art_file")
     fi
 
@@ -558,7 +537,7 @@ if [ "$ANDROID_DATA" = "/data" ] || [ "$ANDROID_DATA" = "" ]; then
     # by default.
     ANDROID_DATA="$ANDROID_DATA/local/tmp/android-data$$"
   fi
-  mkdir -p "$ANDROID_DATA"
+  mkdir -p "$ANDROID_DATA/dalvik-cache/$ISA"
   DELETE_ANDROID_DATA="yes"
 fi
 
@@ -629,9 +608,6 @@ if [ "$ALLOW_DEFAULT_JDWP" = "no" ]; then
   EXTRA_OPTIONS+=(-XjdwpProvider:none)
 fi
 
-# First cleanup any left-over 'oat' files from the last time dalvikvm was run.
-cleanup_oat_directory_for_classpath "$@"
-
 # Protect additional arguments in quotes to preserve whitespaces (used by
 # run-jdwp-test.sh when running on device), '$' (may be used as part of
 # classpath) and other special characters when evaluated.
@@ -653,9 +629,6 @@ if [ "$JIT_PROFILE" = "yes" ]; then
           &> "$ANDROID_DATA/profile_gen.log"
   EXIT_STATUS=$?
 
-  # Remove generated oat files.
-  cleanup_oat_directory_for_classpath "$@"
-
   if [ $EXIT_STATUS != 0 ]; then
     echo "Profile run failed: " >&2
     cat "$ANDROID_DATA/profile_gen.log" >&2
@@ -698,9 +671,6 @@ if [ "$PERF" != "" ]; then
     echo "Perf data saved in: $ANDROID_DATA/perf.data. Generated oat files not removed."
   fi
 else
-  # Perf needs the odex files we generate for proper symbolization, so only remove them
-  # when not running with perf.
-  cleanup_oat_directory_for_classpath "$@"
   # Perf output is placed under $ANDROID_DATA so not cleaned when perf options used.
   clean_android_data
 fi
diff --git a/tools/boot-image-profile-generate.sh b/tools/boot-image-profile-generate.sh
index 2d3e87d5a3..54074ea6db 100755
--- a/tools/boot-image-profile-generate.sh
+++ b/tools/boot-image-profile-generate.sh
@@ -15,13 +15,14 @@
 # limitations under the License.
 
 #
-# This script creates the final boot image profile (suitable to include in the platform build).
+# This script creates the framework boot image and system server profiles
+# (suitable to include in the platform build).
 # The input to the script are:
 #   1) the boot.zip file which contains the boot classpath and system server jars.
 #      This file can be obtained from running `m dist` or by configuring the device with
 #      the `art/tools/boot-image-profile-configure-device.sh` script.
-#   2) the preloaded classes denylist which specify what clases should not be preloaded
-#      in Zygote. Usually located in usually in frameworks/base/config/preloaded-classes-denylist
+#   2) the preloaded classes denylist which specify what classes should not be preloaded
+#      in Zygote. Usually located in frameworks/base/config/preloaded-classes-denylist
 #   3) a list of raw boot image profiles extracted from devices. An example how to do that is
 #      by running `art/tools/boot-image-profile-extract-profile.sh` script.
 #
@@ -58,21 +59,38 @@ shift 3
 
 # Read the profile input args.
 profman_profile_input_args=()
-while [[ "$#" -ge 1 ]] && [[ ! "$1" = '--profman-arg' ]]; do
+boot_image_profman_args=()
+system_server_profman_args=()
+while [[ "$#" -ge 1 ]]; do
+  # Read the profman args.
+  if [[ "$#" -ge 2 ]]; then
+    if [[ "$1" = '--profman-arg' ]]; then
+      boot_image_profman_args+=("$2")
+      system_server_profman_args+=("$2")
+      shift 2
+      continue
+    fi
+
+    if [[ "$1" = '--boot-image-profman-arg' ]]; then
+      boot_image_profman_args+=("$2")
+      shift 2
+      continue
+    fi
+
+    if [[ "$1" = '--system-server-profman-arg' ]]; then
+      system_server_profman_args+=("$2")
+      shift 2
+      continue
+    fi
+  fi
   profman_profile_input_args+=("--profile-file=$1")
   shift
 done
 
-# Read the profman args.
-profman_args=()
-while [[ "$#" -ge 2 ]] && [[ "$1" = '--profman-arg' ]]; do
-  profman_args+=("$2")
-  shift 2
-done
-
 OUT_BOOT_PROFILE="$OUT_DIR"/boot-image-profile.txt
 OUT_PRELOADED_CLASSES="$OUT_DIR"/preloaded-classes
 OUT_SYSTEM_SERVER="$OUT_DIR"/art-profile
+ART_JARS=("core-oj.jar core-libart.jar okhttp.jar bouncycastle.jar apache-xml.jar")
 
 echo "Changing dirs to the build top"
 cd "$ANDROID_BUILD_TOP"
@@ -88,19 +106,31 @@ echo "Processing boot image jar files"
 jar_args=()
 for entry in "$BOOT_JARS"/*
 do
-  jar_args+=("--apk=$entry")
+  # Ignore ART jars, since we want fromework related jars only.
+  jar_name=$(basename "$entry")
+  if [[ " ${ART_JARS[*]} " != *\ ${jar_name}\ * ]]; then
+    jar_args+=("--apk=$entry")
+  fi
 done
-profman_args+=("${jar_args[@]}")
 
 echo "Running profman for boot image profiles"
 # NOTE:
 # You might want to adjust the default generation arguments based on the data
 # For example, to update the selection thresholds you could specify:
-#  --method-threshold=10 \
-#  --class-threshold=10 \
-#  --preloaded-class-threshold=10 \
-#  --special-package=android:1 \
-#  --special-package=com.android.systemui:1 \
+# - For boot image profile:
+#  --boot-image-profman-arg --method-threshold=10 \
+#  --boot-image-profman-arg --class-threshold=10 \
+#  --boot-image-profman-arg --preloaded-class-threshold=10 \
+#  --boot-image-profman-arg --special-package=android:1 \
+#  --boot-image-profman-arg --special-package=com.android.systemui:1 \
+# - For System Server:
+#  --system-server-profman-arg --method-threshold=10 \
+#  --system-server-profman-arg --class-threshold=10 \
+#  --system-server-profman-arg --special-package=android:1 \
+#  --system-server-profman-arg --special-package=com.android.systemui:1 \
+# You can use --profman-arg to specify the arguments for both boot image
+# and system server.
+#
 # The threshold is percentage of total aggregation, that is, a method/class is
 # included in the profile only if it's used by at least x% of the packages.
 # (from 0% - include everything to 100% - include only the items that
@@ -109,28 +139,34 @@ echo "Running profman for boot image profiles"
 # meaning, if the methods is used by that package then the algorithm will use a
 # different selection thresholds.
 # (system server is identified as the "android" package)
+if [[ "${#boot_image_profman_args[*]}" -eq 0 ]]; then
+  boot_image_profman_args+=("--special-package=android:1")
+  boot_image_profman_args+=("--special-package=com.android.systemui:1")
+fi
+boot_image_profman_args+=("${jar_args[@]}")
 profman \
   --generate-boot-image-profile \
   "${profman_profile_input_args[@]}" \
   --out-profile-path="$OUT_BOOT_PROFILE" \
   --out-preloaded-classes-path="$OUT_PRELOADED_CLASSES" \
   --preloaded-classes-denylist="$PRELOADED_DENYLIST" \
-  --special-package=android:1 \
-  --special-package=com.android.systemui:1 \
-  "${profman_args[@]}"
+  "${boot_image_profman_args[@]}"
 
 echo "Done boot image profile"
 
 echo "Running profman for system server"
-# For system server profile we want to include everything usually
-# We also don't have a preloaded-classes file for it, so we ignore the argument.
+# We don't have a preloaded-classes nor a denylist files for System Server, so we ignore the arguments.
+# We also set the thresholds to 0 to include everything if no args are specified.
+if [[ "${#system_server_profman_args[*]}" -eq 0 ]]; then
+  system_server_profman_args+=("--method-threshold=0")
+  system_server_profman_args+=("--class-threshold=0")
+fi
 profman \
   --generate-boot-image-profile \
   "${profman_profile_input_args[@]}" \
   --out-profile-path="$OUT_SYSTEM_SERVER" \
   --apk="$SYSTEM_SERVER_JAR" \
-  --method-threshold=0 \
-  --class-threshold=0
+  "${system_server_profman_args[@]}"
 
 echo "Done system server"
 
diff --git a/tools/buildbot-build.sh b/tools/buildbot-build.sh
index 03751dc13e..214780b741 100755
--- a/tools/buildbot-build.sh
+++ b/tools/buildbot-build.sh
@@ -16,6 +16,8 @@
 
 set -e
 
+export LC_ALL=C  # Generic simple locale
+
 . "$(dirname $0)/buildbot-utils.sh"
 
 shopt -s failglob
@@ -179,6 +181,14 @@ if [[ $build_target == "yes" ]]; then
   # Build/install the required APEXes.
   make_command+=" ${apexes[*]}"
   make_command+=" ${specific_targets}"
+
+  # Although the simulator is run on the host, we reuse the target build to
+  # build the target run tests on the host.
+  if [[ -n "${ART_USE_SIMULATOR}" ]]; then
+    # Build any simulator specific components, such as a target boot image, on
+    # the host.
+    make_command+=" build-art-simulator"
+  fi
 fi
 
 if [[ $installclean == "yes" ]]; then
@@ -315,12 +325,14 @@ if [[ $build_target == "yes" ]]; then
 
   # temporary root for linkerconfig
   linkerconfig_root=$ANDROID_PRODUCT_OUT/art_linkerconfig_root
+  system_linker_config_pb=$linkerconfig_root/system/etc/linker.config.pb
 
   rm -rf $linkerconfig_root
 
   # Linkerconfig reads files from /system/etc
   mkdir -p $linkerconfig_root/system
   cp -r $ANDROID_PRODUCT_OUT/system/etc $linkerconfig_root/system
+  rm -f $system_linker_config_pb  # We create our own below
 
   # Use our smaller public.libraries.txt that contains only the public libraries
   # pushed to the chroot directory.
@@ -356,7 +368,6 @@ EOF
 </apex-info-list>
 EOF
 
-  system_linker_config_pb=$linkerconfig_root/system/etc/linker.config.pb
   # This list needs to be synced with provideLibs in system/etc/linker.config.pb
   # in the targeted platform image.
   # TODO(b/186649223): Create a prebuilt for it in platform-mainline-sdk.
diff --git a/tools/buildbot-setup-device.sh b/tools/buildbot-setup-device.sh
index 9bff48b804..466595f202 100755
--- a/tools/buildbot-setup-device.sh
+++ b/tools/buildbot-setup-device.sh
@@ -93,13 +93,6 @@ adb shell setprop ctl.stop llkd-1
 
 product_name=$(adb shell getprop ro.build.product)
 
-if [ "x$product_name" = xfugu ]; then
-  # Kill logd first, so that when we set the adb buffer size later in this file,
-  # it is brought up again.
-  msginfo "Killing logd, seen leaking on fugu/N"
-  adb shell pkill -9 -U logd logd && msginfo "...logd killed"
-fi
-
 # Update date on device if the difference with host is more than one hour.
 if [ $abs_time_difference_in_seconds -gt $seconds_per_hour ]; then
   msginfo "Update date on device"
diff --git a/tools/buildbot-utils.sh b/tools/buildbot-utils.sh
index 78d22431c0..6bddfef777 100755
--- a/tools/buildbot-utils.sh
+++ b/tools/buildbot-utils.sh
@@ -88,7 +88,7 @@ if [[ -n "$ART_TEST_ON_VM" ]]; then
   export RSYNC_RSH="ssh -q -F $SSH_CONFIG -p $ART_TEST_SSH_PORT" # don't prefix with "ART_", rsync expects this name
 
   if [[ "$TARGET_ARCH" =~ ^(arm64|riscv64)$ ]]; then
-    export ART_TEST_VM_IMG="ubuntu-23.10-server-cloudimg-$TARGET_ARCH.img"
+    export ART_TEST_VM_IMG="ubuntu-24.04-server-cloudimg-$TARGET_ARCH.img"
     export ART_TEST_VM_DIR="$ANDROID_BUILD_TOP/vm/$TARGET_ARCH"
     export ART_TEST_VM="$ART_TEST_VM_DIR/$ART_TEST_VM_IMG"
   else
diff --git a/tools/buildbot-vm.sh b/tools/buildbot-vm.sh
index 720b06f749..054608d362 100755
--- a/tools/buildbot-vm.sh
+++ b/tools/buildbot-vm.sh
@@ -32,7 +32,7 @@ action="$1"
 get_stable_binary() {
     mkdir tmp && cd tmp
     wget "http://security.ubuntu.com/ubuntu/pool/main/$1"
-    7z x "$(basename $1)" && zstd -d data.tar.zst && tar -xf data.tar
+    ar x "$(basename $1)" && zstd -d data.tar.zst && tar -xf data.tar
     mv "$2" ..
     cd .. && rm -rf tmp
 }
@@ -45,11 +45,11 @@ if [[ $action = create ]]; then
 
     # sudo apt install qemu-system-<arch> qemu-efi cloud-image-utils
 
-    # Get the cloud image for Ubunty 23.10 (Mantic Minotaur)
-    wget "http://cloud-images.ubuntu.com/releases/23.10/release/$ART_TEST_VM_IMG"
+    # Get the cloud image for Ubuntu 24.04 LTS (Noble Numbat)
+    wget "http://cloud-images.ubuntu.com/releases/24.04/release/$ART_TEST_VM_IMG"
 
     if [[ "$TARGET_ARCH" = "riscv64" ]]; then
-        # Get U-Boot for Ubuntu 22.04 (Jammy)
+        # Get U-Boot
         get_stable_binary \
             u/u-boot/u-boot-qemu_2024.01+dfsg-5ubuntu2_all.deb \
             usr/lib/u-boot/qemu-riscv64_smode/uboot.elf
@@ -82,10 +82,18 @@ chpasswd:
 users:
   - default
   - name: $ART_TEST_SSH_USER
-    ssh-authorized-keys:
+    ssh_authorized_keys:
       - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCOYmwd9qoYd7rfYI6Q8zzqoZ3BtLC/SQo0WCvBFoJT6JzwU8F7nkN57KBQPLtvX2OBeDnFbtEY8uLtuNEp1Z19VcDbRd3LhyAMYFz6Ox/vWtPfl0hv0kUMQMAne1Bg0tawlNxawP2HXrLOh/FaXdSBSRUHNqMTQEnkIYw4faArDS/zKjVDs0/+e9mhtjL0akLcK04crlk2KD8Q2csya5givdAD7fVNOx7DtckRR47FLM1bERe0t0FlUESx/x7oLjNEmNUrPXV6GSkCoskmKSZC1vwgAf0VrxFADv1EywQXmlNaa4+rzqS4jMYuwi5QCtQXFFZl5qQ1Sh1rnliTRJvJzjXCeq3QPsPzUJInfVGzrPClfHG7whlJE/Uwv8UOF7WHzUt5OBOsW6nZrplldvfYif/qz6dR+RX2G0zi8tC/2Mzahr6toAqtsqbdp3coYvpi/OjHIV3RhyJxG1FtyGYQRnmGPs8R9ic3pupjLFWM9qIilUCjFrUoiw7QAgfUrUc= ubuntu_user@example.com
     sudo: ALL=(ALL) NOPASSWD:ALL
     groups: users, admin
+write_files:
+  - path: /etc/sysctl.d/60-apparmor.conf
+    permissions: 0644
+    owner: root
+    content: |
+      kernel.apparmor_restrict_unprivileged_userns=0
+runcmd:
+  - systemctl restart systemd-sysctl
 EOF
     # meta-data is necessary, even if empty.
     cat >meta-data <<EOF
@@ -99,6 +107,7 @@ elif [[ $action = boot ]]; then
     cp "$(dirname $0)/user-data.img" "$ART_TEST_VM_DIR/user-data.img"
     cd "$ART_TEST_VM_DIR"
     if [[ "$TARGET_ARCH" = "riscv64" ]]; then
+        echo -n > $SCRIPT_DIR/boot.out
         ($ANDROID_BUILD_TOP/device/google/cuttlefish_vmm/qemu/x86_64-linux-gnu/bin/qemu-system-riscv64 \
             -M virt \
             -nographic \
@@ -109,7 +118,7 @@ elif [[ $action = boot ]]; then
             -drive file="$ART_TEST_VM_IMG",if=virtio \
             -drive file=user-data.img,format=raw,if=virtio \
             -device virtio-net-device,netdev=usernet \
-            -netdev user,id=usernet,hostfwd=tcp::$ART_TEST_SSH_PORT-:22 > $SCRIPT_DIR/boot.out &)
+            -netdev user,id=usernet,hostfwd=tcp::$ART_TEST_SSH_PORT-:22 >> $SCRIPT_DIR/boot.out &)
         echo "Now listening for successful boot"
         finish_str='.*finished at.*'
         while IFS= read -d $'\0' -n 1 a ; do
@@ -126,6 +135,7 @@ elif [[ $action = boot ]]; then
         done < <(tail -f $SCRIPT_DIR/boot.out)
 
     elif [[ "$TARGET_ARCH" = "arm64" ]]; then
+        echo -n > $SCRIPT_DIR/boot.out
         (qemu-system-aarch64 \
             -m 16G \
             -smp 8 \
@@ -138,7 +148,7 @@ elif [[ $action = boot ]]; then
             -drive file=user-data.img,format=raw,id=cloud \
             -device virtio-blk-device,drive=hd0 \
             -device virtio-net-device,netdev=usernet \
-            -netdev user,id=usernet,hostfwd=tcp::$ART_TEST_SSH_PORT-:22 > $SCRIPT_DIR/boot.out &)
+            -netdev user,id=usernet,hostfwd=tcp::$ART_TEST_SSH_PORT-:22 >> $SCRIPT_DIR/boot.out &)
         echo "Now listening for successful boot"
         finish_str='.*finished at.*'
         while IFS= read -d $'\0' -n 1 a ; do
diff --git a/tools/checker/Android.bp b/tools/checker/Android.bp
index db2c597bf4..8b14807854 100644
--- a/tools/checker/Android.bp
+++ b/tools/checker/Android.bp
@@ -29,11 +29,6 @@ python_binary_host {
         "**/*.py",
     ],
     main: "checker.py",
-    version: {
-        py3: {
-            embedded_launcher: true,
-        },
-    },
     test_suites: [
         "general-tests",
         "mts-art",
diff --git a/tools/cpp-define-generator/Android.bp b/tools/cpp-define-generator/Android.bp
index babcfa43fe..44097bcdd0 100644
--- a/tools/cpp-define-generator/Android.bp
+++ b/tools/cpp-define-generator/Android.bp
@@ -35,8 +35,12 @@ cc_object {
         "libart_headers",
         "libdexfile_all_headers", // For dex/modifiers.h
     ],
-    // Produce text file rather than binary.
-    cflags: ["-S"],
+    cflags: [
+        // Produce text file rather than binary.
+        "-S",
+        // Suppress an error about the unused -c that precedes -S.
+        "-Wno-unused-command-line-argument",
+    ],
     srcs: ["asm_defines.cc"],
     apex_available: [
         "com.android.art",
diff --git a/tools/cpp-define-generator/asm_defines.def b/tools/cpp-define-generator/asm_defines.def
index fb011a2ba3..86bceb005e 100644
--- a/tools/cpp-define-generator/asm_defines.def
+++ b/tools/cpp-define-generator/asm_defines.def
@@ -23,6 +23,7 @@
 #include "art_method.def"
 #include "code_item.def"
 #include "lockword.def"
+#include "instrumentation.def"
 #include "mirror_array.def"
 #include "mirror_class.def"
 #include "mirror_object.def"
diff --git a/tools/cpp-define-generator/instrumentation.def b/tools/cpp-define-generator/instrumentation.def
new file mode 100644
index 0000000000..e1a13c4d36
--- /dev/null
+++ b/tools/cpp-define-generator/instrumentation.def
@@ -0,0 +1,22 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#if ASM_DEFINE_INCLUDE_DEPENDENCIES
+#include "instrumentation.h"
+#endif
+
+ASM_DEFINE(INSTRUMENTATION_RUN_EXIT_HOOKS_OFFSET,
+           art::instrumentation::Instrumentation::RunExitHooksOffset().Int32Value())
diff --git a/tools/cpp-define-generator/runtime.def b/tools/cpp-define-generator/runtime.def
index fd6567d87e..66a542a5d1 100644
--- a/tools/cpp-define-generator/runtime.def
+++ b/tools/cpp-define-generator/runtime.def
@@ -31,6 +31,4 @@ ASM_DEFINE(RUNTIME_SAVE_REFS_AND_ARGS_METHOD_OFFSET,
 ASM_DEFINE(RUNTIME_SAVE_REFS_ONLY_METHOD_OFFSET,
            art::Runtime::GetCalleeSaveMethodOffset(art::CalleeSaveType::kSaveRefsOnly))
 ASM_DEFINE(RUNTIME_INSTRUMENTATION_OFFSET, art::Runtime::GetInstrumentationOffset().Int32Value())
-ASM_DEFINE(RUN_EXIT_HOOKS_OFFSET_FROM_RUNTIME_INSTANCE,
-           art::Runtime::GetInstrumentationOffset().Int32Value() +
-           art::instrumentation::Instrumentation::RunExitHooksOffset().Int32Value())
+
diff --git a/tools/cpp-define-generator/shadow_frame.def b/tools/cpp-define-generator/shadow_frame.def
index f7e8243fca..654f45a931 100644
--- a/tools/cpp-define-generator/shadow_frame.def
+++ b/tools/cpp-define-generator/shadow_frame.def
@@ -18,16 +18,8 @@
 #include "interpreter/shadow_frame.h"
 #endif
 
-ASM_DEFINE(SHADOWFRAME_CACHED_HOTNESS_COUNTDOWN_OFFSET,
-           art::ShadowFrame::CachedHotnessCountdownOffset())
-ASM_DEFINE(SHADOWFRAME_DEX_INSTRUCTIONS_OFFSET,
-           art::ShadowFrame::DexInstructionsOffset())
 ASM_DEFINE(SHADOWFRAME_DEX_PC_OFFSET,
            art::ShadowFrame::DexPCOffset())
-ASM_DEFINE(SHADOWFRAME_DEX_PC_PTR_OFFSET,
-           art::ShadowFrame::DexPCPtrOffset())
-ASM_DEFINE(SHADOWFRAME_HOTNESS_COUNTDOWN_OFFSET,
-           art::ShadowFrame::HotnessCountdownOffset())
 ASM_DEFINE(SHADOWFRAME_LINK_OFFSET,
            art::ShadowFrame::LinkOffset())
 ASM_DEFINE(SHADOWFRAME_LOCK_COUNT_DATA_OFFSET,
diff --git a/tools/cpp-define-generator/thread.def b/tools/cpp-define-generator/thread.def
index aa621697c8..f5eefad3b6 100644
--- a/tools/cpp-define-generator/thread.def
+++ b/tools/cpp-define-generator/thread.def
@@ -77,3 +77,4 @@ ASM_DEFINE(TRACE_BUFFER_INIT_OFFSET,
 ASM_DEFINE(TRACE_BUFFER_CURRENT_OFFSET,
            art::Thread::TraceBufferCurrPtrOffset<art::kRuntimePointerSize>().Int32Value())
 ASM_DEFINE(TRACE_BUFFER_SIZE, (art::kAlwaysOnTraceBufSize - 1) * sizeof(uintptr_t))
+ASM_DEFINE(LONG_RUNNING_METHOD_THRESHOLD, art::kLongRunningMethodThreshold)
diff --git a/tools/create_minidebuginfo/Android.bp b/tools/create_minidebuginfo/Android.bp
index e78beef8f3..e921cf33b5 100644
--- a/tools/create_minidebuginfo/Android.bp
+++ b/tools/create_minidebuginfo/Android.bp
@@ -37,5 +37,7 @@ art_cc_binary {
         "libelffile",
         "liblzma",
         "liblog",
+        "libz", // For "libartbase".
+        "libziparchive", // For "libartbase".
     ],
 }
diff --git a/tools/fuzzer/Android.bp b/tools/fuzzer/Android.bp
index 45c4c4aad0..94f499d082 100644
--- a/tools/fuzzer/Android.bp
+++ b/tools/fuzzer/Android.bp
@@ -66,7 +66,6 @@ cc_defaults {
     srcs: ["libart_verify_classes_fuzzer.cc"],
 
     defaults: [
-        "art_defaults",
         "libart_fuzzer-defaults",
     ],
 
@@ -84,9 +83,15 @@ cc_defaults {
         ":okhttp",
         ":bouncycastle",
         ":apache-xml",
-        ":core-icu4j",
+        ":core-icu4j-fuzzer",
         ":conscrypt-fuzzer",
     ],
+
+    whole_static_libs: [
+        // `libsigchain` must be statically linked. ASAN init uses sigaction,
+        // which makes `libsigchain.so` initialize earlier than required.
+        "libsigchain",
+    ],
 }
 
 cc_fuzz {
@@ -116,11 +121,27 @@ cc_fuzz {
 }
 
 cc_fuzz {
-    // TODO Add a debug version
     name: "libart_verify_classes_fuzzer",
     defaults: [
+        "art_defaults",
+        "libart-runtime-for-test_static_defaults",
+        "libart-compiler_static_defaults",
+        "libart_verify_classes_fuzzer-defaults",
+    ],
+    // Can not be in defaults due to soong limitations.
+    device_common_corpus: [
+        ":art_runtest_corpus",
+        "class-verifier-corpus/*",
+    ],
+}
+
+cc_fuzz {
+    name: "libart_verify_classes_fuzzerd",
+    defaults: [
+        "art_debug_defaults",
+        "libartd-runtime-for-test_static_defaults",
+        "libartd-compiler_static_defaults",
         "libart_verify_classes_fuzzer-defaults",
-        "libart_static_defaults",
     ],
     // Can not be in defaults due to soong limitations.
     device_common_corpus: [
diff --git a/tools/fuzzer/class-verifier-corpus/b388017887.dex b/tools/fuzzer/class-verifier-corpus/b388017887.dex
new file mode 100644
index 0000000000..c3c0143e40
Binary files /dev/null and b/tools/fuzzer/class-verifier-corpus/b388017887.dex differ
diff --git a/tools/fuzzer/class-verifier-corpus/b391844326.dex b/tools/fuzzer/class-verifier-corpus/b391844326.dex
new file mode 100644
index 0000000000..faf536139b
Binary files /dev/null and b/tools/fuzzer/class-verifier-corpus/b391844326.dex differ
diff --git a/tools/fuzzer/class-verifier-corpus/b391852978.dex b/tools/fuzzer/class-verifier-corpus/b391852978.dex
new file mode 100644
index 0000000000..5f21123335
Binary files /dev/null and b/tools/fuzzer/class-verifier-corpus/b391852978.dex differ
diff --git a/tools/fuzzer/dex-verifier-corpus/b391842969.dex b/tools/fuzzer/dex-verifier-corpus/b391842969.dex
new file mode 100644
index 0000000000..98a57cc8fa
Binary files /dev/null and b/tools/fuzzer/dex-verifier-corpus/b391842969.dex differ
diff --git a/tools/fuzzer/libart_verify_classes_fuzzer.cc b/tools/fuzzer/libart_verify_classes_fuzzer.cc
index 13abd7c613..de99b654cb 100644
--- a/tools/fuzzer/libart_verify_classes_fuzzer.cc
+++ b/tools/fuzzer/libart_verify_classes_fuzzer.cc
@@ -28,6 +28,7 @@
 #include "jni/java_vm_ext.h"
 #include "noop_compiler_callbacks.h"
 #include "runtime.h"
+#include "runtime_intrinsics.h"
 #include "scoped_thread_state_change-inl.h"
 #include "verifier/class_verifier.h"
 #include "well_known_classes.h"
@@ -40,8 +41,6 @@ int skipped_gc_iterations = 0;
 // TODO: These values were obtained from local experimenting. They can be changed after
 // further investigation.
 static constexpr int kMaxSkipGCIterations = 100;
-// Global variable to signal LSAN that we are not leaking memory.
-uint8_t* allocated_signal_stack = nullptr;
 
 namespace art {
 // A class to be friends with ClassLinker and access the internal FindDexCacheDataLocked method.
@@ -121,30 +120,23 @@ extern "C" int LLVMFuzzerInitialize([[maybe_unused]] int* argc, [[maybe_unused]]
       std::make_pair("imageinstructionset",
                      reinterpret_cast<const void*>(GetInstructionSetString(art::kRuntimeISA))));
 
-  // No need for sig chain.
-  options.push_back(std::make_pair("-Xno-sig-chain", nullptr));
-
   if (!art::Runtime::Create(options, false)) {
     LOG(FATAL) << "We should always be able to create the runtime";
     UNREACHABLE();
   }
 
-  // Need well-known-classes.
-  art::WellKnownClasses::Init(art::Thread::Current()->GetJniEnv());
-  // Need a class loader. Fake that we're a compiler.
-  // Note: this will run initializers through the unstarted runtime, so make sure it's
-  //       initialized.
   art::interpreter::UnstartedRuntime::Initialize();
+  art::Runtime::Current()->GetClassLinker()->RunEarlyRootClinits(art::Thread::Current());
+  art::InitializeIntrinsics();
+  art::Runtime::Current()->RunRootClinits(art::Thread::Current());
 
-  art::Thread::Current()->TransitionFromRunnableToSuspended(art::ThreadState::kNative);
+  // Check for heap corruption before running the fuzzer.
+  art::Runtime::Current()->GetHeap()->VerifyHeap();
 
-  // Query the current stack and add it to the global variable. Otherwise LSAN complains about a
-  // non-existing leak.
-  stack_t ss;
-  if (sigaltstack(nullptr, &ss) == -1) {
-    PLOG(FATAL) << "sigaltstack failed";
-  }
-  allocated_signal_stack = reinterpret_cast<uint8_t*>(ss.ss_sp);
+  // Runtime::Create acquired the mutator_lock_ that is normally given away when we
+  // Runtime::Start, give it away now with `TransitionFromSuspendedToRunnable` until we figure out
+  // how to start a Runtime.
+  art::Thread::Current()->TransitionFromRunnableToSuspended(art::ThreadState::kNative);
 
   return 0;
 }
@@ -178,12 +170,14 @@ extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
 
   // Scope for the handles
   {
-    art::StackHandleScope<3> scope(soa.Self());
+    art::StackHandleScope<4> scope(soa.Self());
     art::Handle<art::mirror::ClassLoader> h_loader =
         scope.NewHandle(soa.Decode<art::mirror::ClassLoader>(class_loader));
     art::MutableHandle<art::mirror::Class> h_klass(scope.NewHandle<art::mirror::Class>(nullptr));
     art::MutableHandle<art::mirror::DexCache> h_dex_cache(
         scope.NewHandle<art::mirror::DexCache>(nullptr));
+    art::MutableHandle<art::mirror::ClassLoader> h_dex_cache_class_loader =
+        scope.NewHandle(h_loader.Get());
 
     for (art::ClassAccessor accessor : dex_file.GetClasses()) {
       h_klass.Assign(
@@ -195,12 +189,16 @@ extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
         continue;
       }
       h_dex_cache.Assign(h_klass->GetDexCache());
+
+      // The class loader from the class's dex cache is different from the dex file's class loader
+      // for boot image classes e.g. java.util.AbstractCollection.
+      h_dex_cache_class_loader.Assign(h_klass->GetDexCache()->GetClassLoader());
       art::verifier::ClassVerifier::VerifyClass(soa.Self(),
                                                 /* verifier_deps= */ nullptr,
                                                 h_dex_cache->GetDexFile(),
                                                 h_klass,
                                                 h_dex_cache,
-                                                h_loader,
+                                                h_dex_cache_class_loader,
                                                 *h_klass->GetClassDef(),
                                                 runtime->GetCompilerCallbacks(),
                                                 art::verifier::HardFailLogMode::kLogWarning,
diff --git a/tools/hiddenapi/Android.bp b/tools/hiddenapi/Android.bp
index 906e282df4..c33be398a0 100644
--- a/tools/hiddenapi/Android.bp
+++ b/tools/hiddenapi/Android.bp
@@ -41,7 +41,7 @@ cc_defaults {
     stl: "c++_static",
     static_libs: [
         "libbase",
-        "libcrypto_for_art",
+        "libcrypto_static", // Not FIPS tested - for SHA-1 checksumming of build ID only.
     ],
 }
 
diff --git a/tools/hiddenapi/hiddenapi.cc b/tools/hiddenapi/hiddenapi.cc
index bc73865019..f5d59b9f39 100644
--- a/tools/hiddenapi/hiddenapi.cc
+++ b/tools/hiddenapi/hiddenapi.cc
@@ -148,11 +148,11 @@ class DexClass : public ClassAccessor {
   inline bool IsInterface() const { return HasAccessFlags(kAccInterface); }
 
   inline bool Equals(const DexClass& other) const {
-    bool equals = strcmp(GetDescriptor(), other.GetDescriptor()) == 0;
+    bool equals = GetDescriptorView() == other.GetDescriptorView();
 
     if (equals) {
-      LOG(FATAL) << "Class duplication: " << GetDescriptor() << " in " << dex_file_.GetLocation()
-          << " and " << other.dex_file_.GetLocation();
+      LOG(FATAL) << "Class duplication: " << GetDescriptorView() << " in "
+          << dex_file_.GetLocation() << " and " << other.dex_file_.GetLocation();
     }
 
     return equals;
@@ -192,7 +192,7 @@ class DexMember {
   // Constructs a string with a unique signature of this class member.
   std::string GetApiEntry() const {
     std::stringstream ss;
-    ss << klass_.GetDescriptor() << "->" << GetName() << (IsMethod() ? "" : ":")
+    ss << klass_.GetDescriptorView() << "->" << GetName() << (IsMethod() ? "" : ":")
        << GetSignature();
     return ss.str();
   }
@@ -450,7 +450,7 @@ class Hierarchy final {
   // Returns true if at least one resolvable member was found.
   template<typename Fn>
   bool ForEachResolvableMember(const DexMember& other, Fn fn) {
-    HierarchyClass* klass = FindClass(other.GetDeclaringClass().GetDescriptor());
+    HierarchyClass* klass = FindClass(other.GetDeclaringClass().GetDescriptorView());
     return (klass != nullptr) && klass->ForEachResolvableMember(other, fn);
   }
 
@@ -473,7 +473,7 @@ class Hierarchy final {
       // Example code (`foo` exposed by ClassB):
       //   class ClassA { public void foo() { ... } }
       //   public class ClassB extends ClassA {}
-      HierarchyClass* klass = FindClass(member.GetDeclaringClass().GetDescriptor());
+      HierarchyClass* klass = FindClass(member.GetDeclaringClass().GetDescriptorView());
       CHECK(klass != nullptr);
       bool visible = false;
       klass->ForEachSubClass([&visible, &member](HierarchyClass* subclass) {
@@ -497,7 +497,7 @@ class Hierarchy final {
   }
 
  private:
-  HierarchyClass* FindClass(const std::string_view& descriptor) {
+  HierarchyClass* FindClass(std::string_view descriptor) {
     auto it = classes_.find(descriptor);
     if (it == classes_.end()) {
       return nullptr;
@@ -510,7 +510,7 @@ class Hierarchy final {
     // Create one HierarchyClass entry in `classes_` per class descriptor
     // and add all DexClass objects with the same descriptor to that entry.
     classpath_.ForEachDexClass([this](const DexClass& klass) {
-      classes_[klass.GetDescriptor()].AddDexClass(klass);
+      classes_[klass.GetDescriptorView()].AddDexClass(klass);
     });
 
     // Connect each HierarchyClass to its successors and predecessors.
@@ -532,7 +532,7 @@ class Hierarchy final {
           auto severity = verbose ? ::android::base::WARNING : ::android::base::FATAL;
           LOG(severity)
               << "Superclass/interface " << extends_desc
-              << " of class " << dex_klass.GetDescriptor() << " from dex file \""
+              << " of class " << dex_klass.GetDescriptorView() << " from dex file \""
               << dex_klass.GetDexFile().GetLocation() << "\" was not found. "
               << "Either it is missing or it appears later in the classpath spec.";
         }
@@ -898,7 +898,7 @@ class HiddenApi final {
               CHECK(!force_assign_all_ || api_list_found)
                   << "Could not find hiddenapi flags for dex entry: " << signature;
               if (api_list_found && it->second.GetIntValue() > max_hiddenapi_level_.GetIntValue()) {
-                ApiList without_domain(it->second.GetIntValue());
+                ApiList without_domain = ApiList::FromDexFlags(it->second.GetIntValue());
                 LOG(ERROR) << "Hidden api flag " << without_domain << " for member " << signature
                            << " in " << input_path << " exceeds maximum allowable flag "
                            << max_hiddenapi_level_;
@@ -956,7 +956,7 @@ class HiddenApi final {
       CHECK(api_flag_map.find(signature) == api_flag_map.end()) << path << ":" << line_number
           << ": Duplicate entry: " << signature << kErrorHelp;
 
-      ApiList membership;
+      ApiList membership = ApiList::Invalid();
 
       std::vector<std::string>::iterator apiListBegin = values.begin() + 1;
       std::vector<std::string>::iterator apiListEnd = values.end();
@@ -1098,7 +1098,7 @@ class HiddenApi final {
   //
   // By default this returns a GetIntValue() that is guaranteed to be bigger than
   // any valid value returned by GetIntValue().
-  ApiList max_hiddenapi_level_;
+  ApiList max_hiddenapi_level_ = ApiList::Invalid();
 
   // Whether the input is only a fragment of the whole bootclasspath and may
   // not include a complete set of classes. That requires the tool to ignore missing
diff --git a/tools/hiddenapi/hiddenapi_test.cc b/tools/hiddenapi/hiddenapi_test.cc
index fe7677efff..940a2630d9 100644
--- a/tools/hiddenapi/hiddenapi_test.cc
+++ b/tools/hiddenapi/hiddenapi_test.cc
@@ -178,7 +178,7 @@ class HiddenApiTest : public CommonRuntimeTest {
                                          const dex::ClassDef& class_def,
                                          const DexFile& dex_file) {
     ClassAccessor accessor(dex_file, class_def, /* parse hiddenapi flags */ true);
-    CHECK(accessor.HasClassData()) << "Class " << accessor.GetDescriptor() << " has no data";
+    CHECK(accessor.HasClassData()) << "Class " << accessor.GetDescriptorView() << " has no data";
 
     if (!accessor.HasHiddenapiClassData()) {
       return hiddenapi::ApiList::Sdk();
@@ -189,8 +189,8 @@ class HiddenApiTest : public CommonRuntimeTest {
       if (strcmp(name, dex_file.GetFieldName(fid)) == 0) {
         const uint32_t actual_visibility = field.GetAccessFlags() & kAccVisibilityFlags;
         CHECK_EQ(actual_visibility, expected_visibility)
-            << "Field " << name << " in class " << accessor.GetDescriptor();
-        return hiddenapi::ApiList(field.GetHiddenapiFlags());
+            << "Field " << name << " in class " << accessor.GetDescriptorView();
+        return hiddenapi::ApiList::FromDexFlags(field.GetHiddenapiFlags());
       }
     }
 
@@ -205,7 +205,7 @@ class HiddenApiTest : public CommonRuntimeTest {
                                           const dex::ClassDef& class_def,
                                           const DexFile& dex_file) {
     ClassAccessor accessor(dex_file, class_def, /* parse hiddenapi flags */ true);
-    CHECK(accessor.HasClassData()) << "Class " << accessor.GetDescriptor() << " has no data";
+    CHECK(accessor.HasClassData()) << "Class " << accessor.GetDescriptorView() << " has no data";
 
     if (!accessor.HasHiddenapiClassData()) {
       return hiddenapi::ApiList::Sdk();
@@ -215,11 +215,11 @@ class HiddenApiTest : public CommonRuntimeTest {
       const dex::MethodId& mid = dex_file.GetMethodId(method.GetIndex());
       if (strcmp(name, dex_file.GetMethodName(mid)) == 0) {
         CHECK_EQ(expected_native, method.MemberIsNative())
-            << "Method " << name << " in class " << accessor.GetDescriptor();
+            << "Method " << name << " in class " << accessor.GetDescriptorView();
         const uint32_t actual_visibility = method.GetAccessFlags() & kAccVisibilityFlags;
         CHECK_EQ(actual_visibility, expected_visibility)
-            << "Method " << name << " in class " << accessor.GetDescriptor();
-        return hiddenapi::ApiList(method.GetHiddenapiFlags());
+            << "Method " << name << " in class " << accessor.GetDescriptorView();
+        return hiddenapi::ApiList::FromDexFlags(method.GetHiddenapiFlags());
       }
     }
 
@@ -700,8 +700,9 @@ TEST_F(HiddenApiTest, InstanceFieldCorePlatformApiMatch) {
       << "LMain;->ifield:I,unsupported,core-platform-api" << std::endl;
   auto dex_file = RunHiddenapiEncode(flags_csv, {}, dex);
   ASSERT_NE(dex_file.get(), nullptr);
-  ASSERT_EQ(hiddenapi::ApiList::CorePlatformApi() |
-  hiddenapi::ApiList::Unsupported(), GetIFieldHiddenFlags(*dex_file));
+  ASSERT_EQ(hiddenapi::ApiList::Combine(hiddenapi::ApiList::CorePlatformApi(),
+                                        hiddenapi::ApiList::Unsupported()),
+            GetIFieldHiddenFlags(*dex_file));
 }
 
 TEST_F(HiddenApiTest, InstanceFieldTestApiMatch) {
@@ -712,8 +713,9 @@ TEST_F(HiddenApiTest, InstanceFieldTestApiMatch) {
       << "LMain;->ifield:I,unsupported,test-api" << std::endl;
   auto dex_file = RunHiddenapiEncode(flags_csv, {}, dex);
   ASSERT_NE(dex_file.get(), nullptr);
-  ASSERT_EQ(hiddenapi::ApiList::TestApi()
-  | hiddenapi::ApiList::Unsupported(), GetIFieldHiddenFlags(*dex_file));
+  ASSERT_EQ(
+      hiddenapi::ApiList::Combine(hiddenapi::ApiList::TestApi(), hiddenapi::ApiList::Unsupported()),
+      GetIFieldHiddenFlags(*dex_file));
 }
 
 TEST_F(HiddenApiTest, InstanceFieldUnknownFlagMatch) {
diff --git a/tools/jvmti-agents/field-counts/fieldcount.cc b/tools/jvmti-agents/field-counts/fieldcount.cc
index 526d68f021..2d886fe1f4 100644
--- a/tools/jvmti-agents/field-counts/fieldcount.cc
+++ b/tools/jvmti-agents/field-counts/fieldcount.cc
@@ -242,8 +242,10 @@ static jint AgentStart(JavaVM* vm, char* options, bool is_onload) {
   CHECK_JVMTI(jvmti->SetEventCallbacks(&cb, sizeof(cb)));
   if (is_onload) {
     unsigned char* ptr = nullptr;
-    CHECK_JVMTI(jvmti->Allocate(strlen(options) + 1, &ptr));
-    strcpy(reinterpret_cast<char*>(ptr), options);
+    size_t options_length = strlen(options) + 1;
+    CHECK_JVMTI(jvmti->Allocate(options_length, &ptr));
+    strncpy(reinterpret_cast<char*>(ptr), options, options_length);
+    ptr[options_length - 1] = '\0';  // Ensure null termination
     CHECK_JVMTI(jvmti->SetEnvironmentLocalStorage(ptr));
     CHECK_JVMTI(jvmti->SetEventNotificationMode(JVMTI_ENABLE, JVMTI_EVENT_VM_INIT, nullptr));
   } else {
diff --git a/tools/jvmti-agents/wrapagentproperties/wrapagentproperties.cc b/tools/jvmti-agents/wrapagentproperties/wrapagentproperties.cc
index 39cb20acf2..b2dcaabcfe 100644
--- a/tools/jvmti-agents/wrapagentproperties/wrapagentproperties.cc
+++ b/tools/jvmti-agents/wrapagentproperties/wrapagentproperties.cc
@@ -28,6 +28,7 @@
 #include <sstream>
 #include <string>
 #include <vector>
+#include <cstring>
 
 namespace wrapagentproperties {
 
@@ -185,7 +186,7 @@ struct ExtraJvmtiInterface : public jvmtiInterface_1_ {
       if (res != JVMTI_ERROR_NONE) {
         return res;
       }
-      strcpy(out_prop_ptr[i], p.c_str());
+      memcpy(out_prop_ptr[i], p.c_str(), p.size() + 1);
       i++;
     }
     CHECK_EQ(i, *cnt);
diff --git a/tools/libcore_failures.txt b/tools/libcore_failures.txt
index 8dc525d44a..c17421a0d5 100644
--- a/tools/libcore_failures.txt
+++ b/tools/libcore_failures.txt
@@ -8,14 +8,6 @@
  */
 
 [
-{
-  description: "Os.memfd_create() only supports bionic-based platforms.",
-  result: EXEC_FAILED,
-  modes: [host],
-  names: ["libcore.android.system.OsTest#testMemfdCreate",
-          "libcore.android.system.OsTest#testMemfdCreateErrno",
-          "libcore.android.system.OsTest#testMemfdCreateFlags"]
-},
 {
   description: "Assert.java differences between vogar and junit.",
   result: EXEC_FAILED,
@@ -43,15 +35,6 @@
   modes: [device],
   names: ["libcore.libcore.io.OsTest#testUnixDomainSockets_in_file_system"]
 },
-{
-  description: "TCP_USER_TIMEOUT is not defined on host's tcp.h (glibc-2.15-4.8).",
-  result: EXEC_FAILED,
-  modes: [host],
-  names: ["libcore.android.system.OsConstantsTest#testTcpUserTimeoutIsDefined",
-          "libcore.libcore.io.OsTest#test_socket_tcpUserTimeout_setAndGet",
-          "libcore.libcore.io.OsTest#test_socket_tcpUserTimeout_doesNotWorkOnDatagramSocket"],
-  bug: 30402085
-},
 {
   description: "Issue with incorrect device time (1970)",
   result: EXEC_FAILED,
diff --git a/tools/libcore_fugu_failures.txt b/tools/libcore_fugu_failures.txt
deleted file mode 100644
index 60b43d0571..0000000000
--- a/tools/libcore_fugu_failures.txt
+++ /dev/null
@@ -1,236 +0,0 @@
-/*
- * This file contains expectations for ART's Buildbot when running on fugu devices
- * (Nexus Player, kernel 3.10).
- *
- * The script that uses this file is art/tools/run-libcore-tests.sh.
- */
-
-[
-{
-  description: "Test using memfd_create() syscall, only available from Linux 3.17.",
-  result: EXEC_FAILED,
-  bug: 146113753,
-  modes: [device],
-  names: [
-    "libcore.android.system.OsTest#testMemfdCreate",
-    "libcore.android.system.OsTest#testMemfdCreateErrno",
-    "libcore.android.system.OsTest#testMemfdCreateFlags"
-  ]
-},
-{
-  description: "Test using the getrandom() syscall, only available from Linux 3.17.",
-  result: EXEC_FAILED,
-  bug: 141230711,
-  modes: [device],
-  names: [
-    "libcore.java.math.BigIntegerTest#test_Constructor_IILjava_util_Random",
-    "libcore.java.math.BigIntegerTest#test_probablePrime",
-    "libcore.java.util.UUIDTest#testJava11Implementation_invalidInputs",
-    "libcore.java.util.UUIDTest#testJava8Implementation_allowsLongInputs",
-    "libcore.javax.crypto.CipherInputStreamTest#testDecryptCorruptGCM",
-    "libcore.javax.crypto.CipherOutputStreamTest#testDecryptCorruptGCM",
-    "libcore.libcore.timezone.TelephonyLookupTest#createInstanceWithFallback",
-    "libcore.libcore.timezone.TelephonyLookupTest#getTelephonyNetworkFinder",
-    "libcore.libcore.timezone.TelephonyLookupTest#validateCountryCodeLowerCase",
-    "libcore.libcore.timezone.TelephonyLookupTest#validateDuplicateMccMnc",
-    "libcore.libcore.timezone.TelephonyLookupTest#xmlParsing_emptyFile",
-    "libcore.libcore.timezone.TelephonyLookupTest#xmlParsing_emptyNetworksOk",
-    "libcore.libcore.timezone.TelephonyLookupTest#xmlParsing_missingCountryCodeAttribute",
-    "libcore.libcore.timezone.TelephonyLookupTest#xmlParsing_missingMccAttribute",
-    "libcore.libcore.timezone.TelephonyLookupTest#xmlParsing_missingMncAttribute",
-    "libcore.libcore.timezone.TelephonyLookupTest#xmlParsing_missingNetworks",
-    "libcore.libcore.timezone.TelephonyLookupTest#xmlParsing_truncatedInput",
-    "libcore.libcore.timezone.TelephonyLookupTest#xmlParsing_unexpectedComments",
-    "libcore.libcore.timezone.TelephonyLookupTest#xmlParsing_unexpectedElementsIgnored",
-    "libcore.libcore.timezone.TelephonyLookupTest#xmlParsing_unexpectedRootElement",
-    "libcore.libcore.timezone.TelephonyLookupTest#xmlParsing_unexpectedTextIgnored",
-    "libcore.libcore.timezone.TimeZoneFinderTest#createInstanceWithFallback",
-    "libcore.libcore.timezone.TimeZoneFinderTest#getCountryZonesFinder",
-    "libcore.libcore.timezone.TimeZoneFinderTest#getCountryZonesFinder_empty",
-    "libcore.libcore.timezone.TimeZoneFinderTest#getCountryZonesFinder_invalid",
-    "libcore.libcore.timezone.TimeZoneFinderTest#getIanaVersion",
-    "libcore.libcore.timezone.TimeZoneFinderTest#lookupCountryTimeZones_caseInsensitive",
-    "libcore.libcore.timezone.TimeZoneFinderTest#lookupCountryTimeZones_unknownCountryReturnsNull",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_badCountryDefaultBoost",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_badCountryEverUtc",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_badTimeZoneMappingNotAfter",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_badTimeZoneMappingPicker",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_countryDefaultBoost",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_emptyFile",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_missingCountryCode",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_missingCountryDefault",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_missingCountryEverUtc",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_missingCountryZones",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_missingIanaVersionAttribute",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_noCountriesOk",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_timeZoneMappingNotAfter",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_timeZoneMappingPicker",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_truncatedInput",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_unexpectedChildInTimeZoneIdThrows",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_unexpectedComments",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_unexpectedElementsIgnored",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_unexpectedRootElement",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_unexpectedTextIgnored",
-    "libcore.libcore.timezone.TimeZoneFinderTest#xmlParsing_unknownTimeZoneIdIgnored",
-    "org.apache.harmony.crypto.tests.javax.crypto.CipherInputStream1Test#test_ConstructorLjava_io_InputStreamLjavax_crypto_Cipher",
-    "org.apache.harmony.crypto.tests.javax.crypto.CipherOutputStream1Test#test_ConstructorLjava_io_OutputStreamLjavax_crypto_Cipher",
-    "org.apache.harmony.crypto.tests.javax.crypto.CipherTest#test_",
-    "org.apache.harmony.crypto.tests.javax.crypto.CipherTest#test_doFinalLjava_nio_ByteBufferLjava_nio_ByteBuffer",
-    "org.apache.harmony.crypto.tests.javax.crypto.CipherTest#test_initWithKey",
-    "org.apache.harmony.crypto.tests.javax.crypto.CipherTest#test_initWithSecureRandom",
-    "org.apache.harmony.crypto.tests.javax.crypto.CipherTest#test_updateLjava_nio_ByteBufferLjava_nio_ByteBuffer",
-    "org.apache.harmony.crypto.tests.javax.crypto.EncryptedPrivateKeyInfoTest#test_ROUNDTRIP_GetKeySpecCipher01",
-    "org.apache.harmony.crypto.tests.javax.crypto.EncryptedPrivateKeyInfoTest#test_ROUNDTRIP_GetKeySpecCipher02",
-    "org.apache.harmony.crypto.tests.javax.crypto.EncryptedPrivateKeyInfoTest#test_ROUNDTRIP_GetKeySpecKey01",
-    "org.apache.harmony.crypto.tests.javax.crypto.EncryptedPrivateKeyInfoTest#test_ROUNDTRIP_GetKeySpecKey02",
-    "org.apache.harmony.crypto.tests.javax.crypto.EncryptedPrivateKeyInfoTest#test_ROUNDTRIP_GetKeySpecKeyProvider01",
-    "org.apache.harmony.crypto.tests.javax.crypto.EncryptedPrivateKeyInfoTest#test_ROUNDTRIP_GetKeySpecKeyProvider02",
-    "org.apache.harmony.crypto.tests.javax.crypto.EncryptedPrivateKeyInfoTest#test_ROUNDTRIP_GetKeySpecKeyString01",
-    "org.apache.harmony.crypto.tests.javax.crypto.EncryptedPrivateKeyInfoTest#test_ROUNDTRIP_GetKeySpecKeyString02",
-    "org.apache.harmony.crypto.tests.javax.crypto.EncryptedPrivateKeyInfoTest#test_getAlgName",
-    "org.apache.harmony.crypto.tests.javax.crypto.ExemptionMechanismTest#test_initLjava_security_Key",
-    "org.apache.harmony.crypto.tests.javax.crypto.ExemptionMechanismTest#test_initLjava_security_KeyLjava_security_AlgorithmParameters",
-    "org.apache.harmony.crypto.tests.javax.crypto.ExemptionMechanismTest#test_initLjava_security_KeyLjava_security_spec_AlgorithmParameterSpec",
-    "org.apache.harmony.crypto.tests.javax.crypto.KeyGeneratorTest#testGenerateKey",
-    "org.apache.harmony.crypto.tests.javax.crypto.KeyGeneratorTest#test_initLjava_security_spec_AlgorithmParameterSpec",
-    "org.apache.harmony.crypto.tests.javax.crypto.SealedObjectTest#testDeserialization",
-    "org.apache.harmony.crypto.tests.javax.crypto.SealedObjectTest#testGetAlgorithm",
-    "org.apache.harmony.crypto.tests.javax.crypto.SealedObjectTest#testGetAlgorithmAfterSerialization",
-    "org.apache.harmony.crypto.tests.javax.crypto.SealedObjectTest#testGetObject1",
-    "org.apache.harmony.crypto.tests.javax.crypto.SealedObjectTest#testGetObject2",
-    "org.apache.harmony.crypto.tests.javax.crypto.SealedObjectTest#testGetObject3",
-    "org.apache.harmony.crypto.tests.javax.crypto.SealedObjectTest#testSealedObject1",
-    "org.apache.harmony.crypto.tests.javax.crypto.SecretKeyFactoryTest#test_translateKeyLjavax_crypto_SecretKey",
-    "org.apache.harmony.crypto.tests.javax.crypto.func.CipherAesTest#test_AesISO",
-    "org.apache.harmony.crypto.tests.javax.crypto.func.CipherAesTest#test_AesNoISO",
-    "org.apache.harmony.crypto.tests.javax.crypto.func.CipherAesWrapTest#test_AesWrap",
-    "org.apache.harmony.crypto.tests.javax.crypto.func.CipherDESedeTest#test_DESedeISO",
-    "org.apache.harmony.crypto.tests.javax.crypto.func.CipherDESedeTest#test_DESedeNoISO",
-    "org.apache.harmony.crypto.tests.javax.crypto.func.CipherDESedeWrapTest#test_DESedeWrap",
-    "org.apache.harmony.crypto.tests.javax.crypto.func.CipherDesTest#test_DesISO",
-    "org.apache.harmony.crypto.tests.javax.crypto.func.CipherDesTest#test_DesNoISO",
-    "org.apache.harmony.crypto.tests.javax.crypto.func.CipherPBETest#test_PBEWithMD5AndDES",
-    "org.apache.harmony.crypto.tests.javax.crypto.func.CipherPBETest#test_PBEWithSHAand3KeyTripleDES",
-    "org.apache.harmony.crypto.tests.javax.crypto.func.CipherRSATest#test_RSANoPadding",
-    "org.apache.harmony.crypto.tests.javax.crypto.func.CipherRSATest#test_RSAShortKey",
-    "org.apache.harmony.crypto.tests.javax.crypto.func.KeyGeneratorFunctionalTest#test_",
-    "org.apache.harmony.tests.java.math.BigIntegerTest#test_isProbablePrimeI",
-    "org.apache.harmony.tests.java.math.OldBigIntegerTest#test_ConstructorIILjava_util_Random",
-    "org.apache.harmony.tests.java.math.OldBigIntegerTest#test_isProbablePrimeI",
-    "org.apache.harmony.tests.java.math.OldBigIntegerTest#test_nextProbablePrime",
-    "org.apache.harmony.tests.java.math.OldBigIntegerTest#test_probablePrime",
-    "org.apache.harmony.tests.java.util.ScannerTest#test_ConstructorLjava_nio_file_Path",
-    "org.apache.harmony.tests.java.util.ScannerTest#test_ConstructorLjava_nio_file_PathLjava_lang_String",
-    "org.apache.harmony.tests.java.util.ScannerTest#test_ConstructorLjava_nio_file_PathLjava_lang_String_Exception",
-    "org.apache.harmony.tests.java.util.UUIDTest#test_randomUUID",
-    "org.apache.harmony.tests.javax.security.OldSHA1PRNGSecureRandomTest#testGenerateSeedint02",
-    "org.apache.harmony.tests.javax.security.OldSHA1PRNGSecureRandomTest#testGenerateSeedint03",
-    "org.apache.harmony.tests.javax.security.OldSHA1PRNGSecureRandomTest#testNextBytesbyteArray03",
-    "org.apache.harmony.tests.javax.security.OldSHA1PRNGSecureRandomTest#testSetSeedbyteArray02"
-  ]
-},
-{
-  description: "Test using the getrandom() syscall, only available from Linux 3.17.",
-  result: ERROR,
-  bug: 141230711,
-  modes: [device],
-  names: [
-    "test.java.awt",
-    "test.java.io.ByteArrayInputStream",
-    "test.java.io.ByteArrayOutputStream",
-    "test.java.io.FileReader",
-    "test.java.io.FileWriter",
-    "test.java.io.InputStream",
-    "test.java.io.OutputStream",
-    "test.java.io.PrintStream",
-    "test.java.io.PrintWriter",
-    "test.java.io.Reader",
-    "test.java.io.Writer",
-    "test.java.lang.Boolean",
-    "test.java.lang.ClassLoader",
-    "test.java.lang.Double",
-    "test.java.lang.Float",
-    "test.java.lang.Integer",
-    "test.java.lang.Long",
-    "test.java.lang.StackWalker#main",
-    "test.java.lang.StrictMath.CubeRootTests",
-    "test.java.lang.StrictMath.ExactArithTests",
-    "test.java.lang.StrictMath.Expm1Tests",
-    "test.java.lang.StrictMath.ExpTests",
-    "test.java.lang.StrictMath.HyperbolicTests",
-    "test.java.lang.StrictMath.HypotTests#testAgainstTranslit_shard1",
-    "test.java.lang.StrictMath.HypotTests#testAgainstTranslit_shard2",
-    "test.java.lang.StrictMath.HypotTests#testAgainstTranslit_shard3",
-    "test.java.lang.StrictMath.HypotTests#testAgainstTranslit_shard4",
-    "test.java.lang.StrictMath.HypotTests#testHypot",
-    "test.java.lang.StrictMath.Log1pTests",
-    "test.java.lang.StrictMath.Log10Tests",
-    "test.java.lang.StrictMath.MultiplicationTests",
-    "test.java.lang.StrictMath.PowTests",
-    "test.java.lang.String",
-    "test.java.lang.Thread",
-    "test.java.lang.invoke",
-    "test.java.lang.ref.SoftReference",
-    "test.java.lang.ref.BasicTest",
-    "test.java.lang.ref.EnqueueNullRefTest",
-    "test.java.lang.ref.EnqueuePollRaceTest",
-    "test.java.lang.ref.ReferenceCloneTest",
-    "test.java.lang.ref.ReferenceEnqueuePendingTest",
-    "test.java.math.BigDecimal",
-    "test.java.math.BigInteger#testArithmetic",
-    "test.java.math.BigInteger#testBitCount",
-    "test.java.math.BigInteger#testBitLength",
-    "test.java.math.BigInteger#testbitOps",
-    "test.java.math.BigInteger#testBitwise",
-    "test.java.math.BigInteger#testByteArrayConv",
-    "test.java.math.BigInteger#testConstructor",
-    "test.java.math.BigInteger#testDivideAndReminder",
-    "test.java.math.BigInteger#testDivideLarge",
-    "test.java.math.BigInteger#testModExp",
-    "test.java.math.BigInteger#testMultiplyLarge",
-    "test.java.math.BigInteger#testNextProbablePrime",
-    "test.java.math.BigInteger#testPow",
-    "test.java.math.BigInteger#testSerialize",
-    "test.java.math.BigInteger#testShift",
-    "test.java.math.BigInteger#testSquare",
-    "test.java.math.BigInteger#testSquareLarge",
-    "test.java.math.BigInteger#testSquareRootAndReminder",
-    "test.java.math.BigInteger#testStringConv_generic",
-    "test.java.math.RoundingMode",
-    "test.java.net.DatagramSocket",
-    "test.java.net.Socket",
-    "test.java.net.SocketOptions",
-    "test.java.net.URLDecoder",
-    "test.java.net.URLEncoder",
-    "test.java.nio.channels.Channels",
-    "test.java.nio.channels.SelectionKey",
-    "test.java.nio.channels.Selector",
-    "test.java.nio.file",
-    "test.java.security.cert",
-    "test.java.security.KeyAgreement.KeyAgreementTest",
-    "test.java.security.KeyAgreement.KeySizeTest#testECDHKeySize",
-    "test.java.security.KeyAgreement.KeySpecTest",
-    "test.java.security.KeyAgreement.MultiThreadTest",
-    "test.java.security.KeyAgreement.NegativeTest",
-    "test.java.security.KeyStore",
-    "test.java.security.Provider",
-    "test.java.util.Arrays",
-    "test.java.util.Collection",
-    "test.java.util.Collections",
-    "test.java.util.Date",
-    "test.java.util.EnumMap",
-    "test.java.util.EnumSet",
-    "test.java.util.GregorianCalendar",
-    "test.java.util.LinkedHashMap",
-    "test.java.util.LinkedHashSet",
-    "test.java.util.List",
-    "test.java.util.Map",
-    "test.java.util.Optional",
-    "test.java.util.TimeZone",
-    "test.java.util.concurrent",
-    "test.java.util.function",
-    "test.java.util.stream",
-    "test.java.util.zip.ZipFile"
-  ]
-}
-]
diff --git a/tools/libcore_gcstress_debug_failures.txt b/tools/libcore_gcstress_debug_failures.txt
index 6d987487f5..6fee7e881a 100644
--- a/tools/libcore_gcstress_debug_failures.txt
+++ b/tools/libcore_gcstress_debug_failures.txt
@@ -29,7 +29,8 @@
   description: "Timeouts on host with gcstress and debug.",
   result: EXEC_FAILED,
   modes: [host],
-  names: ["jsr166.ForkJoinPoolTest#testIsQuiescent",
+  names: ["jsr166.ConcurrentSkipListSetTest#testRecursiveSubSets",
+          "jsr166.ForkJoinPoolTest#testIsQuiescent",
           "jsr166.ScheduledExecutorSubclassTest#testTimedInvokeAll4",
           "jsr166.StampedLockTest#testWriteAfterReadLock",
           "jsr166.StampedLockTest#testReadTryLock_Interruptible",
@@ -37,6 +38,7 @@
           "jsr166.StampedLockTest#testReadLockInterruptibly",
           "jsr166.StampedLockTest#testWriteLockInterruptibly",
           "jsr166.TimeUnitTest#testConvert",
+          "jsr166.TreeMapTest#testRecursiveSubMaps",
           "jsr166.TreeSetTest#testRecursiveSubSets",
           "libcore.java.lang.OldThreadTest#test_getState",
           "libcore.java.lang.StringTest#testFastPathString_wellFormedUtf8Sequence",
@@ -53,7 +55,8 @@
           "org.apache.harmony.tests.java.lang.ref.ReferenceQueueTest#test_removeJ",
           "org.apache.harmony.tests.java.lang.ProcessManagerTest#testSleep",
           "org.apache.harmony.tests.java.util.TimerTest#testOverdueTaskExecutesImmediately",
-          "org.apache.harmony.tests.java.util.WeakHashMapTest#test_keySet_hasNext"
+          "org.apache.harmony.tests.java.util.WeakHashMapTest#test_keySet_hasNext",
+          "test.java.util.Collections.RacingCollections#main"
   ]
 },
 {
diff --git a/tools/luci/config/generated/cr-buildbucket.cfg b/tools/luci/config/generated/cr-buildbucket.cfg
index 7ff98559f9..cef584cf10 100644
--- a/tools/luci/config/generated/cr-buildbucket.cfg
+++ b/tools/luci/config/generated/cr-buildbucket.cfg
@@ -11,7 +11,7 @@ buckets {
     group: "project-art-admins"
   }
   acls {
-    group: "all"
+    group: "googlers"
   }
   swarming {
     builders {
@@ -133,7 +133,7 @@ buckets {
       }
     }
     builders {
-      name: "host.x86.gsctress.32"
+      name: "host.x86.gcstress.32"
       swarming_host: "chromium-swarm.appspot.com"
       dimensions: "cores:8"
       dimensions: "os:Ubuntu"
@@ -164,7 +164,7 @@ buckets {
       }
     }
     builders {
-      name: "host.x86.gsctress.64"
+      name: "host.x86.gcstress.64"
       swarming_host: "chromium-swarm.appspot.com"
       dimensions: "cores:8"
       dimensions: "os:Ubuntu"
@@ -194,6 +194,66 @@ buckets {
         value: 100
       }
     }
+    builders {
+      name: "host.x86.gcstress.cmc.32"
+      swarming_host: "chromium-swarm.appspot.com"
+      dimensions: "cores:8"
+      dimensions: "os:Ubuntu"
+      dimensions: "pool:luci.art.ci"
+      recipe {
+        name: "art"
+        cipd_package: "infra/recipe_bundles/chromium.googlesource.com/chromium/tools/build"
+        cipd_version: "refs/heads/main"
+        properties_j: "bitness:32"
+        properties_j: "builder_group:\"client.art\""
+        properties_j: "debug:true"
+        properties_j: "gcstress:true"
+        properties_j: "generational_cc:true"
+        properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--debug\",\"--gcstress\"]"
+      }
+      execution_timeout_secs: 108000
+      expiration_secs: 61200
+      caches {
+        name: "art"
+        path: "art"
+      }
+      build_numbers: YES
+      service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "luci.recipes.use_python3"
+        value: 100
+      }
+    }
+    builders {
+      name: "host.x86.gcstress.cmc.64"
+      swarming_host: "chromium-swarm.appspot.com"
+      dimensions: "cores:8"
+      dimensions: "os:Ubuntu"
+      dimensions: "pool:luci.art.ci"
+      recipe {
+        name: "art"
+        cipd_package: "infra/recipe_bundles/chromium.googlesource.com/chromium/tools/build"
+        cipd_version: "refs/heads/main"
+        properties_j: "bitness:64"
+        properties_j: "builder_group:\"client.art\""
+        properties_j: "debug:true"
+        properties_j: "gcstress:true"
+        properties_j: "generational_cc:true"
+        properties_j: "testrunner_args:[\"--verbose\",\"--host\",\"--debug\",\"--gcstress\"]"
+      }
+      execution_timeout_secs: 108000
+      expiration_secs: 61200
+      caches {
+        name: "art"
+        path: "art"
+      }
+      build_numbers: YES
+      service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "luci.recipes.use_python3"
+        value: 100
+      }
+    }
     builders {
       name: "host.x86.ndebug.32"
       swarming_host: "chromium-swarm.appspot.com"
@@ -511,7 +571,7 @@ buckets {
     builders {
       name: "target.arm.cmc.32"
       swarming_host: "chromium-swarm.appspot.com"
-      dimensions: "device_os:A"
+      dimensions: "device_os:A|B"
       dimensions: "os:Android"
       dimensions: "pool:luci.art.ci"
       recipe {
@@ -542,7 +602,7 @@ buckets {
     builders {
       name: "target.arm.cmc.64"
       swarming_host: "chromium-swarm.appspot.com"
-      dimensions: "device_os:A"
+      dimensions: "device_os:A|B"
       dimensions: "os:Android"
       dimensions: "pool:luci.art.ci"
       recipe {
@@ -571,7 +631,7 @@ buckets {
       }
     }
     builders {
-      name: "target.arm.gsctress.32"
+      name: "target.arm.gcstress.32"
       swarming_host: "chromium-swarm.appspot.com"
       dimensions: "device_os:S"
       dimensions: "os:Android"
@@ -584,7 +644,7 @@ buckets {
         properties_j: "builder_group:\"client.art\""
         properties_j: "concurrent_collector:true"
         properties_j: "debug:true"
-        properties_j: "device:\"target.arm.gsctress.32\""
+        properties_j: "device:\"target.arm.gcstress.32\""
         properties_j: "gcstress:true"
         properties_j: "generational_cc:true"
         properties_j: "product:\"arm_krait\""
@@ -604,7 +664,7 @@ buckets {
       }
     }
     builders {
-      name: "target.arm.gsctress.64"
+      name: "target.arm.gcstress.64"
       swarming_host: "chromium-swarm.appspot.com"
       dimensions: "device_os:S"
       dimensions: "os:Android"
@@ -617,7 +677,71 @@ buckets {
         properties_j: "builder_group:\"client.art\""
         properties_j: "concurrent_collector:true"
         properties_j: "debug:true"
-        properties_j: "device:\"target.arm.gsctress.64\""
+        properties_j: "device:\"target.arm.gcstress.64\""
+        properties_j: "gcstress:true"
+        properties_j: "generational_cc:true"
+        properties_j: "product:\"armv8\""
+        properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--debug\",\"--gcstress\"]"
+      }
+      execution_timeout_secs: 108000
+      expiration_secs: 61200
+      caches {
+        name: "art"
+        path: "art"
+      }
+      build_numbers: YES
+      service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "luci.recipes.use_python3"
+        value: 100
+      }
+    }
+    builders {
+      name: "target.arm.gcstress.cmc.32"
+      swarming_host: "chromium-swarm.appspot.com"
+      dimensions: "device_os:A|B"
+      dimensions: "os:Android"
+      dimensions: "pool:luci.art.ci"
+      recipe {
+        name: "art"
+        cipd_package: "infra/recipe_bundles/chromium.googlesource.com/chromium/tools/build"
+        cipd_version: "refs/heads/main"
+        properties_j: "bitness:32"
+        properties_j: "builder_group:\"client.art\""
+        properties_j: "debug:true"
+        properties_j: "device:\"target.arm.gcstress.cmc.32\""
+        properties_j: "gcstress:true"
+        properties_j: "generational_cc:true"
+        properties_j: "product:\"arm_krait\""
+        properties_j: "testrunner_args:[\"--target\",\"--verbose\",\"--debug\",\"--gcstress\"]"
+      }
+      execution_timeout_secs: 108000
+      expiration_secs: 61200
+      caches {
+        name: "art"
+        path: "art"
+      }
+      build_numbers: YES
+      service_account: "art-ci-builder@chops-service-accounts.iam.gserviceaccount.com"
+      experiments {
+        key: "luci.recipes.use_python3"
+        value: 100
+      }
+    }
+    builders {
+      name: "target.arm.gcstress.cmc.64"
+      swarming_host: "chromium-swarm.appspot.com"
+      dimensions: "device_os:A|B"
+      dimensions: "os:Android"
+      dimensions: "pool:luci.art.ci"
+      recipe {
+        name: "art"
+        cipd_package: "infra/recipe_bundles/chromium.googlesource.com/chromium/tools/build"
+        cipd_version: "refs/heads/main"
+        properties_j: "bitness:64"
+        properties_j: "builder_group:\"client.art\""
+        properties_j: "debug:true"
+        properties_j: "device:\"target.arm.gcstress.cmc.64\""
         properties_j: "gcstress:true"
         properties_j: "generational_cc:true"
         properties_j: "product:\"armv8\""
@@ -774,7 +898,7 @@ buckets {
     group: "project-art-admins"
   }
   acls {
-    group: "all"
+    group: "googlers"
   }
   constraints {
     pools: "luci.art.ci"
diff --git a/tools/luci/config/generated/luci-logdog.cfg b/tools/luci/config/generated/luci-logdog.cfg
index 01a391261d..9d817a1e14 100644
--- a/tools/luci/config/generated/luci-logdog.cfg
+++ b/tools/luci/config/generated/luci-logdog.cfg
@@ -4,6 +4,6 @@
 # For the schema of this file, see ProjectConfig message:
 #   https://config.luci.app/schemas/projects:luci-logdog.cfg
 
-reader_auth_groups: "all"
+reader_auth_groups: "googlers"
 writer_auth_groups: "luci-logdog-chromium-writers"
 archive_gs_bucket: "chromium-luci-logdog"
diff --git a/tools/luci/config/generated/luci-milo.cfg b/tools/luci/config/generated/luci-milo.cfg
index 46d170e40f..92f3addf78 100644
--- a/tools/luci/config/generated/luci-milo.cfg
+++ b/tools/luci/config/generated/luci-milo.cfg
@@ -36,8 +36,13 @@ consoles {
     short_name: "32"
   }
   builders {
-    name: "buildbucket/luci.art.ci/target.arm.gsctress.32"
-    category: "target.arm|gsctress"
+    name: "buildbucket/luci.art.ci/target.arm.gcstress.32"
+    category: "target.arm|gcstress"
+    short_name: "32"
+  }
+  builders {
+    name: "buildbucket/luci.art.ci/target.arm.gcstress.cmc.32"
+    category: "target.arm|gcstress-cmc"
     short_name: "32"
   }
   builders {
@@ -56,8 +61,13 @@ consoles {
     short_name: "64"
   }
   builders {
-    name: "buildbucket/luci.art.ci/target.arm.gsctress.64"
-    category: "target.arm|gsctress"
+    name: "buildbucket/luci.art.ci/target.arm.gcstress.64"
+    category: "target.arm|gcstress"
+    short_name: "64"
+  }
+  builders {
+    name: "buildbucket/luci.art.ci/target.arm.gcstress.cmc.64"
+    category: "target.arm|gcstress-cmc"
     short_name: "64"
   }
   builders {
@@ -91,8 +101,13 @@ consoles {
     short_name: "32"
   }
   builders {
-    name: "buildbucket/luci.art.ci/host.x86.gsctress.32"
-    category: "host.x86|gsctress"
+    name: "buildbucket/luci.art.ci/host.x86.gcstress.32"
+    category: "host.x86|gcstress"
+    short_name: "32"
+  }
+  builders {
+    name: "buildbucket/luci.art.ci/host.x86.gcstress.cmc.32"
+    category: "host.x86|gcstress-cmc"
     short_name: "32"
   }
   builders {
@@ -116,8 +131,13 @@ consoles {
     short_name: "64"
   }
   builders {
-    name: "buildbucket/luci.art.ci/host.x86.gsctress.64"
-    category: "host.x86|gsctress"
+    name: "buildbucket/luci.art.ci/host.x86.gcstress.64"
+    category: "host.x86|gcstress"
+    short_name: "64"
+  }
+  builders {
+    name: "buildbucket/luci.art.ci/host.x86.gcstress.cmc.64"
+    category: "host.x86|gcstress-cmc"
     short_name: "64"
   }
   builders {
diff --git a/tools/luci/config/generated/luci-notify.cfg b/tools/luci/config/generated/luci-notify.cfg
index e070750f3f..e21504573d 100644
--- a/tools/luci/config/generated/luci-notify.cfg
+++ b/tools/luci/config/generated/luci-notify.cfg
@@ -66,7 +66,7 @@ notifiers {
   }
   builders {
     bucket: "ci"
-    name: "host.x86.gsctress.32"
+    name: "host.x86.gcstress.32"
   }
 }
 notifiers {
@@ -79,7 +79,33 @@ notifiers {
   }
   builders {
     bucket: "ci"
-    name: "host.x86.gsctress.64"
+    name: "host.x86.gcstress.64"
+  }
+}
+notifiers {
+  notifications {
+    on_new_status: FAILURE
+    on_new_status: INFRA_FAILURE
+    email {
+      recipients: "art-team+chromium-buildbot@google.com"
+    }
+  }
+  builders {
+    bucket: "ci"
+    name: "host.x86.gcstress.cmc.32"
+  }
+}
+notifiers {
+  notifications {
+    on_new_status: FAILURE
+    on_new_status: INFRA_FAILURE
+    email {
+      recipients: "art-team+chromium-buildbot@google.com"
+    }
+  }
+  builders {
+    bucket: "ci"
+    name: "host.x86.gcstress.cmc.64"
   }
 }
 notifiers {
@@ -248,7 +274,33 @@ notifiers {
   }
   builders {
     bucket: "ci"
-    name: "target.arm.gsctress.32"
+    name: "target.arm.gcstress.32"
+  }
+}
+notifiers {
+  notifications {
+    on_new_status: FAILURE
+    on_new_status: INFRA_FAILURE
+    email {
+      recipients: "art-team+chromium-buildbot@google.com"
+    }
+  }
+  builders {
+    bucket: "ci"
+    name: "target.arm.gcstress.64"
+  }
+}
+notifiers {
+  notifications {
+    on_new_status: FAILURE
+    on_new_status: INFRA_FAILURE
+    email {
+      recipients: "art-team+chromium-buildbot@google.com"
+    }
+  }
+  builders {
+    bucket: "ci"
+    name: "target.arm.gcstress.cmc.32"
   }
 }
 notifiers {
@@ -261,7 +313,7 @@ notifiers {
   }
   builders {
     bucket: "ci"
-    name: "target.arm.gsctress.64"
+    name: "target.arm.gcstress.cmc.64"
   }
 }
 notifiers {
diff --git a/tools/luci/config/generated/luci-scheduler.cfg b/tools/luci/config/generated/luci-scheduler.cfg
index 4bdd3ebd38..a2b426ee19 100644
--- a/tools/luci/config/generated/luci-scheduler.cfg
+++ b/tools/luci/config/generated/luci-scheduler.cfg
@@ -45,23 +45,43 @@ job {
   }
 }
 job {
-  id: "host.x86.gsctress.32"
+  id: "host.x86.gcstress.32"
   realm: "ci"
   acl_sets: "ci"
   buildbucket {
     server: "cr-buildbucket.appspot.com"
     bucket: "ci"
-    builder: "host.x86.gsctress.32"
+    builder: "host.x86.gcstress.32"
   }
 }
 job {
-  id: "host.x86.gsctress.64"
+  id: "host.x86.gcstress.64"
   realm: "ci"
   acl_sets: "ci"
   buildbucket {
     server: "cr-buildbucket.appspot.com"
     bucket: "ci"
-    builder: "host.x86.gsctress.64"
+    builder: "host.x86.gcstress.64"
+  }
+}
+job {
+  id: "host.x86.gcstress.cmc.32"
+  realm: "ci"
+  acl_sets: "ci"
+  buildbucket {
+    server: "cr-buildbucket.appspot.com"
+    bucket: "ci"
+    builder: "host.x86.gcstress.cmc.32"
+  }
+}
+job {
+  id: "host.x86.gcstress.cmc.64"
+  realm: "ci"
+  acl_sets: "ci"
+  buildbucket {
+    server: "cr-buildbucket.appspot.com"
+    bucket: "ci"
+    builder: "host.x86.gcstress.cmc.64"
   }
 }
 job {
@@ -185,23 +205,43 @@ job {
   }
 }
 job {
-  id: "target.arm.gsctress.32"
+  id: "target.arm.gcstress.32"
+  realm: "ci"
+  acl_sets: "ci"
+  buildbucket {
+    server: "cr-buildbucket.appspot.com"
+    bucket: "ci"
+    builder: "target.arm.gcstress.32"
+  }
+}
+job {
+  id: "target.arm.gcstress.64"
+  realm: "ci"
+  acl_sets: "ci"
+  buildbucket {
+    server: "cr-buildbucket.appspot.com"
+    bucket: "ci"
+    builder: "target.arm.gcstress.64"
+  }
+}
+job {
+  id: "target.arm.gcstress.cmc.32"
   realm: "ci"
   acl_sets: "ci"
   buildbucket {
     server: "cr-buildbucket.appspot.com"
     bucket: "ci"
-    builder: "target.arm.gsctress.32"
+    builder: "target.arm.gcstress.cmc.32"
   }
 }
 job {
-  id: "target.arm.gsctress.64"
+  id: "target.arm.gcstress.cmc.64"
   realm: "ci"
   acl_sets: "ci"
   buildbucket {
     server: "cr-buildbucket.appspot.com"
     bucket: "ci"
-    builder: "target.arm.gsctress.64"
+    builder: "target.arm.gcstress.cmc.64"
   }
 }
 job {
@@ -252,8 +292,10 @@ trigger {
   triggers: "host.x86.64"
   triggers: "host.x86.cmc.32"
   triggers: "host.x86.cmc.64"
-  triggers: "host.x86.gsctress.32"
-  triggers: "host.x86.gsctress.64"
+  triggers: "host.x86.gcstress.32"
+  triggers: "host.x86.gcstress.64"
+  triggers: "host.x86.gcstress.cmc.32"
+  triggers: "host.x86.gcstress.cmc.64"
   triggers: "host.x86.ndebug.32"
   triggers: "host.x86.ndebug.64"
   triggers: "host.x86.ngen-cmc.32"
@@ -266,8 +308,10 @@ trigger {
   triggers: "target.arm.64"
   triggers: "target.arm.cmc.32"
   triggers: "target.arm.cmc.64"
-  triggers: "target.arm.gsctress.32"
-  triggers: "target.arm.gsctress.64"
+  triggers: "target.arm.gcstress.32"
+  triggers: "target.arm.gcstress.64"
+  triggers: "target.arm.gcstress.cmc.32"
+  triggers: "target.arm.gcstress.cmc.64"
   triggers: "target.arm.ndebug.32"
   triggers: "target.arm.ndebug.64"
   triggers: "target.arm.poison.32"
@@ -285,8 +329,10 @@ trigger {
   triggers: "host.x86.64"
   triggers: "host.x86.cmc.32"
   triggers: "host.x86.cmc.64"
-  triggers: "host.x86.gsctress.32"
-  triggers: "host.x86.gsctress.64"
+  triggers: "host.x86.gcstress.32"
+  triggers: "host.x86.gcstress.64"
+  triggers: "host.x86.gcstress.cmc.32"
+  triggers: "host.x86.gcstress.cmc.64"
   triggers: "host.x86.ndebug.32"
   triggers: "host.x86.ndebug.64"
   triggers: "host.x86.ngen-cmc.32"
@@ -299,8 +345,10 @@ trigger {
   triggers: "target.arm.64"
   triggers: "target.arm.cmc.32"
   triggers: "target.arm.cmc.64"
-  triggers: "target.arm.gsctress.32"
-  triggers: "target.arm.gsctress.64"
+  triggers: "target.arm.gcstress.32"
+  triggers: "target.arm.gcstress.64"
+  triggers: "target.arm.gcstress.cmc.32"
+  triggers: "target.arm.gcstress.cmc.64"
   triggers: "target.arm.ndebug.32"
   triggers: "target.arm.ndebug.64"
   triggers: "target.arm.poison.32"
@@ -318,8 +366,10 @@ trigger {
   triggers: "host.x86.64"
   triggers: "host.x86.cmc.32"
   triggers: "host.x86.cmc.64"
-  triggers: "host.x86.gsctress.32"
-  triggers: "host.x86.gsctress.64"
+  triggers: "host.x86.gcstress.32"
+  triggers: "host.x86.gcstress.64"
+  triggers: "host.x86.gcstress.cmc.32"
+  triggers: "host.x86.gcstress.cmc.64"
   triggers: "host.x86.ndebug.32"
   triggers: "host.x86.ndebug.64"
   triggers: "host.x86.ngen-cmc.32"
@@ -332,8 +382,10 @@ trigger {
   triggers: "target.arm.64"
   triggers: "target.arm.cmc.32"
   triggers: "target.arm.cmc.64"
-  triggers: "target.arm.gsctress.32"
-  triggers: "target.arm.gsctress.64"
+  triggers: "target.arm.gcstress.32"
+  triggers: "target.arm.gcstress.64"
+  triggers: "target.arm.gcstress.cmc.32"
+  triggers: "target.arm.gcstress.cmc.64"
   triggers: "target.arm.ndebug.32"
   triggers: "target.arm.ndebug.64"
   triggers: "target.arm.poison.32"
@@ -351,8 +403,10 @@ trigger {
   triggers: "host.x86.64"
   triggers: "host.x86.cmc.32"
   triggers: "host.x86.cmc.64"
-  triggers: "host.x86.gsctress.32"
-  triggers: "host.x86.gsctress.64"
+  triggers: "host.x86.gcstress.32"
+  triggers: "host.x86.gcstress.64"
+  triggers: "host.x86.gcstress.cmc.32"
+  triggers: "host.x86.gcstress.cmc.64"
   triggers: "host.x86.ndebug.32"
   triggers: "host.x86.ndebug.64"
   triggers: "host.x86.ngen-cmc.32"
@@ -365,8 +419,10 @@ trigger {
   triggers: "target.arm.64"
   triggers: "target.arm.cmc.32"
   triggers: "target.arm.cmc.64"
-  triggers: "target.arm.gsctress.32"
-  triggers: "target.arm.gsctress.64"
+  triggers: "target.arm.gcstress.32"
+  triggers: "target.arm.gcstress.64"
+  triggers: "target.arm.gcstress.cmc.32"
+  triggers: "target.arm.gcstress.cmc.64"
   triggers: "target.arm.ndebug.32"
   triggers: "target.arm.ndebug.64"
   triggers: "target.arm.poison.32"
@@ -383,6 +439,6 @@ acl_sets {
     granted_to: "group:project-art-admins"
   }
   acls {
-    granted_to: "group:all"
+    granted_to: "group:googlers"
   }
 }
diff --git a/tools/luci/config/generated/project.cfg b/tools/luci/config/generated/project.cfg
index 12b1ae6e69..5619b9ab83 100644
--- a/tools/luci/config/generated/project.cfg
+++ b/tools/luci/config/generated/project.cfg
@@ -5,9 +5,9 @@
 #   https://config.luci.app/schemas/projects:project.cfg
 
 name: "art"
-access: "group:all"
+access: "group:googlers"
 lucicfg {
-  version: "1.43.14"
+  version: "1.44.1"
   package_dir: ".."
   config_dir: "generated"
   entry_point: "main.star"
diff --git a/tools/luci/config/generated/realms.cfg b/tools/luci/config/generated/realms.cfg
index d439ea168c..ade0991042 100644
--- a/tools/luci/config/generated/realms.cfg
+++ b/tools/luci/config/generated/realms.cfg
@@ -12,15 +12,15 @@ realms {
   }
   bindings {
     role: "role/buildbucket.reader"
-    principals: "group:all"
+    principals: "group:googlers"
   }
   bindings {
     role: "role/configs.reader"
-    principals: "group:all"
+    principals: "group:googlers"
   }
   bindings {
     role: "role/logdog.reader"
-    principals: "group:all"
+    principals: "group:googlers"
   }
   bindings {
     role: "role/logdog.writer"
@@ -32,7 +32,7 @@ realms {
   }
   bindings {
     role: "role/scheduler.reader"
-    principals: "group:all"
+    principals: "group:googlers"
   }
   bindings {
     role: "role/swarming.poolOwner"
@@ -44,7 +44,7 @@ realms {
   }
   bindings {
     role: "role/swarming.poolViewer"
-    principals: "group:all"
+    principals: "group:googlers"
   }
   bindings {
     role: "role/swarming.taskTriggerer"
diff --git a/tools/luci/config/main.star b/tools/luci/config/main.star
index a2e4e2afa1..42ee495bbc 100755
--- a/tools/luci/config/main.star
+++ b/tools/luci/config/main.star
@@ -47,7 +47,6 @@ luci.project(
     scheduler = "luci-scheduler.appspot.com",
     swarming = "chromium-swarm.appspot.com",
     acls = [
-        # Publicly readable.
         acl.entry(
             roles = [
                 acl.BUILDBUCKET_READER,
@@ -55,7 +54,7 @@ luci.project(
                 acl.PROJECT_CONFIGS_READER,
                 acl.SCHEDULER_READER,
             ],
-            groups = "all",
+            groups = "googlers",
         ),
         acl.entry(
             roles = [
@@ -76,7 +75,7 @@ luci.project(
         ),
         luci.binding(
             roles = "role/swarming.poolViewer",
-            groups = "all",
+            groups = "googlers",
         ),
     ],
 )
@@ -239,7 +238,7 @@ def add_builder(mode,
 
     # Create builder name based on the configuaration parameters.
     name = mode + '.' + arch
-    name += '.gsctress' if gcstress else ''
+    name += '.gcstress' if gcstress else ''
     name += '.poison' if poison else ''
     name += '.ngen' if ngen else ''
     name += '.cmc' if cmc else ''
@@ -253,6 +252,7 @@ def add_builder(mode,
     category = name.replace(".", "|")
     category = category.replace("host|", "host.")
     category = category.replace("target|", "target.")
+    category = category.replace("gcstress|cmc", "gcstress-cmc")
 
     product = None
     if arch == "arm":
@@ -263,13 +263,13 @@ def add_builder(mode,
     dimensions = {"os": "Android" if mode == "target" else "Ubuntu"}
     if mode == "target":
       if cmc:
-        # Request devices running Android 24Q3 (`AP1A` builds) for
+        # Request devices running at least Android 24Q3 (`AP1A` builds) for
         # (`userfaultfd`-based) Concurrent Mark-Compact GC configurations.
         # Currently (as of 2024-08-22), the only devices within the device pool
         # allocated to ART that are running `AP1A` builds are Pixel 6 devices
         # (all other device types are running older Android versions), which are
         # also the only device model supporting `userfaultfd` among that pool.
-        dimensions |= {"device_os": "A"}
+        dimensions |= {"device_os": "A|B"}
       else:
         # Run all other configurations on Android S since it is the oldest we support.
         # Other than the `AP1A` builds above, all other devices are flashed to `SP2A`.
@@ -321,6 +321,7 @@ def add_builders():
       add_builder(mode, arch, bitness, cmc=True)
       add_builder(mode, arch, bitness, poison=True)
       add_builder(mode, arch, bitness, gcstress=True)
+      add_builder(mode, arch, bitness, cmc=True, gcstress=True)
   add_builder('qemu', 'arm', bitness=64)
   add_builder('qemu', 'riscv', bitness=64)
 
diff --git a/tools/run-libcore-tests.py b/tools/run-libcore-tests.py
index 244f912c72..486e21354c 100755
--- a/tools/run-libcore-tests.py
+++ b/tools/run-libcore-tests.py
@@ -34,8 +34,6 @@ def parse_args():
                       help='Use debug version of ART (device|host only).')
   parser.add_argument('--dry-run', action='store_true',
                       help='Print vogar command-line, but do not run.')
-  parser.add_argument('--no-getrandom', action='store_false', dest='getrandom',
-                      help='Ignore failures from getrandom() (for kernel < 3.17).')
   parser.add_argument('--no-jit', action='store_false', dest='jit',
                       help='Disable JIT (device|host only).')
   parser.add_argument('--gcstress', action='store_true',
@@ -151,6 +149,8 @@ LIBCORE_TEST_NAMES = [
   "test.java.lang.ref.EnqueuePollRaceTest",
   "test.java.lang.ref.ReferenceCloneTest",
   "test.java.lang.ref.ReferenceEnqueuePendingTest",
+  # test.java.lang.runtime
+  "test.java.lang.runtime.SwitchBootstrapsTest",
   # test.java.math
   "test.java.math.BigDecimal",
   # Sharded test.java.math.BigInteger
@@ -268,104 +268,6 @@ DISABLED_GCSTRESS_DEBUG_TESTS = {
   "test.java.util.Collection",
 }
 
-DISABLED_FUGU_TESTS = {
-  "org.apache.harmony.luni.tests.internal.net.www.protocol.http.HttpURLConnection",
-  "org.apache.harmony.luni.tests.internal.net.www.protocol.https.HttpsURLConnection",
-  "test.java.awt",
-  "test.java.io.ByteArrayInputStream",
-  "test.java.io.ByteArrayOutputStream",
-  "test.java.io.InputStream",
-  "test.java.io.OutputStream",
-  "test.java.io.PrintStream",
-  "test.java.io.PrintWriter",
-  "test.java.io.Reader",
-  "test.java.io.Writer",
-  "test.java.lang.Boolean",
-  "test.java.lang.ClassLoader",
-  "test.java.lang.Double",
-  "test.java.lang.Float",
-  "test.java.lang.Integer",
-  "test.java.lang.Long",
-  "test.java.lang.StrictMath.CubeRootTests",
-  "test.java.lang.StrictMath.Expm1Tests",
-  "test.java.lang.StrictMath.ExpTests",
-  "test.java.lang.StrictMath.HyperbolicTests",
-  "test.java.lang.StrictMath.HypotTests#testAgainstTranslit_shard1",
-  "test.java.lang.StrictMath.HypotTests#testAgainstTranslit_shard2",
-  "test.java.lang.StrictMath.HypotTests#testAgainstTranslit_shard3",
-  "test.java.lang.StrictMath.HypotTests#testAgainstTranslit_shard4",
-  "test.java.lang.StrictMath.HypotTests#testHypot",
-  "test.java.lang.StrictMath.Log1pTests",
-  "test.java.lang.StrictMath.Log10Tests",
-  "test.java.lang.StrictMath.MultiplicationTests",
-  "test.java.lang.StrictMath.PowTests",
-  "test.java.lang.String",
-  "test.java.lang.Thread",
-  "test.java.lang.invoke",
-  "test.java.lang.ref.SoftReference",
-  "test.java.lang.ref.BasicTest",
-  "test.java.lang.ref.EnqueueNullRefTest",
-  "test.java.lang.ref.EnqueuePollRaceTest",
-  "test.java.lang.ref.ReferenceCloneTest",
-  "test.java.lang.ref.ReferenceEnqueuePendingTest",
-  "test.java.math.BigDecimal",
-  "test.java.math.BigInteger#testArithmetic",
-  "test.java.math.BigInteger#testBitCount",
-  "test.java.math.BigInteger#testBitLength",
-  "test.java.math.BigInteger#testbitOps",
-  "test.java.math.BigInteger#testBitwise",
-  "test.java.math.BigInteger#testByteArrayConv",
-  "test.java.math.BigInteger#testConstructor",
-  "test.java.math.BigInteger#testDivideAndReminder",
-  "test.java.math.BigInteger#testDivideLarge",
-  "test.java.math.BigInteger#testModExp",
-  "test.java.math.BigInteger#testMultiplyLarge",
-  "test.java.math.BigInteger#testNextProbablePrime",
-  "test.java.math.BigInteger#testPow",
-  "test.java.math.BigInteger#testSerialize",
-  "test.java.math.BigInteger#testShift",
-  "test.java.math.BigInteger#testSquare",
-  "test.java.math.BigInteger#testSquareLarge",
-  "test.java.math.BigInteger#testSquareRootAndReminder",
-  "test.java.math.BigInteger#testStringConv_generic",
-  "test.java.math.RoundingMode",
-  "test.java.net.DatagramSocket",
-  "test.java.net.Socket",
-  "test.java.net.SocketOptions",
-  "test.java.net.URLDecoder",
-  "test.java.net.URLEncoder",
-  "test.java.nio.channels.Channels",
-  "test.java.nio.channels.SelectionKey",
-  "test.java.nio.channels.Selector",
-  "test.java.nio.file",
-  "test.java.security.cert",
-  "test.java.security.KeyAgreement.KeyAgreementTest",
-  "test.java.security.KeyAgreement.KeySizeTest#testECDHKeySize",
-  "test.java.security.KeyAgreement.KeySpecTest",
-  "test.java.security.KeyAgreement.MultiThreadTest",
-  "test.java.security.KeyAgreement.NegativeTest",
-  "test.java.security.KeyStore",
-  "test.java.security.Provider",
-  "test.java.time",
-  "test.java.util.Arrays",
-  "test.java.util.Collection",
-  "test.java.util.Collections",
-  "test.java.util.Date",
-  "test.java.util.EnumMap",
-  "test.java.util.EnumSet",
-  "test.java.util.GregorianCalendar",
-  "test.java.util.LinkedHashMap",
-  "test.java.util.LinkedHashSet",
-  "test.java.util.List",
-  "test.java.util.Map",
-  "test.java.util.Optional",
-  "test.java.util.TestFormatter",
-  "test.java.util.TimeZone",
-  "test.java.util.function",
-  "test.java.util.stream",
-  "tck.java.time",
-}
-
 def get_jar_filename(classpath):
   base_path = (ANDROID_PRODUCT_OUT + "/../..") if ANDROID_PRODUCT_OUT else "out/target"
   base_path = os.path.normpath(base_path)  # Normalize ".." components for readability.
@@ -386,10 +288,8 @@ def get_expected_failures():
       failures.append("art/tools/libcore_gcstress_failures.txt")
     if args.gcstress and args.debug:
       failures.append("art/tools/libcore_gcstress_debug_failures.txt")
-    if args.debug and not args.gcstress and args.getrandom:
+    if args.debug and not args.gcstress:
       failures.append("art/tools/libcore_debug_failures.txt")
-    if not args.getrandom:
-      failures.append("art/tools/libcore_fugu_failures.txt")
   return failures
 
 def get_test_names():
@@ -402,10 +302,6 @@ def get_test_names():
     test_names = list(filter(lambda x: x not in SLOW_OJLUNI_TESTS, test_names))
   if args.gcstress and args.debug:
     test_names = list(filter(lambda x: x not in DISABLED_GCSTRESS_DEBUG_TESTS, test_names))
-  if not args.getrandom:
-    # Disable libcore.highmemorytest due to limited ram on fugu. http://b/258173036
-    test_names = list(filter(lambda x: x not in DISABLED_FUGU_TESTS and
-                                       not x.startswith("libcore.highmemorytest"), test_names))
   return test_names
 
 def get_vogar_command(test_name):
@@ -430,11 +326,6 @@ def get_vogar_command(test_name):
   if args.debug:
     cmd.append("--vm-arg -XXlib:libartd.so --vm-arg -XX:SlowDebug=true")
 
-  # The only device in go/art-buildbot without getrandom is fugu. We limit the amount of memory
-  # per runtime for fugu to avoid low memory killer, fugu has 4-cores 1GB RAM (b/258171768).
-  if not args.getrandom:
-    cmd.append("--vm-arg -Xmx128M")
-
   if args.mode == "device":
     if ART_TEST_CHROOT:
       cmd.append(f"--chroot {ART_TEST_CHROOT} --device-dir=/tmp/vogar/test-{test_name}")
diff --git a/tools/trace_parser/Android.bp b/tools/trace_parser/Android.bp
new file mode 100644
index 0000000000..f28af64c7f
--- /dev/null
+++ b/tools/trace_parser/Android.bp
@@ -0,0 +1,41 @@
+//
+// Copyright (C) 2025 The Android Open Source Project
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+//
+
+package {
+    // See: http://go/android-license-faq
+    // A large-scale-change added 'default_applicable_licenses' to import
+    // all of the 'license_kinds' from "art_license"
+    // to get the below license kinds:
+    //   SPDX-license-identifier-Apache-2.0
+    default_applicable_licenses: ["art_license"],
+}
+
+art_cc_binary {
+    name: "long_running_method_trace_parser",
+    defaults: ["art_debug_defaults"],
+    host_supported: true,
+    device_supported: false,
+    srcs: [
+        "long_running_method_trace_parser.cc",
+    ],
+    static_libs: [
+        "libbase",
+        "libartbase",
+        "liblog",
+        "libz",
+        "libziparchive",
+    ],
+}
diff --git a/tools/trace_parser/long_running_method_trace_parser.cc b/tools/trace_parser/long_running_method_trace_parser.cc
new file mode 100644
index 0000000000..9645a445f8
--- /dev/null
+++ b/tools/trace_parser/long_running_method_trace_parser.cc
@@ -0,0 +1,252 @@
+/*
+ * Copyright (C) 2025 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <stdio.h>
+
+#include <map>
+#include <memory>
+
+#include "base/leb128.h"
+#include "base/os.h"
+#include "base/unix_file/fd_file.h"
+
+namespace art {
+
+// These constants are defined in the ART sources in the following files:
+//
+// - art/runtime/trace.h
+// - art/runtime/trace_profile.cc
+static const int kThreadInfoHeaderV2 = 0;
+static const int kMethodInfoHeaderV2 = 1;
+static const int kEntryHeaderV2 = 2;
+static const int kMethodEntry = 0;
+static const int kMethodExit = 1;
+static const int kAlwaysOnMethodInfoHeaderSize = 11;
+static const int kAlwaysOnTraceHeaderSize = 12;
+
+uint64_t ReadNumber(int num_bytes, uint8_t* header) {
+  uint64_t number = 0;
+  for (int i = 0; i < num_bytes; i++) {
+    uint64_t c = header[i];
+    number += c << (i * 8);
+  }
+  return number;
+}
+
+bool ProcessMethodInfo(std::unique_ptr<File>& file, std::map<uint64_t, std::string>& name_map) {
+  // The first byte that specified the type of the packet is already read in
+  // ParseLongRunningMethodTrace.
+  uint8_t header[kAlwaysOnMethodInfoHeaderSize - 1];
+  if (!file->ReadFully(&header, sizeof(header))) {
+    printf("Couldn't read header\n");
+    return false;
+  }
+  uint64_t id = ReadNumber(8, header);
+  int length = ReadNumber(2, header + 8);
+
+  std::unique_ptr<char[]> name(new char[length]);
+  if (!file->ReadFully(name.get(), length)) {
+    return false;
+  }
+  std::string str(name.get(), length);
+  std::replace(str.begin(), str.end(), '\t', ' ');
+  if (str[str.length() - 1] == '\n') {
+    str.erase(str.length() - 1);
+  }
+  name_map.emplace(id, str);
+  return true;
+}
+
+void PrintTraceEntry(const std::string& method_name,
+                     int event_type,
+                     int* current_depth,
+                     size_t timestamp) {
+  std::string entry;
+  for (int i = 0; i < *current_depth; i++) {
+    entry.push_back('.');
+  }
+  if (event_type == kMethodEntry) {
+    *current_depth += 1;
+    entry.append(".>> ");
+  } else if (event_type == kMethodExit) {
+    *current_depth -= 1;
+    entry.append("<< ");
+  } else {
+    entry.append("?? ");
+  }
+  entry.append(" ");
+  entry.append(method_name);
+  entry.append(" ");
+  entry.append(std::to_string(timestamp));
+  entry.append("\n");
+  printf("%s", entry.c_str());
+}
+
+bool SkipTraceEntries(std::unique_ptr<File>& file) {
+  // The first byte that specified the type of the packet is already read in
+  // ParseLongRunningMethodTrace.
+  uint8_t header[kAlwaysOnTraceHeaderSize - 1];
+  if (!file->ReadFully(header, sizeof(header))) {
+    return false;
+  }
+
+  // Read thread id
+  ReadNumber(4, header);
+  int offset = 4;
+  // Read number of records
+  ReadNumber(3, header + offset);
+  offset += 3;
+  int total_size = ReadNumber(4, header + offset);
+  std::unique_ptr<uint8_t[]> buffer(new uint8_t[total_size]);
+  if (!file->ReadFully(buffer.get(), total_size)) {
+    return false;
+  }
+  return true;
+}
+
+bool ProcessLongRunningMethodTraceEntries(std::unique_ptr<File>& file,
+                         std::map<int64_t, int>& current_depth_map,
+                         std::map<uint64_t, std::string>& method_map) {
+  // The first byte that specified the type of the packet is already read in
+  // ParseLongRunningMethodTrace.
+  uint8_t header[kAlwaysOnTraceHeaderSize - 1];
+  if (!file->ReadFully(header, sizeof(header))) {
+    return false;
+  }
+
+  uint32_t thread_id = ReadNumber(4, header);
+  int offset = 4;
+  int num_records = ReadNumber(3, header + offset);
+  offset += 3;
+  int total_size = ReadNumber(4, header + offset);
+  if (total_size == 0) {
+    return true;
+  }
+  std::unique_ptr<uint8_t[]> buffer(new uint8_t[total_size]);
+  if (!file->ReadFully(buffer.get(), total_size)) {
+    return false;
+  }
+
+  printf("Thread: %d\n", thread_id);
+  int current_depth = 0;
+  if (current_depth_map.find(thread_id) != current_depth_map.end()) {
+    // Get the current call stack depth. If it is the first method we are seeing on this thread
+    // then this map wouldn't have an entry, and we start with the depth of 0.
+    current_depth = current_depth_map[thread_id];
+  }
+
+  const uint8_t* current_buffer_ptr = buffer.get();
+  const uint8_t* end_ptr = buffer.get() + total_size;
+  uint64_t prev_method_id = 0;
+  int64_t prev_timestamp_and_action = 0;
+  for (int i = 0; i < num_records; i++) {
+    // Read timestamp and action
+    int64_t ts_diff = 0;
+    if (!DecodeSignedLeb128Checked(&current_buffer_ptr, end_ptr, &ts_diff)) {
+      LOG(FATAL) << "Reading past the buffer when decoding timestamp";
+    }
+    int64_t timestamp_and_action = prev_timestamp_and_action + ts_diff;
+    prev_timestamp_and_action = timestamp_and_action;
+    bool is_method_exit = timestamp_and_action & 0x1;
+
+    uint64_t method_id;
+    std::string method_name;
+    if (!is_method_exit) {
+      int64_t method_diff = 0;
+      if (!DecodeSignedLeb128Checked(&current_buffer_ptr, end_ptr, &method_diff)) {
+        LOG(FATAL) << "Reading past the buffer when decoding method id";
+      }
+      method_id = prev_method_id + method_diff;
+      prev_method_id = method_id;
+      if (method_map.find(method_id) == method_map.end()) {
+        LOG(FATAL) << "No entry for method " << std::hex << method_id;
+      }
+      method_name = method_map[method_id];
+    }
+
+    PrintTraceEntry(method_name,
+                    is_method_exit? kMethodExit: kMethodEntry,
+                    &current_depth,
+                    timestamp_and_action & ~0x1);
+  }
+  current_depth_map[thread_id] = current_depth;
+  return true;
+}
+
+void ParseLongRunningMethodTrace(char* file_name) {
+  std::unique_ptr<File> file(OS::OpenFileForReading(file_name));
+  if (file == nullptr) {
+    printf("Couldn't open file\n");
+    return;
+  }
+
+  // Map to maintain information about threads and methods
+  std::map<uint64_t, std::string> method_map;
+
+  // Map to Maintain the current depth of the method in the call stack. Used to
+  // correctly indent when printing the trace events.
+  std::map<int64_t, int> current_depth_map;
+
+  // First parse metadata. To keep the implementation of dumping the data
+  // simple, we don't ensure that the information about methods is dumped before the
+  // methods. This is also good if the ANR report got truncated. We will then
+  // have information about how long the methods took and we can infer some of
+  // the method names from the stack trace.
+  while (true) {
+    uint8_t entry_header;
+    if (!file->ReadFully(&entry_header, sizeof(entry_header))) {
+      break;
+    }
+    if (entry_header == kEntryHeaderV2) {
+      if (!SkipTraceEntries(file)) {
+        break;
+      }
+    } else {
+      DCHECK_EQ(entry_header, kMethodInfoHeaderV2);
+      if (!ProcessMethodInfo(file, method_map)) {
+        break;
+      }
+    }
+  }
+
+  // Reset file
+  file->ResetOffset();
+
+  while (true) {
+    uint8_t entry_header;
+    if (!file->ReadFully(&entry_header, sizeof(entry_header))) {
+      break;
+    }
+    if (entry_header != kEntryHeaderV2) {
+      break;
+    }
+    if (!ProcessLongRunningMethodTraceEntries(file, current_depth_map, method_map)) {
+      break;
+    }
+  }
+}
+
+}  // namespace art
+
+int main(int argc, char **argv) {
+  if (argc < 1) {
+    printf("Usage trace <filename>");
+    return -1;
+  }
+
+  art::ParseLongRunningMethodTrace(argv[1]);
+  return 0;
+}
diff --git a/tools/user-data.img b/tools/user-data.img
index 76a614c52b..245bdbdc39 100644
Binary files a/tools/user-data.img and b/tools/user-data.img differ
diff --git a/tools/veridex/api_list_filter.h b/tools/veridex/api_list_filter.h
index 58065db2b2..c8e6aaa378 100644
--- a/tools/veridex/api_list_filter.h
+++ b/tools/veridex/api_list_filter.h
@@ -17,9 +17,10 @@
 #ifndef ART_TOOLS_VERIDEX_API_LIST_FILTER_H_
 #define ART_TOOLS_VERIDEX_API_LIST_FILTER_H_
 
-#include <algorithm>
 #include <android-base/strings.h>
 
+#include <set>
+
 #include "base/hiddenapi_flags.h"
 
 namespace art {
@@ -46,10 +47,10 @@ class ApiListFilter {
     }
 
     if (include_invalid_list) {
-      lists_.push_back(hiddenapi::ApiList());
+      lists_.push_back(hiddenapi::ApiList::Invalid());
     }
     for (size_t i = 0; i < hiddenapi::ApiList::kValueCount; ++i) {
-      hiddenapi::ApiList list = hiddenapi::ApiList(i);
+      hiddenapi::ApiList list = hiddenapi::ApiList::FromIntValue(i);
       if (exclude_set.find(list) == exclude_set.end()) {
           lists_.push_back(list);
       }
diff --git a/tools/veridex/hidden_api.cc b/tools/veridex/hidden_api.cc
index 4156aa954b..1e0e303c57 100644
--- a/tools/veridex/hidden_api.cc
+++ b/tools/veridex/hidden_api.cc
@@ -34,7 +34,7 @@ HiddenApi::HiddenApi(const char* filename, const ApiListFilter& api_list_filter)
     std::vector<std::string> values = android::base::Split(str, ",");
     const std::string& signature = values[0];
 
-    hiddenapi::ApiList membership;
+    hiddenapi::ApiList membership = hiddenapi::ApiList::Invalid();
     bool success = hiddenapi::ApiList::FromNames(values.begin() + 1, values.end(), &membership);
     if (!success) {
       LOG(ERROR) << "Unknown ApiList flag: " << str;
diff --git a/tools/veridex/hidden_api.h b/tools/veridex/hidden_api.h
index a8301743aa..12082c3c70 100644
--- a/tools/veridex/hidden_api.h
+++ b/tools/veridex/hidden_api.h
@@ -45,7 +45,7 @@ class HiddenApi {
 
   hiddenapi::ApiList GetApiList(const std::string& name) const {
     auto it = api_list_.find(name);
-    return (it == api_list_.end()) ? hiddenapi::ApiList() : it->second;
+    return (it == api_list_.end()) ? hiddenapi::ApiList::Invalid() : it->second;
   }
 
   bool ShouldReport(const std::string& signature) const {
diff --git a/tools/veridex/precise_hidden_api_finder.cc b/tools/veridex/precise_hidden_api_finder.cc
index d9eb271bc4..d02a0a2bd8 100644
--- a/tools/veridex/precise_hidden_api_finder.cc
+++ b/tools/veridex/precise_hidden_api_finder.cc
@@ -37,7 +37,7 @@ void PreciseHiddenApiFinder::RunInternal(
     const std::function<void(VeridexResolver*, const ClassAccessor::Method&)>& action) {
   for (const std::unique_ptr<VeridexResolver>& resolver : resolvers) {
     for (ClassAccessor accessor : resolver->GetDexFile().GetClasses()) {
-      if (class_filter.Matches(accessor.GetDescriptor())) {
+      if (class_filter.Matches(accessor.GetDescriptorView())) {
         for (const ClassAccessor::Method& method : accessor.GetMethods()) {
           if (method.GetCodeItem() != nullptr) {
             action(resolver.get(), method);
diff --git a/tools/veridex/resolver.cc b/tools/veridex/resolver.cc
index a56ea6e5ae..211168394e 100644
--- a/tools/veridex/resolver.cc
+++ b/tools/veridex/resolver.cc
@@ -27,7 +27,7 @@ namespace art {
 
 void VeridexResolver::Run() {
   for (ClassAccessor accessor : dex_file_.GetClasses()) {
-    std::string name(accessor.GetDescriptor());
+    std::string name(accessor.GetDescriptorView());
     auto existing = type_map_.find(name);
     const uint32_t type_idx = accessor.GetClassIdx().index_;
     if (existing != type_map_.end()) {
diff --git a/tools/veridex/veridex.cc b/tools/veridex/veridex.cc
index 9c7bd78cff..ec2e96168d 100644
--- a/tools/veridex/veridex.cc
+++ b/tools/veridex/veridex.cc
@@ -270,9 +270,9 @@ class Veridex {
     os << stats.count << " hidden API(s) used: "
        << stats.linking_count << " linked against, "
        << stats.reflection_count << " through reflection" << std::endl;
-    DumpApiListStats(os, stats, hiddenapi::ApiList(), api_list_filter);
+    DumpApiListStats(os, stats, hiddenapi::ApiList::Invalid(), api_list_filter);
     for (size_t i = 0; i < hiddenapi::ApiList::kValueCount; ++i) {
-      DumpApiListStats(os, stats, hiddenapi::ApiList(i), api_list_filter);
+      DumpApiListStats(os, stats, hiddenapi::ApiList::FromIntValue(i), api_list_filter);
     }
   }
 
```

