```diff
diff --git a/icing/document-builder.h b/icing/document-builder.h
index 5d6f14d..5db589f 100644
--- a/icing/document-builder.h
+++ b/icing/document-builder.h
@@ -20,6 +20,7 @@
 #include <string>
 #include <string_view>
 #include <utility>
+#include <vector>
 
 #include "icing/proto/document.pb.h"
 
@@ -113,6 +114,14 @@ class DocumentBuilder {
     return AddBooleanProperty(std::move(property_name), {boolean_values...});
   }
 
+  // Takes a property name and any number of blob handle values.
+  template <typename... V>
+  DocumentBuilder& AddBlobHandleProperty(std::string property_name,
+                                         V... blob_handle_values) {
+    return AddBlobHandleProperty(std::move(property_name),
+                                 {blob_handle_values...});
+  }
+
   // Takes a property name and any number of bytes values.
   template <typename... V>
   DocumentBuilder& AddBytesProperty(std::string property_name,
@@ -133,6 +142,18 @@ class DocumentBuilder {
     return AddVectorProperty(std::move(property_name), {vector_values...});
   }
 
+  // Takes a property name and a list of vector values.
+  DocumentBuilder& AddVectorProperty(
+      std::string property_name,
+      std::vector<PropertyProto::VectorProto> vector_values) {
+    auto property = document_.add_properties();
+    property->set_name(std::move(property_name));
+    for (PropertyProto::VectorProto vector_value : vector_values) {
+      property->mutable_vector_values()->Add(std::move(vector_value));
+    }
+    return *this;
+  }
+
   DocumentProto Build() const { return document_; }
 
  private:
@@ -169,6 +190,19 @@ class DocumentBuilder {
     return *this;
   }
 
+  DocumentBuilder& AddBlobHandleProperty(
+      std::string property_name,
+      std::initializer_list<PropertyProto::BlobHandleProto>
+          blob_handle_values) {
+    auto property = document_.add_properties();
+    property->set_name(std::move(property_name));
+    for (PropertyProto::BlobHandleProto blob_handle_value :
+         blob_handle_values) {
+      property->mutable_blob_handle_values()->Add(std::move(blob_handle_value));
+    }
+    return *this;
+  }
+
   DocumentBuilder& AddBytesProperty(
       std::string property_name,
       std::initializer_list<std::string> bytes_values) {
diff --git a/icing/expand/expander-manager.cc b/icing/expand/expander-manager.cc
new file mode 100644
index 0000000..0bc5c56
--- /dev/null
+++ b/icing/expand/expander-manager.cc
@@ -0,0 +1,135 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include "icing/expand/expander-manager.h"
+
+#include <memory>
+#include <string>
+#include <string_view>
+#include <utility>
+#include <vector>
+
+#include "icing/text_classifier/lib3/utils/base/status.h"
+#include "icing/text_classifier/lib3/utils/base/statusor.h"
+#include "icing/absl_ports/mutex.h"
+#include "icing/expand/expander.h"
+#include "icing/expand/stemming/stemming-expander.h"
+#include "icing/util/logging.h"
+#include "icing/util/status-macros.h"
+#include "unicode/uloc.h"
+
+namespace icing {
+namespace lib {
+
+const Expander& ExpanderManager::GetOrCreateStemmingExpander(
+    const std::string& locale) {
+  {
+    // Check if the expander already exists. This only requires a read lock.
+    absl_ports::shared_lock l(&mutex_);
+    auto itr = stemming_expanders_.find(locale);
+    if (itr != stemming_expanders_.end()) {
+      return *(itr->second);
+    }
+  }
+
+  const char* stemmer_language_code = uloc_getISO3Language(locale.c_str());
+  libtextclassifier3::StatusOr<std::unique_ptr<StemmingExpander>> expander_or =
+      StemmingExpander::Create(stemmer_language_code);
+
+  if (!expander_or.status().ok()) {
+    ICING_VLOG(1) << "Failed to create stemming expander for locale: " << locale
+                  << ". Using default locale: " << default_locale_;
+    {
+      absl_ports::shared_lock l(&mutex_);
+      // stemming_expanders_[default_locale_] is guaranteed to exist as this is
+      // created during initialization.
+      return *stemming_expanders_[default_locale_];
+    }
+  }
+
+  std::unique_ptr<Expander> stemming_expander =
+      std::move(expander_or).ValueOrDie();
+  {
+    absl_ports::unique_lock l(&mutex_);
+    // Check again before emplacing into the map in case the expander was
+    // created by another thread.
+    auto itr = stemming_expanders_.find(locale);
+    if (itr == stemming_expanders_.end()) {
+      itr = stemming_expanders_.emplace(locale, std::move(stemming_expander))
+                .first;
+    }
+    return *(itr->second);
+  }
+}
+
+/* static */ libtextclassifier3::StatusOr<std::unique_ptr<ExpanderManager>>
+ExpanderManager::Create(std::string default_locale,
+                        int max_terms_per_expander) {
+  if (max_terms_per_expander <= 1) {
+    return libtextclassifier3::Status(
+        libtextclassifier3::StatusCode::INVALID_ARGUMENT,
+        "max_num_expanded_terms must be greater than 1.");
+  }
+
+  // Create a default stemming expander using defalt_locale. This is added into
+  // the stemming_expanders_ map during initialization.
+  const char* stemmer_language_code =
+      uloc_getISO3Language(default_locale.c_str());
+  libtextclassifier3::StatusOr<std::unique_ptr<StemmingExpander>> expander_or =
+      StemmingExpander::Create(stemmer_language_code);
+
+  std::unique_ptr<StemmingExpander> expander;
+  if (!expander_or.status().ok()) {
+    ICING_VLOG(1) << "Failed to create expander manager with locale: "
+                  << default_locale
+                  << ". Using default English locale instead.";
+    default_locale = kDefaultEnglishLocale;
+    stemmer_language_code = uloc_getISO3Language(default_locale.c_str());
+    ICING_ASSIGN_OR_RETURN(expander,
+                           StemmingExpander::Create(stemmer_language_code));
+  } else {
+    expander = std::move(expander_or).ValueOrDie();
+  }
+
+  ExpandersMap stemming_expanders;
+  stemming_expanders.emplace(default_locale, std::move(expander));
+  return std::unique_ptr<ExpanderManager>(
+      new ExpanderManager(std::move(stemming_expanders),
+                          std::move(default_locale), max_terms_per_expander));
+}
+
+std::vector<ExpandedTerm> ExpanderManager::ProcessTerm(
+    std::string_view term, TermMatchType::Code term_match_type,
+    const std::string& locale) {
+  switch (term_match_type) {
+    case TermMatchType_Code_UNKNOWN:
+    case TermMatchType::EXACT_ONLY:
+    case TermMatchType::PREFIX: {
+      // Return the original term.
+      std::vector<ExpandedTerm> expanded_terms;
+      expanded_terms.push_back(
+          ExpandedTerm(std::string(term), /*is_stemmed_term=*/false));
+      return expanded_terms;
+    }
+    case TermMatchType_Code_STEMMING: {
+      // The stemming expander returns at most 2 terms, and we don't allow
+      // having max_terms_per_expander < 2, so we don't need to check the size
+      // of the returned vector here.
+      return GetOrCreateStemmingExpander(locale).Expand(term);
+    }
+  }
+}
+
+}  // namespace lib
+}  // namespace icing
diff --git a/icing/expand/expander-manager.h b/icing/expand/expander-manager.h
new file mode 100644
index 0000000..8386c6a
--- /dev/null
+++ b/icing/expand/expander-manager.h
@@ -0,0 +1,112 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef ICING_EXPAND_EXPANDER_MANAGER_H_
+#define ICING_EXPAND_EXPANDER_MANAGER_H_
+
+#include <memory>
+#include <string>
+#include <string_view>
+#include <unordered_map>
+#include <utility>
+#include <vector>
+
+#include "icing/text_classifier/lib3/utils/base/statusor.h"
+#include "icing/absl_ports/mutex.h"
+#include "icing/absl_ports/thread_annotations.h"
+#include "icing/expand/expander.h"
+#include "icing/proto/term.pb.h"
+#include "unicode/uloc.h"
+
+namespace icing {
+namespace lib {
+
+// This class is a wrapper around the various expanders. It is responsible for
+// calling the appropriate expander based on a term's match type and locale.
+//
+// This class is thread-safe.
+class ExpanderManager {
+ public:
+  // This is used as the default locale if the provided locale is invalid.
+  static constexpr std::string_view kDefaultEnglishLocale = ULOC_US;
+
+  // Map of a locale to an expander.
+  //
+  // The Expander instances are managed exclusively by this class, and are never
+  // deleted from the map once created. Therefore we don't need std::shared_ptr
+  // here even though multiple threads may be accessing the same expander
+  // instance at the same time.
+  using ExpandersMap =
+      std::unordered_map<std::string, std::unique_ptr<Expander>>;
+
+  // Factory method to create an ExpanderManager. The expanders will be
+  // initialized in the default locale.
+  //
+  // Returns:
+  //  - An ExpanderManager on success.
+  //  - INVALID_ARGUMENT_ERROR if max_terms_per_expander <= 1.
+  //  - INTERNAL_ERROR ion errors.
+  static libtextclassifier3::StatusOr<std::unique_ptr<ExpanderManager>> Create(
+      std::string default_locale, int max_terms_per_expander);
+
+  // Processes a term according to the term's match type and locale. The
+  // first ExpandedTerm in the returned list will always be the original input
+  // term.
+  //
+  // A new expander will be created when possible if the expander corresponding
+  // to the given term match type and locale does not already exist.
+  // If the locale is not supported, the term will be expanded using the default
+  // locale.
+  //
+  // Returns: a list of expanded terms.
+  std::vector<ExpandedTerm> ProcessTerm(std::string_view term,
+                                        TermMatchType::Code term_match_type,
+                                        const std::string& locale);
+
+  const std::string& default_locale() const { return default_locale_; }
+
+ private:
+  explicit ExpanderManager(ExpandersMap stemming_expanders,
+                           std::string default_locale,
+                           int max_terms_per_expander)
+      : stemming_expanders_(std::move(stemming_expanders)),
+        default_locale_(std::move(default_locale)),
+        max_terms_per_expander_(max_terms_per_expander) {}
+
+  // Returns a stemming expander for the given locale.
+  // - Returns the expander retrieved from the stemming_expanders_ map if an
+  //   instance already exists for the locale.
+  // - Otherwise, creates a new expander instance and adds it to the
+  //   stemming_expanders_ map before returning it.
+  const Expander& GetOrCreateStemmingExpander(const std::string& locale)
+      ICING_LOCKS_EXCLUDED(mutex_);
+
+  // Map of locale to stemming expanders.
+  ExpandersMap stemming_expanders_ ICING_GUARDED_BY(mutex_);
+
+  // Default locale to use for expanders.
+  const std::string default_locale_;
+
+  // Maximum number of terms to expand to for an input term per expander. This
+  // number includes the input term.
+  const int max_terms_per_expander_;
+
+  // Used to provide reader and writer locks
+  mutable absl_ports::shared_mutex mutex_;
+};
+
+}  // namespace lib
+}  // namespace icing
+
+#endif  // ICING_EXPAND_EXPANDER_MANAGER_H_
diff --git a/icing/expand/expander-manager_test.cc b/icing/expand/expander-manager_test.cc
new file mode 100644
index 0000000..4dd49ab
--- /dev/null
+++ b/icing/expand/expander-manager_test.cc
@@ -0,0 +1,265 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and∂∂
+// limitations under the License.
+
+#include "icing/expand/expander-manager.h"
+
+#include <array>
+#include <memory>
+#include <string>
+#include <string_view>
+#include <thread>  // NOLINT
+#include <vector>
+
+#include "icing/text_classifier/lib3/utils/base/status.h"
+#include "gmock/gmock.h"
+#include "gtest/gtest.h"
+#include "icing/expand/expander.h"
+#include "icing/portable/platform.h"
+#include "icing/testing/common-matchers.h"
+#include "unicode/uloc.h"
+
+namespace icing {
+namespace lib {
+
+namespace {
+
+using ::testing::ElementsAre;
+
+constexpr std::string_view kRussianLocale = "ru-RU";
+constexpr std::string_view kTamilLocale = "ta-IN";
+constexpr std::string_view kUnsupportedLocale = "unsupported_locale";
+
+TEST(ExpanderManagerTest, CreateWithInvalidMaxTermsShouldFail) {
+  EXPECT_THAT(ExpanderManager::Create(ULOC_US, /*max_terms_per_expander=*/-1),
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
+  EXPECT_THAT(ExpanderManager::Create(ULOC_US, /*max_terms_per_expander=*/1),
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
+}
+
+TEST(ExpanderManagerTest, CreateWithAnyLocaleShouldSucceed) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<ExpanderManager> expander_manager,
+      ExpanderManager::Create(ULOC_US, /*max_terms_per_expander=*/2));
+  EXPECT_THAT(expander_manager->default_locale(), ULOC_US);
+
+  ICING_ASSERT_OK_AND_ASSIGN(
+      expander_manager, ExpanderManager::Create(ULOC_FRENCH,
+                                                /*max_terms_per_expander=*/2));
+  EXPECT_THAT(expander_manager->default_locale(), ULOC_FRENCH);
+
+  ICING_ASSERT_OK_AND_ASSIGN(
+      expander_manager, ExpanderManager::Create(std::string(kUnsupportedLocale),
+                                                /*max_terms_per_expander=*/2));
+  if (IsStemmingEnabled()) {
+    EXPECT_THAT(expander_manager->default_locale(),
+                ExpanderManager::kDefaultEnglishLocale);
+  } else {
+    EXPECT_THAT(expander_manager->default_locale(),
+                std::string(kUnsupportedLocale));
+  }
+}
+
+TEST(ExpanderManagerTest, ProcessTerm_exactMatchReturnsOriginalTerm) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<ExpanderManager> expander_manager,
+      ExpanderManager::Create(ULOC_US, /*max_terms_per_expander=*/3));
+
+  std::vector<ExpandedTerm> expanded_terms = expander_manager->ProcessTerm(
+      "running", TermMatchType::EXACT_ONLY, ULOC_US);
+  EXPECT_THAT(expander_manager->default_locale(), ULOC_US);
+  EXPECT_THAT(expanded_terms, ElementsAre(ExpandedTerm(
+                                  "running", /*is_stemmed_term_in=*/false)));
+}
+
+TEST(ExpanderManagerTest, ProcessTerm_prefixMatchReturnsOriginalTerm) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<ExpanderManager> expander_manager,
+      ExpanderManager::Create(ULOC_US, /*max_terms_per_expander=*/3));
+
+  std::vector<ExpandedTerm> expanded_terms =
+      expander_manager->ProcessTerm("running", TermMatchType::PREFIX, ULOC_US);
+  EXPECT_THAT(expander_manager->default_locale(), ULOC_US);
+  EXPECT_THAT(expanded_terms, ElementsAre(ExpandedTerm(
+                                  "running", /*is_stemmed_term_in=*/false)));
+}
+
+TEST(ExpanderManagerTest, ProcessTerm_stemmingMatchWithDefaultLocale) {
+  if (!IsStemmingEnabled()) {
+    GTEST_SKIP() << "Skipping test because stemming is not enabled.";
+  }
+
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<ExpanderManager> expander_manager,
+      ExpanderManager::Create(ULOC_US, /*max_terms_per_expander=*/3));
+
+  std::vector<ExpandedTerm> expanded_terms = expander_manager->ProcessTerm(
+      "running", TermMatchType::STEMMING, ULOC_US);
+  EXPECT_THAT(expander_manager->default_locale(), ULOC_US);
+  EXPECT_THAT(expanded_terms,
+              ElementsAre(ExpandedTerm("running", /*is_stemmed_term_in=*/false),
+                          ExpandedTerm("run", /*is_stemmed_term_in=*/true)));
+
+  expanded_terms =
+      expander_manager->ProcessTerm("tests", TermMatchType::STEMMING, ULOC_US);
+  EXPECT_THAT(expander_manager->default_locale(), ULOC_US);
+  EXPECT_THAT(expanded_terms,
+              ElementsAre(ExpandedTerm("tests", /*is_stemmed_term_in=*/false),
+                          ExpandedTerm("test", /*is_stemmed_term_in=*/true)));
+}
+
+TEST(ExpanderManagerTest, ProcessTerm_stemmingMatchWithNonDefaultLocale) {
+  if (!IsStemmingEnabled()) {
+    GTEST_SKIP() << "Skipping test because stemming is not enabled.";
+  }
+
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<ExpanderManager> expander_manager,
+      ExpanderManager::Create(ULOC_FRENCH, /*max_terms_per_expander=*/3));
+
+  std::vector<ExpandedTerm> expanded_terms = expander_manager->ProcessTerm(
+      "running", TermMatchType::STEMMING, ULOC_US);
+  EXPECT_THAT(expander_manager->default_locale(), ULOC_FRENCH);
+  EXPECT_THAT(expanded_terms,
+              ElementsAre(ExpandedTerm("running", /*is_stemmed_term_in=*/false),
+                          ExpandedTerm("run", /*is_stemmed_term_in=*/true)));
+
+  expanded_terms = expander_manager->ProcessTerm(
+      "torpedearon", TermMatchType::STEMMING, "es_ES");
+  EXPECT_THAT(expander_manager->default_locale(), ULOC_FRENCH);
+  EXPECT_THAT(
+      expanded_terms,
+      ElementsAre(ExpandedTerm("torpedearon", /*is_stemmed_term_in=*/false),
+                  ExpandedTerm("torped", /*is_stemmed_term_in=*/true)));
+}
+
+TEST(ExpanderManagerTest,
+     ProcessTerm_stemmingMatchWithUnsupportedLocaleUsesDefault) {
+  if (!IsStemmingEnabled()) {
+    GTEST_SKIP() << "Skipping test because stemming is not enabled.";
+  }
+
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<ExpanderManager> expander_manager,
+      ExpanderManager::Create(ULOC_FRENCH, /*max_terms_per_expander=*/3));
+
+  std::string unsupported_locale_str = std::string(kUnsupportedLocale);
+  std::vector<ExpandedTerm> expanded_terms = expander_manager->ProcessTerm(
+      "running", TermMatchType::STEMMING, unsupported_locale_str);
+  EXPECT_THAT(expander_manager->default_locale(), ULOC_FRENCH);
+  EXPECT_THAT(expanded_terms, ElementsAre(ExpandedTerm(
+                                  "running", /*is_stemmed_term_in=*/false)));
+
+  expanded_terms = expander_manager->ProcessTerm(
+      "majestueuse", TermMatchType::STEMMING, unsupported_locale_str);
+  EXPECT_THAT(expander_manager->default_locale(), ULOC_FRENCH);
+  EXPECT_THAT(
+      expanded_terms,
+      ElementsAre(ExpandedTerm("majestueuse", /*is_stemmed_term_in=*/false),
+                  ExpandedTerm("majestu", /*is_stemmed_term_in=*/true)));
+}
+
+TEST(ExpanderManagerTest,
+     ProcessTerm_stemmingMatchWithUnsupportedDefaultLocaleUsesEnglish) {
+  if (!IsStemmingEnabled()) {
+    GTEST_SKIP() << "Skipping test because stemming is not enabled.";
+  }
+
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<ExpanderManager> expander_manager,
+      ExpanderManager::Create(std::string(kUnsupportedLocale),
+                              /*max_terms_per_expander=*/3));
+
+  std::string unsupported_locale_str = std::string(kUnsupportedLocale);
+  std::vector<ExpandedTerm> expanded_terms = expander_manager->ProcessTerm(
+      "running", TermMatchType::STEMMING, unsupported_locale_str);
+  EXPECT_THAT(expander_manager->default_locale(),
+              ExpanderManager::kDefaultEnglishLocale);
+  EXPECT_THAT(expanded_terms,
+              ElementsAre(ExpandedTerm("running", /*is_stemmed_term_in=*/false),
+                          ExpandedTerm("run", /*is_stemmed_term_in=*/true)));
+
+  expanded_terms = expander_manager->ProcessTerm(
+      "majestueuse", TermMatchType::STEMMING, unsupported_locale_str);
+  EXPECT_THAT(expander_manager->default_locale(),
+              ExpanderManager::kDefaultEnglishLocale);
+  EXPECT_THAT(
+      expanded_terms,
+      ElementsAre(ExpandedTerm("majestueuse", /*is_stemmed_term_in=*/false),
+                  ExpandedTerm("majestueus", /*is_stemmed_term_in=*/true)));
+}
+
+TEST(ExpanderManagerTest, ThreadSafety) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<ExpanderManager> expander_manager,
+      ExpanderManager::Create(ULOC_US,
+                              /*max_terms_per_expander=*/1000));
+
+  constexpr int kNumTerms = 10;
+  constexpr int kNumLocales = 5;
+  constexpr std::array<std::string_view, kNumLocales> kLocales = {
+      ULOC_US, ULOC_FRENCH, kRussianLocale, kTamilLocale, kUnsupportedLocale};
+  constexpr std::array<std::string_view, kNumTerms> kTerms = {
+      "running", "majestueuse", "валяется", "இக்கதையின்", "testing",
+      "test",    "running",     "говорить", "அக்கரையில்", "manager"};
+
+  std::array<std::string_view, kNumTerms> kStems;
+  if (IsStemmingEnabled()) {
+    kStems = {"run",  "majestu", "валя",  "கதை", "test",
+              "test", "running", "говор", "கரை", "manag"};
+  } else {
+    // Stemming is not enabled, so the stemmed terms are the same as the
+    // original terms.
+    kStems = kTerms;
+  }
+
+  // Create kNumThreads threads. Call ProcessTerm() from each thread in
+  // parallel using different locales. There should be no crashes.
+  constexpr int kNumThreads = 50;
+  std::vector<std::vector<ExpandedTerm>> expanded_terms(kNumThreads);
+  auto callable = [&](int thread_id) {
+    std::string locale = std::string(kLocales[thread_id % kNumLocales]);
+    expanded_terms[thread_id] = expander_manager->ProcessTerm(
+        kTerms[thread_id % kNumTerms], TermMatchType::STEMMING, locale);
+  };
+
+  // Spawn threads to call ProcessTerm() in parallel.
+  std::vector<std::thread> thread_objs;
+  for (int i = 0; i < kNumThreads; ++i) {
+    thread_objs.emplace_back(callable, i);
+  }
+
+  // Join threads and verify results
+  for (int i = 0; i < kNumThreads; ++i) {
+    thread_objs[i].join();
+
+    int term_number = i % kNumTerms;
+    if (kTerms[term_number] == kStems[term_number]) {
+      // No stemmed term generated after expansion.
+      EXPECT_THAT(expanded_terms[i],
+                  ElementsAre(ExpandedTerm(std::string(kTerms[term_number]),
+                                           /*is_stemmed_term_in=*/false)));
+    } else {
+      EXPECT_THAT(expanded_terms[i],
+                  ElementsAre(ExpandedTerm(std::string(kTerms[term_number]),
+                                           /*is_stemmed_term_in=*/false),
+                              ExpandedTerm(std::string(kStems[term_number]),
+                                           /*is_stemmed_term_in=*/true)));
+    }
+  }
+}
+
+}  // namespace
+
+}  // namespace lib
+}  // namespace icing
diff --git a/icing/expand/expander.h b/icing/expand/expander.h
new file mode 100644
index 0000000..a3ef464
--- /dev/null
+++ b/icing/expand/expander.h
@@ -0,0 +1,54 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef ICING_EXPAND_EXPANDER_H_
+#define ICING_EXPAND_EXPANDER_H_
+
+#include <string>
+#include <string_view>
+#include <utility>
+#include <vector>
+
+namespace icing {
+namespace lib {
+
+// Struct to hold the text of a term, and whether it has undergone certain
+// expansion operations.
+struct ExpandedTerm {
+  std::string text;
+  bool is_stemmed_term;
+
+  explicit ExpandedTerm(std::string text_in, bool is_stemmed_term_in)
+      : text(std::move(text_in)), is_stemmed_term(is_stemmed_term_in) {}
+
+  bool operator==(const ExpandedTerm& other) const {
+    return text == other.text && is_stemmed_term == other.is_stemmed_term;
+  }
+};
+
+class Expander {
+ public:
+  virtual ~Expander() = default;
+
+  // Expands a given term into a vector of expanded terms. The first term in the
+  // output vector is always the original term.
+  //
+  // See implementation classes for specific expansion behaviors.
+  virtual std::vector<ExpandedTerm> Expand(std::string_view term) const = 0;
+};
+
+}  // namespace lib
+}  // namespace icing
+
+#endif  // ICING_EXPAND_EXPANDER_H_
diff --git a/icing/expand/stemming/simple/none-stemmer-factory.cc b/icing/expand/stemming/simple/none-stemmer-factory.cc
new file mode 100644
index 0000000..41d68cf
--- /dev/null
+++ b/icing/expand/stemming/simple/none-stemmer-factory.cc
@@ -0,0 +1,39 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include <memory>
+#include <string>
+
+#include "icing/text_classifier/lib3/utils/base/statusor.h"
+#include "icing/expand/stemming/simple/none-stemmer.h"
+#include "icing/expand/stemming/stemmer.h"
+
+namespace icing {
+namespace lib {
+
+namespace stemmer_factory {
+
+// Creates a dummy stemmer that returns the input word as the stem.
+// The language_code is ignored in this implementation.
+libtextclassifier3::StatusOr<std::unique_ptr<Stemmer>> Create(
+    std::string language_code) {
+  return std::make_unique<NoneStemmer>();
+}
+
+bool IsStemmingEnabled() { return false; }
+
+}  // namespace stemmer_factory
+
+}  // namespace lib
+}  // namespace icing
diff --git a/icing/expand/stemming/simple/none-stemmer.h b/icing/expand/stemming/simple/none-stemmer.h
new file mode 100644
index 0000000..40e7c7e
--- /dev/null
+++ b/icing/expand/stemming/simple/none-stemmer.h
@@ -0,0 +1,53 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef ICING_EXPAND_STEMMING_SIMPLE_NONE_STEMMER_H_
+#define ICING_EXPAND_STEMMING_SIMPLE_NONE_STEMMER_H_
+
+#include <string>
+#include <string_view>
+
+#include "icing/expand/stemming/stemmer.h"
+
+namespace icing {
+namespace lib {
+
+// A dummy stemmer that just returns the input term.
+//
+// This is useful for importing and building in environments where we cannot
+// have external dependencies for stemming (e.g. Jetpack).
+//
+// Example usage:
+//   auto stemmer = stemmer_factory::Create("en");
+//   stemmer->Stem("running");    // Returns "running"
+class NoneStemmer : public Stemmer {
+ public:
+  static constexpr std::string_view kNoneStemmerLanguageCode = "none";
+
+  explicit NoneStemmer() : language_code_(kNoneStemmerLanguageCode) {};
+
+  std::string Stem(std::string_view term) const override {
+    return std::string(term);
+  }
+
+  const std::string& language_code() const override { return language_code_; }
+
+ private:
+  const std::string language_code_;
+};
+
+}  // namespace lib
+}  // namespace icing
+
+#endif  // ICING_EXPAND_STEMMING_SIMPLE_NONE_STEMMER_H_
diff --git a/icing/expand/stemming/simple/none-stemmer_test.cc b/icing/expand/stemming/simple/none-stemmer_test.cc
new file mode 100644
index 0000000..a1583cf
--- /dev/null
+++ b/icing/expand/stemming/simple/none-stemmer_test.cc
@@ -0,0 +1,93 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and∂∂
+// limitations under the License.
+
+#include <memory>
+#include <string>
+#include <string_view>
+
+#include "gmock/gmock.h"
+#include "gtest/gtest.h"
+#include "icing/expand/stemming/stemmer-factory.h"
+#include "icing/expand/stemming/stemmer.h"
+#include "icing/testing/common-matchers.h"
+
+namespace icing {
+namespace lib {
+
+namespace {
+
+using ::testing::Eq;
+
+constexpr std::string_view kEnglishLanguageCode = "en";
+constexpr std::string_view kFrenchLanguageCode = "fr";
+
+constexpr std::string_view kUnsupportedLanguageCode = "unsupported";
+
+TEST(NoneStemmerTest, Creation) {
+  EXPECT_THAT(stemmer_factory::Create(std::string(kEnglishLanguageCode)),
+              IsOk());
+  EXPECT_THAT(stemmer_factory::Create(std::string(kFrenchLanguageCode)),
+              IsOk());
+  EXPECT_THAT(stemmer_factory::Create(std::string(kUnsupportedLanguageCode)),
+              IsOk());
+}
+
+TEST(NoneStemmerTest, NoStemmingDone) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<Stemmer> english_stemmer,
+      stemmer_factory::Create(
+          /*language_code=*/std::string(kEnglishLanguageCode)));
+  EXPECT_THAT(english_stemmer->Stem(""), Eq(""));
+  EXPECT_THAT(english_stemmer->Stem("  "), Eq("  "));
+  EXPECT_THAT(english_stemmer->Stem("....."), Eq("....."));
+  EXPECT_THAT(english_stemmer->Stem("test"), Eq("test"));
+  EXPECT_THAT(english_stemmer->Stem("tests"), Eq("tests"));
+  EXPECT_THAT(english_stemmer->Stem("testing"), Eq("testing"));
+  EXPECT_THAT(english_stemmer->Stem("majestueuse"), Eq("majestueuse"));
+  EXPECT_THAT(english_stemmer->Stem("你好"), Eq("你好"));
+  EXPECT_THAT(english_stemmer->Stem("валя"), Eq("валя"));
+
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<Stemmer> french_stemmer,
+      stemmer_factory::Create(
+          /*language_code=*/std::string(kFrenchLanguageCode)));
+  EXPECT_THAT(french_stemmer->Stem(""), Eq(""));
+  EXPECT_THAT(french_stemmer->Stem("  "), Eq("  "));
+  EXPECT_THAT(french_stemmer->Stem("....."), Eq("....."));
+  EXPECT_THAT(french_stemmer->Stem("test"), Eq("test"));
+  EXPECT_THAT(french_stemmer->Stem("tests"), Eq("tests"));
+  EXPECT_THAT(french_stemmer->Stem("testing"), Eq("testing"));
+  EXPECT_THAT(french_stemmer->Stem("majestueuse"), Eq("majestueuse"));
+  EXPECT_THAT(french_stemmer->Stem("你好"), Eq("你好"));
+  EXPECT_THAT(french_stemmer->Stem("валя"), Eq("валя"));
+
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<Stemmer> unsupported_stemmer,
+      stemmer_factory::Create(
+          /*language_code=*/std::string(kUnsupportedLanguageCode)));
+  EXPECT_THAT(unsupported_stemmer->Stem(""), Eq(""));
+  EXPECT_THAT(unsupported_stemmer->Stem("  "), Eq("  "));
+  EXPECT_THAT(unsupported_stemmer->Stem("....."), Eq("....."));
+  EXPECT_THAT(unsupported_stemmer->Stem("test"), Eq("test"));
+  EXPECT_THAT(unsupported_stemmer->Stem("tests"), Eq("tests"));
+  EXPECT_THAT(unsupported_stemmer->Stem("testing"), Eq("testing"));
+  EXPECT_THAT(unsupported_stemmer->Stem("majestueuse"), Eq("majestueuse"));
+  EXPECT_THAT(unsupported_stemmer->Stem("你好"), Eq("你好"));
+  EXPECT_THAT(unsupported_stemmer->Stem("валя"), Eq("валя"));
+}
+
+}  // namespace
+
+}  // namespace lib
+}  // namespace icing
diff --git a/icing/expand/stemming/stemmer-factory.h b/icing/expand/stemming/stemmer-factory.h
new file mode 100644
index 0000000..2b33cc7
--- /dev/null
+++ b/icing/expand/stemming/stemmer-factory.h
@@ -0,0 +1,57 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef ICING_EXPAND_STEMMING_STEMMER_FACTORY_H_
+#define ICING_EXPAND_STEMMING_STEMMER_FACTORY_H_
+
+#include <memory>
+#include <string>
+
+#include "icing/text_classifier/lib3/utils/base/statusor.h"
+#include "icing/expand/stemming/stemmer.h"
+
+namespace icing {
+namespace lib {
+
+namespace stemmer_factory {
+
+// Creates a stemmer for the given language code.
+//
+// The language code should be a IETF BCP 47 language tag.
+//   - E.g. "en" for English, "fr" for French, etc.
+//   - See https://en.wikipedia.org/wiki/IETF_language_tag for more
+//     information.
+//
+// This is the header file for the factory function. Implementations are in the
+// .cc files, and we select which stemmer and .cc file to build with in each
+// build rule.
+//
+// Returns:
+//  - A stemmer on success
+//  - INVALID_ARGUMENT_ERROR if the language code is invalid or not supported.
+libtextclassifier3::StatusOr<std::unique_ptr<Stemmer>> Create(
+    std::string language_code);
+
+// Whether stemming is enabled.
+//
+// This is false for the none-stemmer implementation and true for the
+// snowball-stemmer implementation.
+bool IsStemmingEnabled();
+
+}  // namespace stemmer_factory
+
+}  // namespace lib
+}  // namespace icing
+
+#endif  // ICING_EXPAND_STEMMING_STEMMER_FACTORY_H_
diff --git a/icing/expand/stemming/stemmer.h b/icing/expand/stemming/stemmer.h
new file mode 100644
index 0000000..d66e0d6
--- /dev/null
+++ b/icing/expand/stemming/stemmer.h
@@ -0,0 +1,44 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef ICING_EXPAND_STEMMING_STEMMER_H_
+#define ICING_EXPAND_STEMMING_STEMMER_H_
+
+#include <string>
+#include <string_view>
+
+namespace icing {
+namespace lib {
+
+// Stems a given term to its root form.
+//
+// Example usage:
+//   std::unique_ptr<Stemmer> stemmer = stemmer_factory::Create("en");
+//   std::string stem = stemmer->Stem("running");
+class Stemmer {
+ public:
+  virtual ~Stemmer() = default;
+
+  // Returns the stem of a given term.
+  // This returns the same value if the term is already in its stem form.
+  virtual std::string Stem(std::string_view term) const = 0;
+
+  // Returns the language code of the stemmer.
+  virtual const std::string& language_code() const = 0;
+};
+
+}  // namespace lib
+}  // namespace icing
+
+#endif  // ICING_EXPAND_STEMMING_STEMMER_H_
diff --git a/icing/expand/stemming/stemming-expander.cc b/icing/expand/stemming/stemming-expander.cc
new file mode 100644
index 0000000..e601e9e
--- /dev/null
+++ b/icing/expand/stemming/stemming-expander.cc
@@ -0,0 +1,88 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include "icing/expand/stemming/stemming-expander.h"
+
+#include <memory>
+#include <string>
+#include <string_view>
+#include <utility>
+#include <vector>
+
+#include "icing/text_classifier/lib3/utils/base/statusor.h"
+#include "icing/absl_ports/mutex.h"
+#include "icing/expand/expander.h"
+#include "icing/expand/stemming/stemmer-factory.h"
+#include "icing/expand/stemming/stemmer.h"
+#include "icing/util/status-macros.h"
+
+namespace icing {
+namespace lib {
+
+/*static*/ libtextclassifier3::StatusOr<std::unique_ptr<StemmingExpander>>
+StemmingExpander::Create(std::string language_code) {
+  ICING_ASSIGN_OR_RETURN(std::unique_ptr<Stemmer> stemmer,
+                         stemmer_factory::Create(language_code));
+
+  return std::unique_ptr<StemmingExpander>(
+      new StemmingExpander(std::move(language_code), std::move(stemmer)));
+}
+
+std::vector<ExpandedTerm> StemmingExpander::Expand(
+    std::string_view term) const {
+  std::vector<ExpandedTerm> result;
+  // Add the original term.
+  result.emplace_back(std::string(term), /*is_stemmed_term=*/false);
+
+  libtextclassifier3::StatusOr<std::unique_ptr<Stemmer>> stemmer_or =
+      ProduceStemmer();
+  if (!stemmer_or.ok()) {
+    return result;
+  }
+
+  std::unique_ptr<Stemmer> stemmer = std::move(stemmer_or).ValueOrDie();
+  std::string stemmed_term = stemmer->Stem(term);
+  ReturnStemmer(std::move(stemmer));
+
+  if (stemmed_term != term) {
+    result.emplace_back(std::move(stemmed_term), /*is_stemmed_term=*/true);
+  }
+  return result;
+}
+
+libtextclassifier3::StatusOr<std::unique_ptr<Stemmer>>
+StemmingExpander::ProduceStemmer() const {
+  std::unique_ptr<Stemmer> stemmer = nullptr;
+  {
+    absl_ports::unique_lock l(&mutex_);
+    if (cached_stemmer_ != nullptr) {
+      stemmer = std::move(cached_stemmer_);
+    }
+  }
+  if (stemmer == nullptr) {
+    ICING_ASSIGN_OR_RETURN(stemmer, stemmer_factory::Create(language_code_));
+  }
+
+  return stemmer;
+}
+
+void StemmingExpander::ReturnStemmer(std::unique_ptr<Stemmer> stemmer) const {
+  absl_ports::unique_lock l(&mutex_);
+  if (!cached_stemmer_) {
+    cached_stemmer_ = std::move(stemmer);
+  }
+}
+
+}  // namespace lib
+}  // namespace icing
diff --git a/icing/expand/stemming/stemming-expander.h b/icing/expand/stemming/stemming-expander.h
new file mode 100644
index 0000000..5540700
--- /dev/null
+++ b/icing/expand/stemming/stemming-expander.h
@@ -0,0 +1,97 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef ICING_EXPAND_STEMMING_STEMMING_EXPANDER_H_
+#define ICING_EXPAND_STEMMING_STEMMING_EXPANDER_H_
+
+#include <memory>
+#include <string>
+#include <string_view>
+#include <utility>
+#include <vector>
+
+#include "icing/text_classifier/lib3/utils/base/statusor.h"
+#include "icing/absl_ports/mutex.h"
+#include "icing/absl_ports/thread_annotations.h"
+#include "icing/expand/expander.h"
+#include "icing/expand/stemming/stemmer.h"
+
+namespace icing {
+namespace lib {
+
+// Used to expand a given term to its root form.
+//
+// This class is thread-safe.
+class StemmingExpander : public Expander {
+ public:
+  static libtextclassifier3::StatusOr<std::unique_ptr<StemmingExpander>> Create(
+      std::string language_code);
+
+  ~StemmingExpander() override {}
+
+  // Expands the given term to its root form.
+  //
+  // The expanded vector will contain either:
+  //   - A single element containing the original term, if the stemmer does not
+  //     produce a different term, or
+  //   - Two elements (the original term and its stem), with the first element
+  //     being the original term.
+  std::vector<ExpandedTerm> Expand(std::string_view term) const override;
+
+ private:
+  explicit StemmingExpander(std::string language_code,
+                            std::unique_ptr<Stemmer> stemmer)
+      : language_code_(std::move(language_code)),
+        cached_stemmer_(std::move(stemmer)) {}
+
+  // Produces a stemmer in the language of the StemmingExpander that the caller
+  // owns.
+  //  - If cached_stemmer_ is not null, transfers ownership to the caller and
+  //    sets cached_stemmer_ to null.
+  //  - Otherwise, creates a new stemmer and transfers ownership to the caller.
+  //
+  // Note: Caller must call ReturnStemmer() after using the stemmer.
+  //
+  // Returns:
+  //  - A stemmer for a given language on success.‰
+  //  - INVALID_ARGUMENT_ERROR if the language code is invalid or not supported.
+  //  - INTERNAL_ERROR on errors.
+  //
+  // Requires:
+  //  - language_code_ is a valid code for the stemmer.
+  libtextclassifier3::StatusOr<std::unique_ptr<Stemmer>> ProduceStemmer() const
+      ICING_LOCKS_EXCLUDED(mutex_);
+
+  // Caller transfers ownership of stemmer to the StemmingExpander.
+  //  - If cached_stemmer_ is not null, stemmer will be deleted.
+  //  - Otherwise, the stemmer becomes cached_stemmer_.
+  void ReturnStemmer(std::unique_ptr<Stemmer>) const
+      ICING_LOCKS_EXCLUDED(mutex_);
+
+  // The language code of the stemmer.
+  const std::string language_code_;
+
+  // A cached stemmer that is used to expand a term.
+  //
+  // The stemmer is not thread-safe.
+  mutable std::unique_ptr<Stemmer> cached_stemmer_ ICING_GUARDED_BY(mutex_);
+
+  // Used to provide reader and writer locks
+  mutable absl_ports::shared_mutex mutex_;
+};
+
+}  // namespace lib
+}  // namespace icing
+
+#endif  // ICING_EXPAND_STEMMING_STEMMING_EXPANDER_H_
diff --git a/icing/expand/stemming/stemming-expander_none-stemmer_test.cc b/icing/expand/stemming/stemming-expander_none-stemmer_test.cc
new file mode 100644
index 0000000..49a4f51
--- /dev/null
+++ b/icing/expand/stemming/stemming-expander_none-stemmer_test.cc
@@ -0,0 +1,170 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and∂∂
+// limitations under the License.
+
+#include <array>
+#include <memory>
+#include <string>
+#include <string_view>
+#include <thread>  // NOLINT
+#include <vector>
+
+#include "gmock/gmock.h"
+#include "gtest/gtest.h"
+#include "icing/expand/expander.h"
+#include "icing/expand/stemming/stemming-expander.h"
+#include "icing/testing/common-matchers.h"
+
+namespace icing {
+namespace lib {
+
+namespace {
+
+using ::testing::ElementsAre;
+using ::testing::Eq;
+using ::testing::SizeIs;
+
+constexpr std::string_view kEnglishLanguageCode = "en";
+constexpr std::string_view kRandomLanguageCode = "random";
+
+TEST(NoneStemmingExpanderTest, EmptyTerm) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<Expander> expander,
+      StemmingExpander::Create(std::string(kEnglishLanguageCode)));
+
+  std::vector<ExpandedTerm> expanded_terms = expander->Expand("");
+  EXPECT_THAT(expanded_terms, SizeIs(1));
+  EXPECT_THAT(expanded_terms[0].text, Eq(""));
+  EXPECT_FALSE(expanded_terms[0].is_stemmed_term);
+
+  expanded_terms = expander->Expand("  ");
+  EXPECT_THAT(expanded_terms, SizeIs(1));
+  EXPECT_THAT(expanded_terms[0].text, Eq("  "));
+  EXPECT_FALSE(expanded_terms[0].is_stemmed_term);
+}
+
+TEST(NoneStemmingExpanderTest, NonAlphabetSymbols) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<Expander> expander,
+      StemmingExpander::Create(std::string(kEnglishLanguageCode)));
+
+  std::vector<ExpandedTerm> expanded_terms = expander->Expand("....");
+  EXPECT_THAT(expanded_terms, SizeIs(1));
+  EXPECT_THAT(expanded_terms[0].text, Eq("...."));
+  EXPECT_FALSE(expanded_terms[0].is_stemmed_term);
+
+  expanded_terms = expander->Expand("928347");
+  EXPECT_THAT(expanded_terms, SizeIs(1));
+  EXPECT_THAT(expanded_terms[0].text, Eq("928347"));
+  EXPECT_FALSE(expanded_terms[0].is_stemmed_term);
+}
+
+TEST(NoneStemmingExpanderTest, ExpandTermReturnsOriginalTerm) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<Expander> expander,
+      StemmingExpander::Create(std::string(kEnglishLanguageCode)));
+
+  std::vector<ExpandedTerm> expanded_terms = expander->Expand("running");
+  EXPECT_THAT(expanded_terms, SizeIs(1));
+  EXPECT_THAT(expanded_terms[0].text, Eq("running"));
+  EXPECT_FALSE(expanded_terms[0].is_stemmed_term);
+
+  expanded_terms = expander->Expand("abattement");
+  EXPECT_THAT(expanded_terms, SizeIs(1));
+  EXPECT_THAT(expanded_terms[0].text, Eq("abattement"));
+  EXPECT_FALSE(expanded_terms[0].is_stemmed_term);
+}
+
+TEST(NoneStemmingExpanderTest, LanguageCodeDoesNotMatter) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<Expander> english_expander,
+      StemmingExpander::Create(std::string(kEnglishLanguageCode)));
+
+  std::vector<ExpandedTerm> expanded_terms =
+      english_expander->Expand("running");
+  EXPECT_THAT(expanded_terms, SizeIs(1));
+  EXPECT_THAT(expanded_terms[0].text, Eq("running"));
+  EXPECT_FALSE(expanded_terms[0].is_stemmed_term);
+
+  expanded_terms = english_expander->Expand("abattement");
+  EXPECT_THAT(expanded_terms, SizeIs(1));
+  EXPECT_THAT(expanded_terms[0].text, Eq("abattement"));
+  EXPECT_FALSE(expanded_terms[0].is_stemmed_term);
+
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<Expander> random_expander,
+      StemmingExpander::Create(std::string(kRandomLanguageCode)));
+
+  expanded_terms = random_expander->Expand("running");
+  EXPECT_THAT(expanded_terms, SizeIs(1));
+  EXPECT_THAT(expanded_terms[0].text, Eq("running"));
+  EXPECT_FALSE(expanded_terms[0].is_stemmed_term);
+
+  expanded_terms = random_expander->Expand("abattement");
+  EXPECT_THAT(expanded_terms, SizeIs(1));
+  EXPECT_THAT(expanded_terms[0].text, Eq("abattement"));
+  EXPECT_FALSE(expanded_terms[0].is_stemmed_term);
+}
+
+TEST(NoneStemmingExpanderTest, Utf8Characters) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<Expander> expander,
+      StemmingExpander::Create(std::string(kEnglishLanguageCode)));
+
+  std::vector<ExpandedTerm> expanded_terms = expander->Expand("我们");
+  EXPECT_THAT(expanded_terms, SizeIs(1));
+  EXPECT_THAT(expanded_terms[0].text, Eq("我们"));
+  EXPECT_FALSE(expanded_terms[0].is_stemmed_term);
+
+  expanded_terms = expander->Expand("இக்கதையின்");
+  EXPECT_THAT(expanded_terms, SizeIs(1));
+  EXPECT_THAT(expanded_terms[0].text, Eq("இக்கதையின்"));
+  EXPECT_FALSE(expanded_terms[0].is_stemmed_term);
+}
+
+TEST(StemmingExpanderTest, ThreadSafety) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<Expander> expander,
+      StemmingExpander::Create(std::string(kEnglishLanguageCode)));
+
+  constexpr std::array<std::string_view, 5> kTerms = {
+      "running", "management", "tests", "asdfjgjjh", "!!!))))"};
+
+  // Create kNumThreads threads. Call Expand() from each thread in
+  // parallel using different locales. There should be no crashes.
+  constexpr int kNumThreads = 50;
+  std::vector<std::vector<ExpandedTerm>> expanded_terms(kNumThreads);
+  auto callable = [&](int thread_id) {
+    expanded_terms[thread_id] =
+        expander->Expand(kTerms[thread_id % kTerms.size()]);
+  };
+
+  // Spawn threads to call Expand() in parallel.
+  std::vector<std::thread> thread_objs;
+  for (int i = 0; i < kNumThreads; ++i) {
+    thread_objs.emplace_back(callable, i);
+  }
+
+  // Join threads and verify results
+  for (int i = 0; i < kNumThreads; ++i) {
+    thread_objs[i].join();
+    EXPECT_THAT(expanded_terms[i],
+                ElementsAre(ExpandedTerm(std::string(kTerms[i % kTerms.size()]),
+                                         /*is_stemmed_term_in=*/false)));
+  }
+}
+
+}  // namespace
+
+}  // namespace lib
+}  // namespace icing
diff --git a/icing/file/constants.h b/icing/file/constants.h
new file mode 100644
index 0000000..6d76195
--- /dev/null
+++ b/icing/file/constants.h
@@ -0,0 +1,35 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef ICING_FILE_CONSTANTS_H_
+#define ICING_FILE_CONSTANTS_H_
+
+namespace icing {
+namespace lib {
+namespace constants {
+
+// Icing internal max size for protos.
+//
+// WARNING: Changing this to a larger number may invalidate our assumption
+// that that proto size can safely be stored in the last 3 bytes of the proto
+// header.
+static constexpr int kMaxProtoSize = (1 << 24) - 1;  // 16MiB
+static_assert(kMaxProtoSize <= 0x00FFFFFF,
+              "kMaxProtoSize doesn't fit in 3 bytes");
+
+}  // namespace constants
+}  // namespace lib
+}  // namespace icing
+
+#endif  // ICING_FILE_CONSTANTS_H_
diff --git a/icing/file/file-backed-proto-log.h b/icing/file/file-backed-proto-log.h
index 095f832..0c06f77 100644
--- a/icing/file/file-backed-proto-log.h
+++ b/icing/file/file-backed-proto-log.h
@@ -42,6 +42,7 @@
 #include "icing/text_classifier/lib3/utils/base/statusor.h"
 #include "icing/absl_ports/canonical_errors.h"
 #include "icing/absl_ports/str_cat.h"
+#include "icing/file/constants.h"
 #include "icing/file/filesystem.h"
 #include "icing/file/memory-mapped-file.h"
 #include "icing/legacy/core/icing-string-util.h"
@@ -83,7 +84,7 @@ class FileBackedProtoLog {
     // Must specify values for options.
     Options() = delete;
     explicit Options(bool compress_in,
-                     const int32_t max_proto_size_in = kMaxProtoSize)
+                     const int32_t max_proto_size_in = constants::kMaxProtoSize)
         : compress(compress_in), max_proto_size(max_proto_size_in) {}
   };
 
@@ -284,15 +285,6 @@ class FileBackedProtoLog {
   // protos we support.
   static constexpr uint8_t kProtoMagic = 0x5C;
 
-  // Our internal max for protos.
-  //
-  // WARNING: Changing this to a larger number may invalidate our assumption
-  // that that proto size can safely be stored in the last 3 bytes of the proto
-  // header.
-  static constexpr int kMaxProtoSize = (1 << 24) - 1;  // 16MiB
-  static_assert(kMaxProtoSize <= 0x00FFFFFF,
-                "kMaxProtoSize doesn't fit in 3 bytes");
-
   // Chunks of the file to mmap at a time, so we don't mmap the entire file.
   // Only used on 32-bit devices
   static constexpr int kMmapChunkSize = 4 * 1024 * 1024;  // 4MiB
@@ -326,7 +318,7 @@ FileBackedProtoLog<ProtoT>::Create(const Filesystem* filesystem,
 
   // Since we store the proto_size in 3 bytes, we can only support protos of up
   // to 16MiB.
-  if (options.max_proto_size > kMaxProtoSize) {
+  if (options.max_proto_size > constants::kMaxProtoSize) {
     return absl_ports::InvalidArgumentError(IcingStringUtil::StringPrintf(
         "options.max_proto_size must be under 16MiB, was %d",
         options.max_proto_size));
diff --git a/icing/file/file-backed-proto.h b/icing/file/file-backed-proto.h
index 8c5743b..65566c6 100644
--- a/icing/file/file-backed-proto.h
+++ b/icing/file/file-backed-proto.h
@@ -80,7 +80,7 @@ class FileBackedProto {
   // RETURNS:
   //   - the checksum of the proto or 0 if the file is empty/non-existent
   //   - INTERNAL_ERROR if an IO error or a corruption was encountered.
-  libtextclassifier3::StatusOr<Crc32> ComputeChecksum() const
+  libtextclassifier3::StatusOr<Crc32> GetChecksum() const
       ICING_LOCKS_EXCLUDED(mutex_);
 
   // Returns a reference to the proto read from the file. It
@@ -140,7 +140,7 @@ FileBackedProto<ProtoT>::FileBackedProto(const Filesystem& filesystem,
     : filesystem_(&filesystem), file_path_(file_path) {}
 
 template <typename ProtoT>
-libtextclassifier3::StatusOr<Crc32> FileBackedProto<ProtoT>::ComputeChecksum()
+libtextclassifier3::StatusOr<Crc32> FileBackedProto<ProtoT>::GetChecksum()
     const {
   absl_ports::unique_lock l(&mutex_);
   if (cached_proto_ == nullptr) {
diff --git a/icing/file/file-backed-proto_test.cc b/icing/file/file-backed-proto_test.cc
index 009af52..e1073e2 100644
--- a/icing/file/file-backed-proto_test.cc
+++ b/icing/file/file-backed-proto_test.cc
@@ -21,6 +21,7 @@
 #include "icing/proto/document.pb.h"
 #include "icing/testing/common-matchers.h"
 #include "icing/testing/tmp-directory.h"
+#include "icing/util/crc32.h"
 
 using ::icing::lib::portable_equals_proto::EqualsProto;
 using ::testing::Not;
@@ -52,6 +53,40 @@ TEST_F(FileBackedProtoTest, SimpleReadWriteTest) {
   EXPECT_THAT(file_proto.Read(), IsOkAndHolds(Pointee(EqualsProto(document))));
 }
 
+TEST_F(FileBackedProtoTest, GetChecksum) {
+  DocumentProto document =
+      DocumentBuilder().SetKey("namespace", "google.com").Build();
+
+  FileBackedProto<DocumentProto> file_proto(filesystem_, filename_);
+  EXPECT_THAT(file_proto.GetChecksum(), IsOkAndHolds(Crc32(0)));
+
+  ICING_ASSERT_OK(file_proto.Write(std::make_unique<DocumentProto>(document)));
+  EXPECT_THAT(file_proto.GetChecksum(), IsOkAndHolds(Crc32(385085217)));
+}
+
+TEST_F(FileBackedProtoTest, GetChecksumAcrossInstances) {
+  DocumentProto document =
+      DocumentBuilder().SetKey("namespace", "google.com").Build();
+
+  {
+    FileBackedProto<DocumentProto> file_proto(filesystem_, filename_);
+    EXPECT_THAT(file_proto.GetChecksum(), IsOkAndHolds(Crc32(0)));
+  }
+
+  {
+    FileBackedProto<DocumentProto> file_proto(filesystem_, filename_);
+    EXPECT_THAT(file_proto.GetChecksum(), IsOkAndHolds(Crc32(0)));
+    ICING_ASSERT_OK(
+        file_proto.Write(std::make_unique<DocumentProto>(document)));
+    EXPECT_THAT(file_proto.GetChecksum(), IsOkAndHolds(Crc32(385085217)));
+  }
+
+  {
+    FileBackedProto<DocumentProto> file_proto(filesystem_, filename_);
+    EXPECT_THAT(file_proto.GetChecksum(), IsOkAndHolds(Crc32(385085217)));
+  }
+}
+
 TEST_F(FileBackedProtoTest, DataPersistsAcrossMultipleInstancesTest) {
   DocumentProto document =
       DocumentBuilder().SetKey("namespace", "google.com").Build();
diff --git a/icing/file/file-backed-vector.h b/icing/file/file-backed-vector.h
index 7408e8b..72b7fe3 100644
--- a/icing/file/file-backed-vector.h
+++ b/icing/file/file-backed-vector.h
@@ -325,7 +325,7 @@ class FileBackedVector {
   //   OUT_OF_RANGE_ERROR if file cannot be grown (i.e. reach
   //                      mmapped_file_->max_file_size())
   libtextclassifier3::Status Append(const T& value) {
-    return Set(header_->num_elements, value);
+    return Set(header()->num_elements, value);
   }
 
   // Allocates spaces with given length in the end of the vector and returns a
@@ -399,18 +399,28 @@ class FileBackedVector {
   //   INTERNAL_ERROR on IO error
   libtextclassifier3::StatusOr<int64_t> GetElementsFileSize() const;
 
-  // Updates checksum of the vector contents and returns it.
+  // Calculates the checksum of the vector contents and updates the header to
+  // hold this updated value.
   //
   // Returns:
-  //   INTERNAL_ERROR if the vector's internal state is inconsistent
-  libtextclassifier3::StatusOr<Crc32> ComputeChecksum();
+  //   Checksum of the vector on success
+  //   INTERNAL_ERROR on IO error
+  libtextclassifier3::StatusOr<Crc32> UpdateChecksum();
+
+  // Calculates the checksum of the vector contents and returns it. Does NOT
+  // update the header.
+  //
+  // Returns:
+  //   Checksum of the vector on success
+  //   INTERNAL_ERROR on IO error
+  Crc32 GetChecksum() const;
 
   // Accessors.
   const T* array() const {
-    return reinterpret_cast<const T*>(mmapped_file_->region());
+    return reinterpret_cast<const T*>(mmapped_file_->region() + sizeof(Header));
   }
 
-  int32_t num_elements() const { return header_->num_elements; }
+  int32_t num_elements() const { return header()->num_elements; }
 
  public:
   class MutableArrayView {
@@ -489,21 +499,21 @@ class FileBackedVector {
   // Can only be created through the factory ::Create function
   explicit FileBackedVector(const Filesystem& filesystem,
                             const std::string& file_path,
-                            std::unique_ptr<Header> header,
                             MemoryMappedFile&& mmapped_file);
 
   // Initialize a new FileBackedVector, and create the file.
   static libtextclassifier3::StatusOr<std::unique_ptr<FileBackedVector<T>>>
   InitializeNewFile(const Filesystem& filesystem, const std::string& file_path,
-                    ScopedFd fd, MemoryMappedFile::Strategy mmap_strategy,
+                    MemoryMappedFile::Strategy mmap_strategy,
                     int32_t max_file_size, int32_t pre_mapping_mmap_size);
 
   // Initialize a FileBackedVector from an existing file.
   static libtextclassifier3::StatusOr<std::unique_ptr<FileBackedVector<T>>>
   InitializeExistingFile(const Filesystem& filesystem,
-                         const std::string& file_path, ScopedFd fd,
+                         const std::string& file_path,
                          MemoryMappedFile::Strategy mmap_strategy,
-                         int32_t max_file_size, int32_t pre_mapping_mmap_size);
+                         int64_t file_size, int32_t max_file_size,
+                         int32_t pre_mapping_mmap_size);
 
   // Grows the underlying file to hold at least num_elements
   //
@@ -512,13 +522,20 @@ class FileBackedVector {
   libtextclassifier3::Status GrowIfNecessary(int32_t num_elements);
 
   T* mutable_array() const {
-    return reinterpret_cast<T*>(mmapped_file_->mutable_region());
+    return reinterpret_cast<T*>(mmapped_file_->mutable_region() +
+                                sizeof(Header));
+  }
+
+  const Header* header() const {
+    return reinterpret_cast<const Header*>(mmapped_file_->region());
+  }
+  Header* header() {
+    return reinterpret_cast<Header*>(mmapped_file_->mutable_region());
   }
 
   // Cached constructor params.
   const Filesystem* const filesystem_;
   const std::string file_path_;
-  std::unique_ptr<Header> header_;
   std::unique_ptr<MemoryMappedFile> mmapped_file_;
 
   // Offset before which all the elements have been included in the calculation
@@ -579,97 +596,95 @@ FileBackedVector<T>::Create(const Filesystem& filesystem,
         "Invalid max file size for FileBackedVector");
   }
 
-  ScopedFd fd(filesystem.OpenForWrite(file_path.c_str()));
-  if (!fd.is_valid()) {
-    return absl_ports::InternalError(
-        absl_ports::StrCat("Failed to open ", file_path));
-  }
+  int64_t file_size = 0;
+  {
+    ScopedFd fd(filesystem.OpenForWrite(file_path.c_str()));
+    if (!fd.is_valid()) {
+      return absl_ports::InternalError(
+          absl_ports::StrCat("Failed to open ", file_path));
+    }
 
-  int64_t file_size = filesystem.GetFileSize(file_path.c_str());
-  if (file_size == Filesystem::kBadFileSize) {
-    return absl_ports::InternalError(
-        absl_ports::StrCat("Bad file size for file ", file_path));
-  }
+    file_size = filesystem.GetFileSize(fd.get());
+    if (file_size == Filesystem::kBadFileSize) {
+      return absl_ports::InternalError(
+          absl_ports::StrCat("Bad file size for file ", file_path));
+    }
 
-  if (max_file_size < file_size) {
-    return absl_ports::InvalidArgumentError(
-        "Max file size should not be smaller than the existing file size");
+    if (max_file_size < file_size) {
+      return absl_ports::InvalidArgumentError(
+          "Max file size should not be smaller than the existing file size");
+    }
   }
 
-  const bool new_file = file_size == 0;
-  if (new_file) {
-    return InitializeNewFile(filesystem, file_path, std::move(fd),
-                             mmap_strategy, max_file_size,
-                             pre_mapping_mmap_size);
+  if (file_size == 0) {
+    return InitializeNewFile(filesystem, file_path, mmap_strategy,
+                             max_file_size, pre_mapping_mmap_size);
   }
-  return InitializeExistingFile(filesystem, file_path, std::move(fd),
-                                mmap_strategy, max_file_size,
-                                pre_mapping_mmap_size);
+  return InitializeExistingFile(filesystem, file_path, mmap_strategy, file_size,
+                                max_file_size, pre_mapping_mmap_size);
 }
 
 template <typename T>
 libtextclassifier3::StatusOr<std::unique_ptr<FileBackedVector<T>>>
 FileBackedVector<T>::InitializeNewFile(const Filesystem& filesystem,
                                        const std::string& file_path,
-                                       ScopedFd fd,
                                        MemoryMappedFile::Strategy mmap_strategy,
                                        int32_t max_file_size,
                                        int32_t pre_mapping_mmap_size) {
   // Create header.
-  auto header = std::make_unique<Header>();
-  header->magic = FileBackedVector<T>::Header::kMagic;
-  header->element_size = kElementTypeSize;
-  header->header_checksum = header->CalculateHeaderChecksum();
-
-  // We use Write() here, instead of writing through the mmapped region
-  // created below, so we can gracefully handle errors that occur when the
-  // disk is full. See b/77309668 for details.
-  if (!filesystem.PWrite(fd.get(), /*offset=*/0, header.get(),
-                         Header::kHeaderSize)) {
-    return absl_ports::InternalError("Failed to write header");
-  }
-
-  // Close the fd since constructor of MemoryMappedFile calls mmap() and we need
-  // to flush fd before mmap().
-  fd.reset();
-
+  Header header = {FileBackedVector<T>::Header::kMagic, kElementTypeSize,
+                   /*num_elements=*/0, /*vector_checksum=*/0,
+                   /*header_checksum=*/0};
+  header.header_checksum = header.CalculateHeaderChecksum();
+
+  // Create the mmapped file and write the new header to it.
+  // Determine the correct pre_mapping size. max_file_size is specified as the
+  // size of the whole file whereas pre_mapping_mmap_size is just the size of
+  // elements, not including the header.
+  int64_t pre_mapping_mmap_size_long =
+      std::min(max_file_size, pre_mapping_mmap_size + Header::kHeaderSize);
   ICING_ASSIGN_OR_RETURN(
       MemoryMappedFile mmapped_file,
       MemoryMappedFile::Create(filesystem, file_path, mmap_strategy,
-                               max_file_size,
-                               /*pre_mapping_file_offset=*/Header::kHeaderSize,
-                               /*pre_mapping_mmap_size=*/
-                               std::min(max_file_size - Header::kHeaderSize,
-                                        pre_mapping_mmap_size)));
-
-  return std::unique_ptr<FileBackedVector<T>>(new FileBackedVector<T>(
-      filesystem, file_path, std::move(header), std::move(mmapped_file)));
+                               max_file_size, /*pre_mapping_file_offset=*/0,
+                               pre_mapping_mmap_size_long));
+  ICING_RETURN_IF_ERROR(mmapped_file.GrowAndRemapIfNecessary(
+      /*new_file_offset=*/0, sizeof(Header)));
+  memcpy(mmapped_file.mutable_region(), &header, sizeof(header));
+
+  return std::unique_ptr<FileBackedVector<T>>(
+      new FileBackedVector<T>(filesystem, file_path, std::move(mmapped_file)));
 }
 
 template <typename T>
 libtextclassifier3::StatusOr<std::unique_ptr<FileBackedVector<T>>>
 FileBackedVector<T>::InitializeExistingFile(
     const Filesystem& filesystem, const std::string& file_path,
-    const ScopedFd fd, MemoryMappedFile::Strategy mmap_strategy,
+    MemoryMappedFile::Strategy mmap_strategy, int64_t file_size,
     int32_t max_file_size, int32_t pre_mapping_mmap_size) {
-  int64_t file_size = filesystem.GetFileSize(file_path.c_str());
-  if (file_size == Filesystem::kBadFileSize) {
-    return absl_ports::InternalError(
-        absl_ports::StrCat("Bad file size for file ", file_path));
-  }
-
   if (file_size < Header::kHeaderSize) {
     return absl_ports::InternalError(
         absl_ports::StrCat("File header too short for ", file_path));
   }
 
-  auto header = std::make_unique<Header>();
-  if (!filesystem.PRead(fd.get(), header.get(), Header::kHeaderSize,
-                        /*offset=*/0)) {
-    return absl_ports::InternalError(
-        absl_ports::StrCat("Failed to read header of ", file_path));
-  }
-
+  // Mmap the content of the file so its easier to access elements from the
+  // mmapped region. Although users can specify their own pre_mapping_mmap_size,
+  // we should make sure that the pre-map size is at least file_size to make all
+  // existing elements available.
+  // As noted above, pre_mapping_mmap_size is just the size of elements, not
+  // including the header. So we need to add the header size to make it
+  // comparable to file size.
+  int64_t pre_mapping_mmap_size_long = std::max(
+      file_size,
+      static_cast<int64_t>(std::min(
+          max_file_size, pre_mapping_mmap_size + Header::kHeaderSize)));
+  ICING_ASSIGN_OR_RETURN(MemoryMappedFile mmapped_file,
+                         MemoryMappedFile::Create(filesystem, file_path,
+                                                  mmap_strategy, max_file_size,
+                                                  /*pre_mapping_file_offset=*/0,
+                                                  pre_mapping_mmap_size_long));
+
+  const Header* header = reinterpret_cast<const Header*>(mmapped_file.region());
   // Make sure the header is still valid before we use any of its values. This
   // should technically be included in the header_checksum check below, but this
   // is a quick/fast check that can save us from an extra crc computation.
@@ -699,34 +714,19 @@ FileBackedVector<T>::InitializeExistingFile(
         min_file_size, file_size));
   }
 
-  // Mmap the content of the vector, excluding the header so its easier to
-  // access elements from the mmapped region
-  // Although users can specify their own pre_mapping_mmap_size, we should make
-  // sure that the pre-map size is at least file_size - Header::kHeaderSize to
-  // make all existing elements available.
-  ICING_ASSIGN_OR_RETURN(
-      MemoryMappedFile mmapped_file,
-      MemoryMappedFile::Create(
-          filesystem, file_path, mmap_strategy, max_file_size,
-          /*pre_mapping_file_offset=*/Header::kHeaderSize,
-          /*pre_mapping_mmap_size=*/
-          std::max(
-              file_size - Header::kHeaderSize,
-              static_cast<int64_t>(std::min(max_file_size - Header::kHeaderSize,
-                                            pre_mapping_mmap_size)))));
-
   // Check vector contents
-  Crc32 vector_checksum(
-      std::string_view(reinterpret_cast<const char*>(mmapped_file.region()),
-                       header->num_elements * kElementTypeSize));
+  const char* vector_contents =
+      reinterpret_cast<const char*>(mmapped_file.region() + sizeof(Header));
+  Crc32 vector_checksum(std::string_view(
+      vector_contents, header->num_elements * kElementTypeSize));
 
   if (vector_checksum.Get() != header->vector_checksum) {
     return absl_ports::FailedPreconditionError(
         absl_ports::StrCat("Invalid vector contents for ", file_path));
   }
 
-  return std::unique_ptr<FileBackedVector<T>>(new FileBackedVector<T>(
-      filesystem, file_path, std::move(header), std::move(mmapped_file)));
+  return std::unique_ptr<FileBackedVector<T>>(
+      new FileBackedVector<T>(filesystem, file_path, std::move(mmapped_file)));
 }
 
 template <typename T>
@@ -742,14 +742,12 @@ libtextclassifier3::Status FileBackedVector<T>::Delete(
 template <typename T>
 FileBackedVector<T>::FileBackedVector(const Filesystem& filesystem,
                                       const std::string& file_path,
-                                      std::unique_ptr<Header> header,
                                       MemoryMappedFile&& mmapped_file)
     : filesystem_(&filesystem),
       file_path_(file_path),
-      header_(std::move(header)),
       mmapped_file_(
           std::make_unique<MemoryMappedFile>(std::move(mmapped_file))),
-      changes_end_(header_->num_elements) {}
+      changes_end_(header()->num_elements) {}
 
 template <typename T>
 FileBackedVector<T>::~FileBackedVector() {
@@ -778,10 +776,10 @@ libtextclassifier3::StatusOr<const T*> FileBackedVector<T>::Get(
         IcingStringUtil::StringPrintf("Index, %d, was less than 0", idx));
   }
 
-  if (idx >= header_->num_elements) {
+  if (idx >= header()->num_elements) {
     return absl_ports::OutOfRangeError(IcingStringUtil::StringPrintf(
         "Index, %d, was greater than vector size, %d", idx,
-        header_->num_elements));
+        header()->num_elements));
   }
 
   return &array()[idx];
@@ -795,10 +793,10 @@ FileBackedVector<T>::GetMutable(int32_t idx) {
         IcingStringUtil::StringPrintf("Index, %d, was less than 0", idx));
   }
 
-  if (idx >= header_->num_elements) {
+  if (idx >= header()->num_elements) {
     return absl_ports::OutOfRangeError(IcingStringUtil::StringPrintf(
         "Index, %d, was greater than vector size, %d", idx,
-        header_->num_elements));
+        header()->num_elements));
   }
 
   return MutableView(this, &mutable_array()[idx]);
@@ -812,10 +810,10 @@ FileBackedVector<T>::GetMutable(int32_t idx, int32_t len) {
         IcingStringUtil::StringPrintf("Index, %d, was less than 0", idx));
   }
 
-  if (idx > header_->num_elements - len) {
+  if (idx > header()->num_elements - len) {
     return absl_ports::OutOfRangeError(IcingStringUtil::StringPrintf(
         "Index with len, %d %d, was greater than vector size, %d", idx, len,
-        header_->num_elements));
+        header()->num_elements));
   }
 
   return MutableArrayView(this, &mutable_array()[idx], len);
@@ -848,8 +846,8 @@ libtextclassifier3::Status FileBackedVector<T>::Set(int32_t idx, int32_t len,
 
   ICING_RETURN_IF_ERROR(GrowIfNecessary(idx + len));
 
-  if (idx + len > header_->num_elements) {
-    header_->num_elements = idx + len;
+  if (idx + len > header()->num_elements) {
+    header()->num_elements = idx + len;
   }
 
   for (int32_t i = 0; i < len; ++i) {
@@ -872,19 +870,19 @@ FileBackedVector<T>::Allocate(int32_t len) {
     return absl_ports::OutOfRangeError("Invalid allocate length");
   }
 
-  if (len > kMaxNumElements - header_->num_elements) {
+  if (len > kMaxNumElements - header()->num_elements) {
     return absl_ports::OutOfRangeError(
         IcingStringUtil::StringPrintf("Cannot allocate %d elements", len));
   }
 
-  // Although header_->num_elements + len doesn't exceed kMaxNumElements, the
+  // Although header()->num_elements + len doesn't exceed kMaxNumElements, the
   // actual max # of elements are determined by mmapped_file_->max_file_size(),
   // kElementTypeSize, and kHeaderSize. Thus, it is still possible to fail to
   // grow the file.
-  ICING_RETURN_IF_ERROR(GrowIfNecessary(header_->num_elements + len));
+  ICING_RETURN_IF_ERROR(GrowIfNecessary(header()->num_elements + len));
 
-  int32_t start_idx = header_->num_elements;
-  header_->num_elements += len;
+  int32_t start_idx = header()->num_elements;
+  header()->num_elements += len;
 
   return MutableArrayView(this, &mutable_array()[start_idx], len);
 }
@@ -897,7 +895,7 @@ libtextclassifier3::Status FileBackedVector<T>::GrowIfNecessary(
     return libtextclassifier3::Status::OK;
   }
 
-  if (num_elements <= header_->num_elements) {
+  if (num_elements <= header()->num_elements) {
     return libtextclassifier3::Status::OK;
   }
 
@@ -916,8 +914,8 @@ libtextclassifier3::Status FileBackedVector<T>::GrowIfNecessary(
   }
 
   int64_t round_up_file_size_needed = math_util::RoundUpTo(
-      int64_t{least_file_size_needed},
-      int64_t{FileBackedVector<T>::kGrowElements} * kElementTypeSize);
+      static_cast<int64_t>(least_file_size_needed),
+      static_cast<int64_t>(FileBackedVector<T>::kGrowElements) * kElementTypeSize);
 
   // Call GrowAndRemapIfNecessary. It handles file growth internally and remaps
   // intelligently.
@@ -926,10 +924,9 @@ libtextclassifier3::Status FileBackedVector<T>::GrowIfNecessary(
   // round_up_file_size_needed exceeds it, so use the smaller value of them as
   // new_mmap_size.
   ICING_RETURN_IF_ERROR(mmapped_file_->GrowAndRemapIfNecessary(
-      /*new_file_offset=*/Header::kHeaderSize,
+      /*new_file_offset=*/0,
       /*new_mmap_size=*/std::min(round_up_file_size_needed,
-                                 mmapped_file_->max_file_size()) -
-          Header::kHeaderSize));
+                                 mmapped_file_->max_file_size())));
 
   return libtextclassifier3::Status::OK;
 }
@@ -942,10 +939,10 @@ libtextclassifier3::Status FileBackedVector<T>::TruncateTo(
         "Truncated length %d must be >= 0", new_num_elements));
   }
 
-  if (new_num_elements >= header_->num_elements) {
+  if (new_num_elements >= header()->num_elements) {
     return absl_ports::OutOfRangeError(IcingStringUtil::StringPrintf(
         "Truncated length %d must be less than the current size %d",
-        new_num_elements, header_->num_elements));
+        new_num_elements, header()->num_elements));
   }
 
   ICING_VLOG(2)
@@ -953,9 +950,9 @@ libtextclassifier3::Status FileBackedVector<T>::TruncateTo(
   changes_.clear();
   saved_original_buffer_.clear();
   changes_end_ = 0;
-  header_->vector_checksum = 0;
+  header()->vector_checksum = 0;
 
-  header_->num_elements = new_num_elements;
+  header()->num_elements = new_num_elements;
   return libtextclassifier3::Status::OK;
 }
 
@@ -963,7 +960,7 @@ template <typename T>
 libtextclassifier3::Status FileBackedVector<T>::Sort(int32_t begin_idx,
                                                      int32_t end_idx) {
   if (begin_idx < 0 || begin_idx >= end_idx ||
-      end_idx > header_->num_elements) {
+      end_idx > header()->num_elements) {
     return absl_ports::OutOfRangeError(IcingStringUtil::StringPrintf(
         "Invalid sort index, %d, %d", begin_idx, end_idx));
   }
@@ -987,7 +984,7 @@ void FileBackedVector<T>::SetDirty(int32_t idx) {
       changes_.clear();
       saved_original_buffer_.clear();
       changes_end_ = 0;
-      header_->vector_checksum = 0;
+      header()->vector_checksum = 0;
     } else {
       int32_t start_byte = idx * kElementTypeSize;
 
@@ -1000,12 +997,12 @@ void FileBackedVector<T>::SetDirty(int32_t idx) {
 }
 
 template <typename T>
-libtextclassifier3::StatusOr<Crc32> FileBackedVector<T>::ComputeChecksum() {
+libtextclassifier3::StatusOr<Crc32> FileBackedVector<T>::UpdateChecksum() {
   // First apply the modified area. Keep a bitmap of already updated
   // regions so we don't double-update.
   std::vector<bool> updated(changes_end_);
   uint32_t cur_offset = 0;
-  Crc32 cur_crc(header_->vector_checksum);
+  Crc32 cur_crc(header()->vector_checksum);
   int num_partial_crcs = 0;
   int num_truncated = 0;
   int num_overlapped = 0;
@@ -1018,7 +1015,7 @@ libtextclassifier3::StatusOr<Crc32> FileBackedVector<T>::ComputeChecksum() {
     }
 
     // Skip truncated tracked changes.
-    if (change_offset >= header_->num_elements) {
+    if (change_offset >= header()->num_elements) {
       ++num_truncated;
       continue;
     }
@@ -1080,43 +1077,48 @@ libtextclassifier3::StatusOr<Crc32> FileBackedVector<T>::ComputeChecksum() {
   }
 
   // Now update with grown area.
-  if (changes_end_ < header_->num_elements) {
+  if (changes_end_ < header()->num_elements) {
     // Explicitly create the string_view with length
     std::string_view update_str(
         reinterpret_cast<const char*>(array()) +
             changes_end_ * kElementTypeSize,
-        (header_->num_elements - changes_end_) * kElementTypeSize);
+        (header()->num_elements - changes_end_) * kElementTypeSize);
     cur_crc.Append(update_str);
     ICING_VLOG(2) << IcingStringUtil::StringPrintf(
         "Array update tail crc offset %d -> %d", changes_end_,
-        header_->num_elements);
+        header()->num_elements);
   }
 
   // Clear, now that we've applied changes.
   changes_.clear();
   saved_original_buffer_.clear();
-  changes_end_ = header_->num_elements;
+  changes_end_ = header()->num_elements;
 
   // Commit new crc.
-  header_->vector_checksum = cur_crc.Get();
+  header()->vector_checksum = cur_crc.Get();
+  header()->header_checksum = header()->CalculateHeaderChecksum();
   return cur_crc;
 }
 
 template <typename T>
-libtextclassifier3::Status FileBackedVector<T>::PersistToDisk() {
-  // Update and write the header
-  ICING_ASSIGN_OR_RETURN(Crc32 checksum, ComputeChecksum());
-  header_->vector_checksum = checksum.Get();
-  header_->header_checksum = header_->CalculateHeaderChecksum();
-
-  if (!filesystem_->PWrite(file_path_.c_str(), /*offset=*/0, header_.get(),
-                           Header::kHeaderSize)) {
-    return absl_ports::InternalError("Failed to sync header");
+Crc32 FileBackedVector<T>::GetChecksum() const {
+  if (changes_.empty() && changes_end_ == header()->num_elements) {
+    // No changes, just return the checksum cached in the header.
+    return Crc32(header()->vector_checksum);
   }
+  // TODO(b/352778910): Mirror the same logic in UpdateChecksum() to reduce the
+  // cost of GetChecksum.
+  Crc32 cur_crc(std::string_view(reinterpret_cast<const char*>(array()),
+                                 header()->num_elements * kElementTypeSize));
+  return cur_crc;
+}
 
-  MemoryMappedFile::Strategy strategy = mmapped_file_->strategy();
-
-  if (strategy == MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC) {
+template <typename T>
+libtextclassifier3::Status FileBackedVector<T>::PersistToDisk() {
+  // Update and write the header
+  ICING_RETURN_IF_ERROR(UpdateChecksum());
+  if (mmapped_file_->strategy() ==
+      MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC) {
     // Changes should have been applied to the underlying file, but call msync()
     // as an extra safety step to ensure they are written out.
     ICING_RETURN_IF_ERROR(mmapped_file_->PersistToDisk());
diff --git a/icing/file/file-backed-vector_test.cc b/icing/file/file-backed-vector_test.cc
index 524bbc1..a4f5bf8 100644
--- a/icing/file/file-backed-vector_test.cc
+++ b/icing/file/file-backed-vector_test.cc
@@ -198,7 +198,9 @@ TEST_F(FileBackedVectorTest, SimpleShared) {
       FileBackedVector<char>::Create(
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
 
   std::string expected = "abcde";
   Insert(vector.get(), 0, expected);
@@ -208,7 +210,9 @@ TEST_F(FileBackedVectorTest, SimpleShared) {
   uint32_t good_crc_value = 1134899064U;
   const Crc32 good_crc(good_crc_value);
   // Explicit call to update the crc does update the value
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(good_crc));
+  EXPECT_THAT(vector->GetChecksum(), Eq(good_crc));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(good_crc));
+  EXPECT_THAT(vector->GetChecksum(), Eq(good_crc));
 
   // PersistToDisk does nothing bad.
   ICING_EXPECT_OK(vector->PersistToDisk());
@@ -254,7 +258,9 @@ TEST_F(FileBackedVectorTest, SimpleShared) {
   ICING_EXPECT_OK(vector->TruncateTo(0));
 
   // Crc is cleared after truncation and reset to 0.
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
   EXPECT_EQ(0u, vector->num_elements());
 }
 
@@ -266,7 +272,9 @@ TEST_F(FileBackedVectorTest, Get) {
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
 
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
 
   std::string expected = "abc";
   Insert(vector.get(), 0, expected);
@@ -291,7 +299,9 @@ TEST_F(FileBackedVectorTest, SetWithoutGrowing) {
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
 
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
 
   std::string original = "abcde";
   Insert(vector.get(), /*idx=*/0, original);
@@ -311,7 +321,9 @@ TEST_F(FileBackedVectorTest, SetWithGrowing) {
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
 
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
 
   std::string original = "abcde";
   Insert(vector.get(), /*idx=*/0, original);
@@ -350,7 +362,9 @@ TEST_F(FileBackedVectorTest, MutableView) {
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
   Insert(vector.get(), /*idx=*/0, std::string(1000, 'a'));
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(2620640643U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2620640643U)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(2620640643U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2620640643U)));
 
   ICING_ASSERT_OK_AND_ASSIGN(FileBackedVector<char>::MutableView mutable_elt,
                              vector->GetMutable(3));
@@ -370,7 +384,9 @@ TEST_F(FileBackedVectorTest, MutableViewShouldSetDirty) {
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
   Insert(vector.get(), /*idx=*/0, std::string(1000, 'a'));
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(2620640643U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2620640643U)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(2620640643U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2620640643U)));
 
   std::string_view reconstructed_view =
       std::string_view(vector->array(), vector->num_elements());
@@ -380,11 +396,11 @@ TEST_F(FileBackedVectorTest, MutableViewShouldSetDirty) {
 
   // Mutate the element via MutateView
   // If non-const Get() is called, MutateView should set the element index dirty
-  // so that ComputeChecksum() can pick up the change and compute the checksum
+  // so that UpdateChecksum() can pick up the change and compute the checksum
   // correctly. Validate by mapping another array on top.
   mutable_elt.Get() = 'b';
   ASSERT_THAT(vector->Get(3), IsOkAndHolds(Pointee(Eq('b'))));
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc1, vector->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc1, vector->UpdateChecksum());
   Crc32 full_crc1;
   full_crc1.Append(reconstructed_view);
   EXPECT_THAT(crc1, Eq(full_crc1));
@@ -392,7 +408,7 @@ TEST_F(FileBackedVectorTest, MutableViewShouldSetDirty) {
   // Mutate and test again.
   mutable_elt.Get() = 'c';
   ASSERT_THAT(vector->Get(3), IsOkAndHolds(Pointee(Eq('c'))));
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc2, vector->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc2, vector->UpdateChecksum());
   Crc32 full_crc2;
   full_crc2.Append(reconstructed_view);
   EXPECT_THAT(crc2, Eq(full_crc2));
@@ -406,7 +422,9 @@ TEST_F(FileBackedVectorTest, MutableArrayView) {
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
   Insert(vector.get(), /*idx=*/0, std::vector<int>(/*count=*/100, /*value=*/1));
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(2494890115U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2494890115U)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(2494890115U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2494890115U)));
 
   constexpr int kArrayViewOffset = 5;
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -436,7 +454,9 @@ TEST_F(FileBackedVectorTest, MutableArrayViewSetArray) {
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
   Insert(vector.get(), /*idx=*/0, std::vector<int>(/*count=*/100, /*value=*/1));
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(2494890115U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2494890115U)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(2494890115U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2494890115U)));
 
   constexpr int kArrayViewOffset = 3;
   constexpr int kArrayViewLen = 5;
@@ -463,7 +483,9 @@ TEST_F(FileBackedVectorTest, MutableArrayViewSetArrayWithZeroLength) {
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
   Insert(vector.get(), /*idx=*/0, std::vector<int>(/*count=*/100, /*value=*/1));
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(2494890115U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2494890115U)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(2494890115U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2494890115U)));
 
   constexpr int kArrayViewOffset = 3;
   constexpr int kArrayViewLen = 5;
@@ -486,7 +508,9 @@ TEST_F(FileBackedVectorTest, MutableArrayViewIndexOperatorShouldSetDirty) {
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
   Insert(vector.get(), /*idx=*/0, std::vector<int>(/*count=*/100, /*value=*/1));
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(2494890115U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2494890115U)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(2494890115U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2494890115U)));
 
   std::string_view reconstructed_view(
       reinterpret_cast<const char*>(vector->array()),
@@ -499,27 +523,27 @@ TEST_F(FileBackedVectorTest, MutableArrayViewIndexOperatorShouldSetDirty) {
 
   // Use operator[] to mutate elements
   // If non-const operator[] is called, MutateView should set the element index
-  // dirty so that ComputeChecksum() can pick up the change and compute the
+  // dirty so that UpdateChecksum() can pick up the change and compute the
   // checksum correctly. Validate by mapping another array on top.
   mutable_arr[0] = 2;
   ASSERT_THAT(vector->Get(kArrayViewOffset + 0), IsOkAndHolds(Pointee(Eq(2))));
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc1, vector->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc1, vector->UpdateChecksum());
   EXPECT_THAT(crc1, Eq(Crc32(reconstructed_view)));
 
   mutable_arr[1] = 3;
   ASSERT_THAT(vector->Get(kArrayViewOffset + 1), IsOkAndHolds(Pointee(Eq(3))));
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc2, vector->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc2, vector->UpdateChecksum());
   EXPECT_THAT(crc2, Eq(Crc32(reconstructed_view)));
 
   mutable_arr[2] = 4;
   ASSERT_THAT(vector->Get(kArrayViewOffset + 2), IsOkAndHolds(Pointee(Eq(4))));
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc3, vector->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc3, vector->UpdateChecksum());
   EXPECT_THAT(crc3, Eq(Crc32(reconstructed_view)));
 
   // Change the same position. It should set dirty again.
   mutable_arr[0] = 5;
   ASSERT_THAT(vector->Get(kArrayViewOffset + 0), IsOkAndHolds(Pointee(Eq(5))));
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc4, vector->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc4, vector->UpdateChecksum());
   EXPECT_THAT(crc4, Eq(Crc32(reconstructed_view)));
 }
 
@@ -531,7 +555,9 @@ TEST_F(FileBackedVectorTest, MutableArrayViewSetArrayShouldSetDirty) {
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
   Insert(vector.get(), /*idx=*/0, std::vector<int>(/*count=*/100, /*value=*/1));
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(2494890115U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2494890115U)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(2494890115U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2494890115U)));
 
   std::string_view reconstructed_view(
       reinterpret_cast<const char*>(vector->array()),
@@ -547,7 +573,7 @@ TEST_F(FileBackedVectorTest, MutableArrayViewSetArrayShouldSetDirty) {
   mutable_arr.SetArray(/*idx=*/0, change.data(), change.size());
   ASSERT_THAT(Get(vector.get(), kArrayViewOffset, kArrayViewLen),
               ElementsAre(2, 3, 4, 1, 1));
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc, vector->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc, vector->UpdateChecksum());
   EXPECT_THAT(crc, Eq(Crc32(reconstructed_view)));
 }
 
@@ -720,7 +746,9 @@ TEST_F(FileBackedVectorTest, IncrementalCrc_NonOverlappingChanges) {
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
 
   Insert(vector.get(), 0, std::string(num_elements, 'a'));
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(2620640643U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2620640643U)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(2620640643U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2620640643U)));
 
   // Non-overlapping changes to the array, with increasing intervals
   // between updating the checksum. Validate by mapping another array on top.
@@ -730,7 +758,7 @@ TEST_F(FileBackedVectorTest, IncrementalCrc_NonOverlappingChanges) {
 
     if (i >= next_update) {
       ICING_ASSERT_OK_AND_ASSIGN(Crc32 incremental_crc,
-                                 vector->ComputeChecksum());
+                                 vector->UpdateChecksum());
       ICING_LOG(INFO) << "Now crc @" << incremental_crc.Get();
 
       Crc32 full_crc;
@@ -759,7 +787,9 @@ TEST_F(FileBackedVectorTest, IncrementalCrc_OverlappingChanges) {
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
 
   Insert(vector.get(), 0, std::string(num_elements, 'a'));
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(2620640643U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2620640643U)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(2620640643U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(2620640643U)));
 
   // Overlapping changes to the array, with increasing intervals
   // between updating the checksum. Validate by mapping another array on top.
@@ -769,7 +799,7 @@ TEST_F(FileBackedVectorTest, IncrementalCrc_OverlappingChanges) {
 
     if (i >= next_update) {
       ICING_ASSERT_OK_AND_ASSIGN(Crc32 incremental_crc,
-                                 vector->ComputeChecksum());
+                                 vector->UpdateChecksum());
       ICING_LOG(INFO) << "Now crc @" << incremental_crc.Get();
 
       Crc32 full_crc;
@@ -793,7 +823,9 @@ TEST_F(FileBackedVectorTest, SetIntMaxShouldReturnOutOfRangeError) {
       FileBackedVector<int32_t>::Create(
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
 
   // It is an edge case. Since Set() calls GrowIfNecessary(idx + 1), we have to
   // make sure that when idx is INT32_MAX, Set() should handle it correctly.
@@ -823,7 +855,10 @@ TEST_F(FileBackedVectorTest, Grow) {
       FileBackedVector<int32_t>::Create(
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC, max_file_size));
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
+
   // max_num_elements is the allowed max # of elements, so the valid index
   // should be 0 to max_num_elements-1.
   EXPECT_THAT(vector->Set(max_num_elements, 1),
@@ -838,7 +873,9 @@ TEST_F(FileBackedVectorTest, Grow) {
 
   // Crc works?
   const Crc32 good_crc(650981917U);
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(good_crc));
+  EXPECT_THAT(vector->GetChecksum(), Eq(good_crc));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(good_crc));
+  EXPECT_THAT(vector->GetChecksum(), Eq(good_crc));
 
   // PersistToDisk does nothing bad, and ensures the content is still there
   // after we recreate the vector
@@ -919,11 +956,15 @@ TEST_F(FileBackedVectorTest, Delete) {
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
 
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
 
   std::string expected = "abcde";
   Insert(vector.get(), 0, expected);
-  ASSERT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(1134899064U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(1134899064U)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(1134899064U)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(1134899064U)));
   ASSERT_EQ(expected.length(), vector->num_elements());
 
   // Close out the old vector to ensure everything persists properly before we
@@ -947,13 +988,17 @@ TEST_F(FileBackedVectorTest, TruncateTo) {
       FileBackedVector<char>::Create(
           filesystem_, file_path_,
           MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
 
   Insert(vector.get(), 0, "A");
   Insert(vector.get(), 1, "Z");
 
   EXPECT_EQ(2, vector->num_elements());
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(1658635950)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(1658635950)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(1658635950)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(1658635950)));
 
   // Modify 1 element, out of 2 total elements. 1/2 changes exceeds the partial
   // crc limit, so our next checksum call will recompute the entire vector's
@@ -963,12 +1008,16 @@ TEST_F(FileBackedVectorTest, TruncateTo) {
   // checksum will only include "J".
   ICING_EXPECT_OK(vector->TruncateTo(1));
   EXPECT_EQ(1, vector->num_elements());
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(31158534)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(31158534)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(31158534)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(31158534)));
 
   // Truncating clears the checksum and resets it to 0
   ICING_EXPECT_OK(vector->TruncateTo(0));
   EXPECT_EQ(0, vector->num_elements());
-  EXPECT_THAT(vector->ComputeChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
+  EXPECT_THAT(vector->UpdateChecksum(), IsOkAndHolds(Crc32(0)));
+  EXPECT_THAT(vector->GetChecksum(), Eq(Crc32(0)));
 
   // Can't truncate past end.
   EXPECT_THAT(vector->TruncateTo(100),
@@ -1130,7 +1179,7 @@ TEST_F(FileBackedVectorTest, SetDirty) {
   std::string_view reconstructed_view =
       std::string_view(vector->array(), vector->num_elements());
 
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc1, vector->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc1, vector->UpdateChecksum());
   Crc32 full_crc_before_overwrite;
   full_crc_before_overwrite.Append(reconstructed_view);
   EXPECT_THAT(crc1, Eq(full_crc_before_overwrite));
@@ -1147,7 +1196,7 @@ TEST_F(FileBackedVectorTest, SetDirty) {
   ASSERT_THAT(full_crc_before_overwrite, Not(Eq(full_crc_after_overwrite)));
 
   // 3. Without calling SetDirty(), the checksum will be recomputed incorrectly.
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc2, vector->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc2, vector->UpdateChecksum());
   EXPECT_THAT(crc2, Not(Eq(full_crc_after_overwrite)));
 
   // 4. Call SetDirty()
@@ -1156,7 +1205,7 @@ TEST_F(FileBackedVectorTest, SetDirty) {
 
   // 5. The checksum should be computed correctly after calling SetDirty() with
   // correct index.
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc3, vector->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc3, vector->UpdateChecksum());
   EXPECT_THAT(crc3, Eq(full_crc_after_overwrite));
 }
 
diff --git a/icing/file/memory-mapped-file-backed-proto-log-test.cc b/icing/file/memory-mapped-file-backed-proto-log-test.cc
new file mode 100644
index 0000000..13392da
--- /dev/null
+++ b/icing/file/memory-mapped-file-backed-proto-log-test.cc
@@ -0,0 +1,207 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include "icing/file/memory-mapped-file-backed-proto-log.h"
+
+#include <cstdint>
+#include <memory>
+#include <string>
+#include <utility>
+
+#include "icing/text_classifier/lib3/utils/base/status.h"
+#include "gmock/gmock.h"
+#include "gtest/gtest.h"
+#include "icing/document-builder.h"
+#include "icing/file/filesystem.h"
+#include "icing/portable/equals-proto.h"
+#include "icing/proto/document.pb.h"
+#include "icing/testing/common-matchers.h"
+#include "icing/testing/tmp-directory.h"
+#include "icing/util/crc32.h"
+
+namespace icing {
+namespace lib {
+namespace {
+
+using ::icing::lib::portable_equals_proto::EqualsProto;
+using ::testing::HasSubstr;
+using ::testing::StrEq;
+
+static DocumentProto document1 =
+    DocumentBuilder().SetKey("namespace", "uri").Build();
+static DocumentProto document2 =
+    DocumentBuilder().SetKey("namespace2", "uri2").SetScore(10).Build();
+static DocumentProto document3 =
+    DocumentBuilder()
+        .SetKey("namespace3", "uri3")
+        .AddStringProperty("str_property", "a_random_string")
+        .Build();
+
+class MemoryMappedFileBackedProtoLogTest : public ::testing::Test {
+ protected:
+  MemoryMappedFileBackedProtoLogTest() {}
+
+  void SetUp() override {
+    base_dir_ = GetTestTempDir() + "/tmp_dir";
+    ASSERT_TRUE(filesystem_.CreateDirectoryRecursively(base_dir_.c_str()));
+  }
+
+  void TearDown() override {
+    filesystem_.DeleteDirectoryRecursively(base_dir_.c_str());
+  }
+
+  Filesystem filesystem_;
+  std::string base_dir_;
+};
+
+TEST_F(MemoryMappedFileBackedProtoLogTest, WriteAndRead) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      auto mmaped_proto_log,
+      MemoryMappedFileBackedProtoLog<DocumentProto>::Create(
+          filesystem_, base_dir_ + "/proto_log_file"));
+
+  ICING_ASSERT_OK_AND_ASSIGN(int32_t document1_index,
+                             mmaped_proto_log->Write(document1));
+  ICING_ASSERT_OK_AND_ASSIGN(int32_t document2_index,
+                             mmaped_proto_log->Write(document2));
+  ICING_ASSERT_OK_AND_ASSIGN(int32_t document3_index,
+                             mmaped_proto_log->Write(document3));
+
+  ASSERT_THAT(mmaped_proto_log->Read(document1_index),
+              IsOkAndHolds(EqualsProto(document1)));
+  ASSERT_THAT(mmaped_proto_log->Read(document2_index),
+              IsOkAndHolds(EqualsProto(document2)));
+  ASSERT_THAT(mmaped_proto_log->Read(document3_index),
+              IsOkAndHolds(EqualsProto(document3)));
+  mmaped_proto_log->PersistToDisk();
+
+  // Creates a new instance that loads the existing file.
+  ICING_ASSERT_OK_AND_ASSIGN(
+      auto new_mmaped_proto_log,
+      MemoryMappedFileBackedProtoLog<DocumentProto>::Create(
+          filesystem_, base_dir_ + "/proto_log_file"));
+
+  ASSERT_THAT(new_mmaped_proto_log->Read(document1_index),
+              IsOkAndHolds(EqualsProto(document1)));
+  ASSERT_THAT(new_mmaped_proto_log->Read(document2_index),
+              IsOkAndHolds(EqualsProto(document2)));
+  ASSERT_THAT(new_mmaped_proto_log->Read(document3_index),
+              IsOkAndHolds(EqualsProto(document3)));
+}
+
+TEST_F(MemoryMappedFileBackedProtoLogTest, CheckSumTest) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      auto mmaped_proto_log,
+      MemoryMappedFileBackedProtoLog<DocumentProto>::Create(
+          filesystem_, base_dir_ + "/proto_log_file"));
+  Crc32 checksum_before_write = mmaped_proto_log->GetChecksum();
+  ICING_ASSERT_OK(mmaped_proto_log->Write(document1));
+  Crc32 checksum_after_write = mmaped_proto_log->GetChecksum();
+  EXPECT_NE(checksum_before_write, checksum_after_write);
+  EXPECT_THAT(mmaped_proto_log->UpdateChecksum(),
+              IsOkAndHolds(checksum_after_write));
+}
+
+TEST_F(MemoryMappedFileBackedProtoLogTest, ProtoSizeTooLarge) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      auto mmaped_proto_log,
+      MemoryMappedFileBackedProtoLog<DocumentProto>::Create(
+          filesystem_, base_dir_ + "/proto_log_file"));
+
+  // Creates a proto that exceeds the max proto size limit, 16Mb.
+  std::string long_string(17 * 1024 * 1024, 'a');
+  DocumentProto document =
+      DocumentBuilder()
+          .SetKey("namespace", "uri")
+          .AddStringProperty("long_str", std::move(long_string))
+          .Build();
+
+  ASSERT_THAT(mmaped_proto_log->Write(document),
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT,
+                       HasSubstr("Proto data size must be under 16MiB")));
+}
+
+TEST_F(MemoryMappedFileBackedProtoLogTest, Delete) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      auto mmaped_proto_log,
+      MemoryMappedFileBackedProtoLog<DocumentProto>::Create(
+          filesystem_, base_dir_ + "/proto_log_file"));
+
+  std::string file_path = base_dir_ + "/proto_log_file";
+  ICING_ASSERT_OK(mmaped_proto_log->Write(document1));
+  ICING_ASSERT_OK(mmaped_proto_log->Delete(filesystem_, file_path));
+  EXPECT_FALSE(filesystem_.FileExists(file_path.data()));
+}
+
+TEST_F(MemoryMappedFileBackedProtoLogTest, ReadInvalidIndex) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      auto mmaped_proto_log,
+      MemoryMappedFileBackedProtoLog<DocumentProto>::Create(
+          filesystem_, base_dir_ + "/proto_log_file"));
+
+  ICING_ASSERT_OK(mmaped_proto_log->Write(document1));
+  ASSERT_THAT(mmaped_proto_log->Read(/*index=*/-1),
+              StatusIs(libtextclassifier3::StatusCode::OUT_OF_RANGE,
+                       StrEq("Index, -1, is less than 0")));
+  ASSERT_THAT(
+      mmaped_proto_log->Read(/*index=*/16),
+      StatusIs(libtextclassifier3::StatusCode::OUT_OF_RANGE,
+               StrEq("Index, 16, is greater/equal than the upper bound, 16")));
+
+  ASSERT_THAT(
+      mmaped_proto_log->Read(/*index=*/20),
+      StatusIs(libtextclassifier3::StatusCode::OUT_OF_RANGE,
+               StrEq("Index, 20, is greater/equal than the upper bound, 16")));
+
+  ASSERT_THAT(mmaped_proto_log->Read(/*index=*/15),
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT,
+                       StrEq("Proto metadata has invalid magic number")));
+}
+
+TEST_F(MemoryMappedFileBackedProtoLogTest, WriteAndReadWithEmptyProto) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      auto mmaped_proto_log,
+      MemoryMappedFileBackedProtoLog<DocumentProto>::Create(
+          filesystem_, base_dir_ + "/proto_log_file"));
+
+  DocumentProto empty_document;
+  ICING_ASSERT_OK_AND_ASSIGN(int32_t index1,
+                             mmaped_proto_log->Write(empty_document));
+  EXPECT_EQ(index1, 0);
+
+  ICING_ASSERT_OK_AND_ASSIGN(int32_t index2,
+                             mmaped_proto_log->Write(document1));
+  EXPECT_EQ(index2, 4);
+
+  ICING_ASSERT_OK_AND_ASSIGN(int32_t index3,
+                             mmaped_proto_log->Write(empty_document));
+  EXPECT_EQ(index3, 24);
+
+  ICING_ASSERT_OK_AND_ASSIGN(int32_t index4,
+                             mmaped_proto_log->Write(document2));
+  EXPECT_EQ(index4, 28);
+
+  ASSERT_THAT(mmaped_proto_log->Read(index1),
+              IsOkAndHolds(EqualsProto(empty_document)));
+  ASSERT_THAT(mmaped_proto_log->Read(index2),
+              IsOkAndHolds(EqualsProto(document1)));
+  ASSERT_THAT(mmaped_proto_log->Read(index3),
+              IsOkAndHolds(EqualsProto(empty_document)));
+  ASSERT_THAT(mmaped_proto_log->Read(index4),
+              IsOkAndHolds(EqualsProto(document2)));
+}
+
+}  // namespace
+}  // namespace lib
+}  // namespace icing
diff --git a/icing/file/memory-mapped-file-backed-proto-log.h b/icing/file/memory-mapped-file-backed-proto-log.h
new file mode 100644
index 0000000..a5a9d16
--- /dev/null
+++ b/icing/file/memory-mapped-file-backed-proto-log.h
@@ -0,0 +1,242 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef ICING_FILE_MEMORY_MAPPED_FILE_BACKED_PROTO_LOG_H_
+#define ICING_FILE_MEMORY_MAPPED_FILE_BACKED_PROTO_LOG_H_
+
+#include <cstdint>
+#include <cstring>
+#include <memory>
+#include <string>
+#include <utility>
+
+#include "icing/text_classifier/lib3/utils/base/status.h"
+#include "icing/text_classifier/lib3/utils/base/statusor.h"
+#include "icing/absl_ports/canonical_errors.h"
+#include "icing/file/constants.h"
+#include "icing/file/file-backed-vector.h"
+#include "icing/file/filesystem.h"
+#include "icing/file/memory-mapped-file.h"
+#include "icing/legacy/core/icing-string-util.h"
+#include "icing/util/crc32.h"
+#include "icing/util/status-macros.h"
+
+namespace icing {
+namespace lib {
+
+// Memory-mapped-file backed proto log for append-only writes and position based
+// reads.
+//
+// This class is built on top of the FileBackedVector class, which handles the
+// underlying files related operations, such as checksums, flushing to disk.
+//
+// This class is NOT thread-safe.
+template <typename ProtoT>
+class MemoryMappedFileBackedProtoLog {
+ public:
+  // Creates a new MemoryMappedFileBackedProtoLog to read/write content to.
+  //
+  // filesystem: Object to make system level calls
+  // file_path : Specifies the file to persist the log to; must be a path
+  //             within a directory that already exists.
+  //
+  // Return:
+  //   FAILED_PRECONDITION_ERROR if the file checksum doesn't match the stored
+  //                             checksum.
+  //   INTERNAL_ERROR on I/O errors.
+  static libtextclassifier3::StatusOr<
+      std::unique_ptr<MemoryMappedFileBackedProtoLog<ProtoT>>>
+  Create(const Filesystem& filesystem, const std::string& file_path);
+
+  // Deletes the underlying file.
+  static libtextclassifier3::Status Delete(const Filesystem& filesystem,
+                                           const std::string& file_path);
+
+  // Delete copy constructor and assignment operator.
+  MemoryMappedFileBackedProtoLog(const MemoryMappedFileBackedProtoLog&) =
+      delete;
+  MemoryMappedFileBackedProtoLog& operator=(
+      const MemoryMappedFileBackedProtoLog&) = delete;
+
+  // Calculates the checksum of the log contents and returns it. Does NOT
+  // update the header.
+  //
+  // Returns:
+  //   Checksum of the log contents.
+  Crc32 GetChecksum() const;
+
+  // Calculates the checksum of the log contents and updates the header to
+  // hold this updated value.
+  //
+  // Returns:
+  //   Checksum on success
+  //   INTERNAL_ERROR on IO error
+  libtextclassifier3::StatusOr<Crc32> UpdateChecksum();
+
+  // Reads the proto at the given index.
+  //
+  // Returns:
+  //   proto on success
+  //   INTERNAL_ERROR if the index points to an invalid position.
+  //   OUT_OF_RANGE_ERROR if:
+  //     - index < 0 or index >= num_elements - sizeof(ProtoMetadata)
+  libtextclassifier3::StatusOr<ProtoT> Read(int32_t index) const;
+
+  // Appends the proto to the end of the log.
+  //
+  // Returns:
+  //   Index of the newly appended proto, on success.
+  //   INVALID_ARGUMENT if the proto size exceeds the max size limit, 16MiB.
+  libtextclassifier3::StatusOr<int32_t> Write(const ProtoT& proto);
+
+  // Flushes content to underlying file.
+  //
+  // Returns:
+  //   OK on success
+  //   INTERNAL_ERROR on I/O errors
+  libtextclassifier3::Status PersistToDisk();
+
+ private:
+  // The metadata of the proto, it contains 4 bytes, with the most significant
+  // byte being the magic number, and remaining three bytes being the proto
+  // size.
+  // It is stored in front of every proto.
+  using ProtoMetadata = int32_t;
+
+  // Magic number encoded in the most significant byte of the proto metadata.
+  static constexpr uint8_t kProtoMagic = 0x55;
+
+  // Validates the proto metadata and extracts the proto size from it.
+  //
+  // Returns:
+  //       INTERNAL_ERROR if the magic number stored in the metadata is
+  //       invalid.
+  static libtextclassifier3::StatusOr<int32_t> ValidateAndGetProtoSize(
+      ProtoMetadata proto_metadata);
+
+  explicit MemoryMappedFileBackedProtoLog(
+      std::unique_ptr<FileBackedVector<uint8_t>> proto_fbv);
+
+  std::unique_ptr<FileBackedVector<uint8_t>> proto_fbv_;
+};
+
+template <typename ProtoT>
+MemoryMappedFileBackedProtoLog<ProtoT>::MemoryMappedFileBackedProtoLog(
+    std::unique_ptr<FileBackedVector<uint8_t>> proto_fbv)
+    : proto_fbv_(std::move(proto_fbv)) {}
+
+template <typename ProtoT>
+libtextclassifier3::StatusOr<int32_t>
+MemoryMappedFileBackedProtoLog<ProtoT>::ValidateAndGetProtoSize(
+    ProtoMetadata proto_metadata) {
+  uint8_t magic_number = proto_metadata >> 24;
+  if (magic_number != kProtoMagic) {
+    return absl_ports::InvalidArgumentError(
+        "Proto metadata has invalid magic number");
+  }
+  return proto_metadata & 0x00FFFFFF;
+}
+
+template <typename ProtoT>
+Crc32 MemoryMappedFileBackedProtoLog<ProtoT>::GetChecksum() const {
+  return proto_fbv_->GetChecksum();
+}
+
+template <typename ProtoT>
+libtextclassifier3::StatusOr<Crc32>
+MemoryMappedFileBackedProtoLog<ProtoT>::UpdateChecksum() {
+  return proto_fbv_->UpdateChecksum();
+}
+
+template <typename ProtoT>
+libtextclassifier3::StatusOr<
+    std::unique_ptr<MemoryMappedFileBackedProtoLog<ProtoT>>>
+MemoryMappedFileBackedProtoLog<ProtoT>::Create(const Filesystem& filesystem,
+                                               const std::string& file_path) {
+  ICING_ASSIGN_OR_RETURN(std::unique_ptr<FileBackedVector<uint8_t>> proto_fbv,
+                         FileBackedVector<uint8_t>::Create(
+                             filesystem, file_path,
+                             MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
+
+  return std::unique_ptr<MemoryMappedFileBackedProtoLog<ProtoT>>(
+      new MemoryMappedFileBackedProtoLog<ProtoT>(std::move(proto_fbv)));
+}
+
+template <typename ProtoT>
+libtextclassifier3::Status MemoryMappedFileBackedProtoLog<ProtoT>::Delete(
+    const Filesystem& filesystem, const std::string& file_path) {
+  return FileBackedVector<uint8_t>::Delete(filesystem, file_path);
+}
+
+template <typename ProtoT>
+libtextclassifier3::StatusOr<ProtoT>
+MemoryMappedFileBackedProtoLog<ProtoT>::Read(int32_t index) const {
+  if (index < 0) {
+    return absl_ports::OutOfRangeError(
+        IcingStringUtil::StringPrintf("Index, %d, is less than 0", index));
+  }
+  if (index + sizeof(ProtoMetadata) >= proto_fbv_->num_elements()) {
+    return absl_ports::OutOfRangeError(IcingStringUtil::StringPrintf(
+        "Index, %d, is greater/equal than the upper bound, %d", index,
+        proto_fbv_->num_elements() - sizeof(ProtoMetadata)));
+  }
+
+  ProtoMetadata proto_metadata;
+  std::memcpy(&proto_metadata, proto_fbv_->array() + index,
+              sizeof(ProtoMetadata));
+
+  ICING_ASSIGN_OR_RETURN(int32_t proto_size,
+                         ValidateAndGetProtoSize(proto_metadata));
+  ProtoT proto_data;
+  if (!proto_data.ParseFromArray(
+          proto_fbv_->array() + index + sizeof(ProtoMetadata), proto_size)) {
+    return absl_ports::InternalError(
+        "Failed to parse proto from MemoryMappedFileBackedProtoLog");
+  }
+  return proto_data;
+}
+
+template <typename ProtoT>
+libtextclassifier3::StatusOr<int32_t>
+MemoryMappedFileBackedProtoLog<ProtoT>::Write(const ProtoT& proto) {
+  int32_t proto_byte_size = proto.ByteSizeLong();
+  if (proto_byte_size > constants::kMaxProtoSize) {
+    return absl_ports::InvalidArgumentError(IcingStringUtil::StringPrintf(
+        "Proto data size must be under 16MiB, was %d", proto_byte_size));
+  }
+
+  int32_t index_of_new_proto = proto_fbv_->num_elements();
+  ICING_ASSIGN_OR_RETURN(
+      FileBackedVector<uint8_t>::MutableArrayView mutable_array_view,
+      proto_fbv_->Allocate(sizeof(ProtoMetadata) + proto_byte_size));
+
+  ProtoMetadata proto_metadata = (kProtoMagic << 24) | proto_byte_size;
+  uint8_t* byte_ptr = reinterpret_cast<uint8_t*>(&proto_metadata);
+  mutable_array_view.SetArray(/*idx=*/0, byte_ptr, sizeof(ProtoMetadata));
+  proto.SerializeWithCachedSizesToArray(
+      &mutable_array_view[sizeof(ProtoMetadata)]);
+
+  return index_of_new_proto;
+}
+
+template <typename ProtoT>
+libtextclassifier3::Status
+MemoryMappedFileBackedProtoLog<ProtoT>::PersistToDisk() {
+  return proto_fbv_->PersistToDisk();
+}
+
+}  // namespace lib
+}  // namespace icing
+
+#endif  // ICING_FILE_MEMORY_MAPPED_FILE_BACKED_PROTO_LOG_H_
diff --git a/icing/file/persistent-hash-map.cc b/icing/file/persistent-hash-map.cc
index 6936c45..148c7ea 100644
--- a/icing/file/persistent-hash-map.cc
+++ b/icing/file/persistent-hash-map.cc
@@ -413,6 +413,7 @@ PersistentHashMap::InitializeNewFiles(const Filesystem& filesystem,
       new_persistent_hash_map->options_.max_load_factor_percent;
   info_ref.num_deleted_entries = 0;
   info_ref.num_deleted_key_value_bytes = 0;
+
   // Initialize new PersistentStorage. The initial checksums will be computed
   // and set via InitializeNewStorage.
   ICING_RETURN_IF_ERROR(new_persistent_hash_map->InitializeNewStorage());
@@ -479,6 +480,7 @@ PersistentHashMap::InitializeExistingFiles(const Filesystem& filesystem,
           filesystem, std::move(working_path), std::move(options),
           std::move(metadata_mmapped_file), std::move(bucket_storage),
           std::move(entry_storage), std::move(kv_storage)));
+
   // Initialize existing PersistentStorage. Checksums will be validated.
   ICING_RETURN_IF_ERROR(persistent_hash_map->InitializeExistingStorage());
 
@@ -532,9 +534,8 @@ PersistentHashMap::InitializeExistingFiles(const Filesystem& filesystem,
   return persistent_hash_map;
 }
 
-libtextclassifier3::Status PersistentHashMap::PersistStoragesToDisk(
-    bool force) {
-  if (!force && !is_storage_dirty()) {
+libtextclassifier3::Status PersistentHashMap::PersistStoragesToDisk() {
+  if (is_initialized_ && !is_storage_dirty()) {
     return libtextclassifier3::Status::OK;
   }
 
@@ -545,11 +546,10 @@ libtextclassifier3::Status PersistentHashMap::PersistStoragesToDisk(
   return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::Status PersistentHashMap::PersistMetadataToDisk(
-    bool force) {
+libtextclassifier3::Status PersistentHashMap::PersistMetadataToDisk() {
   // We can skip persisting metadata to disk only if both info and storage are
   // clean.
-  if (!force && !is_info_dirty() && !is_storage_dirty()) {
+  if (is_initialized_ && !is_info_dirty() && !is_storage_dirty()) {
     return libtextclassifier3::Status::OK;
   }
 
@@ -561,28 +561,39 @@ libtextclassifier3::Status PersistentHashMap::PersistMetadataToDisk(
   return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::StatusOr<Crc32> PersistentHashMap::ComputeInfoChecksum(
-    bool force) {
-  if (!force && !is_info_dirty()) {
-    return Crc32(crcs().component_crcs.info_crc);
+libtextclassifier3::StatusOr<Crc32>
+PersistentHashMap::UpdateStoragesChecksum() {
+  if (is_initialized_ && !is_storage_dirty()) {
+    return Crc32(crcs().component_crcs.storages_crc);
   }
 
-  return info().ComputeChecksum();
+  // Compute crcs
+  ICING_ASSIGN_OR_RETURN(Crc32 bucket_storage_crc,
+                         bucket_storage_->UpdateChecksum());
+  ICING_ASSIGN_OR_RETURN(Crc32 entry_storage_crc,
+                         entry_storage_->UpdateChecksum());
+  ICING_ASSIGN_OR_RETURN(Crc32 kv_storage_crc, kv_storage_->UpdateChecksum());
+  return Crc32(bucket_storage_crc.Get() ^ entry_storage_crc.Get() ^
+               kv_storage_crc.Get());
 }
 
-libtextclassifier3::StatusOr<Crc32> PersistentHashMap::ComputeStoragesChecksum(
-    bool force) {
-  if (!force && !is_storage_dirty()) {
+libtextclassifier3::StatusOr<Crc32> PersistentHashMap::GetInfoChecksum() const {
+  if (is_initialized_ && !is_info_dirty()) {
+    return Crc32(crcs().component_crcs.info_crc);
+  }
+  return info().GetChecksum();
+}
+
+libtextclassifier3::StatusOr<Crc32> PersistentHashMap::GetStoragesChecksum()
+    const {
+  if (is_initialized_ && !is_storage_dirty()) {
     return Crc32(crcs().component_crcs.storages_crc);
   }
 
   // Compute crcs
-  ICING_ASSIGN_OR_RETURN(Crc32 bucket_storage_crc,
-                         bucket_storage_->ComputeChecksum());
-  ICING_ASSIGN_OR_RETURN(Crc32 entry_storage_crc,
-                         entry_storage_->ComputeChecksum());
-  ICING_ASSIGN_OR_RETURN(Crc32 kv_storage_crc, kv_storage_->ComputeChecksum());
-
+  Crc32 bucket_storage_crc = bucket_storage_->GetChecksum();
+  Crc32 entry_storage_crc = entry_storage_->GetChecksum();
+  Crc32 kv_storage_crc = kv_storage_->GetChecksum();
   return Crc32(bucket_storage_crc.Get() ^ entry_storage_crc.Get() ^
                kv_storage_crc.Get());
 }
diff --git a/icing/file/persistent-hash-map.h b/icing/file/persistent-hash-map.h
index 5f7999d..3a687fc 100644
--- a/icing/file/persistent-hash-map.h
+++ b/icing/file/persistent-hash-map.h
@@ -95,7 +95,7 @@ class PersistentHashMap : public PersistentStorage {
     int32_t num_deleted_entries;
     int32_t num_deleted_key_value_bytes;
 
-    Crc32 ComputeChecksum() const {
+    Crc32 GetChecksum() const {
       return Crc32(
           std::string_view(reinterpret_cast<const char*>(this), sizeof(Info)));
     }
@@ -406,34 +406,21 @@ class PersistentHashMap : public PersistentStorage {
   InitializeExistingFiles(const Filesystem& filesystem,
                           std::string&& working_path, Options&& options);
 
-  // Flushes contents of all storages to underlying files.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
-  libtextclassifier3::Status PersistStoragesToDisk(bool force) override;
+  libtextclassifier3::Status PersistStoragesToDisk() override;
 
-  // Flushes contents of metadata file.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
-  libtextclassifier3::Status PersistMetadataToDisk(bool force) override;
+  libtextclassifier3::Status PersistMetadataToDisk() override;
 
-  // Computes and returns Info checksum.
-  //
-  // Returns:
-  //   - Crc of the Info on success
-  libtextclassifier3::StatusOr<Crc32> ComputeInfoChecksum(bool force) override;
+  libtextclassifier3::Status WriteMetadata() override {
+    // PersistentHashMap::Header is mmapped. Therefore, writes occur when the
+    // metadata is modified. So just return OK.
+    return libtextclassifier3::Status::OK;
+  }
 
-  // Computes and returns all storages checksum. Checksums of bucket_storage_,
-  // entry_storage_ and kv_storage_ will be combined together by XOR.
-  //
-  // Returns:
-  //   - Crc of all storages on success
-  //   - INTERNAL_ERROR if any data inconsistency
-  libtextclassifier3::StatusOr<Crc32> ComputeStoragesChecksum(
-      bool force) override;
+  libtextclassifier3::StatusOr<Crc32> UpdateStoragesChecksum() override;
+
+  libtextclassifier3::StatusOr<Crc32> GetInfoChecksum() const override;
+
+  libtextclassifier3::StatusOr<Crc32> GetStoragesChecksum() const override;
 
   // Find the index of the target entry (that contains the key) from a bucket
   // (specified by bucket index). Also return the previous entry index, since
@@ -500,10 +487,11 @@ class PersistentHashMap : public PersistentStorage {
   }
 
   void SetInfoDirty() { is_info_dirty_ = true; }
-  // When storage is dirty, we have to set info dirty as well. So just expose
-  // SetDirty to set both.
+
+  // When the storage is dirty, then the checksum in the info is invalid and
+  // must be recalculated. Therefore, also mark the info as dirty.
   void SetDirty() {
-    is_info_dirty_ = true;
+    SetInfoDirty();
     is_storage_dirty_ = true;
   }
 
diff --git a/icing/file/persistent-hash-map_test.cc b/icing/file/persistent-hash-map_test.cc
index 5535629..d2e211d 100644
--- a/icing/file/persistent-hash-map_test.cc
+++ b/icing/file/persistent-hash-map_test.cc
@@ -359,6 +359,42 @@ TEST_P(PersistentHashMapTest,
               StatusIs(libtextclassifier3::StatusCode::FAILED_PRECONDITION));
 }
 
+TEST_P(PersistentHashMapTest, InitializationShouldSucceedWithUpdateChecksums) {
+  Options options(/*value_type_size_in=*/sizeof(int));
+  options.pre_mapping_fbv = GetParam();
+
+  // Create new persistent hash map
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<PersistentHashMap> persistent_hash_map1,
+      PersistentHashMap::Create(filesystem_, working_path_, options));
+
+  // Put some key value pairs.
+  ICING_ASSERT_OK(persistent_hash_map1->Put("a", Serialize(1).data()));
+  ICING_ASSERT_OK(persistent_hash_map1->Put("b", Serialize(2).data()));
+  ICING_ASSERT_OK(persistent_hash_map1->Put("c", Serialize(3).data()));
+  // Call Delete() to change PersistentHashMap metadata info
+  // (num_deleted_entries)
+  ICING_ASSERT_OK(persistent_hash_map1->Delete("c"));
+
+  ASSERT_THAT(persistent_hash_map1, Pointee(SizeIs(2)));
+  ASSERT_THAT(GetValueByKey(persistent_hash_map1.get(), "a"), IsOkAndHolds(1));
+  ASSERT_THAT(GetValueByKey(persistent_hash_map1.get(), "b"), IsOkAndHolds(2));
+
+  // After calling UpdateChecksums, all checksums should be recomputed, so
+  // initializing another instance on the same files should succeed, and we
+  // should be able to get the same contents.
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc, persistent_hash_map1->GetChecksum());
+  EXPECT_THAT(persistent_hash_map1->UpdateChecksums(), IsOkAndHolds(Eq(crc)));
+  EXPECT_THAT(persistent_hash_map1->GetChecksum(), IsOkAndHolds(Eq(crc)));
+
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<PersistentHashMap> persistent_hash_map2,
+      PersistentHashMap::Create(filesystem_, working_path_, options));
+  EXPECT_THAT(persistent_hash_map2, Pointee(SizeIs(2)));
+  EXPECT_THAT(GetValueByKey(persistent_hash_map2.get(), "a"), IsOkAndHolds(1));
+  EXPECT_THAT(GetValueByKey(persistent_hash_map2.get(), "b"), IsOkAndHolds(2));
+}
+
 TEST_P(PersistentHashMapTest, InitializationShouldSucceedWithPersistToDisk) {
   Options options(/*value_type_size_in=*/sizeof(int));
   options.pre_mapping_fbv = GetParam();
@@ -460,8 +496,8 @@ TEST_P(PersistentHashMapTest,
 
     // Manually change magic and update checksums.
     info.magic += kCorruptedValueOffset;
-    crcs.component_crcs.info_crc = info.ComputeChecksum().Get();
-    crcs.all_crc = crcs.component_crcs.ComputeChecksum().Get();
+    crcs.component_crcs.info_crc = info.GetChecksum().Get();
+    crcs.all_crc = crcs.component_crcs.GetChecksum().Get();
     ASSERT_TRUE(filesystem_.PWrite(metadata_sfd.get(),
                                    PersistentHashMap::kCrcsMetadataFileOffset,
                                    &crcs, sizeof(Crcs)));
@@ -666,12 +702,10 @@ TEST_P(PersistentHashMapTest,
         FileBackedVector<Bucket>::Create(
             filesystem_, bucket_storage_file_path,
             MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
-    ICING_ASSERT_OK_AND_ASSIGN(Crc32 old_crc,
-                               bucket_storage->ComputeChecksum());
+    ICING_ASSERT_OK_AND_ASSIGN(Crc32 old_crc, bucket_storage->UpdateChecksum());
     ICING_ASSERT_OK(bucket_storage->Append(Bucket()));
     ICING_ASSERT_OK(bucket_storage->PersistToDisk());
-    ICING_ASSERT_OK_AND_ASSIGN(Crc32 new_crc,
-                               bucket_storage->ComputeChecksum());
+    ICING_ASSERT_OK_AND_ASSIGN(Crc32 new_crc, bucket_storage->UpdateChecksum());
     ASSERT_THAT(old_crc, Not(Eq(new_crc)));
   }
 
@@ -712,11 +746,11 @@ TEST_P(PersistentHashMapTest,
         FileBackedVector<Entry>::Create(
             filesystem_, entry_storage_file_path,
             MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
-    ICING_ASSERT_OK_AND_ASSIGN(Crc32 old_crc, entry_storage->ComputeChecksum());
+    ICING_ASSERT_OK_AND_ASSIGN(Crc32 old_crc, entry_storage->UpdateChecksum());
     ICING_ASSERT_OK(entry_storage->Append(
         Entry(/*key_value_index=*/-1, /*next_entry_index=*/-1)));
     ICING_ASSERT_OK(entry_storage->PersistToDisk());
-    ICING_ASSERT_OK_AND_ASSIGN(Crc32 new_crc, entry_storage->ComputeChecksum());
+    ICING_ASSERT_OK_AND_ASSIGN(Crc32 new_crc, entry_storage->UpdateChecksum());
     ASSERT_THAT(old_crc, Not(Eq(new_crc)));
   }
 
@@ -757,10 +791,10 @@ TEST_P(PersistentHashMapTest,
         FileBackedVector<char>::Create(
             filesystem_, kv_storage_file_path,
             MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
-    ICING_ASSERT_OK_AND_ASSIGN(Crc32 old_crc, kv_storage->ComputeChecksum());
+    ICING_ASSERT_OK_AND_ASSIGN(Crc32 old_crc, kv_storage->UpdateChecksum());
     ICING_ASSERT_OK(kv_storage->Append('z'));
     ICING_ASSERT_OK(kv_storage->PersistToDisk());
-    ICING_ASSERT_OK_AND_ASSIGN(Crc32 new_crc, kv_storage->ComputeChecksum());
+    ICING_ASSERT_OK_AND_ASSIGN(Crc32 new_crc, kv_storage->UpdateChecksum());
     ASSERT_THAT(old_crc, Not(Eq(new_crc)));
   }
 
diff --git a/icing/file/persistent-storage.h b/icing/file/persistent-storage.h
index 9cb5e4d..d778a84 100644
--- a/icing/file/persistent-storage.h
+++ b/icing/file/persistent-storage.h
@@ -72,10 +72,13 @@ namespace lib {
 //       - If the derived class uses memory-mapped region directly for metadata,
 //         then it should call MemoryMappedFile::PersistToDisk.
 //       - See crcs() for more details.
-//     - ComputeInfoChecksum: compute the checksum for custom Info.
-//     - ComputeStoragesChecksum: compute the (combined) checksum for all
+//     - GetInfoChecksum: compute the checksum for custom Info.
+//     - GetStoragesChecksum: compute the (combined) checksum for all
 //       (composite) storages. In general, the implementation will be calling
-//       UpdateChecksums for all composite storages and XOR all checksums.
+//       GetChecksum for all composite storages and XOR all checksums.
+//     - UpdateStoragesChecksum: update the (combined) checksum for all
+//       (composite) storages. In general, the implementation will be calling
+//       UpdateChecksum for all composite storages and XOR all checksums.
 //     - crcs(): provide the reference for PersistentStorage to write checksums.
 //       The derived class can either maintain a concrete Crcs instance, or
 //       reinterpret_cast the memory-mapped region to Crcs reference. Either
@@ -104,7 +107,7 @@ class PersistentStorage {
         return info_crc == other.info_crc && storages_crc == other.storages_crc;
       }
 
-      Crc32 ComputeChecksum() const {
+      Crc32 GetChecksum() const {
         return Crc32(std::string_view(reinterpret_cast<const char*>(this),
                                       sizeof(ComponentCrcs)));
       }
@@ -141,16 +144,16 @@ class PersistentStorage {
   //
   // Returns:
   //   - OK on success or already initialized
-  //   - Any errors from ComputeInfoChecksum, ComputeStoragesChecksum, depending
-  //     on actual implementation
+  //   - Any errors from GetInfoChecksum, UpdateStoragesChecksum, depending on
+  //     actual implementation
   libtextclassifier3::Status InitializeNewStorage() {
     if (is_initialized_) {
       return libtextclassifier3::Status::OK;
     }
 
-    ICING_RETURN_IF_ERROR(UpdateChecksumsInternal(/*force=*/true));
-    ICING_RETURN_IF_ERROR(PersistStoragesToDisk(/*force=*/true));
-    ICING_RETURN_IF_ERROR(PersistMetadataToDisk(/*force=*/true));
+    ICING_RETURN_IF_ERROR(UpdateChecksumsInternal());
+    ICING_RETURN_IF_ERROR(PersistStoragesToDisk());
+    ICING_RETURN_IF_ERROR(PersistMetadataToDisk());
 
     is_initialized_ = true;
     return libtextclassifier3::Status::OK;
@@ -167,8 +170,8 @@ class PersistentStorage {
   // Returns:
   //   - OK on success or already initialized
   //   - FAILED_PRECONDITION_ERROR if checksum validation fails.
-  //   - Any errors from ComputeInfoChecksum, ComputeStoragesChecksum, depending
-  //     on actual implementation
+  //   - Any errors from GetInfoChecksum, GetStoragesChecksum, depending on
+  //     actual implementation
   libtextclassifier3::Status InitializeExistingStorage() {
     if (is_initialized_) {
       return libtextclassifier3::Status::OK;
@@ -185,52 +188,59 @@ class PersistentStorage {
   // 2) Updates all checksums by new data.
   // 3) Flushes metadata.
   //
-  // Force flag will be passed down to PersistMetadataToDisk,
-  // PersistStoragesToDisk, ComputeInfoChecksum, ComputeStoragesChecksum.
-  // - If force == true, then performs actual persisting operations/recomputes
-  //   the checksum.
-  // - Otherwise, the derived class can decide itself whether skipping
-  //   persisting operations/doing lazy checksum recomputing if the storage is
-  //   not dirty.
-  //
   // Returns:
   //   - OK on success
   //   - FAILED_PRECONDITION_ERROR if PersistentStorage is uninitialized
   //   - Any errors from PersistStoragesToDisk, UpdateChecksums,
   //     PersistMetadataToDisk, depending on actual implementation
-  libtextclassifier3::Status PersistToDisk(bool force = false) {
+  libtextclassifier3::Status PersistToDisk() {
     if (!is_initialized_) {
       return absl_ports::FailedPreconditionError(absl_ports::StrCat(
           "PersistentStorage ", working_path_, " not initialized"));
     }
 
-    ICING_RETURN_IF_ERROR(UpdateChecksumsInternal(force));
-    ICING_RETURN_IF_ERROR(PersistStoragesToDisk(force));
-    ICING_RETURN_IF_ERROR(PersistMetadataToDisk(force));
+    ICING_RETURN_IF_ERROR(UpdateChecksumsInternal());
+    ICING_RETURN_IF_ERROR(PersistStoragesToDisk());
+    ICING_RETURN_IF_ERROR(PersistMetadataToDisk());
     return libtextclassifier3::Status::OK;
   }
 
   // Updates checksums of all components and returns the overall crc (all_crc)
   // of the persistent storage.
   //
-  // Force flag will be passed down ComputeInfoChecksum,
-  // ComputeStoragesChecksum.
-  // - If force == true, then recomputes the checksum.
-  // - Otherwise, the derived class can decide itself whether doing lazy
-  //   checksum recomputing if the storage is not dirty.
-  //
   // Returns:
   //   - Overall crc of the persistent storage on success
   //   - FAILED_PRECONDITION_ERROR if PersistentStorage is uninitialized
-  //   - Any errors from ComputeInfoChecksum, ComputeStoragesChecksum, depending
+  //   - Any errors from UpdateInfoChecksum, UpdateStoragesChecksum, depending
   //     on actual implementation
-  libtextclassifier3::StatusOr<Crc32> UpdateChecksums(bool force = false) {
+  libtextclassifier3::StatusOr<Crc32> UpdateChecksums() {
+    if (!is_initialized_) {
+      return absl_ports::FailedPreconditionError(absl_ports::StrCat(
+          "PersistentStorage ", working_path_, " not initialized"));
+    }
+
+    ICING_ASSIGN_OR_RETURN(Crc32 crc, UpdateChecksumsInternal());
+    ICING_RETURN_IF_ERROR(WriteMetadata());
+    return crc;
+  }
+
+  // Calculates and returns the overall crc (all_crc) of the persistent storage.
+  //
+  // Returns:
+  //   - Overall crc of the persistent storage on success
+  //   - FAILED_PRECONDITION_ERROR if PersistentStorage is uninitialized
+  //   - Any errors from GetInfoChecksum, GetStoragesChecksum, depending on
+  //     actual implementation
+  libtextclassifier3::StatusOr<Crc32> GetChecksum() const {
     if (!is_initialized_) {
       return absl_ports::FailedPreconditionError(absl_ports::StrCat(
           "PersistentStorage ", working_path_, " not initialized"));
     }
 
-    return UpdateChecksumsInternal(force);
+    ICING_ASSIGN_OR_RETURN(Crc32 info_crc, GetInfoChecksum());
+    ICING_ASSIGN_OR_RETURN(Crc32 storages_crc, GetStoragesChecksum());
+    Crcs::ComponentCrcs crcs = {info_crc.Get(), storages_crc.Get()};
+    return crcs.GetChecksum();
   }
 
  protected:
@@ -246,44 +256,66 @@ class PersistentStorage {
   // correctly, depending on whether they're using memory-mapped regions or
   // concrete instances in the derived class.
   //
+  // It is valid to call this function even when is_initialized_ is false.
+  //
   // Returns:
   //   - OK on success
   //   - Any other errors, depending on actual implementation
-  virtual libtextclassifier3::Status PersistMetadataToDisk(bool force) = 0;
+  virtual libtextclassifier3::Status PersistMetadataToDisk() = 0;
 
   // Flushes contents of all storages to underlying files.
   //
+  // It is valid to call this function even when is_initialized_ is false.
+  //
   // Returns:
   //   - OK on success
   //   - Any other errors, depending on actual implementation
-  virtual libtextclassifier3::Status PersistStoragesToDisk(bool force) = 0;
+  virtual libtextclassifier3::Status PersistStoragesToDisk() = 0;
 
-  // Computes and returns Info checksum.
-  // - If force = true, then recompute the entire checksum.
-  // - Otherwise, the derived class can decide itself whether doing lazy
-  //   checksum computing if the storage is not dirty.
+  // Writes the contents of the metadata, if necessary. Unlike
+  // PersistMetadataToDisk this method does not explicitly flush the metadata to
+  // disk.
+  //
+  // Returns:
+  //   - OK on success
+  //   - Any other errors, depending on actual implementation
+  virtual libtextclassifier3::Status WriteMetadata() = 0;
+
+  // Computes and updates all storages checksums and returns a combined checksum
+  // of all storages. If there are multiple storages, usually we XOR their
+  // checksums together to a single checksum.
   //
   // This function will be mainly called by UpdateChecksums.
   //
+  // It is valid to call this function even when is_initialized_ is false.
+  //
+  // Returns:
+  //   - Crc of all storages on success
+  //   - Any other errors from depending on actual implementation
+  virtual libtextclassifier3::StatusOr<Crc32> UpdateStoragesChecksum() = 0;
+
+  // Computes and returns Info checksum.
+  //
+  // This function will be mainly called by GetChecksum.
+  //
+  // It is valid to call this function even when is_initialized_ is false.
+  //
   // Returns:
   //   - Crc of the Info on success
   //   - Any other errors, depending on actual implementation
-  virtual libtextclassifier3::StatusOr<Crc32> ComputeInfoChecksum(
-      bool force) = 0;
+  virtual libtextclassifier3::StatusOr<Crc32> GetInfoChecksum() const = 0;
 
   // Computes and returns all storages checksum. If there are multiple storages,
   // usually we XOR their checksums together to a single checksum.
-  // - If force = true, then recompute the entire checksum.
-  // - Otherwise, the derived class can decide itself whether doing lazy
-  //   checksum computing if the storage is not dirty.
   //
-  // This function will be mainly called by UpdateChecksums.
+  // This function will be mainly called by GetChecksum.
+  //
+  // It is valid to call this function even when is_initialized_ is false.
   //
   // Returns:
   //   - Crc of all storages on success
-  //   - Any other errors from depending on actual implementation
-  virtual libtextclassifier3::StatusOr<Crc32> ComputeStoragesChecksum(
-      bool force) = 0;
+  //   - Any other errors, depending on actual implementation
+  virtual libtextclassifier3::StatusOr<Crc32> GetStoragesChecksum() const = 0;
 
   // Returns the Crcs instance reference. The derived class can either own a
   // concrete Crcs instance, or reinterpret_cast the memory-mapped region to
@@ -313,13 +345,13 @@ class PersistentStorage {
   //
   // Returns:
   //   - Overall crc of the persistent storage on success
-  //   - Any errors from ComputeInfoChecksum, ComputeStoragesChecksum, depending
-  //     on actual implementation
-  libtextclassifier3::StatusOr<Crc32> UpdateChecksumsInternal(bool force) {
+  //   - Any errors from GetInfoChecksum, UpdateStoragesChecksum, depending on
+  //     actual implementation
+  libtextclassifier3::StatusOr<Crc32> UpdateChecksumsInternal() {
     Crcs& crcs_ref = crcs();
     // Compute and update storages + info checksums.
-    ICING_ASSIGN_OR_RETURN(Crc32 info_crc, ComputeInfoChecksum(force));
-    ICING_ASSIGN_OR_RETURN(Crc32 storages_crc, ComputeStoragesChecksum(force));
+    ICING_ASSIGN_OR_RETURN(Crc32 info_crc, GetInfoChecksum());
+    ICING_ASSIGN_OR_RETURN(Crc32 storages_crc, UpdateStoragesChecksum());
     if (crcs_ref.component_crcs.info_crc == info_crc.Get() &&
         crcs_ref.component_crcs.storages_crc == storages_crc.Get()) {
       // If info and storages crc haven't changed, then we don't have to update
@@ -331,8 +363,9 @@ class PersistentStorage {
     crcs_ref.component_crcs.storages_crc = storages_crc.Get();
 
     // Finally compute and update overall checksum.
-    crcs_ref.all_crc = crcs_ref.component_crcs.ComputeChecksum().Get();
-    return Crc32(crcs_ref.all_crc);
+    Crc32 all_crc = crcs_ref.component_crcs.GetChecksum();
+    crcs_ref.all_crc = all_crc.Get();
+    return all_crc;
   }
 
   // Validates all checksums of the persistent storage.
@@ -340,25 +373,23 @@ class PersistentStorage {
   // Returns:
   //   - OK on success
   //   - FAILED_PRECONDITION_ERROR if any checksum is incorrect.
-  //   - Any errors from ComputeInfoChecksum, ComputeStoragesChecksum, depending
-  //     on actual implementation
-  libtextclassifier3::Status ValidateChecksums() {
+  //   - Any errors from GetInfoChecksum, GetStoragesChecksum, depending on
+  //     actual implementation
+  libtextclassifier3::Status ValidateChecksums() const {
     const Crcs& crcs_ref = crcs();
-    if (crcs_ref.all_crc != crcs_ref.component_crcs.ComputeChecksum().Get()) {
+    if (crcs_ref.all_crc != crcs_ref.component_crcs.GetChecksum().Get()) {
       return absl_ports::FailedPreconditionError("Invalid all crc");
     }
 
-    ICING_ASSIGN_OR_RETURN(Crc32 info_crc, ComputeInfoChecksum(/*force=*/true));
+    ICING_ASSIGN_OR_RETURN(Crc32 info_crc, GetInfoChecksum());
     if (crcs_ref.component_crcs.info_crc != info_crc.Get()) {
       return absl_ports::FailedPreconditionError("Invalid info crc");
     }
 
-    ICING_ASSIGN_OR_RETURN(Crc32 storages_crc,
-                           ComputeStoragesChecksum(/*force=*/true));
+    ICING_ASSIGN_OR_RETURN(Crc32 storages_crc, GetStoragesChecksum());
     if (crcs_ref.component_crcs.storages_crc != storages_crc.Get()) {
       return absl_ports::FailedPreconditionError("Invalid storages crc");
     }
-
     return libtextclassifier3::Status::OK;
   }
 };
diff --git a/icing/file/portable-file-backed-proto-log.h b/icing/file/portable-file-backed-proto-log.h
index a36bd9e..faccc6e 100644
--- a/icing/file/portable-file-backed-proto-log.h
+++ b/icing/file/portable-file-backed-proto-log.h
@@ -66,6 +66,7 @@
 #include "icing/text_classifier/lib3/utils/base/statusor.h"
 #include "icing/absl_ports/canonical_errors.h"
 #include "icing/absl_ports/str_cat.h"
+#include "icing/file/constants.h"
 #include "icing/file/filesystem.h"
 #include "icing/file/memory-mapped-file.h"
 #include "icing/legacy/core/icing-string-util.h"
@@ -113,22 +114,14 @@ class PortableFileBackedProtoLog {
     // Must specify values for options.
     Options() = delete;
     explicit Options(
-        bool compress_in, const int32_t max_proto_size_in = kMaxProtoSize,
+        bool compress_in,
+        const int32_t max_proto_size_in = constants::kMaxProtoSize,
         const int32_t compression_level_in = kDeflateCompressionLevel)
         : compress(compress_in),
           max_proto_size(max_proto_size_in),
           compression_level(compression_level_in) {}
   };
 
-  // Our internal max for protos.
-  //
-  // WARNING: Changing this to a larger number may invalidate our assumption
-  // that that proto size can safely be stored in the last 3 bytes of the proto
-  // header.
-  static constexpr int kMaxProtoSize = (1 << 24) - 1;  // 16MiB
-  static_assert(kMaxProtoSize <= 0x00FFFFFF,
-                "kMaxProtoSize doesn't fit in 3 bytes");
-
   // Level of compression, BEST_SPEED = 1, BEST_COMPRESSION = 9
   static constexpr int kDeflateCompressionLevel = 3;
 
@@ -397,7 +390,8 @@ class PortableFileBackedProtoLog {
   // }
   class Iterator {
    public:
-    Iterator(const Filesystem& filesystem, int fd, int64_t initial_offset);
+    Iterator(const Filesystem& filesystem, int fd, int64_t initial_offset,
+             int64_t file_size);
 
     // Advances to the position of next proto whether it has been erased or not.
     //
@@ -488,18 +482,27 @@ class PortableFileBackedProtoLog {
   //   INTERNAL_ERROR on IO error
   libtextclassifier3::Status PersistToDisk();
 
-  // Calculates the checksum of the log contents. Excludes the header content.
+  // Calculates the checksum of the log contents (excluding the header) and
+  // updates the header.
   //
   // Returns:
   //   Crc of the log content
   //   INTERNAL_ERROR on IO error
-  libtextclassifier3::StatusOr<Crc32> ComputeChecksum();
+  libtextclassifier3::StatusOr<Crc32> UpdateChecksum();
+
+  // Calculates and returns the checksum of the log contents (excluding the
+  // header). Does NOT update the header.
+  //
+  // Returns:
+  //   Crc of the log content
+  //   INTERNAL_ERROR on IO error
+  libtextclassifier3::StatusOr<Crc32> GetChecksum() const;
 
  private:
   // Object can only be instantiated via the ::Create factory.
   PortableFileBackedProtoLog(const Filesystem* filesystem,
                              const std::string& file_path,
-                             std::unique_ptr<Header> header,
+                             std::unique_ptr<Header> header, int64_t file_size,
                              int32_t compression_level);
 
   // Initializes a new proto log.
@@ -531,9 +534,9 @@ class PortableFileBackedProtoLog {
   //   Crc of the content between `start`, inclusive, and `end`, exclusive.
   //   INTERNAL_ERROR on IO error
   //   INVALID_ARGUMENT_ERROR if start and end aren't within the file size
-  static libtextclassifier3::StatusOr<Crc32> ComputeChecksum(
+  static libtextclassifier3::StatusOr<Crc32> GetPartialChecksum(
       const Filesystem* filesystem, const std::string& file_path,
-      Crc32 initial_crc, int64_t start, int64_t end);
+      Crc32 initial_crc, int64_t start, int64_t end, int64_t file_size);
 
   // Reads out the metadata of a proto located at file_offset from the fd.
   // Metadata will be returned in host byte order endianness.
@@ -584,16 +587,19 @@ class PortableFileBackedProtoLog {
   const Filesystem* const filesystem_;
   const std::string file_path_;
   std::unique_ptr<Header> header_;
+  int64_t file_size_;
   const int32_t compression_level_;
 };
 
 template <typename ProtoT>
 PortableFileBackedProtoLog<ProtoT>::PortableFileBackedProtoLog(
     const Filesystem* filesystem, const std::string& file_path,
-    std::unique_ptr<Header> header, int32_t compression_level)
+    std::unique_ptr<Header> header, int64_t file_size,
+    int32_t compression_level)
     : filesystem_(filesystem),
       file_path_(file_path),
       header_(std::move(header)),
+      file_size_(file_size),
       compression_level_(compression_level) {
   fd_.reset(filesystem_->OpenForAppend(file_path.c_str()));
 }
@@ -621,7 +627,7 @@ PortableFileBackedProtoLog<ProtoT>::Create(const Filesystem* filesystem,
 
   // Since we store the proto_size in 3 bytes, we can only support protos of up
   // to 16MiB.
-  if (options.max_proto_size > kMaxProtoSize) {
+  if (options.max_proto_size > constants::kMaxProtoSize) {
     return absl_ports::InvalidArgumentError(IcingStringUtil::StringPrintf(
         "options.max_proto_size must be under 16MiB, was %d",
         options.max_proto_size));
@@ -675,9 +681,9 @@ PortableFileBackedProtoLog<ProtoT>::InitializeNewFile(
 
   CreateResult create_result = {
       std::unique_ptr<PortableFileBackedProtoLog<ProtoT>>(
-          new PortableFileBackedProtoLog<ProtoT>(filesystem, file_path,
-                                                 std::move(header),
-                                                 options.compression_level)),
+          new PortableFileBackedProtoLog<ProtoT>(
+              filesystem, file_path, std::move(header),
+              /*file_size=*/kHeaderReservedBytes, options.compression_level)),
       /*data_loss=*/DataLoss::NONE, /*recalculated_checksum=*/false};
 
   return create_result;
@@ -751,6 +757,7 @@ PortableFileBackedProtoLog<ProtoT>::InitializeExistingFile(
           "Failed to truncate '%s' to size %lld", file_path.data(),
           static_cast<long long>(header->GetRewindOffset())));
     }
+    file_size = header->GetRewindOffset();
     data_loss = DataLoss::PARTIAL;
   }
 
@@ -769,10 +776,10 @@ PortableFileBackedProtoLog<ProtoT>::InitializeExistingFile(
   // we need to throw everything out.
   if (header->GetDirtyFlag()) {
     // Recompute the log's checksum to detect which scenario we're in.
-    ICING_ASSIGN_OR_RETURN(
-        Crc32 calculated_log_checksum,
-        ComputeChecksum(filesystem, file_path, Crc32(),
-                        /*start=*/kHeaderReservedBytes, /*end=*/file_size));
+    ICING_ASSIGN_OR_RETURN(Crc32 calculated_log_checksum,
+                           GetPartialChecksum(filesystem, file_path, Crc32(),
+                                              /*start=*/kHeaderReservedBytes,
+                                              /*end=*/file_size, file_size));
 
     if (header->GetLogChecksum() != calculated_log_checksum.Get()) {
       // Still doesn't match, we're in Scenario 2. Throw out all our data now
@@ -805,7 +812,7 @@ PortableFileBackedProtoLog<ProtoT>::InitializeExistingFile(
   CreateResult create_result = {
       std::unique_ptr<PortableFileBackedProtoLog<ProtoT>>(
           new PortableFileBackedProtoLog<ProtoT>(filesystem, file_path,
-                                                 std::move(header),
+                                                 std::move(header), file_size,
                                                  options.compression_level)),
       data_loss, recalculated_checksum};
 
@@ -814,9 +821,9 @@ PortableFileBackedProtoLog<ProtoT>::InitializeExistingFile(
 
 template <typename ProtoT>
 libtextclassifier3::StatusOr<Crc32>
-PortableFileBackedProtoLog<ProtoT>::ComputeChecksum(
+PortableFileBackedProtoLog<ProtoT>::GetPartialChecksum(
     const Filesystem* filesystem, const std::string& file_path,
-    Crc32 initial_crc, int64_t start, int64_t end) {
+    Crc32 initial_crc, int64_t start, int64_t end, int64_t file_size) {
   ICING_ASSIGN_OR_RETURN(
       MemoryMappedFile mmapped_file,
       MemoryMappedFile::Create(*filesystem, file_path,
@@ -838,7 +845,6 @@ PortableFileBackedProtoLog<ProtoT>::ComputeChecksum(
         static_cast<long long>(end)));
   }
 
-  int64_t file_size = filesystem->GetFileSize(file_path.c_str());
   if (end > file_size) {
     return absl_ports::InvalidArgumentError(IcingStringUtil::StringPrintf(
         "Ending checksum offset of file '%s' must be within "
@@ -953,32 +959,30 @@ PortableFileBackedProtoLog<ProtoT>::WriteProto(const ProtoT& proto) {
         absl_ports::StrCat("Failed to write proto to: ", file_path_));
   }
 
+  // Update file size. The file should have grown by sizeof(Metadata) + size of
+  // the serialized proto.
+  file_size_ += sizeof(host_order_metadata) + final_size;
   return current_position;
 }
 
 template <typename ProtoT>
 libtextclassifier3::StatusOr<ProtoT>
 PortableFileBackedProtoLog<ProtoT>::ReadProto(int64_t file_offset) const {
-  int64_t file_size = filesystem_->GetFileSize(fd_.get());
-  // Read out the metadata
-  if (file_size == Filesystem::kBadFileSize) {
-    return absl_ports::OutOfRangeError("Unable to correctly read size.");
-  }
   ICING_ASSIGN_OR_RETURN(
       int32_t metadata,
-      ReadProtoMetadata(filesystem_, fd_.get(), file_offset, file_size));
+      ReadProtoMetadata(filesystem_, fd_.get(), file_offset, file_size_));
 
   // Copy out however many bytes it says the proto is
   int stored_size = GetProtoSize(metadata);
   file_offset += sizeof(metadata);
 
   // Read the compressed proto out.
-  if (file_offset + stored_size > file_size) {
+  if (file_offset + stored_size > file_size_) {
     return absl_ports::OutOfRangeError(
         IcingStringUtil::StringPrintf("Trying to read from a location, %lld, "
                                       "out of range of the file size, %lld",
                                       static_cast<long long>(file_offset),
-                                      static_cast<long long>(file_size - 1)));
+                                      static_cast<long long>(file_size_ - 1)));
   }
   auto buf = std::make_unique<char[]>(stored_size);
   if (!filesystem_->PRead(fd_.get(), buf.get(), stored_size, file_offset)) {
@@ -1006,23 +1010,18 @@ PortableFileBackedProtoLog<ProtoT>::ReadProto(int64_t file_offset) const {
 template <typename ProtoT>
 libtextclassifier3::Status PortableFileBackedProtoLog<ProtoT>::EraseProto(
     int64_t file_offset) {
-  int64_t file_size = filesystem_->GetFileSize(fd_.get());
-  if (file_size == Filesystem::kBadFileSize) {
-    return absl_ports::OutOfRangeError("Unable to correctly read size.");
-  }
-
   ICING_ASSIGN_OR_RETURN(
       int32_t metadata,
-      ReadProtoMetadata(filesystem_, fd_.get(), file_offset, file_size));
+      ReadProtoMetadata(filesystem_, fd_.get(), file_offset, file_size_));
   // Copy out however many bytes it says the proto is
   int stored_size = GetProtoSize(metadata);
   file_offset += sizeof(metadata);
-  if (file_offset + stored_size > file_size) {
+  if (file_offset + stored_size > file_size_) {
     return absl_ports::OutOfRangeError(
         IcingStringUtil::StringPrintf("Trying to read from a location, %lld, "
                                       "out of range of the file size, %lld",
                                       static_cast<long long>(file_offset),
-                                      static_cast<long long>(file_size - 1)));
+                                      static_cast<long long>(file_size_ - 1)));
   }
   auto buf = std::make_unique<char[]>(stored_size);
 
@@ -1093,27 +1092,18 @@ PortableFileBackedProtoLog<ProtoT>::GetDiskUsage() const {
 template <typename ProtoT>
 libtextclassifier3::StatusOr<int64_t>
 PortableFileBackedProtoLog<ProtoT>::GetElementsFileSize() const {
-  int64_t total_file_size = filesystem_->GetFileSize(file_path_.c_str());
-  if (total_file_size == Filesystem::kBadFileSize) {
-    return absl_ports::InternalError(
-        "Failed to get file size of elments in the proto log");
-  }
-  return total_file_size - kHeaderReservedBytes;
+  return file_size_ - kHeaderReservedBytes;
 }
 
 template <typename ProtoT>
 PortableFileBackedProtoLog<ProtoT>::Iterator::Iterator(
-    const Filesystem& filesystem, int fd, int64_t initial_offset)
+    const Filesystem& filesystem, int fd, int64_t initial_offset,
+    int64_t file_size)
     : filesystem_(&filesystem),
       initial_offset_(initial_offset),
       current_offset_(kInvalidOffset),
-      fd_(fd) {
-  file_size_ = filesystem_->GetFileSize(fd_);
-  if (file_size_ == Filesystem::kBadFileSize) {
-    // Fails all Advance() calls
-    file_size_ = 0;
-  }
-}
+      file_size_(file_size),
+      fd_(fd) {}
 
 template <typename ProtoT>
 libtextclassifier3::Status
@@ -1148,7 +1138,7 @@ template <typename ProtoT>
 typename PortableFileBackedProtoLog<ProtoT>::Iterator
 PortableFileBackedProtoLog<ProtoT>::GetIterator() {
   return Iterator(*filesystem_, fd_.get(),
-                  /*initial_offset=*/kHeaderReservedBytes);
+                  /*initial_offset=*/kHeaderReservedBytes, file_size_);
 }
 
 template <typename ProtoT>
@@ -1210,51 +1200,57 @@ PortableFileBackedProtoLog<ProtoT>::WriteProtoMetadata(
 
 template <typename ProtoT>
 libtextclassifier3::Status PortableFileBackedProtoLog<ProtoT>::PersistToDisk() {
-  int64_t file_size = filesystem_->GetFileSize(file_path_.c_str());
-  if (file_size == header_->GetRewindOffset()) {
+  if (file_size_ == header_->GetRewindOffset()) {
     // No new protos appended, don't need to update the checksum.
     return libtextclassifier3::Status::OK;
   }
 
-  ICING_ASSIGN_OR_RETURN(Crc32 crc, ComputeChecksum());
+  ICING_RETURN_IF_ERROR(UpdateChecksum());
+  if (!filesystem_->DataSync(fd_.get())) {
+    return absl_ports::InternalError(
+        absl_ports::StrCat("Failed to sync data to disk: ", file_path_));
+  }
 
+  return libtextclassifier3::Status::OK;
+}
+
+template <typename ProtoT>
+libtextclassifier3::StatusOr<Crc32>
+PortableFileBackedProtoLog<ProtoT>::UpdateChecksum() {
+  if (file_size_ == header_->GetRewindOffset()) {
+    return Crc32(header_->GetLogChecksum());
+  }
+  ICING_ASSIGN_OR_RETURN(Crc32 crc, GetChecksum());
   header_->SetLogChecksum(crc.Get());
-  header_->SetRewindOffset(file_size);
+  header_->SetRewindOffset(file_size_);
   header_->SetHeaderChecksum(header_->CalculateHeaderChecksum());
 
   if (!filesystem_->PWrite(fd_.get(), /*offset=*/0, header_.get(),
-                           sizeof(Header)) ||
-      !filesystem_->DataSync(fd_.get())) {
+                           sizeof(Header))) {
     return absl_ports::InternalError(
         absl_ports::StrCat("Failed to update header to: ", file_path_));
   }
-
-  return libtextclassifier3::Status::OK;
+  return crc;
 }
 
 template <typename ProtoT>
 libtextclassifier3::StatusOr<Crc32>
-PortableFileBackedProtoLog<ProtoT>::ComputeChecksum() {
-  int64_t file_size = filesystem_->GetFileSize(file_path_.c_str());
-  int64_t new_content_size = file_size - header_->GetRewindOffset();
-  Crc32 crc;
+PortableFileBackedProtoLog<ProtoT>::GetChecksum() const {
+  int64_t new_content_size = file_size_ - header_->GetRewindOffset();
   if (new_content_size == 0) {
     // No new protos appended, return cached checksum
     return Crc32(header_->GetLogChecksum());
   } else if (new_content_size < 0) {
     // File shrunk, recalculate the entire checksum.
-    ICING_ASSIGN_OR_RETURN(
-        crc,
-        ComputeChecksum(filesystem_, file_path_, Crc32(),
-                        /*start=*/kHeaderReservedBytes, /*end=*/file_size));
+    return GetPartialChecksum(filesystem_, file_path_, Crc32(),
+                              /*start=*/kHeaderReservedBytes,
+                              /*end=*/file_size_, file_size_);
   } else {
     // Append new changes to the existing checksum.
-    ICING_ASSIGN_OR_RETURN(
-        crc, ComputeChecksum(
-                 filesystem_, file_path_, Crc32(header_->GetLogChecksum()),
-                 /*start=*/header_->GetRewindOffset(), /*end=*/file_size));
+    return GetPartialChecksum(
+        filesystem_, file_path_, Crc32(header_->GetLogChecksum()),
+        /*start=*/header_->GetRewindOffset(), /*end=*/file_size_, file_size_);
   }
-  return crc;
 }
 
 }  // namespace lib
diff --git a/icing/file/portable-file-backed-proto-log_benchmark.cc b/icing/file/portable-file-backed-proto-log_benchmark.cc
index d7ea4bb..e6935ab 100644
--- a/icing/file/portable-file-backed-proto-log_benchmark.cc
+++ b/icing/file/portable-file-backed-proto-log_benchmark.cc
@@ -204,7 +204,7 @@ void BM_Erase(benchmark::State& state) {
 }
 BENCHMARK(BM_Erase);
 
-void BM_ComputeChecksum(benchmark::State& state) {
+void BM_UpdateChecksum(benchmark::State& state) {
   const Filesystem filesystem;
   const std::string file_path = GetTestTempDir() + "/proto.log";
   int max_proto_size = (1 << 24) - 1;  // 16 MiB
@@ -238,15 +238,15 @@ void BM_ComputeChecksum(benchmark::State& state) {
   }
 
   for (auto _ : state) {
-    testing::DoNotOptimize(proto_log->ComputeChecksum());
+    testing::DoNotOptimize(proto_log->UpdateChecksum());
   }
 
   // Cleanup after ourselves
   filesystem.DeleteFile(file_path.c_str());
 }
-BENCHMARK(BM_ComputeChecksum)->Range(1024, 1 << 20);
+BENCHMARK(BM_UpdateChecksum)->Range(1024, 1 << 20);
 
-void BM_ComputeChecksumWithCachedChecksum(benchmark::State& state) {
+void BM_UpdateChecksumWithCachedChecksum(benchmark::State& state) {
   const Filesystem filesystem;
   const std::string file_path = GetTestTempDir() + "/proto.log";
   int max_proto_size = (1 << 24) - 1;  // 16 MiB
@@ -279,18 +279,18 @@ void BM_ComputeChecksumWithCachedChecksum(benchmark::State& state) {
   ICING_ASSERT_OK(proto_log->WriteProto(document));
   ICING_ASSERT_OK(proto_log->PersistToDisk());
 
-  // This ComputeChecksum call shouldn't need to do any computation since we can
+  // This UpdateChecksum call shouldn't need to do any computation since we can
   // reuse our cached checksum.
   for (auto _ : state) {
-    testing::DoNotOptimize(proto_log->ComputeChecksum());
+    testing::DoNotOptimize(proto_log->UpdateChecksum());
   }
 
   // Cleanup after ourselves
   filesystem.DeleteFile(file_path.c_str());
 }
-BENCHMARK(BM_ComputeChecksumWithCachedChecksum);
+BENCHMARK(BM_UpdateChecksumWithCachedChecksum);
 
-void BM_ComputeChecksumOnlyForTail(benchmark::State& state) {
+void BM_UpdateChecksumOnlyForTail(benchmark::State& state) {
   const Filesystem filesystem;
   const std::string file_path = GetTestTempDir() + "/proto.log";
   int max_proto_size = (1 << 24) - 1;  // 16 MiB
@@ -327,16 +327,16 @@ void BM_ComputeChecksumOnlyForTail(benchmark::State& state) {
   // checksum since we didn't call persist.
   ICING_ASSERT_OK(proto_log->WriteProto(document));
 
-  // ComputeChecksum should be calculating the checksum of the tail and adding
+  // UpdateChecksum should be calculating the checksum of the tail and adding
   // it to the cached checksum we have.
   for (auto _ : state) {
-    testing::DoNotOptimize(proto_log->ComputeChecksum());
+    testing::DoNotOptimize(proto_log->UpdateChecksum());
   }
 
   // Cleanup after ourselves
   filesystem.DeleteFile(file_path.c_str());
 }
-BENCHMARK(BM_ComputeChecksumOnlyForTail);
+BENCHMARK(BM_UpdateChecksumOnlyForTail);
 
 }  // namespace
 }  // namespace lib
diff --git a/icing/file/portable-file-backed-proto-log_test.cc b/icing/file/portable-file-backed-proto-log_test.cc
index cc70151..b564a2c 100644
--- a/icing/file/portable-file-backed-proto-log_test.cc
+++ b/icing/file/portable-file-backed-proto-log_test.cc
@@ -1038,21 +1038,9 @@ TEST_F(PortableFileBackedProtoLogTest, Iterator) {
     ASSERT_THAT(iterator.Advance(),
                 StatusIs(libtextclassifier3::StatusCode::OUT_OF_RANGE));
   }
-
-  {
-    // Iterator with bad filesystem
-    ScopedFd sfd(filesystem_.OpenForRead(file_path_.c_str()));
-    MockFilesystem mock_filesystem;
-    ON_CALL(mock_filesystem, GetFileSize(A<int>()))
-        .WillByDefault(Return(Filesystem::kBadFileSize));
-    PortableFileBackedProtoLog<DocumentProto>::Iterator bad_iterator(
-        mock_filesystem, sfd.get(), /*initial_offset=*/0);
-    ASSERT_THAT(bad_iterator.Advance(),
-                StatusIs(libtextclassifier3::StatusCode::OUT_OF_RANGE));
-  }
 }
 
-TEST_F(PortableFileBackedProtoLogTest, ComputeChecksum) {
+TEST_F(PortableFileBackedProtoLogTest, UpdateChecksum) {
   DocumentProto document = DocumentBuilder().SetKey("namespace", "uri").Build();
   Crc32 checksum;
 
@@ -1068,10 +1056,12 @@ TEST_F(PortableFileBackedProtoLogTest, ComputeChecksum) {
 
     ICING_EXPECT_OK(proto_log->WriteProto(document));
 
-    ICING_ASSERT_OK_AND_ASSIGN(checksum, proto_log->ComputeChecksum());
+    ICING_ASSERT_OK_AND_ASSIGN(checksum, proto_log->GetChecksum());
+    EXPECT_THAT(proto_log->UpdateChecksum(), IsOkAndHolds(Eq(checksum)));
+    EXPECT_THAT(proto_log->GetChecksum(), IsOkAndHolds(Eq(checksum)));
 
     // Calling it twice with no changes should get us the same checksum
-    EXPECT_THAT(proto_log->ComputeChecksum(), IsOkAndHolds(Eq(checksum)));
+    EXPECT_THAT(proto_log->UpdateChecksum(), IsOkAndHolds(Eq(checksum)));
   }
 
   {
@@ -1085,15 +1075,19 @@ TEST_F(PortableFileBackedProtoLogTest, ComputeChecksum) {
     ASSERT_FALSE(create_result.has_data_loss());
 
     // Checksum should be consistent across instances
-    EXPECT_THAT(proto_log->ComputeChecksum(), IsOkAndHolds(Eq(checksum)));
+    ICING_ASSERT_OK_AND_ASSIGN(checksum, proto_log->GetChecksum());
+    EXPECT_THAT(proto_log->UpdateChecksum(), IsOkAndHolds(Eq(checksum)));
+    EXPECT_THAT(proto_log->GetChecksum(), IsOkAndHolds(Eq(checksum)));
 
     // PersistToDisk shouldn't affect the checksum value
     ICING_EXPECT_OK(proto_log->PersistToDisk());
-    EXPECT_THAT(proto_log->ComputeChecksum(), IsOkAndHolds(Eq(checksum)));
+    EXPECT_THAT(proto_log->GetChecksum(), IsOkAndHolds(Eq(checksum)));
 
     // Check that modifying the log leads to a different checksum
     ICING_EXPECT_OK(proto_log->WriteProto(document));
-    EXPECT_THAT(proto_log->ComputeChecksum(), IsOkAndHolds(Not(Eq(checksum))));
+    EXPECT_THAT(proto_log->GetChecksum(), IsOkAndHolds(Not(Eq(checksum))));
+    EXPECT_THAT(proto_log->UpdateChecksum(), IsOkAndHolds(Not(Eq(checksum))));
+    EXPECT_THAT(proto_log->GetChecksum(), IsOkAndHolds(Not(Eq(checksum))));
   }
 }
 
@@ -1200,7 +1194,7 @@ TEST_F(PortableFileBackedProtoLogTest, ChecksumShouldBeCorrectWithErasedProto) {
     // rewind position is 0.
     ICING_ASSERT_OK(proto_log->EraseProto(document1_offset));
 
-    EXPECT_THAT(proto_log->ComputeChecksum(),
+    EXPECT_THAT(proto_log->UpdateChecksum(),
                 IsOkAndHolds(Eq(Crc32(2175574628))));
   }  // New checksum is updated in destructor.
 
@@ -1220,7 +1214,7 @@ TEST_F(PortableFileBackedProtoLogTest, ChecksumShouldBeCorrectWithErasedProto) {
     // is updated.
     ICING_ASSERT_OK(proto_log->EraseProto(document2_offset));
 
-    EXPECT_THAT(proto_log->ComputeChecksum(),
+    EXPECT_THAT(proto_log->UpdateChecksum(),
                 IsOkAndHolds(Eq(Crc32(790877774))));
   }
 
@@ -1243,7 +1237,7 @@ TEST_F(PortableFileBackedProtoLogTest, ChecksumShouldBeCorrectWithErasedProto) {
     // is updated.
     ICING_ASSERT_OK(proto_log->EraseProto(document3_offset));
 
-    EXPECT_THAT(proto_log->ComputeChecksum(),
+    EXPECT_THAT(proto_log->UpdateChecksum(),
                 IsOkAndHolds(Eq(Crc32(2344803210))));
   }  // Checksum is updated with the newly appended document.
 
diff --git a/icing/file/posting_list/flash-index-storage_test.cc b/icing/file/posting_list/flash-index-storage_test.cc
index 203041e..8412a00 100644
--- a/icing/file/posting_list/flash-index-storage_test.cc
+++ b/icing/file/posting_list/flash-index-storage_test.cc
@@ -20,7 +20,6 @@
 #include <memory>
 #include <string>
 #include <utility>
-#include <vector>
 
 #include "icing/text_classifier/lib3/utils/base/status.h"
 #include "gmock/gmock.h"
@@ -39,7 +38,7 @@ namespace lib {
 
 namespace {
 
-using ::testing::ElementsAreArray;
+using ::testing::ElementsAre;
 using ::testing::Eq;
 using ::testing::IsEmpty;
 using ::testing::IsFalse;
@@ -213,21 +212,30 @@ TEST_F(FlashIndexStorageTest, FreeListInMemory) {
     EXPECT_THAT(flash_index_storage.num_blocks(), Eq(2));
     EXPECT_THAT(flash_index_storage.empty(), IsFalse());
 
-    std::vector<Hit> hits1 = {
-        Hit(/*section_id=*/1, /*document_id=*/0, /*term_frequency=*/12,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/6, /*document_id=*/2, /*term_frequency=*/19,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/5, /*document_id=*/2, /*term_frequency=*/100,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/8, /*document_id=*/5, /*term_frequency=*/197,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false)};
-    for (const Hit& hit : hits1) {
-      ICING_ASSERT_OK(
-          serializer_->PrependHit(&posting_list_holder1.posting_list, hit));
-    }
+    Hit hit0(/*section_id=*/1, /*document_id=*/0, /*term_frequency=*/12,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit1(/*section_id=*/6, /*document_id=*/2, /*term_frequency=*/19,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit2(/*section_id=*/5, /*document_id=*/2, /*term_frequency=*/100,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit3(/*section_id=*/8, /*document_id=*/5, /*term_frequency=*/197,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder1.posting_list, hit0));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder1.posting_list, hit1));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder1.posting_list, hit2));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder1.posting_list, hit3));
+
     EXPECT_THAT(serializer_->GetHits(&posting_list_holder1.posting_list),
-                IsOkAndHolds(ElementsAreArray(hits1.rbegin(), hits1.rend())));
+                IsOkAndHolds(ElementsAre(EqualsHit(hit3), EqualsHit(hit2),
+                                         EqualsHit(hit1), EqualsHit(hit0))));
 
     // 2. Get another PL. This should be on the same flash block. There should
     // be no allocation.
@@ -241,21 +249,30 @@ TEST_F(FlashIndexStorageTest, FreeListInMemory) {
     EXPECT_THAT(flash_index_storage.num_blocks(), Eq(2));
     EXPECT_THAT(flash_index_storage.empty(), IsFalse());
 
-    std::vector<Hit> hits2 = {
-        Hit(/*section_id=*/4, /*document_id=*/0, /*term_frequency=*/12,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/8, /*document_id=*/4, /*term_frequency=*/19,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/9, /*document_id=*/7, /*term_frequency=*/100,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/6, /*document_id=*/7, /*term_frequency=*/197,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false)};
-    for (const Hit& hit : hits2) {
-      ICING_ASSERT_OK(
-          serializer_->PrependHit(&posting_list_holder2.posting_list, hit));
-    }
+    Hit hit4(/*section_id=*/4, /*document_id=*/0, /*term_frequency=*/12,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit5(/*section_id=*/8, /*document_id=*/4, /*term_frequency=*/19,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit6(/*section_id=*/9, /*document_id=*/7, /*term_frequency=*/100,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit7(/*section_id=*/6, /*document_id=*/7, /*term_frequency=*/197,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder2.posting_list, hit4));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder2.posting_list, hit5));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder2.posting_list, hit6));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder2.posting_list, hit7));
+
     EXPECT_THAT(serializer_->GetHits(&posting_list_holder2.posting_list),
-                IsOkAndHolds(ElementsAreArray(hits2.rbegin(), hits2.rend())));
+                IsOkAndHolds(ElementsAre(EqualsHit(hit7), EqualsHit(hit6),
+                                         EqualsHit(hit5), EqualsHit(hit4))));
 
     // 3. Now, free the first posting list. This should add it to the free list
     ICING_ASSERT_OK(
@@ -281,21 +298,31 @@ TEST_F(FlashIndexStorageTest, FreeListInMemory) {
     // gone.
     EXPECT_THAT(serializer_->GetHits(&posting_list_holder3.posting_list),
                 IsOkAndHolds(IsEmpty()));
-    std::vector<Hit> hits3 = {
-        Hit(/*section_id=*/7, /*document_id=*/1, /*term_frequency=*/62,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/12, /*document_id=*/3, /*term_frequency=*/45,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/11, /*document_id=*/18, /*term_frequency=*/12,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/7, /*document_id=*/100, /*term_frequency=*/74,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false)};
-    for (const Hit& hit : hits3) {
-      ICING_ASSERT_OK(
-          serializer_->PrependHit(&posting_list_holder3.posting_list, hit));
-    }
+
+    Hit hit8(/*section_id=*/7, /*document_id=*/1, /*term_frequency=*/62,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit9(/*section_id=*/12, /*document_id=*/3, /*term_frequency=*/45,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit10(/*section_id=*/11, /*document_id=*/18, /*term_frequency=*/12,
+              /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+              /*is_stemmed_hit=*/false);
+    Hit hit11(/*section_id=*/7, /*document_id=*/100, /*term_frequency=*/74,
+              /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+              /*is_stemmed_hit=*/false);
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder3.posting_list, hit8));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder3.posting_list, hit9));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder3.posting_list, hit10));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder3.posting_list, hit11));
+
     EXPECT_THAT(serializer_->GetHits(&posting_list_holder3.posting_list),
-                IsOkAndHolds(ElementsAreArray(hits3.rbegin(), hits3.rend())));
+                IsOkAndHolds(ElementsAre(EqualsHit(hit11), EqualsHit(hit10),
+                                         EqualsHit(hit9), EqualsHit(hit8))));
   }
   EXPECT_THAT(flash_index_storage.GetDiskUsage(),
               Eq(2 * flash_index_storage.block_size()));
@@ -326,21 +353,30 @@ TEST_F(FlashIndexStorageTest, FreeListNotInMemory) {
     EXPECT_THAT(flash_index_storage.num_blocks(), Eq(2));
     EXPECT_THAT(flash_index_storage.empty(), IsFalse());
 
-    std::vector<Hit> hits1 = {
-        Hit(/*section_id=*/1, /*document_id=*/0, /*term_frequency=*/12,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/6, /*document_id=*/2, /*term_frequency=*/19,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/5, /*document_id=*/2, /*term_frequency=*/100,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/8, /*document_id=*/5, /*term_frequency=*/197,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false)};
-    for (const Hit& hit : hits1) {
-      ICING_ASSERT_OK(
-          serializer_->PrependHit(&posting_list_holder1.posting_list, hit));
-    }
+    Hit hit0(/*section_id=*/1, /*document_id=*/0, /*term_frequency=*/12,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit1(/*section_id=*/6, /*document_id=*/2, /*term_frequency=*/19,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit2(/*section_id=*/5, /*document_id=*/2, /*term_frequency=*/100,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit3(/*section_id=*/8, /*document_id=*/5, /*term_frequency=*/197,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder1.posting_list, hit0));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder1.posting_list, hit1));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder1.posting_list, hit2));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder1.posting_list, hit3));
+
     EXPECT_THAT(serializer_->GetHits(&posting_list_holder1.posting_list),
-                IsOkAndHolds(ElementsAreArray(hits1.rbegin(), hits1.rend())));
+                IsOkAndHolds(ElementsAre(EqualsHit(hit3), EqualsHit(hit2),
+                                         EqualsHit(hit1), EqualsHit(hit0))));
 
     // 2. Get another PL. This should be on the same flash block. There should
     // be no allocation.
@@ -354,21 +390,30 @@ TEST_F(FlashIndexStorageTest, FreeListNotInMemory) {
     EXPECT_THAT(flash_index_storage.num_blocks(), Eq(2));
     EXPECT_THAT(flash_index_storage.empty(), IsFalse());
 
-    std::vector<Hit> hits2 = {
-        Hit(/*section_id=*/4, /*document_id=*/0, /*term_frequency=*/12,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/8, /*document_id=*/4, /*term_frequency=*/19,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/9, /*document_id=*/7, /*term_frequency=*/100,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/6, /*document_id=*/7, /*term_frequency=*/197,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false)};
-    for (const Hit& hit : hits2) {
-      ICING_ASSERT_OK(
-          serializer_->PrependHit(&posting_list_holder2.posting_list, hit));
-    }
+    Hit hit4(/*section_id=*/4, /*document_id=*/0, /*term_frequency=*/12,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit5(/*section_id=*/8, /*document_id=*/4, /*term_frequency=*/19,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit6(/*section_id=*/9, /*document_id=*/7, /*term_frequency=*/100,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit7(/*section_id=*/6, /*document_id=*/7, /*term_frequency=*/197,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder2.posting_list, hit4));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder2.posting_list, hit5));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder2.posting_list, hit6));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder2.posting_list, hit7));
+
     EXPECT_THAT(serializer_->GetHits(&posting_list_holder2.posting_list),
-                IsOkAndHolds(ElementsAreArray(hits2.rbegin(), hits2.rend())));
+                IsOkAndHolds(ElementsAre(EqualsHit(hit7), EqualsHit(hit6),
+                                         EqualsHit(hit5), EqualsHit(hit4))));
 
     // 3. Now, free the first posting list. This should add it to the free list
     ICING_ASSERT_OK(
@@ -394,21 +439,31 @@ TEST_F(FlashIndexStorageTest, FreeListNotInMemory) {
     // gone.
     EXPECT_THAT(serializer_->GetHits(&posting_list_holder3.posting_list),
                 IsOkAndHolds(IsEmpty()));
-    std::vector<Hit> hits3 = {
-        Hit(/*section_id=*/7, /*document_id=*/1, /*term_frequency=*/62,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/12, /*document_id=*/3, /*term_frequency=*/45,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/11, /*document_id=*/18, /*term_frequency=*/12,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/7, /*document_id=*/100, /*term_frequency=*/74,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false)};
-    for (const Hit& hit : hits3) {
-      ICING_ASSERT_OK(
-          serializer_->PrependHit(&posting_list_holder3.posting_list, hit));
-    }
+
+    Hit hit8(/*section_id=*/7, /*document_id=*/1, /*term_frequency=*/62,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit9(/*section_id=*/12, /*document_id=*/3, /*term_frequency=*/45,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit10(/*section_id=*/11, /*document_id=*/18, /*term_frequency=*/12,
+              /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+              /*is_stemmed_hit=*/false);
+    Hit hit11(/*section_id=*/7, /*document_id=*/100, /*term_frequency=*/74,
+              /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+              /*is_stemmed_hit=*/false);
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder3.posting_list, hit8));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder3.posting_list, hit9));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder3.posting_list, hit10));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder3.posting_list, hit11));
+
     EXPECT_THAT(serializer_->GetHits(&posting_list_holder3.posting_list),
-                IsOkAndHolds(ElementsAreArray(hits3.rbegin(), hits3.rend())));
+                IsOkAndHolds(ElementsAre(EqualsHit(hit11), EqualsHit(hit10),
+                                         EqualsHit(hit9), EqualsHit(hit8))));
   }
   EXPECT_THAT(flash_index_storage.GetDiskUsage(),
               Eq(2 * flash_index_storage.block_size()));
@@ -441,21 +496,30 @@ TEST_F(FlashIndexStorageTest, FreeListInMemoryPersistence) {
       EXPECT_THAT(flash_index_storage.num_blocks(), Eq(2));
       EXPECT_THAT(flash_index_storage.empty(), IsFalse());
 
-      std::vector<Hit> hits1 = {
-          Hit(/*section_id=*/1, /*document_id=*/0, /*term_frequency=*/12,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-          Hit(/*section_id=*/6, /*document_id=*/2, /*term_frequency=*/19,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-          Hit(/*section_id=*/5, /*document_id=*/2, /*term_frequency=*/100,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-          Hit(/*section_id=*/8, /*document_id=*/5, /*term_frequency=*/197,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false)};
-      for (const Hit& hit : hits1) {
-        ICING_ASSERT_OK(
-            serializer_->PrependHit(&posting_list_holder1.posting_list, hit));
-      }
+      Hit hit0(/*section_id=*/1, /*document_id=*/0, /*term_frequency=*/12,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+      Hit hit1(/*section_id=*/6, /*document_id=*/2, /*term_frequency=*/19,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+      Hit hit2(/*section_id=*/5, /*document_id=*/2, /*term_frequency=*/100,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+      Hit hit3(/*section_id=*/8, /*document_id=*/5, /*term_frequency=*/197,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+      ICING_ASSERT_OK(
+          serializer_->PrependHit(&posting_list_holder1.posting_list, hit0));
+      ICING_ASSERT_OK(
+          serializer_->PrependHit(&posting_list_holder1.posting_list, hit1));
+      ICING_ASSERT_OK(
+          serializer_->PrependHit(&posting_list_holder1.posting_list, hit2));
+      ICING_ASSERT_OK(
+          serializer_->PrependHit(&posting_list_holder1.posting_list, hit3));
+
       EXPECT_THAT(serializer_->GetHits(&posting_list_holder1.posting_list),
-                  IsOkAndHolds(ElementsAreArray(hits1.rbegin(), hits1.rend())));
+                  IsOkAndHolds(ElementsAre(EqualsHit(hit3), EqualsHit(hit2),
+                                           EqualsHit(hit1), EqualsHit(hit0))));
 
       // 2. Get another PL. This should be on the same flash block. There should
       // be no allocation.
@@ -469,21 +533,30 @@ TEST_F(FlashIndexStorageTest, FreeListInMemoryPersistence) {
       EXPECT_THAT(flash_index_storage.num_blocks(), Eq(2));
       EXPECT_THAT(flash_index_storage.empty(), IsFalse());
 
-      std::vector<Hit> hits2 = {
-          Hit(/*section_id=*/4, /*document_id=*/0, /*term_frequency=*/12,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-          Hit(/*section_id=*/8, /*document_id=*/4, /*term_frequency=*/19,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-          Hit(/*section_id=*/9, /*document_id=*/7, /*term_frequency=*/100,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-          Hit(/*section_id=*/6, /*document_id=*/7, /*term_frequency=*/197,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false)};
-      for (const Hit& hit : hits2) {
-        ICING_ASSERT_OK(
-            serializer_->PrependHit(&posting_list_holder2.posting_list, hit));
-      }
+      Hit hit4(/*section_id=*/4, /*document_id=*/0, /*term_frequency=*/12,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+      Hit hit5(/*section_id=*/8, /*document_id=*/4, /*term_frequency=*/19,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+      Hit hit6(/*section_id=*/9, /*document_id=*/7, /*term_frequency=*/100,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+      Hit hit7(/*section_id=*/6, /*document_id=*/7, /*term_frequency=*/197,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+      ICING_ASSERT_OK(
+          serializer_->PrependHit(&posting_list_holder2.posting_list, hit4));
+      ICING_ASSERT_OK(
+          serializer_->PrependHit(&posting_list_holder2.posting_list, hit5));
+      ICING_ASSERT_OK(
+          serializer_->PrependHit(&posting_list_holder2.posting_list, hit6));
+      ICING_ASSERT_OK(
+          serializer_->PrependHit(&posting_list_holder2.posting_list, hit7));
+
       EXPECT_THAT(serializer_->GetHits(&posting_list_holder2.posting_list),
-                  IsOkAndHolds(ElementsAreArray(hits2.rbegin(), hits2.rend())));
+                  IsOkAndHolds(ElementsAre(EqualsHit(hit7), EqualsHit(hit6),
+                                           EqualsHit(hit5), EqualsHit(hit4))));
 
       // 3. Now, free the first posting list. This should add it to the free
       // list
@@ -524,21 +597,31 @@ TEST_F(FlashIndexStorageTest, FreeListInMemoryPersistence) {
       // gone.
       EXPECT_THAT(serializer_->GetHits(&posting_list_holder3.posting_list),
                   IsOkAndHolds(IsEmpty()));
-      std::vector<Hit> hits3 = {
-          Hit(/*section_id=*/7, /*document_id=*/1, /*term_frequency=*/62,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-          Hit(/*section_id=*/12, /*document_id=*/3, /*term_frequency=*/45,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-          Hit(/*section_id=*/11, /*document_id=*/18, /*term_frequency=*/12,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-          Hit(/*section_id=*/7, /*document_id=*/100, /*term_frequency=*/74,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false)};
-      for (const Hit& hit : hits3) {
-        ICING_ASSERT_OK(
-            serializer_->PrependHit(&posting_list_holder3.posting_list, hit));
-      }
+
+      Hit hit8(/*section_id=*/7, /*document_id=*/1, /*term_frequency=*/62,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+      Hit hit9(/*section_id=*/12, /*document_id=*/3, /*term_frequency=*/45,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+      Hit hit10(/*section_id=*/11, /*document_id=*/18, /*term_frequency=*/12,
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
+      Hit hit11(/*section_id=*/7, /*document_id=*/100, /*term_frequency=*/74,
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
+      ICING_ASSERT_OK(
+          serializer_->PrependHit(&posting_list_holder3.posting_list, hit8));
+      ICING_ASSERT_OK(
+          serializer_->PrependHit(&posting_list_holder3.posting_list, hit9));
+      ICING_ASSERT_OK(
+          serializer_->PrependHit(&posting_list_holder3.posting_list, hit10));
+      ICING_ASSERT_OK(
+          serializer_->PrependHit(&posting_list_holder3.posting_list, hit11));
+
       EXPECT_THAT(serializer_->GetHits(&posting_list_holder3.posting_list),
-                  IsOkAndHolds(ElementsAreArray(hits3.rbegin(), hits3.rend())));
+                  IsOkAndHolds(ElementsAre(EqualsHit(hit11), EqualsHit(hit10),
+                                           EqualsHit(hit9), EqualsHit(hit8))));
     }
     EXPECT_THAT(flash_index_storage.GetDiskUsage(),
                 Eq(2 * flash_index_storage.block_size()));
@@ -570,21 +653,30 @@ TEST_F(FlashIndexStorageTest, DifferentSizedPostingLists) {
     EXPECT_THAT(flash_index_storage.num_blocks(), Eq(2));
     EXPECT_THAT(flash_index_storage.empty(), IsFalse());
 
-    std::vector<Hit> hits1 = {
-        Hit(/*section_id=*/1, /*document_id=*/0, /*term_frequency=*/12,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/6, /*document_id=*/2, /*term_frequency=*/19,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/5, /*document_id=*/2, /*term_frequency=*/100,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/8, /*document_id=*/5, /*term_frequency=*/197,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false)};
-    for (const Hit& hit : hits1) {
-      ICING_ASSERT_OK(
-          serializer_->PrependHit(&posting_list_holder1.posting_list, hit));
-    }
+    Hit hit0(/*section_id=*/1, /*document_id=*/0, /*term_frequency=*/12,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit1(/*section_id=*/6, /*document_id=*/2, /*term_frequency=*/19,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit2(/*section_id=*/5, /*document_id=*/2, /*term_frequency=*/100,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit3(/*section_id=*/8, /*document_id=*/5, /*term_frequency=*/197,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder1.posting_list, hit0));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder1.posting_list, hit1));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder1.posting_list, hit2));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder1.posting_list, hit3));
+
     EXPECT_THAT(serializer_->GetHits(&posting_list_holder1.posting_list),
-                IsOkAndHolds(ElementsAreArray(hits1.rbegin(), hits1.rend())));
+                IsOkAndHolds(ElementsAre(EqualsHit(hit3), EqualsHit(hit2),
+                                         EqualsHit(hit1), EqualsHit(hit0))));
 
     // 2. Get a PL that is 1/4 block size. Even though a 1/4 block PL could
     // theoretically fit in the same block, we'll allocate a new one because PLs
@@ -601,21 +693,30 @@ TEST_F(FlashIndexStorageTest, DifferentSizedPostingLists) {
     EXPECT_THAT(flash_index_storage.num_blocks(), Eq(3));
     EXPECT_THAT(flash_index_storage.empty(), IsFalse());
 
-    std::vector<Hit> hits2 = {
-        Hit(/*section_id=*/4, /*document_id=*/0, /*term_frequency=*/12,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/8, /*document_id=*/4, /*term_frequency=*/19,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/9, /*document_id=*/7, /*term_frequency=*/100,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-        Hit(/*section_id=*/6, /*document_id=*/7, /*term_frequency=*/197,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false)};
-    for (const Hit& hit : hits2) {
-      ICING_ASSERT_OK(
-          serializer_->PrependHit(&posting_list_holder2.posting_list, hit));
-    }
+    Hit hit4(/*section_id=*/4, /*document_id=*/0, /*term_frequency=*/12,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit5(/*section_id=*/8, /*document_id=*/4, /*term_frequency=*/19,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit6(/*section_id=*/9, /*document_id=*/7, /*term_frequency=*/100,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    Hit hit7(/*section_id=*/6, /*document_id=*/7, /*term_frequency=*/197,
+             /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+             /*is_stemmed_hit=*/false);
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder2.posting_list, hit4));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder2.posting_list, hit5));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder2.posting_list, hit6));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&posting_list_holder2.posting_list, hit7));
+
     EXPECT_THAT(serializer_->GetHits(&posting_list_holder2.posting_list),
-                IsOkAndHolds(ElementsAreArray(hits2.rbegin(), hits2.rend())));
+                IsOkAndHolds(ElementsAre(EqualsHit(hit7), EqualsHit(hit6),
+                                         EqualsHit(hit5), EqualsHit(hit4))));
 
     // 3. Request another 1/4 block-size posting list. This should NOT grow the
     // index because there should be three free posting lists on block2.
diff --git a/icing/file/posting_list/index-block_test.cc b/icing/file/posting_list/index-block_test.cc
index d841e79..3464f8d 100644
--- a/icing/file/posting_list/index-block_test.cc
+++ b/icing/file/posting_list/index-block_test.cc
@@ -16,7 +16,6 @@
 
 #include <memory>
 #include <string>
-#include <vector>
 
 #include "icing/text_classifier/lib3/utils/base/status.h"
 #include "gmock/gmock.h"
@@ -33,7 +32,7 @@ namespace lib {
 
 namespace {
 
-using ::testing::ElementsAreArray;
+using ::testing::ElementsAre;
 using ::testing::Eq;
 using ::testing::IsFalse;
 using ::testing::IsTrue;
@@ -117,18 +116,21 @@ TEST_F(IndexBlockTest, SizeAccessorsWorkCorrectly) {
 TEST_F(IndexBlockTest, IndexBlockChangesPersistAcrossInstances) {
   constexpr int kPostingListBytes = 2004;
 
-  std::vector<Hit> test_hits{
-      Hit(/*section_id=*/2, /*document_id=*/0, Hit::kDefaultTermFrequency,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/5, /*document_id=*/1, /*term_frequency=*/99,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/3, /*document_id=*/3, /*term_frequency=*/17,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/10, /*document_id=*/10, Hit::kDefaultTermFrequency,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-  };
+  Hit hit0(/*section_id=*/2, /*document_id=*/0, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit1(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit2(/*section_id=*/5, /*document_id=*/1, /*term_frequency=*/99,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit3(/*section_id=*/3, /*document_id=*/3, /*term_frequency=*/17,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit4(/*section_id=*/10, /*document_id=*/10, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   PostingListIndex allocated_index;
   {
     // Create an IndexBlock from this newly allocated file block.
@@ -139,13 +141,21 @@ TEST_F(IndexBlockTest, IndexBlockChangesPersistAcrossInstances) {
     // Add hits to the first posting list.
     ICING_ASSERT_OK_AND_ASSIGN(IndexBlock::PostingListAndBlockInfo alloc_info,
                                block.AllocatePostingList());
-    for (const Hit& hit : test_hits) {
-      ICING_ASSERT_OK(
-          serializer_->PrependHit(&alloc_info.posting_list_used, hit));
-    }
-    EXPECT_THAT(
-        serializer_->GetHits(&alloc_info.posting_list_used),
-        IsOkAndHolds(ElementsAreArray(test_hits.rbegin(), test_hits.rend())));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info.posting_list_used, hit0));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info.posting_list_used, hit1));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info.posting_list_used, hit2));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info.posting_list_used, hit3));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info.posting_list_used, hit4));
+
+    EXPECT_THAT(serializer_->GetHits(&alloc_info.posting_list_used),
+                IsOkAndHolds(ElementsAre(EqualsHit(hit4), EqualsHit(hit3),
+                                         EqualsHit(hit2), EqualsHit(hit1),
+                                         EqualsHit(hit0))));
 
     ICING_ASSERT_OK(block.WritePostingListToDisk(
         alloc_info.posting_list_used, alloc_info.posting_list_index));
@@ -160,9 +170,10 @@ TEST_F(IndexBlockTest, IndexBlockChangesPersistAcrossInstances) {
     ICING_ASSERT_OK_AND_ASSIGN(
         IndexBlock::PostingListAndBlockInfo pl_block_info,
         block.GetAllocatedPostingList(allocated_index));
-    EXPECT_THAT(
-        serializer_->GetHits(&pl_block_info.posting_list_used),
-        IsOkAndHolds(ElementsAreArray(test_hits.rbegin(), test_hits.rend())));
+    EXPECT_THAT(serializer_->GetHits(&pl_block_info.posting_list_used),
+                IsOkAndHolds(ElementsAre(EqualsHit(hit4), EqualsHit(hit3),
+                                         EqualsHit(hit2), EqualsHit(hit1),
+                                         EqualsHit(hit0))));
     EXPECT_THAT(block.HasFreePostingLists(), IsOkAndHolds(IsTrue()));
   }
 }
@@ -170,30 +181,40 @@ TEST_F(IndexBlockTest, IndexBlockChangesPersistAcrossInstances) {
 TEST_F(IndexBlockTest, IndexBlockMultiplePostingLists) {
   constexpr int kPostingListBytes = 2004;
 
-  std::vector<Hit> hits_in_posting_list1{
-      Hit(/*section_id=*/2, /*document_id=*/0, Hit::kDefaultTermFrequency,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/5, /*document_id=*/1, /*term_frequency=*/99,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/3, /*document_id=*/3, /*term_frequency=*/17,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/10, /*document_id=*/10, Hit::kDefaultTermFrequency,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-  };
-  std::vector<Hit> hits_in_posting_list2{
-      Hit(/*section_id=*/12, /*document_id=*/220, /*term_frequency=*/88,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/17, /*document_id=*/265, Hit::kDefaultTermFrequency,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/0, /*document_id=*/287, /*term_frequency=*/2,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/11, /*document_id=*/306, /*term_frequency=*/12,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/10, /*document_id=*/306, Hit::kDefaultTermFrequency,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-  };
+  // Add hit0~hit4 to the first posting list.
+  Hit hit0(/*section_id=*/2, /*document_id=*/0, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit1(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit2(/*section_id=*/5, /*document_id=*/1, /*term_frequency=*/99,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit3(/*section_id=*/3, /*document_id=*/3, /*term_frequency=*/17,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit4(/*section_id=*/10, /*document_id=*/10, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+
+  // Add hit5~hit9 to the second posting list.
+  Hit hit5(/*section_id=*/12, /*document_id=*/220, /*term_frequency=*/88,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit6(/*section_id=*/17, /*document_id=*/265, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit7(/*section_id=*/0, /*document_id=*/287, /*term_frequency=*/2,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit8(/*section_id=*/11, /*document_id=*/306, /*term_frequency=*/12,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit9(/*section_id=*/10, /*document_id=*/306, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+
   PostingListIndex allocated_index_1;
   PostingListIndex allocated_index_2;
   {
@@ -206,24 +227,40 @@ TEST_F(IndexBlockTest, IndexBlockMultiplePostingLists) {
     // Add hits to the first posting list.
     ICING_ASSERT_OK_AND_ASSIGN(IndexBlock::PostingListAndBlockInfo alloc_info_1,
                                block.AllocatePostingList());
-    for (const Hit& hit : hits_in_posting_list1) {
-      ICING_ASSERT_OK(
-          serializer_->PrependHit(&alloc_info_1.posting_list_used, hit));
-    }
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info_1.posting_list_used, hit0));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info_1.posting_list_used, hit1));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info_1.posting_list_used, hit2));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info_1.posting_list_used, hit3));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info_1.posting_list_used, hit4));
+
     EXPECT_THAT(serializer_->GetHits(&alloc_info_1.posting_list_used),
-                IsOkAndHolds(ElementsAreArray(hits_in_posting_list1.rbegin(),
-                                              hits_in_posting_list1.rend())));
+                IsOkAndHolds(ElementsAre(EqualsHit(hit4), EqualsHit(hit3),
+                                         EqualsHit(hit2), EqualsHit(hit1),
+                                         EqualsHit(hit0))));
 
     // Add hits to the second posting list.
     ICING_ASSERT_OK_AND_ASSIGN(IndexBlock::PostingListAndBlockInfo alloc_info_2,
                                block.AllocatePostingList());
-    for (const Hit& hit : hits_in_posting_list2) {
-      ICING_ASSERT_OK(
-          serializer_->PrependHit(&alloc_info_2.posting_list_used, hit));
-    }
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info_2.posting_list_used, hit5));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info_2.posting_list_used, hit6));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info_2.posting_list_used, hit7));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info_2.posting_list_used, hit8));
+    ICING_ASSERT_OK(
+        serializer_->PrependHit(&alloc_info_2.posting_list_used, hit9));
+
     EXPECT_THAT(serializer_->GetHits(&alloc_info_2.posting_list_used),
-                IsOkAndHolds(ElementsAreArray(hits_in_posting_list2.rbegin(),
-                                              hits_in_posting_list2.rend())));
+                IsOkAndHolds(ElementsAre(EqualsHit(hit9), EqualsHit(hit8),
+                                         EqualsHit(hit7), EqualsHit(hit6),
+                                         EqualsHit(hit5))));
 
     EXPECT_THAT(block.AllocatePostingList(),
                 StatusIs(libtextclassifier3::StatusCode::RESOURCE_EXHAUSTED));
@@ -247,14 +284,18 @@ TEST_F(IndexBlockTest, IndexBlockMultiplePostingLists) {
         IndexBlock::PostingListAndBlockInfo pl_block_info_1,
         block.GetAllocatedPostingList(allocated_index_1));
     EXPECT_THAT(serializer_->GetHits(&pl_block_info_1.posting_list_used),
-                IsOkAndHolds(ElementsAreArray(hits_in_posting_list1.rbegin(),
-                                              hits_in_posting_list1.rend())));
+                IsOkAndHolds(ElementsAre(EqualsHit(hit4), EqualsHit(hit3),
+                                         EqualsHit(hit2), EqualsHit(hit1),
+                                         EqualsHit(hit0))));
+
     ICING_ASSERT_OK_AND_ASSIGN(
         IndexBlock::PostingListAndBlockInfo pl_block_info_2,
         block.GetAllocatedPostingList(allocated_index_2));
     EXPECT_THAT(serializer_->GetHits(&pl_block_info_2.posting_list_used),
-                IsOkAndHolds(ElementsAreArray(hits_in_posting_list2.rbegin(),
-                                              hits_in_posting_list2.rend())));
+                IsOkAndHolds(ElementsAre(EqualsHit(hit9), EqualsHit(hit8),
+                                         EqualsHit(hit7), EqualsHit(hit6),
+                                         EqualsHit(hit5))));
+
     EXPECT_THAT(block.AllocatePostingList(),
                 StatusIs(libtextclassifier3::StatusCode::RESOURCE_EXHAUSTED));
     EXPECT_THAT(block.HasFreePostingLists(), IsOkAndHolds(IsFalse()));
@@ -270,51 +311,75 @@ TEST_F(IndexBlockTest, IndexBlockReallocatingPostingLists) {
                                  &filesystem_, serializer_.get(), sfd_->get(),
                                  /*offset=*/0, kBlockSize, kPostingListBytes));
 
-  // Add hits to the first posting list.
-  std::vector<Hit> hits_in_posting_list1{
-      Hit(/*section_id=*/2, /*document_id=*/0, Hit::kDefaultTermFrequency,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/5, /*document_id=*/1, /*term_frequency=*/99,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/3, /*document_id=*/3, /*term_frequency=*/17,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/10, /*document_id=*/10, Hit::kDefaultTermFrequency,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-  };
+  // Add hit0~hit4 to the first posting list.
+  Hit hit0(/*section_id=*/2, /*document_id=*/0, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit1(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit2(/*section_id=*/5, /*document_id=*/1, /*term_frequency=*/99,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit3(/*section_id=*/3, /*document_id=*/3, /*term_frequency=*/17,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit4(/*section_id=*/10, /*document_id=*/10, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+
   ICING_ASSERT_OK_AND_ASSIGN(IndexBlock::PostingListAndBlockInfo alloc_info_1,
                              block.AllocatePostingList());
-  for (const Hit& hit : hits_in_posting_list1) {
-    ICING_ASSERT_OK(
-        serializer_->PrependHit(&alloc_info_1.posting_list_used, hit));
-  }
+  ICING_ASSERT_OK(
+      serializer_->PrependHit(&alloc_info_1.posting_list_used, hit0));
+  ICING_ASSERT_OK(
+      serializer_->PrependHit(&alloc_info_1.posting_list_used, hit1));
+  ICING_ASSERT_OK(
+      serializer_->PrependHit(&alloc_info_1.posting_list_used, hit2));
+  ICING_ASSERT_OK(
+      serializer_->PrependHit(&alloc_info_1.posting_list_used, hit3));
+  ICING_ASSERT_OK(
+      serializer_->PrependHit(&alloc_info_1.posting_list_used, hit4));
+
   EXPECT_THAT(serializer_->GetHits(&alloc_info_1.posting_list_used),
-              IsOkAndHolds(ElementsAreArray(hits_in_posting_list1.rbegin(),
-                                            hits_in_posting_list1.rend())));
-
-  // Add hits to the second posting list.
-  std::vector<Hit> hits_in_posting_list2{
-      Hit(/*section_id=*/12, /*document_id=*/220, /*term_frequency=*/88,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/17, /*document_id=*/265, Hit::kDefaultTermFrequency,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/0, /*document_id=*/287, /*term_frequency=*/2,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/11, /*document_id=*/306, /*term_frequency=*/12,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/10, /*document_id=*/306, Hit::kDefaultTermFrequency,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-  };
+              IsOkAndHolds(ElementsAre(EqualsHit(hit4), EqualsHit(hit3),
+                                       EqualsHit(hit2), EqualsHit(hit1),
+                                       EqualsHit(hit0))));
+
+  // Add hit5~hit9 to the second posting list.
+  Hit hit5(/*section_id=*/12, /*document_id=*/220, /*term_frequency=*/88,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit6(/*section_id=*/17, /*document_id=*/265, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit7(/*section_id=*/0, /*document_id=*/287, /*term_frequency=*/2,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit8(/*section_id=*/11, /*document_id=*/306, /*term_frequency=*/12,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit9(/*section_id=*/10, /*document_id=*/306, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+
   ICING_ASSERT_OK_AND_ASSIGN(IndexBlock::PostingListAndBlockInfo alloc_info_2,
                              block.AllocatePostingList());
-  for (const Hit& hit : hits_in_posting_list2) {
-    ICING_ASSERT_OK(
-        serializer_->PrependHit(&alloc_info_2.posting_list_used, hit));
-  }
+  ICING_ASSERT_OK(
+      serializer_->PrependHit(&alloc_info_2.posting_list_used, hit5));
+  ICING_ASSERT_OK(
+      serializer_->PrependHit(&alloc_info_2.posting_list_used, hit6));
+  ICING_ASSERT_OK(
+      serializer_->PrependHit(&alloc_info_2.posting_list_used, hit7));
+  ICING_ASSERT_OK(
+      serializer_->PrependHit(&alloc_info_2.posting_list_used, hit8));
+  ICING_ASSERT_OK(
+      serializer_->PrependHit(&alloc_info_2.posting_list_used, hit9));
+
   EXPECT_THAT(serializer_->GetHits(&alloc_info_2.posting_list_used),
-              IsOkAndHolds(ElementsAreArray(hits_in_posting_list2.rbegin(),
-                                            hits_in_posting_list2.rend())));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit9), EqualsHit(hit8),
+                                       EqualsHit(hit7), EqualsHit(hit6),
+                                       EqualsHit(hit5))));
 
   EXPECT_THAT(block.AllocatePostingList(),
               StatusIs(libtextclassifier3::StatusCode::RESOURCE_EXHAUSTED));
@@ -325,25 +390,30 @@ TEST_F(IndexBlockTest, IndexBlockReallocatingPostingLists) {
   ICING_ASSERT_OK(block.FreePostingList(alloc_info_1.posting_list_index));
   EXPECT_THAT(block.HasFreePostingLists(), IsOkAndHolds(IsTrue()));
 
-  std::vector<Hit> hits_in_posting_list3{
-      Hit(/*section_id=*/12, /*document_id=*/0, /*term_frequency=*/88,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/17, /*document_id=*/1, Hit::kDefaultTermFrequency,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-      Hit(/*section_id=*/0, /*document_id=*/2, /*term_frequency=*/2,
-            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false),
-  };
+  Hit hit10(/*section_id=*/12, /*document_id=*/0, /*term_frequency=*/88,
+            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+            /*is_stemmed_hit=*/false);
+  Hit hit11(/*section_id=*/17, /*document_id=*/1, Hit::kDefaultTermFrequency,
+            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+            /*is_stemmed_hit=*/false);
+  Hit hit12(/*section_id=*/0, /*document_id=*/2, /*term_frequency=*/2,
+            /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+            /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK_AND_ASSIGN(IndexBlock::PostingListAndBlockInfo alloc_info_3,
                              block.AllocatePostingList());
   EXPECT_THAT(alloc_info_3.posting_list_index,
               Eq(alloc_info_3.posting_list_index));
-  for (const Hit& hit : hits_in_posting_list3) {
-    ICING_ASSERT_OK(
-        serializer_->PrependHit(&alloc_info_3.posting_list_used, hit));
-  }
+  ICING_ASSERT_OK(
+      serializer_->PrependHit(&alloc_info_3.posting_list_used, hit10));
+  ICING_ASSERT_OK(
+      serializer_->PrependHit(&alloc_info_3.posting_list_used, hit11));
+  ICING_ASSERT_OK(
+      serializer_->PrependHit(&alloc_info_3.posting_list_used, hit12));
+
   EXPECT_THAT(serializer_->GetHits(&alloc_info_3.posting_list_used),
-              IsOkAndHolds(ElementsAreArray(hits_in_posting_list3.rbegin(),
-                                            hits_in_posting_list3.rend())));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit12), EqualsHit(hit11),
+                                       EqualsHit(hit10))));
+
   EXPECT_THAT(block.AllocatePostingList(),
               StatusIs(libtextclassifier3::StatusCode::RESOURCE_EXHAUSTED));
   EXPECT_THAT(block.HasFreePostingLists(), IsOkAndHolds(IsFalse()));
diff --git a/icing/file/version-util.h b/icing/file/version-util.h
index feadaf6..7e93b5c 100644
--- a/icing/file/version-util.h
+++ b/icing/file/version-util.h
@@ -40,9 +40,7 @@ namespace version_util {
 //
 // TODO(b/314816301): Bump kVersion to 4 for Android V rollout with v2 version
 // detection
-// LINT.IfChange(kVersion)
 inline static constexpr int32_t kVersion = 4;
-// LINT.ThenChange(//depot/google3/icing/schema/schema-store.cc:min_overlay_version_compatibility)
 inline static constexpr int32_t kVersionOne = 1;
 inline static constexpr int32_t kVersionTwo = 2;
 inline static constexpr int32_t kVersionThree = 3;
diff --git a/icing/icing-search-engine.cc b/icing/icing-search-engine.cc
index e8be2c4..769e5de 100644
--- a/icing/icing-search-engine.cc
+++ b/icing/icing-search-engine.cc
@@ -57,6 +57,7 @@
 #include "icing/legacy/index/icing-filesystem.h"
 #include "icing/performance-configuration.h"
 #include "icing/portable/endian.h"
+#include "icing/proto/blob.pb.h"
 #include "icing/proto/debug.pb.h"
 #include "icing/proto/document.pb.h"
 #include "icing/proto/initialize.pb.h"
@@ -89,6 +90,7 @@
 #include "icing/scoring/scored-document-hit.h"
 #include "icing/scoring/scored-document-hits-ranker.h"
 #include "icing/scoring/scoring-processor.h"
+#include "icing/store/blob-store.h"
 #include "icing/store/document-id.h"
 #include "icing/store/document-store.h"
 #include "icing/tokenization/language-segmenter-factory.h"
@@ -106,6 +108,7 @@ namespace lib {
 namespace {
 
 constexpr std::string_view kDocumentSubfolderName = "document_dir";
+constexpr std::string_view kBlobSubfolderName = "blob_dir";
 constexpr std::string_view kIndexSubfolderName = "index_dir";
 constexpr std::string_view kIntegerIndexSubfolderName = "integer_index_dir";
 constexpr std::string_view kQualifiedIdJoinIndexSubfolderName =
@@ -264,6 +267,10 @@ std::string MakeDocumentDirectoryPath(const std::string& base_dir) {
   return absl_ports::StrCat(base_dir, "/", kDocumentSubfolderName);
 }
 
+std::string MakeBlobDirectoryPath(const std::string& base_dir) {
+  return absl_ports::StrCat(base_dir, "/", kBlobSubfolderName);
+}
+
 // Makes a temporary folder path for the document store which will be used
 // during full optimization.
 std::string MakeDocumentTemporaryDirectoryPath(const std::string& base_dir) {
@@ -421,7 +428,7 @@ libtextclassifier3::StatusOr<bool> ScoringExpressionHasRelevanceScoreFunction(
   // to be called only once.
   Lexer lexer(scoring_expression, Lexer::Language::SCORING);
   ICING_ASSIGN_OR_RETURN(std::vector<Lexer::LexerToken> lexer_tokens,
-                         lexer.ExtractTokens());
+                         std::move(lexer).ExtractTokens());
   for (const Lexer::LexerToken& token : lexer_tokens) {
     if (token.type == Lexer::TokenType::FUNCTION_NAME &&
         token.text == RelevanceScoreFunctionScoreExpression::kFunctionName) {
@@ -499,14 +506,18 @@ InitializeResultProto IcingSearchEngine::Initialize() {
 }
 
 void IcingSearchEngine::ResetMembers() {
-  schema_store_.reset();
-  document_store_.reset();
-  language_segmenter_.reset();
-  normalizer_.reset();
-  index_.reset();
-  integer_index_.reset();
-  qualified_id_join_index_.reset();
+  // Reset all members in the reverse order of their initialization to ensure
+  // the dependencies are not violated.
   embedding_index_.reset();
+  qualified_id_join_index_.reset();
+  integer_index_.reset();
+  index_.reset();
+  normalizer_.reset();
+  language_segmenter_.reset();
+  blob_store_.reset();
+  result_state_manager_.reset();
+  document_store_.reset();
+  schema_store_.reset();
 }
 
 libtextclassifier3::Status IcingSearchEngine::CheckInitMarkerFile(
@@ -615,7 +626,6 @@ InitializeResultProto IcingSearchEngine::InternalInitialize() {
 libtextclassifier3::Status IcingSearchEngine::InitializeMembers(
     InitializeStatsProto* initialize_stats) {
   ICING_RETURN_ERROR_IF_NULL(initialize_stats);
-
   // Make sure the base directory exists
   if (!filesystem_->CreateDirectoryRecursively(options_.base_dir().c_str())) {
     return absl_ports::InternalError(absl_ports::StrCat(
@@ -699,17 +709,25 @@ libtextclassifier3::Status IcingSearchEngine::InitializeMembers(
         MakeQualifiedIdJoinIndexWorkingPath(options_.base_dir());
     const std::string embedding_index_dir =
         MakeEmbeddingIndexWorkingPath(options_.base_dir());
+    const std::string blob_store_dir =
+        MakeBlobDirectoryPath(options_.base_dir());
+
     if (!filesystem_->DeleteDirectoryRecursively(doc_store_dir.c_str()) ||
         !filesystem_->DeleteDirectoryRecursively(index_dir.c_str()) ||
         !IntegerIndex::Discard(*filesystem_, integer_index_dir).ok() ||
         !QualifiedIdJoinIndex::Discard(*filesystem_,
                                        qualified_id_join_index_dir)
              .ok() ||
-        !EmbeddingIndex::Discard(*filesystem_, embedding_index_dir).ok()) {
+        !EmbeddingIndex::Discard(*filesystem_, embedding_index_dir).ok() ||
+        !filesystem_->DeleteDirectoryRecursively(blob_store_dir.c_str())) {
       return absl_ports::InternalError(absl_ports::StrCat(
           "Could not delete directories: ", index_dir, ", ", integer_index_dir,
-          ", ", qualified_id_join_index_dir, ", ", embedding_index_dir, " and ",
-          doc_store_dir));
+          ", ", qualified_id_join_index_dir, ", ", embedding_index_dir, ", ",
+          blob_store_dir, " and ", doc_store_dir));
+    }
+    if (options_.enable_blob_store()) {
+      ICING_RETURN_IF_ERROR(
+          InitializeBlobStore(options_.orphan_blob_time_to_live_ms()));
     }
     ICING_ASSIGN_OR_RETURN(
         bool document_store_derived_files_regenerated,
@@ -728,6 +746,10 @@ libtextclassifier3::Status IcingSearchEngine::InitializeMembers(
     // Since we're going to rebuild all indices in this case, the return value
     // of InitializeDocumentStore (document_store_derived_files_regenerated) is
     // unused.
+    if (options_.enable_blob_store()) {
+      ICING_RETURN_IF_ERROR(
+          InitializeBlobStore(options_.orphan_blob_time_to_live_ms()));
+    }
     ICING_RETURN_IF_ERROR(InitializeDocumentStore(
         /*force_recovery_and_revalidate_documents=*/true, initialize_stats));
 
@@ -801,6 +823,10 @@ libtextclassifier3::Status IcingSearchEngine::InitializeMembers(
     initialize_stats->set_embedding_index_restoration_cause(
         InitializeStatsProto::SCHEMA_CHANGES_OUT_OF_SYNC);
   } else if (version_state_change != version_util::StateChange::kCompatible) {
+    if (options_.enable_blob_store()) {
+      ICING_RETURN_IF_ERROR(
+          InitializeBlobStore(options_.orphan_blob_time_to_live_ms()));
+    }
     ICING_ASSIGN_OR_RETURN(bool document_store_derived_files_regenerated,
                            InitializeDocumentStore(
                                /*force_recovery_and_revalidate_documents=*/true,
@@ -824,6 +850,10 @@ libtextclassifier3::Status IcingSearchEngine::InitializeMembers(
     initialize_stats->set_embedding_index_restoration_cause(
         InitializeStatsProto::VERSION_CHANGED);
   } else {
+    if (options_.enable_blob_store()) {
+      ICING_RETURN_IF_ERROR(
+          InitializeBlobStore(options_.orphan_blob_time_to_live_ms()));
+    }
     ICING_ASSIGN_OR_RETURN(
         bool document_store_derived_files_regenerated,
         InitializeDocumentStore(
@@ -911,10 +941,26 @@ libtextclassifier3::StatusOr<bool> IcingSearchEngine::InitializeDocumentStore(
           /*pre_mapping_fbv=*/false, /*use_persistent_hash_map=*/true,
           options_.compression_level(), initialize_stats));
   document_store_ = std::move(create_result.document_store);
-
   return create_result.derived_files_regenerated;
 }
 
+libtextclassifier3::Status IcingSearchEngine::InitializeBlobStore(
+    int32_t orphan_blob_time_to_live_ms) {
+  std::string blob_dir = MakeBlobDirectoryPath(options_.base_dir());
+  // Make sure the sub-directory exists
+  if (!filesystem_->CreateDirectoryRecursively(blob_dir.c_str())) {
+    return absl_ports::InternalError(
+        absl_ports::StrCat("Could not create directory: ", blob_dir));
+  }
+
+  ICING_ASSIGN_OR_RETURN(
+      auto blob_store_or,
+      BlobStore::Create(filesystem_.get(), blob_dir, clock_.get(),
+                        orphan_blob_time_to_live_ms));
+  blob_store_ = std::make_unique<BlobStore>(std::move(blob_store_or));
+  return libtextclassifier3::Status::OK;
+}
+
 libtextclassifier3::Status IcingSearchEngine::InitializeIndex(
     bool document_store_derived_files_regenerated,
     InitializeStatsProto* initialize_stats) {
@@ -1156,8 +1202,14 @@ SetSchemaResultProto IcingSearchEngine::SetSchema(
         std::move(index_incompatible_type));
   }
 
+  // Join index is incompatible and needs rebuild if:
+  // - Any schema type is join incompatible.
+  // - OR existing schema type id assignment has changed, since join index
+  //   stores schema type id (+ joinable property path) as a key to group join
+  //   data.
   bool join_incompatible =
-      !set_schema_result.schema_types_join_incompatible_by_name.empty();
+      !set_schema_result.schema_types_join_incompatible_by_name.empty() ||
+      !set_schema_result.old_schema_type_ids_changed.empty();
   for (const std::string& join_incompatible_type :
        set_schema_result.schema_types_join_incompatible_by_name) {
     result_proto.add_join_incompatible_changed_schema_types(
@@ -1303,14 +1355,15 @@ PutResultProto IcingSearchEngine::Put(DocumentProto&& document) {
   TokenizedDocument tokenized_document(
       std::move(tokenized_document_or).ValueOrDie());
 
-  auto document_id_or = document_store_->Put(
+  auto put_result_or = document_store_->Put(
       tokenized_document.document(), tokenized_document.num_string_tokens(),
       put_document_stats);
-  if (!document_id_or.ok()) {
-    TransformStatus(document_id_or.status(), result_status);
+  if (!put_result_or.ok()) {
+    TransformStatus(put_result_or.status(), result_status);
     return result_proto;
   }
-  DocumentId document_id = document_id_or.ValueOrDie();
+  DocumentId document_id = put_result_or.ValueOrDie().new_document_id;
+  result_proto.set_was_replacement(put_result_or.ValueOrDie().was_replacement);
 
   auto data_indexing_handlers_or = CreateDataIndexingHandlers();
   if (!data_indexing_handlers_or.ok()) {
@@ -1723,11 +1776,19 @@ OptimizeResultProto IcingSearchEngine::Optimize() {
   optimize_stats->set_storage_size_before(
       Filesystem::SanitizeFileSize(before_size));
 
+  // Get all expired blob handles
+  std::unordered_set<std::string> potentially_optimizable_blob_handles;
+  if (blob_store_ != nullptr) {
+    potentially_optimizable_blob_handles =
+        blob_store_->GetPotentiallyOptimizableBlobHandles();
+  }
+
   // TODO(b/143646633): figure out if we need to optimize index and doc store
   // at the same time.
   std::unique_ptr<Timer> optimize_doc_store_timer = clock_->GetNewTimer();
   libtextclassifier3::StatusOr<DocumentStore::OptimizeResult>
-      optimize_result_or = OptimizeDocumentStore(optimize_stats);
+      optimize_result_or = OptimizeDocumentStore(
+          std::move(potentially_optimizable_blob_handles), optimize_stats);
   optimize_stats->set_document_store_optimize_latency_ms(
       optimize_doc_store_timer->GetElapsedMilliseconds());
 
@@ -1741,10 +1802,22 @@ OptimizeResultProto IcingSearchEngine::Optimize() {
     return result_proto;
   }
 
+  libtextclassifier3::Status doc_store_optimize_result_status =
+      optimize_result_or.status();
+  if (blob_store_ != nullptr && doc_store_optimize_result_status.ok()) {
+    // optimize blob store
+    libtextclassifier3::Status blob_store_optimize_status =
+        blob_store_->Optimize(
+            optimize_result_or.ValueOrDie().dead_blob_handles);
+    if (!blob_store_optimize_status.ok()) {
+      TransformStatus(status, result_status);
+      return result_proto;
+    }
+  }
+
   // The status is either OK or DATA_LOSS. The optimized document store is
   // guaranteed to work, so we update index according to the new document store.
   std::unique_ptr<Timer> optimize_index_timer = clock_->GetNewTimer();
-  auto doc_store_optimize_result_status = optimize_result_or.status();
   bool should_rebuild_index =
       !optimize_result_or.ok() ||
       optimize_result_or.ValueOrDie().should_rebuild_index ||
@@ -1934,7 +2007,7 @@ GetOptimizeInfoResultProto IcingSearchEngine::GetOptimizeInfo() {
     return result_proto;
   }
   int64_t index_elements_size = index_elements_size_or.ValueOrDie();
-
+  // TODO(b/273591938): add stats for blob store
   // TODO(b/259744228): add stats for integer index
 
   // Sum up the optimizable sizes from DocumentStore and Index
@@ -2015,15 +2088,29 @@ DebugInfoResultProto IcingSearchEngine::GetDebugInfo(
 
 libtextclassifier3::Status IcingSearchEngine::InternalPersistToDisk(
     PersistType::Code persist_type) {
-  if (persist_type == PersistType::LITE) {
-    return document_store_->PersistToDisk(persist_type);
+  if (blob_store_ != nullptr) {
+    // For all valid PersistTypes, we persist the ground truth. The ground truth
+    // in the schema_store is always persisted immediately after changes are
+    // applied. So there is nothing to do if persist_type is LITE.
+    ICING_RETURN_IF_ERROR(blob_store_->PersistToDisk());
+  }
+  ICING_RETURN_IF_ERROR(document_store_->PersistToDisk(persist_type));
+  if (persist_type == PersistType::RECOVERY_PROOF) {
+    // Persist RECOVERY_PROOF will persist the ground truth and then update all
+    // checksums. There is no need to call document_store_->UpdateChecksum()
+    // because PersistToDisk(RECOVERY_PROOF) will update the checksum anyways.
+    ICING_RETURN_IF_ERROR(schema_store_->UpdateChecksum());
+    index_->UpdateChecksum();
+    ICING_RETURN_IF_ERROR(integer_index_->UpdateChecksums());
+    ICING_RETURN_IF_ERROR(qualified_id_join_index_->UpdateChecksums());
+    ICING_RETURN_IF_ERROR(embedding_index_->UpdateChecksums());
+  } else if (persist_type == PersistType::FULL) {
+    ICING_RETURN_IF_ERROR(schema_store_->PersistToDisk());
+    ICING_RETURN_IF_ERROR(index_->PersistToDisk());
+    ICING_RETURN_IF_ERROR(integer_index_->PersistToDisk());
+    ICING_RETURN_IF_ERROR(qualified_id_join_index_->PersistToDisk());
+    ICING_RETURN_IF_ERROR(embedding_index_->PersistToDisk());
   }
-  ICING_RETURN_IF_ERROR(schema_store_->PersistToDisk());
-  ICING_RETURN_IF_ERROR(document_store_->PersistToDisk(PersistType::FULL));
-  ICING_RETURN_IF_ERROR(index_->PersistToDisk());
-  ICING_RETURN_IF_ERROR(integer_index_->PersistToDisk());
-  ICING_RETURN_IF_ERROR(qualified_id_join_index_->PersistToDisk());
-  ICING_RETURN_IF_ERROR(embedding_index_->PersistToDisk());
 
   return libtextclassifier3::Status::OK;
 }
@@ -2462,8 +2549,95 @@ void IcingSearchEngine::InvalidateNextPageToken(uint64_t next_page_token) {
   result_state_manager_->InvalidateResultState(next_page_token);
 }
 
+BlobProto IcingSearchEngine::OpenWriteBlob(
+    PropertyProto::BlobHandleProto blob_handle) {
+  BlobProto blob_proto;
+  StatusProto* status = blob_proto.mutable_status();
+
+  absl_ports::unique_lock l(&mutex_);
+  if (blob_store_ == nullptr) {
+    status->set_code(StatusProto::FAILED_PRECONDITION);
+    status->set_message(
+        "Open write blob is not supported in this Icing instance!");
+    return blob_proto;
+  }
+
+  if (!initialized_) {
+    status->set_code(StatusProto::FAILED_PRECONDITION);
+    status->set_message("IcingSearchEngine has not been initialized!");
+    return blob_proto;
+  }
+
+  auto write_fd_or = blob_store_->OpenWrite(blob_handle);
+  if (!write_fd_or.ok()) {
+    TransformStatus(write_fd_or.status(), status);
+    return blob_proto;
+  }
+  blob_proto.set_file_descriptor(write_fd_or.ValueOrDie());
+  status->set_code(StatusProto::OK);
+  return blob_proto;
+}
+
+BlobProto IcingSearchEngine::OpenReadBlob(
+    PropertyProto::BlobHandleProto blob_handle) {
+  BlobProto blob_proto;
+  StatusProto* status = blob_proto.mutable_status();
+  absl_ports::shared_lock l(&mutex_);
+  if (blob_store_ == nullptr) {
+    status->set_code(StatusProto::FAILED_PRECONDITION);
+    status->set_message(
+        "Open read blob is not supported in this Icing instance!");
+    return blob_proto;
+  }
+
+  if (!initialized_) {
+    status->set_code(StatusProto::FAILED_PRECONDITION);
+    status->set_message("IcingSearchEngine has not been initialized!");
+    ICING_LOG(ERROR) << status->message();
+    return blob_proto;
+  }
+
+  auto read_fd_or = blob_store_->OpenRead(blob_handle);
+  if (!read_fd_or.ok()) {
+    TransformStatus(read_fd_or.status(), status);
+    return blob_proto;
+  }
+  blob_proto.set_file_descriptor(read_fd_or.ValueOrDie());
+  status->set_code(StatusProto::OK);
+  return blob_proto;
+}
+
+BlobProto IcingSearchEngine::CommitBlob(
+    PropertyProto::BlobHandleProto blob_handle) {
+  BlobProto blob_proto;
+  StatusProto* status = blob_proto.mutable_status();
+  absl_ports::unique_lock l(&mutex_);
+  if (blob_store_ == nullptr) {
+    status->set_code(StatusProto::FAILED_PRECONDITION);
+    status->set_message("Commit blob is not supported in this Icing instance!");
+    return blob_proto;
+  }
+
+  if (!initialized_) {
+    status->set_code(StatusProto::FAILED_PRECONDITION);
+    status->set_message("IcingSearchEngine has not been initialized!");
+    ICING_LOG(ERROR) << status->message();
+    return blob_proto;
+  }
+
+  auto commit_result_or = blob_store_->CommitBlob(blob_handle);
+  if (!commit_result_or.ok()) {
+    TransformStatus(commit_result_or, status);
+    return blob_proto;
+  }
+  status->set_code(StatusProto::OK);
+  return blob_proto;
+}
+
 libtextclassifier3::StatusOr<DocumentStore::OptimizeResult>
-IcingSearchEngine::OptimizeDocumentStore(OptimizeStatsProto* optimize_stats) {
+IcingSearchEngine::OptimizeDocumentStore(
+    std::unordered_set<std::string>&& potentially_optimizable_blob_handles,
+    OptimizeStatsProto* optimize_stats) {
   // Gets the current directory path and an empty tmp directory path for
   // document store optimization.
   const std::string current_document_dir =
@@ -2481,7 +2655,8 @@ IcingSearchEngine::OptimizeDocumentStore(OptimizeStatsProto* optimize_stats) {
   // Copies valid document data to tmp directory
   libtextclassifier3::StatusOr<DocumentStore::OptimizeResult>
       optimize_result_or = document_store_->OptimizeInto(
-          temporary_document_dir, language_segmenter_.get(), optimize_stats);
+          temporary_document_dir, language_segmenter_.get(),
+          std::move(potentially_optimizable_blob_handles), optimize_stats);
 
   // Handles error if any
   if (!optimize_result_or.ok()) {
diff --git a/icing/icing-search-engine.h b/icing/icing-search-engine.h
index 57f0f28..299dc6d 100644
--- a/icing/icing-search-engine.h
+++ b/icing/icing-search-engine.h
@@ -17,7 +17,9 @@
 
 #include <cstdint>
 #include <memory>
+#include <string>
 #include <string_view>
+#include <unordered_set>
 #include <utility>
 #include <vector>
 
@@ -36,6 +38,7 @@
 #include "icing/join/qualified-id-join-index.h"
 #include "icing/legacy/index/icing-filesystem.h"
 #include "icing/performance-configuration.h"
+#include "icing/proto/blob.pb.h"
 #include "icing/proto/debug.pb.h"
 #include "icing/proto/document.pb.h"
 #include "icing/proto/initialize.pb.h"
@@ -52,6 +55,7 @@
 #include "icing/result/result-state-manager.h"
 #include "icing/schema/schema-store.h"
 #include "icing/scoring/scored-document-hit.h"
+#include "icing/store/blob-store.h"
 #include "icing/store/document-id.h"
 #include "icing/store/document-store.h"
 #include "icing/tokenization/language-segmenter.h"
@@ -346,6 +350,39 @@ class IcingSearchEngine {
   void InvalidateNextPageToken(uint64_t next_page_token)
       ICING_LOCKS_EXCLUDED(mutex_);
 
+  // Gets or creates a file for write only purpose for the given blob handle.
+  // To mark the blob is completed written, commitBlob must be called. Once
+  // commitBlob is called, the blob is sealed and rewrite is not allowed.
+  //
+  // Returns:
+  //   File descriptor on success
+  //   InvalidArgumentError on invalid blob handle
+  //   PermissionDeniedError on blob is committed
+  //   INTERNAL_ERROR on IO error
+  BlobProto OpenWriteBlob(PropertyProto::BlobHandleProto blob_handle);
+
+  // Gets or creates a file for read only purpose for the given blob handle.
+  // The blob must be committed by calling commitBlob otherwise it is not
+  // accessible.
+  //
+  // Returns:
+  //   File descriptor on success
+  //   InvalidArgumentError on invalid blob handle
+  //   NotFoundError on blob is not found or is not committed
+  BlobProto OpenReadBlob(PropertyProto::BlobHandleProto blob_handle);
+
+  // Commits the given blob, the blob is open to write via openWrite.
+  // Before the blob is committed, it is not visible to any reader via openRead.
+  // After the blob is committed, it is not allowed to rewrite or update the
+  // content.
+  //
+  // Returns:
+  //   True on the blob is successfuly committed.
+  //   False on the blob is already committed.
+  //   InvalidArgumentError on invalid blob handle or digest is mismatch with
+  //     file content NotFoundError on blob is not found.
+  BlobProto CommitBlob(PropertyProto::BlobHandleProto blob_handle);
+
   // Makes sure that every update/delete received till this point is flushed
   // to disk. If the app crashes after a call to PersistToDisk(), Icing
   // would be able to fully recover all data written up to this point.
@@ -451,14 +488,6 @@ class IcingSearchEngine {
   // components in Icing search engine.
   const PerformanceConfiguration performance_configuration_;
 
-  // Used to manage pagination state of query results. Even though
-  // ResultStateManager has its own reader-writer lock, mutex_ must still be
-  // acquired first in order to adhere to the global lock ordering:
-  //   1. mutex_
-  //   2. result_state_manager_.lock_
-  std::unique_ptr<ResultStateManager> result_state_manager_
-      ICING_GUARDED_BY(mutex_);
-
   // Used to provide reader and writer locks
   absl_ports::shared_mutex mutex_;
 
@@ -466,8 +495,23 @@ class IcingSearchEngine {
   std::unique_ptr<SchemaStore> schema_store_ ICING_GUARDED_BY(mutex_);
 
   // Used to store all valid documents
+  //
+  // Dependencies: schema_store_
   std::unique_ptr<DocumentStore> document_store_ ICING_GUARDED_BY(mutex_);
 
+  // Used to manage pagination state of query results. Even though
+  // ResultStateManager has its own reader-writer lock, mutex_ must still be
+  // acquired first in order to adhere to the global lock ordering:
+  //   1. mutex_
+  //   2. result_state_manager_.lock_
+  //
+  // Dependencies: document_store_
+  std::unique_ptr<ResultStateManager> result_state_manager_
+      ICING_GUARDED_BY(mutex_);
+
+  // Used to store all valid blob data
+  std::unique_ptr<BlobStore> blob_store_ ICING_GUARDED_BY(mutex_);
+
   std::unique_ptr<const LanguageSegmenter> language_segmenter_
       ICING_GUARDED_BY(mutex_);
 
@@ -561,6 +605,15 @@ class IcingSearchEngine {
       InitializeStatsProto* initialize_stats)
       ICING_EXCLUSIVE_LOCKS_REQUIRED(mutex_);
 
+  // Do any initialization necessary to create a BlobStore instance.
+  //
+  // Returns:
+  //   OK on success
+  //   FAILED_PRECONDITION if initialize_stats is null
+  libtextclassifier3::Status InitializeBlobStore(
+      int32_t orphan_blob_time_to_live_ms)
+      ICING_EXCLUSIVE_LOCKS_REQUIRED(mutex_);
+
   // Do any initialization/recovery necessary to create term index, integer
   // index, and qualified id join index instances.
   //
@@ -683,7 +736,8 @@ class IcingSearchEngine {
   //   INTERNAL_ERROR on any IO errors or other errors that we can't recover
   //                  from
   libtextclassifier3::StatusOr<DocumentStore::OptimizeResult>
-  OptimizeDocumentStore(OptimizeStatsProto* optimize_stats)
+  OptimizeDocumentStore(std::unordered_set<std::string>&& mature_blob_handles,
+                        OptimizeStatsProto* optimize_stats)
       ICING_EXCLUSIVE_LOCKS_REQUIRED(mutex_);
 
   // Helper method to restore missing document data in index_, integer_index_,
diff --git a/icing/icing-search-engine_blob_test.cc b/icing/icing-search-engine_blob_test.cc
new file mode 100644
index 0000000..f32d6d1
--- /dev/null
+++ b/icing/icing-search-engine_blob_test.cc
@@ -0,0 +1,1038 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include <array>
+#include <cstddef>
+#include <cstdint>
+#include <memory>
+#include <random>
+#include <string>
+#include <unordered_set>
+#include <utility>
+#include <vector>
+
+#include "gtest/gtest.h"
+#include "icing/document-builder.h"
+#include "icing/file/filesystem.h"
+#include "icing/icing-search-engine.h"
+#include "icing/jni/jni-cache.h"
+#include "icing/legacy/index/icing-filesystem.h"
+#include "icing/portable/equals-proto.h"
+#include "icing/schema-builder.h"
+#include "icing/testing/common-matchers.h"
+#include "icing/testing/fake-clock.h"
+#include "icing/testing/jni-test-helpers.h"
+#include "icing/testing/tmp-directory.h"
+#include "icing/util/clock.h"
+#include "icing/util/sha256.h"
+
+namespace icing {
+namespace lib {
+
+static constexpr int64_t kBlobInfoTTLMs = 7 * 24 * 60 * 60 * 1000;  // 1 Week
+
+namespace {
+
+using ::testing::Eq;
+
+// For mocking purpose, we allow tests to provide a custom Filesystem.
+class TestIcingSearchEngine : public IcingSearchEngine {
+ public:
+  TestIcingSearchEngine(const IcingSearchEngineOptions& options,
+                        std::unique_ptr<const Filesystem> filesystem,
+                        std::unique_ptr<const IcingFilesystem> icing_filesystem,
+                        std::unique_ptr<Clock> clock,
+                        std::unique_ptr<JniCache> jni_cache)
+      : IcingSearchEngine(options, std::move(filesystem),
+                          std::move(icing_filesystem), std::move(clock),
+                          std::move(jni_cache)) {}
+};
+
+std::string GetTestBaseDir() { return GetTestTempDir() + "/icing"; }
+std::string GetTestBaseBlobStoreDir() {
+  return GetTestTempDir() + "/icing/blob_dir";
+}
+
+// This test is meant to cover all tests relating to IcingSearchEngine::Delete*.
+class IcingSearchEngineBlobTest : public testing::Test {
+ protected:
+  void SetUp() override {
+    filesystem_.CreateDirectoryRecursively(GetTestBaseDir().c_str());
+  }
+
+  void TearDown() override {
+    filesystem_.DeleteDirectoryRecursively(GetTestBaseDir().c_str());
+  }
+
+  const Filesystem* filesystem() const { return &filesystem_; }
+
+ private:
+  Filesystem filesystem_;
+};
+
+// Non-zero value so we don't override it to be the current time
+constexpr int64_t kDefaultCreationTimestampMs = 1575492852000;
+
+IcingSearchEngineOptions GetDefaultIcingOptions() {
+  IcingSearchEngineOptions icing_options;
+  icing_options.set_base_dir(GetTestBaseDir());
+  icing_options.set_enable_blob_store(true);
+  icing_options.set_orphan_blob_time_to_live_ms(kBlobInfoTTLMs);
+  return icing_options;
+}
+
+std::vector<unsigned char> GenerateRandomBytes(size_t length) {
+  std::random_device rd;
+  std::mt19937 gen(rd());
+  std::uniform_int_distribution<unsigned char> distribution(0, 255);
+  std::vector<unsigned char> random_bytes(length);
+  for (size_t i = 0; i < length; ++i) {
+    random_bytes[i] = distribution(gen);
+  }
+  return random_bytes;
+}
+
+std::array<uint8_t, 32> CalculateDigest(
+    const std::vector<unsigned char>& data) {
+  Sha256 sha256;
+  sha256.Update(data.data(), data.size());
+  std::array<uint8_t, 32> hash = std::move(sha256).Finalize();
+  return hash;
+}
+
+SchemaProto CreateBlobSchema() {
+  return SchemaBuilder()
+      .AddType(SchemaTypeConfigBuilder()
+                   .SetType("BlobType")
+                   .AddProperty(PropertyConfigBuilder()
+                                    .SetName("blob")
+                                    .SetDataType(TYPE_BLOB_HANDLE)
+                                    .SetCardinality(CARDINALITY_REQUIRED)))
+      .Build();
+}
+
+DocumentProto CreateBlobDocument(std::string name_space, std::string uri,
+                                 PropertyProto::BlobHandleProto blob_handle) {
+  return DocumentBuilder()
+      .SetKey(std::move(name_space), std::move(uri))
+      .SetSchema("BlobType")
+      .AddBlobHandleProperty("blob", blob_handle)
+      .SetCreationTimestampMs(kDefaultCreationTimestampMs)
+      .Build();
+}
+
+TEST_F(IcingSearchEngineBlobTest, InvalidBlobHandle) {
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_label("blob");
+  blob_handle.set_digest("invalid");
+
+  IcingSearchEngine icing(GetDefaultIcingOptions(), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+
+  BlobProto write_blob_proto = icing.OpenWriteBlob(blob_handle);
+  EXPECT_THAT(write_blob_proto.status(),
+              ProtoStatusIs(StatusProto::INVALID_ARGUMENT));
+  BlobProto commit_blob_proto = icing.CommitBlob(blob_handle);
+  EXPECT_THAT(commit_blob_proto.status(),
+              ProtoStatusIs(StatusProto::INVALID_ARGUMENT));
+  BlobProto read_blob_proto = icing.OpenReadBlob(blob_handle);
+  EXPECT_THAT(read_blob_proto.status(),
+              ProtoStatusIs(StatusProto::INVALID_ARGUMENT));
+}
+
+TEST_F(IcingSearchEngineBlobTest, BlobStoreDisabled) {
+  IcingSearchEngineOptions icing_options;
+  icing_options.set_base_dir(GetTestBaseDir());
+  icing_options.set_enable_blob_store(false);
+
+  IcingSearchEngine icing(icing_options, GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_label("blob");
+  std::vector<unsigned char> data = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest = CalculateDigest(data);
+  blob_handle.set_digest((void*)digest.data(), digest.size());
+
+  BlobProto write_blob_proto = icing.OpenWriteBlob(blob_handle);
+  EXPECT_THAT(write_blob_proto.status(),
+              ProtoStatusIs(StatusProto::FAILED_PRECONDITION));
+  BlobProto commit_blob_proto = icing.CommitBlob(blob_handle);
+  EXPECT_THAT(commit_blob_proto.status(),
+              ProtoStatusIs(StatusProto::FAILED_PRECONDITION));
+  BlobProto read_blob_proto = icing.OpenReadBlob(blob_handle);
+  EXPECT_THAT(read_blob_proto.status(),
+              ProtoStatusIs(StatusProto::FAILED_PRECONDITION));
+}
+
+TEST_F(IcingSearchEngineBlobTest, WriteAndReadBlob) {
+  IcingSearchEngine icing(GetDefaultIcingOptions(), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_label("label");
+  std::vector<unsigned char> data = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest = CalculateDigest(data);
+  blob_handle.set_digest((void*)digest.data(), digest.size());
+
+  BlobProto write_blob_proto = icing.OpenWriteBlob(blob_handle);
+  ASSERT_THAT(write_blob_proto.status(), ProtoIsOk());
+  {
+    ScopedFd write_fd(write_blob_proto.file_descriptor());
+    ASSERT_TRUE(filesystem()->Write(write_fd.get(), data.data(), data.size()));
+  }
+
+  BlobProto commit_blob_proto = icing.CommitBlob(blob_handle);
+  ASSERT_THAT(commit_blob_proto.status(), ProtoIsOk());
+
+  BlobProto read_blob_proto = icing.OpenReadBlob(blob_handle);
+  ASSERT_THAT(read_blob_proto.status(), ProtoIsOk());
+  {
+    ScopedFd read_fd(read_blob_proto.file_descriptor());
+
+    uint64_t size = filesystem()->GetFileSize(*read_fd);
+    std::unique_ptr<unsigned char[]> buf =
+        std::make_unique<unsigned char[]>(size);
+    EXPECT_TRUE(filesystem()->Read(read_fd.get(), buf.get(), size));
+    std::string expected_data = std::string(data.begin(), data.end());
+    std::string actual_data = std::string(buf.get(), buf.get() + size);
+    EXPECT_EQ(expected_data, actual_data);
+  }
+}
+
+TEST_F(IcingSearchEngineBlobTest, WriteAndReadBlobByDocument) {
+  IcingSearchEngine icing(GetDefaultIcingOptions(), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_label("label");
+  std::vector<unsigned char> data = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest = CalculateDigest(data);
+  blob_handle.set_digest((void*)digest.data(), digest.size());
+
+  BlobProto write_blob_proto = icing.OpenWriteBlob(blob_handle);
+  ASSERT_THAT(write_blob_proto.status(), ProtoIsOk());
+
+  {
+    ScopedFd write_fd(write_blob_proto.file_descriptor());
+    ASSERT_TRUE(filesystem()->Write(write_fd.get(), data.data(), data.size()));
+  }
+
+  BlobProto commit_blob_proto = icing.CommitBlob(blob_handle);
+  ASSERT_THAT(commit_blob_proto.status(), ProtoIsOk());
+
+  // Set schema and put a document that contains the blob handle
+  ASSERT_THAT(icing.SetSchema(CreateBlobSchema()).status(), ProtoIsOk());
+  ASSERT_THAT(
+      icing.Put(CreateBlobDocument("namespace", "doc1", blob_handle)).status(),
+      ProtoIsOk());
+
+  // Read the document and its blob handle property.
+  GetResultProto get_result =
+      icing.Get("namespace", "doc1", GetResultSpecProto::default_instance());
+  EXPECT_THAT(get_result.status(), ProtoIsOk());
+  PropertyProto::BlobHandleProto out_blob_handle =
+      get_result.document().properties().at(0).blob_handle_values().at(0);
+
+  // use the output blob handle to read blob data.
+  BlobProto read_blob_proto = icing.OpenReadBlob(out_blob_handle);
+  ASSERT_THAT(read_blob_proto.status(), ProtoIsOk());
+  {
+    ScopedFd read_fd(read_blob_proto.file_descriptor());
+
+    uint64_t size = filesystem()->GetFileSize(*read_fd);
+    std::unique_ptr<uint8_t[]> buf = std::make_unique<uint8_t[]>(size);
+    EXPECT_TRUE(filesystem()->Read(read_fd.get(), buf.get(), size));
+
+    std::string expected_data = std::string(data.begin(), data.end());
+    std::string actual_data = std::string(buf.get(), buf.get() + size);
+    EXPECT_EQ(expected_data, actual_data);
+  }
+}
+
+TEST_F(IcingSearchEngineBlobTest, CommitDigestMisMatch) {
+  IcingSearchEngine icing(GetDefaultIcingOptions(), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_label("blob1");
+
+  std::vector<unsigned char> data = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest = CalculateDigest(data);
+  blob_handle.set_digest(std::string(digest.begin(), digest.end()));
+
+  BlobProto write_blob_proto = icing.OpenWriteBlob(blob_handle);
+  ASSERT_THAT(write_blob_proto.status(), ProtoIsOk());
+
+  std::vector<unsigned char> data2 = GenerateRandomBytes(24);
+  {
+    ScopedFd write_fd(write_blob_proto.file_descriptor());
+    ASSERT_TRUE(
+        filesystem()->Write(write_fd.get(), data2.data(), data2.size()));
+  }
+
+  BlobProto commit_blob_proto = icing.CommitBlob(blob_handle);
+  ASSERT_THAT(commit_blob_proto.status(),
+              ProtoStatusIs(StatusProto::INVALID_ARGUMENT));
+}
+
+TEST_F(IcingSearchEngineBlobTest, ReadBlobWithoutPersistToDisk) {
+  IcingSearchEngine icing1(GetDefaultIcingOptions(), GetTestJniCache());
+  EXPECT_THAT(icing1.Initialize().status(), ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_label("blob1");
+
+  std::vector<unsigned char> data = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest = CalculateDigest(data);
+  blob_handle.set_digest((void*)digest.data(), digest.size());
+
+  BlobProto write_blob_proto = icing1.OpenWriteBlob(blob_handle);
+  ASSERT_THAT(write_blob_proto.status(), ProtoIsOk());
+
+  {
+    ScopedFd write_fd(write_blob_proto.file_descriptor());
+    ASSERT_TRUE(filesystem()->Write(write_fd.get(), data.data(), data.size()));
+  }
+
+  BlobProto commit_blob_proto = icing1.CommitBlob(blob_handle);
+  ASSERT_THAT(commit_blob_proto.status(), ProtoIsOk());
+
+  // Recreate icing, the blob info will be dropped since we haven't called
+  // persistToDisk.
+  IcingSearchEngine icing2(GetDefaultIcingOptions(), GetTestJniCache());
+  EXPECT_THAT(icing2.Initialize().status(), ProtoIsOk());
+
+  BlobProto read_blob_proto = icing2.OpenReadBlob(blob_handle);
+  EXPECT_THAT(read_blob_proto.status(), ProtoStatusIs(StatusProto::NOT_FOUND));
+}
+
+TEST_F(IcingSearchEngineBlobTest, ReadBlobWithPersistToDiskFull) {
+  IcingSearchEngine icing1(GetDefaultIcingOptions(), GetTestJniCache());
+  EXPECT_THAT(icing1.Initialize().status(), ProtoIsOk());
+  // set a schema to icing to avoid wipe out all directories.
+  ASSERT_THAT(icing1.SetSchema(CreateBlobSchema()).status(), ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_label("blob1");
+
+  std::vector<unsigned char> data = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest = CalculateDigest(data);
+  blob_handle.set_digest((void*)digest.data(), digest.size());
+
+  BlobProto write_blob_proto = icing1.OpenWriteBlob(blob_handle);
+  ASSERT_THAT(write_blob_proto.status(), ProtoIsOk());
+  {
+    ScopedFd write_fd(write_blob_proto.file_descriptor());
+    ASSERT_TRUE(filesystem()->Write(write_fd.get(), data.data(), data.size()));
+  }
+  BlobProto commit_blob_proto = icing1.CommitBlob(blob_handle);
+  ASSERT_THAT(commit_blob_proto.status(), ProtoIsOk());
+
+  EXPECT_THAT(icing1.PersistToDisk(PersistType::FULL).status(), ProtoIsOk());
+
+  // Recreate icing, the blob info will be dropped since we haven't called
+  // persistToDisk.
+  IcingSearchEngine icing2(GetDefaultIcingOptions(), GetTestJniCache());
+  ASSERT_THAT(icing2.Initialize().status(), ProtoIsOk());
+
+  BlobProto read_blob_proto = icing2.OpenReadBlob(blob_handle);
+  ASSERT_THAT(read_blob_proto.status(), ProtoIsOk());
+  {
+    ScopedFd read_fd(read_blob_proto.file_descriptor());
+    uint64_t size = filesystem()->GetFileSize(*read_fd);
+    std::unique_ptr<uint8_t[]> buf = std::make_unique<uint8_t[]>(size);
+    EXPECT_TRUE(filesystem()->Read(read_fd.get(), buf.get(), size));
+    std::string expected_data = std::string(data.begin(), data.end());
+    std::string actual_data = std::string(buf.get(), buf.get() + size);
+    EXPECT_EQ(expected_data, actual_data);
+  }
+}
+
+TEST_F(IcingSearchEngineBlobTest, ReadBlobWithPersistToDiskLite) {
+  IcingSearchEngine icing1(GetDefaultIcingOptions(), GetTestJniCache());
+  EXPECT_THAT(icing1.Initialize().status(), ProtoIsOk());
+  // set a schema to icing to avoid wipe out all directories.
+  ASSERT_THAT(icing1.SetSchema(CreateBlobSchema()).status(), ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_label("blob1");
+
+  std::vector<unsigned char> data = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest = CalculateDigest(data);
+  blob_handle.set_digest((void*)digest.data(), digest.size());
+
+  BlobProto write_blob_proto = icing1.OpenWriteBlob(blob_handle);
+  ASSERT_THAT(write_blob_proto.status(), ProtoIsOk());
+
+  {
+    ScopedFd write_fd(write_blob_proto.file_descriptor());
+    ASSERT_TRUE(filesystem()->Write(write_fd.get(), data.data(), data.size()));
+  }
+
+  BlobProto commit_blob_proto = icing1.CommitBlob(blob_handle);
+  ASSERT_THAT(commit_blob_proto.status(), ProtoIsOk());
+
+  EXPECT_THAT(icing1.PersistToDisk(PersistType::LITE).status(), ProtoIsOk());
+
+  // Recreate icing, the blob info will be remained since we called
+  // persistToDisk.
+  IcingSearchEngine icing2(GetDefaultIcingOptions(), GetTestJniCache());
+  ASSERT_THAT(icing2.Initialize().status(), ProtoIsOk());
+
+  BlobProto read_blob_proto = icing2.OpenReadBlob(blob_handle);
+  ASSERT_THAT(read_blob_proto.status(), ProtoIsOk());
+  {
+    ScopedFd read_fd(read_blob_proto.file_descriptor());
+    uint64_t size = filesystem()->GetFileSize(*read_fd);
+    std::unique_ptr<uint8_t[]> buf = std::make_unique<uint8_t[]>(size);
+    EXPECT_TRUE(filesystem()->Read(read_fd.get(), buf.get(), size));
+    std::string expected_data = std::string(data.begin(), data.end());
+    std::string actual_data = std::string(buf.get(), buf.get() + size);
+    EXPECT_EQ(expected_data, actual_data);
+  }
+}
+
+TEST_F(IcingSearchEngineBlobTest, BlobOptimize) {
+  auto fake_clock = std::make_unique<FakeClock>();
+  fake_clock->SetSystemTimeMilliseconds(1000);
+  TestIcingSearchEngine icing(GetDefaultIcingOptions(),
+                              std::make_unique<Filesystem>(),
+                              std::make_unique<IcingFilesystem>(),
+                              std::move(fake_clock), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+
+  // set a schema to icing to avoid wipe out all directories.
+  ASSERT_THAT(icing.SetSchema(CreateBlobSchema()).status(), ProtoIsOk());
+
+  std::vector<std::string> file_names;
+  std::unordered_set<std::string> excludes;
+  ASSERT_TRUE(filesystem()->ListDirectory(GetTestBaseBlobStoreDir().c_str(),
+                                          excludes, /*recursive=*/false,
+                                          &file_names));
+  int32_t file_count = file_names.size();
+
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_label("label");
+  std::vector<unsigned char> data = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest = CalculateDigest(data);
+  std::string digestString = std::string(digest.begin(), digest.end());
+  blob_handle.set_digest(std::move(digestString));
+
+  BlobProto writeBlobProto = icing.OpenWriteBlob(blob_handle);
+  ASSERT_THAT(writeBlobProto.status(), ProtoIsOk());
+  {
+    ScopedFd write_fd(writeBlobProto.file_descriptor());
+    ASSERT_TRUE(filesystem()->Write(write_fd.get(), data.data(), data.size()));
+  }
+
+  file_names = std::vector<std::string>();
+  ASSERT_TRUE(filesystem()->ListDirectory(GetTestBaseBlobStoreDir().c_str(),
+                                          excludes, /*recursive=*/false,
+                                          &file_names));
+  ASSERT_THAT(file_names.size(), file_count + 1);
+
+  BlobProto commitBlobProto = icing.CommitBlob(blob_handle);
+  ASSERT_THAT(commitBlobProto.status(), ProtoIsOk());
+
+  // persist blob to disk
+  EXPECT_THAT(icing.PersistToDisk(PersistType::FULL).status(), ProtoIsOk());
+
+  // create second icing in 8 days later
+  auto fake_clock2 = std::make_unique<FakeClock>();
+  fake_clock2->SetSystemTimeMilliseconds(1000 + 8 * 24 * 60 * 60 *
+                                                    1000);  // pass 8 days
+  TestIcingSearchEngine icing2(GetDefaultIcingOptions(),
+                               std::make_unique<Filesystem>(),
+                               std::make_unique<IcingFilesystem>(),
+                               std::move(fake_clock2), GetTestJniCache());
+  ASSERT_THAT(icing2.Initialize().status(), ProtoIsOk());
+
+  // Blob remain before optimize
+  BlobProto readBlobProto = icing2.OpenReadBlob(blob_handle);
+  ASSERT_THAT(readBlobProto.status(), ProtoIsOk());
+  ScopedFd read_fd(readBlobProto.file_descriptor());
+
+  uint64_t size = filesystem()->GetFileSize(*read_fd);
+  std::unique_ptr<uint8_t[]> buf = std::make_unique<uint8_t[]>(size);
+  filesystem()->Read(read_fd.get(), buf.get(), size);
+  close(read_fd.get());
+
+  std::string expected_data = std::string(data.begin(), data.end());
+  std::string actual_data = std::string(buf.get(), buf.get() + size);
+  EXPECT_EQ(expected_data, actual_data);
+
+  // Optimize remove the expired orphan blob.
+  ASSERT_THAT(icing2.Optimize().status(), ProtoIsOk());
+  EXPECT_THAT(icing2.OpenReadBlob(blob_handle).status(),
+              ProtoStatusIs(StatusProto::NOT_FOUND));
+  file_names = std::vector<std::string>();
+  ASSERT_TRUE(filesystem()->ListDirectory(GetTestBaseBlobStoreDir().c_str(),
+                                          excludes, /*recursive=*/false,
+                                          &file_names));
+  ASSERT_THAT(file_names.size(), file_count);
+}
+
+TEST_F(IcingSearchEngineBlobTest, BlobOptimizeWithoutCommit) {
+  auto fake_clock = std::make_unique<FakeClock>();
+  fake_clock->SetSystemTimeMilliseconds(1000);
+  TestIcingSearchEngine icing(GetDefaultIcingOptions(),
+                              std::make_unique<Filesystem>(),
+                              std::make_unique<IcingFilesystem>(),
+                              std::move(fake_clock), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+
+  // set a schema to icing to avoid wipe out all directories.
+  ASSERT_THAT(icing.SetSchema(CreateBlobSchema()).status(), ProtoIsOk());
+
+  // write two blobs but not commit
+  PropertyProto::BlobHandleProto blob_handle1;
+  blob_handle1.set_label("label1");
+  std::vector<unsigned char> data1 = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest1 = CalculateDigest(data1);
+  std::string digestString1 = std::string(digest1.begin(), digest1.end());
+  blob_handle1.set_digest(std::move(digestString1));
+  BlobProto writeBlobProto = icing.OpenWriteBlob(blob_handle1);
+  ASSERT_THAT(writeBlobProto.status(), ProtoIsOk());
+  {
+    ScopedFd write_fd(writeBlobProto.file_descriptor());
+    ASSERT_TRUE(
+        filesystem()->Write(write_fd.get(), data1.data(), data1.size()));
+  }
+
+  PropertyProto::BlobHandleProto blob_handle2;
+  blob_handle2.set_label("label2");
+  std::vector<unsigned char> data2 = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest2 = CalculateDigest(data2);
+  std::string digestString2 = std::string(digest2.begin(), digest2.end());
+  blob_handle2.set_digest(std::move(digestString2));
+  writeBlobProto = icing.OpenWriteBlob(blob_handle2);
+  ASSERT_THAT(writeBlobProto.status(), ProtoIsOk());
+  {
+    ScopedFd write_fd(writeBlobProto.file_descriptor());
+    ASSERT_TRUE(
+        filesystem()->Write(write_fd.get(), data2.data(), data2.size()));
+  }
+  // persist blob to disk
+  EXPECT_THAT(icing.PersistToDisk(PersistType::FULL).status(), ProtoIsOk());
+
+  // create second icing in 8 days later
+  auto fake_clock2 = std::make_unique<FakeClock>();
+  fake_clock2->SetSystemTimeMilliseconds(1000 + 8 * 24 * 60 * 60 *
+                                                    1000);  // pass 8 days
+  TestIcingSearchEngine icing2(GetDefaultIcingOptions(),
+                               std::make_unique<Filesystem>(),
+                               std::make_unique<IcingFilesystem>(),
+                               std::move(fake_clock2), GetTestJniCache());
+  ASSERT_THAT(icing2.Initialize().status(), ProtoIsOk());
+
+  // Blob is able to commit before optimize
+  EXPECT_THAT(icing2.CommitBlob(blob_handle1).status(), ProtoIsOk());
+  // Optimize remove the expired orphan blob. so it's not able to commit.
+  ASSERT_THAT(icing2.Optimize().status(), ProtoIsOk());
+  EXPECT_THAT(icing2.CommitBlob(blob_handle2).status(),
+              ProtoStatusIs(StatusProto::NOT_FOUND));
+}
+
+TEST_F(IcingSearchEngineBlobTest, ReferenceCount) {
+  auto fake_clock = std::make_unique<FakeClock>();
+  fake_clock->SetSystemTimeMilliseconds(1000);
+  TestIcingSearchEngine icing(GetDefaultIcingOptions(),
+                              std::make_unique<Filesystem>(),
+                              std::make_unique<IcingFilesystem>(),
+                              std::move(fake_clock), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_label("label");
+  std::vector<unsigned char> data = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest = CalculateDigest(data);
+  std::string digestString = std::string(digest.begin(), digest.end());
+  blob_handle.set_digest(std::move(digestString));
+
+  BlobProto writeBlobProto = icing.OpenWriteBlob(blob_handle);
+  ASSERT_THAT(writeBlobProto.status(), ProtoIsOk());
+
+  ScopedFd write_fd(writeBlobProto.file_descriptor());
+  ASSERT_TRUE(filesystem()->Write(write_fd.get(), data.data(), data.size()));
+  close(write_fd.get());
+
+  BlobProto commitBlobProto = icing.CommitBlob(blob_handle);
+  ASSERT_THAT(commitBlobProto.status(), ProtoIsOk());
+
+  // Set schema and put a document that contains the blob handle
+  ASSERT_THAT(icing.SetSchema(CreateBlobSchema()).status(), ProtoIsOk());
+  ASSERT_THAT(
+      icing.Put(CreateBlobDocument("namespace", "doc1", blob_handle)).status(),
+      ProtoIsOk());
+
+  // persist to disk
+  EXPECT_THAT(icing.PersistToDisk(PersistType::FULL).status(), ProtoIsOk());
+
+  // create second icing in 8 days later
+  auto fake_clock2 = std::make_unique<FakeClock>();
+  fake_clock2->SetSystemTimeMilliseconds(1000 + 8 * 24 * 60 * 60 *
+                                                    1000);  // pass 8 days
+  TestIcingSearchEngine icing2(GetDefaultIcingOptions(),
+                               std::make_unique<Filesystem>(),
+                               std::make_unique<IcingFilesystem>(),
+                               std::move(fake_clock2), GetTestJniCache());
+  ASSERT_THAT(icing2.Initialize().status(), ProtoIsOk());
+
+  // Optimize won't remove the blob since there is reference document.
+  ASSERT_THAT(icing2.Optimize().status(), ProtoIsOk());
+  BlobProto readBlobProto = icing2.OpenReadBlob(blob_handle);
+  ASSERT_THAT(readBlobProto.status(), ProtoIsOk());
+  {
+    ScopedFd read_fd(readBlobProto.file_descriptor());
+    uint64_t size = filesystem()->GetFileSize(*read_fd);
+    std::unique_ptr<uint8_t[]> buf = std::make_unique<uint8_t[]>(size);
+    ASSERT_TRUE(filesystem()->Read(read_fd.get(), buf.get(), size));
+
+    std::string expected_data = std::string(data.begin(), data.end());
+    std::string actual_data = std::string(buf.get(), buf.get() + size);
+    EXPECT_EQ(expected_data, actual_data);
+  }
+
+  // remove the reference document, now the blob is an orphan.
+  ASSERT_THAT(icing2.Delete("namespace", "doc1").status(), ProtoIsOk());
+  // The blob remain before optimize.
+  readBlobProto = icing2.OpenReadBlob(blob_handle);
+  ASSERT_THAT(readBlobProto.status(), ProtoIsOk());
+  {
+    ScopedFd read_fd2(readBlobProto.file_descriptor());
+
+    uint64_t size = filesystem()->GetFileSize(*read_fd2);
+    std::unique_ptr<uint8_t[]> buf = std::make_unique<uint8_t[]>(size);
+    ASSERT_TRUE(filesystem()->Read(read_fd2.get(), buf.get(), size));
+
+    std::string expected_data = std::string(data.begin(), data.end());
+    std::string actual_data = std::string(buf.get(), buf.get() + size);
+    EXPECT_EQ(expected_data, actual_data);
+  }
+
+  // Optimize remove the expired orphan blob.
+  ASSERT_THAT(icing2.Optimize().status(), ProtoIsOk());
+  EXPECT_THAT(icing2.OpenReadBlob(blob_handle).status(),
+              ProtoStatusIs(StatusProto::NOT_FOUND));
+}
+
+TEST_F(IcingSearchEngineBlobTest, ReferenceCountNestedDocument) {
+  auto fake_clock = std::make_unique<FakeClock>();
+  fake_clock->SetSystemTimeMilliseconds(1000);
+  TestIcingSearchEngine icing(GetDefaultIcingOptions(),
+                              std::make_unique<Filesystem>(),
+                              std::make_unique<IcingFilesystem>(),
+                              std::move(fake_clock), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_label("label");
+  std::vector<unsigned char> data = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest = CalculateDigest(data);
+  std::string digestString = std::string(digest.begin(), digest.end());
+  blob_handle.set_digest(std::move(digestString));
+
+  BlobProto writeBlobProto = icing.OpenWriteBlob(blob_handle);
+  ASSERT_THAT(writeBlobProto.status(), ProtoIsOk());
+
+  ScopedFd write_fd(writeBlobProto.file_descriptor());
+  ASSERT_TRUE(filesystem()->Write(write_fd.get(), data.data(), data.size()));
+  close(write_fd.get());
+
+  BlobProto commitBlobProto = icing.CommitBlob(blob_handle);
+  ASSERT_THAT(commitBlobProto.status(), ProtoIsOk());
+
+  // Set an multi-level schema and put a document that contains the blob handle
+  // in the nested document property.
+  SchemaTypeConfigProto type_a =
+      SchemaTypeConfigBuilder()
+          .SetType("A")
+          .AddProperty(PropertyConfigBuilder()
+                           .SetName("blob")
+                           .SetDataType(TYPE_BLOB_HANDLE)
+                           .SetCardinality(CARDINALITY_OPTIONAL))
+          .Build();
+  SchemaTypeConfigProto type_b =
+      SchemaTypeConfigBuilder()
+          .SetType("B")
+          .AddProperty(
+              PropertyConfigBuilder()
+                  .SetName("nestedDoc")
+                  .SetDataTypeDocument("A", /*index_nested_properties=*/false)
+                  .SetCardinality(CARDINALITY_OPTIONAL))
+          .Build();
+  ASSERT_THAT(
+      icing.SetSchema(SchemaBuilder().AddType(type_a).AddType(type_b).Build())
+          .status(),
+      ProtoIsOk());
+  DocumentProto document_a = DocumentBuilder()
+                                 .SetKey("namespace", "doc_a")
+                                 .SetSchema("A")
+                                 .AddBlobHandleProperty("blob", blob_handle)
+                                 .Build();
+  DocumentProto document_b = DocumentBuilder()
+                                 .SetKey("namespace", "doc_b")
+                                 .SetSchema("B")
+                                 .AddDocumentProperty("nestedDoc", document_a)
+                                 .Build();
+  ASSERT_THAT(icing.Put(document_b).status(), ProtoIsOk());
+
+  // persist to disk
+  EXPECT_THAT(icing.PersistToDisk(PersistType::FULL).status(), ProtoIsOk());
+
+  // create second icing in 8 days later
+  auto fake_clock2 = std::make_unique<FakeClock>();
+  fake_clock2->SetSystemTimeMilliseconds(1000 + 8 * 24 * 60 * 60 *
+                                                    1000);  // pass 8 days
+  TestIcingSearchEngine icing2(GetDefaultIcingOptions(),
+                               std::make_unique<Filesystem>(),
+                               std::make_unique<IcingFilesystem>(),
+                               std::move(fake_clock2), GetTestJniCache());
+  ASSERT_THAT(icing2.Initialize().status(), ProtoIsOk());
+
+  // Optimize won't remove the blob since there is reference document.
+  ASSERT_THAT(icing2.Optimize().status(), ProtoIsOk());
+  BlobProto readBlobProto = icing2.OpenReadBlob(blob_handle);
+  ASSERT_THAT(readBlobProto.status(), ProtoIsOk());
+  {
+    ScopedFd read_fd(readBlobProto.file_descriptor());
+    uint64_t size = filesystem()->GetFileSize(*read_fd);
+    std::unique_ptr<uint8_t[]> buf = std::make_unique<uint8_t[]>(size);
+    ASSERT_TRUE(filesystem()->Read(read_fd.get(), buf.get(), size));
+
+    std::string expected_data = std::string(data.begin(), data.end());
+    std::string actual_data = std::string(buf.get(), buf.get() + size);
+    EXPECT_EQ(expected_data, actual_data);
+  }
+
+  // remove the reference document, now the blob is an orphan.
+  ASSERT_THAT(icing2.Delete("namespace", "doc_b").status(), ProtoIsOk());
+  // The blob remain before optimize.
+  readBlobProto = icing2.OpenReadBlob(blob_handle);
+  ASSERT_THAT(readBlobProto.status(), ProtoIsOk());
+  {
+    ScopedFd read_fd2(readBlobProto.file_descriptor());
+
+    uint64_t size = filesystem()->GetFileSize(*read_fd2);
+    std::unique_ptr<uint8_t[]> buf = std::make_unique<uint8_t[]>(size);
+    ASSERT_TRUE(filesystem()->Read(read_fd2.get(), buf.get(), size));
+
+    std::string expected_data = std::string(data.begin(), data.end());
+    std::string actual_data = std::string(buf.get(), buf.get() + size);
+    EXPECT_EQ(expected_data, actual_data);
+  }
+
+  // Optimize remove the expired orphan blob.
+  ASSERT_THAT(icing2.Optimize().status(), ProtoIsOk());
+  EXPECT_THAT(icing2.OpenReadBlob(blob_handle).status(),
+              ProtoStatusIs(StatusProto::NOT_FOUND));
+}
+
+TEST_F(IcingSearchEngineBlobTest, OptimizeMultipleReferenceDocument) {
+  auto fake_clock = std::make_unique<FakeClock>();
+  fake_clock->SetSystemTimeMilliseconds(1000);
+  TestIcingSearchEngine icing(GetDefaultIcingOptions(),
+                              std::make_unique<Filesystem>(),
+                              std::make_unique<IcingFilesystem>(),
+                              std::move(fake_clock), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_label("label");
+  std::vector<unsigned char> data = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest = CalculateDigest(data);
+  std::string digestString = std::string(digest.begin(), digest.end());
+  blob_handle.set_digest(std::move(digestString));
+
+  BlobProto writeBlobProto = icing.OpenWriteBlob(blob_handle);
+  ASSERT_THAT(writeBlobProto.status(), ProtoIsOk());
+  {
+    ScopedFd write_fd(writeBlobProto.file_descriptor());
+    ASSERT_TRUE(filesystem()->Write(write_fd.get(), data.data(), data.size()));
+  }
+
+  BlobProto commitBlobProto = icing.CommitBlob(blob_handle);
+  ASSERT_THAT(commitBlobProto.status(), ProtoIsOk());
+
+  // Set schema and put 3 documents that contains the blob handle
+  ASSERT_THAT(icing.SetSchema(CreateBlobSchema()).status(), ProtoIsOk());
+  ASSERT_THAT(
+      icing.Put(CreateBlobDocument("namespace", "doc1", blob_handle)).status(),
+      ProtoIsOk());
+  ASSERT_THAT(
+      icing.Put(CreateBlobDocument("namespace", "doc2", blob_handle)).status(),
+      ProtoIsOk());
+  ASSERT_THAT(
+      icing.Put(CreateBlobDocument("namespace", "doc3", blob_handle)).status(),
+      ProtoIsOk());
+
+  // persist to disk
+  EXPECT_THAT(icing.PersistToDisk(PersistType::FULL).status(), ProtoIsOk());
+
+  // create second icing in 8 days later
+  auto fake_clock2 = std::make_unique<FakeClock>();
+  fake_clock2->SetSystemTimeMilliseconds(1000 + 8 * 24 * 60 * 60 *
+                                                    1000);  // pass 8 days
+  TestIcingSearchEngine icing2(GetDefaultIcingOptions(),
+                               std::make_unique<Filesystem>(),
+                               std::make_unique<IcingFilesystem>(),
+                               std::move(fake_clock2), GetTestJniCache());
+  ASSERT_THAT(icing2.Initialize().status(), ProtoIsOk());
+
+  // Optimize won't remove the blob since there are reference documents.
+  ASSERT_THAT(icing2.Optimize().status(), ProtoIsOk());
+  BlobProto readBlobProto = icing2.OpenReadBlob(blob_handle);
+  ASSERT_THAT(readBlobProto.status(), ProtoIsOk());
+  {
+    ScopedFd read_fd(readBlobProto.file_descriptor());
+
+    uint64_t size = filesystem()->GetFileSize(*read_fd);
+    std::unique_ptr<uint8_t[]> buf = std::make_unique<uint8_t[]>(size);
+    filesystem()->Read(read_fd.get(), buf.get(), size);
+    close(read_fd.get());
+
+    std::string expected_data = std::string(data.begin(), data.end());
+    std::string actual_data = std::string(buf.get(), buf.get() + size);
+    EXPECT_EQ(expected_data, actual_data);
+  }
+
+  // remove two reference documents.
+  ASSERT_THAT(icing2.Delete("namespace", "doc1").status(), ProtoIsOk());
+  ASSERT_THAT(icing2.Delete("namespace", "doc2").status(), ProtoIsOk());
+  // The blob remain after optimize.
+  ASSERT_THAT(icing2.Optimize().status(), ProtoIsOk());
+  readBlobProto = icing2.OpenReadBlob(blob_handle);
+  ASSERT_THAT(readBlobProto.status(), ProtoIsOk());
+  {
+    ScopedFd read_fd2(readBlobProto.file_descriptor());
+
+    uint64_t size = filesystem()->GetFileSize(*read_fd2);
+    std::unique_ptr<uint8_t[]> buf = std::make_unique<uint8_t[]>(size);
+    filesystem()->Read(read_fd2.get(), buf.get(), size);
+    close(read_fd2.get());
+
+    std::string expected_data = std::string(data.begin(), data.end());
+    std::string actual_data = std::string(buf.get(), buf.get() + size);
+    EXPECT_EQ(expected_data, actual_data);
+  }
+
+  // remove the last reference document, now the blob become orphan.
+  ASSERT_THAT(icing2.Delete("namespace", "doc3").status(), ProtoIsOk());
+  // Optimize remove the expired orphan blob.
+  ASSERT_THAT(icing2.Optimize().status(), ProtoIsOk());
+  EXPECT_THAT(icing2.OpenReadBlob(blob_handle).status(),
+              ProtoStatusIs(StatusProto::NOT_FOUND));
+}
+
+TEST_F(IcingSearchEngineBlobTest, OptimizeMultipleBlobHandles) {
+  auto fake_clock = std::make_unique<FakeClock>();
+  fake_clock->SetSystemTimeMilliseconds(1000);
+  TestIcingSearchEngine icing(GetDefaultIcingOptions(),
+                              std::make_unique<Filesystem>(),
+                              std::make_unique<IcingFilesystem>(),
+                              std::move(fake_clock), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+
+  std::vector<std::string> file_names;
+  std::unordered_set<std::string> excludes;
+  ASSERT_TRUE(filesystem()->ListDirectory(GetTestBaseBlobStoreDir().c_str(),
+                                          excludes, /*recursive=*/false,
+                                          &file_names));
+  int32_t file_count = file_names.size();
+
+  PropertyProto::BlobHandleProto blob_handle1;
+  blob_handle1.set_label("label1");
+  std::vector<unsigned char> data1 = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest1 = CalculateDigest(data1);
+  std::string digestString1 = std::string(digest1.begin(), digest1.end());
+  blob_handle1.set_digest(std::move(digestString1));
+
+  BlobProto writeBlobProto1 = icing.OpenWriteBlob(blob_handle1);
+  ASSERT_THAT(writeBlobProto1.status(), ProtoIsOk());
+  {
+    ScopedFd write_fd(writeBlobProto1.file_descriptor());
+    ASSERT_TRUE(
+        filesystem()->Write(write_fd.get(), data1.data(), data1.size()));
+  }
+
+  BlobProto commitBlobProto = icing.CommitBlob(blob_handle1);
+  ASSERT_THAT(commitBlobProto.status(), ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle2;
+  blob_handle2.set_label("label2");
+  std::vector<unsigned char> data2 = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest2 = CalculateDigest(data2);
+  std::string digestString2 = std::string(digest2.begin(), digest2.end());
+  blob_handle2.set_digest(std::move(digestString2));
+
+  BlobProto writeBlobProto2 = icing.OpenWriteBlob(blob_handle2);
+  ASSERT_THAT(writeBlobProto2.status(), ProtoIsOk());
+  {
+    ScopedFd write_fd(writeBlobProto2.file_descriptor());
+    ASSERT_TRUE(
+        filesystem()->Write(write_fd.get(), data2.data(), data2.size()));
+  }
+
+  BlobProto commitBlobProto2 = icing.CommitBlob(blob_handle2);
+  ASSERT_THAT(commitBlobProto2.status(), ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle3;
+  blob_handle3.set_label("label3");
+  std::vector<unsigned char> data3 = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest3 = CalculateDigest(data3);
+  std::string digestString3 = std::string(digest3.begin(), digest3.end());
+  blob_handle3.set_digest(std::move(digestString3));
+
+  BlobProto writeBlobProto3 = icing.OpenWriteBlob(blob_handle3);
+  ASSERT_THAT(writeBlobProto3.status(), ProtoIsOk());
+  {
+    ScopedFd write_fd(writeBlobProto3.file_descriptor());
+    ASSERT_TRUE(
+        filesystem()->Write(write_fd.get(), data3.data(), data3.size()));
+  }
+
+  BlobProto commitBlobProto3 = icing.CommitBlob(blob_handle3);
+  ASSERT_THAT(commitBlobProto3.status(), ProtoIsOk());
+
+  file_names = std::vector<std::string>();
+  ASSERT_TRUE(filesystem()->ListDirectory(GetTestBaseBlobStoreDir().c_str(),
+                                          excludes, /*recursive=*/false,
+                                          &file_names));
+  ASSERT_THAT(file_names.size(), file_count + 3);
+
+  // Set schema and put 3 documents that contains the blob handle
+  ASSERT_THAT(icing.SetSchema(CreateBlobSchema()).status(), ProtoIsOk());
+  ASSERT_THAT(
+      icing.Put(CreateBlobDocument("namespace", "doc1", blob_handle1)).status(),
+      ProtoIsOk());
+  ASSERT_THAT(
+      icing.Put(CreateBlobDocument("namespace", "doc2", blob_handle2)).status(),
+      ProtoIsOk());
+  ASSERT_THAT(
+      icing.Put(CreateBlobDocument("namespace", "doc3", blob_handle3)).status(),
+      ProtoIsOk());
+
+  // persist to disk
+  EXPECT_THAT(icing.PersistToDisk(PersistType::FULL).status(), ProtoIsOk());
+
+  // create second icing in 8 days later
+  auto fake_clock2 = std::make_unique<FakeClock>();
+  fake_clock2->SetSystemTimeMilliseconds(1000 + 8 * 24 * 60 * 60 *
+                                                    1000);  // pass 8 days
+  TestIcingSearchEngine icing2(GetDefaultIcingOptions(),
+                               std::make_unique<Filesystem>(),
+                               std::make_unique<IcingFilesystem>(),
+                               std::move(fake_clock2), GetTestJniCache());
+  ASSERT_THAT(icing2.Initialize().status(), ProtoIsOk());
+
+  // Optimize won't remove the blob since there are reference documents.
+  ASSERT_THAT(icing2.Optimize().status(), ProtoIsOk());
+  ASSERT_THAT(icing2.OpenReadBlob(blob_handle1).status(), ProtoIsOk());
+  ASSERT_THAT(icing2.OpenReadBlob(blob_handle2).status(), ProtoIsOk());
+  ASSERT_THAT(icing2.OpenReadBlob(blob_handle3).status(), ProtoIsOk());
+
+  // Remove first two reference documents.
+  ASSERT_THAT(icing2.Delete("namespace", "doc1").status(), ProtoIsOk());
+  ASSERT_THAT(icing2.Delete("namespace", "doc2").status(), ProtoIsOk());
+
+  // First two orphan blobs are removed after optimize .
+  ASSERT_THAT(icing2.Optimize().status(), ProtoIsOk());
+  EXPECT_THAT(icing2.OpenReadBlob(blob_handle1).status(),
+              ProtoStatusIs(StatusProto::NOT_FOUND));
+  EXPECT_THAT(icing2.OpenReadBlob(blob_handle2).status(),
+              ProtoStatusIs(StatusProto::NOT_FOUND));
+  ASSERT_THAT(icing2.OpenReadBlob(blob_handle3).status(), ProtoIsOk());
+  file_names = std::vector<std::string>();
+  ASSERT_TRUE(filesystem()->ListDirectory(GetTestBaseBlobStoreDir().c_str(),
+                                          excludes, /*recursive=*/false,
+                                          &file_names));
+  ASSERT_THAT(file_names.size(), file_count + 1);
+
+  // remove the last reference document, now the all blobs become orphan.
+  ASSERT_THAT(icing2.Delete("namespace", "doc3").status(), ProtoIsOk());
+  // Optimize remove the expired orphan blob.
+  ASSERT_THAT(icing2.Optimize().status(), ProtoIsOk());
+  EXPECT_THAT(icing2.OpenReadBlob(blob_handle3).status(),
+              ProtoStatusIs(StatusProto::NOT_FOUND));
+  file_names = std::vector<std::string>();
+  ASSERT_TRUE(filesystem()->ListDirectory(GetTestBaseBlobStoreDir().c_str(),
+                                          excludes, /*recursive=*/false,
+                                          &file_names));
+  ASSERT_THAT(file_names.size(), file_count);
+}
+
+TEST_F(IcingSearchEngineBlobTest, OptimizeBlobHandlesNoTTL) {
+  auto fake_clock = std::make_unique<FakeClock>();
+  fake_clock->SetSystemTimeMilliseconds(1000);
+  IcingSearchEngineOptions icing_options;
+  icing_options.set_base_dir(GetTestBaseDir());
+  icing_options.set_enable_blob_store(true);
+  // set orphan blob ttl to 0, which means no ttl
+  icing_options.set_orphan_blob_time_to_live_ms(0);
+  TestIcingSearchEngine icing(icing_options, std::make_unique<Filesystem>(),
+                              std::make_unique<IcingFilesystem>(),
+                              std::move(fake_clock), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+
+  // set a schema to icing to avoid wipe out all directories.
+  ASSERT_THAT(icing.SetSchema(CreateBlobSchema()).status(), ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_label("label");
+  std::vector<unsigned char> data = GenerateRandomBytes(24);
+  std::array<uint8_t, 32> digest = CalculateDigest(data);
+  std::string digestString = std::string(digest.begin(), digest.end());
+  blob_handle.set_digest(std::move(digestString));
+
+  BlobProto writeBlobProto = icing.OpenWriteBlob(blob_handle);
+  ASSERT_THAT(writeBlobProto.status(), ProtoIsOk());
+  {
+    ScopedFd write_fd(writeBlobProto.file_descriptor());
+    ASSERT_TRUE(filesystem()->Write(write_fd.get(), data.data(), data.size()));
+  }
+
+  BlobProto commitBlobProto = icing.CommitBlob(blob_handle);
+  ASSERT_THAT(commitBlobProto.status(), ProtoIsOk());
+
+  // persist blob to disk
+  EXPECT_THAT(icing.PersistToDisk(PersistType::FULL).status(), ProtoIsOk());
+
+  // create second icing in a year later 365L * 24 * 60 * 60 * 1000;
+  auto fake_clock2 = std::make_unique<FakeClock>();
+  fake_clock2->SetSystemTimeMilliseconds(1471228928);
+  TestIcingSearchEngine icing2(icing_options, std::make_unique<Filesystem>(),
+                               std::make_unique<IcingFilesystem>(),
+                               std::move(fake_clock2), GetTestJniCache());
+  ASSERT_THAT(icing2.Initialize().status(), ProtoIsOk());
+
+  // Blob remain after optimize
+  ASSERT_THAT(icing2.Optimize().status(), ProtoIsOk());
+  BlobProto readBlobProto = icing2.OpenReadBlob(blob_handle);
+  ASSERT_THAT(readBlobProto.status(), ProtoIsOk());
+  ScopedFd read_fd(readBlobProto.file_descriptor());
+
+  uint64_t size = filesystem()->GetFileSize(*read_fd);
+  std::unique_ptr<uint8_t[]> buf = std::make_unique<uint8_t[]>(size);
+  filesystem()->Read(read_fd.get(), buf.get(), size);
+  close(read_fd.get());
+
+  std::string expected_data = std::string(data.begin(), data.end());
+  std::string actual_data = std::string(buf.get(), buf.get() + size);
+  EXPECT_EQ(expected_data, actual_data);
+}
+
+}  // namespace
+}  // namespace lib
+}  // namespace icing
diff --git a/icing/icing-search-engine_delete_test.cc b/icing/icing-search-engine_delete_test.cc
index c3b1ccd..6546d2d 100644
--- a/icing/icing-search-engine_delete_test.cc
+++ b/icing/icing-search-engine_delete_test.cc
@@ -12,8 +12,6 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
 
-#include "icing/icing-search-engine.h"
-
 #include <cstdint>
 #include <limits>
 #include <memory>
@@ -26,6 +24,7 @@
 #include "icing/document-builder.h"
 #include "icing/file/filesystem.h"
 #include "icing/file/mock-filesystem.h"
+#include "icing/icing-search-engine.h"
 #include "icing/jni/jni-cache.h"
 #include "icing/portable/endian.h"
 #include "icing/portable/equals-proto.h"
diff --git a/icing/icing-search-engine_initialization_test.cc b/icing/icing-search-engine_initialization_test.cc
index a77f185..3ffc2cf 100644
--- a/icing/icing-search-engine_initialization_test.cc
+++ b/icing/icing-search-engine_initialization_test.cc
@@ -72,6 +72,7 @@
 #include "icing/schema-builder.h"
 #include "icing/schema/schema-store.h"
 #include "icing/schema/section.h"
+#include "icing/store/blob-store.h"
 #include "icing/store/document-associated-score-data.h"
 #include "icing/store/document-id.h"
 #include "icing/store/document-log-creator.h"
@@ -221,6 +222,8 @@ std::string GetQualifiedIdJoinIndexDir() {
 
 std::string GetSchemaDir() { return GetTestBaseDir() + "/schema_dir"; }
 
+std::string GetBlobDir() { return GetTestBaseDir() + "/blob_dir"; }
+
 std::string GetHeaderFilename() {
   return GetTestBaseDir() + "/icing_search_engine_header";
 }
@@ -1075,6 +1078,11 @@ TEST_F(IcingSearchEngineInitializationTest,
         std::unique_ptr<SchemaStore> schema_store,
         SchemaStore::Create(filesystem(), GetSchemaDir(), &fake_clock));
 
+    ICING_ASSERT_OK_AND_ASSIGN(
+        BlobStore blob_store,
+        BlobStore::Create(filesystem(), GetBlobDir(), &fake_clock,
+                          /*orphan_blob_time_to_live_ms=*/0));
+
     // Puts message2 into DocumentStore but doesn't index it.
     ICING_ASSERT_OK_AND_ASSIGN(
         DocumentStore::CreateResult create_result,
@@ -5474,6 +5482,11 @@ TEST_P(IcingSearchEngineInitializationVersionChangeTest,
         std::unique_ptr<SchemaStore> schema_store,
         SchemaStore::Create(filesystem(), GetSchemaDir(), &fake_clock));
 
+    ICING_ASSERT_OK_AND_ASSIGN(
+        BlobStore blob_store,
+        BlobStore::Create(filesystem(), GetBlobDir(), &fake_clock,
+                          /*orphan_blob_time_to_live_ms=*/0));
+
     // Put message into DocumentStore
     ICING_ASSERT_OK_AND_ASSIGN(
         DocumentStore::CreateResult create_result,
@@ -5488,7 +5501,9 @@ TEST_P(IcingSearchEngineInitializationVersionChangeTest,
                               /*initialize_stats=*/nullptr));
     std::unique_ptr<DocumentStore> document_store =
         std::move(create_result.document_store);
-    ICING_ASSERT_OK_AND_ASSIGN(DocumentId doc_id, document_store->Put(message));
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                               document_store->Put(message));
+    DocumentId doc_id = put_result.new_document_id;
 
     // Index doc_id with incorrect data
     Index::Options options(GetIndexDir(), /*index_merge_size=*/1024 * 1024,
diff --git a/icing/icing-search-engine_put_test.cc b/icing/icing-search-engine_put_test.cc
index ed72f17..c5f8e2b 100644
--- a/icing/icing-search-engine_put_test.cc
+++ b/icing/icing-search-engine_put_test.cc
@@ -12,8 +12,6 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
 
-#include "icing/icing-search-engine.h"
-
 #include <cstdint>
 #include <limits>
 #include <memory>
@@ -25,6 +23,7 @@
 #include "gtest/gtest.h"
 #include "icing/document-builder.h"
 #include "icing/file/filesystem.h"
+#include "icing/icing-search-engine.h"
 #include "icing/jni/jni-cache.h"
 #include "icing/legacy/index/icing-mock-filesystem.h"
 #include "icing/portable/endian.h"
@@ -60,6 +59,7 @@ namespace lib {
 namespace {
 
 using ::testing::Eq;
+using ::testing::EqualsProto;
 using ::testing::Ge;
 using ::testing::HasSubstr;
 using ::testing::IsEmpty;
@@ -330,6 +330,29 @@ TEST_F(IcingSearchEnginePutTest, IndexingDocMergeFailureResets) {
   }
 }
 
+TEST_F(IcingSearchEnginePutTest, PutDocumentReplacementSucceeds) {
+  DocumentProto document = DocumentBuilder()
+                               .SetKey("icing", "fake_type/0")
+                               .SetSchema("Message")
+                               .AddStringProperty("body", "message body")
+                               .Build();
+
+  IcingSearchEngineOptions options = GetDefaultIcingOptions();
+  options.set_index_merge_size(document.ByteSizeLong());
+  IcingSearchEngine icing(options, GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+  ASSERT_THAT(icing.SetSchema(CreateMessageSchema()).status(), ProtoIsOk());
+
+  PutResultProto put_result_proto = icing.Put(document);
+  EXPECT_THAT(put_result_proto.status(), ProtoIsOk());
+  EXPECT_FALSE(put_result_proto.was_replacement());
+
+  // Putting the document again should succeed.
+  put_result_proto = icing.Put(document);
+  EXPECT_THAT(put_result_proto.status(), ProtoIsOk());
+  EXPECT_TRUE(put_result_proto.was_replacement());
+}
+
 TEST_F(IcingSearchEnginePutTest, PutDocumentShouldLogFunctionLatency) {
   DocumentProto document = DocumentBuilder()
                                .SetKey("icing", "fake_type/0")
@@ -348,6 +371,7 @@ TEST_F(IcingSearchEnginePutTest, PutDocumentShouldLogFunctionLatency) {
 
   PutResultProto put_result_proto = icing.Put(document);
   EXPECT_THAT(put_result_proto.status(), ProtoIsOk());
+  EXPECT_FALSE(put_result_proto.was_replacement());
   EXPECT_THAT(put_result_proto.put_document_stats().latency_ms(), Eq(10));
 }
 
@@ -371,6 +395,7 @@ TEST_F(IcingSearchEnginePutTest, PutDocumentShouldLogDocumentStoreStats) {
 
   PutResultProto put_result_proto = icing.Put(document);
   EXPECT_THAT(put_result_proto.status(), ProtoIsOk());
+  EXPECT_FALSE(put_result_proto.was_replacement());
   EXPECT_THAT(put_result_proto.put_document_stats().document_store_latency_ms(),
               Eq(10));
   size_t document_size = put_result_proto.put_document_stats().document_size();
@@ -397,6 +422,7 @@ TEST_F(IcingSearchEnginePutTest, PutDocumentShouldLogIndexingStats) {
 
   PutResultProto put_result_proto = icing.Put(document);
   EXPECT_THAT(put_result_proto.status(), ProtoIsOk());
+  EXPECT_FALSE(put_result_proto.was_replacement());
   EXPECT_THAT(put_result_proto.put_document_stats().index_latency_ms(), Eq(10));
   // No merge should happen.
   EXPECT_THAT(put_result_proto.put_document_stats().index_merge_latency_ms(),
@@ -435,6 +461,7 @@ TEST_F(IcingSearchEnginePutTest, PutDocumentShouldLogIndexMergeLatency) {
 
   // Putting document2 should trigger an index merge.
   PutResultProto put_result_proto = icing.Put(document2);
+  EXPECT_FALSE(put_result_proto.was_replacement());
   EXPECT_THAT(put_result_proto.status(), ProtoIsOk());
   EXPECT_THAT(put_result_proto.put_document_stats().index_merge_latency_ms(),
               Eq(10));
@@ -476,6 +503,86 @@ TEST_F(IcingSearchEnginePutTest, PutDocumentIndexFailureDeletion) {
   ASSERT_THAT(get_result.status(), ProtoStatusIs(StatusProto::NOT_FOUND));
 }
 
+TEST_F(IcingSearchEnginePutTest, PutAndGetDocumentWithBlobHandle) {
+  auto fake_clock = std::make_unique<FakeClock>();
+  fake_clock->SetTimerElapsedMilliseconds(1000);
+  TestIcingSearchEngine icing(GetDefaultIcingOptions(),
+                              std::make_unique<Filesystem>(),
+                              std::make_unique<IcingFilesystem>(),
+                              std::move(fake_clock), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+  ASSERT_THAT(
+      icing
+          .SetSchema(
+              SchemaBuilder()
+                  .AddType(SchemaTypeConfigBuilder()
+                               .SetType("SchemaType")
+                               .AddProperty(
+                                   PropertyConfigBuilder()
+                                       .SetName("blobHandle")
+                                       .SetDataType(TYPE_BLOB_HANDLE)
+                                       .SetCardinality(CARDINALITY_REQUIRED)))
+                  .Build())
+          .status(),
+      ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_digest(std::string(32, ' '));
+  blob_handle.set_label("label");
+
+  DocumentProto document =
+      DocumentBuilder()
+          .SetKey("namespace", "uri")
+          .SetSchema("SchemaType")
+          .AddBlobHandleProperty("blobHandle", blob_handle)
+          .SetCreationTimestampMs(kDefaultCreationTimestampMs)
+          .Build();
+  ASSERT_THAT(icing.Put(document).status(), ProtoIsOk());
+
+  GetResultProto get_result =
+      icing.Get("namespace", "uri", GetResultSpecProto::default_instance());
+  EXPECT_THAT(get_result.status(), ProtoIsOk());
+  EXPECT_THAT(get_result.document(), EqualsProto(document));
+}
+
+TEST_F(IcingSearchEnginePutTest, PutDocumentWithInvalidBlobHandle) {
+  auto fake_clock = std::make_unique<FakeClock>();
+  fake_clock->SetTimerElapsedMilliseconds(1000);
+  TestIcingSearchEngine icing(GetDefaultIcingOptions(),
+                              std::make_unique<Filesystem>(),
+                              std::make_unique<IcingFilesystem>(),
+                              std::move(fake_clock), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+  ASSERT_THAT(
+      icing
+          .SetSchema(
+              SchemaBuilder()
+                  .AddType(SchemaTypeConfigBuilder()
+                               .SetType("SchemaType")
+                               .AddProperty(
+                                   PropertyConfigBuilder()
+                                       .SetName("blobHandle")
+                                       .SetDataType(TYPE_BLOB_HANDLE)
+                                       .SetCardinality(CARDINALITY_REQUIRED)))
+                  .Build())
+          .status(),
+      ProtoIsOk());
+
+  PropertyProto::BlobHandleProto blob_handle;
+  blob_handle.set_digest("invalid digest");
+  blob_handle.set_label("label");
+
+  DocumentProto document =
+      DocumentBuilder()
+          .SetKey("namespace", "uri")
+          .SetSchema("SchemaType")
+          .AddBlobHandleProperty("blobHandle", blob_handle)
+          .SetCreationTimestampMs(kDefaultCreationTimestampMs)
+          .Build();
+  ASSERT_THAT(icing.Put(document).status(),
+              ProtoStatusIs(StatusProto::INVALID_ARGUMENT));
+}
+
 }  // namespace
 }  // namespace lib
 }  // namespace icing
diff --git a/icing/icing-search-engine_schema_test.cc b/icing/icing-search-engine_schema_test.cc
index c81cc9d..115e4cf 100644
--- a/icing/icing-search-engine_schema_test.cc
+++ b/icing/icing-search-engine_schema_test.cc
@@ -671,6 +671,164 @@ TEST_F(IcingSearchEngineSchemaTest, SetSchema) {
               HasSubstr("'Photo' not found"));
 }
 
+TEST_F(IcingSearchEngineSchemaTest, SetSchema_schemaTypeIdChanged) {
+  auto fake_clock = std::make_unique<FakeClock>();
+  fake_clock->SetTimerElapsedMilliseconds(1000);
+  TestIcingSearchEngine icing(GetDefaultIcingOptions(),
+                              std::make_unique<Filesystem>(),
+                              std::make_unique<IcingFilesystem>(),
+                              std::move(fake_clock), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+
+  SchemaProto schema_one =
+      SchemaBuilder()
+          .AddType(SchemaTypeConfigBuilder()
+                       .SetType("Person")  // schema type id 0
+                       .AddProperty(PropertyConfigBuilder()
+                                        .SetName("name")
+                                        .SetDataTypeString(TERM_MATCH_PREFIX,
+                                                           TOKENIZER_PLAIN)
+                                        .SetCardinality(CARDINALITY_REQUIRED))
+                       .AddProperty(PropertyConfigBuilder()
+                                        .SetName("age")
+                                        .SetDataTypeInt64(NUMERIC_MATCH_RANGE)
+                                        .SetCardinality(CARDINALITY_OPTIONAL)))
+          .AddType(SchemaTypeConfigBuilder()
+                       .SetType("Email")  // schema type id 1
+                       .AddProperty(PropertyConfigBuilder()
+                                        .SetName("subject")
+                                        .SetDataTypeString(TERM_MATCH_PREFIX,
+                                                           TOKENIZER_PLAIN)
+                                        .SetCardinality(CARDINALITY_REQUIRED))
+                       .AddProperty(PropertyConfigBuilder()
+                                        .SetName("sender")
+                                        .SetDataTypeJoinableString(
+                                            JOINABLE_VALUE_TYPE_QUALIFIED_ID)
+                                        .SetCardinality(CARDINALITY_REQUIRED)))
+          .Build();
+
+  // Set schema with Person and Email types.
+  SetSchemaResultProto set_schema_result1 = icing.SetSchema(schema_one);
+  EXPECT_THAT(set_schema_result1.status(), ProtoStatusIs(StatusProto::OK));
+  EXPECT_THAT(set_schema_result1.latency_ms(), Eq(1000));
+
+  // Put a person document and an email document.
+  DocumentProto person_document = DocumentBuilder()
+                                      .SetKey("namespace", "person/1")
+                                      .SetSchema("Person")
+                                      .SetCreationTimestampMs(1000)
+                                      .AddStringProperty("name", "John")
+                                      .AddInt64Property("age", 20)
+                                      .Build();
+  DocumentProto email_document =
+      DocumentBuilder()
+          .SetKey("namespace", "email/1")
+          .SetSchema("Email")
+          .SetCreationTimestampMs(1000)
+          .AddStringProperty("subject", "subject")
+          .AddStringProperty("sender", "namespace#person/1")
+          .Build();
+  EXPECT_THAT(icing.Put(person_document).status(), ProtoIsOk());
+  EXPECT_THAT(icing.Put(email_document).status(), ProtoIsOk());
+
+  // SetSchema again with the schema types reordered. Schema type id will
+  // change. This should succeed and all search features should still work after
+  // schema id changed.
+  SchemaProto schema_two =
+      SchemaBuilder()
+          .AddType(SchemaTypeConfigBuilder()
+                       .SetType("Email")  // schema type id 0
+                       .AddProperty(PropertyConfigBuilder()
+                                        .SetName("subject")
+                                        .SetDataTypeString(TERM_MATCH_PREFIX,
+                                                           TOKENIZER_PLAIN)
+                                        .SetCardinality(CARDINALITY_REQUIRED))
+                       .AddProperty(PropertyConfigBuilder()
+                                        .SetName("sender")
+                                        .SetDataTypeJoinableString(
+                                            JOINABLE_VALUE_TYPE_QUALIFIED_ID)
+                                        .SetCardinality(CARDINALITY_REQUIRED)))
+          .AddType(SchemaTypeConfigBuilder()
+                       .SetType("Person")  // schema type id 1
+                       .AddProperty(PropertyConfigBuilder()
+                                        .SetName("name")
+                                        .SetDataTypeString(TERM_MATCH_PREFIX,
+                                                           TOKENIZER_PLAIN)
+                                        .SetCardinality(CARDINALITY_REQUIRED))
+                       .AddProperty(PropertyConfigBuilder()
+                                        .SetName("age")
+                                        .SetDataTypeInt64(NUMERIC_MATCH_RANGE)
+                                        .SetCardinality(CARDINALITY_OPTIONAL)))
+
+          .Build();
+  SetSchemaResultProto set_schema_result2 = icing.SetSchema(schema_two);
+  EXPECT_THAT(set_schema_result2.status(), ProtoStatusIs(StatusProto::OK));
+  EXPECT_THAT(set_schema_result2.latency_ms(), Eq(1000));
+
+  ResultSpecProto result_spec = ResultSpecProto::default_instance();
+  result_spec.set_max_joined_children_per_parent_to_return(
+      std::numeric_limits<int32_t>::max());
+
+  // Verify term search
+  // We should be able to retrieve the person document by searching "name:John".
+  SearchSpecProto search_spec1;
+  search_spec1.set_query("name:John");
+  search_spec1.set_term_match_type(TermMatchType::EXACT_ONLY);
+
+  SearchResultProto expected_search_result_proto1;
+  expected_search_result_proto1.mutable_status()->set_code(StatusProto::OK);
+  *expected_search_result_proto1.mutable_results()->Add()->mutable_document() =
+      person_document;
+  EXPECT_THAT(
+      icing.Search(search_spec1, GetDefaultScoringSpec(), result_spec),
+      EqualsSearchResultIgnoreStatsAndScores(expected_search_result_proto1));
+
+  // Verify numeric (integer) search
+  // We should be able to retrieve the document by searching "age == 20".
+  SearchSpecProto search_spec2;
+  search_spec2.set_query("age == 20");
+  search_spec2.add_enabled_features(std::string(kNumericSearchFeature));
+
+  SearchResultProto expected_search_result_google::protobuf;
+  expected_search_result_google::protobuf.mutable_status()->set_code(StatusProto::OK);
+  *expected_search_result_google::protobuf.mutable_results()->Add()->mutable_document() =
+      person_document;
+  EXPECT_THAT(
+      icing.Search(search_spec2, GetDefaultScoringSpec(), result_spec),
+      EqualsSearchResultIgnoreStatsAndScores(expected_search_result_google::protobuf));
+
+  // Verify join search: join a query for `name:John` (which will get
+  // person_document) with a child query for `subject:subject` (which will get
+  // email_document) based on the child's `sender` field.
+  SearchSpecProto search_spec3;
+  search_spec3.set_query("name:John");
+  search_spec3.set_term_match_type(TermMatchType::EXACT_ONLY);
+  JoinSpecProto* join_spec = search_spec3.mutable_join_spec();
+  join_spec->set_parent_property_expression(
+      std::string(JoinProcessor::kQualifiedIdExpr));
+  join_spec->set_child_property_expression("sender");
+  join_spec->set_aggregation_scoring_strategy(
+      JoinSpecProto::AggregationScoringStrategy::COUNT);
+  JoinSpecProto::NestedSpecProto* nested_spec =
+      join_spec->mutable_nested_spec();
+  SearchSpecProto* nested_search_spec = nested_spec->mutable_search_spec();
+  nested_search_spec->set_term_match_type(TermMatchType::EXACT_ONLY);
+  nested_search_spec->set_query("subject:subject");
+  *nested_spec->mutable_scoring_spec() = GetDefaultScoringSpec();
+  *nested_spec->mutable_result_spec() = result_spec;
+
+  SearchResultProto expected_search_result_proto3;
+  expected_search_result_proto3.mutable_status()->set_code(StatusProto::OK);
+  SearchResultProto::ResultProto* result_proto =
+      expected_search_result_proto3.mutable_results()->Add();
+  *result_proto->mutable_document() = person_document;
+  *result_proto->mutable_joined_results()->Add()->mutable_document() =
+      email_document;
+  EXPECT_THAT(
+      icing.Search(search_spec3, GetDefaultScoringSpec(), result_spec),
+      EqualsSearchResultIgnoreStatsAndScores(expected_search_result_proto3));
+}
+
 TEST_F(IcingSearchEngineSchemaTest,
        SetSchemaNewIndexedStringPropertyTriggersIndexRestorationAndReturnsOk) {
   IcingSearchEngine icing(GetDefaultIcingOptions(), GetTestJniCache());
@@ -2785,15 +2943,55 @@ TEST_F(IcingSearchEngineSchemaTest, GetSchemaTypeFailedPrecondition) {
 TEST_F(IcingSearchEngineSchemaTest, GetSchemaTypeOk) {
   IcingSearchEngine icing(GetDefaultIcingOptions(), GetTestJniCache());
   ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+  SchemaTypeConfigProto schema_type_config =
+      SchemaTypeConfigBuilder()
+          .SetType("SchemaType")
+          .AddProperty(
+              PropertyConfigBuilder()
+                  .SetName("string")
+                  .SetDataTypeString(TERM_MATCH_PREFIX, TOKENIZER_PLAIN)
+                  .SetCardinality(CARDINALITY_REQUIRED))
+          .AddProperty(
+              PropertyConfigBuilder()
+                  .SetName("document")
+                  .SetDataTypeDocument("NestedType",
+                                       /*index_nested_properties=*/true)
+                  .SetCardinality(CARDINALITY_REQUIRED))
+          .AddProperty(PropertyConfigBuilder()
+                           .SetName("boolean")
+                           .SetDataType(TYPE_BOOLEAN)
+                           .SetCardinality(CARDINALITY_REQUIRED))
+          .AddProperty(PropertyConfigBuilder()
+                           .SetName("int64")
+                           .SetDataType(TYPE_INT64)
+                           .SetCardinality(CARDINALITY_REQUIRED))
+          .AddProperty(PropertyConfigBuilder()
+                           .SetName("double")
+                           .SetDataType(TYPE_DOUBLE)
+                           .SetCardinality(CARDINALITY_REQUIRED))
+          .AddProperty(PropertyConfigBuilder()
+                           .SetName("byte")
+                           .SetDataType(TYPE_BYTES)
+                           .SetCardinality(CARDINALITY_REQUIRED))
+          .AddProperty(PropertyConfigBuilder()
+                           .SetName("blobHandle")
+                           .SetDataType(TYPE_BLOB_HANDLE)
+                           .SetCardinality(CARDINALITY_REQUIRED))
+          .Build();
+  SchemaProto schema =
+      SchemaBuilder()
+          .AddType(SchemaTypeConfigBuilder().SetType("NestedType").Build())
+          .AddType(schema_type_config)
+          .Build();
 
-  EXPECT_THAT(icing.SetSchema(CreateMessageSchema()).status(), ProtoIsOk());
+  EXPECT_THAT(icing.SetSchema(schema).status(), ProtoIsOk());
 
   GetSchemaTypeResultProto expected_get_schema_type_result_proto;
   expected_get_schema_type_result_proto.mutable_status()->set_code(
       StatusProto::OK);
   *expected_get_schema_type_result_proto.mutable_schema_type_config() =
-      CreateMessageSchema().types(0);
-  EXPECT_THAT(icing.GetSchemaType(CreateMessageSchema().types(0).schema_type()),
+      schema_type_config;
+  EXPECT_THAT(icing.GetSchemaType("SchemaType"),
               EqualsProto(expected_get_schema_type_result_proto));
 }
 
diff --git a/icing/icing-search-engine_search_test.cc b/icing/icing-search-engine_search_test.cc
index db97ac4..72121ac 100644
--- a/icing/icing-search-engine_search_test.cc
+++ b/icing/icing-search-engine_search_test.cc
@@ -71,6 +71,7 @@ using ::testing::DoubleNear;
 using ::testing::ElementsAre;
 using ::testing::Eq;
 using ::testing::Gt;
+using ::testing::HasSubstr;
 using ::testing::IsEmpty;
 using ::testing::Lt;
 using ::testing::Ne;
@@ -7317,7 +7318,6 @@ TEST_F(IcingSearchEngineSearchTest, EmbeddingSearch) {
       SearchSpecProto::EmbeddingQueryMetricType::DOT_PRODUCT);
   search_spec.add_enabled_features(
       std::string(kListFilterQueryLanguageFeature));
-  search_spec.add_enabled_features(std::string(kEmbeddingSearchFeature));
 
   // Add an embedding query with semantic scores:
   // - document 0: -0.5 (embedding1), 0.3 (embedding2)
@@ -7342,10 +7342,9 @@ TEST_F(IcingSearchEngineSearchTest, EmbeddingSearch) {
   // The scoring expression for each doc will be evaluated as:
   // - document 0: sum({-0.5, 0.3}) + sum({}) = -0.2
   // - document 1: sum({-0.9}) + sum({}) = -0.9
-  search_spec.set_query("semanticSearch(getSearchSpecEmbedding(0), -1)");
+  search_spec.set_query("semanticSearch(getEmbeddingParameter(0), -1)");
   scoring_spec.set_advanced_scoring_expression(
-      "sum(this.matchedSemanticScores(getSearchSpecEmbedding(0))) + "
-      "sum(this.matchedSemanticScores(getSearchSpecEmbedding(1)))");
+      "sum(this.matchedSemanticScores(getEmbeddingParameter(0)))");
   SearchResultProto results = icing.Search(search_spec, scoring_spec,
                                            ResultSpecProto::default_instance());
   EXPECT_THAT(results.status(), ProtoIsOk());
@@ -7366,9 +7365,9 @@ TEST_F(IcingSearchEngineSearchTest, EmbeddingSearch) {
   // - document 0: sum({-0.5}) = -0.5
   // - document 1: sum({-0.9}) = -0.9
   search_spec.set_query(
-      "embedding1:semanticSearch(getSearchSpecEmbedding(0), -1)");
+      "embedding1:semanticSearch(getEmbeddingParameter(0), -1)");
   scoring_spec.set_advanced_scoring_expression(
-      "sum(this.matchedSemanticScores(getSearchSpecEmbedding(0)))");
+      "sum(this.matchedSemanticScores(getEmbeddingParameter(0)))");
   results = icing.Search(search_spec, scoring_spec,
                          ResultSpecProto::default_instance());
   EXPECT_THAT(results.status(), ProtoIsOk());
@@ -7384,9 +7383,9 @@ TEST_F(IcingSearchEngineSearchTest, EmbeddingSearch) {
   // - document 0: -0.5 (embedding2)
   // The scoring expression for each doc will be evaluated as:
   // - document 0: sum({-0.5}) = -0.5
-  search_spec.set_query("semanticSearch(getSearchSpecEmbedding(1), -1.5)");
+  search_spec.set_query("semanticSearch(getEmbeddingParameter(1), -1.5)");
   scoring_spec.set_advanced_scoring_expression(
-      "sum(this.matchedSemanticScores(getSearchSpecEmbedding(1)))");
+      "sum(this.matchedSemanticScores(getEmbeddingParameter(1)))");
   results = icing.Search(search_spec, scoring_spec,
                          ResultSpecProto::default_instance());
   EXPECT_THAT(results.status(), ProtoIsOk());
@@ -7400,9 +7399,9 @@ TEST_F(IcingSearchEngineSearchTest, EmbeddingSearch) {
   // - document 1: -2.1 (embedding2)
   // The scoring expression for each doc will be evaluated as:
   // - document 1: sum({-2.1}) = -2.1
-  search_spec.set_query("semanticSearch(getSearchSpecEmbedding(1), -10, -1)");
+  search_spec.set_query("semanticSearch(getEmbeddingParameter(1), -10, -1)");
   scoring_spec.set_advanced_scoring_expression(
-      "sum(this.matchedSemanticScores(getSearchSpecEmbedding(1)))");
+      "sum(this.matchedSemanticScores(getEmbeddingParameter(1)))");
   results = icing.Search(search_spec, scoring_spec,
                          ResultSpecProto::default_instance());
   EXPECT_THAT(results.status(), ProtoIsOk());
@@ -7419,11 +7418,11 @@ TEST_F(IcingSearchEngineSearchTest, EmbeddingSearch) {
   // - document 0: sum({-0.5, 0.3}) + sum({-0.5}) = -0.7
   // - document 1: sum({-0.9}) + sum({-2.1}) = -3
   search_spec.set_query(
-      "semanticSearch(getSearchSpecEmbedding(0)) OR "
-      "semanticSearch(getSearchSpecEmbedding(1))");
+      "semanticSearch(getEmbeddingParameter(0)) OR "
+      "semanticSearch(getEmbeddingParameter(1))");
   scoring_spec.set_advanced_scoring_expression(
-      "sum(this.matchedSemanticScores(getSearchSpecEmbedding(0))) + "
-      "sum(this.matchedSemanticScores(getSearchSpecEmbedding(1)))");
+      "sum(this.matchedSemanticScores(getEmbeddingParameter(0))) + "
+      "sum(this.matchedSemanticScores(getEmbeddingParameter(1)))");
   results = icing.Search(search_spec, scoring_spec,
                          ResultSpecProto::default_instance());
   EXPECT_THAT(results.status(), ProtoIsOk());
@@ -7442,9 +7441,9 @@ TEST_F(IcingSearchEngineSearchTest, EmbeddingSearch) {
   // - document 0: sum({}) = 0
   // - document 1: sum({-2.1}) = -2.1
   search_spec.set_query(
-      "foo OR semanticSearch(getSearchSpecEmbedding(1), -10, -1)");
+      "foo OR semanticSearch(getEmbeddingParameter(1), -10, -1)");
   scoring_spec.set_advanced_scoring_expression(
-      "sum(this.matchedSemanticScores(getSearchSpecEmbedding(1)))");
+      "sum(this.matchedSemanticScores(getEmbeddingParameter(1)))");
   results = icing.Search(search_spec, scoring_spec,
                          ResultSpecProto::default_instance());
   EXPECT_THAT(results.status(), ProtoIsOk());
@@ -7456,6 +7455,75 @@ TEST_F(IcingSearchEngineSearchTest, EmbeddingSearch) {
   EXPECT_THAT(results.results(1).score(), DoubleNear(-2.1, kEps));
 }
 
+TEST_F(IcingSearchEngineSearchTest, CannotScoreUnqueriedEmbedding) {
+  SchemaProto schema =
+      SchemaBuilder()
+          .AddType(SchemaTypeConfigBuilder()
+                       .SetType("Email")
+                       .AddProperty(PropertyConfigBuilder()
+                                        .SetName("body")
+                                        .SetDataTypeString(TERM_MATCH_EXACT,
+                                                           TOKENIZER_PLAIN)
+                                        .SetCardinality(CARDINALITY_REPEATED))
+                       .AddProperty(PropertyConfigBuilder()
+                                        .SetName("embedding")
+                                        .SetDataTypeVector(
+                                            EMBEDDING_INDEXING_LINEAR_SEARCH)
+                                        .SetCardinality(CARDINALITY_REPEATED)))
+          .Build();
+  DocumentProto document =
+      DocumentBuilder()
+          .SetKey("icing", "uri0")
+          .SetSchema("Email")
+          .SetCreationTimestampMs(1)
+          .AddStringProperty("body", "foo")
+          .AddVectorProperty(
+              "embedding", CreateVector("my_model", {0.1, 0.2, 0.3, 0.4, 0.5}))
+          .Build();
+
+  IcingSearchEngine icing(GetDefaultIcingOptions(), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+  ASSERT_THAT(icing.SetSchema(schema).status(), ProtoIsOk());
+  ASSERT_THAT(icing.Put(document).status(), ProtoIsOk());
+
+  SearchSpecProto search_spec;
+  search_spec.set_term_match_type(TermMatchType::EXACT_ONLY);
+  search_spec.set_embedding_query_metric_type(
+      SearchSpecProto::EmbeddingQueryMetricType::DOT_PRODUCT);
+  search_spec.add_enabled_features(
+      std::string(kListFilterQueryLanguageFeature));
+  *search_spec.add_embedding_query_vectors() =
+      CreateVector("my_model", {1, -1, -1, 1, -1});
+  *search_spec.add_embedding_query_vectors() =
+      CreateVector("my_model", {-1, -1, 1});
+  ScoringSpecProto scoring_spec = GetDefaultScoringSpec();
+  scoring_spec.set_rank_by(
+      ScoringSpecProto::RankingStrategy::ADVANCED_SCORING_EXPRESSION);
+
+  // Query with DOT_PRODUCT but score with COSINE.
+  search_spec.set_query(
+      "semanticSearch(getEmbeddingParameter(0), -1, 1, \"DOT_PRODUCT\")");
+  scoring_spec.set_advanced_scoring_expression(
+      "sum(this.matchedSemanticScores(getEmbeddingParameter(0), \"COSINE\"))");
+  SearchResultProto results = icing.Search(search_spec, scoring_spec,
+                                           ResultSpecProto::default_instance());
+  EXPECT_THAT(results.status(), ProtoStatusIs(StatusProto::INVALID_ARGUMENT));
+  EXPECT_THAT(results.status().message(),
+              HasSubstr("embedding query index 0 with metric type COSINE has "
+                        "not been queried"));
+
+  // Query with embedding index 0 but score with embedding index 1.
+  search_spec.set_query("semanticSearch(getEmbeddingParameter(0), -1, 1)");
+  scoring_spec.set_advanced_scoring_expression(
+      "sum(this.matchedSemanticScores(getEmbeddingParameter(1)))");
+  results = icing.Search(search_spec, scoring_spec,
+                         ResultSpecProto::default_instance());
+  EXPECT_THAT(results.status(), ProtoStatusIs(StatusProto::INVALID_ARGUMENT));
+  EXPECT_THAT(results.status().message(),
+              HasSubstr("embedding query index 1 with metric type DOT_PRODUCT "
+                        "has not been queried"));
+}
+
 TEST_F(IcingSearchEngineSearchTest, AdditionalScores) {
   SchemaProto schema =
       SchemaBuilder()
@@ -7500,7 +7568,7 @@ TEST_F(IcingSearchEngineSearchTest, AdditionalScores) {
   SearchSpecProto search_spec;
   search_spec.set_term_match_type(TermMatchType::EXACT_ONLY);
   search_spec.set_query(
-      "foo OR semanticSearch(getSearchSpecEmbedding(0), 0, 1)");
+      "foo OR semanticSearch(getEmbeddingParameter(0), 0, 1)");
   // Add an embedding query with semantic scores:
   // - document 0: 0.9 (embedding)
   // - document 1: 0.5 (embedding)
@@ -7510,25 +7578,24 @@ TEST_F(IcingSearchEngineSearchTest, AdditionalScores) {
       SearchSpecProto::EmbeddingQueryMetricType::DOT_PRODUCT);
   search_spec.add_enabled_features(
       std::string(kListFilterQueryLanguageFeature));
-  search_spec.add_enabled_features(std::string(kEmbeddingSearchFeature));
 
   // Create a scoring spec that:
-  // - Uses sum(this.matchedSemanticScores(getSearchSpecEmbedding(0))) for
+  // - Uses sum(this.matchedSemanticScores(getEmbeddingParameter(0))) for
   //   ranking.
   // - Configures the following additional scores:
   //   - this.relevanceScore()
   //   - this.relevanceScore() +
-  //     sum(this.matchedSemanticScores(getSearchSpecEmbedding(1)))
+  //     sum(this.matchedSemanticScores(getEmbeddingParameter(1)))
   ScoringSpecProto scoring_spec = GetDefaultScoringSpec();
   scoring_spec.set_rank_by(
       ScoringSpecProto::RankingStrategy::ADVANCED_SCORING_EXPRESSION);
   scoring_spec.set_advanced_scoring_expression(
-      "sum(this.matchedSemanticScores(getSearchSpecEmbedding(0)))");
+      "sum(this.matchedSemanticScores(getEmbeddingParameter(0)))");
   scoring_spec.add_additional_advanced_scoring_expressions(
       "this.relevanceScore()");
   scoring_spec.add_additional_advanced_scoring_expressions(
       "this.relevanceScore() + "
-      "sum(this.matchedSemanticScores(getSearchSpecEmbedding(0)))");
+      "sum(this.matchedSemanticScores(getEmbeddingParameter(0)))");
   SearchResultProto results = icing.Search(search_spec, scoring_spec,
                                            ResultSpecProto::default_instance());
   EXPECT_THAT(results.status(), ProtoIsOk());
@@ -7621,13 +7688,12 @@ TEST_F(IcingSearchEngineSearchTest,
       SearchSpecProto::EmbeddingQueryMetricType::DOT_PRODUCT);
   search_spec.add_enabled_features(
       std::string(kListFilterQueryLanguageFeature));
-  search_spec.add_enabled_features(std::string(kEmbeddingSearchFeature));
 
   // Create an embedding query with a range that should not match any embedding
   // hits.
   *search_spec.add_embedding_query_vectors() =
       CreateVector("my_model", {1, 1, 1});
-  search_spec.set_query("semanticSearch(getSearchSpecEmbedding(0), 100)");
+  search_spec.set_query("semanticSearch(getEmbeddingParameter(0), 100)");
 
   SearchResultProto results = icing.Search(search_spec, GetDefaultScoringSpec(),
                                            ResultSpecProto::default_instance());
@@ -7635,6 +7701,128 @@ TEST_F(IcingSearchEngineSearchTest,
   EXPECT_THAT(results.results(), IsEmpty());
 }
 
+TEST_F(IcingSearchEngineSearchTest, SearchWithPropertyFiltersEmbedding) {
+  SchemaProto schema =
+      SchemaBuilder()
+          .AddType(SchemaTypeConfigBuilder()
+                       .SetType("Email")
+                       .AddProperty(PropertyConfigBuilder()
+                                        .SetName("body")
+                                        .SetDataTypeString(TERM_MATCH_EXACT,
+                                                           TOKENIZER_PLAIN)
+                                        .SetCardinality(CARDINALITY_REPEATED))
+                       .AddProperty(PropertyConfigBuilder()
+                                        .SetName("embedding1")
+                                        .SetDataTypeVector(
+                                            EMBEDDING_INDEXING_LINEAR_SEARCH)
+                                        .SetCardinality(CARDINALITY_REPEATED))
+                       .AddProperty(PropertyConfigBuilder()
+                                        .SetName("embedding2")
+                                        .SetDataTypeVector(
+                                            EMBEDDING_INDEXING_LINEAR_SEARCH)
+                                        .SetCardinality(CARDINALITY_REPEATED))
+                       .AddProperty(PropertyConfigBuilder()
+                                        .SetName("embedding3")
+                                        .SetDataTypeVector(
+                                            EMBEDDING_INDEXING_LINEAR_SEARCH)
+                                        .SetCardinality(CARDINALITY_REPEATED)))
+          .Build();
+  DocumentProto document0 =
+      DocumentBuilder()
+          .SetKey("icing", "uri0")
+          .SetSchema("Email")
+          .SetCreationTimestampMs(1)
+          .AddStringProperty("body", "foo")
+          .AddVectorProperty("embedding1",
+                             CreateVector("my_model", {0.1, 0.2, 0.3}))
+          .Build();
+  DocumentProto document1 =
+      DocumentBuilder()
+          .SetKey("icing", "uri1")
+          .SetSchema("Email")
+          .SetCreationTimestampMs(1)
+          .AddStringProperty("body", "foo")
+          .AddVectorProperty("embedding2",
+                             CreateVector("my_model", {-0.1, -0.2, -0.3}))
+          .Build();
+  DocumentProto document2 =
+      DocumentBuilder()
+          .SetKey("icing", "uri2")
+          .SetSchema("Email")
+          .SetCreationTimestampMs(1)
+          .AddStringProperty("body", "foo")
+          .AddVectorProperty("embedding3",
+                             CreateVector("my_model", {1.0, 2.0, 3.0}))
+          .Build();
+  DocumentProto document3 =
+      DocumentBuilder()
+          .SetKey("icing", "uri3")
+          .SetSchema("Email")
+          .SetCreationTimestampMs(1)
+          .AddStringProperty("body", "foo")
+          .AddVectorProperty("embedding1",
+                             CreateVector("my_model", {0.1, 0.2, 0.3}))
+          .AddVectorProperty("embedding2",
+                             CreateVector("my_model", {-0.1, -0.2, -0.3}))
+          .AddVectorProperty("embedding3",
+                             CreateVector("my_model", {1.0, 2.0, 3.0}))
+          .Build();
+
+  IcingSearchEngine icing(GetDefaultIcingOptions(), GetTestJniCache());
+  ASSERT_THAT(icing.Initialize().status(), ProtoIsOk());
+  ASSERT_THAT(icing.SetSchema(schema).status(), ProtoIsOk());
+  ASSERT_THAT(icing.Put(document0).status(), ProtoIsOk());
+  ASSERT_THAT(icing.Put(document1).status(), ProtoIsOk());
+  ASSERT_THAT(icing.Put(document2).status(), ProtoIsOk());
+  ASSERT_THAT(icing.Put(document3).status(), ProtoIsOk());
+
+  SearchSpecProto search_spec;
+  search_spec.set_term_match_type(TermMatchType::EXACT_ONLY);
+  search_spec.set_embedding_query_metric_type(
+      SearchSpecProto::EmbeddingQueryMetricType::DOT_PRODUCT);
+  search_spec.add_enabled_features(
+      std::string(kListFilterQueryLanguageFeature));
+  *search_spec.add_embedding_query_vectors() =
+      CreateVector("my_model", {1, 1, 1});
+
+  // Add a property filter for embedding1 and embedding2.
+  TypePropertyMask* email_property_filters =
+      search_spec.add_type_property_filters();
+  email_property_filters->set_schema_type("Email");
+  email_property_filters->add_paths("embedding1");
+  email_property_filters->add_paths("embedding2");
+
+  // Only the documents that have embedding1 or embedding2 will be returned.
+  search_spec.set_query("semanticSearch(getEmbeddingParameter(0))");
+  SearchResultProto results =
+      icing.Search(search_spec, ScoringSpecProto::default_instance(),
+                   ResultSpecProto::default_instance());
+  EXPECT_THAT(results.status(), ProtoIsOk());
+  EXPECT_THAT(results.results(), SizeIs(3));
+  EXPECT_THAT(results.results(0).document(), EqualsProto(document3));
+  EXPECT_THAT(results.results(1).document(), EqualsProto(document1));
+  EXPECT_THAT(results.results(2).document(), EqualsProto(document0));
+
+  // Test that the property filters in SearchSpecProto can work together with
+  // the property filter syntax in the query. Since they both filter on
+  // embedding1, only the documents that have embedding1 will be returned.
+  search_spec.set_query("embedding1:semanticSearch(getEmbeddingParameter(0))");
+  results = icing.Search(search_spec, ScoringSpecProto::default_instance(),
+                         ResultSpecProto::default_instance());
+  EXPECT_THAT(results.status(), ProtoIsOk());
+  EXPECT_THAT(results.results(), SizeIs(2));
+  EXPECT_THAT(results.results(0).document(), EqualsProto(document3));
+  EXPECT_THAT(results.results(1).document(), EqualsProto(document0));
+
+  // No documents will be returned since the property filters in SearchSpecProto
+  // and the property filters in the query are different.
+  search_spec.set_query("embedding3:semanticSearch(getEmbeddingParameter(0))");
+  results = icing.Search(search_spec, ScoringSpecProto::default_instance(),
+                         ResultSpecProto::default_instance());
+  EXPECT_THAT(results.status(), ProtoIsOk());
+  EXPECT_THAT(results.results(), IsEmpty());
+}
+
 }  // namespace
 }  // namespace lib
 }  // namespace icing
diff --git a/icing/icing-search-engine_test.cc b/icing/icing-search-engine_test.cc
index ddb83a8..9ef1dfc 100644
--- a/icing/icing-search-engine_test.cc
+++ b/icing/icing-search-engine_test.cc
@@ -1118,6 +1118,73 @@ TEST_F(IcingSearchEngineTest, PersistToDiskLiteSavesGroundTruth) {
                                   expected_search_result_proto));
 }
 
+TEST_F(IcingSearchEngineTest, PersistToDiskRecoveryProofAvoidsRecovery) {
+  DocumentProto document = CreateMessageDocument("namespace", "uri");
+
+  IcingSearchEngine icing1(GetDefaultIcingOptions(), GetTestJniCache());
+  EXPECT_THAT(icing1.Initialize().status(), ProtoIsOk());
+  EXPECT_THAT(icing1.SetSchema(CreateMessageSchema()).status(), ProtoIsOk());
+  EXPECT_THAT(icing1.Put(document).status(), ProtoIsOk());
+  EXPECT_THAT(
+      icing1.Get("namespace", "uri", GetResultSpecProto::default_instance())
+          .document(),
+      EqualsProto(document));
+  EXPECT_THAT(icing1.PersistToDisk(PersistType::RECOVERY_PROOF).status(),
+              ProtoIsOk());
+
+  // Recreating the index should not trigger a recovery.
+  IcingSearchEngine icing2(GetDefaultIcingOptions(), GetTestJniCache());
+  InitializeResultProto init_result = icing2.Initialize();
+  EXPECT_THAT(init_result.status(), ProtoIsOk());
+  EXPECT_THAT(init_result.initialize_stats().document_store_data_status(),
+              Eq(InitializeStatsProto::NO_DATA_LOSS));
+  EXPECT_THAT(init_result.initialize_stats().schema_store_recovery_cause(),
+              Eq(InitializeStatsProto::NONE));
+  EXPECT_THAT(init_result.initialize_stats().document_store_recovery_cause(),
+              Eq(InitializeStatsProto::NONE));
+  EXPECT_THAT(init_result.initialize_stats().index_restoration_cause(),
+              Eq(InitializeStatsProto::NONE));
+  EXPECT_THAT(init_result.initialize_stats().integer_index_restoration_cause(),
+              Eq(InitializeStatsProto::NONE));
+  EXPECT_THAT(init_result.initialize_stats()
+                  .qualified_id_join_index_restoration_cause(),
+              Eq(InitializeStatsProto::NONE));
+  EXPECT_THAT(
+      init_result.initialize_stats().embedding_index_restoration_cause(),
+      Eq(InitializeStatsProto::NONE));
+
+  // Schema is still intact.
+  GetSchemaResultProto expected_get_schema_result_proto;
+  expected_get_schema_result_proto.mutable_status()->set_code(StatusProto::OK);
+  *expected_get_schema_result_proto.mutable_schema() = CreateMessageSchema();
+
+  EXPECT_THAT(icing2.GetSchema(),
+              EqualsProto(expected_get_schema_result_proto));
+
+  // The document should be found because we called
+  // PersistToDisk(RECOVERY_PROOF)!
+  EXPECT_THAT(
+      icing2.Get("namespace", "uri", GetResultSpecProto::default_instance())
+          .document(),
+      EqualsProto(document));
+
+  // The index should still be intact.
+  SearchSpecProto search_spec;
+  search_spec.set_term_match_type(TermMatchType::PREFIX);
+  search_spec.set_query("message");  // Content in the Message document.
+
+  SearchResultProto expected_search_result_proto;
+  expected_search_result_proto.mutable_status()->set_code(StatusProto::OK);
+  *expected_search_result_proto.mutable_results()->Add()->mutable_document() =
+      document;
+
+  SearchResultProto actual_results =
+      icing2.Search(search_spec, GetDefaultScoringSpec(),
+                    ResultSpecProto::default_instance());
+  EXPECT_THAT(actual_results, EqualsSearchResultIgnoreStatsAndScores(
+                                  expected_search_result_proto));
+}
+
 TEST_F(IcingSearchEngineTest, ResetOk) {
   SchemaProto message_schema = CreateMessageSchema();
   SchemaProto empty_schema = SchemaProto(message_schema);
diff --git a/icing/index/embed/doc-hit-info-iterator-embedding.cc b/icing/index/embed/doc-hit-info-iterator-embedding.cc
index b94e1c6..5a6e547 100644
--- a/icing/index/embed/doc-hit-info-iterator-embedding.cc
+++ b/icing/index/embed/doc-hit-info-iterator-embedding.cc
@@ -41,7 +41,6 @@ namespace lib {
 libtextclassifier3::StatusOr<std::unique_ptr<DocHitInfoIteratorEmbedding>>
 DocHitInfoIteratorEmbedding::Create(
     const PropertyProto::VectorProto* query,
-    std::unique_ptr<SectionRestrictData> section_restrict_data,
     SearchSpecProto::EmbeddingQueryMetricType::Code metric_type,
     double score_low, double score_high,
     EmbeddingQueryResults::EmbeddingQueryScoreMap* score_map,
@@ -68,10 +67,9 @@ DocHitInfoIteratorEmbedding::Create(
                          EmbeddingScorer::Create(metric_type));
 
   return std::unique_ptr<DocHitInfoIteratorEmbedding>(
-      new DocHitInfoIteratorEmbedding(query, std::move(section_restrict_data),
-                                      metric_type, std::move(embedding_scorer),
-                                      score_low, score_high, score_map,
-                                      embedding_index, std::move(pl_accessor)));
+      new DocHitInfoIteratorEmbedding(
+          query, metric_type, std::move(embedding_scorer), score_low,
+          score_high, score_map, embedding_index, std::move(pl_accessor)));
 }
 
 libtextclassifier3::StatusOr<const EmbeddingHit*>
@@ -89,11 +87,8 @@ DocHitInfoIteratorEmbedding::AdvanceToNextEmbeddingHit() {
       cached_embedding_hits_[cached_embedding_hits_idx_];
   if (doc_hit_info_.document_id() == kInvalidDocumentId) {
     doc_hit_info_.set_document_id(embedding_hit.basic_hit().document_id());
-    if (section_restrict_data_ != nullptr) {
-      current_allowed_sections_mask_ =
-          section_restrict_data_->ComputeAllowedSectionsMask(
-              doc_hit_info_.document_id());
-    }
+    current_allowed_sections_mask_ =
+        ComputeAllowedSectionsMask(doc_hit_info_.document_id());
   } else if (doc_hit_info_.document_id() !=
              embedding_hit.basic_hit().document_id()) {
     return nullptr;
diff --git a/icing/index/embed/doc-hit-info-iterator-embedding.h b/icing/index/embed/doc-hit-info-iterator-embedding.h
index 85f3eae..ae126c8 100644
--- a/icing/index/embed/doc-hit-info-iterator-embedding.h
+++ b/icing/index/embed/doc-hit-info-iterator-embedding.h
@@ -37,7 +37,8 @@
 namespace icing {
 namespace lib {
 
-class DocHitInfoIteratorEmbedding : public DocHitInfoLeafIterator {
+class DocHitInfoIteratorEmbedding
+    : public DocHitInfoIteratorHandlingSectionRestrict {
  public:
   // Create a DocHitInfoIterator for iterating through all docs which have an
   // embedding matched with the provided query with a score in the range of
@@ -46,8 +47,8 @@ class DocHitInfoIteratorEmbedding : public DocHitInfoLeafIterator {
   // The iterator will store the matched embedding scores in score_map to
   // prepare for scoring.
   //
-  // The iterator will handle the section restriction logic internally by the
-  // provided section_restrict_data.
+  // The iterator will handle the section restriction logic internally with the
+  // help of DocHitInfoIteratorHandlingSectionRestrict.
   //
   // Returns:
   //   - a DocHitInfoIteratorEmbedding instance on success.
@@ -55,7 +56,6 @@ class DocHitInfoIteratorEmbedding : public DocHitInfoLeafIterator {
   static libtextclassifier3::StatusOr<
       std::unique_ptr<DocHitInfoIteratorEmbedding>>
   Create(const PropertyProto::VectorProto* query,
-         std::unique_ptr<SectionRestrictData> section_restrict_data,
          SearchSpecProto::EmbeddingQueryMetricType::Code metric_type,
          double score_low, double score_high,
          EmbeddingQueryResults::EmbeddingQueryScoreMap* score_map,
@@ -63,12 +63,6 @@ class DocHitInfoIteratorEmbedding : public DocHitInfoLeafIterator {
 
   libtextclassifier3::Status Advance() override;
 
-  // The iterator will internally handle the section restriction logic by itself
-  // to have better control, so that it is able to filter out embedding hits
-  // from unwanted sections to avoid retrieving unnecessary vectors and
-  // calculate scores for them.
-  bool full_section_restriction_applied() const override { return true; }
-
   libtextclassifier3::StatusOr<TrimmedNode> TrimRightMostNode() && override {
     return absl_ports::InvalidArgumentError(
         "Query suggestions for the semanticSearch function are not supported");
@@ -93,7 +87,6 @@ class DocHitInfoIteratorEmbedding : public DocHitInfoLeafIterator {
  private:
   explicit DocHitInfoIteratorEmbedding(
       const PropertyProto::VectorProto* query,
-      std::unique_ptr<SectionRestrictData> section_restrict_data,
       SearchSpecProto::EmbeddingQueryMetricType::Code metric_type,
       std::unique_ptr<EmbeddingScorer> embedding_scorer, double score_low,
       double score_high,
@@ -101,7 +94,6 @@ class DocHitInfoIteratorEmbedding : public DocHitInfoLeafIterator {
       const EmbeddingIndex* embedding_index,
       std::unique_ptr<PostingListEmbeddingHitAccessor> posting_list_accessor)
       : query_(*query),
-        section_restrict_data_(std::move(section_restrict_data)),
         metric_type_(metric_type),
         embedding_scorer_(std::move(embedding_scorer)),
         score_low_(score_low),
@@ -140,8 +132,7 @@ class DocHitInfoIteratorEmbedding : public DocHitInfoLeafIterator {
   libtextclassifier3::Status AdvanceToNextUnfilteredDocument();
 
   // Query information
-  const PropertyProto::VectorProto& query_;                     // Does not own
-  std::unique_ptr<SectionRestrictData> section_restrict_data_;  // Nullable.
+  const PropertyProto::VectorProto& query_;  // Does not own
 
   // Scoring arguments
   SearchSpecProto::EmbeddingQueryMetricType::Code metric_type_;
diff --git a/icing/index/embed/embedding-index.cc b/icing/index/embed/embedding-index.cc
index 63381fd..0f8ad53 100644
--- a/icing/index/embed/embedding-index.cc
+++ b/icing/index/embed/embedding-index.cc
@@ -448,15 +448,14 @@ libtextclassifier3::Status EmbeddingIndex::Optimize(
   return Initialize();
 }
 
-libtextclassifier3::Status EmbeddingIndex::PersistMetadataToDisk(bool force) {
+libtextclassifier3::Status EmbeddingIndex::PersistMetadataToDisk() {
   return metadata_mmapped_file_->PersistToDisk();
 }
 
-libtextclassifier3::Status EmbeddingIndex::PersistStoragesToDisk(bool force) {
+libtextclassifier3::Status EmbeddingIndex::PersistStoragesToDisk() {
   if (is_empty()) {
     return libtextclassifier3::Status::OK;
   }
-
   if (!flash_index_storage_->PersistToDisk()) {
     return absl_ports::InternalError("Fail to persist flash index to disk");
   }
@@ -465,20 +464,26 @@ libtextclassifier3::Status EmbeddingIndex::PersistStoragesToDisk(bool force) {
   return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::StatusOr<Crc32> EmbeddingIndex::ComputeInfoChecksum(
-    bool force) {
-  return info().ComputeChecksum();
+libtextclassifier3::StatusOr<Crc32> EmbeddingIndex::UpdateStoragesChecksum() {
+  if (is_empty()) {
+    return Crc32(0);
+  }
+  ICING_ASSIGN_OR_RETURN(Crc32 embedding_posting_list_mapper_crc,
+                         embedding_posting_list_mapper_->UpdateChecksum());
+  ICING_ASSIGN_OR_RETURN(Crc32 embedding_vectors_crc,
+                         embedding_vectors_->UpdateChecksum());
+  return Crc32(embedding_posting_list_mapper_crc.Get() ^
+               embedding_vectors_crc.Get());
 }
 
-libtextclassifier3::StatusOr<Crc32> EmbeddingIndex::ComputeStoragesChecksum(
-    bool force) {
+libtextclassifier3::StatusOr<Crc32> EmbeddingIndex::GetStoragesChecksum()
+    const {
   if (is_empty()) {
     return Crc32(0);
   }
   ICING_ASSIGN_OR_RETURN(Crc32 embedding_posting_list_mapper_crc,
-                         embedding_posting_list_mapper_->ComputeChecksum());
-  ICING_ASSIGN_OR_RETURN(Crc32 embedding_vectors_crc,
-                         embedding_vectors_->ComputeChecksum());
+                         embedding_posting_list_mapper_->GetChecksum());
+  Crc32 embedding_vectors_crc = embedding_vectors_->GetChecksum();
   return Crc32(embedding_posting_list_mapper_crc.Get() ^
                embedding_vectors_crc.Get());
 }
diff --git a/icing/index/embed/embedding-index.h b/icing/index/embed/embedding-index.h
index bf91a83..dd589c9 100644
--- a/icing/index/embed/embedding-index.h
+++ b/icing/index/embed/embedding-index.h
@@ -55,7 +55,7 @@ class EmbeddingIndex : public PersistentStorage {
     // Padding exists just to reserve space for additional values.
     uint8_t padding_[kPaddingSize];
 
-    Crc32 ComputeChecksum() const {
+    Crc32 GetChecksum() const {
       return Crc32(
           std::string_view(reinterpret_cast<const char*>(this), sizeof(Info)));
     }
@@ -233,33 +233,23 @@ class EmbeddingIndex : public PersistentStorage {
       const std::vector<DocumentId>& document_id_old_to_new,
       EmbeddingIndex* new_index) const;
 
-  // Flushes contents of metadata file.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
-  libtextclassifier3::Status PersistMetadataToDisk(bool force) override;
+  libtextclassifier3::Status PersistMetadataToDisk() override;
 
-  // Flushes contents of all storages to underlying files.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
-  libtextclassifier3::Status PersistStoragesToDisk(bool force) override;
+  libtextclassifier3::Status PersistStoragesToDisk() override;
 
-  // Computes and returns Info checksum.
-  //
-  // Returns:
-  //   - Crc of the Info on success
-  libtextclassifier3::StatusOr<Crc32> ComputeInfoChecksum(bool force) override;
+  libtextclassifier3::Status WriteMetadata() override {
+    // EmbeddingIndex::Header is mmapped. Therefore, writes occur when the
+    // metadata is modified. So just return OK.
+    return libtextclassifier3::Status::OK;
+  }
 
-  // Computes and returns all storages checksum.
-  //
-  // Returns:
-  //   - Crc of all storages on success
-  //   - INTERNAL_ERROR if any data inconsistency
-  libtextclassifier3::StatusOr<Crc32> ComputeStoragesChecksum(
-      bool force) override;
+  libtextclassifier3::StatusOr<Crc32> UpdateStoragesChecksum() override;
+
+  libtextclassifier3::StatusOr<Crc32> GetInfoChecksum() const override {
+    return info().GetChecksum();
+  }
+
+  libtextclassifier3::StatusOr<Crc32> GetStoragesChecksum() const override;
 
   Crcs& crcs() override {
     return *reinterpret_cast<Crcs*>(metadata_mmapped_file_->mutable_region() +
diff --git a/icing/index/embed/embedding-index_test.cc b/icing/index/embed/embedding-index_test.cc
index baa7b94..6171e92 100644
--- a/icing/index/embed/embedding-index_test.cc
+++ b/icing/index/embed/embedding-index_test.cc
@@ -37,6 +37,7 @@
 #include "icing/testing/common-matchers.h"
 #include "icing/testing/embedding-test-utils.h"
 #include "icing/testing/tmp-directory.h"
+#include "icing/util/crc32.h"
 #include "icing/util/status-macros.h"
 
 namespace icing {
@@ -45,6 +46,7 @@ namespace lib {
 namespace {
 
 using ::testing::ElementsAre;
+using ::testing::Eq;
 using ::testing::IsEmpty;
 using ::testing::Test;
 
@@ -64,12 +66,18 @@ class EmbeddingIndexTest : public Test {
 
   libtextclassifier3::StatusOr<std::vector<EmbeddingHit>> GetHits(
       uint32_t dimension, std::string_view model_signature) {
+    return GetHits(embedding_index_.get(), dimension, model_signature);
+  }
+
+  static libtextclassifier3::StatusOr<std::vector<EmbeddingHit>> GetHits(
+      const EmbeddingIndex* embedding_index, uint32_t dimension,
+      std::string_view model_signature) {
     std::vector<EmbeddingHit> hits;
 
     libtextclassifier3::StatusOr<
         std::unique_ptr<PostingListEmbeddingHitAccessor>>
         pl_accessor_or =
-            embedding_index_->GetAccessor(dimension, model_signature);
+            embedding_index->GetAccessor(dimension, model_signature);
     std::unique_ptr<PostingListEmbeddingHitAccessor> pl_accessor;
     if (pl_accessor_or.ok()) {
       pl_accessor = std::move(pl_accessor_or).ValueOrDie();
@@ -90,13 +98,16 @@ class EmbeddingIndexTest : public Test {
   }
 
   std::vector<float> GetRawEmbeddingData() {
-    auto data_or = embedding_index_->GetRawEmbeddingData();
-    if (!data_or.ok()) {
-      return std::vector<float>();
-    }
-    return std::vector<float>(
-        data_or.ValueOrDie(),
-        data_or.ValueOrDie() + embedding_index_->GetTotalVectorSize());
+    return GetRawEmbeddingData(embedding_index_.get());
+  }
+
+  static std::vector<float> GetRawEmbeddingData(
+      const EmbeddingIndex* embedding_index) {
+    ICING_ASSIGN_OR_RETURN(const float* data,
+                           embedding_index->GetRawEmbeddingData(),
+                           std::vector<float>());
+    return std::vector<float>(data,
+                              data + embedding_index->GetTotalVectorSize());
   }
 
   libtextclassifier3::StatusOr<bool> IndexContainsMetadataOnly() {
@@ -117,6 +128,133 @@ TEST_F(EmbeddingIndexTest, EmptyIndexContainsMetadataOnly) {
   EXPECT_THAT(IndexContainsMetadataOnly(), IsOkAndHolds(true));
 }
 
+TEST_F(EmbeddingIndexTest,
+       InitializationShouldFailWithoutPersistToDiskOrDestruction) {
+  // 1. Create index and confirm that data was properly added.
+  std::string embedding_index_dir =
+      GetTestTempDir() + "/embedding_index_test_local";
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<EmbeddingIndex> embedding_index,
+      EmbeddingIndex::Create(&filesystem_, embedding_index_dir));
+
+  PropertyProto::VectorProto vector = CreateVector("model", {0.1, 0.2, 0.3});
+  ICING_ASSERT_OK(embedding_index->BufferEmbedding(
+      BasicHit(/*section_id=*/0, /*document_id=*/0), vector));
+  ICING_ASSERT_OK(embedding_index->CommitBufferToIndex());
+  embedding_index->set_last_added_document_id(0);
+
+  EXPECT_THAT(
+      GetHits(embedding_index.get(), /*dimension=*/3,
+              /*model_signature=*/"model"),
+      IsOkAndHolds(ElementsAre(EmbeddingHit(
+          BasicHit(/*section_id=*/0, /*document_id=*/0), /*location=*/0))));
+  EXPECT_THAT(GetRawEmbeddingData(embedding_index.get()),
+              ElementsAre(0.1, 0.2, 0.3));
+  EXPECT_EQ(embedding_index->last_added_document_id(), 0);
+  // GetChecksum should succeed without updating the checksum.
+  ICING_EXPECT_OK(embedding_index->GetChecksum());
+
+  // 2. Try to create another index with the same directory. This should fail
+  // due to checksum mismatch.
+  EXPECT_THAT(EmbeddingIndex::Create(&filesystem_, embedding_index_dir),
+              StatusIs(libtextclassifier3::StatusCode::FAILED_PRECONDITION));
+
+  embedding_index.reset();
+  filesystem_.DeleteDirectoryRecursively(embedding_index_dir.c_str());
+}
+
+TEST_F(EmbeddingIndexTest, InitializationShouldSucceedWithUpdateChecksums) {
+  // 1. Create index and confirm that data was properly added.
+  std::string embedding_index_dir =
+      GetTestTempDir() + "/embedding_index_test_local";
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<EmbeddingIndex> embedding_index,
+      EmbeddingIndex::Create(&filesystem_, embedding_index_dir));
+
+  PropertyProto::VectorProto vector = CreateVector("model", {0.1, 0.2, 0.3});
+  ICING_ASSERT_OK(embedding_index->BufferEmbedding(
+      BasicHit(/*section_id=*/0, /*document_id=*/0), vector));
+  ICING_ASSERT_OK(embedding_index->CommitBufferToIndex());
+  embedding_index->set_last_added_document_id(0);
+
+  EXPECT_THAT(
+      GetHits(embedding_index.get(), /*dimension=*/3,
+              /*model_signature=*/"model"),
+      IsOkAndHolds(ElementsAre(EmbeddingHit(
+          BasicHit(/*section_id=*/0, /*document_id=*/0), /*location=*/0))));
+  EXPECT_THAT(GetRawEmbeddingData(embedding_index.get()),
+              ElementsAre(0.1, 0.2, 0.3));
+  EXPECT_EQ(embedding_index->last_added_document_id(), 0);
+
+  // 2. Update checksums to reflect the new content.
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc, embedding_index->GetChecksum());
+  EXPECT_THAT(embedding_index->UpdateChecksums(), IsOkAndHolds(Eq(crc)));
+  EXPECT_THAT(embedding_index->GetChecksum(), IsOkAndHolds(Eq(crc)));
+
+  // 3. Create another index and confirm that the data is still there.
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<EmbeddingIndex> embedding_index_two,
+      EmbeddingIndex::Create(&filesystem_, embedding_index_dir));
+
+  EXPECT_THAT(
+      GetHits(embedding_index_two.get(), /*dimension=*/3,
+              /*model_signature=*/"model"),
+      IsOkAndHolds(ElementsAre(EmbeddingHit(
+          BasicHit(/*section_id=*/0, /*document_id=*/0), /*location=*/0))));
+  EXPECT_THAT(GetRawEmbeddingData(embedding_index_two.get()),
+              ElementsAre(0.1, 0.2, 0.3));
+  EXPECT_EQ(embedding_index_two->last_added_document_id(), 0);
+
+  embedding_index.reset();
+  embedding_index_two.reset();
+  filesystem_.DeleteDirectoryRecursively(embedding_index_dir.c_str());
+}
+
+TEST_F(EmbeddingIndexTest, InitializationShouldSucceedWithPersistToDisk) {
+  // 1. Create index and confirm that data was properly added.
+  std::string embedding_index_dir =
+      GetTestTempDir() + "/embedding_index_test_local";
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<EmbeddingIndex> embedding_index,
+      EmbeddingIndex::Create(&filesystem_, embedding_index_dir));
+
+  PropertyProto::VectorProto vector = CreateVector("model", {0.1, 0.2, 0.3});
+  ICING_ASSERT_OK(embedding_index->BufferEmbedding(
+      BasicHit(/*section_id=*/0, /*document_id=*/0), vector));
+  ICING_ASSERT_OK(embedding_index->CommitBufferToIndex());
+  embedding_index->set_last_added_document_id(0);
+
+  EXPECT_THAT(
+      GetHits(embedding_index.get(), /*dimension=*/3,
+              /*model_signature=*/"model"),
+      IsOkAndHolds(ElementsAre(EmbeddingHit(
+          BasicHit(/*section_id=*/0, /*document_id=*/0), /*location=*/0))));
+  EXPECT_THAT(GetRawEmbeddingData(embedding_index.get()),
+              ElementsAre(0.1, 0.2, 0.3));
+  EXPECT_EQ(embedding_index->last_added_document_id(), 0);
+
+  // 2. Update checksums to reflect the new content.
+  ICING_EXPECT_OK(embedding_index->PersistToDisk());
+
+  // 3. Create another index and confirm that the data is still there.
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<EmbeddingIndex> embedding_index_two,
+      EmbeddingIndex::Create(&filesystem_, embedding_index_dir));
+
+  EXPECT_THAT(
+      GetHits(embedding_index_two.get(), /*dimension=*/3,
+              /*model_signature=*/"model"),
+      IsOkAndHolds(ElementsAre(EmbeddingHit(
+          BasicHit(/*section_id=*/0, /*document_id=*/0), /*location=*/0))));
+  EXPECT_THAT(GetRawEmbeddingData(embedding_index_two.get()),
+              ElementsAre(0.1, 0.2, 0.3));
+  EXPECT_EQ(embedding_index_two->last_added_document_id(), 0);
+
+  embedding_index.reset();
+  embedding_index_two.reset();
+  filesystem_.DeleteDirectoryRecursively(embedding_index_dir.c_str());
+}
+
 TEST_F(EmbeddingIndexTest, AddSingleEmbedding) {
   PropertyProto::VectorProto vector = CreateVector("model", {0.1, 0.2, 0.3});
   ICING_ASSERT_OK(embedding_index_->BufferEmbedding(
diff --git a/icing/index/embed/embedding-query-results.h b/icing/index/embed/embedding-query-results.h
index de85489..1679b2a 100644
--- a/icing/index/embed/embedding-query-results.h
+++ b/icing/index/embed/embedding-query-results.h
@@ -38,13 +38,12 @@ struct EmbeddingQueryResults {
                               EmbeddingQueryScoreMap>>
       result_scores;
 
-  // Returns the matched scores for the given query_vector_index, metric_type,
-  // and doc_id. Returns nullptr if (query_vector_index, metric_type) does not
-  // exist in the result_scores map.
-  const std::vector<double>* GetMatchedScoresForDocument(
+  // Get the score map for the given query_vector_index and metric_type. Returns
+  // nullptr if (query_vector_index, metric_type) does not exist in the
+  // result_scores map.
+  const EmbeddingQueryScoreMap* GetScoreMap(
       int query_vector_index,
-      SearchSpecProto::EmbeddingQueryMetricType::Code metric_type,
-      DocumentId doc_id) const {
+      SearchSpecProto::EmbeddingQueryMetricType::Code metric_type) const {
     // Check if a mapping exists for the query_vector_index
     auto outer_it = result_scores.find(query_vector_index);
     if (outer_it == result_scores.end()) {
@@ -55,11 +54,24 @@ struct EmbeddingQueryResults {
     if (inner_it == outer_it->second.end()) {
       return nullptr;
     }
-    const EmbeddingQueryScoreMap& score_map = inner_it->second;
+    return &inner_it->second;
+  }
 
+  // Returns the matched scores for the given query_vector_index, metric_type,
+  // and doc_id. Returns nullptr if (query_vector_index, metric_type, doc_id)
+  // does not exist in the result_scores map.
+  const std::vector<double>* GetMatchedScoresForDocument(
+      int query_vector_index,
+      SearchSpecProto::EmbeddingQueryMetricType::Code metric_type,
+      DocumentId doc_id) const {
+    const EmbeddingQueryScoreMap* score_map =
+        GetScoreMap(query_vector_index, metric_type);
+    if (score_map == nullptr) {
+      return nullptr;
+    }
     // Check if the doc_id exists in the score_map
-    auto scores_it = score_map.find(doc_id);
-    if (scores_it == score_map.end()) {
+    auto scores_it = score_map->find(doc_id);
+    if (scores_it == score_map->end()) {
       return nullptr;
     }
     return &scores_it->second;
diff --git a/icing/index/embedding-indexing-handler_test.cc b/icing/index/embedding-indexing-handler_test.cc
index 556ba6e..a1b3ea8 100644
--- a/icing/index/embedding-indexing-handler_test.cc
+++ b/icing/index/embedding-indexing-handler_test.cc
@@ -290,8 +290,9 @@ TEST_F(EmbeddingIndexingHandlerTest, HandleEmbeddingSection) {
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(tokenized_document.document()));
+  DocumentId document_id = put_result.new_document_id;
 
   ASSERT_THAT(embedding_index_->last_added_document_id(),
               Eq(kInvalidDocumentId));
@@ -348,8 +349,9 @@ TEST_F(EmbeddingIndexingHandlerTest, HandleNestedEmbeddingSection) {
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(tokenized_document.document()));
+  DocumentId document_id = put_result.new_document_id;
 
   ASSERT_THAT(embedding_index_->last_added_document_id(),
               Eq(kInvalidDocumentId));
@@ -462,8 +464,9 @@ TEST_F(EmbeddingIndexingHandlerTest,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(tokenized_document.document()));
+  DocumentId document_id = put_result.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<EmbeddingIndexingHandler> handler,
@@ -543,11 +546,13 @@ TEST_F(EmbeddingIndexingHandlerTest,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document2)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(tokenized_document1.document()));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(tokenized_document2.document()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<EmbeddingIndexingHandler> handler,
diff --git a/icing/index/hit/hit.cc b/icing/index/hit/hit.cc
index 7b7f2f6..6fafa81 100644
--- a/icing/index/hit/hit.cc
+++ b/icing/index/hit/hit.cc
@@ -78,13 +78,14 @@ Hit::Hit(Value value, Flags flags, TermFrequency term_frequency)
 
 Hit::Hit(SectionId section_id, DocumentId document_id,
          Hit::TermFrequency term_frequency, bool is_in_prefix_section,
-         bool is_prefix_hit)
+         bool is_prefix_hit, bool is_stemmed_hit)
     : term_frequency_(term_frequency) {
   // We compute flags first as the value's has_flags bit depends on the flags_
   // field.
   Flags temp_flags = 0;
   bit_util::BitfieldSet(term_frequency != kDefaultTermFrequency,
                         kHasTermFrequency, /*len=*/1, &temp_flags);
+  bit_util::BitfieldSet(is_stemmed_hit, kIsStemmedHit, /*len=*/1, &temp_flags);
   flags_ = temp_flags;
 
   // Values are stored so that when sorted, they appear in document_id
@@ -135,6 +136,10 @@ bool Hit::has_term_frequency() const {
   return bit_util::BitfieldGet(flags(), kHasTermFrequency, 1);
 }
 
+bool Hit::is_stemmed_hit() const {
+  return bit_util::BitfieldGet(flags(), kIsStemmedHit, 1);
+}
+
 bool Hit::CheckFlagsAreConsistent() const {
   bool has_flags = flags_ != kNoEnabledFlags;
   bool has_flags_enabled_in_value =
@@ -150,13 +155,8 @@ bool Hit::CheckFlagsAreConsistent() const {
 
 Hit Hit::TranslateHit(Hit old_hit, DocumentId new_document_id) {
   return Hit(old_hit.section_id(), new_document_id, old_hit.term_frequency(),
-             old_hit.is_in_prefix_section(), old_hit.is_prefix_hit());
-}
-
-bool Hit::EqualsDocumentIdAndSectionId::operator()(const Hit& hit1,
-                                                   const Hit& hit2) const {
-  return (hit1.value() >> kNumFlagsInValueField) ==
-         (hit2.value() >> kNumFlagsInValueField);
+             old_hit.is_in_prefix_section(), old_hit.is_prefix_hit(),
+             old_hit.is_stemmed_hit());
 }
 
 }  // namespace lib
diff --git a/icing/index/hit/hit.h b/icing/index/hit/hit.h
index e971016..10ff4e7 100644
--- a/icing/index/hit/hit.h
+++ b/icing/index/hit/hit.h
@@ -94,6 +94,8 @@ static_assert(sizeof(BasicHit) == 4, "");
 //   - bitmasks in flags fields:
 //     - whether the Hit has a TermFrequency other than the default value
 //       (has_term_frequency)
+//     - whether the Hit is a stemmed hit derived by stemming an original token
+//       in the document (is_stemmed_hit)
 //   - a term frequency for the hit
 //
 // The hit is the most basic unit of the index and, when grouped together by
@@ -128,7 +130,9 @@ class Hit {
     // Whether or not the hit has a term_frequency other than
     // kDefaultTermFrequency.
     kHasTermFrequency = 0,
-    kNumFlagsInFlagsField = 1,
+    // Whether or not the hit is a stemmed hit.
+    kIsStemmedHit = 1,
+    kNumFlagsInFlagsField = 2,
   };
 
   enum FlagOffsetsInValueField {
@@ -173,7 +177,7 @@ class Hit {
   explicit Hit(Value value, Flags flags, TermFrequency term_frequency);
   explicit Hit(SectionId section_id, DocumentId document_id,
                TermFrequency term_frequency, bool is_in_prefix_section,
-               bool is_prefix_hit);
+               bool is_prefix_hit, bool is_stemmed_hit);
 
   bool is_valid() const { return value() != kInvalidValue; }
 
@@ -191,6 +195,8 @@ class Hit {
   bool has_flags() const;
 
   Flags flags() const { return flags_; }
+  // Whether or not the hit is a stemmed hit.
+  bool is_stemmed_hit() const;
   // Whether or not the hit contains a valid term frequency.
   bool has_term_frequency() const;
 
@@ -209,12 +215,18 @@ class Hit {
     }
     return flags() < h2.flags();
   }
-  bool operator==(const Hit& h2) const {
-    return value() == h2.value() && flags() == h2.flags();
-  }
 
   struct EqualsDocumentIdAndSectionId {
-    bool operator()(const Hit& hit1, const Hit& hit2) const;
+    bool operator()(const Hit& hit1, const Hit& hit2) const {
+      return (hit1.value() >> kNumFlagsInValueField) ==
+             (hit2.value() >> kNumFlagsInValueField);
+    }
+  };
+
+  struct EqualsValueAndFlags {
+    bool operator()(const Hit& hit1, const Hit& hit2) const {
+      return hit1.value() == hit2.value() && hit1.flags() == hit2.flags();
+    }
   };
 
  private:
@@ -222,7 +234,8 @@ class Hit {
   // Value bits layout: 1 unused + 22 document_id + 6 section_id + 1
   // is_prefix_hit + 1 is_in_prefix_section + 1 has_flags.
   std::array<char, sizeof(Value)> value_;
-  // Flags bits layout: 1 reserved + 6 unused + 1 has_term_frequency.
+  // Flags bits layout: 1 reserved + 5 unused + 1 is_stemmed_hit + 1
+  // has_term_frequency.
   // The left-most bit is reserved for chaining additional fields in case of
   // future hit expansions.
   Flags flags_;
diff --git a/icing/index/hit/hit_test.cc b/icing/index/hit/hit_test.cc
index 1233e00..92a9239 100644
--- a/icing/index/hit/hit_test.cc
+++ b/icing/index/hit/hit_test.cc
@@ -21,6 +21,7 @@
 #include "gtest/gtest.h"
 #include "icing/schema/section.h"
 #include "icing/store/document-id.h"
+#include "icing/testing/common-matchers.h"
 
 namespace icing {
 namespace lib {
@@ -98,86 +99,121 @@ TEST(BasicHitTest, Comparison) {
 
 TEST(HitTest, HasTermFrequencyFlag) {
   Hit h1(kSomeSectionid, kSomeDocumentId, Hit::kDefaultTermFrequency,
-         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+         /*is_stemmed_hit=*/false);
   EXPECT_THAT(h1.has_term_frequency(), IsFalse());
   EXPECT_THAT(h1.term_frequency(), Eq(Hit::kDefaultTermFrequency));
 
   Hit h2(kSomeSectionid, kSomeDocumentId, kSomeTermFrequency,
-         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+         /*is_stemmed_hit=*/false);
   EXPECT_THAT(h2.has_term_frequency(), IsTrue());
   EXPECT_THAT(h2.term_frequency(), Eq(kSomeTermFrequency));
 }
 
+TEST(HitTest, IsStemmedHitFlag) {
+  Hit h1(kSomeSectionid, kSomeDocumentId, Hit::kDefaultTermFrequency,
+         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+         /*is_stemmed_hit=*/false);
+  EXPECT_THAT(h1.is_stemmed_hit(), IsFalse());
+
+  Hit h2(kSomeSectionid, kSomeDocumentId, kSomeTermFrequency,
+         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+         /*is_stemmed_hit=*/true);
+  EXPECT_THAT(h2.is_stemmed_hit(), IsTrue());
+}
+
 TEST(HitTest, IsPrefixHitFlag) {
   Hit h1(kSomeSectionid, kSomeDocumentId, Hit::kDefaultTermFrequency,
-         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+         /*is_stemmed_hit=*/false);
   EXPECT_THAT(h1.is_prefix_hit(), IsFalse());
 
   Hit h2(kSomeSectionid, kSomeDocumentId, Hit::kDefaultTermFrequency,
-         /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+         /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+         /*is_stemmed_hit=*/false);
   EXPECT_THAT(h2.is_prefix_hit(), IsFalse());
 
   Hit h3(kSomeSectionid, kSomeDocumentId, Hit::kDefaultTermFrequency,
-         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/true);
+         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/true,
+         /*is_stemmed_hit=*/false);
   EXPECT_THAT(h3.is_prefix_hit(), IsTrue());
 
   Hit h4(kSomeSectionid, kSomeDocumentId, kSomeTermFrequency,
-         /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+         /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+         /*is_stemmed_hit=*/false);
   EXPECT_THAT(h4.is_prefix_hit(), IsFalse());
 }
 
 TEST(HitTest, IsInPrefixSectionFlag) {
   Hit h1(kSomeSectionid, kSomeDocumentId, Hit::kDefaultTermFrequency,
-         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+         /*is_stemmed_hit=*/false);
   EXPECT_THAT(h1.is_in_prefix_section(), IsFalse());
 
   Hit h2(kSomeSectionid, kSomeDocumentId, Hit::kDefaultTermFrequency,
-         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/true,
+         /*is_stemmed_hit=*/false);
   EXPECT_THAT(h2.is_in_prefix_section(), IsFalse());
 
   Hit h3(kSomeSectionid, kSomeDocumentId, Hit::kDefaultTermFrequency,
-         /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+         /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+         /*is_stemmed_hit=*/false);
   EXPECT_THAT(h3.is_in_prefix_section(), IsTrue());
 }
 
 TEST(HitTest, HasFlags) {
   Hit h1(kSomeSectionid, kSomeDocumentId, Hit::kDefaultTermFrequency,
-         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+         /*is_stemmed_hit=*/false);
   EXPECT_THAT(h1.has_flags(), IsFalse());
 
   Hit h2(kSomeSectionid, kSomeDocumentId, Hit::kDefaultTermFrequency,
-         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/true);
+         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/true,
+         /*is_stemmed_hit=*/false);
   EXPECT_THAT(h2.has_flags(), IsFalse());
 
   Hit h3(kSomeSectionid, kSomeDocumentId, Hit::kDefaultTermFrequency,
-         /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+         /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+         /*is_stemmed_hit=*/false);
   EXPECT_THAT(h3.has_flags(), IsFalse());
 
-  Hit h4(kSomeSectionid, kSomeDocumentId, kSomeTermFrequency,
-         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+  Hit h4(kSomeSectionid, kSomeDocumentId, Hit::kDefaultTermFrequency,
+         /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+         /*is_stemmed_hit=*/true);
   EXPECT_THAT(h4.has_flags(), IsTrue());
 
   Hit h5(kSomeSectionid, kSomeDocumentId, kSomeTermFrequency,
-         /*is_prefix_hit=*/true, /*is_in_prefix_section=*/true);
+         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+         /*is_stemmed_hit=*/false);
   EXPECT_THAT(h5.has_flags(), IsTrue());
 
   Hit h6(kSomeSectionid, kSomeDocumentId, kSomeTermFrequency,
-         /*is_prefix_hit=*/false, /*is_in_prefix_section=*/true);
+         /*is_prefix_hit=*/true, /*is_in_prefix_section=*/true,
+         /*is_stemmed_hit=*/false);
   EXPECT_THAT(h6.has_flags(), IsTrue());
 
   Hit h7(kSomeSectionid, kSomeDocumentId, kSomeTermFrequency,
-         /*is_prefix_hit=*/true, /*is_in_prefix_section=*/false);
+         /*is_prefix_hit=*/false, /*is_in_prefix_section=*/true,
+         /*is_stemmed_hit=*/true);
   EXPECT_THAT(h7.has_flags(), IsTrue());
+
+  Hit h8(kSomeSectionid, kSomeDocumentId, kSomeTermFrequency,
+         /*is_prefix_hit=*/true, /*is_in_prefix_section=*/false,
+         /*is_stemmed_hit=*/true);
+  EXPECT_THAT(h8.has_flags(), IsTrue());
 }
 
 TEST(HitTest, Accessors) {
   Hit h1(kSomeSectionid, kSomeDocumentId, kSomeTermFrequency,
-         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/true);
+         /*is_in_prefix_section=*/false, /*is_prefix_hit=*/true,
+         /*is_stemmed_hit=*/true);
   EXPECT_THAT(h1.document_id(), Eq(kSomeDocumentId));
   EXPECT_THAT(h1.section_id(), Eq(kSomeSectionid));
   EXPECT_THAT(h1.term_frequency(), Eq(kSomeTermFrequency));
   EXPECT_THAT(h1.is_in_prefix_section(), IsFalse());
   EXPECT_THAT(h1.is_prefix_hit(), IsTrue());
+  EXPECT_THAT(h1.is_stemmed_hit(), IsTrue());
 }
 
 TEST(HitTest, Valid) {
@@ -194,22 +230,24 @@ TEST(HitTest, Valid) {
 
   Hit maximum_document_id_hit(
       kSomeSectionid, kMaxDocumentId, kSomeTermFrequency,
-      /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+      /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+      /*is_stemmed_hit=*/false);
   EXPECT_THAT(maximum_document_id_hit.is_valid(), IsTrue());
 
   Hit maximum_section_id_hit(kMaxSectionId, kSomeDocumentId, kSomeTermFrequency,
                              /*is_in_prefix_section=*/false,
-                             /*is_prefix_hit=*/false);
+                             /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
   EXPECT_THAT(maximum_section_id_hit.is_valid(), IsTrue());
 
   Hit minimum_document_id_hit(kSomeSectionid, 0, kSomeTermFrequency,
                               /*is_in_prefix_section=*/false,
-                              /*is_prefix_hit=*/false);
+                              /*is_prefix_hit=*/false,
+                              /*is_stemmed_hit=*/false);
   EXPECT_THAT(minimum_document_id_hit.is_valid(), IsTrue());
 
   Hit minimum_section_id_hit(0, kSomeDocumentId, kSomeTermFrequency,
                              /*is_in_prefix_section=*/false,
-                             /*is_prefix_hit=*/false);
+                             /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
   EXPECT_THAT(minimum_section_id_hit.is_valid(), IsTrue());
 
   // We use Hit with value Hit::kMaxDocumentIdSortValue for std::lower_bound
@@ -217,9 +255,11 @@ TEST(HitTest, Valid) {
   // contains kMinSectionId, kMaxDocumentId and 3 flags = false) is >=
   // Hit::kMaxDocumentIdSortValue.
   Hit smallest_hit(kMinSectionId, kMaxDocumentId, Hit::kDefaultTermFrequency,
-                   /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                   /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                   /*is_stemmed_hit=*/false);
   ASSERT_THAT(smallest_hit.is_valid(), IsTrue());
   ASSERT_THAT(smallest_hit.has_term_frequency(), IsFalse());
+  ASSERT_THAT(smallest_hit.is_stemmed_hit(), IsFalse());
   ASSERT_THAT(smallest_hit.is_prefix_hit(), IsFalse());
   ASSERT_THAT(smallest_hit.is_in_prefix_section(), IsFalse());
   EXPECT_THAT(smallest_hit.value(), Ge(Hit::kMaxDocumentIdSortValue));
@@ -227,31 +267,37 @@ TEST(HitTest, Valid) {
 
 TEST(HitTest, Comparison) {
   Hit hit(/*section_id=*/1, /*document_id=*/243, Hit::kDefaultTermFrequency,
-          /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+          /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+          /*is_stemmed_hit=*/false);
   // DocumentIds are sorted in ascending order. So a hit with a lower
   // document_id should be considered greater than one with a higher
   // document_id.
   Hit higher_document_id_hit(
       /*section_id=*/1, /*document_id=*/2409, Hit::kDefaultTermFrequency,
-      /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+      /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+      /*is_stemmed_hit=*/false);
   Hit higher_section_id_hit(/*section_id=*/15, /*document_id=*/243,
                             Hit::kDefaultTermFrequency,
                             /*is_in_prefix_section=*/false,
-                            /*is_prefix_hit=*/false);
+                            /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
   // Whether or not a term frequency was set is considered, but the term
   // frequency itself is not.
   Hit term_frequency_hit(/*section_id=*/1, 243, /*term_frequency=*/12,
                          /*is_in_prefix_section=*/false,
-                         /*is_prefix_hit=*/false);
+                         /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
   Hit prefix_hit(/*section_id=*/1, 243, Hit::kDefaultTermFrequency,
                  /*is_in_prefix_section=*/false,
-                 /*is_prefix_hit=*/true);
+                 /*is_prefix_hit=*/true, /*is_stemmed_hit=*/false);
   Hit hit_in_prefix_section(/*section_id=*/1, 243, Hit::kDefaultTermFrequency,
                             /*is_in_prefix_section=*/true,
-                            /*is_prefix_hit=*/false);
+                            /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
   Hit hit_with_all_flags_enabled(/*section_id=*/1, 243, 56,
                                  /*is_in_prefix_section=*/true,
-                                 /*is_prefix_hit=*/true);
+                                 /*is_prefix_hit=*/true,
+                                 /*is_stemmed_hit=*/true);
+  Hit stemmed_hit(/*section_id=*/1, 243, Hit::kDefaultTermFrequency,
+                  /*is_in_prefix_section=*/false,
+                  /*is_prefix_hit=*/false, /*is_stemmed_hit=*/true);
 
   std::vector<Hit> hits{hit,
                         higher_document_id_hit,
@@ -259,16 +305,20 @@ TEST(HitTest, Comparison) {
                         term_frequency_hit,
                         prefix_hit,
                         hit_in_prefix_section,
-                        hit_with_all_flags_enabled};
+                        hit_with_all_flags_enabled,
+                        stemmed_hit};
   std::sort(hits.begin(), hits.end());
-  EXPECT_THAT(hits,
-              ElementsAre(higher_document_id_hit, hit, term_frequency_hit,
-                          hit_in_prefix_section, prefix_hit,
-                          hit_with_all_flags_enabled, higher_section_id_hit));
+  EXPECT_THAT(
+      hits, ElementsAre(EqualsHit(higher_document_id_hit), EqualsHit(hit),
+                        EqualsHit(term_frequency_hit), EqualsHit(stemmed_hit),
+                        EqualsHit(hit_in_prefix_section), EqualsHit(prefix_hit),
+                        EqualsHit(hit_with_all_flags_enabled),
+                        EqualsHit(higher_section_id_hit)));
 
   Hit higher_term_frequency_hit(/*section_id=*/1, 243, /*term_frequency=*/108,
                                 /*is_in_prefix_section=*/false,
-                                /*is_prefix_hit=*/false);
+                                /*is_prefix_hit=*/false,
+                                /*is_stemmed_hit=*/false);
   // The term frequency value is not considered when comparing hits.
   EXPECT_THAT(term_frequency_hit, Not(Lt(higher_term_frequency_hit)));
   EXPECT_THAT(higher_term_frequency_hit, Not(Lt(term_frequency_hit)));
diff --git a/icing/index/index-processor_test.cc b/icing/index/index-processor_test.cc
index 6d6d678..2dce924 100644
--- a/icing/index/index-processor_test.cc
+++ b/icing/index/index-processor_test.cc
@@ -435,10 +435,10 @@ TEST_F(IndexProcessorTest, OneDoc) {
                           TermMatchType::EXACT_ONLY));
   std::vector<DocHitInfoTermFrequencyPair> hits =
       GetHitsWithTermFrequency(std::move(itr));
-  std::unordered_map<SectionId, Hit::TermFrequency> expectedMap{
+  std::unordered_map<SectionId, Hit::TermFrequency> expected_map{
       {kExactSectionId, 1}};
   EXPECT_THAT(hits, ElementsAre(EqualsDocHitInfoWithTermFrequency(
-                        kDocumentId0, expectedMap)));
+                        kDocumentId0, expected_map)));
 
   ICING_ASSERT_OK_AND_ASSIGN(
       itr, index_->GetIterator(
@@ -491,33 +491,34 @@ TEST_F(IndexProcessorTest, MultipleDocs) {
                           TermMatchType::EXACT_ONLY));
   std::vector<DocHitInfoTermFrequencyPair> hits =
       GetHitsWithTermFrequency(std::move(itr));
-  std::unordered_map<SectionId, Hit::TermFrequency> expectedMap1{
+  std::unordered_map<SectionId, Hit::TermFrequency> expected_map_1{
       {kPrefixedSectionId, 2}};
-  std::unordered_map<SectionId, Hit::TermFrequency> expectedMap2{
+  std::unordered_map<SectionId, Hit::TermFrequency> expected_map_2{
       {kExactSectionId, 1}};
   EXPECT_THAT(
-      hits, ElementsAre(
-                EqualsDocHitInfoWithTermFrequency(kDocumentId1, expectedMap1),
-                EqualsDocHitInfoWithTermFrequency(kDocumentId0, expectedMap2)));
+      hits,
+      ElementsAre(
+          EqualsDocHitInfoWithTermFrequency(kDocumentId1, expected_map_1),
+          EqualsDocHitInfoWithTermFrequency(kDocumentId0, expected_map_2)));
 
   ICING_ASSERT_OK_AND_ASSIGN(
       itr, index_->GetIterator(
                "world", /*term_start_index=*/0, /*unnormalized_term_length=*/0,
                1U << kPrefixedSectionId, TermMatchType::EXACT_ONLY));
   hits = GetHitsWithTermFrequency(std::move(itr));
-  std::unordered_map<SectionId, Hit::TermFrequency> expectedMap{
+  std::unordered_map<SectionId, Hit::TermFrequency> expected_map{
       {kPrefixedSectionId, 2}};
   EXPECT_THAT(hits, ElementsAre(EqualsDocHitInfoWithTermFrequency(
-                        kDocumentId1, expectedMap)));
+                        kDocumentId1, expected_map)));
 
   ICING_ASSERT_OK_AND_ASSIGN(
       itr, index_->GetIterator("coffee", /*term_start_index=*/0,
                                /*unnormalized_term_length=*/0,
                                kSectionIdMaskAll, TermMatchType::EXACT_ONLY));
   hits = GetHitsWithTermFrequency(std::move(itr));
-  expectedMap = {{kExactSectionId, Hit::kMaxTermFrequency}};
+  expected_map = {{kExactSectionId, Hit::kMaxTermFrequency}};
   EXPECT_THAT(hits, ElementsAre(EqualsDocHitInfoWithTermFrequency(
-                        kDocumentId1, expectedMap)));
+                        kDocumentId1, expected_map)));
 }
 
 TEST_F(IndexProcessorTest, DocWithNestedProperty) {
@@ -1153,11 +1154,11 @@ TEST_F(IndexProcessorTest, ExactVerbatimProperty) {
                           TermMatchType::EXACT_ONLY));
   std::vector<DocHitInfoTermFrequencyPair> hits =
       GetHitsWithTermFrequency(std::move(itr));
-  std::unordered_map<SectionId, Hit::TermFrequency> expectedMap{
+  std::unordered_map<SectionId, Hit::TermFrequency> expected_map{
       {kVerbatimExactSectionId, 1}};
 
   EXPECT_THAT(hits, ElementsAre(EqualsDocHitInfoWithTermFrequency(
-                        kDocumentId0, expectedMap)));
+                        kDocumentId0, expected_map)));
 }
 
 TEST_F(IndexProcessorTest, PrefixVerbatimProperty) {
@@ -1187,11 +1188,11 @@ TEST_F(IndexProcessorTest, PrefixVerbatimProperty) {
                           TermMatchType::PREFIX));
   std::vector<DocHitInfoTermFrequencyPair> hits =
       GetHitsWithTermFrequency(std::move(itr));
-  std::unordered_map<SectionId, Hit::TermFrequency> expectedMap{
+  std::unordered_map<SectionId, Hit::TermFrequency> expected_map{
       {kVerbatimPrefixedSectionId, 1}};
 
   EXPECT_THAT(hits, ElementsAre(EqualsDocHitInfoWithTermFrequency(
-                        kDocumentId0, expectedMap)));
+                        kDocumentId0, expected_map)));
 }
 
 TEST_F(IndexProcessorTest, VerbatimPropertyDoesntMatchSubToken) {
diff --git a/icing/index/index.cc b/icing/index/index.cc
index d2b6f99..54031fd 100644
--- a/icing/index/index.cc
+++ b/icing/index/index.cc
@@ -242,8 +242,7 @@ Index::FindLiteTermsByPrefix(
     SuggestionScoringSpecProto::SuggestionRankingStrategy::Code score_by,
     const SuggestionResultChecker* suggestion_result_checker) {
   // Finds all the terms that start with the given prefix in the lexicon.
-  IcingDynamicTrie::Iterator term_iterator(lite_index_->lexicon(),
-                                           prefix.c_str());
+  IcingDynamicTrie::Iterator term_iterator(lite_index_->lexicon(), prefix);
 
   std::vector<TermMetadata> term_metadata_list;
   while (term_iterator.IsValid()) {
@@ -259,7 +258,7 @@ Index::FindLiteTermsByPrefix(
     if (hit_score > 0) {
       // There is at least one document in the given namespace has this term.
       term_metadata_list.push_back(
-          TermMetadata(term_iterator.GetKey(), hit_score));
+          TermMetadata(std::string(term_iterator.GetKey()), hit_score));
     }
 
     term_iterator.Advance();
@@ -344,7 +343,7 @@ libtextclassifier3::Status Index::Editor::IndexAllBufferedTerms() {
   for (auto itr = seen_tokens_.begin(); itr != seen_tokens_.end(); itr++) {
     Hit hit(section_id_, document_id_, /*term_frequency=*/itr->second,
             /*is_in_prefix_section=*/term_match_type_ == TermMatchType::PREFIX,
-            /*is_prefix_hit=*/false);
+            /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
     ICING_ASSIGN_OR_RETURN(
         uint32_t term_id, term_id_codec_->EncodeTvi(itr->first, TviType::LITE));
     ICING_RETURN_IF_ERROR(lite_index_->AddHit(term_id, hit));
diff --git a/icing/index/index.h b/icing/index/index.h
index a09e28f..0d436e2 100644
--- a/icing/index/index.h
+++ b/icing/index/index.h
@@ -130,6 +130,22 @@ class Index {
     return main_index_->PersistToDisk();
   }
 
+  // Updates all checksums in the index and returns the combined index checksum.
+  Crc32 UpdateChecksum() {
+    Crc32 lite_crc = lite_index_->UpdateChecksum();
+    Crc32 main_crc = main_index_->UpdateChecksum();
+    main_crc.Append(std::to_string(lite_crc.Get()));
+    return main_crc;
+  }
+
+  // Calculates and returns the combined index checksum.
+  Crc32 GetChecksum() const {
+    Crc32 lite_crc = lite_index_->GetChecksum();
+    Crc32 main_crc = main_index_->GetChecksum();
+    main_crc.Append(std::to_string(lite_crc.Get()));
+    return main_crc;
+  }
+
   // Discard parts of the index if they contain data for document ids greater
   // than document_id.
   //
diff --git a/icing/index/index_test.cc b/icing/index/index_test.cc
index 50b65ad..db94e78 100644
--- a/icing/index/index_test.cc
+++ b/icing/index/index_test.cc
@@ -1671,6 +1671,37 @@ TEST_F(IndexTest, IndexCreateCorruptionFailure) {
               StatusIs(libtextclassifier3::StatusCode::DATA_LOSS));
 }
 
+TEST_F(IndexTest, UpdateChecksum) {
+  // Add some content to the index
+  Index::Editor edit = index_->Edit(kDocumentId0, kSectionId2,
+                                    TermMatchType::PREFIX, /*namespace_id=*/0);
+  ASSERT_THAT(edit.BufferTerm("foo"), IsOk());
+  ASSERT_THAT(edit.BufferTerm("bar"), IsOk());
+  EXPECT_THAT(edit.IndexAllBufferedTerms(), IsOk());
+  Crc32 lite_only_crc = index_->GetChecksum();
+  EXPECT_THAT(index_->UpdateChecksum(), Eq(lite_only_crc));
+  EXPECT_THAT(index_->GetChecksum(), Eq(lite_only_crc));
+
+  // Merge content into the main index.
+  ASSERT_THAT(index_->Merge(), IsOk());
+  Crc32 main_only_crc = index_->GetChecksum();
+  EXPECT_THAT(main_only_crc, Not(Eq(lite_only_crc)));
+  EXPECT_THAT(index_->UpdateChecksum(), Eq(main_only_crc));
+  EXPECT_THAT(index_->GetChecksum(), Eq(main_only_crc));
+
+  // Add some more content to the lite index
+  edit = index_->Edit(kDocumentId1, kSectionId2, TermMatchType::PREFIX,
+                      /*namespace_id=*/0);
+  ASSERT_THAT(edit.BufferTerm("baz"), IsOk());
+  ASSERT_THAT(edit.BufferTerm("bat"), IsOk());
+  EXPECT_THAT(edit.IndexAllBufferedTerms(), IsOk());
+  Crc32 both_crc = index_->GetChecksum();
+  EXPECT_THAT(both_crc, Not(Eq(lite_only_crc)));
+  EXPECT_THAT(both_crc, Not(Eq(main_only_crc)));
+  EXPECT_THAT(index_->UpdateChecksum(), Eq(both_crc));
+  EXPECT_THAT(index_->GetChecksum(), Eq(both_crc));
+}
+
 TEST_F(IndexTest, IndexPersistence) {
   // Add some content to the index
   Index::Editor edit = index_->Edit(kDocumentId0, kSectionId2,
diff --git a/icing/index/integer-section-indexing-handler_test.cc b/icing/index/integer-section-indexing-handler_test.cc
index e71ede2..4da05f9 100644
--- a/icing/index/integer-section-indexing-handler_test.cc
+++ b/icing/index/integer-section-indexing-handler_test.cc
@@ -232,8 +232,9 @@ TEST_F(IntegerSectionIndexingHandlerTest, HandleIntegerSection) {
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(tokenized_document.document()));
+  DocumentId document_id = put_result.new_document_id;
 
   ASSERT_THAT(integer_index_->last_added_document_id(), Eq(kInvalidDocumentId));
   // Handle document.
@@ -281,8 +282,9 @@ TEST_F(IntegerSectionIndexingHandlerTest, HandleNestedIntegerSection) {
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(nested_document)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(tokenized_document.document()));
+  DocumentId document_id = put_result.new_document_id;
 
   ASSERT_THAT(integer_index_->last_added_document_id(), Eq(kInvalidDocumentId));
   // Handle nested_document.
@@ -343,8 +345,9 @@ TEST_F(IntegerSectionIndexingHandlerTest, HandleShouldSkipEmptyIntegerSection) {
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(tokenized_document.document()));
+  DocumentId document_id = put_result.new_document_id;
 
   ASSERT_THAT(integer_index_->last_added_document_id(), Eq(kInvalidDocumentId));
   // Handle document. Index data should remain unchanged since there is no
@@ -443,8 +446,9 @@ TEST_F(IntegerSectionIndexingHandlerTest,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(tokenized_document.document()));
+  DocumentId document_id = put_result.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<IntegerSectionIndexingHandler> handler,
@@ -519,11 +523,13 @@ TEST_F(IntegerSectionIndexingHandlerTest,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document2)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(tokenized_document1.document()));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(tokenized_document2.document()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<IntegerSectionIndexingHandler> handler,
diff --git a/icing/index/iterator/doc-hit-info-iterator-filter_test.cc b/icing/index/iterator/doc-hit-info-iterator-filter_test.cc
index c7f54b2..b088212 100644
--- a/icing/index/iterator/doc-hit-info-iterator-filter_test.cc
+++ b/icing/index/iterator/doc-hit-info-iterator-filter_test.cc
@@ -130,12 +130,16 @@ TEST_F(DocHitInfoIteratorDeletedFilterTest, EmptyOriginalIterator) {
 }
 
 TEST_F(DocHitInfoIteratorDeletedFilterTest, DeletedDocumentsAreFiltered) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(test_document1_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(test_document2_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store_->Put(test_document3_));
+  DocumentId document_id3 = put_result3.new_document_id;
+
   // Deletes test document 2
   ICING_ASSERT_OK(document_store_->Delete(
       test_document2_.namespace_(), test_document2_.uri(),
@@ -156,12 +160,15 @@ TEST_F(DocHitInfoIteratorDeletedFilterTest, DeletedDocumentsAreFiltered) {
 }
 
 TEST_F(DocHitInfoIteratorDeletedFilterTest, NonExistingDocumentsAreFiltered) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(test_document1_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(test_document2_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store_->Put(test_document3_));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   // Document ids 7, 8, 9 are not existing
   std::vector<DocHitInfo> doc_hit_infos = {DocHitInfo(document_id1),
@@ -305,9 +312,9 @@ TEST_F(DocHitInfoIteratorNamespaceFilterTest, EmptyOriginalIterator) {
 
 TEST_F(DocHitInfoIteratorNamespaceFilterTest,
        NonexistentNamespacesReturnsEmpty) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1_namespace1_));
-
+  DocumentId document_id1 = put_result1.new_document_id;
   std::vector<DocHitInfo> doc_hit_infos = {DocHitInfo(document_id1)};
 
   std::unique_ptr<DocHitInfoIterator> original_iterator =
@@ -325,8 +332,9 @@ TEST_F(DocHitInfoIteratorNamespaceFilterTest,
 }
 
 TEST_F(DocHitInfoIteratorNamespaceFilterTest, NoNamespacesReturnsAll) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1_namespace1_));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   std::vector<DocHitInfo> doc_hit_infos = {DocHitInfo(document_id1)};
 
@@ -345,12 +353,15 @@ TEST_F(DocHitInfoIteratorNamespaceFilterTest, NoNamespacesReturnsAll) {
 
 TEST_F(DocHitInfoIteratorNamespaceFilterTest,
        FilterOutExistingDocumentFromDifferentNamespace) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1_namespace1_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2_namespace1_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store_->Put(document1_namespace2_));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   std::vector<DocHitInfo> doc_hit_infos = {DocHitInfo(document_id1),
                                            DocHitInfo(document_id2),
@@ -372,14 +383,18 @@ TEST_F(DocHitInfoIteratorNamespaceFilterTest,
 }
 
 TEST_F(DocHitInfoIteratorNamespaceFilterTest, FilterForMultipleNamespacesOk) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1_namespace1_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2_namespace1_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store_->Put(document1_namespace2_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id4,
+  DocumentId document_id3 = put_result3.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
                              document_store_->Put(document1_namespace3_));
+  DocumentId document_id4 = put_result4.new_document_id;
 
   std::vector<DocHitInfo> doc_hit_infos = {
       DocHitInfo(document_id1), DocHitInfo(document_id2),
@@ -492,9 +507,9 @@ TEST_F(DocHitInfoIteratorSchemaTypeFilterTest, EmptyOriginalIterator) {
 
 TEST_F(DocHitInfoIteratorSchemaTypeFilterTest,
        NonexistentSchemaTypeReturnsEmpty) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1_schema1_));
-
+  DocumentId document_id1 = put_result1.new_document_id;
   std::vector<DocHitInfo> doc_hit_infos = {DocHitInfo(document_id1)};
 
   std::unique_ptr<DocHitInfoIterator> original_iterator =
@@ -512,8 +527,9 @@ TEST_F(DocHitInfoIteratorSchemaTypeFilterTest,
 }
 
 TEST_F(DocHitInfoIteratorSchemaTypeFilterTest, NoSchemaTypesReturnsAll) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1_schema1_));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   std::vector<DocHitInfo> doc_hit_infos = {DocHitInfo(document_id1)};
 
@@ -532,10 +548,12 @@ TEST_F(DocHitInfoIteratorSchemaTypeFilterTest, NoSchemaTypesReturnsAll) {
 
 TEST_F(DocHitInfoIteratorSchemaTypeFilterTest,
        FilterOutExistingDocumentFromDifferentSchemaTypes) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1_schema1_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2_schema2_));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   std::vector<DocHitInfo> doc_hit_infos = {DocHitInfo(document_id1),
                                            DocHitInfo(document_id2)};
@@ -555,13 +573,15 @@ TEST_F(DocHitInfoIteratorSchemaTypeFilterTest,
 }
 
 TEST_F(DocHitInfoIteratorSchemaTypeFilterTest, FilterForMultipleSchemaTypesOk) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1_schema1_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2_schema2_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store_->Put(document3_schema3_));
-
+  DocumentId document_id3 = put_result3.new_document_id;
   std::vector<DocHitInfo> doc_hit_infos = {DocHitInfo(document_id1),
                                            DocHitInfo(document_id2),
                                            DocHitInfo(document_id3)};
@@ -585,25 +605,29 @@ TEST_F(DocHitInfoIteratorSchemaTypeFilterTest, FilterForMultipleSchemaTypesOk) {
 TEST_F(DocHitInfoIteratorSchemaTypeFilterTest,
        FilterIsExactForSchemaTypePolymorphism) {
   // Add some irrelevant documents.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1_schema1_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2_schema2_));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Create a person document and an artist document, where the artist should be
   // able to be interpreted as a person by polymorphism.
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId person_document_id,
+      DocumentStore::PutResult person_put_result,
       document_store_->Put(DocumentBuilder()
                                .SetKey("namespace", "person")
                                .SetSchema("person")
                                .Build()));
+  DocumentId person_document_id = person_put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId artist_document_id,
+      DocumentStore::PutResult artist_put_result,
       document_store_->Put(DocumentBuilder()
                                .SetKey("namespace", "artist")
                                .SetSchema("artist")
                                .Build()));
+  DocumentId artist_document_id = artist_put_result.new_document_id;
 
   std::vector<DocHitInfo> doc_hit_infos = {
       DocHitInfo(document_id1), DocHitInfo(document_id2),
@@ -639,26 +663,30 @@ TEST_F(DocHitInfoIteratorSchemaTypeFilterTest,
        FilterIsExactForSchemaTypeMultipleParentPolymorphism) {
   // Create an email and a message document.
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId email_document_id,
+      DocumentStore::PutResult email_put_result,
       document_store_->Put(DocumentBuilder()
                                .SetKey("namespace", "email")
                                .SetSchema("email")
                                .Build()));
+  DocumentId email_document_id = email_put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId message_document_id,
+      DocumentStore::PutResult message_put_result,
       document_store_->Put(DocumentBuilder()
                                .SetKey("namespace", "message")
                                .SetSchema("message")
                                .Build()));
+  DocumentId message_document_id = message_put_result.new_document_id;
 
   // Create a emailMessage document, which the should be able to be interpreted
   // as both an email and a message by polymorphism.
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId email_message_document_id,
+      DocumentStore::PutResult email_message_put_result,
       document_store_->Put(DocumentBuilder()
                                .SetKey("namespace", "emailMessage")
                                .SetSchema("emailMessage")
                                .Build()));
+  DocumentId email_message_document_id =
+      email_message_put_result.new_document_id;
 
   std::vector<DocHitInfo> doc_hit_infos = {
       DocHitInfo(email_document_id), DocHitInfo(message_document_id),
@@ -762,8 +790,9 @@ TEST_F(DocHitInfoIteratorExpirationFilterTest, TtlZeroIsntFilteredOut) {
                                .SetCreationTimestampMs(0)
                                .SetTtlMs(0)
                                .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
-                             document_store->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
+                             document_store_->Put(document));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   std::vector<DocHitInfo> doc_hit_infos = {DocHitInfo(document_id1)};
   std::unique_ptr<DocHitInfoIterator> original_iterator =
@@ -794,8 +823,9 @@ TEST_F(DocHitInfoIteratorExpirationFilterTest, BeforeTtlNotFilteredOut) {
                                .SetCreationTimestampMs(1)
                                .SetTtlMs(100)
                                .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
-                             document_store->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
+                             document_store_->Put(document));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   std::vector<DocHitInfo> doc_hit_infos = {DocHitInfo(document_id1)};
   std::unique_ptr<DocHitInfoIterator> original_iterator =
@@ -826,8 +856,9 @@ TEST_F(DocHitInfoIteratorExpirationFilterTest, EqualTtlFilteredOut) {
                                .SetCreationTimestampMs(50)
                                .SetTtlMs(100)
                                .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
-                             document_store->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
+                             document_store_->Put(document));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   std::vector<DocHitInfo> doc_hit_infos = {DocHitInfo(document_id1)};
   std::unique_ptr<DocHitInfoIterator> original_iterator =
@@ -859,8 +890,9 @@ TEST_F(DocHitInfoIteratorExpirationFilterTest, PastTtlFilteredOut) {
                                .SetCreationTimestampMs(50)
                                .SetTtlMs(100)
                                .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
-                             document_store->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
+                             document_store_->Put(document));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   std::vector<DocHitInfo> doc_hit_infos = {DocHitInfo(document_id1)};
   std::unique_ptr<DocHitInfoIterator> original_iterator =
@@ -965,20 +997,25 @@ TEST_F(DocHitInfoIteratorFilterTest, CombineAllFiltersOk) {
       std::move(create_result.document_store);
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store->Put(document1_namespace1_schema1_));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store->Put(document2_namespace1_schema1_));
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id3,
+      DocumentStore::PutResult put_result3,
       document_store->Put(document3_namespace2_schema1_));
+  DocumentId document_id3 = put_result3.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id4,
+      DocumentStore::PutResult put_result4,
       document_store->Put(document4_namespace1_schema2_));
+  DocumentId document_id4 = put_result4.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id5,
+      DocumentStore::PutResult put_result5,
       document_store->Put(document5_namespace1_schema1_));
+  DocumentId document_id5 = put_result5.new_document_id;
 
   // Deletes document2, causing it to be filtered out
   ICING_ASSERT_OK(
@@ -1011,14 +1048,17 @@ TEST_F(DocHitInfoIteratorFilterTest, CombineAllFiltersOk) {
 
 TEST_F(DocHitInfoIteratorFilterTest, SectionIdMasksArePopulatedCorrectly) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(document1_namespace1_schema1_));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(document2_namespace1_schema1_));
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id3,
+      DocumentStore::PutResult put_result3,
       document_store_->Put(document3_namespace2_schema1_));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   SectionIdMask section_id_mask1 = 0b01001001;  // hits in sections 0, 3, 6
   SectionIdMask section_id_mask2 = 0b10010010;  // hits in sections 1, 4, 7
@@ -1065,14 +1105,17 @@ TEST_F(DocHitInfoIteratorFilterTest, GetCallStats) {
 
 TEST_F(DocHitInfoIteratorFilterTest, TrimFilterIterator) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(document1_namespace1_schema1_));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(document2_namespace1_schema1_));
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id3,
+      DocumentStore::PutResult put_result3,
       document_store_->Put(document3_namespace2_schema1_));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   // Build an interator tree like:
   //                Filter
diff --git a/icing/index/iterator/doc-hit-info-iterator-property-in-schema_test.cc b/icing/index/iterator/doc-hit-info-iterator-property-in-schema_test.cc
index a77707e..17bd55d 100644
--- a/icing/index/iterator/doc-hit-info-iterator-property-in-schema_test.cc
+++ b/icing/index/iterator/doc-hit-info-iterator-property-in-schema_test.cc
@@ -130,8 +130,9 @@ class DocHitInfoIteratorPropertyInSchemaTest : public ::testing::Test {
 TEST_F(DocHitInfoIteratorPropertyInSchemaTest,
        AdvanceToDocumentWithIndexedProperty) {
   // Populate the DocumentStore's FilterCache with this document's data
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(document1_));
+  DocumentId document_id = put_result.new_document_id;
 
   auto original_iterator = std::make_unique<DocHitInfoIteratorAllDocumentId>(
       document_store_->num_documents());
@@ -150,8 +151,9 @@ TEST_F(DocHitInfoIteratorPropertyInSchemaTest,
 TEST_F(DocHitInfoIteratorPropertyInSchemaTest,
        AdvanceToDocumentWithUnindexedProperty) {
   // Populate the DocumentStore's FilterCache with this document's data
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(document1_));
+  DocumentId document_id = put_result.new_document_id;
 
   auto original_iterator = std::make_unique<DocHitInfoIteratorAllDocumentId>(
       document_store_->num_documents());
@@ -183,8 +185,9 @@ TEST_F(DocHitInfoIteratorPropertyInSchemaTest, NoMatchWithUndefinedProperty) {
 TEST_F(DocHitInfoIteratorPropertyInSchemaTest,
        CorrectlySetsSectionIdMasksAndPopulatesTermMatchInfo) {
   // Populate the DocumentStore's FilterCache with this document's data
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(document1_));
+  DocumentId document_id = put_result.new_document_id;
 
   // Arbitrary section ids for the documents in the DocHitInfoIterators.
   // Created to test correct section_id_mask behavior.
@@ -245,10 +248,12 @@ TEST_F(DocHitInfoIteratorPropertyInSchemaTest,
 
 TEST_F(DocHitInfoIteratorPropertyInSchemaTest,
        FindPropertyDefinedByMultipleTypes) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2_));
+  DocumentId document_id2 = put_result2.new_document_id;
   auto original_iterator = std::make_unique<DocHitInfoIteratorAllDocumentId>(
       document_store_->num_documents());
 
diff --git a/icing/index/iterator/doc-hit-info-iterator-section-restrict.cc b/icing/index/iterator/doc-hit-info-iterator-section-restrict.cc
index 735adaa..f6e440d 100644
--- a/icing/index/iterator/doc-hit-info-iterator-section-restrict.cc
+++ b/icing/index/iterator/doc-hit-info-iterator-section-restrict.cc
@@ -134,7 +134,7 @@ DocHitInfoIteratorSectionRestrict::ApplyRestrictions(
   ChildrenMapper mapper;
   mapper = [&data, &mapper](std::unique_ptr<DocHitInfoIterator> iterator)
       -> std::unique_ptr<DocHitInfoIterator> {
-    if (iterator->full_section_restriction_applied()) {
+    if (iterator->HandleSectionRestriction(data)) {
       return iterator;
     } else if (iterator->is_leaf()) {
       return std::make_unique<DocHitInfoIteratorSectionRestrict>(
diff --git a/icing/index/iterator/doc-hit-info-iterator-section-restrict_test.cc b/icing/index/iterator/doc-hit-info-iterator-section-restrict_test.cc
index 9a3cc29..94e9b5e 100644
--- a/icing/index/iterator/doc-hit-info-iterator-section-restrict_test.cc
+++ b/icing/index/iterator/doc-hit-info-iterator-section-restrict_test.cc
@@ -135,8 +135,9 @@ class DocHitInfoIteratorSectionRestrictTest : public ::testing::Test {
 TEST_F(DocHitInfoIteratorSectionRestrictTest,
        PopulateMatchedTermsStats_IncludesHitWithMatchingSection) {
   // Populate the DocumentStore's FilterCache with this document's data
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(document1_));
+  DocumentId document_id = put_result.new_document_id;
 
   // Arbitrary section ids for the documents in the DocHitInfoIterators.
   // Created to test correct section_id_mask behavior.
@@ -200,8 +201,9 @@ TEST_F(DocHitInfoIteratorSectionRestrictTest, EmptyOriginalIterator) {
 
 TEST_F(DocHitInfoIteratorSectionRestrictTest, IncludesHitWithMatchingSection) {
   // Populate the DocumentStore's FilterCache with this document's data
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(document1_));
+  DocumentId document_id = put_result.new_document_id;
 
   SectionIdMask section_id_mask = 1U << kIndexedSectionId0;
 
@@ -227,8 +229,9 @@ TEST_F(DocHitInfoIteratorSectionRestrictTest, IncludesHitWithMatchingSection) {
 TEST_F(DocHitInfoIteratorSectionRestrictTest,
        IncludesHitWithMultipleMatchingSectionsWithMultipleSectionRestricts) {
   // Populate the DocumentStore's FilterCache with this document's data
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(document1_));
+  DocumentId document_id = put_result.new_document_id;
 
   SectionIdMask section_id_mask = 1U << kIndexedSectionId0;
   section_id_mask |= 1U << kIndexedSectionId1;
@@ -258,8 +261,9 @@ TEST_F(DocHitInfoIteratorSectionRestrictTest,
 TEST_F(DocHitInfoIteratorSectionRestrictTest,
        IncludesHitWithMultipleMatchingSectionsWithSingleSectionRestrict) {
   // Populate the DocumentStore's FilterCache with this document's data
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(document1_));
+  DocumentId document_id = put_result.new_document_id;
 
   SectionIdMask section_id_mask = 1U << kIndexedSectionId0;
   section_id_mask |= 1U << kIndexedSectionId1;
@@ -288,8 +292,9 @@ TEST_F(DocHitInfoIteratorSectionRestrictTest,
 TEST_F(DocHitInfoIteratorSectionRestrictTest,
        IncludesHitWithSingleMatchingSectionsWithMultiSectionRestrict) {
   // Populate the DocumentStore's FilterCache with this document's data
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(document1_));
+  DocumentId document_id = put_result.new_document_id;
 
   SectionIdMask section_id_mask = 1U << kIndexedSectionId1;
 
@@ -337,8 +342,9 @@ TEST_F(DocHitInfoIteratorSectionRestrictTest, NoMatchingDocumentFilterData) {
 TEST_F(DocHitInfoIteratorSectionRestrictTest,
        DoesntIncludeHitWithWrongSectionName) {
   // Populate the DocumentStore's FilterCache with this document's data
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(document1_));
+  DocumentId document_id = put_result.new_document_id;
 
   SectionIdMask section_id_mask = 1U << kIndexedSectionId0;
 
@@ -366,8 +372,9 @@ TEST_F(DocHitInfoIteratorSectionRestrictTest,
 TEST_F(DocHitInfoIteratorSectionRestrictTest,
        DoesntIncludeHitWithNoSectionIds) {
   // Populate the DocumentStore's FilterCache with this document's data
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(document1_));
+  DocumentId document_id = put_result.new_document_id;
 
   // Create a hit that doesn't exist in any sections, so it shouldn't match any
   // section filters
@@ -393,8 +400,9 @@ TEST_F(DocHitInfoIteratorSectionRestrictTest,
 TEST_F(DocHitInfoIteratorSectionRestrictTest,
        DoesntIncludeHitWithDifferentSectionId) {
   // Populate the DocumentStore's FilterCache with this document's data
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(document1_));
+  DocumentId document_id = put_result.new_document_id;
 
   // Anything that's not 0, which is the indexed property
   SectionId not_matching_section_id = 2;
@@ -443,12 +451,15 @@ TEST_F(DocHitInfoIteratorSectionRestrictTest, GetCallStats) {
 TEST_F(DocHitInfoIteratorSectionRestrictTest,
        TrimSectionRestrictIterator_TwoLayer) {
   // Populate the DocumentStore's FilterCache with this document's data
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store_->Put(document3_));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   // 0 is the indexed property
   SectionId matching_section_id = 0;
@@ -501,10 +512,12 @@ TEST_F(DocHitInfoIteratorSectionRestrictTest,
 
 TEST_F(DocHitInfoIteratorSectionRestrictTest, TrimSectionRestrictIterator) {
   // Populate the DocumentStore's FilterCache with this document's data
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2_));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 0 is the indexed property
   SectionId matching_section_id = 0;
diff --git a/icing/index/iterator/doc-hit-info-iterator.h b/icing/index/iterator/doc-hit-info-iterator.h
index 921e4d4..f4f15bb 100644
--- a/icing/index/iterator/doc-hit-info-iterator.h
+++ b/icing/index/iterator/doc-hit-info-iterator.h
@@ -28,12 +28,15 @@
 #include "icing/text_classifier/lib3/utils/base/statusor.h"
 #include "icing/absl_ports/canonical_errors.h"
 #include "icing/index/hit/doc-hit-info.h"
+#include "icing/index/hit/hit.h"
 #include "icing/schema/section.h"
 #include "icing/store/document-id.h"
 
 namespace icing {
 namespace lib {
 
+class SectionRestrictData;
+
 // Data structure that maps a single matched query term to its section mask
 // and the list of term frequencies.
 // TODO(b/158603837): add stat on whether the matched terms are prefix matched
@@ -214,11 +217,15 @@ class DocHitInfoIterator {
 
   virtual bool is_leaf() { return false; }
 
-  // Whether the iterator has already been applied all the required section
-  // restrictions.
-  // If true, calling DocHitInfoIteratorSectionRestrict::ApplyRestrictions on
-  // this iterator will have no effect.
-  virtual bool full_section_restriction_applied() const { return false; }
+  // Try to internally handle the provided section restriction in the iterator.
+  //
+  // Returns:
+  //   - false if the iterator does not support handling section restriction.
+  //   - true if the iterator supports handling section restriction, and the
+  //     section restriction has been applied.
+  virtual bool HandleSectionRestriction(SectionRestrictData* other_data) {
+    return false;
+  }
 
   virtual ~DocHitInfoIterator() = default;
 
diff --git a/icing/index/iterator/section-restrict-data.h b/icing/index/iterator/section-restrict-data.h
index 64d5087..38d7158 100644
--- a/icing/index/iterator/section-restrict-data.h
+++ b/icing/index/iterator/section-restrict-data.h
@@ -20,7 +20,9 @@
 #include <string>
 #include <unordered_map>
 #include <utility>
+#include <vector>
 
+#include "icing/index/iterator/doc-hit-info-iterator.h"
 #include "icing/schema/schema-store.h"
 #include "icing/schema/section.h"
 #include "icing/store/document-id.h"
@@ -104,6 +106,33 @@ class SectionRestrictData {
       const std::set<std::string>& target_sections) const;
 };
 
+// Indicate that the iterator can internally handle the section restriction
+// logic by itself.
+//
+// This is helpful when some iterators want to have better control for
+// optimization. For example, embedding iterator will be able to filter out
+// embedding hits from unwanted sections to avoid retrieving unnecessary vectors
+// and calculate scores for them.
+class DocHitInfoIteratorHandlingSectionRestrict
+    : public DocHitInfoLeafIterator {
+ protected:
+  bool HandleSectionRestriction(SectionRestrictData* other_data) override {
+    section_restrict_data_.push_back(other_data);
+    return true;
+  }
+
+  SectionIdMask ComputeAllowedSectionsMask(DocumentId document_id) {
+    SectionIdMask result = kSectionIdMaskAll;
+    for (SectionRestrictData* section_restrict_data : section_restrict_data_) {
+      result &= section_restrict_data->ComputeAllowedSectionsMask(document_id);
+    }
+    return result;
+  }
+
+  // Does not own the pointers.
+  std::vector<SectionRestrictData*> section_restrict_data_;
+};
+
 }  // namespace lib
 }  // namespace icing
 
diff --git a/icing/index/lite/doc-hit-info-iterator-term-lite.cc b/icing/index/lite/doc-hit-info-iterator-term-lite.cc
index 8b3b339..c356203 100644
--- a/icing/index/lite/doc-hit-info-iterator-term-lite.cc
+++ b/icing/index/lite/doc-hit-info-iterator-term-lite.cc
@@ -117,7 +117,7 @@ DocHitInfoIteratorTermLitePrefix::RetrieveMoreHits() {
   int terms_matched = 0;
   for (LiteIndex::PrefixIterator it = lite_index_->FindTermPrefixes(term_);
        it.IsValid(); it.Advance()) {
-    bool exact_match = strlen(it.GetKey()) == term_len;
+    bool exact_match = it.GetKey().size() == term_len;
     ICING_ASSIGN_OR_RETURN(
         uint32_t term_id,
         term_id_codec_->EncodeTvi(it.GetValueIndex(), TviType::LITE));
diff --git a/icing/index/lite/lite-index-header.h b/icing/index/lite/lite-index-header.h
index 741d173..1aba130 100644
--- a/icing/index/lite/lite-index-header.h
+++ b/icing/index/lite/lite-index-header.h
@@ -17,9 +17,11 @@
 
 #include <cstddef>
 #include <cstdint>
+#include <string_view>
 
 #include "icing/legacy/core/icing-string-util.h"
 #include "icing/store/document-id.h"
+#include "icing/util/crc32.h"
 
 namespace icing {
 namespace lib {
@@ -33,8 +35,8 @@ class LiteIndex_Header {
   // value associated with this header format.
   virtual bool check_magic() const = 0;
 
-  virtual uint32_t lite_index_crc() const = 0;
-  virtual void set_lite_index_crc(uint32_t crc) = 0;
+  virtual Crc32 lite_index_crc() const = 0;
+  virtual void set_lite_index_crc(Crc32 crc) = 0;
 
   virtual uint32_t last_added_docid() const = 0;
   virtual void set_last_added_docid(uint32_t last_added_docid) = 0;
@@ -45,7 +47,7 @@ class LiteIndex_Header {
   virtual uint32_t searchable_end() const = 0;
   virtual void set_searchable_end(uint32_t searchable_end) = 0;
 
-  virtual uint32_t CalculateHeaderCrc() const = 0;
+  virtual Crc32 GetHeaderCrc() const = 0;
 
   virtual void Reset() = 0;
 };
@@ -75,8 +77,10 @@ class LiteIndex_HeaderImpl : public LiteIndex_Header {
     return hdr_->magic == HeaderData::kMagic;
   }
 
-  uint32_t lite_index_crc() const override { return hdr_->lite_index_crc; }
-  void set_lite_index_crc(uint32_t crc) override { hdr_->lite_index_crc = crc; }
+  Crc32 lite_index_crc() const override { return Crc32(hdr_->lite_index_crc); }
+  void set_lite_index_crc(Crc32 crc) override {
+    hdr_->lite_index_crc = crc.Get();
+  }
 
   uint32_t last_added_docid() const override { return hdr_->last_added_docid; }
   void set_last_added_docid(uint32_t last_added_docid) override {
@@ -91,10 +95,11 @@ class LiteIndex_HeaderImpl : public LiteIndex_Header {
     hdr_->searchable_end = searchable_end;
   }
 
-  uint32_t CalculateHeaderCrc() const override {
-    return IcingStringUtil::UpdateCrc32(
-        0, reinterpret_cast<const char *>(hdr_) + offsetof(HeaderData, magic),
+  Crc32 GetHeaderCrc() const override {
+    std::string_view data(
+        reinterpret_cast<const char *>(hdr_) + offsetof(HeaderData, magic),
         sizeof(HeaderData) - offsetof(HeaderData, magic));
+    return Crc32(data);
   }
 
   void Reset() override {
diff --git a/icing/index/lite/lite-index.cc b/icing/index/lite/lite-index.cc
index 3aed7e4..98a34c4 100644
--- a/icing/index/lite/lite-index.cc
+++ b/icing/index/lite/lite-index.cc
@@ -178,7 +178,7 @@ libtextclassifier3::Status LiteIndex::Initialize() {
       goto error;
     }
 
-    UpdateChecksum();
+    UpdateChecksumInternal();
   } else {
     header_mmap_.Remap(hit_buffer_fd_.get(), kHeaderFileOffset, header_size());
     header_ = std::make_unique<LiteIndex_HeaderImpl>(
@@ -198,11 +198,12 @@ libtextclassifier3::Status LiteIndex::Initialize() {
       status = absl_ports::InternalError("Lite index header magic mismatch");
       goto error;
     }
-    Crc32 crc = ComputeChecksum();
-    if (crc.Get() != header_->lite_index_crc()) {
-      status = absl_ports::DataLossError(
-          IcingStringUtil::StringPrintf("Lite index crc check failed: %u vs %u",
-                                        crc.Get(), header_->lite_index_crc()));
+    Crc32 expected_crc(header_->lite_index_crc());
+    Crc32 crc = GetChecksumInternal();
+    if (crc != expected_crc) {
+      status = absl_ports::DataLossError(IcingStringUtil::StringPrintf(
+          "Lite index crc check failed: %u vs %u", crc.Get(),
+          header_->lite_index_crc().Get()));
       goto error;
     }
   }
@@ -224,27 +225,6 @@ error:
   return status;
 }
 
-Crc32 LiteIndex::ComputeChecksum() {
-  IcingTimer timer;
-
-  // Update crcs.
-  uint32_t dependent_crcs[2];
-  hit_buffer_.UpdateCrc();
-  dependent_crcs[0] = hit_buffer_crc_;
-  dependent_crcs[1] = lexicon_.UpdateCrc();
-
-  // Compute the master crc.
-
-  // Header crc, excluding the actual crc field.
-  Crc32 all_crc(header_->CalculateHeaderCrc());
-  all_crc.Append(std::string_view(reinterpret_cast<const char*>(dependent_crcs),
-                                  sizeof(dependent_crcs)));
-  ICING_VLOG(2) << "Lite index crc computed in " << timer.Elapsed() * 1000
-                << "ms";
-
-  return all_crc;
-}
-
 libtextclassifier3::Status LiteIndex::Reset() {
   IcingTimer timer;
 
@@ -254,7 +234,7 @@ libtextclassifier3::Status LiteIndex::Reset() {
   lexicon_.Clear();
   hit_buffer_.Clear();
   header_->Reset();
-  UpdateChecksum();
+  UpdateChecksumInternal();
 
   ICING_VLOG(2) << "Lite index clear in " << timer.Elapsed() * 1000 << "ms";
   return libtextclassifier3::Status::OK;
@@ -274,7 +254,7 @@ libtextclassifier3::Status LiteIndex::PersistToDisk() {
     success = false;
   }
   hit_buffer_.Sync();
-  UpdateChecksum();
+  UpdateChecksumInternal();
   header_mmap_.Sync();
 
   return (success) ? libtextclassifier3::Status::OK
@@ -282,8 +262,49 @@ libtextclassifier3::Status LiteIndex::PersistToDisk() {
                          "Unable to sync lite index components.");
 }
 
-void LiteIndex::UpdateChecksum() {
-  header_->set_lite_index_crc(ComputeChecksum().Get());
+Crc32 LiteIndex::UpdateChecksum() {
+  absl_ports::unique_lock l(&mutex_);
+  return UpdateChecksumInternal();
+}
+
+Crc32 LiteIndex::UpdateChecksumInternal() {
+  IcingTimer timer;
+
+  // Update crcs.
+  uint32_t dependent_crcs[2];
+  hit_buffer_.UpdateCrc();
+  dependent_crcs[0] = hit_buffer_crc_;
+  dependent_crcs[1] = lexicon_.UpdateCrc().Get();
+
+  // Update the header. The header is mmapped. So we don't need to explicitly
+  // write it.
+  Crc32 all_crc(header_->GetHeaderCrc());
+  all_crc.Append(std::string_view(reinterpret_cast<const char*>(dependent_crcs),
+                                  sizeof(dependent_crcs)));
+  header_->set_lite_index_crc(all_crc);
+  ICING_VLOG(2) << "Lite index crc updated in " << timer.Elapsed() * 1000
+                << "ms";
+  return all_crc;
+}
+
+Crc32 LiteIndex::GetChecksum() const {
+  absl_ports::unique_lock l(&mutex_);
+  return GetChecksumInternal();
+}
+
+Crc32 LiteIndex::GetChecksumInternal() const {
+  IcingTimer timer;
+
+  uint32_t dependent_crcs[2];
+  dependent_crcs[0] = hit_buffer_.GetCrc().Get();
+  dependent_crcs[1] = lexicon_.GetCrc().Get();
+
+  Crc32 all_crc(header_->GetHeaderCrc());
+  all_crc.Append(std::string_view(reinterpret_cast<const char*>(dependent_crcs),
+                                  sizeof(dependent_crcs)));
+  ICING_VLOG(2) << "Lite index crc computed in " << timer.Elapsed() * 1000
+                << "ms";
+  return all_crc;
 }
 
 libtextclassifier3::StatusOr<uint32_t> LiteIndex::InsertTerm(
@@ -291,8 +312,7 @@ libtextclassifier3::StatusOr<uint32_t> LiteIndex::InsertTerm(
     NamespaceId namespace_id) {
   absl_ports::unique_lock l(&mutex_);
   uint32_t tvi;
-  libtextclassifier3::Status status =
-      lexicon_.Insert(term.c_str(), "", &tvi, false);
+  libtextclassifier3::Status status = lexicon_.Insert(term, "", &tvi, false);
   if (!status.ok()) {
     ICING_LOG(DBG) << "Unable to add term " << term << " to lexicon!\n"
                    << status.error_message();
@@ -350,7 +370,7 @@ libtextclassifier3::StatusOr<uint32_t> LiteIndex::GetTermId(
   absl_ports::shared_lock l(&mutex_);
   char dummy;
   uint32_t tvi;
-  if (!lexicon_.Find(term.c_str(), &dummy, &tvi)) {
+  if (!lexicon_.Find(term, &dummy, &tvi)) {
     return absl_ports::NotFoundError(
         absl_ports::StrCat("Could not find ", term, " in the lexicon."));
   }
@@ -554,7 +574,7 @@ bool LiteIndex::is_full() const {
           lexicon_.min_free_fraction() < (1.0 - kTrieFullFraction));
 }
 
-std::string LiteIndex::GetDebugInfo(DebugInfoVerbosity::Code verbosity) {
+std::string LiteIndex::GetDebugInfo(DebugInfoVerbosity::Code verbosity) const {
   absl_ports::unique_lock l(&mutex_);
   std::string res;
   std::string lexicon_info;
@@ -570,7 +590,7 @@ std::string LiteIndex::GetDebugInfo(DebugInfoVerbosity::Code verbosity) {
       "lite_lexicon_info:\n%s\n",
       header_->cur_size(), options_.hit_buffer_size,
       header_->last_added_docid(), header_->searchable_end(),
-      ComputeChecksum().Get(), lexicon_info.c_str());
+      GetChecksumInternal().Get(), lexicon_info.c_str());
   return res;
 }
 
@@ -632,7 +652,7 @@ void LiteIndex::SortHitsImpl() {
   header_->set_searchable_end(header_->cur_size());
 
   // Update crc in-line.
-  UpdateChecksum();
+  UpdateChecksumInternal();
 }
 
 libtextclassifier3::Status LiteIndex::Optimize(
@@ -694,7 +714,7 @@ libtextclassifier3::Status LiteIndex::Optimize(
   for (IcingDynamicTrie::Iterator term_iter(lexicon_, /*prefix=*/"");
        term_iter.IsValid(); term_iter.Advance()) {
     if (tvi_to_delete.find(term_iter.GetValueIndex()) != tvi_to_delete.end()) {
-      terms_to_delete.insert(term_iter.GetKey());
+      terms_to_delete.insert(std::string(term_iter.GetKey()));
     }
   }
   for (const std::string& term : terms_to_delete) {
diff --git a/icing/index/lite/lite-index.h b/icing/index/lite/lite-index.h
index 45dc280..149fbea 100644
--- a/icing/index/lite/lite-index.h
+++ b/icing/index/lite/lite-index.h
@@ -24,6 +24,7 @@
 #include <limits>
 #include <memory>
 #include <string>
+#include <string_view>
 #include <vector>
 
 #include "icing/text_classifier/lib3/utils/base/status.h"
@@ -107,7 +108,7 @@ class LiteIndex {
 
     void Advance() { delegate_.Advance(); }
 
-    const char* GetKey() const { return delegate_.GetKey(); }
+    std::string_view GetKey() const { return delegate_.GetKey(); }
 
     uint32_t GetValueIndex() const { return delegate_.GetValueIndex(); }
 
@@ -120,7 +121,7 @@ class LiteIndex {
   PrefixIterator FindTermPrefixes(const std::string& prefix) const
       ICING_LOCKS_EXCLUDED(mutex_) {
     absl_ports::shared_lock l(&mutex_);
-    return PrefixIterator(IcingDynamicTrie::Iterator(lexicon_, prefix.c_str()));
+    return PrefixIterator(IcingDynamicTrie::Iterator(lexicon_, prefix));
   }
 
   // Inserts a term with its properties.
@@ -288,7 +289,7 @@ class LiteIndex {
   // Returns debug information for the index in out.
   // verbosity = BASIC, simplest debug information - size of lexicon, hit buffer
   // verbosity = DETAILED, more detailed debug information from the lexicon.
-  std::string GetDebugInfo(DebugInfoVerbosity::Code verbosity)
+  std::string GetDebugInfo(DebugInfoVerbosity::Code verbosity) const
       ICING_LOCKS_EXCLUDED(mutex_);
 
   // Returns the byte size of all the elements held in the index. This excludes
@@ -340,6 +341,14 @@ class LiteIndex {
       const TermIdCodec* term_id_codec, DocumentId new_last_added_document_id)
       ICING_LOCKS_EXCLUDED(mutex_);
 
+  // Updates the checksums of all index components, updates the combined
+  // checksum and returns it.
+  Crc32 UpdateChecksum() ICING_LOCKS_EXCLUDED(mutex_);
+
+  // Calculates the checksum of the index components and returns the combined
+  // checksum.
+  Crc32 GetChecksum() const ICING_LOCKS_EXCLUDED(mutex_);
+
  private:
   static IcingDynamicTrie::RuntimeOptions MakeTrieRuntimeOptions();
 
@@ -371,11 +380,13 @@ class LiteIndex {
     return header_->cur_size();
   }
 
-  // Calculate the checksum of all sub-components of the LiteIndex
-  Crc32 ComputeChecksum() ICING_EXCLUSIVE_LOCKS_REQUIRED(mutex_);
+  // Calculate the checksum of all sub-components of the LiteIndex and set it in
+  // the header.
+  Crc32 UpdateChecksumInternal() ICING_EXCLUSIVE_LOCKS_REQUIRED(mutex_);
 
-  // Sets the computed checksum in the header
-  void UpdateChecksum() ICING_EXCLUSIVE_LOCKS_REQUIRED(mutex_);
+  // Calculates the checksum of all sub-components of the LiteIndex. Does NOT
+  // update the header.
+  Crc32 GetChecksumInternal() const ICING_EXCLUSIVE_LOCKS_REQUIRED(mutex_);
 
   // Non-locking implementation for UpdateTermProperties.
   libtextclassifier3::Status UpdateTermPropertiesImpl(uint32_t tvi,
diff --git a/icing/index/lite/lite-index_test.cc b/icing/index/lite/lite-index_test.cc
index f8ea94a..458f58c 100644
--- a/icing/index/lite/lite-index_test.cc
+++ b/icing/index/lite/lite-index_test.cc
@@ -20,6 +20,7 @@
 #include <unordered_map>
 #include <vector>
 
+#include "icing/text_classifier/lib3/utils/base/status.h"
 #include "gmock/gmock.h"
 #include "gtest/gtest.h"
 #include "icing/file/filesystem.h"
@@ -38,6 +39,7 @@
 #include "icing/testing/always-false-suggestion-result-checker-impl.h"
 #include "icing/testing/common-matchers.h"
 #include "icing/testing/tmp-directory.h"
+#include "icing/util/crc32.h"
 
 namespace icing {
 namespace lib {
@@ -81,6 +83,154 @@ TEST_F(LiteIndexTest, TermIdHitPairInvalidValue) {
               Eq(Hit::kDefaultTermFrequency));
 }
 
+TEST_F(LiteIndexTest, OutOfDateChecksumFailsInit) {
+  // 1. Create LiteIndex and add some content.
+  std::string lite_index_file_name = index_dir_ + "/test_file.lite-idx.index";
+  // Unsorted tail can contain a max of 8 TermIdHitPairs.
+  LiteIndex::Options options(
+      lite_index_file_name,
+      /*hit_buffer_want_merge_bytes=*/1024 * 1024,
+      /*hit_buffer_sort_at_indexing=*/false,
+      /*hit_buffer_sort_threshold_bytes=*/sizeof(TermIdHitPair) * 8);
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<LiteIndex> lite_index,
+                             LiteIndex::Create(options, &icing_filesystem_));
+  ICING_ASSERT_OK_AND_ASSIGN(
+      term_id_codec_,
+      TermIdCodec::Create(
+          IcingDynamicTrie::max_value_index(IcingDynamicTrie::Options()),
+          IcingDynamicTrie::max_value_index(options.lexicon_options)));
+
+  // Add some hits
+  ICING_ASSERT_OK_AND_ASSIGN(
+      uint32_t foo_tvi,
+      lite_index->InsertTerm("foo", TermMatchType::PREFIX, kNamespace0));
+  ICING_ASSERT_OK_AND_ASSIGN(uint32_t foo_term_id,
+                             term_id_codec_->EncodeTvi(foo_tvi, TviType::LITE));
+  Hit foo_hit0(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+  Hit foo_hit1(/*section_id=*/1, /*document_id=*/1, Hit::kDefaultTermFrequency,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+  ICING_ASSERT_OK(lite_index->AddHit(foo_term_id, foo_hit0));
+  ICING_ASSERT_OK(lite_index->AddHit(foo_term_id, foo_hit1));
+
+  // 2. Create a new LiteIndex. Create should fail because the checksum is out
+  // of date.
+  EXPECT_THAT(LiteIndex::Create(options, &icing_filesystem_),
+              StatusIs(libtextclassifier3::StatusCode::INTERNAL));
+}
+
+TEST_F(LiteIndexTest, UpdatedChecksumPassesInit) {
+  // 1. Create LiteIndex and add some content.
+  std::string lite_index_file_name = index_dir_ + "/test_file.lite-idx.index";
+  // Unsorted tail can contain a max of 8 TermIdHitPairs.
+  LiteIndex::Options options(
+      lite_index_file_name,
+      /*hit_buffer_want_merge_bytes=*/1024 * 1024,
+      /*hit_buffer_sort_at_indexing=*/false,
+      /*hit_buffer_sort_threshold_bytes=*/sizeof(TermIdHitPair) * 8);
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<LiteIndex> lite_index,
+                             LiteIndex::Create(options, &icing_filesystem_));
+  ICING_ASSERT_OK_AND_ASSIGN(
+      term_id_codec_,
+      TermIdCodec::Create(
+          IcingDynamicTrie::max_value_index(IcingDynamicTrie::Options()),
+          IcingDynamicTrie::max_value_index(options.lexicon_options)));
+
+  // Add some hits
+  ICING_ASSERT_OK_AND_ASSIGN(
+      uint32_t foo_tvi,
+      lite_index->InsertTerm("foo", TermMatchType::PREFIX, kNamespace0));
+  ICING_ASSERT_OK_AND_ASSIGN(uint32_t foo_term_id,
+                             term_id_codec_->EncodeTvi(foo_tvi, TviType::LITE));
+  Hit foo_hit0(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+  Hit foo_hit1(/*section_id=*/1, /*document_id=*/1, Hit::kDefaultTermFrequency,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+  ICING_ASSERT_OK(lite_index->AddHit(foo_term_id, foo_hit0));
+  ICING_ASSERT_OK(lite_index->AddHit(foo_term_id, foo_hit1));
+
+  // 2. Updating the checksum should be sufficient to successfully initialize
+  // the next time.
+  Crc32 checksum = lite_index->GetChecksum();
+  EXPECT_THAT(lite_index->UpdateChecksum(), Eq(checksum));
+  EXPECT_THAT(lite_index->GetChecksum(), Eq(checksum));
+
+  // 3. Create a new LiteIndex. Create should succeed because the checksum has
+  // been updated.
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<LiteIndex> lite_index2,
+                             LiteIndex::Create(options, &icing_filesystem_));
+
+  // 4. Verify that the hits in the LiteIndex were kept.
+  std::vector<DocHitInfo> hits1;
+  lite_index2->FetchHits(
+      foo_term_id, kSectionIdMaskAll,
+      /*only_from_prefix_sections=*/false,
+      SuggestionScoringSpecProto::SuggestionRankingStrategy::DOCUMENT_COUNT,
+      /*namespace_checker=*/nullptr, &hits1);
+  EXPECT_THAT(hits1, SizeIs(1));
+  EXPECT_THAT(hits1.back().document_id(), Eq(1));
+  // Check that the hits are coming from section 0 and section 1.
+  EXPECT_THAT(hits1.back().hit_section_ids_mask(), Eq(0b11));
+}
+
+TEST_F(LiteIndexTest, PersistedIndexPassesInit) {
+  // 1. Create LiteIndex and add some content.
+  std::string lite_index_file_name = index_dir_ + "/test_file.lite-idx.index";
+  // Unsorted tail can contain a max of 8 TermIdHitPairs.
+  LiteIndex::Options options(
+      lite_index_file_name,
+      /*hit_buffer_want_merge_bytes=*/1024 * 1024,
+      /*hit_buffer_sort_at_indexing=*/false,
+      /*hit_buffer_sort_threshold_bytes=*/sizeof(TermIdHitPair) * 8);
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<LiteIndex> lite_index,
+                             LiteIndex::Create(options, &icing_filesystem_));
+  ICING_ASSERT_OK_AND_ASSIGN(
+      term_id_codec_,
+      TermIdCodec::Create(
+          IcingDynamicTrie::max_value_index(IcingDynamicTrie::Options()),
+          IcingDynamicTrie::max_value_index(options.lexicon_options)));
+
+  // Add some hits
+  ICING_ASSERT_OK_AND_ASSIGN(
+      uint32_t foo_tvi,
+      lite_index->InsertTerm("foo", TermMatchType::PREFIX, kNamespace0));
+  ICING_ASSERT_OK_AND_ASSIGN(uint32_t foo_term_id,
+                             term_id_codec_->EncodeTvi(foo_tvi, TviType::LITE));
+  Hit foo_hit0(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+  Hit foo_hit1(/*section_id=*/1, /*document_id=*/1, Hit::kDefaultTermFrequency,
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
+  ICING_ASSERT_OK(lite_index->AddHit(foo_term_id, foo_hit0));
+  ICING_ASSERT_OK(lite_index->AddHit(foo_term_id, foo_hit1));
+
+  // 2. PersistToDisk should be sufficient to successfully initialize the next
+  // time.
+  ICING_ASSERT_OK(lite_index->PersistToDisk());
+
+  // 3. Create a new LiteIndex. Create should succeed because we've called
+  // PersistToDisk.
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<LiteIndex> lite_index2,
+                             LiteIndex::Create(options, &icing_filesystem_));
+
+  // 4. Verify that the hits in the LiteIndex were kept.
+  std::vector<DocHitInfo> hits1;
+  lite_index2->FetchHits(
+      foo_term_id, kSectionIdMaskAll,
+      /*only_from_prefix_sections=*/false,
+      SuggestionScoringSpecProto::SuggestionRankingStrategy::DOCUMENT_COUNT,
+      /*namespace_checker=*/nullptr, &hits1);
+  EXPECT_THAT(hits1, SizeIs(1));
+  EXPECT_THAT(hits1.back().document_id(), Eq(1));
+  // Check that the hits are coming from section 0 and section 1.
+  EXPECT_THAT(hits1.back().hit_section_ids_mask(), Eq(0b11));
+}
+
 TEST_F(LiteIndexTest,
        LiteIndexFetchHits_sortAtQuerying_unsortedHitsBelowSortThreshold) {
   // Set up LiteIndex and TermIdCodec
@@ -106,9 +256,11 @@ TEST_F(LiteIndexTest,
   ICING_ASSERT_OK_AND_ASSIGN(uint32_t foo_term_id,
                              term_id_codec_->EncodeTvi(foo_tvi, TviType::LITE));
   Hit foo_hit0(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   Hit foo_hit1(/*section_id=*/1, /*document_id=*/1, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index->AddHit(foo_term_id, foo_hit0));
   ICING_ASSERT_OK(lite_index->AddHit(foo_term_id, foo_hit1));
 
@@ -118,9 +270,11 @@ TEST_F(LiteIndexTest,
   ICING_ASSERT_OK_AND_ASSIGN(uint32_t bar_term_id,
                              term_id_codec_->EncodeTvi(bar_tvi, TviType::LITE));
   Hit bar_hit0(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   Hit bar_hit1(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index->AddHit(bar_term_id, bar_hit0));
   ICING_ASSERT_OK(lite_index->AddHit(bar_term_id, bar_hit1));
 
@@ -188,9 +342,11 @@ TEST_F(LiteIndexTest,
   ICING_ASSERT_OK_AND_ASSIGN(uint32_t foo_term_id,
                              term_id_codec_->EncodeTvi(foo_tvi, TviType::LITE));
   Hit foo_hit0(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   Hit foo_hit1(/*section_id=*/1, /*document_id=*/1, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index->AddHit(foo_term_id, foo_hit0));
   ICING_ASSERT_OK(lite_index->AddHit(foo_term_id, foo_hit1));
 
@@ -200,9 +356,11 @@ TEST_F(LiteIndexTest,
   ICING_ASSERT_OK_AND_ASSIGN(uint32_t bar_term_id,
                              term_id_codec_->EncodeTvi(bar_tvi, TviType::LITE));
   Hit bar_hit0(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   Hit bar_hit1(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index->AddHit(bar_term_id, bar_hit0));
   ICING_ASSERT_OK(lite_index->AddHit(bar_term_id, bar_hit1));
 
@@ -268,36 +426,50 @@ TEST_F(
   // Create 4 hits for docs 0-2, and 2 hits for doc 3 -- 14 in total
   // Doc 0
   Hit doc0_hit0(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc0_hit1(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc0_hit2(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc0_hit3(/*section_id=*/2, /*document_id=*/0, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   // Doc 1
   Hit doc1_hit0(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc1_hit1(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc1_hit2(/*section_id=*/1, /*document_id=*/1, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc1_hit3(/*section_id=*/2, /*document_id=*/1, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   // Doc 2
   Hit doc2_hit0(/*section_id=*/0, /*document_id=*/2, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc2_hit1(/*section_id=*/0, /*document_id=*/2, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc2_hit2(/*section_id=*/1, /*document_id=*/2, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc2_hit3(/*section_id=*/2, /*document_id=*/2, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   // Doc 3
   Hit doc3_hit0(/*section_id=*/0, /*document_id=*/3, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc3_hit1(/*section_id=*/0, /*document_id=*/3, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
 
   // Create terms
   // Foo
@@ -426,49 +598,69 @@ TEST_F(
   // Create 4 hits for docs 0-2, and 2 hits for doc 3 -- 14 in total
   // Doc 0
   Hit doc0_hit0(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc0_hit1(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc0_hit2(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc0_hit3(/*section_id=*/2, /*document_id=*/0, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   // Doc 1
   Hit doc1_hit0(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc1_hit1(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc1_hit2(/*section_id=*/1, /*document_id=*/1, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc1_hit3(/*section_id=*/2, /*document_id=*/1, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   // Doc 2
   Hit doc2_hit0(/*section_id=*/0, /*document_id=*/2, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc2_hit1(/*section_id=*/0, /*document_id=*/2, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc2_hit2(/*section_id=*/1, /*document_id=*/2, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc2_hit3(/*section_id=*/2, /*document_id=*/2, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   // Doc 3
   Hit doc3_hit0(/*section_id=*/0, /*document_id=*/3, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc3_hit1(/*section_id=*/0, /*document_id=*/3, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc3_hit2(/*section_id=*/1, /*document_id=*/3, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc3_hit3(/*section_id=*/2, /*document_id=*/3, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   // Doc 4
   Hit doc4_hit0(/*section_id=*/0, /*document_id=*/4, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc4_hit1(/*section_id=*/0, /*document_id=*/4, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc4_hit2(/*section_id=*/1, /*document_id=*/4, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc4_hit3(/*section_id=*/2, /*document_id=*/4, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
 
   // Create terms
   // Foo
@@ -623,9 +815,11 @@ TEST_F(LiteIndexTest, LiteIndexIterator) {
   ICING_ASSERT_OK_AND_ASSIGN(uint32_t foo_term_id,
                              term_id_codec_->EncodeTvi(tvi, TviType::LITE));
   Hit doc0_hit0(/*section_id=*/0, /*document_id=*/0, /*term_frequency=*/3,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc0_hit1(/*section_id=*/1, /*document_id=*/0, /*term_frequency=*/5,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   SectionIdMask doc0_section_id_mask = 0b11;
   std::unordered_map<SectionId, Hit::TermFrequency>
       expected_section_ids_tf_map0 = {{0, 3}, {1, 5}};
@@ -633,9 +827,11 @@ TEST_F(LiteIndexTest, LiteIndexIterator) {
   ICING_ASSERT_OK(lite_index->AddHit(foo_term_id, doc0_hit1));
 
   Hit doc1_hit1(/*section_id=*/1, /*document_id=*/1, /*term_frequency=*/7,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc1_hit2(/*section_id=*/2, /*document_id=*/1, /*term_frequency=*/11,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   SectionIdMask doc1_section_id_mask = 0b110;
   std::unordered_map<SectionId, Hit::TermFrequency>
       expected_section_ids_tf_map1 = {{1, 7}, {2, 11}};
@@ -692,9 +888,11 @@ TEST_F(LiteIndexTest, LiteIndexIterator_sortAtIndexingDisabled) {
   ICING_ASSERT_OK_AND_ASSIGN(uint32_t foo_term_id,
                              term_id_codec_->EncodeTvi(tvi, TviType::LITE));
   Hit doc0_hit0(/*section_id=*/0, /*document_id=*/0, /*term_frequency=*/3,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc0_hit1(/*section_id=*/1, /*document_id=*/0, /*term_frequency=*/5,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   SectionIdMask doc0_section_id_mask = 0b11;
   std::unordered_map<SectionId, Hit::TermFrequency>
       expected_section_ids_tf_map0 = {{0, 3}, {1, 5}};
@@ -702,9 +900,11 @@ TEST_F(LiteIndexTest, LiteIndexIterator_sortAtIndexingDisabled) {
   ICING_ASSERT_OK(lite_index->AddHit(foo_term_id, doc0_hit1));
 
   Hit doc1_hit1(/*section_id=*/1, /*document_id=*/1, /*term_frequency=*/7,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   Hit doc1_hit2(/*section_id=*/2, /*document_id=*/1, /*term_frequency=*/11,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   SectionIdMask doc1_section_id_mask = 0b110;
   std::unordered_map<SectionId, Hit::TermFrequency>
       expected_section_ids_tf_map1 = {{1, 7}, {2, 11}};
@@ -761,9 +961,11 @@ TEST_F(LiteIndexTest, LiteIndexHitBufferSize) {
   ICING_ASSERT_OK_AND_ASSIGN(uint32_t foo_term_id,
                              term_id_codec_->EncodeTvi(tvi, TviType::LITE));
   Hit hit0(/*section_id=*/0, /*document_id=*/0, /*term_frequency=*/3,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   Hit hit1(/*section_id=*/1, /*document_id=*/0, /*term_frequency=*/5,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index->AddHit(foo_term_id, hit0));
   ICING_ASSERT_OK(lite_index->AddHit(foo_term_id, hit1));
 
diff --git a/icing/index/lite/lite-index_thread-safety_test.cc b/icing/index/lite/lite-index_thread-safety_test.cc
index a73ca28..3b00590 100644
--- a/icing/index/lite/lite-index_thread-safety_test.cc
+++ b/icing/index/lite/lite-index_thread-safety_test.cc
@@ -120,10 +120,10 @@ TEST_F(LiteIndexThreadSafetyTest, SimultaneousFetchHits_singleTerm) {
                              term_id_codec_->EncodeTvi(foo_tvi, TviType::LITE));
   Hit doc_hit0(/*section_id=*/kSectionId0, /*document_id=*/kDocumentId0,
                Hit::kDefaultTermFrequency, /*is_in_prefix_section=*/false,
-               /*is_prefix_hit=*/false);
+               /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
   Hit doc_hit1(/*section_id=*/kSectionId0, /*document_id=*/kDocumentId1,
                Hit::kDefaultTermFrequency, /*is_in_prefix_section=*/false,
-               /*is_prefix_hit=*/false);
+               /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foo_term_id, doc_hit0));
   ICING_ASSERT_OK(lite_index_->AddHit(foo_term_id, doc_hit1));
 
@@ -168,10 +168,10 @@ TEST_F(LiteIndexThreadSafetyTest, SimultaneousFetchHits_multipleTerms) {
                                term_id_codec_->EncodeTvi(tvi, TviType::LITE));
     Hit doc_hit0(/*section_id=*/kSectionId0, /*document_id=*/kDocumentId0,
                  Hit::kDefaultTermFrequency, /*is_in_prefix_section=*/false,
-                 /*is_prefix_hit=*/false);
+                 /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
     Hit doc_hit1(/*section_id=*/kSectionId0, /*document_id=*/kDocumentId1,
                  Hit::kDefaultTermFrequency, /*is_in_prefix_section=*/false,
-                 /*is_prefix_hit=*/false);
+                 /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
     ICING_ASSERT_OK(lite_index_->AddHit(term_id, doc_hit0));
     ICING_ASSERT_OK(lite_index_->AddHit(term_id, doc_hit1));
   }
@@ -223,7 +223,7 @@ TEST_F(LiteIndexThreadSafetyTest, SimultaneousAddHitAndFetchHits_singleTerm) {
                              term_id_codec_->EncodeTvi(foo_tvi, TviType::LITE));
   Hit doc_hit0(/*section_id=*/kSectionId0, /*document_id=*/kDocumentId0,
                Hit::kDefaultTermFrequency, /*is_in_prefix_section=*/false,
-               /*is_prefix_hit=*/false);
+               /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foo_term_id, doc_hit0));
 
   // Create kNumThreads threads. Every even-numbered thread calls FetchHits and
@@ -244,7 +244,7 @@ TEST_F(LiteIndexThreadSafetyTest, SimultaneousAddHitAndFetchHits_singleTerm) {
       // Odd-numbered thread calls AddHit.
       Hit doc_hit(/*section_id=*/thread_id / 2, /*document_id=*/kDocumentId0,
                   Hit::kDefaultTermFrequency, /*is_in_prefix_section=*/false,
-                  /*is_prefix_hit=*/false);
+                  /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
       ICING_ASSERT_OK(lite_index_->AddHit(foo_term_id, doc_hit));
     }
   };
@@ -290,7 +290,7 @@ TEST_F(LiteIndexThreadSafetyTest,
                              term_id_codec_->EncodeTvi(foo_tvi, TviType::LITE));
   Hit doc_hit0(/*section_id=*/kSectionId0, /*document_id=*/kDocumentId0,
                Hit::kDefaultTermFrequency, /*is_in_prefix_section=*/false,
-               /*is_prefix_hit=*/false);
+               /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foo_term_id, doc_hit0));
 
   // Create kNumThreads threads. Every even-numbered thread calls FetchHits and
@@ -320,7 +320,7 @@ TEST_F(LiteIndexThreadSafetyTest,
       // AddHit to section 0 of a new doc.
       Hit doc_hit(/*section_id=*/kSectionId0, /*document_id=*/thread_id / 2,
                   Hit::kDefaultTermFrequency, /*is_in_prefix_section=*/false,
-                  /*is_prefix_hit=*/false);
+                  /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
       ICING_ASSERT_OK(lite_index_->AddHit(term_id, doc_hit));
     }
   };
@@ -354,10 +354,10 @@ TEST_F(LiteIndexThreadSafetyTest, ManyAddHitAndOneFetchHits_multipleTerms) {
                                term_id_codec_->EncodeTvi(tvi, TviType::LITE));
     Hit doc_hit0(/*section_id=*/kSectionId0, /*document_id=*/kDocumentId0,
                  Hit::kDefaultTermFrequency, /*is_in_prefix_section=*/false,
-                 /*is_prefix_hit=*/false);
+                 /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
     Hit doc_hit1(/*section_id=*/kSectionId1, /*document_id=*/kDocumentId0,
                  Hit::kDefaultTermFrequency, /*is_in_prefix_section=*/false,
-                 /*is_prefix_hit=*/false);
+                 /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
     ICING_ASSERT_OK(lite_index_->AddHit(term_id, doc_hit0));
     ICING_ASSERT_OK(lite_index_->AddHit(term_id, doc_hit1));
   }
@@ -390,7 +390,8 @@ TEST_F(LiteIndexThreadSafetyTest, ManyAddHitAndOneFetchHits_multipleTerms) {
       // AddHit to section (thread_id % 5 + 1) of doc 0.
       Hit doc_hit(/*section_id=*/thread_id % 5 + 1,
                   /*document_id=*/kDocumentId0, Hit::kDefaultTermFrequency,
-                  /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                  /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                  /*is_stemmed_hit=*/false);
       ICING_ASSERT_OK(lite_index_->AddHit(term_id, doc_hit));
     }
   };
diff --git a/icing/index/lite/term-id-hit-pair_test.cc b/icing/index/lite/term-id-hit-pair_test.cc
index 28855b4..78e56ea 100644
--- a/icing/index/lite/term-id-hit-pair_test.cc
+++ b/icing/index/lite/term-id-hit-pair_test.cc
@@ -23,6 +23,7 @@
 #include "icing/index/hit/hit.h"
 #include "icing/schema/section.h"
 #include "icing/store/document-id.h"
+#include "icing/testing/common-matchers.h"
 
 namespace icing {
 namespace lib {
@@ -40,31 +41,43 @@ static constexpr uint32_t kSomeLargerTermId = 0b101010101111111100000001;
 
 TEST(TermIdHitPairTest, Accessors) {
   Hit hit1(kSomeSectionid, kSomeDocumentId, kSomeTermFrequency,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   Hit hit2(kSomeSectionid, kSomeDocumentId, kSomeTermFrequency,
-           /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true);
+           /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true,
+           /*is_stemmed_hit=*/false);
   Hit hit3(kSomeSectionid, kSomeDocumentId, kSomeTermFrequency,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit4(kSomeSectionid, kSomeDocumentId, kSomeTermFrequency,
+           /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true,
+           /*is_stemmed_hit=*/true);
   Hit invalid_hit(Hit::kInvalidValue);
 
   TermIdHitPair term_id_hit_pair_1(kSomeTermId, hit1);
   EXPECT_THAT(term_id_hit_pair_1.term_id(), Eq(kSomeTermId));
-  EXPECT_THAT(term_id_hit_pair_1.hit(), Eq(hit1));
+  EXPECT_THAT(term_id_hit_pair_1.hit(), EqualsHit(hit1));
 
   TermIdHitPair term_id_hit_pair_2(kSomeLargerTermId, hit2);
   EXPECT_THAT(term_id_hit_pair_2.term_id(), Eq(kSomeLargerTermId));
-  EXPECT_THAT(term_id_hit_pair_2.hit(), Eq(hit2));
+  EXPECT_THAT(term_id_hit_pair_2.hit(), EqualsHit(hit2));
 
   TermIdHitPair term_id_hit_pair_3(kSomeTermId, invalid_hit);
   EXPECT_THAT(term_id_hit_pair_3.term_id(), Eq(kSomeTermId));
-  EXPECT_THAT(term_id_hit_pair_3.hit(), Eq(invalid_hit));
+  EXPECT_THAT(term_id_hit_pair_3.hit(), EqualsHit(invalid_hit));
+
+  TermIdHitPair term_id_hit_pair_4(kSomeTermId, hit4);
+  EXPECT_THAT(term_id_hit_pair_4.term_id(), Eq(kSomeTermId));
+  EXPECT_THAT(term_id_hit_pair_4.hit(), EqualsHit(hit4));
 }
 
 TEST(TermIdHitPairTest, Comparison) {
   Hit hit(kSomeSectionid, kSomeDocumentId, kSomeTermFrequency,
-          /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+          /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+          /*is_stemmed_hit=*/false);
   Hit smaller_hit(/*section_id=*/1, /*document_id=*/100, /*term_frequency=*/1,
-                  /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                  /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                  /*is_stemmed_hit=*/false);
 
   TermIdHitPair term_id_hit_pair(kSomeTermId, hit);
   TermIdHitPair term_id_hit_pair_equal(kSomeTermId, hit);
diff --git a/icing/index/main/main-index-merger.cc b/icing/index/main/main-index-merger.cc
index cc130c2..8397e2c 100644
--- a/icing/index/main/main-index-merger.cc
+++ b/icing/index/main/main-index-merger.cc
@@ -77,7 +77,7 @@ class HitSelector {
           best_prefix_hit_.term_id(),
           Hit(prefix_hit.section_id(), prefix_hit.document_id(),
               final_term_frequency, prefix_hit.is_in_prefix_section(),
-              prefix_hit.is_prefix_hit()));
+              prefix_hit.is_prefix_hit(), prefix_hit.is_stemmed_hit()));
       (*hits)[pos++] = best_prefix_hit_;
       // Ensure sorted.
       if (best_prefix_hit_.hit() < best_exact_hit_.hit()) {
@@ -113,7 +113,8 @@ class HitSelector {
           term_id_hit_pair.term_id(),
           Hit(hit.section_id(), hit.document_id(), final_term_frequency,
               best_prefix_hit_.hit().is_in_prefix_section(),
-              best_prefix_hit_.hit().is_prefix_hit()));
+              best_prefix_hit_.hit().is_prefix_hit(),
+              best_prefix_hit_.hit().is_stemmed_hit()));
     }
   }
 
@@ -131,7 +132,8 @@ class HitSelector {
           term_id_hit_pair.term_id(),
           Hit(hit.section_id(), hit.document_id(), final_term_frequency,
               best_exact_hit_.hit().is_in_prefix_section(),
-              best_exact_hit_.hit().is_prefix_hit()));
+              best_exact_hit_.hit().is_prefix_hit(),
+              best_exact_hit_.hit().is_stemmed_hit()));
     }
   }
 
@@ -149,7 +151,7 @@ class HitComparator {
         main_tvi_to_block_index_(&main_tvi_to_block_index) {}
 
   bool operator()(const TermIdHitPair& lhs, const TermIdHitPair& rhs) const {
-    // Primary sort by index block. This acheives two things:
+    // Primary sort by index block. This achieves two things:
     // 1. It reduces the number of flash writes by grouping together new hits
     // for terms whose posting lists might share the same index block.
     // 2. More importantly, this ensures that newly added backfill branch points
@@ -287,7 +289,8 @@ MainIndexMerger::TranslateAndExpandLiteHits(
       size_t len = itr_prefixes->second.second;
       size_t offset_end_exclusive = offset + len;
       Hit prefix_hit(hit.section_id(), hit.document_id(), hit.term_frequency(),
-                     /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true);
+                     /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true,
+                     /*is_stemmed_hit=*/false);
       for (; offset < offset_end_exclusive; ++offset) {
         // Take the tvi (in the main lexicon) of each prefix term.
         uint32_t prefix_main_tvi =
diff --git a/icing/index/main/main-index-merger.h b/icing/index/main/main-index-merger.h
index 1413a8f..527d45e 100644
--- a/icing/index/main/main-index-merger.h
+++ b/icing/index/main/main-index-merger.h
@@ -15,10 +15,11 @@
 #ifndef ICING_INDEX_MAIN_MAIN_INDEX_MERGER_H_
 #define ICING_INDEX_MAIN_MAIN_INDEX_MERGER_H_
 
-#include <memory>
+#include <vector>
 
 #include "icing/text_classifier/lib3/utils/base/statusor.h"
 #include "icing/index/lite/lite-index.h"
+#include "icing/index/lite/term-id-hit-pair.h"
 #include "icing/index/main/main-index.h"
 #include "icing/index/term-id-codec.h"
 
diff --git a/icing/index/main/main-index-merger_test.cc b/icing/index/main/main-index-merger_test.cc
index 333e338..8a8aeef 100644
--- a/icing/index/main/main-index-merger_test.cc
+++ b/icing/index/main/main-index-merger_test.cc
@@ -93,10 +93,12 @@ TEST_F(MainIndexMergerTest, TranslateTermNotAdded) {
       term_id_codec_->EncodeTvi(fool_tvi, TviType::LITE));
 
   Hit doc0_hit(/*section_id=*/0, /*document_id=*/0, /*term_frequency=*/57,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc0_hit));
   Hit doc1_hit(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(fool_term_id, doc1_hit));
 
   // 2. Build up a fake LexiconMergeOutputs
@@ -132,10 +134,12 @@ TEST_F(MainIndexMergerTest, PrefixExpansion) {
       term_id_codec_->EncodeTvi(fool_tvi, TviType::LITE));
 
   Hit doc0_hit(/*section_id=*/0, /*document_id=*/0, /*term_frequency=*/57,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc0_hit));
   Hit doc1_hit(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(fool_term_id, doc1_hit));
 
   // 2. Build up a fake LexiconMergeOutputs
@@ -146,7 +150,8 @@ TEST_F(MainIndexMergerTest, PrefixExpansion) {
       term_id_codec_->EncodeTvi(foo_main_tvi, TviType::MAIN));
   Hit doc1_prefix_hit(/*section_id=*/0, /*document_id=*/1,
                       Hit::kDefaultTermFrequency,
-                      /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true);
+                      /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true,
+                      /*is_stemmed_hit=*/false);
 
   uint32_t foot_main_tvi = 5;
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -195,11 +200,12 @@ TEST_F(MainIndexMergerTest, DedupePrefixAndExactWithDifferentTermFrequencies) {
                              term_id_codec_->EncodeTvi(foo_tvi, TviType::LITE));
 
   Hit foot_doc0_hit(/*section_id=*/0, /*document_id=*/0, /*term_frequency=*/57,
-                    /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+                    /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+                    /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, foot_doc0_hit));
   Hit foo_doc0_hit(/*section_id=*/0, /*document_id=*/0,
                    Hit::kDefaultTermFrequency, /*is_in_prefix_section=*/true,
-                   /*is_prefix_hit=*/false);
+                   /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foo_term_id, foo_doc0_hit));
 
   // 2. Build up a fake LexiconMergeOutputs
@@ -212,7 +218,8 @@ TEST_F(MainIndexMergerTest, DedupePrefixAndExactWithDifferentTermFrequencies) {
   // hit for 'foot'. The final prefix hit has term frequency equal to 58.
   Hit doc0_prefix_hit(/*section_id=*/0, /*document_id=*/0,
                       /*term_frequency=*/58,
-                      /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true);
+                      /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true,
+                      /*is_stemmed_hit=*/false);
 
   uint32_t foot_main_tvi = 5;
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -259,16 +266,18 @@ TEST_F(MainIndexMergerTest, DedupeWithExactSameTermFrequencies) {
                              term_id_codec_->EncodeTvi(foo_tvi, TviType::LITE));
 
   Hit foot_doc0_hit(/*section_id=*/0, /*document_id=*/0, /*term_frequency=*/57,
-                    /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+                    /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+                    /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, foot_doc0_hit));
   Hit foo_doc0_hit(/*section_id=*/0, /*document_id=*/0, /*term_frequency=*/57,
-                   /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+                   /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+                   /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foo_term_id, foo_doc0_hit));
   // The prefix hit should take the sum as term_frequency - 114.
   Hit prefix_foo_doc0_hit(/*section_id=*/0, /*document_id=*/0,
                           /*term_frequency=*/114,
                           /*is_in_prefix_section=*/true,
-                          /*is_prefix_hit=*/true);
+                          /*is_prefix_hit=*/true, /*is_stemmed_hit=*/false);
 
   // 2. Build up a fake LexiconMergeOutputs
   // This is some made up number that doesn't matter for this test.
@@ -324,11 +333,13 @@ TEST_F(MainIndexMergerTest, DedupePrefixExpansion) {
 
   Hit foot_doc0_hit(/*section_id=*/0, /*document_id=*/0,
                     /*term_frequency=*/Hit::kMaxTermFrequency,
-                    /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+                    /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+                    /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, foot_doc0_hit));
   Hit fool_doc0_hit(/*section_id=*/0, /*document_id=*/0,
                     Hit::kDefaultTermFrequency,
-                    /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+                    /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+                    /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(fool_term_id, fool_doc0_hit));
 
   // 2. Build up a fake LexiconMergeOutputs
@@ -341,7 +352,8 @@ TEST_F(MainIndexMergerTest, DedupePrefixExpansion) {
   // kMaxTermFrequency.
   Hit doc0_prefix_hit(/*section_id=*/0, /*document_id=*/0,
                       /*term_frequency=*/Hit::kMaxTermFrequency,
-                      /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true);
+                      /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true,
+                      /*is_stemmed_hit=*/false);
 
   uint32_t foot_main_tvi = 5;
   ICING_ASSERT_OK_AND_ASSIGN(
diff --git a/icing/index/main/main-index.cc b/icing/index/main/main-index.cc
index 85ee4dc..997cf13 100644
--- a/icing/index/main/main-index.cc
+++ b/icing/index/main/main-index.cc
@@ -13,25 +13,42 @@
 // limitations under the License.
 #include "icing/index/main/main-index.h"
 
+#include <algorithm>
 #include <cstdint>
 #include <cstring>
+#include <iterator>
 #include <memory>
 #include <string>
-#include <unordered_set>
+#include <unordered_map>
+#include <utility>
+#include <vector>
 
+#include "icing/text_classifier/lib3/utils/base/status.h"
+#include "icing/text_classifier/lib3/utils/base/statusor.h"
 #include "icing/absl_ports/canonical_errors.h"
 #include "icing/absl_ports/str_cat.h"
 #include "icing/file/destructible-directory.h"
+#include "icing/file/filesystem.h"
 #include "icing/file/posting_list/flash-index-storage.h"
+#include "icing/file/posting_list/posting-list-accessor.h"
 #include "icing/file/posting_list/posting-list-common.h"
+#include "icing/file/posting_list/posting-list-identifier.h"
+#include "icing/index/hit/hit.h"
+#include "icing/index/lite/term-id-hit-pair.h"
+#include "icing/index/main/posting-list-hit-accessor.h"
 #include "icing/index/main/posting-list-hit-serializer.h"
 #include "icing/index/term-id-codec.h"
+#include "icing/index/term-metadata.h"
 #include "icing/index/term-property-id.h"
 #include "icing/legacy/core/icing-string-util.h"
 #include "icing/legacy/index/icing-dynamic-trie.h"
+#include "icing/legacy/index/icing-filesystem.h"
 #include "icing/proto/debug.pb.h"
 #include "icing/proto/storage.pb.h"
 #include "icing/proto/term.pb.h"
+#include "icing/store/document-id.h"
+#include "icing/store/namespace-id.h"
+#include "icing/store/suggestion-result-checker.h"
 #include "icing/util/logging.h"
 #include "icing/util/status-macros.h"
 
@@ -66,7 +83,7 @@ FindTermResult FindShortestValidTermWithPrefixHits(
   uint32_t tvi = 0;
   bool found = false;
   bool exact = false;
-  for (IcingDynamicTrie::Iterator it(*lexicon, prefix.c_str()); it.IsValid();
+  for (IcingDynamicTrie::Iterator it(*lexicon, prefix); it.IsValid();
        it.Advance()) {
     PostingListIdentifier posting_list_id = PostingListIdentifier::kInvalid;
     memcpy(&posting_list_id, it.GetValue(), sizeof(posting_list_id));
@@ -80,7 +97,7 @@ FindTermResult FindShortestValidTermWithPrefixHits(
     // "fo" will have an invalid posting_list_id because it hasn't been
     // backfilled yet, so we need to continue iterating to "foo".
     if (posting_list_id.is_valid()) {
-      exact = (prefix.size() == strlen(it.GetKey()));
+      exact = (prefix.size() == it.GetKey().size());
       tvi = it.GetValueIndex();
       // Found it. Does it have prefix hits?
       found = exact || hits_in_prefix_section.HasProperty(tvi);
@@ -174,7 +191,7 @@ IndexStorageInfoProto MainIndex::GetStorageInfo(
 libtextclassifier3::StatusOr<std::unique_ptr<PostingListHitAccessor>>
 MainIndex::GetAccessorForExactTerm(const std::string& term) {
   PostingListIdentifier posting_list_id = PostingListIdentifier::kInvalid;
-  if (!main_lexicon_->Find(term.c_str(), &posting_list_id)) {
+  if (!main_lexicon_->Find(term, &posting_list_id)) {
     return absl_ports::NotFoundError(IcingStringUtil::StringPrintf(
         "Term %s is not present in main lexicon.", term.c_str()));
   }
@@ -195,12 +212,12 @@ MainIndex::GetAccessorForPrefixTerm(const std::string& prefix) {
   // won't have a posting list, but "ba" will suffice.
   IcingDynamicTrie::PropertyReader hits_in_prefix_section(
       *main_lexicon_, GetHasHitsInPrefixSectionPropertyId());
-  IcingDynamicTrie::Iterator main_itr(*main_lexicon_, prefix.c_str());
+  IcingDynamicTrie::Iterator main_itr(*main_lexicon_, prefix);
   if (!main_itr.IsValid()) {
     return absl_ports::NotFoundError(IcingStringUtil::StringPrintf(
         "Term: %s is not present in the main lexicon.", prefix.c_str()));
   }
-  exact = (prefix.length() == strlen(main_itr.GetKey()));
+  exact = (prefix.size() == main_itr.GetKey().size());
 
   if (!exact && !hits_in_prefix_section.HasProperty(main_itr.GetValueIndex())) {
     // Found it, but it doesn't have prefix hits. Exit early. No need to
@@ -240,7 +257,7 @@ MainIndex::FindTermsByPrefix(
     SuggestionScoringSpecProto::SuggestionRankingStrategy::Code score_by,
     const SuggestionResultChecker* suggestion_result_checker) {
   // Finds all the terms that start with the given prefix in the lexicon.
-  IcingDynamicTrie::Iterator term_iterator(*main_lexicon_, prefix.c_str());
+  IcingDynamicTrie::Iterator term_iterator(*main_lexicon_, prefix);
 
   std::vector<TermMetadata> term_metadata_list;
   while (term_iterator.IsValid()) {
@@ -306,7 +323,8 @@ MainIndex::FindTermsByPrefix(
       ICING_ASSIGN_OR_RETURN(hits, pl_accessor->GetNextHitsBatch());
     }
     if (score > 0) {
-      term_metadata_list.push_back(TermMetadata(term_iterator.GetKey(), score));
+      term_metadata_list.push_back(
+          TermMetadata(std::string(term_iterator.GetKey()), score));
     }
 
     term_iterator.Advance();
@@ -330,7 +348,7 @@ MainIndex::AddBackfillBranchPoints(const IcingDynamicTrie& other_lexicon) {
     if (prefix_len <= 0) {
       continue;
     }
-    prefix.assign(other_term_itr.GetKey(), prefix_len);
+    prefix.assign(other_term_itr.GetKey().substr(0, prefix_len));
 
     // Figure out backfill tvi. Might not exist since all children terms could
     // only contain hits from non-prefix sections.
@@ -351,9 +369,9 @@ MainIndex::AddBackfillBranchPoints(const IcingDynamicTrie& other_lexicon) {
     uint32_t branching_prefix_tvi;
     bool new_key;
     PostingListIdentifier posting_list_id = PostingListIdentifier::kInvalid;
-    libtextclassifier3::Status status = main_lexicon_->Insert(
-        prefix.c_str(), &posting_list_id, &branching_prefix_tvi,
-        /*replace=*/false, &new_key);
+    libtextclassifier3::Status status =
+        main_lexicon_->Insert(prefix, &posting_list_id, &branching_prefix_tvi,
+                              /*replace=*/false, &new_key);
     if (!status.ok()) {
       ICING_LOG(DBG) << "Could not insert branching prefix\n"
                      << status.error_message();
@@ -441,12 +459,12 @@ MainIndex::AddBranchPoints(const IcingDynamicTrie& other_lexicon,
         continue;
       }
 
-      prefix.assign(other_term_itr.GetKey(), prefix_length);
+      prefix.assign(other_term_itr.GetKey().substr(0, prefix_length));
       uint32_t prefix_tvi;
       bool new_key;
       PostingListIdentifier posting_list_id = PostingListIdentifier::kInvalid;
       libtextclassifier3::Status status =
-          main_lexicon_->Insert(prefix.c_str(), &posting_list_id, &prefix_tvi,
+          main_lexicon_->Insert(prefix, &posting_list_id, &prefix_tvi,
                                 /*replace=*/false, &new_key);
       if (!status.ok()) {
         ICING_LOG(DBG) << "Could not insert prefix: " << prefix << "\n"
@@ -651,6 +669,7 @@ libtextclassifier3::Status MainIndex::AddPrefixBackfillHits(
     ICING_ASSIGN_OR_RETURN(tmp, backfill_accessor->GetNextHitsBatch());
   }
 
+  Hit::EqualsValueAndFlags hit_equals_value_and_flags_comparator;
   Hit last_added_hit(Hit::kInvalidValue);
   // The hits in backfill_hits are in the reverse order of how they were added.
   // Iterate in reverse to add them to this new posting list in the correct
@@ -666,8 +685,8 @@ libtextclassifier3::Status MainIndex::AddPrefixBackfillHits(
     const Hit backfill_hit(hit.section_id(), hit.document_id(),
                            hit.term_frequency(),
                            /*is_in_prefix_section=*/true,
-                           /*is_prefix_hit=*/true);
-    if (backfill_hit == last_added_hit) {
+                           /*is_prefix_hit=*/true, /*is_stemmed_hit=*/false);
+    if (hit_equals_value_and_flags_comparator(backfill_hit, last_added_hit)) {
       // Skip duplicate values due to overriding of the is_prefix flag.
       continue;
     }
@@ -740,8 +759,9 @@ libtextclassifier3::Status MainIndex::Optimize(
 }
 
 libtextclassifier3::StatusOr<DocumentId> MainIndex::TransferAndAddHits(
-    const std::vector<DocumentId>& document_id_old_to_new, const char* term,
-    PostingListHitAccessor& old_pl_accessor, MainIndex* new_index) {
+    const std::vector<DocumentId>& document_id_old_to_new,
+    std::string_view term, PostingListHitAccessor& old_pl_accessor,
+    MainIndex* new_index) {
   std::vector<Hit> new_hits;
   bool has_no_exact_hits = true;
   bool has_hits_in_prefix_section = false;
diff --git a/icing/index/main/main-index.h b/icing/index/main/main-index.h
index 9e570d5..20eeb8e 100644
--- a/icing/index/main/main-index.h
+++ b/icing/index/main/main-index.h
@@ -15,12 +15,20 @@
 #ifndef ICING_INDEX_MAIN_MAIN_INDEX_H_
 #define ICING_INDEX_MAIN_MAIN_INDEX_H_
 
+#include <cstddef>
+#include <cstdint>
 #include <memory>
+#include <string>
+#include <unordered_map>
+#include <utility>
+#include <vector>
 
 #include "icing/text_classifier/lib3/utils/base/status.h"
 #include "icing/text_classifier/lib3/utils/base/statusor.h"
+#include "icing/absl_ports/canonical_errors.h"
 #include "icing/file/filesystem.h"
 #include "icing/file/posting_list/flash-index-storage.h"
+#include "icing/file/posting_list/posting-list-identifier.h"
 #include "icing/index/lite/term-id-hit-pair.h"
 #include "icing/index/main/posting-list-hit-accessor.h"
 #include "icing/index/main/posting-list-hit-serializer.h"
@@ -32,8 +40,9 @@
 #include "icing/proto/scoring.pb.h"
 #include "icing/proto/storage.pb.h"
 #include "icing/proto/term.pb.h"
-#include "icing/store/namespace-id.h"
+#include "icing/store/document-id.h"
 #include "icing/store/suggestion-result-checker.h"
+#include "icing/util/crc32.h"
 #include "icing/util/status-macros.h"
 
 namespace icing {
@@ -91,7 +100,7 @@ class MainIndex {
   // input prefix must be normalized, otherwise inaccurate results may be
   // returned. If scoring_match_type is EXACT, only exact hit will be counted
   // and it is PREFIX, both prefix and exact hits will be counted. Results are
-  // not sorted specifically and are in lexigraphical order. Number of results
+  // not sorted specifically and are in lexicographical order. Number of results
   // are no more than 'num_to_return'.
   //
   // Returns:
@@ -174,6 +183,12 @@ class MainIndex {
     return absl_ports::InternalError("Unable to sync main index components.");
   }
 
+  // Updates and returns the checksums of the components in the MainIndex.
+  Crc32 UpdateChecksum() { return main_lexicon_->UpdateCrc(); }
+
+  // Calculates and returns the checksums of the components in the MainIndex.
+  Crc32 GetChecksum() const { return main_lexicon_->GetCrc(); }
+
   DocumentId last_added_document_id() const {
     return flash_index_storage_->get_last_indexed_docid();
   }
@@ -324,8 +339,9 @@ class MainIndex {
   //   largest document id added to the translated posting list, on success
   //   INTERNAL_ERROR on IO error
   static libtextclassifier3::StatusOr<DocumentId> TransferAndAddHits(
-      const std::vector<DocumentId>& document_id_old_to_new, const char* term,
-      PostingListHitAccessor& old_pl_accessor, MainIndex* new_index);
+      const std::vector<DocumentId>& document_id_old_to_new,
+      std::string_view term, PostingListHitAccessor& old_pl_accessor,
+      MainIndex* new_index);
 
   // Transfer hits from the current main index to new_index.
   //
diff --git a/icing/index/main/main-index_test.cc b/icing/index/main/main-index_test.cc
index db9dbe2..b68422b 100644
--- a/icing/index/main/main-index_test.cc
+++ b/icing/index/main/main-index_test.cc
@@ -162,7 +162,8 @@ TEST_F(MainIndexTest, MainIndexGetAccessorForPrefixReturnsValidAccessor) {
                              term_id_codec_->EncodeTvi(tvi, TviType::LITE));
 
   Hit doc0_hit(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc0_hit));
 
   // 2. Create the main index. It should have no entries in its lexicon.
@@ -188,7 +189,8 @@ TEST_F(MainIndexTest, MainIndexGetAccessorForPrefixReturnsNotFound) {
                              term_id_codec_->EncodeTvi(tvi, TviType::LITE));
 
   Hit doc0_hit(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc0_hit));
 
   // 2. Create the main index. It should have no entries in its lexicon.
@@ -227,7 +229,8 @@ TEST_F(MainIndexTest, MainIndexGetAccessorForExactReturnsValidAccessor) {
                              term_id_codec_->EncodeTvi(tvi, TviType::LITE));
 
   Hit doc0_hit(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc0_hit));
 
   // 2. Create the main index. It should have no entries in its lexicon.
@@ -264,18 +267,21 @@ TEST_F(MainIndexTest, MergeIndexToEmpty) {
                              term_id_codec_->EncodeTvi(tvi, TviType::LITE));
 
   Hit doc0_hit(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc0_hit));
   ICING_ASSERT_OK(lite_index_->AddHit(fool_term_id, doc0_hit));
   ICING_ASSERT_OK(lite_index_->AddHit(far_term_id, doc0_hit));
 
   Hit doc1_hit(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc1_hit));
   ICING_ASSERT_OK(lite_index_->AddHit(fool_term_id, doc1_hit));
 
   Hit doc2_hit(/*section_id=*/0, /*document_id=*/2, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(fool_term_id, doc2_hit));
   ICING_ASSERT_OK(lite_index_->AddHit(far_term_id, doc2_hit));
 
@@ -342,18 +348,21 @@ TEST_F(MainIndexTest, MergeIndexToPreexisting) {
                              term_id_codec_->EncodeTvi(tvi, TviType::LITE));
 
   Hit doc0_hit(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc0_hit));
   ICING_ASSERT_OK(lite_index_->AddHit(fool_term_id, doc0_hit));
   ICING_ASSERT_OK(lite_index_->AddHit(far_term_id, doc0_hit));
 
   Hit doc1_hit(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc1_hit));
   ICING_ASSERT_OK(lite_index_->AddHit(fool_term_id, doc1_hit));
 
   Hit doc2_hit(/*section_id=*/0, /*document_id=*/2, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(fool_term_id, doc2_hit));
   ICING_ASSERT_OK(lite_index_->AddHit(far_term_id, doc2_hit));
 
@@ -397,14 +406,16 @@ TEST_F(MainIndexTest, MergeIndexToPreexisting) {
                              term_id_codec_->EncodeTvi(tvi, TviType::LITE));
 
   Hit doc3_hit(/*section_id=*/0, /*document_id=*/3, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc3_hit));
   ICING_ASSERT_OK(lite_index_->AddHit(four_term_id, doc3_hit));
   ICING_ASSERT_OK(lite_index_->AddHit(foul_term_id, doc3_hit));
   ICING_ASSERT_OK(lite_index_->AddHit(fall_term_id, doc3_hit));
 
   Hit doc4_hit(/*section_id=*/0, /*document_id=*/4, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(four_term_id, doc4_hit));
   ICING_ASSERT_OK(lite_index_->AddHit(foul_term_id, doc4_hit));
 
@@ -459,15 +470,18 @@ TEST_F(MainIndexTest, ExactRetrievedInPrefixSearch) {
                              term_id_codec_->EncodeTvi(tvi, TviType::LITE));
 
   Hit doc0_hit(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc0_hit));
 
   Hit doc1_hit(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foo_term_id, doc1_hit));
 
   Hit doc2_hit(/*section_id=*/0, /*document_id=*/2, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc2_hit));
 
   // 2. Create the main index. It should have no entries in its lexicon.
@@ -510,15 +524,18 @@ TEST_F(MainIndexTest, PrefixNotRetrievedInExactSearch) {
                              term_id_codec_->EncodeTvi(tvi, TviType::LITE));
 
   Hit doc0_hit(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc0_hit));
 
   Hit doc1_hit(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foo_term_id, doc1_hit));
 
   Hit doc2_hit(/*section_id=*/0, /*document_id=*/2, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foo_term_id, doc2_hit));
 
   // 2. Create the main index. It should have no entries in its lexicon.
@@ -564,17 +581,20 @@ TEST_F(MainIndexTest,
         document_id % Hit::kMaxTermFrequency + 1);
     Hit doc_hit0(
         /*section_id=*/0, /*document_id=*/document_id, term_frequency,
-        /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+        /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+        /*is_stemmed_hit=*/false);
     ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc_hit0));
 
     Hit doc_hit1(
         /*section_id=*/1, /*document_id=*/document_id, term_frequency,
-        /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+        /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+        /*is_stemmed_hit=*/false);
     ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc_hit1));
 
     Hit doc_hit2(
         /*section_id=*/2, /*document_id=*/document_id, term_frequency,
-        /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+        /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+        /*is_stemmed_hit=*/false);
     ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc_hit2));
   }
 
@@ -629,7 +649,8 @@ TEST_F(MainIndexTest, MergeIndexBackfilling) {
                              term_id_codec_->EncodeTvi(tvi, TviType::LITE));
 
   Hit doc0_hit(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(fool_term_id, doc0_hit));
 
   // 2. Create the main index. It should have no entries in its lexicon.
@@ -658,7 +679,8 @@ TEST_F(MainIndexTest, MergeIndexBackfilling) {
                              term_id_codec_->EncodeTvi(tvi, TviType::LITE));
 
   Hit doc1_hit(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
-               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+               /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+               /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(lite_index_->AddHit(foot_term_id, doc1_hit));
 
   // 5. Merge the index. The main index should now contain "fool", "foot"
@@ -692,7 +714,8 @@ TEST_F(MainIndexTest, OneHitInTheFirstPageForTwoPagesMainIndex) {
   uint32_t num_docs = 2038;
   for (DocumentId document_id = 0; document_id < num_docs; ++document_id) {
     Hit doc_hit(section_id, document_id, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
     ICING_ASSERT_OK(lite_index_->AddHit(foo_term_id, doc_hit));
   }
 
diff --git a/icing/index/main/posting-list-hit-accessor_test.cc b/icing/index/main/posting-list-hit-accessor_test.cc
index c2460ff..81fc09a 100644
--- a/icing/index/main/posting-list-hit-accessor_test.cc
+++ b/icing/index/main/posting-list-hit-accessor_test.cc
@@ -40,9 +40,9 @@ namespace lib {
 namespace {
 
 using ::testing::ElementsAre;
-using ::testing::ElementsAreArray;
 using ::testing::Eq;
 using ::testing::Lt;
+using ::testing::Pointwise;
 using ::testing::SizeIs;
 
 class PostingListHitAccessorTest : public ::testing::Test {
@@ -96,8 +96,10 @@ TEST_F(PostingListHitAccessorTest, HitsAddAndRetrieveProperly) {
   // Retrieve some hits.
   ICING_ASSERT_OK_AND_ASSIGN(PostingListHolder pl_holder,
                              flash_index_storage_->GetPostingList(result.id));
-  EXPECT_THAT(serializer_->GetHits(&pl_holder.posting_list),
-              IsOkAndHolds(ElementsAreArray(hits1.rbegin(), hits1.rend())));
+  EXPECT_THAT(
+      serializer_->GetHits(&pl_holder.posting_list),
+      IsOkAndHolds(Pointwise(EqualsHit(),
+                             std::vector<Hit>(hits1.rbegin(), hits1.rend()))));
   EXPECT_THAT(pl_holder.next_block_index, Eq(kInvalidBlockIndex));
 }
 
@@ -108,7 +110,8 @@ TEST_F(PostingListHitAccessorTest, PreexistingPLKeepOnSameBlock) {
                                      serializer_.get()));
   // Add a single hit. This will fit in a min-sized posting list.
   Hit hit1(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(pl_accessor->PrependHit(hit1));
   PostingListAccessor::FinalizeResult result1 =
       std::move(*pl_accessor).Finalize();
@@ -137,7 +140,7 @@ TEST_F(PostingListHitAccessorTest, PreexistingPLKeepOnSameBlock) {
   ICING_ASSERT_OK_AND_ASSIGN(PostingListHolder pl_holder,
                              flash_index_storage_->GetPostingList(result2.id));
   EXPECT_THAT(serializer_->GetHits(&pl_holder.posting_list),
-              IsOkAndHolds(ElementsAre(hit2, hit1)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit2), EqualsHit(hit1))));
 }
 
 TEST_F(PostingListHitAccessorTest, PreexistingPLReallocateToLargerPL) {
@@ -146,8 +149,8 @@ TEST_F(PostingListHitAccessorTest, PreexistingPLReallocateToLargerPL) {
       PostingListHitAccessor::Create(flash_index_storage_.get(),
                                      serializer_.get()));
   // Use a small posting list of 30 bytes. The first 17 hits will be compressed
-  // to one byte each and will be able to fit in the 18 byte padded region. The
-  // last hit will fit in one of the special hits. The posting list will be
+  // to one byte each and will be able to fit in the 18-byte compressed region.
+  // The last hit will fit in one of the special hits. The posting list will be
   // ALMOST_FULL and can fit at most 2 more hits.
   std::vector<Hit> hits1 =
       CreateHits(/*num_hits=*/18, /*desired_byte_length=*/1);
@@ -207,8 +210,10 @@ TEST_F(PostingListHitAccessorTest, PreexistingPLReallocateToLargerPL) {
   hits1.push_back(single_hit);
   ICING_ASSERT_OK_AND_ASSIGN(PostingListHolder pl_holder,
                              flash_index_storage_->GetPostingList(result3.id));
-  EXPECT_THAT(serializer_->GetHits(&pl_holder.posting_list),
-              IsOkAndHolds(ElementsAreArray(hits1.rbegin(), hits1.rend())));
+  EXPECT_THAT(
+      serializer_->GetHits(&pl_holder.posting_list),
+      IsOkAndHolds(Pointwise(EqualsHit(),
+                             std::vector<Hit>(hits1.rbegin(), hits1.rend()))));
 }
 
 TEST_F(PostingListHitAccessorTest, MultiBlockChainsBlocksProperly) {
@@ -243,7 +248,8 @@ TEST_F(PostingListHitAccessorTest, MultiBlockChainsBlocksProperly) {
   ASSERT_THAT(second_block_hits, SizeIs(Lt(hits1.size())));
   auto first_block_hits_start = hits1.rbegin() + second_block_hits.size();
   EXPECT_THAT(second_block_hits,
-              ElementsAreArray(hits1.rbegin(), first_block_hits_start));
+              Pointwise(EqualsHit(), std::vector<Hit>(hits1.rbegin(),
+                                                      first_block_hits_start)));
 
   // Now retrieve all of the hits that were on the first block.
   uint32_t first_block_id = pl_holder.next_block_index;
@@ -253,9 +259,10 @@ TEST_F(PostingListHitAccessorTest, MultiBlockChainsBlocksProperly) {
                               /*posting_list_index_bits=*/0);
   ICING_ASSERT_OK_AND_ASSIGN(pl_holder,
                              flash_index_storage_->GetPostingList(pl_id));
-  EXPECT_THAT(
-      serializer_->GetHits(&pl_holder.posting_list),
-      IsOkAndHolds(ElementsAreArray(first_block_hits_start, hits1.rend())));
+  EXPECT_THAT(serializer_->GetHits(&pl_holder.posting_list),
+              IsOkAndHolds(Pointwise(
+                  EqualsHit(),
+                  std::vector<Hit>(first_block_hits_start, hits1.rend()))));
 }
 
 TEST_F(PostingListHitAccessorTest, PreexistingMultiBlockReusesBlocksProperly) {
@@ -310,7 +317,8 @@ TEST_F(PostingListHitAccessorTest, PreexistingMultiBlockReusesBlocksProperly) {
   ASSERT_THAT(second_block_hits, SizeIs(Lt(hits1.size())));
   auto first_block_hits_start = hits1.rbegin() + second_block_hits.size();
   EXPECT_THAT(second_block_hits,
-              ElementsAreArray(hits1.rbegin(), first_block_hits_start));
+              Pointwise(EqualsHit(), std::vector<Hit>(hits1.rbegin(),
+                                                      first_block_hits_start)));
 
   // Now retrieve all of the hits that were on the first block.
   uint32_t first_block_id = pl_holder.next_block_index;
@@ -320,9 +328,10 @@ TEST_F(PostingListHitAccessorTest, PreexistingMultiBlockReusesBlocksProperly) {
                               /*posting_list_index_bits=*/0);
   ICING_ASSERT_OK_AND_ASSIGN(pl_holder,
                              flash_index_storage_->GetPostingList(pl_id));
-  EXPECT_THAT(
-      serializer_->GetHits(&pl_holder.posting_list),
-      IsOkAndHolds(ElementsAreArray(first_block_hits_start, hits1.rend())));
+  EXPECT_THAT(serializer_->GetHits(&pl_holder.posting_list),
+              IsOkAndHolds(Pointwise(
+                  EqualsHit(),
+                  std::vector<Hit>(first_block_hits_start, hits1.rend()))));
 }
 
 TEST_F(PostingListHitAccessorTest, InvalidHitReturnsInvalidArgument) {
@@ -341,16 +350,19 @@ TEST_F(PostingListHitAccessorTest, HitsNotDecreasingReturnsInvalidArgument) {
       PostingListHitAccessor::Create(flash_index_storage_.get(),
                                      serializer_.get()));
   Hit hit1(/*section_id=*/3, /*document_id=*/1, Hit::kDefaultTermFrequency,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(pl_accessor->PrependHit(hit1));
 
   Hit hit2(/*section_id=*/6, /*document_id=*/1, Hit::kDefaultTermFrequency,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   EXPECT_THAT(pl_accessor->PrependHit(hit2),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 
   Hit hit3(/*section_id=*/2, /*document_id=*/0, Hit::kDefaultTermFrequency,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/true);
   EXPECT_THAT(pl_accessor->PrependHit(hit3),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
@@ -372,7 +384,8 @@ TEST_F(PostingListHitAccessorTest, PreexistingPostingListNoHitsAdded) {
       PostingListHitAccessor::Create(flash_index_storage_.get(),
                                      serializer_.get()));
   Hit hit1(/*section_id=*/3, /*document_id=*/1, Hit::kDefaultTermFrequency,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(pl_accessor->PrependHit(hit1));
   PostingListAccessor::FinalizeResult result1 =
       std::move(*pl_accessor).Finalize();
diff --git a/icing/index/main/posting-list-hit-serializer.cc b/icing/index/main/posting-list-hit-serializer.cc
index 88c0754..236ea54 100644
--- a/icing/index/main/posting-list-hit-serializer.cc
+++ b/icing/index/main/posting-list-hit-serializer.cc
@@ -198,7 +198,7 @@ libtextclassifier3::Status PostingListHitSerializer::PrependHitToAlmostFull(
   // in the padded area and put new hit at the special position 1.
   // Calling ValueOrDie is safe here because 1 < kNumSpecialData.
   Hit cur = GetSpecialHit(posting_list_used, /*index=*/1).ValueOrDie();
-  if (cur < hit || cur == hit) {
+  if (!(hit < cur)) {
     return absl_ports::InvalidArgumentError(
         "Hit being prepended must be strictly less than the most recent Hit");
   }
@@ -293,7 +293,7 @@ libtextclassifier3::Status PostingListHitSerializer::PrependHitToNotFull(
   // Term-frequency is not used for hit comparison so it's ok to pass in the
   // default term-frequency here.
   Hit cur(cur_value, cur_flags, Hit::kDefaultTermFrequency);
-  if (cur < hit || cur == hit) {
+  if (!(hit < cur)) {
     return absl_ports::InvalidArgumentError(IcingStringUtil::StringPrintf(
         "Hit (value=%d, flags=%d) being prepended must be strictly less than "
         "the most recent Hit (value=%d, flags=%d)",
diff --git a/icing/index/main/posting-list-hit-serializer.h b/icing/index/main/posting-list-hit-serializer.h
index 08c792c..793f1e6 100644
--- a/icing/index/main/posting-list-hit-serializer.h
+++ b/icing/index/main/posting-list-hit-serializer.h
@@ -114,17 +114,6 @@ class PostingListHitSerializer : public PostingListSerializer {
   libtextclassifier3::Status PrependHit(PostingListUsed* posting_list_used,
                                         const Hit& hit) const;
 
-  // Prepend hits to the posting list. Hits should be sorted in descending order
-  // (as defined by the less than operator for Hit)
-  //
-  // Returns the number of hits that could be prepended to the posting list. If
-  // keep_prepended is true, whatever could be prepended is kept, otherwise the
-  // posting list is left in its original state.
-  template <class T, Hit (*GetHit)(const T&)>
-  libtextclassifier3::StatusOr<uint32_t> PrependHitArray(
-      PostingListUsed* posting_list_used, const T* array, uint32_t num_hits,
-      bool keep_prepended) const;
-
   // Retrieves the hits stored in the posting list.
   //
   // RETURNS:
@@ -364,33 +353,6 @@ class PostingListHitSerializer : public PostingListSerializer {
       uint32_t* offset) const;
 };
 
-// Inlined functions. Implementation details below. Avert eyes!
-template <class T, Hit (*GetHit)(const T&)>
-libtextclassifier3::StatusOr<uint32_t>
-PostingListHitSerializer::PrependHitArray(PostingListUsed* posting_list_used,
-                                          const T* array, uint32_t num_hits,
-                                          bool keep_prepended) const {
-  if (!IsPostingListValid(posting_list_used)) {
-    return 0;
-  }
-
-  // Prepend hits working backwards from array[num_hits - 1].
-  uint32_t i;
-  for (i = 0; i < num_hits; ++i) {
-    if (!PrependHit(posting_list_used, GetHit(array[num_hits - i - 1])).ok()) {
-      break;
-    }
-  }
-  if (i != num_hits && !keep_prepended) {
-    // Didn't fit. Undo everything and check that we have the same offset as
-    // before. PopFrontHits guarantees that it will remove all 'i' hits so long
-    // as there are at least 'i' hits in the posting list, which we know there
-    // are.
-    ICING_RETURN_IF_ERROR(PopFrontHits(posting_list_used, /*num_hits=*/i));
-  }
-  return i;
-}
-
 }  // namespace lib
 }  // namespace icing
 
diff --git a/icing/index/main/posting-list-hit-serializer_test.cc b/icing/index/main/posting-list-hit-serializer_test.cc
index ea135ef..7072ef6 100644
--- a/icing/index/main/posting-list-hit-serializer_test.cc
+++ b/icing/index/main/posting-list-hit-serializer_test.cc
@@ -14,11 +14,9 @@
 
 #include "icing/index/main/posting-list-hit-serializer.h"
 
-#include <algorithm>
 #include <cstddef>
 #include <cstdint>
-#include <deque>
-#include <iterator>
+#include <cstring>
 #include <limits>
 #include <vector>
 
@@ -32,31 +30,21 @@
 #include "icing/store/document-id.h"
 #include "icing/testing/common-matchers.h"
 #include "icing/testing/hit-test-utils.h"
+#include "icing/util/math-util.h"
 
 using testing::ElementsAre;
-using testing::ElementsAreArray;
 using testing::Eq;
 using testing::Gt;
 using testing::IsEmpty;
 using testing::IsFalse;
 using testing::IsTrue;
 using testing::Le;
-using testing::Lt;
 
 namespace icing {
 namespace lib {
 
 namespace {
 
-struct HitElt {
-  HitElt() = default;
-  explicit HitElt(const Hit &hit_in) : hit(hit_in) {}
-
-  static Hit get_hit(const HitElt &hit_elt) { return hit_elt.hit; }
-
-  Hit hit;
-};
-
 TEST(PostingListHitSerializerTest, PostingListUsedPrependHitNotFull) {
   PostingListHitSerializer serializer;
 
@@ -69,7 +57,8 @@ TEST(PostingListHitSerializerTest, PostingListUsedPrependHitNotFull) {
 
   // Make used.
   Hit hit0(/*section_id=*/0, /*document_id=*/0, /*term_frequency=*/56,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit0));
   // Size = sizeof(uncompressed hit0::Value)
   //        + sizeof(hit0::Flags)
@@ -77,10 +66,12 @@ TEST(PostingListHitSerializerTest, PostingListUsedPrependHitNotFull) {
   int expected_size =
       sizeof(Hit::Value) + sizeof(Hit::Flags) + sizeof(Hit::TermFrequency);
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(expected_size));
-  EXPECT_THAT(serializer.GetHits(&pl_used), IsOkAndHolds(ElementsAre(hit0)));
+  EXPECT_THAT(serializer.GetHits(&pl_used),
+              IsOkAndHolds(ElementsAre(EqualsHit(hit0))));
 
   Hit hit1(/*section_id=*/0, /*document_id=*/1, Hit::kDefaultTermFrequency,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   uint8_t delta_buf[VarInt::kMaxEncodedLen64];
   size_t delta_len = PostingListHitSerializer::EncodeNextHitValue(
       /*next_hit_value=*/hit1.value(),
@@ -93,10 +84,11 @@ TEST(PostingListHitSerializerTest, PostingListUsedPrependHitNotFull) {
   expected_size += delta_len;
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(expected_size));
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAre(hit1, hit0)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit1), EqualsHit(hit0))));
 
   Hit hit2(/*section_id=*/0, /*document_id=*/2, /*term_frequency=*/56,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   delta_len = PostingListHitSerializer::EncodeNextHitValue(
       /*next_hit_value=*/hit2.value(),
       /*curr_hit_value=*/hit1.value(), delta_buf);
@@ -110,10 +102,12 @@ TEST(PostingListHitSerializerTest, PostingListUsedPrependHitNotFull) {
   expected_size += delta_len + sizeof(Hit::Flags) + sizeof(Hit::TermFrequency);
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(expected_size));
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAre(hit2, hit1, hit0)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit2), EqualsHit(hit1),
+                                       EqualsHit(hit0))));
 
   Hit hit3(/*section_id=*/0, /*document_id=*/3, Hit::kDefaultTermFrequency,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   delta_len = PostingListHitSerializer::EncodeNextHitValue(
       /*next_hit_value=*/hit3.value(),
       /*curr_hit_value=*/hit2.value(), delta_buf);
@@ -128,7 +122,8 @@ TEST(PostingListHitSerializerTest, PostingListUsedPrependHitNotFull) {
   expected_size += delta_len;
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(expected_size));
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAre(hit3, hit2, hit1, hit0)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit3), EqualsHit(hit2),
+                                       EqualsHit(hit1), EqualsHit(hit0))));
 }
 
 TEST(PostingListHitSerializerTest,
@@ -147,11 +142,12 @@ TEST(PostingListHitSerializerTest,
   // Adding hit1: NOT_FULL -> NOT_FULL
   // Adding hit2: NOT_FULL -> NOT_FULL
   Hit hit0(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   Hit hit1 = CreateHit(hit0, /*desired_byte_length=*/3);
   Hit hit2 = CreateHit(hit1, /*desired_byte_length=*/2, /*term_frequency=*/57,
                        /*is_in_prefix_section=*/true,
-                       /*is_prefix_hit=*/true);
+                       /*is_prefix_hit=*/true, /*is_stemmed_hit=*/false);
   EXPECT_THAT(hit2.has_flags(), IsTrue());
   EXPECT_THAT(hit2.has_term_frequency(), IsTrue());
   ICING_EXPECT_OK(serializer.PrependHit(&pl_used, hit0));
@@ -163,12 +159,14 @@ TEST(PostingListHitSerializerTest,
                       sizeof(Hit::TermFrequency) + 2 + 3;
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(expected_size));
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAre(hit2, hit1, hit0)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit2), EqualsHit(hit1),
+                                       EqualsHit(hit0))));
 
   // Add one more hit to transition NOT_FULL -> ALMOST_FULL
   Hit hit3 =
       CreateHit(hit2, /*desired_byte_length=*/3, Hit::kDefaultTermFrequency,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                /*is_stemmed_hit=*/false);
   EXPECT_THAT(hit3.has_flags(), IsFalse());
   ICING_EXPECT_OK(serializer.PrependHit(&pl_used, hit3));
   // Storing them in the compressed region requires 4 (hit3::Value) + 3
@@ -183,7 +181,8 @@ TEST(PostingListHitSerializerTest,
   expected_size = pl_size - sizeof(Hit);
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(expected_size));
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAre(hit3, hit2, hit1, hit0)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit3), EqualsHit(hit2),
+                                       EqualsHit(hit1), EqualsHit(hit0))));
 
   // Add one more hit to transition ALMOST_FULL -> ALMOST_FULL
   Hit hit4 = CreateHit(hit3, /*desired_byte_length=*/2);
@@ -195,7 +194,9 @@ TEST(PostingListHitSerializerTest,
   // hits and the posting list will remain in ALMOST_FULL.
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(expected_size));
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAre(hit4, hit3, hit2, hit1, hit0)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit4), EqualsHit(hit3),
+                                       EqualsHit(hit2), EqualsHit(hit1),
+                                       EqualsHit(hit0))));
 
   // Add one more hit to transition ALMOST_FULL -> FULL
   Hit hit5 = CreateHit(hit4, /*desired_byte_length=*/2);
@@ -206,7 +207,9 @@ TEST(PostingListHitSerializerTest,
   // making the posting list FULL.
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(pl_size));
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAre(hit5, hit4, hit3, hit2, hit1, hit0)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit5), EqualsHit(hit4),
+                                       EqualsHit(hit3), EqualsHit(hit2),
+                                       EqualsHit(hit1), EqualsHit(hit0))));
 
   // The posting list is FULL. Adding another hit should fail.
   Hit hit6 = CreateHit(hit5, /*desired_byte_length=*/1);
@@ -229,7 +232,8 @@ TEST(PostingListHitSerializerTest, PostingListUsedPrependHitAlmostFull) {
   // Adding hit1: NOT_FULL -> NOT_FULL
   // Adding hit2: NOT_FULL -> NOT_FULL
   Hit hit0(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   Hit hit1 = CreateHit(hit0, /*desired_byte_length=*/3);
   Hit hit2 = CreateHit(hit1, /*desired_byte_length=*/3);
   ICING_EXPECT_OK(serializer.PrependHit(&pl_used, hit0));
@@ -240,7 +244,8 @@ TEST(PostingListHitSerializerTest, PostingListUsedPrependHitAlmostFull) {
   int expected_size = sizeof(Hit::Value) + 3 + 3;
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(expected_size));
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAre(hit2, hit1, hit0)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit2), EqualsHit(hit1),
+                                       EqualsHit(hit0))));
 
   // Add one more hit to transition NOT_FULL -> ALMOST_FULL
   Hit hit3 = CreateHit(hit2, /*desired_byte_length=*/3);
@@ -257,7 +262,8 @@ TEST(PostingListHitSerializerTest, PostingListUsedPrependHitAlmostFull) {
   expected_size = pl_size - sizeof(Hit);
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(expected_size));
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAre(hit3, hit2, hit1, hit0)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit3), EqualsHit(hit2),
+                                       EqualsHit(hit1), EqualsHit(hit0))));
 
   // Add one more hit to transition ALMOST_FULL -> ALMOST_FULL
   Hit hit4 = CreateHit(hit3, /*desired_byte_length=*/2);
@@ -269,7 +275,9 @@ TEST(PostingListHitSerializerTest, PostingListUsedPrependHitAlmostFull) {
   // hits and the posting list will remain in ALMOST_FULL.
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(expected_size));
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAre(hit4, hit3, hit2, hit1, hit0)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit4), EqualsHit(hit3),
+                                       EqualsHit(hit2), EqualsHit(hit1),
+                                       EqualsHit(hit0))));
 
   // Add one more hit to transition ALMOST_FULL -> FULL
   Hit hit5 = CreateHit(hit4, /*desired_byte_length=*/2);
@@ -280,7 +288,9 @@ TEST(PostingListHitSerializerTest, PostingListUsedPrependHitAlmostFull) {
   // making the posting list FULL.
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(pl_size));
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAre(hit5, hit4, hit3, hit2, hit1, hit0)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit5), EqualsHit(hit4),
+                                       EqualsHit(hit3), EqualsHit(hit2),
+                                       EqualsHit(hit1), EqualsHit(hit0))));
 
   // The posting list is FULL. Adding another hit should fail.
   Hit hit6 = CreateHit(hit5, /*desired_byte_length=*/1);
@@ -299,11 +309,12 @@ TEST(PostingListHitSerializerTest, PrependHitsWithSameValue) {
 
   // Fill up the compressed region.
   Hit hit0(/*section_id=*/0, /*document_id=*/0, Hit::kDefaultTermFrequency,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   Hit hit1 = CreateHit(hit0, /*desired_byte_length=*/3);
   Hit hit2 = CreateHit(hit1, /*desired_byte_length=*/2, /*term_frequency=*/57,
                        /*is_in_prefix_section=*/true,
-                       /*is_prefix_hit=*/true);
+                       /*is_prefix_hit=*/true, /*is_stemmed_hit=*/false);
   // Create hit3 with the same value but different flags as hit2 (hit3_flags
   // is set to have all currently-defined flags enabled)
   Hit::Flags hit3_flags = 0;
@@ -334,7 +345,8 @@ TEST(PostingListHitSerializerTest, PrependHitsWithSameValue) {
 
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(bytes_used));
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAre(hit2, hit3, hit1, hit0)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit2), EqualsHit(hit3),
+                                       EqualsHit(hit1), EqualsHit(hit0))));
 }
 
 TEST(PostingListHitSerializerTest, PostingListUsedMinSize) {
@@ -352,319 +364,35 @@ TEST(PostingListHitSerializerTest, PostingListUsedMinSize) {
   // Add a hit, PL should shift to ALMOST_FULL state
   Hit hit0(/*section_id=*/1, /*document_id=*/0, /*term_frequency=*/0,
            /*is_in_prefix_section=*/false,
-           /*is_prefix_hit=*/true);
+           /*is_prefix_hit=*/true, /*is_stemmed_hit=*/false);
   ICING_EXPECT_OK(serializer.PrependHit(&pl_used, hit0));
   // Size = sizeof(uncompressed hit0)
   int expected_size = sizeof(Hit);
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Le(expected_size));
-  EXPECT_THAT(serializer.GetHits(&pl_used), IsOkAndHolds(ElementsAre(hit0)));
+  EXPECT_THAT(serializer.GetHits(&pl_used),
+              IsOkAndHolds(ElementsAre(EqualsHit(hit0))));
 
   // Add the smallest hit possible - no term_frequency, non-prefix hit and a
   // delta of 0b10. PL should shift to FULL state.
   Hit hit1(/*section_id=*/0, /*document_id=*/0, /*term_frequency=*/0,
            /*is_in_prefix_section=*/false,
-           /*is_prefix_hit=*/false);
+           /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
   ICING_EXPECT_OK(serializer.PrependHit(&pl_used, hit1));
   // Size = sizeof(uncompressed hit1) + sizeof(uncompressed hit0)
   expected_size += sizeof(Hit);
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Le(expected_size));
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAre(hit1, hit0)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit1), EqualsHit(hit0))));
 
   // Try to add the smallest hit possible. Should fail
   Hit hit2(/*section_id=*/0, /*document_id=*/0, /*term_frequency=*/0,
            /*is_in_prefix_section=*/false,
-           /*is_prefix_hit=*/false);
+           /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false);
   EXPECT_THAT(serializer.PrependHit(&pl_used, hit2),
               StatusIs(libtextclassifier3::StatusCode::RESOURCE_EXHAUSTED));
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Le(expected_size));
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAre(hit1, hit0)));
-}
-
-TEST(PostingListHitSerializerTest,
-     PostingListPrependHitArrayMinSizePostingList) {
-  PostingListHitSerializer serializer;
-
-  // Min Size = 12
-  int pl_size = serializer.GetMinPostingListSize();
-  ICING_ASSERT_OK_AND_ASSIGN(
-      PostingListUsed pl_used,
-      PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
-
-  std::vector<HitElt> hits_in;
-  hits_in.emplace_back(Hit(/*section_id=*/1, /*document_id=*/0,
-                           Hit::kDefaultTermFrequency,
-                           /*is_in_prefix_section=*/false,
-                           /*is_prefix_hit=*/false));
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/1));
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/1));
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/1));
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/1));
-  std::reverse(hits_in.begin(), hits_in.end());
-
-  // Add five hits. The PL is in the empty state and an empty min size PL can
-  // only fit two hits. So PrependHitArray should fail.
-  ICING_ASSERT_OK_AND_ASSIGN(
-      uint32_t num_can_prepend,
-      (serializer.PrependHitArray<HitElt, HitElt::get_hit>(
-          &pl_used, &hits_in[0], hits_in.size(), /*keep_prepended=*/false)));
-  EXPECT_THAT(num_can_prepend, Eq(2));
-
-  int can_fit_hits = num_can_prepend;
-  // The PL has room for 2 hits. We should be able to add them without any
-  // problem, transitioning the PL from EMPTY -> ALMOST_FULL -> FULL
-  const HitElt *hits_in_ptr = hits_in.data() + (hits_in.size() - 2);
-  ICING_ASSERT_OK_AND_ASSIGN(
-      num_can_prepend,
-      (serializer.PrependHitArray<HitElt, HitElt::get_hit>(
-          &pl_used, hits_in_ptr, can_fit_hits, /*keep_prepended=*/false)));
-  EXPECT_THAT(num_can_prepend, Eq(can_fit_hits));
-  EXPECT_THAT(pl_size, Eq(serializer.GetBytesUsed(&pl_used)));
-  std::deque<Hit> hits_pushed;
-  std::transform(hits_in.rbegin(),
-                 hits_in.rend() - hits_in.size() + can_fit_hits,
-                 std::front_inserter(hits_pushed), HitElt::get_hit);
-  EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAreArray(hits_pushed)));
-}
-
-TEST(PostingListHitSerializerTest, PostingListPrependHitArrayPostingList) {
-  PostingListHitSerializer serializer;
-
-  // Size = 36
-  int pl_size = 3 * serializer.GetMinPostingListSize();
-  ICING_ASSERT_OK_AND_ASSIGN(
-      PostingListUsed pl_used,
-      PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
-
-  std::vector<HitElt> hits_in;
-  hits_in.emplace_back(Hit(/*section_id=*/1, /*document_id=*/0,
-                           Hit::kDefaultTermFrequency,
-                           /*is_in_prefix_section=*/false,
-                           /*is_prefix_hit=*/false));
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/1));
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/1));
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/1));
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/1));
-  std::reverse(hits_in.begin(), hits_in.end());
-  // The last hit is uncompressed and the four before it should only take one
-  // byte. Total use = 8 bytes.
-  // ----------------------
-  // 35     delta(Hit #0)
-  // 34     delta(Hit #1)
-  // 33     delta(Hit #2)
-  // 32     delta(Hit #3)
-  // 31-28  Hit #4
-  // 27-12  <unused>
-  // 11-6   kSpecialHit
-  // 5-0    Offset=28
-  // ----------------------
-  int byte_size = sizeof(Hit::Value) + hits_in.size() - 1;
-
-  // Add five hits. The PL is in the empty state and should be able to fit all
-  // five hits without issue, transitioning the PL from EMPTY -> NOT_FULL.
-  ICING_ASSERT_OK_AND_ASSIGN(
-      uint32_t num_could_fit,
-      (serializer.PrependHitArray<HitElt, HitElt::get_hit>(
-          &pl_used, &hits_in[0], hits_in.size(), /*keep_prepended=*/false)));
-  EXPECT_THAT(num_could_fit, Eq(hits_in.size()));
-  EXPECT_THAT(byte_size, Eq(serializer.GetBytesUsed(&pl_used)));
-  std::deque<Hit> hits_pushed;
-  std::transform(hits_in.rbegin(), hits_in.rend(),
-                 std::front_inserter(hits_pushed), HitElt::get_hit);
-  EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAreArray(hits_pushed)));
-
-  Hit first_hit = CreateHit(hits_in.begin()->hit, /*desired_byte_length=*/1);
-  hits_in.clear();
-  hits_in.emplace_back(first_hit);
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/2));
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/1));
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/2));
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/3));
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/2));
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/3));
-  std::reverse(hits_in.begin(), hits_in.end());
-  // Size increased by the deltas of these hits (1+2+1+2+3+2+3) = 14 bytes
-  // ----------------------
-  // 35     delta(Hit #0)
-  // 34     delta(Hit #1)
-  // 33     delta(Hit #2)
-  // 32     delta(Hit #3)
-  // 31     delta(Hit #4)
-  // 30-29  delta(Hit #5)
-  // 28     delta(Hit #6)
-  // 27-26  delta(Hit #7)
-  // 25-23  delta(Hit #8)
-  // 22-21  delta(Hit #9)
-  // 20-18  delta(Hit #10)
-  // 17-14  Hit #11
-  // 13-12  <unused>
-  // 11-6   kSpecialHit
-  // 5-0    Offset=14
-  // ----------------------
-  byte_size += 14;
-
-  // Add these 7 hits. The PL is currently in the NOT_FULL state and should
-  // remain in the NOT_FULL state.
-  ICING_ASSERT_OK_AND_ASSIGN(
-      num_could_fit,
-      (serializer.PrependHitArray<HitElt, HitElt::get_hit>(
-          &pl_used, &hits_in[0], hits_in.size(), /*keep_prepended=*/false)));
-  EXPECT_THAT(num_could_fit, Eq(hits_in.size()));
-  EXPECT_THAT(byte_size, Eq(serializer.GetBytesUsed(&pl_used)));
-  // All hits from hits_in were added.
-  std::transform(hits_in.rbegin(), hits_in.rend(),
-                 std::front_inserter(hits_pushed), HitElt::get_hit);
-  EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAreArray(hits_pushed)));
-
-  first_hit = CreateHit(hits_in.begin()->hit, /*desired_byte_length=*/3);
-  hits_in.clear();
-  hits_in.emplace_back(first_hit);
-  // ----------------------
-  // 35     delta(Hit #0)
-  // 34     delta(Hit #1)
-  // 33     delta(Hit #2)
-  // 32     delta(Hit #3)
-  // 31     delta(Hit #4)
-  // 30-29  delta(Hit #5)
-  // 28     delta(Hit #6)
-  // 27-26  delta(Hit #7)
-  // 25-23  delta(Hit #8)
-  // 22-21  delta(Hit #9)
-  // 20-18  delta(Hit #10)
-  // 17-15  delta(Hit #11)
-  // 14-12  <unused>
-  // 11-6   Hit #12
-  // 5-0    kSpecialHit
-  // ----------------------
-  byte_size = 30;  // 36 - 6
-
-  // Add this 1 hit. The PL is currently in the NOT_FULL state and should
-  // transition to the ALMOST_FULL state - even though there is still some
-  // unused space. This is because the unused space (3 bytes) is less than
-  // the size of a uncompressed Hit.
-  ICING_ASSERT_OK_AND_ASSIGN(
-      num_could_fit,
-      (serializer.PrependHitArray<HitElt, HitElt::get_hit>(
-          &pl_used, &hits_in[0], hits_in.size(), /*keep_prepended=*/false)));
-  EXPECT_THAT(num_could_fit, Eq(hits_in.size()));
-  EXPECT_THAT(byte_size, Eq(serializer.GetBytesUsed(&pl_used)));
-  // All hits from hits_in were added.
-  std::transform(hits_in.rbegin(), hits_in.rend(),
-                 std::front_inserter(hits_pushed), HitElt::get_hit);
-  EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAreArray(hits_pushed)));
-
-  first_hit = CreateHit(hits_in.begin()->hit, /*desired_byte_length=*/1);
-  hits_in.clear();
-  hits_in.emplace_back(first_hit);
-  hits_in.emplace_back(
-      CreateHit(hits_in.rbegin()->hit, /*desired_byte_length=*/3));
-  std::reverse(hits_in.begin(), hits_in.end());
-  // ----------------------
-  // 35     delta(Hit #0)
-  // 34     delta(Hit #1)
-  // 33     delta(Hit #2)
-  // 32     delta(Hit #3)
-  // 31     delta(Hit #4)
-  // 30-29  delta(Hit #5)
-  // 28     delta(Hit #6)
-  // 27-26  delta(Hit #7)
-  // 25-23  delta(Hit #8)
-  // 22-21  delta(Hit #9)
-  // 20-18  delta(Hit #10)
-  // 17-15  delta(Hit #11)
-  // 14     delta(Hit #12)
-  // 13-12  <unused>
-  // 11-6   Hit #13
-  // 5-0    Hit #14
-  // ----------------------
-
-  // Add these 2 hits.
-  // - The PL is currently in the ALMOST_FULL state. Adding the first hit should
-  //   keep the PL in ALMOST_FULL because the delta between
-  //   Hit #13 and Hit #14 (1 byte) can fit in the unused area (3 bytes).
-  // - Adding the second hit should transition to the FULL state because the
-  //   delta between Hit #14 and Hit #15 (3 bytes) is larger than the remaining
-  //   unused area (2 byte).
-  ICING_ASSERT_OK_AND_ASSIGN(
-      num_could_fit,
-      (serializer.PrependHitArray<HitElt, HitElt::get_hit>(
-          &pl_used, &hits_in[0], hits_in.size(), /*keep_prepended=*/false)));
-  EXPECT_THAT(num_could_fit, Eq(hits_in.size()));
-  EXPECT_THAT(pl_size, Eq(serializer.GetBytesUsed(&pl_used)));
-  // All hits from hits_in were added.
-  std::transform(hits_in.rbegin(), hits_in.rend(),
-                 std::front_inserter(hits_pushed), HitElt::get_hit);
-  EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAreArray(hits_pushed)));
-}
-
-TEST(PostingListHitSerializerTest, PostingListPrependHitArrayTooManyHits) {
-  PostingListHitSerializer serializer;
-
-  static constexpr int kNumHits = 128;
-  static constexpr int kDeltaSize = 1;
-  static constexpr size_t kHitsSize =
-      ((kNumHits - 2) * kDeltaSize + (2 * sizeof(Hit))) / sizeof(Hit) *
-      sizeof(Hit);
-
-  // Create an array with one too many hits
-  std::vector<Hit> hits_in_too_many =
-      CreateHits(kNumHits + 1, /*desired_byte_length=*/1);
-  std::vector<HitElt> hit_elts_in_too_many;
-  for (const Hit &hit : hits_in_too_many) {
-    hit_elts_in_too_many.emplace_back(hit);
-  }
-  // Reverse so that hits are inserted in descending order
-  std::reverse(hit_elts_in_too_many.begin(), hit_elts_in_too_many.end());
-
-  ICING_ASSERT_OK_AND_ASSIGN(
-      PostingListUsed pl_used,
-      PostingListUsed::CreateFromUnitializedRegion(
-          &serializer, serializer.GetMinPostingListSize()));
-  // PrependHitArray should fail because hit_elts_in_too_many is far too large
-  // for the minimum size pl.
-  ICING_ASSERT_OK_AND_ASSIGN(
-      uint32_t num_could_fit,
-      (serializer.PrependHitArray<HitElt, HitElt::get_hit>(
-          &pl_used, &hit_elts_in_too_many[0], hit_elts_in_too_many.size(),
-          /*keep_prepended=*/false)));
-  ASSERT_THAT(num_could_fit, Eq(2));
-  ASSERT_THAT(num_could_fit, Lt(hit_elts_in_too_many.size()));
-  ASSERT_THAT(serializer.GetBytesUsed(&pl_used), Eq(0));
-  ASSERT_THAT(serializer.GetHits(&pl_used), IsOkAndHolds(IsEmpty()));
-
-  ICING_ASSERT_OK_AND_ASSIGN(
-      pl_used,
-      PostingListUsed::CreateFromUnitializedRegion(&serializer, kHitsSize));
-  // PrependHitArray should fail because hit_elts_in_too_many is one hit too
-  // large for this pl.
-  ICING_ASSERT_OK_AND_ASSIGN(
-      num_could_fit,
-      (serializer.PrependHitArray<HitElt, HitElt::get_hit>(
-          &pl_used, &hit_elts_in_too_many[0], hit_elts_in_too_many.size(),
-          /*keep_prepended=*/false)));
-  ASSERT_THAT(num_could_fit, Eq(hit_elts_in_too_many.size() - 1));
-  ASSERT_THAT(serializer.GetBytesUsed(&pl_used), Eq(0));
-  ASSERT_THAT(serializer.GetHits(&pl_used), IsOkAndHolds(IsEmpty()));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit1), EqualsHit(hit0))));
 }
 
 TEST(PostingListHitSerializerTest,
@@ -678,7 +406,8 @@ TEST(PostingListHitSerializerTest,
       PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
 
   Hit max_valued_hit(kMaxSectionId, kMinDocumentId, Hit::kMaxTermFrequency,
-                     /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true);
+                     /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true,
+                     /*is_stemmed_hit=*/false);
   ICING_ASSERT_OK(serializer.PrependHit(&pl, max_valued_hit));
   uint32_t bytes_used = serializer.GetBytesUsed(&pl);
   ASSERT_THAT(bytes_used, sizeof(Hit::Value) + sizeof(Hit::Flags) +
@@ -688,7 +417,8 @@ TEST(PostingListHitSerializerTest,
               Le(pl_size - PostingListHitSerializer::kSpecialHitsSize));
 
   Hit min_valued_hit(kMinSectionId, kMaxDocumentId, Hit::kMaxTermFrequency,
-                     /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true);
+                     /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true,
+                     /*is_stemmed_hit=*/false);
   uint8_t delta_buf[VarInt::kMaxEncodedLen64];
   size_t delta_len = PostingListHitSerializer::EncodeNextHitValue(
       /*next_hit_value=*/min_valued_hit.value(),
@@ -701,7 +431,8 @@ TEST(PostingListHitSerializerTest,
   ASSERT_THAT(delta_len, Gt(4));
   ICING_ASSERT_OK(serializer.PrependHit(
       &pl, Hit(kMinSectionId, kMaxDocumentId, Hit::kMaxTermFrequency,
-               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true)));
+               /*is_in_prefix_section=*/true, /*is_prefix_hit=*/true,
+               /*is_stemmed_hit=*/false)));
   // Status should jump to full directly.
   ASSERT_THAT(serializer.GetBytesUsed(&pl), Eq(pl_size));
   ICING_ASSERT_OK(serializer.PopFrontHits(&pl, 1));
@@ -760,11 +491,23 @@ TEST(PostingListHitSerializerTest, GetMinPostingListToFitForNotFullPL) {
       PostingListUsed pl_used,
       PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
   // Create and add some hits to make pl_used NOT_FULL
-  std::vector<Hit> hits_in =
-      CreateHits(/*num_hits=*/7, /*desired_byte_length=*/1);
-  for (const Hit &hit : hits_in) {
-    ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit));
-  }
+  Hit hit0(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit1 = CreateHit(/*last_hit=*/hit0, /*desired_byte_length=*/1);
+  Hit hit2 = CreateHit(/*last_hit=*/hit1, /*desired_byte_length=*/1);
+  Hit hit3 = CreateHit(/*last_hit=*/hit2, /*desired_byte_length=*/1);
+  Hit hit4 = CreateHit(/*last_hit=*/hit3, /*desired_byte_length=*/1);
+  Hit hit5 = CreateHit(/*last_hit=*/hit4, /*desired_byte_length=*/1);
+  Hit hit6 = CreateHit(/*last_hit=*/hit5, /*desired_byte_length=*/1);
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit0));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit1));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit2));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit3));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit4));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit5));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit6));
+
   // ----------------------
   // 23     delta(Hit #0)
   // 22     delta(Hit #1)
@@ -781,9 +524,11 @@ TEST(PostingListHitSerializerTest, GetMinPostingListToFitForNotFullPL) {
 
   // Check that all hits have been inserted
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(bytes_used));
-  std::deque<Hit> hits_pushed(hits_in.rbegin(), hits_in.rend());
-  EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAreArray(hits_pushed)));
+  EXPECT_THAT(
+      serializer.GetHits(&pl_used),
+      IsOkAndHolds(ElementsAre(
+          EqualsHit(hit6), EqualsHit(hit5), EqualsHit(hit4), EqualsHit(hit3),
+          EqualsHit(hit2), EqualsHit(hit1), EqualsHit(hit0))));
 
   // Get the min size to fit for the hits in pl_used. Moving the hits in pl_used
   // into a posting list with this min size should make it ALMOST_FULL, which we
@@ -802,30 +547,36 @@ TEST(PostingListHitSerializerTest, GetMinPostingListToFitForNotFullPL) {
   uint32_t min_size_to_fit = serializer.GetMinPostingListSizeToFit(&pl_used);
   EXPECT_THAT(min_size_to_fit, Eq(expected_min_size));
 
+  // min_size_to_fit is the smallest PL size that can fit all the hits, but
+  // PostingListUsed and serializer require that the PL size is a multiple of
+  // sizeof(Hit). So we need to round up.
+  uint32_t min_size_to_fit_rounded_up =
+      math_util::RoundUpTo(min_size_to_fit, static_cast<uint32_t>(sizeof(Hit)));
+
+  expected_min_size += sizeof(Hit);
+
   // Also check that this min size to fit posting list actually does fit all the
   // hits and can only hit one more hit in the ALMOST_FULL state.
   ICING_ASSERT_OK_AND_ASSIGN(PostingListUsed min_size_to_fit_pl,
                              PostingListUsed::CreateFromUnitializedRegion(
-                                 &serializer, min_size_to_fit));
-  for (const Hit &hit : hits_in) {
-    ICING_ASSERT_OK(serializer.PrependHit(&min_size_to_fit_pl, hit));
-  }
-
-  // Adding another hit to the min-size-to-fit posting list should succeed
-  Hit hit = CreateHit(hits_in.back(), /*desired_byte_length=*/1);
-  ICING_ASSERT_OK(serializer.PrependHit(&min_size_to_fit_pl, hit));
-  // Adding any other hits should fail with RESOURCE_EXHAUSTED error.
-  EXPECT_THAT(serializer.PrependHit(&min_size_to_fit_pl,
-                                    CreateHit(hit, /*desired_byte_length=*/1)),
-              StatusIs(libtextclassifier3::StatusCode::RESOURCE_EXHAUSTED));
+                                 &serializer, min_size_to_fit_rounded_up));
+  ICING_ASSERT_OK(serializer.PrependHit(&min_size_to_fit_pl, hit0));
+  ICING_ASSERT_OK(serializer.PrependHit(&min_size_to_fit_pl, hit1));
+  ICING_ASSERT_OK(serializer.PrependHit(&min_size_to_fit_pl, hit2));
+  ICING_ASSERT_OK(serializer.PrependHit(&min_size_to_fit_pl, hit3));
+  ICING_ASSERT_OK(serializer.PrependHit(&min_size_to_fit_pl, hit4));
+  ICING_ASSERT_OK(serializer.PrependHit(&min_size_to_fit_pl, hit5));
+  ICING_ASSERT_OK(serializer.PrependHit(&min_size_to_fit_pl, hit6));
 
   // Check that all hits have been inserted and the min-fit posting list is now
-  // FULL.
+  // ALMOST_FULL.
   EXPECT_THAT(serializer.GetBytesUsed(&min_size_to_fit_pl),
-              Eq(min_size_to_fit));
-  hits_pushed.emplace_front(hit);
-  EXPECT_THAT(serializer.GetHits(&min_size_to_fit_pl),
-              IsOkAndHolds(ElementsAreArray(hits_pushed)));
+              Eq(min_size_to_fit_rounded_up - sizeof(Hit)));
+  EXPECT_THAT(
+      serializer.GetHits(&min_size_to_fit_pl),
+      IsOkAndHolds(ElementsAre(
+          EqualsHit(hit6), EqualsHit(hit5), EqualsHit(hit4), EqualsHit(hit3),
+          EqualsHit(hit2), EqualsHit(hit1), EqualsHit(hit0))));
 }
 
 TEST(PostingListHitSerializerTest, GetMinPostingListToFitForTwoHits) {
@@ -838,13 +589,12 @@ TEST(PostingListHitSerializerTest, GetMinPostingListToFitForTwoHits) {
       PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
 
   // Create and add 2 hits
-  Hit first_hit(/*section_id=*/1, /*document_id=*/0, /*term_frequency=*/5,
-                /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
-  std::vector<Hit> hits_in =
-      CreateHits(first_hit, /*num_hits=*/2, /*desired_byte_length=*/4);
-  for (const Hit &hit : hits_in) {
-    ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit));
-  }
+  Hit hit0(/*section_id=*/1, /*document_id=*/0, /*term_frequency=*/5,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit1 = CreateHit(/*last_hit=*/hit0, /*desired_byte_length=*/4);
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit0));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit1));
   // ----------------------
   // 35     term-frequency(Hit #0)
   // 34     flags(Hit #0)
@@ -859,9 +609,8 @@ TEST(PostingListHitSerializerTest, GetMinPostingListToFitForTwoHits) {
   int bytes_used = 12;
 
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(bytes_used));
-  std::deque<Hit> hits_pushed(hits_in.rbegin(), hits_in.rend());
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAreArray(hits_pushed)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit1), EqualsHit(hit0))));
 
   // GetMinPostingListSizeToFit should return min posting list size.
   EXPECT_THAT(serializer.GetMinPostingListSizeToFit(&pl_used),
@@ -878,11 +627,15 @@ TEST(PostingListHitSerializerTest, GetMinPostingListToFitForThreeSmallHits) {
       PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
   // Create and add 3 small hits that fit in the size range where we should be
   // checking for whether the PL has only 2 hits
-  std::vector<Hit> hits_in =
-      CreateHits(/*num_hits=*/3, /*desired_byte_length=*/1);
-  for (const Hit &hit : hits_in) {
-    ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit));
-  }
+  Hit hit0(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit1 = CreateHit(/*last_hit=*/hit0, /*desired_byte_length=*/1);
+  Hit hit2 = CreateHit(/*last_hit=*/hit1, /*desired_byte_length=*/1);
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit0));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit1));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit2));
+
   // ----------------------
   // 23     delta(Hit #0)
   // 22     delta(Hit #1)
@@ -894,9 +647,9 @@ TEST(PostingListHitSerializerTest, GetMinPostingListToFitForThreeSmallHits) {
   int bytes_used = 6;
 
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(bytes_used));
-  std::deque<Hit> hits_pushed(hits_in.rbegin(), hits_in.rend());
   EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAreArray(hits_pushed)));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit2), EqualsHit(hit1),
+                                       EqualsHit(hit0))));
 
   // Get the min size to fit for the hits in pl_used. Moving the hits in pl_used
   // into a posting list with this min size should make it ALMOST_FULL, which we
@@ -926,11 +679,23 @@ TEST(PostingListHitSerializerTest,
       PostingListUsed pl_used,
       PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
   // Create and add some hits to make pl_used ALMOST_FULL
-  std::vector<Hit> hits_in =
-      CreateHits(/*num_hits=*/7, /*desired_byte_length=*/2);
-  for (const Hit &hit : hits_in) {
-    ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit));
-  }
+  Hit hit0(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit1 = CreateHit(/*last_hit=*/hit0, /*desired_byte_length=*/2);
+  Hit hit2 = CreateHit(/*last_hit=*/hit1, /*desired_byte_length=*/2);
+  Hit hit3 = CreateHit(/*last_hit=*/hit2, /*desired_byte_length=*/2);
+  Hit hit4 = CreateHit(/*last_hit=*/hit3, /*desired_byte_length=*/2);
+  Hit hit5 = CreateHit(/*last_hit=*/hit4, /*desired_byte_length=*/2);
+  Hit hit6 = CreateHit(/*last_hit=*/hit5, /*desired_byte_length=*/2);
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit0));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit1));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit2));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit3));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit4));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit5));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit6));
+
   // ----------------------
   // 23-22     delta(Hit #0)
   // 21-20     delta(Hit #1)
@@ -944,25 +709,27 @@ TEST(PostingListHitSerializerTest,
   int bytes_used = 18;
 
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(bytes_used));
-  std::deque<Hit> hits_pushed(hits_in.rbegin(), hits_in.rend());
-  EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAreArray(hits_pushed)));
+  EXPECT_THAT(
+      serializer.GetHits(&pl_used),
+      IsOkAndHolds(ElementsAre(
+          EqualsHit(hit6), EqualsHit(hit5), EqualsHit(hit4), EqualsHit(hit3),
+          EqualsHit(hit2), EqualsHit(hit1), EqualsHit(hit0))));
 
   // GetMinPostingListSizeToFit should return the same size as pl_used.
-  uint32_t min_size_to_fit = serializer.GetMinPostingListSizeToFit(&pl_used);
-  EXPECT_THAT(min_size_to_fit, Eq(pl_size));
+  EXPECT_THAT(serializer.GetMinPostingListSizeToFit(&pl_used), Eq(pl_size));
 
   // Add another hit to make the posting list FULL
-  Hit hit = CreateHit(hits_in.back(), /*desired_byte_length=*/1);
-  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit));
+  Hit hit7 = CreateHit(/*last_hit=*/hit6, /*desired_byte_length=*/1);
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used, hit7));
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(pl_size));
-  hits_pushed.emplace_front(hit);
-  EXPECT_THAT(serializer.GetHits(&pl_used),
-              IsOkAndHolds(ElementsAreArray(hits_pushed)));
+  EXPECT_THAT(
+      serializer.GetHits(&pl_used),
+      IsOkAndHolds(ElementsAre(
+          EqualsHit(hit7), EqualsHit(hit6), EqualsHit(hit5), EqualsHit(hit4),
+          EqualsHit(hit3), EqualsHit(hit2), EqualsHit(hit1), EqualsHit(hit0))));
 
   // GetMinPostingListSizeToFit should still be the same size as pl_used.
-  min_size_to_fit = serializer.GetMinPostingListSizeToFit(&pl_used);
-  EXPECT_THAT(min_size_to_fit, Eq(pl_size));
+  EXPECT_THAT(serializer.GetMinPostingListSizeToFit(&pl_used), Eq(pl_size));
 }
 
 TEST(PostingListHitSerializerTest, MoveFrom) {
@@ -972,24 +739,49 @@ TEST(PostingListHitSerializerTest, MoveFrom) {
   ICING_ASSERT_OK_AND_ASSIGN(
       PostingListUsed pl_used1,
       PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
-  std::vector<Hit> hits1 =
-      CreateHits(/*num_hits=*/5, /*desired_byte_length=*/1);
-  for (const Hit &hit : hits1) {
-    ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit));
-  }
+  Hit hit0(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit1 = CreateHit(/*last_hit=*/hit0, /*desired_byte_length=*/1);
+  Hit hit2 = CreateHit(/*last_hit=*/hit1, /*desired_byte_length=*/1);
+  Hit hit3 = CreateHit(/*last_hit=*/hit2, /*desired_byte_length=*/1);
+  Hit hit4 = CreateHit(/*last_hit=*/hit3, /*desired_byte_length=*/1);
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit0));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit1));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit2));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit3));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit4));
+  EXPECT_THAT(serializer.GetHits(&pl_used1),
+              IsOkAndHolds(ElementsAre(EqualsHit(hit4), EqualsHit(hit3),
+                                       EqualsHit(hit2), EqualsHit(hit1),
+                                       EqualsHit(hit0))));
 
   ICING_ASSERT_OK_AND_ASSIGN(
       PostingListUsed pl_used2,
       PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
-  std::vector<Hit> hits2 =
-      CreateHits(/*num_hits=*/5, /*desired_byte_length=*/2);
-  for (const Hit &hit : hits2) {
-    ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, hit));
-  }
+  Hit another_hit0(/*section_id=*/1, /*document_id=*/100,
+                   Hit::kDefaultTermFrequency,
+                   /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                   /*is_stemmed_hit=*/false);
+  Hit another_hit1 =
+      CreateHit(/*last_hit=*/another_hit0, /*desired_byte_length=*/2);
+  Hit another_hit2 =
+      CreateHit(/*last_hit=*/another_hit1, /*desired_byte_length=*/2);
+  Hit another_hit3 =
+      CreateHit(/*last_hit=*/another_hit2, /*desired_byte_length=*/2);
+  Hit another_hit4 =
+      CreateHit(/*last_hit=*/another_hit3, /*desired_byte_length=*/2);
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit0));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit1));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit2));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit3));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit4));
 
   ICING_ASSERT_OK(serializer.MoveFrom(/*dst=*/&pl_used2, /*src=*/&pl_used1));
   EXPECT_THAT(serializer.GetHits(&pl_used2),
-              IsOkAndHolds(ElementsAreArray(hits1.rbegin(), hits1.rend())));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit4), EqualsHit(hit3),
+                                       EqualsHit(hit2), EqualsHit(hit1),
+                                       EqualsHit(hit0))));
   EXPECT_THAT(serializer.GetHits(&pl_used1), IsOkAndHolds(IsEmpty()));
 }
 
@@ -1000,15 +792,25 @@ TEST(PostingListHitSerializerTest, MoveFromNullArgumentReturnsInvalidArgument) {
   ICING_ASSERT_OK_AND_ASSIGN(
       PostingListUsed pl_used1,
       PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
-  std::vector<Hit> hits = CreateHits(/*num_hits=*/5, /*desired_byte_length=*/1);
-  for (const Hit &hit : hits) {
-    ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit));
-  }
+  Hit hit0(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit1 = CreateHit(/*last_hit=*/hit0, /*desired_byte_length=*/1);
+  Hit hit2 = CreateHit(/*last_hit=*/hit1, /*desired_byte_length=*/1);
+  Hit hit3 = CreateHit(/*last_hit=*/hit2, /*desired_byte_length=*/1);
+  Hit hit4 = CreateHit(/*last_hit=*/hit3, /*desired_byte_length=*/1);
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit0));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit1));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit2));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit3));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit4));
 
   EXPECT_THAT(serializer.MoveFrom(/*dst=*/&pl_used1, /*src=*/nullptr),
               StatusIs(libtextclassifier3::StatusCode::FAILED_PRECONDITION));
   EXPECT_THAT(serializer.GetHits(&pl_used1),
-              IsOkAndHolds(ElementsAreArray(hits.rbegin(), hits.rend())));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit4), EqualsHit(hit3),
+                                       EqualsHit(hit2), EqualsHit(hit1),
+                                       EqualsHit(hit0))));
 }
 
 TEST(PostingListHitSerializerTest,
@@ -1019,31 +821,54 @@ TEST(PostingListHitSerializerTest,
   ICING_ASSERT_OK_AND_ASSIGN(
       PostingListUsed pl_used1,
       PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
-  std::vector<Hit> hits1 =
-      CreateHits(/*num_hits=*/5, /*desired_byte_length=*/1);
-  for (const Hit &hit : hits1) {
-    ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit));
-  }
+  Hit hit0(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit1 = CreateHit(/*last_hit=*/hit0, /*desired_byte_length=*/1);
+  Hit hit2 = CreateHit(/*last_hit=*/hit1, /*desired_byte_length=*/1);
+  Hit hit3 = CreateHit(/*last_hit=*/hit2, /*desired_byte_length=*/1);
+  Hit hit4 = CreateHit(/*last_hit=*/hit3, /*desired_byte_length=*/1);
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit0));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit1));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit2));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit3));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit4));
 
   ICING_ASSERT_OK_AND_ASSIGN(
       PostingListUsed pl_used2,
       PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
-  std::vector<Hit> hits2 =
-      CreateHits(/*num_hits=*/5, /*desired_byte_length=*/2);
-  for (const Hit &hit : hits2) {
-    ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, hit));
-  }
+  Hit another_hit0(/*section_id=*/1, /*document_id=*/100,
+                   Hit::kDefaultTermFrequency,
+                   /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                   /*is_stemmed_hit=*/false);
+  Hit another_hit1 =
+      CreateHit(/*last_hit=*/another_hit0, /*desired_byte_length=*/2);
+  Hit another_hit2 =
+      CreateHit(/*last_hit=*/another_hit1, /*desired_byte_length=*/2);
+  Hit another_hit3 =
+      CreateHit(/*last_hit=*/another_hit2, /*desired_byte_length=*/2);
+  Hit another_hit4 =
+      CreateHit(/*last_hit=*/another_hit3, /*desired_byte_length=*/2);
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit0));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit1));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit2));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit3));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit4));
 
   // Write invalid hits to the beginning of pl_used1 to make it invalid.
   Hit invalid_hit(Hit::kInvalidValue);
-  Hit *first_hit = reinterpret_cast<Hit *>(pl_used1.posting_list_buffer());
-  *first_hit = invalid_hit;
-  ++first_hit;
-  *first_hit = invalid_hit;
+  memcpy(pl_used1.posting_list_buffer(), &invalid_hit, sizeof(invalid_hit));
+  memcpy(pl_used1.posting_list_buffer() + sizeof(invalid_hit), &invalid_hit,
+         sizeof(invalid_hit));
+
+  // MoveFrom should return error, and pl_used2 should be unchanged.
   EXPECT_THAT(serializer.MoveFrom(/*dst=*/&pl_used2, /*src=*/&pl_used1),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
-  EXPECT_THAT(serializer.GetHits(&pl_used2),
-              IsOkAndHolds(ElementsAreArray(hits2.rbegin(), hits2.rend())));
+  EXPECT_THAT(
+      serializer.GetHits(&pl_used2),
+      IsOkAndHolds(ElementsAre(EqualsHit(another_hit4), EqualsHit(another_hit3),
+                               EqualsHit(another_hit2), EqualsHit(another_hit1),
+                               EqualsHit(another_hit0))));
 }
 
 TEST(PostingListHitSerializerTest,
@@ -1054,31 +879,53 @@ TEST(PostingListHitSerializerTest,
   ICING_ASSERT_OK_AND_ASSIGN(
       PostingListUsed pl_used1,
       PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
-  std::vector<Hit> hits1 =
-      CreateHits(/*num_hits=*/5, /*desired_byte_length=*/1);
-  for (const Hit &hit : hits1) {
-    ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit));
-  }
+  Hit hit0(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit1 = CreateHit(/*last_hit=*/hit0, /*desired_byte_length=*/1);
+  Hit hit2 = CreateHit(/*last_hit=*/hit1, /*desired_byte_length=*/1);
+  Hit hit3 = CreateHit(/*last_hit=*/hit2, /*desired_byte_length=*/1);
+  Hit hit4 = CreateHit(/*last_hit=*/hit3, /*desired_byte_length=*/1);
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit0));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit1));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit2));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit3));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit4));
 
   ICING_ASSERT_OK_AND_ASSIGN(
       PostingListUsed pl_used2,
       PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
-  std::vector<Hit> hits2 =
-      CreateHits(/*num_hits=*/5, /*desired_byte_length=*/2);
-  for (const Hit &hit : hits2) {
-    ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, hit));
-  }
+  Hit another_hit0(/*section_id=*/1, /*document_id=*/100,
+                   Hit::kDefaultTermFrequency,
+                   /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                   /*is_stemmed_hit=*/false);
+  Hit another_hit1 =
+      CreateHit(/*last_hit=*/another_hit0, /*desired_byte_length=*/2);
+  Hit another_hit2 =
+      CreateHit(/*last_hit=*/another_hit1, /*desired_byte_length=*/2);
+  Hit another_hit3 =
+      CreateHit(/*last_hit=*/another_hit2, /*desired_byte_length=*/2);
+  Hit another_hit4 =
+      CreateHit(/*last_hit=*/another_hit3, /*desired_byte_length=*/2);
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit0));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit1));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit2));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit3));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit4));
 
   // Write invalid hits to the beginning of pl_used2 to make it invalid.
   Hit invalid_hit(Hit::kInvalidValue);
-  Hit *first_hit = reinterpret_cast<Hit *>(pl_used2.posting_list_buffer());
-  *first_hit = invalid_hit;
-  ++first_hit;
-  *first_hit = invalid_hit;
+  memcpy(pl_used2.posting_list_buffer(), &invalid_hit, sizeof(invalid_hit));
+  memcpy(pl_used2.posting_list_buffer() + sizeof(invalid_hit), &invalid_hit,
+         sizeof(invalid_hit));
+
+  // MoveFrom should return error, and pl_used1 should be unchanged.
   EXPECT_THAT(serializer.MoveFrom(/*dst=*/&pl_used2, /*src=*/&pl_used1),
               StatusIs(libtextclassifier3::StatusCode::FAILED_PRECONDITION));
   EXPECT_THAT(serializer.GetHits(&pl_used1),
-              IsOkAndHolds(ElementsAreArray(hits1.rbegin(), hits1.rend())));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit4), EqualsHit(hit3),
+                                       EqualsHit(hit2), EqualsHit(hit1),
+                                       EqualsHit(hit0))));
 }
 
 TEST(PostingListHitSerializerTest, MoveToPostingListTooSmall) {
@@ -1088,28 +935,37 @@ TEST(PostingListHitSerializerTest, MoveToPostingListTooSmall) {
   ICING_ASSERT_OK_AND_ASSIGN(
       PostingListUsed pl_used1,
       PostingListUsed::CreateFromUnitializedRegion(&serializer, pl_size));
-  std::vector<Hit> hits1 =
-      CreateHits(/*num_hits=*/5, /*desired_byte_length=*/1);
-  for (const Hit &hit : hits1) {
-    ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit));
-  }
+  Hit hit0(/*section_id=*/1, /*document_id=*/0, Hit::kDefaultTermFrequency,
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
+  Hit hit1 = CreateHit(/*last_hit=*/hit0, /*desired_byte_length=*/1);
+  Hit hit2 = CreateHit(/*last_hit=*/hit1, /*desired_byte_length=*/1);
+  Hit hit3 = CreateHit(/*last_hit=*/hit2, /*desired_byte_length=*/1);
+  Hit hit4 = CreateHit(/*last_hit=*/hit3, /*desired_byte_length=*/1);
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit0));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit1));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit2));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit3));
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used1, hit4));
 
   ICING_ASSERT_OK_AND_ASSIGN(
       PostingListUsed pl_used2,
       PostingListUsed::CreateFromUnitializedRegion(
           &serializer, serializer.GetMinPostingListSize()));
-  std::vector<Hit> hits2 =
-      CreateHits(/*num_hits=*/1, /*desired_byte_length=*/2);
-  for (const Hit &hit : hits2) {
-    ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, hit));
-  }
+  Hit another_hit(/*section_id=*/1, /*document_id=*/100,
+                  Hit::kDefaultTermFrequency,
+                  /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                  /*is_stemmed_hit=*/false);
+  ICING_ASSERT_OK(serializer.PrependHit(&pl_used2, another_hit));
 
   EXPECT_THAT(serializer.MoveFrom(/*dst=*/&pl_used2, /*src=*/&pl_used1),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
   EXPECT_THAT(serializer.GetHits(&pl_used1),
-              IsOkAndHolds(ElementsAreArray(hits1.rbegin(), hits1.rend())));
+              IsOkAndHolds(ElementsAre(EqualsHit(hit4), EqualsHit(hit3),
+                                       EqualsHit(hit2), EqualsHit(hit1),
+                                       EqualsHit(hit0))));
   EXPECT_THAT(serializer.GetHits(&pl_used2),
-              IsOkAndHolds(ElementsAreArray(hits2.rbegin(), hits2.rend())));
+              IsOkAndHolds(ElementsAre(EqualsHit(another_hit))));
 }
 
 TEST(PostingListHitSerializerTest, PopHitsWithTermFrequenciesAndFlags) {
@@ -1141,7 +997,8 @@ TEST(PostingListHitSerializerTest, PopHitsWithTermFrequenciesAndFlags) {
   int bytes_used = 18;
 
   Hit hit0(/*section_id=*/0, /*document_id=*/0, /*term_frequency=*/5,
-           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+           /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+           /*is_stemmed_hit=*/false);
   Hit hit1 = CreateHit(hit0, /*desired_byte_length=*/2);
   Hit hit2 = CreateHit(hit1, /*desired_byte_length=*/2);
   Hit hit3 = CreateHit(hit2, /*desired_byte_length=*/2);
@@ -1152,7 +1009,8 @@ TEST(PostingListHitSerializerTest, PopHitsWithTermFrequenciesAndFlags) {
 
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Hit> hits_out,
                              serializer.GetHits(&pl_used));
-  EXPECT_THAT(hits_out, ElementsAre(hit3, hit2, hit1, hit0));
+  EXPECT_THAT(hits_out, ElementsAre(EqualsHit(hit3), EqualsHit(hit2),
+                                    EqualsHit(hit1), EqualsHit(hit0)));
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(bytes_used));
 
   // Now, pop the last hit. The posting list should contain the first three
@@ -1171,7 +1029,8 @@ TEST(PostingListHitSerializerTest, PopHitsWithTermFrequenciesAndFlags) {
   // ----------------------
   ICING_ASSERT_OK(serializer.PopFrontHits(&pl_used, 1));
   ICING_ASSERT_OK_AND_ASSIGN(hits_out, serializer.GetHits(&pl_used));
-  EXPECT_THAT(hits_out, ElementsAre(hit2, hit1, hit0));
+  EXPECT_THAT(hits_out,
+              ElementsAre(EqualsHit(hit2), EqualsHit(hit1), EqualsHit(hit0)));
   EXPECT_THAT(serializer.GetBytesUsed(&pl_used), Eq(bytes_used));
 }
 
diff --git a/icing/index/numeric/dummy-numeric-index.h b/icing/index/numeric/dummy-numeric-index.h
index d18f2aa..0afcd42 100644
--- a/icing/index/numeric/dummy-numeric-index.h
+++ b/icing/index/numeric/dummy-numeric-index.h
@@ -197,21 +197,28 @@ class DummyNumericIndex : public NumericIndex<T> {
     memset(dummy_crcs_buffer_.get(), 0, sizeof(PersistentStorage::Crcs));
   }
 
-  libtextclassifier3::Status PersistStoragesToDisk(bool force) override {
+  libtextclassifier3::Status PersistStoragesToDisk() override {
     return libtextclassifier3::Status::OK;
   }
 
-  libtextclassifier3::Status PersistMetadataToDisk(bool force) override {
+  libtextclassifier3::Status PersistMetadataToDisk() override {
     return libtextclassifier3::Status::OK;
   }
 
-  libtextclassifier3::StatusOr<Crc32> ComputeInfoChecksum(bool force) override {
-    return Crc32(0);
+  libtextclassifier3::Status WriteMetadata() override {
+    return libtextclassifier3::Status::OK;
+  }
+
+  libtextclassifier3::StatusOr<Crc32> UpdateStoragesChecksum() override {
+    return Crc32();
+  }
+
+  libtextclassifier3::StatusOr<Crc32> GetInfoChecksum() const override {
+    return Crc32();
   }
 
-  libtextclassifier3::StatusOr<Crc32> ComputeStoragesChecksum(
-      bool force) override {
-    return Crc32(0);
+  libtextclassifier3::StatusOr<Crc32> GetStoragesChecksum() const override {
+    return Crc32();
   }
 
   PersistentStorage::Crcs& crcs() override {
diff --git a/icing/index/numeric/integer-index-storage.cc b/icing/index/numeric/integer-index-storage.cc
index 72e0266..22ed842 100644
--- a/icing/index/numeric/integer-index-storage.cc
+++ b/icing/index/numeric/integer-index-storage.cc
@@ -856,6 +856,7 @@ IntegerIndexStorage::InitializeNewFiles(
   Info& info_ref = new_integer_index_storage->info();
   info_ref.magic = Info::kMagic;
   info_ref.num_data = 0;
+
   // Initialize new PersistentStorage. The initial checksums will be computed
   // and set via InitializeNewStorage.
   ICING_RETURN_IF_ERROR(new_integer_index_storage->InitializeNewStorage());
@@ -913,6 +914,7 @@ IntegerIndexStorage::InitializeExistingFiles(
           std::make_unique<MemoryMappedFile>(std::move(metadata_mmapped_file)),
           std::move(sorted_buckets), std::move(unsorted_buckets),
           std::make_unique<FlashIndexStorage>(std::move(flash_index_storage))));
+
   // Initialize existing PersistentStorage. Checksums will be validated.
   ICING_RETURN_IF_ERROR(integer_index_storage->InitializeExistingStorage());
 
@@ -947,9 +949,8 @@ IntegerIndexStorage::FlushDataIntoNewSortedBucket(
       Bucket(key_lower, key_upper, pl_id, data.size()));
 }
 
-libtextclassifier3::Status IntegerIndexStorage::PersistStoragesToDisk(
-    bool force) {
-  if (!force && !is_storage_dirty()) {
+libtextclassifier3::Status IntegerIndexStorage::PersistStoragesToDisk() {
+  if (is_initialized_ && !is_storage_dirty()) {
     return libtextclassifier3::Status::OK;
   }
 
@@ -959,44 +960,59 @@ libtextclassifier3::Status IntegerIndexStorage::PersistStoragesToDisk(
     return absl_ports::InternalError(
         "Fail to persist FlashIndexStorage to disk");
   }
+  is_storage_dirty_ = false;
   return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::Status IntegerIndexStorage::PersistMetadataToDisk(
-    bool force) {
+libtextclassifier3::Status IntegerIndexStorage::PersistMetadataToDisk() {
   // We can skip persisting metadata to disk only if both info and storage are
   // clean.
-  if (!force && !is_info_dirty() && !is_storage_dirty()) {
+  if (is_initialized_ && !is_info_dirty() && !is_storage_dirty()) {
     return libtextclassifier3::Status::OK;
   }
 
   // Changes should have been applied to the underlying file when using
   // MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC, but call msync() as an
   // extra safety step to ensure they are written out.
-  return metadata_mmapped_file_->PersistToDisk();
+  ICING_RETURN_IF_ERROR(metadata_mmapped_file_->PersistToDisk());
+  is_info_dirty_ = false;
+  return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::StatusOr<Crc32> IntegerIndexStorage::ComputeInfoChecksum(
-    bool force) {
-  if (!force && !is_info_dirty()) {
+libtextclassifier3::StatusOr<Crc32>
+IntegerIndexStorage::UpdateStoragesChecksum() {
+  if (is_initialized_ && !is_storage_dirty()) {
+    return Crc32(crcs().component_crcs.storages_crc);
+  }
+
+  // Compute crcs
+  ICING_ASSIGN_OR_RETURN(Crc32 sorted_buckets_crc,
+                         sorted_buckets_->UpdateChecksum());
+  ICING_ASSIGN_OR_RETURN(Crc32 unsorted_buckets_crc,
+                         unsorted_buckets_->UpdateChecksum());
+
+  // TODO(b/259744228): implement and include flash_index_storage checksum
+  return Crc32(sorted_buckets_crc.Get() ^ unsorted_buckets_crc.Get());
+}
+
+libtextclassifier3::StatusOr<Crc32> IntegerIndexStorage::GetInfoChecksum()
+    const {
+  if (is_initialized_ && !is_info_dirty()) {
     return Crc32(crcs().component_crcs.info_crc);
   }
 
-  return info().ComputeChecksum();
+  return info().GetChecksum();
 }
 
-libtextclassifier3::StatusOr<Crc32>
-IntegerIndexStorage::ComputeStoragesChecksum(bool force) {
-  if (!force && !is_storage_dirty()) {
+libtextclassifier3::StatusOr<Crc32> IntegerIndexStorage::GetStoragesChecksum()
+    const {
+  if (is_initialized_ && !is_storage_dirty()) {
     return Crc32(crcs().component_crcs.storages_crc);
   }
 
   // Compute crcs
-  ICING_ASSIGN_OR_RETURN(Crc32 sorted_buckets_crc,
-                         sorted_buckets_->ComputeChecksum());
-  ICING_ASSIGN_OR_RETURN(Crc32 unsorted_buckets_crc,
-                         unsorted_buckets_->ComputeChecksum());
-
+  Crc32 sorted_buckets_crc = sorted_buckets_->GetChecksum();
+  Crc32 unsorted_buckets_crc = unsorted_buckets_->GetChecksum();
   // TODO(b/259744228): implement and include flash_index_storage checksum
   return Crc32(sorted_buckets_crc.Get() ^ unsorted_buckets_crc.Get());
 }
diff --git a/icing/index/numeric/integer-index-storage.h b/icing/index/numeric/integer-index-storage.h
index 0c1afbb..99693a1 100644
--- a/icing/index/numeric/integer-index-storage.h
+++ b/icing/index/numeric/integer-index-storage.h
@@ -80,7 +80,7 @@ class IntegerIndexStorage : public PersistentStorage {
     int32_t magic;
     int32_t num_data;
 
-    Crc32 ComputeChecksum() const {
+    Crc32 GetChecksum() const {
       return Crc32(
           std::string_view(reinterpret_cast<const char*>(this), sizeof(Info)));
     }
@@ -388,35 +388,21 @@ class IntegerIndexStorage : public PersistentStorage {
       int64_t key_lower, int64_t key_upper,
       std::vector<IntegerIndexData>&& data, IntegerIndexStorage* storage);
 
-  // Flushes contents of all storages to underlying files.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
-  libtextclassifier3::Status PersistStoragesToDisk(bool force) override;
+  libtextclassifier3::Status PersistStoragesToDisk() override;
 
-  // Flushes contents of metadata file.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
-  libtextclassifier3::Status PersistMetadataToDisk(bool force) override;
+  libtextclassifier3::Status PersistMetadataToDisk() override;
 
-  // Computes and returns Info checksum.
-  //
-  // Returns:
-  //   - Crc of the Info on success
-  libtextclassifier3::StatusOr<Crc32> ComputeInfoChecksum(bool force) override;
+  libtextclassifier3::Status WriteMetadata() override {
+    // IntegerIndexStorage::Header is mmapped. Therefore, writes occur when the
+    // metadata is modified. So just return OK.
+    return libtextclassifier3::Status::OK;
+  }
 
-  // Computes and returns all storages checksum. Checksums of sorted_buckets_,
-  // unsorted_buckets_ will be combined together by XOR.
-  // TODO(b/259744228): implement and include flash_index_storage checksum
-  //
-  // Returns:
-  //   - Crc of all storages on success
-  //   - INTERNAL_ERROR if any data inconsistency
-  libtextclassifier3::StatusOr<Crc32> ComputeStoragesChecksum(
-      bool force) override;
+  libtextclassifier3::StatusOr<Crc32> UpdateStoragesChecksum() override;
+
+  libtextclassifier3::StatusOr<Crc32> GetInfoChecksum() const override;
+
+  libtextclassifier3::StatusOr<Crc32> GetStoragesChecksum() const override;
 
   // Helper function to add keys in range [it_start, it_end) into the given
   // bucket. It handles the bucket and its corresponding posting list(s) to make
@@ -477,10 +463,11 @@ class IntegerIndexStorage : public PersistentStorage {
   }
 
   void SetInfoDirty() { is_info_dirty_ = true; }
-  // When storage is dirty, we have to set info dirty as well. So just expose
-  // SetDirty to set both.
+
+  // When the storage is dirty, then the checksum in the info is invalid and
+  // must be recalculated. Therefore, also mark the info as dirty.
   void SetDirty() {
-    is_info_dirty_ = true;
+    SetInfoDirty();
     is_storage_dirty_ = true;
   }
 
diff --git a/icing/index/numeric/integer-index-storage_test.cc b/icing/index/numeric/integer-index-storage_test.cc
index a632bc8..f50e480 100644
--- a/icing/index/numeric/integer-index-storage_test.cc
+++ b/icing/index/numeric/integer-index-storage_test.cc
@@ -362,6 +362,51 @@ TEST_P(IntegerIndexStorageTest,
       StatusIs(libtextclassifier3::StatusCode::FAILED_PRECONDITION));
 }
 
+TEST_P(IntegerIndexStorageTest,
+       InitializationShouldSucceedWithUpdateChecksums) {
+  // Create new integer index storage
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<IntegerIndexStorage> storage1,
+      IntegerIndexStorage::Create(
+          filesystem_, working_path_,
+          Options(IntegerIndexStorage::kDefaultNumDataThresholdForBucketSplit,
+                  /*pre_mapping_fbv_in=*/GetParam()),
+          serializer_.get()));
+
+  // Insert some data.
+  ICING_ASSERT_OK(storage1->AddKeys(/*document_id=*/0, /*section_id=*/20,
+                                    /*new_keys=*/{0, 100, -100}));
+  ICING_ASSERT_OK(storage1->AddKeys(/*document_id=*/1, /*section_id=*/2,
+                                    /*new_keys=*/{3, -1000, 500}));
+  ICING_ASSERT_OK(storage1->AddKeys(/*document_id=*/2, /*section_id=*/15,
+                                    /*new_keys=*/{-6, 321, 98}));
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::vector<DocHitInfo> doc_hit_info_vec,
+      Query(storage1.get(),
+            /*key_lower=*/std::numeric_limits<int64_t>::min(),
+            /*key_upper=*/std::numeric_limits<int64_t>::max()));
+
+  // After calling UpdateChecksums, all checksums should be recomputed and
+  // synced correctly to disk, so initializing another instance on the same
+  // files should succeed, and we should be able to get the same contents.
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc, storage1->GetChecksum());
+  EXPECT_THAT(storage1->UpdateChecksums(), IsOkAndHolds(Eq(crc)));
+  EXPECT_THAT(storage1->GetChecksum(), IsOkAndHolds(Eq(crc)));
+
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<IntegerIndexStorage> storage2,
+      IntegerIndexStorage::Create(
+          filesystem_, working_path_,
+          Options(IntegerIndexStorage::kDefaultNumDataThresholdForBucketSplit,
+                  /*pre_mapping_fbv_in=*/GetParam()),
+          serializer_.get()));
+  EXPECT_THAT(
+      Query(storage2.get(), /*key_lower=*/std::numeric_limits<int64_t>::min(),
+            /*key_upper=*/std::numeric_limits<int64_t>::max()),
+      IsOkAndHolds(
+          ElementsAreArray(doc_hit_info_vec.begin(), doc_hit_info_vec.end())));
+}
+
 TEST_P(IntegerIndexStorageTest, InitializationShouldSucceedWithPersistToDisk) {
   // Create new integer index storage
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -575,13 +620,11 @@ TEST_P(IntegerIndexStorageTest,
         FileBackedVector<Bucket>::Create(
             filesystem_, sorted_buckets_file_path,
             MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
-    ICING_ASSERT_OK_AND_ASSIGN(Crc32 old_crc,
-                               sorted_buckets->ComputeChecksum());
+    ICING_ASSERT_OK_AND_ASSIGN(Crc32 old_crc, sorted_buckets->UpdateChecksum());
     ICING_ASSERT_OK(sorted_buckets->Append(Bucket(
         /*key_lower=*/0, /*key_upper=*/std::numeric_limits<int64_t>::max())));
     ICING_ASSERT_OK(sorted_buckets->PersistToDisk());
-    ICING_ASSERT_OK_AND_ASSIGN(Crc32 new_crc,
-                               sorted_buckets->ComputeChecksum());
+    ICING_ASSERT_OK_AND_ASSIGN(Crc32 new_crc, sorted_buckets->UpdateChecksum());
     ASSERT_THAT(old_crc, Not(Eq(new_crc)));
   }
 
@@ -630,12 +673,12 @@ TEST_P(IntegerIndexStorageTest,
             /*max_file_size=*/sizeof(Bucket) * 100 +
                 FileBackedVector<Bucket>::Header::kHeaderSize));
     ICING_ASSERT_OK_AND_ASSIGN(Crc32 old_crc,
-                               unsorted_buckets->ComputeChecksum());
+                               unsorted_buckets->UpdateChecksum());
     ICING_ASSERT_OK(unsorted_buckets->Append(Bucket(
         /*key_lower=*/0, /*key_upper=*/std::numeric_limits<int64_t>::max())));
     ICING_ASSERT_OK(unsorted_buckets->PersistToDisk());
     ICING_ASSERT_OK_AND_ASSIGN(Crc32 new_crc,
-                               unsorted_buckets->ComputeChecksum());
+                               unsorted_buckets->UpdateChecksum());
     ASSERT_THAT(old_crc, Not(Eq(new_crc)));
   }
 
diff --git a/icing/index/numeric/integer-index.cc b/icing/index/numeric/integer-index.cc
index 8c80698..62fb215 100644
--- a/icing/index/numeric/integer-index.cc
+++ b/icing/index/numeric/integer-index.cc
@@ -424,6 +424,7 @@ IntegerIndex::InitializeNewFiles(const Filesystem& filesystem,
   info_ref.last_added_document_id = kInvalidDocumentId;
   info_ref.num_data_threshold_for_bucket_split =
       num_data_threshold_for_bucket_split;
+
   // Initialize new PersistentStorage. The initial checksums will be computed
   // and set via InitializeNewStorage.
   ICING_RETURN_IF_ERROR(new_integer_index->InitializeNewStorage());
@@ -487,6 +488,7 @@ IntegerIndex::InitializeExistingFiles(
       std::move(property_to_storage_map), std::move(wildcard_property_storage),
       std::move(wildcard_properties_set), std::move(wildcard_index_storage),
       num_data_threshold_for_bucket_split, pre_mapping_fbv));
+
   // Initialize existing PersistentStorage. Checksums will be validated.
   ICING_RETURN_IF_ERROR(integer_index->InitializeExistingStorage());
 
@@ -582,8 +584,8 @@ libtextclassifier3::Status IntegerIndex::TransferIndex(
   return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::Status IntegerIndex::PersistStoragesToDisk(bool force) {
-  if (!force && !is_storage_dirty()) {
+libtextclassifier3::Status IntegerIndex::PersistStoragesToDisk() {
+  if (is_initialized_ && !is_storage_dirty()) {
     return libtextclassifier3::Status::OK;
   }
 
@@ -595,32 +597,62 @@ libtextclassifier3::Status IntegerIndex::PersistStoragesToDisk(bool force) {
   if (wildcard_index_storage_) {
     ICING_RETURN_IF_ERROR(wildcard_index_storage_->PersistToDisk());
   }
+  is_storage_dirty_ = false;
   return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::Status IntegerIndex::PersistMetadataToDisk(bool force) {
-  if (!force && !is_info_dirty() && !is_storage_dirty()) {
+libtextclassifier3::Status IntegerIndex::PersistMetadataToDisk() {
+  if (is_initialized_ && !is_info_dirty() && !is_storage_dirty()) {
     return libtextclassifier3::Status::OK;
   }
 
   // Changes should have been applied to the underlying file when using
   // MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC, but call msync() as an
   // extra safety step to ensure they are written out.
-  return metadata_mmapped_file_->PersistToDisk();
+  ICING_RETURN_IF_ERROR(metadata_mmapped_file_->PersistToDisk());
+  is_info_dirty_ = false;
+  return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::StatusOr<Crc32> IntegerIndex::ComputeInfoChecksum(
-    bool force) {
-  if (!force && !is_info_dirty()) {
-    return Crc32(crcs().component_crcs.info_crc);
+libtextclassifier3::StatusOr<Crc32> IntegerIndex::UpdateStoragesChecksum() {
+  if (is_initialized_ && !is_storage_dirty()) {
+    return Crc32(crcs().component_crcs.storages_crc);
+  }
+
+  // XOR all crcs of all storages. Since XOR is commutative and associative,
+  // the order doesn't matter.
+  uint32_t storages_checksum = 0;
+  for (auto& [property_path, storage] : property_to_storage_map_) {
+    ICING_ASSIGN_OR_RETURN(Crc32 storage_crc, storage->UpdateChecksums());
+    storage_crc.Append(property_path);
+
+    storages_checksum ^= storage_crc.Get();
   }
 
-  return info().ComputeChecksum();
+  if (wildcard_index_storage_ != nullptr) {
+    ICING_ASSIGN_OR_RETURN(Crc32 storage_crc,
+                           wildcard_index_storage_->UpdateChecksums());
+    storages_checksum ^= storage_crc.Get();
+  }
+
+  // FileBackedProto always keeps its checksum up to date. So we just need to
+  // retrieve the checksum.
+  ICING_ASSIGN_OR_RETURN(Crc32 wildcard_properties_crc,
+                         wildcard_property_storage_->GetChecksum());
+  storages_checksum ^= wildcard_properties_crc.Get();
+
+  return Crc32(storages_checksum);
 }
 
-libtextclassifier3::StatusOr<Crc32> IntegerIndex::ComputeStoragesChecksum(
-    bool force) {
-  if (!force && !is_storage_dirty()) {
+libtextclassifier3::StatusOr<Crc32> IntegerIndex::GetInfoChecksum() const {
+  if (is_initialized_ && !is_info_dirty()) {
+    return Crc32(crcs().component_crcs.info_crc);
+  }
+  return info().GetChecksum();
+}
+
+libtextclassifier3::StatusOr<Crc32> IntegerIndex::GetStoragesChecksum() const {
+  if (is_initialized_ && !is_storage_dirty()) {
     return Crc32(crcs().component_crcs.storages_crc);
   }
 
@@ -628,7 +660,7 @@ libtextclassifier3::StatusOr<Crc32> IntegerIndex::ComputeStoragesChecksum(
   // the order doesn't matter.
   uint32_t storages_checksum = 0;
   for (auto& [property_path, storage] : property_to_storage_map_) {
-    ICING_ASSIGN_OR_RETURN(Crc32 storage_crc, storage->UpdateChecksums());
+    ICING_ASSIGN_OR_RETURN(Crc32 storage_crc, storage->GetChecksum());
     storage_crc.Append(property_path);
 
     storages_checksum ^= storage_crc.Get();
@@ -636,12 +668,12 @@ libtextclassifier3::StatusOr<Crc32> IntegerIndex::ComputeStoragesChecksum(
 
   if (wildcard_index_storage_ != nullptr) {
     ICING_ASSIGN_OR_RETURN(Crc32 storage_crc,
-                           wildcard_index_storage_->UpdateChecksums());
+                           wildcard_index_storage_->GetChecksum());
     storages_checksum ^= storage_crc.Get();
   }
 
   ICING_ASSIGN_OR_RETURN(Crc32 wildcard_properties_crc,
-                         wildcard_property_storage_->ComputeChecksum());
+                         wildcard_property_storage_->GetChecksum());
   storages_checksum ^= wildcard_properties_crc.Get();
 
   return Crc32(storages_checksum);
diff --git a/icing/index/numeric/integer-index.h b/icing/index/numeric/integer-index.h
index e7a3127..5e49aee 100644
--- a/icing/index/numeric/integer-index.h
+++ b/icing/index/numeric/integer-index.h
@@ -65,7 +65,7 @@ class IntegerIndex : public NumericIndex<int64_t> {
     DocumentId last_added_document_id;
     int32_t num_data_threshold_for_bucket_split;
 
-    Crc32 ComputeChecksum() const {
+    Crc32 GetChecksum() const {
       return Crc32(
           std::string_view(reinterpret_cast<const char*>(this), sizeof(Info)));
     }
@@ -122,8 +122,6 @@ class IntegerIndex : public NumericIndex<int64_t> {
 
   ~IntegerIndex() override;
 
-  // Returns an Editor instance for adding new records into integer index for a
-  // given property, DocumentId and SectionId. See Editor for more details.
   std::unique_ptr<typename NumericIndex<int64_t>::Editor> Edit(
       std::string_view property_path, DocumentId document_id,
       SectionId section_id) override {
@@ -132,46 +130,15 @@ class IntegerIndex : public NumericIndex<int64_t> {
                                     pre_mapping_fbv_);
   }
 
-  // Returns a DocHitInfoIterator for iterating through all docs which have the
-  // specified (integer) property contents in range [query_key_lower,
-  // query_key_upper].
-  // When iterating through all relevant doc hits, it:
-  // - Merges multiple SectionIds of doc hits with same DocumentId into a single
-  //   SectionIdMask and constructs DocHitInfo.
-  // - Returns DocHitInfo in descending DocumentId order.
-  //
-  // Returns:
-  //   - On success: a DocHitInfoIterator instance
-  //   - NOT_FOUND_ERROR if the given property_path doesn't exist
-  //   - Any IntegerIndexStorage errors
   libtextclassifier3::StatusOr<std::unique_ptr<DocHitInfoIterator>> GetIterator(
       std::string_view property_path, int64_t key_lower, int64_t key_upper,
       const DocumentStore& document_store, const SchemaStore& schema_store,
       int64_t current_time_ms) const override;
 
-  // Reduces internal file sizes by reclaiming space and ids of deleted
-  // documents. Integer index will convert all data (hits) to the new document
-  // ids and regenerate all index files. If all data in a property path are
-  // completely deleted, then the underlying storage will be discarded as well.
-  //
-  // - document_id_old_to_new: a map for converting old document id to new
-  //   document id.
-  // - new_last_added_document_id: will be used to update the last added
-  //                               document id in the integer index.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on IO error
   libtextclassifier3::Status Optimize(
       const std::vector<DocumentId>& document_id_old_to_new,
       DocumentId new_last_added_document_id) override;
 
-  // Clears all integer index data by discarding all existing storages, and set
-  // last_added_document_id to kInvalidDocumentId.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
   libtextclassifier3::Status Clear() override;
 
   DocumentId last_added_document_id() const override {
@@ -312,35 +279,21 @@ class IntegerIndex : public NumericIndex<int64_t> {
   libtextclassifier3::Status TransferWildcardStorage(
       IntegerIndex* new_integer_index) const;
 
-  // Flushes contents of all storages to underlying files.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
-  libtextclassifier3::Status PersistStoragesToDisk(bool force) override;
+  libtextclassifier3::Status PersistStoragesToDisk() override;
 
-  // Flushes contents of metadata file.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
-  libtextclassifier3::Status PersistMetadataToDisk(bool force) override;
+  libtextclassifier3::Status PersistMetadataToDisk() override;
 
-  // Computes and returns Info checksum.
-  //
-  // Returns:
-  //   - Crc of the Info on success
-  libtextclassifier3::StatusOr<Crc32> ComputeInfoChecksum(bool force) override;
+  libtextclassifier3::Status WriteMetadata() override {
+    // IntegerIndex::Header is mmapped. Therefore, writes occur when the
+    // metadata is modified. So just return OK.
+    return libtextclassifier3::Status::OK;
+  }
 
-  // Computes and returns all storages checksum. Checksums of (storage_crc,
-  // property_path) for all existing property paths will be combined together by
-  // XOR.
-  //
-  // Returns:
-  //   - Crc of all storages on success
-  //   - INTERNAL_ERROR if any data inconsistency
-  libtextclassifier3::StatusOr<Crc32> ComputeStoragesChecksum(
-      bool force) override;
+  libtextclassifier3::StatusOr<Crc32> UpdateStoragesChecksum() override;
+
+  libtextclassifier3::StatusOr<Crc32> GetInfoChecksum() const override;
+
+  libtextclassifier3::StatusOr<Crc32> GetStoragesChecksum() const override;
 
   Crcs& crcs() override {
     return *reinterpret_cast<Crcs*>(metadata_mmapped_file_->mutable_region() +
@@ -363,10 +316,11 @@ class IntegerIndex : public NumericIndex<int64_t> {
   }
 
   void SetInfoDirty() { is_info_dirty_ = true; }
-  // When storage is dirty, we have to set info dirty as well. So just expose
-  // SetDirty to set both.
+
+  // When the storage is dirty, then the checksum in the info is invalid and
+  // must be recalculated. Therefore, also mark the info as dirty.
   void SetDirty() {
-    is_info_dirty_ = true;
+    SetInfoDirty();
     is_storage_dirty_ = true;
   }
 
diff --git a/icing/index/numeric/integer-index_test.cc b/icing/index/numeric/integer-index_test.cc
index 42056c6..4563b5b 100644
--- a/icing/index/numeric/integer-index_test.cc
+++ b/icing/index/numeric/integer-index_test.cc
@@ -142,7 +142,10 @@ class NumericIndexIntegerTest : public ::testing::Test {
     }
     ICING_ASSIGN_OR_RETURN(
         DocumentStore::OptimizeResult doc_store_optimize_result,
-        doc_store_->OptimizeInto(document_store_compact_dir, nullptr));
+        doc_store_->OptimizeInto(document_store_compact_dir,
+                                 /*lang_segmenter=*/nullptr,
+                                 /*potentially_optimizable_blob_handles=*/
+                                 std::unordered_set<std::string>()));
 
     doc_store_.reset();
     if (!filesystem_.SwapFiles(document_store_dir.c_str(),
@@ -1224,6 +1227,47 @@ TEST_P(IntegerIndexTest,
       StatusIs(libtextclassifier3::StatusCode::FAILED_PRECONDITION));
 }
 
+TEST_P(IntegerIndexTest, InitializationShouldSucceedWithUpdateChecksums) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<IntegerIndex> integer_index1,
+      IntegerIndex::Create(filesystem_, working_path_,
+                           GetParam().num_data_threshold_for_bucket_split,
+                           GetParam().pre_mapping_fbv));
+
+  // Insert some data.
+  Index(integer_index1.get(), kDefaultTestPropertyPath, /*document_id=*/0,
+        /*section_id=*/20, /*keys=*/{0, 100, -100});
+  Index(integer_index1.get(), kDefaultTestPropertyPath, /*document_id=*/1,
+        /*section_id=*/2, /*keys=*/{3, -1000, 500});
+  Index(integer_index1.get(), kDefaultTestPropertyPath, /*document_id=*/2,
+        /*section_id=*/15, /*keys=*/{-6, 321, 98});
+  integer_index1->set_last_added_document_id(2);
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::vector<DocHitInfo> doc_hit_info_vec,
+      Query(integer_index1.get(), kDefaultTestPropertyPath,
+            /*key_lower=*/std::numeric_limits<int64_t>::min(),
+            /*key_upper=*/std::numeric_limits<int64_t>::max()));
+
+  // After calling UpdateChecksums, all checksums should be recomputed and
+  // written correctly to disk, so initializing another instance on the same
+  // files should succeed, and we should be able to get the same contents.
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc, integer_index1->GetChecksum());
+  EXPECT_THAT(integer_index1->UpdateChecksums(), IsOkAndHolds(Eq(crc)));
+  EXPECT_THAT(integer_index1->GetChecksum(), IsOkAndHolds(Eq(crc)));
+
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<IntegerIndex> integer_index2,
+      IntegerIndex::Create(filesystem_, working_path_,
+                           GetParam().num_data_threshold_for_bucket_split,
+                           GetParam().pre_mapping_fbv));
+  EXPECT_THAT(integer_index2->last_added_document_id(), Eq(2));
+  EXPECT_THAT(Query(integer_index2.get(), kDefaultTestPropertyPath,
+                    /*key_lower=*/std::numeric_limits<int64_t>::min(),
+                    /*key_upper=*/std::numeric_limits<int64_t>::max()),
+              IsOkAndHolds(ElementsAreArray(doc_hit_info_vec.begin(),
+                                            doc_hit_info_vec.end())));
+}
+
 TEST_P(IntegerIndexTest, InitializationShouldSucceedWithPersistToDisk) {
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<IntegerIndex> integer_index1,
diff --git a/icing/index/numeric/numeric-index.h b/icing/index/numeric/numeric-index.h
index d094d3d..208d0c0 100644
--- a/icing/index/numeric/numeric-index.h
+++ b/icing/index/numeric/numeric-index.h
@@ -28,6 +28,7 @@
 #include "icing/schema/section.h"
 #include "icing/store/document-id.h"
 #include "icing/store/document-store.h"
+#include "icing/util/crc32.h"
 
 namespace icing {
 namespace lib {
@@ -182,17 +183,20 @@ class NumericIndex : public PersistentStorage {
       : PersistentStorage(filesystem, std::move(working_path),
                           working_path_type) {}
 
-  virtual libtextclassifier3::Status PersistStoragesToDisk(
-      bool force) override = 0;
+  virtual libtextclassifier3::Status PersistStoragesToDisk() override = 0;
 
-  virtual libtextclassifier3::Status PersistMetadataToDisk(
-      bool force) override = 0;
+  virtual libtextclassifier3::Status PersistMetadataToDisk() override = 0;
 
-  virtual libtextclassifier3::StatusOr<Crc32> ComputeInfoChecksum(
-      bool force) override = 0;
+  virtual libtextclassifier3::Status WriteMetadata() override = 0;
 
-  virtual libtextclassifier3::StatusOr<Crc32> ComputeStoragesChecksum(
-      bool force) override = 0;
+  virtual libtextclassifier3::StatusOr<Crc32> UpdateStoragesChecksum()
+      override = 0;
+
+  virtual libtextclassifier3::StatusOr<Crc32> GetInfoChecksum()
+      const override = 0;
+
+  virtual libtextclassifier3::StatusOr<Crc32> GetStoragesChecksum()
+      const override = 0;
 
   virtual Crcs& crcs() override = 0;
   virtual const Crcs& crcs() const override = 0;
diff --git a/icing/index/property-existence-indexing-handler_test.cc b/icing/index/property-existence-indexing-handler_test.cc
index e8c0773..3228734 100644
--- a/icing/index/property-existence-indexing-handler_test.cc
+++ b/icing/index/property-existence-indexing-handler_test.cc
@@ -256,14 +256,17 @@ TEST_F(PropertyExistenceIndexingHandlerTest, HandlePropertyExistence) {
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document2)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id0,
+      DocumentStore::PutResult put_result0,
       document_store_->Put(tokenized_document0.document()));
+  DocumentId document_id0 = put_result0.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(tokenized_document1.document()));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(tokenized_document2.document()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<PropertyExistenceIndexingHandler> handler,
@@ -373,8 +376,9 @@ TEST_F(PropertyExistenceIndexingHandlerTest, HandleNestedPropertyExistence) {
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(root_document)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(tokenized_root_document.document()));
+  DocumentId document_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<PropertyExistenceIndexingHandler> handler,
       PropertyExistenceIndexingHandler::Create(&fake_clock_, index.get()));
@@ -485,14 +489,17 @@ TEST_F(PropertyExistenceIndexingHandlerTest, SingleEmptyStringIsNonExisting) {
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document2)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id0,
+      DocumentStore::PutResult put_result0,
       document_store_->Put(tokenized_document0.document()));
+  DocumentId document_id0 = put_result0.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(tokenized_document1.document()));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(tokenized_document2.document()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<PropertyExistenceIndexingHandler> handler,
diff --git a/icing/index/term-indexing-handler_test.cc b/icing/index/term-indexing-handler_test.cc
index b23228f..7b97f88 100644
--- a/icing/index/term-indexing-handler_test.cc
+++ b/icing/index/term-indexing-handler_test.cc
@@ -262,8 +262,9 @@ TEST_F(TermIndexingHandlerTest, HandleBothStringSectionAndPropertyExistence) {
                                 std::move(document)));
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(tokenized_document.document()));
+  DocumentId document_id = put_result.new_document_id;
 
   EXPECT_THAT(index->last_added_document_id(), Eq(kInvalidDocumentId));
 
@@ -326,8 +327,9 @@ TEST_F(TermIndexingHandlerTest,
                                 std::move(document)));
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(tokenized_document.document()));
+  DocumentId document_id = put_result.new_document_id;
 
   EXPECT_THAT(index->last_added_document_id(), Eq(kInvalidDocumentId));
 
@@ -403,24 +405,27 @@ TEST_F(TermIndexingHandlerTest, HandleIntoLiteIndex_sortInIndexingTriggered) {
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document0)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id0,
+      DocumentStore::PutResult put_result0,
       document_store_->Put(tokenized_document0.document()));
+  DocumentId document_id0 = put_result0.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
       TokenizedDocument tokenized_document1,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document1)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(tokenized_document1.document()));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
       TokenizedDocument tokenized_document2,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document2)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(tokenized_document2.document()));
+  DocumentId document_id2 = put_result2.new_document_id;
   EXPECT_THAT(index->last_added_document_id(), Eq(kInvalidDocumentId));
 
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -545,24 +550,27 @@ TEST_F(TermIndexingHandlerTest, HandleIntoLiteIndex_enableSortInIndexing) {
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document0)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id0,
+      DocumentStore::PutResult put_result0,
       document_store_->Put(tokenized_document0.document()));
+  DocumentId document_id0 = put_result0.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
       TokenizedDocument tokenized_document1,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document1)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(tokenized_document1.document()));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
       TokenizedDocument tokenized_document2,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
                                 std::move(document2)));
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(tokenized_document2.document()));
+  DocumentId document_id2 = put_result2.new_document_id;
   EXPECT_THAT(index->last_added_document_id(), Eq(kInvalidDocumentId));
 
   ICING_ASSERT_OK_AND_ASSIGN(
diff --git a/icing/jni/icing-search-engine-jni.cc b/icing/jni/icing-search-engine-jni.cc
index a0883fa..afba2a1 100644
--- a/icing/jni/icing-search-engine-jni.cc
+++ b/icing/jni/icing-search-engine-jni.cc
@@ -21,6 +21,7 @@
 #include "icing/jni/jni-cache.h"
 #include "icing/jni/scoped-primitive-array-critical.h"
 #include "icing/jni/scoped-utf-chars.h"
+#include "icing/proto/blob.pb.h"
 #include "icing/proto/document.pb.h"
 #include "icing/proto/initialize.pb.h"
 #include "icing/proto/optimize.pb.h"
@@ -35,7 +36,8 @@
 #include <google/protobuf/message_lite.h>
 
 namespace {
-
+// TODO(b/347054358): Increase this class's test coverage for Failed to parse
+// Proto cases.
 bool ParseProtoFromJniByteArray(JNIEnv* env, jbyteArray bytes,
                                 google::protobuf::MessageLite* protobuf) {
   icing::lib::ScopedPrimitiveArrayCritical<uint8_t> scoped_array(env, bytes);
@@ -247,6 +249,57 @@ void nativeInvalidateNextPageToken(JNIEnv* env, jclass clazz, jobject object,
   return;
 }
 
+jbyteArray nativeOpenWriteBlob(
+    JNIEnv* env, jclass clazz, jobject object, jbyteArray blob_handle_bytes) {
+  icing::lib::IcingSearchEngine* icing =
+      GetIcingSearchEnginePointer(env, object);
+
+  icing::lib::PropertyProto::BlobHandleProto blob_handle;
+  if (!ParseProtoFromJniByteArray(env, blob_handle_bytes, &blob_handle)) {
+    ICING_LOG(icing::lib::ERROR)
+        << "Failed to parse BlobHandle in nativeOpenWriteBlob";
+    return nullptr;
+  }
+
+  icing::lib::BlobProto blob_result_proto = icing->OpenWriteBlob(blob_handle);
+
+  return SerializeProtoToJniByteArray(env, blob_result_proto);
+}
+
+jbyteArray nativeOpenReadBlob(
+    JNIEnv* env, jclass clazz, jobject object, jbyteArray blob_handle_bytes) {
+  icing::lib::IcingSearchEngine* icing =
+      GetIcingSearchEnginePointer(env, object);
+
+  icing::lib::PropertyProto::BlobHandleProto blob_handle;
+  if (!ParseProtoFromJniByteArray(env, blob_handle_bytes, &blob_handle)) {
+    ICING_LOG(icing::lib::ERROR)
+        << "Failed to parse BlobHandle in nativeOpenReadBlob";
+    return nullptr;
+  }
+
+  icing::lib::BlobProto blob_result_proto = icing->OpenReadBlob(blob_handle);
+
+  return SerializeProtoToJniByteArray(env, blob_result_proto);
+}
+
+jbyteArray nativeCommitBlob(
+    JNIEnv* env, jclass clazz, jobject object, jbyteArray blob_handle_bytes) {
+  icing::lib::IcingSearchEngine* icing =
+      GetIcingSearchEnginePointer(env, object);
+
+  icing::lib::PropertyProto::BlobHandleProto blob_handle;
+  if (!ParseProtoFromJniByteArray(env, blob_handle_bytes, &blob_handle)) {
+    ICING_LOG(icing::lib::ERROR)
+        << "Failed to parse BlobHandle in nativeCommitBlob";
+    return nullptr;
+  }
+
+  icing::lib::BlobProto blob_result_proto = icing->CommitBlob(blob_handle);
+
+  return SerializeProtoToJniByteArray(env, blob_result_proto);
+}
+
 jbyteArray nativeSearch(JNIEnv* env, jclass clazz, jobject object,
                         jbyteArray search_spec_bytes,
                         jbyteArray scoring_spec_bytes,
@@ -486,6 +539,8 @@ jint JNI_OnLoad(JavaVM* vm, void* reserved) {
       env->GetFieldID(java_class, "nativePointer", "J");
 
   // Register your class' native methods.
+  // TODO(b/629896095): Add blob methods pre-register here when g3 JNI build
+  // pick up the blob APIs.
   static const JNINativeMethod methods[] = {
       {"nativeCreate", "([B)J", reinterpret_cast<void*>(nativeCreate)},
       {"nativeDestroy", "(Lcom/google/android/icing/IcingSearchEngineImpl;)V",
@@ -520,6 +575,15 @@ jint JNI_OnLoad(JavaVM* vm, void* reserved) {
       {"nativeInvalidateNextPageToken",
        "(Lcom/google/android/icing/IcingSearchEngineImpl;J)V",
        reinterpret_cast<void*>(nativeInvalidateNextPageToken)},
+      {"nativeOpenWriteBlob",
+       "(Lcom/google/android/icing/IcingSearchEngineImpl;[B)[B",
+       reinterpret_cast<void*>(nativeOpenWriteBlob)},
+      {"nativeOpenReadBlob",
+       "(Lcom/google/android/icing/IcingSearchEngineImpl;[B)[B",
+       reinterpret_cast<void*>(nativeOpenReadBlob)},
+      {"nativeCommitBlob",
+       "(Lcom/google/android/icing/IcingSearchEngineImpl;[B)[B",
+       reinterpret_cast<void*>(nativeCommitBlob)},
       {"nativeSearch",
        "(Lcom/google/android/icing/IcingSearchEngineImpl;[B[B[BJ)[B",
        reinterpret_cast<void*>(nativeSearch)},
diff --git a/icing/join/join-processor_test.cc b/icing/join/join-processor_test.cc
index 77fa9f3..a205125 100644
--- a/icing/join/join-processor_test.cc
+++ b/icing/join/join-processor_test.cc
@@ -190,7 +190,9 @@ class JoinProcessorTest : public ::testing::Test {
 
   libtextclassifier3::StatusOr<DocumentId> PutAndIndexDocument(
       const DocumentProto& document) {
-    ICING_ASSIGN_OR_RETURN(DocumentId document_id, doc_store_->Put(document));
+    ICING_ASSIGN_OR_RETURN(DocumentStore::PutResult put_result,
+                           doc_store_->Put(document));
+    DocumentId document_id = put_result.new_document_id;
     ICING_ASSIGN_OR_RETURN(
         TokenizedDocument tokenized_document,
         TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
diff --git a/icing/join/qualified-id-join-index-impl-v1.cc b/icing/join/qualified-id-join-index-impl-v1.cc
index cdcb5a9..a9be0c1 100644
--- a/icing/join/qualified-id-join-index-impl-v1.cc
+++ b/icing/join/qualified-id-join-index-impl-v1.cc
@@ -18,6 +18,7 @@
 #include <memory>
 #include <string>
 #include <string_view>
+#include <utility>
 #include <vector>
 
 #include "icing/text_classifier/lib3/utils/base/status.h"
@@ -309,6 +310,7 @@ QualifiedIdJoinIndexImplV1::InitializeNewFiles(const Filesystem& filesystem,
   // Initialize info content.
   new_index->info().magic = Info::kMagic;
   new_index->info().last_added_document_id = kInvalidDocumentId;
+
   // Initialize new PersistentStorage. The initial checksums will be computed
   // and set via InitializeNewStorage.
   ICING_RETURN_IF_ERROR(new_index->InitializeNewStorage());
@@ -372,6 +374,7 @@ QualifiedIdJoinIndexImplV1::InitializeExistingFiles(
           filesystem, std::move(working_path), std::move(metadata_buffer),
           std::move(doc_join_info_mapper), std::move(qualified_id_storage),
           pre_mapping_fbv, use_persistent_hash_map));
+
   // Initialize existing PersistentStorage. Checksums will be validated.
   ICING_RETURN_IF_ERROR(type_joinable_index->InitializeExistingStorage());
 
@@ -409,66 +412,88 @@ libtextclassifier3::Status QualifiedIdJoinIndexImplV1::TransferIndex(
   }
 
   // TODO(b/268521214): transfer delete propagation storage
-
   return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::Status QualifiedIdJoinIndexImplV1::PersistMetadataToDisk(
-    bool force) {
-  if (!force && !is_info_dirty() && !is_storage_dirty()) {
+libtextclassifier3::Status QualifiedIdJoinIndexImplV1::PersistMetadataToDisk() {
+  if (is_initialized_ && !is_info_dirty() && !is_storage_dirty()) {
     return libtextclassifier3::Status::OK;
   }
 
   std::string metadata_file_path = GetMetadataFilePath(working_path_);
-
   ScopedFd sfd(filesystem_.OpenForWrite(metadata_file_path.c_str()));
-  if (!sfd.is_valid()) {
-    return absl_ports::InternalError("Fail to open metadata file for write");
-  }
-
-  if (!filesystem_.PWrite(sfd.get(), /*offset=*/0, metadata_buffer_.get(),
-                          kMetadataFileSize)) {
-    return absl_ports::InternalError("Fail to write metadata file");
-  }
-
+  ICING_RETURN_IF_ERROR(InternalWriteMetadata(sfd));
   if (!filesystem_.DataSync(sfd.get())) {
     return absl_ports::InternalError("Fail to sync metadata to disk");
   }
-
+  is_info_dirty_ = false;
   return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::Status QualifiedIdJoinIndexImplV1::PersistStoragesToDisk(
-    bool force) {
-  if (!force && !is_storage_dirty()) {
+libtextclassifier3::Status QualifiedIdJoinIndexImplV1::PersistStoragesToDisk() {
+  if (is_initialized_ && !is_storage_dirty()) {
     return libtextclassifier3::Status::OK;
   }
 
   ICING_RETURN_IF_ERROR(doc_join_info_mapper_->PersistToDisk());
   ICING_RETURN_IF_ERROR(qualified_id_storage_->PersistToDisk());
+  is_storage_dirty_ = false;
   return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::StatusOr<Crc32>
-QualifiedIdJoinIndexImplV1::ComputeInfoChecksum(bool force) {
-  if (!force && !is_info_dirty()) {
-    return Crc32(crcs().component_crcs.info_crc);
+libtextclassifier3::Status QualifiedIdJoinIndexImplV1::WriteMetadata() {
+  if (is_initialized_ && !is_info_dirty() && !is_storage_dirty()) {
+    return libtextclassifier3::Status::OK;
   }
 
-  return info().ComputeChecksum();
+  std::string metadata_file_path = GetMetadataFilePath(working_path_);
+  ScopedFd sfd(filesystem_.OpenForWrite(metadata_file_path.c_str()));
+  return InternalWriteMetadata(std::move(sfd));
+}
+
+libtextclassifier3::Status QualifiedIdJoinIndexImplV1::InternalWriteMetadata(
+    const ScopedFd& sfd) {
+  if (!sfd.is_valid()) {
+    return absl_ports::InternalError("Fail to open metadata file for write");
+  }
+  if (!filesystem_.PWrite(sfd.get(), /*offset=*/0, metadata_buffer_.get(),
+                          kMetadataFileSize)) {
+    return absl_ports::InternalError("Fail to write metadata file");
+  }
+  return libtextclassifier3::Status::OK;
 }
 
 libtextclassifier3::StatusOr<Crc32>
-QualifiedIdJoinIndexImplV1::ComputeStoragesChecksum(bool force) {
-  if (!force && !is_storage_dirty()) {
+QualifiedIdJoinIndexImplV1::UpdateStoragesChecksum() {
+  if (is_initialized_ && !is_storage_dirty()) {
     return Crc32(crcs().component_crcs.storages_crc);
   }
 
   ICING_ASSIGN_OR_RETURN(Crc32 doc_join_info_mapper_crc,
-                         doc_join_info_mapper_->ComputeChecksum());
+                         doc_join_info_mapper_->UpdateChecksum());
   ICING_ASSIGN_OR_RETURN(Crc32 qualified_id_storage_crc,
-                         qualified_id_storage_->ComputeChecksum());
+                         qualified_id_storage_->UpdateChecksum());
+  return Crc32(doc_join_info_mapper_crc.Get() ^ qualified_id_storage_crc.Get());
+}
 
+libtextclassifier3::StatusOr<Crc32>
+QualifiedIdJoinIndexImplV1::GetInfoChecksum() const {
+  // Info checksum is not cached and is calculated on the fly. Just call Get.
+  if (is_initialized_ && !is_info_dirty()) {
+    return Crc32(crcs().component_crcs.info_crc);
+  }
+  return info().GetChecksum();
+}
+
+libtextclassifier3::StatusOr<Crc32>
+QualifiedIdJoinIndexImplV1::GetStoragesChecksum() const {
+  if (is_initialized_ && !is_storage_dirty()) {
+    return Crc32(crcs().component_crcs.storages_crc);
+  }
+
+  ICING_ASSIGN_OR_RETURN(Crc32 doc_join_info_mapper_crc,
+                         doc_join_info_mapper_->GetChecksum());
+  Crc32 qualified_id_storage_crc = qualified_id_storage_->GetChecksum();
   return Crc32(doc_join_info_mapper_crc.Get() ^ qualified_id_storage_crc.Get());
 }
 
diff --git a/icing/join/qualified-id-join-index-impl-v1.h b/icing/join/qualified-id-join-index-impl-v1.h
index 9314602..0d8da56 100644
--- a/icing/join/qualified-id-join-index-impl-v1.h
+++ b/icing/join/qualified-id-join-index-impl-v1.h
@@ -51,7 +51,7 @@ class QualifiedIdJoinIndexImplV1 : public QualifiedIdJoinIndex {
     int32_t magic;
     DocumentId last_added_document_id;
 
-    Crc32 ComputeChecksum() const {
+    Crc32 GetChecksum() const {
       return Crc32(
           std::string_view(reinterpret_cast<const char*>(this), sizeof(Info)));
     }
@@ -115,68 +115,28 @@ class QualifiedIdJoinIndexImplV1 : public QualifiedIdJoinIndex {
                                  DocumentId document_id,
                                  std::vector<NamespaceFingerprintIdentifier>&&
                                      ref_namespace_fingerprint_ids) override {
-    return absl_ports::UnimplementedError("This API is not supported in V2");
+    return absl_ports::UnimplementedError("This API is not supported in V1");
   }
 
   // v2 only API. Returns UNIMPLEMENTED_ERROR.
   libtextclassifier3::StatusOr<std::unique_ptr<JoinDataIteratorBase>>
   GetIterator(SchemaTypeId schema_type_id,
               JoinablePropertyId joinable_property_id) const override {
-    return absl_ports::UnimplementedError("This API is not supported in V2");
+    return absl_ports::UnimplementedError("This API is not supported in V1");
   }
 
-  // Puts a new data into index: DocJoinInfo (DocumentId, JoinablePropertyId)
-  // references to ref_qualified_id_str (the identifier of another document).
-  //
-  // REQUIRES: ref_qualified_id_str contains no '\0'.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INVALID_ARGUMENT_ERROR if doc_join_info is invalid
-  //   - Any KeyMapper errors
   libtextclassifier3::Status Put(
       const DocJoinInfo& doc_join_info,
       std::string_view ref_qualified_id_str) override;
 
-  // Gets the referenced document's qualified id string by DocJoinInfo.
-  //
-  // Returns:
-  //   - A qualified id string referenced by the given DocJoinInfo (DocumentId,
-  //     JoinablePropertyId) on success
-  //   - INVALID_ARGUMENT_ERROR if doc_join_info is invalid
-  //   - NOT_FOUND_ERROR if doc_join_info doesn't exist
-  //   - Any KeyMapper errors
   libtextclassifier3::StatusOr<std::string_view> Get(
       const DocJoinInfo& doc_join_info) const override;
 
-  // Reduces internal file sizes by reclaiming space and ids of deleted
-  // documents. Qualified id type joinable index will convert all entries to the
-  // new document ids.
-  //
-  // - document_id_old_to_new: a map for converting old document id to new
-  //   document id.
-  // - namespace_id_old_to_new: a map for converting old namespace id to new
-  //   namespace id. It is unused in this implementation since we store raw
-  //   qualified id string (which contains raw namespace string).
-  // - new_last_added_document_id: will be used to update the last added
-  //                               document id in the qualified id type joinable
-  //                               index.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error. This could potentially leave the index in
-  //     an invalid state and the caller should handle it properly (e.g. discard
-  //     and rebuild)
   libtextclassifier3::Status Optimize(
       const std::vector<DocumentId>& document_id_old_to_new,
       const std::vector<NamespaceId>& namespace_id_old_to_new,
       DocumentId new_last_added_document_id) override;
 
-  // Clears all data and set last_added_document_id to kInvalidDocumentId.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
   libtextclassifier3::Status Clear() override;
 
   bool is_v2() const override { return false; }
@@ -237,33 +197,19 @@ class QualifiedIdJoinIndexImplV1 : public QualifiedIdJoinIndex {
       const std::vector<DocumentId>& document_id_old_to_new,
       QualifiedIdJoinIndexImplV1* new_index) const;
 
-  // Flushes contents of metadata file.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
-  libtextclassifier3::Status PersistMetadataToDisk(bool force) override;
+  libtextclassifier3::Status PersistMetadataToDisk() override;
 
-  // Flushes contents of all storages to underlying files.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
-  libtextclassifier3::Status PersistStoragesToDisk(bool force) override;
+  libtextclassifier3::Status PersistStoragesToDisk() override;
 
-  // Computes and returns Info checksum.
-  //
-  // Returns:
-  //   - Crc of the Info on success
-  libtextclassifier3::StatusOr<Crc32> ComputeInfoChecksum(bool force) override;
+  libtextclassifier3::Status WriteMetadata() override;
 
-  // Computes and returns all storages checksum.
-  //
-  // Returns:
-  //   - Crc of all storages on success
-  //   - INTERNAL_ERROR if any data inconsistency
-  libtextclassifier3::StatusOr<Crc32> ComputeStoragesChecksum(
-      bool force) override;
+  libtextclassifier3::Status InternalWriteMetadata(const ScopedFd& sfd);
+
+  libtextclassifier3::StatusOr<Crc32> UpdateStoragesChecksum() override;
+
+  libtextclassifier3::StatusOr<Crc32> GetInfoChecksum() const override;
+
+  libtextclassifier3::StatusOr<Crc32> GetStoragesChecksum() const override;
 
   Crcs& crcs() override {
     return *reinterpret_cast<Crcs*>(metadata_buffer_.get() +
@@ -286,10 +232,11 @@ class QualifiedIdJoinIndexImplV1 : public QualifiedIdJoinIndex {
   }
 
   void SetInfoDirty() { is_info_dirty_ = true; }
+
   // When storage is dirty, we have to set info dirty as well. So just expose
   // SetDirty to set both.
   void SetDirty() {
-    is_info_dirty_ = true;
+    SetInfoDirty();
     is_storage_dirty_ = true;
   }
 
diff --git a/icing/join/qualified-id-join-index-impl-v1_test.cc b/icing/join/qualified-id-join-index-impl-v1_test.cc
index a6e19bb..bde9066 100644
--- a/icing/join/qualified-id-join-index-impl-v1_test.cc
+++ b/icing/join/qualified-id-join-index-impl-v1_test.cc
@@ -168,6 +168,8 @@ TEST_P(QualifiedIdJoinIndexImplV1Test,
   ICING_ASSERT_OK(
       index->Put(DocJoinInfo(/*document_id=*/5, /*joinable_property_id=*/20),
                  /*ref_qualified_id_str=*/"namespace#uriC"));
+  // GetChecksum should succeed without updating the checksum.
+  ICING_EXPECT_OK(index->GetChecksum());
 
   // Without calling PersistToDisk, checksums will not be recomputed or synced
   // to disk, so initializing another instance on the same files should fail.
@@ -179,6 +181,53 @@ TEST_P(QualifiedIdJoinIndexImplV1Test,
                            : libtextclassifier3::StatusCode::INTERNAL));
 }
 
+TEST_P(QualifiedIdJoinIndexImplV1Test,
+       InitializationShouldSucceedWithUpdateChecksums) {
+  const QualifiedIdJoinIndexImplV1TestParam& param = GetParam();
+
+  // Create new qualified id join index
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<QualifiedIdJoinIndexImplV1> index1,
+      QualifiedIdJoinIndexImplV1::Create(filesystem_, working_path_,
+                                         param.pre_mapping_fbv,
+                                         param.use_persistent_hash_map));
+
+  // Insert some data.
+  ICING_ASSERT_OK(
+      index1->Put(DocJoinInfo(/*document_id=*/1, /*joinable_property_id=*/20),
+                  /*ref_qualified_id_str=*/"namespace#uriA"));
+  ICING_ASSERT_OK(
+      index1->Put(DocJoinInfo(/*document_id=*/3, /*joinable_property_id=*/20),
+                  /*ref_qualified_id_str=*/"namespace#uriB"));
+  ICING_ASSERT_OK(
+      index1->Put(DocJoinInfo(/*document_id=*/5, /*joinable_property_id=*/20),
+                  /*ref_qualified_id_str=*/"namespace#uriC"));
+  ASSERT_THAT(index1, Pointee(SizeIs(3)));
+
+  // After calling UpdateChecksums, all checksums should be recomputed and
+  // synced correctly to disk, so initializing another instance on the same
+  // files should succeed, and we should be able to get the same contents.
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc, index1->GetChecksum());
+  EXPECT_THAT(index1->UpdateChecksums(), IsOkAndHolds(Eq(crc)));
+  EXPECT_THAT(index1->GetChecksum(), IsOkAndHolds(Eq(crc)));
+
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<QualifiedIdJoinIndexImplV1> index2,
+      QualifiedIdJoinIndexImplV1::Create(filesystem_, working_path_,
+                                         param.pre_mapping_fbv,
+                                         param.use_persistent_hash_map));
+  EXPECT_THAT(index2, Pointee(SizeIs(3)));
+  EXPECT_THAT(
+      index2->Get(DocJoinInfo(/*document_id=*/1, /*joinable_property_id=*/20)),
+      IsOkAndHolds(/*ref_qualified_id_str=*/"namespace#uriA"));
+  EXPECT_THAT(
+      index2->Get(DocJoinInfo(/*document_id=*/3, /*joinable_property_id=*/20)),
+      IsOkAndHolds(/*ref_qualified_id_str=*/"namespace#uriB"));
+  EXPECT_THAT(
+      index2->Get(DocJoinInfo(/*document_id=*/5, /*joinable_property_id=*/20)),
+      IsOkAndHolds(/*ref_qualified_id_str=*/"namespace#uriC"));
+}
+
 TEST_P(QualifiedIdJoinIndexImplV1Test,
        InitializationShouldSucceedWithPersistToDisk) {
   const QualifiedIdJoinIndexImplV1TestParam& param = GetParam();
@@ -312,8 +361,8 @@ TEST_P(QualifiedIdJoinIndexImplV1Test,
         metadata_buffer.get() +
         QualifiedIdJoinIndexImplV1::kInfoMetadataBufferOffset);
     info->magic += kCorruptedValueOffset;
-    crcs->component_crcs.info_crc = info->ComputeChecksum().Get();
-    crcs->all_crc = crcs->component_crcs.ComputeChecksum().Get();
+    crcs->component_crcs.info_crc = info->GetChecksum().Get();
+    crcs->all_crc = crcs->component_crcs.GetChecksum().Get();
     ASSERT_THAT(filesystem_.PWrite(
                     metadata_sfd.get(), /*offset=*/0, metadata_buffer.get(),
                     QualifiedIdJoinIndexImplV1::kMetadataFileSize),
@@ -468,10 +517,10 @@ TEST_P(QualifiedIdJoinIndexImplV1Test,
                                      filesystem_, mapper_working_path,
                                      /*maximum_size_bytes=*/128 * 1024 * 1024));
     }
-    ICING_ASSERT_OK_AND_ASSIGN(Crc32 old_crc, mapper->ComputeChecksum());
+    ICING_ASSERT_OK_AND_ASSIGN(Crc32 old_crc, mapper->UpdateChecksum());
     ICING_ASSERT_OK(mapper->Put("foo", 12345));
     ICING_ASSERT_OK(mapper->PersistToDisk());
-    ICING_ASSERT_OK_AND_ASSIGN(Crc32 new_crc, mapper->ComputeChecksum());
+    ICING_ASSERT_OK_AND_ASSIGN(Crc32 new_crc, mapper->UpdateChecksum());
     ASSERT_THAT(old_crc, Not(Eq(new_crc)));
   }
 
@@ -512,12 +561,12 @@ TEST_P(QualifiedIdJoinIndexImplV1Test,
             filesystem_, qualified_id_storage_path,
             MemoryMappedFile::Strategy::READ_WRITE_AUTO_SYNC));
     ICING_ASSERT_OK_AND_ASSIGN(Crc32 old_crc,
-                               qualified_id_storage->ComputeChecksum());
+                               qualified_id_storage->UpdateChecksum());
     ICING_ASSERT_OK(qualified_id_storage->Append('a'));
     ICING_ASSERT_OK(qualified_id_storage->Append('b'));
     ICING_ASSERT_OK(qualified_id_storage->PersistToDisk());
     ICING_ASSERT_OK_AND_ASSIGN(Crc32 new_crc,
-                               qualified_id_storage->ComputeChecksum());
+                               qualified_id_storage->UpdateChecksum());
     ASSERT_THAT(old_crc, Not(Eq(new_crc)));
   }
 
diff --git a/icing/join/qualified-id-join-index-impl-v2.cc b/icing/join/qualified-id-join-index-impl-v2.cc
index 70fd13c..04a535c 100644
--- a/icing/join/qualified-id-join-index-impl-v2.cc
+++ b/icing/join/qualified-id-join-index-impl-v2.cc
@@ -465,6 +465,7 @@ QualifiedIdJoinIndexImplV2::InitializeNewFiles(const Filesystem& filesystem,
   new_join_index->info().magic = Info::kMagic;
   new_join_index->info().num_data = 0;
   new_join_index->info().last_added_document_id = kInvalidDocumentId;
+
   // Initialize new PersistentStorage. The initial checksums will be computed
   // and set via InitializeNewStorage.
   ICING_RETURN_IF_ERROR(new_join_index->InitializeNewStorage());
@@ -513,6 +514,7 @@ QualifiedIdJoinIndexImplV2::InitializeExistingFiles(
           std::move(posting_list_serializer),
           std::make_unique<FlashIndexStorage>(std::move(flash_index_storage)),
           pre_mapping_fbv));
+
   // Initialize existing PersistentStorage. Checksums will be validated.
   ICING_RETURN_IF_ERROR(join_index->InitializeExistingStorage());
 
@@ -614,34 +616,23 @@ libtextclassifier3::Status QualifiedIdJoinIndexImplV2::TransferIndex(
   return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::Status QualifiedIdJoinIndexImplV2::PersistMetadataToDisk(
-    bool force) {
-  if (!force && !is_info_dirty() && !is_storage_dirty()) {
+libtextclassifier3::Status QualifiedIdJoinIndexImplV2::PersistMetadataToDisk() {
+  if (is_initialized_ && !is_info_dirty() && !is_storage_dirty()) {
     return libtextclassifier3::Status::OK;
   }
 
   std::string metadata_file_path = GetMetadataFilePath(working_path_);
-
   ScopedFd sfd(filesystem_.OpenForWrite(metadata_file_path.c_str()));
-  if (!sfd.is_valid()) {
-    return absl_ports::InternalError("Fail to open metadata file for write");
-  }
-
-  if (!filesystem_.PWrite(sfd.get(), /*offset=*/0, metadata_buffer_.get(),
-                          kMetadataFileSize)) {
-    return absl_ports::InternalError("Fail to write metadata file");
-  }
-
+  ICING_RETURN_IF_ERROR(InternalWriteMetadata(sfd));
   if (!filesystem_.DataSync(sfd.get())) {
     return absl_ports::InternalError("Fail to sync metadata to disk");
   }
-
+  is_info_dirty_ = false;
   return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::Status QualifiedIdJoinIndexImplV2::PersistStoragesToDisk(
-    bool force) {
-  if (!force && !is_storage_dirty()) {
+libtextclassifier3::Status QualifiedIdJoinIndexImplV2::PersistStoragesToDisk() {
+  if (is_initialized_ && !is_storage_dirty()) {
     return libtextclassifier3::Status::OK;
   }
 
@@ -651,30 +642,54 @@ libtextclassifier3::Status QualifiedIdJoinIndexImplV2::PersistStoragesToDisk(
     return absl_ports::InternalError(
         "Fail to persist FlashIndexStorage to disk");
   }
-
+  is_storage_dirty_ = false;
   return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::StatusOr<Crc32>
-QualifiedIdJoinIndexImplV2::ComputeInfoChecksum(bool force) {
-  if (!force && !is_info_dirty()) {
-    return Crc32(crcs().component_crcs.info_crc);
+libtextclassifier3::Status QualifiedIdJoinIndexImplV2::WriteMetadata() {
+  if (is_initialized_ && !is_info_dirty() && !is_storage_dirty()) {
+    return libtextclassifier3::Status::OK;
   }
 
-  return info().ComputeChecksum();
+  std::string metadata_file_path = GetMetadataFilePath(working_path_);
+  ScopedFd sfd(filesystem_.OpenForWrite(metadata_file_path.c_str()));
+  return InternalWriteMetadata(sfd);
+}
+
+libtextclassifier3::Status QualifiedIdJoinIndexImplV2::InternalWriteMetadata(
+    const ScopedFd& sfd) {
+  if (!sfd.is_valid()) {
+    return absl_ports::InternalError("Fail to open metadata file for write");
+  }
+  if (!filesystem_.PWrite(sfd.get(), /*offset=*/0, metadata_buffer_.get(),
+                          kMetadataFileSize)) {
+    return absl_ports::InternalError("Fail to write metadata file");
+  }
+  return libtextclassifier3::Status::OK;
 }
 
 libtextclassifier3::StatusOr<Crc32>
-QualifiedIdJoinIndexImplV2::ComputeStoragesChecksum(bool force) {
-  if (!force && !is_storage_dirty()) {
+QualifiedIdJoinIndexImplV2::UpdateStoragesChecksum() {
+  if (is_initialized_ && !is_storage_dirty()) {
     return Crc32(crcs().component_crcs.storages_crc);
   }
+  return schema_joinable_id_to_posting_list_mapper_->UpdateChecksum();
+}
 
-  ICING_ASSIGN_OR_RETURN(
-      Crc32 schema_joinable_id_to_posting_list_mapper_crc,
-      schema_joinable_id_to_posting_list_mapper_->ComputeChecksum());
+libtextclassifier3::StatusOr<Crc32>
+QualifiedIdJoinIndexImplV2::GetInfoChecksum() const {
+  if (is_initialized_ && !is_info_dirty()) {
+    return Crc32(crcs().component_crcs.info_crc);
+  }
+  return info().GetChecksum();
+}
 
-  return Crc32(schema_joinable_id_to_posting_list_mapper_crc.Get());
+libtextclassifier3::StatusOr<Crc32>
+QualifiedIdJoinIndexImplV2::GetStoragesChecksum() const {
+  if (is_initialized_ && !is_storage_dirty()) {
+    return Crc32(crcs().component_crcs.storages_crc);
+  }
+  return schema_joinable_id_to_posting_list_mapper_->GetChecksum();
 }
 
 }  // namespace lib
diff --git a/icing/join/qualified-id-join-index-impl-v2.h b/icing/join/qualified-id-join-index-impl-v2.h
index 2b0bf3f..d45ca41 100644
--- a/icing/join/qualified-id-join-index-impl-v2.h
+++ b/icing/join/qualified-id-join-index-impl-v2.h
@@ -85,13 +85,13 @@ class QualifiedIdJoinIndexImplV2 : public QualifiedIdJoinIndex {
   };
 
   struct Info {
-    static constexpr int32_t kMagic = 0x12d1c074;
+    static constexpr int32_t kMagic = 0x32e374a7;
 
     int32_t magic;
     int32_t num_data;
     DocumentId last_added_document_id;
 
-    Crc32 ComputeChecksum() const {
+    Crc32 GetChecksum() const {
       return Crc32(
           std::string_view(reinterpret_cast<const char*>(this), sizeof(Info)));
     }
@@ -163,59 +163,21 @@ class QualifiedIdJoinIndexImplV2 : public QualifiedIdJoinIndex {
     return absl_ports::UnimplementedError("This API is not supported in V2");
   }
 
-  // Puts a list of referenced (parent) NamespaceFingerprintIdentifiers into
-  // the join index, given the (child) DocumentId, SchemaTypeId and
-  // JoinablePropertyId.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INVALID_ARGUMENT_ERROR if schema_type_id, joinable_property_id, or
-  //     document_id is invalid
-  //   - Any KeyMapper/FlashIndexStorage errors
   libtextclassifier3::Status Put(SchemaTypeId schema_type_id,
                                  JoinablePropertyId joinable_property_id,
                                  DocumentId document_id,
                                  std::vector<NamespaceFingerprintIdentifier>&&
                                      ref_namespace_fingerprint_ids) override;
 
-  // Returns a JoinDataIterator for iterating through all join data of the
-  // specified (schema_type_id, joinable_property_id).
-  //
-  // Returns:
-  //   - On success: a JoinDataIterator
-  //   - INVALID_ARGUMENT_ERROR if schema_type_id or joinable_property_id is
-  //     invalid
-  //   - Any KeyMapper/FlashIndexStorage errors
   libtextclassifier3::StatusOr<std::unique_ptr<JoinDataIteratorBase>>
   GetIterator(SchemaTypeId schema_type_id,
               JoinablePropertyId joinable_property_id) const override;
 
-  // Reduces internal file sizes by reclaiming space and ids of deleted
-  // documents. Qualified id join index will convert all entries to the new
-  // document ids and namespace ids.
-  //
-  // - document_id_old_to_new: a map for converting old document id to new
-  //   document id.
-  // - namespace_id_old_to_new: a map for converting old namespace id to new
-  //   namespace id.
-  // - new_last_added_document_id: will be used to update the last added
-  //                               document id in the qualified id join index.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error. This could potentially leave the index in
-  //     an invalid state and the caller should handle it properly (e.g. discard
-  //     and rebuild)
   libtextclassifier3::Status Optimize(
       const std::vector<DocumentId>& document_id_old_to_new,
       const std::vector<NamespaceId>& namespace_id_old_to_new,
       DocumentId new_last_added_document_id) override;
 
-  // Clears all data and set last_added_document_id to kInvalidDocumentId.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
   libtextclassifier3::Status Clear() override;
 
   bool is_v2() const override { return true; }
@@ -280,33 +242,19 @@ class QualifiedIdJoinIndexImplV2 : public QualifiedIdJoinIndex {
       const std::vector<NamespaceId>& namespace_id_old_to_new,
       QualifiedIdJoinIndexImplV2* new_index) const;
 
-  // Flushes contents of metadata file.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
-  libtextclassifier3::Status PersistMetadataToDisk(bool force) override;
+  libtextclassifier3::Status PersistMetadataToDisk() override;
 
-  // Flushes contents of all storages to underlying files.
-  //
-  // Returns:
-  //   - OK on success
-  //   - INTERNAL_ERROR on I/O error
-  libtextclassifier3::Status PersistStoragesToDisk(bool force) override;
+  libtextclassifier3::Status PersistStoragesToDisk() override;
 
-  // Computes and returns Info checksum.
-  //
-  // Returns:
-  //   - Crc of the Info on success
-  libtextclassifier3::StatusOr<Crc32> ComputeInfoChecksum(bool force) override;
+  libtextclassifier3::Status WriteMetadata() override;
 
-  // Computes and returns all storages checksum.
-  //
-  // Returns:
-  //   - Crc of all storages on success
-  //   - INTERNAL_ERROR if any data inconsistency
-  libtextclassifier3::StatusOr<Crc32> ComputeStoragesChecksum(
-      bool force) override;
+  libtextclassifier3::Status InternalWriteMetadata(const ScopedFd& sfd);
+
+  libtextclassifier3::StatusOr<Crc32> UpdateStoragesChecksum() override;
+
+  libtextclassifier3::StatusOr<Crc32> GetInfoChecksum() const override;
+
+  libtextclassifier3::StatusOr<Crc32> GetStoragesChecksum() const override;
 
   Crcs& crcs() override {
     return *reinterpret_cast<Crcs*>(metadata_buffer_.get() +
diff --git a/icing/join/qualified-id-join-index-impl-v2_test.cc b/icing/join/qualified-id-join-index-impl-v2_test.cc
index d73d6c2..d1d1b69 100644
--- a/icing/join/qualified-id-join-index-impl-v2_test.cc
+++ b/icing/join/qualified-id-join-index-impl-v2_test.cc
@@ -183,6 +183,8 @@ TEST_F(QualifiedIdJoinIndexImplV2Test,
   ICING_ASSERT_OK(index->Put(
       /*schema_type_id=*/2, /*joinable_property_id=*/1, /*document_id=*/12,
       /*ref_namespace_fingerprint_ids=*/{id4}));
+  // GetChecksum should succeed without updating the checksum.
+  ICING_EXPECT_OK(index->GetChecksum());
 
   // Without calling PersistToDisk, checksums will not be recomputed or synced
   // to disk, so initializing another instance on the same files should fail.
@@ -191,6 +193,59 @@ TEST_F(QualifiedIdJoinIndexImplV2Test,
               StatusIs(libtextclassifier3::StatusCode::FAILED_PRECONDITION));
 }
 
+TEST_F(QualifiedIdJoinIndexImplV2Test,
+       InitializationShouldSucceedWithUpdateChecksums) {
+  NamespaceFingerprintIdentifier id1(/*namespace_id=*/1, /*fingerprint=*/12);
+  NamespaceFingerprintIdentifier id2(/*namespace_id=*/1, /*fingerprint=*/34);
+  NamespaceFingerprintIdentifier id3(/*namespace_id=*/1, /*fingerprint=*/56);
+  NamespaceFingerprintIdentifier id4(/*namespace_id=*/1, /*fingerprint=*/78);
+
+  // Create new qualified id join index
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<QualifiedIdJoinIndexImplV2> index1,
+      QualifiedIdJoinIndexImplV2::Create(filesystem_, working_path_,
+                                         /*pre_mapping_fbv=*/false));
+
+  // Insert some data.
+  ICING_ASSERT_OK(index1->Put(
+      /*schema_type_id=*/2, /*joinable_property_id=*/1, /*document_id=*/5,
+      /*ref_namespace_fingerprint_ids=*/{id2, id1}));
+  ICING_ASSERT_OK(index1->Put(
+      /*schema_type_id=*/3, /*joinable_property_id=*/10, /*document_id=*/6,
+      /*ref_namespace_fingerprint_ids=*/{id3}));
+  ICING_ASSERT_OK(index1->Put(
+      /*schema_type_id=*/2, /*joinable_property_id=*/1, /*document_id=*/12,
+      /*ref_namespace_fingerprint_ids=*/{id4}));
+  ASSERT_THAT(index1, Pointee(SizeIs(4)));
+
+  // After calling UpdateChecksums, all checksums should be recomputed and
+  // synced correctly to disk, so initializing another instance on the same
+  // files should succeed, and we should be able to get the same contents.
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc, index1->GetChecksum());
+  EXPECT_THAT(index1->UpdateChecksums(), IsOkAndHolds(Eq(crc)));
+  EXPECT_THAT(index1->GetChecksum(), IsOkAndHolds(Eq(crc)));
+
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<QualifiedIdJoinIndexImplV2> index2,
+      QualifiedIdJoinIndexImplV2::Create(filesystem_, working_path_,
+                                         /*pre_mapping_fbv=*/false));
+  EXPECT_THAT(index2, Pointee(SizeIs(4)));
+  EXPECT_THAT(
+      GetJoinData(*index2, /*schema_type_id=*/2, /*joinable_property_id=*/1),
+      IsOkAndHolds(
+          ElementsAre(DocumentIdToJoinInfo<NamespaceFingerprintIdentifier>(
+                          /*document_id=*/12, /*join_info=*/id4),
+                      DocumentIdToJoinInfo<NamespaceFingerprintIdentifier>(
+                          /*document_id=*/5, /*join_info=*/id2),
+                      DocumentIdToJoinInfo<NamespaceFingerprintIdentifier>(
+                          /*document_id=*/5, /*join_info=*/id1))));
+  EXPECT_THAT(
+      GetJoinData(*index2, /*schema_type_id=*/3, /*joinable_property_id=*/10),
+      IsOkAndHolds(
+          ElementsAre(DocumentIdToJoinInfo<NamespaceFingerprintIdentifier>(
+              /*document_id=*/6, /*join_info=*/id3))));
+}
+
 TEST_F(QualifiedIdJoinIndexImplV2Test,
        InitializationShouldSucceedWithPersistToDisk) {
   NamespaceFingerprintIdentifier id1(/*namespace_id=*/1, /*fingerprint=*/12);
@@ -334,8 +389,8 @@ TEST_F(QualifiedIdJoinIndexImplV2Test,
         metadata_buffer.get() +
         QualifiedIdJoinIndexImplV2::kInfoMetadataBufferOffset);
     info->magic += kCorruptedValueOffset;
-    crcs->component_crcs.info_crc = info->ComputeChecksum().Get();
-    crcs->all_crc = crcs->component_crcs.ComputeChecksum().Get();
+    crcs->component_crcs.info_crc = info->GetChecksum().Get();
+    crcs->all_crc = crcs->component_crcs.GetChecksum().Get();
     ASSERT_THAT(filesystem_.PWrite(
                     metadata_sfd.get(), /*offset=*/0, metadata_buffer.get(),
                     QualifiedIdJoinIndexImplV2::kMetadataFileSize),
@@ -478,10 +533,10 @@ TEST_F(
         PersistentHashMapKeyMapper<PostingListIdentifier>::Create(
             filesystem_, std::move(mapper_working_path),
             /*pre_mapping_fbv=*/false));
-    ICING_ASSERT_OK_AND_ASSIGN(Crc32 old_crc, mapper->ComputeChecksum());
+    ICING_ASSERT_OK_AND_ASSIGN(Crc32 old_crc, mapper->UpdateChecksum());
     ICING_ASSERT_OK(mapper->Put("foo", PostingListIdentifier::kInvalid));
     ICING_ASSERT_OK(mapper->PersistToDisk());
-    ICING_ASSERT_OK_AND_ASSIGN(Crc32 new_crc, mapper->ComputeChecksum());
+    ICING_ASSERT_OK_AND_ASSIGN(Crc32 new_crc, mapper->UpdateChecksum());
     ASSERT_THAT(old_crc, Not(Eq(new_crc)));
   }
 
diff --git a/icing/join/qualified-id-join-index.h b/icing/join/qualified-id-join-index.h
index 4e487f9..9b40c4f 100644
--- a/icing/join/qualified-id-join-index.h
+++ b/icing/join/qualified-id-join-index.h
@@ -165,17 +165,18 @@ class QualifiedIdJoinIndex : public PersistentStorage {
       : PersistentStorage(filesystem, std::move(working_path),
                           kWorkingPathType) {}
 
-  virtual libtextclassifier3::Status PersistStoragesToDisk(
-      bool force) override = 0;
+  virtual libtextclassifier3::Status PersistStoragesToDisk() override = 0;
 
-  virtual libtextclassifier3::Status PersistMetadataToDisk(
-      bool force) override = 0;
+  virtual libtextclassifier3::Status PersistMetadataToDisk() override = 0;
 
-  virtual libtextclassifier3::StatusOr<Crc32> ComputeInfoChecksum(
-      bool force) override = 0;
+  virtual libtextclassifier3::StatusOr<Crc32> UpdateStoragesChecksum()
+      override = 0;
 
-  virtual libtextclassifier3::StatusOr<Crc32> ComputeStoragesChecksum(
-      bool force) override = 0;
+  virtual libtextclassifier3::StatusOr<Crc32> GetInfoChecksum()
+      const override = 0;
+
+  virtual libtextclassifier3::StatusOr<Crc32> GetStoragesChecksum()
+      const override = 0;
 
   virtual Crcs& crcs() override = 0;
   virtual const Crcs& crcs() const override = 0;
diff --git a/icing/join/qualified-id-join-indexing-handler_test.cc b/icing/join/qualified-id-join-indexing-handler_test.cc
index b518cae..95c0327 100644
--- a/icing/join/qualified-id-join-indexing-handler_test.cc
+++ b/icing/join/qualified-id-join-indexing-handler_test.cc
@@ -265,8 +265,9 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest, HandleJoinableProperty) {
           .SetSchema(std::string(kReferencedType))
           .AddStringProperty(std::string(kPropertyName), "one")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId ref_doc_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              doc_store_->Put(referenced_document));
+  DocumentId ref_doc_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       NamespaceId ref_doc_ns_id,
       doc_store_->GetNamespaceId(referenced_document.namespace_()));
@@ -283,7 +284,8 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest, HandleJoinableProperty) {
           .AddStringProperty(std::string(kPropertyQualifiedId),
                              "pkg$db/ns#ref_type/1")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId doc_id, doc_store_->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(put_result, doc_store_->Put(document));
+  DocumentId doc_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       TokenizedDocument tokenized_document,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
@@ -323,8 +325,9 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest, HandleNestedJoinableProperty) {
           .SetSchema(std::string(kReferencedType))
           .AddStringProperty(std::string(kPropertyName), "one")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId ref_doc_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store_->Put(referenced_document1));
+  DocumentId ref_doc_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       NamespaceId ref_doc_ns_id1,
       doc_store_->GetNamespaceId(referenced_document1.namespace_()));
@@ -342,8 +345,9 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest, HandleNestedJoinableProperty) {
           .SetSchema(std::string(kReferencedType))
           .AddStringProperty(std::string(kPropertyName), "two")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId ref_doc_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              doc_store_->Put(referenced_document2));
+  DocumentId ref_doc_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       NamespaceId ref_doc_ns_id2,
       doc_store_->GetNamespaceId(referenced_document2.namespace_()));
@@ -373,8 +377,9 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest, HandleNestedJoinableProperty) {
           .AddStringProperty(std::string(kPropertyQualifiedId2),
                              "pkg$db/ns#ref_type/1")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId doc_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              doc_store_->Put(nested_document));
+  DocumentId doc_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       TokenizedDocument tokenized_document,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
@@ -436,7 +441,9 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest,
           .AddStringProperty(std::string(kPropertyQualifiedId),
                              std::string(kInvalidFormatQualifiedId))
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId doc_id, doc_store_->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                             doc_store_->Put(document));
+  DocumentId doc_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       TokenizedDocument tokenized_document,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
@@ -478,7 +485,9 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest,
               std::string(kPropertyQualifiedId),
               absl_ports::StrCat(kUnknownNamespace, "#", "ref_type/1"))
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId doc_id, doc_store_->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                             doc_store_->Put(document));
+  DocumentId doc_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       TokenizedDocument tokenized_document,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
@@ -512,7 +521,9 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest, HandleShouldSkipEmptyQualifiedId) {
                                .SetKey("icing", "fake_type/1")
                                .SetSchema(std::string(kFakeType))
                                .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId doc_id, doc_store_->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                             doc_store_->Put(document));
+  DocumentId doc_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       TokenizedDocument tokenized_document,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
@@ -552,8 +563,9 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest,
           .SetSchema(std::string(kReferencedType))
           .AddStringProperty(std::string(kPropertyName), "one")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId ref_doc_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              doc_store_->Put(referenced_document));
+  DocumentId ref_doc_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       NamespaceId ref_doc_ns_id,
       doc_store_->GetNamespaceId(referenced_document.namespace_()));
@@ -624,8 +636,9 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest,
           .SetSchema(std::string(kReferencedType))
           .AddStringProperty(std::string(kPropertyName), "one")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId ref_doc_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              doc_store_->Put(referenced_document));
+  DocumentId ref_doc_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       NamespaceId ref_doc_ns_id,
       doc_store_->GetNamespaceId(referenced_document.namespace_()));
@@ -642,7 +655,8 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest,
           .AddStringProperty(std::string(kPropertyQualifiedId),
                              "pkg$db/ns#ref_type/1")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId doc_id, doc_store_->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(put_result, doc_store_->Put(document));
+  DocumentId doc_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       TokenizedDocument tokenized_document,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
@@ -700,8 +714,9 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest,
           .SetSchema(std::string(kReferencedType))
           .AddStringProperty(std::string(kPropertyName), "one")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId ref_doc_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              doc_store_->Put(referenced_document));
+  DocumentId ref_doc_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       NamespaceId ref_doc_ns_id,
       doc_store_->GetNamespaceId(referenced_document.namespace_()));
@@ -718,7 +733,8 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest,
           .AddStringProperty(std::string(kPropertyQualifiedId),
                              "pkg$db/ns#ref_type/1")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId doc_id, doc_store_->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(put_result, doc_store_->Put(document));
+  DocumentId doc_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       TokenizedDocument tokenized_document,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
@@ -758,8 +774,9 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest,
           .SetSchema(std::string(kReferencedType))
           .AddStringProperty(std::string(kPropertyName), "one")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId ref_doc_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              doc_store_->Put(referenced_document));
+  DocumentId ref_doc_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       NamespaceId ref_doc_ns_id,
       doc_store_->GetNamespaceId(referenced_document.namespace_()));
@@ -776,7 +793,8 @@ TEST_F(QualifiedIdJoinIndexingHandlerTest,
           .AddStringProperty(std::string(kPropertyQualifiedId),
                              "pkg$db/ns#ref_type/1")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId doc_id, doc_store_->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(put_result, doc_store_->Put(document));
+  DocumentId doc_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       TokenizedDocument tokenized_document,
       TokenizedDocument::Create(schema_store_.get(), lang_segmenter_.get(),
diff --git a/icing/legacy/index/icing-array-storage.cc b/icing/legacy/index/icing-array-storage.cc
index de5178a..fcd6b68 100644
--- a/icing/legacy/index/icing-array-storage.cc
+++ b/icing/legacy/index/icing-array-storage.cc
@@ -17,13 +17,16 @@
 #include <sys/mman.h>
 
 #include <algorithm>
-#include <cinttypes>
+#include <cstddef>
+#include <cstdint>
+#include <cstring>
 
 #include "icing/legacy/core/icing-string-util.h"
 #include "icing/legacy/core/icing-timer.h"
 #include "icing/legacy/index/icing-bit-util.h"
 #include "icing/legacy/index/icing-filesystem.h"
 #include "icing/legacy/index/icing-mmapper.h"
+#include "icing/util/crc32.h"
 #include "icing/util/logging.h"
 
 using std::max;
@@ -65,13 +68,15 @@ bool IcingArrayStorage::Init(int fd, size_t fd_offset, bool map_shared,
     return false;
   }
   if (file_size < fd_offset) {
-    ICING_LOG(ERROR) << "Array storage file size " << file_size << " less than offset " << fd_offset;
+    ICING_LOG(ERROR) << "Array storage file size " << file_size
+                     << " less than offset " << fd_offset;
     return false;
   }
 
   uint32_t capacity_num_elts = (file_size - fd_offset) / elt_size;
   if (capacity_num_elts < num_elts) {
-    ICING_LOG(ERROR) << "Array storage num elts " << num_elts << " > capacity num elts " << capacity_num_elts;
+    ICING_LOG(ERROR) << "Array storage num elts " << num_elts
+                     << " > capacity num elts " << capacity_num_elts;
     return false;
   }
 
@@ -104,7 +109,8 @@ bool IcingArrayStorage::Init(int fd, size_t fd_offset, bool map_shared,
     if (init_crc) {
       *crc_ptr_ = crc;
     } else if (crc != *crc_ptr_) {
-      ICING_LOG(ERROR) << "Array storage bad crc " << crc << " vs " << *crc_ptr_;
+      ICING_LOG(ERROR) << "Array storage bad crc " << crc << " vs "
+                       << *crc_ptr_;
       goto failed;
     }
   }
@@ -199,15 +205,17 @@ bool IcingArrayStorage::GrowIfNecessary(uint32_t num_elts) {
   uint64_t new_file_size = fd_offset_ + uint64_t{num_elts} * elt_size_;
   // Grow to kGrowElts boundary.
   new_file_size = AlignUp(new_file_size, kGrowElts * elt_size_);
-  if (!filesystem_.Grow(fd_, new_file_size)) {
+  if (!filesystem_.GrowUsingPWrite(fd_, new_file_size)) {
     return false;
   }
   capacity_num_ = (new_file_size - fd_offset_) / elt_size_;
   return true;
 }
 
-void IcingArrayStorage::UpdateCrc() {
-  if (!crc_ptr_) return;
+Crc32 IcingArrayStorage::UpdateCrc() {
+  if (!crc_ptr_) {
+    return Crc32();
+  }
 
   // First apply the modified area. Keep a bitmap of already updated
   // regions so we don't double-update.
@@ -272,8 +280,8 @@ void IcingArrayStorage::UpdateCrc() {
   }
   if (!changes_.empty()) {
     ICING_VLOG(2) << "Array update partial crcs " << num_partial_crcs
-        << " truncated " << num_truncated << " overlapped " << num_overlapped
-        << " duplicate " << num_duplicate;
+                  << " truncated " << num_truncated << " overlapped "
+                  << num_overlapped << " duplicate " << num_duplicate;
   }
 
   // Now update with grown area.
@@ -281,7 +289,8 @@ void IcingArrayStorage::UpdateCrc() {
     cur_crc = IcingStringUtil::UpdateCrc32(
         cur_crc, array_cast<char>() + changes_end_ * elt_size_,
         (cur_num_ - changes_end_) * elt_size_);
-    ICING_VLOG(2) << "Array update tail crc offset " << changes_end_ << " -> " << cur_num_;
+    ICING_VLOG(2) << "Array update tail crc offset " << changes_end_ << " -> "
+                  << cur_num_;
   }
 
   // Clear, now that we've applied changes.
@@ -291,6 +300,21 @@ void IcingArrayStorage::UpdateCrc() {
 
   // Commit new crc.
   *crc_ptr_ = cur_crc;
+  return Crc32(cur_crc);
+}
+
+Crc32 IcingArrayStorage::GetCrc() const {
+  if (!crc_ptr_) {
+    return Crc32();
+  }
+  // TODO(b/352778910): Mirror the same logic in UpdateCrc() to reduce the
+  // cost of GetCrc().
+  if (changes_.empty() && changes_end_ == cur_num_) {
+    // No changes, just return the crc.
+    return Crc32(*crc_ptr_);
+  }
+  Crc32 crc(std::string_view(array_cast<char>(), cur_num_ * elt_size_));
+  return crc;
 }
 
 void IcingArrayStorage::Warm() const {
@@ -315,6 +339,7 @@ void IcingArrayStorage::Clear() {
 // flushed to the underlying file, but strangely a sync isn't done.
 // If map_shared_ is true, then we call sync.
 uint32_t IcingArrayStorage::Sync() {
+  UpdateCrc();
   if (!map_shared_) {
     IcingTimer timer;
     uint32_t num_flushed = 0;     // pages flushed
@@ -335,7 +360,8 @@ uint32_t IcingArrayStorage::Sync() {
         if (pwrite(fd_, array() + dirty_start, dirty_end - dirty_start,
                    fd_offset_ + dirty_start) !=
             static_cast<ssize_t>(dirty_end - dirty_start)) {
-          ICING_LOG(ERROR) << "Flushing pages failed (" << dirty_start << ", " << dirty_end << ")";
+          ICING_LOG(ERROR) << "Flushing pages failed (" << dirty_start << ", "
+                           << dirty_end << ")";
         }
         in_dirty = false;
       } else if (!in_dirty && is_dirty) {
@@ -354,7 +380,8 @@ uint32_t IcingArrayStorage::Sync() {
       if (pwrite(fd_, array() + dirty_start, dirty_end - dirty_start,
                  fd_offset_ + dirty_start) !=
           static_cast<ssize_t>(dirty_end - dirty_start)) {
-        ICING_LOG(ERROR) << "Flushing pages failed (" << dirty_start << ", " << dirty_end << ")";
+        ICING_LOG(ERROR) << "Flushing pages failed (" << dirty_start << ", "
+                         << dirty_end << ")";
       }
     }
 
@@ -369,7 +396,9 @@ uint32_t IcingArrayStorage::Sync() {
     }
 
     if (num_flushed > 0) {
-      ICING_VLOG(1) << "Flushing " << num_flushed << "/" << dirty_pages_size << " " << num_contiguous << " contiguous pages in " << timer.Elapsed() * 1000 << "ms.";
+      ICING_VLOG(1) << "Flushing " << num_flushed << "/" << dirty_pages_size
+                    << " " << num_contiguous << " contiguous pages in "
+                    << timer.Elapsed() * 1000 << "ms.";
     }
 
     return num_flushed;
diff --git a/icing/legacy/index/icing-array-storage.h b/icing/legacy/index/icing-array-storage.h
index 0d93172..6932f8b 100644
--- a/icing/legacy/index/icing-array-storage.h
+++ b/icing/legacy/index/icing-array-storage.h
@@ -26,6 +26,7 @@
 
 #include "icing/legacy/index/icing-filesystem.h"
 #include "icing/legacy/index/icing-mmapper.h"
+#include "icing/util/crc32.h"
 
 namespace icing {
 namespace lib {
@@ -82,7 +83,11 @@ class IcingArrayStorage {
   void Truncate(uint32_t len);
 
   // Push changes to crc into crc_ptr. No effect if crc_ptr is NULL.
-  void UpdateCrc();
+  Crc32 UpdateCrc();
+
+  // Returns the crc of the current content or 0 if crc_ptr is NULL. Does not
+  // modify crc_ptr.
+  Crc32 GetCrc() const;
 
   // Write and sync dirty pages to fd starting at offset. Returns
   // number of pages synced.
diff --git a/icing/legacy/index/icing-array-storage_test.cc b/icing/legacy/index/icing-array-storage_test.cc
new file mode 100644
index 0000000..690c356
--- /dev/null
+++ b/icing/legacy/index/icing-array-storage_test.cc
@@ -0,0 +1,170 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include "icing/legacy/index/icing-array-storage.h"
+
+#include <cstdint>
+#include <string>
+
+#include "gmock/gmock.h"
+#include "gtest/gtest.h"
+#include "icing/legacy/index/icing-filesystem.h"
+#include "icing/testing/tmp-directory.h"
+#include "icing/util/crc32.h"
+
+namespace icing {
+namespace lib {
+
+namespace {
+
+using testing::Eq;
+
+class IcingArrayStorageTest : public ::testing::Test {
+ protected:
+  void SetUp() override {
+    array_files_dir_ = GetTestTempDir() + "/array_files";
+    filesystem_.CreateDirectoryRecursively(array_files_dir_.c_str());
+    array_file_ = array_files_dir_ + "/array_file";
+  }
+
+  void TearDown() override {
+    filesystem_.DeleteDirectoryRecursively(array_files_dir_.c_str());
+  }
+
+  IcingFilesystem filesystem_;
+  std::string array_files_dir_;
+  std::string array_file_;
+};
+
+TEST_F(IcingArrayStorageTest, UpdateCrcNoCrcPtr) {
+  IcingArrayStorage storage(filesystem_);
+  IcingScopedFd sfd(filesystem_.OpenForWrite(array_file_.c_str()));
+  ASSERT_TRUE(sfd.is_valid());
+  ASSERT_TRUE(storage.Init(sfd.get(), /*fd_offset=*/0, /*map_shared=*/true,
+                           /*elt_size=*/sizeof(uint32_t), /*num_elts=*/0,
+                           /*max_num_elts=*/1000, /*crc_ptr=*/nullptr,
+                           /*init_crc=*/false));
+
+  // Initial Crc should be 0.
+  EXPECT_THAT(storage.GetCrc(), Eq(Crc32()));
+  EXPECT_THAT(storage.UpdateCrc(), Eq(Crc32()));
+  EXPECT_THAT(storage.GetCrc(), Eq(Crc32()));
+
+  uint32_t* val = storage.GetMutableMem<uint32_t>(0, 1);
+  *val = 5;
+
+  // Because there is no crc_ptr, the crc should remain 0 even though we have
+  // added content.
+  EXPECT_THAT(storage.GetCrc(), Eq(Crc32()));
+  EXPECT_THAT(storage.UpdateCrc(), Eq(Crc32()));
+  EXPECT_THAT(storage.GetCrc(), Eq(Crc32()));
+}
+
+TEST_F(IcingArrayStorageTest, UpdateCrc) {
+  IcingArrayStorage storage(filesystem_);
+  uint32_t crc = 0;
+  IcingScopedFd sfd(filesystem_.OpenForWrite(array_file_.c_str()));
+  ASSERT_TRUE(sfd.is_valid());
+  ASSERT_TRUE(storage.Init(sfd.get(), /*fd_offset=*/0, /*map_shared=*/true,
+                           /*elt_size=*/sizeof(uint32_t), /*num_elts=*/0,
+                           /*max_num_elts=*/1000, &crc, /*init_crc=*/false));
+
+  // Initial Crc should be 0.
+  EXPECT_THAT(storage.GetCrc(), Eq(Crc32()));
+  EXPECT_THAT(storage.UpdateCrc(), Eq(Crc32()));
+  EXPECT_THAT(storage.GetCrc(), Eq(Crc32()));
+
+  uint32_t* val = storage.GetMutableMem<uint32_t>(0, 1);
+  *val = 5;
+
+  EXPECT_THAT(storage.GetCrc(), Eq(Crc32(937357362)));
+  EXPECT_THAT(storage.UpdateCrc(), Eq(Crc32(937357362)));
+  EXPECT_THAT(storage.GetCrc(), Eq(Crc32(937357362)));
+}
+
+TEST_F(IcingArrayStorageTest, GetCrcDoesNotUpdateHeader) {
+  uint32_t crc = 0;
+  IcingScopedFd sfd(filesystem_.OpenForWrite(array_file_.c_str()));
+  ASSERT_TRUE(sfd.is_valid());
+
+  {
+    IcingArrayStorage storage_one(filesystem_);
+    ASSERT_TRUE(
+        storage_one.Init(sfd.get(), /*fd_offset=*/0, /*map_shared=*/true,
+                         /*elt_size=*/sizeof(uint32_t), /*num_elts=*/0,
+                         /*max_num_elts=*/1000, &crc, /*init_crc=*/false));
+
+    // Initial Crc should be 0.
+    EXPECT_THAT(storage_one.GetCrc(), Eq(Crc32()));
+    EXPECT_THAT(storage_one.UpdateCrc(), Eq(Crc32()));
+    EXPECT_THAT(storage_one.GetCrc(), Eq(Crc32()));
+
+    uint32_t* val = storage_one.GetMutableMem<uint32_t>(0, 1);
+    *val = 5;
+
+    EXPECT_THAT(storage_one.GetCrc(), Eq(Crc32(937357362)));
+    EXPECT_THAT(crc, Eq(0));
+  }
+
+  {
+    IcingArrayStorage storage_two(filesystem_);
+    // Init should fail because the updated checksum was never written to crc.
+    ASSERT_FALSE(
+        storage_two.Init(sfd.get(), /*fd_offset=*/0, /*map_shared=*/true,
+                         /*elt_size=*/sizeof(uint32_t), /*num_elts=*/1,
+                         /*max_num_elts=*/1000, &crc, /*init_crc=*/false));
+  }
+}
+
+TEST_F(IcingArrayStorageTest, UpdateCrcDoesUpdateHeader) {
+  uint32_t crc = 0;
+  IcingScopedFd sfd(filesystem_.OpenForWrite(array_file_.c_str()));
+  ASSERT_TRUE(sfd.is_valid());
+
+  {
+    IcingArrayStorage storage_one(filesystem_);
+    ASSERT_TRUE(
+        storage_one.Init(sfd.get(), /*fd_offset=*/0, /*map_shared=*/true,
+                         /*elt_size=*/sizeof(uint32_t), /*num_elts=*/0,
+                         /*max_num_elts=*/1000, &crc, /*init_crc=*/false));
+
+    // Initial Crc should be 0.
+    EXPECT_THAT(storage_one.GetCrc(), Eq(Crc32()));
+    EXPECT_THAT(storage_one.UpdateCrc(), Eq(Crc32()));
+    EXPECT_THAT(storage_one.GetCrc(), Eq(Crc32()));
+
+    uint32_t* val = storage_one.GetMutableMem<uint32_t>(0, 1);
+    *val = 5;
+
+    EXPECT_THAT(storage_one.GetCrc(), Eq(Crc32(937357362)));
+    EXPECT_THAT(storage_one.UpdateCrc(), Eq(Crc32(937357362)));
+    EXPECT_THAT(storage_one.GetCrc(), Eq(Crc32(937357362)));
+    EXPECT_THAT(crc, Eq(937357362));
+  }
+
+  {
+    IcingArrayStorage storage_two(filesystem_);
+    // Init should fail because the updated checksum was never written to crc.
+    ASSERT_TRUE(
+        storage_two.Init(sfd.get(), /*fd_offset=*/0, /*map_shared=*/true,
+                         /*elt_size=*/sizeof(uint32_t), /*num_elts=*/1,
+                         /*max_num_elts=*/1000, &crc, /*init_crc=*/false));
+    EXPECT_THAT(storage_two.GetCrc().Get(), Eq(937357362));
+  }
+}
+
+}  // namespace
+
+}  // namespace lib
+}  // namespace icing
\ No newline at end of file
diff --git a/icing/legacy/index/icing-dynamic-trie.cc b/icing/legacy/index/icing-dynamic-trie.cc
index 378b666..4514385 100644
--- a/icing/legacy/index/icing-dynamic-trie.cc
+++ b/icing/legacy/index/icing-dynamic-trie.cc
@@ -73,10 +73,14 @@
 #include <cstdint>
 #include <cstring>
 #include <memory>
+#include <ostream>
+#include <string>
+#include <string_view>
 #include <utility>
+#include <vector>
 
+#include "icing/text_classifier/lib3/utils/base/status.h"
 #include "icing/absl_ports/canonical_errors.h"
-#include "icing/legacy/core/icing-packed-pod.h"
 #include "icing/legacy/core/icing-string-util.h"
 #include "icing/legacy/core/icing-timer.h"
 #include "icing/legacy/index/icing-array-storage.h"
@@ -84,23 +88,17 @@
 #include "icing/legacy/index/icing-flash-bitmap.h"
 #include "icing/legacy/index/icing-mmapper.h"
 #include "icing/legacy/index/proto/icing-dynamic-trie-header.pb.h"
+#include "icing/util/crc32.h"
 #include "icing/util/i18n-utils.h"
 #include "icing/util/logging.h"
 #include "icing/util/math-util.h"
 #include "icing/util/status-macros.h"
 
-using std::inplace_merge;
-using std::lower_bound;
-using std::max;
-using std::mismatch;
-using std::pair;
-using std::sort;
-using std::vector;
-
 namespace icing {
 namespace lib {
 
 namespace {
+
 constexpr uint32_t kInvalidNodeIndex = (1U << 24) - 1;
 constexpr uint32_t kInvalidNextIndex = ~0U;
 
@@ -108,6 +106,17 @@ void ResetMutableNext(IcingDynamicTrie::Next &mutable_next) {
   mutable_next.set_val(0xff);
   mutable_next.set_node_index(kInvalidNodeIndex);
 }
+
+// Helper function to check that there is no termination character '\0' in the
+// key.
+bool IsKeyValid(std::string_view key) {
+  return key.find('\0') == std::string_view::npos;  // NOLINT
+}
+
+char GetCharOrNull(std::string_view s, int pos) {
+  return (pos < s.size()) ? s[pos] : '\0';
+}
+
 }  // namespace
 
 // Based on the bit field widths.
@@ -215,7 +224,7 @@ class IcingDynamicTrie::CandidateSet {
 
   bool empty() const { return candidates_.empty(); }
 
-  void Release(vector<OriginalMatch> *ret) {
+  void Release(std::vector<OriginalMatch> *ret) {
     if (!empty()) {
       ICING_LOG(FATAL) << "Candidate set not empty before releasing matches";
     }
@@ -231,9 +240,9 @@ class IcingDynamicTrie::CandidateSet {
   const bool prefix_;
 
   std::string cur_prefix_;
-  vector<Candidate> candidates_;
+  std::vector<Candidate> candidates_;
 
-  vector<IcingDynamicTrie::OriginalMatch> matches_;
+  std::vector<IcingDynamicTrie::OriginalMatch> matches_;
 };
 
 // Options.
@@ -300,7 +309,10 @@ class IcingDynamicTrie::IcingDynamicTrieStorage {
   char *GetMutableSuffix(uint32_t idx, uint32_t len);
 
   // Update crcs based on current contents. Returns all_crc or kNoCrc.
-  uint32_t UpdateCrc();
+  Crc32 UpdateCrc();
+
+  // Calculates the current crc and returns it.
+  Crc32 GetCrc() const;
 
   // Allocators.
   uint32_t nodes_left() const;
@@ -313,7 +325,7 @@ class IcingDynamicTrie::IcingDynamicTrieStorage {
   libtextclassifier3::StatusOr<Next *> AllocNextArray(int size);
   void FreeNextArray(Next *next, int log2_size);
   // REQUIRES: suffixes_left() >= strlen(suffix) + 1 + value_size()
-  uint32_t MakeSuffix(const char *suffix, const void *value,
+  uint32_t MakeSuffix(std::string_view suffix, const void *value,
                       uint32_t *value_index);
 
   const IcingDynamicTrieHeader &hdr() const { return hdr_.hdr; }
@@ -334,14 +346,28 @@ class IcingDynamicTrie::IcingDynamicTrieStorage {
   // filename is the header and the rest correspond to ArrayType enum
   // values.
   static void GetFilenames(const std::string &file_basename,
-                           vector<std::string> *filenames);
+                           std::vector<std::string> *filenames);
   static std::string GetHeaderFilename(const std::string &file_basename);
 
-  uint32_t GetHeaderCrc() const;
+  // TODO(b/353398502) Improve the handling of the header to avoid this weird
+  // pattern with both an mmapped header and an in-memory header.
+  // Calculates the crc of the header as it is represented in the
+  // header_mmapper_.
+  Crc32 GetWrittenHeaderCrc() const;
+
+  // Calculates the crc of the hdr_.
+  //
+  // This value will deviate from GetWrittenHeaderCrc() if the header is
+  // modified after initialization or the last call to WriteHeader().
+  //
+  // NOTE: This function will need to serialize hdr_ to calculate the crc.
+  // Therefore, if given the choice, prefer to use GetWrittenHeaderCrc().
+  Crc32 GetHeaderCrc() const;
 
-  uint32_t GetAllCrc() const;
+  // Returns the crc of the cached crcs.
+  Crc32 GetCachedCrc() const;
 
-  uint32_t UpdateCrcInternal(bool write_hdr);
+  Crc32 UpdateCrcInternal(bool write_hdr);
 
   // Initializes hdr_ with options and writes the resulting header to disk.
   bool CreateNewHeader(IcingScopedFd sfd, const Options &options);
@@ -383,12 +409,14 @@ class IcingDynamicTrie::IcingDynamicTrieStorage {
     uint32_t header_crc;
     uint32_t array_crcs[NUM_ARRAY_TYPES];
   };
-  Crcs *crcs_;
+  Crcs *cached_crcs_;
 
   static uint32_t serialized_header_max() {
     return IcingMMapper::system_page_size() - sizeof(Crcs);
   }
 
+  static Crc32 GetAllCrcs(const Crcs &crcs);
+
   RuntimeOptions runtime_options_;
 
   // Info kept about each array (NODE, NEXT, SUFFIX) to manage
@@ -405,7 +433,7 @@ IcingDynamicTrie::IcingDynamicTrieStorage::IcingDynamicTrieStorage(
     const IcingFilesystem *filesystem)
     : file_basename_(file_basename),
       hdr_mmapper_(false, MAP_SHARED),
-      crcs_(nullptr),
+      cached_crcs_(nullptr),
       runtime_options_(runtime_options),
       array_storage_(NUM_ARRAY_TYPES, IcingArrayStorage(*filesystem)),
       filesystem_(filesystem) {}
@@ -419,7 +447,7 @@ IcingDynamicTrie::IcingDynamicTrieStorage::~IcingDynamicTrieStorage() {
 }
 
 void IcingDynamicTrie::IcingDynamicTrieStorage::GetFilenames(
-    const std::string &file_basename, vector<std::string> *filenames) {
+    const std::string &file_basename, std::vector<std::string> *filenames) {
   const char *kArrayFilenameSuffixes[NUM_ARRAY_TYPES] = {
       ".n",
       ".x",
@@ -445,7 +473,7 @@ bool IcingDynamicTrie::IcingDynamicTrieStorage::Init() {
       runtime_options_.storage_policy == RuntimeOptions::kMapSharedWithCrc;
 
   // Open files.
-  vector<std::string> filenames;
+  std::vector<std::string> filenames;
   GetFilenames(file_basename_, &filenames);
   for (size_t i = 0; i < filenames.size(); i++) {
     uint64_t file_size = filesystem_->GetFileSize(filenames[i].c_str());
@@ -479,17 +507,19 @@ bool IcingDynamicTrie::IcingDynamicTrieStorage::Init() {
   }
 
   // Point crcs_ to correct region.
-  crcs_ = reinterpret_cast<Crcs *>(hdr_mmapper_.address() +
-                                   serialized_header_max());
-  if (crcs_->header_crc == kNoCrc) {
+  cached_crcs_ = reinterpret_cast<Crcs *>(hdr_mmapper_.address() +
+                                          serialized_header_max());
+  // Header hasn't been initialized yet. So we should check what's actually
+  // written to checksum the header.
+  if (cached_crcs_->header_crc == kNoCrc) {
     // Create crcs.
-    crcs_->header_crc = GetHeaderCrc();
+    cached_crcs_->header_crc = GetWrittenHeaderCrc().Get();
 
     // Do the same for the arrays.
     init_crcs = true;
   } else {
     // Verify crc.
-    if (crcs_->header_crc != GetHeaderCrc()) {
+    if (cached_crcs_->header_crc != GetWrittenHeaderCrc().Get()) {
       ICING_LOG(ERROR) << "Trie header crc failed";
       goto failed;
     }
@@ -506,34 +536,35 @@ bool IcingDynamicTrie::IcingDynamicTrieStorage::Init() {
   // We have the header set up. Now read in the arrays.
   if (!array_storage_[NODE].Init(array_fds_[NODE].get(), 0, map_shared,
                                  sizeof(Node), hdr_.hdr.num_nodes(),
-                                 hdr_.hdr.max_nodes(), &crcs_->array_crcs[NODE],
-                                 init_crcs)) {
+                                 hdr_.hdr.max_nodes(),
+                                 &cached_crcs_->array_crcs[NODE], init_crcs)) {
     ICING_LOG(ERROR) << "Trie mmap node failed";
     goto failed;
   }
 
   if (!array_storage_[NEXT].Init(array_fds_[NEXT].get(), 0, map_shared,
                                  sizeof(Next), hdr_.hdr.num_nexts(),
-                                 hdr_.hdr.max_nexts(), &crcs_->array_crcs[NEXT],
-                                 init_crcs)) {
+                                 hdr_.hdr.max_nexts(),
+                                 &cached_crcs_->array_crcs[NEXT], init_crcs)) {
     ICING_LOG(ERROR) << "Trie mmap next failed";
     goto failed;
   }
 
-  if (!array_storage_[SUFFIX].Init(array_fds_[SUFFIX].get(), 0, map_shared,
-                                   sizeof(char), hdr_.hdr.suffixes_size(),
-                                   hdr_.hdr.max_suffixes_size(),
-                                   &crcs_->array_crcs[SUFFIX], init_crcs)) {
+  if (!array_storage_[SUFFIX].Init(
+          array_fds_[SUFFIX].get(), 0, map_shared, sizeof(char),
+          hdr_.hdr.suffixes_size(), hdr_.hdr.max_suffixes_size(),
+          &cached_crcs_->array_crcs[SUFFIX], init_crcs)) {
     ICING_LOG(ERROR) << "Trie mmap suffix failed";
     goto failed;
   }
 
-  // Overall crc.
+  // All of the cached crcs are now up to date. Either calculate+set the all crc
+  // or calculate+check the all crc.
   if (init_crcs) {
-    crcs_->all_crc = GetAllCrc();
+    cached_crcs_->all_crc = GetCachedCrc().Get();
   } else {
     // Verify crc.
-    if (crcs_->all_crc != GetAllCrc()) {
+    if (cached_crcs_->all_crc != GetCachedCrc().Get()) {
       ICING_LOG(ERROR) << "Trie all crc failed";
       goto failed;
     }
@@ -542,7 +573,7 @@ bool IcingDynamicTrie::IcingDynamicTrieStorage::Init() {
   return true;
 
 failed:
-  crcs_ = nullptr;
+  cached_crcs_ = nullptr;
   hdr_mmapper_.Unmap();
   hdr_.Invalidate();
   for (int i = 0; i < NUM_ARRAY_TYPES; i++) {
@@ -555,7 +586,7 @@ failed:
 
 bool IcingDynamicTrie::IcingDynamicTrieStorage::CreateIfNotExist(
     const Options &options) {
-  vector<std::string> filenames;
+  std::vector<std::string> filenames;
   GetFilenames(file_basename_, &filenames);
 
   // Check already exists. Just header file check is enough.
@@ -615,7 +646,7 @@ bool IcingDynamicTrie::IcingDynamicTrieStorage::CreateNewHeader(
 bool IcingDynamicTrie::IcingDynamicTrieStorage::Remove(
     const std::string &file_basename, const IcingFilesystem &filesystem) {
   bool success = true;
-  vector<std::string> files;
+  std::vector<std::string> files;
   GetFilenames(file_basename, &files);
   for (size_t i = 0; i < files.size(); i++) {
     if (!filesystem.DeleteFile(files[i].c_str())) {
@@ -780,55 +811,91 @@ void IcingDynamicTrie::IcingDynamicTrieStorage::FreeNextArray(Next *next,
 }
 
 uint32_t IcingDynamicTrie::IcingDynamicTrieStorage::MakeSuffix(
-    const char *suffix, const void *value, uint32_t *value_index) {
-  int suffix_len = strlen(suffix);
-  if (suffixes_left() < suffix_len + 1 + value_size()) {
+    std::string_view suffix, const void *value, uint32_t *value_index) {
+  if (suffixes_left() < suffix.size() + 1 + value_size()) {
     ICING_LOG(FATAL) << "'suffix' buffer not enough";
   }
 
-  char *start =
-      GetMutableSuffix(hdr_.hdr.suffixes_size(), suffix_len + 1 + value_size());
-  memcpy(start, suffix, suffix_len + 1);
-  memcpy(start + suffix_len + 1, value, value_size());
-  if (value_index) *value_index = GetSuffixIndex(start + suffix_len + 1);
-  hdr_.hdr.set_suffixes_size(hdr_.hdr.suffixes_size() + suffix_len + 1 +
+  char *start = GetMutableSuffix(hdr_.hdr.suffixes_size(),
+                                 suffix.size() + 1 + value_size());
+  // Copy suffix.
+  memcpy(start, suffix.data(), suffix.size());
+  // Set a '\0' after suffix.
+  memset(start + suffix.size(), /*value=*/0, /*num=*/1);
+  // Copy value.
+  memcpy(start + suffix.size() + 1, value, value_size());
+  if (value_index) *value_index = GetSuffixIndex(start + suffix.size() + 1);
+  hdr_.hdr.set_suffixes_size(hdr_.hdr.suffixes_size() + suffix.size() + 1 +
                              value_size());
 
   return GetSuffixIndex(start);
 }
 
-uint32_t IcingDynamicTrie::IcingDynamicTrieStorage::GetHeaderCrc() const {
-  return IcingStringUtil::UpdateCrc32(
-      0, reinterpret_cast<const char *>(hdr_mmapper_.address()),
-      serialized_header_max());
+Crc32 IcingDynamicTrie::IcingDynamicTrieStorage::GetWrittenHeaderCrc() const {
+  std::string_view data(reinterpret_cast<const char *>(hdr_mmapper_.address()),
+                        serialized_header_max());
+  return Crc32(data);
+}
+
+Crc32 IcingDynamicTrie::IcingDynamicTrieStorage::GetHeaderCrc() const {
+  // Create a buffer that is the same as the mmapped header.
+  auto hdr_data = std::make_unique<uint8_t[]>(serialized_header_max());
+  std::memcpy(hdr_data.get(), hdr_mmapper_.address(), serialized_header_max());
+
+  // Serialize the in-memory header to the buffer and then checksum it.
+  hdr_.SerializeToArray(hdr_data.get(), serialized_header_max());
+  std::string_view data(reinterpret_cast<const char *>(hdr_data.get()),
+                        serialized_header_max());
+  return Crc32(data);
+}
+
+Crc32 IcingDynamicTrie::IcingDynamicTrieStorage::GetCachedCrc() const {
+  return GetAllCrcs(*cached_crcs_);
 }
 
-uint32_t IcingDynamicTrie::IcingDynamicTrieStorage::GetAllCrc() const {
+/*static*/ Crc32 IcingDynamicTrie::IcingDynamicTrieStorage::GetAllCrcs(
+    const IcingDynamicTrie::IcingDynamicTrieStorage::Crcs &crcs) {
   // Append array crcs to header crc.
-  return IcingStringUtil::UpdateCrc32(
-      crcs_->header_crc, reinterpret_cast<const char *>(crcs_->array_crcs),
-      sizeof(crcs_->array_crcs));
+  Crc32 crc(crcs.header_crc);
+  std::string_view data(reinterpret_cast<const char *>(crcs.array_crcs),
+                        sizeof(cached_crcs_->array_crcs));
+  crc.Append(data);
+  return crc;
 }
 
-uint32_t IcingDynamicTrie::IcingDynamicTrieStorage::UpdateCrc() {
+Crc32 IcingDynamicTrie::IcingDynamicTrieStorage::GetCrc() const {
+  // crcs_ holds cached values. We want the crcs that represent the content *as
+  // it exists now*.
+  Crcs crcs;
+  crcs.header_crc = GetHeaderCrc().Get();
+  for (int i = 0; i < NUM_ARRAY_TYPES; i++) {
+    crcs.array_crcs[i] = array_storage_[i].GetCrc().Get();
+  }
+  return GetAllCrcs(crcs);
+}
+
+Crc32 IcingDynamicTrie::IcingDynamicTrieStorage::UpdateCrc() {
   return UpdateCrcInternal(true);
 }
 
-uint32_t IcingDynamicTrie::IcingDynamicTrieStorage::UpdateCrcInternal(
+Crc32 IcingDynamicTrie::IcingDynamicTrieStorage::UpdateCrcInternal(
     bool write_hdr) {
   if (write_hdr && !WriteHeader()) {
     ICING_LOG(ERROR) << "Flushing trie header failed: " << strerror(errno);
   }
 
-  crcs_->header_crc = GetHeaderCrc();
-
+  // Since we just wrote the header, GetHeaderCrc() and GetWrittenHeaderCrc()
+  // are equivalent. We call GetWrittenHeaderCrc() because it avoids serializing
+  // the header to recalculate the crc.
+  cached_crcs_->header_crc = GetWrittenHeaderCrc().Get();
   for (int i = 0; i < NUM_ARRAY_TYPES; i++) {
     array_storage_[i].UpdateCrc();
   }
 
-  crcs_->all_crc = GetAllCrc();
-
-  return crcs_->all_crc;
+  // All of the cached crcs are now up to date.
+  Crc32 all_crc = GetAllCrcs(*cached_crcs_);
+  cached_crcs_->all_crc = all_crc.Get();
+  return all_crc;
 }
 
 bool IcingDynamicTrie::IcingDynamicTrieStorage::WriteHeader() {
@@ -965,7 +1032,7 @@ class IcingDynamicTrie::Dumper {
   explicit Dumper(const IcingDynamicTrie &trie)
       : all_props_(trie), del_prop_(trie), storage_(trie.storage_.get()) {}
 
-  void Dump(std::ostream *pretty_print, vector<std::string> *keys) const {
+  void Dump(std::ostream *pretty_print, std::vector<std::string> *keys) const {
     if (storage_->empty()) {
       *pretty_print << "(empty)\n";
     } else {
@@ -1006,7 +1073,8 @@ class IcingDynamicTrie::Dumper {
   //   ret - the stream to pretty print to
   //   keys - the keys encountered are appended to this
   void DumpNodeRecursive(const std::string &prefix, const Node &node, int level,
-                         std::ostream *ret, vector<std::string> *keys) const {
+                         std::ostream *ret,
+                         std::vector<std::string> *keys) const {
     if (node.is_leaf()) {
       // Dump suffix and value.
       for (int i = 0; i < level; i++) {
@@ -1117,7 +1185,7 @@ bool IcingDynamicTrie::Remove() {
   }
 
   // Also remove property bitmaps.
-  vector<std::string> files;
+  std::vector<std::string> files;
   if (!filesystem_->GetMatchingFiles((property_bitmaps_prefix_ + "*").c_str(),
                                      &files)) {
     return false;
@@ -1153,8 +1221,8 @@ bool IcingDynamicTrie::Sync() {
 
   Warm();
 
-  ICING_VLOG(1) << "Syncing dynamic trie " << filename_base_.c_str()
-      << " took " << timer.Elapsed() * 1000 << "ms";
+  ICING_VLOG(1) << "Syncing dynamic trie " << filename_base_.c_str() << " took "
+                << timer.Elapsed() * 1000 << "ms";
 
   return success;
 }
@@ -1231,10 +1299,11 @@ bool IcingDynamicTrie::InitPropertyBitmaps() {
           : 0;
 
   // Discover property bitmaps by scanning the dir.
-  vector<std::string> files;
+  std::vector<std::string> files;
   if (!filesystem_->GetMatchingFiles((property_bitmaps_prefix_ + "*").c_str(),
                                      &files)) {
-    ICING_LOG(ERROR) << "Could not get files at prefix " << property_bitmaps_prefix_;
+    ICING_LOG(ERROR) << "Could not get files at prefix "
+                     << property_bitmaps_prefix_;
     goto failed;
   }
   for (size_t i = 0; i < files.size(); i++) {
@@ -1292,15 +1361,6 @@ void IcingDynamicTrie::Warm() const {
   return storage_->Warm();
 }
 
-void IcingDynamicTrie::OnSleep() {
-  if (!is_initialized()) {
-    ICING_LOG(FATAL) << "DynamicTrie not initialized";
-  }
-
-  // Update crcs so we can verify when we come back.
-  UpdateCrc();
-}
-
 uint32_t IcingDynamicTrie::size() const {
   if (!is_initialized()) {
     ICING_LOG(FATAL) << "DynamicTrie not initialized";
@@ -1313,7 +1373,7 @@ void IcingDynamicTrie::CollectStatsRecursive(const Node &node, Stats *stats,
   if (node.is_leaf()) {
     stats->num_leaves++;
     stats->sum_depth += depth;
-    stats->max_depth = max(stats->max_depth, depth);
+    stats->max_depth = std::max(stats->max_depth, depth);
     const char *suffix = storage_->GetSuffix(node.next_index());
     stats->suffixes_used += strlen(suffix) + 1 + value_size();
     if (!suffix[0]) {
@@ -1334,7 +1394,7 @@ void IcingDynamicTrie::CollectStatsRecursive(const Node &node, Stats *stats,
       ICING_LOG(FATAL) << "No valid node in 'next' array";
     }
     stats->sum_children += i;
-    stats->max_children = max(stats->max_children, i);
+    stats->max_children = std::max(stats->max_children, i);
 
     stats->child_counts[i - 1]++;
     stats->wasted[node.log2_num_children()] +=
@@ -1444,7 +1504,7 @@ std::string IcingDynamicTrie::Stats::DumpStats(int verbosity) const {
 }
 
 void IcingDynamicTrie::DumpTrie(std::ostream *pretty_print,
-                                vector<std::string> *keys) const {
+                                std::vector<std::string> *keys) const {
   if (!is_initialized()) {
     ICING_LOG(FATAL) << "DynamicTrie not initialized";
   }
@@ -1513,7 +1573,7 @@ bool IcingDynamicTrie::SortNextArray(const Node *node) {
   return true;
 }
 
-libtextclassifier3::Status IcingDynamicTrie::Insert(const char *key,
+libtextclassifier3::Status IcingDynamicTrie::Insert(std::string_view key,
                                                     const void *value,
                                                     uint32_t *value_index,
                                                     bool replace,
@@ -1524,15 +1584,19 @@ libtextclassifier3::Status IcingDynamicTrie::Insert(const char *key,
 
   if (pnew_key) *pnew_key = false;
 
+  if (!IsKeyValid(key)) {
+    return absl_ports::InvalidArgumentError(
+        "Key cannot contain a null character '\\0'");
+  }
+
   // Find out ahead of time whether things will fit. A conservative
   // check based on allocations made below.
   //
   // IMPORTANT: This needs to be updated if the alloc patterns below
   // change.
-  size_t key_len = strlen(key);
-  if (!(storage_->nodes_left() >= 2 + key_len + 1 &&
-        storage_->nexts_left() >= 2 + key_len + 1 + kMaxNextArraySize &&
-        storage_->suffixes_left() >= key_len + 1 + value_size())) {
+  if (!(storage_->nodes_left() >= 2 + key.size() + 1 &&
+        storage_->nexts_left() >= 2 + key.size() + 1 + kMaxNextArraySize &&
+        storage_->suffixes_left() >= key.size() + 1 + value_size())) {
     return absl_ports::ResourceExhaustedError("No more space left");
   }
 
@@ -1556,38 +1620,40 @@ libtextclassifier3::Status IcingDynamicTrie::Insert(const char *key,
     Node *split_node = storage_->GetMutableNode(best_node_index);
     const char *prev_suffix = storage_->GetSuffix(split_node->next_index());
 
-    // Find the common prefix length.
-    const char *prev_suffix_cur = prev_suffix;
-    const char *key_cur = key + key_offset;
-    while (*prev_suffix_cur && *prev_suffix_cur == *key_cur) {
-      prev_suffix_cur++;
-      key_cur++;
-    }
-
-    // Equal strings?
-    if (*prev_suffix_cur == 0 && *key_cur == 0) {
-      // Update value if replace == true and return.
+    // Find the common prefix length starting from prev_suffix[0] and
+    // key[key_offset].
+    // - prev_suffix terminates with '\0'.
+    // - key is a std::string_view object, so it may not be null-terminated.
+    // - key doesn't contain '\0' because it's checked in IsKeyValid() above.
+    int common_prefix_len = 0;
+    while (key_offset + common_prefix_len < key.size() &&
+           prev_suffix[common_prefix_len] ==
+               key[key_offset + common_prefix_len]) {
+      ++common_prefix_len;
+    }
+
+    // Equal strings
+    bool strings_equal = prev_suffix[common_prefix_len] == '\0' &&
+                         key_offset + common_prefix_len >= key.size();
+    if (strings_equal) {
       if (value_index) {
-        *value_index = storage_->GetSuffixIndex(prev_suffix_cur + 1);
+        *value_index =
+            storage_->GetSuffixIndex(prev_suffix + common_prefix_len + 1);
       }
+      // Update value if replace == true and return.
       if (replace) {
         char *mutable_prev_suffix_cur = storage_->GetMutableSuffix(
-            storage_->GetSuffixIndex(prev_suffix_cur + 1), value_size());
+            storage_->GetSuffixIndex(prev_suffix + common_prefix_len + 1),
+            value_size());
         memcpy(mutable_prev_suffix_cur, value, value_size());
       }
       return libtextclassifier3::Status::OK;
     }
 
-    if (*prev_suffix_cur == *key_cur) {
-      ICING_LOG(FATAL) << "The suffix cursor and key cursor should diverge "
-                          "after finding the common prefix.";
-    }
-
     // Create single-branch children for the common prefix
     // length. After the loop, split_node points to the node that
     // will have more than 1 char.
-    int common_len = prev_suffix_cur - prev_suffix;
-    for (int i = 0; i < common_len; i++) {
+    for (int i = 0; i < common_prefix_len; i++) {
       // Create a single-branch child node.
       ICING_ASSIGN_OR_RETURN(Next * split_next, storage_->AllocNextArray(1));
       split_node->set_next_index(storage_->GetNextArrayIndex(split_next));
@@ -1607,27 +1673,30 @@ libtextclassifier3::Status IcingDynamicTrie::Insert(const char *key,
     split_node->set_log2_num_children(1);
     Node *prev_suffix_node = storage_->AllocNode();
     Node *key_node = storage_->AllocNode();
-    split_next[0].set_val(*(prev_suffix + common_len));
+    split_next[0].set_val(*(prev_suffix + common_prefix_len));
     split_next[0].set_node_index(storage_->GetNodeIndex(prev_suffix_node));
-    if (*(prev_suffix + common_len)) {
+    if (*(prev_suffix + common_prefix_len)) {
       uint32_t next_index =
-          storage_->GetSuffixIndex(prev_suffix + common_len) + 1;
+          storage_->GetSuffixIndex(prev_suffix + common_prefix_len) + 1;
       prev_suffix_node->set_next_index(next_index);
     } else {
-      uint32_t next_index = storage_->GetSuffixIndex(prev_suffix + common_len);
+      uint32_t next_index =
+          storage_->GetSuffixIndex(prev_suffix + common_prefix_len);
       prev_suffix_node->set_next_index(next_index);
     }
+
+    char next_val = GetCharOrNull(key, key_offset + common_prefix_len);
     prev_suffix_node->set_is_leaf(true);
     prev_suffix_node->set_log2_num_children(0);
-    split_next[1].set_val(*(key + key_offset + common_len));
+    split_next[1].set_val(next_val);
     split_next[1].set_node_index(storage_->GetNodeIndex(key_node));
-    if (*(key + key_offset + common_len)) {
+    if (next_val != '\0') {
       uint32_t next_index = storage_->MakeSuffix(
-          key + key_offset + common_len + 1, value, value_index);
+          key.substr(key_offset + common_prefix_len + 1), value, value_index);
       key_node->set_next_index(next_index);
     } else {
-      uint32_t next_index = storage_->MakeSuffix(key + key_offset + common_len,
-                                                 value, value_index);
+      uint32_t next_index = storage_->MakeSuffix(
+          key.substr(key_offset + common_prefix_len), value, value_index);
       key_node->set_next_index(next_index);
     }
     key_node->set_is_leaf(true);
@@ -1640,13 +1709,13 @@ libtextclassifier3::Status IcingDynamicTrie::Insert(const char *key,
 
     // Add our value as a node + suffix.
     Node *new_leaf_node = storage_->AllocNode();
-    if (*(key + key_offset)) {
+    if (key_offset < key.size()) {
       uint32_t next_index =
-          storage_->MakeSuffix(key + key_offset + 1, value, value_index);
+          storage_->MakeSuffix(key.substr(key_offset + 1), value, value_index);
       new_leaf_node->set_next_index(next_index);
     } else {
       uint32_t next_index =
-          storage_->MakeSuffix(key + key_offset, value, value_index);
+          storage_->MakeSuffix(key.substr(key_offset), value, value_index);
       new_leaf_node->set_next_index(next_index);
     }
     new_leaf_node->set_is_leaf(true);
@@ -1665,9 +1734,9 @@ libtextclassifier3::Status IcingDynamicTrie::Insert(const char *key,
     }
 
     // Write a link to our new leaf node and sort.
-    new_next[next_len].set_val(*(key + key_offset));
+    new_next[next_len].set_val(GetCharOrNull(key, key_offset));
     new_next[next_len].set_node_index(storage_->GetNodeIndex(new_leaf_node));
-    inplace_merge(new_next, new_next + next_len, new_next + next_len + 1);
+    std::inplace_merge(new_next, new_next + next_len, new_next + next_len + 1);
     next_len++;
 
     // If this was new, update the parent node and free the old next
@@ -1721,32 +1790,43 @@ void IcingDynamicTrie::SetValueAtIndex(uint32_t value_index,
          value_size());
 }
 
-bool IcingDynamicTrie::Find(const char *key, void *value,
+bool IcingDynamicTrie::Find(std::string_view key, void *value,
                             uint32_t *value_index) const {
   if (!is_initialized()) {
     ICING_LOG(FATAL) << "DynamicTrie not initialized";
   }
 
+  if (!IsKeyValid(key)) {
+    return false;
+  }
+
   uint32_t best_node_index;
   int key_offset;
   FindBestNode(key, &best_node_index, &key_offset, false);
 
+  if (key_offset < 0) {
+    return false;
+  }
+
   const Node *best_node = storage_->GetNode(best_node_index);
-  if (key_offset >= 0 && best_node->is_leaf() &&
-      !strcmp(key + key_offset, storage_->GetSuffix(best_node->next_index()))) {
-    uint32_t vidx = best_node->next_index() +
-                    strlen(storage_->GetSuffix(best_node->next_index())) + 1;
-    if (value_index) *value_index = vidx;
-    if (value) memcpy(value, storage_->GetSuffix(vidx), value_size());
-    return true;
-  } else {
+  if (!best_node->is_leaf()) {
     return false;
   }
+
+  std::string_view suffix(storage_->GetSuffix(best_node->next_index()));
+  if (key.substr(key_offset) != suffix) {
+    return false;
+  }
+
+  uint32_t vidx = best_node->next_index() + suffix.size() + 1;
+  if (value_index) *value_index = vidx;
+  if (value) memcpy(value, storage_->GetSuffix(vidx), value_size());
+  return true;
 }
 
 IcingDynamicTrie::Iterator::Iterator(const IcingDynamicTrie &trie,
-                                     const char *prefix, bool reverse)
-    : cur_key_(prefix),
+                                     std::string prefix, bool reverse)
+    : cur_key_(std::move(prefix)),
       cur_suffix_(nullptr),
       cur_suffix_len_(0),
       single_leaf_match_(false),
@@ -1792,6 +1872,13 @@ void IcingDynamicTrie::Iterator::BranchToLeaf(uint32_t node_index,
 }
 
 void IcingDynamicTrie::Iterator::Reset() {
+  if (!IsKeyValid(cur_key_)) {
+    // Set invalid and return.
+    cur_suffix_ = nullptr;
+    cur_suffix_len_ = 0;
+    return;
+  }
+
   size_t strip_len = branch_stack_.size() + cur_suffix_len_;
 
   if (cur_key_.size() < strip_len) {
@@ -1812,7 +1899,7 @@ void IcingDynamicTrie::Iterator::Reset() {
   // Find node matching prefix.
   uint32_t node_index;
   int key_offset;
-  trie_.FindBestNode(cur_key_.c_str(), &node_index, &key_offset, true);
+  trie_.FindBestNode(cur_key_, &node_index, &key_offset, true);
 
   // Two cases/states:
   //
@@ -1901,10 +1988,10 @@ bool IcingDynamicTrie::Iterator::IsValid() const {
   return cur_suffix_ != nullptr;
 }
 
-const char *IcingDynamicTrie::Iterator::GetKey() const {
+std::string_view IcingDynamicTrie::Iterator::GetKey() const {
   // cur_key_ can have a NULL in it so cur_key_ can be wrong but
   // cur_key_.c_str() is always right.
-  return IsValid() ? cur_key_.c_str() : nullptr;
+  return IsValid() ? cur_key_.c_str() : std::string_view();
 }
 
 const void *IcingDynamicTrie::Iterator::GetValue() const {
@@ -2098,7 +2185,7 @@ const IcingDynamicTrie::Next *IcingDynamicTrie::LowerBound(
   Next key_next(key_char, node_index);
   if (end - start >= kBinarySearchCutoff) {
     // Binary search.
-    return lower_bound(start, end, key_next);
+    return std::lower_bound(start, end, key_next);
   } else {
     // Linear search.
     const Next *found;
@@ -2112,9 +2199,9 @@ const IcingDynamicTrie::Next *IcingDynamicTrie::LowerBound(
   }
 }
 
-void IcingDynamicTrie::FindBestNode(const char *key, uint32_t *best_node_index,
-                                    int *key_offset, bool prefix,
-                                    bool utf8) const {
+void IcingDynamicTrie::FindBestNode(std::string_view key,
+                                    uint32_t *best_node_index, int *key_offset,
+                                    bool prefix, bool utf8) const {
   // Find the best node such that:
   //
   // - If key is NOT in the trie, key[0..key_offset) is a prefix to
@@ -2134,11 +2221,12 @@ void IcingDynamicTrie::FindBestNode(const char *key, uint32_t *best_node_index,
   }
 
   const Node *cur_node = storage_->GetRootNode();
-  const char *cur_key = key;
+  int cur_key_idx = 0;
+  int utf8_key_idx = 0;
   const Node *utf8_node = cur_node;
-  const char *utf8_key = cur_key;
   while (!cur_node->is_leaf()) {
-    const Next *found = GetNextByChar(cur_node, *cur_key);
+    char cur_char = GetCharOrNull(key, cur_key_idx);
+    const Next *found = GetNextByChar(cur_node, cur_char);
     if (!found) break;
 
     if (prefix && found->val() == 0) {
@@ -2148,29 +2236,35 @@ void IcingDynamicTrie::FindBestNode(const char *key, uint32_t *best_node_index,
     cur_node = storage_->GetNode(found->node_index());
 
     // End of key.
-    if (*cur_key == 0) {
+    if (cur_key_idx >= key.size()) {
       break;
     }
-    cur_key++;
 
-    if (utf8 && i18n_utils::IsLeadUtf8Byte(*cur_key)) {
+    ++cur_key_idx;
+    cur_char = GetCharOrNull(key, cur_key_idx);
+
+    if (utf8 && i18n_utils::IsLeadUtf8Byte(cur_char)) {
       utf8_node = cur_node;
-      utf8_key = cur_key;
+      utf8_key_idx = cur_key_idx;
     }
   }
 
   if (utf8) {
     // Rewind.
     cur_node = utf8_node;
-    cur_key = utf8_key;
+    cur_key_idx = utf8_key_idx;
   }
 
   *best_node_index = storage_->GetNodeIndex(cur_node);
-  *key_offset = reinterpret_cast<const char *>(cur_key) - key;
+  *key_offset = cur_key_idx;
 }
 
-int IcingDynamicTrie::FindNewBranchingPrefixLength(const char *key,
+int IcingDynamicTrie::FindNewBranchingPrefixLength(std::string_view key,
                                                    bool utf8) const {
+  if (!IsKeyValid(key)) {
+    return kNoBranchFound;
+  }
+
   if (storage_->empty()) {
     return kNoBranchFound;
   }
@@ -2178,31 +2272,50 @@ int IcingDynamicTrie::FindNewBranchingPrefixLength(const char *key,
   uint32_t best_node_index;
   int key_offset;
   FindBestNode(key, &best_node_index, &key_offset, /*prefix=*/true, utf8);
+  if (key_offset < 0) {
+    return kNoBranchFound;
+  }
+
   const Node *cur_node = storage_->GetNode(best_node_index);
-  const char *cur_key = key + key_offset;
   if (cur_node->is_leaf()) {
     // Prefix in the trie. Split at leaf.
     const char *prev_suffix = storage_->GetSuffix(cur_node->next_index());
-    while (*prev_suffix != '\0' && *prev_suffix == *cur_key) {
-      prev_suffix++;
-      cur_key++;
+    int additional_branch_prefix_len = 0;
+    // Find the additional prefix length starting from prev_suffix[0] and
+    // key[key_offset].
+    // - prev_suffix terminates with '\0'.
+    // - key is a std::string_view object, so it may not be null-terminated.
+    // - key doesn't contain '\0' because it's checked in IsKeyValid() above.
+    while (key_offset + additional_branch_prefix_len < key.size() &&
+           prev_suffix[additional_branch_prefix_len] ==
+               key[key_offset + additional_branch_prefix_len]) {
+      ++additional_branch_prefix_len;
+    }
+
+    // Equal strings. No branching.
+    bool strings_equal =
+        prev_suffix[additional_branch_prefix_len] == '\0' &&
+        key_offset + additional_branch_prefix_len >= key.size();
+    if (strings_equal) {
+      return kNoBranchFound;
     }
 
-    // Equal strings? No branching.
-    if (*prev_suffix == '\0' && *cur_key == '\0') {
-      return kNoBranchFound;
+    // The remaining key (after key_offset) is a prefix of the suffix, so the
+    // branching prefix length is key length.
+    if (key_offset + additional_branch_prefix_len >= key.size()) {
+      return key.size();
     }
 
     if (utf8) {
       // Rewind to utf8 boundary.
-      size_t offset = i18n_utils::SafeTruncateUtf8Length(key, cur_key - key);
-      cur_key = key + offset;
+      return i18n_utils::SafeTruncateUtf8Length(
+          key.data(), key_offset + additional_branch_prefix_len);
     }
 
-    return cur_key - key;
+    return key_offset + additional_branch_prefix_len;
   } else if (cur_node->log2_num_children() == 0) {
     // Intermediate node going from no branching to branching.
-    return cur_key - key;
+    return key_offset;
   }
 
   // If we've reached this point, then we're already at a branch point. So there
@@ -2210,23 +2323,27 @@ int IcingDynamicTrie::FindNewBranchingPrefixLength(const char *key,
   return kNoBranchFound;
 }
 
-std::vector<int> IcingDynamicTrie::FindBranchingPrefixLengths(const char *key,
-                                                              bool utf8) const {
+std::vector<int> IcingDynamicTrie::FindBranchingPrefixLengths(
+    std::string_view key, bool utf8) const {
   std::vector<int> prefix_lengths;
 
+  if (!IsKeyValid(key)) {
+    return prefix_lengths;
+  }
+
   if (storage_->empty()) {
     return prefix_lengths;
   }
 
   const Node *cur_node = storage_->GetRootNode();
-  const char *cur_key = key;
-  while (*cur_key && !cur_node->is_leaf()) {
+  int idx = 0;
+  while (idx < key.size() && !cur_node->is_leaf()) {
     // Branching prefix?
     if (cur_node->log2_num_children() > 0) {
-      int len = cur_key - key;
+      int len = idx;
       if (utf8) {
         // Do not cut mid-utf8. Walk up to utf8 boundary.
-        len = i18n_utils::SafeTruncateUtf8Length(key, len);
+        len = i18n_utils::SafeTruncateUtf8Length(key.data(), len);
         if (prefix_lengths.empty() || len != prefix_lengths.back()) {
           prefix_lengths.push_back(len);
         }
@@ -2236,22 +2353,26 @@ std::vector<int> IcingDynamicTrie::FindBranchingPrefixLengths(const char *key,
     }
 
     // Move to next.
-    const Next *found = GetNextByChar(cur_node, *cur_key);
+    const Next *found = GetNextByChar(cur_node, key[idx]);
     if (found == nullptr) {
       break;
     }
     cur_node = storage_->GetNode(found->node_index());
 
-    ++cur_key;
+    ++idx;
   }
   return prefix_lengths;
 }
 
-bool IcingDynamicTrie::IsBranchingTerm(const char *key) const {
+bool IcingDynamicTrie::IsBranchingTerm(std::string_view key) const {
   if (!is_initialized()) {
     ICING_LOG(FATAL) << "DynamicTrie not initialized";
   }
 
+  if (!IsKeyValid(key)) {
+    return false;
+  }
+
   if (storage_->empty()) {
     return false;
   }
@@ -2259,6 +2380,10 @@ bool IcingDynamicTrie::IsBranchingTerm(const char *key) const {
   uint32_t best_node_index;
   int key_offset;
   FindBestNode(key, &best_node_index, &key_offset, /*prefix=*/true);
+  if (key_offset < 0) {
+    return false;
+  }
+
   const Node *cur_node = storage_->GetNode(best_node_index);
 
   if (cur_node->is_leaf()) {
@@ -2266,7 +2391,7 @@ bool IcingDynamicTrie::IsBranchingTerm(const char *key) const {
   }
 
   // There is no intermediate node for key in the trie.
-  if (key[key_offset] != '\0') {
+  if (key_offset < key.size()) {
     return false;
   }
 
@@ -2288,10 +2413,11 @@ void IcingDynamicTrie::GetDebugInfo(int verbosity, std::string *out) const {
   out->append(stats.DumpStats(verbosity));
 
   // Property files.
-  vector<std::string> files;
+  std::vector<std::string> files;
   if (!filesystem_->GetMatchingFiles((property_bitmaps_prefix_ + "*").c_str(),
                                      &files)) {
-    ICING_LOG(ERROR) << "Could not get files at prefix " << property_bitmaps_prefix_;
+    ICING_LOG(ERROR) << "Could not get files at prefix "
+                     << property_bitmaps_prefix_;
     return;
   }
   for (size_t i = 0; i < files.size(); i++) {
@@ -2311,12 +2437,13 @@ double IcingDynamicTrie::min_free_fraction() const {
     ICING_LOG(FATAL) << "DynamicTrie not initialized";
   }
 
-  return 1.0 - max(max(static_cast<double>(storage_->hdr().num_nodes()) /
-                           storage_->hdr().max_nodes(),
-                       static_cast<double>(storage_->hdr().num_nexts()) /
-                           storage_->hdr().max_nexts()),
-                   static_cast<double>(storage_->hdr().suffixes_size()) /
-                       storage_->hdr().max_suffixes_size());
+  return 1.0 -
+         std::max(std::max(static_cast<double>(storage_->hdr().num_nodes()) /
+                               storage_->hdr().max_nodes(),
+                           static_cast<double>(storage_->hdr().num_nexts()) /
+                               storage_->hdr().max_nexts()),
+                  static_cast<double>(storage_->hdr().suffixes_size()) /
+                      storage_->hdr().max_suffixes_size());
 }
 
 uint32_t IcingDynamicTrie::value_size() const {
@@ -2327,32 +2454,63 @@ uint32_t IcingDynamicTrie::max_value_index() const {
   return storage_->hdr().max_suffixes_size();
 }
 
-uint32_t IcingDynamicTrie::UpdateCrc() {
+Crc32 IcingDynamicTrie::UpdateCrc() {
   if (!is_initialized()) {
     ICING_LOG(FATAL) << "DynamicTrie not initialized";
   }
 
   if (runtime_options_.storage_policy != RuntimeOptions::kMapSharedWithCrc) {
-    return kNoCrc;
+    return Crc32();
   }
 
   // Combine storage crc with property bitmap crcs.
-  uint32_t crc = storage_->UpdateCrc();
+  Crc32 crc = storage_->UpdateCrc();
 
   // Update crcs on bitmaps.
   for (size_t i = 0; i < property_bitmaps_.size(); ++i) {
     if (property_bitmaps_[i]) {
       // Combine property id with the bitmap crc.
-      uint64_t this_crc = property_bitmaps_[i]->UpdateCrc();
-      this_crc = (this_crc << 32) | i;
-      crc = IcingStringUtil::UpdateCrc32(
-          crc, reinterpret_cast<const char *>(&this_crc), sizeof(this_crc));
+      uint64_t property_crc = property_bitmaps_[i]->UpdateCrc().Get();
+      property_crc = (property_crc << 32) | i;
+      std::string_view property_crc_str(
+          reinterpret_cast<const char *>(&property_crc), sizeof(property_crc));
+      crc.Append(property_crc_str);
     }
   }
-  uint32_t this_crc = deleted_bitmap_->UpdateCrc();
-  crc = IcingStringUtil::UpdateCrc32(
-      crc, reinterpret_cast<const char *>(&this_crc), sizeof(this_crc));
+  uint32_t deleted_crc = deleted_bitmap_->UpdateCrc().Get();
+  std::string_view deleted_crc_str(reinterpret_cast<const char *>(&deleted_crc),
+                                   sizeof(deleted_crc));
+  crc.Append(deleted_crc_str);
+  return crc;
+}
 
+Crc32 IcingDynamicTrie::GetCrc() const {
+  if (!is_initialized()) {
+    ICING_LOG(FATAL) << "DynamicTrie not initialized";
+  }
+
+  if (runtime_options_.storage_policy != RuntimeOptions::kMapSharedWithCrc) {
+    return Crc32();
+  }
+
+  // Combine storage crc with property bitmap crcs.
+  Crc32 crc = storage_->GetCrc();
+
+  // Update crcs on bitmaps.
+  for (size_t i = 0; i < property_bitmaps_.size(); ++i) {
+    if (property_bitmaps_[i]) {
+      // Combine property id with the bitmap crc.
+      uint64_t property_crc = property_bitmaps_[i]->GetCrc().Get();
+      property_crc = (property_crc << 32) | i;
+      std::string_view property_crc_str(
+          reinterpret_cast<const char *>(&property_crc), sizeof(property_crc));
+      crc.Append(property_crc_str);
+    }
+  }
+  uint32_t deleted_crc = deleted_bitmap_->UpdateCrc().Get();
+  std::string_view deleted_crc_str(reinterpret_cast<const char *>(&deleted_crc),
+                                   sizeof(deleted_crc));
+  crc.Append(deleted_crc_str);
   return crc;
 }
 
@@ -2419,12 +2577,16 @@ bool IcingDynamicTrie::ClearDeleted(uint32_t value_index) {
 // 2. Remove the suffix and the value.
 // 3. Reset the nexts that point to the nodes to be removed.
 // 4. Sort any next array if needed.
-bool IcingDynamicTrie::Delete(const std::string_view key) {
+bool IcingDynamicTrie::Delete(std::string_view key) {
   if (!is_initialized()) {
     ICING_LOG(ERROR) << "DynamicTrie not initialized";
     return false;
   }
 
+  if (!IsKeyValid(key)) {
+    return false;
+  }
+
   if (storage_->empty()) {
     // Nothing to delete.
     return true;
diff --git a/icing/legacy/index/icing-dynamic-trie.h b/icing/legacy/index/icing-dynamic-trie.h
index 18748d7..301ee24 100644
--- a/icing/legacy/index/icing-dynamic-trie.h
+++ b/icing/legacy/index/icing-dynamic-trie.h
@@ -38,6 +38,7 @@
 #include <cstdint>
 #include <memory>
 #include <string>
+#include <string_view>
 #include <unordered_map>
 #include <vector>
 
@@ -49,6 +50,7 @@
 #include "icing/legacy/index/icing-mmapper.h"
 #include "icing/legacy/index/icing-storage.h"
 #include "icing/legacy/index/proto/icing-dynamic-trie-header.pb.h"
+#include "icing/util/crc32.h"
 #include "icing/util/i18n-utils.h"
 #include "unicode/utf8.h"
 
@@ -311,9 +313,6 @@ class IcingDynamicTrie : public IIcingStorage {
   // Tell kernel we will access the memory shortly.
   void Warm() const;
 
-  // Potentially about to get nuked.
-  void OnSleep() override;
-
   // Insert value at key. If key already exists and replace == true,
   // replaces old value with value. We take a copy of value.
   //
@@ -327,14 +326,14 @@ class IcingDynamicTrie : public IIcingStorage {
   //   OK on success
   //   RESOURCE_EXHAUSTED if no disk space is available
   //   INTERNAL_ERROR if there are inconsistencies in the dynamic trie.
-  libtextclassifier3::Status Insert(const char *key, const void *value) {
+  libtextclassifier3::Status Insert(std::string_view key, const void *value) {
     return Insert(key, value, nullptr, true, nullptr);
   }
-  libtextclassifier3::Status Insert(const char *key, const void *value,
+  libtextclassifier3::Status Insert(std::string_view key, const void *value,
                                     uint32_t *value_index, bool replace) {
     return Insert(key, value, value_index, replace, nullptr);
   }
-  libtextclassifier3::Status Insert(const char *key, const void *value,
+  libtextclassifier3::Status Insert(std::string_view key, const void *value,
                                     uint32_t *value_index, bool replace,
                                     bool *pnew_key);
 
@@ -355,10 +354,10 @@ class IcingDynamicTrie : public IIcingStorage {
   // value_index is modified.
   //
   // REQUIRES: value a buffer of size value_size()
-  bool Find(const char *key, void *value) const {
+  bool Find(std::string_view key, void *value) const {
     return Find(key, value, nullptr);
   }
-  bool Find(const char *key, void *value, uint32_t *value_index) const;
+  bool Find(std::string_view key, void *value, uint32_t *value_index) const;
 
   // Find the input key and all keys that are a variant of the input
   // key according to a variant map. Currently supports
@@ -383,18 +382,19 @@ class IcingDynamicTrie : public IIcingStorage {
   // Return prefix of any new branches created if key were inserted. If utf8 is
   // true, does not cut key mid-utf8. Returns kNoBranchFound if no branches
   // would be created.
-  int FindNewBranchingPrefixLength(const char *key, bool utf8) const;
+  int FindNewBranchingPrefixLength(std::string_view key, bool utf8) const;
 
   // Find all prefixes of key where the trie branches. Excludes the key
   // itself. If utf8 is true, does not cut key mid-utf8.
-  std::vector<int> FindBranchingPrefixLengths(const char *key, bool utf8) const;
+  std::vector<int> FindBranchingPrefixLengths(std::string_view key,
+                                              bool utf8) const;
 
   // Check if key is a branching term.
   //
   // key is a branching term, if and only if there exists terms s1 and s2 in the
   // trie such that key is the maximum common prefix of s1 and s2, but s1 and s2
   // are not prefixes of each other.
-  bool IsBranchingTerm(const char *key) const;
+  bool IsBranchingTerm(std::string_view key) const;
 
   void GetDebugInfo(int verbosity, std::string *out) const override;
 
@@ -407,7 +407,12 @@ class IcingDynamicTrie : public IIcingStorage {
   // If in kMapSharedWithCrc mode, update crcs and return the master
   // crc, else return kNoCrc. This crc includes both the trie files
   // and property bitmaps.
-  uint32_t UpdateCrc();
+  Crc32 UpdateCrc() override;
+
+  // If in kMapSharedWithCrc mode, calculates crcs and return the master
+  // crc, else return kNoCrc. This crc includes both the trie files
+  // and property bitmaps. Does NOT update any stored crcs.
+  Crc32 GetCrc() const;
 
   // Store dynamic properties for each value.  When a property is added to
   // a value, the deleted flag is cleared for it (if it was previously set).
@@ -509,15 +514,15 @@ class IcingDynamicTrie : public IIcingStorage {
   //                    iterator pattern in our codebase.
   class Iterator {
    public:
-    Iterator(const IcingDynamicTrie &trie, const char *prefix,
+    Iterator(const IcingDynamicTrie &trie, std::string prefix,
              bool reverse = false);
     void Reset();
     bool Advance();
 
-    // If !IsValid(), GetKey() will return NULL and GetValue() will
-    // return 0.
+    // If !IsValid(), GetKey() will return a std::string_view object with
+    // data() == nullptr and size() == 0, and GetValue() will return nullptr.
     bool IsValid() const;
-    const char *GetKey() const;
+    std::string_view GetKey() const;
     // This points directly to the underlying data and is valid while
     // the trie is alive. We keep ownership of the pointer.
     const void *GetValue() const;
@@ -620,8 +625,8 @@ class IcingDynamicTrie : public IIcingStorage {
   // Returns the number of valid nexts in the array.
   int GetValidNextsSize(const IcingDynamicTrie::Next *next_array_start,
                         int next_array_length) const;
-  void FindBestNode(const char *key, uint32_t *best_node_index, int *key_offset,
-                    bool prefix, bool utf8 = false) const;
+  void FindBestNode(std::string_view key, uint32_t *best_node_index,
+                    int *key_offset, bool prefix, bool utf8 = false) const;
 
   // For value properties.  This truncates the data by clearing it, but leaving
   // the storage intact.
diff --git a/icing/legacy/index/icing-dynamic-trie_test.cc b/icing/legacy/index/icing-dynamic-trie_test.cc
index ec7e277..2b5a8b9 100644
--- a/icing/legacy/index/icing-dynamic-trie_test.cc
+++ b/icing/legacy/index/icing-dynamic-trie_test.cc
@@ -17,10 +17,9 @@
 #include <cstddef>
 #include <cstdint>
 #include <cstdio>
-#include <memory>
 #include <string>
-#include <unordered_map>
 #include <unordered_set>
+#include <utility>
 #include <vector>
 
 #include "icing/text_classifier/lib3/utils/hash/farmhash.h"
@@ -40,7 +39,8 @@ namespace {
 
 using testing::ContainerEq;
 using testing::ElementsAre;
-using testing::StrEq;
+using testing::Eq;
+using testing::Not;
 
 constexpr std::string_view kKeys[] = {
     "", "ab", "ac", "abd", "bac", "bb", "bacd", "abbb", "abcdefg",
@@ -61,7 +61,7 @@ class IcingDynamicTrieTest : public ::testing::Test {
     for (uint32_t i = 0; i < kNumKeys; i++) {
       key.clear();
       IcingStringUtil::SStringAppendF(&key, 0, "%u+%010u", i % 2, i);
-      ASSERT_THAT(trie->Insert(key.c_str(), &i), IsOk());
+      ASSERT_THAT(trie->Insert(key, &i), IsOk());
     }
   }
 
@@ -71,7 +71,7 @@ class IcingDynamicTrieTest : public ::testing::Test {
       key.clear();
       IcingStringUtil::SStringAppendF(&key, 0, "%u+%010u", i % 2, i);
       uint32_t val;
-      bool found = trie.Find(key.c_str(), &val);
+      bool found = trie.Find(key, &val);
       EXPECT_TRUE(found);
       EXPECT_EQ(i, val);
     }
@@ -105,7 +105,7 @@ std::vector<std::pair<std::string, int>> RetrieveKeyValuePairs(
   for (; term_iter.IsValid(); term_iter.Advance()) {
     uint32_t val;
     memcpy(&val, term_iter.GetValue(), sizeof(val));
-    key_value.push_back(std::make_pair(term_iter.GetKey(), val));
+    key_value.push_back(std::make_pair(std::string(term_iter.GetKey()), val));
   }
   return key_value;
 }
@@ -127,10 +127,10 @@ TEST_F(IcingDynamicTrieTest, Simple) {
   ASSERT_TRUE(trie.Init());
 
   for (uint32_t i = 0; i < kNumKeys; i++) {
-    ASSERT_THAT(trie.Insert(kKeys[i].data(), &i), IsOk());
+    ASSERT_THAT(trie.Insert(kKeys[i], &i), IsOk());
 
     uint32_t val;
-    bool found = trie.Find(kKeys[i].data(), &val);
+    bool found = trie.Find(kKeys[i], &val);
     EXPECT_TRUE(found) << kKeys[i];
     if (found) EXPECT_EQ(i, val) << kKeys[i] << " " << val;
   }
@@ -168,7 +168,7 @@ TEST_F(IcingDynamicTrieTest, Iterator) {
   ASSERT_TRUE(trie.Init());
 
   for (uint32_t i = 0; i < kNumKeys; i++) {
-    ASSERT_THAT(trie.Insert(kKeys[i].data(), &i), IsOk());
+    ASSERT_THAT(trie.Insert(kKeys[i], &i), IsOk());
   }
 
   // Should get the entire trie.
@@ -235,16 +235,16 @@ TEST_F(IcingDynamicTrieTest, Iterator) {
   };
 
   for (size_t k = 0; k < ABSL_ARRAYSIZE(kOneMatch); k++) {
-    IcingDynamicTrie::Iterator it_single(trie, kOneMatch[k].data());
+    IcingDynamicTrie::Iterator it_single(trie, std::string(kOneMatch[k]));
     ASSERT_TRUE(it_single.IsValid()) << kOneMatch[k];
-    EXPECT_THAT(it_single.GetKey(), StrEq(kOneMatchMatched[k].data()));
+    EXPECT_THAT(it_single.GetKey(), Eq(kOneMatchMatched[k]));
     EXPECT_FALSE(it_single.Advance()) << kOneMatch[k];
     EXPECT_FALSE(it_single.IsValid()) << kOneMatch[k];
 
     // Should get same results after calling Reset
     it_single.Reset();
     ASSERT_TRUE(it_single.IsValid()) << kOneMatch[k];
-    EXPECT_THAT(it_single.GetKey(), StrEq(kOneMatchMatched[k].data()));
+    EXPECT_THAT(it_single.GetKey(), Eq(kOneMatchMatched[k]));
     EXPECT_FALSE(it_single.Advance()) << kOneMatch[k];
     EXPECT_FALSE(it_single.IsValid()) << kOneMatch[k];
   }
@@ -256,7 +256,7 @@ TEST_F(IcingDynamicTrieTest, Iterator) {
       "abcdefh",
   };
   for (size_t k = 0; k < ABSL_ARRAYSIZE(kNoMatch); k++) {
-    IcingDynamicTrie::Iterator it_empty(trie, kNoMatch[k].data());
+    IcingDynamicTrie::Iterator it_empty(trie, std::string(kNoMatch[k]));
     EXPECT_FALSE(it_empty.IsValid());
     it_empty.Reset();
     EXPECT_FALSE(it_empty.IsValid());
@@ -278,7 +278,7 @@ TEST_F(IcingDynamicTrieTest, IteratorReverse) {
   ASSERT_TRUE(trie.Init());
 
   for (uint32_t i = 0; i < kNumKeys; i++) {
-    ASSERT_THAT(trie.Insert(kKeys[i].data(), &i), IsOk());
+    ASSERT_THAT(trie.Insert(kKeys[i], &i), IsOk());
   }
 
   // Should get the entire trie.
@@ -343,17 +343,17 @@ TEST_F(IcingDynamicTrieTest, IteratorReverse) {
   };
 
   for (size_t k = 0; k < ABSL_ARRAYSIZE(kOneMatch); k++) {
-    IcingDynamicTrie::Iterator it_single(trie, kOneMatch[k].data(),
+    IcingDynamicTrie::Iterator it_single(trie, std::string(kOneMatch[k]),
                                          /*reverse=*/true);
     ASSERT_TRUE(it_single.IsValid()) << kOneMatch[k];
-    EXPECT_THAT(it_single.GetKey(), StrEq(kOneMatchMatched[k].data()));
+    EXPECT_THAT(it_single.GetKey(), Eq(kOneMatchMatched[k]));
     EXPECT_FALSE(it_single.Advance()) << kOneMatch[k];
     EXPECT_FALSE(it_single.IsValid()) << kOneMatch[k];
 
     // Should get same results after calling Reset
     it_single.Reset();
     ASSERT_TRUE(it_single.IsValid()) << kOneMatch[k];
-    EXPECT_THAT(it_single.GetKey(), StrEq(kOneMatchMatched[k].data()));
+    EXPECT_THAT(it_single.GetKey(), Eq(kOneMatchMatched[k]));
     EXPECT_FALSE(it_single.Advance()) << kOneMatch[k];
     EXPECT_FALSE(it_single.IsValid()) << kOneMatch[k];
   }
@@ -365,7 +365,7 @@ TEST_F(IcingDynamicTrieTest, IteratorReverse) {
       "abcdefh",
   };
   for (size_t k = 0; k < ABSL_ARRAYSIZE(kNoMatch); k++) {
-    IcingDynamicTrie::Iterator it_empty(trie, kNoMatch[k].data(),
+    IcingDynamicTrie::Iterator it_empty(trie, std::string(kNoMatch[k]),
                                         /*reverse=*/true);
     EXPECT_FALSE(it_empty.IsValid());
     it_empty.Reset();
@@ -394,7 +394,7 @@ TEST_F(IcingDynamicTrieTest, IteratorLoadTest) {
   // Randomly generate 1024 terms.
   for (int i = 0; i < 1024; ++i) {
     std::string term = RandomString("abcdefg", 5, &random) + std::to_string(i);
-    ASSERT_THAT(trie.Insert(term.c_str(), &i), IsOk());
+    ASSERT_THAT(trie.Insert(term, &i), IsOk());
     exp_key_values.push_back(std::make_pair(term, i));
   }
   // Lexicographically sort the expected keys.
@@ -437,7 +437,7 @@ TEST_F(IcingDynamicTrieTest, Persistence) {
     ASSERT_TRUE(trie.Init());
 
     for (uint32_t i = 0; i < kCommonEnglishWordArrayLen; i++) {
-      ASSERT_THAT(trie.Insert(kCommonEnglishWords[i].data(), &i), IsOk());
+      ASSERT_THAT(trie.Insert(kCommonEnglishWords[i], &i), IsOk());
     }
     // Explicitly omit sync.
 
@@ -451,7 +451,7 @@ TEST_F(IcingDynamicTrieTest, Persistence) {
     EXPECT_EQ(0U, trie.size());
 
     for (uint32_t i = 0; i < kCommonEnglishWordArrayLen; i++) {
-      ASSERT_THAT(trie.Insert(kCommonEnglishWords[i].data(), &i), IsOk());
+      ASSERT_THAT(trie.Insert(kCommonEnglishWords[i], &i), IsOk());
     }
     trie.Sync();
 
@@ -468,7 +468,7 @@ TEST_F(IcingDynamicTrieTest, Persistence) {
     uint32_t matched_count = 0;
     for (size_t i = 0; i < kCommonEnglishWordArrayLen; i++) {
       uint32_t val;
-      bool found = trie.Find(kCommonEnglishWords[i].data(), &val);
+      bool found = trie.Find(kCommonEnglishWords[i], &val);
       if (found) {
         found_count++;
         if (i == val) {
@@ -499,10 +499,10 @@ TEST_F(IcingDynamicTrieTest, PersistenceShared) {
 
     uint32_t next_reopen = kCommonEnglishWordArrayLen / 16;
     for (uint32_t i = 0; i < kCommonEnglishWordArrayLen; i++) {
-      ASSERT_THAT(trie.Insert(kCommonEnglishWords[i].data(), &i), IsOk());
+      ASSERT_THAT(trie.Insert(kCommonEnglishWords[i], &i), IsOk());
 
       if (i == next_reopen) {
-        ASSERT_NE(0u, trie.UpdateCrc());
+        ASSERT_NE(Crc32(), trie.UpdateCrc());
         trie.Close();
         ASSERT_TRUE(trie.Init());
 
@@ -531,7 +531,7 @@ TEST_F(IcingDynamicTrieTest, PersistenceShared) {
     uint32_t matched_count = 0;
     for (size_t i = 0; i < kCommonEnglishWordArrayLen; i++) {
       uint32_t val;
-      bool found = trie.Find(kCommonEnglishWords[i].data(), &val);
+      bool found = trie.Find(kCommonEnglishWords[i], &val);
       if (found) {
         found_count++;
         if (i == val) {
@@ -554,6 +554,121 @@ TEST_F(IcingDynamicTrieTest, PersistenceShared) {
   ASSERT_TRUE(trie.Init());
 }
 
+TEST_F(IcingDynamicTrieTest, UpdateCrc) {
+  IcingFilesystem filesystem;
+  IcingDynamicTrie::RuntimeOptions runtime_options;
+  runtime_options.storage_policy =
+      IcingDynamicTrie::RuntimeOptions::kMapSharedWithCrc;
+  IcingDynamicTrie trie_one(trie_files_prefix_, runtime_options, &filesystem);
+  ASSERT_TRUE(trie_one.CreateIfNotExist(IcingDynamicTrie::Options()));
+  ASSERT_TRUE(trie_one.Init());
+
+  // Initial Crcs of the various storages are 0. However, the crc of the header
+  // is not zero and the way in which IcingDynamicTrie combines the crcs of its
+  // components (by effectively taking a crc of the crcs) means that the crc of
+  // an empty dynamic trie is not 0.
+  Crc32 initial_crc = trie_one.GetCrc();
+  EXPECT_THAT(trie_one.UpdateCrc(), Eq(initial_crc));
+  EXPECT_THAT(trie_one.GetCrc(), Eq(initial_crc));
+
+  int val = 3;
+  ASSERT_THAT(trie_one.Insert("foo", &val), IsOk());
+
+  Crc32 updated_crc = trie_one.GetCrc();
+  EXPECT_THAT(updated_crc, Not(Eq(initial_crc)));
+  EXPECT_THAT(trie_one.UpdateCrc(), Eq(updated_crc));
+  EXPECT_THAT(trie_one.GetCrc(), Eq(updated_crc));
+}
+
+TEST_F(IcingDynamicTrieTest, GetCrcDoesntPreserveContent) {
+  IcingFilesystem filesystem;
+  IcingDynamicTrie::RuntimeOptions runtime_options;
+  runtime_options.storage_policy =
+      IcingDynamicTrie::RuntimeOptions::kMapSharedWithCrc;
+  IcingDynamicTrie trie_one(trie_files_prefix_, runtime_options, &filesystem);
+  ASSERT_TRUE(trie_one.CreateIfNotExist(IcingDynamicTrie::Options()));
+  ASSERT_TRUE(trie_one.Init());
+
+  // Initial Crcs of the various storages are 0. However, the crc of the header
+  // is not zero and the way in which IcingDynamicTrie combines the crcs of its
+  // components (by effectively taking a crc of the crcs) means that the crc of
+  // an empty dynamic trie is not 0.
+  Crc32 initial_crc = trie_one.GetCrc();
+  EXPECT_THAT(trie_one.UpdateCrc(), Eq(initial_crc));
+  EXPECT_THAT(trie_one.GetCrc(), Eq(initial_crc));
+
+  // Insert one value and update that crc.
+  int val = 3;
+  ASSERT_THAT(trie_one.Insert("foo", &val), IsOk());
+  int val_out = 0;
+  ASSERT_TRUE(trie_one.Find("foo", &val_out));
+  ASSERT_THAT(val_out, Eq(val));
+
+  // Create a second trie.
+  // Somewhat counterintuitively, it will successfully init.
+  // UpdateCrc does two things:
+  // 1. It updates and writes the header which includes the sizes of the
+  //    internal arrays.
+  // 2. It updates the cached crcs of the internal arrays and stores them in the
+  //    header.
+  // Without the call to UpdateCrc, the header will contain the original (empty)
+  // size of the array and the original crcs (all 0).
+  //
+  // This means that a failure to call UpdateCrc can have two outcomes:
+  // - If the changes made to the trie after the last call to UpdateCrc only
+  //   appended content to the trie's internal arrays, then the trie will
+  //   successfully init, but the newly added content will be lost.
+  // - If the changes made did not just append content to the trie's internal
+  //   arrays (such as causing a branch in the trie), then the trie will fail to
+  //   init.
+  //
+  // This test happens to fall into the first category. This means:
+  // 1. Init will return true
+  // 2. GetCrc will return the crc of the original (empty) trie.
+  // 3. Find will return false.
+  IcingDynamicTrie trie_two(trie_files_prefix_, runtime_options, &filesystem);
+  EXPECT_TRUE(trie_two.Init());
+  EXPECT_THAT(trie_two.GetCrc(), Eq(initial_crc));
+  EXPECT_FALSE(trie_two.Find("foo", &val_out));
+}
+
+TEST_F(IcingDynamicTrieTest, UpdateCrcPreservesNewContent) {
+  IcingFilesystem filesystem;
+  IcingDynamicTrie::RuntimeOptions runtime_options;
+  runtime_options.storage_policy =
+      IcingDynamicTrie::RuntimeOptions::kMapSharedWithCrc;
+  IcingDynamicTrie trie_one(trie_files_prefix_, runtime_options, &filesystem);
+  ASSERT_TRUE(trie_one.CreateIfNotExist(IcingDynamicTrie::Options()));
+  ASSERT_TRUE(trie_one.Init());
+
+  // Initial Crcs of the various storages are 0. However, the crc of the header
+  // is not zero and the way in which IcingDynamicTrie combines the crcs of its
+  // components (by effectively taking a crc of the crcs) means that the crc of
+  // an empty dynamic trie is not 0.
+  Crc32 initial_crc = trie_one.GetCrc();
+  EXPECT_THAT(trie_one.UpdateCrc(), Eq(initial_crc));
+  EXPECT_THAT(trie_one.GetCrc(), Eq(initial_crc));
+
+  int val = 3;
+  ASSERT_THAT(trie_one.Insert("foo", &val), IsOk());
+  int val_out = 0;
+  ASSERT_TRUE(trie_one.Find("foo", &val_out));
+  ASSERT_THAT(val_out, Eq(val));
+
+  Crc32 updated_crc = trie_one.GetCrc();
+  EXPECT_THAT(updated_crc, Not(Eq(initial_crc)));
+  EXPECT_THAT(trie_one.UpdateCrc(), Eq(updated_crc));
+  EXPECT_THAT(trie_one.GetCrc(), Eq(updated_crc));
+
+  // Create a second trie. It should init successfully, have the same crc as
+  // the first trie and hold the same value for "foo".
+  IcingDynamicTrie trie_two(trie_files_prefix_, runtime_options, &filesystem);
+  EXPECT_TRUE(trie_two.Init());
+  EXPECT_THAT(trie_two.GetCrc(), Eq(updated_crc));
+  EXPECT_TRUE(trie_two.Find("foo", &val_out));
+  EXPECT_THAT(val_out, Eq(val));
+}
+
 TEST_F(IcingDynamicTrieTest, Sync) {
   IcingFilesystem filesystem;
   {
@@ -563,10 +678,10 @@ TEST_F(IcingDynamicTrieTest, Sync) {
     ASSERT_TRUE(trie.Init());
 
     for (uint32_t i = 0; i < kNumKeys; i++) {
-      ASSERT_THAT(trie.Insert(kKeys[i].data(), &i), IsOk());
+      ASSERT_THAT(trie.Insert(kKeys[i], &i), IsOk());
 
       uint32_t val;
-      bool found = trie.Find(kKeys[i].data(), &val);
+      bool found = trie.Find(kKeys[i], &val);
       EXPECT_TRUE(found) << kKeys[i];
       if (found) EXPECT_EQ(i, val) << kKeys[i] << " " << val;
     }
@@ -578,7 +693,7 @@ TEST_F(IcingDynamicTrieTest, Sync) {
 
     for (uint32_t i = 0; i < kNumKeys; i++) {
       uint32_t val;
-      bool found = trie.Find(kKeys[i].data(), &val);
+      bool found = trie.Find(kKeys[i], &val);
       EXPECT_TRUE(found) << kKeys[i];
       if (found) EXPECT_EQ(i, val) << kKeys[i] << " " << val;
     }
@@ -591,7 +706,7 @@ TEST_F(IcingDynamicTrieTest, Sync) {
 
     for (uint32_t i = 0; i < kNumKeys; i++) {
       uint32_t val;
-      bool found = trie.Find(kKeys[i].data(), &val);
+      bool found = trie.Find(kKeys[i], &val);
       EXPECT_TRUE(found) << kKeys[i];
       if (found) EXPECT_EQ(i, val) << kKeys[i] << " " << val;
     }
@@ -621,16 +736,16 @@ TEST_F(IcingDynamicTrieTest, LimitsSmall) {
   ASSERT_LT(3U, kNumKeys);
 
   for (uint32_t i = 0; i < 3; i++) {
-    ASSERT_THAT(trie.Insert(kKeys[i].data(), &i), IsOk()) << i;
+    ASSERT_THAT(trie.Insert(kKeys[i], &i), IsOk()) << i;
 
     uint32_t val;
-    bool found = trie.Find(kKeys[i].data(), &val);
+    bool found = trie.Find(kKeys[i], &val);
     EXPECT_TRUE(found) << kKeys[i];
     if (found) EXPECT_EQ(i, val) << kKeys[i] << " " << val;
   }
 
   uint32_t val = 3;
-  EXPECT_THAT(trie.Insert(kKeys[3].data(), &val),
+  EXPECT_THAT(trie.Insert(kKeys[3], &val),
               StatusIs(libtextclassifier3::StatusCode::RESOURCE_EXHAUSTED));
 
   StatsDump(trie);
@@ -657,7 +772,7 @@ TEST_F(IcingDynamicTrieTest, DISABLEDFingerprintedKeys) {
     IcingStringUtil::SStringAppendF(
         &key, 1000, "content://gmail-ls/account/conversation/%u/message/%u", i,
         10 * i);
-    ASSERT_THAT(trie.Insert(key.c_str(), &i), IsOk());
+    ASSERT_THAT(trie.Insert(key, &i), IsOk());
 
     // Now compute a fingerprint.
     uint64_t fpkey = tc3farmhash::Fingerprint64(key);
@@ -1002,7 +1117,7 @@ TEST_F(IcingDynamicTrieTest, IteratorShouldWorkAfterDeletion) {
   IcingDynamicTrie::Iterator iterator_all(trie, "");
   std::vector<std::string> results;
   for (; iterator_all.IsValid(); iterator_all.Advance()) {
-    results.emplace_back(iterator_all.GetKey());
+    results.push_back(std::string(iterator_all.GetKey()));
   }
   EXPECT_THAT(results, ElementsAre("bar", "foo"));
 
@@ -1010,7 +1125,7 @@ TEST_F(IcingDynamicTrieTest, IteratorShouldWorkAfterDeletion) {
   IcingDynamicTrie::Iterator iterator_b(trie, "b");
   results.clear();
   for (; iterator_b.IsValid(); iterator_b.Advance()) {
-    results.emplace_back(iterator_b.GetKey());
+    results.push_back(std::string(iterator_b.GetKey()));
   }
   EXPECT_THAT(results, ElementsAre("bar"));
 }
@@ -1056,7 +1171,7 @@ TEST_F(IcingDynamicTrieTest, DeletionResortsFullNextArray) {
   std::vector<std::string> remaining;
   for (IcingDynamicTrie::Iterator term_iter(trie, /*prefix=*/"");
        term_iter.IsValid(); term_iter.Advance()) {
-    remaining.push_back(term_iter.GetKey());
+    remaining.push_back(std::string(term_iter.GetKey()));
   }
   EXPECT_THAT(remaining, ElementsAre("far", "fjord", "fudge"));
 }
@@ -1080,7 +1195,7 @@ TEST_F(IcingDynamicTrieTest, DeletionResortsPartiallyFilledNextArray) {
   std::vector<std::string> remaining;
   for (IcingDynamicTrie::Iterator term_iter(trie, /*prefix=*/"");
        term_iter.IsValid(); term_iter.Advance()) {
-    remaining.push_back(term_iter.GetKey());
+    remaining.push_back(std::string(term_iter.GetKey()));
   }
   EXPECT_THAT(remaining, ElementsAre("far", "fudge"));
 }
@@ -1099,7 +1214,7 @@ TEST_F(IcingDynamicTrieTest, DeletionLoadTest) {
   // Randomly generate 2048 terms.
   for (int i = 0; i < 2048; ++i) {
     terms.push_back(RandomString("abcdefg", 5, &random));
-    ASSERT_THAT(trie.Insert(terms.back().c_str(), &value), IsOk());
+    ASSERT_THAT(trie.Insert(terms.back(), &value), IsOk());
   }
 
   // Randomly delete 1024 terms.
@@ -1107,27 +1222,27 @@ TEST_F(IcingDynamicTrieTest, DeletionLoadTest) {
   std::shuffle(terms.begin(), terms.end(), random);
   for (int i = 0; i < 1024; ++i) {
     exp_remaining.erase(terms[i]);
-    ASSERT_TRUE(trie.Delete(terms[i].c_str()));
+    ASSERT_TRUE(trie.Delete(terms[i]));
   }
 
   // Check that the iterator still works, and the remaining terms are correct.
   std::unordered_set<std::string> remaining;
   for (IcingDynamicTrie::Iterator term_iter(trie, /*prefix=*/"");
        term_iter.IsValid(); term_iter.Advance()) {
-    remaining.insert(term_iter.GetKey());
+    remaining.insert(std::string(term_iter.GetKey()));
   }
   EXPECT_THAT(remaining, ContainerEq(exp_remaining));
 
   // Check that we can still insert terms after delete.
   for (int i = 0; i < 2048; ++i) {
     std::string term = RandomString("abcdefg", 5, &random);
-    ASSERT_THAT(trie.Insert(term.c_str(), &value), IsOk());
+    ASSERT_THAT(trie.Insert(term, &value), IsOk());
     exp_remaining.insert(term);
   }
   remaining.clear();
   for (IcingDynamicTrie::Iterator term_iter(trie, /*prefix=*/"");
        term_iter.IsValid(); term_iter.Advance()) {
-    remaining.insert(term_iter.GetKey());
+    remaining.insert(std::string(term_iter.GetKey()));
   }
   EXPECT_THAT(remaining, ContainerEq(exp_remaining));
 }
@@ -1161,7 +1276,7 @@ TEST_F(IcingDynamicTrieTest, TrieShouldRespectLimits) {
     // Inserts all the test words before the last one.
     uint32_t value = 0;
     for (size_t i = 0; i < kCommonEnglishWordArrayLen - 1; ++i) {
-      ASSERT_THAT(trie.Insert(kCommonEnglishWords[i].data(), &value), IsOk());
+      ASSERT_THAT(trie.Insert(kCommonEnglishWords[i], &value), IsOk());
     }
 
     IcingDynamicTrieHeader header;
@@ -1199,14 +1314,13 @@ TEST_F(IcingDynamicTrieTest, TrieShouldRespectLimits) {
     // Inserts all the test words before the last one.
     uint32_t value = 0;
     for (size_t i = 0; i < kCommonEnglishWordArrayLen - 1; ++i) {
-      ASSERT_THAT(trie.Insert(kCommonEnglishWords[i].data(), &value), IsOk());
+      ASSERT_THAT(trie.Insert(kCommonEnglishWords[i], &value), IsOk());
     }
 
     // Fails to insert the last word because no enough nodes left.
-    EXPECT_THAT(
-        trie.Insert(kCommonEnglishWords[kCommonEnglishWordArrayLen - 1].data(),
-                    &value),
-        StatusIs(libtextclassifier3::StatusCode::RESOURCE_EXHAUSTED));
+    EXPECT_THAT(trie.Insert(kCommonEnglishWords[kCommonEnglishWordArrayLen - 1],
+                            &value),
+                StatusIs(libtextclassifier3::StatusCode::RESOURCE_EXHAUSTED));
   }
 
   // Test a trie with just enough number of nexts.
@@ -1222,14 +1336,13 @@ TEST_F(IcingDynamicTrieTest, TrieShouldRespectLimits) {
     // Inserts all the test words before the last one.
     uint32_t value = 0;
     for (size_t i = 0; i < kCommonEnglishWordArrayLen - 1; ++i) {
-      ASSERT_THAT(trie.Insert(kCommonEnglishWords[i].data(), &value), IsOk());
+      ASSERT_THAT(trie.Insert(kCommonEnglishWords[i], &value), IsOk());
     }
 
     // Fails to insert the last word because no enough nexts left.
-    EXPECT_THAT(
-        trie.Insert(kCommonEnglishWords[kCommonEnglishWordArrayLen - 1].data(),
-                    &value),
-        StatusIs(libtextclassifier3::StatusCode::RESOURCE_EXHAUSTED));
+    EXPECT_THAT(trie.Insert(kCommonEnglishWords[kCommonEnglishWordArrayLen - 1],
+                            &value),
+                StatusIs(libtextclassifier3::StatusCode::RESOURCE_EXHAUSTED));
   }
 
   // Test a trie with just enough suffixes size.
@@ -1245,14 +1358,13 @@ TEST_F(IcingDynamicTrieTest, TrieShouldRespectLimits) {
     // Inserts all the test words before the last one.
     uint32_t value = 0;
     for (size_t i = 0; i < kCommonEnglishWordArrayLen - 1; ++i) {
-      ASSERT_THAT(trie.Insert(kCommonEnglishWords[i].data(), &value), IsOk());
+      ASSERT_THAT(trie.Insert(kCommonEnglishWords[i], &value), IsOk());
     }
 
     // Fails to insert the last word because no enough space for more suffixes.
-    EXPECT_THAT(
-        trie.Insert(kCommonEnglishWords[kCommonEnglishWordArrayLen - 1].data(),
-                    &value),
-        StatusIs(libtextclassifier3::StatusCode::RESOURCE_EXHAUSTED));
+    EXPECT_THAT(trie.Insert(kCommonEnglishWords[kCommonEnglishWordArrayLen - 1],
+                            &value),
+                StatusIs(libtextclassifier3::StatusCode::RESOURCE_EXHAUSTED));
   }
 }
 
diff --git a/icing/legacy/index/icing-filesystem.cc b/icing/legacy/index/icing-filesystem.cc
index fbf5a27..175e075 100644
--- a/icing/legacy/index/icing-filesystem.cc
+++ b/icing/legacy/index/icing-filesystem.cc
@@ -27,10 +27,16 @@
 
 #include <algorithm>
 #include <cerrno>
+#include <cstddef>
+#include <cstdint>
+#include <cstdio>
+#include <cstring>
+#include <memory>
+#include <string>
 #include <unordered_set>
+#include <vector>
 
 #include "icing/absl_ports/str_cat.h"
-#include "icing/legacy/core/icing-string-util.h"
 #include "icing/legacy/index/icing-mmapper.h"
 #include "icing/legacy/portable/icing-zlib.h"
 #include "icing/util/logging.h"
@@ -70,7 +76,8 @@ void LogOpenFileDescriptors() {
   }
   int fd_lim = rlim.rlim_cur;
   if (fd_lim > kMaxFileDescriptorsToStat) {
-    ICING_LOG(ERROR) << "Maximum number of file descriptors (" << fd_lim << ") too large.";
+    ICING_LOG(ERROR) << "Maximum number of file descriptors (" << fd_lim
+                     << ") too large.";
     fd_lim = kMaxFileDescriptorsToStat;
   }
   ICING_LOG(ERROR) << "Listing up to " << fd_lim << " file descriptors.";
@@ -177,7 +184,8 @@ bool IcingFilesystem::DeleteFile(const char *file_name) const {
   int ret = unlink(file_name);
   bool success = (ret == 0) || (errno == ENOENT);
   if (!success) {
-    ICING_LOG(ERROR) << "Deleting file " << file_name << " failed: " << strerror(errno);
+    ICING_LOG(ERROR) << "Deleting file " << file_name
+                     << " failed: " << strerror(errno);
   }
   return success;
 }
@@ -186,7 +194,8 @@ bool IcingFilesystem::DeleteDirectory(const char *dir_name) const {
   int ret = rmdir(dir_name);
   bool success = (ret == 0) || (errno == ENOENT);
   if (!success) {
-    ICING_LOG(ERROR) << "Deleting directory " << dir_name << " failed: " << strerror(errno);
+    ICING_LOG(ERROR) << "Deleting directory " << dir_name
+                     << " failed: " << strerror(errno);
   }
   return success;
 }
@@ -234,7 +243,8 @@ bool IcingFilesystem::FileExists(const char *file_name) const {
     exists = S_ISREG(st.st_mode) != 0;
   } else {
     if (errno != ENOENT) {
-      ICING_LOG(ERROR) << "Unable to stat file " << file_name << ": " << strerror(errno);
+      ICING_LOG(ERROR) << "Unable to stat file " << file_name << ": "
+                       << strerror(errno);
     }
     exists = false;
   }
@@ -248,7 +258,8 @@ bool IcingFilesystem::DirectoryExists(const char *dir_name) const {
     exists = S_ISDIR(st.st_mode) != 0;
   } else {
     if (errno != ENOENT) {
-      ICING_LOG(ERROR) << "Unable to stat directory " << dir_name << ": " << strerror(errno);
+      ICING_LOG(ERROR) << "Unable to stat directory " << dir_name << ": "
+                       << strerror(errno);
     }
     exists = false;
   }
@@ -370,7 +381,8 @@ uint64_t IcingFilesystem::GetFileSize(const char *filename) const {
   struct stat st;
   uint64_t size = kBadFileSize;
   if (stat(filename, &st) < 0) {
-    ICING_LOG(ERROR) << "Unable to stat file " << filename << ": " << strerror(errno);
+    ICING_LOG(ERROR) << "Unable to stat file " << filename << ": "
+                     << strerror(errno);
   } else {
     size = st.st_size;
   }
@@ -405,6 +417,31 @@ bool IcingFilesystem::Grow(int fd, uint64_t new_size) const {
   return (ret == 0);
 }
 
+bool IcingFilesystem::GrowUsingPWrite(int fd, uint64_t new_size) const {
+  uint64_t curr_file_size = GetFileSize(fd);
+  if (curr_file_size == kBadFileSize) {
+    return false;
+  }
+  if (new_size <= curr_file_size) {
+    return true;
+  }
+
+  uint64_t page_size = IcingMMapper::system_page_size();
+  auto buf = std::make_unique<uint8_t[]>(page_size);
+  uint64_t size_to_write = std::min(page_size - (curr_file_size % page_size),
+                                    new_size - curr_file_size);
+  while (size_to_write > 0 && curr_file_size < new_size) {
+    if (!PWrite(fd, curr_file_size, buf.get(), size_to_write)) {
+      ICING_LOG(ERROR) << "Failed to grow file using pwrite.";
+      return false;
+    }
+    curr_file_size += size_to_write;
+    size_to_write = std::min(page_size - (curr_file_size % page_size),
+                             new_size - curr_file_size);
+  }
+  return true;
+}
+
 bool IcingFilesystem::Write(int fd, const void *data, size_t data_size) const {
   size_t write_len = data_size;
   do {
@@ -456,7 +493,8 @@ bool IcingFilesystem::DataSync(int fd) const {
 bool IcingFilesystem::RenameFile(const char *old_name,
                                  const char *new_name) const {
   if (rename(old_name, new_name) < 0) {
-    ICING_LOG(ERROR) << "Unable to rename file " << old_name << " to " << new_name << ": " << strerror(errno);
+    ICING_LOG(ERROR) << "Unable to rename file " << old_name << " to "
+                     << new_name << ": " << strerror(errno);
     return false;
   }
   return true;
@@ -494,7 +532,8 @@ bool IcingFilesystem::CreateDirectory(const char *dir_name) const {
     if (mkdir(dir_name, S_IRUSR | S_IWUSR | S_IXUSR) == 0) {
       success = true;
     } else {
-      ICING_LOG(ERROR) << "Creating directory " << dir_name << " failed: " << strerror(errno);
+      ICING_LOG(ERROR) << "Creating directory " << dir_name
+                       << " failed: " << strerror(errno);
     }
   }
   return success;
diff --git a/icing/legacy/index/icing-filesystem.h b/icing/legacy/index/icing-filesystem.h
index ce75a82..6e00de0 100644
--- a/icing/legacy/index/icing-filesystem.h
+++ b/icing/legacy/index/icing-filesystem.h
@@ -167,6 +167,22 @@ class IcingFilesystem {
   // position pointer.
   virtual bool Grow(int fd, uint64_t new_size) const;
 
+  // Implementation of Grow that calls Pwrite to directly append zero-filled
+  // byte arrays into the file. Does not change the position pointer.
+  //
+  // This is different from Grow (which uses ftruncate), which doesn't actually
+  // allocate an underlying disk block. Grow has lead to crashes in the past as
+  // there is no effective way to signal that disk block allocation failed, and
+  // we end up trying to access memory-mapped region with unallocated disk
+  // block.
+  //
+  // Using Pwrite forces block allocation and ensures that we can return false
+  // when the disk block is not allocated.
+  //
+  // Note: This method does not change the file size if new_size <= current
+  // file size. Caller should use Truncate for reducing the file size.
+  virtual bool GrowUsingPWrite(int fd, uint64_t new_size) const;
+
   // Writes to a file.  Returns true if all the data was successfully
   // written.  Handles interrupted writes.
   virtual bool Write(int fd, const void *data, size_t data_size) const;
diff --git a/icing/legacy/index/icing-filesystem_test.cc b/icing/legacy/index/icing-filesystem_test.cc
new file mode 100644
index 0000000..a724f36
--- /dev/null
+++ b/icing/legacy/index/icing-filesystem_test.cc
@@ -0,0 +1,136 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include "icing/legacy/index/icing-filesystem.h"
+
+#include <cstddef>
+#include <cstdio>
+#include <string>
+
+#include "gmock/gmock.h"
+#include "gtest/gtest.h"
+#include "icing/testing/tmp-directory.h"
+
+using ::testing::Eq;
+using ::testing::Ne;
+
+namespace icing {
+namespace lib {
+
+namespace {
+
+class IcingFilesystemTest : public testing::Test {
+ protected:
+  void SetUp() override {
+    temp_dir_ = GetTestTempDir() + "/icing_filesystem";
+    IcingFilesystem filesystem;
+    ASSERT_TRUE(filesystem.CreateDirectoryRecursively(temp_dir_.c_str()));
+  }
+
+  void TearDown() override {
+    IcingFilesystem filesystem;
+    EXPECT_TRUE(filesystem.DeleteDirectoryRecursively(temp_dir_.c_str()));
+  }
+
+  // Write junk data of given size to the given file descriptor
+  void WriteJunk(int fd, size_t size) {
+    const int kBufLen = 1024;
+    int buf[kBufLen];
+    for (int i = 0; i < kBufLen; ++i) {
+      buf[i] = i;
+    }
+    const int kBufSize = kBufLen * sizeof(int);
+
+    IcingFilesystem filesystem;
+    for (size_t i = 0; i < size / kBufSize; ++i) {
+      EXPECT_TRUE(filesystem.Write(fd, buf, kBufSize));
+    }
+    if (size % kBufSize) {
+      EXPECT_TRUE(filesystem.Write(fd, buf, size % kBufSize));
+    }
+  }
+
+  std::string temp_dir_;
+};
+
+TEST_F(IcingFilesystemTest, FSync) {
+  IcingFilesystem filesystem;
+  const std::string foo_file = temp_dir_ + "/foo_file";
+  int fd = filesystem.OpenForWrite(foo_file.c_str());
+  ASSERT_THAT(fd, Ne(-1));
+  EXPECT_TRUE(filesystem.DataSync(fd));
+  close(fd);
+}
+
+TEST_F(IcingFilesystemTest, Truncate) {
+  IcingFilesystem filesystem;
+  const std::string foo_file = temp_dir_ + "/foo_file";
+  const char* filename = foo_file.c_str();
+  int fd = filesystem.OpenForWrite(filename);
+  ASSERT_THAT(fd, Ne(-1));
+  char data[10000] = {0};  // Zero-init to satisfy msan.
+  EXPECT_TRUE(filesystem.Write(fd, data, sizeof(data)));
+  close(fd);
+  EXPECT_THAT(filesystem.GetFileSize(filename), Eq(sizeof(data)));
+  EXPECT_TRUE(filesystem.Truncate(filename, sizeof(data) / 2));
+  EXPECT_THAT(filesystem.GetFileSize(filename), Eq(sizeof(data) / 2));
+  EXPECT_TRUE(filesystem.Truncate(filename, 0));
+  EXPECT_THAT(filesystem.GetFileSize(filename), Eq(0u));
+}
+
+TEST_F(IcingFilesystemTest, Grow) {
+  IcingFilesystem filesystem;
+  const std::string foo_file = temp_dir_ + "/foo_file";
+  const char* filename = foo_file.c_str();
+  int fd = filesystem.OpenForWrite(filename);
+  ASSERT_THAT(fd, Ne(-1));
+  char data[10000] = {0};  // Zero-init to satisfy msan.
+  EXPECT_TRUE(filesystem.Write(fd, data, sizeof(data)));
+
+  EXPECT_THAT(filesystem.GetFileSize(filename), Eq(sizeof(data)));
+  EXPECT_TRUE(filesystem.Grow(fd, sizeof(data) * 2));
+  EXPECT_THAT(filesystem.GetFileSize(filename), Eq(sizeof(data) * 2));
+  close(fd);
+}
+
+TEST_F(IcingFilesystemTest, GrowUsingWrite) {
+  IcingFilesystem filesystem;
+  const std::string foo_file_grow = temp_dir_ + "/foo_file_grow";
+  const std::string foo_file_grow_with_write =
+      temp_dir_ + "/foo_file_grow_with_write";
+
+  const char* filename = foo_file_grow_with_write.c_str();
+
+  int fd = filesystem.OpenForWrite(filename);
+  ASSERT_THAT(fd, Ne(-1));
+
+  char data[10000] = {0};  // Zero-init to satisfy msan.
+  EXPECT_TRUE(filesystem.Write(fd, data, sizeof(data)));
+  // lseek file pointer to a random place in the file
+  off_t position = lseek(fd, 10, SEEK_SET);
+
+  EXPECT_THAT(filesystem.GetFileSize(fd), Eq(sizeof(data)));
+  EXPECT_THAT(lseek(fd, 0, SEEK_CUR), Eq(position));
+
+  EXPECT_TRUE(filesystem.GrowUsingPWrite(fd, sizeof(data) * 2));
+  EXPECT_THAT(filesystem.GetFileSize(fd), Eq(sizeof(data) * 2));
+  // Verify that the file pointer position is unchanged after the grow.
+  EXPECT_THAT(lseek(fd, 0, SEEK_CUR), Eq(position));
+  close(fd);
+}
+
+}  // namespace
+
+}  // namespace lib
+}  // namespace icing
diff --git a/icing/legacy/index/icing-flash-bitmap.cc b/icing/legacy/index/icing-flash-bitmap.cc
index 774308f..571fc21 100644
--- a/icing/legacy/index/icing-flash-bitmap.cc
+++ b/icing/legacy/index/icing-flash-bitmap.cc
@@ -16,12 +16,18 @@
 
 #include <sys/mman.h>
 
+#include <cstddef>
+#include <cstdint>
+#include <cstring>
 #include <memory>
+#include <string>
 
 #include "icing/legacy/core/icing-string-util.h"
 #include "icing/legacy/core/icing-timer.h"
 #include "icing/legacy/index/icing-bit-util.h"
 #include "icing/legacy/index/icing-filesystem.h"
+#include "icing/legacy/index/icing-mmapper.h"
+#include "icing/util/crc32.h"
 #include "icing/util/logging.h"
 
 namespace icing {
@@ -82,11 +88,13 @@ bool IcingFlashBitmap::Verify() const {
   }
   Accessor accessor(mmapper_.get());
   if (accessor.header()->magic != kMagic) {
-    ICING_LOG(ERROR) << "Flash bitmap " << filename_ << " has incorrect magic header";
+    ICING_LOG(ERROR) << "Flash bitmap " << filename_
+                     << " has incorrect magic header";
     return false;
   }
   if (accessor.header()->version != kCurVersion) {
-    ICING_LOG(ERROR) << "Flash bitmap " << filename_ << " has incorrect version";
+    ICING_LOG(ERROR) << "Flash bitmap " << filename_
+                     << " has incorrect version";
     return false;
   }
   if (accessor.header()->dirty) {
@@ -96,7 +104,8 @@ bool IcingFlashBitmap::Verify() const {
   uint32_t crc =
       IcingStringUtil::UpdateCrc32(0, accessor.data(), accessor.data_size());
   if (accessor.header()->crc != crc) {
-    ICING_LOG(ERROR) << "Flash bitmap " << filename_ << " has incorrect CRC32 " << accessor.header()->crc << " " << crc;
+    ICING_LOG(ERROR) << "Flash bitmap " << filename_ << " has incorrect CRC32 "
+                     << accessor.header()->crc << " " << crc;
     return false;
   }
   return true;
@@ -125,7 +134,7 @@ bool IcingFlashBitmap::Init() {
 
   // Make sure we have something to mmap.
   if (orig_file_size < kGrowSize) {
-    if (!filesystem_->Grow(fd.get(), kGrowSize)) {
+    if (!filesystem_->GrowUsingPWrite(fd.get(), kGrowSize)) {
       goto error;
     }
     file_size = kGrowSize;
@@ -226,7 +235,7 @@ bool IcingFlashBitmap::Delete() {
   return filesystem_->DeleteFile(filename_.c_str());
 }
 
-bool IcingFlashBitmap::Sync() const {
+bool IcingFlashBitmap::Sync() {
   if (!is_initialized()) {
     ICING_LOG(FATAL) << "Bitmap not initialized";
   }
@@ -243,23 +252,37 @@ uint64_t IcingFlashBitmap::GetDiskUsage() const {
   return filesystem_->GetFileDiskUsage(filename_.c_str());
 }
 
-uint32_t IcingFlashBitmap::UpdateCrc() const {
+Crc32 IcingFlashBitmap::UpdateCrc() {
+  if (mmapper_ == nullptr) {
+    // Non-existent mmapper means file does not exist.
+    return Crc32();
+  }
   Accessor accessor(mmapper_.get());
   if (open_type_ == READ_WRITE && accessor.header()->dirty) {
-    accessor.header()->crc = IcingStringUtil::UpdateCrc32(
-        kEmptyCrc, accessor.data(), accessor.data_size());
+    Crc32 crc(std::string_view(accessor.data(), accessor.data_size()));
+    accessor.header()->crc = crc.Get();
     accessor.header()->dirty = false;
   }
+  return Crc32(accessor.header()->crc);
+}
 
-  // Non-existent mmapper means file does not exist. An empty file has
-  // a crc of kEmptyCrc, so just return that.
-  return mmapper_.get() ? accessor.header()->crc : kEmptyCrc;
+Crc32 IcingFlashBitmap::GetCrc() const {
+  if (mmapper_ == nullptr) {
+    // Non-existent mmapper means file does not exist.
+    return Crc32();
+  }
+  Accessor accessor(mmapper_.get());
+  if (open_type_ != READ_WRITE || !accessor.header()->dirty) {
+    return Crc32(accessor.header()->crc);
+  }
+  return Crc32(std::string_view(accessor.data(), accessor.data_size()));
 }
 
 bool IcingFlashBitmap::Grow(size_t new_file_size) {
   IcingScopedFd fd(filesystem_->OpenForWrite(filename_.c_str()));
-  if (!filesystem_->Grow(fd.get(), new_file_size)) {
-    ICING_LOG(ERROR) << "Grow " << filename_ << " to new size " << new_file_size << " failed";
+  if (!filesystem_->GrowUsingPWrite(fd.get(), new_file_size)) {
+    ICING_LOG(ERROR) << "GrowUsingPWrite " << filename_ << " to new size "
+                     << new_file_size << " failed";
     return false;
   }
   if (!mmapper_->Remap(fd.get(), 0, new_file_size)) {
diff --git a/icing/legacy/index/icing-flash-bitmap.h b/icing/legacy/index/icing-flash-bitmap.h
index 6bb9591..49c5004 100644
--- a/icing/legacy/index/icing-flash-bitmap.h
+++ b/icing/legacy/index/icing-flash-bitmap.h
@@ -43,6 +43,7 @@
 
 #include "icing/legacy/index/icing-filesystem.h"
 #include "icing/legacy/index/icing-mmapper.h"
+#include "icing/util/crc32.h"
 
 namespace icing {
 namespace lib {
@@ -51,7 +52,6 @@ class IcingFlashBitmap {
  public:
   using Word = uint32_t;
 
-  static constexpr uint32_t kEmptyCrc = 0;
   static constexpr size_t kGrowSize = (1u << 12);  // 4KB;
   static constexpr size_t kWordBits = 8 * sizeof(Word);
 
@@ -92,7 +92,7 @@ class IcingFlashBitmap {
   bool Clear() { return Delete() && Init(); }
 
   // Sync the changes to disk.
-  bool Sync() const;
+  bool Sync();
 
   uint64_t GetDiskUsage() const;
 
@@ -118,7 +118,11 @@ class IcingFlashBitmap {
   const std::string &filename() const { return filename_; }
 
   // If the bitmap is dirty, update the crc and mark it clean.
-  uint32_t UpdateCrc() const;
+  Crc32 UpdateCrc();
+
+  // Calculates the crc and returns it. This function does NOT update the crc
+  // in the header.
+  Crc32 GetCrc() const;
 
  private:
   class Accessor;
diff --git a/icing/legacy/index/icing-flash-bitmap_test.cc b/icing/legacy/index/icing-flash-bitmap_test.cc
new file mode 100644
index 0000000..e4e27fc
--- /dev/null
+++ b/icing/legacy/index/icing-flash-bitmap_test.cc
@@ -0,0 +1,113 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include "icing/legacy/index/icing-flash-bitmap.h"
+
+#include <string>
+
+#include "gmock/gmock.h"
+#include "gtest/gtest.h"
+#include "icing/legacy/index/icing-filesystem.h"
+#include "icing/testing/tmp-directory.h"
+#include "icing/util/crc32.h"
+
+namespace icing {
+namespace lib {
+
+namespace {
+
+using testing::Eq;
+
+class IcingFlashBitmapTest : public ::testing::Test {
+ protected:
+  void SetUp() override {
+    bitmap_files_dir_ = GetTestTempDir() + "/array_files";
+    filesystem_.CreateDirectoryRecursively(bitmap_files_dir_.c_str());
+    bitmap_file_ = bitmap_files_dir_ + "/array_file";
+  }
+
+  void TearDown() override {
+    filesystem_.DeleteDirectoryRecursively(bitmap_files_dir_.c_str());
+  }
+
+  IcingFilesystem filesystem_;
+  std::string bitmap_files_dir_;
+  std::string bitmap_file_;
+};
+
+TEST_F(IcingFlashBitmapTest, UpdateCrc) {
+  IcingFlashBitmap bitmap(bitmap_file_, &filesystem_);
+  ASSERT_TRUE(bitmap.Init());
+
+  // Initial Crc should be 0.
+  EXPECT_THAT(bitmap.GetCrc(), Eq(Crc32()));
+  EXPECT_THAT(bitmap.UpdateCrc(), Eq(Crc32()));
+  EXPECT_THAT(bitmap.GetCrc(), Eq(Crc32()));
+
+  EXPECT_TRUE(bitmap.SetBit(0, true));
+
+  EXPECT_THAT(bitmap.GetCrc(), Eq(Crc32(1180633950)));
+  EXPECT_THAT(bitmap.UpdateCrc(), Eq(Crc32(1180633950)));
+  EXPECT_THAT(bitmap.GetCrc(), Eq(Crc32(1180633950)));
+}
+
+TEST_F(IcingFlashBitmapTest, GetCrcDoesNotUpdateHeader) {
+  IcingFlashBitmap bitmap_one(bitmap_file_, &filesystem_);
+  ASSERT_TRUE(bitmap_one.Init());
+
+  // Initial Crc should be 0.
+  EXPECT_THAT(bitmap_one.GetCrc(), Eq(Crc32()));
+  EXPECT_THAT(bitmap_one.UpdateCrc(), Eq(Crc32()));
+  EXPECT_THAT(bitmap_one.GetCrc(), Eq(Crc32()));
+
+  EXPECT_TRUE(bitmap_one.SetBit(0, true));
+
+  EXPECT_THAT(bitmap_one.GetCrc(), Eq(Crc32(1180633950)));
+
+  // Create a second bitmap for the same file. This should see the changes made
+  // above, but they won't match the checksum (which should still be 0). This
+  // should cause init to fail.
+  IcingFlashBitmap bitmap_two(bitmap_file_, &filesystem_);
+  EXPECT_TRUE(bitmap_two.Init());
+  EXPECT_FALSE(bitmap_two.Verify());
+}
+
+TEST_F(IcingFlashBitmapTest, UpdateCrcDoesUpdateHeader) {
+  IcingFlashBitmap bitmap_one(bitmap_file_, &filesystem_);
+  ASSERT_TRUE(bitmap_one.Init());
+
+  // Initial Crc should be 0.
+  EXPECT_THAT(bitmap_one.GetCrc(), Eq(Crc32()));
+  EXPECT_THAT(bitmap_one.UpdateCrc(), Eq(Crc32()));
+  EXPECT_THAT(bitmap_one.GetCrc(), Eq(Crc32()));
+
+  EXPECT_TRUE(bitmap_one.SetBit(0, true));
+
+  EXPECT_THAT(bitmap_one.GetCrc(), Eq(Crc32(1180633950)));
+  EXPECT_THAT(bitmap_one.UpdateCrc(), Eq(Crc32(1180633950)));
+  EXPECT_THAT(bitmap_one.GetCrc(), Eq(Crc32(1180633950)));
+
+  // Create a second bitmap for the same file. This should see the changes made
+  // above, but they won't match the checksum (which should still be 0). This
+  // should cause init to fail.
+  IcingFlashBitmap bitmap_two(bitmap_file_, &filesystem_);
+  EXPECT_TRUE(bitmap_two.Init());
+  EXPECT_TRUE(bitmap_two.Verify());
+  EXPECT_THAT(bitmap_one.GetCrc(), Eq(Crc32(1180633950)));
+}
+
+}  // namespace
+
+}  // namespace lib
+}  // namespace icing
\ No newline at end of file
diff --git a/icing/legacy/index/icing-mock-filesystem.h b/icing/legacy/index/icing-mock-filesystem.h
index 122ee7b..e2af303 100644
--- a/icing/legacy/index/icing-mock-filesystem.h
+++ b/icing/legacy/index/icing-mock-filesystem.h
@@ -18,7 +18,6 @@
 #include <cstdint>
 #include <cstdio>
 #include <cstring>
-#include <memory>
 #include <string>
 #include <vector>
 
@@ -114,6 +113,11 @@ class IcingMockFilesystem : public IcingFilesystem {
       return real_icing_filesystem_.Grow(fd, new_size);
     });
 
+    ON_CALL(*this, GrowUsingPWrite)
+        .WillByDefault([this](int fd, uint64_t new_size) {
+          return real_icing_filesystem_.GrowUsingPWrite(fd, new_size);
+        });
+
     ON_CALL(*this, Write)
         .WillByDefault([this](int fd, const void *data, size_t data_size) {
           return real_icing_filesystem_.Write(fd, data, data_size);
@@ -209,6 +213,9 @@ class IcingMockFilesystem : public IcingFilesystem {
 
   MOCK_METHOD(bool, Grow, (int fd, uint64_t new_size), (const, override));
 
+  MOCK_METHOD(bool, GrowUsingPWrite, (int fd, uint64_t new_size),
+              (const, override));
+
   MOCK_METHOD(bool, Write, (int fd, const void *data, size_t data_size),
               (const, override));
   MOCK_METHOD(bool, PWrite,
diff --git a/icing/legacy/index/icing-storage-collection.cc b/icing/legacy/index/icing-storage-collection.cc
index d31f892..59fcf1c 100644
--- a/icing/legacy/index/icing-storage-collection.cc
+++ b/icing/legacy/index/icing-storage-collection.cc
@@ -14,8 +14,11 @@
 
 #include "icing/legacy/index/icing-storage-collection.h"
 
+#include <string>
+
 #include "icing/legacy/core/icing-compat.h"
 #include "icing/legacy/index/icing-filesystem.h"
+#include "icing/util/crc32.h"
 
 namespace icing {
 namespace lib {
@@ -103,10 +106,13 @@ uint64_t IcingStorageCollection::GetDiskUsage() const {
   return total;
 }
 
-void IcingStorageCollection::OnSleep() {
+Crc32 IcingStorageCollection::UpdateCrc() {
+  Crc32 crc32;
   for (size_t i = 0; i < files_.size(); ++i) {
-    files_[i].file->OnSleep();
+    Crc32 this_crc = files_[i].file->UpdateCrc();
+    crc32.Append(std::to_string(this_crc.Get()));
   }
+  return crc32;
 }
 
 void IcingStorageCollection::GetDebugInfo(int verbosity,
diff --git a/icing/legacy/index/icing-storage-collection.h b/icing/legacy/index/icing-storage-collection.h
index dedfe33..30c7951 100644
--- a/icing/legacy/index/icing-storage-collection.h
+++ b/icing/legacy/index/icing-storage-collection.h
@@ -25,6 +25,7 @@
 #include <vector>
 
 #include "icing/legacy/index/icing-storage.h"
+#include "icing/util/crc32.h"
 
 namespace icing {
 namespace lib {
@@ -44,7 +45,9 @@ class IcingStorageCollection : public IIcingStorage {
   bool Remove() override;
   bool Sync() override;
   uint64_t GetDiskUsage() const override;
-  void OnSleep() override;
+
+  Crc32 UpdateCrc() override;
+
   void GetDebugInfo(int verbosity, std::string *out) const override;
 
  private:
diff --git a/icing/legacy/index/icing-storage.h b/icing/legacy/index/icing-storage.h
index 58b6aa1..c56f8a9 100644
--- a/icing/legacy/index/icing-storage.h
+++ b/icing/legacy/index/icing-storage.h
@@ -23,6 +23,8 @@
 #include <cstdint>
 #include <string>
 
+#include "icing/util/crc32.h"
+
 namespace icing {
 namespace lib {
 
@@ -70,10 +72,9 @@ class IIcingStorage {
   // Returns kBadFileSize on error.
   virtual uint64_t GetDiskUsage() const = 0;
 
-  // Optional handler for when our process is entering a vulnerable
-  // state (highly likely to get killed). Default implementation does
-  // nothing.
-  virtual void OnSleep() {}
+  // Updates any checksums that this storage maintains.
+  // By default, does nothing.
+  virtual Crc32 UpdateCrc() { return Crc32(); }
 
   virtual void GetDebugInfo(int verbosity, std::string* out) const = 0;
 
diff --git a/icing/monkey_test/icing-monkey-test-runner.cc b/icing/monkey_test/icing-monkey-test-runner.cc
index 76e41ce..bc04089 100644
--- a/icing/monkey_test/icing-monkey-test-runner.cc
+++ b/icing/monkey_test/icing-monkey-test-runner.cc
@@ -18,8 +18,11 @@
 #include <array>
 #include <cstdint>
 #include <functional>
+#include <iomanip>
+#include <ios>
 #include <memory>
 #include <random>
+#include <sstream>
 #include <string>
 #include <utility>
 #include <vector>
@@ -41,6 +44,7 @@
 #include "icing/proto/search.pb.h"
 #include "icing/proto/status.pb.h"
 #include "icing/proto/term.pb.h"
+#include "icing/query/query-features.h"
 #include "icing/result/result-state-manager.h"
 #include "icing/testing/common-matchers.h"
 #include "icing/testing/tmp-directory.h"
@@ -58,21 +62,53 @@ using ::testing::Not;
 using ::testing::SizeIs;
 using ::testing::UnorderedElementsAreArray;
 
+bool GetRandomBoolean(MonkeyTestRandomEngine* random) {
+  std::uniform_int_distribution<> dist(0, 1);
+  return dist(*random) == 1;
+}
+
 SearchSpecProto GenerateRandomSearchSpecProto(
     MonkeyTestRandomEngine* random,
     MonkeyDocumentGenerator* document_generator) {
-  // Get a random token from the language set as a single term query.
-  std::string query(document_generator->GetToken());
-  std::uniform_int_distribution<> dist(0, 1);
-  TermMatchType::Code term_match_type = TermMatchType::EXACT_ONLY;
-  if (dist(*random) == 1) {
-    term_match_type = TermMatchType::PREFIX;
-    // Randomly drop a suffix of query to test prefix query.
-    std::uniform_int_distribution<> size_dist(1, query.size());
-    query.resize(size_dist(*random));
+  SearchSpecProto search_spec;
+  std::string query;
+
+  // 50% chance of doing a term query, and 50% chance of doing an embedding
+  // query.
+  if (GetRandomBoolean(random)) {
+    // Get a random token from the language set as a single term query.
+    query = document_generator->GetToken();
+    TermMatchType::Code term_match_type = TermMatchType::EXACT_ONLY;
+    if (GetRandomBoolean(random)) {
+      term_match_type = TermMatchType::PREFIX;
+      // Randomly drop a suffix of query to test prefix query.
+      std::uniform_int_distribution<> size_dist(1, query.size());
+      query.resize(size_dist(*random));
+    }
+    search_spec.set_term_match_type(term_match_type);
+  } else {
+    std::uniform_real_distribution<float> range_dist(-1.0, 1.0);
+    float low = range_dist(*random);
+    float high = range_dist(*random);
+    if (low > high) {
+      std::swap(low, high);
+    }
+
+    std::ostringstream stream;
+    stream << std::fixed << std::setprecision(2)
+           << "semanticSearch(getEmbeddingParameter(0), " << low << ", " << high
+           << ")";
+    query = stream.str();
+    search_spec.set_embedding_query_metric_type(
+        SearchSpecProto::EmbeddingQueryMetricType::COSINE);
+    search_spec.add_enabled_features(
+        std::string(kListFilterQueryLanguageFeature));
+    *search_spec.add_embedding_query_vectors() =
+        document_generator->GetRandomVector();
   }
+
   // 50% chance of getting a section restriction.
-  if (dist(*random) == 1) {
+  if (GetRandomBoolean(random)) {
     const SchemaTypeConfigProto& type_config = document_generator->GetType();
     if (type_config.properties_size() > 0) {
       std::uniform_int_distribution<> prop_dist(
@@ -82,8 +118,6 @@ SearchSpecProto GenerateRandomSearchSpecProto(
           query);
     }
   }
-  SearchSpecProto search_spec;
-  search_spec.set_term_match_type(term_match_type);
   search_spec.set_query(query);
   return search_spec;
 }
@@ -425,6 +459,7 @@ void IcingMonkeyTestRunner::DoSearch() {
   const ResultSpecProto::SnippetSpecProto snippet_spec =
       result_spec->snippet_spec();
   bool is_projection_enabled = !result_spec->type_property_masks().empty();
+  bool is_embedding_query = !search_spec->embedding_query_vectors().empty();
 
   ICING_LOG(INFO) << "Monkey searching by query: " << search_spec->query()
                   << ", term_match_type: " << search_spec->term_match_type();
@@ -469,7 +504,8 @@ void IcingMonkeyTestRunner::DoSearch() {
   if (exp_documents.size() >= 30000) {
     return;
   }
-  if (snippet_spec.num_matches_per_property() > 0 && !is_projection_enabled) {
+  if (snippet_spec.num_matches_per_property() > 0 && !is_projection_enabled &&
+      !is_embedding_query) {
     ASSERT_THAT(num_snippeted,
                 Eq(std::min<uint32_t>(exp_documents.size(),
                                       snippet_spec.num_to_snippet())));
@@ -502,9 +538,7 @@ void IcingMonkeyTestRunner::DoOptimize() {
 }
 
 void IcingMonkeyTestRunner::CreateIcingSearchEngine() {
-  std::uniform_int_distribution<> dist(0, 1);
-
-  bool always_rebuild_index_optimize = dist(random_);
+  bool always_rebuild_index_optimize = GetRandomBoolean(&random_);
   float optimize_rebuild_index_threshold =
       always_rebuild_index_optimize ? 0.0 : 0.9;
 
@@ -516,7 +550,7 @@ void IcingMonkeyTestRunner::CreateIcingSearchEngine() {
   // The method will be called every time when we ReloadFromDisk(), so randomly
   // flip this flag to test document store's compatibility.
   icing_options.set_document_store_namespace_id_fingerprint(
-      (bool)dist(random_));
+      GetRandomBoolean(&random_));
   icing_ = std::make_unique<IcingSearchEngine>(icing_options);
   ASSERT_THAT(icing_->Initialize().status(), ProtoIsOk());
 }
diff --git a/icing/monkey_test/icing-search-engine_monkey_test.cc b/icing/monkey_test/icing-search-engine_monkey_test.cc
index 436e27b..6d87ba9 100644
--- a/icing/monkey_test/icing-search-engine_monkey_test.cc
+++ b/icing/monkey_test/icing-search-engine_monkey_test.cc
@@ -44,7 +44,9 @@ TEST(IcingSearchEngineMonkeyTest, MonkeyTest) {
                                     kTotalNumSections,
                                     kTotalNumSections + 1,
                                     kTotalNumSections * 2};
-  config.possible_num_tokens_ = {0, 1, 4, 16, 64, 256};
+  config.possible_num_tokens = {0, 1, 4, 16, 64, 256};
+  config.possible_num_vectors = {0, 1, 4};
+  config.possible_vector_dimensions = {128, 512, 768};
   config.monkey_api_schedules = {
       {&IcingMonkeyTestRunner::DoPut, 500},
       {&IcingMonkeyTestRunner::DoSearch, 200},
@@ -74,7 +76,9 @@ TEST(DISABLED_IcingSearchEngineMonkeyTest, MonkeyManyDocTest) {
   // Due to the large amount of documents, we need to make each document smaller
   // to finish the test.
   config.possible_num_properties = {0, 1, 2};
-  config.possible_num_tokens_ = {0, 1, 4};
+  config.possible_num_tokens = {0, 1, 4};
+  config.possible_num_vectors = {0, 1, 2};
+  config.possible_vector_dimensions = {128};
 
   // No deletion is performed to preserve a large number of documents.
   config.monkey_api_schedules = {
diff --git a/icing/monkey_test/in-memory-icing-search-engine.cc b/icing/monkey_test/in-memory-icing-search-engine.cc
index 7baa06e..417153c 100644
--- a/icing/monkey_test/in-memory-icing-search-engine.cc
+++ b/icing/monkey_test/in-memory-icing-search-engine.cc
@@ -29,6 +29,7 @@
 #include "icing/absl_ports/canonical_errors.h"
 #include "icing/absl_ports/str_cat.h"
 #include "icing/absl_ports/str_join.h"
+#include "icing/index/embed/embedding-scorer.h"
 #include "icing/monkey_test/monkey-tokenized-document.h"
 #include "icing/proto/document.pb.h"
 #include "icing/proto/schema.pb.h"
@@ -50,6 +51,46 @@ bool IsPrefix(std::string_view s1, std::string_view s2) {
   return s1 == s2.substr(0, s1.length());
 }
 
+const std::string_view kSemanticSearchPrefix =
+    "semanticSearch(getEmbeddingParameter(0)";
+
+libtextclassifier3::StatusOr<std::pair<double, double>> GetEmbeddingSearchRange(
+    std::string_view s) {
+  std::vector<double> values;
+  std::string current_number;
+  int i = s.find(kSemanticSearchPrefix) + kSemanticSearchPrefix.length();
+  for (; i < s.size(); ++i) {
+    char c = s[i];
+    if (c == '.' || c == '-' || (c >= '0' && c <= '9')) {
+      current_number += c;
+    } else {
+      if (!current_number.empty()) {
+        values.push_back(std::stod(current_number));
+        current_number.clear();
+      }
+    }
+  }
+  if (values.size() != 2) {
+    return absl_ports::InvalidArgumentError(
+        absl_ports::StrCat("Not an embedding search.", s));
+  }
+  return std::make_pair(values[0], values[1]);
+}
+
+bool DoesVectorsMatch(const EmbeddingScorer *scorer,
+                      std::pair<double, double> embedding_search_range,
+                      const PropertyProto::VectorProto &vector1,
+                      const PropertyProto::VectorProto &vector2) {
+  if (vector1.model_signature() != vector2.model_signature() ||
+      vector1.values_size() != vector2.values_size()) {
+    return false;
+  }
+  float score = scorer->Score(vector1.values_size(), vector1.values().data(),
+                              vector2.values().data());
+  return embedding_search_range.first <= score &&
+         score <= embedding_search_range.second;
+}
+
 }  // namespace
 
 libtextclassifier3::StatusOr<const PropertyConfigProto *>
@@ -68,8 +109,8 @@ InMemoryIcingSearchEngine::GetPropertyConfig(
   return &property_iter->second;
 }
 
-libtextclassifier3::StatusOr<TermMatchType::Code>
-InMemoryIcingSearchEngine::GetTermMatchType(
+libtextclassifier3::StatusOr<InMemoryIcingSearchEngine::PropertyIndexInfo>
+InMemoryIcingSearchEngine::GetPropertyIndexInfo(
     const std::string &schema_type,
     const MonkeyTokenizedSection &section) const {
   bool in_indexable_properties_list = false;
@@ -87,11 +128,20 @@ InMemoryIcingSearchEngine::GetTermMatchType(
         GetPropertyConfig(curr_schema_type,
                           std::string(properties_in_path[i])));
     if (prop->data_type() == PropertyConfigProto::DataType::STRING) {
-      return prop->string_indexing_config().term_match_type();
+      TermMatchType::Code term_match_type =
+          prop->string_indexing_config().term_match_type();
+      bool indexable = term_match_type != TermMatchType::UNKNOWN;
+      return PropertyIndexInfo{indexable, term_match_type};
+    }
+    if (prop->data_type() == PropertyConfigProto::DataType::VECTOR) {
+      bool indexable =
+          prop->embedding_indexing_config().embedding_indexing_type() !=
+          EmbeddingIndexingConfig::EmbeddingIndexingType::UNKNOWN;
+      return PropertyIndexInfo{indexable};
     }
 
     if (prop->data_type() != PropertyConfigProto::DataType::DOCUMENT) {
-      return TermMatchType::Code::TermMatchType_Code_UNKNOWN;
+      return PropertyIndexInfo{/*indexable=*/false};
     }
 
     bool old_all_indexable_from_top = all_indexable_from_top;
@@ -112,46 +162,69 @@ InMemoryIcingSearchEngine::GetTermMatchType(
       }
       // Check in_indexable_properties_list again.
       if (!in_indexable_properties_list) {
-        return TermMatchType::Code::TermMatchType_Code_UNKNOWN;
+        return PropertyIndexInfo{/*indexable=*/false};
       }
     }
     curr_schema_type = prop->document_indexing_config().GetTypeName();
   }
-  return TermMatchType::Code::TermMatchType_Code_UNKNOWN;
+  return PropertyIndexInfo{/*indexable=*/false};
 }
 
 libtextclassifier3::StatusOr<bool>
 InMemoryIcingSearchEngine::DoesDocumentMatchQuery(
-    const MonkeyTokenizedDocument &document, const std::string &query,
-    TermMatchType::Code term_match_type) const {
+    const MonkeyTokenizedDocument &document,
+    const SearchSpecProto &search_spec) const {
+  std::string_view query = search_spec.query();
   std::vector<std::string_view> strs = absl_ports::StrSplit(query, ":");
-  std::string_view query_term;
   std::string_view section_restrict;
   if (strs.size() > 1) {
     section_restrict = strs[0];
-    query_term = strs[1];
-  } else {
-    query_term = query;
+    query = strs[1];
+  }
+
+  // Preprocess for embedding search.
+  libtextclassifier3::StatusOr<std::pair<double, double>>
+      embedding_search_range_or = GetEmbeddingSearchRange(query);
+  std::unique_ptr<EmbeddingScorer> embedding_scorer;
+  if (embedding_search_range_or.ok()) {
+    ICING_ASSIGN_OR_RETURN(
+        embedding_scorer,
+        EmbeddingScorer::Create(search_spec.embedding_query_metric_type()));
   }
+
   for (const MonkeyTokenizedSection &section : document.tokenized_sections) {
     if (!section_restrict.empty() && section.path != section_restrict) {
       continue;
     }
     ICING_ASSIGN_OR_RETURN(
-        TermMatchType::Code section_term_match_type,
-        GetTermMatchType(document.document.schema(), section));
-    if (section_term_match_type == TermMatchType::UNKNOWN) {
+        PropertyIndexInfo property_index_info,
+        GetPropertyIndexInfo(document.document.schema(), section));
+    if (!property_index_info.indexable) {
       // Skip non-indexable property.
       continue;
     }
-    for (const std::string &token : section.token_sequence) {
-      if (section_term_match_type == TermMatchType::EXACT_ONLY ||
-          term_match_type == TermMatchType::EXACT_ONLY) {
-        if (token == query_term) {
+
+    if (embedding_search_range_or.ok()) {
+      // Process embedding search.
+      for (const PropertyProto::VectorProto &vector :
+           section.embedding_vectors) {
+        if (DoesVectorsMatch(embedding_scorer.get(),
+                             embedding_search_range_or.ValueOrDie(),
+                             search_spec.embedding_query_vectors(0), vector)) {
+          return true;
+        }
+      }
+    } else {
+      // Process term search.
+      for (const std::string &token : section.token_sequence) {
+        if (property_index_info.term_match_type == TermMatchType::EXACT_ONLY ||
+            search_spec.term_match_type() == TermMatchType::EXACT_ONLY) {
+          if (token == query) {
+            return true;
+          }
+        } else if (IsPrefix(query, token)) {
           return true;
         }
-      } else if (IsPrefix(query_term, token)) {
-        return true;
       }
     }
   }
@@ -338,9 +411,7 @@ InMemoryIcingSearchEngine::InternalSearch(
   std::vector<DocumentId> matched_doc_ids;
   for (DocumentId doc_id : existing_doc_ids_) {
     ICING_ASSIGN_OR_RETURN(
-        bool match,
-        DoesDocumentMatchQuery(documents_[doc_id], search_spec.query(),
-                               search_spec.term_match_type()));
+        bool match, DoesDocumentMatchQuery(documents_[doc_id], search_spec));
     if (match) {
       matched_doc_ids.push_back(doc_id);
     }
diff --git a/icing/monkey_test/in-memory-icing-search-engine.h b/icing/monkey_test/in-memory-icing-search-engine.h
index 98e7e4c..8e92f10 100644
--- a/icing/monkey_test/in-memory-icing-search-engine.h
+++ b/icing/monkey_test/in-memory-icing-search-engine.h
@@ -100,9 +100,7 @@ class InMemoryIcingSearchEngine {
       const std::string &schema_type);
 
   // Deletes all Documents that match the query specified in search_spec.
-  // Currently, only the "query" and "term_match_type" fields are recognized by
-  // the in-memory Icing, and only single term queries with possible section
-  // restrictions are supported.
+  // Check the comments of Search() for the supported query types.
   //
   // Returns:
   //   The number of deleted documents on success
@@ -111,9 +109,17 @@ class InMemoryIcingSearchEngine {
       const SearchSpecProto &search_spec);
 
   // Retrieves documents according to search_spec.
-  // Currently, only the "query" and "term_match_type" fields are recognized by
-  // the in-memory Icing, and only single term queries with possible section
+  // Currently, only the "query", "term_match_type", "embedding_query_vectors",
+  // and "embedding_query_metric_type" fields are recognized by the in-memory
+  // Icing.
+  //
+  // For term based queries, only single term queries with possible section
   // restrictions are supported.
+  //
+  // For embedding based queries, only the fixed format of
+  // `semanticSearch(getEmbeddingParameter(0), low, high)` is supported, where
+  // `low` and `high` are floating point numbers that specify the score range.
+  // Section restrictions are also recognized.
   libtextclassifier3::StatusOr<std::vector<DocumentProto>> Search(
       const SearchSpecProto &search_spec) const;
 
@@ -152,13 +158,20 @@ class InMemoryIcingSearchEngine {
   libtextclassifier3::StatusOr<const PropertyConfigProto *> GetPropertyConfig(
       const std::string &schema_type, const std::string &property_name) const;
 
-  libtextclassifier3::StatusOr<TermMatchType::Code> GetTermMatchType(
+  struct PropertyIndexInfo {
+    // Whether the property is indexable.
+    bool indexable;
+    // The term match type if the property is of type string.
+    TermMatchType::Code term_match_type =
+        TermMatchType::Code::TermMatchType_Code_UNKNOWN;
+  };
+  libtextclassifier3::StatusOr<PropertyIndexInfo> GetPropertyIndexInfo(
       const std::string &schema_type,
       const MonkeyTokenizedSection &section) const;
 
   libtextclassifier3::StatusOr<bool> DoesDocumentMatchQuery(
-      const MonkeyTokenizedDocument &document, const std::string &query,
-      TermMatchType::Code term_match_type) const;
+      const MonkeyTokenizedDocument &document,
+      const SearchSpecProto &search_spec) const;
 };
 
 }  // namespace lib
diff --git a/icing/monkey_test/monkey-test-generators.cc b/icing/monkey_test/monkey-test-generators.cc
index 0d5ad73..9f98d15 100644
--- a/icing/monkey_test/monkey-test-generators.cc
+++ b/icing/monkey_test/monkey-test-generators.cc
@@ -51,29 +51,46 @@ PropertyConfigProto::Cardinality::Code GetRandomCardinality(
   return kCardinalities[dist(*random)];
 }
 
-TermMatchType::Code GetRandomTermMatchType(MonkeyTestRandomEngine* random) {
-  std::uniform_int_distribution<> dist(0, kTermMatchTypes.size() - 1);
+TermMatchType::Code GetRandomIndexableTermMatchType(
+    MonkeyTestRandomEngine* random) {
+  std::uniform_int_distribution<> dist(1, kTermMatchTypes.size() - 1);
   return kTermMatchTypes[dist(*random)];
 }
 
+bool GetRandomBoolean(MonkeyTestRandomEngine* random) {
+  std::uniform_int_distribution<> dist(0, 1);
+  return dist(*random) == 1;
+}
+
 // TODO: Update this function when supporting document_indexing_config.
 bool IsIndexableProperty(const PropertyConfigProto& property) {
   return property.string_indexing_config().term_match_type() !=
-         TermMatchType::UNKNOWN;
+             TermMatchType::UNKNOWN ||
+         property.embedding_indexing_config().embedding_indexing_type() !=
+             EmbeddingIndexingConfig::EmbeddingIndexingType::UNKNOWN;
 }
 
-void SetStringIndexingConfig(PropertyConfigProto& property,
-                             TermMatchType::Code term_match_type) {
-  if (term_match_type != TermMatchType::UNKNOWN) {
+void SetStringIndexingConfig(MonkeyTestRandomEngine* random,
+                             PropertyConfigProto& property, bool indexable) {
+  property.clear_string_indexing_config();
+  if (indexable) {
     StringIndexingConfig* string_indexing_config =
         property.mutable_string_indexing_config();
-    string_indexing_config->set_term_match_type(term_match_type);
+    string_indexing_config->set_term_match_type(
+        GetRandomIndexableTermMatchType(random));
     // TODO: Try to add different TokenizerTypes. VERBATIM, RFC822, and URL are
     // the remaining candidates to consider.
     string_indexing_config->set_tokenizer_type(
         StringIndexingConfig::TokenizerType::PLAIN);
-  } else {
-    property.clear_string_indexing_config();
+  }
+}
+
+void SetEmbeddingIndexingConfig(MonkeyTestRandomEngine* random,
+                                PropertyConfigProto& property, bool indexable) {
+  property.clear_embedding_indexing_config();
+  if (indexable) {
+    property.mutable_embedding_indexing_config()->set_embedding_indexing_type(
+        EmbeddingIndexingConfig::EmbeddingIndexingType::LINEAR_SEARCH);
   }
 }
 
@@ -127,17 +144,22 @@ MonkeySchemaGenerator::UpdateSchemaResult MonkeySchemaGenerator::UpdateSchema(
 
 PropertyConfigProto MonkeySchemaGenerator::GenerateProperty(
     const SchemaTypeConfigProto& type_config,
-    PropertyConfigProto::Cardinality::Code cardinality,
-    TermMatchType::Code term_match_type) {
+    PropertyConfigProto::Cardinality::Code cardinality, bool indexable) {
   PropertyConfigProto prop;
   prop.set_property_name(
       "MonkeyTestProp" +
       std::to_string(num_properties_generated_[type_config.schema_type()]++));
-  // TODO: Perhaps in future iterations we will want to generate more than just
-  // string properties.
-  prop.set_data_type(PropertyConfigProto::DataType::STRING);
+  // TODO: Perhaps in future iterations we will want to generate more types of
+  // properties.
+  // Currently, we are generating either a string or a vector property.
+  if (GetRandomBoolean(random_)) {
+    prop.set_data_type(PropertyConfigProto::DataType::STRING);
+    SetStringIndexingConfig(random_, prop, indexable);
+  } else {
+    prop.set_data_type(PropertyConfigProto::DataType::VECTOR);
+    SetEmbeddingIndexingConfig(random_, prop, indexable);
+  }
   prop.set_cardinality(cardinality);
-  SetStringIndexingConfig(prop, term_match_type);
   return prop;
 }
 
@@ -166,13 +188,23 @@ void MonkeySchemaGenerator::UpdateProperty(
     property.set_cardinality(new_cardinality);
   }
 
+  bool old_indexable = IsIndexableProperty(property);
+  bool new_indexable = GetRandomBoolean(random_);
+  bool index_incompatible = old_indexable != new_indexable;
   if (property.data_type() == PropertyConfigProto::DataType::STRING) {
-    TermMatchType::Code new_term_match_type = GetRandomTermMatchType(random_);
-    if (new_term_match_type !=
-        property.string_indexing_config().term_match_type()) {
-      SetStringIndexingConfig(property, new_term_match_type);
-      result.schema_types_index_incompatible.insert(type_config.schema_type());
+    TermMatchType::Code old_term_match_type =
+        property.string_indexing_config().term_match_type();
+    SetStringIndexingConfig(random_, property, new_indexable);
+    TermMatchType::Code new_term_match_type =
+        property.string_indexing_config().term_match_type();
+    if (old_term_match_type != new_term_match_type) {
+      index_incompatible = true;
     }
+  } else if (property.data_type() == PropertyConfigProto::DataType::VECTOR) {
+    SetEmbeddingIndexingConfig(random_, property, new_indexable);
+  }
+  if (index_incompatible) {
+    result.schema_types_index_incompatible.insert(type_config.schema_type());
   }
 }
 
@@ -187,15 +219,15 @@ SchemaTypeConfigProto MonkeySchemaGenerator::GenerateType() {
 
   int num_indexed_properties = 0;
   for (int i = 0; i < total_num_properties; ++i) {
-    TermMatchType::Code term_match_type = TermMatchType::UNKNOWN;
+    bool indexable = false;
     if (num_indexed_properties < kTotalNumSections) {
-      term_match_type = GetRandomTermMatchType(random_);
+      indexable = GetRandomBoolean(random_);
     }
-    if (term_match_type != TermMatchType::UNKNOWN) {
+    if (indexable) {
       num_indexed_properties += 1;
     }
-    (*type_config.add_properties()) = GenerateProperty(
-        type_config, GetRandomCardinality(random_), term_match_type);
+    (*type_config.add_properties()) =
+        GenerateProperty(type_config, GetRandomCardinality(random_), indexable);
   }
   return type_config;
 }
@@ -248,9 +280,10 @@ void MonkeySchemaGenerator::UpdateType(SchemaTypeConfigProto& type_config,
     if (new_cardinality == PropertyConfigProto::Cardinality::REQUIRED) {
       result.schema_types_incompatible.insert(type_config.schema_type());
     }
-    PropertyConfigProto new_property = GenerateProperty(
-        type_config, new_cardinality, GetRandomTermMatchType(random_));
-    if (IsIndexableProperty(new_property)) {
+    bool indexable = GetRandomBoolean(random_);
+    PropertyConfigProto new_property =
+        GenerateProperty(type_config, new_cardinality, indexable);
+    if (indexable) {
       result.schema_types_index_incompatible.insert(type_config.schema_type());
     }
     (*type_config.add_properties()) = std::move(new_property);
@@ -296,18 +329,20 @@ std::string MonkeyDocumentGenerator::GetUri() const {
 }
 
 int MonkeyDocumentGenerator::GetNumTokens() const {
-  std::uniform_int_distribution<> dist(
-      0, config_->possible_num_tokens_.size() - 1);
-  int n = config_->possible_num_tokens_[dist(*random_)];
+  std::uniform_int_distribution<> dist(0,
+                                       config_->possible_num_tokens.size() - 1);
+  int n = config_->possible_num_tokens[dist(*random_)];
   // Add some noise
   std::uniform_real_distribution<> real_dist(0.5, 1);
   float p = real_dist(*random_);
   return n * p;
 }
 
-std::vector<std::string> MonkeyDocumentGenerator::GetPropertyContent() const {
-  std::vector<std::string> content;
+std::vector<std::string> MonkeyDocumentGenerator::GetStringPropertyContent()
+    const {
   int num_tokens = GetNumTokens();
+  std::vector<std::string> content;
+  content.reserve(num_tokens);
   while (num_tokens) {
     content.push_back(std::string(GetToken()));
     --num_tokens;
@@ -315,6 +350,52 @@ std::vector<std::string> MonkeyDocumentGenerator::GetPropertyContent() const {
   return content;
 }
 
+int MonkeyDocumentGenerator::GetNumVectors(
+    PropertyConfigProto::Cardinality::Code cardinality) const {
+  if (cardinality == PropertyConfigProto::Cardinality::REQUIRED) {
+    return 1;
+  } else if (cardinality == PropertyConfigProto::Cardinality::OPTIONAL) {
+    std::uniform_int_distribution<> dist(0, 1);
+    return dist(*random_);
+  }
+
+  // For repeated properties:
+  std::uniform_int_distribution<> dist(
+      0, config_->possible_num_vectors.size() - 1);
+  int n = config_->possible_num_vectors[dist(*random_)];
+  // Add some noise
+  std::uniform_real_distribution<> real_dist(0.5, 1);
+  float p = real_dist(*random_);
+  return n * p;
+}
+
+PropertyProto::VectorProto MonkeyDocumentGenerator::GetRandomVector() const {
+  std::uniform_int_distribution<> dimension_dist(
+      0, config_->possible_vector_dimensions.size() - 1);
+  std::uniform_real_distribution<float> value_dist(-1.0, 1.0);
+
+  PropertyProto::VectorProto vector;
+  vector.set_model_signature("model");
+  int dimension = config_->possible_vector_dimensions[dimension_dist(*random_)];
+  for (int i = 0; i < dimension; ++i) {
+    vector.add_values(value_dist(*random_));
+  }
+  return vector;
+}
+
+std::vector<PropertyProto::VectorProto>
+MonkeyDocumentGenerator::GetVectorPropertyContent(
+    PropertyConfigProto::Cardinality::Code cardinality) const {
+  int num_vectors = GetNumVectors(cardinality);
+  std::vector<PropertyProto::VectorProto> content;
+  content.reserve(num_vectors);
+  while (num_vectors) {
+    content.push_back(GetRandomVector());
+    --num_vectors;
+  }
+  return content;
+}
+
 MonkeyTokenizedDocument MonkeyDocumentGenerator::GenerateDocument() {
   MonkeyTokenizedDocument document;
   const SchemaTypeConfigProto& type_config = GetType();
@@ -326,16 +407,29 @@ MonkeyTokenizedDocument MonkeyDocumentGenerator::GenerateDocument() {
           .SetUri(GetUri())
           .SetCreationTimestampMs(clock_.GetSystemTimeMilliseconds());
   for (const PropertyConfigProto& prop : type_config.properties()) {
-    std::vector<std::string> prop_content = GetPropertyContent();
-    doc_builder.AddStringProperty(prop.property_name(),
-                                  absl_ports::StrJoin(prop_content, " "));
-    // No matter whether the property is indexable currently, we have to create
-    // a section for it since a non-indexable property can become indexable
-    // after a schema type change. The in-memory icing will automatically skip
-    // sections that are non-indexable at the time of search requests.
-    MonkeyTokenizedSection section = {prop.property_name(),
-                                      std::move(prop_content)};
-    document.tokenized_sections.push_back(std::move(section));
+    if (prop.data_type() == PropertyConfigProto::DataType::STRING) {
+      std::vector<std::string> prop_content = GetStringPropertyContent();
+      doc_builder.AddStringProperty(prop.property_name(),
+                                    absl_ports::StrJoin(prop_content, " "));
+      // No matter whether the property is indexable currently, we have to
+      // create a section for it since a non-indexable property can become
+      // indexable after a schema type change. The in-memory icing will
+      // automatically skip sections that are non-indexable at the time of
+      // search requests.
+      MonkeyTokenizedSection section = {prop.property_name(),
+                                        std::move(prop_content)};
+      document.tokenized_sections.push_back(std::move(section));
+    } else {
+      std::vector<PropertyProto::VectorProto> prop_content =
+          GetVectorPropertyContent(prop.cardinality());
+      doc_builder.AddVectorProperty(prop.property_name(), prop_content);
+
+      // Similar to the string property, no matter whether the property is
+      // indexable currently, we have to create a section for it.
+      MonkeyTokenizedSection section = {
+          prop.property_name(), /*token_sequence=*/{}, std::move(prop_content)};
+      document.tokenized_sections.push_back(std::move(section));
+    }
   }
   document.document = doc_builder.Build();
   ++num_docs_generated_;
diff --git a/icing/monkey_test/monkey-test-generators.h b/icing/monkey_test/monkey-test-generators.h
index 72a4723..49ad530 100644
--- a/icing/monkey_test/monkey-test-generators.h
+++ b/icing/monkey_test/monkey-test-generators.h
@@ -27,7 +27,6 @@
 #include "icing/monkey_test/monkey-test-util.h"
 #include "icing/monkey_test/monkey-tokenized-document.h"
 #include "icing/proto/schema.pb.h"
-#include "icing/proto/term.pb.h"
 #include "icing/util/clock.h"
 
 namespace icing {
@@ -56,8 +55,7 @@ class MonkeySchemaGenerator {
  private:
   PropertyConfigProto GenerateProperty(
       const SchemaTypeConfigProto& type_config,
-      PropertyConfigProto::Cardinality::Code cardinality,
-      TermMatchType::Code term_match_type);
+      PropertyConfigProto::Cardinality::Code cardinality, bool indexable);
 
   void UpdateProperty(const SchemaTypeConfigProto& type_config,
                       PropertyConfigProto& property,
@@ -102,13 +100,20 @@ class MonkeyDocumentGenerator {
     return kCommonWords[dist(*random_)];
   }
 
+  PropertyProto::VectorProto GetRandomVector() const;
+
   std::string GetNamespace() const;
 
   std::string GetUri() const;
 
   int GetNumTokens() const;
 
-  std::vector<std::string> GetPropertyContent() const;
+  int GetNumVectors(PropertyConfigProto::Cardinality::Code cardinality) const;
+
+  std::vector<std::string> GetStringPropertyContent() const;
+
+  std::vector<PropertyProto::VectorProto> GetVectorPropertyContent(
+      PropertyConfigProto::Cardinality::Code cardinality) const;
 
   MonkeyTokenizedDocument GenerateDocument();
 
diff --git a/icing/monkey_test/monkey-test-util.h b/icing/monkey_test/monkey-test-util.h
index d6053d8..919640c 100644
--- a/icing/monkey_test/monkey-test-util.h
+++ b/icing/monkey_test/monkey-test-util.h
@@ -51,9 +51,16 @@ struct IcingMonkeyTestRunnerConfiguration {
   // property, 2 properties, 3 properties and 4 properties.
   std::vector<int> possible_num_properties;
 
-  // The possible number of tokens that may appear in generated documents, with
-  // a noise factor from 0.5 to 1 applied.
-  std::vector<int> possible_num_tokens_;
+  // The possible number of tokens that may appear in a string property of
+  // generated documents, with a noise factor from 0.5 to 1 applied.
+  std::vector<int> possible_num_tokens;
+
+  // The possible number of embedding vectors that may appear in a repeated
+  // vector property of generated documents.
+  std::vector<int> possible_num_vectors;
+
+  // The possible dimensions for the randomly generated embedding vectors.
+  std::vector<int> possible_vector_dimensions;
 
   // An array of pairs of monkey test APIs with frequencies.
   // If f_sum is the sum of all the frequencies, an operation with frequency f
diff --git a/icing/monkey_test/monkey-tokenized-document.h b/icing/monkey_test/monkey-tokenized-document.h
index 87b77bb..cad838e 100644
--- a/icing/monkey_test/monkey-tokenized-document.h
+++ b/icing/monkey_test/monkey-tokenized-document.h
@@ -26,6 +26,7 @@ namespace lib {
 struct MonkeyTokenizedSection {
   std::string path;
   std::vector<std::string> token_sequence;
+  std::vector<PropertyProto::VectorProto> embedding_vectors;
 };
 
 struct MonkeyTokenizedDocument {
diff --git a/icing/portable/platform.h b/icing/portable/platform.h
index 6d8c668..f7d8734 100644
--- a/icing/portable/platform.h
+++ b/icing/portable/platform.h
@@ -18,6 +18,7 @@
 #include "unicode/uconfig.h"  // IWYU pragma: keep
 // clang-format: do not reorder the above include.
 
+#include "icing/expand/stemming/stemmer-factory.h"
 #include "unicode/uvernum.h"
 
 namespace icing {
@@ -46,6 +47,11 @@ inline bool IsIcuTokenization() {
 inline int GetIcuTokenizationVersion() {
   return IsIcuTokenization() ? U_ICU_VERSION_MAJOR_NUM : 0;
 }
+// Indicates whether stemming is enabled.
+//
+// This is false if stemmer_factory is compiled with the none-stemmer
+// implementation and true for the snowball-stemmer implementation.
+inline bool IsStemmingEnabled() { return stemmer_factory::IsStemmingEnabled(); }
 
 // Whether we're running on android_x86
 inline bool IsAndroidX86() {
diff --git a/icing/query/advanced_query_parser/abstract-syntax-tree-test-utils.h b/icing/query/advanced_query_parser/abstract-syntax-tree-test-utils.h
index 42be07d..57d4c67 100644
--- a/icing/query/advanced_query_parser/abstract-syntax-tree-test-utils.h
+++ b/icing/query/advanced_query_parser/abstract-syntax-tree-test-utils.h
@@ -29,7 +29,6 @@ namespace lib {
 // A visitor that simply collects the nodes and flattens them in left-side
 // depth-first order.
 enum class NodeType {
-  kFunctionName,
   kString,
   kText,
   kMember,
@@ -60,9 +59,6 @@ MATCHER_P2(EqualsNodeInfo, value, type, "") {
 
 class SimpleVisitor : public AbstractSyntaxTreeVisitor {
  public:
-  void VisitFunctionName(const FunctionNameNode* node) override {
-    nodes_.push_back({node->value(), NodeType::kFunctionName});
-  }
   void VisitString(const StringNode* node) override {
     nodes_.push_back({node->value(), NodeType::kString});
   }
@@ -79,11 +75,10 @@ class SimpleVisitor : public AbstractSyntaxTreeVisitor {
     nodes_.push_back({"", NodeType::kMember});
   }
   void VisitFunction(const FunctionNode* node) override {
-    node->function_name()->Accept(this);
     for (const std::unique_ptr<Node>& arg : node->args()) {
       arg->Accept(this);
     }
-    nodes_.push_back({"", NodeType::kFunction});
+    nodes_.push_back({node->function_name(), NodeType::kFunction});
   }
   void VisitUnaryOperator(const UnaryOperatorNode* node) override {
     node->child()->Accept(this);
diff --git a/icing/query/advanced_query_parser/abstract-syntax-tree.h b/icing/query/advanced_query_parser/abstract-syntax-tree.h
index 67049ad..3260084 100644
--- a/icing/query/advanced_query_parser/abstract-syntax-tree.h
+++ b/icing/query/advanced_query_parser/abstract-syntax-tree.h
@@ -24,7 +24,6 @@
 namespace icing {
 namespace lib {
 
-class FunctionNameNode;
 class StringNode;
 class TextNode;
 class MemberNode;
@@ -36,7 +35,6 @@ class AbstractSyntaxTreeVisitor {
  public:
   virtual ~AbstractSyntaxTreeVisitor() = default;
 
-  virtual void VisitFunctionName(const FunctionNameNode* node) = 0;
   virtual void VisitString(const StringNode* node) = 0;
   virtual void VisitText(const TextNode* node) = 0;
   virtual void VisitMember(const MemberNode* node) = 0;
@@ -51,10 +49,10 @@ class Node {
   virtual void Accept(AbstractSyntaxTreeVisitor* visitor) const = 0;
 };
 
-class TerminalNode : public Node {
+class StringNode : public Node {
  public:
-  explicit TerminalNode(std::string value, std::string_view raw_value,
-                        bool is_prefix)
+  explicit StringNode(std::string value, std::string_view raw_value,
+                      bool is_prefix = false)
       : value_(std::move(value)),
         raw_value_(raw_value),
         is_prefix_(is_prefix) {}
@@ -66,39 +64,39 @@ class TerminalNode : public Node {
 
   std::string_view raw_value() const { return raw_value_; }
 
+  void Accept(AbstractSyntaxTreeVisitor* visitor) const override {
+    visitor->VisitString(this);
+  }
+
  private:
   std::string value_;
   std::string_view raw_value_;
   bool is_prefix_;
 };
 
-class FunctionNameNode : public TerminalNode {
- public:
-  explicit FunctionNameNode(std::string value)
-      : TerminalNode(std::move(value), /*raw_value=*/"", /*is_prefix=*/false) {}
-  void Accept(AbstractSyntaxTreeVisitor* visitor) const override {
-    visitor->VisitFunctionName(this);
-  }
-};
-
-class StringNode : public TerminalNode {
- public:
-  explicit StringNode(std::string value, std::string_view raw_value,
-                      bool is_prefix = false)
-      : TerminalNode(std::move(value), raw_value, is_prefix) {}
-  void Accept(AbstractSyntaxTreeVisitor* visitor) const override {
-    visitor->VisitString(this);
-  }
-};
-
-class TextNode : public TerminalNode {
+class TextNode : public Node {
  public:
   explicit TextNode(std::string value, std::string_view raw_value,
                     bool is_prefix = false)
-      : TerminalNode(std::move(value), raw_value, is_prefix) {}
+      : value_(std::move(value)),
+        raw_value_(raw_value),
+        is_prefix_(is_prefix) {}
+
+  const std::string& value() const& { return value_; }
+  std::string value() && { return std::move(value_); }
+
+  bool is_prefix() const { return is_prefix_; }
+
+  std::string_view raw_value() const { return raw_value_; }
+
   void Accept(AbstractSyntaxTreeVisitor* visitor) const override {
     visitor->VisitText(this);
   }
+
+ private:
+  std::string value_;
+  std::string_view raw_value_;
+  bool is_prefix_;
 };
 
 class MemberNode : public Node {
@@ -124,20 +122,21 @@ class MemberNode : public Node {
 
 class FunctionNode : public Node {
  public:
-  explicit FunctionNode(std::unique_ptr<FunctionNameNode> function_name)
+  explicit FunctionNode(std::string function_name)
       : FunctionNode(std::move(function_name), {}) {}
-  explicit FunctionNode(std::unique_ptr<FunctionNameNode> function_name,
+  explicit FunctionNode(std::string function_name,
                         std::vector<std::unique_ptr<Node>> args)
-      : function_name_(std::move(function_name)), args_(std::move(args)) {}
+      : function_name_(std::move(function_name)),
+        args_(std::move(args)) {}
 
   void Accept(AbstractSyntaxTreeVisitor* visitor) const override {
     visitor->VisitFunction(this);
   }
-  const FunctionNameNode* function_name() const { return function_name_.get(); }
+  const std::string& function_name() const { return function_name_; }
   const std::vector<std::unique_ptr<Node>>& args() const { return args_; }
 
  private:
-  std::unique_ptr<FunctionNameNode> function_name_;
+  std::string function_name_;
   std::vector<std::unique_ptr<Node>> args_;
 };
 
diff --git a/icing/query/advanced_query_parser/abstract-syntax-tree_test.cc b/icing/query/advanced_query_parser/abstract-syntax-tree_test.cc
index 5e28278..fae2fb9 100644
--- a/icing/query/advanced_query_parser/abstract-syntax-tree_test.cc
+++ b/icing/query/advanced_query_parser/abstract-syntax-tree_test.cc
@@ -63,45 +63,37 @@ TEST(AbstractSyntaxTreeTest, Composite) {
 
 TEST(AbstractSyntaxTreeTest, Function) {
   // foo()
-  std::unique_ptr<Node> root =
-      std::make_unique<FunctionNode>(std::make_unique<FunctionNameNode>("foo"));
+  std::unique_ptr<Node> root = std::make_unique<FunctionNode>("foo");
   SimpleVisitor visitor;
   root->Accept(&visitor);
 
   EXPECT_THAT(visitor.nodes(),
-              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("", NodeType::kFunction)));
+              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunction)));
 
   std::string_view query = "foo(\"bar\")";
   std::vector<std::unique_ptr<Node>> args;
   args.push_back(std::make_unique<StringNode>("bar", query.substr(5, 3)));
-  root = std::make_unique<FunctionNode>(
-      std::make_unique<FunctionNameNode>("foo"), std::move(args));
+  root = std::make_unique<FunctionNode>("foo", std::move(args));
   visitor = SimpleVisitor();
   root->Accept(&visitor);
 
   EXPECT_THAT(visitor.nodes(),
-              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("bar", NodeType::kString),
-                          EqualsNodeInfo("", NodeType::kFunction)));
+              ElementsAre(EqualsNodeInfo("bar", NodeType::kString),
+                          EqualsNodeInfo("foo", NodeType::kFunction)));
 
   query = "foo(bar(\"baz\"))";
   std::vector<std::unique_ptr<Node>> inner_args;
   inner_args.push_back(std::make_unique<StringNode>("baz", query.substr(9, 3)));
   args.clear();
-  args.push_back(std::make_unique<FunctionNode>(
-      std::make_unique<FunctionNameNode>("bar"), std::move(inner_args)));
-  root = std::make_unique<FunctionNode>(
-      std::make_unique<FunctionNameNode>("foo"), std::move(args));
+  args.push_back(std::make_unique<FunctionNode>("bar", std::move(inner_args)));
+  root = std::make_unique<FunctionNode>("foo", std::move(args));
   visitor = SimpleVisitor();
   root->Accept(&visitor);
 
   EXPECT_THAT(visitor.nodes(),
-              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("bar", NodeType::kFunctionName),
-                          EqualsNodeInfo("baz", NodeType::kString),
-                          EqualsNodeInfo("", NodeType::kFunction),
-                          EqualsNodeInfo("", NodeType::kFunction)));
+              ElementsAre(EqualsNodeInfo("baz", NodeType::kString),
+                          EqualsNodeInfo("bar", NodeType::kFunction),
+                          EqualsNodeInfo("foo", NodeType::kFunction)));
 }
 
 TEST(AbstractSyntaxTreeTest, Restriction) {
diff --git a/icing/query/advanced_query_parser/lexer.cc b/icing/query/advanced_query_parser/lexer.cc
index 0dd0bb0..0848c46 100644
--- a/icing/query/advanced_query_parser/lexer.cc
+++ b/icing/query/advanced_query_parser/lexer.cc
@@ -15,6 +15,7 @@
 #include "icing/query/advanced_query_parser/lexer.h"
 
 #include <string>
+#include <utility>
 
 #include "icing/absl_ports/canonical_errors.h"
 #include "icing/absl_ports/str_cat.h"
@@ -246,7 +247,7 @@ bool Lexer::ConsumeText() {
 }
 
 libtextclassifier3::StatusOr<std::vector<Lexer::LexerToken>>
-Lexer::ExtractTokens() {
+Lexer::ExtractTokens() && {
   while (current_char_ != '\0') {
     // Clear out any non-text before matching a Text.
     while (ConsumeNonText()) {
@@ -263,7 +264,7 @@ Lexer::ExtractTokens() {
                            std::to_string(kMaxNumTokens), ", but got ",
                            std::to_string(tokens_.size()), " tokens."));
   }
-  return tokens_;
+  return std::move(tokens_);
 }
 
 }  // namespace lib
diff --git a/icing/query/advanced_query_parser/lexer.h b/icing/query/advanced_query_parser/lexer.h
index b313fa7..4510696 100644
--- a/icing/query/advanced_query_parser/lexer.h
+++ b/icing/query/advanced_query_parser/lexer.h
@@ -89,7 +89,7 @@ class Lexer {
   // Returns:
   //   A vector of LexerToken on success
   //   INVALID_ARGUMENT on syntax error.
-  libtextclassifier3::StatusOr<std::vector<LexerToken>> ExtractTokens();
+  libtextclassifier3::StatusOr<std::vector<LexerToken>> ExtractTokens() &&;
 
  private:
   // Advance to current_index_ + n.
diff --git a/icing/query/advanced_query_parser/lexer_fuzz_test.cc b/icing/query/advanced_query_parser/lexer_fuzz_test.cc
index f9190db..86338d0 100644
--- a/icing/query/advanced_query_parser/lexer_fuzz_test.cc
+++ b/icing/query/advanced_query_parser/lexer_fuzz_test.cc
@@ -15,6 +15,7 @@
 #include <cstdint>
 #include <memory>
 #include <string_view>
+#include <utility>
 
 #include "icing/query/advanced_query_parser/lexer.h"
 
@@ -26,10 +27,10 @@ extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
 
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>(text, Lexer::Language::QUERY);
-  lexer->ExtractTokens();
+  std::move(*lexer).ExtractTokens();
 
   lexer = std::make_unique<Lexer>(text, Lexer::Language::SCORING);
-  lexer->ExtractTokens();
+  std::move(*lexer).ExtractTokens();
   return 0;
 }
 
diff --git a/icing/query/advanced_query_parser/lexer_test.cc b/icing/query/advanced_query_parser/lexer_test.cc
index ec0e663..8571619 100644
--- a/icing/query/advanced_query_parser/lexer_test.cc
+++ b/icing/query/advanced_query_parser/lexer_test.cc
@@ -15,6 +15,8 @@
 #include "icing/query/advanced_query_parser/lexer.h"
 
 #include <memory>
+#include <utility>
+#include <vector>
 
 #include "gmock/gmock.h"
 #include "gtest/gtest.h"
@@ -47,22 +49,22 @@ TEST(LexerTest, SimpleQuery) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("foo", Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("foo", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("fooAND", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("fooAND", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("ORfoo", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("ORfoo", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("fooANDbar", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens, ElementsAre(EqualsLexerToken("fooANDbar",
                                                    Lexer::TokenType::TEXT)));
 }
@@ -71,25 +73,25 @@ TEST(LexerTest, PrefixQuery) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("foo*", Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("foo", Lexer::TokenType::TEXT),
                           EqualsLexerToken("", Lexer::TokenType::STAR)));
 
   lexer = std::make_unique<Lexer>("fooAND*", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("fooAND", Lexer::TokenType::TEXT),
                           EqualsLexerToken("", Lexer::TokenType::STAR)));
 
   lexer = std::make_unique<Lexer>("*ORfoo", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("", Lexer::TokenType::STAR),
                           EqualsLexerToken("ORfoo", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("fooANDbar*", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("fooANDbar", Lexer::TokenType::TEXT),
                           EqualsLexerToken("", Lexer::TokenType::STAR)));
@@ -99,22 +101,22 @@ TEST(LexerTest, SimpleStringQuery) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("\"foo\"", Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("foo", Lexer::TokenType::STRING)));
 
   lexer = std::make_unique<Lexer>("\"fooAND\"", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens, ElementsAre(EqualsLexerToken("fooAND",
                                                    Lexer::TokenType::STRING)));
 
   lexer = std::make_unique<Lexer>("\"ORfoo\"", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("ORfoo", Lexer::TokenType::STRING)));
 
   lexer = std::make_unique<Lexer>("\"fooANDbar\"", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens, ElementsAre(EqualsLexerToken("fooANDbar",
                                                    Lexer::TokenType::STRING)));
 }
@@ -123,28 +125,28 @@ TEST(LexerTest, TwoTermQuery) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("foo AND bar", Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("foo", Lexer::TokenType::TEXT),
                           EqualsLexerToken(Lexer::TokenType::AND),
                           EqualsLexerToken("bar", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("foo && bar", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("foo", Lexer::TokenType::TEXT),
                           EqualsLexerToken(Lexer::TokenType::AND),
                           EqualsLexerToken("bar", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("foo&&bar", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("foo", Lexer::TokenType::TEXT),
                           EqualsLexerToken(Lexer::TokenType::AND),
                           EqualsLexerToken("bar", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("foo OR \"bar\"", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("foo", Lexer::TokenType::TEXT),
                           EqualsLexerToken(Lexer::TokenType::OR),
@@ -156,34 +158,34 @@ TEST(LexerTest, QueryWithSpecialSymbol) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("foo\\ \\&\\&bar", Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens, ElementsAre(EqualsLexerToken("foo &&bar",
                                                    Lexer::TokenType::TEXT)));
   lexer = std::make_unique<Lexer>("foo\\&\\&bar&&baz", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("foo&&bar", Lexer::TokenType::TEXT),
                           EqualsLexerToken(Lexer::TokenType::AND),
                           EqualsLexerToken("baz", Lexer::TokenType::TEXT)));
   lexer = std::make_unique<Lexer>("foo\\\"", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("foo\"", Lexer::TokenType::TEXT)));
 
   // With quotation marks
   lexer = std::make_unique<Lexer>("\"foo &&bar\"", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens, ElementsAre(EqualsLexerToken("foo &&bar",
                                                    Lexer::TokenType::STRING)));
   lexer = std::make_unique<Lexer>("\"foo&&bar\"&&baz", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(
       tokens,
       ElementsAre(EqualsLexerToken("foo&&bar", Lexer::TokenType::STRING),
                   EqualsLexerToken(Lexer::TokenType::AND),
                   EqualsLexerToken("baz", Lexer::TokenType::TEXT)));
   lexer = std::make_unique<Lexer>("\"foo\\\"\"", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens, ElementsAre(EqualsLexerToken("foo\\\"",
                                                    Lexer::TokenType::STRING)));
 }
@@ -192,7 +194,7 @@ TEST(LexerTest, TextInStringShouldBeOriginal) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("\"foo\\nbar\"", Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens, ElementsAre(EqualsLexerToken("foo\\nbar",
                                                    Lexer::TokenType::STRING)));
 }
@@ -201,7 +203,7 @@ TEST(LexerTest, QueryWithFunctionCalls) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("foo AND fun(bar)", Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(
       tokens,
       ElementsAre(EqualsLexerToken("foo", Lexer::TokenType::TEXT),
@@ -213,7 +215,7 @@ TEST(LexerTest, QueryWithFunctionCalls) {
 
   // Not a function call
   lexer = std::make_unique<Lexer>("foo AND fun (bar)", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("foo", Lexer::TokenType::TEXT),
                           EqualsLexerToken(Lexer::TokenType::AND),
@@ -227,14 +229,14 @@ TEST(LexerTest, QueryWithComparator) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("name: foo", Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("name", Lexer::TokenType::TEXT),
                           EqualsLexerToken(":", Lexer::TokenType::COMPARATOR),
                           EqualsLexerToken("foo", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("email.name:foo", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("email", Lexer::TokenType::TEXT),
                           EqualsLexerToken(Lexer::TokenType::DOT),
@@ -243,42 +245,42 @@ TEST(LexerTest, QueryWithComparator) {
                           EqualsLexerToken("foo", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("age > 20", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("age", Lexer::TokenType::TEXT),
                           EqualsLexerToken(">", Lexer::TokenType::COMPARATOR),
                           EqualsLexerToken("20", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("age>=20", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("age", Lexer::TokenType::TEXT),
                           EqualsLexerToken(">=", Lexer::TokenType::COMPARATOR),
                           EqualsLexerToken("20", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("age <20", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("age", Lexer::TokenType::TEXT),
                           EqualsLexerToken("<", Lexer::TokenType::COMPARATOR),
                           EqualsLexerToken("20", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("age<= 20", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("age", Lexer::TokenType::TEXT),
                           EqualsLexerToken("<=", Lexer::TokenType::COMPARATOR),
                           EqualsLexerToken("20", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("age == 20", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("age", Lexer::TokenType::TEXT),
                           EqualsLexerToken("==", Lexer::TokenType::COMPARATOR),
                           EqualsLexerToken("20", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("age != 20", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("age", Lexer::TokenType::TEXT),
                           EqualsLexerToken("!=", Lexer::TokenType::COMPARATOR),
@@ -291,7 +293,7 @@ TEST(LexerTest, ComplexQuery) {
       "NOT verbatimSearch(\"hello world\")",
       Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(
       tokens,
       ElementsAre(
@@ -338,7 +340,7 @@ TEST(LexerTest, UTF8WhiteSpace) {
       "\xe2\x80\x8a",
       Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("foo", Lexer::TokenType::TEXT),
                           EqualsLexerToken("bar", Lexer::TokenType::TEXT)));
@@ -348,7 +350,7 @@ TEST(LexerTest, CJKT) {
   std::unique_ptr<Lexer> lexer = std::make_unique<Lexer>(
       "我 && 每天 || 走路 OR 去 -上班", Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("我", Lexer::TokenType::TEXT),
                           EqualsLexerToken(Lexer::TokenType::AND),
@@ -362,7 +364,7 @@ TEST(LexerTest, CJKT) {
 
   lexer = std::make_unique<Lexer>("私&& は ||毎日 AND 仕事 -に 歩い て い ます",
                                   Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("私", Lexer::TokenType::TEXT),
                           EqualsLexerToken(Lexer::TokenType::AND),
@@ -380,7 +382,7 @@ TEST(LexerTest, CJKT) {
 
   lexer = std::make_unique<Lexer>("ញុំ&&ដើរទៅ||ធ្វើការ-រាល់ថ្ងៃ",
                                   Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(
       tokens,
       ElementsAre(EqualsLexerToken("ញុំ", Lexer::TokenType::TEXT),
@@ -396,7 +398,7 @@ TEST(LexerTest, CJKT) {
       "\xe2\x80\x89"  // White Space
       "출근합니다",
       Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(
       tokens,
       ElementsAre(EqualsLexerToken("나는", Lexer::TokenType::TEXT),
@@ -407,15 +409,15 @@ TEST(LexerTest, CJKT) {
 TEST(LexerTest, SyntaxError) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("\"foo", Lexer::Language::QUERY);
-  EXPECT_THAT(lexer->ExtractTokens(),
+  EXPECT_THAT(std::move(*lexer).ExtractTokens(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 
   lexer = std::make_unique<Lexer>("\"foo\\", Lexer::Language::QUERY);
-  EXPECT_THAT(lexer->ExtractTokens(),
+  EXPECT_THAT(std::move(*lexer).ExtractTokens(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 
   lexer = std::make_unique<Lexer>("foo\\", Lexer::Language::QUERY);
-  EXPECT_THAT(lexer->ExtractTokens(),
+  EXPECT_THAT(std::move(*lexer).ExtractTokens(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
@@ -425,24 +427,24 @@ TEST(LexerTest, SpecialSymbolAsText) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("age=20", Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("age=20", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("age !20", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("age", Lexer::TokenType::TEXT),
                           EqualsLexerToken("!20", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("foo& bar", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("foo&", Lexer::TokenType::TEXT),
                           EqualsLexerToken("bar", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("foo | bar", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("foo", Lexer::TokenType::TEXT),
                           EqualsLexerToken("|", Lexer::TokenType::TEXT),
@@ -453,14 +455,14 @@ TEST(LexerTest, ScoringArithmetic) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("1 + 2", Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("1", Lexer::TokenType::TEXT),
                           EqualsLexerToken(Lexer::TokenType::PLUS),
                           EqualsLexerToken("2", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("1+2*3/4", Lexer::Language::SCORING);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("1", Lexer::TokenType::TEXT),
                           EqualsLexerToken(Lexer::TokenType::PLUS),
@@ -472,14 +474,14 @@ TEST(LexerTest, ScoringArithmetic) {
 
   // Arithmetic operators will not be produced in query language.
   lexer = std::make_unique<Lexer>("1 + 2", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("1", Lexer::TokenType::TEXT),
                           EqualsLexerToken("+", Lexer::TokenType::TEXT),
                           EqualsLexerToken("2", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("1+2*3/4", Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("1+2", Lexer::TokenType::TEXT),
                           EqualsLexerToken("", Lexer::TokenType::STAR),
@@ -492,26 +494,26 @@ TEST(LexerTest, LogicOperatorNotInScoring) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("1 && 2", Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("1", Lexer::TokenType::TEXT),
                           EqualsLexerToken("&&", Lexer::TokenType::TEXT),
                           EqualsLexerToken("2", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("1&&2", Lexer::Language::SCORING);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("1&&2", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("1&&2 ||3", Lexer::Language::SCORING);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("1&&2", Lexer::TokenType::TEXT),
                           EqualsLexerToken("||3", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("1 AND 2 OR 3 AND NOT 4",
                                   Lexer::Language::SCORING);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("1", Lexer::TokenType::TEXT),
                           EqualsLexerToken("AND", Lexer::TokenType::TEXT),
@@ -527,20 +529,20 @@ TEST(LexerTest, ComparatorNotInScoring) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("1 > 2", Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("1", Lexer::TokenType::TEXT),
                           EqualsLexerToken(">", Lexer::TokenType::TEXT),
                           EqualsLexerToken("2", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("1>2", Lexer::Language::SCORING);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("1>2", Lexer::TokenType::TEXT)));
 
   lexer = std::make_unique<Lexer>("1>2>=3 <= 4:5== 6<7<=8!= 9",
                                   Lexer::Language::SCORING);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("1>2>=3", Lexer::TokenType::TEXT),
                           EqualsLexerToken("<=", Lexer::TokenType::TEXT),
@@ -551,7 +553,7 @@ TEST(LexerTest, ComparatorNotInScoring) {
   // Comparator should be produced in query language.
   lexer = std::make_unique<Lexer>("1>2>=3 <= 4:5== 6<7<=8!= 9",
                                   Lexer::Language::QUERY);
-  ICING_ASSERT_OK_AND_ASSIGN(tokens, lexer->ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(tokens, std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("1", Lexer::TokenType::TEXT),
                           EqualsLexerToken(">", Lexer::TokenType::COMPARATOR),
@@ -578,7 +580,7 @@ TEST(LexerTest, ComplexScoring) {
       ") * pow(2.3, DocumentScore())",
       Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(
       tokens,
       ElementsAre(
@@ -623,7 +625,7 @@ TEST(LexerTest, NoAmbiguousTokenizing) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("foo:bar:baz", Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> invalidQueryTokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(invalidQueryTokens,
               ElementsAre(EqualsLexerToken("foo", Lexer::TokenType::TEXT),
                           EqualsLexerToken(":", Lexer::TokenType::COMPARATOR),
@@ -633,7 +635,7 @@ TEST(LexerTest, NoAmbiguousTokenizing) {
 
   lexer = std::make_unique<Lexer>("foo:\"bar:baz\"", Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> validQueryTokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(
       validQueryTokens,
       ElementsAre(EqualsLexerToken("foo", Lexer::TokenType::TEXT),
@@ -645,7 +647,7 @@ TEST(LexerTest, WhiteSpacesDoNotAffectColonTokenization) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("a:b c : d e: f g :h", Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("a", Lexer::TokenType::TEXT),
                           EqualsLexerToken(":", Lexer::TokenType::COMPARATOR),
@@ -667,7 +669,7 @@ TEST(LexerTest, ColonInTextRequiresExplicitEscaping) {
   std::unique_ptr<Lexer> lexer =
       std::make_unique<Lexer>("foo:bar\\:baz", Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> tokens,
-                             lexer->ExtractTokens());
+                             std::move(*lexer).ExtractTokens());
   EXPECT_THAT(tokens,
               ElementsAre(EqualsLexerToken("foo", Lexer::TokenType::TEXT),
                           EqualsLexerToken(":", Lexer::TokenType::COMPARATOR),
@@ -680,7 +682,7 @@ TEST(LexerTest, QueryShouldRejectTokensBeyondLimit) {
     query.push_back('(');
   }
   Lexer lexer(query, Lexer::Language::QUERY);
-  EXPECT_THAT(lexer.ExtractTokens(),
+  EXPECT_THAT(std::move(lexer).ExtractTokens(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
@@ -690,7 +692,7 @@ TEST(LexerTest, ScoringShouldRejectTokensBeyondLimit) {
     scoring.push_back('(');
   }
   Lexer lexer(scoring, Lexer::Language::SCORING);
-  EXPECT_THAT(lexer.ExtractTokens(),
+  EXPECT_THAT(std::move(lexer).ExtractTokens(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
diff --git a/icing/query/advanced_query_parser/parser.cc b/icing/query/advanced_query_parser/parser.cc
index 82576a1..dc62487 100644
--- a/icing/query/advanced_query_parser/parser.cc
+++ b/icing/query/advanced_query_parser/parser.cc
@@ -61,16 +61,14 @@ libtextclassifier3::StatusOr<std::unique_ptr<TextNode>> Parser::ConsumeText() {
   return text_node;
 }
 
-libtextclassifier3::StatusOr<std::unique_ptr<FunctionNameNode>>
-Parser::ConsumeFunctionName() {
+libtextclassifier3::StatusOr<std::string> Parser::ConsumeFunctionName() {
   if (!Match(Lexer::TokenType::FUNCTION_NAME)) {
     return absl_ports::InvalidArgumentError(
         "Unable to consume token as FUNCTION_NAME.");
   }
-  auto function_name_node =
-      std::make_unique<FunctionNameNode>(std::move(current_token_->text));
+  std::string function_name = std::move(current_token_->text);
   ++current_token_;
-  return function_name_node;
+  return function_name;
 }
 
 // stringElement
@@ -147,8 +145,7 @@ Parser::ConsumeMember() {
 //    ;
 libtextclassifier3::StatusOr<std::unique_ptr<FunctionNode>>
 Parser::ConsumeFunction() {
-  ICING_ASSIGN_OR_RETURN(std::unique_ptr<FunctionNameNode> function_name,
-                         ConsumeFunctionName());
+  ICING_ASSIGN_OR_RETURN(std::string function_name, ConsumeFunctionName());
   ICING_RETURN_IF_ERROR(Consume(Lexer::TokenType::LPAREN));
 
   std::vector<std::unique_ptr<Node>> args;
diff --git a/icing/query/advanced_query_parser/parser.h b/icing/query/advanced_query_parser/parser.h
index a48c562..95a4014 100644
--- a/icing/query/advanced_query_parser/parser.h
+++ b/icing/query/advanced_query_parser/parser.h
@@ -91,8 +91,7 @@ class Parser {
 
   libtextclassifier3::StatusOr<std::unique_ptr<TextNode>> ConsumeText();
 
-  libtextclassifier3::StatusOr<std::unique_ptr<FunctionNameNode>>
-  ConsumeFunctionName();
+  libtextclassifier3::StatusOr<std::string> ConsumeFunctionName();
 
   libtextclassifier3::StatusOr<std::unique_ptr<StringNode>>
   ConsumeStringElement();
diff --git a/icing/query/advanced_query_parser/parser_integration_test.cc b/icing/query/advanced_query_parser/parser_integration_test.cc
index fa1bd2e..c69d80e 100644
--- a/icing/query/advanced_query_parser/parser_integration_test.cc
+++ b/icing/query/advanced_query_parser/parser_integration_test.cc
@@ -12,6 +12,11 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
 
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
 #include "gmock/gmock.h"
 #include "gtest/gtest.h"
 #include "icing/query/advanced_query_parser/abstract-syntax-tree-test-utils.h"
@@ -34,7 +39,7 @@ TEST(ParserIntegrationTest, EmptyQuery) {
   std::string query = "";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -45,7 +50,7 @@ TEST(ParserIntegrationTest, EmptyScoring) {
   std::string query = "";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   EXPECT_THAT(parser.ConsumeScoring(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
@@ -55,7 +60,7 @@ TEST(ParserIntegrationTest, SingleTerm) {
   std::string query = "foo";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -77,7 +82,7 @@ TEST(ParserIntegrationTest, ImplicitAnd) {
   std::string query = "foo bar";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -104,7 +109,7 @@ TEST(ParserIntegrationTest, Or) {
   std::string query = "foo OR bar";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -131,7 +136,7 @@ TEST(ParserIntegrationTest, And) {
   std::string query = "foo AND bar";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -158,7 +163,7 @@ TEST(ParserIntegrationTest, Not) {
   std::string query = "NOT foo";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -183,7 +188,7 @@ TEST(ParserIntegrationTest, Minus) {
   std::string query = "-foo";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -208,7 +213,7 @@ TEST(ParserIntegrationTest, Has) {
   std::string query = "subject:foo";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -235,7 +240,7 @@ TEST(ParserIntegrationTest, HasNested) {
   std::string query = "sender.name:foo";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -263,102 +268,93 @@ TEST(ParserIntegrationTest, EmptyFunction) {
   std::string query = "foo()";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
 
   // Expected AST:
   //    function
-  //       |
-  // function_name
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   // SimpleVisitor ordering
-  //   { function_name, function }
+  //   { function }
   EXPECT_THAT(visitor.nodes(),
-              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("", NodeType::kFunction)));
+              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunction)));
 }
 
 TEST(ParserIntegrationTest, FunctionSingleArg) {
   std::string query = "foo(\"bar\")";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
 
   // Expected AST:
   //           function
-  //           /     \
-  // function_name  string
+  //              |
+  //           string
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   // SimpleVisitor ordering
-  //   { function_name, string, function }
+  //   { string, function }
   EXPECT_THAT(visitor.nodes(),
-              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("bar", NodeType::kString),
-                          EqualsNodeInfo("", NodeType::kFunction)));
+              ElementsAre(EqualsNodeInfo("bar", NodeType::kString),
+                          EqualsNodeInfo("foo", NodeType::kFunction)));
 }
 
 TEST(ParserIntegrationTest, FunctionMultiArg) {
   std::string query = "foo(\"bar\", \"baz\")";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
 
   // Expected AST:
   //                function
-  //              /    |    \
-  // function_name  string  string
+  //                /     \
+  //            string  string
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   // SimpleVisitor ordering
-  //   { function_name, string, string, function }
+  //   { string, string, function }
   EXPECT_THAT(visitor.nodes(),
-              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("bar", NodeType::kString),
+              ElementsAre(EqualsNodeInfo("bar", NodeType::kString),
                           EqualsNodeInfo("baz", NodeType::kString),
-                          EqualsNodeInfo("", NodeType::kFunction)));
+                          EqualsNodeInfo("foo", NodeType::kFunction)));
 }
 
 TEST(ParserIntegrationTest, FunctionNested) {
   std::string query = "foo(bar())";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
 
   // Expected AST:
   //          function
-  //          /      \
-  // function_name  function
-  //                    |
-  //              function_name
+  //             |
+  //          function
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   // SimpleVisitor ordering
-  //   { function_name, function_name, function, function }
+  //   { function, function }
   EXPECT_THAT(visitor.nodes(),
-              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("bar", NodeType::kFunctionName),
-                          EqualsNodeInfo("", NodeType::kFunction),
-                          EqualsNodeInfo("", NodeType::kFunction)));
+              ElementsAre(EqualsNodeInfo("bar", NodeType::kFunction),
+                          EqualsNodeInfo("foo", NodeType::kFunction)));
 }
 
 TEST(ParserIntegrationTest, FunctionWithTrailingSequence) {
   std::string query = "foo() OR bar";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -367,15 +363,14 @@ TEST(ParserIntegrationTest, FunctionWithTrailingSequence) {
   //             OR
   //          /      \
   //     function   member
-  //        |         |
-  //  function_name  text
+  //                  |
+  //                text
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   // SimpleVisitor ordering
-  //   { function_name, function, text, member, OR }
+  //   { function, text, member, OR }
   EXPECT_THAT(visitor.nodes(),
-              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("", NodeType::kFunction),
+              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunction),
                           EqualsNodeInfo("bar", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
                           EqualsNodeInfo("OR", NodeType::kNaryOperator)));
@@ -385,7 +380,7 @@ TEST(ParserIntegrationTest, Composite) {
   std::string query = "foo OR (bar baz)";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -417,7 +412,7 @@ TEST(ParserIntegrationTest, CompositeWithTrailingSequence) {
   std::string query = "(bar baz) OR foo";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -449,7 +444,7 @@ TEST(ParserIntegrationTest, Complex) {
   std::string query = "foo bar:baz OR pal(\"bat\")";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -460,15 +455,15 @@ TEST(ParserIntegrationTest, Complex) {
   //     member            OR
   //       |          /         \
   //     text      :             function
-  //              / \            /       \
-  //         member member  function_name string
+  //              / \               |
+  //         member member        string
   //           |       |
   //          text    text
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   // SimpleVisitor ordering
-  //   { text, member, text, member, text, member, :, function_name, string,
-  //     function, OR, AND }
+  //   { text, member, text, member, text, member, :, string, function, OR,
+  //     AND }
   EXPECT_THAT(visitor.nodes(),
               ElementsAre(EqualsNodeInfo("foo", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
@@ -477,9 +472,8 @@ TEST(ParserIntegrationTest, Complex) {
                           EqualsNodeInfo("baz", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
                           EqualsNodeInfo(":", NodeType::kNaryOperator),
-                          EqualsNodeInfo("pal", NodeType::kFunctionName),
                           EqualsNodeInfo("bat", NodeType::kString),
-                          EqualsNodeInfo("", NodeType::kFunction),
+                          EqualsNodeInfo("pal", NodeType::kFunction),
                           EqualsNodeInfo("OR", NodeType::kNaryOperator),
                           EqualsNodeInfo("AND", NodeType::kNaryOperator)));
 }
@@ -488,7 +482,7 @@ TEST(ParserIntegrationTest, InvalidHas) {
   std::string query = "foo:";  //  No right hand operand to :
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   EXPECT_THAT(parser.ConsumeQuery(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
@@ -498,7 +492,7 @@ TEST(ParserIntegrationTest, InvalidComposite) {
   std::string query = "(foo bar";  // No terminating RPAREN
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   EXPECT_THAT(parser.ConsumeQuery(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
@@ -508,7 +502,7 @@ TEST(ParserIntegrationTest, InvalidMember) {
   std::string query = "foo.";  // DOT must have succeeding TEXT
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   EXPECT_THAT(parser.ConsumeQuery(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
@@ -518,7 +512,7 @@ TEST(ParserIntegrationTest, InvalidOr) {
   std::string query = "foo OR";  // No right hand operand to OR
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   EXPECT_THAT(parser.ConsumeQuery(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
@@ -528,7 +522,7 @@ TEST(ParserIntegrationTest, InvalidAnd) {
   std::string query = "foo AND";  // No right hand operand to AND
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   EXPECT_THAT(parser.ConsumeQuery(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
@@ -538,7 +532,7 @@ TEST(ParserIntegrationTest, InvalidNot) {
   std::string query = "NOT";  // No right hand operand to NOT
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   EXPECT_THAT(parser.ConsumeQuery(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
@@ -548,7 +542,7 @@ TEST(ParserIntegrationTest, InvalidMinus) {
   std::string query = "-";  // No right hand operand to -
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   EXPECT_THAT(parser.ConsumeQuery(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
@@ -558,7 +552,7 @@ TEST(ParserIntegrationTest, InvalidFunctionCallNoRparen) {
   std::string query = "foo(";  // No terminating RPAREN
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   EXPECT_THAT(parser.ConsumeQuery(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
@@ -568,7 +562,7 @@ TEST(ParserIntegrationTest, InvalidFunctionArgsHangingComma) {
   std::string query = "foo(\"bar\",)";  // no valid arg following COMMA
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   EXPECT_THAT(parser.ConsumeQuery(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
@@ -578,7 +572,7 @@ TEST(ParserIntegrationTest, ScoringPlus) {
   std::string scoring = "1 + 1 + 1";
   Lexer lexer(scoring, Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeScoring());
@@ -605,7 +599,7 @@ TEST(ParserIntegrationTest, ScoringMinus) {
   std::string scoring = "1 - 1 - 1";
   Lexer lexer(scoring, Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeScoring());
@@ -632,7 +626,7 @@ TEST(ParserIntegrationTest, ScoringUnaryMinus) {
   std::string scoring = "1 + -1 + 1";
   Lexer lexer(scoring, Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeScoring());
@@ -662,7 +656,7 @@ TEST(ParserIntegrationTest, ScoringPlusMinus) {
   std::string scoring = "11 + 12 - 13 + 14";
   Lexer lexer(scoring, Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeScoring());
@@ -697,7 +691,7 @@ TEST(ParserIntegrationTest, ScoringTimes) {
   std::string scoring = "1 * 1 * 1";
   Lexer lexer(scoring, Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeScoring());
@@ -724,7 +718,7 @@ TEST(ParserIntegrationTest, ScoringDiv) {
   std::string scoring = "1 / 1 / 1";
   Lexer lexer(scoring, Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeScoring());
@@ -751,7 +745,7 @@ TEST(ParserIntegrationTest, ScoringTimesDiv) {
   std::string scoring = "11 / 12 * 13 / 14 / 15";
   Lexer lexer(scoring, Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeScoring());
@@ -789,7 +783,7 @@ TEST(ParserIntegrationTest, ComplexScoring) {
   std::string scoring = "1 + pow((2 * sin(3)), 4) + -5 / 6";
   Lexer lexer(scoring, Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeScoring());
@@ -799,17 +793,15 @@ TEST(ParserIntegrationTest, ComplexScoring) {
   EXPECT_THAT(node,
               ElementsAre(EqualsNodeInfo("1", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
-                          EqualsNodeInfo("pow", NodeType::kFunctionName),
                           EqualsNodeInfo("2", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
-                          EqualsNodeInfo("sin", NodeType::kFunctionName),
                           EqualsNodeInfo("3", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
-                          EqualsNodeInfo("", NodeType::kFunction),
+                          EqualsNodeInfo("sin", NodeType::kFunction),
                           EqualsNodeInfo("TIMES", NodeType::kNaryOperator),
                           EqualsNodeInfo("4", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
-                          EqualsNodeInfo("", NodeType::kFunction),
+                          EqualsNodeInfo("pow", NodeType::kFunction),
                           EqualsNodeInfo("5", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
                           EqualsNodeInfo("MINUS", NodeType::kUnaryOperator),
@@ -821,7 +813,7 @@ TEST(ParserIntegrationTest, ComplexScoring) {
   // Without parentheses in function arguments.
   scoring = "1 + pow(2 * sin(3), 4) + -5 / 6";
   lexer = Lexer(scoring, Lexer::Language::SCORING);
-  ICING_ASSERT_OK_AND_ASSIGN(lexer_tokens, lexer.ExtractTokens());
+  ICING_ASSERT_OK_AND_ASSIGN(lexer_tokens, std::move(lexer).ExtractTokens());
   parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(tree_root, parser.ConsumeScoring());
   visitor = SimpleVisitor();
@@ -833,7 +825,7 @@ TEST(ParserIntegrationTest, ScoringMemberFunction) {
   std::string scoring = "this.CreationTimestamp()";
   Lexer lexer(scoring, Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeScoring());
@@ -842,15 +834,12 @@ TEST(ParserIntegrationTest, ScoringMemberFunction) {
   //       member
   //     /        \
   //  text     function
-  //               |
-  //          function_name
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   EXPECT_THAT(
       visitor.nodes(),
       ElementsAre(EqualsNodeInfo("this", NodeType::kText),
-                  EqualsNodeInfo("CreationTimestamp", NodeType::kFunctionName),
-                  EqualsNodeInfo("", NodeType::kFunction),
+                  EqualsNodeInfo("CreationTimestamp", NodeType::kFunction),
                   EqualsNodeInfo("", NodeType::kMember)));
 }
 
@@ -858,7 +847,7 @@ TEST(ParserIntegrationTest, QueryMemberFunction) {
   std::string query = "this.foo()";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -867,14 +856,11 @@ TEST(ParserIntegrationTest, QueryMemberFunction) {
   //       member
   //     /        \
   //  text     function
-  //               |
-  //          function_name
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   EXPECT_THAT(visitor.nodes(),
               ElementsAre(EqualsNodeInfo("this", NodeType::kText),
-                          EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("", NodeType::kFunction),
+                          EqualsNodeInfo("foo", NodeType::kFunction),
                           EqualsNodeInfo("", NodeType::kMember)));
 }
 
@@ -882,7 +868,7 @@ TEST(ParserIntegrationTest, ScoringComplexMemberFunction) {
   std::string scoring = "a.b.fun(c, d)";
   Lexer lexer(scoring, Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeScoring());
@@ -891,21 +877,20 @@ TEST(ParserIntegrationTest, ScoringComplexMemberFunction) {
   //                member
   //         /        |          \
   //  text          text         function
-  //                        /        |       \
-  //               function_name   member    member
-  //                                 |         |
-  //                                text      text
+  //                              /     \
+  //                          member    member
+  //                            |         |
+  //                           text      text
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   EXPECT_THAT(visitor.nodes(),
               ElementsAre(EqualsNodeInfo("a", NodeType::kText),
                           EqualsNodeInfo("b", NodeType::kText),
-                          EqualsNodeInfo("fun", NodeType::kFunctionName),
                           EqualsNodeInfo("c", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
                           EqualsNodeInfo("d", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
-                          EqualsNodeInfo("", NodeType::kFunction),
+                          EqualsNodeInfo("fun", NodeType::kFunction),
                           EqualsNodeInfo("", NodeType::kMember)));
 }
 
@@ -913,7 +898,7 @@ TEST(ParserTest, QueryComplexMemberFunction) {
   std::string query = "this.abc.fun(def, ghi)";
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> tree_root,
                              parser.ConsumeQuery());
@@ -922,21 +907,20 @@ TEST(ParserTest, QueryComplexMemberFunction) {
   //                member
   //         /        |          \
   //  text          text         function
-  //                        /        |       \
-  //               function_name   member    member
-  //                                 |         |
-  //                                text      text
+  //                            /       \
+  //                         member    member
+  //                           |         |
+  //                          text      text
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   EXPECT_THAT(visitor.nodes(),
               ElementsAre(EqualsNodeInfo("this", NodeType::kText),
                           EqualsNodeInfo("abc", NodeType::kText),
-                          EqualsNodeInfo("fun", NodeType::kFunctionName),
                           EqualsNodeInfo("def", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
                           EqualsNodeInfo("ghi", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
-                          EqualsNodeInfo("", NodeType::kFunction),
+                          EqualsNodeInfo("fun", NodeType::kFunction),
                           EqualsNodeInfo("", NodeType::kMember)));
 }
 
@@ -953,7 +937,7 @@ TEST(ParserTest, QueryShouldNotStackOverflowAtMaxNumTokens) {
 
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   EXPECT_THAT(lexer_tokens, SizeIs(Lexer::kMaxNumTokens));
   Parser parser = Parser::Create(std::move(lexer_tokens));
   EXPECT_THAT(parser.ConsumeQuery(), IsOk());
@@ -972,7 +956,7 @@ TEST(ParserTest, ScoringShouldNotStackOverflowAtMaxNumTokens) {
 
   Lexer lexer(scoring, Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   EXPECT_THAT(lexer_tokens, SizeIs(Lexer::kMaxNumTokens));
   Parser parser = Parser::Create(std::move(lexer_tokens));
   EXPECT_THAT(parser.ConsumeScoring(), IsOk());
@@ -985,7 +969,7 @@ TEST(ParserTest, InvalidQueryShouldNotStackOverflowAtMaxNumTokens) {
   }
   Lexer lexer(query, Lexer::Language::QUERY);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   EXPECT_THAT(lexer_tokens, SizeIs(Lexer::kMaxNumTokens));
   Parser parser = Parser::Create(std::move(lexer_tokens));
   EXPECT_THAT(parser.ConsumeQuery(),
@@ -999,7 +983,7 @@ TEST(ParserTest, InvalidScoringShouldNotStackOverflowAtMaxNumTokens) {
   }
   Lexer lexer(scoring, Lexer::Language::SCORING);
   ICING_ASSERT_OK_AND_ASSIGN(std::vector<Lexer::LexerToken> lexer_tokens,
-                             lexer.ExtractTokens());
+                             std::move(lexer).ExtractTokens());
   EXPECT_THAT(lexer_tokens, SizeIs(Lexer::kMaxNumTokens));
   Parser parser = Parser::Create(std::move(lexer_tokens));
   EXPECT_THAT(parser.ConsumeScoring(),
diff --git a/icing/query/advanced_query_parser/parser_test.cc b/icing/query/advanced_query_parser/parser_test.cc
index 824c2ce..73b7458 100644
--- a/icing/query/advanced_query_parser/parser_test.cc
+++ b/icing/query/advanced_query_parser/parser_test.cc
@@ -270,15 +270,12 @@ TEST(ParserTest, EmptyFunction) {
 
   // Expected AST:
   //    function
-  //       |
-  // function_name
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   // SimpleVisitor ordering
-  //   { function_name, function }
+  //   { function }
   EXPECT_THAT(visitor.nodes(),
-              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("", NodeType::kFunction)));
+              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunction)));
 }
 
 TEST(ParserTest, FunctionSingleArg) {
@@ -294,16 +291,15 @@ TEST(ParserTest, FunctionSingleArg) {
 
   // Expected AST:
   //           function
-  //           /     \
-  // function_name  string
+  //              |
+  //            string
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   // SimpleVisitor ordering
-  //   { function_name, string, function }
+  //   { string, function }
   EXPECT_THAT(visitor.nodes(),
-              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("bar", NodeType::kString),
-                          EqualsNodeInfo("", NodeType::kFunction)));
+              ElementsAre(EqualsNodeInfo("bar", NodeType::kString),
+                          EqualsNodeInfo("foo", NodeType::kFunction)));
 }
 
 TEST(ParserTest, FunctionMultiArg) {
@@ -321,17 +317,16 @@ TEST(ParserTest, FunctionMultiArg) {
 
   // Expected AST:
   //                function
-  //              /    |    \
-  // function_name  string  string
+  //                 /    \
+  //             string  string
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   // SimpleVisitor ordering
-  //   { function_name, string, string, function }
+  //   { string, string, function }
   EXPECT_THAT(visitor.nodes(),
-              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("bar", NodeType::kString),
+              ElementsAre(EqualsNodeInfo("bar", NodeType::kString),
                           EqualsNodeInfo("baz", NodeType::kString),
-                          EqualsNodeInfo("", NodeType::kFunction)));
+                          EqualsNodeInfo("foo", NodeType::kFunction)));
 }
 
 TEST(ParserTest, FunctionNested) {
@@ -349,19 +344,15 @@ TEST(ParserTest, FunctionNested) {
 
   // Expected AST:
   //          function
-  //          /      \
-  // function_name  function
-  //                    |
-  //              function_name
+  //             |
+  //          function
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   // SimpleVisitor ordering
-  //   { function_name, function_name, function, function }
+  //   { function, function }
   EXPECT_THAT(visitor.nodes(),
-              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("bar", NodeType::kFunctionName),
-                          EqualsNodeInfo("", NodeType::kFunction),
-                          EqualsNodeInfo("", NodeType::kFunction)));
+              ElementsAre(EqualsNodeInfo("bar", NodeType::kFunction),
+                          EqualsNodeInfo("foo", NodeType::kFunction)));
 }
 
 TEST(ParserTest, FunctionWithTrailingSequence) {
@@ -380,15 +371,14 @@ TEST(ParserTest, FunctionWithTrailingSequence) {
   //             OR
   //          /      \
   //     function   member
-  //        |         |
-  //  function_name  text
+  //                  |
+  //                text
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   // SimpleVisitor ordering
-  //   { function_name, function, text, member, OR }
+  //   { function, text, member, OR }
   EXPECT_THAT(visitor.nodes(),
-              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("", NodeType::kFunction),
+              ElementsAre(EqualsNodeInfo("foo", NodeType::kFunction),
                           EqualsNodeInfo("bar", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
                           EqualsNodeInfo("OR", NodeType::kNaryOperator)));
@@ -488,15 +478,15 @@ TEST(ParserTest, Complex) {
   //     member            OR
   //       |          /         \
   //     text      :             function
-  //              / \            /       \
-  //         member member  function_name string
+  //              / \               |
+  //         member member        string
   //           |       |
   //          text    text
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   // SimpleVisitor ordering
-  //   { text, member, text, member, text, member, :, function_name, string,
-  //     function, OR, AND }
+  //   { text, member, text, member, text, member, :, string, function, OR,
+  //     AND }
   EXPECT_THAT(visitor.nodes(),
               ElementsAre(EqualsNodeInfo("foo", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
@@ -505,9 +495,8 @@ TEST(ParserTest, Complex) {
                           EqualsNodeInfo("baz", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
                           EqualsNodeInfo(":", NodeType::kNaryOperator),
-                          EqualsNodeInfo("pal", NodeType::kFunctionName),
                           EqualsNodeInfo("bat", NodeType::kString),
-                          EqualsNodeInfo("", NodeType::kFunction),
+                          EqualsNodeInfo("pal", NodeType::kFunction),
                           EqualsNodeInfo("OR", NodeType::kNaryOperator),
                           EqualsNodeInfo("AND", NodeType::kNaryOperator)));
 }
@@ -887,17 +876,15 @@ TEST(ParserTest, ComplexScoring) {
   EXPECT_THAT(node,
               ElementsAre(EqualsNodeInfo("1", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
-                          EqualsNodeInfo("pow", NodeType::kFunctionName),
                           EqualsNodeInfo("2", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
-                          EqualsNodeInfo("sin", NodeType::kFunctionName),
                           EqualsNodeInfo("3", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
-                          EqualsNodeInfo("", NodeType::kFunction),
+                          EqualsNodeInfo("sin", NodeType::kFunction),
                           EqualsNodeInfo("TIMES", NodeType::kNaryOperator),
                           EqualsNodeInfo("4", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
-                          EqualsNodeInfo("", NodeType::kFunction),
+                          EqualsNodeInfo("pow", NodeType::kFunction),
                           EqualsNodeInfo("5", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
                           EqualsNodeInfo("MINUS", NodeType::kUnaryOperator),
@@ -952,15 +939,12 @@ TEST(ParserTest, ScoringMemberFunction) {
   //       member
   //     /        \
   //  text     function
-  //               |
-  //          function_name
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   EXPECT_THAT(
       visitor.nodes(),
       ElementsAre(EqualsNodeInfo("this", NodeType::kText),
-                  EqualsNodeInfo("CreationTimestamp", NodeType::kFunctionName),
-                  EqualsNodeInfo("", NodeType::kFunction),
+                  EqualsNodeInfo("CreationTimestamp", NodeType::kFunction),
                   EqualsNodeInfo("", NodeType::kMember)));
 }
 
@@ -980,14 +964,11 @@ TEST(ParserTest, QueryMemberFunction) {
   //       member
   //     /        \
   //  text     function
-  //               |
-  //          function_name
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   EXPECT_THAT(visitor.nodes(),
               ElementsAre(EqualsNodeInfo("this", NodeType::kText),
-                          EqualsNodeInfo("foo", NodeType::kFunctionName),
-                          EqualsNodeInfo("", NodeType::kFunction),
+                          EqualsNodeInfo("foo", NodeType::kFunction),
                           EqualsNodeInfo("", NodeType::kMember)));
 }
 
@@ -1012,21 +993,20 @@ TEST(ParserTest, ScoringComplexMemberFunction) {
   //                member
   //         /        |          \
   //  text          text         function
-  //                        /        |       \
-  //               function_name   member    member
-  //                                 |         |
-  //                                text      text
+  //                              /      \
+  //                          member    member
+  //                            |         |
+  //                           text      text
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   EXPECT_THAT(visitor.nodes(),
               ElementsAre(EqualsNodeInfo("a", NodeType::kText),
                           EqualsNodeInfo("b", NodeType::kText),
-                          EqualsNodeInfo("fun", NodeType::kFunctionName),
                           EqualsNodeInfo("c", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
                           EqualsNodeInfo("d", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
-                          EqualsNodeInfo("", NodeType::kFunction),
+                          EqualsNodeInfo("fun", NodeType::kFunction),
                           EqualsNodeInfo("", NodeType::kMember)));
 }
 
@@ -1051,21 +1031,20 @@ TEST(ParserTest, QueryComplexMemberFunction) {
   //                member
   //         /        |          \
   //  text          text         function
-  //                        /        |       \
-  //               function_name   member    member
-  //                                 |         |
-  //                                text      text
+  //                            /       \
+  //                         member    member
+  //                           |         |
+  //                          text      text
   SimpleVisitor visitor;
   tree_root->Accept(&visitor);
   EXPECT_THAT(visitor.nodes(),
               ElementsAre(EqualsNodeInfo("this", NodeType::kText),
                           EqualsNodeInfo("abc", NodeType::kText),
-                          EqualsNodeInfo("fun", NodeType::kFunctionName),
                           EqualsNodeInfo("def", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
                           EqualsNodeInfo("ghi", NodeType::kText),
                           EqualsNodeInfo("", NodeType::kMember),
-                          EqualsNodeInfo("", NodeType::kFunction),
+                          EqualsNodeInfo("fun", NodeType::kFunction),
                           EqualsNodeInfo("", NodeType::kMember)));
 }
 
diff --git a/icing/query/advanced_query_parser/query-visitor.cc b/icing/query/advanced_query_parser/query-visitor.cc
index 12426ed..6178525 100644
--- a/icing/query/advanced_query_parser/query-visitor.cc
+++ b/icing/query/advanced_query_parser/query-visitor.cc
@@ -175,7 +175,7 @@ QueryVisitor::CreateTermIterator(const QueryTerm& query_term) {
   }
   TermMatchType::Code match_type = GetTermMatchType(query_term.is_prefix_val);
   int unnormalized_term_start =
-      query_term.raw_term.data() - raw_query_text_.data();
+      query_term.raw_term.data() - search_spec_.query().c_str();
   if (!processing_not_) {
     // 1. Add term to property_query_terms_map
     if (pending_property_restricts_.has_active_property_restricts()) {
@@ -193,7 +193,8 @@ QueryVisitor::CreateTermIterator(const QueryTerm& query_term) {
           std::unique_ptr<DocHitInfoIterator> term_iterator,
           index_.GetIterator(query_term.term, unnormalized_term_start,
                              query_term.raw_term.length(), kSectionIdMaskAll,
-                             match_type_, needs_term_frequency_info_));
+                             search_spec_.term_match_type(),
+                             needs_term_frequency_info_));
       query_term_iterators_[query_term.term] =
           std::make_unique<DocHitInfoIteratorFilter>(
               std::move(term_iterator), &document_store_, &schema_store_,
@@ -254,18 +255,18 @@ void QueryVisitor::RegisterFunctions() {
   registered_functions_.insert(
       {has_property_function.name(), std::move(has_property_function)});
 
-  // vector_index getSearchSpecEmbedding(long);
-  auto get_search_spec_embedding = [](std::vector<PendingValue>&& args) {
+  // vector_index getEmbeddingParameter(long);
+  auto get_embedding_parameter = [](std::vector<PendingValue>&& args) {
     return PendingValue::CreateVectorIndexPendingValue(
         args.at(0).long_val().ValueOrDie());
   };
-  Function get_search_spec_embedding_function =
-      Function::Create(DataType::kVectorIndex, "getSearchSpecEmbedding",
+  Function get_embedding_parameter_function =
+      Function::Create(DataType::kVectorIndex, "getEmbeddingParameter",
                        {Param(DataType::kLong)},
-                       std::move(get_search_spec_embedding))
+                       std::move(get_embedding_parameter))
           .ValueOrDie();
-  registered_functions_.insert({get_search_spec_embedding_function.name(),
-                                std::move(get_search_spec_embedding_function)});
+  registered_functions_.insert({get_embedding_parameter_function.name(),
+                                get_embedding_parameter_function});
 
   // DocHitInfoIterator semanticSearch(vector_index, double, double, string);
   auto semantic_search = [this](std::vector<PendingValue>&& args) {
@@ -282,16 +283,18 @@ void QueryVisitor::RegisterFunctions() {
   registered_functions_.insert(
       {semantic_search_function.name(), std::move(semantic_search_function)});
 
-  // DocHitInfoIterator tokenize(std::string);
-  auto tokenize = [this](std::vector<PendingValue>&& args) {
-    return this->TokenizeFunction(std::move(args));
+  // DocHitInfoIterator getSearchStringParameter(long);
+  auto get_search_string_parameter = [this](std::vector<PendingValue>&& args) {
+    return this->GetSearchStringParameterFunction(std::move(args));
   };
-  Function tokenize_function =
-      Function::Create(DataType::kDocumentIterator, "tokenize",
-                       {Param(DataType::kString)}, std::move(tokenize))
+  Function get_search_string_parameter_function =
+      Function::Create(DataType::kDocumentIterator, "getSearchStringParameter",
+                       {Param(DataType::kLong)},
+                       std::move(get_search_string_parameter))
           .ValueOrDie();
   registered_functions_.insert(
-      {tokenize_function.name(), std::move(tokenize_function)});
+      {get_search_string_parameter_function.name(),
+       std::move(get_search_string_parameter_function)});
 }
 
 libtextclassifier3::StatusOr<PendingValue> QueryVisitor::SearchFunction(
@@ -316,7 +319,7 @@ libtextclassifier3::StatusOr<PendingValue> QueryVisitor::SearchFunction(
   const QueryTerm* query = args.at(0).string_val().ValueOrDie();
   Lexer lexer(query->term, Lexer::Language::QUERY);
   ICING_ASSIGN_OR_RETURN(std::vector<Lexer::LexerToken> lexer_tokens,
-                         lexer.ExtractTokens());
+                         std::move(lexer).ExtractTokens());
 
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSIGN_OR_RETURN(std::unique_ptr<Node> tree_root,
@@ -330,9 +333,8 @@ libtextclassifier3::StatusOr<PendingValue> QueryVisitor::SearchFunction(
   } else {
     QueryVisitor query_visitor(
         &index_, &numeric_index_, &embedding_index_, &document_store_,
-        &schema_store_, &normalizer_, &tokenizer_, query->raw_term,
-        embedding_query_vectors_, filter_options_, match_type_,
-        embedding_query_metric_type_, needs_term_frequency_info_,
+        &schema_store_, &normalizer_, &tokenizer_, search_spec_,
+        filter_options_, needs_term_frequency_info_,
         pending_property_restricts_, processing_not_, current_time_ms_);
     tree_root->Accept(&query_visitor);
     ICING_ASSIGN_OR_RETURN(query_result,
@@ -413,19 +415,17 @@ libtextclassifier3::StatusOr<PendingValue> QueryVisitor::HasPropertyFunction(
 
 libtextclassifier3::StatusOr<PendingValue> QueryVisitor::SemanticSearchFunction(
     std::vector<PendingValue>&& args) {
-  features_.insert(kEmbeddingSearchFeature);
-
   int64_t vector_index = args.at(0).vector_index_val().ValueOrDie();
-  if (embedding_query_vectors_ == nullptr || vector_index < 0 ||
-      vector_index >= embedding_query_vectors_->size()) {
-    return absl_ports::InvalidArgumentError("Got invalid vector search index!");
+  if (vector_index < 0 ||
+      vector_index >= search_spec_.embedding_query_vectors_size()) {
+    return absl_ports::OutOfRangeError("Got invalid vector search index!");
   }
 
   // Handle default values for the optional arguments.
   double low = -std::numeric_limits<double>::infinity();
   double high = std::numeric_limits<double>::infinity();
   SearchSpecProto::EmbeddingQueryMetricType::Code metric_type =
-      embedding_query_metric_type_;
+      search_spec_.embedding_query_metric_type();
   if (args.size() >= 2) {
     low = args.at(1).double_val().ValueOrDie();
   }
@@ -443,35 +443,29 @@ libtextclassifier3::StatusOr<PendingValue> QueryVisitor::SemanticSearchFunction(
         embedding_util::GetEmbeddingQueryMetricTypeFromName(metric));
   }
 
-  // Create SectionRestrictData for section restriction.
-  std::unique_ptr<SectionRestrictData> section_restrict_data = nullptr;
-  if (pending_property_restricts_.has_active_property_restricts()) {
-    std::unordered_map<std::string, std::set<std::string>>
-        type_property_filters;
-    type_property_filters[std::string(SchemaStore::kSchemaTypeWildcard)] =
-        pending_property_restricts_.active_property_restricts();
-    section_restrict_data = std::make_unique<SectionRestrictData>(
-        &document_store_, &schema_store_, current_time_ms_,
-        type_property_filters);
-  }
-
   // Create and return iterator.
   EmbeddingQueryResults::EmbeddingQueryScoreMap* score_map =
       &embedding_query_results_.result_scores[vector_index][metric_type];
-  ICING_ASSIGN_OR_RETURN(std::unique_ptr<DocHitInfoIterator> iterator,
-                         DocHitInfoIteratorEmbedding::Create(
-                             &embedding_query_vectors_->at(vector_index),
-                             std::move(section_restrict_data), metric_type, low,
-                             high, score_map, &embedding_index_));
+  ICING_ASSIGN_OR_RETURN(
+      std::unique_ptr<DocHitInfoIterator> iterator,
+      DocHitInfoIteratorEmbedding::Create(
+          &search_spec_.embedding_query_vectors(vector_index), metric_type, low,
+          high, score_map, &embedding_index_));
   return PendingValue(std::move(iterator));
 }
 
-libtextclassifier3::StatusOr<PendingValue> QueryVisitor::TokenizeFunction(
+libtextclassifier3::StatusOr<PendingValue>
+QueryVisitor::GetSearchStringParameterFunction(
     std::vector<PendingValue>&& args) {
-  features_.insert(kTokenizeFeature);
-
-  QueryTerm text_value = std::move(args.at(0)).string_val().ValueOrDie();
-  text_value.is_prefix_val = false;  // the prefix operator cannot be used here.
+  int64_t string_index = args.at(0).long_val().ValueOrDie();
+  if (string_index < 0 ||
+      string_index >= search_spec_.query_parameter_strings_size()) {
+    return absl_ports::OutOfRangeError("Got invalid string search index!");
+  }
+  const std::string& string_value =
+      search_spec_.query_parameter_strings(string_index);
+  // the prefix operator cannot be used here.
+  QueryTerm text_value = {string_value, string_value, /*is_prefix_val=*/false};
   ICING_ASSIGN_OR_RETURN(std::unique_ptr<DocHitInfoIterator> iterator,
                          ProduceTextTokenIterators(std::move(text_value)));
   return PendingValue(std::move(iterator));
@@ -811,11 +805,6 @@ libtextclassifier3::Status QueryVisitor::ProcessHasOperator(
   return libtextclassifier3::Status::OK;
 }
 
-void QueryVisitor::VisitFunctionName(const FunctionNameNode* node) {
-  pending_error_ = absl_ports::UnimplementedError(
-      "Function Name node visiting not implemented yet.");
-}
-
 void QueryVisitor::VisitString(const StringNode* node) {
   // A STRING node can only be a term. Create the iterator now.
   auto unescaped_string_or = string_util::UnescapeStringValue(node->value());
@@ -916,10 +905,10 @@ void QueryVisitor::VisitMember(const MemberNode* node) {
 
 void QueryVisitor::VisitFunction(const FunctionNode* node) {
   // 1. Get the associated function.
-  auto itr = registered_functions_.find(node->function_name()->value());
+  auto itr = registered_functions_.find(node->function_name());
   if (itr == registered_functions_.end()) {
     pending_error_ = absl_ports::InvalidArgumentError(absl_ports::StrCat(
-        "Function ", node->function_name()->value(), " is not supported."));
+        "Function ", node->function_name(), " is not supported."));
     return;
   }
   const Function& function = itr->second;
diff --git a/icing/query/advanced_query_parser/query-visitor.h b/icing/query/advanced_query_parser/query-visitor.h
index f28ef2f..6089ef4 100644
--- a/icing/query/advanced_query_parser/query-visitor.h
+++ b/icing/query/advanced_query_parser/query-visitor.h
@@ -53,27 +53,22 @@ namespace lib {
 // the parser.
 class QueryVisitor : public AbstractSyntaxTreeVisitor {
  public:
-  explicit QueryVisitor(
-      Index* index, const NumericIndex<int64_t>* numeric_index,
-      const EmbeddingIndex* embedding_index,
-      const DocumentStore* document_store, const SchemaStore* schema_store,
-      const Normalizer* normalizer, const Tokenizer* tokenizer,
-      std::string_view raw_query_text,
-      const google::protobuf::RepeatedPtrField<PropertyProto::VectorProto>*
-          embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options filter_options,
-      TermMatchType::Code match_type,
-      SearchSpecProto::EmbeddingQueryMetricType::Code
-          embedding_query_metric_type,
-      bool needs_term_frequency_info, int64_t current_time_ms)
+  explicit QueryVisitor(Index* index,
+                        const NumericIndex<int64_t>* numeric_index,
+                        const EmbeddingIndex* embedding_index,
+                        const DocumentStore* document_store,
+                        const SchemaStore* schema_store,
+                        const Normalizer* normalizer,
+                        const Tokenizer* tokenizer,
+                        const SearchSpecProto& search_spec,
+                        DocHitInfoIteratorFilter::Options filter_options,
+                        bool needs_term_frequency_info, int64_t current_time_ms)
       : QueryVisitor(index, numeric_index, embedding_index, document_store,
-                     schema_store, normalizer, tokenizer, raw_query_text,
-                     embedding_query_vectors, filter_options, match_type,
-                     embedding_query_metric_type, needs_term_frequency_info,
-                     PendingPropertyRestricts(),
-                     /*processing_not=*/false, current_time_ms) {}
+                     schema_store, normalizer, tokenizer, search_spec,
+                     filter_options, needs_term_frequency_info,
+                     PendingPropertyRestricts(), /*processing_not=*/false,
+                     current_time_ms) {}
 
-  void VisitFunctionName(const FunctionNameNode* node) override;
   void VisitString(const StringNode* node) override;
   void VisitText(const TextNode* node) override;
   void VisitMember(const MemberNode* node) override;
@@ -116,21 +111,18 @@ class QueryVisitor : public AbstractSyntaxTreeVisitor {
     std::vector<std::set<std::string>> pending_property_restricts_;
   };
 
-  explicit QueryVisitor(
-      Index* index, const NumericIndex<int64_t>* numeric_index,
-      const EmbeddingIndex* embedding_index,
-      const DocumentStore* document_store, const SchemaStore* schema_store,
-      const Normalizer* normalizer, const Tokenizer* tokenizer,
-      std::string_view raw_query_text,
-      const google::protobuf::RepeatedPtrField<PropertyProto::VectorProto>*
-          embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options filter_options,
-      TermMatchType::Code match_type,
-      SearchSpecProto::EmbeddingQueryMetricType::Code
-          embedding_query_metric_type,
-      bool needs_term_frequency_info,
-      PendingPropertyRestricts pending_property_restricts, bool processing_not,
-      int64_t current_time_ms)
+  explicit QueryVisitor(Index* index,
+                        const NumericIndex<int64_t>* numeric_index,
+                        const EmbeddingIndex* embedding_index,
+                        const DocumentStore* document_store,
+                        const SchemaStore* schema_store,
+                        const Normalizer* normalizer,
+                        const Tokenizer* tokenizer,
+                        const SearchSpecProto& search_spec,
+                        DocHitInfoIteratorFilter::Options filter_options,
+                        bool needs_term_frequency_info,
+                        PendingPropertyRestricts pending_property_restricts,
+                        bool processing_not, int64_t current_time_ms)
       : index_(*index),
         numeric_index_(*numeric_index),
         embedding_index_(*embedding_index),
@@ -138,11 +130,8 @@ class QueryVisitor : public AbstractSyntaxTreeVisitor {
         schema_store_(*schema_store),
         normalizer_(*normalizer),
         tokenizer_(*tokenizer),
-        raw_query_text_(raw_query_text),
-        embedding_query_vectors_(embedding_query_vectors),
+        search_spec_(search_spec),
         filter_options_(std::move(filter_options)),
-        match_type_(match_type),
-        embedding_query_metric_type_(embedding_query_metric_type),
         needs_term_frequency_info_(needs_term_frequency_info),
         pending_property_restricts_(std::move(pending_property_restricts)),
         processing_not_(processing_not),
@@ -299,7 +288,7 @@ class QueryVisitor : public AbstractSyntaxTreeVisitor {
 
   // Implementation of the semanticSearch(vector, low, high, metric) custom
   // function. This function is used for supporting vector search with a
-  // syntax like `semanticSearch(getSearchSpecEmbedding(0), 0.5, 1, "COSINE")`.
+  // syntax like `semanticSearch(getEmbeddingParameter(0), 0.5, 1, "COSINE")`.
   //
   // low, high, metric are optional parameters:
   //   - low is default to negative infinity
@@ -309,16 +298,23 @@ class QueryVisitor : public AbstractSyntaxTreeVisitor {
   // Returns:
   //   - a Pending Value of type DocHitIterator that returns all documents with
   //     an embedding vector that has a score within [low, high].
+  //   - OUT_OF_RANGE if index provided to getEmbeddingParameter is out of
+  //     bounds of SearchSpec.embedding_query_vectors()
   //   - any errors returned by Lexer::ExtractTokens
   libtextclassifier3::StatusOr<PendingValue> SemanticSearchFunction(
       std::vector<PendingValue>&& args);
 
-  // Implementation of the tokenize(string) custom function.
+  // Implementation of the getSearchStringParameter(index) custom function.
+  // Retrieves the parameterized string stored at
+  // SearchSpec.query_parameter_strings(index).
+  //
   // Returns:
   //   - a Pending Value holding a DocHitIterator that returns hits for all
   //     documents containing the normalized tokens present in the string.
+  //   - OUT_OF_RANGE if index is out of bounds of
+  //     SearchSpec.query_parameter_strings()
   //   - any errors returned by ProduceTextTokenIterators
-  libtextclassifier3::StatusOr<PendingValue> TokenizeFunction(
+  libtextclassifier3::StatusOr<PendingValue> GetSearchStringParameterFunction(
       std::vector<PendingValue>&& args);
 
   // Handles a NaryOperatorNode where the operator is HAS (':') and pushes an
@@ -337,7 +333,7 @@ class QueryVisitor : public AbstractSyntaxTreeVisitor {
   // Returns the correct match type to apply based on both the match type and
   // whether the prefix operator is currently present.
   TermMatchType::Code GetTermMatchType(bool is_prefix) const {
-    return (is_prefix) ? TermMatchType::PREFIX : match_type_;
+    return (is_prefix) ? TermMatchType::PREFIX : search_spec_.term_match_type();
   }
 
   std::stack<PendingValue> pending_values_;
@@ -363,12 +359,10 @@ class QueryVisitor : public AbstractSyntaxTreeVisitor {
   const Normalizer& normalizer_;                // Does not own!
   const Tokenizer& tokenizer_;                  // Does not own!
 
-  std::string_view raw_query_text_;
-  const google::protobuf::RepeatedPtrField<PropertyProto::VectorProto>*
-      embedding_query_vectors_;  // Nullable, does not own!
+  const SearchSpecProto& search_spec_;
+
   DocHitInfoIteratorFilter::Options filter_options_;
-  TermMatchType::Code match_type_;
-  SearchSpecProto::EmbeddingQueryMetricType::Code embedding_query_metric_type_;
+
   // Whether or not term_frequency information is needed. This affects:
   //  - how DocHitInfoIteratorTerms are constructed
   //  - whether the QueryTermIteratorsMap is populated in the QueryResults.
diff --git a/icing/query/advanced_query_parser/query-visitor_test.cc b/icing/query/advanced_query_parser/query-visitor_test.cc
index e873357..e07e12e 100644
--- a/icing/query/advanced_query_parser/query-visitor_test.cc
+++ b/icing/query/advanced_query_parser/query-visitor_test.cc
@@ -16,6 +16,7 @@
 
 #include <cstdint>
 #include <initializer_list>
+#include <iterator>
 #include <limits>
 #include <memory>
 #include <string>
@@ -117,6 +118,48 @@ std::vector<T> ExtractKeys(const std::unordered_map<T, U>& map) {
   return keys;
 }
 
+SearchSpecProto CreateSearchSpec(
+    std::string query, TermMatchType::Code term_match_type,
+    std::vector<std::string> query_parameter_strings,
+    std::vector<PropertyProto::VectorProto> embedding_query_vectors,
+    SearchSpecProto::EmbeddingQueryMetricType::Code embedding_metric_type) {
+  SearchSpecProto search_spec;
+  search_spec.set_query(std::move(query));
+  search_spec.set_term_match_type(term_match_type);
+  search_spec.set_embedding_query_metric_type(embedding_metric_type);
+  search_spec.mutable_embedding_query_vectors()->Add(
+      std::make_move_iterator(embedding_query_vectors.begin()),
+      std::make_move_iterator(embedding_query_vectors.end()));
+  search_spec.mutable_query_parameter_strings()->Add(
+      std::make_move_iterator(query_parameter_strings.begin()),
+      std::make_move_iterator(query_parameter_strings.end()));
+  return search_spec;
+}
+
+SearchSpecProto CreateSearchSpec(
+    std::string query, TermMatchType::Code term_match_type,
+    std::vector<PropertyProto::VectorProto> embedding_query_vectors,
+    SearchSpecProto::EmbeddingQueryMetricType::Code embedding_metric_type) {
+  return CreateSearchSpec(
+      std::move(query), term_match_type, /*query_parameter_strings=*/{},
+      std::move(embedding_query_vectors), embedding_metric_type);
+}
+
+SearchSpecProto CreateSearchSpec(
+    std::string query, TermMatchType::Code term_match_type,
+    std::vector<std::string> query_parameter_strings) {
+  return CreateSearchSpec(
+      std::move(query), term_match_type, std::move(query_parameter_strings),
+      /*embedding_query_vectors=*/{}, EMBEDDING_METRIC_UNKNOWN);
+}
+
+SearchSpecProto CreateSearchSpec(std::string query,
+                                 TermMatchType::Code term_match_type) {
+  return CreateSearchSpec(
+      std::move(query), term_match_type, /*query_parameter_strings=*/{},
+      /*embedding_query_vectors=*/{}, EMBEDDING_METRIC_UNKNOWN);
+}
+
 enum class QueryType {
   kPlain,
   kSearch,
@@ -200,27 +243,30 @@ class QueryVisitorTest : public ::testing::TestWithParam<QueryType> {
       std::string_view query) {
     Lexer lexer(query, Lexer::Language::QUERY);
     ICING_ASSIGN_OR_RETURN(std::vector<Lexer::LexerToken> lexer_tokens,
-                           lexer.ExtractTokens());
+                           std::move(lexer).ExtractTokens());
     Parser parser = Parser::Create(std::move(lexer_tokens));
     return parser.ConsumeQuery();
   }
 
   libtextclassifier3::StatusOr<QueryResults> ProcessQuery(
-      const std::string& query) {
-    ICING_ASSIGN_OR_RETURN(std::unique_ptr<Node> root_node,
-                           ParseQueryHelper(query));
+      const SearchSpecProto& search_spec, const Node* root_node) {
     QueryVisitor query_visitor(
         index_.get(), numeric_index_.get(), embedding_index_.get(),
         document_store_.get(), schema_store_.get(), normalizer_.get(),
-        tokenizer_.get(), query, /*embedding_query_vectors=*/nullptr,
-        DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-        EMBEDDING_METRIC_UNKNOWN,
-        /*needs_term_frequency_info_=*/true,
-        clock_.GetSystemTimeMilliseconds());
+        tokenizer_.get(), search_spec, DocHitInfoIteratorFilter::Options(),
+        /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
     root_node->Accept(&query_visitor);
     return std::move(query_visitor).ConsumeResults();
   }
 
+  libtextclassifier3::StatusOr<QueryResults> ProcessQuery(
+      const std::string& query) {
+    ICING_ASSIGN_OR_RETURN(std::unique_ptr<Node> root_node,
+                           ParseQueryHelper(query));
+    SearchSpecProto search_spec = CreateSearchSpec(query, TERM_MATCH_PREFIX);
+    return ProcessQuery(search_spec, root_node.get());
+  }
+
   std::string EscapeString(std::string_view str) {
     std::string result;
     result.reserve(str.size());
@@ -610,15 +656,10 @@ TEST_P(QueryVisitorTest, LessThanTooManyOperandsInvalid) {
   args.push_back(std::move(member_node));
   args.push_back(std::move(extra_value_node));
   auto root_node = std::make_unique<NaryOperatorNode>("<", std::move(args));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, /*embedding_query_vectors=*/nullptr,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_UNKNOWN,
-      /*needs_term_frequency_info_=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
-  EXPECT_THAT(std::move(query_visitor).ConsumeResults(),
+
+  SearchSpecProto search_spec =
+      CreateSearchSpec(std::string(query), TERM_MATCH_PREFIX);
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
@@ -639,15 +680,9 @@ TEST_P(QueryVisitorTest, LessThanTooFewOperandsInvalid) {
   std::vector<std::unique_ptr<Node>> args;
   args.push_back(std::move(member_node));
   auto root_node = std::make_unique<NaryOperatorNode>("<", std::move(args));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, /*embedding_query_vectors=*/nullptr,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_UNKNOWN,
-      /*needs_term_frequency_info_=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
-  EXPECT_THAT(std::move(query_visitor).ConsumeResults(),
+  SearchSpecProto search_spec =
+      CreateSearchSpec(std::string(query), TERM_MATCH_PREFIX);
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
@@ -685,12 +720,11 @@ TEST_P(QueryVisitorTest, LessThanNonExistentPropertyNotFound) {
 }
 
 TEST_P(QueryVisitorTest, NeverVisitedReturnsInvalid) {
+  SearchSpecProto search_spec = CreateSearchSpec("", TERM_MATCH_PREFIX);
   QueryVisitor query_visitor(
       index_.get(), numeric_index_.get(), embedding_index_.get(),
       document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), "", /*embedding_query_vectors=*/nullptr,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_UNKNOWN,
+      tokenizer_.get(), search_spec, DocHitInfoIteratorFilter::Options(),
       /*needs_term_frequency_info_=*/true, clock_.GetSystemTimeMilliseconds());
   EXPECT_THAT(std::move(query_visitor).ConsumeResults(),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
@@ -833,18 +867,11 @@ TEST_P(QueryVisitorTest, SingleTermTermFrequencyEnabled) {
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   std::string query = CreateQuery("foo");
+  SearchSpecProto search_spec = CreateSearchSpec(query, TERM_MATCH_PREFIX);
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, /*embedding_query_vectors=*/nullptr,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_UNKNOWN,
-      /*needs_term_frequency_info_=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
   ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results,
-                             std::move(query_visitor).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_terms), UnorderedElementsAre(""));
   EXPECT_THAT(query_results.query_terms[""], UnorderedElementsAre("foo"));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators),
@@ -889,13 +916,12 @@ TEST_P(QueryVisitorTest, SingleTermTermFrequencyDisabled) {
   std::string query = CreateQuery("foo");
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
+  SearchSpecProto search_spec = CreateSearchSpec(query, TERM_MATCH_PREFIX);
   QueryVisitor query_visitor(
       index_.get(), numeric_index_.get(), embedding_index_.get(),
       document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, /*embedding_query_vectors=*/nullptr,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_UNKNOWN,
-      /*needs_term_frequency_info_=*/false, clock_.GetSystemTimeMilliseconds());
+      tokenizer_.get(), search_spec, DocHitInfoIteratorFilter::Options(),
+      /*needs_term_frequency_info=*/false, clock_.GetSystemTimeMilliseconds());
   root_node->Accept(&query_visitor);
   ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results,
                              std::move(query_visitor).ConsumeResults());
@@ -943,16 +969,9 @@ TEST_P(QueryVisitorTest, SingleTermPrefix) {
   std::string query = CreateQuery("fo");
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, /*embedding_query_vectors=*/nullptr,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_EXACT,
-      EMBEDDING_METRIC_UNKNOWN,
-      /*needs_term_frequency_info_=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
+  SearchSpecProto search_spec = CreateSearchSpec(query, TERM_MATCH_EXACT);
   ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results,
-                             std::move(query_visitor).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_terms), UnorderedElementsAre(""));
   EXPECT_THAT(query_results.query_terms[""], UnorderedElementsAre("fo"));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators),
@@ -961,16 +980,8 @@ TEST_P(QueryVisitorTest, SingleTermPrefix) {
 
   query = CreateQuery("fo*");
   ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
-  QueryVisitor query_visitor_two(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, /*embedding_query_vectors=*/nullptr,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_EXACT,
-      EMBEDDING_METRIC_UNKNOWN,
-      /*needs_term_frequency_info_=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor_two);
   ICING_ASSERT_OK_AND_ASSIGN(query_results,
-                             std::move(query_visitor_two).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_terms), UnorderedElementsAre(""));
   EXPECT_THAT(query_results.query_terms[""], UnorderedElementsAre("fo"));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators),
@@ -1025,16 +1036,9 @@ TEST_P(QueryVisitorTest, SegmentationWithPrefix) {
   std::string query = CreateQuery("ba?fo");
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, /*embedding_query_vectors=*/nullptr,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_EXACT,
-      EMBEDDING_METRIC_UNKNOWN,
-      /*needs_term_frequency_info_=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
+  SearchSpecProto search_spec = CreateSearchSpec(query, TERM_MATCH_EXACT);
   ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results,
-                             std::move(query_visitor).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_terms), UnorderedElementsAre(""));
   EXPECT_THAT(query_results.query_terms[""], UnorderedElementsAre("ba", "fo"));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators),
@@ -1050,16 +1054,8 @@ TEST_P(QueryVisitorTest, SegmentationWithPrefix) {
   // either "bar" or "fo".
   query = CreateQuery("ba?fo*");
   ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
-  QueryVisitor query_visitor_two(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, /*embedding_query_vectors=*/nullptr,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_EXACT,
-      EMBEDDING_METRIC_UNKNOWN,
-      /*needs_term_frequency_info_=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor_two);
   ICING_ASSERT_OK_AND_ASSIGN(query_results,
-                             std::move(query_visitor_two).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_terms), UnorderedElementsAre(""));
   EXPECT_THAT(query_results.query_terms[""], UnorderedElementsAre("ba", "fo"));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators),
@@ -1127,16 +1123,9 @@ TEST_P(QueryVisitorTest, SingleVerbatimTermPrefix) {
   std::string query = CreateQuery("\"foo:bar(\"*");
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, /*embedding_query_vectors=*/nullptr,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_EXACT,
-      EMBEDDING_METRIC_UNKNOWN,
-      /*needs_term_frequency_info_=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
+  SearchSpecProto search_spec = CreateSearchSpec(query, TERM_MATCH_EXACT);
   ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results,
-                             std::move(query_visitor).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(query_results.features_in_use,
               UnorderedElementsAre(kVerbatimSearchFeature,
                                    kListFilterQueryLanguageFeature));
@@ -2517,7 +2506,9 @@ TEST_P(QueryVisitorTest, PropertyRestrictsPopCorrectly) {
   // - Doc 0: Contains 'val0', 'val1', 'val2' in 'prop0'. Shouldn't match.
   DocumentProto doc =
       DocumentBuilder().SetKey("ns", "uri0").SetSchema("type").Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId docid0, document_store_->Put(doc));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
+                             document_store_->Put(doc));
+  DocumentId docid0 = put_result0.new_document_id;
   Index::Editor editor =
       index_->Edit(docid0, prop0_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("val0"));
@@ -2527,7 +2518,9 @@ TEST_P(QueryVisitorTest, PropertyRestrictsPopCorrectly) {
 
   // - Doc 1: Contains 'val0', 'val1', 'val2' in 'prop1'. Should match.
   doc = DocumentBuilder(doc).SetUri("uri1").Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId docid1, document_store_->Put(doc));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
+                             document_store_->Put(doc));
+  DocumentId docid1 = put_result1.new_document_id;
   editor = index_->Edit(docid1, prop1_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("val0"));
   ICING_ASSERT_OK(editor.BufferTerm("val1"));
@@ -2536,7 +2529,9 @@ TEST_P(QueryVisitorTest, PropertyRestrictsPopCorrectly) {
 
   // - Doc 2: Contains 'val0', 'val1', 'val2' in 'prop2'. Shouldn't match.
   doc = DocumentBuilder(doc).SetUri("uri2").Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId docid2, document_store_->Put(doc));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
+                             document_store_->Put(doc));
+  DocumentId docid2 = put_result2.new_document_id;
   editor = index_->Edit(docid2, prop2_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("val0"));
   ICING_ASSERT_OK(editor.BufferTerm("val1"));
@@ -2545,7 +2540,9 @@ TEST_P(QueryVisitorTest, PropertyRestrictsPopCorrectly) {
 
   // - Doc 3: Contains 'val0' in 'prop0', 'val1' in 'prop1' etc. Should match.
   doc = DocumentBuilder(doc).SetUri("uri3").Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId docid3, document_store_->Put(doc));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
+                             document_store_->Put(doc));
+  DocumentId docid3 = put_result3.new_document_id;
   editor = index_->Edit(docid3, prop0_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("val0"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
@@ -2559,7 +2556,9 @@ TEST_P(QueryVisitorTest, PropertyRestrictsPopCorrectly) {
   // - Doc 4: Contains 'val1' in 'prop0', 'val2' in 'prop1', 'val0' in 'prop2'.
   //          Shouldn't match.
   doc = DocumentBuilder(doc).SetUri("uri4").Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId docid4, document_store_->Put(doc));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
+                             document_store_->Put(doc));
+  DocumentId docid4 = put_result4.new_document_id;
   editor = index_->Edit(docid4, prop0_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("val1"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
@@ -2623,7 +2622,9 @@ TEST_P(QueryVisitorTest, UnsatisfiablePropertyRestrictsPopCorrectly) {
   // - Doc 0: Contains 'val0', 'val1', 'val2' in 'prop0'. Should match.
   DocumentProto doc =
       DocumentBuilder().SetKey("ns", "uri0").SetSchema("type").Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId docid0, document_store_->Put(doc));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
+                             document_store_->Put(doc));
+  DocumentId docid0 = put_result0.new_document_id;
   Index::Editor editor =
       index_->Edit(docid0, prop0_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("val0"));
@@ -2633,7 +2634,9 @@ TEST_P(QueryVisitorTest, UnsatisfiablePropertyRestrictsPopCorrectly) {
 
   // - Doc 1: Contains 'val0', 'val1', 'val2' in 'prop1'. Shouldn't match.
   doc = DocumentBuilder(doc).SetUri("uri1").Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId docid1, document_store_->Put(doc));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
+                             document_store_->Put(doc));
+  DocumentId docid1 = put_result1.new_document_id;
   editor = index_->Edit(docid1, prop1_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("val0"));
   ICING_ASSERT_OK(editor.BufferTerm("val1"));
@@ -2642,7 +2645,9 @@ TEST_P(QueryVisitorTest, UnsatisfiablePropertyRestrictsPopCorrectly) {
 
   // - Doc 2: Contains 'val0', 'val1', 'val2' in 'prop2'. Should match.
   doc = DocumentBuilder(doc).SetUri("uri2").Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId docid2, document_store_->Put(doc));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
+                             document_store_->Put(doc));
+  DocumentId docid2 = put_result2.new_document_id;
   editor = index_->Edit(docid2, prop2_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("val0"));
   ICING_ASSERT_OK(editor.BufferTerm("val1"));
@@ -2651,7 +2656,9 @@ TEST_P(QueryVisitorTest, UnsatisfiablePropertyRestrictsPopCorrectly) {
 
   // - Doc 3: Contains 'val0' in 'prop0', 'val1' in 'prop1' etc. Should match.
   doc = DocumentBuilder(doc).SetUri("uri3").Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId docid3, document_store_->Put(doc));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
+                             document_store_->Put(doc));
+  DocumentId docid3 = put_result3.new_document_id;
   editor = index_->Edit(docid3, prop0_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("val0"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
@@ -2665,7 +2672,9 @@ TEST_P(QueryVisitorTest, UnsatisfiablePropertyRestrictsPopCorrectly) {
   // - Doc 4: Contains 'val1' in 'prop0', 'val2' in 'prop1', 'val0' in 'prop2'.
   //          Shouldn't match.
   doc = DocumentBuilder(doc).SetUri("uri4").Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId docid4, document_store_->Put(doc));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
+                             document_store_->Put(doc));
+  DocumentId docid4 = put_result4.new_document_id;
   editor = index_->Edit(docid4, prop0_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("val1"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
@@ -2881,57 +2890,66 @@ TEST_F(QueryVisitorTest, SearchFunctionNestedPropertyRestrictsNarrowing) {
   NamespaceId ns_id = 0;
   DocumentProto doc =
       DocumentBuilder().SetKey("ns", "uri0").SetSchema("type").Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId docid0, document_store_->Put(doc));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
+                             document_store_->Put(doc));
+  DocumentId docid0 = put_result0.new_document_id;
   Index::Editor editor =
       index_->Edit(kDocumentId0, prop0_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri1").Build()));
+  DocumentId docid1 = put_result1.new_document_id;
   editor = index_->Edit(docid1, prop1_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri2").Build()));
+  DocumentId docid2 = put_result2.new_document_id;
   editor = index_->Edit(docid2, prop2_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid3,
+      DocumentStore::PutResult put_result3,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri3").Build()));
+  DocumentId docid3 = put_result3.new_document_id;
   editor = index_->Edit(docid3, prop3_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid4,
+      DocumentStore::PutResult put_result4,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri4").Build()));
+  DocumentId docid4 = put_result4.new_document_id;
   editor = index_->Edit(docid4, prop4_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid5,
+      DocumentStore::PutResult put_result5,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri5").Build()));
+  DocumentId docid5 = put_result5.new_document_id;
   editor = index_->Edit(docid5, prop5_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid6,
+      DocumentStore::PutResult put_result6,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri6").Build()));
+  DocumentId docid6 = put_result6.new_document_id;
   editor = index_->Edit(docid6, prop6_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid7,
+      DocumentStore::PutResult put_result7,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri7").Build()));
+  DocumentId docid7 = put_result7.new_document_id;
   editor = index_->Edit(docid7, prop7_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
@@ -3034,57 +3052,66 @@ TEST_F(QueryVisitorTest, SearchFunctionNestedPropertyRestrictsExpanding) {
   NamespaceId ns_id = 0;
   DocumentProto doc =
       DocumentBuilder().SetKey("ns", "uri0").SetSchema("type").Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId docid0, document_store_->Put(doc));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
+                             document_store_->Put(doc));
+  DocumentId docid0 = put_result0.new_document_id;
   Index::Editor editor =
       index_->Edit(kDocumentId0, prop0_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri1").Build()));
+  DocumentId docid1 = put_result1.new_document_id;
   editor = index_->Edit(docid1, prop1_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri2").Build()));
+  DocumentId docid2 = put_result2.new_document_id;
   editor = index_->Edit(docid2, prop2_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid3,
+      DocumentStore::PutResult put_result3,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri3").Build()));
+  DocumentId docid3 = put_result3.new_document_id;
   editor = index_->Edit(docid3, prop3_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid4,
+      DocumentStore::PutResult put_result4,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri4").Build()));
+  DocumentId docid4 = put_result4.new_document_id;
   editor = index_->Edit(docid4, prop4_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid5,
+      DocumentStore::PutResult put_result5,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri5").Build()));
+  DocumentId docid5 = put_result5.new_document_id;
   editor = index_->Edit(docid5, prop5_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid6,
+      DocumentStore::PutResult put_result6,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri6").Build()));
+  DocumentId docid6 = put_result6.new_document_id;
   editor = index_->Edit(docid6, prop6_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid7,
+      DocumentStore::PutResult put_result7,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri7").Build()));
+  DocumentId docid7 = put_result7.new_document_id;
   editor = index_->Edit(docid7, prop7_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
@@ -3139,7 +3166,7 @@ TEST_F(QueryVisitorTest, SearchFunctionNestedPropertyRestrictsExpanding) {
               ElementsAre(docid6, docid0));
 }
 
-TEST_P(QueryVisitorTest, TokenizeFunctionHandlesPunctuation) {
+TEST_P(QueryVisitorTest, QueryStringParameterHandlesPunctuation) {
   PropertyConfigProto prop =
       PropertyConfigBuilder()
           .SetName("prop0")
@@ -3159,7 +3186,9 @@ TEST_P(QueryVisitorTest, TokenizeFunctionHandlesPunctuation) {
   NamespaceId ns_id = 0;
   DocumentProto doc =
       DocumentBuilder().SetKey("ns", "uri0").SetSchema("type").Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId docid0, document_store_->Put(doc));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
+                             document_store_->Put(doc));
+  DocumentId docid0 = put_result0.new_document_id;
   Index::Editor editor =
       index_->Edit(kDocumentId0, prop0_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
@@ -3167,24 +3196,30 @@ TEST_P(QueryVisitorTest, TokenizeFunctionHandlesPunctuation) {
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri1").Build()));
+  DocumentId docid1 = put_result1.new_document_id;
   editor = index_->Edit(docid1, prop0_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("bar"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri2").Build()));
+  DocumentId docid2 = put_result2.new_document_id;
   editor = index_->Edit(docid2, prop0_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
-  std::string query = R"(tokenize("foo."))";
-  ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results, ProcessQuery(query));
-  EXPECT_THAT(
-      query_results.features_in_use,
-      UnorderedElementsAre(kListFilterQueryLanguageFeature, kTokenizeFeature));
+  std::string query = "getSearchStringParameter(0)";
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, {"foo."});
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
+                             ParseQueryHelper(query));
+  ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results,
+                             ProcessQuery(search_spec, root_node.get()));
+  EXPECT_THAT(query_results.features_in_use,
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(ExtractKeys(query_results.query_terms), UnorderedElementsAre(""));
   EXPECT_THAT(query_results.query_terms[""], UnorderedElementsAre("foo"));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators),
@@ -3192,11 +3227,11 @@ TEST_P(QueryVisitorTest, TokenizeFunctionHandlesPunctuation) {
   EXPECT_THAT(GetDocumentIds(query_results.root_iterator.get()),
               ElementsAre(docid2, docid0));
 
-  query = R"(tokenize("bar, foo"))";
-  ICING_ASSERT_OK_AND_ASSIGN(query_results, ProcessQuery(query));
-  EXPECT_THAT(
-      query_results.features_in_use,
-      UnorderedElementsAre(kListFilterQueryLanguageFeature, kTokenizeFeature));
+  search_spec = CreateSearchSpec(query, TERM_MATCH_PREFIX, {"bar, foo"});
+  ICING_ASSERT_OK_AND_ASSIGN(query_results,
+                             ProcessQuery(search_spec, root_node.get()));
+  EXPECT_THAT(query_results.features_in_use,
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(ExtractKeys(query_results.query_terms), UnorderedElementsAre(""));
   EXPECT_THAT(query_results.query_terms[""],
               UnorderedElementsAre("foo", "bar"));
@@ -3205,11 +3240,11 @@ TEST_P(QueryVisitorTest, TokenizeFunctionHandlesPunctuation) {
   EXPECT_THAT(GetDocumentIds(query_results.root_iterator.get()),
               ElementsAre(docid0));
 
-  query = R"(tokenize("\"bar, \"foo\""))";
-  ICING_ASSERT_OK_AND_ASSIGN(query_results, ProcessQuery(query));
-  EXPECT_THAT(
-      query_results.features_in_use,
-      UnorderedElementsAre(kListFilterQueryLanguageFeature, kTokenizeFeature));
+  search_spec = CreateSearchSpec(query, TERM_MATCH_PREFIX, {"\"bar, \"foo\""});
+  ICING_ASSERT_OK_AND_ASSIGN(query_results,
+                             ProcessQuery(search_spec, root_node.get()));
+  EXPECT_THAT(query_results.features_in_use,
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(ExtractKeys(query_results.query_terms), UnorderedElementsAre(""));
   EXPECT_THAT(query_results.query_terms[""],
               UnorderedElementsAre("foo", "bar"));
@@ -3218,11 +3253,11 @@ TEST_P(QueryVisitorTest, TokenizeFunctionHandlesPunctuation) {
   EXPECT_THAT(GetDocumentIds(query_results.root_iterator.get()),
               ElementsAre(docid0));
 
-  query = R"(tokenize("bar foo( "))";
-  ICING_ASSERT_OK_AND_ASSIGN(query_results, ProcessQuery(query));
-  EXPECT_THAT(
-      query_results.features_in_use,
-      UnorderedElementsAre(kListFilterQueryLanguageFeature, kTokenizeFeature));
+  search_spec = CreateSearchSpec(query, TERM_MATCH_PREFIX, {"bar foo( "});
+  ICING_ASSERT_OK_AND_ASSIGN(query_results,
+                             ProcessQuery(search_spec, root_node.get()));
+  EXPECT_THAT(query_results.features_in_use,
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(ExtractKeys(query_results.query_terms), UnorderedElementsAre(""));
   EXPECT_THAT(query_results.query_terms[""],
               UnorderedElementsAre("foo", "bar"));
@@ -3231,11 +3266,11 @@ TEST_P(QueryVisitorTest, TokenizeFunctionHandlesPunctuation) {
   EXPECT_THAT(GetDocumentIds(query_results.root_iterator.get()),
               ElementsAre(docid0));
 
-  query = R"(tokenize("bar ) foo"))";
-  ICING_ASSERT_OK_AND_ASSIGN(query_results, ProcessQuery(query));
-  EXPECT_THAT(
-      query_results.features_in_use,
-      UnorderedElementsAre(kListFilterQueryLanguageFeature, kTokenizeFeature));
+  search_spec = CreateSearchSpec(query, TERM_MATCH_PREFIX, {"bar ) foo"});
+  ICING_ASSERT_OK_AND_ASSIGN(query_results,
+                             ProcessQuery(search_spec, root_node.get()));
+  EXPECT_THAT(query_results.features_in_use,
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(ExtractKeys(query_results.query_terms), UnorderedElementsAre(""));
   EXPECT_THAT(query_results.query_terms[""],
               UnorderedElementsAre("foo", "bar"));
@@ -3245,7 +3280,7 @@ TEST_P(QueryVisitorTest, TokenizeFunctionHandlesPunctuation) {
               ElementsAre(docid0));
 }
 
-TEST_P(QueryVisitorTest, TokenizeFunctionPropertyRestricts) {
+TEST_P(QueryVisitorTest, QueryStringParameterPropertyRestricts) {
   PropertyConfigProto prop =
       PropertyConfigBuilder()
           .SetName("prop0")
@@ -3272,7 +3307,9 @@ TEST_P(QueryVisitorTest, TokenizeFunctionPropertyRestricts) {
   NamespaceId ns_id = 0;
   DocumentProto doc =
       DocumentBuilder().SetKey("ns", "uri0").SetSchema("type").Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId docid0, document_store_->Put(doc));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
+                             document_store_->Put(doc));
+  DocumentId docid0 = put_result0.new_document_id;
   Index::Editor editor =
       index_->Edit(docid0, prop0_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
@@ -3280,8 +3317,9 @@ TEST_P(QueryVisitorTest, TokenizeFunctionPropertyRestricts) {
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri1").Build()));
+  DocumentId docid1 = put_result1.new_document_id;
   editor = index_->Edit(docid1, prop0_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("bar"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
@@ -3290,8 +3328,9 @@ TEST_P(QueryVisitorTest, TokenizeFunctionPropertyRestricts) {
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId docid2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(DocumentBuilder(doc).SetUri("uri2").Build()));
+  DocumentId docid2 = put_result2.new_document_id;
   editor = index_->Edit(docid2, prop0_id, TERM_MATCH_PREFIX, ns_id);
   ICING_ASSERT_OK(editor.BufferTerm("bar"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
@@ -3299,11 +3338,15 @@ TEST_P(QueryVisitorTest, TokenizeFunctionPropertyRestricts) {
   ICING_ASSERT_OK(editor.BufferTerm("foo"));
   ICING_ASSERT_OK(editor.IndexAllBufferedTerms());
 
-  std::string query = R"(prop0:tokenize("bar, foo"))";
-  ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results, ProcessQuery(query));
-  EXPECT_THAT(
-      query_results.features_in_use,
-      UnorderedElementsAre(kListFilterQueryLanguageFeature, kTokenizeFeature));
+  std::string query = "prop0:getSearchStringParameter(0)";
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
+                             ParseQueryHelper(query));
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, {"bar, foo"});
+  ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results,
+                             ProcessQuery(search_spec, root_node.get()));
+  EXPECT_THAT(query_results.features_in_use,
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(ExtractKeys(query_results.query_terms),
               UnorderedElementsAre("prop0"));
   EXPECT_THAT(query_results.query_terms["prop0"],
@@ -3313,14 +3356,16 @@ TEST_P(QueryVisitorTest, TokenizeFunctionPropertyRestricts) {
   EXPECT_THAT(GetDocumentIds(query_results.root_iterator.get()),
               ElementsAre(docid0));
 
-  std::string level_one_query = R"(tokenize("bar, foo"))";
+  std::string level_one_query = "getSearchStringParameter(0)";
   std::string level_two_query =
       absl_ports::StrCat(R"(search(")", EscapeString(level_one_query),
                          R"(", createList("prop0", "prop1")))");
-  ICING_ASSERT_OK_AND_ASSIGN(query_results, ProcessQuery(level_two_query));
-  EXPECT_THAT(
-      query_results.features_in_use,
-      UnorderedElementsAre(kListFilterQueryLanguageFeature, kTokenizeFeature));
+  ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(level_two_query));
+  search_spec = CreateSearchSpec(query, TERM_MATCH_PREFIX, {"bar, foo"});
+  ICING_ASSERT_OK_AND_ASSIGN(query_results,
+                             ProcessQuery(search_spec, root_node.get()));
+  EXPECT_THAT(query_results.features_in_use,
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(ExtractKeys(query_results.query_terms),
               UnorderedElementsAre("prop0", "prop1"));
   EXPECT_THAT(query_results.query_terms["prop0"],
@@ -3333,30 +3378,80 @@ TEST_P(QueryVisitorTest, TokenizeFunctionPropertyRestricts) {
               ElementsAre(docid1, docid0));
 }
 
-TEST_P(QueryVisitorTest, TokenizeFunctionNoArgsReturnsInvalidArgument) {
-  std::string query = R"(tokenize())";
-  EXPECT_THAT(ProcessQuery(query),
+TEST_P(QueryVisitorTest, QueryStringParameterNoParamsReturnsOutOfRange) {
+  std::string query = "getSearchStringParameter(0)";
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
+                             ParseQueryHelper(query));
+  // Create a spec without any parameter strings.
+  SearchSpecProto search_spec = CreateSearchSpec(query, TERM_MATCH_PREFIX);
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
+              StatusIs(libtextclassifier3::StatusCode::OUT_OF_RANGE));
+}
+
+TEST_P(QueryVisitorTest, QueryStringParameterNegativeIndexReturnsOutOfRange) {
+  std::string query = "getSearchStringParameter(-1)";
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
+                             ParseQueryHelper(query));
+  // Create a spec without any parameter strings.
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, {"bar, foo"});
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
+              StatusIs(libtextclassifier3::StatusCode::OUT_OF_RANGE));
+}
+
+TEST_P(QueryVisitorTest, QueryStringParameterTooLargeIndexReturnsOutOfRange) {
+  std::string query = "getSearchStringParameter(2)";
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
+                             ParseQueryHelper(query));
+  // Create a spec without any parameter strings.
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, {"bar, foo"});
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
+              StatusIs(libtextclassifier3::StatusCode::OUT_OF_RANGE));
+}
+
+TEST_P(QueryVisitorTest, QueryStringParameterNoArgsReturnsInvalidArgument) {
+  std::string query = "getSearchStringParameter()";
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
+                             ParseQueryHelper(query));
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, {"bar, foo"});
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
-TEST_P(QueryVisitorTest, TokenizeFunctionTooManyArgsReturnsInvalidArgument) {
-  std::string query = R"(tokenize("foo", createList("subject"), "bar"))";
-  EXPECT_THAT(ProcessQuery(query),
+TEST_P(QueryVisitorTest,
+       QueryStringParameterTooManyArgsReturnsInvalidArgument) {
+  std::string query =
+      R"(getSearchStringParameter(0, createList("subject"), "bar"))";
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
+                             ParseQueryHelper(query));
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, {"bar, foo"});
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 
-  query = R"(tokenize("foo", createList("subject"), "bar", 8))";
-  EXPECT_THAT(ProcessQuery(query),
+  query = R"(getSearchStringParameter(0, createList("subject"), "bar", 8))";
+  ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
+  search_spec = CreateSearchSpec(query, TERM_MATCH_PREFIX, {"bar, foo"});
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
 TEST_P(QueryVisitorTest,
-       TokenizeFunctionFirstArgNotStringReturnsInvalidArgument) {
-  std::string query = R"(tokenize(7))";
-  EXPECT_THAT(ProcessQuery(query),
+       QueryStringParameterFirstArgNotStringReturnsInvalidArgument) {
+  std::string query = R"(getSearchStringParameter("bar"))";
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
+                             ParseQueryHelper(query));
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, {"bar, foo"});
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 
-  query = R"(tokenize(createList("foo")))";
-  EXPECT_THAT(ProcessQuery(query),
+  query = R"(getSearchStringParameter(createList("foo")))";
+  ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
+  search_spec = CreateSearchSpec(query, TERM_MATCH_PREFIX, {"bar, foo"});
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
@@ -3666,203 +3761,176 @@ TEST_P(QueryVisitorTest,
 TEST_F(QueryVisitorTest,
        SemanticSearchFunctionWithNoArgumentReturnsInvalidArgument) {
   // Create two embedding queries.
-  google::protobuf::RepeatedPtrField<PropertyProto::VectorProto> embedding_query_vectors;
-  *embedding_query_vectors.Add() = CreateVector("my_model1", {0.1, 0.2, 0.3});
-  *embedding_query_vectors.Add() = CreateVector("my_model2", {-1, 2, -3, 4});
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      CreateVector("my_model1", {0.1, 0.2, 0.3}),
+      CreateVector("my_model2", {-1, 2, -3, 4})};
 
   std::string query = "semanticSearch()";
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
-  EXPECT_THAT(std::move(query_visitor).ConsumeResults(),
+  SearchSpecProto search_spec = CreateSearchSpec(
+      query, TERM_MATCH_PREFIX, std::move(embedding_query_vectors),
+      EMBEDDING_METRIC_COSINE);
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
 TEST_F(QueryVisitorTest,
        SemanticSearchFunctionWithIncorrectArgumentTypeReturnsInvalidArgument) {
   // Create two embedding queries.
-  google::protobuf::RepeatedPtrField<PropertyProto::VectorProto> embedding_query_vectors;
-  *embedding_query_vectors.Add() = CreateVector("my_model1", {0.1, 0.2, 0.3});
-  *embedding_query_vectors.Add() = CreateVector("my_model2", {-1, 2, -3, 4});
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      CreateVector("my_model1", {0.1, 0.2, 0.3}),
+      CreateVector("my_model2", {-1, 2, -3, 4})};
 
   std::string query = "semanticSearch(0)";
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
-  EXPECT_THAT(std::move(query_visitor).ConsumeResults(),
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_COSINE);
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
 TEST_F(QueryVisitorTest,
        SemanticSearchFunctionWithExtraArgumentReturnsInvalidArgument) {
   // Create two embedding queries.
-  google::protobuf::RepeatedPtrField<PropertyProto::VectorProto> embedding_query_vectors;
-  *embedding_query_vectors.Add() = CreateVector("my_model1", {0.1, 0.2, 0.3});
-  *embedding_query_vectors.Add() = CreateVector("my_model2", {-1, 2, -3, 4});
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      CreateVector("my_model1", {0.1, 0.2, 0.3}),
+      CreateVector("my_model2", {-1, 2, -3, 4})};
 
   std::string query =
-      "semanticSearch(getSearchSpecEmbedding(0), 0.5, 1, \"COSINE\", 0)";
+      "semanticSearch(getEmbeddingParameter(0), 0.5, 1, \"COSINE\", 0)";
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
-  EXPECT_THAT(std::move(query_visitor).ConsumeResults(),
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_COSINE);
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
 TEST_F(QueryVisitorTest,
-       GetSearchSpecEmbeddingFunctionWithExtraArgumentReturnsInvalidArgument) {
+       GetEmbeddingParameterFunctionWithExtraArgumentReturnsInvalidArgument) {
   // Create two embedding queries.
-  google::protobuf::RepeatedPtrField<PropertyProto::VectorProto> embedding_query_vectors;
-  *embedding_query_vectors.Add() = CreateVector("my_model1", {0.1, 0.2, 0.3});
-  *embedding_query_vectors.Add() = CreateVector("my_model2", {-1, 2, -3, 4});
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      CreateVector("my_model1", {0.1, 0.2, 0.3}),
+      CreateVector("my_model2", {-1, 2, -3, 4})};
 
   // The embedding query index is invalid, since there are only 2 queries.
-  std::string query = "semanticSearch(getSearchSpecEmbedding(0, 1))";
+  std::string query = "semanticSearch(getEmbeddingParameter(0, 1))";
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
-  EXPECT_THAT(std::move(query_visitor).ConsumeResults(),
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_COSINE);
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
 TEST_F(QueryVisitorTest,
-       SemanticSearchFunctionWithInvalidIndexReturnsInvalidArgument) {
+       SemanticSearchFunctionWithNoVectorParamsIndexReturnsOutOfRange) {
+  // The embedding query index is invalid, since there are no query embeddings.
+  std::string query = "semanticSearch(getEmbeddingParameter(0))";
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
+                             ParseQueryHelper(query));
+  SearchSpecProto search_spec = CreateSearchSpec(query, TERM_MATCH_PREFIX);
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
+              StatusIs(libtextclassifier3::StatusCode::OUT_OF_RANGE));
+}
+
+TEST_F(QueryVisitorTest,
+       SemanticSearchFunctionWithNegativeIndexReturnsOutOfRange) {
   // Create two embedding queries.
-  google::protobuf::RepeatedPtrField<PropertyProto::VectorProto> embedding_query_vectors;
-  *embedding_query_vectors.Add() = CreateVector("my_model1", {0.1, 0.2, 0.3});
-  *embedding_query_vectors.Add() = CreateVector("my_model2", {-1, 2, -3, 4});
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      CreateVector("my_model1", {0.1, 0.2, 0.3}),
+      CreateVector("my_model2", {-1, 2, -3, 4})};
 
   // The embedding query index is invalid, since there are only 2 queries.
-  std::string query = "semanticSearch(getSearchSpecEmbedding(10))";
+  std::string query = "semanticSearch(getEmbeddingParameter(-1))";
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
-  EXPECT_THAT(std::move(query_visitor).ConsumeResults(),
-              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_COSINE);
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
+              StatusIs(libtextclassifier3::StatusCode::OUT_OF_RANGE));
+}
+
+TEST_F(QueryVisitorTest,
+       SemanticSearchFunctionWithTooHighIndexReturnsOutOfRange) {
+  // Create two embedding queries.
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      CreateVector("my_model1", {0.1, 0.2, 0.3}),
+      CreateVector("my_model2", {-1, 2, -3, 4})};
+
+  // The embedding query index is invalid, since there are only 2 queries.
+  std::string query = "semanticSearch(getEmbeddingParameter(10))";
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
+                             ParseQueryHelper(query));
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_COSINE);
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
+              StatusIs(libtextclassifier3::StatusCode::OUT_OF_RANGE));
 }
 
 TEST_F(QueryVisitorTest,
        SemanticSearchFunctionWithInvalidMetricReturnsInvalidArgument) {
   // Create two embedding queries.
-  google::protobuf::RepeatedPtrField<PropertyProto::VectorProto> embedding_query_vectors;
-  *embedding_query_vectors.Add() = CreateVector("my_model1", {0.1, 0.2, 0.3});
-  *embedding_query_vectors.Add() = CreateVector("my_model2", {-1, 2, -3, 4});
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      CreateVector("my_model1", {0.1, 0.2, 0.3}),
+      CreateVector("my_model2", {-1, 2, -3, 4})};
 
   // The embedding query metric is invalid.
   std::string query =
-      "semanticSearch(getSearchSpecEmbedding(0), -10, 10, \"UNKNOWN\")";
+      "semanticSearch(getEmbeddingParameter(0), -10, 10, \"UNKNOWN\")";
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
-  EXPECT_THAT(std::move(query_visitor).ConsumeResults(),
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_COSINE);
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 
   // Passing an unknown default metric type without overriding it in the query
   // expression is also considered invalid.
-  query = "semanticSearch(getSearchSpecEmbedding(0), -10, 10)";
+  query = "semanticSearch(getEmbeddingParameter(0), -10, 10)";
   ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
-  QueryVisitor query_visitor2(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_UNKNOWN,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor2);
-  EXPECT_THAT(std::move(query_visitor2).ConsumeResults(),
+  search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_UNKNOWN);
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
 TEST_F(QueryVisitorTest,
        SemanticSearchFunctionWithInvalidRangeReturnsInvalidArgument) {
   // Create two embedding queries.
-  google::protobuf::RepeatedPtrField<PropertyProto::VectorProto> embedding_query_vectors;
-  *embedding_query_vectors.Add() = CreateVector("my_model1", {0.1, 0.2, 0.3});
-  *embedding_query_vectors.Add() = CreateVector("my_model2", {-1, 2, -3, 4});
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      CreateVector("my_model1", {0.1, 0.2, 0.3}),
+      CreateVector("my_model2", {-1, 2, -3, 4})};
 
   // The expression is invalid, since low > high.
-  std::string query = "semanticSearch(getSearchSpecEmbedding(0), 10, -10)";
+  std::string query = "semanticSearch(getEmbeddingParameter(0), 10, -10)";
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
-  EXPECT_THAT(std::move(query_visitor).ConsumeResults(),
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_COSINE);
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 
   // Floating point values are also checked.
-  query = "semanticSearch(getSearchSpecEmbedding(0), 10.2, 10.1)";
+  query = "semanticSearch(getEmbeddingParameter(0), 10.2, 10.1)";
   ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
-  QueryVisitor query_visitor2(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor2);
-  EXPECT_THAT(std::move(query_visitor2).ConsumeResults(),
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 
   // low == high is allowed.
-  query = "semanticSearch(getSearchSpecEmbedding(0), 10.1, 10.1)";
+  query = "semanticSearch(getEmbeddingParameter(0), 10.1, 10.1)";
   ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
-  QueryVisitor query_visitor3(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor3);
-  EXPECT_THAT(std::move(query_visitor3).ConsumeResults(), IsOk());
+  EXPECT_THAT(ProcessQuery(search_spec, root_node.get()), IsOk());
 }
 
 TEST_F(QueryVisitorTest, SemanticSearchFunctionSimpleLowerBound) {
@@ -3879,28 +3947,22 @@ TEST_F(QueryVisitorTest, SemanticSearchFunctionSimpleLowerBound) {
 
   // Create an embedding query that has a semantic score of 1 with vector0 and
   // -1 with vector1.
-  google::protobuf::RepeatedPtrField<PropertyProto::VectorProto> embedding_query_vectors;
-  *embedding_query_vectors.Add() = CreateVector("my_model", {0.1, 0.2, 0.3});
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      CreateVector("my_model", {0.1, 0.2, 0.3})};
 
   // The query should match vector0 only.
-  std::string query = "semanticSearch(getSearchSpecEmbedding(0), 0.5)";
+  std::string query = "semanticSearch(getEmbeddingParameter(0), 0.5)";
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_COSINE);
   ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results,
-                             std::move(query_visitor).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators), IsEmpty());
   EXPECT_THAT(query_results.query_terms, IsEmpty());
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(GetDocumentIds(query_results.root_iterator.get()),
               ElementsAre(kDocumentId0));
   EXPECT_THAT(
@@ -3909,23 +3971,14 @@ TEST_F(QueryVisitorTest, SemanticSearchFunctionSimpleLowerBound) {
       Pointee(UnorderedElementsAre(DoubleNear(1, kEps))));
 
   // The query should match both vector0 and vector1.
-  query = "semanticSearch(getSearchSpecEmbedding(0), -1.5)";
+  query = "semanticSearch(getEmbeddingParameter(0), -1.5)";
   ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
-  QueryVisitor query_visitor2(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor2);
   ICING_ASSERT_OK_AND_ASSIGN(query_results,
-                             std::move(query_visitor2).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators), IsEmpty());
   EXPECT_THAT(query_results.query_terms, IsEmpty());
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(GetDocumentIds(query_results.root_iterator.get()),
               ElementsAre(kDocumentId1, kDocumentId0));
   EXPECT_THAT(
@@ -3939,23 +3992,14 @@ TEST_F(QueryVisitorTest, SemanticSearchFunctionSimpleLowerBound) {
 
   // The query should match nothing, since there is no vector with a
   // score >= 1.01.
-  query = "semanticSearch(getSearchSpecEmbedding(0), 1.01)";
+  query = "semanticSearch(getEmbeddingParameter(0), 1.01)";
   ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
-  QueryVisitor query_visitor3(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor3);
   ICING_ASSERT_OK_AND_ASSIGN(query_results,
-                             std::move(query_visitor3).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators), IsEmpty());
   EXPECT_THAT(query_results.query_terms, IsEmpty());
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(GetDocumentIds(query_results.root_iterator.get()), IsEmpty());
 }
 
@@ -3973,28 +4017,22 @@ TEST_F(QueryVisitorTest, SemanticSearchFunctionSimpleUpperBound) {
 
   // Create an embedding query that has a semantic score of 1 with vector0 and
   // -1 with vector1.
-  google::protobuf::RepeatedPtrField<PropertyProto::VectorProto> embedding_query_vectors;
-  *embedding_query_vectors.Add() = CreateVector("my_model", {0.1, 0.2, 0.3});
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      CreateVector("my_model", {0.1, 0.2, 0.3})};
 
   // The query should match vector1 only.
-  std::string query = "semanticSearch(getSearchSpecEmbedding(0), -100, 0.5)";
+  std::string query = "semanticSearch(getEmbeddingParameter(0), -100, 0.5)";
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_COSINE);
   ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results,
-                             std::move(query_visitor).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators), IsEmpty());
   EXPECT_THAT(query_results.query_terms, IsEmpty());
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(GetDocumentIds(query_results.root_iterator.get()),
               ElementsAre(kDocumentId1));
   EXPECT_THAT(
@@ -4003,23 +4041,14 @@ TEST_F(QueryVisitorTest, SemanticSearchFunctionSimpleUpperBound) {
       Pointee(UnorderedElementsAre(DoubleNear(-1, kEps))));
 
   // The query should match both vector0 and vector1.
-  query = "semanticSearch(getSearchSpecEmbedding(0), -100, 1.5)";
+  query = "semanticSearch(getEmbeddingParameter(0), -100, 1.5)";
   ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
-  QueryVisitor query_visitor2(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor2);
   ICING_ASSERT_OK_AND_ASSIGN(query_results,
-                             std::move(query_visitor2).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators), IsEmpty());
   EXPECT_THAT(query_results.query_terms, IsEmpty());
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(GetDocumentIds(query_results.root_iterator.get()),
               ElementsAre(kDocumentId1, kDocumentId0));
   EXPECT_THAT(
@@ -4033,23 +4062,14 @@ TEST_F(QueryVisitorTest, SemanticSearchFunctionSimpleUpperBound) {
 
   // The query should match nothing, since there is no vector with a
   // score <= -1.01.
-  query = "semanticSearch(getSearchSpecEmbedding(0), -100, -1.01)";
+  query = "semanticSearch(getEmbeddingParameter(0), -100, -1.01)";
   ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
-  QueryVisitor query_visitor3(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor3);
   ICING_ASSERT_OK_AND_ASSIGN(query_results,
-                             std::move(query_visitor3).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators), IsEmpty());
   EXPECT_THAT(query_results.query_terms, IsEmpty());
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(GetDocumentIds(query_results.root_iterator.get()), IsEmpty());
 }
 
@@ -4064,30 +4084,24 @@ TEST_F(QueryVisitorTest, SemanticSearchFunctionMetricOverride) {
   // - a cosine semantic score of 1
   // - a dot product semantic score of 0.14
   // - a euclidean semantic score of 0
-  google::protobuf::RepeatedPtrField<PropertyProto::VectorProto> embedding_query_vectors;
-  *embedding_query_vectors.Add() = CreateVector("my_model", {0.1, 0.2, 0.3});
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      CreateVector("my_model", {0.1, 0.2, 0.3})};
 
   // Create a query that overrides the metric to COSINE.
   std::string query =
-      "semanticSearch(getSearchSpecEmbedding(0), 0.95, 1.05, \"COSINE\")";
+      "semanticSearch(getEmbeddingParameter(0), 0.95, 1.05, \"COSINE\")";
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      // The default metric to be overridden
-      EMBEDDING_METRIC_DOT_PRODUCT,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
+  // The default metric to be overridden
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_DOT_PRODUCT);
   ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results,
-                             std::move(query_visitor).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators), IsEmpty());
   EXPECT_THAT(query_results.query_terms, IsEmpty());
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(GetDocumentIds(query_results.root_iterator.get()),
               ElementsAre(kDocumentId0));
   EXPECT_THAT(
@@ -4096,25 +4110,14 @@ TEST_F(QueryVisitorTest, SemanticSearchFunctionMetricOverride) {
       Pointee(UnorderedElementsAre(DoubleNear(1, kEps))));
 
   // Create a query that overrides the metric to DOT_PRODUCT.
-  query =
-      "semanticSearch(getSearchSpecEmbedding(0), 0.1, 0.2, \"DOT_PRODUCT\")";
+  query = "semanticSearch(getEmbeddingParameter(0), 0.1, 0.2, \"DOT_PRODUCT\")";
   ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
-  QueryVisitor query_visitor2(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      // The default metric to be overridden
-      EMBEDDING_METRIC_COSINE,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor2);
   ICING_ASSERT_OK_AND_ASSIGN(query_results,
-                             std::move(query_visitor2).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators), IsEmpty());
   EXPECT_THAT(query_results.query_terms, IsEmpty());
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(GetDocumentIds(query_results.root_iterator.get()),
               ElementsAre(kDocumentId0));
   EXPECT_THAT(
@@ -4124,24 +4127,14 @@ TEST_F(QueryVisitorTest, SemanticSearchFunctionMetricOverride) {
 
   // Create a query that overrides the metric to EUCLIDEAN.
   query =
-      "semanticSearch(getSearchSpecEmbedding(0), -0.05, 0.05, \"EUCLIDEAN\")";
+      "semanticSearch(getEmbeddingParameter(0), -0.05, 0.05, \"EUCLIDEAN\")";
   ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
-  QueryVisitor query_visitor3(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      // The default metric to be overridden
-      EMBEDDING_METRIC_UNKNOWN,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor3);
   ICING_ASSERT_OK_AND_ASSIGN(query_results,
-                             std::move(query_visitor3).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators), IsEmpty());
   EXPECT_THAT(query_results.query_terms, IsEmpty());
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   EXPECT_THAT(GetDocumentIds(query_results.root_iterator.get()),
               ElementsAre(kDocumentId0));
   EXPECT_THAT(
@@ -4171,43 +4164,35 @@ TEST_F(QueryVisitorTest, SemanticSearchFunctionMultipleQueries) {
   ICING_ASSERT_OK(embedding_index_->CommitBufferToIndex());
 
   // Create two embedding queries.
-  google::protobuf::RepeatedPtrField<PropertyProto::VectorProto> embedding_query_vectors;
-  // Semantic scores for this query:
-  // - document 0: -2 (section 0), 0 (section 1)
-  // - document 1: 6 (section 0)
-  PropertyProto::VectorProto* embedding_query = embedding_query_vectors.Add();
-  *embedding_query = CreateVector("my_model1", {-1, -1, 1});
-  // Semantic scores for this query:
-  // - document 0: 4 (section 2)
-  // - document 1: -2 (section 1)
-  embedding_query = embedding_query_vectors.Add();
-  *embedding_query = CreateVector("my_model2", {-1, 1, -1, -1});
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      // Semantic scores for this query:
+      // - document 0: -2 (section 0), 0 (section 1)
+      // - document 1: 6 (section 0)
+      CreateVector("my_model1", {-1, -1, 1}),
+      // Semantic scores for this query:
+      // - document 0: 4 (section 2)
+      // - document 1: -2 (section 1)
+      CreateVector("my_model2", {-1, 1, -1, -1})};
 
   // The query can only match document 0:
-  // - The "semanticSearch(getSearchSpecEmbedding(0), -5)" part should match
+  // - The "semanticSearch(getEmbeddingParameter(0), -5)" part should match
   //   semantic scores {-2, 0}.
-  // - The "semanticSearch(getSearchSpecEmbedding(1), 0)" part should match
+  // - The "semanticSearch(getEmbeddingParameter(1), 0)" part should match
   //   semantic scores {4}.
   std::string query =
-      "semanticSearch(getSearchSpecEmbedding(0), -5) AND "
-      "semanticSearch(getSearchSpecEmbedding(1), 0)";
+      "semanticSearch(getEmbeddingParameter(0), -5) AND "
+      "semanticSearch(getEmbeddingParameter(1), 0)";
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_DOT_PRODUCT,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_DOT_PRODUCT);
   ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results,
-                             std::move(query_visitor).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators), IsEmpty());
   EXPECT_THAT(query_results.query_terms, IsEmpty());
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   DocHitInfoIterator* itr = query_results.root_iterator.get();
   // Check results for document 0.
   ICING_ASSERT_OK(itr->Advance());
@@ -4228,34 +4213,25 @@ TEST_F(QueryVisitorTest, SemanticSearchFunctionMultipleQueries) {
 
   // The query can match both document 0 and document 1:
   // For document 0:
-  // - The "semanticSearch(getSearchSpecEmbedding(0), 1)" part should return
+  // - The "semanticSearch(getEmbeddingParameter(0), 1)" part should return
   //   semantic scores {}.
-  // - The "semanticSearch(getSearchSpecEmbedding(1), 0.1)" part should return
+  // - The "semanticSearch(getEmbeddingParameter(1), 0.1)" part should return
   //   semantic scores {4}.
   // For document 1:
-  // - The "semanticSearch(getSearchSpecEmbedding(0), 1)" part should return
+  // - The "semanticSearch(getEmbeddingParameter(0), 1)" part should return
   //   semantic scores {6}.
-  // - The "semanticSearch(getSearchSpecEmbedding(1), 0.1)" part should return
+  // - The "semanticSearch(getEmbeddingParameter(1), 0.1)" part should return
   //   semantic scores {}.
   query =
-      "semanticSearch(getSearchSpecEmbedding(0), 1) OR "
-      "semanticSearch(getSearchSpecEmbedding(1), 0.1)";
+      "semanticSearch(getEmbeddingParameter(0), 1) OR "
+      "semanticSearch(getEmbeddingParameter(1), 0.1)";
   ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
-  QueryVisitor query_visitor2(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_DOT_PRODUCT,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor2);
   ICING_ASSERT_OK_AND_ASSIGN(query_results,
-                             std::move(query_visitor2).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators), IsEmpty());
   EXPECT_THAT(query_results.query_terms, IsEmpty());
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   itr = query_results.root_iterator.get();
   // Check results for document 1.
   ICING_ASSERT_OK(itr->Advance());
@@ -4303,35 +4279,28 @@ TEST_F(QueryVisitorTest,
   ICING_ASSERT_OK(embedding_index_->CommitBufferToIndex());
 
   // Create two embedding queries.
-  google::protobuf::RepeatedPtrField<PropertyProto::VectorProto> embedding_query_vectors;
   // Semantic scores for this query:
   // - document 0: -2 (section 0), 0 (section 1)
   // - document 1: 6 (section 0)
-  PropertyProto::VectorProto* embedding_query = embedding_query_vectors.Add();
-  *embedding_query = CreateVector("my_model1", {-1, -1, 1});
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      CreateVector("my_model1", {-1, -1, 1})};
 
   // The query should match both document 0 and document 1, since the overall
   // range is [-10, 10]. The scores in the results should be merged.
   std::string query =
-      "semanticSearch(getSearchSpecEmbedding(0), -10, 0) OR "
-      "semanticSearch(getSearchSpecEmbedding(0), 0.0001, 10)";
+      "semanticSearch(getEmbeddingParameter(0), -10, 0) OR "
+      "semanticSearch(getEmbeddingParameter(0), 0.0001, 10)";
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_DOT_PRODUCT,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_DOT_PRODUCT);
   ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results,
-                             std::move(query_visitor).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators), IsEmpty());
   EXPECT_THAT(query_results.query_terms, IsEmpty());
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   DocHitInfoIterator* itr = query_results.root_iterator.get();
   // Check results for document 1.
   ICING_ASSERT_OK(itr->Advance());
@@ -4357,24 +4326,15 @@ TEST_F(QueryVisitorTest,
   // The same query appears twice, in which case all the scores in the results
   // should repeat twice.
   query =
-      "semanticSearch(getSearchSpecEmbedding(0), -10, 10) OR "
-      "semanticSearch(getSearchSpecEmbedding(0), -10, 10)";
+      "semanticSearch(getEmbeddingParameter(0), -10, 10) OR "
+      "semanticSearch(getEmbeddingParameter(0), -10, 10)";
   ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
-  QueryVisitor query_visitor2(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_DOT_PRODUCT,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor2);
   ICING_ASSERT_OK_AND_ASSIGN(query_results,
-                             std::move(query_visitor2).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators), IsEmpty());
   EXPECT_THAT(query_results.query_terms, IsEmpty());
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   itr = query_results.root_iterator.get();
   // Check results for document 1.
   ICING_ASSERT_OK(itr->Advance());
@@ -4421,34 +4381,27 @@ TEST_F(QueryVisitorTest, SemanticSearchFunctionHybridQueries) {
   // Create an embedding query with semantic scores:
   // - document 0: -2
   // - document 1: 6
-  google::protobuf::RepeatedPtrField<PropertyProto::VectorProto> embedding_query_vectors;
-  PropertyProto::VectorProto* embedding_query = embedding_query_vectors.Add();
-  *embedding_query = CreateVector("my_model1", {-1, -1, 1});
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      CreateVector("my_model1", {-1, -1, 1})};
 
   // Perform a hybrid search:
-  // - The "semanticSearch(getSearchSpecEmbedding(0), 0)" part only matches
+  // - The "semanticSearch(getEmbeddingParameter(0), 0)" part only matches
   //   document 1.
   // - The "foo" part only matches document 0.
-  std::string query = "semanticSearch(getSearchSpecEmbedding(0), 0) OR foo";
+  std::string query = "semanticSearch(getEmbeddingParameter(0), 0) OR foo";
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_DOT_PRODUCT,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_DOT_PRODUCT);
   ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results,
-                             std::move(query_visitor).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators),
               UnorderedElementsAre("foo"));
   EXPECT_THAT(ExtractKeys(query_results.query_terms), UnorderedElementsAre(""));
   EXPECT_THAT(query_results.query_terms[""], UnorderedElementsAre("foo"));
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   DocHitInfoIterator* itr = query_results.root_iterator.get();
   // Check results for document 1.
   ICING_ASSERT_OK(itr->Advance());
@@ -4472,29 +4425,20 @@ TEST_F(QueryVisitorTest, SemanticSearchFunctionHybridQueries) {
               StatusIs(libtextclassifier3::StatusCode::RESOURCE_EXHAUSTED));
 
   // Perform another hybrid search:
-  // - The "semanticSearch(getSearchSpecEmbedding(0), -5)" part matches both
+  // - The "semanticSearch(getEmbeddingParameter(0), -5)" part matches both
   //   document 0 and 1.
   // - The "foo" part only matches document 0.
   // As a result, only document 0 will be returned.
-  query = "semanticSearch(getSearchSpecEmbedding(0), -5) AND foo";
+  query = "semanticSearch(getEmbeddingParameter(0), -5) AND foo";
   ICING_ASSERT_OK_AND_ASSIGN(root_node, ParseQueryHelper(query));
-  QueryVisitor query_visitor2(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_DOT_PRODUCT,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor2);
   ICING_ASSERT_OK_AND_ASSIGN(query_results,
-                             std::move(query_visitor2).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators),
               UnorderedElementsAre("foo"));
   EXPECT_THAT(ExtractKeys(query_results.query_terms), UnorderedElementsAre(""));
   EXPECT_THAT(query_results.query_terms[""], UnorderedElementsAre("foo"));
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   itr = query_results.root_iterator.get();
   // Check results for document 0.
   ICING_ASSERT_OK(itr->Advance());
@@ -4551,30 +4495,23 @@ TEST_F(QueryVisitorTest, SemanticSearchFunctionSectionRestriction) {
   // Create an embedding query with semantic scores:
   // - document 0: -2 (section 0), 6 (section 1)
   // - document 1: 2 (section 0), -6 (section 1)
-  google::protobuf::RepeatedPtrField<PropertyProto::VectorProto> embedding_query_vectors;
-  PropertyProto::VectorProto* embedding_query = embedding_query_vectors.Add();
-  *embedding_query = CreateVector("my_model1", {-1, -1, 1});
+  std::vector<PropertyProto::VectorProto> embedding_query_vectors = {
+      CreateVector("my_model1", {-1, -1, 1})};
 
   // An embedding query with section restriction. The scores returned should
   // only be limited to the section restricted.
-  std::string query = "prop1:semanticSearch(getSearchSpecEmbedding(0), -100)";
+  std::string query = "prop1:semanticSearch(getEmbeddingParameter(0), -100)";
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Node> root_node,
                              ParseQueryHelper(query));
-  QueryVisitor query_visitor(
-      index_.get(), numeric_index_.get(), embedding_index_.get(),
-      document_store_.get(), schema_store_.get(), normalizer_.get(),
-      tokenizer_.get(), query, &embedding_query_vectors,
-      DocHitInfoIteratorFilter::Options(), TERM_MATCH_PREFIX,
-      EMBEDDING_METRIC_DOT_PRODUCT,
-      /*needs_term_frequency_info=*/true, clock_.GetSystemTimeMilliseconds());
-  root_node->Accept(&query_visitor);
+  SearchSpecProto search_spec =
+      CreateSearchSpec(query, TERM_MATCH_PREFIX, embedding_query_vectors,
+                       EMBEDDING_METRIC_DOT_PRODUCT);
   ICING_ASSERT_OK_AND_ASSIGN(QueryResults query_results,
-                             std::move(query_visitor).ConsumeResults());
+                             ProcessQuery(search_spec, root_node.get()));
   EXPECT_THAT(ExtractKeys(query_results.query_term_iterators), IsEmpty());
   EXPECT_THAT(query_results.query_terms, IsEmpty());
   EXPECT_THAT(query_results.features_in_use,
-              UnorderedElementsAre(kListFilterQueryLanguageFeature,
-                                   kEmbeddingSearchFeature));
+              UnorderedElementsAre(kListFilterQueryLanguageFeature));
   DocHitInfoIterator* itr = query_results.root_iterator.get();
   // Check results.
   ICING_ASSERT_OK(itr->Advance());
diff --git a/icing/query/query-features.h b/icing/query/query-features.h
index 313e3b7..c84dc7d 100644
--- a/icing/query/query-features.h
+++ b/icing/query/query-features.h
@@ -53,19 +53,19 @@ constexpr Feature kHasPropertyFunctionFeature =
     "HAS_PROPERTY_FUNCTION";  // Features#HAS_PROPERTY_FUNCTION
 
 // This feature relates to the use of embedding searches in the advanced query
-// language. Ex. `semanticSearch(getSearchSpecEmbedding(0), 0.5, 1, "COSINE")`.
-constexpr Feature kEmbeddingSearchFeature =
+// language. Ex. `semanticSearch(getEmbeddingParameter(0), 0.5, 1, "COSINE")`.
+//
+// Deprecated: This feature is not necessary. The availability of this feature
+// is already controlled by the existence of the embedding_query_vectors in the
+// SearchSpecProto. This API was never publicly released in Jetpack or Android,
+// so it should be safe to delete once all google3 references are removed.
+constexpr Feature kEmbeddingSearchFeatureDeprecated =
     "EMBEDDING_SEARCH";  // Features#EMBEDDING_SEARCH
 
-// This feature relates to the use of the tokenize function which returns an
-// iterator that ANDs all of the normalized tokens in its string.
-// Ex. `tokenize("foo.bar\" baz(")`.
-constexpr Feature kTokenizeFeature = "TOKENIZE";  // Features#TOKENIZE
-
 inline std::unordered_set<Feature> GetQueryFeaturesSet() {
   return {kNumericSearchFeature,           kVerbatimSearchFeature,
           kListFilterQueryLanguageFeature, kHasPropertyFunctionFeature,
-          kEmbeddingSearchFeature,         kTokenizeFeature};
+          kEmbeddingSearchFeatureDeprecated};
 }
 
 }  // namespace lib
diff --git a/icing/query/query-processor.cc b/icing/query/query-processor.cc
index ed3b5ca..fcd4f73 100644
--- a/icing/query/query-processor.cc
+++ b/icing/query/query-processor.cc
@@ -131,7 +131,7 @@ libtextclassifier3::StatusOr<QueryResults> QueryProcessor::ParseAdvancedQuery(
   std::unique_ptr<Timer> lexer_timer = clock_.GetNewTimer();
   Lexer lexer(search_spec.query(), Lexer::Language::QUERY);
   ICING_ASSIGN_OR_RETURN(std::vector<Lexer::LexerToken> lexer_tokens,
-                         lexer.ExtractTokens());
+                         std::move(lexer).ExtractTokens());
   if (search_stats != nullptr) {
     search_stats->set_query_processor_lexer_extract_token_latency_ms(
         lexer_timer->GetElapsedMilliseconds());
@@ -164,10 +164,8 @@ libtextclassifier3::StatusOr<QueryResults> QueryProcessor::ParseAdvancedQuery(
   std::unique_ptr<Timer> query_visitor_timer = clock_.GetNewTimer();
   QueryVisitor query_visitor(
       &index_, &numeric_index_, &embedding_index_, &document_store_,
-      &schema_store_, &normalizer_, plain_tokenizer.get(), search_spec.query(),
-      &search_spec.embedding_query_vectors(), std::move(options),
-      search_spec.term_match_type(), search_spec.embedding_query_metric_type(),
-      needs_term_frequency_info, current_time_ms);
+      &schema_store_, &normalizer_, plain_tokenizer.get(), search_spec,
+      std::move(options), needs_term_frequency_info, current_time_ms);
   tree_root->Accept(&query_visitor);
   ICING_ASSIGN_OR_RETURN(QueryResults results,
                          std::move(query_visitor).ConsumeResults());
diff --git a/icing/query/query-processor_benchmark.cc b/icing/query/query-processor_benchmark.cc
index 091717b..fd096bf 100644
--- a/icing/query/query-processor_benchmark.cc
+++ b/icing/query/query-processor_benchmark.cc
@@ -181,7 +181,8 @@ void BM_QueryOneTerm(benchmark::State& state) {
                                          .SetKey("icing", "type1")
                                          .SetSchema("type1")
                                          .Build())
-                               .ValueOrDie();
+                               .ValueOrDie()
+                               .new_document_id;
 
   const std::string input_string(state.range(0), 'A');
   AddTokenToIndex(index.get(), document_id, /*section_id=*/0,
@@ -315,7 +316,8 @@ void BM_QueryFiveTerms(benchmark::State& state) {
                                          .SetKey("icing", "type1")
                                          .SetSchema("type1")
                                          .Build())
-                               .ValueOrDie();
+                               .ValueOrDie()
+                               .new_document_id;
 
   int term_length = state.range(0) / 5;
 
@@ -467,7 +469,8 @@ void BM_QueryDiacriticTerm(benchmark::State& state) {
                                          .SetKey("icing", "type1")
                                          .SetSchema("type1")
                                          .Build())
-                               .ValueOrDie();
+                               .ValueOrDie()
+                               .new_document_id;
 
   std::string input_string;
   while (input_string.length() < state.range(0)) {
@@ -604,7 +607,8 @@ void BM_QueryHiragana(benchmark::State& state) {
                                          .SetKey("icing", "type1")
                                          .SetSchema("type1")
                                          .Build())
-                               .ValueOrDie();
+                               .ValueOrDie()
+                               .new_document_id;
 
   std::string input_string;
   while (input_string.length() < state.range(0)) {
diff --git a/icing/query/query-processor_test.cc b/icing/query/query-processor_test.cc
index 0052e6a..536ab5f 100644
--- a/icing/query/query-processor_test.cc
+++ b/icing/query/query-processor_test.cc
@@ -282,16 +282,18 @@ TEST_F(QueryProcessorTest, EmptyQueryMatchAllDocuments) {
                   /*allow_circular_schema_definitions=*/false),
               IsOk());
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // We don't need to insert anything in the index since the empty query will
   // match all DocumentIds from the DocumentStore
@@ -324,11 +326,12 @@ TEST_F(QueryProcessorTest, QueryTermNormalized) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id = put_result.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -384,11 +387,12 @@ TEST_F(QueryProcessorTest, OneTermPrefixMatch) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id = put_result.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -439,11 +443,12 @@ TEST_F(QueryProcessorTest, OneTermPrefixMatchWithMaxSectionID) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id = put_result.new_document_id;
 
   // Populate the index
   SectionId section_id = kMaxSectionId;
@@ -496,11 +501,12 @@ TEST_F(QueryProcessorTest, OneTermExactMatch) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id = put_result.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -551,11 +557,12 @@ TEST_F(QueryProcessorTest, AndSameTermExactMatch) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that the DocHitInfoIterators will see
   // that the document exists and not filter out the DocumentId as deleted.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(DocumentBuilder()
-                                                      .SetKey("namespace", "1")
+                                                      .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id = put_result.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -608,11 +615,12 @@ TEST_F(QueryProcessorTest, AndTwoTermExactMatch) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that the DocHitInfoIterators will see
   // that the document exists and not filter out the DocumentId as deleted.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id = put_result.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -668,11 +676,12 @@ TEST_F(QueryProcessorTest, AndSameTermPrefixMatch) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that the DocHitInfoIterators will see
   // that the document exists and not filter out the DocumentId as deleted.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id = put_result.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -725,11 +734,12 @@ TEST_F(QueryProcessorTest, AndTwoTermPrefixMatch) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that the DocHitInfoIterators will see
   // that the document exists and not filter out the DocumentId as deleted.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id = put_result.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -786,11 +796,12 @@ TEST_F(QueryProcessorTest, AndTwoTermPrefixAndExactMatch) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that the DocHitInfoIterators will see
   // that the document exists and not filter out the DocumentId as deleted.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id = put_result.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -847,16 +858,18 @@ TEST_F(QueryProcessorTest, OrTwoTermExactMatch) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that the DocHitInfoIterators will see
   // that the document exists and not filter out the DocumentId as deleted.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -921,16 +934,18 @@ TEST_F(QueryProcessorTest, OrTwoTermPrefixMatch) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that the DocHitInfoIterators will see
   // that the document exists and not filter out the DocumentId as deleted.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -995,16 +1010,18 @@ TEST_F(QueryProcessorTest, OrTwoTermPrefixAndExactMatch) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that the DocHitInfoIterators will see
   // that the document exists and not filter out the DocumentId as deleted.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -1067,16 +1084,18 @@ TEST_F(QueryProcessorTest, CombinedAndOrTerms) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that the DocHitInfoIterators will see
   // that the document exists and not filter out the DocumentId as deleted.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
   // Populate the index
   SectionId section_id = 0;
   SectionIdMask section_id_mask = 1U << section_id;
@@ -1238,16 +1257,18 @@ TEST_F(QueryProcessorTest, OneGroup) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that the DocHitInfoIterators will see
   // that the document exists and not filter out the DocumentId as deleted.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -1304,16 +1325,18 @@ TEST_F(QueryProcessorTest, TwoGroups) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that the DocHitInfoIterators will see
   // that the document exists and not filter out the DocumentId as deleted.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -1373,16 +1396,18 @@ TEST_F(QueryProcessorTest, ManyLevelNestedGrouping) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that the DocHitInfoIterators will see
   // that the document exists and not filter out the DocumentId as deleted.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -1439,16 +1464,18 @@ TEST_F(QueryProcessorTest, OneLevelNestedGrouping) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that the DocHitInfoIterators will see
   // that the document exists and not filter out the DocumentId as deleted.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -1507,16 +1534,18 @@ TEST_F(QueryProcessorTest, ExcludeTerm) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that they'll bump the
   // last_added_document_id, which will give us the proper exclusion results
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -1561,16 +1590,18 @@ TEST_F(QueryProcessorTest, ExcludeNonexistentTerm) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that they'll bump the
   // last_added_document_id, which will give us the proper exclusion results
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
   // Populate the index
   SectionId section_id = 0;
   TermMatchType::Code term_match_type = TermMatchType::EXACT_ONLY;
@@ -1613,16 +1644,18 @@ TEST_F(QueryProcessorTest, ExcludeAnd) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that they'll bump the
   // last_added_document_id, which will give us the proper exclusion results
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -1697,16 +1730,18 @@ TEST_F(QueryProcessorTest, ExcludeOr) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that they'll bump the
   // last_added_document_id, which will give us the proper exclusion results
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -1784,16 +1819,18 @@ TEST_F(QueryProcessorTest, WithoutTermFrequency) {
   // These documents don't actually match to the tokens in the index. We're
   // just inserting the documents so that the DocHitInfoIterators will see
   // that the document exists and not filter out the DocumentId as deleted.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -1883,16 +1920,18 @@ TEST_F(QueryProcessorTest, DeletedFilter) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
   EXPECT_THAT(document_store_->Delete("namespace", "1",
                                       fake_clock_.GetSystemTimeMilliseconds()),
               IsOk());
@@ -1949,16 +1988,18 @@ TEST_F(QueryProcessorTest, NamespaceFilter) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace2", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -2015,16 +2056,18 @@ TEST_F(QueryProcessorTest, SchemaTypeFilter) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // schema types populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("message")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
@@ -2082,11 +2125,12 @@ TEST_F(QueryProcessorTest, PropertyFilterForOneDocument) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // schema types populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id = put_result.new_document_id;
 
   // Populate the index
   TermMatchType::Code term_match_type = TermMatchType::EXACT_ONLY;
@@ -2154,16 +2198,18 @@ TEST_F(QueryProcessorTest, PropertyFilterAcrossSchemaTypes) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // schema types populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId message_document_id,
+  DocumentId email_document_id = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("message")
                                                       .Build()));
+  DocumentId message_document_id = put_result2.new_document_id;
 
   // Populate the index
   TermMatchType::Code term_match_type = TermMatchType::EXACT_ONLY;
@@ -2227,16 +2273,18 @@ TEST_F(QueryProcessorTest, PropertyFilterWithinSchemaType) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // schema types populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId message_document_id,
+  DocumentId email_document_id = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("message")
                                                       .Build()));
+  DocumentId message_document_id = put_result2.new_document_id;
 
   // Populate the index
   TermMatchType::Code term_match_type = TermMatchType::EXACT_ONLY;
@@ -2318,11 +2366,12 @@ TEST_F(QueryProcessorTest, NestedPropertyFilter) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // schema types populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId email_document_id = put_result1.new_document_id;
 
   // Populate the index
   TermMatchType::Code term_match_type = TermMatchType::EXACT_ONLY;
@@ -2382,16 +2431,18 @@ TEST_F(QueryProcessorTest, PropertyFilterRespectsDifferentSectionIds) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // schema types populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId message_document_id,
+  DocumentId email_document_id = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("message")
                                                       .Build()));
+  DocumentId message_document_id = put_result2.new_document_id;
 
   // Populate the index
   TermMatchType::Code term_match_type = TermMatchType::EXACT_ONLY;
@@ -2445,11 +2496,12 @@ TEST_F(QueryProcessorTest, NonexistentPropertyFilterReturnsEmptyResults) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // schema types populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId email_document_id = put_result1.new_document_id;
 
   // Populate the index
   TermMatchType::Code term_match_type = TermMatchType::EXACT_ONLY;
@@ -2502,11 +2554,12 @@ TEST_F(QueryProcessorTest, UnindexedPropertyFilterReturnsEmptyResults) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // schema types populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId email_document_id = put_result1.new_document_id;
 
   // Populate the index
   TermMatchType::Code term_match_type = TermMatchType::EXACT_ONLY;
@@ -2562,16 +2615,18 @@ TEST_F(QueryProcessorTest, PropertyFilterTermAndUnrestrictedTerm) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // schema types populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId message_document_id,
+  DocumentId email_document_id = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("message")
                                                       .Build()));
+  DocumentId message_document_id = put_result2.new_document_id;
 
   // Poplate the index
   TermMatchType::Code term_match_type = TermMatchType::EXACT_ONLY;
@@ -2668,16 +2723,18 @@ TEST_F(QueryProcessorTest, TypePropertyFilter) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // schema types populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId message_document_id,
+  DocumentId email_document_id = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("message")
                                                       .Build()));
+  DocumentId message_document_id = put_result2.new_document_id;
 
   // Poplate the index
   TermMatchType::Code term_match_type = TermMatchType::EXACT_ONLY;
@@ -2795,16 +2852,18 @@ TEST_F(QueryProcessorTest, TypePropertyFilterWithSectionRestrict) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // schema types populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId message_document_id,
+  DocumentId email_document_id = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("message")
                                                       .Build()));
+  DocumentId message_document_id = put_result2.new_document_id;
 
   // Poplate the index
   TermMatchType::Code term_match_type = TermMatchType::EXACT_ONLY;
@@ -2890,13 +2949,14 @@ TEST_F(QueryProcessorTest, DocumentBeforeTtlNotFilteredOut) {
   document_store_ = std::move(create_result.document_store);
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(DocumentBuilder()
                                .SetKey("namespace", "1")
                                .SetSchema("email")
                                .SetCreationTimestampMs(10)
                                .SetTtlMs(100)
                                .Build()));
+  DocumentId document_id = put_result.new_document_id;
 
   // Populate the index
   int section_id = 0;
@@ -2952,13 +3012,14 @@ TEST_F(QueryProcessorTest, DocumentPastTtlFilteredOut) {
   document_store_ = std::move(create_result.document_store);
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(DocumentBuilder()
                                .SetKey("namespace", "1")
                                .SetSchema("email")
                                .SetCreationTimestampMs(50)
                                .SetTtlMs(100)
                                .Build()));
+  DocumentId document_id = put_result.new_document_id;
 
   // Populate the index
   int section_id = 0;
@@ -3011,34 +3072,36 @@ TEST_F(QueryProcessorTest, NumericFilter) {
                   schema, /*ignore_errors_and_delete_documents=*/false,
                   /*allow_circular_schema_definitions=*/false),
               IsOk());
-
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_one_id,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(DocumentBuilder()
                                .SetKey("namespace", "1")
                                .SetSchema("transaction")
                                .AddInt64Property("price", 10)
                                .Build()));
+  DocumentId document_one_id = put_result1.new_document_id;
   ICING_ASSERT_OK(
       AddToNumericIndex(document_one_id, "price", price_section_id, 10));
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_two_id,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(DocumentBuilder()
                                .SetKey("namespace", "2")
                                .SetSchema("transaction")
                                .AddInt64Property("price", 25)
                                .Build()));
+  DocumentId document_two_id = put_result2.new_document_id;
   ICING_ASSERT_OK(
       AddToNumericIndex(document_two_id, "price", price_section_id, 25));
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_three_id,
+      DocumentStore::PutResult put_result3,
       document_store_->Put(DocumentBuilder()
                                .SetKey("namespace", "3")
                                .SetSchema("transaction")
                                .AddInt64Property("cost", 2)
                                .Build()));
+  DocumentId document_three_id = put_result3.new_document_id;
   ICING_ASSERT_OK(
       AddToNumericIndex(document_three_id, "cost", cost_section_id, 2));
 
@@ -3111,12 +3174,13 @@ TEST_F(QueryProcessorTest, NumericFilterWithoutEnablingFeatureFails) {
               IsOk());
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_one_id,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(DocumentBuilder()
                                .SetKey("namespace", "1")
                                .SetSchema("transaction")
                                .AddInt64Property("price", 10)
                                .Build()));
+  DocumentId document_one_id = put_result1.new_document_id;
   ICING_ASSERT_OK(
       AddToNumericIndex(document_one_id, "price", price_section_id, 10));
 
@@ -3167,11 +3231,12 @@ TEST_F(QueryProcessorTest, GroupingInSectionRestriction) {
   //   Doc2:
   //     prop1: "foo bar"
   //     prop2: ""
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id0,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "0")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id0 = put_result0.new_document_id;
   EXPECT_THAT(
       AddTokenToIndex(document_id0, prop1_section_id, term_match_type, "foo"),
       IsOk());
@@ -3179,11 +3244,12 @@ TEST_F(QueryProcessorTest, GroupingInSectionRestriction) {
       AddTokenToIndex(document_id0, prop2_section_id, term_match_type, "bar"),
       IsOk());
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id1 = put_result1.new_document_id;
   EXPECT_THAT(
       AddTokenToIndex(document_id1, prop1_section_id, term_match_type, "bar"),
       IsOk());
@@ -3191,11 +3257,12 @@ TEST_F(QueryProcessorTest, GroupingInSectionRestriction) {
       AddTokenToIndex(document_id1, prop2_section_id, term_match_type, "foo"),
       IsOk());
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id2 = put_result2.new_document_id;
   EXPECT_THAT(
       AddTokenToIndex(document_id2, prop1_section_id, term_match_type, "foo"),
       IsOk());
@@ -3279,11 +3346,12 @@ TEST_F(QueryProcessorTest, ParseAdvancedQueryShouldSetSearchStats) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(DocumentBuilder()
-                                                      .SetKey("namespace1", "1")
+                                                      .SetKey("namespace", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId document_id = put_result.new_document_id;
 
   // Populate the index
   SectionId section_id = 0;
diff --git a/icing/query/suggestion-processor.cc b/icing/query/suggestion-processor.cc
index dfebb98..ba80093 100644
--- a/icing/query/suggestion-processor.cc
+++ b/icing/query/suggestion-processor.cc
@@ -259,6 +259,20 @@ SuggestionProcessor::QuerySuggestions(
   search_spec.set_query(suggestion_spec.prefix());
   search_spec.set_term_match_type(
       suggestion_spec.scoring_spec().scoring_match_type());
+  for (const PropertyProto::VectorProto& vector :
+       suggestion_spec.embedding_query_vectors()) {
+    *search_spec.add_embedding_query_vectors() = vector;
+  }
+  search_spec.set_embedding_query_metric_type(
+      suggestion_spec.embedding_query_metric_type());
+  for (const std::string& query_parameter_string :
+       suggestion_spec.query_parameter_strings()) {
+    search_spec.add_query_parameter_strings(query_parameter_string);
+  }
+  for (const std::string& enabled_feature :
+       suggestion_spec.enabled_features()) {
+    search_spec.add_enabled_features(enabled_feature);
+  }
   ICING_ASSIGN_OR_RETURN(
       QueryResults query_results,
       query_processor->ParseSearch(search_spec,
diff --git a/icing/query/suggestion-processor_test.cc b/icing/query/suggestion-processor_test.cc
index 0e2158f..4347a69 100644
--- a/icing/query/suggestion-processor_test.cc
+++ b/icing/query/suggestion-processor_test.cc
@@ -34,6 +34,7 @@
 #include "icing/jni/jni-cache.h"
 #include "icing/legacy/index/icing-filesystem.h"
 #include "icing/portable/platform.h"
+#include "icing/query/query-features.h"
 #include "icing/schema-builder.h"
 #include "icing/schema/schema-store.h"
 #include "icing/schema/section.h"
@@ -199,16 +200,18 @@ TEST_F(SuggestionProcessorTest, MultipleTermsTest_And) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId0,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId1,
+  DocumentId documentId0 = put_result0.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId documentId1 = put_result1.new_document_id;
 
   ASSERT_THAT(AddTokenToIndex(documentId0, kSectionId2,
                               TermMatchType::EXACT_ONLY, "foo"),
@@ -246,16 +249,18 @@ TEST_F(SuggestionProcessorTest, MultipleTermsTest_AndNary) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId0,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId1,
+  DocumentId documentId0 = put_result0.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId documentId1 = put_result1.new_document_id;
 
   ASSERT_THAT(AddTokenToIndex(documentId0, kSectionId2,
                               TermMatchType::EXACT_ONLY, "foo"),
@@ -297,16 +302,18 @@ TEST_F(SuggestionProcessorTest, MultipleTermsTest_Or) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId0,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId1,
+  DocumentId documentId0 = put_result0.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId documentId1 = put_result1.new_document_id;
 
   ASSERT_THAT(AddTokenToIndex(documentId0, kSectionId2,
                               TermMatchType::EXACT_ONLY, "fo"),
@@ -350,21 +357,24 @@ TEST_F(SuggestionProcessorTest, MultipleTermsTest_OrNary) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId0,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId1,
+  DocumentId documentId0 = put_result0.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "2")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId2,
+  DocumentId documentId1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "3")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId documentId2 = put_result2.new_document_id;
 
   ASSERT_THAT(AddTokenToIndex(documentId0, kSectionId2,
                               TermMatchType::EXACT_ONLY, "fo"),
@@ -416,16 +426,18 @@ TEST_F(SuggestionProcessorTest, MultipleTermsTest_NormalizedTerm) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId0,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId1,
+  DocumentId documentId0 = put_result0.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "2")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId documentId1 = put_result1.new_document_id;
 
   ASSERT_THAT(AddTokenToIndex(documentId0, kSectionId2,
                               TermMatchType::EXACT_ONLY, "foo"),
@@ -478,11 +490,12 @@ TEST_F(SuggestionProcessorTest, NonExistentPrefixTest) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId0,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId documentId0 = put_result0.new_document_id;
 
   ASSERT_THAT(AddTokenToIndex(documentId0, kSectionId2,
                               TermMatchType::EXACT_ONLY, "foo"),
@@ -514,11 +527,12 @@ TEST_F(SuggestionProcessorTest, PrefixTrailingSpaceTest) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId0,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId documentId0 = put_result0.new_document_id;
 
   ASSERT_THAT(AddTokenToIndex(documentId0, kSectionId2,
                               TermMatchType::EXACT_ONLY, "foo"),
@@ -551,11 +565,12 @@ TEST_F(SuggestionProcessorTest, NormalizePrefixTest) {
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId0,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId documentId0 = put_result0.new_document_id;
   ASSERT_THAT(AddTokenToIndex(documentId0, kSectionId2,
                               TermMatchType::EXACT_ONLY, "foo"),
               IsOk());
@@ -604,11 +619,12 @@ TEST_F(SuggestionProcessorTest, ParenthesesOperatorPrefixTest) {
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId0,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId documentId0 = put_result0.new_document_id;
   ASSERT_THAT(AddTokenToIndex(documentId0, kSectionId2,
                               TermMatchType::EXACT_ONLY, "foo"),
               IsOk());
@@ -651,11 +667,12 @@ TEST_F(SuggestionProcessorTest, OtherSpecialPrefixTest) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId0,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId documentId0 = put_result0.new_document_id;
 
   ASSERT_THAT(AddTokenToIndex(documentId0, kSectionId2,
                               TermMatchType::EXACT_ONLY, "foo"),
@@ -687,12 +704,70 @@ TEST_F(SuggestionProcessorTest, OtherSpecialPrefixTest) {
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 
   suggestion_spec.set_prefix(
-      "bar OR semanticSearch(getSearchSpecEmbedding(0), 0.5, 1)");
+      "bar OR semanticSearch(getEmbeddingParameter(0), 0.5, 1)");
+  suggestion_spec.add_enabled_features(
+      std::string(kListFilterQueryLanguageFeature));
+  suggestion_spec.set_embedding_query_metric_type(
+      SearchSpecProto::EmbeddingQueryMetricType::DOT_PRODUCT);
+  PropertyProto::VectorProto* vector =
+      suggestion_spec.add_embedding_query_vectors();
+  vector->set_model_signature("model_signature");
+  vector->add_values(0.1);
   EXPECT_THAT(suggestion_processor_->QuerySuggestions(
                   suggestion_spec, fake_clock_.GetSystemTimeMilliseconds()),
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
+TEST_F(SuggestionProcessorTest, SemanticSearchPrefixTest) {
+  // Create the schema and document store
+  SchemaProto schema = SchemaBuilder()
+                           .AddType(SchemaTypeConfigBuilder().SetType("email"))
+                           .Build();
+  ASSERT_THAT(schema_store_->SetSchema(
+                  schema, /*ignore_errors_and_delete_documents=*/false,
+                  /*allow_circular_schema_definitions=*/false),
+              IsOk());
+
+  // These documents don't actually match to the tokens in the index. We're
+  // inserting the documents to get the appropriate number of documents and
+  // namespaces populated.
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
+                             document_store_->Put(DocumentBuilder()
+                                                      .SetKey("namespace1", "1")
+                                                      .SetSchema("email")
+                                                      .Build()));
+  DocumentId documentId0 = put_result0.new_document_id;
+
+  ASSERT_THAT(AddTokenToIndex(documentId0, kSectionId2,
+                              TermMatchType::EXACT_ONLY, "foo"),
+              IsOk());
+
+  SuggestionSpecProto suggestion_spec;
+  suggestion_spec.set_num_to_return(10);
+  suggestion_spec.mutable_scoring_spec()->set_scoring_match_type(
+      TermMatchType::PREFIX);
+
+  // Suggesting without adding embedding query vectors will cause a failure.
+  suggestion_spec.set_prefix(
+      "semanticSearch(getEmbeddingParameter(0), 0.5, 1) OR foo");
+  suggestion_spec.add_enabled_features(
+      std::string(kListFilterQueryLanguageFeature));
+  EXPECT_THAT(suggestion_processor_->QuerySuggestions(
+                  suggestion_spec, fake_clock_.GetSystemTimeMilliseconds()),
+              StatusIs(libtextclassifier3::StatusCode::OUT_OF_RANGE));
+
+  // Adding embedding query vectors will allow us to successfully suggest.
+  suggestion_spec.set_embedding_query_metric_type(
+      SearchSpecProto::EmbeddingQueryMetricType::DOT_PRODUCT);
+  PropertyProto::VectorProto* vector =
+      suggestion_spec.add_embedding_query_vectors();
+  vector->set_model_signature("model_signature");
+  vector->add_values(0.1);
+  EXPECT_THAT(suggestion_processor_->QuerySuggestions(
+                  suggestion_spec, fake_clock_.GetSystemTimeMilliseconds()),
+              StatusIs(libtextclassifier3::StatusCode::OK));
+}
+
 TEST_F(SuggestionProcessorTest, InvalidPrefixTest) {
   // Create the schema and document store
   SchemaProto schema = SchemaBuilder()
@@ -706,11 +781,12 @@ TEST_F(SuggestionProcessorTest, InvalidPrefixTest) {
   // These documents don't actually match to the tokens in the index. We're
   // inserting the documents to get the appropriate number of documents and
   // namespaces populated.
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId documentId0,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result0,
                              document_store_->Put(DocumentBuilder()
                                                       .SetKey("namespace1", "1")
                                                       .SetSchema("email")
                                                       .Build()));
+  DocumentId documentId0 = put_result0.new_document_id;
 
   ASSERT_THAT(AddTokenToIndex(documentId0, kSectionId2,
                               TermMatchType::EXACT_ONLY, "original"),
diff --git a/icing/result/result-retriever-v2_group-result-limiter_test.cc b/icing/result/result-retriever-v2_group-result-limiter_test.cc
index 7fe8a75..215035e 100644
--- a/icing/result/result-retriever-v2_group-result-limiter_test.cc
+++ b/icing/result/result-retriever-v2_group-result-limiter_test.cc
@@ -131,8 +131,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(1)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document2 = DocumentBuilder()
                                 .SetKey("namespace", "uri/2")
@@ -140,8 +141,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(2)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   std::vector<ScoredDocumentHit> scored_document_hits = {
       ScoredDocumentHit(document_id1, kSectionIdMaskNone, document1.score()),
@@ -191,8 +193,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(1)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document2 = DocumentBuilder()
                                 .SetKey("namespace", "uri/2")
@@ -200,8 +203,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(2)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   std::vector<ScoredDocumentHit> scored_document_hits = {
       ScoredDocumentHit(document_id1, kSectionIdMaskNone, document1.score()),
@@ -247,8 +251,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(1)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document2 = DocumentBuilder()
                                 .SetKey("namespace", "uri/2")
@@ -256,8 +261,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(2)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   DocumentProto document3 = DocumentBuilder()
                                 .SetKey("namespace", "uri/3")
@@ -265,8 +271,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(3)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store_->Put(document3));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   DocumentProto document4 = DocumentBuilder()
                                 .SetKey("namespace", "uri/4")
@@ -274,8 +281,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(4)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id4,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
                              document_store_->Put(document4));
+  DocumentId document_id4 = put_result4.new_document_id;
 
   std::vector<ScoredDocumentHit> scored_document_hits = {
       ScoredDocumentHit(document_id1, kSectionIdMaskNone, document1.score()),
@@ -333,8 +341,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(1)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document2 = DocumentBuilder()
                                 .SetKey("namespace1", "uri/2")
@@ -342,8 +351,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(2)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   DocumentProto document3 = DocumentBuilder()
                                 .SetKey("namespace2", "uri/3")
@@ -351,8 +361,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(3)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store_->Put(document3));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   DocumentProto document4 = DocumentBuilder()
                                 .SetKey("namespace2", "uri/4")
@@ -360,8 +371,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(4)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id4,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
                              document_store_->Put(document4));
+  DocumentId document_id4 = put_result4.new_document_id;
 
   std::vector<ScoredDocumentHit> scored_document_hits = {
       ScoredDocumentHit(document_id1, kSectionIdMaskNone, document1.score()),
@@ -415,8 +427,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(1)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document2 = DocumentBuilder()
                                 .SetKey("namespace", "uri/2")
@@ -424,8 +437,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(2)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   std::vector<ScoredDocumentHit> scored_document_hits = {
       ScoredDocumentHit(document_id1, kSectionIdMaskNone, document1.score()),
@@ -479,8 +493,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(1)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document2 = DocumentBuilder()
                                 .SetKey("namespace", "uri/2")
@@ -488,8 +503,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(2)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   std::vector<ScoredDocumentHit> scored_document_hits = {
       ScoredDocumentHit(document_id1, kSectionIdMaskNone, document1.score()),
@@ -544,8 +560,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(1)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document2 = DocumentBuilder()
                                 .SetKey("namespace1", "uri/2")
@@ -553,8 +570,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(2)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   DocumentProto document3 = DocumentBuilder()
                                 .SetKey("namespace2", "uri/3")
@@ -562,8 +580,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(3)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store_->Put(document3));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   DocumentProto document4 = DocumentBuilder()
                                 .SetKey("namespace2", "uri/4")
@@ -571,8 +590,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(4)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id4,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
                              document_store_->Put(document4));
+  DocumentId document_id4 = put_result4.new_document_id;
 
   DocumentProto document5 = DocumentBuilder()
                                 .SetKey("namespace3", "uri/5")
@@ -580,8 +600,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(5)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id5,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result5,
                              document_store_->Put(document5));
+  DocumentId document_id5 = put_result5.new_document_id;
 
   DocumentProto document6 = DocumentBuilder()
                                 .SetKey("namespace3", "uri/6")
@@ -589,8 +610,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(6)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id6,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result6,
                              document_store_->Put(document6));
+  DocumentId document_id6 = put_result6.new_document_id;
 
   std::vector<ScoredDocumentHit> scored_document_hits = {
       ScoredDocumentHit(document_id1, kSectionIdMaskNone, document1.score()),
@@ -655,8 +677,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(1)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document2 = DocumentBuilder()
                                 .SetKey("namespace", "uri/2")
@@ -664,8 +687,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(2)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   DocumentProto document3 = DocumentBuilder()
                                 .SetKey("namespace", "uri/3")
@@ -673,8 +697,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(3)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store_->Put(document3));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   DocumentProto document4 = DocumentBuilder()
                                 .SetKey("namespace", "uri/4")
@@ -682,8 +707,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(4)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id4,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
                              document_store_->Put(document4));
+  DocumentId document_id4 = put_result4.new_document_id;
 
   DocumentProto document5 = DocumentBuilder()
                                 .SetKey("namespace", "uri/5")
@@ -691,8 +717,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(5)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id5,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result5,
                              document_store_->Put(document5));
+  DocumentId document_id5 = put_result5.new_document_id;
 
   DocumentProto document6 = DocumentBuilder()
                                 .SetKey("namespace", "uri/6")
@@ -700,8 +727,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(6)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id6,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result6,
                              document_store_->Put(document6));
+  DocumentId document_id6 = put_result6.new_document_id;
 
   std::vector<ScoredDocumentHit> scored_document_hits = {
       ScoredDocumentHit(document_id1, kSectionIdMaskNone, document1.score()),
@@ -766,8 +794,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(1)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document2 = DocumentBuilder()
                                 .SetKey("namespace1", "uri/2")
@@ -775,8 +804,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(2)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   DocumentProto document3 = DocumentBuilder()
                                 .SetKey("namespace1", "uri/3")
@@ -784,8 +814,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(3)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store_->Put(document3));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   DocumentProto document4 = DocumentBuilder()
                                 .SetKey("namespace2", "uri/4")
@@ -793,8 +824,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(4)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id4,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
                              document_store_->Put(document4));
+  DocumentId document_id4 = put_result4.new_document_id;
 
   DocumentProto document5 = DocumentBuilder()
                                 .SetKey("namespace3", "uri/5")
@@ -802,8 +834,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(5)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id5,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result5,
                              document_store_->Put(document5));
+  DocumentId document_id5 = put_result5.new_document_id;
 
   DocumentProto document6 = DocumentBuilder()
                                 .SetKey("namespace3", "uri/6")
@@ -811,8 +844,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(6)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id6,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result6,
                              document_store_->Put(document6));
+  DocumentId document_id6 = put_result6.new_document_id;
 
   std::vector<ScoredDocumentHit> scored_document_hits = {
       ScoredDocumentHit(document_id1, kSectionIdMaskNone, document1.score()),
@@ -881,8 +915,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(1)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document2 = DocumentBuilder()
                                 .SetKey("namespace", "uri/2")
@@ -890,8 +925,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(2)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   std::vector<ScoredDocumentHit> scored_document_hits = {
       ScoredDocumentHit(document_id1, kSectionIdMaskNone, document1.score()),
@@ -943,8 +979,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(1)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document2 = DocumentBuilder()
                                 .SetKey("namespace", "uri/2")
@@ -952,8 +989,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(2)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   std::vector<ScoredDocumentHit> scored_document_hits = {
       ScoredDocumentHit(document_id1, kSectionIdMaskNone, document1.score()),
@@ -1005,8 +1043,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(1)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document1));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document2 = DocumentBuilder()
                                 .SetKey("namespace1", "uri/2")
@@ -1014,8 +1053,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(2)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   DocumentProto document3 = DocumentBuilder()
                                 .SetKey("namespace1", "uri/3")
@@ -1023,8 +1063,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(3)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store_->Put(document3));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   DocumentProto document4 = DocumentBuilder()
                                 .SetKey("namespace2", "uri/4")
@@ -1032,8 +1073,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(4)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id4,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
                              document_store_->Put(document4));
+  DocumentId document_id4 = put_result4.new_document_id;
 
   DocumentProto document5 = DocumentBuilder()
                                 .SetKey("namespace2", "uri/5")
@@ -1041,8 +1083,9 @@ TEST_F(ResultRetrieverV2GroupResultLimiterTest,
                                 .SetScore(5)
                                 .SetCreationTimestampMs(1000)
                                 .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id5,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result5,
                              document_store_->Put(document5));
+  DocumentId document_id5 = put_result5.new_document_id;
 
   std::vector<ScoredDocumentHit> scored_document_hits = {
       ScoredDocumentHit(document_id1, kSectionIdMaskNone, document1.score()),
diff --git a/icing/result/result-retriever-v2_projection_test.cc b/icing/result/result-retriever-v2_projection_test.cc
index 21da8a1..f846ef6 100644
--- a/icing/result/result-retriever-v2_projection_test.cc
+++ b/icing/result/result-retriever-v2_projection_test.cc
@@ -265,8 +265,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionTopLevelLeadNodeFieldPath) {
           .AddStringProperty(
               "body", "Oh what a beautiful morning! Oh what a beautiful day!")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -277,8 +278,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionTopLevelLeadNodeFieldPath) {
           .AddStringProperty("body",
                              "Count all the sheep and tell them 'Hello'.")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
@@ -359,8 +361,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionNestedLeafNodeFieldPath) {
           .AddStringProperty(
               "body", "Oh what a beautiful morning! Oh what a beautiful day!")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -378,8 +381,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionNestedLeafNodeFieldPath) {
           .AddStringProperty("body",
                              "Count all the sheep and tell them 'Hello'.")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
@@ -471,8 +475,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionIntermediateNodeFieldPath) {
           .AddStringProperty(
               "body", "Oh what a beautiful morning! Oh what a beautiful day!")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -490,8 +495,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionIntermediateNodeFieldPath) {
           .AddStringProperty("body",
                              "Count all the sheep and tell them 'Hello'.")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
@@ -586,8 +592,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionMultipleNestedFieldPaths) {
           .AddStringProperty(
               "body", "Oh what a beautiful morning! Oh what a beautiful day!")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -605,8 +612,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionMultipleNestedFieldPaths) {
           .AddStringProperty("body",
                              "Count all the sheep and tell them 'Hello'.")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
@@ -694,8 +702,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionEmptyFieldPath) {
           .AddStringProperty(
               "body", "Oh what a beautiful morning! Oh what a beautiful day!")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -706,8 +715,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionEmptyFieldPath) {
           .AddStringProperty("body",
                              "Count all the sheep and tell them 'Hello'.")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
@@ -775,8 +785,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionInvalidFieldPath) {
           .AddStringProperty(
               "body", "Oh what a beautiful morning! Oh what a beautiful day!")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -787,8 +798,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionInvalidFieldPath) {
           .AddStringProperty("body",
                              "Count all the sheep and tell them 'Hello'.")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
@@ -857,8 +869,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionValidAndInvalidFieldPath) {
           .AddStringProperty(
               "body", "Oh what a beautiful morning! Oh what a beautiful day!")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -869,8 +882,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionValidAndInvalidFieldPath) {
           .AddStringProperty("body",
                              "Count all the sheep and tell them 'Hello'.")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
@@ -944,8 +958,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionMultipleTypesNoWildcards) {
           .AddStringProperty(
               "body", "Oh what a beautiful morning! Oh what a beautiful day!")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -955,8 +970,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionMultipleTypesNoWildcards) {
           .AddStringProperty("name", "Joe Fox")
           .AddStringProperty("emailAddress", "ny152@aol.com")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
@@ -1031,8 +1047,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionMultipleTypesWildcard) {
           .AddStringProperty(
               "body", "Oh what a beautiful morning! Oh what a beautiful day!")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -1042,8 +1059,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionMultipleTypesWildcard) {
           .AddStringProperty("name", "Joe Fox")
           .AddStringProperty("emailAddress", "ny152@aol.com")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
@@ -1120,8 +1138,9 @@ TEST_F(ResultRetrieverV2ProjectionTest,
           .AddStringProperty(
               "body", "Oh what a beautiful morning! Oh what a beautiful day!")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -1131,8 +1150,9 @@ TEST_F(ResultRetrieverV2ProjectionTest,
           .AddStringProperty("name", "Joe Fox")
           .AddStringProperty("emailAddress", "ny152@aol.com")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
@@ -1222,8 +1242,9 @@ TEST_F(ResultRetrieverV2ProjectionTest,
                   .AddStringProperty("emailAddress", "mr.body123@gmail.com")
                   .Build())
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -1233,8 +1254,9 @@ TEST_F(ResultRetrieverV2ProjectionTest,
           .AddStringProperty("name", "Joe Fox")
           .AddStringProperty("emailAddress", "ny152@aol.com")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
@@ -1328,8 +1350,9 @@ TEST_F(ResultRetrieverV2ProjectionTest,
                   .AddStringProperty("emailAddress", "mr.body123@gmail.com")
                   .Build())
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -1339,8 +1362,9 @@ TEST_F(ResultRetrieverV2ProjectionTest,
           .AddStringProperty("name", "Joe Fox")
           .AddStringProperty("emailAddress", "ny152@aol.com")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
@@ -1422,8 +1446,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionJoinDocuments) {
           .AddStringProperty("name", "Joe Fox")
           .AddStringProperty("emailAddress", "ny152@aol.com")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId person_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(person_document));
+  DocumentId person_document_id = put_result1.new_document_id;
 
   // 2. Add two Email documents
   DocumentProto email_document1 =
@@ -1435,8 +1460,10 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionJoinDocuments) {
           .AddStringProperty(
               "body", "Oh what a beautiful morning! Oh what a beautiful day!")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id1,
+
+  ICING_ASSERT_OK_AND_ASSIGN(put_result1,
                              document_store_->Put(email_document1));
+  DocumentId email_document_id1 = put_result1.new_document_id;
 
   DocumentProto email_document2 =
       DocumentBuilder()
@@ -1447,8 +1474,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionJoinDocuments) {
           .AddStringProperty("body",
                              "Count all the sheep and tell them 'Hello'.")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(email_document2));
+  DocumentId email_document_id2 = put_result2.new_document_id;
 
   // 3. Setup the joined scored results.
   std::vector<SectionId> person_hit_section_ids = {
@@ -1568,8 +1596,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionPolymorphism) {
           .AddStringProperty("name", "Joe Fox")
           .AddStringProperty("emailAddress", "ny152@aol.com")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -1579,8 +1608,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionPolymorphism) {
           .AddStringProperty("name", "Joe Artist")
           .AddStringProperty("emailAddress", "artist@aol.com")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<ScoredDocumentHit> scored_document_hits = {
@@ -1653,8 +1683,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionTransitivePolymorphism) {
           .AddStringProperty("name", "Joe Fox")
           .AddStringProperty("emailAddress", "ny152@aol.com")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -1664,8 +1695,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionTransitivePolymorphism) {
           .AddStringProperty("name", "Joe Musician")
           .AddStringProperty("emailAddress", "Musician@aol.com")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<ScoredDocumentHit> scored_document_hits = {
@@ -1738,8 +1770,9 @@ TEST_F(ResultRetrieverV2ProjectionTest,
                                .SetSchema("Artist")
                                .AddStringProperty("name", "Joe Artist")
                                .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(document));
+  DocumentId document_id = put_result.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<ScoredDocumentHit> scored_document_hits = {
@@ -1798,8 +1831,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionPolymorphismMerge) {
           .AddStringProperty("name", "Joe Fox")
           .AddStringProperty("emailAddress", "ny152@aol.com")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(document_one));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   DocumentProto document_two =
       DocumentBuilder()
@@ -1809,8 +1843,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionPolymorphismMerge) {
           .AddStringProperty("name", "Joe Artist")
           .AddStringProperty("emailAddress", "artist@aol.com")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(document_two));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<ScoredDocumentHit> scored_document_hits = {
@@ -1891,8 +1926,9 @@ TEST_F(ResultRetrieverV2ProjectionTest, ProjectionMultipleParentPolymorphism) {
                                .AddStringProperty("phoneNumber", "12345")
                                .AddStringProperty("phoneModel", "pixel")
                                .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(document));
+  DocumentId document_id = put_result.new_document_id;
 
   // 2. Setup the scored results.
   std::vector<ScoredDocumentHit> scored_document_hits = {
diff --git a/icing/result/result-retriever-v2_snippet_test.cc b/icing/result/result-retriever-v2_snippet_test.cc
index 1c598f5..7bad0d8 100644
--- a/icing/result/result-retriever-v2_snippet_test.cc
+++ b/icing/result/result-retriever-v2_snippet_test.cc
@@ -209,14 +209,17 @@ ResultSpecProto CreateResultSpec(int num_per_page) {
 TEST_F(ResultRetrieverV2SnippetTest,
        DefaultSnippetSpecShouldDisableSnippeting) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(CreateEmailDocument(/*id=*/1)));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(CreateEmailDocument(/*id=*/2)));
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id3,
+      DocumentStore::PutResult put_result3,
       document_store_->Put(CreateEmailDocument(/*id=*/3)));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "subject"),
                                             GetSectionId("Email", "body")};
@@ -259,14 +262,17 @@ TEST_F(ResultRetrieverV2SnippetTest,
 
 TEST_F(ResultRetrieverV2SnippetTest, SimpleSnippeted) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(CreateEmailDocument(/*id=*/1)));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(CreateEmailDocument(/*id=*/2)));
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id3,
+      DocumentStore::PutResult put_result3,
       document_store_->Put(CreateEmailDocument(/*id=*/3)));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "subject"),
                                             GetSectionId("Email", "body")};
@@ -369,14 +375,17 @@ TEST_F(ResultRetrieverV2SnippetTest, SimpleSnippeted) {
 
 TEST_F(ResultRetrieverV2SnippetTest, OnlyOneDocumentSnippeted) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(CreateEmailDocument(/*id=*/1)));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(CreateEmailDocument(/*id=*/2)));
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id3,
+      DocumentStore::PutResult put_result3,
       document_store_->Put(CreateEmailDocument(/*id=*/3)));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "subject"),
                                             GetSectionId("Email", "body")};
@@ -448,14 +457,17 @@ TEST_F(ResultRetrieverV2SnippetTest, OnlyOneDocumentSnippeted) {
 
 TEST_F(ResultRetrieverV2SnippetTest, ShouldSnippetAllResults) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(CreateEmailDocument(/*id=*/1)));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(CreateEmailDocument(/*id=*/2)));
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id3,
+      DocumentStore::PutResult put_result3,
       document_store_->Put(CreateEmailDocument(/*id=*/3)));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "subject"),
                                             GetSectionId("Email", "body")};
@@ -504,14 +516,17 @@ TEST_F(ResultRetrieverV2SnippetTest, ShouldSnippetAllResults) {
 
 TEST_F(ResultRetrieverV2SnippetTest, ShouldSnippetSomeResults) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(CreateEmailDocument(/*id=*/1)));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(CreateEmailDocument(/*id=*/2)));
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id3,
+      DocumentStore::PutResult put_result3,
       document_store_->Put(CreateEmailDocument(/*id=*/3)));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "subject"),
                                             GetSectionId("Email", "body")};
@@ -563,14 +578,17 @@ TEST_F(ResultRetrieverV2SnippetTest, ShouldSnippetSomeResults) {
 
 TEST_F(ResultRetrieverV2SnippetTest, ShouldNotSnippetAnyResults) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(CreateEmailDocument(/*id=*/1)));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(CreateEmailDocument(/*id=*/2)));
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id3,
+      DocumentStore::PutResult put_result3,
       document_store_->Put(CreateEmailDocument(/*id=*/3)));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "subject"),
                                             GetSectionId("Email", "body")};
@@ -624,14 +642,17 @@ TEST_F(ResultRetrieverV2SnippetTest, ShouldNotSnippetAnyResults) {
 TEST_F(ResultRetrieverV2SnippetTest,
        ShouldNotSnippetAnyResultsForNonPositiveNumMatchesPerProperty) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(CreateEmailDocument(/*id=*/1)));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(CreateEmailDocument(/*id=*/2)));
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id3,
+      DocumentStore::PutResult put_result3,
       document_store_->Put(CreateEmailDocument(/*id=*/3)));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "subject"),
                                             GetSectionId("Email", "body")};
@@ -686,24 +707,30 @@ TEST_F(ResultRetrieverV2SnippetTest,
 
 TEST_F(ResultRetrieverV2SnippetTest, JoinSnippeted) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId person_document_id1,
+      DocumentStore::PutResult person_put_result1,
       document_store_->Put(CreatePersonDocument(/*id=*/1)));
+  DocumentId person_document_id1 = person_put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId person_document_id2,
+      DocumentStore::PutResult person_put_result2,
       document_store_->Put(CreatePersonDocument(/*id=*/2)));
+  DocumentId person_document_id2 = person_put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId person_document_id3,
+      DocumentStore::PutResult person_put_result3,
       document_store_->Put(CreatePersonDocument(/*id=*/3)));
+  DocumentId person_document_id3 = person_put_result3.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId email_document_id1,
+      DocumentStore::PutResult email_put_result1,
       document_store_->Put(CreateEmailDocument(/*id=*/1)));
+  DocumentId email_document_id1 = email_put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId email_document_id2,
+      DocumentStore::PutResult email_put_result2,
       document_store_->Put(CreateEmailDocument(/*id=*/2)));
+  DocumentId email_document_id2 = email_put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId email_document_id3,
+      DocumentStore::PutResult email_put_result3,
       document_store_->Put(CreateEmailDocument(/*id=*/3)));
+  DocumentId email_document_id3 = email_put_result3.new_document_id;
 
   std::vector<SectionId> person_hit_section_ids = {
       GetSectionId("Person", "name")};
@@ -920,21 +947,26 @@ TEST_F(ResultRetrieverV2SnippetTest, JoinSnippeted) {
 
 TEST_F(ResultRetrieverV2SnippetTest, ShouldSnippetAllJoinedResults) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId person_document_id1,
+      DocumentStore::PutResult person_put_result1,
       document_store_->Put(CreatePersonDocument(/*id=*/1)));
+  DocumentId person_document_id1 = person_put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId person_document_id2,
+      DocumentStore::PutResult person_put_result2,
       document_store_->Put(CreatePersonDocument(/*id=*/2)));
+  DocumentId person_document_id2 = person_put_result2.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId email_document_id1,
+      DocumentStore::PutResult email_put_result1,
       document_store_->Put(CreateEmailDocument(/*id=*/1)));
+  DocumentId email_document_id1 = email_put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId email_document_id2,
+      DocumentStore::PutResult email_put_result2,
       document_store_->Put(CreateEmailDocument(/*id=*/2)));
+  DocumentId email_document_id2 = email_put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId email_document_id3,
+      DocumentStore::PutResult email_put_result3,
       document_store_->Put(CreateEmailDocument(/*id=*/3)));
+  DocumentId email_document_id3 = email_put_result3.new_document_id;
 
   std::vector<SectionId> person_hit_section_ids = {
       GetSectionId("Person", "name")};
@@ -1039,21 +1071,26 @@ TEST_F(ResultRetrieverV2SnippetTest, ShouldSnippetAllJoinedResults) {
 
 TEST_F(ResultRetrieverV2SnippetTest, ShouldSnippetSomeJoinedResults) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId person_document_id1,
+      DocumentStore::PutResult person_put_result1,
       document_store_->Put(CreatePersonDocument(/*id=*/1)));
+  DocumentId person_document_id1 = person_put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId person_document_id2,
+      DocumentStore::PutResult person_put_result2,
       document_store_->Put(CreatePersonDocument(/*id=*/2)));
+  DocumentId person_document_id2 = person_put_result2.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId email_document_id1,
+      DocumentStore::PutResult email_put_result1,
       document_store_->Put(CreateEmailDocument(/*id=*/1)));
+  DocumentId email_document_id1 = email_put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId email_document_id2,
+      DocumentStore::PutResult email_put_result2,
       document_store_->Put(CreateEmailDocument(/*id=*/2)));
+  DocumentId email_document_id2 = email_put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId email_document_id3,
+      DocumentStore::PutResult email_put_result3,
       document_store_->Put(CreateEmailDocument(/*id=*/3)));
+  DocumentId email_document_id3 = email_put_result3.new_document_id;
 
   std::vector<SectionId> person_hit_section_ids = {
       GetSectionId("Person", "name")};
diff --git a/icing/result/result-retriever-v2_test.cc b/icing/result/result-retriever-v2_test.cc
index a86dc21..4ae4e4c 100644
--- a/icing/result/result-retriever-v2_test.cc
+++ b/icing/result/result-retriever-v2_test.cc
@@ -261,16 +261,21 @@ TEST_F(ResultRetrieverV2Test, ShouldRetrieveSimpleResults) {
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store->Put(CreateDocument(/*id=*/1)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              doc_store->Put(CreateDocument(/*id=*/2)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              doc_store->Put(CreateDocument(/*id=*/3)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id4,
+  DocumentId document_id3 = put_result3.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
                              doc_store->Put(CreateDocument(/*id=*/4)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id5,
+  DocumentId document_id4 = put_result4.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result5,
                              doc_store->Put(CreateDocument(/*id=*/5)));
+  DocumentId document_id5 = put_result5.new_document_id;
 
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
                                             GetSectionId("Email", "body")};
@@ -354,10 +359,12 @@ TEST_F(ResultRetrieverV2Test, ShouldIgnoreNonInternalErrors) {
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store->Put(CreateDocument(/*id=*/1)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              doc_store->Put(CreateDocument(/*id=*/2)));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   DocumentId invalid_document_id = -1;
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
@@ -436,8 +443,9 @@ TEST_F(ResultRetrieverV2Test,
           .AddStringProperty("name", "Joe Fox")
           .AddStringProperty("emailAddress", "ny152@aol.com")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId person_document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store->Put(person_document1));
+  DocumentId person_document_id1 = put_result1.new_document_id;
 
   DocumentProto person_document2 =
       DocumentBuilder()
@@ -447,8 +455,9 @@ TEST_F(ResultRetrieverV2Test,
           .AddStringProperty("name", "Meg Ryan")
           .AddStringProperty("emailAddress", "shopgirl@aol.com")
           .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId person_document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              doc_store->Put(person_document2));
+  DocumentId person_document_id2 = put_result2.new_document_id;
 
   // 2. Add 4 Email documents
   DocumentProto email_document1 = DocumentBuilder()
@@ -458,8 +467,8 @@ TEST_F(ResultRetrieverV2Test,
                                       .AddStringProperty("name", "Test 1")
                                       .AddStringProperty("body", "Test 1")
                                       .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id1,
-                             doc_store->Put(email_document1));
+  ICING_ASSERT_OK_AND_ASSIGN(put_result1, doc_store->Put(email_document1));
+  DocumentId email_document_id1 = put_result1.new_document_id;
 
   DocumentProto email_document2 = DocumentBuilder()
                                       .SetKey("namespace", "Email/2")
@@ -468,8 +477,8 @@ TEST_F(ResultRetrieverV2Test,
                                       .AddStringProperty("name", "Test 2")
                                       .AddStringProperty("body", "Test 2")
                                       .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id2,
-                             doc_store->Put(email_document2));
+  ICING_ASSERT_OK_AND_ASSIGN(put_result2, doc_store->Put(email_document2));
+  DocumentId email_document_id2 = put_result2.new_document_id;
 
   DocumentProto email_document3 = DocumentBuilder()
                                       .SetKey("namespace", "Email/3")
@@ -478,8 +487,9 @@ TEST_F(ResultRetrieverV2Test,
                                       .AddStringProperty("name", "Test 3")
                                       .AddStringProperty("body", "Test 3")
                                       .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id3,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              doc_store->Put(email_document3));
+  DocumentId email_document_id3 = put_result3.new_document_id;
 
   DocumentProto email_document4 = DocumentBuilder()
                                       .SetKey("namespace", "Email/4")
@@ -488,8 +498,9 @@ TEST_F(ResultRetrieverV2Test,
                                       .AddStringProperty("name", "Test 4")
                                       .AddStringProperty("body", "Test 4")
                                       .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id4,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
                              doc_store->Put(email_document4));
+  DocumentId email_document_id4 = put_result4.new_document_id;
 
   // 3. Setup the joined scored results.
   std::vector<SectionId> person_hit_section_ids = {
@@ -585,10 +596,12 @@ TEST_F(ResultRetrieverV2Test, ShouldIgnoreInternalErrors) {
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store->Put(CreateDocument(/*id=*/1)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              doc_store->Put(CreateDocument(/*id=*/2)));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
                                             GetSectionId("Email", "body")};
@@ -633,16 +646,21 @@ TEST_F(ResultRetrieverV2Test, ShouldUpdateResultState) {
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store->Put(CreateDocument(/*id=*/1)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              doc_store->Put(CreateDocument(/*id=*/2)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              doc_store->Put(CreateDocument(/*id=*/3)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id4,
+  DocumentId document_id3 = put_result3.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
                              doc_store->Put(CreateDocument(/*id=*/4)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id5,
+  DocumentId document_id4 = put_result4.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result5,
                              doc_store->Put(CreateDocument(/*id=*/5)));
+  DocumentId document_id5 = put_result5.new_document_id;
 
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
                                             GetSectionId("Email", "body")};
@@ -731,10 +749,12 @@ TEST_F(ResultRetrieverV2Test, ShouldUpdateNumTotalHits) {
                                             GetSectionId("Email", "body")};
   SectionIdMask hit_section_id_mask = CreateSectionIdMask(hit_section_ids);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store->Put(CreateDocument(/*id=*/1)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              doc_store->Put(CreateDocument(/*id=*/2)));
+  DocumentId document_id2 = put_result2.new_document_id;
   std::vector<ScoredDocumentHit> scored_document_hits1 = {
       {document_id1, hit_section_id_mask, /*score=*/0},
       {document_id2, hit_section_id_mask, /*score=*/0}};
@@ -754,12 +774,15 @@ TEST_F(ResultRetrieverV2Test, ShouldUpdateNumTotalHits) {
     ASSERT_THAT(num_total_hits_, Eq(2));
   }
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              doc_store->Put(CreateDocument(/*id=*/3)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id4,
+  DocumentId document_id3 = put_result3.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
                              doc_store->Put(CreateDocument(/*id=*/4)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id5,
+  DocumentId document_id4 = put_result4.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result5,
                              doc_store->Put(CreateDocument(/*id=*/5)));
+  DocumentId document_id5 = put_result5.new_document_id;
   std::vector<ScoredDocumentHit> scored_document_hits2 = {
       {document_id3, hit_section_id_mask, /*score=*/0},
       {document_id4, hit_section_id_mask, /*score=*/0},
@@ -835,10 +858,12 @@ TEST_F(ResultRetrieverV2Test, ShouldLimitNumTotalBytesPerPage) {
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store->Put(CreateDocument(/*id=*/1)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              doc_store->Put(CreateDocument(/*id=*/2)));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
                                             GetSectionId("Email", "body")};
@@ -895,10 +920,12 @@ TEST_F(ResultRetrieverV2Test,
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store->Put(CreateDocument(/*id=*/1)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              doc_store->Put(CreateDocument(/*id=*/2)));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
                                             GetSectionId("Email", "body")};
@@ -957,10 +984,12 @@ TEST_F(ResultRetrieverV2Test,
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store->Put(CreateDocument(/*id=*/1)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              doc_store->Put(CreateDocument(/*id=*/2)));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   std::vector<SectionId> hit_section_ids = {GetSectionId("Email", "name"),
                                             GetSectionId("Email", "body")};
diff --git a/icing/result/result-state-manager_test.cc b/icing/result/result-state-manager_test.cc
index 6b75858..b06eb94 100644
--- a/icing/result/result-state-manager_test.cc
+++ b/icing/result/result-state-manager_test.cc
@@ -184,12 +184,15 @@ class ResultStateManagerTest : public testing::Test {
 };
 
 TEST_F(ResultStateManagerTest, ShouldCacheAndRetrieveFirstPageOnePage) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store().Put(CreateDocument(/*id=*/1)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store().Put(CreateDocument(/*id=*/2)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store().Put(CreateDocument(/*id=*/3)));
+  DocumentId document_id3 = put_result3.new_document_id;
   std::vector<ScoredDocumentHit> scored_document_hits = {
       {document_id1, kSectionIdMaskNone, /*score=*/1},
       {document_id2, kSectionIdMaskNone, /*score=*/1},
@@ -223,16 +226,21 @@ TEST_F(ResultStateManagerTest, ShouldCacheAndRetrieveFirstPageOnePage) {
 }
 
 TEST_F(ResultStateManagerTest, ShouldCacheAndRetrieveFirstPageMultiplePages) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store().Put(CreateDocument(/*id=*/1)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store().Put(CreateDocument(/*id=*/2)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store().Put(CreateDocument(/*id=*/3)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id4,
+  DocumentId document_id3 = put_result3.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
                              document_store().Put(CreateDocument(/*id=*/4)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id5,
+  DocumentId document_id4 = put_result4.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result5,
                              document_store().Put(CreateDocument(/*id=*/5)));
+  DocumentId document_id5 = put_result5.new_document_id;
   std::vector<ScoredDocumentHit> scored_document_hits = {
       {document_id1, kSectionIdMaskNone, /*score=*/1},
       {document_id2, kSectionIdMaskNone, /*score=*/1},
@@ -326,10 +334,12 @@ TEST_F(ResultStateManagerTest, EmptyRankerShouldReturnEmptyFirstPage) {
 }
 
 TEST_F(ResultStateManagerTest, ShouldAllowEmptyFirstPage) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store().Put(CreateDocument(/*id=*/1)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store().Put(CreateDocument(/*id=*/2)));
+  DocumentId document_id2 = put_result2.new_document_id;
   std::vector<ScoredDocumentHit> scored_document_hits = {
       {document_id1, kSectionIdMaskNone, /*score=*/1},
       {document_id2, kSectionIdMaskNone, /*score=*/1}};
@@ -363,14 +373,18 @@ TEST_F(ResultStateManagerTest, ShouldAllowEmptyFirstPage) {
 }
 
 TEST_F(ResultStateManagerTest, ShouldAllowEmptyLastPage) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store().Put(CreateDocument(/*id=*/1)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store().Put(CreateDocument(/*id=*/2)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store().Put(CreateDocument(/*id=*/3)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id4,
+  DocumentId document_id3 = put_result3.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
                              document_store().Put(CreateDocument(/*id=*/4)));
+  DocumentId document_id4 = put_result4.new_document_id;
   std::vector<ScoredDocumentHit> scored_document_hits = {
       {document_id1, kSectionIdMaskNone, /*score=*/1},
       {document_id2, kSectionIdMaskNone, /*score=*/1},
@@ -574,18 +588,24 @@ TEST_F(ResultStateManagerTest,
 }
 
 TEST_F(ResultStateManagerTest, ShouldInvalidateOneToken) {
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store().Put(CreateDocument(/*id=*/1)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store().Put(CreateDocument(/*id=*/2)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store().Put(CreateDocument(/*id=*/3)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id4,
+  DocumentId document_id3 = put_result3.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result4,
                              document_store().Put(CreateDocument(/*id=*/4)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id5,
+  DocumentId document_id4 = put_result4.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result5,
                              document_store().Put(CreateDocument(/*id=*/5)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id6,
+  DocumentId document_id5 = put_result5.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result6,
                              document_store().Put(CreateDocument(/*id=*/6)));
+  DocumentId document_id6 = put_result6.new_document_id;
   std::vector<ScoredDocumentHit> scored_document_hits1 = {
       {document_id1, kSectionIdMaskNone, /*score=*/1},
       {document_id2, kSectionIdMaskNone, /*score=*/1},
diff --git a/icing/result/snippet-retriever.cc b/icing/result/snippet-retriever.cc
index fcaba4c..9b10c33 100644
--- a/icing/result/snippet-retriever.cc
+++ b/icing/result/snippet-retriever.cc
@@ -101,6 +101,8 @@ std::string NormalizeToken(const Normalizer& normalizer, const Token& token) {
       [[fallthrough]];
     case Token::Type::URL_SUFFIX_INNERMOST:
       [[fallthrough]];
+    case Token::Type::TRIGRAM:
+      [[fallthrough]];
     case Token::Type::REGULAR:
       return normalizer.NormalizeTerm(token.text);
     case Token::Type::VERBATIM:
@@ -190,6 +192,8 @@ CharacterIterator FindMatchEnd(const Normalizer& normalizer, const Token& token,
       [[fallthrough]];
     case Token::Type::URL_SUFFIX_INNERMOST:
       [[fallthrough]];
+    case Token::Type::TRIGRAM:
+      [[fallthrough]];
     case Token::Type::REGULAR:
       return normalizer.FindNormalizedMatchEndPosition(token.text,
                                                        match_query_term);
diff --git a/icing/schema-builder.h b/icing/schema-builder.h
index c702afe..9387440 100644
--- a/icing/schema-builder.h
+++ b/icing/schema-builder.h
@@ -50,6 +50,7 @@ constexpr StringIndexingConfig::TokenizerType::Code TOKENIZER_URL =
 constexpr TermMatchType::Code TERM_MATCH_UNKNOWN = TermMatchType::UNKNOWN;
 constexpr TermMatchType::Code TERM_MATCH_EXACT = TermMatchType::EXACT_ONLY;
 constexpr TermMatchType::Code TERM_MATCH_PREFIX = TermMatchType::PREFIX;
+constexpr TermMatchType::Code TERM_MATCH_STEMMING = TermMatchType::STEMMING;
 
 constexpr IntegerIndexingConfig::NumericMatchType::Code NUMERIC_MATCH_UNKNOWN =
     IntegerIndexingConfig::NumericMatchType::UNKNOWN;
@@ -79,6 +80,8 @@ constexpr PropertyConfigProto::DataType::Code TYPE_DOCUMENT =
     PropertyConfigProto::DataType::DOCUMENT;
 constexpr PropertyConfigProto::DataType::Code TYPE_VECTOR =
     PropertyConfigProto::DataType::VECTOR;
+constexpr PropertyConfigProto::DataType::Code TYPE_BLOB_HANDLE =
+    PropertyConfigProto::DataType::BLOB_HANDLE;
 
 constexpr JoinableConfig::ValueType::Code JOINABLE_VALUE_TYPE_NONE =
     JoinableConfig::ValueType::NONE;
diff --git a/icing/schema/property-util.cc b/icing/schema/property-util.cc
index 7c86122..4c07306 100644
--- a/icing/schema/property-util.cc
+++ b/icing/schema/property-util.cc
@@ -141,6 +141,15 @@ ExtractPropertyValues<PropertyProto::VectorProto>(
       property.vector_values().begin(), property.vector_values().end());
 }
 
+template <>
+libtextclassifier3::StatusOr<std::vector<PropertyProto::BlobHandleProto>>
+ExtractPropertyValues<PropertyProto::BlobHandleProto>(
+    const PropertyProto& property) {
+  return std::vector<PropertyProto::BlobHandleProto>(
+      property.blob_handle_values().begin(),
+      property.blob_handle_values().end());
+}
+
 }  // namespace property_util
 
 }  // namespace lib
diff --git a/icing/schema/property-util.h b/icing/schema/property-util.h
index c409a9d..55e4f20 100644
--- a/icing/schema/property-util.h
+++ b/icing/schema/property-util.h
@@ -171,6 +171,11 @@ libtextclassifier3::StatusOr<std::vector<PropertyProto::VectorProto>>
 ExtractPropertyValues<PropertyProto::VectorProto>(
     const PropertyProto& property);
 
+template <>
+libtextclassifier3::StatusOr<std::vector<PropertyProto::BlobHandleProto>>
+ExtractPropertyValues<PropertyProto::BlobHandleProto>(
+    const PropertyProto& property);
+
 template <typename T>
 libtextclassifier3::StatusOr<std::vector<T>> ExtractPropertyValuesFromDocument(
     const DocumentProto& document, std::string_view property_path) {
diff --git a/icing/schema/property-util_test.cc b/icing/schema/property-util_test.cc
index 5e8a430..1bfd5cd 100644
--- a/icing/schema/property-util_test.cc
+++ b/icing/schema/property-util_test.cc
@@ -17,6 +17,7 @@
 #include <cstdint>
 #include <string>
 #include <string_view>
+#include <vector>
 
 #include "icing/text_classifier/lib3/utils/base/status.h"
 #include "gmock/gmock.h"
@@ -109,6 +110,29 @@ TEST(PropertyUtilTest, ExtractPropertyValuesTypeVector) {
       IsOkAndHolds(ElementsAre(EqualsProto(vector1), EqualsProto(vector2))));
 }
 
+TEST(PropertyUtilTest, ExtractPropertyValuesTypeBlobHandle) {
+  PropertyProto::BlobHandleProto blob_handle1;
+  blob_handle1.set_label("label1");
+  std::vector<unsigned char> bytes1(32);
+  std::string digestString1 = std::string(bytes1.begin(), bytes1.end());
+  blob_handle1.set_digest(digestString1);
+  PropertyProto::BlobHandleProto blob_handle2;
+  blob_handle2.set_label("label2");
+  std::vector<unsigned char> bytes2(32);
+  std::string digestString2 = std::string(bytes2.begin(), bytes2.end());
+  blob_handle2.set_digest(digestString2);
+
+  PropertyProto property;
+  *property.mutable_blob_handle_values()->Add() = blob_handle1;
+  *property.mutable_blob_handle_values()->Add() = blob_handle2;
+
+  EXPECT_THAT(
+      property_util::ExtractPropertyValues<PropertyProto::BlobHandleProto>(
+          property),
+      IsOkAndHolds(
+          ElementsAre(EqualsProto(blob_handle1), EqualsProto(blob_handle2))));
+}
+
 TEST(PropertyUtilTest, ExtractPropertyValuesMismatchedType) {
   PropertyProto property;
   property.mutable_int64_values()->Add(123);
diff --git a/icing/schema/schema-store.cc b/icing/schema/schema-store.cc
index 6830787..2b04adb 100644
--- a/icing/schema/schema-store.cc
+++ b/icing/schema/schema-store.cc
@@ -14,7 +14,6 @@
 
 #include "icing/schema/schema-store.h"
 
-#include <algorithm>
 #include <cinttypes>
 #include <cstdint>
 #include <limits>
@@ -34,6 +33,7 @@
 #include "icing/file/file-backed-proto.h"
 #include "icing/file/filesystem.h"
 #include "icing/file/version-util.h"
+#include "icing/legacy/core/icing-string-util.h"
 #include "icing/proto/debug.pb.h"
 #include "icing/proto/document.pb.h"
 #include "icing/proto/logging.pb.h"
@@ -43,11 +43,13 @@
 #include "icing/schema/backup-schema-producer.h"
 #include "icing/schema/joinable-property.h"
 #include "icing/schema/property-util.h"
+#include "icing/schema/schema-property-iterator.h"
 #include "icing/schema/schema-type-manager.h"
 #include "icing/schema/schema-util.h"
 #include "icing/schema/section.h"
 #include "icing/store/document-filter-data.h"
 #include "icing/store/dynamic-trie-key-mapper.h"
+#include "icing/util/clock.h"
 #include "icing/util/crc32.h"
 #include "icing/util/logging.h"
 #include "icing/util/status-macros.h"
@@ -121,20 +123,23 @@ std::unordered_set<SchemaTypeId> SchemaTypeIdsChanged(
 }  // namespace
 
 /* static */ libtextclassifier3::StatusOr<SchemaStore::Header>
-SchemaStore::Header::Read(const Filesystem* filesystem,
-                          const std::string& path) {
-  Header header;
-  ScopedFd sfd(filesystem->OpenForRead(path.c_str()));
+SchemaStore::Header::Read(const Filesystem* filesystem, std::string path) {
+  if (!filesystem->FileExists(path.c_str())) {
+    return absl_ports::NotFoundError(
+        absl_ports::StrCat("Header file is empty: ", path));
+  }
+
+  SerializedHeader serialized_header;
+  ScopedFd sfd(filesystem->OpenForWrite(path.c_str()));
   if (!sfd.is_valid()) {
-    return absl_ports::NotFoundError("SchemaStore header doesn't exist");
+    return absl_ports::InternalError("Unable to open or create header file.");
   }
 
   // If file is sizeof(LegacyHeader), then it must be LegacyHeader.
   int64_t file_size = filesystem->GetFileSize(sfd.get());
   if (file_size == sizeof(LegacyHeader)) {
     LegacyHeader legacy_header;
-    if (!filesystem->Read(path.c_str(), &legacy_header,
-                          sizeof(legacy_header))) {
+    if (!filesystem->Read(sfd.get(), &legacy_header, sizeof(legacy_header))) {
       return absl_ports::InternalError(
           absl_ports::StrCat("Couldn't read: ", path));
     }
@@ -142,35 +147,55 @@ SchemaStore::Header::Read(const Filesystem* filesystem,
       return absl_ports::InternalError(
           absl_ports::StrCat("Invalid header kMagic for file: ", path));
     }
-    header.set_checksum(legacy_header.checksum);
-  } else if (file_size == sizeof(Header)) {
-    if (!filesystem->Read(path.c_str(), &header, sizeof(header))) {
+    serialized_header.checksum = legacy_header.checksum;
+  } else if (file_size == sizeof(SerializedHeader)) {
+    if (!filesystem->Read(sfd.get(), &serialized_header,
+                          sizeof(serialized_header))) {
       return absl_ports::InternalError(
           absl_ports::StrCat("Couldn't read: ", path));
     }
-    if (header.magic() != Header::kMagic) {
+    if (serialized_header.magic != Header::kMagic) {
       return absl_ports::InternalError(
           absl_ports::StrCat("Invalid header kMagic for file: ", path));
     }
-  } else {
+  } else if (file_size != 0) {
+    // file is neither the legacy header, the new header nor empty. Something is
+    // wrong here.
     int legacy_header_size = sizeof(LegacyHeader);
-    int header_size = sizeof(Header);
+    int header_size = sizeof(SerializedHeader);
     return absl_ports::InternalError(IcingStringUtil::StringPrintf(
         "Unexpected header size %" PRId64 ". Expected %d or %d", file_size,
         legacy_header_size, header_size));
   }
-  return header;
+  return Header(serialized_header, std::move(path), std::move(sfd), filesystem);
 }
 
-libtextclassifier3::Status SchemaStore::Header::Write(
-    const Filesystem* filesystem, const std::string& path) {
-  ScopedFd scoped_fd(filesystem->OpenForWrite(path.c_str()));
+libtextclassifier3::Status SchemaStore::Header::Write() {
+  if (!dirty_) {
+    return libtextclassifier3::Status::OK;
+  }
+  if (!header_fd_.is_valid() && !filesystem_->FileExists(path_.c_str())) {
+    header_fd_.reset(filesystem_->OpenForWrite(path_.c_str()));
+  }
   // This should overwrite the header.
-  if (!scoped_fd.is_valid() ||
-      !filesystem->Write(scoped_fd.get(), this, sizeof(*this)) ||
-      !filesystem->DataSync(scoped_fd.get())) {
+  if (!header_fd_.is_valid() ||
+      !filesystem_->PWrite(header_fd_.get(), /*offset=*/0, &serialized_header_,
+                           sizeof(serialized_header_))) {
     return absl_ports::InternalError(
-        absl_ports::StrCat("Failed to write SchemaStore header: ", path));
+        absl_ports::StrCat("Failed to write SchemaStore header"));
+  }
+  dirty_ = false;
+  return libtextclassifier3::Status::OK;
+}
+
+libtextclassifier3::Status SchemaStore::Header::PersistToDisk() {
+  if (dirty_) {
+    ICING_RETURN_IF_ERROR(Write());
+  }
+  // This should overwrite the header.
+  if (!header_fd_.is_valid() || !filesystem_->DataSync(header_fd_.get())) {
+    return absl_ports::InternalError(
+        absl_ports::StrCat("Failed to sync SchemaStore header."));
   }
   return libtextclassifier3::Status::OK;
 }
@@ -213,9 +238,9 @@ libtextclassifier3::StatusOr<std::unique_ptr<SchemaStore>> SchemaStore::Create(
   if (header.overlay_created()) {
     header.SetOverlayInfo(
         /*overlay_created=*/false,
-        /*min_overlay_version_compatibility=*/ std::numeric_limits<
+        /*min_overlay_version_compatibility=*/std::numeric_limits<
             int32_t>::max());
-    ICING_RETURN_IF_ERROR(header.Write(filesystem, header_filename));
+    ICING_RETURN_IF_ERROR(header.Write());
   }
   std::string schema_overlay_filename = MakeOverlaySchemaFilename(base_dir);
   if (!filesystem->DeleteFile(schema_overlay_filename.c_str())) {
@@ -263,20 +288,17 @@ libtextclassifier3::StatusOr<std::unique_ptr<SchemaStore>> SchemaStore::Create(
       // fallthrough
     case version_util::StateChange::kCompatible:
       return libtextclassifier3::Status::OK;
-    case version_util::StateChange::kVersionZeroRollForward:
+    case version_util::StateChange::kVersionZeroRollForward: {
       // We've rolled forward. The schema overlay file, if it exists, is
       // possibly stale. We must throw it out.
       header_or = Header::Read(filesystem, header_filename);
-      if (!header_or.ok()) {
-        return header_or.status();
-      }
+      ICING_RETURN_IF_ERROR(header_or.status());
       return SchemaStore::DiscardOverlaySchema(filesystem, base_dir,
                                                header_or.ValueOrDie());
-    case version_util::StateChange::kRollBack:
+    }
+    case version_util::StateChange::kRollBack: {
       header_or = Header::Read(filesystem, header_filename);
-      if (!header_or.ok()) {
-        return header_or.status();
-      }
+      ICING_RETURN_IF_ERROR(header_or.status());
       if (header_or.ValueOrDie().min_overlay_version_compatibility() <=
           new_version) {
         // We've been rolled back, but the overlay schema claims that it
@@ -287,13 +309,12 @@ libtextclassifier3::StatusOr<std::unique_ptr<SchemaStore>> SchemaStore::Create(
       // support. We must throw it out.
       return SchemaStore::DiscardOverlaySchema(filesystem, base_dir,
                                                header_or.ValueOrDie());
+    }
     case version_util::StateChange::kUndetermined:
       // It's not clear what version we're on, but the base schema should always
       // be safe to use. Throw out the overlay.
       header_or = Header::Read(filesystem, header_filename);
-      if (!header_or.ok()) {
-        return header_or.status();
-      }
+      ICING_RETURN_IF_ERROR(header_or.status());
       return SchemaStore::DiscardOverlaySchema(filesystem, base_dir,
                                                header_or.ValueOrDie());
   }
@@ -359,7 +380,8 @@ libtextclassifier3::Status SchemaStore::LoadSchema() {
   if (!header_or.ok() && !absl_ports::IsNotFound(header_or.status())) {
     return header_or.status();
   } else if (!header_or.ok()) {
-    header_ = std::make_unique<Header>();
+    header_ =
+        std::make_unique<Header>(filesystem_, MakeHeaderFilename(base_dir_));
   } else {
     header_exists = true;
     header_ = std::make_unique<Header>(std::move(header_or).ValueOrDie());
@@ -442,8 +464,9 @@ libtextclassifier3::Status SchemaStore::InitializeDerivedFiles() {
           *filesystem_, MakeSchemaTypeMapperFilename(base_dir_),
           kSchemaTypeMapperMaxSize));
 
-  ICING_ASSIGN_OR_RETURN(Crc32 checksum, ComputeChecksum());
-  if (checksum.Get() != header_->checksum()) {
+  Crc32 expected_checksum(header_->checksum());
+  ICING_ASSIGN_OR_RETURN(Crc32 checksum, GetChecksum());
+  if (checksum != expected_checksum) {
     return absl_ports::InternalError(
         "Combined checksum of SchemaStore was inconsistent");
   }
@@ -499,9 +522,8 @@ libtextclassifier3::Status SchemaStore::RegenerateDerivedFiles(
   }
 
   // Write the header
-  ICING_ASSIGN_OR_RETURN(Crc32 checksum, ComputeChecksum());
-  header_->set_checksum(checksum.Get());
-  return header_->Write(filesystem_, MakeHeaderFilename(base_dir_));
+  ICING_RETURN_IF_ERROR(UpdateChecksum());
+  return libtextclassifier3::Status::OK;
 }
 
 libtextclassifier3::Status SchemaStore::BuildInMemoryCache() {
@@ -571,35 +593,63 @@ libtextclassifier3::Status SchemaStore::ResetSchemaTypeMapper() {
   return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::StatusOr<Crc32> SchemaStore::ComputeChecksum() const {
-  // Base schema checksum
-  auto schema_proto_or = schema_file_->Read();
-  if (absl_ports::IsNotFound(schema_proto_or.status())) {
-    return Crc32();
+libtextclassifier3::StatusOr<Crc32> SchemaStore::GetChecksum() const {
+  ICING_ASSIGN_OR_RETURN(Crc32 schema_checksum, schema_file_->GetChecksum());
+  // We've gotten the schema_checksum successfully. This means that
+  // schema_file_->Read() will only return either a schema or NOT_FOUND.
+  // Sadly, we actually need to differentiate between an existing, but empty
+  // schema and a non-existent schema (both of which will have a checksum of 0).
+  // For existing, but empty schemas, we need to continue with the checksum
+  // calculation of the other components.
+  if (schema_checksum == Crc32() &&
+      absl_ports::IsNotFound(schema_file_->Read().status())) {
+    return schema_checksum;
   }
-  ICING_ASSIGN_OR_RETURN(const SchemaProto* schema_proto, schema_proto_or);
-  Crc32 schema_checksum;
-  schema_checksum.Append(schema_proto->SerializeAsString());
 
-  Crc32 overlay_schema_checksum;
+  Crc32 total_checksum;
+  total_checksum.Append(std::to_string(schema_checksum.Get()));
   if (overlay_schema_file_ != nullptr) {
-    auto schema_proto_or = schema_file_->Read();
-    if (schema_proto_or.ok()) {
-      ICING_ASSIGN_OR_RETURN(schema_proto, schema_proto_or);
-      overlay_schema_checksum.Append(schema_proto->SerializeAsString());
-    }
+    ICING_ASSIGN_OR_RETURN(Crc32 overlay_schema_checksum,
+                           overlay_schema_file_->GetChecksum());
+    total_checksum.Append(std::to_string(overlay_schema_checksum.Get()));
   }
 
   ICING_ASSIGN_OR_RETURN(Crc32 schema_type_mapper_checksum,
-                         schema_type_mapper_->ComputeChecksum());
+                         schema_type_mapper_->GetChecksum());
+  total_checksum.Append(std::to_string(schema_type_mapper_checksum.Get()));
+  return total_checksum;
+}
 
+libtextclassifier3::StatusOr<Crc32> SchemaStore::UpdateChecksum() {
+  // FileBackedProto always keeps its checksum up to date. So we just need to
+  // retrieve the checksum.
+  ICING_ASSIGN_OR_RETURN(Crc32 schema_checksum, schema_file_->GetChecksum());
+  // We've gotten the schema_checksum successfully. This means that
+  // schema_file_->Read() will only return either a schema or NOT_FOUND.
+  // Sadly, we actually need to differentiate between an existing, but empty
+  // schema and a non-existent schema (both of which will have a checksum of 0).
+  // For existing, but empty schemas, we need to continue with the checksum
+  // calculation of the other components so that we will correctly write the
+  // header.
+  if (schema_checksum == Crc32() &&
+      absl_ports::IsNotFound(schema_file_->Read().status())) {
+    return schema_checksum;
+  }
   Crc32 total_checksum;
   total_checksum.Append(std::to_string(schema_checksum.Get()));
+
   if (overlay_schema_file_ != nullptr) {
+    ICING_ASSIGN_OR_RETURN(Crc32 overlay_schema_checksum,
+                           overlay_schema_file_->GetChecksum());
     total_checksum.Append(std::to_string(overlay_schema_checksum.Get()));
   }
+
+  ICING_ASSIGN_OR_RETURN(Crc32 schema_type_mapper_checksum,
+                         schema_type_mapper_->UpdateChecksum());
   total_checksum.Append(std::to_string(schema_type_mapper_checksum.Get()));
 
+  header_->set_checksum(total_checksum.Get());
+  ICING_RETURN_IF_ERROR(header_->Write());
   return total_checksum;
 }
 
@@ -627,15 +677,15 @@ libtextclassifier3::StatusOr<const SchemaStore::SetSchemaResult>
 SchemaStore::SetSchema(SchemaProto&& new_schema,
                        bool ignore_errors_and_delete_documents,
                        bool allow_circular_schema_definitions) {
-  ICING_ASSIGN_OR_RETURN(
-      SchemaUtil::DependentMap new_dependent_map,
-      SchemaUtil::Validate(new_schema, allow_circular_schema_definitions));
-
   SetSchemaResult result;
 
   auto schema_proto_or = GetSchema();
   if (absl_ports::IsNotFound(schema_proto_or.status())) {
-    // We don't have a pre-existing schema, so anything is valid.
+    // We don't have a pre-existing schema, so anything is valid as long as the
+    // new schema is valid.
+    ICING_RETURN_IF_ERROR(
+        SchemaUtil::Validate(new_schema, allow_circular_schema_definitions));
+
     result.success = true;
     for (const SchemaTypeConfigProto& type_config : new_schema.types()) {
       result.schema_types_new_by_name.insert(type_config.schema_type());
@@ -655,7 +705,11 @@ SchemaStore::SetSchema(SchemaProto&& new_schema,
       return result;
     }
 
-    // Different schema, track the differences and see if we can still write it
+    // Different schema -- validate the new schema, track the differences and
+    // see if we can still write it
+    ICING_ASSIGN_OR_RETURN(
+        SchemaUtil::DependentMap new_dependent_map,
+        SchemaUtil::Validate(new_schema, allow_circular_schema_definitions));
     SchemaUtil::SchemaDelta schema_delta =
         SchemaUtil::ComputeCompatibilityDelta(old_schema, new_schema,
                                               new_dependent_map);
@@ -781,7 +835,7 @@ libtextclassifier3::StatusOr<SchemaTypeId> SchemaStore::GetSchemaTypeId(
 }
 
 libtextclassifier3::StatusOr<const std::string*> SchemaStore::GetSchemaType(
-      SchemaTypeId schema_type_id) const {
+    SchemaTypeId schema_type_id) const {
   ICING_RETURN_IF_ERROR(CheckSchemaSet());
   if (const auto it = reverse_schema_type_mapper_.find(schema_type_id);
       it == reverse_schema_type_mapper_.end()) {
@@ -838,10 +892,9 @@ libtextclassifier3::Status SchemaStore::PersistToDisk() {
     return libtextclassifier3::Status::OK;
   }
   ICING_RETURN_IF_ERROR(schema_type_mapper_->PersistToDisk());
-  // Write the header
-  ICING_ASSIGN_OR_RETURN(Crc32 checksum, ComputeChecksum());
-  header_->set_checksum(checksum.Get());
-  return header_->Write(filesystem_, MakeHeaderFilename(base_dir_));
+  ICING_RETURN_IF_ERROR(UpdateChecksum());
+  ICING_RETURN_IF_ERROR(header_->PersistToDisk());
+  return libtextclassifier3::Status::OK;
 }
 
 SchemaStoreStorageInfoProto SchemaStore::GetStorageInfo() const {
@@ -928,7 +981,7 @@ libtextclassifier3::StatusOr<SchemaDebugInfoProto> SchemaStore::GetDebugInfo()
     ICING_ASSIGN_OR_RETURN(const SchemaProto* schema, GetSchema());
     *debug_info.mutable_schema() = *schema;
   }
-  ICING_ASSIGN_OR_RETURN(Crc32 crc, ComputeChecksum());
+  ICING_ASSIGN_OR_RETURN(Crc32 crc, GetChecksum());
   debug_info.set_crc(crc.Get());
   return debug_info;
 }
@@ -983,5 +1036,33 @@ SchemaStore::ExpandTypePropertyMasks(
   return result;
 }
 
+libtextclassifier3::StatusOr<
+    std::unordered_map<std::string, std::vector<std::string>>>
+SchemaStore::ConstructBlobPropertyMap() const {
+  ICING_ASSIGN_OR_RETURN(const SchemaProto* schema, GetSchema());
+  std::unordered_map<std::string, std::vector<std::string>> blob_property_map;
+  for (const SchemaTypeConfigProto& type_config : schema->types()) {
+    SchemaPropertyIterator iterator(type_config, type_config_map_);
+    std::vector<std::string> blob_properties;
+
+    libtextclassifier3::Status status = iterator.Advance();
+    while (status.ok()) {
+      if (iterator.GetCurrentPropertyConfig().data_type() ==
+          PropertyConfigProto::DataType::BLOB_HANDLE) {
+        blob_properties.push_back(iterator.GetCurrentPropertyPath());
+      }
+      status = iterator.Advance();
+    }
+    if (!absl_ports::IsOutOfRange(status)) {
+      return status;
+    }
+    if (!blob_properties.empty()) {
+      blob_property_map.insert(
+          {type_config.schema_type(), std::move(blob_properties)});
+    }
+  }
+  return blob_property_map;
+}
+
 }  // namespace lib
 }  // namespace icing
diff --git a/icing/schema/schema-store.h b/icing/schema/schema-store.h
index 88968b1..77cd87f 100644
--- a/icing/schema/schema-store.h
+++ b/icing/schema/schema-store.h
@@ -68,64 +68,104 @@ class SchemaStore {
    public:
     static constexpr int32_t kMagic = 0x72650d0a;
 
-    explicit Header()
-        : magic_(kMagic),
-          checksum_(0),
-          overlay_created_(false),
-          min_overlay_version_compatibility_(
-              std::numeric_limits<int32_t>::max()) {
-      memset(overlay_created_padding_, 0, kOverlayCreatedPaddingSize);
-      memset(padding_, 0, kPaddingSize);
+    explicit Header(const Filesystem* filesystem, std::string path)
+        : path_(std::move(path)), filesystem_(filesystem) {}
+
+    Header(Header&& other)
+        : serialized_header_(std::move(other.serialized_header_)),
+          path_(std::move(other.path_)),
+          header_fd_(std::move(other.header_fd_)),
+          filesystem_(other.filesystem_),
+          dirty_(other.dirty_) {}
+
+    Header& operator=(Header&& other) {
+      serialized_header_ = std::move(other.serialized_header_);
+      path_ = std::move(other.path_);
+      header_fd_ = std::move(other.header_fd_);
+      filesystem_ = other.filesystem_;
+      dirty_ = other.dirty_;
+      return *this;
     }
 
+    struct SerializedHeader {
+      explicit SerializedHeader()
+          : magic(kMagic),
+            checksum(0),
+            overlay_created(false),
+            min_overlay_version_compatibility(
+                std::numeric_limits<int32_t>::max()) {
+        memset(overlay_created_padding, 0, kOverlayCreatedPaddingSize);
+        memset(padding, 0, kPaddingSize);
+      }
+      // Holds the magic as a quick sanity check against file corruption.
+      int32_t magic;
+
+      // Checksum of the SchemaStore's sub-component's checksums.
+      uint32_t checksum;
+
+      bool overlay_created;
+      // Three bytes of padding due to the fact that
+      // min_overlay_version_compatibility_ has an alignof() == 4 and the offset
+      // of overlay_created_padding_ == 9.
+      static constexpr int kOverlayCreatedPaddingSize = 3;
+      uint8_t overlay_created_padding[kOverlayCreatedPaddingSize];
+
+      int32_t min_overlay_version_compatibility;
+
+      static constexpr int kPaddingSize = 1008;
+      // Padding exists just to reserve space for additional values.
+      uint8_t padding[kPaddingSize];
+    };
+    static_assert(sizeof(SerializedHeader) == 1024);
+
     // RETURNS:
     //   - On success, a valid Header instance
     //   - NOT_FOUND if header file doesn't exist
     //   - INTERNAL if unable to read header
     static libtextclassifier3::StatusOr<Header> Read(
-        const Filesystem* filesystem, const std::string& path);
+        const Filesystem* filesystem, std::string path);
+
+    libtextclassifier3::Status Write();
 
-    libtextclassifier3::Status Write(const Filesystem* filesystem,
-                                     const std::string& path);
+    libtextclassifier3::Status PersistToDisk();
 
-    int32_t magic() const { return magic_; }
+    int32_t magic() const { return serialized_header_.magic; }
 
-    uint32_t checksum() const { return checksum_; }
-    void set_checksum(uint32_t checksum) { checksum_ = checksum; }
+    uint32_t checksum() const { return serialized_header_.checksum; }
+    void set_checksum(uint32_t checksum) {
+      dirty_ = true;
+      serialized_header_.checksum = checksum;
+    }
 
-    bool overlay_created() const { return overlay_created_; }
+    bool overlay_created() const { return serialized_header_.overlay_created; }
 
     int32_t min_overlay_version_compatibility() const {
-      return min_overlay_version_compatibility_;
+      return serialized_header_.min_overlay_version_compatibility;
     }
 
     void SetOverlayInfo(bool overlay_created,
                         int32_t min_overlay_version_compatibility) {
-      overlay_created_ = overlay_created;
-      min_overlay_version_compatibility_ = min_overlay_version_compatibility;
+      dirty_ = true;
+      serialized_header_.overlay_created = overlay_created;
+      serialized_header_.min_overlay_version_compatibility =
+          min_overlay_version_compatibility;
     }
 
    private:
-    // Holds the magic as a quick sanity check against file corruption.
-    int32_t magic_;
-
-    // Checksum of the SchemaStore's sub-component's checksums.
-    uint32_t checksum_;
-
-    bool overlay_created_;
-    // Three bytes of padding due to the fact that
-    // min_overlay_version_compatibility_ has an alignof() == 4 and the offset
-    // of overlay_created_padding_ == 9.
-    static constexpr int kOverlayCreatedPaddingSize = 3;
-    uint8_t overlay_created_padding_[kOverlayCreatedPaddingSize];
-
-    int32_t min_overlay_version_compatibility_;
-
-    static constexpr int kPaddingSize = 1008;
-    // Padding exists just to reserve space for additional values.
-    uint8_t padding_[kPaddingSize];
+    explicit Header(SerializedHeader serialized_header, std::string path,
+                    ScopedFd header_fd, const Filesystem* filesystem)
+        : serialized_header_(std::move(serialized_header)),
+          path_(std::move(path)),
+          header_fd_(std::move(header_fd)),
+          filesystem_(filesystem),
+          dirty_(false) {}
+
+    SerializedHeader serialized_header_;
+    std::string path_;
+    ScopedFd header_fd_;
+    const Filesystem* filesystem_;  // Not owned.
+    bool dirty_;
   };
-  static_assert(sizeof(Header) == 1024);
 
   // Holds information on what may have been affected by the new schema. This is
   // generally data that other classes may depend on from the SchemaStore,
@@ -276,6 +316,16 @@ class SchemaStore {
   libtextclassifier3::StatusOr<const SchemaTypeConfigProto*>
   GetSchemaTypeConfig(std::string_view schema_type) const;
 
+  // Get a map contains all schema_type name to its blob property paths.
+  //
+  // Returns:
+  //   A map contains all schema_type name to its blob property paths on success
+  //   FAILED_PRECONDITION if schema hasn't been set yet
+  //   INTERNAL on any I/O errors
+  libtextclassifier3::StatusOr<
+      std::unordered_map<std::string, std::vector<std::string>>>
+  ConstructBlobPropertyMap() const;
+
   // Returns the schema type of the passed in SchemaTypeId
   //
   // Returns:
@@ -373,13 +423,21 @@ class SchemaStore {
   //   INTERNAL on I/O errors.
   libtextclassifier3::Status PersistToDisk();
 
-  // Computes the combined checksum of the schema store - includes the ground
-  // truth and all derived files.
+  // Recomputes the combined checksum of components of the schema store and
+  // updates the header.
+  //
+  // Returns:
+  //   - the checksum on success
+  //   - INTERNAL on I/O errors.
+  libtextclassifier3::StatusOr<Crc32> UpdateChecksum();
+
+  // Recomputes the combined checksum of components of the schema store. Does
+  // NOT update the header.
   //
   // Returns:
-  //   Combined checksum on success
-  //   INTERNAL_ERROR on compute error
-  libtextclassifier3::StatusOr<Crc32> ComputeChecksum() const;
+  //   - the checksum on success
+  //   - INTERNAL on I/O errors.
+  libtextclassifier3::StatusOr<Crc32> GetChecksum() const;
 
   // Returns:
   //   - On success, the section metadata list for the specified schema type
diff --git a/icing/schema/schema-store_test.cc b/icing/schema/schema-store_test.cc
index ca5cdd3..b2da7fa 100644
--- a/icing/schema/schema-store_test.cc
+++ b/icing/schema/schema-store_test.cc
@@ -55,6 +55,7 @@ using ::testing::Ge;
 using ::testing::Gt;
 using ::testing::HasSubstr;
 using ::testing::Not;
+using ::testing::Pair;
 using ::testing::Pointee;
 using ::testing::Return;
 using ::testing::SizeIs;
@@ -132,13 +133,13 @@ TEST_F(SchemaStoreTest, SchemaStoreMoveConstructible) {
       schema, /*ignore_errors_and_delete_documents=*/false,
       /*allow_circular_schema_definitions=*/false));
   ICING_ASSERT_OK_AND_ASSIGN(Crc32 expected_checksum,
-                             schema_store->ComputeChecksum());
+                             schema_store->UpdateChecksum());
 
   // Move construct an instance of SchemaStore
   SchemaStore move_constructed_schema_store(std::move(*schema_store));
   EXPECT_THAT(move_constructed_schema_store.GetSchema(),
               IsOkAndHolds(Pointee(EqualsProto(schema))));
-  EXPECT_THAT(move_constructed_schema_store.ComputeChecksum(),
+  EXPECT_THAT(move_constructed_schema_store.UpdateChecksum(),
               IsOkAndHolds(Eq(expected_checksum)));
   SectionMetadata expected_metadata(/*id_in=*/0, TYPE_STRING, TOKENIZER_PLAIN,
                                     TERM_MATCH_EXACT, NUMERIC_MATCH_UNKNOWN,
@@ -166,7 +167,7 @@ TEST_F(SchemaStoreTest, SchemaStoreMoveAssignment) {
       schema1, /*ignore_errors_and_delete_documents=*/false,
       /*allow_circular_schema_definitions=*/false));
   ICING_ASSERT_OK_AND_ASSIGN(Crc32 expected_checksum,
-                             schema_store->ComputeChecksum());
+                             schema_store->UpdateChecksum());
 
   // Construct another instance of SchemaStore
   SchemaProto schema2 =
@@ -189,7 +190,7 @@ TEST_F(SchemaStoreTest, SchemaStoreMoveAssignment) {
   *move_assigned_schema_store = std::move(*schema_store);
   EXPECT_THAT(move_assigned_schema_store->GetSchema(),
               IsOkAndHolds(Pointee(EqualsProto(schema1))));
-  EXPECT_THAT(move_assigned_schema_store->ComputeChecksum(),
+  EXPECT_THAT(move_assigned_schema_store->UpdateChecksum(),
               IsOkAndHolds(Eq(expected_checksum)));
   SectionMetadata expected_metadata(/*id_in=*/0, TYPE_STRING, TOKENIZER_PLAIN,
                                     TERM_MATCH_EXACT, NUMERIC_MATCH_UNKNOWN,
@@ -398,7 +399,7 @@ TEST_F(SchemaStoreTest, CreateNoPreviousSchemaOk) {
               StatusIs(libtextclassifier3::StatusCode::FAILED_PRECONDITION));
 
   // The apis to persist and checksum data should succeed.
-  EXPECT_THAT(store->ComputeChecksum(), IsOkAndHolds(Crc32()));
+  EXPECT_THAT(store->UpdateChecksum(), IsOkAndHolds(Crc32()));
   EXPECT_THAT(store->PersistToDisk(), IsOk());
 }
 
@@ -1245,16 +1246,17 @@ TEST_F(SchemaStoreTest, GetSchemaTypeId) {
   EXPECT_THAT(schema_store->GetSchemaTypeId(second_type), IsOkAndHolds(1));
 }
 
-TEST_F(SchemaStoreTest, ComputeChecksumDefaultOnEmptySchemaStore) {
+TEST_F(SchemaStoreTest, UpdateChecksumDefaultOnEmptySchemaStore) {
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<SchemaStore> schema_store,
       SchemaStore::Create(&filesystem_, schema_store_dir_, &fake_clock_));
 
-  Crc32 default_checksum;
-  EXPECT_THAT(schema_store->ComputeChecksum(), IsOkAndHolds(default_checksum));
+  EXPECT_THAT(schema_store->GetChecksum(), IsOkAndHolds(Crc32()));
+  EXPECT_THAT(schema_store->UpdateChecksum(), IsOkAndHolds(Crc32()));
+  EXPECT_THAT(schema_store->GetChecksum(), IsOkAndHolds(Crc32()));
 }
 
-TEST_F(SchemaStoreTest, ComputeChecksumSameBetweenCalls) {
+TEST_F(SchemaStoreTest, UpdateChecksumSameBetweenCalls) {
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<SchemaStore> schema_store,
       SchemaStore::Create(&filesystem_, schema_store_dir_, &fake_clock_));
@@ -1266,13 +1268,16 @@ TEST_F(SchemaStoreTest, ComputeChecksumSameBetweenCalls) {
       foo_schema, /*ignore_errors_and_delete_documents=*/false,
       /*allow_circular_schema_definitions=*/false));
 
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum, schema_store->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum, schema_store->GetChecksum());
+  EXPECT_THAT(schema_store->UpdateChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(schema_store->GetChecksum(), IsOkAndHolds(checksum));
 
   // Calling it again doesn't change the checksum
-  EXPECT_THAT(schema_store->ComputeChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(schema_store->UpdateChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(schema_store->GetChecksum(), IsOkAndHolds(checksum));
 }
 
-TEST_F(SchemaStoreTest, ComputeChecksumSameAcrossInstances) {
+TEST_F(SchemaStoreTest, UpdateChecksumSameAcrossInstances) {
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<SchemaStore> schema_store,
       SchemaStore::Create(&filesystem_, schema_store_dir_, &fake_clock_));
@@ -1284,7 +1289,9 @@ TEST_F(SchemaStoreTest, ComputeChecksumSameAcrossInstances) {
       foo_schema, /*ignore_errors_and_delete_documents=*/false,
       /*allow_circular_schema_definitions=*/false));
 
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum, schema_store->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum, schema_store->GetChecksum());
+  EXPECT_THAT(schema_store->UpdateChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(schema_store->GetChecksum(), IsOkAndHolds(checksum));
 
   // Destroy the previous instance and recreate SchemaStore
   schema_store.reset();
@@ -1292,10 +1299,12 @@ TEST_F(SchemaStoreTest, ComputeChecksumSameAcrossInstances) {
   ICING_ASSERT_OK_AND_ASSIGN(
       schema_store,
       SchemaStore::Create(&filesystem_, schema_store_dir_, &fake_clock_));
-  EXPECT_THAT(schema_store->ComputeChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(schema_store->GetChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(schema_store->UpdateChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(schema_store->GetChecksum(), IsOkAndHolds(checksum));
 }
 
-TEST_F(SchemaStoreTest, ComputeChecksumChangesOnModification) {
+TEST_F(SchemaStoreTest, UpdateChecksumChangesOnModification) {
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<SchemaStore> schema_store,
       SchemaStore::Create(&filesystem_, schema_store_dir_, &fake_clock_));
@@ -1307,7 +1316,9 @@ TEST_F(SchemaStoreTest, ComputeChecksumChangesOnModification) {
       foo_schema, /*ignore_errors_and_delete_documents=*/false,
       /*allow_circular_schema_definitions=*/false));
 
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum, schema_store->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum, schema_store->GetChecksum());
+  EXPECT_THAT(schema_store->UpdateChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(schema_store->GetChecksum(), IsOkAndHolds(checksum));
 
   // Modifying the SchemaStore changes the checksum
   SchemaProto foo_bar_schema =
@@ -1320,7 +1331,11 @@ TEST_F(SchemaStoreTest, ComputeChecksumChangesOnModification) {
       foo_bar_schema, /*ignore_errors_and_delete_documents=*/false,
       /*allow_circular_schema_definitions=*/false));
 
-  EXPECT_THAT(schema_store->ComputeChecksum(), IsOkAndHolds(Not(Eq(checksum))));
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 updated_checksum,
+                             schema_store->GetChecksum());
+  EXPECT_THAT(updated_checksum, Not(Eq(checksum)));
+  EXPECT_THAT(schema_store->UpdateChecksum(), IsOkAndHolds(updated_checksum));
+  EXPECT_THAT(schema_store->GetChecksum(), IsOkAndHolds(updated_checksum));
 }
 
 TEST_F(SchemaStoreTest, PersistToDiskFineForEmptySchemaStore) {
@@ -1332,6 +1347,45 @@ TEST_F(SchemaStoreTest, PersistToDiskFineForEmptySchemaStore) {
   ICING_EXPECT_OK(schema_store->PersistToDisk());
 }
 
+TEST_F(SchemaStoreTest, UpdateChecksumAvoidsRecovery) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<SchemaStore> schema_store,
+      SchemaStore::Create(&filesystem_, schema_store_dir_, &fake_clock_));
+
+  SchemaProto schema =
+      SchemaBuilder().AddType(SchemaTypeConfigBuilder().SetType("foo")).Build();
+
+  ICING_EXPECT_OK(schema_store->SetSchema(
+      schema, /*ignore_errors_and_delete_documents=*/false,
+      /*allow_circular_schema_definitions=*/false));
+
+  // UpdateChecksum should update the schema store checksum. Therefore, we
+  // should not need a recovery on reinitialization.
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 crc, schema_store->GetChecksum());
+  EXPECT_THAT(schema_store->UpdateChecksum(), IsOkAndHolds(crc));
+  EXPECT_THAT(schema_store->GetChecksum(), IsOkAndHolds(crc));
+
+  ICING_ASSERT_OK_AND_ASSIGN(const SchemaProto* actual_schema,
+                             schema_store->GetSchema());
+  EXPECT_THAT(*actual_schema, EqualsProto(schema));
+
+  // And we get the same schema back on reinitialization
+  InitializeStatsProto initialize_stats;
+  ICING_ASSERT_OK_AND_ASSIGN(
+      std::unique_ptr<SchemaStore> schema_store_two,
+      SchemaStore::Create(&filesystem_, schema_store_dir_, &fake_clock_,
+                          &initialize_stats));
+  EXPECT_THAT(initialize_stats.schema_store_recovery_cause(),
+              Eq(InitializeStatsProto::NONE));
+  ICING_ASSERT_OK_AND_ASSIGN(actual_schema, schema_store_two->GetSchema());
+  EXPECT_THAT(*actual_schema, EqualsProto(schema));
+
+  // The checksum should be the same.
+  EXPECT_THAT(schema_store_two->GetChecksum(), IsOkAndHolds(crc));
+  EXPECT_THAT(schema_store_two->UpdateChecksum(), IsOkAndHolds(crc));
+  EXPECT_THAT(schema_store_two->GetChecksum(), IsOkAndHolds(crc));
+}
+
 TEST_F(SchemaStoreTest, PersistToDiskPreservesAcrossInstances) {
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<SchemaStore> schema_store,
@@ -1363,9 +1417,12 @@ TEST_F(SchemaStoreTest, PersistToDiskPreservesAcrossInstances) {
   schema_store.reset();
 
   // And we get the same schema back on reinitialization
+  InitializeStatsProto initialize_stats;
   ICING_ASSERT_OK_AND_ASSIGN(
-      schema_store,
-      SchemaStore::Create(&filesystem_, schema_store_dir_, &fake_clock_));
+      schema_store, SchemaStore::Create(&filesystem_, schema_store_dir_,
+                                        &fake_clock_, &initialize_stats));
+  EXPECT_THAT(initialize_stats.schema_store_recovery_cause(),
+              Eq(InitializeStatsProto::NONE));
   ICING_ASSERT_OK_AND_ASSIGN(actual_schema, schema_store->GetSchema());
   EXPECT_THAT(*actual_schema, EqualsProto(schema));
 }
@@ -3189,6 +3246,141 @@ TEST_F(SchemaStoreTest, MigrateSchemaVersionUndeterminedDiscardsOverlaySchema) {
   }
 }
 
+TEST_F(SchemaStoreTest, GetTypeWithBlobProperties) {
+  SchemaTypeConfigProto type_a =
+      SchemaTypeConfigBuilder()
+          .SetType("A")
+          .AddProperty(PropertyConfigBuilder()
+                           .SetName("blob")
+                           .SetDataType(TYPE_BLOB_HANDLE)
+                           .SetCardinality(CARDINALITY_OPTIONAL))
+          .AddProperty(
+              PropertyConfigBuilder()
+                  .SetName("nonBlob")
+                  .SetDataTypeString(TERM_MATCH_PREFIX, TOKENIZER_PLAIN)
+                  .SetCardinality(CARDINALITY_OPTIONAL))
+          .Build();
+
+  SchemaTypeConfigProto type_b =
+      SchemaTypeConfigBuilder()
+          .SetType("B")
+          .AddProperty(
+              PropertyConfigBuilder()
+                  .SetName("typeA")
+                  .SetDataTypeDocument("A", /*index_nested_properties=*/false)
+                  .SetCardinality(CARDINALITY_OPTIONAL))
+          .AddProperty(
+              PropertyConfigBuilder()
+                  .SetName("nonBlob")
+                  .SetDataTypeString(TERM_MATCH_PREFIX, TOKENIZER_PLAIN)
+                  .SetCardinality(CARDINALITY_OPTIONAL))
+          .Build();
+
+  SchemaTypeConfigProto type_c =
+      SchemaTypeConfigBuilder()
+          .SetType("C")
+          .AddProperty(
+              PropertyConfigBuilder()
+                  .SetName("nonBlob")
+                  .SetDataTypeString(TERM_MATCH_PREFIX, TOKENIZER_PLAIN)
+                  .SetCardinality(CARDINALITY_OPTIONAL))
+          .Build();
+
+  // type_a contains blob property.
+  // type_b contains nested type_a, which contains blob property.
+  // type_c contains no blob property.
+  SchemaProto schema =
+      SchemaBuilder().AddType(type_a).AddType(type_b).AddType(type_c).Build();
+
+  {
+    // Create an instance of the schema store and set the schema.
+    ICING_ASSERT_OK_AND_ASSIGN(
+        std::unique_ptr<SchemaStore> schema_store,
+        SchemaStore::Create(&filesystem_, schema_store_dir_, &fake_clock_));
+    ICING_ASSERT_OK(schema_store->SetSchema(
+        schema, /*ignore_errors_and_delete_documents=*/false,
+        /*allow_circular_schema_definitions=*/false));
+
+    EXPECT_THAT(schema_store->ConstructBlobPropertyMap(),
+                IsOkAndHolds(UnorderedElementsAre(
+                    Pair("A", UnorderedElementsAre("blob")),
+                    Pair("B", UnorderedElementsAre("typeA.blob")))));
+  }
+}
+
+TEST_F(SchemaStoreTest, GetTypeWithMultiLevelBlobProperties) {
+  SchemaTypeConfigProto type_a =
+      SchemaTypeConfigBuilder()
+          .SetType("A")
+          .AddProperty(PropertyConfigBuilder()
+                           .SetName("blob")
+                           .SetDataType(TYPE_BLOB_HANDLE)
+                           .SetCardinality(CARDINALITY_OPTIONAL))
+          .AddProperty(
+              PropertyConfigBuilder()
+                  .SetName("nonBlob")
+                  .SetDataTypeString(TERM_MATCH_PREFIX, TOKENIZER_PLAIN)
+                  .SetCardinality(CARDINALITY_OPTIONAL))
+          .Build();
+
+  SchemaTypeConfigProto type_b =
+      SchemaTypeConfigBuilder()
+          .SetType("B")
+          .AddProperty(
+              PropertyConfigBuilder()
+                  .SetName("typeA")
+                  .SetDataTypeDocument("A", /*index_nested_properties=*/false)
+                  .SetCardinality(CARDINALITY_OPTIONAL))
+          .AddProperty(
+              PropertyConfigBuilder()
+                  .SetName("nonBlob")
+                  .SetDataTypeString(TERM_MATCH_PREFIX, TOKENIZER_PLAIN)
+                  .SetCardinality(CARDINALITY_OPTIONAL))
+          .Build();
+
+  SchemaTypeConfigProto type_c =
+      SchemaTypeConfigBuilder()
+          .SetType("C")
+          .AddProperty(
+              PropertyConfigBuilder()
+                  .SetName("typeB")
+                  .SetDataTypeDocument("B", /*index_nested_properties=*/false)
+                  .SetCardinality(CARDINALITY_OPTIONAL))
+          .AddProperty(PropertyConfigBuilder()
+                           .SetName("blob")
+                           .SetDataType(TYPE_BLOB_HANDLE)
+                           .SetCardinality(CARDINALITY_OPTIONAL))
+          .AddProperty(
+              PropertyConfigBuilder()
+                  .SetName("nonBlob")
+                  .SetDataTypeString(TERM_MATCH_PREFIX, TOKENIZER_PLAIN)
+                  .SetCardinality(CARDINALITY_OPTIONAL))
+          .Build();
+
+  // type_a contains blob property.
+  // type_b contains nested type_a, which contains blob property.
+  // type_c contains blob property and nested type_b, which contains blob
+  // property.
+  SchemaProto schema =
+      SchemaBuilder().AddType(type_a).AddType(type_b).AddType(type_c).Build();
+  {
+    // Create an instance of the schema store and set the schema.
+    ICING_ASSERT_OK_AND_ASSIGN(
+        std::unique_ptr<SchemaStore> schema_store,
+        SchemaStore::Create(&filesystem_, schema_store_dir_, &fake_clock_));
+    ICING_ASSERT_OK(schema_store->SetSchema(
+        schema, /*ignore_errors_and_delete_documents=*/false,
+        /*allow_circular_schema_definitions=*/false));
+
+    EXPECT_THAT(
+        schema_store->ConstructBlobPropertyMap(),
+        IsOkAndHolds(UnorderedElementsAre(
+            Pair("A", UnorderedElementsAre("blob")),
+            Pair("B", UnorderedElementsAre("typeA.blob")),
+            Pair("C", UnorderedElementsAre("blob", "typeB.typeA.blob")))));
+  }
+}
+
 }  // namespace
 
 }  // namespace lib
diff --git a/icing/schema/schema-util.cc b/icing/schema/schema-util.cc
index 976d1b7..cd591bc 100644
--- a/icing/schema/schema-util.cc
+++ b/icing/schema/schema-util.cc
@@ -838,6 +838,7 @@ libtextclassifier3::Status SchemaUtil::ValidateDocumentIndexingConfig(
     case PropertyConfigProto::DataType::DOUBLE:
     case PropertyConfigProto::DataType::BOOLEAN:
     case PropertyConfigProto::DataType::BYTES:
+    case PropertyConfigProto::DataType::BLOB_HANDLE:
       return false;
   }
 }
diff --git a/icing/scoring/advanced_scoring/advanced-scorer.cc b/icing/scoring/advanced_scoring/advanced-scorer.cc
index 9e4a09d..68b6971 100644
--- a/icing/scoring/advanced_scoring/advanced-scorer.cc
+++ b/icing/scoring/advanced_scoring/advanced-scorer.cc
@@ -52,7 +52,7 @@ GetScoreExpression(std::string_view scoring_expression, double default_score,
                    Bm25fCalculator* bm25f_calculator) {
   Lexer lexer(scoring_expression, Lexer::Language::SCORING);
   ICING_ASSIGN_OR_RETURN(std::vector<Lexer::LexerToken> lexer_tokens,
-                         lexer.ExtractTokens());
+                         std::move(lexer).ExtractTokens());
   Parser parser = Parser::Create(std::move(lexer_tokens));
   ICING_ASSIGN_OR_RETURN(std::unique_ptr<Node> tree_root,
                          parser.ConsumeScoring());
diff --git a/icing/scoring/advanced_scoring/advanced-scorer_test.cc b/icing/scoring/advanced_scoring/advanced-scorer_test.cc
index f06c7d8..8841d2c 100644
--- a/icing/scoring/advanced_scoring/advanced-scorer_test.cc
+++ b/icing/scoring/advanced_scoring/advanced-scorer_test.cc
@@ -231,8 +231,9 @@ TEST_F(AdvancedScorerTest, InvalidAdvancedScoringSpec) {
 
 TEST_F(AdvancedScorerTest, SimpleExpression) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(CreateDocument("namespace", "uri")));
+  DocumentId document_id = put_result.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<Scorer> scorer,
@@ -250,8 +251,9 @@ TEST_F(AdvancedScorerTest, SimpleExpression) {
 
 TEST_F(AdvancedScorerTest, BasicPureArithmeticExpression) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(CreateDocument("namespace", "uri")));
+  DocumentId document_id = put_result.new_document_id;
   DocHitInfo docHitInfo = DocHitInfo(document_id);
 
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -317,8 +319,9 @@ TEST_F(AdvancedScorerTest, BasicPureArithmeticExpression) {
 
 TEST_F(AdvancedScorerTest, BasicMathFunctionExpression) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(CreateDocument("namespace", "uri")));
+  DocumentId document_id = put_result.new_document_id;
   DocHitInfo docHitInfo = DocHitInfo(document_id);
 
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -454,10 +457,11 @@ TEST_F(AdvancedScorerTest, BasicMathFunctionExpression) {
 
 TEST_F(AdvancedScorerTest, DocumentScoreCreationTimestampFunctionExpression) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(CreateDocument(
           "namespace", "uri", /*score=*/123,
           /*creation_timestamp_ms=*/kDefaultCreationTimestampMs)));
+  DocumentId document_id = put_result.new_document_id;
   DocHitInfo docHitInfo = DocHitInfo(document_id);
 
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -495,8 +499,9 @@ TEST_F(AdvancedScorerTest, DocumentScoreCreationTimestampFunctionExpression) {
 
 TEST_F(AdvancedScorerTest, DocumentUsageFunctionExpression) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(CreateDocument("namespace", "uri")));
+  DocumentId document_id = put_result.new_document_id;
   DocHitInfo docHitInfo = DocHitInfo(document_id);
 
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -550,8 +555,9 @@ TEST_F(AdvancedScorerTest, DocumentUsageFunctionExpression) {
 
 TEST_F(AdvancedScorerTest, DocumentUsageFunctionOutOfRange) {
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(CreateDocument("namespace", "uri")));
+  DocumentId document_id = put_result.new_document_id;
   DocHitInfo docHitInfo = DocHitInfo(document_id);
 
   const double default_score = 123;
@@ -599,8 +605,9 @@ TEST_F(AdvancedScorerTest, RelevanceScoreFunctionScoreExpression) {
           .SetCreationTimestampMs(kDefaultCreationTimestampMs)
           .Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store_->Put(test_document));
+  DocumentId document_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<AdvancedScorer> scorer,
       AdvancedScorer::Create(CreateAdvancedScoringSpec("this.relevanceScore()"),
@@ -620,16 +627,19 @@ TEST_F(AdvancedScorerTest, ChildrenScoresFunctionScoreExpression) {
   const double default_score = 123;
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id_1,
+      DocumentStore::PutResult put_result1,
       document_store_->Put(CreateDocument("namespace", "uri1")));
+  DocumentId document_id_1 = put_result1.new_document_id;
   DocHitInfo docHitInfo1 = DocHitInfo(document_id_1);
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id_2,
+      DocumentStore::PutResult put_result2,
       document_store_->Put(CreateDocument("namespace", "uri2")));
+  DocumentId document_id_2 = put_result2.new_document_id;
   DocHitInfo docHitInfo2 = DocHitInfo(document_id_2);
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id_3,
+      DocumentStore::PutResult put_result3,
       document_store_->Put(CreateDocument("namespace", "uri3")));
+  DocumentId document_id_3 = put_result3.new_document_id;
   DocHitInfo docHitInfo3 = DocHitInfo(document_id_3);
 
   // Create a JoinChildrenFetcher that matches:
@@ -724,12 +734,15 @@ TEST_F(AdvancedScorerTest, PropertyWeightsFunctionScoreExpression) {
   DocumentProto test_document_3 =
       DocumentBuilder().SetKey("namespace", "uri3").SetSchema("person").Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id_1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(test_document_1));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id_2,
+  DocumentId document_id_1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(test_document_2));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id_3,
+  DocumentId document_id_2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store_->Put(test_document_3));
+  DocumentId document_id_3 = put_result3.new_document_id;
 
   ScoringSpecProto spec_proto = CreateAdvancedScoringSpec("");
 
@@ -817,10 +830,12 @@ TEST_F(AdvancedScorerTest,
   DocumentProto test_document_2 =
       DocumentBuilder().SetKey("namespace", "uri2").SetSchema("person").Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id_1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store_->Put(test_document_1));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id_2,
+  DocumentId document_id_1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store_->Put(test_document_2));
+  DocumentId document_id_2 = put_result2.new_document_id;
 
   ScoringSpecProto spec_proto = CreateAdvancedScoringSpec("");
 
@@ -918,9 +933,10 @@ TEST_F(AdvancedScorerTest, InvalidChildrenScoresFunctionScoreExpression) {
 TEST_F(AdvancedScorerTest, ComplexExpression) {
   const int64_t creation_timestamp_ms = 123;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(CreateDocument("namespace", "uri", /*score=*/123,
                                           creation_timestamp_ms)));
+  DocumentId document_id = put_result.new_document_id;
   DocHitInfo docHitInfo = DocHitInfo(document_id);
 
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -982,48 +998,94 @@ TEST_F(AdvancedScorerTest, EmptyExpression) {
       StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
 }
 
+TEST_F(AdvancedScorerTest, ConstantEvaluationErrorShouldReturnAnError) {
+  libtextclassifier3::StatusOr<std::unique_ptr<AdvancedScorer>> scorer_or =
+      AdvancedScorer::Create(
+          CreateAdvancedScoringSpec("log(0)"), /*default_score=*/0,
+          kDefaultSemanticMetricType, document_store_.get(),
+          schema_store_.get(), fake_clock_.GetSystemTimeMilliseconds(),
+          /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_);
+  EXPECT_THAT(scorer_or,
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
+  EXPECT_THAT(scorer_or.status().error_message(),
+              HasSubstr("Got a non-finite value"));
+
+  scorer_or = AdvancedScorer::Create(
+      CreateAdvancedScoringSpec("1 / 0"), /*default_score=*/0,
+      kDefaultSemanticMetricType, document_store_.get(), schema_store_.get(),
+      fake_clock_.GetSystemTimeMilliseconds(),
+      /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_);
+  EXPECT_THAT(scorer_or,
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
+  EXPECT_THAT(scorer_or.status().error_message(),
+              HasSubstr("Got a non-finite value"));
+
+  scorer_or = AdvancedScorer::Create(
+      CreateAdvancedScoringSpec("sqrt(-1)"), /*default_score=*/0,
+      kDefaultSemanticMetricType, document_store_.get(), schema_store_.get(),
+      fake_clock_.GetSystemTimeMilliseconds(),
+      /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_);
+  EXPECT_THAT(scorer_or,
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
+  EXPECT_THAT(scorer_or.status().error_message(),
+              HasSubstr("Got a non-finite value"));
+
+  scorer_or = AdvancedScorer::Create(
+      CreateAdvancedScoringSpec("pow(-1, 0.5)"), /*default_score=*/0,
+      kDefaultSemanticMetricType, document_store_.get(), schema_store_.get(),
+      fake_clock_.GetSystemTimeMilliseconds(),
+      /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_);
+  EXPECT_THAT(scorer_or,
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
+  EXPECT_THAT(scorer_or.status().error_message(),
+              HasSubstr("Got a non-finite value"));
+}
+
 TEST_F(AdvancedScorerTest, EvaluationErrorShouldReturnDefaultScore) {
   const double default_score = 123;
 
+  // Put a document with score 0, so that "this.documentScore()" will return a
+  // non-constant 0.
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
-      document_store_->Put(CreateDocument("namespace", "uri")));
+      DocumentStore::PutResult put_result,
+      document_store_->Put(CreateDocument("namespace", "uri", /*score=*/0)));
+  DocumentId document_id = put_result.new_document_id;
   DocHitInfo docHitInfo = DocHitInfo(document_id);
 
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<Scorer> scorer,
       AdvancedScorer::Create(
-          CreateAdvancedScoringSpec("log(0)"), default_score,
+          CreateAdvancedScoringSpec("log(this.documentScore())"), default_score,
           kDefaultSemanticMetricType, document_store_.get(),
           schema_store_.get(), fake_clock_.GetSystemTimeMilliseconds(),
           /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_));
   EXPECT_THAT(scorer->GetScore(docHitInfo), DoubleNear(default_score, kEps));
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      scorer, AdvancedScorer::Create(CreateAdvancedScoringSpec("1 / 0"),
-                                     default_score, kDefaultSemanticMetricType,
-                                     document_store_.get(), schema_store_.get(),
-                                     fake_clock_.GetSystemTimeMilliseconds(),
-                                     /*join_children_fetcher=*/nullptr,
-                                     &empty_embedding_query_results_));
+      scorer,
+      AdvancedScorer::Create(
+          CreateAdvancedScoringSpec("1 / this.documentScore()"), default_score,
+          kDefaultSemanticMetricType, document_store_.get(),
+          schema_store_.get(), fake_clock_.GetSystemTimeMilliseconds(),
+          /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_));
   EXPECT_THAT(scorer->GetScore(docHitInfo), DoubleNear(default_score, kEps));
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      scorer, AdvancedScorer::Create(CreateAdvancedScoringSpec("sqrt(-1)"),
-                                     default_score, kDefaultSemanticMetricType,
-                                     document_store_.get(), schema_store_.get(),
-                                     fake_clock_.GetSystemTimeMilliseconds(),
-                                     /*join_children_fetcher=*/nullptr,
-                                     &empty_embedding_query_results_));
+      scorer,
+      AdvancedScorer::Create(
+          CreateAdvancedScoringSpec("sqrt(this.documentScore() - 1)"),
+          default_score, kDefaultSemanticMetricType, document_store_.get(),
+          schema_store_.get(), fake_clock_.GetSystemTimeMilliseconds(),
+          /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_));
   EXPECT_THAT(scorer->GetScore(docHitInfo), DoubleNear(default_score, kEps));
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      scorer, AdvancedScorer::Create(CreateAdvancedScoringSpec("pow(-1, 0.5)"),
-                                     default_score, kDefaultSemanticMetricType,
-                                     document_store_.get(), schema_store_.get(),
-                                     fake_clock_.GetSystemTimeMilliseconds(),
-                                     /*join_children_fetcher=*/nullptr,
-                                     &empty_embedding_query_results_));
+      scorer,
+      AdvancedScorer::Create(
+          CreateAdvancedScoringSpec("pow(this.documentScore() - 1, 0.5)"),
+          default_score, kDefaultSemanticMetricType, document_store_.get(),
+          schema_store_.get(), fake_clock_.GetSystemTimeMilliseconds(),
+          /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_));
   EXPECT_THAT(scorer->GetScore(docHitInfo), DoubleNear(default_score, kEps));
 }
 
@@ -1207,14 +1269,21 @@ TEST_F(AdvancedScorerTest, DocumentFunctionTypeError) {
 
 TEST_F(AdvancedScorerTest,
        MatchedSemanticScoresFunctionScoreExpressionTypeError) {
+  EmbeddingQueryResults embedding_query_results;
+  embedding_query_results
+      .result_scores[/*query_vector_index=*/0]
+                    [SearchSpecProto::EmbeddingQueryMetricType::COSINE]
+                    [/*document_id=*/0]
+      .push_back(/*semantic_score=*/0.1);
+
   libtextclassifier3::StatusOr<std::unique_ptr<AdvancedScorer>> scorer_or =
       AdvancedScorer::Create(
           CreateAdvancedScoringSpec(
-              "sum(matchedSemanticScores(getSearchSpecEmbedding(0)))"),
+              "sum(matchedSemanticScores(getEmbeddingParameter(0)))"),
           kDefaultSemanticMetricType, kDefaultSemanticMetricType,
           document_store_.get(), schema_store_.get(),
           fake_clock_.GetSystemTimeMilliseconds(),
-          /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_);
+          /*join_children_fetcher=*/nullptr, &embedding_query_results);
   EXPECT_THAT(scorer_or,
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
   EXPECT_THAT(scorer_or.status().error_message(),
@@ -1225,7 +1294,7 @@ TEST_F(AdvancedScorerTest,
       kDefaultSemanticMetricType, kDefaultSemanticMetricType,
       document_store_.get(), schema_store_.get(),
       fake_clock_.GetSystemTimeMilliseconds(),
-      /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_);
+      /*join_children_fetcher=*/nullptr, &embedding_query_results);
   EXPECT_THAT(scorer_or,
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
   EXPECT_THAT(scorer_or.status().error_message(),
@@ -1233,11 +1302,11 @@ TEST_F(AdvancedScorerTest,
 
   scorer_or = AdvancedScorer::Create(
       CreateAdvancedScoringSpec(
-          "sum(this.matchedSemanticScores(getSearchSpecEmbedding(0), 0))"),
+          "sum(this.matchedSemanticScores(getEmbeddingParameter(0), 0))"),
       kDefaultSemanticMetricType, kDefaultSemanticMetricType,
       document_store_.get(), schema_store_.get(),
       fake_clock_.GetSystemTimeMilliseconds(),
-      /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_);
+      /*join_children_fetcher=*/nullptr, &embedding_query_results);
   EXPECT_THAT(scorer_or,
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
   EXPECT_THAT(scorer_or.status().error_message(),
@@ -1245,11 +1314,11 @@ TEST_F(AdvancedScorerTest,
 
   scorer_or = AdvancedScorer::Create(
       CreateAdvancedScoringSpec("sum(this.matchedSemanticScores("
-                                "getSearchSpecEmbedding(0), \"COSINE\", 0))"),
+                                "getEmbeddingParameter(0), \"COSINE\", 0))"),
       kDefaultSemanticMetricType, kDefaultSemanticMetricType,
       document_store_.get(), schema_store_.get(),
       fake_clock_.GetSystemTimeMilliseconds(),
-      /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_);
+      /*join_children_fetcher=*/nullptr, &embedding_query_results);
   EXPECT_THAT(scorer_or,
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
   EXPECT_THAT(scorer_or.status().error_message(),
@@ -1257,11 +1326,11 @@ TEST_F(AdvancedScorerTest,
 
   scorer_or = AdvancedScorer::Create(
       CreateAdvancedScoringSpec("sum(this.matchedSemanticScores("
-                                "getSearchSpecEmbedding(0), \"COSIGN\"))"),
+                                "getEmbeddingParameter(0), \"COSIGN\"))"),
       kDefaultSemanticMetricType, kDefaultSemanticMetricType,
       document_store_.get(), schema_store_.get(),
       fake_clock_.GetSystemTimeMilliseconds(),
-      /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_);
+      /*join_children_fetcher=*/nullptr, &embedding_query_results);
   EXPECT_THAT(scorer_or,
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
   EXPECT_THAT(scorer_or.status().error_message(),
@@ -1269,27 +1338,122 @@ TEST_F(AdvancedScorerTest,
 
   scorer_or = AdvancedScorer::Create(
       CreateAdvancedScoringSpec(
-          "sum(this.matchedSemanticScores(getSearchSpecEmbedding(\"0\")))"),
+          "sum(this.matchedSemanticScores(getEmbeddingParameter(\"0\")))"),
       kDefaultSemanticMetricType, kDefaultSemanticMetricType,
       document_store_.get(), schema_store_.get(),
       fake_clock_.GetSystemTimeMilliseconds(),
-      /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_);
+      /*join_children_fetcher=*/nullptr, &embedding_query_results);
   EXPECT_THAT(scorer_or,
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
   EXPECT_THAT(scorer_or.status().error_message(),
-              HasSubstr("getSearchSpecEmbedding got invalid argument type"));
+              HasSubstr("getEmbeddingParameter got invalid argument type"));
 
   scorer_or = AdvancedScorer::Create(
       CreateAdvancedScoringSpec(
-          "sum(this.matchedSemanticScores(getSearchSpecEmbedding()))"),
+          "sum(this.matchedSemanticScores(getEmbeddingParameter()))"),
       kDefaultSemanticMetricType, kDefaultSemanticMetricType,
       document_store_.get(), schema_store_.get(),
       fake_clock_.GetSystemTimeMilliseconds(),
+      /*join_children_fetcher=*/nullptr, &embedding_query_results);
+  EXPECT_THAT(scorer_or,
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
+  EXPECT_THAT(scorer_or.status().error_message(),
+              HasSubstr("getEmbeddingParameter must have 1 argument"));
+}
+
+TEST_F(AdvancedScorerTest,
+       MatchedSemanticScoresFunctionScoreExpressionNotQueried) {
+  EmbeddingQueryResults embedding_query_results;
+  embedding_query_results
+      .result_scores[/*query_vector_index=*/0]
+                    [SearchSpecProto::EmbeddingQueryMetricType::COSINE]
+                    [/*document_id=*/0]
+      .push_back(/*semantic_score=*/0.1);
+  embedding_query_results
+      .result_scores[/*query_vector_index=*/1]
+                    [SearchSpecProto::EmbeddingQueryMetricType::DOT_PRODUCT]
+                    [/*document_id=*/1]
+      .push_back(/*semantic_score=*/0.2);
+
+  libtextclassifier3::StatusOr<std::unique_ptr<AdvancedScorer>> scorer_or =
+      AdvancedScorer::Create(CreateAdvancedScoringSpec(
+                                 "sum(this.matchedSemanticScores("
+                                 "getEmbeddingParameter(0), \"DOT_PRODUCT\"))"),
+                             /*default_score=*/0, kDefaultSemanticMetricType,
+                             document_store_.get(), schema_store_.get(),
+                             fake_clock_.GetSystemTimeMilliseconds(),
+                             /*join_children_fetcher=*/nullptr,
+                             &embedding_query_results);
+  EXPECT_THAT(scorer_or,
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
+  EXPECT_THAT(scorer_or.status().error_message(),
+              HasSubstr("embedding query index 0 with metric type DOT_PRODUCT "
+                        "has not been queried"));
+
+  scorer_or = AdvancedScorer::Create(
+      CreateAdvancedScoringSpec("sum(this.matchedSemanticScores("
+                                "getEmbeddingParameter(1), \"COSINE\"))"),
+      /*default_score=*/0, kDefaultSemanticMetricType, document_store_.get(),
+      schema_store_.get(), fake_clock_.GetSystemTimeMilliseconds(),
+      /*join_children_fetcher=*/nullptr, &embedding_query_results);
+  EXPECT_THAT(scorer_or,
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
+  EXPECT_THAT(scorer_or.status().error_message(),
+              HasSubstr("embedding query index 1 with metric type COSINE "
+                        "has not been queried"));
+
+  scorer_or = AdvancedScorer::Create(
+      CreateAdvancedScoringSpec("sum(this.matchedSemanticScores("
+                                "getEmbeddingParameter(2)))"),
+      /*default_score=*/0, kDefaultSemanticMetricType, document_store_.get(),
+      schema_store_.get(), fake_clock_.GetSystemTimeMilliseconds(),
+      /*join_children_fetcher=*/nullptr, &embedding_query_results);
+  EXPECT_THAT(scorer_or,
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
+  EXPECT_THAT(scorer_or.status().error_message(),
+              HasSubstr("embedding query index 2 with metric type DOT_PRODUCT "
+                        "has not been queried"));
+}
+
+TEST_F(AdvancedScorerTest,
+       GetEmbeddingParameterFunctionScoreExpressionInvalidIndex) {
+  // Embedding query index must be non-negative.
+  libtextclassifier3::StatusOr<std::unique_ptr<AdvancedScorer>> scorer_or =
+      AdvancedScorer::Create(
+          CreateAdvancedScoringSpec("sum(this.matchedSemanticScores("
+                                    "getEmbeddingParameter(-1)))"),
+          /*default_score=*/0, kDefaultSemanticMetricType,
+          document_store_.get(), schema_store_.get(),
+          fake_clock_.GetSystemTimeMilliseconds(),
+          /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_);
+  EXPECT_THAT(scorer_or,
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
+  EXPECT_THAT(scorer_or.status().error_message(),
+              HasSubstr("must be a non-negative integer"));
+
+  // Embedding query index is too large.
+  scorer_or = AdvancedScorer::Create(
+      CreateAdvancedScoringSpec("sum(this.matchedSemanticScores("
+                                "getEmbeddingParameter(pow(2, 50))))"),
+      /*default_score=*/0, kDefaultSemanticMetricType, document_store_.get(),
+      schema_store_.get(), fake_clock_.GetSystemTimeMilliseconds(),
+      /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_);
+  EXPECT_THAT(scorer_or,
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
+  EXPECT_THAT(scorer_or.status().error_message(),
+              HasSubstr("exceeds the maximum value of uint32"));
+
+  // Embedding query index should be an integer.
+  scorer_or = AdvancedScorer::Create(
+      CreateAdvancedScoringSpec("sum(this.matchedSemanticScores("
+                                "getEmbeddingParameter(0.5)))"),
+      /*default_score=*/0, kDefaultSemanticMetricType, document_store_.get(),
+      schema_store_.get(), fake_clock_.GetSystemTimeMilliseconds(),
       /*join_children_fetcher=*/nullptr, &empty_embedding_query_results_);
   EXPECT_THAT(scorer_or,
               StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
   EXPECT_THAT(scorer_or.status().error_message(),
-              HasSubstr("getSearchSpecEmbedding must have 1 argument"));
+              HasSubstr("must be an integer"));
 }
 
 void AddEntryToEmbeddingQueryScoreMap(
@@ -1357,7 +1521,7 @@ TEST_F(AdvancedScorerTest, MatchedSemanticScoresFunctionScoreExpression) {
       std::unique_ptr<Scorer> scorer,
       AdvancedScorer::Create(
           CreateAdvancedScoringSpec(
-              "sum(this.matchedSemanticScores(getSearchSpecEmbedding(0)))"),
+              "sum(this.matchedSemanticScores(getEmbeddingParameter(0)))"),
           kDefaultScore, /*default_semantic_metric_type=*/
           SearchSpecProto::EmbeddingQueryMetricType::DOT_PRODUCT,
           document_store_.get(), schema_store_.get(),
@@ -1372,7 +1536,7 @@ TEST_F(AdvancedScorerTest, MatchedSemanticScoresFunctionScoreExpression) {
       scorer,
       AdvancedScorer::Create(
           CreateAdvancedScoringSpec("sum(this.matchedSemanticScores("
-                                    "getSearchSpecEmbedding(0), \"COSINE\"))"),
+                                    "getEmbeddingParameter(0), \"COSINE\"))"),
           kDefaultScore, /*default_semantic_metric_type=*/
           SearchSpecProto::EmbeddingQueryMetricType::DOT_PRODUCT,
           document_store_.get(), schema_store_.get(),
@@ -1385,11 +1549,11 @@ TEST_F(AdvancedScorerTest, MatchedSemanticScoresFunctionScoreExpression) {
   ICING_ASSERT_OK_AND_ASSIGN(
       scorer, AdvancedScorer::Create(
                   CreateAdvancedScoringSpec(
-                      "sum(this.matchedSemanticScores(getSearchSpecEmbedding(0)"
+                      "sum(this.matchedSemanticScores(getEmbeddingParameter(0)"
                       ", \"COSINE\")) + "
-                      "sum(this.matchedSemanticScores(getSearchSpecEmbedding(0)"
+                      "sum(this.matchedSemanticScores(getEmbeddingParameter(0)"
                       ", \"DOT_PRODUCT\")) + "
-                      "sum(this.matchedSemanticScores(getSearchSpecEmbedding(0)"
+                      "sum(this.matchedSemanticScores(getEmbeddingParameter(0)"
                       ", \"EUCLIDEAN\"))"),
                   kDefaultScore, /*default_semantic_metric_type=*/
                   SearchSpecProto::EmbeddingQueryMetricType::DOT_PRODUCT,
@@ -1406,7 +1570,7 @@ TEST_F(AdvancedScorerTest, MatchedSemanticScoresFunctionScoreExpression) {
       scorer,
       AdvancedScorer::Create(
           CreateAdvancedScoringSpec(
-              "sum(this.matchedSemanticScores(getSearchSpecEmbedding(1)))"),
+              "sum(this.matchedSemanticScores(getEmbeddingParameter(1)))"),
           kDefaultScore, /*default_semantic_metric_type=*/
           SearchSpecProto::EmbeddingQueryMetricType::DOT_PRODUCT,
           document_store_.get(), schema_store_.get(),
@@ -1416,30 +1580,32 @@ TEST_F(AdvancedScorerTest, MatchedSemanticScoresFunctionScoreExpression) {
   EXPECT_THAT(scorer->GetScore(doc_hit_info_1), DoubleNear(0.2, kEps));
 
   // The second query does not contain cosine scores.
-  ICING_ASSERT_OK_AND_ASSIGN(
-      scorer,
+  libtextclassifier3::StatusOr<std::unique_ptr<AdvancedScorer>> scorer_or =
       AdvancedScorer::Create(
           CreateAdvancedScoringSpec("sum(this.matchedSemanticScores("
-                                    "getSearchSpecEmbedding(1), \"COSINE\"))"),
+                                    "getEmbeddingParameter(1), \"COSINE\"))"),
           kDefaultScore, /*default_semantic_metric_type=*/
           SearchSpecProto::EmbeddingQueryMetricType::DOT_PRODUCT,
           document_store_.get(), schema_store_.get(),
           fake_clock_.GetSystemTimeMilliseconds(),
-          /*join_children_fetcher=*/nullptr, &embedding_query_results));
-  EXPECT_THAT(scorer->GetScore(doc_hit_info_0), DoubleNear(0, kEps));
-  EXPECT_THAT(scorer->GetScore(doc_hit_info_1), DoubleNear(0, kEps));
+          /*join_children_fetcher=*/nullptr, &embedding_query_results);
+  EXPECT_THAT(scorer_or,
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT));
+  EXPECT_THAT(scorer_or.status().error_message(),
+              HasSubstr("embedding query index 1 with metric type COSINE "
+                        "has not been queried"));
 }
 
 TEST_F(AdvancedScorerTest, ListRelatedFunctions) {
   DocumentId document_id_0 = 0;
   DocHitInfo doc_hit_info_0(document_id_0);
-  EmbeddingQueryResults embedding_query_results;
 
-  // Construct a score map so that:
-  // - this.matchedSemanticScores(getSearchSpecEmbedding(0)) returns
+  // Construct an EmbeddingQueryResults so that:
+  // - this.matchedSemanticScores(getEmbeddingParameter(0)) returns
   //   {4, 5, 2, 1, 3}.
-  // - this.matchedSemanticScores(getSearchSpecEmbedding(1)) returns an empty
+  // - this.matchedSemanticScores(getEmbeddingParameter(1)) returns an empty
   //   list.
+  EmbeddingQueryResults embedding_query_results;
   EmbeddingQueryResults::EmbeddingQueryScoreMap* score_map =
       &embedding_query_results
            .result_scores[0][SearchSpecProto::EmbeddingQueryMetricType::COSINE];
@@ -1453,13 +1619,16 @@ TEST_F(AdvancedScorerTest, ListRelatedFunctions) {
                                    /*semantic_score=*/1, document_id_0);
   AddEntryToEmbeddingQueryScoreMap(*score_map,
                                    /*semantic_score=*/3, document_id_0);
+  score_map =
+      &embedding_query_results
+           .result_scores[1][SearchSpecProto::EmbeddingQueryMetricType::COSINE];
 
   // maxOrDefault({4, 5, 2, 1, 3}, 100) = 5
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<Scorer> scorer,
       AdvancedScorer::Create(
           CreateAdvancedScoringSpec("maxOrDefault(this.matchedSemanticScores("
-                                    "getSearchSpecEmbedding(0)), 100)"),
+                                    "getEmbeddingParameter(0)), 100)"),
           kDefaultScore, /*default_semantic_metric_type=*/
           SearchSpecProto::EmbeddingQueryMetricType::COSINE,
           document_store_.get(), schema_store_.get(),
@@ -1472,7 +1641,7 @@ TEST_F(AdvancedScorerTest, ListRelatedFunctions) {
       scorer,
       AdvancedScorer::Create(
           CreateAdvancedScoringSpec("minOrDefault(this.matchedSemanticScores("
-                                    "getSearchSpecEmbedding(0)), -100)"),
+                                    "getEmbeddingParameter(0)), -100)"),
           kDefaultScore, /*default_semantic_metric_type=*/
           SearchSpecProto::EmbeddingQueryMetricType::COSINE,
           document_store_.get(), schema_store_.get(),
@@ -1485,7 +1654,7 @@ TEST_F(AdvancedScorerTest, ListRelatedFunctions) {
       scorer,
       AdvancedScorer::Create(
           CreateAdvancedScoringSpec("maxOrDefault(this.matchedSemanticScores("
-                                    "getSearchSpecEmbedding(1)), 100)"),
+                                    "getEmbeddingParameter(1)), 100)"),
           kDefaultScore, /*default_semantic_metric_type=*/
           SearchSpecProto::EmbeddingQueryMetricType::COSINE,
           document_store_.get(), schema_store_.get(),
@@ -1498,7 +1667,7 @@ TEST_F(AdvancedScorerTest, ListRelatedFunctions) {
       scorer,
       AdvancedScorer::Create(
           CreateAdvancedScoringSpec("minOrDefault(this.matchedSemanticScores("
-                                    "getSearchSpecEmbedding(1)), -100)"),
+                                    "getEmbeddingParameter(1)), -100)"),
           kDefaultScore, /*default_semantic_metric_type=*/
           SearchSpecProto::EmbeddingQueryMetricType::COSINE,
           document_store_.get(), schema_store_.get(),
@@ -1511,7 +1680,7 @@ TEST_F(AdvancedScorerTest, ListRelatedFunctions) {
       scorer, AdvancedScorer::Create(
                   CreateAdvancedScoringSpec(
                       "sum(filterByRange(this.matchedSemanticScores("
-                      "getSearchSpecEmbedding(0)), 2, 4))"),
+                      "getEmbeddingParameter(0)), 2, 4))"),
                   kDefaultScore, /*default_semantic_metric_type=*/
                   SearchSpecProto::EmbeddingQueryMetricType::COSINE,
                   document_store_.get(), schema_store_.get(),
@@ -1523,9 +1692,10 @@ TEST_F(AdvancedScorerTest, ListRelatedFunctions) {
 TEST_F(AdvancedScorerTest, AdditionalScores) {
   const int64_t creation_timestamp_ms = 123;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       document_store_->Put(CreateDocument("namespace", "uri", /*score=*/123,
                                           creation_timestamp_ms)));
+  DocumentId document_id = put_result.new_document_id;
   DocHitInfo docHitInfo = DocHitInfo(document_id);
 
   ScoringSpecProto scoring_spec =
diff --git a/icing/scoring/advanced_scoring/score-expression.cc b/icing/scoring/advanced_scoring/score-expression.cc
index 2caa927..8bd151e 100644
--- a/icing/scoring/advanced_scoring/score-expression.cc
+++ b/icing/scoring/advanced_scoring/score-expression.cc
@@ -18,6 +18,7 @@
 #include <cmath>
 #include <cstdint>
 #include <cstdlib>
+#include <limits>
 #include <memory>
 #include <numeric>
 #include <optional>
@@ -94,8 +95,10 @@ OperatorScoreExpression::Create(
   if (children_all_constant_double) {
     // Because all of the children are constants, this expression does not
     // depend on the DocHitInto or query_it that are passed into it.
-    return ConstantScoreExpression::Create(
-        expression->EvaluateDouble(DocHitInfo(), /*query_it=*/nullptr));
+    ICING_ASSIGN_OR_RETURN(double constant_value,
+                           expression->EvaluateDouble(DocHitInfo(),
+                                                      /*query_it=*/nullptr));
+    return ConstantScoreExpression::Create(constant_value);
   }
   return expression;
 }
@@ -277,8 +280,10 @@ MathFunctionScoreExpression::Create(
   if (args_all_constant_double) {
     // Because all of the arguments are constants, this expression does not
     // depend on the DocHitInto or query_it that are passed into it.
-    return ConstantScoreExpression::Create(
-        expression->EvaluateDouble(DocHitInfo(), /*query_it=*/nullptr));
+    ICING_ASSIGN_OR_RETURN(double constant_value,
+                           expression->EvaluateDouble(DocHitInfo(),
+                                                      /*query_it=*/nullptr));
+    return ConstantScoreExpression::Create(constant_value);
   }
   return expression;
 }
@@ -671,8 +676,10 @@ SchemaTypeId PropertyWeightsFunctionScoreExpression::GetSchemaTypeId(
 }
 
 libtextclassifier3::StatusOr<std::unique_ptr<ScoreExpression>>
-GetSearchSpecEmbeddingFunctionScoreExpression::Create(
+GetEmbeddingParameterFunctionScoreExpression::Create(
     std::vector<std::unique_ptr<ScoreExpression>> args) {
+  ICING_RETURN_IF_ERROR(CheckChildrenNotNull(args));
+
   if (args.size() != 1) {
     return absl_ports::InvalidArgumentError(
         absl_ports::StrCat(kFunctionName, " must have 1 argument."));
@@ -683,22 +690,30 @@ GetSearchSpecEmbeddingFunctionScoreExpression::Create(
   }
   bool is_constant = args[0]->is_constant();
   std::unique_ptr<ScoreExpression> expression =
-      std::unique_ptr<GetSearchSpecEmbeddingFunctionScoreExpression>(
-          new GetSearchSpecEmbeddingFunctionScoreExpression(
-              std::move(args[0])));
+      std::unique_ptr<GetEmbeddingParameterFunctionScoreExpression>(
+          new GetEmbeddingParameterFunctionScoreExpression(std::move(args[0])));
   if (is_constant) {
-    return ConstantScoreExpression::Create(
-        expression->EvaluateDouble(DocHitInfo(), /*query_it=*/nullptr),
-        expression->type());
+    ICING_ASSIGN_OR_RETURN(double constant_value,
+                           expression->EvaluateDouble(DocHitInfo(),
+                                                      /*query_it=*/nullptr));
+    return ConstantScoreExpression::Create(constant_value, expression->type());
   }
   return expression;
 }
 
 libtextclassifier3::StatusOr<double>
-GetSearchSpecEmbeddingFunctionScoreExpression::EvaluateDouble(
+GetEmbeddingParameterFunctionScoreExpression::EvaluateDouble(
     const DocHitInfo& hit_info, const DocHitInfoIterator* query_it) const {
   ICING_ASSIGN_OR_RETURN(double raw_query_index,
                          arg_->EvaluateDouble(hit_info, query_it));
+  if (raw_query_index < 0) {
+    return absl_ports::InvalidArgumentError(
+        "The index of an embedding query must be a non-negative integer.");
+  }
+  if (raw_query_index > std::numeric_limits<uint32_t>::max()) {
+    return absl_ports::InvalidArgumentError(
+        "The index of an embedding query exceeds the maximum value of uint32.");
+  }
   uint32_t query_index = (uint32_t)raw_query_index;
   if (query_index != raw_query_index) {
     return absl_ports::InvalidArgumentError(
@@ -724,7 +739,8 @@ MatchedSemanticScoresFunctionScoreExpression::Create(
     return absl_ports::InvalidArgumentError(
         absl_ports::StrCat(kFunctionName, " got invalid number of arguments."));
   }
-  if (args[1]->type() != ScoreExpressionType::kVectorIndex) {
+  ScoreExpression* embedding_index_arg = args[1].get();
+  if (embedding_index_arg->type() != ScoreExpressionType::kVectorIndex) {
     return absl_ports::InvalidArgumentError(absl_ports::StrCat(
         kFunctionName, " got invalid argument type for embedding vector."));
   }
@@ -745,6 +761,20 @@ MatchedSemanticScoresFunctionScoreExpression::Create(
         metric_type,
         embedding_util::GetEmbeddingQueryMetricTypeFromName(metric));
   }
+  if (embedding_index_arg->is_constant()) {
+    ICING_ASSIGN_OR_RETURN(
+        uint32_t embedding_index,
+        embedding_index_arg->EvaluateDouble(DocHitInfo(),
+                                            /*query_it=*/nullptr));
+    if (embedding_query_results->GetScoreMap(embedding_index, metric_type) ==
+        nullptr) {
+      return absl_ports::InvalidArgumentError(absl_ports::StrCat(
+          "The embedding query index ", std::to_string(embedding_index),
+          " with metric type ",
+          SearchSpecProto::EmbeddingQueryMetricType::Code_Name(metric_type),
+          " has not been queried."));
+    }
+  }
   return std::unique_ptr<MatchedSemanticScoresFunctionScoreExpression>(
       new MatchedSemanticScoresFunctionScoreExpression(
           std::move(args), metric_type, *embedding_query_results));
diff --git a/icing/scoring/advanced_scoring/score-expression.h b/icing/scoring/advanced_scoring/score-expression.h
index 00abb32..2bb5871 100644
--- a/icing/scoring/advanced_scoring/score-expression.h
+++ b/icing/scoring/advanced_scoring/score-expression.h
@@ -35,6 +35,7 @@
 #include "icing/store/document-filter-data.h"
 #include "icing/store/document-id.h"
 #include "icing/store/document-store.h"
+#include "icing/util/status-macros.h"
 
 namespace icing {
 namespace lib {
@@ -123,8 +124,7 @@ class ThisExpression : public ScoreExpression {
 class ConstantScoreExpression : public ScoreExpression {
  public:
   static std::unique_ptr<ConstantScoreExpression> Create(
-      libtextclassifier3::StatusOr<double> c,
-      ScoreExpressionType type = ScoreExpressionType::kDouble) {
+      double c, ScoreExpressionType type = ScoreExpressionType::kDouble) {
     return std::unique_ptr<ConstantScoreExpression>(
         new ConstantScoreExpression(c, type));
   }
@@ -139,11 +139,10 @@ class ConstantScoreExpression : public ScoreExpression {
   bool is_constant() const override { return true; }
 
  private:
-  explicit ConstantScoreExpression(libtextclassifier3::StatusOr<double> c,
-                                   ScoreExpressionType type)
+  explicit ConstantScoreExpression(double c, ScoreExpressionType type)
       : c_(c), type_(type) {}
 
-  libtextclassifier3::StatusOr<double> c_;
+  double c_;
   ScoreExpressionType type_;
 };
 
@@ -425,12 +424,12 @@ class PropertyWeightsFunctionScoreExpression : public ScoreExpression {
   int64_t current_time_ms_;
 };
 
-class GetSearchSpecEmbeddingFunctionScoreExpression : public ScoreExpression {
+class GetEmbeddingParameterFunctionScoreExpression : public ScoreExpression {
  public:
-  static constexpr std::string_view kFunctionName = "getSearchSpecEmbedding";
+  static constexpr std::string_view kFunctionName = "getEmbeddingParameter";
 
   // RETURNS:
-  //   - A GetSearchSpecEmbeddingFunctionScoreExpression instance on success if
+  //   - A GetEmbeddingParameterFunctionScoreExpression instance on success if
   //     not simplifiable.
   //   - A ConstantScoreExpression instance on success if simplifiable.
   //   - FAILED_PRECONDITION on any null pointer in children.
@@ -447,7 +446,7 @@ class GetSearchSpecEmbeddingFunctionScoreExpression : public ScoreExpression {
   }
 
  private:
-  explicit GetSearchSpecEmbeddingFunctionScoreExpression(
+  explicit GetEmbeddingParameterFunctionScoreExpression(
       std::unique_ptr<ScoreExpression> arg)
       : arg_(std::move(arg)) {}
   std::unique_ptr<ScoreExpression> arg_;
diff --git a/icing/scoring/advanced_scoring/scoring-visitor.cc b/icing/scoring/advanced_scoring/scoring-visitor.cc
index 6cd82fa..4d38c0e 100644
--- a/icing/scoring/advanced_scoring/scoring-visitor.cc
+++ b/icing/scoring/advanced_scoring/scoring-visitor.cc
@@ -29,11 +29,6 @@
 namespace icing {
 namespace lib {
 
-void ScoringVisitor::VisitFunctionName(const FunctionNameNode* node) {
-  pending_error_ = absl_ports::InternalError(
-      "FunctionNameNode should be handled in VisitFunction!");
-}
-
 void ScoringVisitor::VisitString(const StringNode* node) {
   stack_.push_back(StringExpression::Create(node->value()));
 }
@@ -97,7 +92,7 @@ void ScoringVisitor::VisitFunctionHelper(const FunctionNode* node,
     }
     args.push_back(pop_stack());
   }
-  const std::string& function_name = node->function_name()->value();
+  const std::string& function_name = node->function_name();
   libtextclassifier3::StatusOr<std::unique_ptr<ScoreExpression>> expression =
       absl_ports::InvalidArgumentError(
           absl_ports::StrCat("Unknown function: ", function_name));
@@ -137,10 +132,10 @@ void ScoringVisitor::VisitFunctionHelper(const FunctionNode* node,
         ListOperationFunctionScoreExpression::kFunctionNames.at(function_name),
         std::move(args));
   } else if (function_name ==
-             GetSearchSpecEmbeddingFunctionScoreExpression::kFunctionName) {
-    // getSearchSpecEmbedding function
+                 GetEmbeddingParameterFunctionScoreExpression::kFunctionName) {
+    // getEmbeddingParameter function
     expression =
-        GetSearchSpecEmbeddingFunctionScoreExpression::Create(std::move(args));
+        GetEmbeddingParameterFunctionScoreExpression::Create(std::move(args));
   } else if (function_name ==
              MatchedSemanticScoresFunctionScoreExpression::kFunctionName) {
     // matchedSemanticScores function
diff --git a/icing/scoring/advanced_scoring/scoring-visitor.h b/icing/scoring/advanced_scoring/scoring-visitor.h
index bb5e6ba..3c4a374 100644
--- a/icing/scoring/advanced_scoring/scoring-visitor.h
+++ b/icing/scoring/advanced_scoring/scoring-visitor.h
@@ -58,7 +58,6 @@ class ScoringVisitor : public AbstractSyntaxTreeVisitor {
         embedding_query_results_(*embedding_query_results),
         current_time_ms_(current_time_ms) {}
 
-  void VisitFunctionName(const FunctionNameNode* node) override;
   void VisitString(const StringNode* node) override;
   void VisitText(const TextNode* node) override;
   void VisitMember(const MemberNode* node) override;
diff --git a/icing/scoring/score-and-rank_benchmark.cc b/icing/scoring/score-and-rank_benchmark.cc
index fc06308..b203b6f 100644
--- a/icing/scoring/score-and-rank_benchmark.cc
+++ b/icing/scoring/score-and-rank_benchmark.cc
@@ -156,10 +156,11 @@ void BM_ScoreAndRankDocumentHitsByDocumentScore(benchmark::State& state) {
   std::vector<DocHitInfo> doc_hit_infos;
   for (int i = 0; i < num_of_documents; i++) {
     ICING_ASSERT_OK_AND_ASSIGN(
-        DocumentId document_id,
+        DocumentStore::PutResult put_result,
         document_store->Put(CreateEmailDocument(
             /*id=*/i, /*document_score=*/distribution(random_generator),
             /*creation_timestamp_ms=*/1)));
+    DocumentId document_id = put_result.new_document_id;
     doc_hit_infos.emplace_back(document_id);
   }
 
@@ -267,10 +268,11 @@ void BM_ScoreAndRankDocumentHitsByCreationTime(benchmark::State& state) {
   std::vector<DocHitInfo> doc_hit_infos;
   for (int i = 0; i < num_of_documents; i++) {
     ICING_ASSERT_OK_AND_ASSIGN(
-        DocumentId document_id,
+        DocumentStore::PutResult put_result,
         document_store->Put(CreateEmailDocument(
             /*id=*/i, /*document_score=*/1,
             /*creation_timestamp_ms=*/distribution(random_generator))));
+    DocumentId document_id = put_result.new_document_id;
     doc_hit_infos.emplace_back(document_id);
   }
 
@@ -374,9 +376,10 @@ void BM_ScoreAndRankDocumentHitsNoScoring(benchmark::State& state) {
   std::vector<DocHitInfo> doc_hit_infos;
   for (int i = 0; i < num_of_documents; i++) {
     ICING_ASSERT_OK_AND_ASSIGN(
-        DocumentId document_id,
+        DocumentStore::PutResult put_result,
         document_store->Put(CreateEmailDocument(/*id=*/i, /*document_score=*/1,
                                                 /*creation_timestamp_ms=*/1)));
+    DocumentId document_id = put_result.new_document_id;
     doc_hit_infos.emplace_back(document_id);
   }
 
@@ -487,11 +490,12 @@ void BM_ScoreAndRankDocumentHitsByRelevanceScoring(benchmark::State& state) {
   std::vector<DocHitInfoTermFrequencyPair> doc_hit_infos;
   for (int i = 0; i < num_of_documents; i++) {
     ICING_ASSERT_OK_AND_ASSIGN(
-        DocumentId document_id,
+        DocumentStore::PutResult put_result,
         document_store->Put(CreateEmailDocument(
                                 /*id=*/i, /*document_score=*/1,
                                 /*creation_timestamp_ms=*/1),
                             /*num_tokens=*/10));
+    DocumentId document_id = put_result.new_document_id;
     DocHitInfoTermFrequencyPair doc_hit =
         DocHitInfo(document_id, section_id_mask);
     // Set five matches for term "foo" for each document hit.
diff --git a/icing/scoring/scorer_test.cc b/icing/scoring/scorer_test.cc
index 4424cbc..097d0c7 100644
--- a/icing/scoring/scorer_test.cc
+++ b/icing/scoring/scorer_test.cc
@@ -187,8 +187,9 @@ TEST_P(ScorerTest, ShouldGetDefaultDocumentScore) {
           .SetCreationTimestampMs(fake_clock1().GetSystemTimeMilliseconds())
           .Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store()->Put(test_document));
+  DocumentId document_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<Scorer> scorer,
       scorer_factory::Create(
@@ -213,8 +214,9 @@ TEST_P(ScorerTest, ShouldGetCorrectDocumentScore) {
           .SetCreationTimestampMs(fake_clock2().GetSystemTimeMilliseconds())
           .Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store()->Put(test_document));
+  DocumentId document_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<Scorer> scorer,
       scorer_factory::Create(
@@ -241,8 +243,9 @@ TEST_P(ScorerTest, QueryIteratorNullRelevanceScoreShouldReturnDefaultScore) {
           .SetCreationTimestampMs(fake_clock2().GetSystemTimeMilliseconds())
           .Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store()->Put(test_document));
+  DocumentId document_id = put_result.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<Scorer> scorer,
       scorer_factory::Create(
@@ -274,10 +277,12 @@ TEST_P(ScorerTest, ShouldGetCorrectCreationTimestampScore) {
           .SetCreationTimestampMs(fake_clock2().GetSystemTimeMilliseconds())
           .Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store()->Put(test_document1));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store()->Put(test_document2));
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<Scorer> scorer,
       scorer_factory::Create(
@@ -305,8 +310,9 @@ TEST_P(ScorerTest, ShouldGetCorrectUsageCountScoreForType1) {
           .SetCreationTimestampMs(fake_clock1().GetSystemTimeMilliseconds())
           .Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store()->Put(test_document));
+  DocumentId document_id = put_result.new_document_id;
 
   // Create 3 scorers for 3 different usage types.
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -358,8 +364,9 @@ TEST_P(ScorerTest, ShouldGetCorrectUsageCountScoreForType2) {
           .SetCreationTimestampMs(fake_clock1().GetSystemTimeMilliseconds())
           .Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store()->Put(test_document));
+  DocumentId document_id = put_result.new_document_id;
 
   // Create 3 scorers for 3 different usage types.
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -411,8 +418,9 @@ TEST_P(ScorerTest, ShouldGetCorrectUsageCountScoreForType3) {
           .SetCreationTimestampMs(fake_clock1().GetSystemTimeMilliseconds())
           .Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store()->Put(test_document));
+  DocumentId document_id = put_result.new_document_id;
 
   // Create 3 scorers for 3 different usage types.
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -464,8 +472,9 @@ TEST_P(ScorerTest, ShouldGetCorrectUsageTimestampScoreForType1) {
           .SetCreationTimestampMs(fake_clock1().GetSystemTimeMilliseconds())
           .Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store()->Put(test_document));
+  DocumentId document_id = put_result.new_document_id;
 
   // Create 3 scorers for 3 different usage types.
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -539,8 +548,9 @@ TEST_P(ScorerTest, ShouldGetCorrectUsageTimestampScoreForType2) {
           .SetCreationTimestampMs(fake_clock1().GetSystemTimeMilliseconds())
           .Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store()->Put(test_document));
+  DocumentId document_id = put_result.new_document_id;
 
   // Create 3 scorers for 3 different usage types.
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -614,8 +624,9 @@ TEST_P(ScorerTest, ShouldGetCorrectUsageTimestampScoreForType3) {
           .SetCreationTimestampMs(fake_clock1().GetSystemTimeMilliseconds())
           .Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store()->Put(test_document));
+  DocumentId document_id = put_result.new_document_id;
 
   // Create 3 scorers for 3 different usage types.
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -723,8 +734,9 @@ TEST_P(ScorerTest, ShouldScaleUsageTimestampScoreForMaxTimestamp) {
           .SetCreationTimestampMs(fake_clock1().GetSystemTimeMilliseconds())
           .Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              document_store()->Put(test_document));
+  DocumentId document_id = put_result.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
       std::unique_ptr<Scorer> scorer1,
diff --git a/icing/scoring/scoring-processor_test.cc b/icing/scoring/scoring-processor_test.cc
index feb67e7..0e46464 100644
--- a/icing/scoring/scoring-processor_test.cc
+++ b/icing/scoring/scoring-processor_test.cc
@@ -163,10 +163,11 @@ CreateAndInsertsDocumentsWithScores(DocumentStore* document_store,
   std::vector<DocHitInfo> doc_hit_infos;
   std::vector<ScoredDocumentHit> scored_document_hits;
   for (int i = 0; i < scores.size(); i++) {
-    ICING_ASSIGN_OR_RETURN(DocumentId document_id,
+    ICING_ASSIGN_OR_RETURN(DocumentStore::PutResult put_result,
                            document_store->Put(CreateDocument(
                                "icing", "email/" + std::to_string(i),
                                scores.at(i), kDefaultCreationTimestampMs)));
+    DocumentId document_id = put_result.new_document_id;
     doc_hit_infos.emplace_back(document_id);
     scored_document_hits.emplace_back(document_id, kSectionIdMaskNone,
                                       scores.at(i));
@@ -260,9 +261,10 @@ TEST_P(ScoringProcessorTest, ShouldHandleEmptyDocHitIterator) {
 TEST_P(ScoringProcessorTest, ShouldHandleNonPositiveNumToScore) {
   // Sets up documents
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store()->Put(CreateDocument("icing", "email/1", /*score=*/1,
                                            kDefaultCreationTimestampMs)));
+  DocumentId document_id1 = put_result1.new_document_id;
   DocHitInfo doc_hit_info1(document_id1);
 
   // Creates a dummy DocHitInfoIterator
@@ -369,14 +371,17 @@ TEST_P(ScoringProcessorTest,
                      /*creation_timestamp_ms=*/kDefaultCreationTimestampMs);
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store()->Put(document1, /*num_tokens=*/10));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store()->Put(document2, /*num_tokens=*/100));
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id3,
+      DocumentStore::PutResult put_result3,
       document_store()->Put(document3, /*num_tokens=*/50));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   DocHitInfoTermFrequencyPair doc_hit_info1 = DocHitInfo(document_id1);
   doc_hit_info1.UpdateSection(/*section_id*/ 0, /*hit_term_frequency=*/1);
@@ -441,14 +446,17 @@ TEST_P(ScoringProcessorTest,
                      /*creation_timestamp_ms=*/kDefaultCreationTimestampMs);
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store()->Put(document1, /*num_tokens=*/10));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store()->Put(document2, /*num_tokens=*/10));
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id3,
+      DocumentStore::PutResult put_result3,
       document_store()->Put(document3, /*num_tokens=*/10));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   DocHitInfoTermFrequencyPair doc_hit_info1 = DocHitInfo(document_id1);
   doc_hit_info1.UpdateSection(/*section_id*/ 0, /*hit_term_frequency=*/1);
@@ -512,14 +520,17 @@ TEST_P(ScoringProcessorTest,
                      /*creation_timestamp_ms=*/kDefaultCreationTimestampMs);
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store()->Put(document1, /*num_tokens=*/10));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store()->Put(document2, /*num_tokens=*/10));
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id3,
+      DocumentStore::PutResult put_result3,
       document_store()->Put(document3, /*num_tokens=*/10));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   DocHitInfoTermFrequencyPair doc_hit_info1 = DocHitInfo(document_id1);
   // Document 1 contains the query term "foo" 5 times
@@ -582,8 +593,9 @@ TEST_P(ScoringProcessorTest,
                      /*creation_timestamp_ms=*/kDefaultCreationTimestampMs);
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store()->Put(document1, /*num_tokens=*/10));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   // Document 1 contains the term "foo" 0 times in the "subject" property
   DocHitInfoTermFrequencyPair doc_hit_info1 = DocHitInfo(document_id1);
@@ -633,11 +645,13 @@ TEST_P(ScoringProcessorTest,
                      /*creation_timestamp_ms=*/kDefaultCreationTimestampMs);
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store()->Put(document1, /*num_tokens=*/1));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store()->Put(document2, /*num_tokens=*/1));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Document 1 contains the term "foo" 1 time in the "body" property
   SectionId body_section_id = 0;
@@ -708,11 +722,13 @@ TEST_P(ScoringProcessorTest,
                      /*creation_timestamp_ms=*/kDefaultCreationTimestampMs);
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store()->Put(document1, /*num_tokens=*/1));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store()->Put(document2, /*num_tokens=*/1));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Document 1 contains the term "foo" 1 time in the "body" property
   SectionId body_section_id = 0;
@@ -783,8 +799,9 @@ TEST_P(ScoringProcessorTest,
                      /*creation_timestamp_ms=*/kDefaultCreationTimestampMs);
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store()->Put(document1, /*num_tokens=*/1));
+  DocumentId document_id1 = put_result1.new_document_id;
 
   // Document 1 contains the term "foo" 1 time in the "body" property
   SectionId body_section_id = 0;
@@ -876,11 +893,13 @@ TEST_P(ScoringProcessorTest,
                      /*creation_timestamp_ms=*/kDefaultCreationTimestampMs);
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       document_store()->Put(document1, /*num_tokens=*/1));
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       document_store()->Put(document2, /*num_tokens=*/1));
+  DocumentId document_id2 = put_result2.new_document_id;
 
   // Document 1 contains the term "foo" 1 time in the "body" property
   SectionId body_section_id = 0;
@@ -952,12 +971,15 @@ TEST_P(ScoringProcessorTest, ShouldScoreByCreationTimestamp) {
       CreateDocument("icing", "email/3", kDefaultScore,
                      /*creation_timestamp_ms=*/1571100003333);
   // Intentionally inserts documents in a different order
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store()->Put(document1));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
-                             document_store()->Put(document3));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store()->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
+                             document_store()->Put(document3));
+  DocumentId document_id3 = put_result3.new_document_id;
   DocHitInfo doc_hit_info1(document_id1);
   DocHitInfo doc_hit_info2(document_id2);
   DocHitInfo doc_hit_info3(document_id3);
@@ -1003,12 +1025,15 @@ TEST_P(ScoringProcessorTest, ShouldScoreByUsageCount) {
       CreateDocument("icing", "email/3", kDefaultScore,
                      /*creation_timestamp_ms=*/kDefaultCreationTimestampMs);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store()->Put(document1));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store()->Put(document2));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store()->Put(document3));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   // Report usage for doc1 once and doc2 twice.
   UsageReport usage_report_doc1 = CreateUsageReport(
@@ -1066,12 +1091,15 @@ TEST_P(ScoringProcessorTest, ShouldScoreByUsageTimestamp) {
       CreateDocument("icing", "email/3", kDefaultScore,
                      /*creation_timestamp_ms=*/kDefaultCreationTimestampMs);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store()->Put(document1));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store()->Put(document2));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id2 = put_result2.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store()->Put(document3));
+  DocumentId document_id3 = put_result3.new_document_id;
 
   // Report usage for doc1 and doc2.
   UsageReport usage_report_doc1 = CreateUsageReport(
@@ -1166,12 +1194,15 @@ TEST_P(ScoringProcessorTest, ShouldWrapResultsWhenNoScoring) {
                                            kDefaultCreationTimestampMs);
 
   // Intentionally inserts documents in a different order
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store()->Put(document1));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id3,
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
                              document_store()->Put(document3));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              document_store()->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
+  DocumentId document_id3 = put_result3.new_document_id;
   DocHitInfo doc_hit_info1(document_id1);
   DocHitInfo doc_hit_info2(document_id2);
   DocHitInfo doc_hit_info3(document_id3);
diff --git a/icing/store/blob-store.cc b/icing/store/blob-store.cc
new file mode 100644
index 0000000..ca943af
--- /dev/null
+++ b/icing/store/blob-store.cc
@@ -0,0 +1,322 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include "icing/store/blob-store.h"
+
+#include <algorithm>
+#include <array>
+#include <cstdint>
+#include <iterator>
+#include <limits>
+#include <memory>
+#include <string>
+#include <string_view>
+#include <unordered_set>
+#include <utility>
+#include <vector>
+
+#include "icing/text_classifier/lib3/utils/base/status.h"
+#include "icing/text_classifier/lib3/utils/base/statusor.h"
+#include "icing/absl_ports/canonical_errors.h"
+#include "icing/absl_ports/str_cat.h"
+#include "icing/file/destructible-directory.h"
+#include "icing/file/filesystem.h"
+#include "icing/proto/document.pb.h"
+#include "icing/store/dynamic-trie-key-mapper.h"
+#include "icing/store/key-mapper.h"
+#include "icing/util/clock.h"
+#include "icing/util/encode-util.h"
+#include "icing/util/logging.h"
+#include "icing/util/sha256.h"
+#include "icing/util/status-macros.h"
+
+namespace icing {
+namespace lib {
+
+static constexpr std::string_view kKeyMapperDir = "key_mapper";
+// - Key: sha 256 digest (32 bytes)
+// - Value: BlobInfo (24 bytes)
+// Allow max 1M of blob info entries.
+static constexpr int32_t kBlobInfoMapperMaxSize = 56 * 1024 * 1024;  // 56 MiB
+static constexpr int32_t kSha256LengthBytes = 32;
+static constexpr int32_t kReadBufferSize = 8192;
+
+std::string MakeKeyMapperDir(const std::string& base_dir) {
+  return absl_ports::StrCat(base_dir, "/", kKeyMapperDir);
+}
+
+namespace {
+
+libtextclassifier3::Status ValidateBlobHandle(
+    const PropertyProto::BlobHandleProto& blob_handle) {
+  if (blob_handle.digest().size() != kSha256LengthBytes) {
+    return absl_ports::InvalidArgumentError(
+        "Invalid blob handle. The digest is not sha 256 digest.");
+  }
+  return libtextclassifier3::Status::OK;
+}
+
+}  // namespace
+
+libtextclassifier3::StatusOr<BlobStore> BlobStore::Create(
+    const Filesystem* filesystem, std::string base_dir, const Clock* clock,
+    int64_t orphan_blob_time_to_live_ms) {
+  ICING_RETURN_ERROR_IF_NULL(filesystem);
+  ICING_RETURN_ERROR_IF_NULL(clock);
+  ICING_ASSIGN_OR_RETURN(
+      std::unique_ptr<KeyMapper<BlobInfo>> blob_info_mapper,
+      DynamicTrieKeyMapper<BlobInfo>::Create(
+          *filesystem, MakeKeyMapperDir(base_dir), kBlobInfoMapperMaxSize));
+
+  // Load existing file names (excluding the directory of key mapper).
+  std::vector<std::string> file_names;
+  std::unordered_set<std::string> excludes = {kKeyMapperDir.data()};
+  if (!filesystem->ListDirectory(base_dir.c_str(), excludes,
+                                 /*recursive=*/false, &file_names)) {
+    return absl_ports::InternalError("Failed to list directory.");
+  }
+  std::unordered_set<std::string> known_file_names(
+      std::make_move_iterator(file_names.begin()),
+      std::make_move_iterator(file_names.end()));
+  if (orphan_blob_time_to_live_ms <= 0) {
+    orphan_blob_time_to_live_ms = std::numeric_limits<int64_t>::max();
+  }
+  return BlobStore(filesystem, std::move(base_dir), clock,
+                   orphan_blob_time_to_live_ms, std::move(blob_info_mapper),
+                   std::move(known_file_names));
+}
+
+libtextclassifier3::StatusOr<int> BlobStore::OpenWrite(
+    const PropertyProto::BlobHandleProto& blob_handle) {
+  ICING_RETURN_IF_ERROR(ValidateBlobHandle(blob_handle));
+  std::string blob_handle_str =
+      encode_util::EncodeStringToCString(blob_handle.digest()) +
+      blob_handle.label();
+  ICING_ASSIGN_OR_RETURN(BlobInfo blob_info,
+                         GetOrCreateBlobInfo(blob_handle_str));
+  if (blob_info.is_committed) {
+    return absl_ports::AlreadyExistsError(
+        "Rewriting the committed blob is not allowed.");
+  }
+  std::string file_name = absl_ports::StrCat(
+      base_dir_, "/", std::to_string(blob_info.creation_time_ms));
+  int file_descriptor = filesystem_.OpenForWrite(file_name.c_str());
+  if (file_descriptor < 0) {
+    return absl_ports::InternalError(absl_ports::StrCat(
+        "Failed to open blob file for handle: ", blob_handle.label()));
+  }
+  return file_descriptor;
+}
+
+libtextclassifier3::StatusOr<int> BlobStore::OpenRead(
+    const PropertyProto::BlobHandleProto& blob_handle) {
+  ICING_RETURN_IF_ERROR(ValidateBlobHandle(blob_handle));
+  std::string blob_handle_str =
+      encode_util::EncodeStringToCString(blob_handle.digest()) +
+      blob_handle.label();
+  ICING_ASSIGN_OR_RETURN(BlobInfo blob_info,
+                         blob_info_mapper_->Get(blob_handle_str));
+  if (!blob_info.is_committed) {
+    return absl_ports::NotFoundError("Cannot find blob file to read.");
+  }
+
+  std::string file_name = absl_ports::StrCat(
+      base_dir_, "/", std::to_string(blob_info.creation_time_ms));
+  int file_descriptor = filesystem_.OpenForRead(file_name.c_str());
+  if (file_descriptor < 0) {
+    return absl_ports::InternalError(absl_ports::StrCat(
+        "Failed to open blob file for handle: ", blob_handle.label()));
+  }
+  return file_descriptor;
+}
+
+libtextclassifier3::Status BlobStore::CommitBlob(
+    const PropertyProto::BlobHandleProto& blob_handle) {
+  ICING_RETURN_IF_ERROR(ValidateBlobHandle(blob_handle));
+  std::string blob_handle_str =
+      encode_util::EncodeStringToCString(blob_handle.digest()) +
+      blob_handle.label();
+  ICING_ASSIGN_OR_RETURN(BlobInfo blob_info,
+                         blob_info_mapper_->Get(blob_handle_str));
+  if (blob_info.is_committed) {
+    return absl_ports::AlreadyExistsError(absl_ports::StrCat(
+        "The blob is already committed for handle: ", blob_handle.label()));
+  }
+
+  // Read the file and verify the digest.
+  std::string file_name = absl_ports::StrCat(
+      base_dir_, "/", std::to_string(blob_info.creation_time_ms));
+  ScopedFd sfd(filesystem_.OpenForRead(file_name.c_str()));
+  if (!sfd.is_valid()) {
+    return absl_ports::InternalError(absl_ports::StrCat(
+        "Failed to open blob file for handle: ", blob_handle.label()));
+  }
+
+  int64_t file_size = filesystem_.GetFileSize(sfd.get());
+  if (file_size == Filesystem::kBadFileSize) {
+    return absl_ports::InternalError(absl_ports::StrCat(
+        "Failed to get file size for handle: ", blob_handle.label()));
+  }
+
+  Sha256 sha256;
+  // Read 8 KiB per iteration
+  int64_t prev_total_read_size = 0;
+  auto buffer = std::make_unique<uint8_t[]>(kReadBufferSize);
+  while (prev_total_read_size < file_size) {
+    int32_t size_to_read =
+        std::min<int32_t>(kReadBufferSize, file_size - prev_total_read_size);
+    if (!filesystem_.Read(sfd.get(), buffer.get(), size_to_read)) {
+      return absl_ports::InternalError(absl_ports::StrCat(
+          "Failed to read blob file for handle: ", blob_handle.label()));
+    }
+
+    sha256.Update(buffer.get(), size_to_read);
+    prev_total_read_size += size_to_read;
+  }
+  std::array<uint8_t, 32> hash = std::move(sha256).Finalize();
+
+  const std::string& digest = blob_handle.digest();
+  if (digest.length() != hash.size() ||
+      digest.compare(0, digest.length(),
+                     reinterpret_cast<const char*>(hash.data()),
+                     hash.size()) != 0) {
+    return absl_ports::InvalidArgumentError(
+        "The blob content doesn't match to the digest.");
+  }
+
+  // Mark the blob is committed
+  blob_info.is_committed = true;
+  has_mutated_ = true;
+  ICING_RETURN_IF_ERROR(blob_info_mapper_->Put(blob_handle_str, blob_info));
+
+  return libtextclassifier3::Status::OK;
+}
+
+libtextclassifier3::Status BlobStore::PersistToDisk() {
+  if (has_mutated_) {
+    ICING_RETURN_IF_ERROR(blob_info_mapper_->PersistToDisk());
+    has_mutated_ = false;
+  }
+  return libtextclassifier3::Status::OK;
+}
+
+libtextclassifier3::StatusOr<BlobStore::BlobInfo>
+BlobStore::GetOrCreateBlobInfo(const std::string& blob_handle_str) {
+  libtextclassifier3::StatusOr<BlobInfo> blob_info_or =
+      blob_info_mapper_->Get(blob_handle_str);
+
+  if (absl_ports::IsNotFound(blob_info_or.status())) {
+    // Create a new blob info, we are using creation time as the unique file
+    // name.
+    int64_t timestamp = clock_.GetSystemTimeMilliseconds();
+    std::string file_name = std::to_string(timestamp);
+    while (known_file_names_.find(file_name) != known_file_names_.end()) {
+      ++timestamp;
+      file_name = std::to_string(timestamp);
+    }
+    known_file_names_.insert(file_name);
+
+    BlobInfo blob_info = {timestamp, /*is_committed=*/false};
+    ICING_RETURN_IF_ERROR(blob_info_mapper_->Put(blob_handle_str, blob_info));
+    has_mutated_ = true;
+    return blob_info;
+  }
+
+  return blob_info_or;
+}
+
+std::unordered_set<std::string>
+BlobStore::GetPotentiallyOptimizableBlobHandles() {
+  int64_t current_time_ms = clock_.GetSystemTimeMilliseconds();
+  if (orphan_blob_time_to_live_ms_ > current_time_ms) {
+    // Nothing to optimize, return empty set.
+    return std::unordered_set<std::string>();
+  }
+  int64_t expired_threshold =
+      clock_.GetSystemTimeMilliseconds() - orphan_blob_time_to_live_ms_;
+  std::unique_ptr<typename KeyMapper<BlobInfo>::Iterator> itr =
+      blob_info_mapper_->GetIterator();
+
+  std::unordered_set<std::string> expired_blob_handles;
+  while (itr->Advance()) {
+    if (itr->GetValue().creation_time_ms < expired_threshold) {
+      expired_blob_handles.insert(std::string(itr->GetKey()));
+    }
+  }
+  return expired_blob_handles;
+}
+
+libtextclassifier3::Status BlobStore::Optimize(
+    const std::unordered_set<std::string>& dead_blob_handles) {
+  if (dead_blob_handles.empty()) {
+    // nothing to optimize, return early.
+    return libtextclassifier3::Status::OK;
+  }
+
+  // Create the temp blob store directory.
+  std::string temp_blob_store_dir_path = base_dir_ + "_temp";
+  if (!filesystem_.DeleteDirectoryRecursively(
+          temp_blob_store_dir_path.c_str())) {
+    ICING_LOG(ERROR) << "Recursively deleting "
+                     << temp_blob_store_dir_path.c_str();
+    return absl_ports::InternalError(
+        "Unable to delete temp directory to prepare to build new blob store.");
+  }
+  DestructibleDirectory temp_blob_store_dir(&filesystem_,
+                                            temp_blob_store_dir_path);
+  if (!temp_blob_store_dir.is_valid()) {
+    return absl_ports::InternalError(
+        "Unable to create temp directory to build new blob store.");
+  }
+
+  // Destroy the old blob info mapper and replace it with the new one.
+  std::string new_key_mapper_dir = MakeKeyMapperDir(temp_blob_store_dir_path);
+  ICING_ASSIGN_OR_RETURN(
+      std::unique_ptr<KeyMapper<BlobInfo>> new_blob_info_mapper,
+      DynamicTrieKeyMapper<BlobInfo>::Create(filesystem_, new_key_mapper_dir,
+                                             kBlobInfoMapperMaxSize));
+  std::unique_ptr<typename KeyMapper<BlobInfo>::Iterator> itr =
+      blob_info_mapper_->GetIterator();
+  while (itr->Advance()) {
+    if (dead_blob_handles.find(std::string(itr->GetKey())) ==
+        dead_blob_handles.end()) {
+      ICING_RETURN_IF_ERROR(
+          new_blob_info_mapper->Put(itr->GetKey(), itr->GetValue()));
+    } else {
+      // Delete the file of dead blobs.
+      std::string file_name = absl_ports::StrCat(
+          base_dir_, "/", std::to_string(itr->GetValue().creation_time_ms));
+      if (!filesystem_.DeleteFile(file_name.c_str())) {
+        return absl_ports::InternalError(
+            absl_ports::StrCat("Failed to delete blob file: ", file_name));
+      }
+    }
+  }
+  new_blob_info_mapper.reset();
+  // Then we swap the new key mapper directory with the old one.
+  if (!filesystem_.SwapFiles(MakeKeyMapperDir(base_dir_).c_str(),
+                             new_key_mapper_dir.c_str())) {
+    return absl_ports::InternalError(
+        "Unable to apply new blob store due to failed swap!");
+  }
+  ICING_ASSIGN_OR_RETURN(
+      blob_info_mapper_,
+      DynamicTrieKeyMapper<BlobInfo>::Create(
+          filesystem_, MakeKeyMapperDir(base_dir_), kBlobInfoMapperMaxSize));
+
+  return libtextclassifier3::Status::OK;
+}
+
+}  // namespace lib
+}  // namespace icing
diff --git a/icing/store/blob-store.h b/icing/store/blob-store.h
new file mode 100644
index 0000000..821f896
--- /dev/null
+++ b/icing/store/blob-store.h
@@ -0,0 +1,171 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef ICING_STORE_BLOB_STORE_H_
+#define ICING_STORE_BLOB_STORE_H_
+
+#include <cstdint>
+#include <memory>
+#include <string>
+#include <string_view>
+#include <unordered_set>
+#include <utility>
+
+#include "icing/text_classifier/lib3/utils/base/status.h"
+#include "icing/text_classifier/lib3/utils/base/statusor.h"
+#include "icing/file/filesystem.h"
+#include "icing/proto/document.pb.h"
+#include "icing/store/key-mapper.h"
+#include "icing/util/clock.h"
+
+namespace icing {
+namespace lib {
+
+// Provides storage interfaces for Blobs.
+//
+// The BlobStore is responsible for storing blobs in a directory and for
+// ensuring that the directory is in a consistent state.
+//
+// A blob is a file that is stored in the BlobStore. A blob is identified by
+// a blob handle, which is a unique identifier for the blob.
+//
+// Any blob that is written to the BlobStore must be committed before it can be
+// read. A blob can be committed only once. After a blob is committed, it is
+// not allowed to be updated.
+//
+// The BlobStore is not thread-safe.
+class BlobStore {
+ public:
+  // BlobInfo holds information about a blob. This struct will be stored as the
+  // value in the dynamic trie key mapper, so it must be packed to avoid
+  // padding (which potentially causes use-of-uninitialized-value errors).
+  struct BlobInfo {
+    // The creation time of the blob. This is used to determine when to delete
+    // the orphaned blobs.
+    // We are using creation_time_ms to be file name of the blob, so this field
+    // is unique for each blob.
+    int64_t creation_time_ms;
+    bool is_committed;
+
+    // The param needed for dynamic trie, we shouldn't call this constructor
+    // directly.
+    BlobInfo() : BlobInfo(/*creation_time_ms=*/-1, /*is_committed=*/false) {}
+
+    BlobInfo(int64_t creation_time_ms, bool is_committed)
+        : creation_time_ms(creation_time_ms), is_committed(is_committed) {}
+  } __attribute__((packed));
+  static_assert(sizeof(BlobInfo) == 9, "Invalid BlobInfo size");
+
+  // Factory function to create a BlobStore instance. The base directory is
+  // used to persist blobs. If a blob store was previously created with
+  // this directory, it will reload the files saved by the last instance.
+  //
+  // The callers must create the base directory before calling this function.
+  //
+  // Returns:
+  //   A BlobStore on success
+  //   FAILED_PRECONDITION on any null pointer input
+  //   INTERNAL_ERROR on I/O error
+  static libtextclassifier3::StatusOr<BlobStore> Create(
+      const Filesystem* filesystem, std::string base_dir, const Clock* clock,
+      int64_t orphan_blob_time_to_live_ms);
+
+  // Gets or creates a file for write only purpose for the given blob handle.
+  // To mark the blob is completed written, CommitBlob must be called. Once
+  // CommitBlob is called, the blob is sealed and rewrite is not allowed.
+  //
+  // Returns:
+  //   File descriptor (writable) on success
+  //   INVALID_ARGUMENT on invalid blob handle
+  //   ALREADY_EXISTS if the blob has already been committed
+  //   INTERNAL_ERROR on IO error
+  libtextclassifier3::StatusOr<int> OpenWrite(
+      const PropertyProto::BlobHandleProto& blob_handle);
+
+  // Gets a file for read only purpose for the given blob handle.
+  // Will only succeed for blobs that were committed by calling CommitBlob.
+  //
+  // Returns:
+  //   File descriptor (read only) on success
+  //   INVALID_ARGUMENT on invalid blob handle
+  //   NOT_FOUND on blob is not found or is not committed
+  libtextclassifier3::StatusOr<int> OpenRead(
+      const PropertyProto::BlobHandleProto& blob_handle);
+
+  // Commits the given blob, if the blob is finished wrote via OpenWrite.
+  // Before the blob is committed, it is not visible to any reader via OpenRead.
+  // After the blob is committed, it is not allowed to rewrite or update the
+  // content.
+  //
+  // Returns:
+  //   OK on the blob is successfully committed.
+  //   ALREADY_EXISTS on the blob is already committed, this is no op.
+  //   INVALID_ARGUMENT on invalid blob handle or digest is mismatch with
+  //                        file content.
+  //   NOT_FOUND on blob is not found.
+  libtextclassifier3::Status CommitBlob(
+      const PropertyProto::BlobHandleProto& blob_handle);
+
+  // Persists the blobs to disk.
+  libtextclassifier3::Status PersistToDisk();
+
+  // Gets the potentially optimizable blob handles.
+  //
+  // A blob will be consider as a potentially optimizable blob if it created
+  // before the orphan_blob_time_to_live_ms. And the blob should be removed if
+  // it has no reference document links to it.
+  std::unordered_set<std::string> GetPotentiallyOptimizableBlobHandles();
+
+  // Optimize the blob store and remove dead blob files.
+  //
+  // A blob will be consider as a dead blob and removed if it meets BOTH of
+  // following conditions
+  //  1: has no reference document links to it
+  //  2: It's mature.
+  //
+  // Returns:
+  //   OK on success
+  //   INTERNAL_ERROR on IO error
+  libtextclassifier3::Status Optimize(
+      const std::unordered_set<std::string>& dead_blob_handles);
+
+ private:
+  explicit BlobStore(const Filesystem* filesystem, std::string base_dir,
+                     const Clock* clock, int64_t orphan_blob_time_to_live_ms,
+                     std::unique_ptr<KeyMapper<BlobInfo>> blob_info_mapper,
+                     std::unordered_set<std::string> known_file_names)
+      : filesystem_(*filesystem),
+        base_dir_(std::move(base_dir)),
+        clock_(*clock),
+        orphan_blob_time_to_live_ms_(orphan_blob_time_to_live_ms),
+        blob_info_mapper_(std::move(blob_info_mapper)),
+        known_file_names_(std::move(known_file_names)) {}
+
+  libtextclassifier3::StatusOr<BlobStore::BlobInfo> GetOrCreateBlobInfo(
+      const std::string& blob_handle_str);
+
+  const Filesystem& filesystem_;
+  std::string base_dir_;
+  const Clock& clock_;
+  int64_t orphan_blob_time_to_live_ms_;
+
+  std::unique_ptr<KeyMapper<BlobInfo>> blob_info_mapper_;
+  std::unordered_set<std::string> known_file_names_;
+  bool has_mutated_ = false;
+};
+
+}  // namespace lib
+}  // namespace icing
+
+#endif  // ICING_STORE_BLOB_STORE_H_
diff --git a/icing/store/document-log-creator.cc b/icing/store/document-log-creator.cc
index 2abd315..683e6e1 100644
--- a/icing/store/document-log-creator.cc
+++ b/icing/store/document-log-creator.cc
@@ -23,6 +23,7 @@
 #include "icing/absl_ports/annotate.h"
 #include "icing/absl_ports/canonical_errors.h"
 #include "icing/absl_ports/str_cat.h"
+#include "icing/file/constants.h"
 #include "icing/file/file-backed-proto-log.h"
 #include "icing/file/filesystem.h"
 #include "icing/file/portable-file-backed-proto-log.h"
@@ -96,8 +97,7 @@ DocumentLogCreator::Create(const Filesystem* filesystem,
       PortableFileBackedProtoLog<DocumentWrapper>::Create(
           filesystem, MakeDocumentLogFilenameV1(base_dir),
           PortableFileBackedProtoLog<DocumentWrapper>::Options(
-              /*compress_in=*/true,
-              PortableFileBackedProtoLog<DocumentWrapper>::kMaxProtoSize,
+              /*compress_in=*/true, constants::kMaxProtoSize,
               compression_level)));
 
   CreateResult create_result = {std::move(log_create_result),
@@ -133,7 +133,7 @@ libtextclassifier3::Status DocumentLogCreator::MigrateFromV0ToV1(
           PortableFileBackedProtoLog<DocumentWrapper>::Options(
               /*compress_in=*/true,
               /*max_proto_size_in=*/
-              PortableFileBackedProtoLog<DocumentWrapper>::kMaxProtoSize,
+              constants::kMaxProtoSize,
               /*compression_level_in=*/compression_level));
   if (!v1_create_result_or.ok()) {
     return absl_ports::Annotate(
diff --git a/icing/store/document-store.cc b/icing/store/document-store.cc
index a039eb1..e5ed547 100644
--- a/icing/store/document-store.cc
+++ b/icing/store/document-store.cc
@@ -21,6 +21,7 @@
 #include <string>
 #include <string_view>
 #include <unordered_map>
+#include <unordered_set>
 #include <utility>
 #include <vector>
 
@@ -45,6 +46,7 @@
 #include "icing/proto/schema.pb.h"
 #include "icing/proto/storage.pb.h"
 #include "icing/proto/usage.pb.h"
+#include "icing/schema/property-util.h"
 #include "icing/schema/schema-store.h"
 #include "icing/store/corpus-associated-scoring-data.h"
 #include "icing/store/corpus-id.h"
@@ -53,6 +55,7 @@
 #include "icing/store/document-id.h"
 #include "icing/store/document-log-creator.h"
 #include "icing/store/dynamic-trie-key-mapper.h"
+#include "icing/store/key-mapper.h"
 #include "icing/store/namespace-fingerprint-identifier.h"
 #include "icing/store/namespace-id.h"
 #include "icing/store/persistent-hash-map-key-mapper.h"
@@ -238,6 +241,38 @@ CreateUriMapper(const Filesystem& filesystem, const std::string& base_dir,
   }
 }
 
+// Find the existing blob handles in the given document and remove them from the
+// dead_blob_handles set. Those are the blob handles that are still in use.
+//
+// The type_blob_map is a map from schema type to a set of blob property names.
+void RemoveAliveBlobHandles(
+    const DocumentProto& document,
+    const std::unordered_map<std::string, std::vector<std::string>>&
+        type_blob_property_map,
+    std::unordered_set<std::string>& dead_blob_handles) {
+  if (dead_blob_handles.empty() ||
+      type_blob_property_map.find(document.schema()) ==
+          type_blob_property_map.end()) {
+    // This document does not have any blob properties.
+    return;
+  }
+  const std::vector<std::string>& blob_property_paths =
+      type_blob_property_map.at(document.schema());
+
+  for (const std::string& blob_property_path : blob_property_paths) {
+    auto content_or = property_util::ExtractPropertyValuesFromDocument<
+        PropertyProto::BlobHandleProto>(document, blob_property_path);
+    if (content_or.ok()) {
+      for (const PropertyProto::BlobHandleProto& blob_handle :
+           content_or.ValueOrDie()) {
+        dead_blob_handles.erase(
+            encode_util::EncodeStringToCString(blob_handle.digest()) +
+            blob_handle.label());
+      }
+    }
+  }
+}
+
 }  // namespace
 
 std::string DocumentStore::MakeFingerprint(
@@ -273,13 +308,13 @@ DocumentStore::DocumentStore(const Filesystem* filesystem,
       use_persistent_hash_map_(use_persistent_hash_map),
       compression_level_(compression_level) {}
 
-libtextclassifier3::StatusOr<DocumentId> DocumentStore::Put(
+libtextclassifier3::StatusOr<DocumentStore::PutResult> DocumentStore::Put(
     const DocumentProto& document, int32_t num_tokens,
     PutDocumentStatsProto* put_document_stats) {
   return Put(DocumentProto(document), num_tokens, put_document_stats);
 }
 
-libtextclassifier3::StatusOr<DocumentId> DocumentStore::Put(
+libtextclassifier3::StatusOr<DocumentStore::PutResult> DocumentStore::Put(
     DocumentProto&& document, int32_t num_tokens,
     PutDocumentStatsProto* put_document_stats) {
   document.mutable_internal_fields()->set_length_in_tokens(num_tokens);
@@ -535,8 +570,9 @@ libtextclassifier3::Status DocumentStore::InitializeExistingDerivedFiles() {
   ICING_RETURN_IF_ERROR(
       usage_store_->TruncateTo(document_id_mapper_->num_elements()));
 
-  ICING_ASSIGN_OR_RETURN(Crc32 checksum, ComputeChecksum());
-  if (checksum.Get() != header.checksum) {
+  Crc32 expected_checksum(header.checksum);
+  ICING_ASSIGN_OR_RETURN(Crc32 checksum, GetChecksum());
+  if (checksum != expected_checksum) {
     return absl_ports::InternalError(
         "Combined checksum of DocStore was inconsistent");
   }
@@ -682,9 +718,7 @@ libtextclassifier3::Status DocumentStore::RegenerateDerivedFiles(
       usage_store_->TruncateTo(document_id_mapper_->num_elements()));
 
   // Write the header
-  ICING_ASSIGN_OR_RETURN(Crc32 checksum, ComputeChecksum());
-  ICING_RETURN_IF_ERROR(UpdateHeader(checksum));
-
+  ICING_RETURN_IF_ERROR(UpdateChecksum());
   return libtextclassifier3::Status::OK;
 }
 
@@ -831,12 +865,12 @@ libtextclassifier3::Status DocumentStore::ResetCorpusMapper() {
   return libtextclassifier3::Status::OK;
 }
 
-libtextclassifier3::StatusOr<Crc32> DocumentStore::ComputeChecksum() const {
+libtextclassifier3::StatusOr<Crc32> DocumentStore::GetChecksum() const {
   Crc32 total_checksum;
 
   // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
   // that can support error logging.
-  auto checksum_or = document_log_->ComputeChecksum();
+  auto checksum_or = document_log_->GetChecksum();
   if (!checksum_or.ok()) {
     ICING_LOG(ERROR) << checksum_or.status().error_message()
                      << "Failed to compute checksum of DocumentLog";
@@ -846,7 +880,7 @@ libtextclassifier3::StatusOr<Crc32> DocumentStore::ComputeChecksum() const {
 
   // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
   // that can support error logging.
-  checksum_or = document_key_mapper_->ComputeChecksum();
+  checksum_or = document_key_mapper_->GetChecksum();
   if (!checksum_or.ok()) {
     ICING_LOG(ERROR) << checksum_or.status().error_message()
                      << "Failed to compute checksum of DocumentKeyMapper";
@@ -856,7 +890,7 @@ libtextclassifier3::StatusOr<Crc32> DocumentStore::ComputeChecksum() const {
 
   // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
   // that can support error logging.
-  checksum_or = document_id_mapper_->ComputeChecksum();
+  checksum_or = document_id_mapper_->GetChecksum();
   if (!checksum_or.ok()) {
     ICING_LOG(ERROR) << checksum_or.status().error_message()
                      << "Failed to compute checksum of DocumentIdMapper";
@@ -866,7 +900,7 @@ libtextclassifier3::StatusOr<Crc32> DocumentStore::ComputeChecksum() const {
 
   // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
   // that can support error logging.
-  checksum_or = score_cache_->ComputeChecksum();
+  checksum_or = score_cache_->GetChecksum();
   if (!checksum_or.ok()) {
     ICING_LOG(ERROR) << checksum_or.status().error_message()
                      << "Failed to compute checksum of score cache";
@@ -876,7 +910,7 @@ libtextclassifier3::StatusOr<Crc32> DocumentStore::ComputeChecksum() const {
 
   // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
   // that can support error logging.
-  checksum_or = filter_cache_->ComputeChecksum();
+  checksum_or = filter_cache_->GetChecksum();
   if (!checksum_or.ok()) {
     ICING_LOG(ERROR) << checksum_or.status().error_message()
                      << "Failed to compute checksum of filter cache";
@@ -886,7 +920,7 @@ libtextclassifier3::StatusOr<Crc32> DocumentStore::ComputeChecksum() const {
 
   // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
   // that can support error logging.
-  checksum_or = namespace_mapper_->ComputeChecksum();
+  checksum_or = namespace_mapper_->GetChecksum();
   if (!checksum_or.ok()) {
     ICING_LOG(ERROR) << checksum_or.status().error_message()
                      << "Failed to compute checksum of namespace mapper";
@@ -896,7 +930,7 @@ libtextclassifier3::StatusOr<Crc32> DocumentStore::ComputeChecksum() const {
 
   // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
   // that can support error logging.
-  checksum_or = corpus_mapper_->ComputeChecksum();
+  checksum_or = corpus_mapper_->GetChecksum();
   if (!checksum_or.ok()) {
     ICING_LOG(ERROR) << checksum_or.status().error_message()
                      << "Failed to compute checksum of corpus mapper";
@@ -906,7 +940,7 @@ libtextclassifier3::StatusOr<Crc32> DocumentStore::ComputeChecksum() const {
 
   // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
   // that can support error logging.
-  checksum_or = corpus_score_cache_->ComputeChecksum();
+  checksum_or = corpus_score_cache_->GetChecksum();
   if (!checksum_or.ok()) {
     ICING_LOG(WARNING) << checksum_or.status().error_message()
                        << "Failed to compute checksum of score cache";
@@ -927,28 +961,111 @@ libtextclassifier3::StatusOr<Crc32> DocumentStore::ComputeChecksum() const {
   total_checksum.Append(std::to_string(namespace_mapper_checksum.Get()));
   total_checksum.Append(std::to_string(corpus_mapper_checksum.Get()));
   total_checksum.Append(std::to_string(corpus_score_cache_checksum.Get()));
-
   return total_checksum;
 }
 
-bool DocumentStore::HeaderExists() {
-  if (!filesystem_->FileExists(MakeHeaderFilename(base_dir_).c_str())) {
-    return false;
+libtextclassifier3::StatusOr<Crc32> DocumentStore::UpdateChecksum() {
+  Crc32 total_checksum;
+
+  // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
+  // that can support error logging.
+  auto checksum_or = document_log_->UpdateChecksum();
+  if (!checksum_or.ok()) {
+    ICING_LOG(ERROR) << checksum_or.status().error_message()
+                     << "Failed to compute checksum of DocumentLog";
+    return checksum_or.status();
   }
+  Crc32 document_log_checksum = std::move(checksum_or).ValueOrDie();
 
-  int64_t file_size =
-      filesystem_->GetFileSize(MakeHeaderFilename(base_dir_).c_str());
+  // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
+  // that can support error logging.
+  checksum_or = document_key_mapper_->UpdateChecksum();
+  if (!checksum_or.ok()) {
+    ICING_LOG(ERROR) << checksum_or.status().error_message()
+                     << "Failed to compute checksum of DocumentKeyMapper";
+    return checksum_or.status();
+  }
+  Crc32 document_key_mapper_checksum = std::move(checksum_or).ValueOrDie();
 
-  // If it's been truncated to size 0 before, we consider it to be a new file
-  return file_size != 0 && file_size != Filesystem::kBadFileSize;
-}
+  // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
+  // that can support error logging.
+  checksum_or = document_id_mapper_->UpdateChecksum();
+  if (!checksum_or.ok()) {
+    ICING_LOG(ERROR) << checksum_or.status().error_message()
+                     << "Failed to compute checksum of DocumentIdMapper";
+    return checksum_or.status();
+  }
+  Crc32 document_id_mapper_checksum = std::move(checksum_or).ValueOrDie();
+
+  // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
+  // that can support error logging.
+  checksum_or = score_cache_->UpdateChecksum();
+  if (!checksum_or.ok()) {
+    ICING_LOG(ERROR) << checksum_or.status().error_message()
+                     << "Failed to compute checksum of score cache";
+    return checksum_or.status();
+  }
+  Crc32 score_cache_checksum = std::move(checksum_or).ValueOrDie();
+
+  // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
+  // that can support error logging.
+  checksum_or = filter_cache_->UpdateChecksum();
+  if (!checksum_or.ok()) {
+    ICING_LOG(ERROR) << checksum_or.status().error_message()
+                     << "Failed to compute checksum of filter cache";
+    return checksum_or.status();
+  }
+  Crc32 filter_cache_checksum = std::move(checksum_or).ValueOrDie();
+
+  // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
+  // that can support error logging.
+  checksum_or = namespace_mapper_->UpdateChecksum();
+  if (!checksum_or.ok()) {
+    ICING_LOG(ERROR) << checksum_or.status().error_message()
+                     << "Failed to compute checksum of namespace mapper";
+    return checksum_or.status();
+  }
+  Crc32 namespace_mapper_checksum = std::move(checksum_or).ValueOrDie();
+
+  // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
+  // that can support error logging.
+  checksum_or = corpus_mapper_->UpdateChecksum();
+  if (!checksum_or.ok()) {
+    ICING_LOG(ERROR) << checksum_or.status().error_message()
+                     << "Failed to compute checksum of corpus mapper";
+    return checksum_or.status();
+  }
+  Crc32 corpus_mapper_checksum = std::move(checksum_or).ValueOrDie();
+
+  // TODO(b/144458732): Implement a more robust version of TC_ASSIGN_OR_RETURN
+  // that can support error logging.
+  checksum_or = corpus_score_cache_->UpdateChecksum();
+  if (!checksum_or.ok()) {
+    ICING_LOG(WARNING) << checksum_or.status().error_message()
+                       << "Failed to compute checksum of score cache";
+    return checksum_or.status();
+  }
+  Crc32 corpus_score_cache_checksum = std::move(checksum_or).ValueOrDie();
+
+  // NOTE: We purposely don't include usage_store checksum here because we can't
+  // regenerate it from ground truth documents. If it gets corrupted, we'll just
+  // clear all usage reports, but we shouldn't throw everything else in the
+  // document store out.
+
+  total_checksum.Append(std::to_string(document_log_checksum.Get()));
+  total_checksum.Append(std::to_string(document_key_mapper_checksum.Get()));
+  total_checksum.Append(std::to_string(document_id_mapper_checksum.Get()));
+  total_checksum.Append(std::to_string(score_cache_checksum.Get()));
+  total_checksum.Append(std::to_string(filter_cache_checksum.Get()));
+  total_checksum.Append(std::to_string(namespace_mapper_checksum.Get()));
+  total_checksum.Append(std::to_string(corpus_mapper_checksum.Get()));
+  total_checksum.Append(std::to_string(corpus_score_cache_checksum.Get()));
 
-libtextclassifier3::Status DocumentStore::UpdateHeader(const Crc32& checksum) {
   // Write the header
   DocumentStore::Header header;
   header.magic =
       DocumentStore::Header::GetCurrentMagic(namespace_id_fingerprint_);
-  header.checksum = checksum.Get();
+  header.checksum = total_checksum.Get();
 
   // This should overwrite the header.
   ScopedFd sfd(
@@ -959,11 +1076,24 @@ libtextclassifier3::Status DocumentStore::UpdateHeader(const Crc32& checksum) {
     return absl_ports::InternalError(absl_ports::StrCat(
         "Failed to write DocStore header: ", MakeHeaderFilename(base_dir_)));
   }
-  return libtextclassifier3::Status::OK;
+  return total_checksum;
 }
 
-libtextclassifier3::StatusOr<DocumentId> DocumentStore::InternalPut(
-    DocumentProto&& document, PutDocumentStatsProto* put_document_stats) {
+bool DocumentStore::HeaderExists() {
+  if (!filesystem_->FileExists(MakeHeaderFilename(base_dir_).c_str())) {
+    return false;
+  }
+
+  int64_t file_size =
+      filesystem_->GetFileSize(MakeHeaderFilename(base_dir_).c_str());
+
+  // If it's been truncated to size 0 before, we consider it to be a new file
+  return file_size != 0 && file_size != Filesystem::kBadFileSize;
+}
+
+libtextclassifier3::StatusOr<DocumentStore::PutResult>
+DocumentStore::InternalPut(DocumentProto&& document,
+                           PutDocumentStatsProto* put_document_stats) {
   std::unique_ptr<Timer> put_timer = clock_.GetNewTimer();
   ICING_RETURN_IF_ERROR(document_validator_.Validate(document));
 
@@ -1014,6 +1144,8 @@ libtextclassifier3::StatusOr<DocumentId> DocumentStore::InternalPut(
         "Exceeded maximum number of documents. Try calling Optimize to reclaim "
         "some space.");
   }
+  PutResult put_result;
+  put_result.new_document_id = new_document_id;
 
   // Update namespace maps
   ICING_ASSIGN_OR_RETURN(
@@ -1051,6 +1183,7 @@ libtextclassifier3::StatusOr<DocumentId> DocumentStore::InternalPut(
                                           expiration_timestamp_ms)));
 
   if (old_document_id_or.ok()) {
+    put_result.was_replacement = true;
     // The old document exists, copy over the usage scores and delete the old
     // document.
     DocumentId old_document_id = old_document_id_or.ValueOrDie();
@@ -1074,7 +1207,7 @@ libtextclassifier3::StatusOr<DocumentId> DocumentStore::InternalPut(
         put_timer->GetElapsedMilliseconds());
   }
 
-  return new_document_id;
+  return put_result;
 }
 
 libtextclassifier3::StatusOr<DocumentProto> DocumentStore::Get(
@@ -1115,9 +1248,9 @@ libtextclassifier3::StatusOr<DocumentProto> DocumentStore::Get(
 libtextclassifier3::StatusOr<DocumentProto> DocumentStore::Get(
     DocumentId document_id, bool clear_internal_fields) const {
   int64_t current_time_ms = clock_.GetSystemTimeMilliseconds();
-  auto document_filter_data_optional_ =
+  auto document_filter_data_optional =
       GetAliveDocumentFilterData(document_id, current_time_ms);
-  if (!document_filter_data_optional_) {
+  if (!document_filter_data_optional) {
     // The document doesn't exist. Let's check if the document id is invalid, we
     // will return InvalidArgumentError. Otherwise we should return NOT_FOUND
     // error.
@@ -1285,9 +1418,9 @@ libtextclassifier3::Status DocumentStore::Delete(
 
 libtextclassifier3::Status DocumentStore::Delete(DocumentId document_id,
                                                  int64_t current_time_ms) {
-  auto document_filter_data_optional_ =
+  auto document_filter_data_optional =
       GetAliveDocumentFilterData(document_id, current_time_ms);
-  if (!document_filter_data_optional_) {
+  if (!document_filter_data_optional) {
     // The document doesn't exist. We should return InvalidArgumentError if the
     // document id is invalid. Otherwise we should return NOT_FOUND error.
     if (!IsDocumentIdValid(document_id)) {
@@ -1555,11 +1688,14 @@ libtextclassifier3::StatusOr<int> DocumentStore::BatchDelete(
 
 libtextclassifier3::Status DocumentStore::PersistToDisk(
     PersistType::Code persist_type) {
+  ICING_RETURN_IF_ERROR(document_log_->PersistToDisk());
   if (persist_type == PersistType::LITE) {
     // only persist the document log.
-    return document_log_->PersistToDisk();
+    return libtextclassifier3::Status::OK;
+  }
+  if (persist_type == PersistType::RECOVERY_PROOF) {
+    return UpdateChecksum().status();
   }
-  ICING_RETURN_IF_ERROR(document_log_->PersistToDisk());
   ICING_RETURN_IF_ERROR(document_key_mapper_->PersistToDisk());
   ICING_RETURN_IF_ERROR(document_id_mapper_->PersistToDisk());
   ICING_RETURN_IF_ERROR(score_cache_->PersistToDisk());
@@ -1570,9 +1706,7 @@ libtextclassifier3::Status DocumentStore::PersistToDisk(
   ICING_RETURN_IF_ERROR(corpus_score_cache_->PersistToDisk());
 
   // Update the combined checksum and write to header file.
-  ICING_ASSIGN_OR_RETURN(Crc32 checksum, ComputeChecksum());
-  ICING_RETURN_IF_ERROR(UpdateHeader(checksum));
-
+  ICING_RETURN_IF_ERROR(UpdateChecksum());
   return libtextclassifier3::Status::OK;
 }
 
@@ -1837,9 +1971,10 @@ libtextclassifier3::Status DocumentStore::Optimize() {
 }
 
 libtextclassifier3::StatusOr<DocumentStore::OptimizeResult>
-DocumentStore::OptimizeInto(const std::string& new_directory,
-                            const LanguageSegmenter* lang_segmenter,
-                            OptimizeStatsProto* stats) const {
+DocumentStore::OptimizeInto(
+    const std::string& new_directory, const LanguageSegmenter* lang_segmenter,
+    std::unordered_set<std::string>&& potentially_optimizable_blob_handles,
+    OptimizeStatsProto* stats) const {
   // Validates directory
   if (new_directory == base_dir_) {
     return absl_ports::InvalidArgumentError(
@@ -1861,9 +1996,21 @@ DocumentStore::OptimizeInto(const std::string& new_directory,
   int num_deleted_documents = 0;
   int num_expired_documents = 0;
   UsageStore::UsageScores default_usage;
-
   OptimizeResult result;
   result.document_id_old_to_new.resize(document_cnt, kInvalidDocumentId);
+  result.dead_blob_handles = std::move(potentially_optimizable_blob_handles);
+
+  // Get the blob property map from the schema store.
+  auto type_blob_property_map_or = schema_store_->ConstructBlobPropertyMap();
+  if (num_documents() == 0) {
+    // If we fail to retrieve this map when there *are* documents in
+    // doc store, then something is seriously wrong. Return error.
+    return result;
+  }
+  std::unordered_map<std::string, std::vector<std::string>>
+      type_blob_property_map =
+          std::move(type_blob_property_map_or).ValueOrDie();
+
   int64_t current_time_ms = clock_.GetSystemTimeMilliseconds();
   for (DocumentId document_id = 0; document_id < document_cnt; document_id++) {
     auto document_or = Get(document_id, /*clear_internal_fields=*/false);
@@ -1885,8 +2032,12 @@ DocumentStore::OptimizeInto(const std::string& new_directory,
 
     // Guaranteed to have a document now.
     DocumentProto document_to_keep = std::move(document_or).ValueOrDie();
+    // Remove blobs that still have reference are removed from the
+    // expired_blob_handles. So that all remaining are dead blob.
+    RemoveAliveBlobHandles(document_to_keep, type_blob_property_map,
+                           result.dead_blob_handles);
 
-    libtextclassifier3::StatusOr<DocumentId> new_document_id_or;
+    libtextclassifier3::StatusOr<PutResult> put_result_or;
     if (document_to_keep.internal_fields().length_in_tokens() == 0) {
       auto tokenized_document_or = TokenizedDocument::Create(
           schema_store_, lang_segmenter, document_to_keep);
@@ -1898,36 +2049,33 @@ DocumentStore::OptimizeInto(const std::string& new_directory,
       }
       TokenizedDocument tokenized_document(
           std::move(tokenized_document_or).ValueOrDie());
-      new_document_id_or = new_doc_store->Put(
+      put_result_or = new_doc_store->Put(
           std::move(document_to_keep), tokenized_document.num_string_tokens());
     } else {
       // TODO(b/144458732): Implement a more robust version of
       // TC_ASSIGN_OR_RETURN that can support error logging.
-      new_document_id_or =
-          new_doc_store->InternalPut(std::move(document_to_keep));
+      put_result_or = new_doc_store->InternalPut(std::move(document_to_keep));
     }
-    if (!new_document_id_or.ok()) {
-      ICING_LOG(ERROR) << new_document_id_or.status().error_message()
+    if (!put_result_or.ok()) {
+      ICING_LOG(ERROR) << put_result_or.status().error_message()
                        << "Failed to write into new document store";
-      return new_document_id_or.status();
+      return put_result_or.status();
     }
 
-    result.document_id_old_to_new[document_id] =
-        new_document_id_or.ValueOrDie();
+    DocumentId new_document_id = put_result_or.ValueOrDie().new_document_id;
+    result.document_id_old_to_new[document_id] = new_document_id;
 
     // Copy over usage scores.
     ICING_ASSIGN_OR_RETURN(UsageStore::UsageScores usage_scores,
                            usage_store_->GetUsageScores(document_id));
     if (!(usage_scores == default_usage)) {
-      // If the usage scores for this document are the default (no usage), then
-      // don't bother setting it. No need to possibly allocate storage if
+      // If the usage scores for this document are the default (no usage),
+      // then don't bother setting it. No need to possibly allocate storage if
       // there's nothing interesting to store.
-      DocumentId new_document_id = new_document_id_or.ValueOrDie();
       ICING_RETURN_IF_ERROR(
           new_doc_store->SetUsageScores(new_document_id, usage_scores));
     }
   }
-
   // Construct namespace_id_old_to_new
   int namespace_cnt = namespace_mapper_->num_keys();
   std::unordered_map<NamespaceId, std::string> old_namespaces =
@@ -2136,7 +2284,7 @@ libtextclassifier3::StatusOr<DocumentDebugInfoProto>
 DocumentStore::GetDebugInfo(int verbosity) const {
   DocumentDebugInfoProto debug_info;
   *debug_info.mutable_document_storage_info() = GetStorageInfo();
-  ICING_ASSIGN_OR_RETURN(Crc32 crc, ComputeChecksum());
+  ICING_ASSIGN_OR_RETURN(Crc32 crc, GetChecksum());
   debug_info.set_crc(crc.Get());
   if (verbosity > 0) {
     ICING_ASSIGN_OR_RETURN(
diff --git a/icing/store/document-store.h b/icing/store/document-store.h
index c228e8b..0caead0 100644
--- a/icing/store/document-store.h
+++ b/icing/store/document-store.h
@@ -19,11 +19,11 @@
 #include <memory>
 #include <string>
 #include <string_view>
+#include <unordered_set>
 #include <vector>
 
 #include "icing/text_classifier/lib3/utils/base/status.h"
 #include "icing/text_classifier/lib3/utils/base/statusor.h"
-#include "icing/file/file-backed-proto-log.h"
 #include "icing/file/file-backed-vector.h"
 #include "icing/file/filesystem.h"
 #include "icing/file/portable-file-backed-proto-log.h"
@@ -180,17 +180,23 @@ class DocumentStore {
   // If put_document_stats is present, the fields related to DocumentStore will
   // be populated.
   //
-  // Returns:
-  //   A newly generated document id on success
-  //   RESOURCE_EXHAUSED if exceeds maximum number of allowed documents
-  //   FAILED_PRECONDITION if schema hasn't been set yet
-  //   NOT_FOUND if the schema_type or a property config of the document doesn't
-  //     exist in schema
-  //   INTERNAL_ERROR on IO error
-  libtextclassifier3::StatusOr<DocumentId> Put(
+  //  Returns:
+  //   - On success, a PutResult with the DocumentId of the newly added document
+  //     and a bool indicating whether this is a new document or a replacement
+  //     of an existing document.
+  //   - RESOURCE_EXHAUSTED if exceeds maximum number of allowed documents
+  //   - FAILED_PRECONDITION if schema hasn't been set yet
+  //   - NOT_FOUND if the schema_type or a property config of the document
+  //     doesn't exist in schema
+  //   - INTERNAL_ERROR on IO error
+  struct PutResult {
+    DocumentId new_document_id = kInvalidDocumentId;
+    bool was_replacement = false;
+  };
+  libtextclassifier3::StatusOr<PutResult> Put(
       const DocumentProto& document, int32_t num_tokens = 0,
       PutDocumentStatsProto* put_document_stats = nullptr);
-  libtextclassifier3::StatusOr<DocumentId> Put(
+  libtextclassifier3::StatusOr<PutResult> Put(
       DocumentProto&& document, int32_t num_tokens = 0,
       PutDocumentStatsProto* put_document_stats = nullptr);
 
@@ -472,6 +478,9 @@ class DocumentStore {
     // should rebuild index instead of adopting the id changes via the 2 vectors
     // above. It will be set to true if finding any id inconsistency.
     bool should_rebuild_index = false;
+
+    // A set of blob handles that are dead and need to be removed.
+    std::unordered_set<std::string> dead_blob_handles;
   };
   // Copy data from current base directory into a new directory. Any outdated or
   // deleted data won't be copied. During the process, document/namespace ids
@@ -492,6 +501,7 @@ class DocumentStore {
   //   INTERNAL_ERROR on IO error
   libtextclassifier3::StatusOr<OptimizeResult> OptimizeInto(
       const std::string& new_directory, const LanguageSegmenter* lang_segmenter,
+      std::unordered_set<std::string>&& expired_blob_handles,
       OptimizeStatsProto* stats = nullptr) const;
 
   // Calculates status for a potential Optimize call. Includes how many docs
@@ -503,13 +513,20 @@ class DocumentStore {
   //   INTERNAL_ERROR on IO error
   libtextclassifier3::StatusOr<OptimizeInfo> GetOptimizeInfo() const;
 
-  // Computes the combined checksum of the document store - includes the ground
-  // truth and all derived files.
+  // Update, replace and persist the header file. Creates the header file if it
+  // doesn't exist.
+  //
+  // Returns:
+  //   OK on success
+  //   INTERNAL on I/O error
+  libtextclassifier3::StatusOr<Crc32> UpdateChecksum();
+
+  // Calculates and returns the checksum of the document store.
   //
   // Returns:
-  //   Combined checksum on success
-  //   INTERNAL_ERROR on compute error
-  libtextclassifier3::StatusOr<Crc32> ComputeChecksum() const;
+  //   OK on success
+  //   INTERNAL on I/O error
+  libtextclassifier3::StatusOr<Crc32> GetChecksum() const;
 
   // Get debug information for the document store.
   // verbosity <= 0, simplest debug information
@@ -699,15 +716,7 @@ class DocumentStore {
   // if it doesn't exist.
   bool HeaderExists();
 
-  // Update, replace and persist the header file. Creates the header file if it
-  // doesn't exist.
-  //
-  // Returns:
-  //   OK on success
-  //   INTERNAL on I/O error
-  libtextclassifier3::Status UpdateHeader(const Crc32& checksum);
-
-  libtextclassifier3::StatusOr<DocumentId> InternalPut(
+  libtextclassifier3::StatusOr<PutResult> InternalPut(
       DocumentProto&& document,
       PutDocumentStatsProto* put_document_stats = nullptr);
 
diff --git a/icing/store/document-store_benchmark.cc b/icing/store/document-store_benchmark.cc
index d78b44a..3523e14 100644
--- a/icing/store/document-store_benchmark.cc
+++ b/icing/store/document-store_benchmark.cc
@@ -307,7 +307,7 @@ void BM_Create(benchmark::State& state) {
 }
 BENCHMARK(BM_Create);
 
-void BM_ComputeChecksum(benchmark::State& state) {
+void BM_UpdateChecksum(benchmark::State& state) {
   Filesystem filesystem;
   Clock clock;
 
@@ -331,10 +331,10 @@ void BM_ComputeChecksum(benchmark::State& state) {
   ICING_ASSERT_OK(document_store->PersistToDisk(PersistType::LITE));
 
   for (auto s : state) {
-    benchmark::DoNotOptimize(document_store->ComputeChecksum());
+    benchmark::DoNotOptimize(document_store->UpdateChecksum());
   }
 }
-BENCHMARK(BM_ComputeChecksum);
+BENCHMARK(BM_UpdateChecksum);
 
 }  // namespace
 
diff --git a/icing/store/document-store_test.cc b/icing/store/document-store_test.cc
index 2d4cd99..776f19f 100644
--- a/icing/store/document-store_test.cc
+++ b/icing/store/document-store_test.cc
@@ -15,11 +15,12 @@
 #include "icing/store/document-store.h"
 
 #include <cstdint>
-#include <filesystem>
 #include <limits>
 #include <memory>
 #include <optional>
 #include <string>
+#include <unordered_set>
+#include <vector>
 
 #include "icing/text_classifier/lib3/utils/base/status.h"
 #include "gmock/gmock.h"
@@ -303,10 +304,14 @@ TEST_P(DocumentStoreTest, PutAndGetInSameNamespaceOk) {
       std::move(create_result.document_store);
 
   // Both documents have namespace of "icing"
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store->Put(test_document1_));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
-                             doc_store->Put(DocumentProto(test_document2_)));
+  EXPECT_FALSE(put_result1.was_replacement);
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
+                             doc_store->Put(test_document2_));
+  EXPECT_FALSE(put_result2.was_replacement);
+  DocumentId document_id2 = put_result2.new_document_id;
 
   EXPECT_THAT(doc_store->Get(document_id1),
               IsOkAndHolds(EqualsProto(test_document1_)));
@@ -334,10 +339,14 @@ TEST_P(DocumentStoreTest, PutAndGetAcrossNamespacesOk) {
                                    .SetCreationTimestampMs(0)
                                    .Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store->Put(foo_document));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  EXPECT_FALSE(put_result1.was_replacement);
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              doc_store->Put(DocumentProto(bar_document)));
+  EXPECT_FALSE(put_result2.was_replacement);
+  DocumentId document_id2 = put_result2.new_document_id;
 
   EXPECT_THAT(doc_store->Get(document_id1),
               IsOkAndHolds(EqualsProto(foo_document)));
@@ -359,10 +368,14 @@ TEST_P(DocumentStoreTest, PutSameKey) {
   DocumentProto document1 = DocumentProto(test_document1_);
   DocumentProto document2 = DocumentProto(test_document1_);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store->Put(document1));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  EXPECT_FALSE(put_result1.was_replacement);
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              doc_store->Put(document2));
+  DocumentId document_id2 = put_result2.new_document_id;
+  EXPECT_TRUE(put_result2.was_replacement);
   EXPECT_THAT(document_id1, Not(document_id2));
   // document2 overrides document1, so document_id1 becomes invalid
   EXPECT_THAT(doc_store->Get(document_id1),
@@ -373,7 +386,10 @@ TEST_P(DocumentStoreTest, PutSameKey) {
   // Makes sure that old doc ids are not getting reused.
   DocumentProto document3 = DocumentProto(test_document1_);
   document3.set_uri("another/uri/1");
-  EXPECT_THAT(doc_store->Put(document3), IsOkAndHolds(Not(document_id1)));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result3,
+                             doc_store->Put(document3));
+  EXPECT_FALSE(put_result3.was_replacement);
+  EXPECT_THAT(put_result3.new_document_id, Not(document_id1));
 }
 
 TEST_P(DocumentStoreTest, IsDocumentExistingWithoutStatus) {
@@ -384,10 +400,14 @@ TEST_P(DocumentStoreTest, IsDocumentExistingWithoutStatus) {
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
-                             doc_store->Put(DocumentProto(test_document1_)));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
-                             doc_store->Put(DocumentProto(test_document2_)));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
+                             doc_store->Put(test_document1_));
+  EXPECT_FALSE(put_result1.was_replacement);
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
+                             doc_store->Put(test_document2_));
+  EXPECT_FALSE(put_result2.was_replacement);
+  DocumentId document_id2 = put_result2.new_document_id;
 
   EXPECT_TRUE(doc_store->GetAliveDocumentFilterData(
       document_id1, fake_clock_.GetSystemTimeMilliseconds()));
@@ -476,8 +496,10 @@ TEST_P(DocumentStoreTest, GetInvalidDocumentId) {
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
-                             doc_store->Put(DocumentProto(test_document1_)));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                             doc_store->Put(test_document1_));
+  EXPECT_FALSE(put_result.was_replacement);
+  DocumentId document_id = put_result.new_document_id;
 
   DocumentId invalid_document_id_negative = -1;
   EXPECT_THAT(doc_store->Get(invalid_document_id_negative),
@@ -763,32 +785,40 @@ TEST_P(DocumentStoreTest, DeleteBySchemaTypeOk) {
                                        .SetSchema("email")
                                        .SetCreationTimestampMs(1)
                                        .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_1_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult email_put_result1,
                              document_store->Put(email_document_1));
+  EXPECT_FALSE(email_put_result1.was_replacement);
+  DocumentId email_1_document_id = email_put_result1.new_document_id;
 
   DocumentProto email_document_2 = DocumentBuilder()
                                        .SetKey("namespace2", "2")
                                        .SetSchema("email")
                                        .SetCreationTimestampMs(1)
                                        .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_2_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult email_put_result2,
                              document_store->Put(email_document_2));
+  EXPECT_FALSE(email_put_result2.was_replacement);
+  DocumentId email_2_document_id = email_put_result2.new_document_id;
 
   DocumentProto message_document = DocumentBuilder()
                                        .SetKey("namespace", "3")
                                        .SetSchema("message")
                                        .SetCreationTimestampMs(1)
                                        .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId message_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult message_put_result,
                              document_store->Put(message_document));
+  EXPECT_FALSE(message_put_result.was_replacement);
+  DocumentId message_document_id = message_put_result.new_document_id;
 
   DocumentProto person_document = DocumentBuilder()
                                       .SetKey("namespace", "4")
                                       .SetSchema("person")
                                       .SetCreationTimestampMs(1)
                                       .Build();
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId person_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult person_put_result,
                              document_store->Put(person_document));
+  EXPECT_FALSE(person_put_result.was_replacement);
+  DocumentId person_document_id = person_put_result.new_document_id;
 
   // Delete the "email" type and ensure that it works across both
   // email_document's namespaces. And that other documents aren't affected.
@@ -904,11 +934,14 @@ TEST_P(DocumentStoreTest, DeleteBySchemaTypeRecoversOk) {
     std::unique_ptr<DocumentStore> document_store =
         std::move(create_result.document_store);
 
-    ICING_ASSERT_OK_AND_ASSIGN(email_document_id,
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult email_put_result,
                                document_store->Put(email_document));
-    ICING_ASSERT_OK_AND_ASSIGN(message_document_id,
+    EXPECT_FALSE(email_put_result.was_replacement);
+    email_document_id = email_put_result.new_document_id;
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult message_put_result,
                                document_store->Put(message_document));
-
+    EXPECT_FALSE(message_put_result.was_replacement);
+    message_document_id = message_put_result.new_document_id;
     // Delete "email". "message" documents should still be retrievable.
     DocumentStore::DeleteByGroupResult group_result =
         document_store->DeleteBySchemaType("email");
@@ -998,10 +1031,14 @@ TEST_P(DocumentStoreTest, DeletedSchemaTypeFromSchemaStoreRecoversOk) {
     std::unique_ptr<DocumentStore> document_store =
         std::move(create_result.document_store);
 
-    ICING_ASSERT_OK_AND_ASSIGN(email_document_id,
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult email_put_result,
                                document_store->Put(email_document));
-    ICING_ASSERT_OK_AND_ASSIGN(message_document_id,
+    EXPECT_FALSE(email_put_result.was_replacement);
+    email_document_id = email_put_result.new_document_id;
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult message_put_result,
                                document_store->Put(message_document));
+    EXPECT_FALSE(message_put_result.was_replacement);
+    message_document_id = message_put_result.new_document_id;
 
     // Delete "email". "message" documents should still be retrievable.
     DocumentStore::DeleteByGroupResult group_result =
@@ -1094,10 +1131,11 @@ TEST_P(DocumentStoreTest, OptimizeIntoSingleNamespace) {
       filesystem_.GetFileSize(original_document_log.c_str());
 
   // Optimizing into the same directory is not allowed
-  EXPECT_THAT(
-      doc_store->OptimizeInto(document_store_dir_, lang_segmenter_.get()),
-      StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT,
-               HasSubstr("directory is the same")));
+  EXPECT_THAT(doc_store->OptimizeInto(
+                  document_store_dir_, lang_segmenter_.get(),
+                  /*expired_blob_handles=*/std::unordered_set<std::string>()),
+              StatusIs(libtextclassifier3::StatusCode::INVALID_ARGUMENT,
+                       HasSubstr("directory is the same")));
 
   std::string optimized_dir = document_store_dir_ + "_optimize";
   std::string optimized_document_log =
@@ -1109,7 +1147,9 @@ TEST_P(DocumentStoreTest, OptimizeIntoSingleNamespace) {
   ASSERT_TRUE(filesystem_.CreateDirectoryRecursively(optimized_dir.c_str()));
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::OptimizeResult optimize_result1,
-      doc_store->OptimizeInto(optimized_dir, lang_segmenter_.get()));
+      doc_store->OptimizeInto(
+          optimized_dir, lang_segmenter_.get(),
+          /*expired_blob_handles=*/std::unordered_set<std::string>()));
   EXPECT_THAT(optimize_result1.document_id_old_to_new, ElementsAre(0, 1, 2));
   EXPECT_THAT(optimize_result1.namespace_id_old_to_new, ElementsAre(0));
   EXPECT_THAT(optimize_result1.should_rebuild_index, IsFalse());
@@ -1126,7 +1166,9 @@ TEST_P(DocumentStoreTest, OptimizeIntoSingleNamespace) {
   // DocumentId 0 is removed.
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::OptimizeResult optimize_result2,
-      doc_store->OptimizeInto(optimized_dir, lang_segmenter_.get()));
+      doc_store->OptimizeInto(
+          optimized_dir, lang_segmenter_.get(),
+          /*expired_blob_handles=*/std::unordered_set<std::string>()));
   EXPECT_THAT(optimize_result2.document_id_old_to_new,
               ElementsAre(kInvalidDocumentId, 0, 1));
   EXPECT_THAT(optimize_result2.namespace_id_old_to_new, ElementsAre(0));
@@ -1146,7 +1188,9 @@ TEST_P(DocumentStoreTest, OptimizeIntoSingleNamespace) {
   // DocumentId 0 is removed, and DocumentId 2 is expired.
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::OptimizeResult optimize_result3,
-      doc_store->OptimizeInto(optimized_dir, lang_segmenter_.get()));
+      doc_store->OptimizeInto(
+          optimized_dir, lang_segmenter_.get(),
+          /*expired_blob_handles=*/std::unordered_set<std::string>()));
   EXPECT_THAT(optimize_result3.document_id_old_to_new,
               ElementsAre(kInvalidDocumentId, 0, kInvalidDocumentId));
   EXPECT_THAT(optimize_result3.namespace_id_old_to_new, ElementsAre(0));
@@ -1165,7 +1209,9 @@ TEST_P(DocumentStoreTest, OptimizeIntoSingleNamespace) {
   // id will be invalid.
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::OptimizeResult optimize_result4,
-      doc_store->OptimizeInto(optimized_dir, lang_segmenter_.get()));
+      doc_store->OptimizeInto(
+          optimized_dir, lang_segmenter_.get(),
+          /*expired_blob_handles=*/std::unordered_set<std::string>()));
   EXPECT_THAT(
       optimize_result4.document_id_old_to_new,
       ElementsAre(kInvalidDocumentId, kInvalidDocumentId, kInvalidDocumentId));
@@ -1245,7 +1291,9 @@ TEST_P(DocumentStoreTest, OptimizeIntoMultipleNamespaces) {
   ASSERT_TRUE(filesystem_.CreateDirectoryRecursively(optimized_dir.c_str()));
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::OptimizeResult optimize_result1,
-      doc_store->OptimizeInto(optimized_dir, lang_segmenter_.get()));
+      doc_store->OptimizeInto(
+          optimized_dir, lang_segmenter_.get(),
+          /*expired_blob_handles=*/std::unordered_set<std::string>()));
   EXPECT_THAT(optimize_result1.document_id_old_to_new,
               ElementsAre(0, 1, 2, 3, 4));
   EXPECT_THAT(optimize_result1.namespace_id_old_to_new, ElementsAre(0, 1, 2));
@@ -1270,7 +1318,9 @@ TEST_P(DocumentStoreTest, OptimizeIntoMultipleNamespaces) {
                                     fake_clock_.GetSystemTimeMilliseconds()));
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::OptimizeResult optimize_result2,
-      doc_store->OptimizeInto(optimized_dir, lang_segmenter_.get()));
+      doc_store->OptimizeInto(
+          optimized_dir, lang_segmenter_.get(),
+          /*expired_blob_handles=*/std::unordered_set<std::string>()));
   EXPECT_THAT(optimize_result2.document_id_old_to_new,
               ElementsAre(kInvalidDocumentId, 0, 1, 2, 3));
   EXPECT_THAT(optimize_result2.namespace_id_old_to_new, ElementsAre(0, 1, 2));
@@ -1295,7 +1345,9 @@ TEST_P(DocumentStoreTest, OptimizeIntoMultipleNamespaces) {
                                     fake_clock_.GetSystemTimeMilliseconds()));
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::OptimizeResult optimize_result3,
-      doc_store->OptimizeInto(optimized_dir, lang_segmenter_.get()));
+      doc_store->OptimizeInto(
+          optimized_dir, lang_segmenter_.get(),
+          /*expired_blob_handles=*/std::unordered_set<std::string>()));
   EXPECT_THAT(optimize_result3.document_id_old_to_new,
               ElementsAre(kInvalidDocumentId, kInvalidDocumentId, 0, 1, 2));
   EXPECT_THAT(optimize_result3.namespace_id_old_to_new, ElementsAre(1, 0, 2));
@@ -1319,7 +1371,9 @@ TEST_P(DocumentStoreTest, OptimizeIntoMultipleNamespaces) {
                                     fake_clock_.GetSystemTimeMilliseconds()));
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::OptimizeResult optimize_result4,
-      doc_store->OptimizeInto(optimized_dir, lang_segmenter_.get()));
+      doc_store->OptimizeInto(
+          optimized_dir, lang_segmenter_.get(),
+          /*expired_blob_handles=*/std::unordered_set<std::string>()));
   EXPECT_THAT(optimize_result4.document_id_old_to_new,
               ElementsAre(kInvalidDocumentId, kInvalidDocumentId, 0,
                           kInvalidDocumentId, 1));
@@ -1344,7 +1398,9 @@ TEST_P(DocumentStoreTest, OptimizeIntoMultipleNamespaces) {
                                     fake_clock_.GetSystemTimeMilliseconds()));
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::OptimizeResult optimize_result5,
-      doc_store->OptimizeInto(optimized_dir, lang_segmenter_.get()));
+      doc_store->OptimizeInto(
+          optimized_dir, lang_segmenter_.get(),
+          /*expired_blob_handles=*/std::unordered_set<std::string>()));
   EXPECT_THAT(optimize_result5.document_id_old_to_new,
               ElementsAre(kInvalidDocumentId, kInvalidDocumentId, 0,
                           kInvalidDocumentId, kInvalidDocumentId));
@@ -1368,7 +1424,9 @@ TEST_P(DocumentStoreTest, OptimizeIntoMultipleNamespaces) {
                                     fake_clock_.GetSystemTimeMilliseconds()));
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::OptimizeResult optimize_result6,
-      doc_store->OptimizeInto(optimized_dir, lang_segmenter_.get()));
+      doc_store->OptimizeInto(
+          optimized_dir, lang_segmenter_.get(),
+          /*expired_blob_handles=*/std::unordered_set<std::string>()));
   EXPECT_THAT(
       optimize_result6.document_id_old_to_new,
       ElementsAre(kInvalidDocumentId, kInvalidDocumentId, kInvalidDocumentId,
@@ -1395,7 +1453,9 @@ TEST_P(DocumentStoreTest, OptimizeIntoForEmptyDocumentStore) {
 
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::OptimizeResult optimize_result,
-      doc_store->OptimizeInto(optimized_dir, lang_segmenter_.get()));
+      doc_store->OptimizeInto(
+          optimized_dir, lang_segmenter_.get(),
+          /*expired_blob_handles=*/std::unordered_set<std::string>()));
   EXPECT_THAT(optimize_result.document_id_old_to_new, IsEmpty());
   EXPECT_THAT(optimize_result.namespace_id_old_to_new, IsEmpty());
   EXPECT_THAT(optimize_result.should_rebuild_index, IsFalse());
@@ -1413,11 +1473,15 @@ TEST_P(DocumentStoreTest, ShouldRecoverFromDataLoss) {
         std::move(create_result.document_store);
 
     ICING_ASSERT_OK_AND_ASSIGN(
-        document_id1,
+        DocumentStore::PutResult put_result1,
         doc_store->Put(DocumentProto(test_document1_), /*num_tokens=*/4));
+    EXPECT_FALSE(put_result1.was_replacement);
+    document_id1 = put_result1.new_document_id;
     ICING_ASSERT_OK_AND_ASSIGN(
-        document_id2,
+        DocumentStore::PutResult put_result2,
         doc_store->Put(DocumentProto(test_document2_), /*num_tokens=*/4));
+    EXPECT_FALSE(put_result2.was_replacement);
+    document_id2 = put_result2.new_document_id;
     EXPECT_THAT(doc_store->Get(document_id1),
                 IsOkAndHolds(EqualsProto(test_document1_)));
     EXPECT_THAT(doc_store->Get(document_id2),
@@ -1504,11 +1568,15 @@ TEST_P(DocumentStoreTest, ShouldRecoverFromCorruptDerivedFile) {
         std::move(create_result.document_store);
 
     ICING_ASSERT_OK_AND_ASSIGN(
-        document_id1,
+        DocumentStore::PutResult put_result1,
         doc_store->Put(DocumentProto(test_document1_), /*num_tokens=*/4));
+    EXPECT_FALSE(put_result1.was_replacement);
+    document_id1 = put_result1.new_document_id;
     ICING_ASSERT_OK_AND_ASSIGN(
-        document_id2,
+        DocumentStore::PutResult put_result2,
         doc_store->Put(DocumentProto(test_document2_), /*num_tokens=*/4));
+    EXPECT_FALSE(put_result2.was_replacement);
+    document_id2 = put_result2.new_document_id;
     EXPECT_THAT(doc_store->Get(document_id1),
                 IsOkAndHolds(EqualsProto(test_document1_)));
     EXPECT_THAT(doc_store->Get(document_id2),
@@ -1615,11 +1683,15 @@ TEST_P(DocumentStoreTest, ShouldRecoverFromDiscardDerivedFiles) {
         std::move(create_result.document_store);
 
     ICING_ASSERT_OK_AND_ASSIGN(
-        document_id1,
+        DocumentStore::PutResult put_result1,
         doc_store->Put(DocumentProto(test_document1_), /*num_tokens=*/4));
+    EXPECT_FALSE(put_result1.was_replacement);
+    document_id1 = put_result1.new_document_id;
     ICING_ASSERT_OK_AND_ASSIGN(
-        document_id2,
+        DocumentStore::PutResult put_result2,
         doc_store->Put(DocumentProto(test_document2_), /*num_tokens=*/4));
+    EXPECT_FALSE(put_result2.was_replacement);
+    document_id2 = put_result2.new_document_id;
     EXPECT_THAT(doc_store->Get(document_id1),
                 IsOkAndHolds(EqualsProto(test_document1_)));
     EXPECT_THAT(doc_store->Get(document_id2),
@@ -1713,11 +1785,15 @@ TEST_P(DocumentStoreTest, ShouldRecoverFromBadChecksum) {
         std::move(create_result.document_store);
 
     ICING_ASSERT_OK_AND_ASSIGN(
-        document_id1,
+        DocumentStore::PutResult put_result1,
         doc_store->Put(DocumentProto(test_document1_), /*num_tokens=*/4));
+    EXPECT_FALSE(put_result1.was_replacement);
+    document_id1 = put_result1.new_document_id;
     ICING_ASSERT_OK_AND_ASSIGN(
-        document_id2,
+        DocumentStore::PutResult put_result2,
         doc_store->Put(DocumentProto(test_document2_), /*num_tokens=*/4));
+    EXPECT_FALSE(put_result2.was_replacement);
+    document_id2 = put_result2.new_document_id;
     EXPECT_THAT(doc_store->Get(document_id1),
                 IsOkAndHolds(EqualsProto(test_document1_)));
     EXPECT_THAT(doc_store->Get(document_id2),
@@ -1807,6 +1883,7 @@ TEST_P(DocumentStoreTest, GetStorageInfo) {
   doc_store_storage_info = doc_store->GetStorageInfo();
   EXPECT_THAT(doc_store_storage_info.document_store_size(),
               Gt(empty_doc_store_size));
+  doc_store.reset();
 
   // Bad file system
   MockFilesystem mock_filesystem;
@@ -1833,8 +1910,10 @@ TEST_P(DocumentStoreTest, MaxDocumentId) {
   // Since the DocumentStore is empty, we get an invalid DocumentId
   EXPECT_THAT(doc_store->last_added_document_id(), Eq(kInvalidDocumentId));
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store->Put(DocumentProto(test_document1_)));
+  EXPECT_FALSE(put_result1.was_replacement);
+  DocumentId document_id1 = put_result1.new_document_id;
   EXPECT_THAT(doc_store->last_added_document_id(), Eq(document_id1));
 
   // Still returns the last DocumentId even if it was deleted
@@ -1842,8 +1921,10 @@ TEST_P(DocumentStoreTest, MaxDocumentId) {
                                     fake_clock_.GetSystemTimeMilliseconds()));
   EXPECT_THAT(doc_store->last_added_document_id(), Eq(document_id1));
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              doc_store->Put(DocumentProto(test_document2_)));
+  EXPECT_FALSE(put_result2.was_replacement);
+  DocumentId document_id2 = put_result2.new_document_id;
   EXPECT_THAT(doc_store->last_added_document_id(), Eq(document_id2));
 }
 
@@ -2093,11 +2174,15 @@ TEST_P(DocumentStoreTest, GetDocumentAssociatedScoreDataSameCorpus) {
           .Build();
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       doc_store->Put(DocumentProto(document1), /*num_tokens=*/5));
+  EXPECT_FALSE(put_result1.was_replacement);
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       doc_store->Put(DocumentProto(document2), /*num_tokens=*/7));
+  EXPECT_FALSE(put_result2.was_replacement);
+  DocumentId document_id2 = put_result2.new_document_id;
 
   EXPECT_THAT(
       doc_store->GetDocumentAssociatedScoreData(document_id1),
@@ -2137,11 +2222,15 @@ TEST_P(DocumentStoreTest, GetDocumentAssociatedScoreDataDifferentCorpus) {
           .Build();
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
+      DocumentStore::PutResult put_result1,
       doc_store->Put(DocumentProto(document1), /*num_tokens=*/5));
+  EXPECT_FALSE(put_result1.was_replacement);
+  DocumentId document_id1 = put_result1.new_document_id;
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
+      DocumentStore::PutResult put_result2,
       doc_store->Put(DocumentProto(document2), /*num_tokens=*/7));
+  EXPECT_FALSE(put_result2.was_replacement);
+  DocumentId document_id2 = put_result2.new_document_id;
 
   EXPECT_THAT(
       doc_store->GetDocumentAssociatedScoreData(document_id1),
@@ -2187,8 +2276,10 @@ TEST_P(DocumentStoreTest, DeleteClearsFilterCache) {
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
-                             doc_store->Put(test_document1_));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                             doc_store->Put(DocumentProto(test_document1_)));
+  EXPECT_FALSE(put_result.was_replacement);
+  DocumentId document_id = put_result.new_document_id;
 
   ICING_ASSERT_HAS_VALUE_AND_ASSIGN(
       DocumentFilterData doc_filter_data,
@@ -2214,8 +2305,11 @@ TEST_P(DocumentStoreTest, DeleteClearsScoreCache) {
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
-                             doc_store->Put(test_document1_, /*num_tokens=*/4));
+  ICING_ASSERT_OK_AND_ASSIGN(
+      DocumentStore::PutResult put_result,
+      doc_store->Put(DocumentProto(test_document1_), /*num_tokens=*/4));
+  EXPECT_FALSE(put_result.was_replacement);
+  DocumentId document_id = put_result.new_document_id;
 
   EXPECT_THAT(doc_store->GetDocumentAssociatedScoreData(document_id),
               IsOkAndHolds(DocumentAssociatedScoreData(
@@ -2243,8 +2337,10 @@ TEST_P(DocumentStoreTest, DeleteShouldPreventUsageScores) {
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
-                             doc_store->Put(test_document1_));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                             doc_store->Put(DocumentProto(test_document1_)));
+  EXPECT_FALSE(put_result.was_replacement);
+  DocumentId document_id = put_result.new_document_id;
 
   // Report usage with type 1.
   UsageReport usage_report_type1 = CreateUsageReport(
@@ -2292,7 +2388,10 @@ TEST_P(DocumentStoreTest, ExpirationShouldPreventUsageScores) {
                                .SetTtlMs(100)
                                .Build();
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id, doc_store->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                             doc_store->Put(document));
+  EXPECT_FALSE(put_result.was_replacement);
+  DocumentId document_id = put_result.new_document_id;
 
   // Some arbitrary time before the document's creation time (10) + ttl (100)
   fake_clock_.SetSystemTimeMilliseconds(109);
@@ -2340,7 +2439,10 @@ TEST_P(DocumentStoreTest,
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id, doc_store->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                             doc_store->Put(document));
+  EXPECT_FALSE(put_result.was_replacement);
+  DocumentId document_id = put_result.new_document_id;
   ICING_ASSERT_HAS_VALUE_AND_ASSIGN(
       DocumentFilterData doc_filter_data,
       doc_store->GetAliveDocumentFilterData(
@@ -2366,7 +2468,10 @@ TEST_P(DocumentStoreTest, ExpirationTimestampIsInt64MaxIfTtlIsZero) {
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id, doc_store->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                             doc_store->Put(document));
+  EXPECT_FALSE(put_result.was_replacement);
+  DocumentId document_id = put_result.new_document_id;
 
   ICING_ASSERT_HAS_VALUE_AND_ASSIGN(
       DocumentFilterData doc_filter_data,
@@ -2397,7 +2502,10 @@ TEST_P(DocumentStoreTest, ExpirationTimestampIsInt64MaxOnOverflow) {
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id, doc_store->Put(document));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                             doc_store->Put(document));
+  EXPECT_FALSE(put_result.was_replacement);
+  DocumentId document_id = put_result.new_document_id;
 
   ICING_ASSERT_HAS_VALUE_AND_ASSIGN(
       DocumentFilterData doc_filter_data,
@@ -2432,8 +2540,10 @@ TEST_P(DocumentStoreTest, CreationTimestampShouldBePopulated) {
       std::move(create_result.document_store);
 
   ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
+      DocumentStore::PutResult put_result,
       doc_store->Put(document_without_creation_timestamp));
+  EXPECT_FALSE(put_result.was_replacement);
+  DocumentId document_id = put_result.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(DocumentProto document_with_creation_timestamp,
                              doc_store->Get(document_id));
@@ -2464,10 +2574,14 @@ TEST_P(DocumentStoreTest, ShouldWriteAndReadScoresCorrectly) {
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id1,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              doc_store->Put(document1));
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id2,
+  EXPECT_FALSE(put_result1.was_replacement);
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
                              doc_store->Put(document2));
+  EXPECT_FALSE(put_result2.was_replacement);
+  DocumentId document_id2 = put_result2.new_document_id;
 
   EXPECT_THAT(doc_store->GetDocumentAssociatedScoreData(document_id1),
               IsOkAndHolds(DocumentAssociatedScoreData(
@@ -2482,7 +2596,7 @@ TEST_P(DocumentStoreTest, ShouldWriteAndReadScoresCorrectly) {
                   /*length_in_tokens=*/0)));
 }
 
-TEST_P(DocumentStoreTest, ComputeChecksumSameBetweenCalls) {
+TEST_P(DocumentStoreTest, GetChecksumDoesntUpdateStoredChecksum) {
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::CreateResult create_result,
       CreateDocumentStore(&filesystem_, document_store_dir_, &fake_clock_,
@@ -2491,13 +2605,17 @@ TEST_P(DocumentStoreTest, ComputeChecksumSameBetweenCalls) {
       std::move(create_result.document_store);
 
   ICING_EXPECT_OK(document_store->Put(test_document1_));
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum, document_store->ComputeChecksum());
+  // GetChecksum should succeed without updating the checksum.
+  ICING_EXPECT_OK(document_store->GetChecksum());
 
-  // Calling ComputeChecksum again shouldn't change anything
-  EXPECT_THAT(document_store->ComputeChecksum(), IsOkAndHolds(checksum));
+  // Create another instance of DocumentStore
+  ICING_ASSERT_OK_AND_ASSIGN(
+      create_result, CreateDocumentStore(&filesystem_, document_store_dir_,
+                                         &fake_clock_, schema_store_.get()));
+  EXPECT_TRUE(create_result.derived_files_regenerated);
 }
 
-TEST_P(DocumentStoreTest, ComputeChecksumSameAcrossInstances) {
+TEST_P(DocumentStoreTest, UpdateChecksumNextInitializationSucceeds) {
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::CreateResult create_result,
       CreateDocumentStore(&filesystem_, document_store_dir_, &fake_clock_,
@@ -2506,19 +2624,23 @@ TEST_P(DocumentStoreTest, ComputeChecksumSameAcrossInstances) {
       std::move(create_result.document_store);
 
   ICING_EXPECT_OK(document_store->Put(test_document1_));
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum, document_store->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum, document_store->GetChecksum());
+  EXPECT_THAT(document_store->UpdateChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(document_store->GetChecksum(), IsOkAndHolds(checksum));
 
-  // Destroy the previous instance and recreate DocumentStore
-  document_store.reset();
+  // Create another instance of DocumentStore
   ICING_ASSERT_OK_AND_ASSIGN(
       create_result, CreateDocumentStore(&filesystem_, document_store_dir_,
                                          &fake_clock_, schema_store_.get()));
-  document_store = std::move(create_result.document_store);
+  EXPECT_FALSE(create_result.derived_files_regenerated);
 
-  EXPECT_THAT(document_store->ComputeChecksum(), IsOkAndHolds(checksum));
+  std::unique_ptr<DocumentStore> document_store_two = std::move(create_result.document_store);
+  EXPECT_THAT(document_store_two->GetChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(document_store_two->UpdateChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(document_store_two->GetChecksum(), IsOkAndHolds(checksum));
 }
 
-TEST_P(DocumentStoreTest, ComputeChecksumChangesOnNewDocument) {
+TEST_P(DocumentStoreTest, UpdateChecksumSameBetweenCalls) {
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::CreateResult create_result,
       CreateDocumentStore(&filesystem_, document_store_dir_, &fake_clock_,
@@ -2527,14 +2649,35 @@ TEST_P(DocumentStoreTest, ComputeChecksumChangesOnNewDocument) {
       std::move(create_result.document_store);
 
   ICING_EXPECT_OK(document_store->Put(test_document1_));
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum, document_store->ComputeChecksum());
+  // GetChecksum should return the same value as UpdateChecksum
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum, document_store->GetChecksum());
+  EXPECT_THAT(document_store->UpdateChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(document_store->GetChecksum(), IsOkAndHolds(checksum));
+
+  // Calling UpdateChecksum again shouldn't change anything
+  EXPECT_THAT(document_store->UpdateChecksum(), IsOkAndHolds(checksum));
+}
+
+TEST_P(DocumentStoreTest, UpdateChecksumChangesOnNewDocument) {
+  ICING_ASSERT_OK_AND_ASSIGN(
+      DocumentStore::CreateResult create_result,
+      CreateDocumentStore(&filesystem_, document_store_dir_, &fake_clock_,
+                          schema_store_.get()));
+  std::unique_ptr<DocumentStore> document_store =
+      std::move(create_result.document_store);
+
+  ICING_EXPECT_OK(document_store->Put(test_document1_));
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum, document_store->GetChecksum());
+  EXPECT_THAT(document_store->UpdateChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(document_store->GetChecksum(), IsOkAndHolds(checksum));
 
   ICING_EXPECT_OK(document_store->Put(test_document2_));
-  EXPECT_THAT(document_store->ComputeChecksum(),
+  EXPECT_THAT(document_store->GetChecksum(), IsOkAndHolds(Not(Eq(checksum))));
+  EXPECT_THAT(document_store->UpdateChecksum(),
               IsOkAndHolds(Not(Eq(checksum))));
 }
 
-TEST_P(DocumentStoreTest, ComputeChecksumDoesntChangeOnNewUsage) {
+TEST_P(DocumentStoreTest, UpdateChecksumDoesntChangeOnNewUsage) {
   ICING_ASSERT_OK_AND_ASSIGN(
       DocumentStore::CreateResult create_result,
       CreateDocumentStore(&filesystem_, document_store_dir_, &fake_clock_,
@@ -2543,13 +2686,17 @@ TEST_P(DocumentStoreTest, ComputeChecksumDoesntChangeOnNewUsage) {
       std::move(create_result.document_store);
 
   ICING_EXPECT_OK(document_store->Put(test_document1_));
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum, document_store->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum, document_store->GetChecksum());
+  EXPECT_THAT(document_store->UpdateChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(document_store->GetChecksum(), IsOkAndHolds(checksum));
 
   UsageReport usage_report =
       CreateUsageReport(test_document1_.namespace_(), test_document1_.uri(),
                         /*timestamp_ms=*/1000, UsageReport::USAGE_TYPE1);
   ICING_EXPECT_OK(document_store->ReportUsage(usage_report));
-  EXPECT_THAT(document_store->ComputeChecksum(), IsOkAndHolds(Eq(checksum)));
+  EXPECT_THAT(document_store->GetChecksum(), IsOkAndHolds(checksum));
+  EXPECT_THAT(document_store->UpdateChecksum(), IsOkAndHolds(Eq(checksum)));
+  EXPECT_THAT(document_store->GetChecksum(), IsOkAndHolds(checksum));
 }
 
 TEST_P(DocumentStoreTest, RegenerateDerivedFilesSkipsUnknownSchemaTypeIds) {
@@ -2603,7 +2750,10 @@ TEST_P(DocumentStoreTest, RegenerateDerivedFilesSkipsUnknownSchemaTypeIds) {
 
     // Insert and verify a "email "document
     ICING_ASSERT_OK_AND_ASSIGN(
-        email_document_id, document_store->Put(DocumentProto(email_document)));
+        DocumentStore::PutResult email_put_result,
+        document_store->Put(DocumentProto(email_document)));
+    EXPECT_FALSE(email_put_result.was_replacement);
+    email_document_id = email_put_result.new_document_id;
     EXPECT_THAT(document_store->Get(email_document_id),
                 IsOkAndHolds(EqualsProto(email_document)));
     ICING_ASSERT_HAS_VALUE_AND_ASSIGN(
@@ -2616,8 +2766,10 @@ TEST_P(DocumentStoreTest, RegenerateDerivedFilesSkipsUnknownSchemaTypeIds) {
 
     // Insert and verify a "message" document
     ICING_ASSERT_OK_AND_ASSIGN(
-        message_document_id,
+        DocumentStore::PutResult message_put_result,
         document_store->Put(DocumentProto(message_document)));
+    EXPECT_FALSE(message_put_result.was_replacement);
+    message_document_id = message_put_result.new_document_id;
     EXPECT_THAT(document_store->Get(message_document_id),
                 IsOkAndHolds(EqualsProto(message_document)));
     ICING_ASSERT_HAS_VALUE_AND_ASSIGN(
@@ -2731,16 +2883,22 @@ TEST_P(DocumentStoreTest, UpdateSchemaStoreUpdatesSchemaTypeIds) {
   std::unique_ptr<DocumentStore> document_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id,
-                             document_store->Put(email_document));
+  ICING_ASSERT_OK_AND_ASSIGN(
+      DocumentStore::PutResult email_put_result,
+      document_store->Put(DocumentProto(email_document)));
+  EXPECT_FALSE(email_put_result.was_replacement);
+  DocumentId email_document_id = email_put_result.new_document_id;
   ICING_ASSERT_HAS_VALUE_AND_ASSIGN(
       DocumentFilterData email_data,
       document_store->GetAliveDocumentFilterData(
           email_document_id, fake_clock_.GetSystemTimeMilliseconds()));
   EXPECT_THAT(email_data.schema_type_id(), Eq(old_email_schema_type_id));
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId message_document_id,
-                             document_store->Put(message_document));
+  ICING_ASSERT_OK_AND_ASSIGN(
+      DocumentStore::PutResult message_put_result,
+      document_store->Put(DocumentProto(message_document)));
+  EXPECT_FALSE(message_put_result.was_replacement);
+  DocumentId message_document_id = message_put_result.new_document_id;
   ICING_ASSERT_HAS_VALUE_AND_ASSIGN(
       DocumentFilterData message_data,
       document_store->GetAliveDocumentFilterData(
@@ -2829,13 +2987,21 @@ TEST_P(DocumentStoreTest, UpdateSchemaStoreDeletesInvalidDocuments) {
   std::unique_ptr<DocumentStore> document_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_without_subject_document_id,
-                             document_store->Put(email_without_subject));
+  ICING_ASSERT_OK_AND_ASSIGN(
+      DocumentStore::PutResult email_without_subject_put_result,
+      document_store->Put(DocumentProto(email_without_subject)));
+  EXPECT_FALSE(email_without_subject_put_result.was_replacement);
+  DocumentId email_without_subject_document_id =
+      email_without_subject_put_result.new_document_id;
   EXPECT_THAT(document_store->Get(email_without_subject_document_id),
               IsOkAndHolds(EqualsProto(email_without_subject)));
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_with_subject_document_id,
-                             document_store->Put(email_with_subject));
+  ICING_ASSERT_OK_AND_ASSIGN(
+      DocumentStore::PutResult email_with_subject_put_result,
+      document_store->Put(DocumentProto(email_with_subject)));
+  EXPECT_FALSE(email_with_subject_put_result.was_replacement);
+  DocumentId email_with_subject_document_id =
+      email_with_subject_put_result.new_document_id;
   EXPECT_THAT(document_store->Get(email_with_subject_document_id),
               IsOkAndHolds(EqualsProto(email_with_subject)));
 
@@ -2902,13 +3068,17 @@ TEST_P(DocumentStoreTest,
   std::unique_ptr<DocumentStore> document_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult email_put_result,
                              document_store->Put(email_document));
+  EXPECT_FALSE(email_put_result.was_replacement);
+  DocumentId email_document_id = email_put_result.new_document_id;
   EXPECT_THAT(document_store->Get(email_document_id),
               IsOkAndHolds(EqualsProto(email_document)));
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId message_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult message_put_result,
                              document_store->Put(message_document));
+  EXPECT_FALSE(message_put_result.was_replacement);
+  DocumentId message_document_id = message_put_result.new_document_id;
   EXPECT_THAT(document_store->Get(message_document_id),
               IsOkAndHolds(EqualsProto(message_document)));
 
@@ -2977,16 +3147,20 @@ TEST_P(DocumentStoreTest, OptimizedUpdateSchemaStoreUpdatesSchemaTypeIds) {
   std::unique_ptr<DocumentStore> document_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult email_put_result,
                              document_store->Put(email_document));
+  EXPECT_FALSE(email_put_result.was_replacement);
+  DocumentId email_document_id = email_put_result.new_document_id;
   ICING_ASSERT_HAS_VALUE_AND_ASSIGN(
       DocumentFilterData email_data,
       document_store->GetAliveDocumentFilterData(
           email_document_id, fake_clock_.GetSystemTimeMilliseconds()));
   EXPECT_THAT(email_data.schema_type_id(), Eq(old_email_schema_type_id));
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId message_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult message_put_result,
                              document_store->Put(message_document));
+  EXPECT_FALSE(message_put_result.was_replacement);
+  DocumentId message_document_id = message_put_result.new_document_id;
   ICING_ASSERT_HAS_VALUE_AND_ASSIGN(
       DocumentFilterData message_data,
       document_store->GetAliveDocumentFilterData(
@@ -3078,13 +3252,21 @@ TEST_P(DocumentStoreTest, OptimizedUpdateSchemaStoreDeletesInvalidDocuments) {
   std::unique_ptr<DocumentStore> document_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_without_subject_document_id,
-                             document_store->Put(email_without_subject));
+  ICING_ASSERT_OK_AND_ASSIGN(
+      DocumentStore::PutResult email_without_subject_put_result,
+      document_store->Put(email_without_subject));
+  EXPECT_FALSE(email_without_subject_put_result.was_replacement);
+  DocumentId email_without_subject_document_id =
+      email_without_subject_put_result.new_document_id;
   EXPECT_THAT(document_store->Get(email_without_subject_document_id),
               IsOkAndHolds(EqualsProto(email_without_subject)));
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_with_subject_document_id,
-                             document_store->Put(email_with_subject));
+  ICING_ASSERT_OK_AND_ASSIGN(
+      DocumentStore::PutResult email_with_subject_put_result,
+      document_store->Put(email_with_subject));
+  EXPECT_FALSE(email_with_subject_put_result.was_replacement);
+  DocumentId email_with_subject_document_id =
+      email_with_subject_put_result.new_document_id;
   EXPECT_THAT(document_store->Get(email_with_subject_document_id),
               IsOkAndHolds(EqualsProto(email_with_subject)));
 
@@ -3154,13 +3336,17 @@ TEST_P(DocumentStoreTest,
   std::unique_ptr<DocumentStore> document_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId email_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult email_put_result,
                              document_store->Put(email_document));
+  EXPECT_FALSE(email_put_result.was_replacement);
+  DocumentId email_document_id = email_put_result.new_document_id;
   EXPECT_THAT(document_store->Get(email_document_id),
               IsOkAndHolds(EqualsProto(email_document)));
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId message_document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult message_put_result,
                              document_store->Put(message_document));
+  EXPECT_FALSE(message_put_result.was_replacement);
+  DocumentId message_document_id = message_put_result.new_document_id;
   EXPECT_THAT(document_store->Get(message_document_id),
               IsOkAndHolds(EqualsProto(message_document)));
 
@@ -3224,8 +3410,9 @@ TEST_P(DocumentStoreTest, GetOptimizeInfo) {
   std::string optimized_dir = document_store_dir_ + "_optimize";
   EXPECT_TRUE(filesystem_.DeleteDirectoryRecursively(optimized_dir.c_str()));
   EXPECT_TRUE(filesystem_.CreateDirectoryRecursively(optimized_dir.c_str()));
-  ICING_ASSERT_OK(
-      document_store->OptimizeInto(optimized_dir, lang_segmenter_.get()));
+  ICING_ASSERT_OK(document_store->OptimizeInto(
+      optimized_dir, lang_segmenter_.get(),
+      /*expired_blob_handles=*/std::unordered_set<std::string>()));
   document_store.reset();
   ICING_ASSERT_OK_AND_ASSIGN(
       create_result, CreateDocumentStore(&filesystem_, optimized_dir,
@@ -3320,8 +3507,10 @@ TEST_P(DocumentStoreTest, ReportUsageWithDifferentTimestampsAndGetUsageScores) {
   std::unique_ptr<DocumentStore> document_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store->Put(test_document1_));
+  EXPECT_FALSE(put_result1.was_replacement);
+  DocumentId document_id = put_result1.new_document_id;
 
   // Report usage with type 1 and time 1.
   UsageReport usage_report_type1_time1 = CreateUsageReport(
@@ -3412,8 +3601,10 @@ TEST_P(DocumentStoreTest, ReportUsageWithDifferentTypesAndGetUsageScores) {
   std::unique_ptr<DocumentStore> document_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                              document_store->Put(test_document1_));
+  EXPECT_FALSE(put_result1.was_replacement);
+  DocumentId document_id = put_result1.new_document_id;
 
   // Report usage with type 1.
   UsageReport usage_report_type1 = CreateUsageReport(
@@ -3465,8 +3656,10 @@ TEST_P(DocumentStoreTest, UsageScoresShouldNotBeClearedOnChecksumMismatch) {
     std::unique_ptr<DocumentStore> document_store =
         std::move(create_result.document_store);
 
-    ICING_ASSERT_OK_AND_ASSIGN(document_id,
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
                                document_store->Put(test_document1_));
+    EXPECT_FALSE(put_result1.was_replacement);
+    document_id = put_result1.new_document_id;
 
     // Report usage with type 1.
     UsageReport usage_report_type1 = CreateUsageReport(
@@ -3510,8 +3703,10 @@ TEST_P(DocumentStoreTest, UsageScoresShouldBeAvailableAfterDataLoss) {
     std::unique_ptr<DocumentStore> document_store =
         std::move(create_result.document_store);
 
-    ICING_ASSERT_OK_AND_ASSIGN(
-        document_id, document_store->Put(DocumentProto(test_document1_)));
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
+                               document_store->Put(test_document1_));
+    EXPECT_FALSE(put_result1.was_replacement);
+    document_id = put_result1.new_document_id;
 
     // Report usage with type 1.
     UsageReport usage_report_type1 = CreateUsageReport(
@@ -3563,9 +3758,10 @@ TEST_P(DocumentStoreTest, UsageScoresShouldBeCopiedOverToUpdatedDocument) {
   std::unique_ptr<DocumentStore> document_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id,
-      document_store->Put(DocumentProto(test_document1_)));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
+                             document_store->Put(test_document1_));
+  EXPECT_FALSE(put_result1.was_replacement);
+  DocumentId document_id = put_result1.new_document_id;
 
   // Report usage with type 1.
   UsageReport usage_report_type1 = CreateUsageReport(
@@ -3582,9 +3778,10 @@ TEST_P(DocumentStoreTest, UsageScoresShouldBeCopiedOverToUpdatedDocument) {
   EXPECT_THAT(actual_scores, Eq(expected_scores));
 
   // Update the document.
-  ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId updated_document_id,
-      document_store->Put(DocumentProto(test_document1_)));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
+                             document_store->Put(test_document1_));
+  EXPECT_TRUE(put_result2.was_replacement);
+  DocumentId updated_document_id = put_result2.new_document_id;
   // We should get a different document id.
   ASSERT_THAT(updated_document_id, Not(Eq(document_id)));
 
@@ -3604,12 +3801,14 @@ TEST_P(DocumentStoreTest, UsageScoresShouldPersistOnOptimize) {
   std::unique_ptr<DocumentStore> document_store =
       std::move(create_result.document_store);
 
-  ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id1,
-      document_store->Put(DocumentProto(test_document1_)));
-  ICING_ASSERT_OK_AND_ASSIGN(
-      DocumentId document_id2,
-      document_store->Put(DocumentProto(test_document2_)));
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
+                             document_store->Put(test_document1_));
+  EXPECT_FALSE(put_result1.was_replacement);
+  DocumentId document_id1 = put_result1.new_document_id;
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result2,
+                             document_store->Put(test_document2_));
+  EXPECT_FALSE(put_result2.was_replacement);
+  DocumentId document_id2 = put_result2.new_document_id;
   ICING_ASSERT_OK(document_store->Delete(
       document_id1, fake_clock_.GetSystemTimeMilliseconds()));
 
@@ -3630,8 +3829,9 @@ TEST_P(DocumentStoreTest, UsageScoresShouldPersistOnOptimize) {
   // Run optimize
   std::string optimized_dir = document_store_dir_ + "/optimize_test";
   filesystem_.CreateDirectoryRecursively(optimized_dir.c_str());
-  ICING_ASSERT_OK(
-      document_store->OptimizeInto(optimized_dir, lang_segmenter_.get()));
+  ICING_ASSERT_OK(document_store->OptimizeInto(
+      optimized_dir, lang_segmenter_.get(),
+      /*expired_blob_handles=*/std::unordered_set<std::string>()));
 
   // Get optimized document store
   ICING_ASSERT_OK_AND_ASSIGN(
@@ -3661,8 +3861,10 @@ TEST_P(DocumentStoreTest, DetectPartialDataLoss) {
     EXPECT_THAT(create_result.data_loss, Eq(DataLoss::NONE));
     EXPECT_THAT(create_result.derived_files_regenerated, IsFalse());
 
-    ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
-                               doc_store->Put(DocumentProto(test_document1_)));
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                               doc_store->Put(test_document1_));
+    EXPECT_FALSE(put_result.was_replacement);
+    DocumentId document_id = put_result.new_document_id;
     EXPECT_THAT(doc_store->Get(document_id),
                 IsOkAndHolds(EqualsProto(test_document1_)));
   }
@@ -3714,8 +3916,10 @@ TEST_P(DocumentStoreTest, DetectCompleteDataLoss) {
     // initialization.
     corruptible_offset = filesystem_.GetFileSize(document_log_file.c_str());
 
-    ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
-                               doc_store->Put(DocumentProto(test_document1_)));
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                               doc_store->Put(test_document1_));
+    EXPECT_FALSE(put_result.was_replacement);
+    DocumentId document_id = put_result.new_document_id;
     EXPECT_THAT(doc_store->Get(document_id),
                 IsOkAndHolds(EqualsProto(test_document1_)));
   }
@@ -3978,7 +4182,10 @@ TEST_P(DocumentStoreTest, InitializeForceRecoveryUpdatesTypeIds) {
                 document1_creation_timestamp_)  // A random timestamp
             .SetTtlMs(document1_ttl_)
             .Build();
-    ICING_ASSERT_OK_AND_ASSIGN(docid, doc_store->Put(doc));
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                               doc_store->Put(doc));
+    EXPECT_FALSE(put_result.was_replacement);
+    docid = put_result.new_document_id;
     ICING_ASSERT_HAS_VALUE_AND_ASSIGN(
         DocumentFilterData filter_data,
         doc_store->GetAliveDocumentFilterData(
@@ -4090,7 +4297,10 @@ TEST_P(DocumentStoreTest, InitializeDontForceRecoveryDoesntUpdateTypeIds) {
                 document1_creation_timestamp_)  // A random timestamp
             .SetTtlMs(document1_ttl_)
             .Build();
-    ICING_ASSERT_OK_AND_ASSIGN(docid, doc_store->Put(doc));
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
+                               doc_store->Put(doc));
+    EXPECT_FALSE(put_result.was_replacement);
+    docid = put_result.new_document_id;
     ICING_ASSERT_HAS_VALUE_AND_ASSIGN(
         DocumentFilterData filter_data,
         doc_store->GetAliveDocumentFilterData(
@@ -4203,10 +4413,16 @@ TEST_P(DocumentStoreTest, InitializeForceRecoveryDeletesInvalidDocument) {
         std::move(create_result.document_store);
 
     DocumentId docid = kInvalidDocumentId;
-    ICING_ASSERT_OK_AND_ASSIGN(docid, doc_store->Put(docWithBody));
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_with_body_result,
+                               doc_store->Put(docWithBody));
+    EXPECT_FALSE(put_with_body_result.was_replacement);
+    docid = put_with_body_result.new_document_id;
     ASSERT_NE(docid, kInvalidDocumentId);
     docid = kInvalidDocumentId;
-    ICING_ASSERT_OK_AND_ASSIGN(docid, doc_store->Put(docWithoutBody));
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_without_body_result,
+                               doc_store->Put(docWithoutBody));
+    EXPECT_FALSE(put_without_body_result.was_replacement);
+    docid = put_without_body_result.new_document_id;
     ASSERT_NE(docid, kInvalidDocumentId);
 
     ASSERT_THAT(doc_store->Get(docWithBody.namespace_(), docWithBody.uri()),
@@ -4317,10 +4533,16 @@ TEST_P(DocumentStoreTest, InitializeDontForceRecoveryKeepsInvalidDocument) {
         std::move(create_result.document_store);
 
     DocumentId docid = kInvalidDocumentId;
-    ICING_ASSERT_OK_AND_ASSIGN(docid, doc_store->Put(docWithBody));
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_with_body_result,
+                               doc_store->Put(docWithBody));
+    EXPECT_FALSE(put_with_body_result.was_replacement);
+    docid = put_with_body_result.new_document_id;
     ASSERT_NE(docid, kInvalidDocumentId);
     docid = kInvalidDocumentId;
-    ICING_ASSERT_OK_AND_ASSIGN(docid, doc_store->Put(docWithoutBody));
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_without_body_result,
+                               doc_store->Put(docWithoutBody));
+    EXPECT_FALSE(put_without_body_result.was_replacement);
+    docid = put_without_body_result.new_document_id;
     ASSERT_NE(docid, kInvalidDocumentId);
 
     ASSERT_THAT(doc_store->Get(docWithBody.namespace_(), docWithBody.uri()),
@@ -4681,7 +4903,10 @@ TEST_P(DocumentStoreTest, SwitchKeyMapperTypeShouldRegenerateDerivedFiles) {
 
     std::unique_ptr<DocumentStore> doc_store =
         std::move(create_result.document_store);
-    ICING_ASSERT_OK_AND_ASSIGN(document_id1, doc_store->Put(test_document1_));
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
+                               doc_store->Put(test_document1_));
+    EXPECT_FALSE(put_result1.was_replacement);
+    document_id1 = put_result1.new_document_id;
 
     if (GetParam().use_persistent_hash_map) {
       EXPECT_THAT(filesystem_.DirectoryExists(
@@ -4764,7 +4989,10 @@ TEST_P(DocumentStoreTest, SameKeyMapperTypeShouldNotRegenerateDerivedFiles) {
 
     std::unique_ptr<DocumentStore> doc_store =
         std::move(create_result.document_store);
-    ICING_ASSERT_OK_AND_ASSIGN(document_id1, doc_store->Put(test_document1_));
+    ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result1,
+                               doc_store->Put(test_document1_));
+    EXPECT_FALSE(put_result1.was_replacement);
+    document_id1 = put_result1.new_document_id;
 
     if (GetParam().use_persistent_hash_map) {
       EXPECT_THAT(filesystem_.DirectoryExists(
@@ -4841,8 +5069,10 @@ TEST_P(DocumentStoreTest, GetDocumentIdByNamespaceFingerprintIdentifier) {
 
   std::unique_ptr<DocumentStore> doc_store =
       std::move(create_result.document_store);
-  ICING_ASSERT_OK_AND_ASSIGN(DocumentId document_id,
+  ICING_ASSERT_OK_AND_ASSIGN(DocumentStore::PutResult put_result,
                              doc_store->Put(test_document1_));
+  EXPECT_FALSE(put_result.was_replacement);
+  DocumentId document_id = put_result.new_document_id;
 
   ICING_ASSERT_OK_AND_ASSIGN(
       NamespaceId namespace_id,
diff --git a/icing/store/dynamic-trie-key-mapper.h b/icing/store/dynamic-trie-key-mapper.h
index 63e8488..f7fd665 100644
--- a/icing/store/dynamic-trie-key-mapper.h
+++ b/icing/store/dynamic-trie-key-mapper.h
@@ -94,7 +94,9 @@ class DynamicTrieKeyMapper : public KeyMapper<T, Formatter> {
 
   libtextclassifier3::StatusOr<int64_t> GetElementsSize() const override;
 
-  libtextclassifier3::StatusOr<Crc32> ComputeChecksum() override;
+  libtextclassifier3::StatusOr<Crc32> UpdateChecksum() override;
+
+  libtextclassifier3::StatusOr<Crc32> GetChecksum() const override;
 
  private:
   class Iterator : public KeyMapper<T, Formatter>::Iterator {
@@ -112,10 +114,7 @@ class DynamicTrieKeyMapper : public KeyMapper<T, Formatter> {
       return itr_.Advance();
     }
 
-    std::string_view GetKey() const override {
-      const char* key = itr_.GetKey();
-      return std::string_view(key);
-    }
+    std::string_view GetKey() const override { return itr_.GetKey(); }
 
     T GetValue() const override {
       T value;
@@ -229,13 +228,12 @@ libtextclassifier3::Status DynamicTrieKeyMapper<T, Formatter>::Initialize(
 template <typename T, typename Formatter>
 libtextclassifier3::StatusOr<T> DynamicTrieKeyMapper<T, Formatter>::GetOrPut(
     std::string_view key, T next_value) {
-  std::string string_key(key);
   uint32_t value_index;
   libtextclassifier3::Status status =
-      trie_.Insert(string_key.c_str(), &next_value, &value_index,
+      trie_.Insert(key, &next_value, &value_index,
                    /*replace=*/false);
   if (!status.ok()) {
-    ICING_LOG(DBG) << "Unable to insert key " << string_key
+    ICING_LOG(DBG) << "Unable to insert key " << key
                    << " into DynamicTrieKeyMapper " << file_prefix_ << ".\n"
                    << status.error_message();
     return status;
@@ -256,10 +254,9 @@ libtextclassifier3::StatusOr<T> DynamicTrieKeyMapper<T, Formatter>::GetOrPut(
 template <typename T, typename Formatter>
 libtextclassifier3::Status DynamicTrieKeyMapper<T, Formatter>::Put(
     std::string_view key, T value) {
-  std::string string_key(key);
-  libtextclassifier3::Status status = trie_.Insert(string_key.c_str(), &value);
+  libtextclassifier3::Status status = trie_.Insert(key, &value);
   if (!status.ok()) {
-    ICING_LOG(DBG) << "Unable to insert key " << string_key
+    ICING_LOG(DBG) << "Unable to insert key " << key
                    << " into DynamicTrieKeyMapper " << file_prefix_ << ".\n"
                    << status.error_message();
     return status;
@@ -270,11 +267,10 @@ libtextclassifier3::Status DynamicTrieKeyMapper<T, Formatter>::Put(
 template <typename T, typename Formatter>
 libtextclassifier3::StatusOr<T> DynamicTrieKeyMapper<T, Formatter>::Get(
     std::string_view key) const {
-  std::string string_key(key);
   T value;
-  if (!trie_.Find(string_key.c_str(), &value)) {
+  if (!trie_.Find(key, &value)) {
     return absl_ports::NotFoundError(
-        absl_ports::StrCat("Key not found ", Formatter()(string_key),
+        absl_ports::StrCat("Key not found ", Formatter()(key),
                            " in DynamicTrieKeyMapper ", file_prefix_, "."));
   }
   return value;
@@ -324,8 +320,14 @@ DynamicTrieKeyMapper<T, Formatter>::GetElementsSize() const {
 
 template <typename T, typename Formatter>
 libtextclassifier3::StatusOr<Crc32>
-DynamicTrieKeyMapper<T, Formatter>::ComputeChecksum() {
-  return Crc32(trie_.UpdateCrc());
+DynamicTrieKeyMapper<T, Formatter>::UpdateChecksum() {
+  return trie_.UpdateCrc();
+}
+
+template <typename T, typename Formatter>
+libtextclassifier3::StatusOr<Crc32>
+DynamicTrieKeyMapper<T, Formatter>::GetChecksum() const {
+  return trie_.GetCrc();
 }
 
 }  // namespace lib
diff --git a/icing/store/key-mapper.h b/icing/store/key-mapper.h
index 2767da8..83f725c 100644
--- a/icing/store/key-mapper.h
+++ b/icing/store/key-mapper.h
@@ -124,8 +124,11 @@ class KeyMapper {
   //   INTERNAL_ERROR on IO error
   virtual libtextclassifier3::StatusOr<int64_t> GetElementsSize() const = 0;
 
-  // Computes and returns the checksum of the header and contents.
-  virtual libtextclassifier3::StatusOr<Crc32> ComputeChecksum() = 0;
+  // Computes the checksum of the key mapper and updates the header.
+  virtual libtextclassifier3::StatusOr<Crc32> UpdateChecksum() = 0;
+
+  // Returns the checksum of the key mapper. Does NOT update the header.
+  virtual libtextclassifier3::StatusOr<Crc32> GetChecksum() const = 0;
 
  private:
   static_assert(std::is_trivially_copyable<T>::value,
diff --git a/icing/store/persistent-hash-map-key-mapper.h b/icing/store/persistent-hash-map-key-mapper.h
index 0596fe3..e8b5832 100644
--- a/icing/store/persistent-hash-map-key-mapper.h
+++ b/icing/store/persistent-hash-map-key-mapper.h
@@ -132,10 +132,14 @@ class PersistentHashMapKeyMapper : public KeyMapper<T, Formatter> {
     return persistent_hash_map_->GetElementsSize();
   }
 
-  libtextclassifier3::StatusOr<Crc32> ComputeChecksum() override {
+  libtextclassifier3::StatusOr<Crc32> UpdateChecksum() override {
     return persistent_hash_map_->UpdateChecksums();
   }
 
+  libtextclassifier3::StatusOr<Crc32> GetChecksum() const override {
+    return persistent_hash_map_->GetChecksum();
+  }
+
  private:
   class Iterator : public KeyMapper<T, Formatter>::Iterator {
    public:
diff --git a/icing/store/usage-store.cc b/icing/store/usage-store.cc
index 546067d..1d3856d 100644
--- a/icing/store/usage-store.cc
+++ b/icing/store/usage-store.cc
@@ -17,6 +17,7 @@
 #include "icing/file/file-backed-vector.h"
 #include "icing/proto/usage.pb.h"
 #include "icing/store/document-id.h"
+#include "icing/util/crc32.h"
 
 namespace icing {
 namespace lib {
@@ -213,8 +214,12 @@ libtextclassifier3::Status UsageStore::PersistToDisk() {
   return usage_score_cache_->PersistToDisk();
 }
 
-libtextclassifier3::StatusOr<Crc32> UsageStore::ComputeChecksum() {
-  return usage_score_cache_->ComputeChecksum();
+libtextclassifier3::StatusOr<Crc32> UsageStore::UpdateChecksum() {
+  return usage_score_cache_->UpdateChecksum();
+}
+
+Crc32 UsageStore::GetChecksum() const {
+  return usage_score_cache_->GetChecksum();
 }
 
 libtextclassifier3::StatusOr<int64_t> UsageStore::GetElementsFileSize() const {
diff --git a/icing/store/usage-store.h b/icing/store/usage-store.h
index 3c7a55e..2bc0293 100644
--- a/icing/store/usage-store.h
+++ b/icing/store/usage-store.h
@@ -17,6 +17,7 @@
 #include "icing/file/file-backed-vector.h"
 #include "icing/proto/usage.pb.h"
 #include "icing/store/document-id.h"
+#include "icing/util/crc32.h"
 
 #ifndef ICING_STORE_USAGE_STORE_H_
 #define ICING_STORE_USAGE_STORE_H_
@@ -141,12 +142,16 @@ class UsageStore {
   //   INTERNAL on I/O error
   libtextclassifier3::Status PersistToDisk();
 
-  // Updates checksum of the usage scores and returns it.
+  // Updates checksum of the usage scores and saves it in the header.
   //
   // Returns:
   //   A Crc32 on success
   //   INTERNAL_ERROR if the internal state is inconsistent
-  libtextclassifier3::StatusOr<Crc32> ComputeChecksum();
+  libtextclassifier3::StatusOr<Crc32> UpdateChecksum();
+
+  // Calculates the checksum of the usage scores and returns it. Does NOT update
+  // the checksum in the header.
+  Crc32 GetChecksum() const;
 
   // Returns the file size of the all the elements held in the UsageStore. File
   // size is in bytes. This excludes the size of any internal metadata, e.g. any
diff --git a/icing/store/usage-store_test.cc b/icing/store/usage-store_test.cc
index 07fe2c5..929720f 100644
--- a/icing/store/usage-store_test.cc
+++ b/icing/store/usage-store_test.cc
@@ -18,6 +18,7 @@
 #include "gtest/gtest.h"
 #include "icing/testing/common-matchers.h"
 #include "icing/testing/tmp-directory.h"
+#include "icing/util/crc32.h"
 
 namespace icing {
 namespace lib {
@@ -343,27 +344,27 @@ TEST_F(UsageStoreTest, PersistToDisk) {
   EXPECT_THAT(usage_store->PersistToDisk(), IsOk());
 }
 
-TEST_F(UsageStoreTest, ComputeChecksum) {
+TEST_F(UsageStoreTest, UpdateChecksum) {
   ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<UsageStore> usage_store,
                              UsageStore::Create(&filesystem_, test_dir_));
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum1, usage_store->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum1, usage_store->UpdateChecksum());
 
   // Create usage scores with some random numbers.
   UsageStore::UsageScores scores = CreateUsageScores(
       /*type1_timestamp=*/7, /*type2_timestamp=*/9, /*type3_timestamp=*/1,
       /*type1_count=*/3, /*type2_count=*/4, /*type3_count=*/9);
   ICING_ASSERT_OK(usage_store->SetUsageScores(/*document_id=*/1, scores));
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum2, usage_store->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum2, usage_store->UpdateChecksum());
 
   ICING_ASSERT_OK(usage_store->SetUsageScores(/*document_id=*/2, scores));
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum3, usage_store->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum3, usage_store->UpdateChecksum());
 
   EXPECT_THAT(checksum1, Not(Eq(checksum2)));
   EXPECT_THAT(checksum1, Not(Eq(checksum3)));
   EXPECT_THAT(checksum2, Not(Eq(checksum3)));
 
   // Without changing the store, checksum should be the same.
-  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum4, usage_store->ComputeChecksum());
+  ICING_ASSERT_OK_AND_ASSIGN(Crc32 checksum4, usage_store->UpdateChecksum());
   EXPECT_THAT(checksum3, Eq(checksum4));
 }
 
diff --git a/icing/testing/common-matchers.h b/icing/testing/common-matchers.h
index 1c631b1..fd117b8 100644
--- a/icing/testing/common-matchers.h
+++ b/icing/testing/common-matchers.h
@@ -18,6 +18,7 @@
 #include <algorithm>
 #include <cinttypes>
 #include <cmath>
+#include <functional>
 #include <string>
 #include <vector>
 
@@ -586,6 +587,18 @@ MATCHER_P(EqualsSearchResultIgnoreStatsAndScores, expected, "") {
                             actual_copy, result_listener);
 }
 
+MATCHER_P(EqualsHit, expected_hit, "") {
+  const Hit& actual = arg;
+  return actual.value() == expected_hit.value() &&
+         actual.flags() == expected_hit.flags() &&
+         actual.term_frequency() == expected_hit.term_frequency();
+}
+
+MATCHER(EqualsHit, "") {
+  return ExplainMatchResult(EqualsHit(std::get<1>(arg)), std::get<0>(arg),
+                            result_listener);
+}
+
 // TODO(tjbarron) Remove this once icing has switched to depend on TC3 Status
 #define ICING_STATUS_MACROS_CONCAT_NAME(x, y) \
   ICING_STATUS_MACROS_CONCAT_IMPL(x, y)
diff --git a/icing/testing/hit-test-utils.cc b/icing/testing/hit-test-utils.cc
index c235e23..5aba779 100644
--- a/icing/testing/hit-test-utils.cc
+++ b/icing/testing/hit-test-utils.cc
@@ -29,22 +29,23 @@ namespace lib {
 // Returns a hit that has a delta of desired_byte_length from last_hit.
 Hit CreateHit(const Hit& last_hit, int desired_byte_length) {
   return CreateHit(last_hit, desired_byte_length, last_hit.term_frequency(),
-                   /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false);
+                   /*is_in_prefix_section=*/false, /*is_prefix_hit=*/false,
+                   /*is_stemmed_hit=*/false);
 }
 
 // Returns a hit that has a delta of desired_byte_length from last_hit, with
 // the desired term_frequency and flags
 Hit CreateHit(const Hit& last_hit, int desired_byte_length,
               Hit::TermFrequency term_frequency, bool is_in_prefix_section,
-              bool is_prefix_hit) {
+              bool is_prefix_hit, bool is_stemmed_hit) {
   Hit hit = last_hit;
   uint8_t buf[5];
   do {
     hit = (hit.section_id() == kMinSectionId)
               ? Hit(kMaxSectionId, hit.document_id() + 1, term_frequency,
-                    is_in_prefix_section, is_prefix_hit)
+                    is_in_prefix_section, is_prefix_hit, is_stemmed_hit)
               : Hit(hit.section_id() - 1, hit.document_id(), term_frequency,
-                    is_in_prefix_section, is_prefix_hit);
+                    is_in_prefix_section, is_prefix_hit, is_stemmed_hit);
   } while (PostingListHitSerializer::EncodeNextHitValue(
                /*next_hit_value=*/hit.value(),
                /*curr_hit_value=*/last_hit.value(), buf) < desired_byte_length);
@@ -61,7 +62,7 @@ std::vector<Hit> CreateHits(DocumentId start_docid, int num_hits,
   }
   hits.push_back(Hit(/*section_id=*/1, /*document_id=*/start_docid,
                      Hit::kDefaultTermFrequency, /*is_in_prefix_section=*/false,
-                     /*is_prefix_hit=*/false));
+                     /*is_prefix_hit=*/false, /*is_stemmed_hit=*/false));
   while (hits.size() < num_hits) {
     hits.push_back(CreateHit(hits.back(), desired_byte_length));
   }
diff --git a/icing/testing/hit-test-utils.h b/icing/testing/hit-test-utils.h
index e041c22..afe5550 100644
--- a/icing/testing/hit-test-utils.h
+++ b/icing/testing/hit-test-utils.h
@@ -32,7 +32,7 @@ Hit CreateHit(const Hit& last_hit, int desired_byte_length);
 // the desired term_frequency and flags
 Hit CreateHit(const Hit& last_hit, int desired_byte_length,
               Hit::TermFrequency term_frequency, bool is_in_prefix_section,
-              bool is_prefix_hit);
+              bool is_prefix_hit, bool is_stemmed_hit);
 
 // Returns a vector of num_hits Hits with the first hit starting at start_docid
 // and with desired_byte_length deltas.
diff --git a/icing/testing/recorder-test-utils.cc b/icing/testing/recorder-test-utils.cc
deleted file mode 100644
index ca87fd0..0000000
--- a/icing/testing/recorder-test-utils.cc
+++ /dev/null
@@ -1,70 +0,0 @@
-// Copyright (C) 2019 Google LLC
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//      http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-#include "icing/testing/recorder-test-utils.h"
-
-#include <list>
-#include <unordered_map>
-#include <unordered_set>
-
-#include "perftools/profiles/proto/profile.proto.h"
-#include "gmock/gmock.h"
-#include "gtest/gtest.h"
-#include "icing/file/filesystem.h"
-#include "util/gzip/gzipstring.h"
-
-namespace icing {
-namespace lib {
-
-namespace {
-
-// The implementation for these functions is shamelessly plagiarized from
-// https://cs.corp.google.com/search/?q=heap/alloc_recorder_test.cc
-void ReadProfileFromFile(const std::string& filename,
-                         perftools::profiles::Profile* profile) {
-  Filesystem filesystem;
-  ScopedFd sfd(filesystem.OpenForRead(filename.c_str()));
-  int size = filesystem.GetFileSize(sfd.get());
-  std::string str_profilez;
-  str_profilez.resize(size);
-  ASSERT_TRUE(filesystem.Read(sfd.get(), str_profilez.data(), size));
-
-  std::string str_profile;
-  ASSERT_TRUE(GunzipString(str_profilez, &str_profile));
-
-  ASSERT_TRUE(profile->ParseFromString(str_profile));
-}
-
-}  // namespace
-
-ProfileInfo SummarizeProfileProto(const std::string& filename) {
-  perftools::profiles::Profile profile;
-  ReadProfileFromFile(filename, &profile);
-  ProfileInfo profile_info{};
-  for (const auto &sample : profile.sample()) {
-    EXPECT_THAT(sample.value(), testing::SizeIs(6));
-    profile_info.peak_objects += sample.value(0);
-    profile_info.peak_bytes += sample.value(1);
-    profile_info.total_alloc_objects += sample.value(2);
-    profile_info.total_alloc_bytes += sample.value(3);
-    profile_info.inuse_objects += sample.value(4);
-    profile_info.inuse_bytes += sample.value(5);
-  }
-
-  return profile_info;
-}
-
-}  //  namespace lib
-}  //  namespace icing
-
diff --git a/icing/tokenization/combined-tokenizer_test.cc b/icing/tokenization/combined-tokenizer_test.cc
index 098c4de..0411748 100644
--- a/icing/tokenization/combined-tokenizer_test.cc
+++ b/icing/tokenization/combined-tokenizer_test.cc
@@ -35,6 +35,8 @@
 #include "icing/portable/platform.h"
 #include "icing/proto/schema.pb.h"
 #include "icing/query/query-processor.h"
+#include "icing/query/query-results.h"
+#include "icing/query/query-terms.h"
 #include "icing/schema/schema-store.h"
 #include "icing/store/document-store.h"
 #include "icing/testing/common-matchers.h"
@@ -52,8 +54,6 @@
 #include "icing/transform/normalizer.h"
 #include "icing/util/status-macros.h"
 #include "unicode/uloc.h"
-#include "icing/query/query-results.h"
-#include "icing/query/query-terms.h"
 
 namespace icing {
 namespace lib {
diff --git a/icing/tokenization/token.h b/icing/tokenization/token.h
index 05d6fe4..f285d13 100644
--- a/icing/tokenization/token.h
+++ b/icing/tokenization/token.h
@@ -60,6 +60,8 @@ struct Token {
     URL_SUFFIX,
     URL_SUFFIX_INNERMOST,
 
+    TRIGRAM,  // Trigram token of the text
+
     // Indicates errors
     INVALID,
   };
diff --git a/icing/tokenization/trigram-tokenizer.cc b/icing/tokenization/trigram-tokenizer.cc
new file mode 100644
index 0000000..4fca591
--- /dev/null
+++ b/icing/tokenization/trigram-tokenizer.cc
@@ -0,0 +1,291 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include "icing/tokenization/trigram-tokenizer.h"
+
+#include <cstdint>
+#include <deque>
+#include <memory>
+#include <string_view>
+#include <utility>
+#include <vector>
+
+#include "icing/text_classifier/lib3/utils/base/statusor.h"
+#include "icing/absl_ports/canonical_errors.h"
+#include "icing/tokenization/token.h"
+#include "icing/tokenization/tokenizer.h"
+#include "icing/util/character-iterator.h"
+#include "icing/util/i18n-utils.h"
+#include "icing/util/status-macros.h"
+
+namespace icing {
+namespace lib {
+
+class TrigramTokenizerIterator : public Tokenizer::Iterator {
+ public:
+  explicit TrigramTokenizerIterator(std::string_view text) : text_(text) {}
+
+  bool Advance() override {
+    if (utf8_char_itrs_.empty()) {
+      // First call to Advance().
+      return Initialize();
+    }
+
+    // Derive the next CharacterIterator pointing to the next utf8 character.
+    CharacterIterator next_char_itr = utf8_char_itrs_.back();
+    if (!next_char_itr.AdvanceToUtf32(next_char_itr.utf32_index() + 1)) {
+      return false;
+    }
+
+    // We reach the end of the text, so there is no more trigram token.
+    if (next_char_itr.utf8_index() >= text_.length()) {
+      return false;
+    }
+
+    // Update the deque.
+    utf8_char_itrs_.pop_front();
+    utf8_char_itrs_.push_back(std::move(next_char_itr));
+
+    return true;
+  }
+
+  std::vector<Token> GetTokens() const override {
+    int start_byte_idx = utf8_char_itrs_.front().utf8_index();
+    int end_byte_idx =
+        utf8_char_itrs_.back().utf8_index() +
+        i18n_utils::GetUtf8Length(utf8_char_itrs_.back().GetCurrentChar());
+
+    return {Token(/*type_in=*/Token::Type::TRIGRAM,
+                  /*text_in=*/text_.substr(start_byte_idx,
+                                           end_byte_idx - start_byte_idx))};
+  };
+
+  // Returns a character iterator to the start of the current trigram token.
+  //
+  // REQUIRES: the preceding Advance() call returned true.
+  libtextclassifier3::StatusOr<CharacterIterator> CalculateTokenStart()
+      override {
+    // The start index of the current trigram is utf8_char_itrs_.front().
+    return utf8_char_itrs_.front();
+  }
+
+  // Returns a character iterator to right after the end of the current trigram
+  // token.
+  //
+  // REQUIRES: the preceding Advance() call returned true.
+  libtextclassifier3::StatusOr<CharacterIterator> CalculateTokenEndExclusive()
+      override {
+    // Derive the end exclusive CharacterIterator by advancing
+    // utf8_char_itrs_.back() once.
+    CharacterIterator end_exclusive_itr = utf8_char_itrs_.back();
+    if (!end_exclusive_itr.AdvanceToUtf32(end_exclusive_itr.utf32_index() +
+                                          1)) {
+      return absl_ports::OutOfRangeError(
+          "Unable to create the end exclusive iterator of the current trigram. "
+          "This should not happen.");
+    }
+    return end_exclusive_itr;
+  }
+
+  // Resets the iterator to the leftmost trigram token starting after
+  // utf32_offset. The iterator state is undefined if the reset fails.
+  //
+  // Note: utf32_offset can be negative, which means it will be reset to the
+  // first trigram token.
+  //
+  // Returns: true if successfully resets.
+  bool ResetToTokenStartingAfter(int32_t utf32_offset) override {
+    // Initialize if necessary.
+    if (utf8_char_itrs_.empty() && !Initialize()) {
+      return false;
+    }
+
+    // Step 1: rewind iterators until utf8_char_itrs_.front() points to a utf8
+    //         character that starts before or at utf32_offset.
+    while (utf8_char_itrs_.front().utf32_index() > utf32_offset) {
+      if (utf8_char_itrs_.front().utf32_index() == 0) {
+        // Unable to rewind to the previous trigram token since we were already
+        // at the first trigram token.
+        //
+        // It means utf32_offset is too small and all trigram tokens start after
+        // it, so simply return true here since the first trigram token is the
+        // leftmost trigram token starting after utf32_offset.
+        return true;
+      }
+
+      // Attempt to rewind to the previous trigram token.
+      // This should always succeed since we checked the condition above, unless
+      // there is any invalid state or encoded bytes in text_.
+      if (!Rewind()) {
+        return false;
+      }
+    }
+
+    // Step 2: advance iterators until utf8_char_itrs_.front() points to a utf8
+    //         character that starts after utf32_offset.
+    //
+    // Note: if rewinded in step 1, then it is guaranteed to run only one
+    //       iteration here.
+    while (utf8_char_itrs_.front().utf32_index() <= utf32_offset) {
+      if (!Advance()) {
+        // It means we reached the end of the text but still cannot find a
+        // trigram token starting after utf32_offset.
+        return false;
+      }
+    }
+
+    return true;
+  }
+
+  // Resets the iterator to the rightmost trigram token ending before
+  // utf32_offset. The iterator state is undefined if the reset fails.
+  //
+  // Returns: true if successfully resets.
+  bool ResetToTokenEndingBefore(int32_t utf32_offset) override {
+    // Initialize if necessary.
+    if (utf8_char_itrs_.empty() && !Initialize()) {
+      return false;
+    }
+
+    // Step 1: advance iterators until utf8_char_itrs_.back() points to a utf8
+    //         character that ends after or at utf32_offset.
+    while (utf8_char_itrs_.back().utf32_index() < utf32_offset) {
+      // Derive the next CharacterIterator pointing to the next utf8 character.
+      CharacterIterator next_char_itr = utf8_char_itrs_.back();
+      if (!next_char_itr.AdvanceToUtf32(next_char_itr.utf32_index() + 1)) {
+        return false;
+      }
+
+      if (next_char_itr.utf8_index() >= text_.length()) {
+        // The next character is out of the text, so we're already at the last
+        // trigram token but it is still not ending after or at utf32_offset.
+        //
+        // It means utf32_offset is too large and all trigram tokens end before
+        // it, so simply return true here since the last trigram token is the
+        // rightmost trigram token ending before utf32_offset.
+        return true;
+      }
+
+      // Update the deque.
+      utf8_char_itrs_.pop_front();
+      utf8_char_itrs_.push_back(std::move(next_char_itr));
+    }
+
+    // Step 2: rewind iterators until utf8_char_itrs_.back() points to a utf8
+    //         character that ends before utf32_offset.
+    //
+    // Note: if advanced in step 1, then it is guaranteed to run only one
+    //       iteration here.
+    while (utf8_char_itrs_.back().utf32_index() >= utf32_offset) {
+      if (!Rewind()) {
+        // It means we reached the start of the text but still cannot find a
+        // trigram token ending before utf32_offset.
+        return false;
+      }
+    }
+
+    return true;
+  }
+
+  bool ResetToStart() override {
+    // Don't rewind it since it potentially calls RewindToUtf8() and rewinds
+    // characters one by one, which is not necessary because we know we want to
+    // start from the beginning of the text.
+    utf8_char_itrs_.clear();
+    return Initialize();
+  }
+
+ private:
+  static constexpr int kNgramLength = 3;
+
+  // Initializes utf8_char_itrs_ with 3 CharacterIterators pointing to 3 valid
+  // utf8 characters of the first trigram.
+  //
+  // REQUIRES: utf8_char_itrs_.empty()
+  bool Initialize() {
+    bool result = true;
+
+    CharacterIterator iterator(text_, /*utf8_index=*/0, /*utf16_index=*/0,
+                               /*utf32_index=*/0);
+    for (int i = 0; i < kNgramLength; ++i) {
+      if (iterator.utf8_index() >= text_.length()) {
+        // It means the text is too short (with # of characters < 3) to form a
+        // trigram.
+        result = false;
+        break;
+      }
+
+      utf8_char_itrs_.push_back(iterator);
+      // Advance to the next character.
+      if (!iterator.AdvanceToUtf32(iterator.utf32_index() + 1)) {
+        result = false;
+        break;
+      }
+    }
+
+    if (!result) {
+      // If failed to initialize, then clear the deque to avoid the next
+      // Advance() call to generate invalid CharacterIterators.
+      utf8_char_itrs_.clear();
+    }
+    return result;
+  }
+
+  // Rewinds utf8_char_itrs_ to point to the previous trigram token. Fails if
+  // it is already at the first trigram token.
+  //
+  // REQUIRES: !utf8_char_itrs_.empty()
+  bool Rewind() {
+    // Derive the CharacterIterator pointing to the previous character.
+    CharacterIterator prev_char_itr = utf8_char_itrs_.front();
+    if (!prev_char_itr.RewindToUtf32(prev_char_itr.utf32_index() - 1)) {
+      return false;
+    }
+
+    utf8_char_itrs_.pop_back();
+    utf8_char_itrs_.push_front(std::move(prev_char_itr));
+    return true;
+  }
+
+  std::string_view text_;  // Does not own the actual string data.
+
+  // This deque maintains 3 CharacterIterators pointing to 3 utf8 characters of
+  // the current trigram.
+  // - Starting byte indices of the current trigram are
+  //   utf8_char_itrs_[0].utf8_index(), utf8_char_itrs_[1].utf8_index(), and
+  //   utf8_char_itrs_[2].utf8_index().
+  // - When advancing or rewinding, we can derive the next or previous
+  //   CharacterIterators by the back or front element of this deque.
+  std::deque<CharacterIterator> utf8_char_itrs_;
+};
+
+libtextclassifier3::StatusOr<std::unique_ptr<Tokenizer::Iterator>>
+TrigramTokenizer::Tokenize(std::string_view text) const {
+  return std::make_unique<TrigramTokenizerIterator>(text);
+}
+
+libtextclassifier3::StatusOr<std::vector<Token>> TrigramTokenizer::TokenizeAll(
+    std::string_view text) const {
+  ICING_ASSIGN_OR_RETURN(std::unique_ptr<Tokenizer::Iterator> iterator,
+                         Tokenize(text));
+  std::vector<Token> tokens;
+  while (iterator->Advance()) {
+    std::vector<Token> batch_tokens = iterator->GetTokens();
+    tokens.insert(tokens.end(), batch_tokens.begin(), batch_tokens.end());
+  }
+  return tokens;
+}
+
+}  // namespace lib
+}  // namespace icing
diff --git a/icing/tokenization/trigram-tokenizer.h b/icing/tokenization/trigram-tokenizer.h
new file mode 100644
index 0000000..f1493f7
--- /dev/null
+++ b/icing/tokenization/trigram-tokenizer.h
@@ -0,0 +1,68 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef ICING_TOKENIZATION_TRIGRAM_TOKENIZER_H_
+#define ICING_TOKENIZATION_TRIGRAM_TOKENIZER_H_
+
+#include <memory>
+#include <string_view>
+#include <vector>
+
+#include "icing/text_classifier/lib3/utils/base/statusor.h"
+#include "icing/tokenization/token.h"
+#include "icing/tokenization/tokenizer.h"
+
+namespace icing {
+namespace lib {
+
+// A tokenizer that splits the input text into trigrams. Trigrams are useful for
+// several features, e.g. approximate substring matching, spell checking, etc.
+//
+// Trigram definition: 3 consecutive characters in a text.
+//
+// Note: TrigramTokenizer does not do any additional normalization or
+// segmentation. It simply yields trigram tokens from the input text without
+// skipping any special characters (e.g. punctuations, whitespaces). It is up to
+// the caller to handle normalization and segmentation before applying this
+// tokenizer.
+// - The caller can use PlainTokenizer to normalize and segment a text to
+//   several "term tokens" first, and then apply TrigramTokenizer to each of
+//   them to split the normalized and segmented term into trigrams.
+// - On the other hand, the caller can use VerbatimTokenizer instead to avoid
+//   normalization and segmentation, and then apply TrigramTokenizer to the
+//   verbatim text.
+//
+// Example:
+//
+// Input text: "abcdefg"
+// Output tokens: ["abc", "bcd", "cde", "def", "efg"]
+//
+// Input text: "我每天走路去上班"
+// Output tokens: ["我每天", "每天走", "天走路", "走路去", "去上班"]
+//
+// Input text: "foo bar"
+// Output tokens: ["foo", "oo ", "o b", " ba", "bar"]
+class TrigramTokenizer : public Tokenizer {
+ public:
+  libtextclassifier3::StatusOr<std::unique_ptr<Tokenizer::Iterator>> Tokenize(
+      std::string_view text) const override;
+
+  libtextclassifier3::StatusOr<std::vector<Token>> TokenizeAll(
+      std::string_view text) const override;
+};
+
+}  // namespace lib
+}  // namespace icing
+
+#endif  // ICING_TOKENIZATION_TRIGRAM_TOKENIZER_H_
diff --git a/icing/tokenization/trigram-tokenizer_test.cc b/icing/tokenization/trigram-tokenizer_test.cc
new file mode 100644
index 0000000..a1466c5
--- /dev/null
+++ b/icing/tokenization/trigram-tokenizer_test.cc
@@ -0,0 +1,1657 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include "icing/tokenization/trigram-tokenizer.h"
+
+#include <memory>
+#include <string_view>
+
+#include "gmock/gmock.h"
+#include "gtest/gtest.h"
+#include "icing/testing/common-matchers.h"
+#include "icing/tokenization/token.h"
+#include "icing/tokenization/tokenizer.h"
+#include "icing/util/character-iterator.h"
+
+namespace icing {
+namespace lib {
+
+namespace {
+
+using ::testing::ElementsAre;
+using ::testing::Eq;
+using ::testing::IsEmpty;
+using ::testing::IsFalse;
+using ::testing::IsTrue;
+
+TEST(TrigramTokenizerTest, TokenizeAll_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abcdefg");
+  EXPECT_THAT(
+      tokenizer.TokenizeAll(s),
+      IsOkAndHolds(ElementsAre(EqualsToken(Token::Type::TRIGRAM, "abc"),
+                               EqualsToken(Token::Type::TRIGRAM, "bcd"),
+                               EqualsToken(Token::Type::TRIGRAM, "cde"),
+                               EqualsToken(Token::Type::TRIGRAM, "def"),
+                               EqualsToken(Token::Type::TRIGRAM, "efg"))));
+}
+
+TEST(TrigramTokenizerTest, TokenizeAll_CJKT) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每天走路去上班");
+  EXPECT_THAT(
+      tokenizer.TokenizeAll(s),
+      IsOkAndHolds(ElementsAre(EqualsToken(Token::Type::TRIGRAM, "我每天"),
+                               EqualsToken(Token::Type::TRIGRAM, "每天走"),
+                               EqualsToken(Token::Type::TRIGRAM, "天走路"),
+                               EqualsToken(Token::Type::TRIGRAM, "走路去"),
+                               EqualsToken(Token::Type::TRIGRAM, "路去上"),
+                               EqualsToken(Token::Type::TRIGRAM, "去上班"))));
+}
+
+TEST(TrigramTokenizerTest, TokenizeAll_latinLettersWithAccents) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("āăąḃḅḇčćç");
+  EXPECT_THAT(
+      tokenizer.TokenizeAll(s),
+      IsOkAndHolds(ElementsAre(EqualsToken(Token::Type::TRIGRAM, "āăą"),
+                               EqualsToken(Token::Type::TRIGRAM, "ăąḃ"),
+                               EqualsToken(Token::Type::TRIGRAM, "ąḃḅ"),
+                               EqualsToken(Token::Type::TRIGRAM, "ḃḅḇ"),
+                               EqualsToken(Token::Type::TRIGRAM, "ḅḇč"),
+                               EqualsToken(Token::Type::TRIGRAM, "ḇčć"),
+                               EqualsToken(Token::Type::TRIGRAM, "čćç"))));
+}
+
+TEST(TrigramTokenizerTest, TokenizeAll_whitespaceShouldBeTrigrams) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("foo bar");
+  EXPECT_THAT(
+      tokenizer.TokenizeAll(s),
+      IsOkAndHolds(ElementsAre(EqualsToken(Token::Type::TRIGRAM, "foo"),
+                               EqualsToken(Token::Type::TRIGRAM, "oo "),
+                               EqualsToken(Token::Type::TRIGRAM, "o b"),
+                               EqualsToken(Token::Type::TRIGRAM, " ba"),
+                               EqualsToken(Token::Type::TRIGRAM, "bar"))));
+}
+
+TEST(TrigramTokenizerTest, TokenizeAll_punctuationShouldBeTrigrams) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("foo`~!@#$%^&*()-+=[]{}\\|;:'\",./<>?bar");
+  EXPECT_THAT(
+      tokenizer.TokenizeAll(s),
+      IsOkAndHolds(ElementsAre(EqualsToken(Token::Type::TRIGRAM, "foo"),
+                               EqualsToken(Token::Type::TRIGRAM, "oo`"),
+                               EqualsToken(Token::Type::TRIGRAM, "o`~"),
+                               EqualsToken(Token::Type::TRIGRAM, "`~!"),
+                               EqualsToken(Token::Type::TRIGRAM, "~!@"),
+                               EqualsToken(Token::Type::TRIGRAM, "!@#"),
+                               EqualsToken(Token::Type::TRIGRAM, "@#$"),
+                               EqualsToken(Token::Type::TRIGRAM, "#$%"),
+                               EqualsToken(Token::Type::TRIGRAM, "$%^"),
+                               EqualsToken(Token::Type::TRIGRAM, "%^&"),
+                               EqualsToken(Token::Type::TRIGRAM, "^&*"),
+                               EqualsToken(Token::Type::TRIGRAM, "&*("),
+                               EqualsToken(Token::Type::TRIGRAM, "*()"),
+                               EqualsToken(Token::Type::TRIGRAM, "()-"),
+                               EqualsToken(Token::Type::TRIGRAM, ")-+"),
+                               EqualsToken(Token::Type::TRIGRAM, "-+="),
+                               EqualsToken(Token::Type::TRIGRAM, "+=["),
+                               EqualsToken(Token::Type::TRIGRAM, "=[]"),
+                               EqualsToken(Token::Type::TRIGRAM, "[]{"),
+                               EqualsToken(Token::Type::TRIGRAM, "]{}"),
+                               EqualsToken(Token::Type::TRIGRAM, "{}\\"),
+                               EqualsToken(Token::Type::TRIGRAM, "}\\|"),
+                               EqualsToken(Token::Type::TRIGRAM, "\\|;"),
+                               EqualsToken(Token::Type::TRIGRAM, "|;:"),
+                               EqualsToken(Token::Type::TRIGRAM, ";:'"),
+                               EqualsToken(Token::Type::TRIGRAM, ":'\""),
+                               EqualsToken(Token::Type::TRIGRAM, "'\","),
+                               EqualsToken(Token::Type::TRIGRAM, "\",."),
+                               EqualsToken(Token::Type::TRIGRAM, ",./"),
+                               EqualsToken(Token::Type::TRIGRAM, "./<"),
+                               EqualsToken(Token::Type::TRIGRAM, "/<>"),
+                               EqualsToken(Token::Type::TRIGRAM, "<>?"),
+                               EqualsToken(Token::Type::TRIGRAM, ">?b"),
+                               EqualsToken(Token::Type::TRIGRAM, "?ba"),
+                               EqualsToken(Token::Type::TRIGRAM, "bar"))));
+}
+
+TEST(TrigramTokenizerTest, TokenizeAll_emptyStringShouldReturnEmpty) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("");
+  EXPECT_THAT(tokenizer.TokenizeAll(s), IsOkAndHolds(IsEmpty()));
+}
+
+TEST(TrigramTokenizerTest, TokenizeAll_tooShortStringShouldReturnEmpty_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s1("a");
+  EXPECT_THAT(tokenizer.TokenizeAll(s1), IsOkAndHolds(IsEmpty()));
+
+  std::string_view s2("ab");
+  EXPECT_THAT(tokenizer.TokenizeAll(s2), IsOkAndHolds(IsEmpty()));
+}
+
+TEST(TrigramTokenizerTest, TokenizeAll_tooShortStringShouldReturnEmpty_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s1("我");
+  EXPECT_THAT(tokenizer.TokenizeAll(s1), IsOkAndHolds(IsEmpty()));
+
+  std::string_view s2("我每");
+  EXPECT_THAT(tokenizer.TokenizeAll(s2), IsOkAndHolds(IsEmpty()));
+}
+
+TEST(TrigramTokenizerTest,
+     TokenizeAll_exactThreeCharsShouldReturnOneTrigram_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abc");
+  EXPECT_THAT(
+      tokenizer.TokenizeAll(s),
+      IsOkAndHolds(ElementsAre(EqualsToken(Token::Type::TRIGRAM, "abc"))));
+}
+
+TEST(TrigramTokenizerTest,
+     TokenizeAll_exactThreeCharsShouldReturnOneTrigram_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每天");
+  EXPECT_THAT(
+      tokenizer.TokenizeAll(s),
+      IsOkAndHolds(ElementsAre(EqualsToken(Token::Type::TRIGRAM, "我每天"))));
+}
+
+TEST(TrigramTokenizerTest, CalculateTokenStartAndEndExclusive_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abcd");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to "abc".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "abc")));
+  ICING_ASSERT_OK_AND_ASSIGN(CharacterIterator start_itr0,
+                             itr->CalculateTokenStart());
+  ICING_ASSERT_OK_AND_ASSIGN(CharacterIterator end_itr0,
+                             itr->CalculateTokenEndExclusive());
+  EXPECT_THAT(start_itr0.utf8_index(), Eq(0));
+  EXPECT_THAT(end_itr0.utf8_index(), Eq(3));
+
+  // Advance to "bcd".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "bcd")));
+  ICING_ASSERT_OK_AND_ASSIGN(CharacterIterator start_itr1,
+                             itr->CalculateTokenStart());
+  ICING_ASSERT_OK_AND_ASSIGN(CharacterIterator end_itr1,
+                             itr->CalculateTokenEndExclusive());
+  EXPECT_THAT(start_itr1.utf8_index(), Eq(1));
+  EXPECT_THAT(end_itr1.utf8_index(), Eq(4));
+
+  ASSERT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest, CalculateTokenStartAndEndExclusive_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每天走");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to "我每天".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "我每天")));
+  ICING_ASSERT_OK_AND_ASSIGN(CharacterIterator start_itr0,
+                             itr->CalculateTokenStart());
+  ICING_ASSERT_OK_AND_ASSIGN(CharacterIterator end_itr0,
+                             itr->CalculateTokenEndExclusive());
+  EXPECT_THAT(start_itr0.utf8_index(), Eq(0));
+  EXPECT_THAT(end_itr0.utf8_index(), Eq(s.find("走")));
+
+  // Advance to "每天走".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "每天走")));
+  ICING_ASSERT_OK_AND_ASSIGN(CharacterIterator start_itr1,
+                             itr->CalculateTokenStart());
+  ICING_ASSERT_OK_AND_ASSIGN(CharacterIterator end_itr1,
+                             itr->CalculateTokenEndExclusive());
+  EXPECT_THAT(start_itr1.utf8_index(), Eq(s.find("每")));
+  EXPECT_THAT(end_itr1.utf8_index(), Eq(s.length()));
+
+  ASSERT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest, ResetToTokenStartingAfter_resetBackward_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abcdefgh");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance 5 times to "efg".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "efg")));
+
+  // Reset backward to after utf32_offset 1.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/1), IsTrue());
+
+  // Verify the iterator is pointing to "cde" since it is the leftmost trigram
+  // token after utf32_offset 1.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "def".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "def")));
+  // Advance to "efg".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "efg")));
+  // Advance to "fgh".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "fgh")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+
+  // Reset again.
+  // Reset backward to after utf32_offset 0.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/0), IsTrue());
+
+  // Verify the iterator is pointing to "bcd" since it is the leftmost trigram
+  // token after utf32_offset 0.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "bcd")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "cde".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+  // Advance to "def".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "def")));
+  // Advance to "efg".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "efg")));
+  // Advance to "fgh".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "fgh")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest, ResetToTokenStartingAfter_resetBackward_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每天走路去上班");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance 5 times to "路去上".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "路去上")));
+
+  // Reset backward to after "每". Note that the utf32_offset of "每" is 1.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/1), IsTrue());
+
+  // Verify the iterator is pointing to "天走路" since it is the leftmost
+  // trigram token after "每".
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "天走路")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "走路去".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "走路去")));
+  // Advance to "路去上".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "路去上")));
+  // Advance to "去上班".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "去上班")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+
+  // Reset again.
+  // Reset backward to after utf32_offset 0.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/0), IsTrue());
+
+  // Verify the iterator is pointing to "每天走" since it is the leftmost
+  // trigram token after utf32_offset 0.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "每天走")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "天走路".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "天走路")));
+  // Advance to "走路去".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "走路去")));
+  // Advance to "路去上".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "路去上")));
+  // Advance to "去上班".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "去上班")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest, ResetToTokenStartingAfter_resetForward_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abcdefgh");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance 1 time to "abc".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "abc")));
+
+  // Reset forward to after utf32_offset 2.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/2), IsTrue());
+
+  // Verify the iterator is pointing to "def" since it is the leftmost trigram
+  // token after utf32_offset 2.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "def")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "efg".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "efg")));
+  // Advance to "fgh".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "fgh")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest, ResetToTokenStartingAfter_resetForward_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每天走路去上班");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance 1 time to "我每天".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "我每天")));
+
+  // Reset forward to after "天". Note that the utf32_offset of "天" is 2.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/2), IsTrue());
+
+  // Verify the iterator is pointing to "走路去" since it is the leftmost
+  // trigram token after "天".
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "走路去")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "路去上".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "路去上")));
+  // Advance to "去上班".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "去上班")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenStartingAfter_negativeUtf32OffsetShouldResetToStart) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abcde");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance twice to "bcd".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "bcd")));
+
+  // Reset to after utf32_offset -1.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/-1), IsTrue());
+
+  // Verify the iterator is pointing to "abc" since it is the leftmost trigram
+  // token after utf32_offset -1.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "abc")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "bcd".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "bcd")));
+  // Advance to "cde".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+
+  // Reset to after utf32_offset -2.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/-2), IsTrue());
+
+  // Verify the iterator is pointing to "abc" since it is the leftmost trigram
+  // token after utf32_offset -2.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "abc")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "bcd".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "bcd")));
+  // Advance to "cde".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenStartingAfter_utf32OffsetAfterOrAtLastTokenShouldFail_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abcde");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance twice to "bcd".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "bcd")));
+
+  // Reset to after utf32_offset 2. Since the last token is "cde" and the
+  // starting utf32_offset of it is 2, ResetToTokenStartingAfter() should fail
+  // with utf32_offset >= 2 since there is no token after it.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/2), IsFalse());
+
+  // Reset to after utf32_offset 3.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/3), IsFalse());
+
+  // Reset to after utf32_offset 4.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/4), IsFalse());
+
+  // Reset to after utf32_offset 5.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/5), IsFalse());
+
+  // Reset to after utf32_offset 6.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/6), IsFalse());
+
+  // Reset to after utf32_offset 7.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/7), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenStartingAfter_utf32OffsetAfterOrAtLastTokenShouldFail_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每天走路");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance once to "我每天".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "我每天")));
+
+  // Reset to after "天" (with utf32_offset 2). Since the last token is
+  // "天走路", ResetToTokenStartingAfter() should fail with utf32_offset >= 2
+  // since there is no token after it.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/2), IsFalse());
+
+  // Reset to after utf32_offset 3.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/3), IsFalse());
+
+  // Reset to after utf32_offset 4.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/4), IsFalse());
+
+  // Reset to after utf32_offset 5.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/5), IsFalse());
+
+  // Reset to after utf32_offset 6.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/6), IsFalse());
+
+  // Reset to after utf32_offset 7.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/7), IsFalse());
+}
+
+TEST(
+    TrigramTokenizerTest,
+    ResetToTokenStartingAfter_fromInitStateWithValidStringShouldSucceed_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abcdef");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToTokenStartingAfter() with a valid utf32_offset without any
+  // Advance().
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/1), IsTrue());
+
+  // Verify the iterator is pointing to "cde" since it is the leftmost trigram
+  // token after utf32_offset 1.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "def".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "def")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenStartingAfter_fromInitStateWithValidStringShouldSucceed_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每天走路去");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToTokenStartingAfter() with a valid utf32_offset without any
+  // Advance().
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/1), IsTrue());
+
+  // Verify the iterator is pointing to "天走路" since it is the leftmost
+  // trigram token after utf32_offset 1.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "天走路")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "走路去".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "走路去")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenStartingAfter_fromInitStateWithEmptyStringShouldFail) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToTokenStartingAfter() without any Advance(). This should fail
+  // since the text is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/-1), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/0), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/1), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/2), IsFalse());
+}
+
+TEST(
+    TrigramTokenizerTest,
+    ResetToTokenStartingAfter_fromInitStateWithTooShortStringShouldFail_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("ab");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToTokenStartingAfter() without any Advance(). This should fail
+  // since the text is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/-1), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/0), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/1), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/2), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenStartingAfter_fromInitStateWithTooShortStringShouldFail_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToTokenStartingAfter() without any Advance(). This should fail
+  // since the text is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/-1), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/0), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/1), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/2), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenStartingAfter_fromEndStateWithValidStringShouldSucceed_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abcdef");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToTokenStartingAfter() with a valid utf32_offset when reaching
+  // the end.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/1), IsTrue());
+
+  // Verify the iterator is pointing to "cde" since it is the leftmost trigram
+  // token after utf32_offset 1.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "def".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "def")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenStartingAfter_fromEndStateWithValidStringShouldSucceed_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每天走路去");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToTokenStartingAfter() with a valid utf32_offset when reaching
+  // the end.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/1), IsTrue());
+
+  // Verify the iterator is pointing to "天走路" since it is the leftmost
+  // trigram token after utf32_offset 1.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "天走路")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "走路去".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "走路去")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenStartingAfter_fromEndStateWithEmptyStringShouldFail) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToTokenStartingAfter() when reaching the end. This should fail
+  // since the text is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/-1), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/0), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/1), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/2), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenStartingAfter_fromEndStateWithTooShortStringShouldFail_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("ab");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToTokenStartingAfter() when reaching the end. This should fail
+  // since the text is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/-1), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/0), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/1), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/2), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenStartingAfter_fromEndStateWithTooShortStringShouldFail_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToTokenStartingAfter() when reaching the end. This should fail
+  // since the text is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/-1), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/0), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/1), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenStartingAfter(/*utf32_offset=*/2), IsFalse());
+}
+
+TEST(TrigramTokenizerTest, ResetToTokenEndingBefore_resetBackward_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abcdefgh");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance 5 times to "efg".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "efg")));
+
+  // Reset backward to before utf32_offset 5.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/5), IsTrue());
+
+  // Verify the iterator is pointing to "cde" since it is the rightmost trigram
+  // token before utf32_offset 5.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "def".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "def")));
+  // Advance to "efg".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "efg")));
+  // Advance to "fgh".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "fgh")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest, ResetToTokenEndingBefore_resetBackward_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每天走路去上班");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance 5 times to "路去上".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "路去上")));
+
+  // Reset backward to before "去". Note that the utf32_offset of "去" is 5.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/5), IsTrue());
+
+  // Verify the iterator is pointing to "天走路" since it is the rightmost
+  // trigram token before "每".
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "天走路")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "走路去".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "走路去")));
+  // Advance to "路去上".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "路去上")));
+  // Advance to "去上班".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "去上班")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest, ResetToTokenEndingBefore_resetForward_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abcdefgh");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance 1 time to "abc".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "abc")));
+
+  // Reset forward to before utf32_offset 6.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/6), IsTrue());
+
+  // Verify the iterator is pointing to "def" since it is the rightmost trigram
+  // token before utf32_offset 6.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "def")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "efg".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "efg")));
+  // Advance to "fgh".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "fgh")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest, ResetToTokenEndingBefore_resetForward_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每天走路去上班");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance 1 time to "我每天".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "我每天")));
+
+  // Reset forward to before "上". Note that the utf32_offset of "上" is 6.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/6), IsTrue());
+
+  // Verify the iterator is pointing to "走路去" since it is the rightmost
+  // trigram token before "上".
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "走路去")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "路去上".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "路去上")));
+  // Advance to "去上班".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "去上班")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(
+    TrigramTokenizerTest,
+    ResetToTokenEndingBefore_utf32OffsetAfterOrAtLastTokenShouldResetToLast_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abcde");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance once to "abc".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "abc")));
+
+  // Reset to before utf32_offset 5. Since the last token "cde" ends at
+  // utf32_offset 4, the iterator should be reset to "cde" which is the
+  // rightmost trigram token before utf32_offset 5.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/5), IsTrue());
+
+  // Verify the iterator is pointing to "cde" since it is the rightmost trigram
+  // token before utf32_offset 5.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+
+  // Since it is the last token, the next Advance() should fail.
+  EXPECT_THAT(itr->Advance(), IsFalse());
+
+  // Reset to before utf32_offset 6. Since the last token "cde" ends at
+  // utf32_offset 4, the iterator should be reset to "cde" which is the
+  // rightmost trigram token before utf32_offset 6.
+  //
+  // ResetToTokenEndingBefore should still work even if utf32_offset 6 is
+  // greater than # of characters in the text.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/6), IsTrue());
+
+  // Verify the iterator is pointing to "cde" since it is the rightmost trigram
+  // token before utf32_offset 6.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+
+  // Since it is the last token, the next Advance() should fail.
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(
+    TrigramTokenizerTest,
+    ResetToTokenEndingBefore_utf32OffsetAfterOrAtLastTokenShouldResetToLast_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每天走路");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance once to "我每天".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "我每天")));
+
+  // Reset to before utf32_offset 5. Since the last token "天走路" ends at
+  // utf32_offset 4, the iterator should be reset to "天走路" which is the
+  // rightmost trigram token before utf32_offset 5.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/5), IsTrue());
+
+  // Verify the iterator is pointing to "天走路" since it is the rightmost
+  // trigram token before utf32_offset 5.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "天走路")));
+
+  // Since it is the last token, the next Advance() should fail.
+  EXPECT_THAT(itr->Advance(), IsFalse());
+
+  // Reset to before utf32_offset 6. Since the last token "天走路" ends at
+  // utf32_offset 4, the iterator should be reset to "天走路" which is the
+  // rightmost trigram token before utf32_offset 6.
+  //
+  // ResetToTokenEndingBefore should still work even if utf32_offset 6 is
+  // greater than # of characters in the text.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/6), IsTrue());
+
+  // Verify the iterator is pointing to "天走路" since it is the rightmost
+  // trigram token before utf32_offset 6.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "天走路")));
+
+  // Since it is the last token, the next Advance() should fail.
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenEndingBefore_utf32OffsetBeforeOrAtFirstTokenShouldFail_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abcde");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance twice to "bcd".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "bcd")));
+
+  // Reset to before utf32_offset 2. Since the first token is "abc" and the
+  // ending utf32_offset of it is 2, ResetToTokenEndingBefore() should fail with
+  // utf32_offset <= 2 since there is no token before it.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/2), IsFalse());
+
+  // Reset to before utf32_offset 1.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/1), IsFalse());
+
+  // Reset to before utf32_offset 0.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/0), IsFalse());
+
+  // Reset to before utf32_offset -1.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/-1), IsFalse());
+
+  // Reset to before utf32_offset -2.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/-2), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenEndingBefore_utf32OffsetBeforeOrAtFirstTokenShouldFail_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每天走路");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance once to "我每天".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "我每天")));
+
+  // Reset to before "天" (with utf32_offset 2). Since the first token is
+  // "天走路", ResetToTokenEndingBefore() should fail with utf32_offset <= 2
+  // since there is no token before it.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/2), IsFalse());
+
+  // Reset to before utf32_offset 1.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/1), IsFalse());
+
+  // Reset to before utf32_offset 0.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/0), IsFalse());
+
+  // Reset to before utf32_offset -1.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/-1), IsFalse());
+
+  // Reset to before utf32_offset -2.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/-2), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenEndingBefore_fromInitStateWithValidStringShouldSucceed_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abcdef");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToTokenEndingBefore() with a valid utf32_offset without any
+  // Advance().
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/5), IsTrue());
+
+  // Verify the iterator is pointing to "cde" since it is the rightmost trigram
+  // token before utf32_offset 5.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "def".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "def")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenEndingBefore_fromInitStateWithValidStringShouldSucceed_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每天走路去");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToTokenEndingBefore() with a valid utf32_offset without any
+  // Advance().
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/5), IsTrue());
+
+  // Verify the iterator is pointing to "天走路" since it is the rightmost
+  // trigram token before utf32_offset 5.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "天走路")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "走路去".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "走路去")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenEndingBefore_fromInitStateWithEmptyStringShouldFail) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToTokenEndingBefore() without any Advance(). This should fail
+  // since the text is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/0), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/1), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/2), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/3), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/4), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/5), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/6), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenEndingBefore_fromInitStateWithTooShortStringShouldFail_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("ab");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToTokenEndingBefore() without any Advance(). This should fail
+  // since the text is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/2), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/3), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/4), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/5), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/6), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenEndingBefore_fromInitStateWithTooShortStringShouldFail_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToTokenEndingBefore() without any Advance(). This should fail
+  // since the text is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/2), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/3), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/4), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/5), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/6), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenEndingBefore_fromEndStateWithValidStringShouldSucceed_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("abcdef");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToTokenEndingBefore() with a valid utf32_offset when reaching the
+  // end.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/5), IsTrue());
+
+  // Verify the iterator is pointing to "cde" since it is the rightmost trigram
+  // token before utf32_offset 5.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "def".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "def")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenEndingBefore_fromEndStateWithValidStringShouldSucceed_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每天走路去");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToTokenEndingBefore() with a valid utf32_offset when reaching the
+  // end.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/5), IsTrue());
+
+  // Verify the iterator is pointing to "天走路" since it is the rightmost
+  // trigram token before utf32_offset 5.
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "天走路")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "走路去".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "走路去")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenEndingBefore_fromEndStateWithEmptyStringShouldFail) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToTokenEndingBefore() when reaching the end. This should fail
+  // since the text is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/0), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/1), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/2), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/3), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/4), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/5), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/6), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenEndingBefore_fromEndStateWithTooShortStringShouldFail_ascii) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("ab");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToTokenEndingBefore() when reaching the end. This should fail
+  // since the text is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/2), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/3), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/4), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/5), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/6), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToTokenEndingBefore_fromEndStateWithTooShortStringShouldFail_utf8) {
+  TrigramTokenizer tokenizer;
+
+  std::string_view s("我每");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToTokenEndingBefore() when reaching the end. This should fail
+  // since the text is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/2), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/3), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/4), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/5), IsFalse());
+  EXPECT_THAT(itr->ResetToTokenEndingBefore(/*utf32_offset=*/6), IsFalse());
+}
+
+TEST(TrigramTokenizerTest, ResetToStart_ascii) {
+  TrigramTokenizer tokenizer = TrigramTokenizer();
+
+  std::string_view s("abcdef");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance 3 times to "cde".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+
+  // Call ResetToStart().
+  EXPECT_THAT(itr->ResetToStart(), IsTrue());
+
+  // Verify the iterator is pointing to the start trigram "abc".
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "abc")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "bcd".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "bcd")));
+  // Advance to "cde".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+  // Advance to "def".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "def")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest, ResetToStart_utf8) {
+  TrigramTokenizer tokenizer = TrigramTokenizer();
+
+  std::string_view s("我每天走路去");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance 3 times to "天走路".
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "天走路")));
+
+  // Call ResetToStart().
+  EXPECT_THAT(itr->ResetToStart(), IsTrue());
+
+  // Verify the iterator is pointing to the start trigram "我每天".
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "我每天")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "每天走".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "每天走")));
+  // Advance to "天走路".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "天走路")));
+  // Advance to "走路去".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "走路去")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToStart_fromInitStateWithValidStringShouldSucceed_ascii) {
+  TrigramTokenizer tokenizer = TrigramTokenizer();
+
+  std::string_view s("abcde");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToStart() without any Advance().
+  EXPECT_THAT(itr->ResetToStart(), IsTrue());
+
+  // Verify the iterator is pointing to "abc".
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "abc")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "bcd".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "bcd")));
+  // Advance to "cde".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToStart_fromInitStateWithValidStringShouldSucceed_utf8) {
+  TrigramTokenizer tokenizer = TrigramTokenizer();
+
+  std::string_view s("我每天走路");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToStart() without any Advance().
+  EXPECT_THAT(itr->ResetToStart(), IsTrue());
+
+  // Verify the iterator is pointing to "我每天".
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "我每天")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "每天走".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "每天走")));
+  // Advance to "天走路".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "天走路")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToStart_fromInitStateWithEmptyStringShouldFail) {
+  TrigramTokenizer tokenizer = TrigramTokenizer();
+
+  std::string_view s("");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToStart() without any Advance(). This should fail since the text
+  // is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToStart(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToStart_fromInitStateWithTooShortStringShouldFail_ascii) {
+  TrigramTokenizer tokenizer = TrigramTokenizer();
+
+  std::string_view s("ab");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToStart() without any Advance(). This should fail since the text
+  // is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToStart(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToStart_fromInitStateWithTooShortStringShouldFail_utf8) {
+  TrigramTokenizer tokenizer = TrigramTokenizer();
+
+  std::string_view s("我每");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Call ResetToStart() without any Advance(). This should fail since the text
+  // is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToStart(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToStart_fromEndStateWithValidStringShouldSucceed_ascii) {
+  TrigramTokenizer tokenizer = TrigramTokenizer();
+
+  std::string_view s("abcdef");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToStart() when reaching the end.
+  EXPECT_THAT(itr->ResetToStart(), IsTrue());
+
+  // Verify the iterator is pointing to the start trigram "abc".
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "abc")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "bcd".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "bcd")));
+  // Advance to "cde".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "cde")));
+  // Advance to "def".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "def")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToStart_fromEndStateWithValidStringShouldSucceed_utf8) {
+  TrigramTokenizer tokenizer = TrigramTokenizer();
+
+  std::string_view s("我每天走路去");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsTrue());
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToStart() when reaching the end.
+  EXPECT_THAT(itr->ResetToStart(), IsTrue());
+
+  // Verify the iterator is pointing to the start trigram "我每天".
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "我每天")));
+
+  // Verify the iterator is still valid and able to advance to the rest of the
+  // tokens.
+  // Advance to "每天走".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "每天走")));
+  // Advance to "天走路".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "天走路")));
+  // Advance to "走路去".
+  EXPECT_THAT(itr->Advance(), IsTrue());
+  EXPECT_THAT(itr->GetTokens(),
+              ElementsAre(EqualsToken(Token::Type::TRIGRAM, "走路去")));
+
+  EXPECT_THAT(itr->Advance(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest, ResetToStart_fromEndStateWithEmptyStringShouldFail) {
+  TrigramTokenizer tokenizer = TrigramTokenizer();
+
+  std::string_view s("");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToStart() when reaching the end. This should fail since the text
+  // is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToStart(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToStart_fromEndStateWithTooShortStringShouldFail_ascii) {
+  TrigramTokenizer tokenizer = TrigramTokenizer();
+
+  std::string_view s("ab");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToStart() when reaching the end. This should fail since the text
+  // is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToStart(), IsFalse());
+}
+
+TEST(TrigramTokenizerTest,
+     ResetToStart_fromEndStateWithTooShortStringShouldFail_utf8) {
+  TrigramTokenizer tokenizer = TrigramTokenizer();
+
+  std::string_view s("我每");
+  ICING_ASSERT_OK_AND_ASSIGN(std::unique_ptr<Tokenizer::Iterator> itr,
+                             tokenizer.Tokenize(s));
+
+  // Advance to the end.
+  ASSERT_THAT(itr->Advance(), IsFalse());
+
+  // Call ResetToStart() when reaching the end. This should fail since the text
+  // is too short to form a trigram.
+  EXPECT_THAT(itr->ResetToStart(), IsFalse());
+}
+
+}  // namespace
+
+}  // namespace lib
+}  // namespace icing
diff --git a/icing/util/character-iterator.cc b/icing/util/character-iterator.cc
index 0ab1e50..3e310e9 100644
--- a/icing/util/character-iterator.cc
+++ b/icing/util/character-iterator.cc
@@ -14,7 +14,10 @@
 
 #include "icing/util/character-iterator.h"
 
+#include <string_view>
+
 #include "icing/util/i18n-utils.h"
+#include "unicode/utypes.h"
 
 namespace icing {
 namespace lib {
@@ -32,7 +35,7 @@ int GetUTF8StartPosition(std::string_view text, int current_byte_index) {
 
 }  // namespace
 
-UChar32 CharacterIterator::GetCurrentChar() {
+UChar32 CharacterIterator::GetCurrentChar() const {
   if (cached_current_char_ == i18n_utils::kInvalidUChar32) {
     // Our indices point to the right character, we just need to read that
     // character. No need to worry about an error. If GetUChar32At fails, then
diff --git a/icing/util/character-iterator.h b/icing/util/character-iterator.h
index 893718a..0960965 100644
--- a/icing/util/character-iterator.h
+++ b/icing/util/character-iterator.h
@@ -15,8 +15,12 @@
 #ifndef ICING_UTIL_CHARACTER_ITERATOR_H_
 #define ICING_UTIL_CHARACTER_ITERATOR_H_
 
+#include <string>
+#include <string_view>
+
 #include "icing/legacy/core/icing-string-util.h"
 #include "icing/util/i18n-utils.h"
+#include "unicode/utypes.h"
 
 namespace icing {
 namespace lib {
@@ -36,7 +40,7 @@ class CharacterIterator {
 
   // Returns the character that the iterator currently points to.
   // i18n_utils::kInvalidUChar32 if unable to read that character.
-  UChar32 GetCurrentChar();
+  UChar32 GetCurrentChar() const;
 
   // Moves current position to desired_utf8_index.
   // REQUIRES: 0 <= desired_utf8_index <= text_.length()
@@ -104,7 +108,7 @@ class CharacterIterator {
   void ResetToStartIfNecessary();
 
   std::string_view text_;
-  UChar32 cached_current_char_;
+  mutable UChar32 cached_current_char_;
   int utf8_index_;
   int utf16_index_;
   int utf32_index_;
diff --git a/icing/util/crc32.h b/icing/util/crc32.h
index 207a80a..dfa7756 100644
--- a/icing/util/crc32.h
+++ b/icing/util/crc32.h
@@ -41,6 +41,10 @@ class Crc32 {
     return crc_ == other.Get();
   }
 
+  inline bool operator!=(const Crc32& other) const {
+    return crc_ != other.Get();
+  }
+
   // Returns the checksum of all the data that has been processed till now.
   uint32_t Get() const;
 
diff --git a/icing/util/document-validator.cc b/icing/util/document-validator.cc
index bc75334..916b34e 100644
--- a/icing/util/document-validator.cc
+++ b/icing/util/document-validator.cc
@@ -37,6 +37,8 @@
 namespace icing {
 namespace lib {
 
+static constexpr int32_t kSha256LengthBytes = 32;
+
 using PropertyConfigMap =
     std::unordered_map<std::string_view, const PropertyConfigProto*>;
 
@@ -139,10 +141,22 @@ libtextclassifier3::Status DocumentValidator::Validate(
       for (const PropertyProto::VectorProto& vector_value :
            property.vector_values()) {
         if (vector_value.values_size() == 0) {
-          return absl_ports::InvalidArgumentError(IcingStringUtil::StringPrintf(
-              "Property '%s' contains empty vectors for key: (%s, %s).",
-              property.name().c_str(), document.namespace_().c_str(),
-              document.uri().c_str()));
+          return absl_ports::InvalidArgumentError(absl_ports::StrCat(
+              "Property '", property.name(),
+              "' contains empty vectors for key: (", document.namespace_(),
+              ", ", document.uri(), ")."));
+        }
+      }
+    } else if (property_config.data_type() ==
+               PropertyConfigProto::DataType::BLOB_HANDLE) {
+      value_size = property.blob_handle_values_size();
+      for (const PropertyProto::BlobHandleProto& blob_handle_value :
+           property.blob_handle_values()) {
+        if (blob_handle_value.digest().size() != kSha256LengthBytes) {
+          return absl_ports::InvalidArgumentError(absl_ports::StrCat(
+              "Property '", property.name(),
+              "' contains non sha-256 blob digest for key: (",
+              document.namespace_(), ", ", document.uri(), ")."));
         }
       }
     }
diff --git a/icing/util/encode-util.cc b/icing/util/encode-util.cc
index 2642da7..32a2b09 100644
--- a/icing/util/encode-util.cc
+++ b/icing/util/encode-util.cc
@@ -44,6 +44,29 @@ uint64_t DecodeIntFromCString(std::string_view encoded_str) {
   return value;
 }
 
+std::string EncodeStringToCString(std::string input) {
+  std::string encoded_str;
+  // use uint32_t to store 4 bytes, using unsigned types to keep automatically
+  // remove extra left most bits.
+  uint32_t bit_buffer = 0;
+  int bit_count = 0;
+  for (unsigned char c : input) {
+    // Add the next byte to the buffer.
+    bit_buffer = (bit_buffer << 8) | c;
+    bit_count += 8;
+    // Encode the first 7 bits of the byte buffer by adding 1 in front.
+    while (bit_count >= 7) {
+      bit_count -= 7;
+      encoded_str += ((bit_buffer >> bit_count) & 0x7F) | 0x80;
+    }
+  }
+  // Encode the remaining byte.
+  if (bit_count > 0) {
+    encoded_str += ((bit_buffer << (7 - bit_count)) & 0x7F) | 0x80;
+  }
+  return encoded_str;
+}
+
 }  // namespace encode_util
 
 }  // namespace lib
diff --git a/icing/util/encode-util.h b/icing/util/encode-util.h
index 5a31acb..ca0d8a9 100644
--- a/icing/util/encode-util.h
+++ b/icing/util/encode-util.h
@@ -37,6 +37,18 @@ std::string EncodeIntToCString(uint64_t value);
 // integer.
 uint64_t DecodeIntFromCString(std::string_view encoded_str);
 
+// Converts the given string which may contains 0-byte to a C string.
+//
+// The output C string that does not contain 0-byte since C string uses 0-byte
+// as terminator.
+// This will increase the size of the encoded_str.
+// new_length = ceil(old_length / 7.0 * 8.0)
+// Eg1: This increases the size from 32-bytes of sha-256 hash to 37-bytes C
+// string.
+// Eg2: This increases the size from 1-byte string to 2-bytes C string.
+// Eg3: This increases the size from 2-byte string to 3-bytes C string.
+std::string EncodeStringToCString(std::string input);
+
 }  // namespace encode_util
 
 }  // namespace lib
diff --git a/icing/util/encode-util_test.cc b/icing/util/encode-util_test.cc
index c6cb984..699be90 100644
--- a/icing/util/encode-util_test.cc
+++ b/icing/util/encode-util_test.cc
@@ -84,6 +84,90 @@ TEST(EncodeUtilTest, MultipleValidEncodedCStringIntConversionsAreReversible) {
               Eq("Youtube"));
 }
 
+TEST(EncodeUtilTest, EncodeStringToCString) {
+  std::string digest;
+  digest.push_back(0b00000000);  // '\0'
+  digest.push_back(0b00000001);  // '\1'
+  digest.push_back(0b00000010);  // '\2'
+  digest.push_back(0b00000011);  // '\3'
+  digest.push_back(0b00000100);  // '\4'
+  digest.push_back(0b00000101);  // '\5'
+  digest.push_back(0b00000110);  // '\6'
+  digest.push_back(0b00000111);  // '\7'
+  digest.push_back(0b00001000);  // '\8'
+  digest.push_back(0b00001001);  // '\9'
+  std::string encoded_digest;
+  // 1 + first 7 bits from '\0'
+  encoded_digest.push_back(0b10000000);
+  // 1 + last 1 bit from '\0' + first 6 bits from '\1'
+  encoded_digest.push_back(0b10000000);
+  // 1 + last 2 bits from '\1' + first 5 bits from '\2'
+  encoded_digest.push_back(0b10100000);
+  // 1 + last 3 bits from '\2' + first 4 bits from '\3'
+  encoded_digest.push_back(0b10100000);
+  // 1 + last 4 bits from '\3' + first 3 bits from '\4'
+  encoded_digest.push_back(0b10011000);
+  // 1 + last 5 bits from '\4' + first 2 bits from '\5'
+  encoded_digest.push_back(0b10010000);
+  // 1 + last 6 bits from '\5' + first 1 bits from '\6'
+  encoded_digest.push_back(0b10001010);
+  // 1 + last 7 bits from '\6'
+  encoded_digest.push_back(0b10000110);
+  // 1 + first 7 bits from '\7'
+  encoded_digest.push_back(0b10000011);
+  // 1 + last 1 bit from '\7' + first 6 bits from '\8'
+  encoded_digest.push_back(0b11000010);
+  // 1 + last 2 bit from '\8' + first 5 bits from '\9'
+  encoded_digest.push_back(0b10000001);
+  // 1 + last 3 bit from '\9' + filled with 0s
+  encoded_digest.push_back(0b10010000);
+
+  EXPECT_THAT(EncodeStringToCString(digest), Eq(encoded_digest));
+}
+
+TEST(EncodeUtilTest, EncodeEmptyStringToCString) {
+  std::string digest;
+  std::string encoded_digest;
+
+  EXPECT_THAT(EncodeStringToCString(digest), Eq(encoded_digest));
+}
+
+TEST(EncodeUtilTest, EncodeMiddle0ByteStringToCStringConversions) {
+  std::string digest;
+  digest.push_back(0b00000001);  // '\1'
+  digest.push_back(0b00000010);  // '\2'
+  digest.push_back(0b00000011);  // '\3'
+  digest.push_back(0b00000000);  // '\0'
+  digest.push_back(0b00000100);  // '\4'
+  digest.push_back(0b00000101);  // '\5'
+  digest.push_back(0b00000110);  // '\6'
+  std::string encoded_digest;
+  // 1 + first 7 bits from '\1'
+  encoded_digest.push_back(0b10000000);
+  // 1 + last 1 bit from '\1' + first 6 bits from '\2'
+  encoded_digest.push_back(0b11000000);
+  // 1 + last 2 bits from '\2' + first 5 bits from '\3'
+  encoded_digest.push_back(0b11000000);
+  // 1 + last 3 bits from '\3' + first 4 bits from '\0'
+  encoded_digest.push_back(0b10110000);
+  // 1 + last 4 bits from '\0' + first 3 bits from '\4'
+  encoded_digest.push_back(0b10000000);
+  // 1 + last 5 bits from '\4' + first 2 bits from '\5'
+  encoded_digest.push_back(0b10010000);
+  // 1 + last 6 bits from '\5' + first 1 bits from '\6'
+  encoded_digest.push_back(0b10001010);
+  // 1 + last 7 bits from '\6'
+  encoded_digest.push_back(0b10000110);
+
+  EXPECT_THAT(EncodeStringToCString(digest), Eq(encoded_digest));
+}
+
+TEST(EncodeUtilTest, Encode32BytesDigestToCStringLength) {
+  std::string digest(32, 0b00000000);
+  // 37 = ceil(32 / 7.0 * 8.0)
+  EXPECT_THAT(EncodeStringToCString(digest).size(), Eq(37));
+}
+
 }  // namespace
 
 }  // namespace encode_util
diff --git a/icing/util/sha256.cc b/icing/util/sha256.cc
new file mode 100644
index 0000000..06b4152
--- /dev/null
+++ b/icing/util/sha256.cc
@@ -0,0 +1,171 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include "icing/util/sha256.h"
+
+#include <array>
+#include <cstdint>
+#include <cstring>
+
+namespace icing {
+namespace lib {
+
+// Constants for SHA-256 algorithm
+constexpr uint32_t k[64] = {
+    0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5, 0x3956c25b, 0x59f111f1,
+    0x923f82a4, 0xab1c5ed5, 0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,
+    0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174, 0xe49b69c1, 0xefbe4786,
+    0x0fc19dc6, 0x240ca1cc, 0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,
+    0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7, 0xc6e00bf3, 0xd5a79147,
+    0x06ca6351, 0x14292967, 0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,
+    0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85, 0xa2bfe8a1, 0xa81a664b,
+    0xc24b8b70, 0xc76c51a3, 0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,
+    0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5, 0x391c0cb3, 0x4ed8aa4a,
+    0x5b9cca4f, 0x682e6ff3, 0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,
+    0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2};
+
+constexpr uint8_t kPaddingFirstByte = 0x80;
+constexpr uint8_t kNullChar = '\0';
+
+// Function to perform a right rotation on a 32-bit value
+uint32_t RightRotate(uint32_t value, unsigned int count) {
+  return value >> count | value << (32 - count);
+}
+
+Sha256::Sha256() {
+  state_[0] = 0x6a09e667;
+  state_[1] = 0xbb67ae85;
+  state_[2] = 0x3c6ef372;
+  state_[3] = 0xa54ff53a;
+  state_[4] = 0x510e527f;
+  state_[5] = 0x9b05688c;
+  state_[6] = 0x1f83d9ab;
+  state_[7] = 0x5be0cd19;
+  count_ = 0;
+  memset(buffer_.data(), 0, sizeof(buffer_));
+}
+
+void Sha256::Transform() {
+  uint32_t w[64];
+  int t = 0;
+  // Process the first 16 words of the message block
+  for (; t < 16; ++t) {
+      uint32_t tmp = static_cast<uint32_t>(buffer_[t * 4]) << 24;
+      tmp |= static_cast<uint32_t>(buffer_[t * 4 + 1]) << 16;
+      tmp |= static_cast<uint32_t>(buffer_[t * 4 + 2]) << 8;
+      tmp |= static_cast<uint32_t>(buffer_[t * 4 + 3]);
+      w[t] = tmp;
+  }
+
+  // Extend the first 16 words into the remaining 48 words of the message
+  // schedule
+  for (; t < 64; t++) {
+    // Calculate the next word in the message schedule based on the previous
+    // words
+    uint32_t s0 = RightRotate(w[t - 15], 7) ^ RightRotate(w[t - 15], 18) ^
+                  (w[t - 15] >> 3);
+    uint32_t s1 = RightRotate(w[t - 2], 17) ^ RightRotate(w[t - 2], 19) ^
+                  (w[t - 2] >> 10);
+    w[t] = w[t - 16] + s0 + w[t - 7] + s1;
+  }
+
+  uint32_t a = state_[0];
+  uint32_t b = state_[1];
+  uint32_t c = state_[2];
+  uint32_t d = state_[3];
+  uint32_t e = state_[4];
+  uint32_t f = state_[5];
+  uint32_t g = state_[6];
+  uint32_t h = state_[7];
+
+  for (int i = 0; i < 64; i++) {
+    uint32_t sigma0 =
+        RightRotate(a, 2) ^ RightRotate(a, 13) ^ RightRotate(a, 22);
+    uint32_t majority = (a & b) ^ (a & c) ^ (b & c);
+    uint32_t temp2 = sigma0 + majority;
+    uint32_t sigma1 =
+        RightRotate(e, 6) ^ RightRotate(e, 11) ^ RightRotate(e, 25);
+    uint32_t choice = (e & f) ^ ((~e) & g);
+    uint32_t temp1 = h + sigma1 + choice + k[i] + w[i];
+
+    h = g;
+    g = f;
+    f = e;
+    e = d + temp1;
+    d = c;
+    c = b;
+    b = a;
+    a = temp1 + temp2;
+  }
+
+  state_[0] += a;
+  state_[1] += b;
+  state_[2] += c;
+  state_[3] += d;
+  state_[4] += e;
+  state_[5] += f;
+  state_[6] += g;
+  state_[7] += h;
+}
+
+void Sha256::Update(const uint8_t* data, size_t length) {
+  int i = static_cast<int>(count_ & 0b111111);
+  count_ += length;
+  while (length--) {
+    buffer_[i] = *data;
+    ++data;
+    ++i;
+    if (i == 64) {
+      Transform();
+      i = 0;
+    }
+  }
+}
+
+std::array<uint8_t, 32> Sha256::Finalize() && {
+  uint64_t bits_count = count_ << 3;
+
+  // SHA-256 padding: the message is padded with a '1' bit, then with '0' bits
+  // until the message length modulo 512 is 448 bits (56 bytes modulo 64).
+  Update(&kPaddingFirstByte, 1);
+
+  // Pad with '0' bits until the message length modulo 64 is 56 bytes, leaving
+  // 8 bytes for the length of the original message.
+  while (count_ % 64 != 56) {
+    Update(&kNullChar, 1);
+  }
+
+  // Append the length of the original message (in bits) to the end of the
+  // padded message in big-endian order.
+  for (int i = 0; i < 8; ++i) {
+    uint8_t tmp = static_cast<uint8_t>(bits_count >> 56);
+    bits_count <<= 8;
+    Update(&tmp, 1);
+  }
+
+  // Convert the state array to a 32-byte sha256 hash array in big-endian order.
+  std::array<uint8_t, 32> hash;
+  for (int i = 0, j = 0; i < 8; i++) {
+    uint32_t tmp = state_[i];
+    hash[j++] = static_cast<uint8_t>(tmp >> 24);
+    hash[j++] = static_cast<uint8_t>(tmp >> 16);
+    hash[j++] = static_cast<uint8_t>(tmp >> 8);
+    hash[j++] = static_cast<uint8_t>(tmp);
+  }
+
+  return hash;
+}
+
+}  // namespace lib
+}  // namespace icing
diff --git a/icing/util/sha256.h b/icing/util/sha256.h
new file mode 100644
index 0000000..62113e4
--- /dev/null
+++ b/icing/util/sha256.h
@@ -0,0 +1,52 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef ICING_UTIL_SHA256_H_
+#define ICING_UTIL_SHA256_H_
+
+#include <array>
+#include <cstddef>
+#include <cstdint>
+
+namespace icing {
+namespace lib {
+
+class Sha256 {
+ public:
+  Sha256();
+  
+  // Update the SHA256 context with additional data
+  void Update(const uint8_t* data, size_t length);
+
+  // Finalize the SHA256 computation and obtain the 32-byte hash.
+  std::array<uint8_t, 32> Finalize() &&;
+
+ private:
+  // Array to hold the current hash state
+  uint32_t state_[8];
+
+  // Total number of bytes processed
+  uint64_t count_;
+
+  // The 64-byte buffer to store the input data, sha-256 block size is 64 bytes.
+  std::array<uint8_t, 64> buffer_;
+
+  // Processes a block of input data and updates the hash state.
+  void Transform();
+};
+
+}  // namespace lib
+}  // namespace icing
+
+#endif  // ICING_UTIL_SHA256_H_
diff --git a/java/src/com/google/android/icing/IcingSearchEngine.java b/java/src/com/google/android/icing/IcingSearchEngine.java
index a8a571e..c8d4516 100644
--- a/java/src/com/google/android/icing/IcingSearchEngine.java
+++ b/java/src/com/google/android/icing/IcingSearchEngine.java
@@ -16,6 +16,7 @@ package com.google.android.icing;
 
 import androidx.annotation.NonNull;
 import androidx.annotation.Nullable;
+import com.google.android.icing.proto.BlobProto;
 import com.google.android.icing.proto.DebugInfoResultProto;
 import com.google.android.icing.proto.DebugInfoVerbosity;
 import com.google.android.icing.proto.DeleteByNamespaceResultProto;
@@ -35,6 +36,7 @@ import com.google.android.icing.proto.LogSeverity;
 import com.google.android.icing.proto.OptimizeResultProto;
 import com.google.android.icing.proto.PersistToDiskResultProto;
 import com.google.android.icing.proto.PersistType;
+import com.google.android.icing.proto.PropertyProto;
 import com.google.android.icing.proto.PutResultProto;
 import com.google.android.icing.proto.ReportUsageResultProto;
 import com.google.android.icing.proto.ResetResultProto;
@@ -165,6 +167,27 @@ public class IcingSearchEngine implements IcingSearchEngineInterface {
     icingSearchEngineImpl.invalidateNextPageToken(nextPageToken);
   }
 
+  @NonNull
+  @Override
+  public BlobProto openWriteBlob(PropertyProto.BlobHandleProto blobHandle) {
+    return IcingSearchEngineUtils.byteArrayToBlobProto(
+        icingSearchEngineImpl.openWriteBlob(blobHandle.toByteArray()));
+  }
+
+  @NonNull
+  @Override
+  public BlobProto openReadBlob(PropertyProto.BlobHandleProto blobHandle) {
+    return IcingSearchEngineUtils.byteArrayToBlobProto(
+        icingSearchEngineImpl.openReadBlob(blobHandle.toByteArray()));
+  }
+
+  @NonNull
+  @Override
+  public BlobProto commitBlob(PropertyProto.BlobHandleProto blobHandle) {
+    return IcingSearchEngineUtils.byteArrayToBlobProto(
+        icingSearchEngineImpl.commitBlob(blobHandle.toByteArray()));
+  }
+
   @NonNull
   @Override
   public DeleteResultProto delete(@NonNull String namespace, @NonNull String uri) {
diff --git a/java/src/com/google/android/icing/IcingSearchEngineImpl.java b/java/src/com/google/android/icing/IcingSearchEngineImpl.java
index 1e8f8b5..2a76d92 100644
--- a/java/src/com/google/android/icing/IcingSearchEngineImpl.java
+++ b/java/src/com/google/android/icing/IcingSearchEngineImpl.java
@@ -156,6 +156,24 @@ public class IcingSearchEngineImpl implements Closeable {
     nativeInvalidateNextPageToken(this, nextPageToken);
   }
 
+  @NonNull
+  public byte[] openWriteBlob(@NonNull byte[] blobHandleBytes) {
+    throwIfClosed();
+    return nativeOpenWriteBlob(this, blobHandleBytes);
+  }
+
+  @NonNull
+  public byte[] openReadBlob(@NonNull byte[] blobHandleBytes) {
+    throwIfClosed();
+    return nativeOpenReadBlob(this, blobHandleBytes);
+  }
+
+  @NonNull
+  public byte[] commitBlob(@NonNull byte[] blobHandleBytes) {
+    throwIfClosed();
+    return nativeCommitBlob(this, blobHandleBytes);
+  }
+
   @Nullable
   public byte[] delete(@NonNull String namespace, @NonNull String uri) {
     throwIfClosed();
@@ -289,6 +307,15 @@ public class IcingSearchEngineImpl implements Closeable {
   private static native void nativeInvalidateNextPageToken(
       IcingSearchEngineImpl instance, long nextPageToken);
 
+  private static native byte[] nativeOpenWriteBlob(
+      IcingSearchEngineImpl instance, byte[] blobHandleBytes);
+
+  private static native byte[] nativeOpenReadBlob(
+      IcingSearchEngineImpl instance, byte[] blobHandleBytes);
+
+  private static native byte[] nativeCommitBlob(
+      IcingSearchEngineImpl instance, byte[] blobHandleBytes);
+
   private static native byte[] nativeDelete(
       IcingSearchEngineImpl instance, String namespace, String uri);
 
diff --git a/java/src/com/google/android/icing/IcingSearchEngineInterface.java b/java/src/com/google/android/icing/IcingSearchEngineInterface.java
index 67f60ed..65b8085 100644
--- a/java/src/com/google/android/icing/IcingSearchEngineInterface.java
+++ b/java/src/com/google/android/icing/IcingSearchEngineInterface.java
@@ -1,5 +1,6 @@
 package com.google.android.icing;
 
+import com.google.android.icing.proto.BlobProto;
 import com.google.android.icing.proto.DebugInfoResultProto;
 import com.google.android.icing.proto.DebugInfoVerbosity;
 import com.google.android.icing.proto.DeleteByNamespaceResultProto;
@@ -17,6 +18,7 @@ import com.google.android.icing.proto.InitializeResultProto;
 import com.google.android.icing.proto.OptimizeResultProto;
 import com.google.android.icing.proto.PersistToDiskResultProto;
 import com.google.android.icing.proto.PersistType;
+import com.google.android.icing.proto.PropertyProto;
 import com.google.android.icing.proto.PutResultProto;
 import com.google.android.icing.proto.ReportUsageResultProto;
 import com.google.android.icing.proto.ResetResultProto;
@@ -95,6 +97,15 @@ public interface IcingSearchEngineInterface extends Closeable {
   /** Invalidates the next page token. */
   void invalidateNextPageToken(long nextPageToken);
 
+  /** Gets a file descriptor to write blob data. */
+  BlobProto openWriteBlob(PropertyProto.BlobHandleProto blobHandle);
+
+  /** Gets a file descriptor to read blob data. */
+  BlobProto openReadBlob(PropertyProto.BlobHandleProto blobHandle);
+
+  /** Marks the blob is committed. */
+  BlobProto commitBlob(PropertyProto.BlobHandleProto blobHandle);
+
   /**
    * Deletes the document.
    *
diff --git a/java/src/com/google/android/icing/IcingSearchEngineUtils.java b/java/src/com/google/android/icing/IcingSearchEngineUtils.java
index 0913216..13ded3a 100644
--- a/java/src/com/google/android/icing/IcingSearchEngineUtils.java
+++ b/java/src/com/google/android/icing/IcingSearchEngineUtils.java
@@ -17,6 +17,7 @@ package com.google.android.icing;
 import android.util.Log;
 import androidx.annotation.NonNull;
 import androidx.annotation.Nullable;
+import com.google.android.icing.proto.BlobProto;
 import com.google.android.icing.proto.DebugInfoResultProto;
 import com.google.android.icing.proto.DeleteByNamespaceResultProto;
 import com.google.android.icing.proto.DeleteByQueryResultProto;
@@ -48,6 +49,7 @@ import com.google.protobuf.InvalidProtocolBufferException;
  * <p>It is also being used by AppSearch dynamite 0p client APIs to convert byte arrays to the
  * protos.
  */
+// TODO(b/347054358): Add unit tests for this class.
 public final class IcingSearchEngineUtils {
   private static final String TAG = "IcingSearchEngineUtils";
   private static final ExtensionRegistryLite EXTENSION_REGISTRY_LITE =
@@ -237,6 +239,31 @@ public final class IcingSearchEngineUtils {
     }
   }
 
+  /**
+   * Converts a byte array to a {@link BlobProto}.
+   *
+   * @param blobBytes the byte array to convert
+   * @return the {@link BlobProto}
+   */
+  @NonNull
+  public static BlobProto byteArrayToBlobProto(@Nullable byte[] blobBytes) {
+    if (blobBytes == null) {
+      Log.e(TAG, "Received null BlobProto from native.");
+      return BlobProto.newBuilder()
+          .setStatus(StatusProto.newBuilder().setCode(StatusProto.Code.INTERNAL))
+          .build();
+    }
+
+    try {
+      return BlobProto.newBuilder().mergeFrom(blobBytes, EXTENSION_REGISTRY_LITE).build();
+    } catch (InvalidProtocolBufferException e) {
+      Log.e(TAG, "Error parsing BlobProto.", e);
+      return BlobProto.newBuilder()
+          .setStatus(StatusProto.newBuilder().setCode(StatusProto.Code.INTERNAL))
+          .build();
+    }
+  }
+
   private static void setNativeToJavaJniLatency(
       SearchResultProto.Builder searchResultProtoBuilder) {
     int nativeToJavaLatencyMs =
diff --git a/java/tests/instrumentation/src/com/google/android/icing/IcingSearchEngineTest.java b/java/tests/instrumentation/src/com/google/android/icing/IcingSearchEngineTest.java
index 2bbd621..e7b114f 100644
--- a/java/tests/instrumentation/src/com/google/android/icing/IcingSearchEngineTest.java
+++ b/java/tests/instrumentation/src/com/google/android/icing/IcingSearchEngineTest.java
@@ -18,6 +18,7 @@ import static com.google.common.truth.Truth.assertThat;
 import static com.google.common.truth.Truth.assertWithMessage;
 
 import com.google.android.icing.IcingSearchEngine;
+import com.google.android.icing.proto.BlobProto;
 import com.google.android.icing.proto.DebugInfoResultProto;
 import com.google.android.icing.proto.DebugInfoVerbosity;
 import com.google.android.icing.proto.DeleteByNamespaceResultProto;
@@ -61,11 +62,20 @@ import com.google.android.icing.proto.SuggestionSpecProto;
 import com.google.android.icing.proto.TermMatchType;
 import com.google.android.icing.proto.TermMatchType.Code;
 import com.google.android.icing.proto.UsageReport;
+import com.google.protobuf.ByteString;
 import java.io.File;
+import java.io.FileDescriptor;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.lang.reflect.Field;
+import java.security.MessageDigest;
+import java.security.NoSuchAlgorithmException;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.Random;
 import org.junit.After;
 import org.junit.Before;
+import org.junit.Ignore;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
@@ -119,6 +129,21 @@ public final class IcingSearchEngineTest {
         .build();
   }
 
+  /** Generate an array contains random bytes for the given length. */
+  private static byte[] generateRandomBytes(int length) {
+    byte[] bytes = new byte[length];
+    Random rd = new Random(); // creating Random object
+    rd.nextBytes(bytes);
+    return bytes;
+  }
+
+  /** Calculate the sha-256 digest for the given data. */
+  private static byte[] calculateDigest(byte[] data) throws NoSuchAlgorithmException {
+    MessageDigest messageDigest = MessageDigest.getInstance("SHA-256");
+    messageDigest.update(data);
+    return messageDigest.digest();
+  }
+
   @Before
   public void setUp() throws Exception {
     tempDir = temporaryFolder.newFolder();
@@ -271,6 +296,61 @@ public final class IcingSearchEngineTest {
     assertThat(searchResultProto.getResultsCount()).isEqualTo(0);
   }
 
+  @Ignore // b/350530146
+  @Test
+  public void writeAndReadBlob_blobContentMatches() throws Exception {
+    // 1 Arrange: set up IcingSearchEngine with and blob data
+    File tempDir = temporaryFolder.newFolder();
+    IcingSearchEngineOptions options =
+        IcingSearchEngineOptions.newBuilder()
+            .setBaseDir(tempDir.getCanonicalPath())
+            .setEnableBlobStore(true)
+            .build();
+    IcingSearchEngine icing = new IcingSearchEngine(options);
+    assertStatusOk(icing.initialize().getStatus());
+
+    byte[] data = generateRandomBytes(100); // 10 Bytes
+    byte[] digest = calculateDigest(data);
+    PropertyProto.BlobHandleProto blobHandle =
+        PropertyProto.BlobHandleProto.newBuilder()
+            .setLabel("label")
+            .setDigest(ByteString.copyFrom(digest))
+            .build();
+
+    // 2 Act: write the blob and read it back.
+    BlobProto openWriteBlobProto = icing.openWriteBlob(blobHandle);
+    assertStatusOk(openWriteBlobProto.getStatus());
+    Field field = FileDescriptor.class.getDeclaredField("fd");
+    field.setAccessible(true); // Make the field accessible
+
+    // Create a new FileDescriptor object
+    FileDescriptor writeFd = new FileDescriptor();
+
+    // Set the file descriptor value using reflection
+    field.setInt(writeFd, openWriteBlobProto.getFileDescriptor());
+
+    try (FileOutputStream outputStream = new FileOutputStream(writeFd)) {
+      outputStream.write(data);
+    }
+
+    // Commit and read the blob.
+    BlobProto commitBlobProto = icing.commitBlob(blobHandle);
+    assertStatusOk(commitBlobProto.getStatus());
+
+    BlobProto openReadBlobProto = icing.openReadBlob(blobHandle);
+    assertStatusOk(openReadBlobProto.getStatus());
+
+    FileDescriptor readFd = new FileDescriptor();
+    field.setInt(readFd, openReadBlobProto.getFileDescriptor());
+    byte[] output = new byte[data.length];
+    try (FileInputStream inputStream = new FileInputStream(readFd)) {
+      inputStream.read(output);
+    }
+
+    // 3 Assert: the blob content matches.
+    assertThat(output).isEqualTo(data);
+  }
+
   @Test
   public void testDelete() throws Exception {
     assertStatusOk(icingSearchEngine.initialize().getStatus());
diff --git a/proto/icing/proto/blob.proto b/proto/icing/proto/blob.proto
new file mode 100644
index 0000000..f2cfe0d
--- /dev/null
+++ b/proto/icing/proto/blob.proto
@@ -0,0 +1,40 @@
+// Copyright (C) 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+syntax = "proto2";
+
+package icing.lib;
+
+import "icing/proto/status.proto";
+
+option java_package = "com.google.android.icing.proto";
+option java_multiple_files = true;
+option objc_class_prefix = "ICNG";
+
+// Defines the blob operation result proto that user try to read/write/commit a
+// blob to Icing.
+//
+// Next tag: 3
+message BlobProto {
+  // Status code can be one of:
+  //   OK
+  //   FAILED_PRECONDITION
+  //
+  // See status.proto for more details.
+  //
+  optional StatusProto status = 1;
+
+  // The file decriptor of the blob file from Icing.
+  optional int32 file_descriptor = 2;
+}
diff --git a/proto/icing/proto/document.proto b/proto/icing/proto/document.proto
index 919769e..0fee0c0 100644
--- a/proto/icing/proto/document.proto
+++ b/proto/icing/proto/document.proto
@@ -78,7 +78,7 @@ message DocumentProto {
 }
 
 // Holds a property field of the Document.
-// Next tag: 9
+// Next tag: 10
 message PropertyProto {
   // Name of the property.
   // See icing.lib.PropertyConfigProto.property_name for details.
@@ -103,10 +103,20 @@ message PropertyProto {
     optional string model_signature = 2;
   }
   repeated VectorProto vector_values = 8;
+
+  // Holds a blob handle field of the property.
+  // Next tag: 3
+  message BlobHandleProto {
+    // The sha-256 hashed digest for a blob.
+    optional bytes digest = 1;
+    // The label of the blob.
+    optional string label = 2;
+  }
+  repeated BlobHandleProto blob_handle_values = 9;
 }
 
 // Result of a call to IcingSearchEngine.Put
-// Next tag: 3
+// Next tag: 4
 message PutResultProto {
   // Status code can be one of:
   //   OK
@@ -126,6 +136,9 @@ message PutResultProto {
   // be accurate only when the status above is OK. See logging.proto for
   // details.
   optional PutDocumentStatsProto put_document_stats = 2;
+
+  // Whether or not the document was a replacement of an existing document.
+  optional bool was_replacement = 3;
 }
 
 // Result of a call to IcingSearchEngine.Get
diff --git a/proto/icing/proto/initialize.proto b/proto/icing/proto/initialize.proto
index 6bc5aca..102a59d 100644
--- a/proto/icing/proto/initialize.proto
+++ b/proto/icing/proto/initialize.proto
@@ -73,7 +73,7 @@ message IcingSearchEngineVersionProto {
   repeated IcingSearchEngineFeatureInfoProto enabled_features = 3;
 }
 
-// Next tag: 16
+// Next tag: 18
 message IcingSearchEngineOptions {
   // Directory to persist files for Icing. Required.
   // If Icing was previously initialized with this directory, it will reload
@@ -180,6 +180,15 @@ message IcingSearchEngineOptions {
   // is required to support the hasProperty function in advanced query.
   optional bool build_property_existence_metadata_hits = 15;
 
+  // Whether to enable blob store.
+  // If set to true, the BlobStore will be created to store and retrieve blobs.
+  optional bool enable_blob_store = 16;
+
+  // The time to live for an orphan blob. If the blob has no reference document
+  // for this amount of time, it will be deleted.
+  // If set to 0, the blob will never be deleted.
+  optional int64 orphan_blob_time_to_live_ms = 17;
+
   reserved 2;
 }
 
diff --git a/proto/icing/proto/internal/scorable_property_set.proto b/proto/icing/proto/internal/scorable_property_set.proto
new file mode 100644
index 0000000..13d1101
--- /dev/null
+++ b/proto/icing/proto/internal/scorable_property_set.proto
@@ -0,0 +1,74 @@
+// Copyright 2024 Google LLC
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+syntax = "proto2";
+
+package icing.lib;
+
+option java_package = "com.google.android.icing.internal.proto";
+option java_multiple_files = true;
+option objc_class_prefix = "ICNG";
+
+// Message that contains the data payload of a scorable property.
+//
+// Next tag: 4
+message ScorablePropertyProto {
+  // The following three fields should be treated as oneof.
+  repeated int64 int64_values = 1 [packed = true];
+  repeated double double_values = 2 [packed = true];
+  repeated bool boolean_values = 3 [packed = true];
+}
+
+// Message that contains a set of scorable properties derived from a document
+// proto. This will be stored in the icing document store to allow faster
+// access of scorable properties during the scoring/ranking phase.
+//
+// TODO(b/357105837): Always use the util class(link TBD after cl/660373640 is
+// submitted) to interpret and populate this proto.
+//
+// See go/appsearch-docjoin-dd for the design doc.
+//
+// Next tag: 2
+message ScorablePropertySetProto {
+  // The order of the properties in this proto should match the order of the
+  // scorable properties in the schema proto.
+  //
+  // For example, if you have a schema like:
+  //   SchemaTypeConfigBuilder()
+  //    .setType('email')
+  //    .AddProperty('subject')    // not scorable
+  //    .AddProperty('importance') // scorable
+  //    .AddProperty('title')      // not scorable
+  //    .AddProperty('score')      // scorable
+  //
+  // And an email document like:
+  //   Document email_document =
+  //     DocumentBuilder()
+  //       .AddStringProperty('subject', 'foo')
+  //       .AddDoubleProperty('score', 1.5, 2.5, 3.5)
+  //       .Build();
+  //
+  // When email_document is put into the icing document store, the following
+  // proto will be generated and stored in icing:
+  // {
+  //   properties: {} // empty entry for the 'importance' property
+  //   properties: {
+  //     double_values: [1.5, 2.5, 3.5] // The 'score' property
+  //   }
+  // }
+  //
+  // During the scoring/ranking phase, the implicit property order can be used
+  // to lookup the property's name from the schema store.
+  repeated ScorablePropertyProto properties = 1;
+}
diff --git a/proto/icing/proto/persist.proto b/proto/icing/proto/persist.proto
index 8d6b372..802195d 100644
--- a/proto/icing/proto/persist.proto
+++ b/proto/icing/proto/persist.proto
@@ -30,8 +30,10 @@ message PersistType {
     UNKNOWN = 0;
 
     // Only persist the ground truth. A successful PersistToDisk(LITE) should
-    // ensure that no data is lost the next time Icing initializes. This
-    // should be called after each batch of mutations.
+    // ensure that no data is lost the next time Icing initializes. However, a
+    // recovery will be required unless a subsequent call to
+    // PersistToDisk(RECOVERY_PROOF) or PersistToDisk(FULL) is made before
+    // shutdown. This should be called after each batch of mutations.
     LITE = 1;
 
     // Persists all data in internal Icing components. A successful
@@ -40,6 +42,16 @@ message PersistType {
     // structures the next time Icing initializes. This should be called at
     // some point before the app terminates.
     FULL = 2;
+
+    // Persists the ground truth and updates the checksums of all internal Icing
+    // components. A successful PersistToDisk(RECOVERY_PROOF) should ensure that
+    // no data is lost and make it unlikely that a recovery is needed the next
+    // time Icing initializes. However, it is not guaranteed that a recovery
+    // will not be needed. This is because changes to index components may or
+    // may not be flushed to disk in the background and if some changes aren't
+    // persisted then we will have a checksum mismatch and a recovery. To
+    // guarantee that recovery won't occur PersistToDisk(FULL) must be called.
+    RECOVERY_PROOF = 3;
   }
   optional Code code = 1;
 }
diff --git a/proto/icing/proto/schema.proto b/proto/icing/proto/schema.proto
index 99439bb..e547c4e 100644
--- a/proto/icing/proto/schema.proto
+++ b/proto/icing/proto/schema.proto
@@ -94,6 +94,14 @@ message StringIndexingConfig {
   // Content in this property should be returned for queries that are either
   // exact matches or query matches of the tokens appearing in this property.
   // Ex. A property with "fool" *should* match a query for "foo".
+  //
+  // TermMatchType.Code=STEMMING
+  // Content in this property should also be returned for queries that are stems
+  // of the tokens appearing in this property.
+  // Ex. A property with "running" *should* match a query for "run".
+  //
+  // TODO: b/344915547 - Refactor this to be a repeated field so that clients
+  // can choose multiple fuzzy match types.
   optional TermMatchType.Code term_match_type = 1;
 
   message TokenizerType {
@@ -235,7 +243,7 @@ message JoinableConfig {
 // Describes the schema of a single property of Documents that belong to a
 // specific SchemaTypeConfigProto. These can be considered as a rich, structured
 // type for each property of Documents accepted by IcingSearchEngine.
-// Next tag: 11
+// Next tag: 12
 message PropertyConfigProto {
   // REQUIRED: Name that uniquely identifies a property within an Document of
   // a specific SchemaTypeConfigProto.
@@ -275,6 +283,9 @@ message PropertyConfigProto {
 
       // A list of floats. Vector type is used for embedding searches.
       VECTOR = 7;
+
+      // A handle to uniquely identify a large blob of data.
+      BLOB_HANDLE = 8;
     }
   }
   optional DataType.Code data_type = 2;
@@ -340,6 +351,27 @@ message PropertyConfigProto {
   // OPTIONAL: Describes how vector properties should be indexed. Vector
   // properties that do not set the indexing config will not be indexed.
   optional EmbeddingIndexingConfig embedding_indexing_config = 10;
+
+  // OPTIONAL: Describes how a property can be used to for scoring.
+  //
+  // The ScorableType should only be enabled for the following data types:
+  // - INT64
+  // - DOUBLE
+  // - BOOLEAN
+  message ScorableType {
+    enum Code {
+      // This value should not be used on purpose.
+      // It will be treated as DISABLED in icing.
+      UNKNOWN = 0;
+
+      // Property is disabled for scoring.
+      DISABLED = 1;
+
+      // Property is enabled for scoring.
+      ENABLED = 2;
+    }
+  }
+  optional ScorableType.Code scorable_type = 11;
 }
 
 // List of all supported types constitutes the schema used by Icing.
diff --git a/proto/icing/proto/search.proto b/proto/icing/proto/search.proto
index 3b3a955..6271ecd 100644
--- a/proto/icing/proto/search.proto
+++ b/proto/icing/proto/search.proto
@@ -27,7 +27,7 @@ option java_multiple_files = true;
 option objc_class_prefix = "ICNG";
 
 // Client-supplied specifications on what documents to retrieve.
-// Next tag: 13
+// Next tag: 14
 message SearchSpecProto {
   // REQUIRED: The "raw" query string that users may type. For example, "cat"
   // will search for documents with the term cat in it.
@@ -49,6 +49,15 @@ message SearchSpecProto {
   // the token.
   // Ex. A query term "foo" will match indexed tokens like "foo", "foot", and
   // "football".
+  //
+  // TermMatchType.Code=STEMMING
+  // Query terms will match indexed tokens when the query term is a stem of
+  // the token, or relates to the token via the same stem.
+  // Ex. A query term "dance" will match indexed tokens like "dance", "dances",
+  // and "dancing".
+  //
+  // TODO: b/344915547 - Refactor this to be a repeated field so that clients
+  // can choose multiple fuzzy match types.
   optional TermMatchType.Code term_match_type = 2;
 
   // OPTIONAL: Only search for documents that have the specified namespaces. If
@@ -109,6 +118,10 @@ message SearchSpecProto {
   // queries.
   optional EmbeddingQueryMetricType.Code embedding_query_metric_type = 12;
 
+  // Strings to be used as parameters in the query. The strings will be treated
+  // as pure TEXT and will be normalized and tokenized.
+  repeated string query_parameter_strings = 13;
+
   reserved 6;
 }
 
@@ -415,7 +428,7 @@ message GetResultSpecProto {
   repeated TypePropertyMask type_property_masks = 1;
 }
 
-// Next tag: 8
+// Next tag: 12
 message SuggestionSpecProto {
   // REQUIRED: The "raw" prefix string that users may type. For example, "f"
   // will search for suggested query that start with "f" like "foo", "fool".
@@ -466,6 +479,21 @@ message SuggestionSpecProto {
   // suggestions for different types, separate RunSuggestion()'s will need to be
   // made.
   repeated TypePropertyMask type_property_filters = 7;
+
+  // The vectors to be used in embedding queries.
+  repeated PropertyProto.VectorProto embedding_query_vectors = 8;
+
+  // The default metric type used to calculate the scores for embedding
+  // queries.
+  optional SearchSpecProto.EmbeddingQueryMetricType.Code
+      embedding_query_metric_type = 9;
+
+  // Strings to be used as parameters in the query. The strings will be treated
+  // as pure TEXT and will be normalized and tokenized.
+  repeated string query_parameter_strings = 10;
+
+  // Features enabled in this suggestion spec.
+  repeated string enabled_features = 11;
 }
 
 // A group that holds namespace and document_uris under it.
diff --git a/proto/icing/proto/term.proto b/proto/icing/proto/term.proto
index adf2ad6..322d81c 100644
--- a/proto/icing/proto/term.proto
+++ b/proto/icing/proto/term.proto
@@ -18,13 +18,12 @@ package icing.lib;
 
 option java_package = "com.google.android.icing.proto";
 option java_multiple_files = true;
-
 option objc_class_prefix = "ICNG";
 
 // Encapsulates the configurations on how Icing should query/index these terms.
 // Next tag: 0
 message TermMatchType {
-  // Next tag: 3
+  // Next tag: 4
   enum Code {
     // A default for all other use-cases.
     UNKNOWN = 0;
@@ -36,5 +35,9 @@ message TermMatchType {
     // A term is a prefix of other terms.
     // Ex. "foo" is a prefix of "foot"
     PREFIX = 2;
+
+    // A term is a stem of other terms.
+    // Ex. "run" is a stem for "running" and "runs"
+    STEMMING = 3;
   }
 }
diff --git a/proto/visibility.proto b/proto/icing/proto/visibility.proto
similarity index 75%
rename from proto/visibility.proto
rename to proto/icing/proto/visibility.proto
index b54e3db..df81992 100644
--- a/proto/visibility.proto
+++ b/proto/icing/proto/visibility.proto
@@ -23,31 +23,34 @@ option objc_class_prefix = "APSC";
 // Defines proto contains visibility overlay settings.
 // Next tag: 3
 message AndroidVOverlayProto {
-  // OPTIONAL: the standard visibility settings. Caller could have access if they match ANY of the
-  // requirements in this VisibilityConfig.
+  // OPTIONAL: the standard visibility settings. Caller could have access if
+  // they match ANY of the requirements in this VisibilityConfig.
   optional VisibilityConfigProto visibility_config = 1;
 
-  // OPTIONAL: The set of VisibilityConfig that have access. Caller could have access only if they
-  // match ALL of the requirements in a VisibilityConfig.
+  // OPTIONAL: The set of VisibilityConfig that have access. Caller could have
+  // access only if they match ALL of the requirements in a VisibilityConfig.
   repeated VisibilityConfigProto visible_to_configs = 2;
 }
 
-// Defines proto contains all required requirements of a nested VisibilityConfig.
-// The requirements in a VisibleToConfigProto is "AND" relationship. A caller must match ALL
-// requirements to access the schema. For example, a caller must hold required permissions AND it is
-// a specified package.
-// Next tag: 5
+// Defines proto contains all required requirements of a nested
+// VisibilityConfig. The requirements in a VisibleToConfigProto is "AND"
+// relationship. A caller must match ALL requirements to access the schema. For
+// example, a caller must hold required permissions AND it is a specified
+// package. Next tag: 5
 message VisibilityConfigProto {
   // OPTIONAL: whether this schema could be visible to the platform.
   optional bool not_platform_surfaceable = 1;
 
-  // OPTIONAL: the package identifiers of the while list packages that have access to this schema.
+  // OPTIONAL: the package identifiers of the while list packages that have
+  // access to this schema.
   repeated PackageIdentifierProto visible_to_packages = 2;
 
-  // OPTIONAL: the required permission combinations that the caller need to hold.
+  // OPTIONAL: the required permission combinations that the caller need to
+  // hold.
   repeated VisibleToPermissionProto visible_to_permissions = 3;
 
-  // OPTIONAL: the package identifier of the target package for public visibility.
+  // OPTIONAL: the package identifier of the target package for public
+  // visibility.
   optional PackageIdentifierProto publicly_visible_target_package = 4;
 }
 
@@ -61,9 +64,10 @@ message PackageIdentifierProto {
   optional bytes package_sha256cert = 2;
 }
 
-// Defines a proto to represent a set of required Android permission combination.
-// Next tag: 2
+// Defines a proto to represent a set of required Android permission
+// combination. Next tag: 2
 message VisibleToPermissionProto {
-  // REQUIRED: the required permissions that the caller need to hold to get access.
-  repeated uint32 permissions = 1;
-}
\ No newline at end of file
+  // REQUIRED: the required permissions that the caller need to hold to get
+  // access.
+  repeated uint32 permissions = 1 [packed = true];
+}
diff --git a/synced_AOSP_CL_number.txt b/synced_AOSP_CL_number.txt
index c5bf8c9..5b9c321 100644
--- a/synced_AOSP_CL_number.txt
+++ b/synced_AOSP_CL_number.txt
@@ -1 +1 @@
-set(synced_AOSP_CL_number=625823934)
+set(synced_AOSP_CL_number=669068792)
```

