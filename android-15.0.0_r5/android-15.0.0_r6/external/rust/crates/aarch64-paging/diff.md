```diff
diff --git a/.cargo_vcs_info.json b/.cargo_vcs_info.json
index a1fe0b1..213c8a6 100644
--- a/.cargo_vcs_info.json
+++ b/.cargo_vcs_info.json
@@ -1,6 +1,6 @@
 {
   "git": {
-    "sha1": "f58c903c1fe461a2b06944b8bc5d71653ce2fb02"
+    "sha1": "ddae44c5342da18b54cf07f67218cd1e2765546f"
   },
   "path_in_vcs": ""
 }
\ No newline at end of file
diff --git a/Android.bp b/Android.bp
index 7d08917..4bf606a 100644
--- a/Android.bp
+++ b/Android.bp
@@ -1,45 +1,15 @@
 // This file is generated by cargo_embargo.
-// Do not modify this file after the first "rust_*" or "genrule" module
-// because the changes will be overridden on upgrade.
-// Content before the first "rust_*" or "genrule" module is preserved.
+// Do not modify this file because the changes will be overridden on upgrade.
 
 package {
-    default_applicable_licenses: [
-        "external_rust_crates_aarch64-paging_license",
-    ],
+    default_applicable_licenses: ["external_rust_crates_aarch64-paging_license"],
 }
 
-// Added automatically by a large-scale-change that took the approach of
-// 'apply every license found to every target'. While this makes sure we respect
-// every license restriction, it may not be entirely correct.
-//
-// e.g. GPL in an MIT project might only apply to the contrib/ directory.
-//
-// Please consider splitting the single license below into multiple licenses,
-// taking care not to lose any license_kind information, and overriding the
-// default license using the 'licenses: [...]' property on targets as needed.
-//
-// For unused files, consider creating a 'fileGroup' with "//visibility:private"
-// to attach the license to, and including a comment whether the files may be
-// used in the current project.
-//
-// large-scale-change included anything that looked like it might be a license
-// text as a license_text. e.g. LICENSE, NOTICE, COPYING etc.
-//
-// Please consider removing redundant or irrelevant files from 'license_text:'.
-// See: http://go/android-license-faq
 license {
     name: "external_rust_crates_aarch64-paging_license",
     visibility: [":__subpackages__"],
-    license_kinds: [
-        "SPDX-license-identifier-Apache-2.0",
-        "SPDX-license-identifier-MIT",
-    ],
-    license_text: [
-        "LICENSE",
-        "LICENSE-APACHE",
-        "LICENSE-MIT",
-    ],
+    license_kinds: ["SPDX-license-identifier-Apache-2.0"],
+    license_text: ["LICENSE.android"],
 }
 
 rust_test {
@@ -47,7 +17,7 @@ rust_test {
     host_supported: true,
     crate_name: "aarch64_paging",
     cargo_env_compat: true,
-    cargo_pkg_version: "0.5.0",
+    cargo_pkg_version: "0.7.0",
     crate_root: "src/lib.rs",
     test_suites: ["general-tests"],
     auto_gen_config: true,
@@ -73,7 +43,7 @@ rust_library_rlib {
     host_supported: true,
     crate_name: "aarch64_paging",
     cargo_env_compat: true,
-    cargo_pkg_version: "0.5.0",
+    cargo_pkg_version: "0.7.0",
     crate_root: "src/lib.rs",
     edition: "2021",
     features: [
diff --git a/CHANGELOG.md b/CHANGELOG.md
index e41ca1e..da981ba 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -1,5 +1,46 @@
 # Changelog
 
+## 0.7.0
+
+### Breaking changes
+
+- `Translation::allocate_table` and `Translation::deallocate_table` now takes `&mut self` rather
+  than `&self.
+
+### Other changes
+
+- The `Translation` type parameter to `Mapping` no longer needs to be `Clone`.
+- `IdMap`, `LinearMap`, `Mapping` and `RootTable` are now `Sync`.
+
+## 0.6.0
+
+### Breaking changes
+
+- Added support for EL2 and EL3 page tables. This requires a new parameter to `IdMap::new`,
+  `LinearMap::new`, `Mapping::new` and `RootTable::new`.
+- `Attributes::EXECUTE_NEVER` renamed to `Attributes::UXN`.
+- `Attributes::DEVICE_NGNRE` and `NORMAL` have been removed in favour of `ATTRIBUTE_INDEX_*`,
+  `OUTER_SHAREABLE` and `INNER_SHAREABLE`, to avoid making assumptions about how the MAIR registers
+  are programmed.
+
+### New features
+
+- Added `root_address`, `mark_active` and `mark_inactive` methods to `IdMap`, `LinearMap` and
+  `Mapping`. These may be used to activate and deactivate the page table manually rather than
+  calling `activate` and `deactivate`.
+- Added `NS` and `PXN` bits to `Attributes`.
+
+### Bug fixes
+
+- When an invalid descriptor is split into a table, the table descriptors aren't set unless to
+  non-zero values unless the original descriptor was.
+
+### Other changes
+
+- `Attributes::ACCESSED` is no longer automatically set on all new mappings. To maintain existing
+  behaviour you should explicitly set `Attributes::ACCESSED` whenever calling `map_range` for a
+  valid mapping.
+
 ## 0.5.0
 
 ### Bug fixes
diff --git a/Cargo.toml b/Cargo.toml
index 1a30de6..d50888f 100644
--- a/Cargo.toml
+++ b/Cargo.toml
@@ -12,12 +12,12 @@
 [package]
 edition = "2021"
 name = "aarch64-paging"
-version = "0.5.0"
+version = "0.7.0"
 authors = [
     "Ard Biesheuvel <ardb@google.com>",
     "Andrew Walbran <qwandor@google.com>",
 ]
-description = "A library to manipulate AArch64 VMSA EL1 page tables."
+description = "A library to manipulate AArch64 VMSA page tables."
 readme = "README.md"
 keywords = [
     "arm",
@@ -39,7 +39,7 @@ all-features = true
 default-target = "aarch64-unknown-none"
 
 [dependencies.bitflags]
-version = "2.0.2"
+version = "2.6.0"
 
 [features]
 alloc = []
diff --git a/Cargo.toml.orig b/Cargo.toml.orig
index 0e61775..e7bacef 100644
--- a/Cargo.toml.orig
+++ b/Cargo.toml.orig
@@ -1,9 +1,9 @@
 [package]
 name = "aarch64-paging"
-version = "0.5.0"
+version = "0.7.0"
 edition = "2021"
 license = "MIT OR Apache-2.0"
-description = "A library to manipulate AArch64 VMSA EL1 page tables."
+description = "A library to manipulate AArch64 VMSA page tables."
 authors = [
   "Ard Biesheuvel <ardb@google.com>",
   "Andrew Walbran <qwandor@google.com>",
@@ -13,7 +13,7 @@ keywords = ["arm", "aarch64", "cortex-a", "vmsa", "pagetable"]
 categories = ["embedded", "no-std", "hardware-support"]
 
 [dependencies]
-bitflags = "2.0.2"
+bitflags = "2.6.0"
 
 [features]
 default = ["alloc"]
diff --git a/LICENSE.android b/LICENSE.android
new file mode 120000
index 0000000..6b579aa
--- /dev/null
+++ b/LICENSE.android
@@ -0,0 +1 @@
+LICENSE-APACHE
\ No newline at end of file
diff --git a/METADATA b/METADATA
index 78ae9f5..6797541 100644
--- a/METADATA
+++ b/METADATA
@@ -1,23 +1,20 @@
 # This project was upgraded with external_updater.
-# Usage: tools/external_updater/updater.sh update rust/crates/aarch64-paging
+# Usage: tools/external_updater/updater.sh update external/rust/crates/aarch64-paging
 # For more info, check https://cs.android.com/android/platform/superproject/+/main:tools/external_updater/README.md
 
 name: "aarch64-paging"
-description: "A library to manipulate AArch64 VMSA EL1 page tables."
+description: "A library to manipulate AArch64 VMSA page tables."
 third_party {
-  url {
-    type: HOMEPAGE
-    value: "https://crates.io/crates/aarch64-paging"
-  }
-  url {
-    type: ARCHIVE
-    value: "https://static.crates.io/crates/aarch64-paging/aarch64-paging-0.5.0.crate"
-  }
-  version: "0.5.0"
   license_type: NOTICE
   last_upgrade_date {
-    year: 2023
-    month: 10
-    day: 25
+    year: 2024
+    month: 7
+    day: 24
+  }
+  homepage: "https://crates.io/crates/aarch64-paging"
+  identifier {
+    type: "Archive"
+    value: "https://static.crates.io/crates/aarch64-paging/aarch64-paging-0.7.0.crate"
+    version: "0.7.0"
   }
 }
diff --git a/README.md b/README.md
index 2151df2..206f315 100644
--- a/README.md
+++ b/README.md
@@ -3,9 +3,15 @@
 [![crates.io page](https://img.shields.io/crates/v/aarch64-paging.svg)](https://crates.io/crates/aarch64-paging)
 [![docs.rs page](https://docs.rs/aarch64-paging/badge.svg)](https://docs.rs/aarch64-paging)
 
-This crate provides a library to manipulate EL1 page tables conforming to the AArch64 Virtual Memory
+This crate provides a library to manipulate page tables conforming to the AArch64 Virtual Memory
 System Architecture.
 
+Currently it only supports:
+
+- stage 1 page tables
+- 4 KiB pages
+- EL3, NS-EL2, NS-EL2&0 and NS-EL1&0 translation regimes
+
 This is not an officially supported Google product.
 
 ## License
diff --git a/cargo_embargo.json b/cargo_embargo.json
index e403355..685debb 100644
--- a/cargo_embargo.json
+++ b/cargo_embargo.json
@@ -9,7 +9,8 @@
       "alloc": true,
       "no_std": true,
       "force_rlib": true,
-      "add_module_block": "cargo2android_module.bp"
+      "add_module_block": "cargo2android_module.bp",
+      "license_text": "LICENSE.android"
     }
   },
   "run_cargo": false
diff --git a/src/idmap.rs b/src/idmap.rs
index 8b25356..b5f77b3 100644
--- a/src/idmap.rs
+++ b/src/idmap.rs
@@ -9,7 +9,7 @@
 use crate::{
     paging::{
         deallocate, Attributes, Constraints, Descriptor, MemoryRegion, PageTable, PhysicalAddress,
-        Translation, VaRange, VirtualAddress,
+        Translation, TranslationRegime, VaRange, VirtualAddress,
     },
     MapError, Mapping,
 };
@@ -26,7 +26,7 @@ impl IdTranslation {
 }
 
 impl Translation for IdTranslation {
-    fn allocate_table(&self) -> (NonNull<PageTable>, PhysicalAddress) {
+    fn allocate_table(&mut self) -> (NonNull<PageTable>, PhysicalAddress) {
         let table = PageTable::new();
 
         // Physical address is the same as the virtual address because we are using identity mapping
@@ -34,7 +34,7 @@ impl Translation for IdTranslation {
         (table, PhysicalAddress(table.as_ptr() as usize))
     }
 
-    unsafe fn deallocate_table(&self, page_table: NonNull<PageTable>) {
+    unsafe fn deallocate_table(&mut self, page_table: NonNull<PageTable>) {
         deallocate(page_table);
     }
 
@@ -60,18 +60,19 @@ impl Translation for IdTranslation {
 /// ```no_run
 /// use aarch64_paging::{
 ///     idmap::IdMap,
-///     paging::{Attributes, MemoryRegion},
+///     paging::{Attributes, MemoryRegion, TranslationRegime},
 /// };
 ///
 /// const ASID: usize = 1;
 /// const ROOT_LEVEL: usize = 1;
+/// const NORMAL_CACHEABLE: Attributes = Attributes::ATTRIBUTE_INDEX_1.union(Attributes::INNER_SHAREABLE);
 ///
-/// // Create a new page table with identity mapping.
-/// let mut idmap = IdMap::new(ASID, ROOT_LEVEL);
+/// // Create a new EL1 page table with identity mapping.
+/// let mut idmap = IdMap::new(ASID, ROOT_LEVEL, TranslationRegime::El1And0);
 /// // Map a 2 MiB region of memory as read-write.
 /// idmap.map_range(
 ///     &MemoryRegion::new(0x80200000, 0x80400000),
-///     Attributes::NORMAL | Attributes::NON_GLOBAL | Attributes::VALID,
+///     NORMAL_CACHEABLE | Attributes::NON_GLOBAL | Attributes::VALID | Attributes::ACCESSED,
 /// ).unwrap();
 /// // SAFETY: Everything the program uses is within the 2 MiB region mapped above.
 /// unsafe {
@@ -90,7 +91,8 @@ impl Translation for IdTranslation {
 /// // Now change the mapping to read-only and executable.
 /// idmap.map_range(
 ///     &MemoryRegion::new(0x80200000, 0x80400000),
-///     Attributes::NORMAL | Attributes::NON_GLOBAL | Attributes::READ_ONLY | Attributes::VALID,
+///     NORMAL_CACHEABLE | Attributes::NON_GLOBAL | Attributes::READ_ONLY | Attributes::VALID
+///     | Attributes::ACCESSED,
 /// ).unwrap();
 /// // SAFETY: Everything the program will used is mapped in by this page table.
 /// unsafe {
@@ -104,9 +106,15 @@ pub struct IdMap {
 
 impl IdMap {
     /// Creates a new identity-mapping page table with the given ASID and root level.
-    pub fn new(asid: usize, rootlevel: usize) -> Self {
+    pub fn new(asid: usize, rootlevel: usize, translation_regime: TranslationRegime) -> Self {
         Self {
-            mapping: Mapping::new(IdTranslation, asid, rootlevel, VaRange::Lower),
+            mapping: Mapping::new(
+                IdTranslation,
+                asid,
+                rootlevel,
+                translation_regime,
+                VaRange::Lower,
+            ),
         }
     }
 
@@ -248,8 +256,8 @@ impl IdMap {
     ///
     /// The callback function receives the following arguments:
     ///
-    /// - The full virtual address range mapped by each visited page table descriptor, which may
-    ///   exceed the original range passed to `walk_range`, due to alignment to block boundaries.
+    /// - The range covered by the current step in the walk. This is always a subrange of `range`
+    ///   even when the descriptor covers a region that exceeds it.
     /// - The page table descriptor itself.
     /// - The level of a translation table the descriptor belongs to.
     ///
@@ -272,6 +280,37 @@ impl IdMap {
     {
         self.mapping.walk_range(range, f)
     }
+
+    /// Returns the physical address of the root table.
+    ///
+    /// This may be used to activate the page table by setting the appropriate TTBRn_ELx if you wish
+    /// to do so yourself rather than by calling [`activate`](Self::activate). Make sure to call
+    /// [`mark_active`](Self::mark_active) after doing so.
+    pub fn root_address(&self) -> PhysicalAddress {
+        self.mapping.root_address()
+    }
+
+    /// Marks the page table as active.
+    ///
+    /// This should be called if the page table is manually activated by calling
+    /// [`root_address`](Self::root_address) and setting some TTBR with it. This will cause
+    /// [`map_range`](Self::map_range) and [`modify_range`](Self::modify_range) to perform extra
+    /// checks to avoid violating break-before-make requirements.
+    ///
+    /// It is called automatically by [`activate`](Self::activate).
+    pub fn mark_active(&mut self, previous_ttbr: usize) {
+        self.mapping.mark_active(previous_ttbr);
+    }
+
+    /// Marks the page table as inactive.
+    ///
+    /// This may be called after manually disabling the use of the page table, such as by setting
+    /// the relevant TTBR to a different address.
+    ///
+    /// It is called automatically by [`deactivate`](Self::deactivate).
+    pub fn mark_inactive(&mut self) {
+        self.mapping.mark_inactive();
+    }
 }
 
 #[cfg(test)]
@@ -283,11 +322,14 @@ mod tests {
     };
 
     const MAX_ADDRESS_FOR_ROOT_LEVEL_1: usize = 1 << 39;
+    const DEVICE_NGNRE: Attributes = Attributes::ATTRIBUTE_INDEX_0;
+    const NORMAL_CACHEABLE: Attributes =
+        Attributes::ATTRIBUTE_INDEX_1.union(Attributes::INNER_SHAREABLE);
 
     #[test]
     fn map_valid() {
         // A single byte at the start of the address space.
-        let mut idmap = IdMap::new(1, 1);
+        let mut idmap = IdMap::new(1, 1, TranslationRegime::El1And0);
         // SAFETY: This doesn't actually activate the page table in tests, it just treats it as
         // active for the sake of BBM rules.
         unsafe {
@@ -296,13 +338,13 @@ mod tests {
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(0, 1),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Ok(())
         );
 
         // Two pages at the start of the address space.
-        let mut idmap = IdMap::new(1, 1);
+        let mut idmap = IdMap::new(1, 1, TranslationRegime::El1And0);
         // SAFETY: This doesn't actually activate the page table in tests, it just treats it as
         // active for the sake of BBM rules.
         unsafe {
@@ -311,13 +353,13 @@ mod tests {
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(0, PAGE_SIZE * 2),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Ok(())
         );
 
         // A single byte at the end of the address space.
-        let mut idmap = IdMap::new(1, 1);
+        let mut idmap = IdMap::new(1, 1, TranslationRegime::El1And0);
         // SAFETY: This doesn't actually activate the page table in tests, it just treats it as
         // active for the sake of BBM rules.
         unsafe {
@@ -329,13 +371,13 @@ mod tests {
                     MAX_ADDRESS_FOR_ROOT_LEVEL_1 - 1,
                     MAX_ADDRESS_FOR_ROOT_LEVEL_1
                 ),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Ok(())
         );
 
         // Two pages, on the boundary between two subtables.
-        let mut idmap = IdMap::new(1, 1);
+        let mut idmap = IdMap::new(1, 1, TranslationRegime::El1And0);
         // SAFETY: This doesn't actually activate the page table in tests, it just treats it as
         // active for the sake of BBM rules.
         unsafe {
@@ -344,13 +386,13 @@ mod tests {
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(PAGE_SIZE * 1023, PAGE_SIZE * 1025),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Ok(())
         );
 
         // The entire valid address space.
-        let mut idmap = IdMap::new(1, 1);
+        let mut idmap = IdMap::new(1, 1, TranslationRegime::El1And0);
         // SAFETY: This doesn't actually activate the page table in tests, it just treats it as
         // active for the sake of BBM rules.
         unsafe {
@@ -359,7 +401,7 @@ mod tests {
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(0, MAX_ADDRESS_FOR_ROOT_LEVEL_1),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Ok(())
         );
@@ -368,11 +410,11 @@ mod tests {
     #[test]
     fn map_break_before_make() {
         const BLOCK_SIZE: usize = PAGE_SIZE << BITS_PER_LEVEL;
-        let mut idmap = IdMap::new(1, 1);
+        let mut idmap = IdMap::new(1, 1, TranslationRegime::El1And0);
         idmap
             .map_range_with_constraints(
                 &MemoryRegion::new(BLOCK_SIZE, 2 * BLOCK_SIZE),
-                Attributes::NORMAL | Attributes::VALID,
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED,
                 Constraints::NO_BLOCK_MAPPINGS,
             )
             .unwrap();
@@ -386,16 +428,16 @@ mod tests {
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(BLOCK_SIZE, BLOCK_SIZE + PAGE_SIZE),
-                Attributes::NORMAL | Attributes::VALID,
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED,
             ),
             Ok(())
         );
 
-        let mut idmap = IdMap::new(1, 1);
+        let mut idmap = IdMap::new(1, 1, TranslationRegime::El1And0);
         idmap
             .map_range(
                 &MemoryRegion::new(BLOCK_SIZE, 2 * BLOCK_SIZE),
-                Attributes::NORMAL | Attributes::VALID,
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED,
             )
             .ok();
         // SAFETY: This doesn't actually activate the page table in tests, it just treats it as
@@ -409,7 +451,7 @@ mod tests {
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(BLOCK_SIZE - PAGE_SIZE, 2 * BLOCK_SIZE + PAGE_SIZE),
-                Attributes::NORMAL | Attributes::VALID,
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED,
             ),
             Ok(())
         );
@@ -418,7 +460,7 @@ mod tests {
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(BLOCK_SIZE, BLOCK_SIZE + PAGE_SIZE),
-                Attributes::NORMAL | Attributes::VALID,
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED,
             ),
             Err(MapError::BreakBeforeMakeViolation(MemoryRegion::new(
                 BLOCK_SIZE,
@@ -431,17 +473,17 @@ mod tests {
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(0, BLOCK_SIZE + PAGE_SIZE),
-                Attributes::NORMAL | Attributes::VALID | Attributes::READ_ONLY,
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED | Attributes::READ_ONLY,
             ),
             Err(MapError::BreakBeforeMakeViolation(MemoryRegion::new(
-                0,
+                BLOCK_SIZE,
                 BLOCK_SIZE + PAGE_SIZE
             )))
         );
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(0, BLOCK_SIZE),
-                Attributes::NORMAL | Attributes::VALID | Attributes::READ_ONLY,
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED | Attributes::READ_ONLY,
             ),
             Ok(())
         );
@@ -450,10 +492,10 @@ mod tests {
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(0, BLOCK_SIZE),
-                Attributes::DEVICE_NGNRE | Attributes::VALID | Attributes::NON_GLOBAL,
+                DEVICE_NGNRE | Attributes::VALID | Attributes::ACCESSED | Attributes::NON_GLOBAL,
             ),
             Err(MapError::BreakBeforeMakeViolation(MemoryRegion::new(
-                0, BLOCK_SIZE
+                0, PAGE_SIZE
             )))
         );
 
@@ -461,18 +503,15 @@ mod tests {
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(PAGE_SIZE, BLOCK_SIZE + PAGE_SIZE),
-                Attributes::NORMAL,
+                NORMAL_CACHEABLE,
             ),
             Err(MapError::BreakBeforeMakeViolation(MemoryRegion::new(
-                PAGE_SIZE,
+                BLOCK_SIZE,
                 BLOCK_SIZE + PAGE_SIZE
             )))
         );
         assert_eq!(
-            idmap.map_range(
-                &MemoryRegion::new(PAGE_SIZE, BLOCK_SIZE),
-                Attributes::NORMAL,
-            ),
+            idmap.map_range(&MemoryRegion::new(PAGE_SIZE, BLOCK_SIZE), NORMAL_CACHEABLE),
             Ok(())
         );
 
@@ -480,7 +519,7 @@ mod tests {
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(0, 2 * PAGE_SIZE),
-                Attributes::NORMAL | Attributes::VALID,
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED,
             ),
             Ok(())
         );
@@ -489,7 +528,10 @@ mod tests {
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(0, PAGE_SIZE),
-                Attributes::NORMAL | Attributes::VALID | Attributes::NON_GLOBAL,
+                NORMAL_CACHEABLE
+                    | Attributes::VALID
+                    | Attributes::ACCESSED
+                    | Attributes::NON_GLOBAL,
             ),
             Ok(())
         );
@@ -498,7 +540,7 @@ mod tests {
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(0, PAGE_SIZE),
-                Attributes::NORMAL | Attributes::VALID,
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED,
             ),
             Err(MapError::BreakBeforeMakeViolation(MemoryRegion::new(
                 0, PAGE_SIZE
@@ -514,7 +556,7 @@ mod tests {
         assert_eq!(
             idmap.map_range(
                 &MemoryRegion::new(0, PAGE_SIZE),
-                Attributes::NORMAL | Attributes::VALID,
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED,
             ),
             Ok(())
         );
@@ -522,7 +564,7 @@ mod tests {
 
     #[test]
     fn map_out_of_range() {
-        let mut idmap = IdMap::new(1, 1);
+        let mut idmap = IdMap::new(1, 1, TranslationRegime::El1And0);
 
         // One byte, just past the edge of the valid range.
         assert_eq!(
@@ -531,7 +573,7 @@ mod tests {
                     MAX_ADDRESS_FOR_ROOT_LEVEL_1,
                     MAX_ADDRESS_FOR_ROOT_LEVEL_1 + 1,
                 ),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Err(MapError::AddressRange(VirtualAddress(
                 MAX_ADDRESS_FOR_ROOT_LEVEL_1 + PAGE_SIZE
@@ -541,8 +583,8 @@ mod tests {
         // From 0 to just past the valid range.
         assert_eq!(
             idmap.map_range(
-                &MemoryRegion::new(0, MAX_ADDRESS_FOR_ROOT_LEVEL_1 + 1,),
-                Attributes::NORMAL | Attributes::VALID
+                &MemoryRegion::new(0, MAX_ADDRESS_FOR_ROOT_LEVEL_1 + 1),
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Err(MapError::AddressRange(VirtualAddress(
                 MAX_ADDRESS_FOR_ROOT_LEVEL_1 + PAGE_SIZE
@@ -551,14 +593,15 @@ mod tests {
     }
 
     fn make_map() -> IdMap {
-        let mut idmap = IdMap::new(1, 1);
+        let mut idmap = IdMap::new(1, 1, TranslationRegime::El1And0);
         idmap
             .map_range(
                 &MemoryRegion::new(0, PAGE_SIZE * 2),
-                Attributes::NORMAL
+                NORMAL_CACHEABLE
                     | Attributes::NON_GLOBAL
                     | Attributes::READ_ONLY
-                    | Attributes::VALID,
+                    | Attributes::VALID
+                    | Attributes::ACCESSED,
             )
             .unwrap();
         // SAFETY: This doesn't actually activate the page table in tests, it just treats it as
@@ -618,7 +661,7 @@ mod tests {
     #[test]
     fn breakup_invalid_block() {
         const BLOCK_RANGE: usize = 0x200000;
-        let mut idmap = IdMap::new(1, 1);
+        let mut idmap = IdMap::new(1, 1, TranslationRegime::El1And0);
         // SAFETY: This doesn't actually activate the page table in tests, it just treats it as
         // active for the sake of BBM rules.
         unsafe {
@@ -627,13 +670,16 @@ mod tests {
         idmap
             .map_range(
                 &MemoryRegion::new(0, BLOCK_RANGE),
-                Attributes::NORMAL | Attributes::NON_GLOBAL | Attributes::SWFLAG_0,
+                NORMAL_CACHEABLE | Attributes::NON_GLOBAL | Attributes::SWFLAG_0,
             )
             .unwrap();
         idmap
             .map_range(
                 &MemoryRegion::new(0, PAGE_SIZE),
-                Attributes::NORMAL | Attributes::NON_GLOBAL | Attributes::VALID,
+                NORMAL_CACHEABLE
+                    | Attributes::NON_GLOBAL
+                    | Attributes::VALID
+                    | Attributes::ACCESSED,
             )
             .unwrap();
         idmap
@@ -650,4 +696,28 @@ mod tests {
             )
             .unwrap();
     }
+
+    /// When an unmapped entry is split into a table, all entries should be zero.
+    #[test]
+    fn split_table_zero() {
+        let mut idmap = IdMap::new(1, 1, TranslationRegime::El1And0);
+
+        idmap
+            .map_range(
+                &MemoryRegion::new(0, PAGE_SIZE),
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED,
+            )
+            .unwrap();
+        idmap
+            .walk_range(
+                &MemoryRegion::new(PAGE_SIZE, PAGE_SIZE * 20),
+                &mut |_, descriptor, _| {
+                    assert!(!descriptor.is_valid());
+                    assert_eq!(descriptor.flags(), Some(Attributes::empty()));
+                    assert_eq!(descriptor.output_address(), PhysicalAddress(0));
+                    Ok(())
+                },
+            )
+            .unwrap();
+    }
 }
diff --git a/src/lib.rs b/src/lib.rs
index a1ccd97..2495aa6 100644
--- a/src/lib.rs
+++ b/src/lib.rs
@@ -6,13 +6,12 @@
 //!
 //! Currently it only supports:
 //!   - stage 1 page tables
-//!   - EL1
 //!   - 4 KiB pages
+//!   - EL3, NS-EL2, NS-EL2&0 and NS-EL1&0 translation regimes
 //!
 //! Full support is provided for identity mapping ([`IdMap`](idmap::IdMap)) and linear mapping
 //! ([`LinearMap`](linearmap::LinearMap)). If you want to use a different mapping scheme, you must
-//! provide an implementation of the [`Translation`](paging::Translation) trait and then use
-//! [`Mapping`] directly.
+//! provide an implementation of the [`Translation`] trait and then use [`Mapping`] directly.
 //!
 //! # Example
 //!
@@ -20,18 +19,19 @@
 //! # #[cfg(feature = "alloc")] {
 //! use aarch64_paging::{
 //!     idmap::IdMap,
-//!     paging::{Attributes, MemoryRegion},
+//!     paging::{Attributes, MemoryRegion, TranslationRegime},
 //! };
 //!
 //! const ASID: usize = 1;
 //! const ROOT_LEVEL: usize = 1;
+//! const NORMAL_CACHEABLE: Attributes = Attributes::ATTRIBUTE_INDEX_1.union(Attributes::INNER_SHAREABLE);
 //!
-//! // Create a new page table with identity mapping.
-//! let mut idmap = IdMap::new(ASID, ROOT_LEVEL);
+//! // Create a new EL1 page table with identity mapping.
+//! let mut idmap = IdMap::new(ASID, ROOT_LEVEL, TranslationRegime::El1And0);
 //! // Map a 2 MiB region of memory as read-write.
 //! idmap.map_range(
 //!     &MemoryRegion::new(0x80200000, 0x80400000),
-//!     Attributes::NORMAL | Attributes::NON_GLOBAL | Attributes::VALID,
+//!     NORMAL_CACHEABLE | Attributes::NON_GLOBAL | Attributes::VALID | Attributes::ACCESSED,
 //! ).unwrap();
 //! // SAFETY: Everything the program uses is within the 2 MiB region mapped above.
 //! unsafe {
@@ -58,7 +58,7 @@ use core::arch::asm;
 use core::fmt::{self, Display, Formatter};
 use paging::{
     Attributes, Constraints, Descriptor, MemoryRegion, PhysicalAddress, RootTable, Translation,
-    VaRange, VirtualAddress,
+    TranslationRegime, VaRange, VirtualAddress,
 };
 
 /// An error attempting to map some range in the page table.
@@ -110,7 +110,7 @@ impl Display for MapError {
 /// switch back to a previous static page table, and then `activate` again after making the desired
 /// changes.
 #[derive(Debug)]
-pub struct Mapping<T: Translation + Clone> {
+pub struct Mapping<T: Translation> {
     root: RootTable<T>,
     #[allow(unused)]
     asid: usize,
@@ -118,11 +118,20 @@ pub struct Mapping<T: Translation + Clone> {
     previous_ttbr: Option<usize>,
 }
 
-impl<T: Translation + Clone> Mapping<T> {
+impl<T: Translation> Mapping<T> {
     /// Creates a new page table with the given ASID, root level and translation mapping.
-    pub fn new(translation: T, asid: usize, rootlevel: usize, va_range: VaRange) -> Self {
+    pub fn new(
+        translation: T,
+        asid: usize,
+        rootlevel: usize,
+        translation_regime: TranslationRegime,
+        va_range: VaRange,
+    ) -> Self {
+        if !translation_regime.supports_asid() && asid != 0 {
+            panic!("{:?} doesn't support ASID, must be 0.", translation_regime);
+        }
         Self {
-            root: RootTable::new(translation, rootlevel, va_range),
+            root: RootTable::new(translation, rootlevel, translation_regime, va_range),
             asid,
             previous_ttbr: None,
         }
@@ -133,13 +142,13 @@ impl<T: Translation + Clone> Mapping<T> {
         self.previous_ttbr.is_some()
     }
 
-    /// Activates the page table by setting `TTBRn_EL1` to point to it, and saves the previous value
-    /// of `TTBRn_EL1` so that it may later be restored by [`deactivate`](Self::deactivate).
+    /// Activates the page table by setting `TTBRn_ELx` to point to it, and saves the previous value
+    /// of `TTBRn_ELx` so that it may later be restored by [`deactivate`](Self::deactivate).
     ///
-    /// Panics if a previous value of `TTBRn_EL1` is already saved and not yet used by a call to
+    /// Panics if a previous value of `TTBRn_ELx` is already saved and not yet used by a call to
     /// `deactivate`.
     ///
-    /// In test builds or builds that do not target aarch64, the `TTBRn_EL1` access is omitted.
+    /// In test builds or builds that do not target aarch64, the `TTBRn_ELx` access is omitted.
     ///
     /// # Safety
     ///
@@ -154,40 +163,75 @@ impl<T: Translation + Clone> Mapping<T> {
         let mut previous_ttbr = usize::MAX;
 
         #[cfg(all(not(test), target_arch = "aarch64"))]
-        // SAFETY: Safe because we trust that self.root.to_physical() returns a valid physical
-        // address of a page table, and the `Drop` implementation will reset `TTBRn_EL1` before it
-        // becomes invalid.
+        // SAFETY: Safe because we trust that self.root_address() returns a valid physical address
+        // of a page table, and the `Drop` implementation will reset `TTBRn_ELx` before it becomes
+        // invalid.
         unsafe {
-            match self.root.va_range() {
-                VaRange::Lower => asm!(
+            match (self.root.translation_regime(), self.root.va_range()) {
+                (TranslationRegime::El1And0, VaRange::Lower) => asm!(
                     "mrs   {previous_ttbr}, ttbr0_el1",
                     "msr   ttbr0_el1, {ttbrval}",
                     "isb",
-                    ttbrval = in(reg) self.root.to_physical().0 | (self.asid << 48),
+                    ttbrval = in(reg) self.root_address().0 | (self.asid << 48),
                     previous_ttbr = out(reg) previous_ttbr,
                     options(preserves_flags),
                 ),
-                VaRange::Upper => asm!(
+                (TranslationRegime::El1And0, VaRange::Upper) => asm!(
                     "mrs   {previous_ttbr}, ttbr1_el1",
                     "msr   ttbr1_el1, {ttbrval}",
                     "isb",
-                    ttbrval = in(reg) self.root.to_physical().0 | (self.asid << 48),
+                    ttbrval = in(reg) self.root_address().0 | (self.asid << 48),
+                    previous_ttbr = out(reg) previous_ttbr,
+                    options(preserves_flags),
+                ),
+                (TranslationRegime::El2And0, VaRange::Lower) => asm!(
+                    "mrs   {previous_ttbr}, ttbr0_el2",
+                    "msr   ttbr0_el2, {ttbrval}",
+                    "isb",
+                    ttbrval = in(reg) self.root_address().0 | (self.asid << 48),
+                    previous_ttbr = out(reg) previous_ttbr,
+                    options(preserves_flags),
+                ),
+                (TranslationRegime::El2And0, VaRange::Upper) => asm!(
+                    "mrs   {previous_ttbr}, s3_4_c2_c0_1", // ttbr1_el2
+                    "msr   s3_4_c2_c0_1, {ttbrval}",
+                    "isb",
+                    ttbrval = in(reg) self.root_address().0 | (self.asid << 48),
+                    previous_ttbr = out(reg) previous_ttbr,
+                    options(preserves_flags),
+                ),
+                (TranslationRegime::El2, VaRange::Lower) => asm!(
+                    "mrs   {previous_ttbr}, ttbr0_el2",
+                    "msr   ttbr0_el2, {ttbrval}",
+                    "isb",
+                    ttbrval = in(reg) self.root_address().0,
+                    previous_ttbr = out(reg) previous_ttbr,
+                    options(preserves_flags),
+                ),
+                (TranslationRegime::El3, VaRange::Lower) => asm!(
+                    "mrs   {previous_ttbr}, ttbr0_el3",
+                    "msr   ttbr0_el3, {ttbrval}",
+                    "isb",
+                    ttbrval = in(reg) self.root_address().0,
                     previous_ttbr = out(reg) previous_ttbr,
                     options(preserves_flags),
                 ),
+                _ => {
+                    panic!("Invalid combination of exception level and VA range.");
+                }
             }
         }
-        self.previous_ttbr = Some(previous_ttbr);
+        self.mark_active(previous_ttbr);
     }
 
-    /// Deactivates the page table, by setting `TTBRn_EL1` back to the value it had before
+    /// Deactivates the page table, by setting `TTBRn_ELx` back to the value it had before
     /// [`activate`](Self::activate) was called, and invalidating the TLB for this page table's
     /// configured ASID.
     ///
-    /// Panics if there is no saved `TTBRn_EL1` value because `activate` has not previously been
+    /// Panics if there is no saved `TTBRn_ELx` value because `activate` has not previously been
     /// called.
     ///
-    /// In test builds or builds that do not target aarch64, the `TTBRn_EL1` access is omitted.
+    /// In test builds or builds that do not target aarch64, the `TTBRn_ELx` access is omitted.
     ///
     /// # Safety
     ///
@@ -197,11 +241,11 @@ impl<T: Translation + Clone> Mapping<T> {
         assert!(self.active());
 
         #[cfg(all(not(test), target_arch = "aarch64"))]
-        // SAFETY: Safe because this just restores the previously saved value of `TTBRn_EL1`, which
+        // SAFETY: Safe because this just restores the previously saved value of `TTBRn_ELx`, which
         // must have been valid.
         unsafe {
-            match self.root.va_range() {
-                VaRange::Lower => asm!(
+            match (self.root.translation_regime(), self.root.va_range()) {
+                (TranslationRegime::El1And0, VaRange::Lower) => asm!(
                     "msr   ttbr0_el1, {ttbrval}",
                     "isb",
                     "tlbi  aside1, {asid}",
@@ -211,7 +255,7 @@ impl<T: Translation + Clone> Mapping<T> {
                     ttbrval = in(reg) self.previous_ttbr.unwrap(),
                     options(preserves_flags),
                 ),
-                VaRange::Upper => asm!(
+                (TranslationRegime::El1And0, VaRange::Upper) => asm!(
                     "msr   ttbr1_el1, {ttbrval}",
                     "isb",
                     "tlbi  aside1, {asid}",
@@ -221,9 +265,38 @@ impl<T: Translation + Clone> Mapping<T> {
                     ttbrval = in(reg) self.previous_ttbr.unwrap(),
                     options(preserves_flags),
                 ),
+                (TranslationRegime::El2And0, VaRange::Lower) => asm!(
+                    "msr   ttbr0_el2, {ttbrval}",
+                    "isb",
+                    "tlbi  aside1, {asid}",
+                    "dsb   nsh",
+                    "isb",
+                    asid = in(reg) self.asid << 48,
+                    ttbrval = in(reg) self.previous_ttbr.unwrap(),
+                    options(preserves_flags),
+                ),
+                (TranslationRegime::El2And0, VaRange::Upper) => asm!(
+                    "msr   s3_4_c2_c0_1, {ttbrval}", // ttbr1_el2
+                    "isb",
+                    "tlbi  aside1, {asid}",
+                    "dsb   nsh",
+                    "isb",
+                    asid = in(reg) self.asid << 48,
+                    ttbrval = in(reg) self.previous_ttbr.unwrap(),
+                    options(preserves_flags),
+                ),
+                (TranslationRegime::El2, VaRange::Lower) => {
+                    panic!("EL2 page table can't safety be deactivated.");
+                }
+                (TranslationRegime::El3, VaRange::Lower) => {
+                    panic!("EL3 page table can't safety be deactivated.");
+                }
+                _ => {
+                    panic!("Invalid combination of exception level and VA range.");
+                }
             }
         }
-        self.previous_ttbr = None;
+        self.mark_inactive();
     }
 
     /// Checks whether the given range can be mapped or updated while the translation is live,
@@ -232,21 +305,23 @@ impl<T: Translation + Clone> Mapping<T> {
     where
         F: Fn(&MemoryRegion, &mut Descriptor, usize) -> Result<(), ()> + ?Sized,
     {
-        self.walk_range(
+        self.root.visit_range(
             range,
             &mut |mr: &MemoryRegion, d: &Descriptor, level: usize| {
                 if d.is_valid() {
+                    let err = MapError::BreakBeforeMakeViolation(mr.clone());
+
                     if !mr.is_block(level) {
                         // Cannot split a live block mapping
-                        return Err(());
+                        return Err(err);
                     }
 
                     // Get the new flags and output address for this descriptor by applying
                     // the updater function to a copy
                     let (flags, oa) = {
                         let mut dd = *d;
-                        updater(mr, &mut dd, level)?;
-                        (dd.flags().ok_or(())?, dd.output_address())
+                        updater(mr, &mut dd, level).or(Err(err.clone()))?;
+                        (dd.flags().ok_or(err.clone())?, dd.output_address())
                     };
 
                     if !flags.contains(Attributes::VALID) {
@@ -256,26 +331,26 @@ impl<T: Translation + Clone> Mapping<T> {
 
                     if oa != d.output_address() {
                         // Cannot change output address on a live mapping
-                        return Err(());
+                        return Err(err);
                     }
 
                     let desc_flags = d.flags().unwrap();
 
-                    if (desc_flags ^ flags).intersects(Attributes::NORMAL) {
+                    if (desc_flags ^ flags).intersects(
+                        Attributes::ATTRIBUTE_INDEX_MASK | Attributes::SHAREABILITY_MASK,
+                    ) {
                         // Cannot change memory type
-                        return Err(());
+                        return Err(err);
                     }
 
                     if (desc_flags - flags).contains(Attributes::NON_GLOBAL) {
                         // Cannot convert from non-global to global
-                        return Err(());
+                        return Err(err);
                     }
                 }
                 Ok(())
             },
         )
-        .map_err(|_| MapError::BreakBeforeMakeViolation(range.clone()))?;
-        Ok(())
     }
 
     /// Maps the given range of virtual addresses to the corresponding range of physical addresses
@@ -381,9 +456,40 @@ impl<T: Translation + Clone> Mapping<T> {
     {
         self.root.walk_range(range, f)
     }
+
+    /// Returns the physical address of the root table.
+    ///
+    /// This may be used to activate the page table by setting the appropriate TTBRn_ELx if you wish
+    /// to do so yourself rather than by calling [`activate`](Self::activate). Make sure to call
+    /// [`mark_active`](Self::mark_active) after doing so.
+    pub fn root_address(&self) -> PhysicalAddress {
+        self.root.to_physical()
+    }
+
+    /// Marks the page table as active.
+    ///
+    /// This should be called if the page table is manually activated by calling
+    /// [`root_address`](Self::root_address) and setting some TTBR with it. This will cause
+    /// [`map_range`](Self::map_range) and [`modify_range`](Self::modify_range) to perform extra
+    /// checks to avoid violating break-before-make requirements.
+    ///
+    /// It is called automatically by [`activate`](Self::activate).
+    pub fn mark_active(&mut self, previous_ttbr: usize) {
+        self.previous_ttbr = Some(previous_ttbr);
+    }
+
+    /// Marks the page table as inactive.
+    ///
+    /// This may be called after manually disabling the use of the page table, such as by setting
+    /// the relevant TTBR to a different address.
+    ///
+    /// It is called automatically by [`deactivate`](Self::deactivate).
+    pub fn mark_inactive(&mut self) {
+        self.previous_ttbr = None;
+    }
 }
 
-impl<T: Translation + Clone> Drop for Mapping<T> {
+impl<T: Translation> Drop for Mapping<T> {
     fn drop(&mut self) {
         if self.previous_ttbr.is_some() {
             #[cfg(target_arch = "aarch64")]
@@ -395,3 +501,25 @@ impl<T: Translation + Clone> Drop for Mapping<T> {
         }
     }
 }
+
+#[cfg(test)]
+mod tests {
+    #[cfg(feature = "alloc")]
+    use self::idmap::IdTranslation;
+    #[cfg(feature = "alloc")]
+    use super::*;
+
+    #[cfg(feature = "alloc")]
+    #[test]
+    #[should_panic]
+    fn no_el2_asid() {
+        Mapping::new(IdTranslation, 1, 1, TranslationRegime::El2, VaRange::Lower);
+    }
+
+    #[cfg(feature = "alloc")]
+    #[test]
+    #[should_panic]
+    fn no_el3_asid() {
+        Mapping::new(IdTranslation, 1, 1, TranslationRegime::El3, VaRange::Lower);
+    }
+}
diff --git a/src/linearmap.rs b/src/linearmap.rs
index be9d8aa..783d939 100644
--- a/src/linearmap.rs
+++ b/src/linearmap.rs
@@ -9,7 +9,7 @@
 use crate::{
     paging::{
         deallocate, is_aligned, Attributes, Constraints, Descriptor, MemoryRegion, PageTable,
-        PhysicalAddress, Translation, VaRange, VirtualAddress, PAGE_SIZE,
+        PhysicalAddress, Translation, TranslationRegime, VaRange, VirtualAddress, PAGE_SIZE,
     },
     MapError, Mapping,
 };
@@ -48,7 +48,7 @@ impl LinearTranslation {
 }
 
 impl Translation for LinearTranslation {
-    fn allocate_table(&self) -> (NonNull<PageTable>, PhysicalAddress) {
+    fn allocate_table(&mut self) -> (NonNull<PageTable>, PhysicalAddress) {
         let table = PageTable::new();
         // Assume that the same linear mapping is used everywhere.
         let va = VirtualAddress(table.as_ptr() as usize);
@@ -59,7 +59,7 @@ impl Translation for LinearTranslation {
         (table, pa)
     }
 
-    unsafe fn deallocate_table(&self, page_table: NonNull<PageTable>) {
+    unsafe fn deallocate_table(&mut self, page_table: NonNull<PageTable>) {
         deallocate(page_table);
     }
 
@@ -106,9 +106,21 @@ impl LinearMap {
     /// `va + offset`.
     ///
     /// The `offset` must be a multiple of [`PAGE_SIZE`]; if not this will panic.
-    pub fn new(asid: usize, rootlevel: usize, offset: isize, va_range: VaRange) -> Self {
+    pub fn new(
+        asid: usize,
+        rootlevel: usize,
+        offset: isize,
+        translation_regime: TranslationRegime,
+        va_range: VaRange,
+    ) -> Self {
         Self {
-            mapping: Mapping::new(LinearTranslation::new(offset), asid, rootlevel, va_range),
+            mapping: Mapping::new(
+                LinearTranslation::new(offset),
+                asid,
+                rootlevel,
+                translation_regime,
+                va_range,
+            ),
         }
     }
 
@@ -260,8 +272,8 @@ impl LinearMap {
     ///
     /// The callback function receives the following arguments:
     ///
-    /// - The full virtual address range mapped by each visited page table descriptor, which may
-    ///   exceed the original range passed to `walk_range`, due to alignment to block boundaries.
+    /// - The range covered by the current step in the walk. This is always a subrange of `range`
+    ///   even when the descriptor covers a region that exceeds it.
     /// - The page table descriptor itself.
     /// - The level of a translation table the descriptor belongs to.
     ///
@@ -284,6 +296,37 @@ impl LinearMap {
     {
         self.mapping.walk_range(range, f)
     }
+
+    /// Returns the physical address of the root table.
+    ///
+    /// This may be used to activate the page table by setting the appropriate TTBRn_ELx if you wish
+    /// to do so yourself rather than by calling [`activate`](Self::activate). Make sure to call
+    /// [`mark_active`](Self::mark_active) after doing so.
+    pub fn root_address(&self) -> PhysicalAddress {
+        self.mapping.root_address()
+    }
+
+    /// Marks the page table as active.
+    ///
+    /// This should be called if the page table is manually activated by calling
+    /// [`root_address`](Self::root_address) and setting some TTBR with it. This will cause
+    /// [`map_range`](Self::map_range) and [`modify_range`](Self::modify_range) to perform extra
+    /// checks to avoid violating break-before-make requirements.
+    ///
+    /// It is called automatically by [`activate`](Self::activate).
+    pub fn mark_active(&mut self, previous_ttbr: usize) {
+        self.mapping.mark_active(previous_ttbr);
+    }
+
+    /// Marks the page table as inactive.
+    ///
+    /// This may be called after manually disabling the use of the page table, such as by setting
+    /// the relevant TTBR to a different address.
+    ///
+    /// It is called automatically by [`deactivate`](Self::deactivate).
+    pub fn mark_inactive(&mut self) {
+        self.mapping.mark_inactive();
+    }
 }
 
 #[cfg(test)]
@@ -297,38 +340,40 @@ mod tests {
     const MAX_ADDRESS_FOR_ROOT_LEVEL_1: usize = 1 << 39;
     const GIB_512_S: isize = 512 * 1024 * 1024 * 1024;
     const GIB_512: usize = 512 * 1024 * 1024 * 1024;
+    const NORMAL_CACHEABLE: Attributes =
+        Attributes::ATTRIBUTE_INDEX_1.union(Attributes::INNER_SHAREABLE);
 
     #[test]
     fn map_valid() {
         // A single byte at the start of the address space.
-        let mut pagetable = LinearMap::new(1, 1, 4096, VaRange::Lower);
+        let mut pagetable = LinearMap::new(1, 1, 4096, TranslationRegime::El1And0, VaRange::Lower);
         assert_eq!(
             pagetable.map_range(
                 &MemoryRegion::new(0, 1),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Ok(())
         );
 
         // Two pages at the start of the address space.
-        let mut pagetable = LinearMap::new(1, 1, 4096, VaRange::Lower);
+        let mut pagetable = LinearMap::new(1, 1, 4096, TranslationRegime::El1And0, VaRange::Lower);
         assert_eq!(
             pagetable.map_range(
                 &MemoryRegion::new(0, PAGE_SIZE * 2),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Ok(())
         );
 
         // A single byte at the end of the address space.
-        let mut pagetable = LinearMap::new(1, 1, 4096, VaRange::Lower);
+        let mut pagetable = LinearMap::new(1, 1, 4096, TranslationRegime::El1And0, VaRange::Lower);
         assert_eq!(
             pagetable.map_range(
                 &MemoryRegion::new(
                     MAX_ADDRESS_FOR_ROOT_LEVEL_1 - 1,
                     MAX_ADDRESS_FOR_ROOT_LEVEL_1
                 ),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Ok(())
         );
@@ -336,11 +381,17 @@ mod tests {
         // The entire valid address space. Use an offset that is a multiple of the level 2 block
         // size to avoid mapping everything as pages as that is really slow.
         const LEVEL_2_BLOCK_SIZE: usize = PAGE_SIZE << BITS_PER_LEVEL;
-        let mut pagetable = LinearMap::new(1, 1, LEVEL_2_BLOCK_SIZE as isize, VaRange::Lower);
+        let mut pagetable = LinearMap::new(
+            1,
+            1,
+            LEVEL_2_BLOCK_SIZE as isize,
+            TranslationRegime::El1And0,
+            VaRange::Lower,
+        );
         assert_eq!(
             pagetable.map_range(
                 &MemoryRegion::new(0, MAX_ADDRESS_FOR_ROOT_LEVEL_1),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Ok(())
         );
@@ -349,34 +400,52 @@ mod tests {
     #[test]
     fn map_valid_negative_offset() {
         // A single byte which maps to IPA 0.
-        let mut pagetable = LinearMap::new(1, 1, -(PAGE_SIZE as isize), VaRange::Lower);
+        let mut pagetable = LinearMap::new(
+            1,
+            1,
+            -(PAGE_SIZE as isize),
+            TranslationRegime::El1And0,
+            VaRange::Lower,
+        );
         assert_eq!(
             pagetable.map_range(
                 &MemoryRegion::new(PAGE_SIZE, PAGE_SIZE + 1),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Ok(())
         );
 
         // Two pages at the start of the address space.
-        let mut pagetable = LinearMap::new(1, 1, -(PAGE_SIZE as isize), VaRange::Lower);
+        let mut pagetable = LinearMap::new(
+            1,
+            1,
+            -(PAGE_SIZE as isize),
+            TranslationRegime::El1And0,
+            VaRange::Lower,
+        );
         assert_eq!(
             pagetable.map_range(
                 &MemoryRegion::new(PAGE_SIZE, PAGE_SIZE * 3),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Ok(())
         );
 
         // A single byte at the end of the address space.
-        let mut pagetable = LinearMap::new(1, 1, -(PAGE_SIZE as isize), VaRange::Lower);
+        let mut pagetable = LinearMap::new(
+            1,
+            1,
+            -(PAGE_SIZE as isize),
+            TranslationRegime::El1And0,
+            VaRange::Lower,
+        );
         assert_eq!(
             pagetable.map_range(
                 &MemoryRegion::new(
                     MAX_ADDRESS_FOR_ROOT_LEVEL_1 - 1,
                     MAX_ADDRESS_FOR_ROOT_LEVEL_1
                 ),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Ok(())
         );
@@ -384,11 +453,17 @@ mod tests {
         // The entire valid address space. Use an offset that is a multiple of the level 2 block
         // size to avoid mapping everything as pages as that is really slow.
         const LEVEL_2_BLOCK_SIZE: usize = PAGE_SIZE << BITS_PER_LEVEL;
-        let mut pagetable = LinearMap::new(1, 1, -(LEVEL_2_BLOCK_SIZE as isize), VaRange::Lower);
+        let mut pagetable = LinearMap::new(
+            1,
+            1,
+            -(LEVEL_2_BLOCK_SIZE as isize),
+            TranslationRegime::El1And0,
+            VaRange::Lower,
+        );
         assert_eq!(
             pagetable.map_range(
                 &MemoryRegion::new(LEVEL_2_BLOCK_SIZE, MAX_ADDRESS_FOR_ROOT_LEVEL_1),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Ok(())
         );
@@ -396,7 +471,7 @@ mod tests {
 
     #[test]
     fn map_out_of_range() {
-        let mut pagetable = LinearMap::new(1, 1, 4096, VaRange::Lower);
+        let mut pagetable = LinearMap::new(1, 1, 4096, TranslationRegime::El1And0, VaRange::Lower);
 
         // One byte, just past the edge of the valid range.
         assert_eq!(
@@ -405,7 +480,7 @@ mod tests {
                     MAX_ADDRESS_FOR_ROOT_LEVEL_1,
                     MAX_ADDRESS_FOR_ROOT_LEVEL_1 + 1,
                 ),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Err(MapError::AddressRange(VirtualAddress(
                 MAX_ADDRESS_FOR_ROOT_LEVEL_1 + PAGE_SIZE
@@ -416,7 +491,7 @@ mod tests {
         assert_eq!(
             pagetable.map_range(
                 &MemoryRegion::new(0, MAX_ADDRESS_FOR_ROOT_LEVEL_1 + 1),
-                Attributes::NORMAL | Attributes::VALID
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED
             ),
             Err(MapError::AddressRange(VirtualAddress(
                 MAX_ADDRESS_FOR_ROOT_LEVEL_1 + PAGE_SIZE
@@ -426,11 +501,11 @@ mod tests {
 
     #[test]
     fn map_invalid_offset() {
-        let mut pagetable = LinearMap::new(1, 1, -4096, VaRange::Lower);
+        let mut pagetable = LinearMap::new(1, 1, -4096, TranslationRegime::El1And0, VaRange::Lower);
 
         // One byte, with an offset which would map it to a negative IPA.
         assert_eq!(
-            pagetable.map_range(&MemoryRegion::new(0, 1), Attributes::NORMAL,),
+            pagetable.map_range(&MemoryRegion::new(0, 1), NORMAL_CACHEABLE),
             Err(MapError::InvalidVirtualAddress(VirtualAddress(0)))
         );
     }
@@ -526,11 +601,12 @@ mod tests {
     #[test]
     fn block_mapping() {
         // Test that block mapping is used when the PA is appropriately aligned...
-        let mut pagetable = LinearMap::new(1, 1, 1 << 30, VaRange::Lower);
+        let mut pagetable =
+            LinearMap::new(1, 1, 1 << 30, TranslationRegime::El1And0, VaRange::Lower);
         pagetable
             .map_range(
                 &MemoryRegion::new(0, 1 << 30),
-                Attributes::NORMAL | Attributes::VALID,
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED,
             )
             .unwrap();
         assert_eq!(
@@ -539,11 +615,12 @@ mod tests {
         );
 
         // ...but not when it is not.
-        let mut pagetable = LinearMap::new(1, 1, 1 << 29, VaRange::Lower);
+        let mut pagetable =
+            LinearMap::new(1, 1, 1 << 29, TranslationRegime::El1And0, VaRange::Lower);
         pagetable
             .map_range(
                 &MemoryRegion::new(0, 1 << 30),
-                Attributes::NORMAL | Attributes::VALID,
+                NORMAL_CACHEABLE | Attributes::VALID | Attributes::ACCESSED,
             )
             .unwrap();
         assert_eq!(
@@ -553,9 +630,9 @@ mod tests {
     }
 
     fn make_map() -> LinearMap {
-        let mut lmap = LinearMap::new(1, 1, 4096, VaRange::Lower);
+        let mut lmap = LinearMap::new(1, 1, 4096, TranslationRegime::El1And0, VaRange::Lower);
         // Mapping VA range 0x0 - 0x2000 to PA range 0x1000 - 0x3000
-        lmap.map_range(&MemoryRegion::new(0, PAGE_SIZE * 2), Attributes::NORMAL)
+        lmap.map_range(&MemoryRegion::new(0, PAGE_SIZE * 2), NORMAL_CACHEABLE)
             .unwrap();
         lmap
     }
@@ -599,15 +676,15 @@ mod tests {
     fn breakup_invalid_block() {
         const BLOCK_RANGE: usize = 0x200000;
 
-        let mut lmap = LinearMap::new(1, 1, 0x1000, VaRange::Lower);
+        let mut lmap = LinearMap::new(1, 1, 0x1000, TranslationRegime::El1And0, VaRange::Lower);
         lmap.map_range(
             &MemoryRegion::new(0, BLOCK_RANGE),
-            Attributes::NORMAL | Attributes::NON_GLOBAL | Attributes::SWFLAG_0,
+            NORMAL_CACHEABLE | Attributes::NON_GLOBAL | Attributes::SWFLAG_0,
         )
         .unwrap();
         lmap.map_range(
             &MemoryRegion::new(0, PAGE_SIZE),
-            Attributes::NORMAL | Attributes::NON_GLOBAL | Attributes::VALID,
+            NORMAL_CACHEABLE | Attributes::NON_GLOBAL | Attributes::VALID | Attributes::ACCESSED,
         )
         .unwrap();
         lmap.modify_range(
diff --git a/src/paging.rs b/src/paging.rs
index 9dac27b..a5bb0f5 100644
--- a/src/paging.rs
+++ b/src/paging.rs
@@ -37,6 +37,30 @@ pub enum VaRange {
     Upper,
 }
 
+/// Which translation regime a page table is for.
+///
+/// This depends on the exception level, among other things.
+#[derive(Copy, Clone, Debug, Eq, PartialEq)]
+pub enum TranslationRegime {
+    /// Secure EL3.
+    El3,
+    /// Non-secure EL2.
+    El2,
+    /// Non-secure EL2&0, with VHE.
+    El2And0,
+    /// Non-secure EL1&0, stage 1.
+    El1And0,
+}
+
+impl TranslationRegime {
+    /// Returns whether this translation regime supports use of an ASID.
+    ///
+    /// This also implies that it supports two VA ranges.
+    pub(crate) fn supports_asid(self) -> bool {
+        matches!(self, Self::El2And0 | Self::El1And0)
+    }
+}
+
 /// An aarch64 virtual address, the input type of a stage 1 page table.
 #[derive(Copy, Clone, Eq, Ord, PartialEq, PartialOrd)]
 pub struct VirtualAddress(pub usize);
@@ -134,7 +158,7 @@ pub(crate) fn granularity_at_level(level: usize) -> usize {
 pub trait Translation {
     /// Allocates a zeroed page, which is already mapped, to be used for a new subtable of some
     /// pagetable. Returns both a pointer to the page and its physical address.
-    fn allocate_table(&self) -> (NonNull<PageTable>, PhysicalAddress);
+    fn allocate_table(&mut self) -> (NonNull<PageTable>, PhysicalAddress);
 
     /// Deallocates the page which was previous allocated by [`allocate_table`](Self::allocate_table).
     ///
@@ -142,7 +166,7 @@ pub trait Translation {
     ///
     /// The memory must have been allocated by `allocate_table` on the same `Translation`, and not
     /// yet deallocated.
-    unsafe fn deallocate_table(&self, page_table: NonNull<PageTable>);
+    unsafe fn deallocate_table(&mut self, page_table: NonNull<PageTable>);
 
     /// Given the physical address of a subtable, returns the virtual address at which it is mapped.
     fn physical_to_virtual(&self, pa: PhysicalAddress) -> NonNull<PageTable>;
@@ -228,6 +252,7 @@ pub struct RootTable<T: Translation> {
     table: PageTableWithLevel<T>,
     translation: T,
     pa: PhysicalAddress,
+    translation_regime: TranslationRegime,
     va_range: VaRange,
 }
 
@@ -237,15 +262,27 @@ impl<T: Translation> RootTable<T> {
     /// The level must be between 0 and 3; level -1 (for 52-bit addresses with LPA2) is not
     /// currently supported by this library. The value of `TCR_EL1.T0SZ` must be set appropriately
     /// to match.
-    pub fn new(translation: T, level: usize, va_range: VaRange) -> Self {
+    pub fn new(
+        mut translation: T,
+        level: usize,
+        translation_regime: TranslationRegime,
+        va_range: VaRange,
+    ) -> Self {
         if level > LEAF_LEVEL {
             panic!("Invalid root table level {}.", level);
         }
-        let (table, pa) = PageTableWithLevel::new(&translation, level);
+        if !translation_regime.supports_asid() && va_range != VaRange::Lower {
+            panic!(
+                "{:?} doesn't have an upper virtual address range.",
+                translation_regime
+            );
+        }
+        let (table, pa) = PageTableWithLevel::new(&mut translation, level);
         RootTable {
             table,
             translation,
             pa,
+            translation_regime,
             va_range,
         }
     }
@@ -276,7 +313,7 @@ impl<T: Translation> RootTable<T> {
         }
         self.verify_region(range)?;
         self.table
-            .map_range(&self.translation, range, pa, flags, constraints);
+            .map_range(&mut self.translation, range, pa, flags, constraints);
         Ok(())
     }
 
@@ -285,11 +322,18 @@ impl<T: Translation> RootTable<T> {
         self.pa
     }
 
-    /// Returns the TTBR for which this table is intended.
+    /// Returns the virtual address range for which this table is intended.
+    ///
+    /// This affects which TTBR register is used.
     pub fn va_range(&self) -> VaRange {
         self.va_range
     }
 
+    /// Returns the translation regime for which this table is intended.
+    pub fn translation_regime(&self) -> TranslationRegime {
+        self.translation_regime
+    }
+
     /// Returns a reference to the translation used for this page table.
     pub fn translation(&self) -> &T {
         &self.translation
@@ -337,7 +381,7 @@ impl<T: Translation> RootTable<T> {
         F: Fn(&MemoryRegion, &mut Descriptor, usize) -> Result<(), ()> + ?Sized,
     {
         self.verify_region(range)?;
-        self.table.modify_range(&self.translation, range, f)
+        self.table.modify_range(&mut self.translation, range, f)
     }
 
     /// Applies the provided callback function to the page table descriptors covering a given
@@ -345,8 +389,8 @@ impl<T: Translation> RootTable<T> {
     ///
     /// The callback function receives the following arguments:
     ///
-    /// - The full virtual address range mapped by each visited page table descriptor, which may
-    ///   exceed the original range passed to `walk_range`, due to alignment to block boundaries.
+    /// - The range covered by the current step in the walk. This is always a subrange of `range`
+    ///   even when the descriptor covers a region that exceeds it.
     /// - The page table descriptor itself.
     /// - The level of a translation table the descriptor belongs to.
     ///
@@ -366,9 +410,19 @@ impl<T: Translation> RootTable<T> {
     pub fn walk_range<F>(&self, range: &MemoryRegion, f: &mut F) -> Result<(), MapError>
     where
         F: FnMut(&MemoryRegion, &Descriptor, usize) -> Result<(), ()>,
+    {
+        self.visit_range(range, &mut |mr, desc, level| {
+            f(mr, desc, level).map_err(|_| MapError::PteUpdateFault(*desc))
+        })
+    }
+
+    // Private version of `walk_range` using a closure that returns MapError on error
+    pub(crate) fn visit_range<F>(&self, range: &MemoryRegion, f: &mut F) -> Result<(), MapError>
+    where
+        F: FnMut(&MemoryRegion, &Descriptor, usize) -> Result<(), MapError>,
     {
         self.verify_region(range)?;
-        self.table.walk_range(&self.translation, range, f)
+        self.table.visit_range(&self.translation, range, f)
     }
 
     /// Returns the level of mapping used for the given virtual address:
@@ -419,7 +473,11 @@ impl<T: Translation> Debug for RootTable<T> {
 
 impl<T: Translation> Drop for RootTable<T> {
     fn drop(&mut self) {
-        self.table.free(&self.translation)
+        // SAFETY: We created the table in `RootTable::new` by calling `PageTableWithLevel::new`
+        // with `self.translation`. Subtables were similarly created by
+        // `PageTableWithLevel::split_entry` calling `PageTableWithLevel::new` with the same
+        // translation.
+        unsafe { self.table.free(&mut self.translation) }
     }
 }
 
@@ -455,19 +513,31 @@ bitflags! {
         const VALID         = 1 << 0;
         const TABLE_OR_PAGE = 1 << 1;
 
-        // The following memory types assume that the MAIR registers
-        // have been programmed accordingly.
-        const DEVICE_NGNRE  = 0 << 2;
-        const NORMAL        = 1 << 2 | 3 << 8; // inner shareable
+        const ATTRIBUTE_INDEX_0 = 0 << 2;
+        const ATTRIBUTE_INDEX_1 = 1 << 2;
+        const ATTRIBUTE_INDEX_2 = 2 << 2;
+        const ATTRIBUTE_INDEX_3 = 3 << 2;
+        const ATTRIBUTE_INDEX_4 = 4 << 2;
+        const ATTRIBUTE_INDEX_5 = 5 << 2;
+        const ATTRIBUTE_INDEX_6 = 6 << 2;
+        const ATTRIBUTE_INDEX_7 = 7 << 2;
 
+        const OUTER_SHAREABLE = 2 << 8;
+        const INNER_SHAREABLE = 3 << 8;
+
+        const NS            = 1 << 5;
         const USER          = 1 << 6;
         const READ_ONLY     = 1 << 7;
         const ACCESSED      = 1 << 10;
         const NON_GLOBAL    = 1 << 11;
         const DBM           = 1 << 51;
-        const EXECUTE_NEVER = 3 << 53;
+        /// Privileged Execute-never, if two privilege levels are supported.
+        const PXN           = 1 << 53;
+        /// Unprivileged Execute-never, or just Execute-never if only one privilege level is
+        /// supported.
+        const UXN           = 1 << 54;
 
-        /// Software flags in block and page descriptor entries.
+        // Software flags in block and page descriptor entries.
         const SWFLAG_0 = 1 << 55;
         const SWFLAG_1 = 1 << 56;
         const SWFLAG_2 = 1 << 57;
@@ -475,6 +545,14 @@ bitflags! {
     }
 }
 
+impl Attributes {
+    /// Mask for the bits determining the shareability of the mapping.
+    pub const SHAREABILITY_MASK: Self = Self::INNER_SHAREABLE;
+
+    /// Mask for the bits determining the attribute index of the mapping.
+    pub const ATTRIBUTE_INDEX_MASK: Self = Self::ATTRIBUTE_INDEX_7;
+}
+
 /// Smart pointer which owns a [`PageTable`] and knows what level it is at. This allows it to
 /// implement `Debug` and `Drop`, as walking the page table hierachy requires knowing the starting
 /// level.
@@ -489,10 +567,13 @@ struct PageTableWithLevel<T: Translation> {
 // with appropriate synchronization. This type manages ownership for the raw pointer.
 unsafe impl<T: Translation + Send> Send for PageTableWithLevel<T> {}
 
+// SAFETY: &Self only allows reading from the page table, which is safe to do from any thread.
+unsafe impl<T: Translation + Sync> Sync for PageTableWithLevel<T> {}
+
 impl<T: Translation> PageTableWithLevel<T> {
     /// Allocates a new, zeroed, appropriately-aligned page table with the given translation,
     /// returning both a pointer to it and its physical address.
-    fn new(translation: &T, level: usize) -> (Self, PhysicalAddress) {
+    fn new(translation: &mut T, level: usize) -> (Self, PhysicalAddress) {
         assert!(level <= LEAF_LEVEL);
         let (table, pa) = translation.allocate_table();
         (
@@ -536,7 +617,7 @@ impl<T: Translation> PageTableWithLevel<T> {
     /// Convert the descriptor in `entry` from a block mapping to a table mapping of
     /// the same range with the same attributes
     fn split_entry(
-        translation: &T,
+        translation: &mut T,
         chunk: &MemoryRegion,
         entry: &mut Descriptor,
         level: usize,
@@ -545,8 +626,10 @@ impl<T: Translation> PageTableWithLevel<T> {
         let old = *entry;
         let (mut subtable, subtable_pa) = Self::new(translation, level + 1);
         if let Some(old_flags) = old.flags() {
-            if !old_flags.contains(Attributes::TABLE_OR_PAGE) {
-                let old_pa = old.output_address();
+            let old_pa = old.output_address();
+            if !old_flags.contains(Attributes::TABLE_OR_PAGE)
+                && (!old_flags.is_empty() || old_pa.0 != 0)
+            {
                 // `old` was a block entry, so we need to split it.
                 // Recreate the entire block in the newly added table.
                 let a = align_down(chunk.0.start.0, granularity);
@@ -575,7 +658,7 @@ impl<T: Translation> PageTableWithLevel<T> {
     /// should be checked by the caller beforehand.
     fn map_range(
         &mut self,
-        translation: &T,
+        translation: &mut T,
         range: &MemoryRegion,
         mut pa: PhysicalAddress,
         flags: Attributes,
@@ -589,7 +672,7 @@ impl<T: Translation> PageTableWithLevel<T> {
 
             if level == LEAF_LEVEL {
                 // Put down a page mapping.
-                entry.set(pa, flags | Attributes::ACCESSED | Attributes::TABLE_OR_PAGE);
+                entry.set(pa, flags | Attributes::TABLE_OR_PAGE);
             } else if chunk.is_block(level)
                 && !entry.is_table_or_page()
                 && is_aligned(pa.0, granularity)
@@ -598,7 +681,7 @@ impl<T: Translation> PageTableWithLevel<T> {
                 // Rather than leak the entire subhierarchy, only put down
                 // a block mapping if the region is not already covered by
                 // a table mapping.
-                entry.set(pa, flags | Attributes::ACCESSED);
+                entry.set(pa, flags);
             } else {
                 let mut subtable = entry
                     .subtable(translation, level)
@@ -649,19 +732,25 @@ impl<T: Translation> PageTableWithLevel<T> {
 
     /// Frees the memory used by this pagetable and all subtables. It is not valid to access the
     /// page table after this.
-    fn free(&mut self, translation: &T) {
+    ///
+    /// # Safety
+    ///
+    /// The table and all its subtables must have been created by `PageTableWithLevel::new` with the
+    /// same `translation`.
+    unsafe fn free(&mut self, translation: &mut T) {
         // SAFETY: Safe because we know that the pointer is aligned, initialised and dereferencable,
         // and the PageTable won't be mutated while we are freeing it.
         let table = unsafe { self.table.as_ref() };
         for entry in table.entries {
             if let Some(mut subtable) = entry.subtable(translation, self.level) {
-                // Safe because the subtable was allocated by `PageTableWithLevel::new` with the
-                // global allocator and appropriate layout.
+                // Safe because our caller promised that all our subtables were created by
+                // `PageTableWithLevel::new` with the same `translation`.
                 subtable.free(translation);
             }
         }
-        // SAFETY: Safe because the table was allocated by `PageTableWithLevel::new` with the global
-        // allocator and appropriate layout.
+        // SAFETY: Safe because our caller promised that the table was created by
+        // `PageTableWithLevel::new` with `translation`, which then allocated it by calling
+        // `allocate_table` on `translation`.
         unsafe {
             // Actually free the memory used by the `PageTable`.
             translation.deallocate_table(self.table);
@@ -672,7 +761,7 @@ impl<T: Translation> PageTableWithLevel<T> {
     /// If the range is not aligned to block boundaries, block descriptors will be split up.
     fn modify_range<F>(
         &mut self,
-        translation: &T,
+        translation: &mut T,
         range: &MemoryRegion,
         f: &F,
     ) -> Result<(), MapError>
@@ -699,24 +788,19 @@ impl<T: Translation> PageTableWithLevel<T> {
         Ok(())
     }
 
-    /// Walks a range of page table entries and passes each one to a caller provided function
-    /// If the range is not aligned to block boundaries, it will be expanded.
-    fn walk_range<F>(
-        &self,
-        translation: &T,
-        range: &MemoryRegion,
-        f: &mut F,
-    ) -> Result<(), MapError>
+    /// Walks a range of page table entries and passes each one to a caller provided function.
+    /// If the function returns an error, the walk is terminated and the error value is passed on
+    fn visit_range<F, E>(&self, translation: &T, range: &MemoryRegion, f: &mut F) -> Result<(), E>
     where
-        F: FnMut(&MemoryRegion, &Descriptor, usize) -> Result<(), ()>,
+        F: FnMut(&MemoryRegion, &Descriptor, usize) -> Result<(), E>,
     {
         let level = self.level;
         for chunk in range.split(level) {
             let entry = self.get_entry(chunk.0.start);
             if let Some(subtable) = entry.subtable(translation, level) {
-                subtable.walk_range(translation, &chunk, f)?;
+                subtable.visit_range(translation, &chunk, f)?;
             } else {
-                f(&chunk, entry, level).map_err(|_| MapError::PteUpdateFault(*entry))?;
+                f(&chunk, entry, level)?;
             }
         }
         Ok(())
@@ -883,6 +967,8 @@ pub(crate) const fn is_aligned(value: usize, alignment: usize) -> bool {
 mod tests {
     use super::*;
     #[cfg(feature = "alloc")]
+    use crate::idmap::IdTranslation;
+    #[cfg(feature = "alloc")]
     use alloc::{format, string::ToString, vec, vec::Vec};
 
     #[cfg(feature = "alloc")]
@@ -1013,4 +1099,18 @@ mod tests {
             ]
         );
     }
+
+    #[cfg(feature = "alloc")]
+    #[test]
+    #[should_panic]
+    fn no_el2_ttbr1() {
+        RootTable::<IdTranslation>::new(IdTranslation, 1, TranslationRegime::El2, VaRange::Upper);
+    }
+
+    #[cfg(feature = "alloc")]
+    #[test]
+    #[should_panic]
+    fn no_el3_ttbr1() {
+        RootTable::<IdTranslation>::new(IdTranslation, 1, TranslationRegime::El3, VaRange::Upper);
+    }
 }
```

