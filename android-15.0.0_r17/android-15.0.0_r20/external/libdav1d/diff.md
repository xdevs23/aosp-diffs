```diff
diff --git a/Android.bp b/Android.bp
index 9a4b4cd..78b3891 100644
--- a/Android.bp
+++ b/Android.bp
@@ -129,6 +129,56 @@ libdav1d_tmpl_srcs = [
     "src/recon_tmpl.c",
 ]
 
+cc_library_static {
+    name: "libdav1d_dotprod_i8mm",
+    defaults: [
+        "libdav1d_defaults",
+    ],
+    vendor_available: true,
+    visibility: ["//visibility:private"],
+
+    cflags: [
+        "-Xclang -target-feature",
+        // This comment prevents bpfmt from sorting the flags incorrectly.
+        "-Xclang +dotprod",
+        // This comment prevents bpfmt from sorting the flags incorrectly.
+        "-Xclang -target-feature",
+        // This comment prevents bpfmt from sorting the flags incorrectly.
+        "-Xclang +i8mm",
+    ],
+
+    arch: {
+        arm64: {
+            srcs: [
+                "src/arm/64/mc_dotprod.S",
+            ],
+        },
+    },
+}
+
+cc_library_static {
+    name: "libdav1d_sve2",
+    defaults: [
+        "libdav1d_defaults",
+    ],
+    vendor_available: true,
+    visibility: ["//visibility:private"],
+
+    cflags: [
+        "-Xclang -target-feature",
+        // This comment prevents bpfmt from sorting the flags incorrectly.
+        "-Xclang +sve2",
+    ],
+
+    arch: {
+        arm64: {
+            srcs: [
+                "src/arm/64/mc16_sve.S",
+            ],
+        },
+    },
+}
+
 cc_library_static {
     name: "libdav1d_8bit",
     defaults: [
@@ -211,6 +261,7 @@ cc_library_static {
 
     srcs: [
         "src/cdf.c",
+        "src/ctx.c",
         "src/cpu.c",
         "src/data.c",
         "src/decode.c",
@@ -271,6 +322,8 @@ cc_library_static {
     whole_static_libs: [
         "libdav1d_8bit",
         "libdav1d_16bit",
+        "libdav1d_dotprod_i8mm",
+        "libdav1d_sve2",
     ],
 }
 
diff --git a/METADATA b/METADATA
index 67433d1..f747044 100644
--- a/METADATA
+++ b/METADATA
@@ -1,6 +1,6 @@
 # This project was upgraded with external_updater.
 # Usage: tools/external_updater/updater.sh update external/libdav1d
-# For more info, check https://cs.android.com/android/platform/superproject/+/main:tools/external_updater/README.md
+# For more info, check https://cs.android.com/android/platform/superproject/main/+/main:tools/external_updater/README.md
 
 name: "libdav1d"
 description: "An AV1 cross-platform decoder, open-source, and focused on speed and correctness."
@@ -8,13 +8,13 @@ third_party {
   license_type: NOTICE
   last_upgrade_date {
     year: 2024
-    month: 5
-    day: 28
+    month: 10
+    day: 17
   }
   homepage: "https://code.videolan.org/videolan/dav1d/"
   identifier {
     type: "Git"
     value: "https://code.videolan.org/videolan/dav1d.git"
-    version: "1.4.2"
+    version: "1.5.0"
   }
 }
diff --git a/NEWS b/NEWS
index 16825ff..691737d 100644
--- a/NEWS
+++ b/NEWS
@@ -1,3 +1,30 @@
+Changes for 1.5.0 'Sonic':
+--------------------------
+
+1.5.0 is a major release of dav1d, that:
+ - WARNING: we removed some of the SSE2 optimizations, so if you care about
+            systems without SSSE3, you should be careful when updating!
+ - Add Arm OpenBSD run-time CPU feature
+ - Optimize index offset calculations for decode_coefs
+ - picture: copy HDR10+ and T35 metadata only to visible frames
+ - SSSE3 new optimizations for 6-tap (8bit and hbd)
+ - AArch64/SVE: Add HBD subpel filters using 128-bit SVE2
+ - AArch64: Add USMMLA implempentation for 6-tap H/HV
+ - AArch64: Optimize Armv8.0 NEON for HBD horizontal filters and 6-tap filters
+ - Power9: Optimized ITX till 16x4.
+ - Loongarch: numerous optimizations
+ - RISC-V optimizations for pal, cdef_filter, ipred, mc_blend, mc_bdir, itx
+ - Allow playing videos in full-screen mode in dav1dplay
+
+
+Changes for 1.4.3 'Road Runner':
+--------------------------------
+
+1.4.3 is a small release focused on security issues
+ - AArch64: Fix potential out of bounds access in DotProd H/HV filters
+ - cli: Prevent buffer over-read
+
+
 Changes for 1.4.2 'Road Runner':
 --------------------------------
 
diff --git a/config/arm32/config.h b/config/arm32/config.h
index a3eb2dd..83b803e 100644
--- a/config/arm32/config.h
+++ b/config/arm32/config.h
@@ -39,18 +39,58 @@
 
 #define ENDIANNESS_BIG 0
 
+#define HAVE_ALIGNED_ALLOC 0
+
 #define HAVE_ASM 1
 
+#define HAVE_AS_ARCHEXT_DOTPROD_DIRECTIVE 0
+
+#define HAVE_AS_ARCHEXT_I8MM_DIRECTIVE 0
+
+#define HAVE_AS_ARCHEXT_SVE2_DIRECTIVE 0
+
+#define HAVE_AS_ARCHEXT_SVE_DIRECTIVE 0
+
+#define HAVE_AS_ARCH_DIRECTIVE 0
+
 #define HAVE_AS_FUNC 0
 
 #define HAVE_C11_GENERIC 1
 
 #define HAVE_CLOCK_GETTIME 1
 
+#define HAVE_DLSYM 0
+
+#define HAVE_DOTPROD 0
+
+#define HAVE_ELF_AUX_INFO 0
+
 #define HAVE_GETAUXVAL 1
 
+#define HAVE_I8MM 0
+
+#define HAVE_IO_H 0
+
+#define HAVE_MEMALIGN 1
+
 #define HAVE_POSIX_MEMALIGN 1
 
+#define HAVE_PTHREAD_GETAFFINITY_NP 0
+
+#define HAVE_PTHREAD_NP_H 0
+
+#define HAVE_PTHREAD_SETAFFINITY_NP 0
+
+#define HAVE_PTHREAD_SETNAME_NP 1
+
+#define HAVE_PTHREAD_SET_NAME_NP 0
+
+#define HAVE_SVE 0
+
+#define HAVE_SVE2 0
+
+#define HAVE_SYS_TYPES_H 1
+
 #define HAVE_UNISTD_H 1
 
 #define PIC 3
diff --git a/config/arm64/config.h b/config/arm64/config.h
index 86c8f15..4a5ede4 100644
--- a/config/arm64/config.h
+++ b/config/arm64/config.h
@@ -29,6 +29,8 @@
 
 #define ARCH_X86_64 0
 
+#define AS_ARCH_LEVEL armv8.2-a
+
 #define CONFIG_16BPC 1
 
 #define CONFIG_8BPC 1
@@ -39,15 +41,17 @@
 
 #define ENDIANNESS_BIG 0
 
+#define HAVE_ALIGNED_ALLOC 0
+
 #define HAVE_ASM 1
 
-#define HAVE_AS_ARCHEXT_DOTPROD_DIRECTIVE 0
+#define HAVE_AS_ARCHEXT_DOTPROD_DIRECTIVE 1
 
-#define HAVE_AS_ARCHEXT_I8MM_DIRECTIVE 0
+#define HAVE_AS_ARCHEXT_I8MM_DIRECTIVE 1
 
-#define HAVE_AS_ARCHEXT_SVE2_DIRECTIVE 0
+#define HAVE_AS_ARCHEXT_SVE2_DIRECTIVE 1
 
-#define HAVE_AS_ARCHEXT_SVE_DIRECTIVE 0
+#define HAVE_AS_ARCHEXT_SVE_DIRECTIVE 1
 
 #define HAVE_AS_ARCH_DIRECTIVE 0
 
@@ -57,17 +61,37 @@
 
 #define HAVE_CLOCK_GETTIME 1
 
-#define HAVE_DOTPROD 0
+#define HAVE_DLSYM 0
+
+#define HAVE_DOTPROD 1
+
+#define HAVE_ELF_AUX_INFO 0
 
 #define HAVE_GETAUXVAL 1
 
-#define HAVE_I8MM 0
+#define HAVE_I8MM 1
+
+#define HAVE_IO_H 0
+
+#define HAVE_MEMALIGN 1
 
 #define HAVE_POSIX_MEMALIGN 1
 
-#define HAVE_SVE 0
+#define HAVE_PTHREAD_GETAFFINITY_NP 0
+
+#define HAVE_PTHREAD_NP_H 0
+
+#define HAVE_PTHREAD_SETAFFINITY_NP 0
+
+#define HAVE_PTHREAD_SETNAME_NP 1
+
+#define HAVE_PTHREAD_SET_NAME_NP 0
+
+#define HAVE_SVE 1
+
+#define HAVE_SVE2 1
 
-#define HAVE_SVE2 0
+#define HAVE_SYS_TYPES_H 1
 
 #define HAVE_UNISTD_H 1
 
diff --git a/config/riscv64/config.h b/config/riscv64/config.h
index 88accca..38cfceb 100644
--- a/config/riscv64/config.h
+++ b/config/riscv64/config.h
@@ -39,14 +39,58 @@
 
 #define ENDIANNESS_BIG 0
 
+#define HAVE_ALIGNED_ALLOC 0
+
 #define HAVE_ASM 0
 
+#define HAVE_AS_ARCHEXT_DOTPROD_DIRECTIVE 0
+
+#define HAVE_AS_ARCHEXT_I8MM_DIRECTIVE 0
+
+#define HAVE_AS_ARCHEXT_SVE2_DIRECTIVE 0
+
+#define HAVE_AS_ARCHEXT_SVE_DIRECTIVE 0
+
+#define HAVE_AS_ARCH_DIRECTIVE 0
+
+#define HAVE_AS_FUNC 0
+
 #define HAVE_C11_GENERIC 1
 
 #define HAVE_CLOCK_GETTIME 1
 
+#define HAVE_DLSYM 0
+
+#define HAVE_DOTPROD 0
+
+#define HAVE_ELF_AUX_INFO 0
+
+#define HAVE_GETAUXVAL 0
+
+#define HAVE_I8MM 0
+
+#define HAVE_IO_H 0
+
+#define HAVE_MEMALIGN 1
+
 #define HAVE_POSIX_MEMALIGN 1
 
+#define HAVE_PTHREAD_GETAFFINITY_NP 0
+
+#define HAVE_PTHREAD_NP_H 0
+
+#define HAVE_PTHREAD_SETAFFINITY_NP 0
+
+#define HAVE_PTHREAD_SETNAME_NP 1
+
+#define HAVE_PTHREAD_SET_NAME_NP 0
+
+#define HAVE_SVE 0
+
+#define HAVE_SVE2 0
+
+#define HAVE_SYS_TYPES_H 1
+
 #define HAVE_UNISTD_H 1
 
 #define TRIM_DSP_FUNCTIONS 1
diff --git a/config/x86_32/config.h b/config/x86_32/config.h
index 790e15d..13421c4 100644
--- a/config/x86_32/config.h
+++ b/config/x86_32/config.h
@@ -39,14 +39,58 @@
 
 #define ENDIANNESS_BIG 0
 
+#define HAVE_ALIGNED_ALLOC 0
+
 #define HAVE_ASM 0
 
+#define HAVE_AS_ARCHEXT_DOTPROD_DIRECTIVE 0
+
+#define HAVE_AS_ARCHEXT_I8MM_DIRECTIVE 0
+
+#define HAVE_AS_ARCHEXT_SVE2_DIRECTIVE 0
+
+#define HAVE_AS_ARCHEXT_SVE_DIRECTIVE 0
+
+#define HAVE_AS_ARCH_DIRECTIVE 0
+
+#define HAVE_AS_FUNC 0
+
 #define HAVE_C11_GENERIC 1
 
 #define HAVE_CLOCK_GETTIME 1
 
+#define HAVE_DLSYM 0
+
+#define HAVE_DOTPROD 0
+
+#define HAVE_ELF_AUX_INFO 0
+
+#define HAVE_GETAUXVAL 0
+
+#define HAVE_I8MM 0
+
+#define HAVE_IO_H 0
+
+#define HAVE_MEMALIGN 1
+
 #define HAVE_POSIX_MEMALIGN 1
 
+#define HAVE_PTHREAD_GETAFFINITY_NP 0
+
+#define HAVE_PTHREAD_NP_H 0
+
+#define HAVE_PTHREAD_SETAFFINITY_NP 0
+
+#define HAVE_PTHREAD_SETNAME_NP 1
+
+#define HAVE_PTHREAD_SET_NAME_NP 0
+
+#define HAVE_SVE 0
+
+#define HAVE_SVE2 0
+
+#define HAVE_SYS_TYPES_H 1
+
 #define HAVE_UNISTD_H 1
 
 #define TRIM_DSP_FUNCTIONS 1
diff --git a/config/x86_64/config.h b/config/x86_64/config.h
index e34fcab..5921bae 100644
--- a/config/x86_64/config.h
+++ b/config/x86_64/config.h
@@ -39,14 +39,58 @@
 
 #define ENDIANNESS_BIG 0
 
+#define HAVE_ALIGNED_ALLOC 0
+
 #define HAVE_ASM 0
 
+#define HAVE_AS_ARCHEXT_DOTPROD_DIRECTIVE 0
+
+#define HAVE_AS_ARCHEXT_I8MM_DIRECTIVE 0
+
+#define HAVE_AS_ARCHEXT_SVE2_DIRECTIVE 0
+
+#define HAVE_AS_ARCHEXT_SVE_DIRECTIVE 0
+
+#define HAVE_AS_ARCH_DIRECTIVE 0
+
+#define HAVE_AS_FUNC 0
+
 #define HAVE_C11_GENERIC 1
 
 #define HAVE_CLOCK_GETTIME 1
 
+#define HAVE_DLSYM 0
+
+#define HAVE_DOTPROD 0
+
+#define HAVE_ELF_AUX_INFO 0
+
+#define HAVE_GETAUXVAL 0
+
+#define HAVE_I8MM 0
+
+#define HAVE_IO_H 0
+
+#define HAVE_MEMALIGN 1
+
 #define HAVE_POSIX_MEMALIGN 1
 
+#define HAVE_PTHREAD_GETAFFINITY_NP 0
+
+#define HAVE_PTHREAD_NP_H 0
+
+#define HAVE_PTHREAD_SETAFFINITY_NP 0
+
+#define HAVE_PTHREAD_SETNAME_NP 1
+
+#define HAVE_PTHREAD_SET_NAME_NP 0
+
+#define HAVE_SVE 0
+
+#define HAVE_SVE2 0
+
+#define HAVE_SYS_TYPES_H 1
+
 #define HAVE_UNISTD_H 1
 
 #define TRIM_DSP_FUNCTIONS 1
diff --git a/examples/dav1dplay.c b/examples/dav1dplay.c
index 9cca8e8..1f64944 100644
--- a/examples/dav1dplay.c
+++ b/examples/dav1dplay.c
@@ -120,6 +120,7 @@ static void dp_settings_print_usage(const char *const app,
             " --highquality:        enable high quality rendering\n"
             " --zerocopy/-z:        enable zero copy upload path\n"
             " --gpugrain/-g:        enable GPU grain synthesis\n"
+            " --fullscreen/-f:      enable full screen mode\n"
             " --version/-v:         print version and exit\n"
             " --renderer/-r:        select renderer backend (default: auto)\n");
     exit(1);
@@ -144,7 +145,7 @@ static void dp_rd_ctx_parse_args(Dav1dPlayRenderContext *rd_ctx,
     Dav1dSettings *lib_settings = &rd_ctx->lib_settings;
 
     // Short options
-    static const char short_opts[] = "i:vuzgr:";
+    static const char short_opts[] = "i:vuzgfr:";
 
     enum {
         ARG_THREADS = 256,
@@ -162,6 +163,7 @@ static void dp_rd_ctx_parse_args(Dav1dPlayRenderContext *rd_ctx,
         { "highquality",    0, NULL, ARG_HIGH_QUALITY },
         { "zerocopy",       0, NULL, 'z' },
         { "gpugrain",       0, NULL, 'g' },
+        { "fullscreen",     0, NULL, 'f'},
         { "renderer",       0, NULL, 'r'},
         { NULL,             0, NULL, 0 },
     };
@@ -186,6 +188,9 @@ static void dp_rd_ctx_parse_args(Dav1dPlayRenderContext *rd_ctx,
             case 'g':
                 settings->gpugrain = true;
                 break;
+            case 'f':
+                settings->fullscreen = true;
+                break;
             case 'r':
                 settings->renderer_name = optarg;
                 break;
@@ -240,35 +245,37 @@ static Dav1dPlayRenderContext *dp_rd_ctx_create(int argc, char **argv)
         return NULL;
     }
 
+    // Parse and validate arguments
+    dav1d_default_settings(&rd_ctx->lib_settings);
+    memset(&rd_ctx->settings, 0, sizeof(rd_ctx->settings));
+    dp_rd_ctx_parse_args(rd_ctx, argc, argv);
+
+    // Init SDL2 library
+    if (SDL_Init(SDL_INIT_VIDEO | SDL_INIT_TIMER) < 0) {
+        fprintf(stderr, "SDL_Init failed: %s\n", SDL_GetError());
+        goto fail;
+    }
+
     // Register a custom event to notify our SDL main thread
     // about new frames
     rd_ctx->event_types = SDL_RegisterEvents(3);
     if (rd_ctx->event_types == UINT32_MAX) {
         fprintf(stderr, "Failure to create custom SDL event types!\n");
-        free(rd_ctx);
-        return NULL;
+        goto fail;
     }
 
     rd_ctx->fifo = dp_fifo_create(5);
     if (rd_ctx->fifo == NULL) {
         fprintf(stderr, "Failed to create FIFO for output pictures!\n");
-        free(rd_ctx);
-        return NULL;
+        goto fail;
     }
 
     rd_ctx->lock = SDL_CreateMutex();
     if (rd_ctx->lock == NULL) {
         fprintf(stderr, "SDL_CreateMutex failed: %s\n", SDL_GetError());
-        dp_fifo_destroy(rd_ctx->fifo);
-        free(rd_ctx);
-        return NULL;
+        goto fail;
     }
 
-    // Parse and validate arguments
-    dav1d_default_settings(&rd_ctx->lib_settings);
-    memset(&rd_ctx->settings, 0, sizeof(rd_ctx->settings));
-    dp_rd_ctx_parse_args(rd_ctx, argc, argv);
-
     // Select renderer
     renderer_info = dp_get_renderer(rd_ctx->settings.renderer_name);
 
@@ -279,15 +286,21 @@ static Dav1dPlayRenderContext *dp_rd_ctx_create(int argc, char **argv)
         printf("Using %s renderer\n", renderer_info->name);
     }
 
-    rd_ctx->rd_priv = (renderer_info) ? renderer_info->create_renderer() : NULL;
+    rd_ctx->rd_priv = (renderer_info) ? renderer_info->create_renderer(&rd_ctx->settings) : NULL;
     if (rd_ctx->rd_priv == NULL) {
-        SDL_DestroyMutex(rd_ctx->lock);
-        dp_fifo_destroy(rd_ctx->fifo);
-        free(rd_ctx);
-        return NULL;
+        goto fail;
     }
 
     return rd_ctx;
+
+fail:
+    if (rd_ctx->lock)
+        SDL_DestroyMutex(rd_ctx->lock);
+    if (rd_ctx->fifo)
+        dp_fifo_destroy(rd_ctx->fifo);
+    free(rd_ctx);
+    SDL_Quit();
+    return NULL;
 }
 
 /**
@@ -662,10 +675,6 @@ int main(int argc, char **argv)
         return 1;
     }
 
-    // Init SDL2 library
-    if (SDL_Init(SDL_INIT_VIDEO | SDL_INIT_TIMER) < 0)
-        return 10;
-
     // Create render context
     Dav1dPlayRenderContext *rd_ctx = dp_rd_ctx_create(argc, argv);
     if (rd_ctx == NULL) {
@@ -711,9 +720,7 @@ int main(int argc, char **argv)
             if (e->type == SDL_QUIT) {
                 dp_rd_ctx_request_shutdown(rd_ctx);
                 dp_fifo_flush(rd_ctx->fifo, destroy_pic);
-                SDL_FlushEvent(rd_ctx->event_types + DAV1D_EVENT_NEW_FRAME);
-                SDL_FlushEvent(rd_ctx->event_types + DAV1D_EVENT_SEEK_FRAME);
-                num_frame_events = 0;
+                goto out;
             } else if (e->type == SDL_WINDOWEVENT) {
                 if (e->window.event == SDL_WINDOWEVENT_SIZE_CHANGED) {
                     // TODO: Handle window resizes
@@ -724,6 +731,10 @@ int main(int argc, char **argv)
                 SDL_KeyboardEvent *kbde = (SDL_KeyboardEvent *)e;
                 if (kbde->keysym.sym == SDLK_SPACE) {
                     dp_rd_ctx_toggle_pause(rd_ctx);
+                } else if (kbde->keysym.sym == SDLK_ESCAPE) {
+                    dp_rd_ctx_request_shutdown(rd_ctx);
+                    dp_fifo_flush(rd_ctx->fifo, destroy_pic);
+                    goto out;
                 } else if (kbde->keysym.sym == SDLK_LEFT ||
                            kbde->keysym.sym == SDLK_RIGHT)
                 {
@@ -776,5 +787,6 @@ out:;
     int decoder_ret = 0;
     SDL_WaitThread(decoder_thread, &decoder_ret);
     dp_rd_ctx_destroy(rd_ctx);
+    SDL_Quit();
     return decoder_ret;
 }
diff --git a/examples/dp_renderer.h b/examples/dp_renderer.h
index 354e140..513e2ad 100644
--- a/examples/dp_renderer.h
+++ b/examples/dp_renderer.h
@@ -30,22 +30,32 @@
 #include "dav1d/dav1d.h"
 
 #include <SDL.h>
-#ifdef HAVE_PLACEBO
+#if HAVE_PLACEBO
 # include <libplacebo/config.h>
 #endif
 
 // Check libplacebo Vulkan rendering
-#if defined(HAVE_VULKAN) && defined(SDL_VIDEO_VULKAN)
+#if HAVE_VULKAN && defined(SDL_VIDEO_VULKAN)
 # if defined(PL_HAVE_VULKAN) && PL_HAVE_VULKAN
-#  define HAVE_RENDERER_PLACEBO
-#  define HAVE_PLACEBO_VULKAN
+#  define HAVE_RENDERER_PLACEBO 1
+#  define HAVE_PLACEBO_VULKAN 1
 # endif
 #endif
 
 // Check libplacebo OpenGL rendering
 #if defined(PL_HAVE_OPENGL) && PL_HAVE_OPENGL
-# define HAVE_RENDERER_PLACEBO
-# define HAVE_PLACEBO_OPENGL
+# define HAVE_RENDERER_PLACEBO 1
+# define HAVE_PLACEBO_OPENGL 1
+#endif
+
+#ifndef HAVE_RENDERER_PLACEBO
+#define HAVE_RENDERER_PLACEBO 0
+#endif
+#ifndef HAVE_PLACEBO_VULKAN
+#define HAVE_PLACEBO_VULKAN 0
+#endif
+#ifndef HAVE_PLACEBO_OPENGL
+#define HAVE_PLACEBO_OPENGL 0
 #endif
 
 /**
@@ -61,6 +71,7 @@ typedef struct {
     int untimed;
     int zerocopy;
     int gpugrain;
+    int fullscreen;
 } Dav1dPlaySettings;
 
 #define WINDOW_WIDTH  910
@@ -82,7 +93,7 @@ typedef struct rdr_info
     // Cookie passed to the renderer implementation callbacks
     void *cookie;
     // Callback to create the renderer
-    void* (*create_renderer)(void);
+    void* (*create_renderer)(const Dav1dPlaySettings *settings);
     // Callback to destroy the renderer
     void (*destroy_renderer)(void *cookie);
     // Callback to the render function that renders a prevously sent frame
diff --git a/examples/dp_renderer_placebo.c b/examples/dp_renderer_placebo.c
index 4ab1415..972cc57 100644
--- a/examples/dp_renderer_placebo.c
+++ b/examples/dp_renderer_placebo.c
@@ -26,17 +26,17 @@
 
 #include "dp_renderer.h"
 
-#ifdef HAVE_RENDERER_PLACEBO
+#if HAVE_RENDERER_PLACEBO
 #include <assert.h>
 
 #include <libplacebo/renderer.h>
 #include <libplacebo/utils/dav1d.h>
 
-#ifdef HAVE_PLACEBO_VULKAN
+#if HAVE_PLACEBO_VULKAN
 # include <libplacebo/vulkan.h>
 # include <SDL_vulkan.h>
 #endif
-#ifdef HAVE_PLACEBO_OPENGL
+#if HAVE_PLACEBO_OPENGL
 # include <libplacebo/opengl.h>
 # include <SDL_opengl.h>
 #endif
@@ -53,7 +53,7 @@ typedef struct renderer_priv_ctx
     pl_log log;
     // Placebo renderer
     pl_renderer renderer;
-#ifdef HAVE_PLACEBO_VULKAN
+#if HAVE_PLACEBO_VULKAN
     // Placebo Vulkan handle
     pl_vulkan vk;
     // Placebo Vulkan instance
@@ -61,9 +61,11 @@ typedef struct renderer_priv_ctx
     // Vulkan surface
     VkSurfaceKHR surf;
 #endif
-#ifdef HAVE_PLACEBO_OPENGL
+#if HAVE_PLACEBO_OPENGL
     // Placebo OpenGL handle
     pl_opengl gl;
+    // SDL OpenGL context
+    SDL_GLContext gl_context;
 #endif
     // Placebo GPU
     pl_gpu gpu;
@@ -77,13 +79,18 @@ typedef struct renderer_priv_ctx
 } Dav1dPlayRendererPrivateContext;
 
 static Dav1dPlayRendererPrivateContext*
-    placebo_renderer_create_common(int window_flags)
+    placebo_renderer_create_common(const Dav1dPlaySettings *settings, int window_flags)
 {
+    if (settings->fullscreen)
+        window_flags |= SDL_WINDOW_FULLSCREEN_DESKTOP;
+
     // Create Window
     SDL_Window *sdlwin = dp_create_sdl_window(window_flags | SDL_WINDOW_RESIZABLE);
     if (sdlwin == NULL)
         return NULL;
 
+    SDL_ShowCursor(0);
+
     // Alloc
     Dav1dPlayRendererPrivateContext *const rd_priv_ctx =
         calloc(1, sizeof(Dav1dPlayRendererPrivateContext));
@@ -118,24 +125,25 @@ static Dav1dPlayRendererPrivateContext*
     return rd_priv_ctx;
 }
 
-#ifdef HAVE_PLACEBO_OPENGL
-static void *placebo_renderer_create_gl(void)
+#if HAVE_PLACEBO_OPENGL
+static void *placebo_renderer_create_gl(const Dav1dPlaySettings *settings)
 {
     SDL_Window *sdlwin = NULL;
     SDL_GL_SetAttribute(SDL_GL_CONTEXT_FLAGS, SDL_GL_CONTEXT_DEBUG_FLAG);
 
     // Common init
     Dav1dPlayRendererPrivateContext *rd_priv_ctx =
-        placebo_renderer_create_common(SDL_WINDOW_OPENGL);
+        placebo_renderer_create_common(settings, SDL_WINDOW_OPENGL);
 
     if (rd_priv_ctx == NULL)
         return NULL;
     sdlwin = rd_priv_ctx->win;
 
-    SDL_GLContext glcontext = SDL_GL_CreateContext(sdlwin);
-    SDL_GL_MakeCurrent(sdlwin, glcontext);
+    rd_priv_ctx->gl_context = SDL_GL_CreateContext(sdlwin);
+    SDL_GL_MakeCurrent(sdlwin, rd_priv_ctx->gl_context);
 
     rd_priv_ctx->gl = pl_opengl_create(rd_priv_ctx->log, pl_opengl_params(
+        .allow_software = true,
 #ifndef NDEBUG
         .debug = true,
 #endif
@@ -173,14 +181,14 @@ static void *placebo_renderer_create_gl(void)
 }
 #endif
 
-#ifdef HAVE_PLACEBO_VULKAN
-static void *placebo_renderer_create_vk(void)
+#if HAVE_PLACEBO_VULKAN
+static void *placebo_renderer_create_vk(const Dav1dPlaySettings *settings)
 {
     SDL_Window *sdlwin = NULL;
 
     // Common init
     Dav1dPlayRendererPrivateContext *rd_priv_ctx =
-        placebo_renderer_create_common(SDL_WINDOW_VULKAN);
+        placebo_renderer_create_common(settings, SDL_WINDOW_VULKAN);
 
     if (rd_priv_ctx == NULL)
         return NULL;
@@ -270,16 +278,18 @@ static void placebo_renderer_destroy(void *cookie)
     for (int i = 0; i < 3; i++)
         pl_tex_destroy(rd_priv_ctx->gpu, &(rd_priv_ctx->plane_tex[i]));
 
-#ifdef HAVE_PLACEBO_VULKAN
+#if HAVE_PLACEBO_VULKAN
     if (rd_priv_ctx->vk) {
         pl_vulkan_destroy(&(rd_priv_ctx->vk));
         vkDestroySurfaceKHR(rd_priv_ctx->vk_inst->instance, rd_priv_ctx->surf, NULL);
         pl_vk_inst_destroy(&(rd_priv_ctx->vk_inst));
     }
 #endif
-#ifdef HAVE_PLACEBO_OPENGL
+#if HAVE_PLACEBO_OPENGL
     if (rd_priv_ctx->gl)
         pl_opengl_destroy(&(rd_priv_ctx->gl));
+    if (rd_priv_ctx->gl_context)
+        SDL_GL_DeleteContext(rd_priv_ctx->gl_context);
 #endif
 
     SDL_DestroyWindow(rd_priv_ctx->win);
@@ -382,7 +392,7 @@ static void placebo_release_pic(Dav1dPicture *pic, void *cookie)
     SDL_UnlockMutex(rd_priv_ctx->lock);
 }
 
-#ifdef HAVE_PLACEBO_VULKAN
+#if HAVE_PLACEBO_VULKAN
 const Dav1dPlayRenderInfo rdr_placebo_vk = {
     .name = "placebo-vk",
     .create_renderer = placebo_renderer_create_vk,
@@ -397,7 +407,7 @@ const Dav1dPlayRenderInfo rdr_placebo_vk = {
 const Dav1dPlayRenderInfo rdr_placebo_vk = { NULL };
 #endif
 
-#ifdef HAVE_PLACEBO_OPENGL
+#if HAVE_PLACEBO_OPENGL
 const Dav1dPlayRenderInfo rdr_placebo_gl = {
     .name = "placebo-gl",
     .create_renderer = placebo_renderer_create_gl,
diff --git a/examples/dp_renderer_sdl.c b/examples/dp_renderer_sdl.c
index 735b066..39e6ac8 100644
--- a/examples/dp_renderer_sdl.c
+++ b/examples/dp_renderer_sdl.c
@@ -43,12 +43,18 @@ typedef struct renderer_priv_ctx
     SDL_Texture *tex;
 } Dav1dPlayRendererPrivateContext;
 
-static void *sdl_renderer_create(void)
+static void *sdl_renderer_create(const Dav1dPlaySettings *settings)
 {
-    SDL_Window *win = dp_create_sdl_window(0);
+    int window_flags = 0;
+    if (settings->fullscreen)
+        window_flags |= SDL_WINDOW_FULLSCREEN_DESKTOP;
+
+    SDL_Window *win = dp_create_sdl_window(window_flags);
     if (win == NULL)
         return NULL;
 
+    SDL_ShowCursor(0);
+
     // Alloc
     Dav1dPlayRendererPrivateContext *rd_priv_ctx = malloc(sizeof(Dav1dPlayRendererPrivateContext));
     if (rd_priv_ctx == NULL) {
@@ -79,7 +85,9 @@ static void sdl_renderer_destroy(void *cookie)
     Dav1dPlayRendererPrivateContext *rd_priv_ctx = cookie;
     assert(rd_priv_ctx != NULL);
 
+    SDL_DestroyTexture(rd_priv_ctx->tex);
     SDL_DestroyRenderer(rd_priv_ctx->renderer);
+    SDL_DestroyWindow(rd_priv_ctx->win);
     SDL_DestroyMutex(rd_priv_ctx->lock);
     free(rd_priv_ctx);
 }
@@ -142,6 +150,7 @@ static int sdl_update_texture(void *cookie, Dav1dPicture *dav1d_pic,
     if (texture == NULL) {
         texture = SDL_CreateTexture(rd_priv_ctx->renderer, SDL_PIXELFORMAT_IYUV,
             SDL_TEXTUREACCESS_STREAMING, width, height);
+        SDL_RenderSetLogicalSize(rd_priv_ctx->renderer, width, height);
     }
 
     SDL_UpdateYUVTexture(texture, NULL,
diff --git a/examples/meson.build b/examples/meson.build
index 2b2b8bd..adbf85b 100644
--- a/examples/meson.build
+++ b/examples/meson.build
@@ -48,19 +48,23 @@ if sdl2_dependency.found()
 
     placebo_dependency = dependency('libplacebo', version: '>= 4.160.0', required: false)
 
-    if placebo_dependency.found()
+    have_vulkan = false
+    have_placebo = placebo_dependency.found()
+    if have_placebo
         dav1dplay_deps += placebo_dependency
-        dav1dplay_cflags += '-DHAVE_PLACEBO'
 
         # If libplacebo is found, we might be able to use Vulkan
         # with it, in which case we need the Vulkan library too.
         vulkan_dependency = dependency('vulkan', required: false)
         if vulkan_dependency.found()
             dav1dplay_deps += vulkan_dependency
-            dav1dplay_cflags += '-DHAVE_VULKAN'
+            have_vulkan = true
         endif
     endif
 
+    dav1dplay_cflags += '-DHAVE_PLACEBO=' + (have_placebo ? '1' : '0')
+    dav1dplay_cflags += '-DHAVE_VULKAN=' + (have_vulkan ? '1' : '0')
+
     dav1dplay = executable('dav1dplay',
         dav1dplay_sources,
         rev_target,
diff --git a/include/common/attributes.h b/include/common/attributes.h
index cd058ab..c8758c1 100644
--- a/include/common/attributes.h
+++ b/include/common/attributes.h
@@ -189,9 +189,13 @@ static inline int clzll(const unsigned long long mask) {
 #ifndef static_assert
 #define CHECK_OFFSET(type, field, name) \
     struct check_##type##_##field { int x[(name == offsetof(type, field)) ? 1 : -1]; }
+#define CHECK_SIZE(type, size) \
+    struct check_##type##_size { int x[(size == sizeof(type)) ? 1 : -1]; }
 #else
 #define CHECK_OFFSET(type, field, name) \
     static_assert(name == offsetof(type, field), #field)
+#define CHECK_SIZE(type, size) \
+    static_assert(size == sizeof(type), #type)
 #endif
 
 #ifdef _MSC_VER
diff --git a/include/common/intops.h b/include/common/intops.h
index 2d21998..089da5e 100644
--- a/include/common/intops.h
+++ b/include/common/intops.h
@@ -65,11 +65,11 @@ static inline int apply_sign64(const int v, const int64_t s) {
 }
 
 static inline int ulog2(const unsigned v) {
-    return 31 - clz(v);
+    return 31 ^ clz(v);
 }
 
 static inline int u64log2(const uint64_t v) {
-    return 63 - clzll(v);
+    return 63 ^ clzll(v);
 }
 
 static inline unsigned inv_recenter(const unsigned r, const unsigned v) {
diff --git a/include/compat/getopt.h b/include/compat/getopt.h
index 930e002..ad59769 100644
--- a/include/compat/getopt.h
+++ b/include/compat/getopt.h
@@ -13,7 +13,9 @@
 #define __GETOPT_H__
 
 /* All the headers include this file. */
+#ifdef _WIN32
 #include <crtdefs.h>
+#endif
 
 #ifdef __cplusplus
 extern "C" {
diff --git a/include/dav1d/meson.build b/include/dav1d/meson.build
index 68faaf9..dfb69a1 100644
--- a/include/dav1d/meson.build
+++ b/include/dav1d/meson.build
@@ -22,24 +22,15 @@
 # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
-# installed version.h header generation
-version_h_data = configuration_data()
-version_h_data.set('DAV1D_API_VERSION_MAJOR', dav1d_api_version_major)
-version_h_data.set('DAV1D_API_VERSION_MINOR', dav1d_api_version_minor)
-version_h_data.set('DAV1D_API_VERSION_PATCH', dav1d_api_version_revision)
-version_h_target = configure_file(input: 'version.h.in',
-                                  output: 'version.h',
-                                  configuration: version_h_data)
-
 dav1d_api_headers = [
     'common.h',
     'data.h',
     'dav1d.h',
     'headers.h',
     'picture.h',
+    'version.h',
 ]
 
 # install headers
 install_headers(dav1d_api_headers,
-                version_h_target,
                 subdir : 'dav1d')
diff --git a/include/dav1d/version.h b/include/dav1d/version.h
index a6115fb..43df603 100644
--- a/include/dav1d/version.h
+++ b/include/dav1d/version.h
@@ -1,5 +1,5 @@
 /*
- * Copyright © 2019, VideoLAN and dav1d authors
+ * Copyright © 2019-2024, VideoLAN and dav1d authors
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
diff --git a/include/vcs_version.h b/include/vcs_version.h
index 30a3fbf..5838ece 100644
--- a/include/vcs_version.h
+++ b/include/vcs_version.h
@@ -1,2 +1,2 @@
 /* auto-generated, do not edit */
-#define DAV1D_VERSION "1.4.2-0-g805d9e5"
+#define DAV1D_VERSION "1.5.0-120-gc83cabaf"
diff --git a/meson.build b/meson.build
index f5010ac..798abc1 100644
--- a/meson.build
+++ b/meson.build
@@ -1,4 +1,4 @@
-# Copyright © 2018-2022, VideoLAN and dav1d authors
+# Copyright © 2018-2024, VideoLAN and dav1d authors
 # All rights reserved.
 #
 # Redistribution and use in source and binary forms, with or without
@@ -23,19 +23,13 @@
 # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 project('dav1d', ['c'],
-    version: '1.4.2',
+    version: '1.5.0',
     default_options: ['c_std=c99',
                       'warning_level=2',
                       'buildtype=release',
                       'b_ndebug=if-release'],
     meson_version: '>= 0.49.0')
 
-dav1d_soname_version       = '7.0.0'
-dav1d_api_version_array    = dav1d_soname_version.split('.')
-dav1d_api_version_major    = dav1d_api_version_array[0]
-dav1d_api_version_minor    = dav1d_api_version_array[1]
-dav1d_api_version_revision = dav1d_api_version_array[2]
-
 dav1d_src_root = meson.current_source_dir()
 cc = meson.get_compiler('c')
 
@@ -48,7 +42,18 @@ cdata_asm = configuration_data()
 # Include directories
 dav1d_inc_dirs = include_directories(['.', 'include/dav1d', 'include'])
 
-
+dav1d_api_version_major    = cc.get_define('DAV1D_API_VERSION_MAJOR',
+                                           prefix: '#include "dav1d/version.h"',
+                                           include_directories: dav1d_inc_dirs).strip()
+dav1d_api_version_minor    = cc.get_define('DAV1D_API_VERSION_MINOR',
+                                           prefix: '#include "dav1d/version.h"',
+                                           include_directories: dav1d_inc_dirs).strip()
+dav1d_api_version_revision = cc.get_define('DAV1D_API_VERSION_PATCH',
+                                           prefix: '#include "dav1d/version.h"',
+                                           include_directories: dav1d_inc_dirs).strip()
+dav1d_soname_version       = '@0@.@1@.@2@'.format(dav1d_api_version_major,
+                                                  dav1d_api_version_minor,
+                                                  dav1d_api_version_revision)
 
 #
 # Option handling
@@ -98,6 +103,10 @@ if host_machine.system() in ['linux', 'gnu', 'emscripten']
     add_project_arguments('-D_GNU_SOURCE', language: 'c')
 endif
 
+have_clock_gettime = false
+have_posix_memalign = false
+have_memalign = false
+have_aligned_alloc = false
 if host_machine.system() == 'windows'
     cdata.set('_WIN32_WINNT',           '0x0601')
     cdata.set('UNICODE',                1) # Define to 1 for Unicode (Wide Chars) APIs
@@ -145,20 +154,25 @@ else
 
     rt_dependency = []
     if cc.has_function('clock_gettime', prefix : '#include <time.h>', args : test_args)
-        cdata.set('HAVE_CLOCK_GETTIME', 1)
+        have_clock_gettime = true
     elif host_machine.system() not in ['darwin', 'ios', 'tvos']
         rt_dependency = cc.find_library('rt', required: false)
         if not cc.has_function('clock_gettime', prefix : '#include <time.h>', args : test_args, dependencies : rt_dependency)
             error('clock_gettime not found')
         endif
-        cdata.set('HAVE_CLOCK_GETTIME', 1)
+        have_clock_gettime = true
     endif
 
-    if cc.has_function('posix_memalign', prefix : '#include <stdlib.h>', args : test_args)
-        cdata.set('HAVE_POSIX_MEMALIGN', 1)
-    endif
+    have_posix_memalign = cc.has_function('posix_memalign', prefix : '#include <stdlib.h>', args : test_args)
+    have_memalign = cc.has_function('memalign', prefix : '#include <malloc.h>', args : test_args)
+    have_aligned_alloc = cc.has_function('aligned_alloc', prefix : '#include <stdlib.h>', args : test_args)
 endif
 
+cdata.set10('HAVE_CLOCK_GETTIME', have_clock_gettime)
+cdata.set10('HAVE_POSIX_MEMALIGN', have_posix_memalign)
+cdata.set10('HAVE_MEMALIGN', have_memalign)
+cdata.set10('HAVE_ALIGNED_ALLOC', have_aligned_alloc)
+
 # check for fseeko on android. It is not always available if _FILE_OFFSET_BITS is defined to 64
 have_fseeko = true
 if host_machine.system() == 'android'
@@ -175,12 +189,12 @@ if host_machine.system() == 'android'
 endif
 
 libdl_dependency = []
+have_dlsym = false
 if host_machine.system() == 'linux'
     libdl_dependency = cc.find_library('dl', required : false)
-    if cc.has_function('dlsym', prefix : '#include <dlfcn.h>', args : test_args, dependencies : libdl_dependency)
-        cdata.set('HAVE_DLSYM', 1)
-    endif
+    have_dlsym = cc.has_function('dlsym', prefix : '#include <dlfcn.h>', args : test_args, dependencies : libdl_dependency)
 endif
+cdata.set10('HAVE_DLSYM', have_dlsym)
 
 libm_dependency = cc.find_library('m', required: false)
 
@@ -209,19 +223,13 @@ if host_machine.cpu_family().startswith('wasm')
     stdatomic_dependencies += thread_dependency.partial_dependency(compile_args: true)
 endif
 
-if cc.check_header('unistd.h')
-    cdata.set('HAVE_UNISTD_H', 1)
-endif
-
-if cc.check_header('io.h')
-    cdata.set('HAVE_IO_H', 1)
-endif
-
-if cc.check_header('pthread_np.h')
-    cdata.set('HAVE_PTHREAD_NP_H', 1)
-    test_args += '-DHAVE_PTHREAD_NP_H'
-endif
+cdata.set10('HAVE_SYS_TYPES_H', cc.check_header('sys/types.h'))
+cdata.set10('HAVE_UNISTD_H', cc.check_header('unistd.h'))
+cdata.set10('HAVE_IO_H', cc.check_header('io.h'))
 
+have_pthread_np = cc.check_header('pthread_np.h')
+cdata.set10('HAVE_PTHREAD_NP_H', have_pthread_np)
+test_args += '-DHAVE_PTHREAD_NP_H=' + (have_pthread_np ? '1' : '0')
 
 # Function checks
 
@@ -234,35 +242,32 @@ else
     getopt_dependency = []
 endif
 
+have_getauxval = false
+have_elf_aux_info = false
 if (host_machine.cpu_family() == 'aarch64' or
     host_machine.cpu_family().startswith('arm') or
     host_machine.cpu_family().startswith('loongarch') or
     host_machine.cpu() == 'ppc64le' or
     host_machine.cpu_family().startswith('riscv'))
-    if cc.has_function('getauxval', prefix : '#include <sys/auxv.h>', args : test_args)
-        cdata.set('HAVE_GETAUXVAL', 1)
-    endif
-    if cc.has_function('elf_aux_info', prefix : '#include <sys/auxv.h>', args : test_args)
-        cdata.set('HAVE_ELF_AUX_INFO', 1)
-    endif
+    have_getauxval = cc.has_function('getauxval', prefix : '#include <sys/auxv.h>', args : test_args)
+    have_elf_aux_info = cc.has_function('elf_aux_info', prefix : '#include <sys/auxv.h>', args : test_args)
 endif
 
+cdata.set10('HAVE_GETAUXVAL', have_getauxval)
+cdata.set10('HAVE_ELF_AUX_INFO', have_elf_aux_info)
+
 pthread_np_prefix = '''
 #include <pthread.h>
-#ifdef HAVE_PTHREAD_NP_H
+#if HAVE_PTHREAD_NP_H
 #include <pthread_np.h>
 #endif
 '''
-if cc.has_function('pthread_getaffinity_np', prefix : pthread_np_prefix, args : test_args, dependencies : thread_dependency)
-    cdata.set('HAVE_PTHREAD_GETAFFINITY_NP', 1)
-endif
-if cc.has_function('pthread_setaffinity_np', prefix : pthread_np_prefix, args : test_args, dependencies : thread_dependency)
-    cdata.set('HAVE_PTHREAD_SETAFFINITY_NP', 1)
-endif
+cdata.set10('HAVE_PTHREAD_GETAFFINITY_NP', cc.has_function('pthread_getaffinity_np', prefix : pthread_np_prefix, args : test_args, dependencies : thread_dependency))
+cdata.set10('HAVE_PTHREAD_SETAFFINITY_NP', cc.has_function('pthread_setaffinity_np', prefix : pthread_np_prefix, args : test_args, dependencies : thread_dependency))
+cdata.set10('HAVE_PTHREAD_SETNAME_NP', cc.has_function('pthread_setname_np', prefix : pthread_np_prefix, args : test_args, dependencies : thread_dependency))
+cdata.set10('HAVE_PTHREAD_SET_NAME_NP', cc.has_function('pthread_set_name_np', prefix : pthread_np_prefix, args : test_args, dependencies : thread_dependency))
 
-if cc.compiles('int x = _Generic(0, default: 0);', name: '_Generic', args: test_args)
-    cdata.set('HAVE_C11_GENERIC', 1)
-endif
+cdata.set10('HAVE_C11_GENERIC', cc.compiles('int x = _Generic(0, default: 0);', name: '_Generic', args: test_args))
 
 # Compiler flag tests
 
@@ -343,6 +348,17 @@ endif
 
 cdata.set10('ARCH_AARCH64', host_machine.cpu_family() == 'aarch64' or host_machine.cpu() == 'arm64')
 cdata.set10('ARCH_ARM',     host_machine.cpu_family().startswith('arm') and host_machine.cpu() != 'arm64')
+
+have_as_func = false
+have_as_arch = false
+aarch64_extensions = {
+    'dotprod': 'udot v0.4s, v0.16b, v0.16b',
+    'i8mm':    'usdot v0.4s, v0.16b, v0.16b',
+    'sve':     'whilelt p0.s, x0, x1',
+    'sve2':    'sqrdmulh z0.s, z0.s, z0.s',
+}
+supported_aarch64_archexts = []
+supported_aarch64_instructions = []
 if (is_asm_enabled and
     (host_machine.cpu_family() == 'aarch64' or
      host_machine.cpu_family().startswith('arm')))
@@ -353,7 +369,6 @@ if (is_asm_enabled and
 );
 '''
     have_as_func = cc.compiles(as_func_code)
-    cdata.set10('HAVE_AS_FUNC', have_as_func)
 
     # fedora package build infrastructure uses a gcc specs file to enable
     # '-fPIE' by default. The chosen way only adds '-fPIE' to the C compiler
@@ -374,7 +389,6 @@ if (is_asm_enabled and
 
     if host_machine.cpu_family() == 'aarch64'
         have_as_arch = cc.compiles('''__asm__ (".arch armv8-a");''')
-        cdata.set10('HAVE_AS_ARCH_DIRECTIVE', have_as_arch)
         as_arch_str = ''
         if have_as_arch
             as_arch_level = 'armv8-a'
@@ -403,13 +417,7 @@ if (is_asm_enabled and
             cdata.set('AS_ARCH_LEVEL', as_arch_level)
             as_arch_str = '".arch ' + as_arch_level + '\\n"'
         endif
-        extensions = {
-            'dotprod': 'udot v0.4s, v0.16b, v0.16b',
-            'i8mm':    'usdot v0.4s, v0.16b, v0.16b',
-            'sve':     'whilelt p0.s, x0, x1',
-            'sve2':    'sqrdmulh z0.s, z0.s, z0.s',
-        }
-        foreach name, instr : extensions
+        foreach name, instr : aarch64_extensions
             # Test for support for the various extensions. First test if
             # the assembler supports the .arch_extension directive for
             # enabling/disabling the extension, then separately check whether
@@ -420,19 +428,27 @@ if (is_asm_enabled and
             code += '".arch_extension ' + name + '\\n"'
             code += ');'
             supports_archext = cc.compiles(code)
-            cdata.set10('HAVE_AS_ARCHEXT_' + name.to_upper() + '_DIRECTIVE', supports_archext)
             code = '__asm__ (' + as_arch_str
             if supports_archext
+                supported_aarch64_archexts += name
                 code += '".arch_extension ' + name + '\\n"'
             endif
             code += '"' + instr + '\\n"'
             code += ');'
-            supports_instr = cc.compiles(code, name: name.to_upper())
-            cdata.set10('HAVE_' + name.to_upper(), supports_instr)
+            if cc.compiles(code, name: name.to_upper())
+                supported_aarch64_instructions += name
+            endif
         endforeach
     endif
 endif
 
+cdata.set10('HAVE_AS_FUNC', have_as_func)
+cdata.set10('HAVE_AS_ARCH_DIRECTIVE', have_as_arch)
+foreach name, _ : aarch64_extensions
+    cdata.set10('HAVE_AS_ARCHEXT_' + name.to_upper() + '_DIRECTIVE', name in supported_aarch64_archexts)
+    cdata.set10('HAVE_' + name.to_upper(), name in supported_aarch64_instructions)
+endforeach
+
 cdata.set10('ARCH_X86', host_machine.cpu_family().startswith('x86'))
 cdata.set10('ARCH_X86_64', host_machine.cpu_family() == 'x86_64')
 cdata.set10('ARCH_X86_32', host_machine.cpu_family() == 'x86')
diff --git a/package/crossfiles/arm64-iPhoneOS.meson b/package/crossfiles/arm64-iPhoneOS.meson
new file mode 100644
index 0000000..2031227
--- /dev/null
+++ b/package/crossfiles/arm64-iPhoneOS.meson
@@ -0,0 +1,27 @@
+[binaries]
+c = ['clang', '-arch', 'arm64', '-isysroot', '/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk']
+cpp = ['clang++', '-arch', 'arm64', '-isysroot', '/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk']
+objc = ['clang', '-arch', 'arm64', '-isysroot', '/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk']
+objcpp = ['clang++', '-arch', 'arm64', '-isysroot', '/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk']
+ar = 'ar'
+strip = 'strip'
+
+[built-in options]
+c_args = ['-miphoneos-version-min=11.0']
+cpp_args = ['-miphoneos-version-min=11.0']
+c_link_args = ['-miphoneos-version-min=11.0']
+cpp_link_args = ['-miphoneos-version-min=11.0']
+objc_args = ['-miphoneos-version-min=11.0']
+objcpp_args = ['-miphoneos-version-min=11.0']
+
+[properties]
+root = '/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer'
+needs_exe_wrapper = true
+
+[host_machine]
+system = 'darwin'
+subsystem = 'ios'
+kernel = 'xnu'
+cpu_family = 'aarch64'
+cpu = 'aarch64'
+endian = 'little'
diff --git a/package/crossfiles/x86_64-iPhoneSimulator.meson b/package/crossfiles/x86_64-iPhoneSimulator.meson
new file mode 100644
index 0000000..fb8a24c
--- /dev/null
+++ b/package/crossfiles/x86_64-iPhoneSimulator.meson
@@ -0,0 +1,27 @@
+[binaries]
+c = ['clang', '-arch', 'x86_64', '-isysroot', '/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator.sdk']
+cpp = ['clang++', '-arch', 'x86_64', '-isysroot', '/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator.sdk']
+objc = ['clang', '-arch', 'x86_64', '-isysroot', '/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator.sdk']
+objcpp = ['clang++', '-arch', 'x86_64', '-isysroot', '/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator.sdk']
+ar = 'ar'
+strip = 'strip'
+
+[built-in options]
+c_args = ['-miphoneos-version-min=11.0']
+cpp_args = ['-miphoneos-version-min=11.0']
+c_link_args = ['-miphoneos-version-min=11.0']
+cpp_link_args = ['-miphoneos-version-min=11.0']
+objc_args = ['-miphoneos-version-min=11.0']
+objcpp_args = ['-miphoneos-version-min=11.0']
+
+[properties]
+root = '/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer'
+needs_exe_wrapper = true
+
+[host_machine]
+system = 'darwin'
+subsystem = 'ios-simulator'
+kernel = 'xnu'
+cpu_family = 'x86_64'
+cpu = 'x86_64'
+endian = 'little'
diff --git a/src/arm/32/util.S b/src/arm/32/util.S
index c3710d3..38d63f8 100644
--- a/src/arm/32/util.S
+++ b/src/arm/32/util.S
@@ -31,18 +31,36 @@
 
 #include "config.h"
 #include "src/arm/asm.S"
+#include "src/arm/arm-arch.h"
+
+.macro v4bx rd
+#if __ARM_ARCH >= 5 || defined(__ARM_ARCH_4T__)
+        bx              \rd
+#else
+        mov             pc, \rd
+#endif
+.endm
+
+.macro v4blx rd
+#if __ARM_ARCH >= 5
+        blx             \rd
+#else
+        mov             lr,  pc
+        v4bx            \rd
+#endif
+.endm
 
 .macro movrel_local rd, val, offset=0
-#if defined(PIC)
+#if (__ARM_ARCH >= 7 || defined(__ARM_ARCH_6T2__)) && !defined(PIC)
+        movw            \rd, #:lower16:\val+\offset
+        movt            \rd, #:upper16:\val+\offset
+#else
         ldr             \rd,  90001f
         b               90002f
 90001:
         .word           \val + \offset - (90002f + 8 - 4 * CONFIG_THUMB)
 90002:
         add             \rd,  \rd,  pc
-#else
-        movw            \rd, #:lower16:\val+\offset
-        movt            \rd, #:upper16:\val+\offset
 #endif
 .endm
 
diff --git a/src/arm/64/filmgrain.S b/src/arm/64/filmgrain.S
index aa7f18b..864ceba 100644
--- a/src/arm/64/filmgrain.S
+++ b/src/arm/64/filmgrain.S
@@ -884,12 +884,12 @@ function generate_grain_\type\()_8bpc_neon, export=1
 .else
         add             x4,  x1,  #FGD_AR_COEFFS_UV
 .endif
-        adr             x16, L(gen_grain_\type\()_tbl)
+        movrel          x16, gen_grain_\type\()_tbl
         ldr             w17, [x1, #FGD_AR_COEFF_LAG]
         add             w9,  w9,  #4
-        ldrh            w17, [x16, w17, uxtw #1]
+        ldrsw           x17, [x16, w17, uxtw #2]
         dup             v31.8h,  w9    // 4 + data->grain_scale_shift
-        sub             x16, x16, w17, uxtw
+        add             x16, x16, x17
         neg             v31.8h,  v31.8h
 
 .ifc \type, uv_444
@@ -1075,13 +1075,14 @@ L(generate_grain_\type\()_lag3):
         ldp             x30, x19, [sp], #96
         AARCH64_VALIDATE_LINK_REGISTER
         ret
-
-L(gen_grain_\type\()_tbl):
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag0)
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag1)
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag2)
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag3)
 endfunc
+
+jumptable gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag0) - gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag1) - gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag2) - gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag3) - gen_grain_\type\()_tbl
+endjumptable
 .endm
 
 gen_grain_82 y
@@ -1118,12 +1119,12 @@ function generate_grain_\type\()_8bpc_neon, export=1
         ldr             w2,  [x1, #FGD_SEED]
         ldr             w9,  [x1, #FGD_GRAIN_SCALE_SHIFT]
         add             x4,  x1,  #FGD_AR_COEFFS_UV
-        adr             x16, L(gen_grain_\type\()_tbl)
+        movrel          x16, gen_grain_\type\()_tbl
         ldr             w17, [x1, #FGD_AR_COEFF_LAG]
         add             w9,  w9,  #4
-        ldrh            w17, [x16, w17, uxtw #1]
+        ldrsw           x17, [x16, w17, uxtw #2]
         dup             v31.8h,  w9    // 4 + data->grain_scale_shift
-        sub             x16, x16, w17, uxtw
+        add             x16, x16, x17
         neg             v31.8h,  v31.8h
 
         cmp             w13, #0
@@ -1272,13 +1273,14 @@ L(generate_grain_\type\()_lag3):
         ldp             x30, x19, [sp], #96
         AARCH64_VALIDATE_LINK_REGISTER
         ret
-
-L(gen_grain_\type\()_tbl):
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag0)
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag1)
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag2)
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag3)
 endfunc
+
+jumptable gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag0) - gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag1) - gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag2) - gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag3) - gen_grain_\type\()_tbl
+endjumptable
 .endm
 
 gen_grain_44 uv_420
@@ -1407,18 +1409,18 @@ function fgy_32x32_8bpc_neon, export=1
         add_offset      x5,  w6,  x10, x5,  x9
 
         ldr             w11, [sp, #24]         // type
-        adr             x13, L(fgy_loop_tbl)
+        movrel          x13, fgy_loop_tbl
 
         add             x4,  x12, #32          // grain_lut += FG_BLOCK_SIZE * bx
         add             x6,  x14, x9,  lsl #5  // grain_lut += grain_stride * FG_BLOCK_SIZE * by
 
         tst             w11, #1
-        ldrh            w11, [x13, w11, uxtw #1]
+        ldrsw           x11, [x13, w11, uxtw #2]
 
         add             x8,  x16, x9,  lsl #5  // grain_lut += grain_stride * FG_BLOCK_SIZE * by
         add             x8,  x8,  #32          // grain_lut += FG_BLOCK_SIZE * bx
 
-        sub             x11, x13, w11, uxtw
+        add             x11, x13, x11
 
         b.eq            1f
         // y overlap
@@ -1555,14 +1557,15 @@ L(loop_\ox\oy):
         fgy             0, 1
         fgy             1, 0
         fgy             1, 1
-
-L(fgy_loop_tbl):
-        .hword L(fgy_loop_tbl) - L(loop_00)
-        .hword L(fgy_loop_tbl) - L(loop_01)
-        .hword L(fgy_loop_tbl) - L(loop_10)
-        .hword L(fgy_loop_tbl) - L(loop_11)
 endfunc
 
+jumptable fgy_loop_tbl
+        .word L(loop_00) - fgy_loop_tbl
+        .word L(loop_01) - fgy_loop_tbl
+        .word L(loop_10) - fgy_loop_tbl
+        .word L(loop_11) - fgy_loop_tbl
+endjumptable
+
 // void dav1d_fguv_32x32_420_8bpc_neon(pixel *const dst,
 //                                     const pixel *const src,
 //                                     const ptrdiff_t stride,
@@ -1646,11 +1649,11 @@ function fguv_32x32_\layout\()_8bpc_neon, export=1
         ldr             w13, [sp, #64]         // type
 
         movrel          x16, overlap_coeffs_\sx
-        adr             x14, L(fguv_loop_sx\sx\()_tbl)
+        movrel          x14, fguv_loop_sx\sx\()_tbl
 
         ld1             {v27.8b, v28.8b}, [x16] // overlap_coeffs
         tst             w13, #1
-        ldrh            w13, [x14, w13, uxtw #1]
+        ldrsw           x13, [x14, w13, uxtw #2]
 
         b.eq            1f
         // y overlap
@@ -1658,7 +1661,7 @@ function fguv_32x32_\layout\()_8bpc_neon, export=1
         mov             w9,  #(2 >> \sy)
 
 1:
-        sub             x13, x14, w13, uxtw
+        add             x13, x14, x13
 
 .if \sy
         movi            v25.16b, #23
@@ -1848,18 +1851,19 @@ L(fguv_loop_sx0_csfl\csfl\()_\ox\oy):
         ldr             x30,      [sp], #32
         AARCH64_VALIDATE_LINK_REGISTER
         ret
-
-L(fguv_loop_sx0_tbl):
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl0_00)
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl0_01)
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl0_10)
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl0_11)
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl1_00)
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl1_01)
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl1_10)
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl1_11)
 endfunc
 
+jumptable fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl0_00) - fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl0_01) - fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl0_10) - fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl0_11) - fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl1_00) - fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl1_01) - fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl1_10) - fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl1_11) - fguv_loop_sx0_tbl
+endjumptable
+
 function fguv_loop_sx1_neon
 .macro fguv_loop_sx1 csfl, ox, oy
 L(fguv_loop_sx1_csfl\csfl\()_\ox\oy):
@@ -1997,14 +2001,15 @@ L(fguv_loop_sx1_csfl\csfl\()_\ox\oy):
         ldr             x30,      [sp], #32
         AARCH64_VALIDATE_LINK_REGISTER
         ret
-
-L(fguv_loop_sx1_tbl):
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl0_00)
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl0_01)
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl0_10)
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl0_11)
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl1_00)
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl1_01)
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl1_10)
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl1_11)
 endfunc
+
+jumptable fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl0_00) - fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl0_01) - fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl0_10) - fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl0_11) - fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl1_00) - fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl1_01) - fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl1_10) - fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl1_11) - fguv_loop_sx1_tbl
+endjumptable
diff --git a/src/arm/64/filmgrain16.S b/src/arm/64/filmgrain16.S
index 75252ac..aa6b75b 100644
--- a/src/arm/64/filmgrain16.S
+++ b/src/arm/64/filmgrain16.S
@@ -740,12 +740,12 @@ function generate_grain_\type\()_16bpc_neon, export=1
         add             x4,  x1,  #FGD_AR_COEFFS_UV
 .endif
         add             w9,  w9,  w15 // grain_scale_shift - bitdepth_min_8
-        adr             x16, L(gen_grain_\type\()_tbl)
+        movrel          x16, gen_grain_\type\()_tbl
         ldr             w17, [x1, #FGD_AR_COEFF_LAG]
         add             w9,  w9,  #4
-        ldrh            w17, [x16, w17, uxtw #1]
+        ldrsw           x17, [x16, w17, uxtw #2]
         dup             v31.8h,  w9    // 4 - bitdepth_min_8 + data->grain_scale_shift
-        sub             x16, x16, w17, uxtw
+        add             x16, x16, x17
         neg             v31.8h,  v31.8h
 
 .ifc \type, uv_444
@@ -945,13 +945,14 @@ L(generate_grain_\type\()_lag3):
         ldp             x30, x19, [sp], #96
         AARCH64_VALIDATE_LINK_REGISTER
         ret
-
-L(gen_grain_\type\()_tbl):
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag0)
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag1)
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag2)
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag3)
 endfunc
+
+jumptable gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag0) - gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag1) - gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag2) - gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag3) - gen_grain_\type\()_tbl
+endjumptable
 .endm
 
 gen_grain_82 y
@@ -991,12 +992,12 @@ function generate_grain_\type\()_16bpc_neon, export=1
         ldr             w9,  [x1, #FGD_GRAIN_SCALE_SHIFT]
         add             x4,  x1,  #FGD_AR_COEFFS_UV
         add             w9,  w9,  w15 // grain_scale_shift - bitdepth_min_8
-        adr             x16, L(gen_grain_\type\()_tbl)
+        movrel          x16, gen_grain_\type\()_tbl
         ldr             w17, [x1, #FGD_AR_COEFF_LAG]
         add             w9,  w9,  #4
-        ldrh            w17, [x16, w17, uxtw #1]
+        ldrsw           x17, [x16, w17, uxtw #2]
         dup             v31.8h,  w9    // 4 - bitdepth_min_8 + data->grain_scale_shift
-        sub             x16, x16, w17, uxtw
+        add             x16, x16, x17
         neg             v31.8h,  v31.8h
 
         cmp             w13, #0
@@ -1155,13 +1156,14 @@ L(generate_grain_\type\()_lag3):
         ldp             x30, x19, [sp], #96
         AARCH64_VALIDATE_LINK_REGISTER
         ret
-
-L(gen_grain_\type\()_tbl):
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag0)
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag1)
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag2)
-        .hword L(gen_grain_\type\()_tbl) - L(generate_grain_\type\()_lag3)
 endfunc
+
+jumptable gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag0) - gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag1) - gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag2) - gen_grain_\type\()_tbl
+        .word L(generate_grain_\type\()_lag3) - gen_grain_\type\()_tbl
+endjumptable
 .endm
 
 gen_grain_44 uv_420
@@ -1306,18 +1308,18 @@ function fgy_32x32_16bpc_neon, export=1
         add_offset      x5,  w6,  x10, x5,  x9
 
         ldr             w11, [sp, #88]         // type
-        adr             x13, L(fgy_loop_tbl)
+        movrel          x13, fgy_loop_tbl
 
         add             x4,  x12, #32*2        // grain_lut += FG_BLOCK_SIZE * bx
         add             x6,  x14, x9,  lsl #5  // grain_lut += grain_stride * FG_BLOCK_SIZE * by
 
         tst             w11, #1
-        ldrh            w11, [x13, w11, uxtw #1]
+        ldrsw           x11, [x13, w11, uxtw #2]
 
         add             x8,  x16, x9,  lsl #5  // grain_lut += grain_stride * FG_BLOCK_SIZE * by
         add             x8,  x8,  #32*2        // grain_lut += FG_BLOCK_SIZE * bx
 
-        sub             x11, x13, w11, uxtw
+        add             x11, x13, x11
 
         b.eq            1f
         // y overlap
@@ -1480,14 +1482,15 @@ L(loop_\ox\oy):
         fgy             0, 1
         fgy             1, 0
         fgy             1, 1
-
-L(fgy_loop_tbl):
-        .hword L(fgy_loop_tbl) - L(loop_00)
-        .hword L(fgy_loop_tbl) - L(loop_01)
-        .hword L(fgy_loop_tbl) - L(loop_10)
-        .hword L(fgy_loop_tbl) - L(loop_11)
 endfunc
 
+jumptable fgy_loop_tbl
+        .word L(loop_00) - fgy_loop_tbl
+        .word L(loop_01) - fgy_loop_tbl
+        .word L(loop_10) - fgy_loop_tbl
+        .word L(loop_11) - fgy_loop_tbl
+endjumptable
+
 // void dav1d_fguv_32x32_420_16bpc_neon(pixel *const dst,
 //                                      const pixel *const src,
 //                                      const ptrdiff_t stride,
@@ -1589,11 +1592,11 @@ function fguv_32x32_\layout\()_16bpc_neon, export=1
         ldr             w13, [sp, #112]        // type
 
         movrel          x16, overlap_coeffs_\sx
-        adr             x14, L(fguv_loop_sx\sx\()_tbl)
+        movrel          x14, fguv_loop_sx\sx\()_tbl
 
         ld1             {v27.4h, v28.4h}, [x16] // overlap_coeffs
         tst             w13, #1
-        ldrh            w13, [x14, w13, uxtw #1]
+        ldrsw           x13, [x14, w13, uxtw #2]
 
         b.eq            1f
         // y overlap
@@ -1601,7 +1604,7 @@ function fguv_32x32_\layout\()_16bpc_neon, export=1
         mov             w9,  #(2 >> \sy)
 
 1:
-        sub             x13, x14, w13, uxtw
+        add             x13, x14, x13
 
 .if \sy
         movi            v25.8h,  #23
@@ -1818,18 +1821,19 @@ L(fguv_loop_sx0_csfl\csfl\()_\ox\oy):
         ldr             x30,      [sp], #80
         AARCH64_VALIDATE_LINK_REGISTER
         ret
-
-L(fguv_loop_sx0_tbl):
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl0_00)
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl0_01)
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl0_10)
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl0_11)
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl1_00)
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl1_01)
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl1_10)
-        .hword L(fguv_loop_sx0_tbl) - L(fguv_loop_sx0_csfl1_11)
 endfunc
 
+jumptable fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl0_00) - fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl0_01) - fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl0_10) - fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl0_11) - fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl1_00) - fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl1_01) - fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl1_10) - fguv_loop_sx0_tbl
+        .word L(fguv_loop_sx0_csfl1_11) - fguv_loop_sx0_tbl
+endjumptable
+
 function fguv_loop_sx1_neon
 .macro fguv_loop_sx1 csfl, ox, oy
 L(fguv_loop_sx1_csfl\csfl\()_\ox\oy):
@@ -1984,14 +1988,15 @@ L(fguv_loop_sx1_csfl\csfl\()_\ox\oy):
         ldr             x30,      [sp], #80
         AARCH64_VALIDATE_LINK_REGISTER
         ret
-
-L(fguv_loop_sx1_tbl):
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl0_00)
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl0_01)
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl0_10)
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl0_11)
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl1_00)
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl1_01)
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl1_10)
-        .hword L(fguv_loop_sx1_tbl) - L(fguv_loop_sx1_csfl1_11)
 endfunc
+
+jumptable fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl0_00) - fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl0_01) - fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl0_10) - fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl0_11) - fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl1_00) - fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl1_01) - fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl1_10) - fguv_loop_sx1_tbl
+        .word L(fguv_loop_sx1_csfl1_11) - fguv_loop_sx1_tbl
+endjumptable
diff --git a/src/arm/64/ipred.S b/src/arm/64/ipred.S
index 709238e..5a375d8 100644
--- a/src/arm/64/ipred.S
+++ b/src/arm/64/ipred.S
@@ -34,16 +34,17 @@
 //                             const int max_width, const int max_height);
 function ipred_dc_128_8bpc_neon, export=1
         clz             w3,  w3
-        adr             x5,  L(ipred_dc_128_tbl)
+        movrel          x5,  ipred_dc_128_tbl
         sub             w3,  w3,  #25
-        ldrh            w3,  [x5, w3, uxtw #1]
+        ldrsw           x3,  [x5, w3, uxtw #2]
         movi            v0.16b,  #128
-        sub             x5,  x5,  w3, uxtw
+        add             x5,  x5,  x3
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
-4:
+40:
         AARCH64_VALID_JUMP_TARGET
+4:
         st1             {v0.s}[0],  [x0], x1
         st1             {v0.s}[0],  [x6], x1
         subs            w4,  w4,  #4
@@ -51,8 +52,9 @@ function ipred_dc_128_8bpc_neon, export=1
         st1             {v0.s}[0],  [x6], x1
         b.gt            4b
         ret
-8:
+80:
         AARCH64_VALID_JUMP_TARGET
+8:
         st1             {v0.8b},  [x0], x1
         st1             {v0.8b},  [x6], x1
         subs            w4,  w4,  #4
@@ -60,8 +62,9 @@ function ipred_dc_128_8bpc_neon, export=1
         st1             {v0.8b},  [x6], x1
         b.gt            8b
         ret
-16:
+160:
         AARCH64_VALID_JUMP_TARGET
+16:
         st1             {v0.16b}, [x0], x1
         st1             {v0.16b}, [x6], x1
         subs            w4,  w4,  #4
@@ -93,26 +96,27 @@ function ipred_dc_128_8bpc_neon, export=1
         st1             {v0.16b, v1.16b, v2.16b, v3.16b}, [x6], x1
         b.gt            64b
         ret
-
-L(ipred_dc_128_tbl):
-        .hword L(ipred_dc_128_tbl) - 640b
-        .hword L(ipred_dc_128_tbl) - 320b
-        .hword L(ipred_dc_128_tbl) -  16b
-        .hword L(ipred_dc_128_tbl) -   8b
-        .hword L(ipred_dc_128_tbl) -   4b
 endfunc
 
+jumptable ipred_dc_128_tbl
+        .word 640b - ipred_dc_128_tbl
+        .word 320b - ipred_dc_128_tbl
+        .word 160b - ipred_dc_128_tbl
+        .word 80b  - ipred_dc_128_tbl
+        .word 40b  - ipred_dc_128_tbl
+endjumptable
+
 // void ipred_v_8bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                        const pixel *const topleft,
 //                        const int width, const int height, const int a,
 //                        const int max_width, const int max_height);
 function ipred_v_8bpc_neon, export=1
         clz             w3,  w3
-        adr             x5,  L(ipred_v_tbl)
+        movrel          x5,  ipred_v_tbl
         sub             w3,  w3,  #25
-        ldrh            w3,  [x5, w3, uxtw #1]
+        ldrsw           x3,  [x5, w3, uxtw #2]
         add             x2,  x2,  #1
-        sub             x5,  x5,  w3, uxtw
+        add             x5,  x5,  x3
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
@@ -171,32 +175,34 @@ function ipred_v_8bpc_neon, export=1
         st1             {v0.16b, v1.16b, v2.16b, v3.16b}, [x6], x1
         b.gt            64b
         ret
-
-L(ipred_v_tbl):
-        .hword L(ipred_v_tbl) - 640b
-        .hword L(ipred_v_tbl) - 320b
-        .hword L(ipred_v_tbl) - 160b
-        .hword L(ipred_v_tbl) -  80b
-        .hword L(ipred_v_tbl) -  40b
 endfunc
 
+jumptable ipred_v_tbl
+        .word 640b - ipred_v_tbl
+        .word 320b - ipred_v_tbl
+        .word 160b - ipred_v_tbl
+        .word 80b  - ipred_v_tbl
+        .word 40b  - ipred_v_tbl
+endjumptable
+
 // void ipred_h_8bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                        const pixel *const topleft,
 //                        const int width, const int height, const int a,
 //                        const int max_width, const int max_height);
 function ipred_h_8bpc_neon, export=1
         clz             w3,  w3
-        adr             x5,  L(ipred_h_tbl)
+        movrel          x5,  ipred_h_tbl
         sub             w3,  w3,  #25
-        ldrh            w3,  [x5, w3, uxtw #1]
+        ldrsw           x3,  [x5, w3, uxtw #2]
         sub             x2,  x2,  #4
-        sub             x5,  x5,  w3, uxtw
+        add             x5,  x5,  x3
         mov             x7,  #-4
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
-4:
+40:
         AARCH64_VALID_JUMP_TARGET
+4:
         ld4r            {v0.8b, v1.8b, v2.8b, v3.8b},  [x2], x7
         st1             {v3.s}[0],  [x0], x1
         st1             {v2.s}[0],  [x6], x1
@@ -205,8 +211,9 @@ function ipred_h_8bpc_neon, export=1
         st1             {v0.s}[0],  [x6], x1
         b.gt            4b
         ret
-8:
+80:
         AARCH64_VALID_JUMP_TARGET
+8:
         ld4r            {v0.8b, v1.8b, v2.8b, v3.8b},  [x2], x7
         st1             {v3.8b},  [x0], x1
         st1             {v2.8b},  [x6], x1
@@ -215,8 +222,9 @@ function ipred_h_8bpc_neon, export=1
         st1             {v0.8b},  [x6], x1
         b.gt            8b
         ret
-16:
+160:
         AARCH64_VALID_JUMP_TARGET
+16:
         ld4r            {v0.16b, v1.16b, v2.16b, v3.16b},  [x2], x7
         st1             {v3.16b}, [x0], x1
         st1             {v2.16b}, [x6], x1
@@ -225,8 +233,9 @@ function ipred_h_8bpc_neon, export=1
         st1             {v0.16b}, [x6], x1
         b.gt            16b
         ret
-32:
+320:
         AARCH64_VALID_JUMP_TARGET
+32:
         ld4r            {v0.16b, v1.16b, v2.16b, v3.16b},  [x2], x7
         str             q3,  [x0, #16]
         str             q2,  [x6, #16]
@@ -239,8 +248,9 @@ function ipred_h_8bpc_neon, export=1
         st1             {v0.16b}, [x6], x1
         b.gt            32b
         ret
-64:
+640:
         AARCH64_VALID_JUMP_TARGET
+64:
         ld4r            {v0.16b, v1.16b, v2.16b, v3.16b},  [x2], x7
         str             q3,  [x0, #16]
         str             q2,  [x6, #16]
@@ -257,26 +267,27 @@ function ipred_h_8bpc_neon, export=1
         st1             {v0.16b}, [x6], x1
         b.gt            64b
         ret
-
-L(ipred_h_tbl):
-        .hword L(ipred_h_tbl) - 64b
-        .hword L(ipred_h_tbl) - 32b
-        .hword L(ipred_h_tbl) - 16b
-        .hword L(ipred_h_tbl) -  8b
-        .hword L(ipred_h_tbl) -  4b
 endfunc
 
+jumptable ipred_h_tbl
+        .word 640b - ipred_h_tbl
+        .word 320b - ipred_h_tbl
+        .word 160b - ipred_h_tbl
+        .word 80b  - ipred_h_tbl
+        .word 40b  - ipred_h_tbl
+endjumptable
+
 // void ipred_dc_top_8bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                             const pixel *const topleft,
 //                             const int width, const int height, const int a,
 //                             const int max_width, const int max_height);
 function ipred_dc_top_8bpc_neon, export=1
         clz             w3,  w3
-        adr             x5,  L(ipred_dc_top_tbl)
+        movrel          x5,  ipred_dc_top_tbl
         sub             w3,  w3,  #25
-        ldrh            w3,  [x5, w3, uxtw #1]
+        ldrsw           x3,  [x5, w3, uxtw #2]
         add             x2,  x2,  #1
-        sub             x5,  x5,  w3, uxtw
+        add             x5,  x5,  x3
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
@@ -362,15 +373,16 @@ function ipred_dc_top_8bpc_neon, export=1
         st1             {v0.16b, v1.16b, v2.16b, v3.16b}, [x6], x1
         b.gt            64b
         ret
-
-L(ipred_dc_top_tbl):
-        .hword L(ipred_dc_top_tbl) - 640b
-        .hword L(ipred_dc_top_tbl) - 320b
-        .hword L(ipred_dc_top_tbl) - 160b
-        .hword L(ipred_dc_top_tbl) -  80b
-        .hword L(ipred_dc_top_tbl) -  40b
 endfunc
 
+jumptable ipred_dc_top_tbl
+        .word 640b - ipred_dc_top_tbl
+        .word 320b - ipred_dc_top_tbl
+        .word 160b - ipred_dc_top_tbl
+        .word 80b  - ipred_dc_top_tbl
+        .word 40b  - ipred_dc_top_tbl
+endjumptable
+
 // void ipred_dc_left_8bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                              const pixel *const topleft,
 //                              const int width, const int height, const int a,
@@ -379,13 +391,13 @@ function ipred_dc_left_8bpc_neon, export=1
         sub             x2,  x2,  w4, uxtw
         clz             w3,  w3
         clz             w7,  w4
-        adr             x5,  L(ipred_dc_left_tbl)
+        movrel          x5,  ipred_dc_left_tbl
         sub             w3,  w3,  #20 // 25 leading bits, minus table offset 5
         sub             w7,  w7,  #25
-        ldrh            w3,  [x5, w3, uxtw #1]
-        ldrh            w7,  [x5, w7, uxtw #1]
-        sub             x3,  x5,  w3, uxtw
-        sub             x5,  x5,  w7, uxtw
+        ldrsw           x3,  [x5, w3, uxtw #2]
+        ldrsw           x7,  [x5, w7, uxtw #2]
+        add             x3,  x5,  x3
+        add             x5,  x5,  x7
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
@@ -399,12 +411,13 @@ L(ipred_dc_left_h4):
         br              x3
 L(ipred_dc_left_w4):
         AARCH64_VALID_JUMP_TARGET
+1:
         st1             {v0.s}[0],  [x0], x1
         st1             {v0.s}[0],  [x6], x1
         subs            w4,  w4,  #4
         st1             {v0.s}[0],  [x0], x1
         st1             {v0.s}[0],  [x6], x1
-        b.gt            L(ipred_dc_left_w4)
+        b.gt            1b
         ret
 
 L(ipred_dc_left_h8):
@@ -416,12 +429,13 @@ L(ipred_dc_left_h8):
         br              x3
 L(ipred_dc_left_w8):
         AARCH64_VALID_JUMP_TARGET
+1:
         st1             {v0.8b},  [x0], x1
         st1             {v0.8b},  [x6], x1
         subs            w4,  w4,  #4
         st1             {v0.8b},  [x0], x1
         st1             {v0.8b},  [x6], x1
-        b.gt            L(ipred_dc_left_w8)
+        b.gt            1b
         ret
 
 L(ipred_dc_left_h16):
@@ -433,12 +447,13 @@ L(ipred_dc_left_h16):
         br              x3
 L(ipred_dc_left_w16):
         AARCH64_VALID_JUMP_TARGET
+1:
         st1             {v0.16b}, [x0], x1
         st1             {v0.16b}, [x6], x1
         subs            w4,  w4,  #4
         st1             {v0.16b}, [x0], x1
         st1             {v0.16b}, [x6], x1
-        b.gt            L(ipred_dc_left_w16)
+        b.gt            1b
         ret
 
 L(ipred_dc_left_h32):
@@ -488,20 +503,21 @@ L(ipred_dc_left_w64):
         st1             {v0.16b, v1.16b, v2.16b, v3.16b}, [x6], x1
         b.gt            1b
         ret
-
-L(ipred_dc_left_tbl):
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_h64)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_h32)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_h16)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_h8)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_h4)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_w64)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_w32)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_w16)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_w8)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_w4)
 endfunc
 
+jumptable ipred_dc_left_tbl
+        .word L(ipred_dc_left_h64) - ipred_dc_left_tbl
+        .word L(ipred_dc_left_h32) - ipred_dc_left_tbl
+        .word L(ipred_dc_left_h16) - ipred_dc_left_tbl
+        .word L(ipred_dc_left_h8)  - ipred_dc_left_tbl
+        .word L(ipred_dc_left_h4)  - ipred_dc_left_tbl
+        .word L(ipred_dc_left_w64) - ipred_dc_left_tbl
+        .word L(ipred_dc_left_w32) - ipred_dc_left_tbl
+        .word L(ipred_dc_left_w16) - ipred_dc_left_tbl
+        .word L(ipred_dc_left_w8)  - ipred_dc_left_tbl
+        .word L(ipred_dc_left_w4)  - ipred_dc_left_tbl
+endjumptable
+
 // void ipred_dc_8bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                         const pixel *const topleft,
 //                         const int width, const int height, const int a,
@@ -512,16 +528,16 @@ function ipred_dc_8bpc_neon, export=1
         clz             w3,  w3
         clz             w6,  w4
         dup             v16.8h, w7               // width + height
-        adr             x5,  L(ipred_dc_tbl)
+        movrel          x5,  ipred_dc_tbl
         rbit            w7,  w7                  // rbit(width + height)
         sub             w3,  w3,  #20            // 25 leading bits, minus table offset 5
         sub             w6,  w6,  #25
         clz             w7,  w7                  // ctz(width + height)
-        ldrh            w3,  [x5, w3, uxtw #1]
-        ldrh            w6,  [x5, w6, uxtw #1]
+        ldrsw           x3,  [x5, w3, uxtw #2]
+        ldrsw           x6,  [x5, w6, uxtw #2]
         neg             w7,  w7                  // -ctz(width + height)
-        sub             x3,  x5,  w3, uxtw
-        sub             x5,  x5,  w6, uxtw
+        add             x3,  x5,  x3
+        add             x5,  x5,  x6
         ushr            v16.8h,  v16.8h,  #1     // (width + height) >> 1
         dup             v17.8h,  w7              // -ctz(width + height)
         add             x6,  x0,  x1
@@ -713,33 +729,34 @@ L(ipred_dc_w64):
         st1             {v0.16b, v1.16b, v2.16b, v3.16b}, [x6], x1
         b.gt            2b
         ret
-
-L(ipred_dc_tbl):
-        .hword L(ipred_dc_tbl) - L(ipred_dc_h64)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_h32)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_h16)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_h8)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_h4)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_w64)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_w32)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_w16)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_w8)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_w4)
 endfunc
 
+jumptable ipred_dc_tbl
+        .word L(ipred_dc_h64) - ipred_dc_tbl
+        .word L(ipred_dc_h32) - ipred_dc_tbl
+        .word L(ipred_dc_h16) - ipred_dc_tbl
+        .word L(ipred_dc_h8)  - ipred_dc_tbl
+        .word L(ipred_dc_h4)  - ipred_dc_tbl
+        .word L(ipred_dc_w64) - ipred_dc_tbl
+        .word L(ipred_dc_w32) - ipred_dc_tbl
+        .word L(ipred_dc_w16) - ipred_dc_tbl
+        .word L(ipred_dc_w8)  - ipred_dc_tbl
+        .word L(ipred_dc_w4)  - ipred_dc_tbl
+endjumptable
+
 // void ipred_paeth_8bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                            const pixel *const topleft,
 //                            const int width, const int height, const int a,
 //                            const int max_width, const int max_height);
 function ipred_paeth_8bpc_neon, export=1
         clz             w9,  w3
-        adr             x5,  L(ipred_paeth_tbl)
+        movrel          x5,  ipred_paeth_tbl
         sub             w9,  w9,  #25
-        ldrh            w9,  [x5, w9, uxtw #1]
+        ldrsw           x9,  [x5, w9, uxtw #2]
         ld1r            {v4.16b},  [x2]
         add             x8,  x2,  #1
         sub             x2,  x2,  #4
-        sub             x5,  x5,  w9, uxtw
+        add             x5,  x5,  x9
         mov             x7,  #-4
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
@@ -898,15 +915,16 @@ function ipred_paeth_8bpc_neon, export=1
         b               1b
 9:
         ret
-
-L(ipred_paeth_tbl):
-        .hword L(ipred_paeth_tbl) - 640b
-        .hword L(ipred_paeth_tbl) - 320b
-        .hword L(ipred_paeth_tbl) - 160b
-        .hword L(ipred_paeth_tbl) -  80b
-        .hword L(ipred_paeth_tbl) -  40b
 endfunc
 
+jumptable ipred_paeth_tbl
+        .word 640b - ipred_paeth_tbl
+        .word 320b - ipred_paeth_tbl
+        .word 160b - ipred_paeth_tbl
+        .word 80b  - ipred_paeth_tbl
+        .word 40b  - ipred_paeth_tbl
+endjumptable
+
 // void ipred_smooth_8bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                             const pixel *const topleft,
 //                             const int width, const int height, const int a,
@@ -916,13 +934,13 @@ function ipred_smooth_8bpc_neon, export=1
         add             x11, x10, w4, uxtw
         add             x10, x10, w3, uxtw
         clz             w9,  w3
-        adr             x5,  L(ipred_smooth_tbl)
+        movrel          x5,  ipred_smooth_tbl
         sub             x12, x2,  w4, uxtw
         sub             w9,  w9,  #25
-        ldrh            w9,  [x5, w9, uxtw #1]
+        ldrsw           x9,  [x5, w9, uxtw #2]
         ld1r            {v4.16b},  [x12] // bottom
         add             x8,  x2,  #1
-        sub             x5,  x5,  w9, uxtw
+        add             x5,  x5,  x9
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
@@ -1079,15 +1097,16 @@ function ipred_smooth_8bpc_neon, export=1
         b               1b
 9:
         ret
-
-L(ipred_smooth_tbl):
-        .hword L(ipred_smooth_tbl) - 640b
-        .hword L(ipred_smooth_tbl) - 320b
-        .hword L(ipred_smooth_tbl) - 160b
-        .hword L(ipred_smooth_tbl) -  80b
-        .hword L(ipred_smooth_tbl) -  40b
 endfunc
 
+jumptable ipred_smooth_tbl
+        .word 640b - ipred_smooth_tbl
+        .word 320b - ipred_smooth_tbl
+        .word 160b - ipred_smooth_tbl
+        .word 80b  - ipred_smooth_tbl
+        .word 40b  - ipred_smooth_tbl
+endjumptable
+
 // void ipred_smooth_v_8bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                               const pixel *const topleft,
 //                               const int width, const int height, const int a,
@@ -1096,13 +1115,13 @@ function ipred_smooth_v_8bpc_neon, export=1
         movrel          x7,  X(sm_weights)
         add             x7,  x7,  w4, uxtw
         clz             w9,  w3
-        adr             x5,  L(ipred_smooth_v_tbl)
+        movrel          x5,  ipred_smooth_v_tbl
         sub             x8,  x2,  w4, uxtw
         sub             w9,  w9,  #25
-        ldrh            w9,  [x5, w9, uxtw #1]
+        ldrsw           x9,  [x5, w9, uxtw #2]
         ld1r            {v4.16b},  [x8] // bottom
         add             x2,  x2,  #1
-        sub             x5,  x5,  w9, uxtw
+        add             x5,  x5,  x9
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
@@ -1220,15 +1239,16 @@ function ipred_smooth_v_8bpc_neon, export=1
         b               1b
 9:
         ret
-
-L(ipred_smooth_v_tbl):
-        .hword L(ipred_smooth_v_tbl) - 640b
-        .hword L(ipred_smooth_v_tbl) - 320b
-        .hword L(ipred_smooth_v_tbl) - 160b
-        .hword L(ipred_smooth_v_tbl) -  80b
-        .hword L(ipred_smooth_v_tbl) -  40b
 endfunc
 
+jumptable ipred_smooth_v_tbl
+        .word 640b - ipred_smooth_v_tbl
+        .word 320b - ipred_smooth_v_tbl
+        .word 160b - ipred_smooth_v_tbl
+        .word 80b  - ipred_smooth_v_tbl
+        .word 40b  - ipred_smooth_v_tbl
+endjumptable
+
 // void ipred_smooth_h_8bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                               const pixel *const topleft,
 //                               const int width, const int height, const int a,
@@ -1237,12 +1257,12 @@ function ipred_smooth_h_8bpc_neon, export=1
         movrel          x8,  X(sm_weights)
         add             x8,  x8,  w3, uxtw
         clz             w9,  w3
-        adr             x5,  L(ipred_smooth_h_tbl)
+        movrel          x5,  ipred_smooth_h_tbl
         add             x12, x2,  w3, uxtw
         sub             w9,  w9,  #25
-        ldrh            w9,  [x5, w9, uxtw #1]
+        ldrsw           x9,  [x5, w9, uxtw #2]
         ld1r            {v5.16b},  [x12] // right
-        sub             x5,  x5,  w9, uxtw
+        add             x5,  x5,  x9
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
@@ -1366,15 +1386,16 @@ function ipred_smooth_h_8bpc_neon, export=1
         b               1b
 9:
         ret
-
-L(ipred_smooth_h_tbl):
-        .hword L(ipred_smooth_h_tbl) - 640b
-        .hword L(ipred_smooth_h_tbl) - 320b
-        .hword L(ipred_smooth_h_tbl) - 160b
-        .hword L(ipred_smooth_h_tbl) -  80b
-        .hword L(ipred_smooth_h_tbl) -  40b
 endfunc
 
+jumptable ipred_smooth_h_tbl
+        .word 640b - ipred_smooth_h_tbl
+        .word 320b - ipred_smooth_h_tbl
+        .word 160b - ipred_smooth_h_tbl
+        .word 80b  - ipred_smooth_h_tbl
+        .word 40b  - ipred_smooth_h_tbl
+endjumptable
+
 const padding_mask_buf
         .byte 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
         .byte 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
@@ -1653,11 +1674,11 @@ endfunc
 //                               const int dx, const int max_base_x);
 function ipred_z1_fill1_8bpc_neon, export=1
         clz             w9,  w3
-        adr             x8,  L(ipred_z1_fill1_tbl)
+        movrel          x8,  ipred_z1_fill1_tbl
         sub             w9,  w9,  #25
-        ldrh            w9,  [x8, w9, uxtw #1]
+        ldrsw           x9,  [x8, w9, uxtw #2]
         add             x10, x2,  w6,  uxtw       // top[max_base_x]
-        sub             x8,  x8,  w9,  uxtw
+        add             x8,  x8,  x9
         ld1r            {v31.16b}, [x10]          // padding
         mov             w7,  w5
         mov             w15, #64
@@ -1815,15 +1836,16 @@ function ipred_z1_fill1_8bpc_neon, export=1
         add             x13, x13, x1
         mov             w3,  w12
         b               169b
-
-L(ipred_z1_fill1_tbl):
-        .hword L(ipred_z1_fill1_tbl) - 640b
-        .hword L(ipred_z1_fill1_tbl) - 320b
-        .hword L(ipred_z1_fill1_tbl) - 160b
-        .hword L(ipred_z1_fill1_tbl) -  80b
-        .hword L(ipred_z1_fill1_tbl) -  40b
 endfunc
 
+jumptable ipred_z1_fill1_tbl
+        .word 640b - ipred_z1_fill1_tbl
+        .word 320b - ipred_z1_fill1_tbl
+        .word 160b - ipred_z1_fill1_tbl
+        .word 80b  - ipred_z1_fill1_tbl
+        .word 40b  - ipred_z1_fill1_tbl
+endjumptable
+
 function ipred_z1_fill2_8bpc_neon, export=1
         cmp             w3,  #8
         add             x10, x2,  w6,  uxtw       // top[max_base_x]
@@ -1940,11 +1962,11 @@ endconst
 //                               const int dx, const int dy);
 function ipred_z2_fill1_8bpc_neon, export=1
         clz             w10, w4
-        adr             x9,  L(ipred_z2_fill1_tbl)
+        movrel          x9,  ipred_z2_fill1_tbl
         sub             w10, w10, #25
-        ldrh            w10, [x9, w10, uxtw #1]
+        ldrsw           x10, [x9, w10, uxtw #2]
         mov             w8,  #(1 << 6)            // xpos = 1 << 6
-        sub             x9,  x9,  w10, uxtw
+        add             x9,  x9,  x10
         sub             w8,  w8,  w6              // xpos -= dx
 
         movrel          x11, increments
@@ -2650,15 +2672,16 @@ function ipred_z2_fill1_8bpc_neon, export=1
         ldp             d10, d11, [sp, #0x10]
         ldp             d8,  d9,  [sp], 0x40
         ret
-
-L(ipred_z2_fill1_tbl):
-        .hword L(ipred_z2_fill1_tbl) - 640b
-        .hword L(ipred_z2_fill1_tbl) - 320b
-        .hword L(ipred_z2_fill1_tbl) - 160b
-        .hword L(ipred_z2_fill1_tbl) -  80b
-        .hword L(ipred_z2_fill1_tbl) -  40b
 endfunc
 
+jumptable ipred_z2_fill1_tbl
+        .word 640b - ipred_z2_fill1_tbl
+        .word 320b - ipred_z2_fill1_tbl
+        .word 160b - ipred_z2_fill1_tbl
+        .word 80b  - ipred_z2_fill1_tbl
+        .word 40b  - ipred_z2_fill1_tbl
+endjumptable
+
 function ipred_z2_fill2_8bpc_neon, export=1
         cmp             w4,  #8
         mov             w8,  #(2 << 6)            // xpos = 2 << 6
@@ -3160,11 +3183,11 @@ endfunc
 function ipred_z3_fill1_8bpc_neon, export=1
         cmp             w6,  #64
         clz             w9,  w3
-        adr             x8,  L(ipred_z3_fill1_tbl)
+        movrel          x8,  ipred_z3_fill1_tbl
         sub             w9,  w9,  #25
-        ldrh            w9,  [x8, w9, uxtw #1]
+        ldrsw           x9,  [x8, w9, uxtw #2]
         add             x10, x2,  w6,  uxtw       // left[max_base_y]
-        sub             x8,  x8,  w9,  uxtw
+        add             x8,  x8,  x9
         movrel          x11, increments
         ld1r            {v31.16b}, [x10]          // padding
         ld1             {v30.8h},  [x11]          // increments
@@ -3502,19 +3525,20 @@ L(ipred_z3_fill1_large_h16):
         b               1b
 9:
         ret
-
-L(ipred_z3_fill1_tbl):
-        .hword L(ipred_z3_fill1_tbl) - 640b
-        .hword L(ipred_z3_fill1_tbl) - 320b
-        .hword L(ipred_z3_fill1_tbl) - 160b
-        .hword L(ipred_z3_fill1_tbl) -  80b
-        .hword L(ipred_z3_fill1_tbl) -  40b
 endfunc
 
+jumptable ipred_z3_fill1_tbl
+        .word 640b - ipred_z3_fill1_tbl
+        .word 320b - ipred_z3_fill1_tbl
+        .word 160b - ipred_z3_fill1_tbl
+        .word 80b  - ipred_z3_fill1_tbl
+        .word 40b  - ipred_z3_fill1_tbl
+endjumptable
+
 function ipred_z3_fill_padding_neon, export=0
         cmp             w3,  #16
-        adr             x8,  L(ipred_z3_fill_padding_tbl)
-        b.gt            L(ipred_z3_fill_padding_wide)
+        movrel          x8,  ipred_z3_fill_padding_tbl
+        b.gt            ipred_z3_fill_padding_wide
         // w3 = remaining width, w4 = constant height
         mov             w12, w4
 
@@ -3524,12 +3548,13 @@ function ipred_z3_fill_padding_neon, export=0
         // power of two in the remaining width, and repeating.
         clz             w9,  w3
         sub             w9,  w9,  #25
-        ldrh            w9,  [x8, w9, uxtw #1]
-        sub             x9,  x8,  w9,  uxtw
+        ldrsw           x9,  [x8, w9, uxtw #2]
+        add             x9,  x8,  x9
         br              x9
 
-2:
+20:
         AARCH64_VALID_JUMP_TARGET
+2:
         st1             {v31.h}[0], [x0],  x1
         subs            w4,  w4,  #4
         st1             {v31.h}[0], [x13], x1
@@ -3547,8 +3572,9 @@ function ipred_z3_fill_padding_neon, export=0
         mov             w4,  w12
         b               1b
 
-4:
+40:
         AARCH64_VALID_JUMP_TARGET
+4:
         st1             {v31.s}[0], [x0],  x1
         subs            w4,  w4,  #4
         st1             {v31.s}[0], [x13], x1
@@ -3566,14 +3592,15 @@ function ipred_z3_fill_padding_neon, export=0
         mov             w4,  w12
         b               1b
 
-8:
+80:
         AARCH64_VALID_JUMP_TARGET
+8:
         st1             {v31.8b}, [x0],  x1
         subs            w4,  w4,  #4
         st1             {v31.8b}, [x13], x1
         st1             {v31.8b}, [x0],  x1
         st1             {v31.8b}, [x13], x1
-        b.gt            4b
+        b.gt            8b
         subs            w3,  w3,  #8
         lsr             x1,  x1,  #1
         msub            x0,  x1,  x12, x0         // ptr -= h * stride
@@ -3585,16 +3612,17 @@ function ipred_z3_fill_padding_neon, export=0
         mov             w4,  w12
         b               1b
 
-16:
-32:
-64:
+160:
+320:
+640:
         AARCH64_VALID_JUMP_TARGET
+16:
         st1             {v31.16b}, [x0],  x1
         subs            w4,  w4,  #4
         st1             {v31.16b}, [x13], x1
         st1             {v31.16b}, [x0],  x1
         st1             {v31.16b}, [x13], x1
-        b.gt            4b
+        b.gt            16b
         subs            w3,  w3,  #16
         lsr             x1,  x1,  #1
         msub            x0,  x1,  x12, x0         // ptr -= h * stride
@@ -3608,16 +3636,18 @@ function ipred_z3_fill_padding_neon, export=0
 
 9:
         ret
+endfunc
 
-L(ipred_z3_fill_padding_tbl):
-        .hword L(ipred_z3_fill_padding_tbl) - 64b
-        .hword L(ipred_z3_fill_padding_tbl) - 32b
-        .hword L(ipred_z3_fill_padding_tbl) - 16b
-        .hword L(ipred_z3_fill_padding_tbl) -  8b
-        .hword L(ipred_z3_fill_padding_tbl) -  4b
-        .hword L(ipred_z3_fill_padding_tbl) -  2b
+jumptable ipred_z3_fill_padding_tbl
+        .word 640b - ipred_z3_fill_padding_tbl
+        .word 320b - ipred_z3_fill_padding_tbl
+        .word 160b - ipred_z3_fill_padding_tbl
+        .word 80b  - ipred_z3_fill_padding_tbl
+        .word 40b  - ipred_z3_fill_padding_tbl
+        .word 20b  - ipred_z3_fill_padding_tbl
+endjumptable
 
-L(ipred_z3_fill_padding_wide):
+function ipred_z3_fill_padding_wide
         // Fill a WxH rectangle with padding, with W > 16.
         lsr             x1,  x1,  #1
         mov             w12, w3
@@ -3770,13 +3800,13 @@ function ipred_filter_8bpc_neon, export=1
         add             x6,  x6,  w5, uxtw
         ld1             {v16.8b, v17.8b, v18.8b, v19.8b}, [x6], #32
         clz             w9,  w3
-        adr             x5,  L(ipred_filter_tbl)
+        movrel          x5,  ipred_filter_tbl
         ld1             {v20.8b, v21.8b, v22.8b}, [x6]
         sub             w9,  w9,  #26
-        ldrh            w9,  [x5, w9, uxtw #1]
+        ldrsw           x9,  [x5, w9, uxtw #2]
         sxtl            v16.8h,  v16.8b
         sxtl            v17.8h,  v17.8b
-        sub             x5,  x5,  w9, uxtw
+        add             x5,  x5,  x9
         sxtl            v18.8h,  v18.8b
         sxtl            v19.8h,  v19.8b
         add             x6,  x0,  x1
@@ -3916,30 +3946,32 @@ function ipred_filter_8bpc_neon, export=1
         b               1b
 9:
         ret
-
-L(ipred_filter_tbl):
-        .hword L(ipred_filter_tbl) - 320b
-        .hword L(ipred_filter_tbl) - 160b
-        .hword L(ipred_filter_tbl) -  80b
-        .hword L(ipred_filter_tbl) -  40b
 endfunc
 
+jumptable ipred_filter_tbl
+        .word 320b - ipred_filter_tbl
+        .word 160b - ipred_filter_tbl
+        .word 80b  - ipred_filter_tbl
+        .word 40b  - ipred_filter_tbl
+endjumptable
+
 // void pal_pred_8bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                         const pixel *const pal, const uint8_t *idx,
 //                         const int w, const int h);
 function pal_pred_8bpc_neon, export=1
         ld1             {v0.8b}, [x2]
         clz             w9,  w4
-        adr             x6,  L(pal_pred_tbl)
+        movrel          x6,  pal_pred_tbl
         sub             w9,  w9,  #25
         movi            v31.16b, #7
-        ldrh            w9,  [x6, w9, uxtw #1]
-        sub             x6,  x6,  w9, uxtw
+        ldrsw           x9,  [x6, w9, uxtw #2]
+        add             x6,  x6,  x9
         add             x2,  x0,  x1
         lsl             x1,  x1,  #1
         br              x6
-4:
+40:
         AARCH64_VALID_JUMP_TARGET
+4:
         ld1             {v1.8b}, [x3], #8
         subs            w5,  w5,  #4
         ushr            v3.8b,   v1.8b,   #4
@@ -3952,8 +3984,9 @@ function pal_pred_8bpc_neon, export=1
         st1             {v1.s}[3], [x2], x1
         b.gt            4b
         ret
-8:
+80:
         AARCH64_VALID_JUMP_TARGET
+8:
         ld1             {v1.16b}, [x3], #16
         subs            w5,  w5,  #4
         ushr            v4.16b,  v1.16b,  #4
@@ -3968,8 +4001,9 @@ function pal_pred_8bpc_neon, export=1
         st1             {v2.d}[1], [x2], x1
         b.gt            8b
         ret
-16:
+160:
         AARCH64_VALID_JUMP_TARGET
+16:
         ld1             {v1.16b, v2.16b}, [x3], #32
         subs            w5,  w5,  #4
         ushr            v5.16b,  v1.16b,  #4
@@ -3990,8 +4024,9 @@ function pal_pred_8bpc_neon, export=1
         st1             {v4.16b}, [x2], x1
         b.gt            16b
         ret
-32:
+320:
         AARCH64_VALID_JUMP_TARGET
+32:
         ld1             {v16.16b, v17.16b, v18.16b, v19.16b}, [x3], #64
         subs            w5,  w5,  #4
         ushr            v21.16b, v16.16b, #4
@@ -4024,8 +4059,9 @@ function pal_pred_8bpc_neon, export=1
         st1             {v22.16b, v23.16b}, [x2], x1
         b.gt            32b
         ret
-64:
+640:
         AARCH64_VALID_JUMP_TARGET
+64:
         ld1             {v16.16b, v17.16b, v18.16b, v19.16b}, [x3], #64
         subs            w5,  w5,  #2
         ushr            v21.16b, v16.16b, #4
@@ -4056,32 +4092,34 @@ function pal_pred_8bpc_neon, export=1
         st1             {v20.16b, v21.16b, v22.16b, v23.16b}, [x2], x1
         b.gt            64b
         ret
-
-L(pal_pred_tbl):
-        .hword L(pal_pred_tbl) - 64b
-        .hword L(pal_pred_tbl) - 32b
-        .hword L(pal_pred_tbl) - 16b
-        .hword L(pal_pred_tbl) -  8b
-        .hword L(pal_pred_tbl) -  4b
 endfunc
 
+jumptable pal_pred_tbl
+        .word 640b - pal_pred_tbl
+        .word 320b - pal_pred_tbl
+        .word 160b - pal_pred_tbl
+        .word 80b  - pal_pred_tbl
+        .word 40b  - pal_pred_tbl
+endjumptable
+
 // void ipred_cfl_128_8bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                              const pixel *const topleft,
 //                              const int width, const int height,
 //                              const int16_t *ac, const int alpha);
 function ipred_cfl_128_8bpc_neon, export=1
         clz             w9,  w3
-        adr             x7,  L(ipred_cfl_128_tbl)
+        movrel          x7,  ipred_cfl_128_tbl
         sub             w9,  w9,  #26
-        ldrh            w9,  [x7, w9, uxtw #1]
+        ldrsw           x9,  [x7, w9, uxtw #2]
         movi            v0.8h,   #128 // dc
         dup             v1.8h,   w6   // alpha
-        sub             x7,  x7,  w9, uxtw
+        add             x7,  x7,  x9
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x7
 L(ipred_cfl_splat_w4):
         AARCH64_VALID_JUMP_TARGET
+1:
         ld1             {v2.8h, v3.8h}, [x5], #32
         mul             v2.8h,   v2.8h,   v1.8h  // diff = ac * alpha
         mul             v3.8h,   v3.8h,   v1.8h
@@ -4100,10 +4138,11 @@ L(ipred_cfl_splat_w4):
         subs            w4,  w4,  #4
         st1             {v3.s}[0],  [x0], x1
         st1             {v3.s}[1],  [x6], x1
-        b.gt            L(ipred_cfl_splat_w4)
+        b.gt            1b
         ret
 L(ipred_cfl_splat_w8):
         AARCH64_VALID_JUMP_TARGET
+1:
         ld1             {v2.8h, v3.8h, v4.8h, v5.8h}, [x5], #64
         mul             v2.8h,   v2.8h,   v1.8h  // diff = ac * alpha
         mul             v3.8h,   v3.8h,   v1.8h
@@ -4134,7 +4173,7 @@ L(ipred_cfl_splat_w8):
         subs            w4,  w4,  #4
         st1             {v4.8b},  [x0], x1
         st1             {v5.8b},  [x6], x1
-        b.gt            L(ipred_cfl_splat_w8)
+        b.gt            1b
         ret
 L(ipred_cfl_splat_w16):
         AARCH64_VALID_JUMP_TARGET
@@ -4180,27 +4219,28 @@ L(ipred_cfl_splat_w16):
         mov             w3,  w9
         b.gt            1b
         ret
-
-L(ipred_cfl_128_tbl):
-L(ipred_cfl_splat_tbl):
-        .hword L(ipred_cfl_128_tbl) - L(ipred_cfl_splat_w16)
-        .hword L(ipred_cfl_128_tbl) - L(ipred_cfl_splat_w16)
-        .hword L(ipred_cfl_128_tbl) - L(ipred_cfl_splat_w8)
-        .hword L(ipred_cfl_128_tbl) - L(ipred_cfl_splat_w4)
 endfunc
 
+jumptable ipred_cfl_128_tbl
+ipred_cfl_splat_tbl:
+        .word L(ipred_cfl_splat_w16) - ipred_cfl_128_tbl
+        .word L(ipred_cfl_splat_w16) - ipred_cfl_128_tbl
+        .word L(ipred_cfl_splat_w8)  - ipred_cfl_128_tbl
+        .word L(ipred_cfl_splat_w4)  - ipred_cfl_128_tbl
+endjumptable
+
 // void ipred_cfl_top_8bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                              const pixel *const topleft,
 //                              const int width, const int height,
 //                              const int16_t *ac, const int alpha);
 function ipred_cfl_top_8bpc_neon, export=1
         clz             w9,  w3
-        adr             x7,  L(ipred_cfl_top_tbl)
+        movrel          x7,  ipred_cfl_top_tbl
         sub             w9,  w9,  #26
-        ldrh            w9,  [x7, w9, uxtw #1]
+        ldrsw           x9,  [x7, w9, uxtw #2]
         dup             v1.8h,   w6   // alpha
         add             x2,  x2,  #1
-        sub             x7,  x7,  w9, uxtw
+        add             x7,  x7,  x9
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x7
@@ -4234,14 +4274,15 @@ function ipred_cfl_top_8bpc_neon, export=1
         urshr           v2.4h,   v2.4h,   #5
         dup             v0.8h,   v2.h[0]
         b               L(ipred_cfl_splat_w16)
-
-L(ipred_cfl_top_tbl):
-        .hword L(ipred_cfl_top_tbl) - 32b
-        .hword L(ipred_cfl_top_tbl) - 16b
-        .hword L(ipred_cfl_top_tbl) -  8b
-        .hword L(ipred_cfl_top_tbl) -  4b
 endfunc
 
+jumptable ipred_cfl_top_tbl
+        .word 32b - ipred_cfl_top_tbl
+        .word 16b - ipred_cfl_top_tbl
+        .word 8b  - ipred_cfl_top_tbl
+        .word 4b  - ipred_cfl_top_tbl
+endjumptable
+
 // void ipred_cfl_left_8bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                               const pixel *const topleft,
 //                               const int width, const int height,
@@ -4250,15 +4291,15 @@ function ipred_cfl_left_8bpc_neon, export=1
         sub             x2,  x2,  w4, uxtw
         clz             w9,  w3
         clz             w8,  w4
-        adr             x10, L(ipred_cfl_splat_tbl)
-        adr             x7,  L(ipred_cfl_left_tbl)
+        movrel          x10, ipred_cfl_splat_tbl
+        movrel          x7,  ipred_cfl_left_tbl
         sub             w9,  w9,  #26
         sub             w8,  w8,  #26
-        ldrh            w9,  [x10, w9, uxtw #1]
-        ldrh            w8,  [x7,  w8, uxtw #1]
+        ldrsw           x9,  [x10, w9, uxtw #2]
+        ldrsw           x8,  [x7,  w8, uxtw #2]
         dup             v1.8h,   w6   // alpha
-        sub             x9,  x10, w9, uxtw
-        sub             x7,  x7,  w8, uxtw
+        add             x9,  x10, x9
+        add             x7,  x7,  x8
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x7
@@ -4296,14 +4337,15 @@ L(ipred_cfl_left_h32):
         urshr           v2.4h,   v2.4h,   #5
         dup             v0.8h,   v2.h[0]
         br              x9
-
-L(ipred_cfl_left_tbl):
-        .hword L(ipred_cfl_left_tbl) - L(ipred_cfl_left_h32)
-        .hword L(ipred_cfl_left_tbl) - L(ipred_cfl_left_h16)
-        .hword L(ipred_cfl_left_tbl) - L(ipred_cfl_left_h8)
-        .hword L(ipred_cfl_left_tbl) - L(ipred_cfl_left_h4)
 endfunc
 
+jumptable ipred_cfl_left_tbl
+        .word L(ipred_cfl_left_h32) - ipred_cfl_left_tbl
+        .word L(ipred_cfl_left_h16) - ipred_cfl_left_tbl
+        .word L(ipred_cfl_left_h8)  - ipred_cfl_left_tbl
+        .word L(ipred_cfl_left_h4)  - ipred_cfl_left_tbl
+endjumptable
+
 // void ipred_cfl_8bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                          const pixel *const topleft,
 //                          const int width, const int height,
@@ -4315,16 +4357,16 @@ function ipred_cfl_8bpc_neon, export=1
         clz             w9,  w3
         clz             w6,  w4
         dup             v16.8h, w8               // width + height
-        adr             x7,  L(ipred_cfl_tbl)
+        movrel          x7,  ipred_cfl_tbl
         rbit            w8,  w8                  // rbit(width + height)
         sub             w9,  w9,  #22            // 26 leading bits, minus table offset 4
         sub             w6,  w6,  #26
         clz             w8,  w8                  // ctz(width + height)
-        ldrh            w9,  [x7, w9, uxtw #1]
-        ldrh            w6,  [x7, w6, uxtw #1]
+        ldrsw           x9,  [x7, w9, uxtw #2]
+        ldrsw           x6,  [x7, w6, uxtw #2]
         neg             w8,  w8                  // -ctz(width + height)
-        sub             x9,  x7,  w9, uxtw
-        sub             x7,  x7,  w6, uxtw
+        add             x9,  x7,  x9
+        add             x7,  x7,  x6
         ushr            v16.8h,  v16.8h,  #1     // (width + height) >> 1
         dup             v17.8h,  w8              // -ctz(width + height)
         add             x6,  x0,  x1
@@ -4440,32 +4482,33 @@ L(ipred_cfl_w32):
 1:
         dup             v0.8h,   v0.h[0]
         b               L(ipred_cfl_splat_w16)
-
-L(ipred_cfl_tbl):
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_h32)
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_h16)
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_h8)
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_h4)
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_w32)
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_w16)
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_w8)
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_w4)
 endfunc
 
+jumptable ipred_cfl_tbl
+        .word L(ipred_cfl_h32) - ipred_cfl_tbl
+        .word L(ipred_cfl_h16) - ipred_cfl_tbl
+        .word L(ipred_cfl_h8)  - ipred_cfl_tbl
+        .word L(ipred_cfl_h4)  - ipred_cfl_tbl
+        .word L(ipred_cfl_w32) - ipred_cfl_tbl
+        .word L(ipred_cfl_w16) - ipred_cfl_tbl
+        .word L(ipred_cfl_w8)  - ipred_cfl_tbl
+        .word L(ipred_cfl_w4)  - ipred_cfl_tbl
+endjumptable
+
 // void cfl_ac_420_8bpc_neon(int16_t *const ac, const pixel *const ypx,
 //                           const ptrdiff_t stride, const int w_pad,
 //                           const int h_pad, const int cw, const int ch);
 function ipred_cfl_ac_420_8bpc_neon, export=1
         clz             w8,  w5
         lsl             w4,  w4,  #2
-        adr             x7,  L(ipred_cfl_ac_420_tbl)
+        movrel          x7,  ipred_cfl_ac_420_tbl
         sub             w8,  w8,  #27
-        ldrh            w8,  [x7, w8, uxtw #1]
+        ldrsw           x8,  [x7, w8, uxtw #2]
         movi            v16.8h,  #0
         movi            v17.8h,  #0
         movi            v18.8h,  #0
         movi            v19.8h,  #0
-        sub             x7,  x7,  w8, uxtw
+        add             x7,  x7,  x8
         sub             w8,  w6,  w4         // height - h_pad
         rbit            w9,  w5              // rbit(width)
         rbit            w10, w6              // rbit(height)
@@ -4604,9 +4647,9 @@ L(ipred_cfl_ac_420_w8_subtract_dc):
 
 L(ipred_cfl_ac_420_w16):
         AARCH64_VALID_JUMP_TARGET
-        adr             x7,  L(ipred_cfl_ac_420_w16_tbl)
-        ldrh            w3,  [x7, w3, uxtw #1]
-        sub             x7,  x7,  w3, uxtw
+        movrel          x7,  ipred_cfl_ac_420_w16_tbl
+        ldrsw           x3,  [x7, w3, uxtw #2]
+        add             x7,  x7,  x3
         br              x7
 
 L(ipred_cfl_ac_420_w16_wpad0):
@@ -4762,34 +4805,35 @@ L(ipred_cfl_ac_420_w16_hpad):
         // Double the height and reuse the w8 summing/subtracting
         lsl             w6,  w6,  #1
         b               L(ipred_cfl_ac_420_w8_calc_subtract_dc)
-
-L(ipred_cfl_ac_420_tbl):
-        .hword L(ipred_cfl_ac_420_tbl) - L(ipred_cfl_ac_420_w16)
-        .hword L(ipred_cfl_ac_420_tbl) - L(ipred_cfl_ac_420_w8)
-        .hword L(ipred_cfl_ac_420_tbl) - L(ipred_cfl_ac_420_w4)
-        .hword 0
-
-L(ipred_cfl_ac_420_w16_tbl):
-        .hword L(ipred_cfl_ac_420_w16_tbl) - L(ipred_cfl_ac_420_w16_wpad0)
-        .hword L(ipred_cfl_ac_420_w16_tbl) - L(ipred_cfl_ac_420_w16_wpad1)
-        .hword L(ipred_cfl_ac_420_w16_tbl) - L(ipred_cfl_ac_420_w16_wpad2)
-        .hword L(ipred_cfl_ac_420_w16_tbl) - L(ipred_cfl_ac_420_w16_wpad3)
 endfunc
 
+jumptable ipred_cfl_ac_420_tbl
+        .word L(ipred_cfl_ac_420_w16) - ipred_cfl_ac_420_tbl
+        .word L(ipred_cfl_ac_420_w8)  - ipred_cfl_ac_420_tbl
+        .word L(ipred_cfl_ac_420_w4)  - ipred_cfl_ac_420_tbl
+endjumptable
+
+jumptable ipred_cfl_ac_420_w16_tbl
+        .word L(ipred_cfl_ac_420_w16_wpad0) - ipred_cfl_ac_420_w16_tbl
+        .word L(ipred_cfl_ac_420_w16_wpad1) - ipred_cfl_ac_420_w16_tbl
+        .word L(ipred_cfl_ac_420_w16_wpad2) - ipred_cfl_ac_420_w16_tbl
+        .word L(ipred_cfl_ac_420_w16_wpad3) - ipred_cfl_ac_420_w16_tbl
+endjumptable
+
 // void cfl_ac_422_8bpc_neon(int16_t *const ac, const pixel *const ypx,
 //                           const ptrdiff_t stride, const int w_pad,
 //                           const int h_pad, const int cw, const int ch);
 function ipred_cfl_ac_422_8bpc_neon, export=1
         clz             w8,  w5
         lsl             w4,  w4,  #2
-        adr             x7,  L(ipred_cfl_ac_422_tbl)
+        movrel          x7,  ipred_cfl_ac_422_tbl
         sub             w8,  w8,  #27
-        ldrh            w8,  [x7, w8, uxtw #1]
+        ldrsw           x8,  [x7, w8, uxtw #2]
         movi            v16.8h,  #0
         movi            v17.8h,  #0
         movi            v18.8h,  #0
         movi            v19.8h,  #0
-        sub             x7,  x7,  w8, uxtw
+        add             x7,  x7,  x8
         sub             w8,  w6,  w4         // height - h_pad
         rbit            w9,  w5              // rbit(width)
         rbit            w10, w6              // rbit(height)
@@ -4880,9 +4924,9 @@ L(ipred_cfl_ac_422_w8_wpad):
 
 L(ipred_cfl_ac_422_w16):
         AARCH64_VALID_JUMP_TARGET
-        adr             x7,  L(ipred_cfl_ac_422_w16_tbl)
-        ldrh            w3,  [x7, w3, uxtw #1]
-        sub             x7,  x7,  w3, uxtw
+        movrel          x7,  ipred_cfl_ac_422_w16_tbl
+        ldrsw           x3,  [x7, w3, uxtw #2]
+        add             x7,  x7,  x3
         br              x7
 
 L(ipred_cfl_ac_422_w16_wpad0):
@@ -4984,34 +5028,35 @@ L(ipred_cfl_ac_422_w16_wpad3):
         mov             v0.16b,  v2.16b
         mov             v1.16b,  v3.16b
         b               L(ipred_cfl_ac_420_w16_hpad)
-
-L(ipred_cfl_ac_422_tbl):
-        .hword L(ipred_cfl_ac_422_tbl) - L(ipred_cfl_ac_422_w16)
-        .hword L(ipred_cfl_ac_422_tbl) - L(ipred_cfl_ac_422_w8)
-        .hword L(ipred_cfl_ac_422_tbl) - L(ipred_cfl_ac_422_w4)
-        .hword 0
-
-L(ipred_cfl_ac_422_w16_tbl):
-        .hword L(ipred_cfl_ac_422_w16_tbl) - L(ipred_cfl_ac_422_w16_wpad0)
-        .hword L(ipred_cfl_ac_422_w16_tbl) - L(ipred_cfl_ac_422_w16_wpad1)
-        .hword L(ipred_cfl_ac_422_w16_tbl) - L(ipred_cfl_ac_422_w16_wpad2)
-        .hword L(ipred_cfl_ac_422_w16_tbl) - L(ipred_cfl_ac_422_w16_wpad3)
 endfunc
 
+jumptable ipred_cfl_ac_422_tbl
+        .word L(ipred_cfl_ac_422_w16) - ipred_cfl_ac_422_tbl
+        .word L(ipred_cfl_ac_422_w8) - ipred_cfl_ac_422_tbl
+        .word L(ipred_cfl_ac_422_w4) - ipred_cfl_ac_422_tbl
+endjumptable
+
+jumptable ipred_cfl_ac_422_w16_tbl
+        .word L(ipred_cfl_ac_422_w16_wpad0) - ipred_cfl_ac_422_w16_tbl
+        .word L(ipred_cfl_ac_422_w16_wpad1) - ipred_cfl_ac_422_w16_tbl
+        .word L(ipred_cfl_ac_422_w16_wpad2) - ipred_cfl_ac_422_w16_tbl
+        .word L(ipred_cfl_ac_422_w16_wpad3) - ipred_cfl_ac_422_w16_tbl
+endjumptable
+
 // void cfl_ac_444_8bpc_neon(int16_t *const ac, const pixel *const ypx,
 //                           const ptrdiff_t stride, const int w_pad,
 //                           const int h_pad, const int cw, const int ch);
 function ipred_cfl_ac_444_8bpc_neon, export=1
         clz             w8,  w5
         lsl             w4,  w4,  #2
-        adr             x7,  L(ipred_cfl_ac_444_tbl)
+        movrel          x7,  ipred_cfl_ac_444_tbl
         sub             w8,  w8,  #26
-        ldrh            w8,  [x7, w8, uxtw #1]
+        ldrsw           x8,  [x7, w8, uxtw #2]
         movi            v16.8h,  #0
         movi            v17.8h,  #0
         movi            v18.8h,  #0
         movi            v19.8h,  #0
-        sub             x7,  x7,  w8, uxtw
+        add             x7,  x7,  x8
         sub             w8,  w6,  w4         // height - h_pad
         rbit            w9,  w5              // rbit(width)
         rbit            w10, w6              // rbit(height)
@@ -5132,9 +5177,10 @@ L(ipred_cfl_ac_444_w16_wpad):
 
 L(ipred_cfl_ac_444_w32):
         AARCH64_VALID_JUMP_TARGET
-        adr             x7,  L(ipred_cfl_ac_444_w32_tbl)
-        ldrh            w3,  [x7, w3, uxtw] // (w3>>1) << 1
-        sub             x7,  x7,  w3, uxtw
+        movrel          x7,  ipred_cfl_ac_444_w32_tbl
+        lsr             w3,  w3,  #1
+        ldrsw           x3,  [x7, w3, uxtw #2]
+        add             x7,  x7,  x3
         br              x7
 
 L(ipred_cfl_ac_444_w32_wpad0):
@@ -5279,16 +5325,18 @@ L(ipred_cfl_ac_444_w32_hpad):
         urshl           v4.2s,   v0.2s,   v31.2s  // (sum + (1 << (log2sz - 1))) >>= log2sz
         dup             v4.8h,   v4.h[0]
         b               L(ipred_cfl_ac_420_w8_subtract_dc)
-
-L(ipred_cfl_ac_444_tbl):
-        .hword L(ipred_cfl_ac_444_tbl) - L(ipred_cfl_ac_444_w32)
-        .hword L(ipred_cfl_ac_444_tbl) - L(ipred_cfl_ac_444_w16)
-        .hword L(ipred_cfl_ac_444_tbl) - L(ipred_cfl_ac_444_w8)
-        .hword L(ipred_cfl_ac_444_tbl) - L(ipred_cfl_ac_444_w4)
-
-L(ipred_cfl_ac_444_w32_tbl):
-        .hword L(ipred_cfl_ac_444_w32_tbl) - L(ipred_cfl_ac_444_w32_wpad0)
-        .hword L(ipred_cfl_ac_444_w32_tbl) - L(ipred_cfl_ac_444_w32_wpad2)
-        .hword L(ipred_cfl_ac_444_w32_tbl) - L(ipred_cfl_ac_444_w32_wpad4)
-        .hword L(ipred_cfl_ac_444_w32_tbl) - L(ipred_cfl_ac_444_w32_wpad6)
 endfunc
+
+jumptable ipred_cfl_ac_444_tbl
+        .word L(ipred_cfl_ac_444_w32) - ipred_cfl_ac_444_tbl
+        .word L(ipred_cfl_ac_444_w16) - ipred_cfl_ac_444_tbl
+        .word L(ipred_cfl_ac_444_w8)  - ipred_cfl_ac_444_tbl
+        .word L(ipred_cfl_ac_444_w4)  - ipred_cfl_ac_444_tbl
+endjumptable
+
+jumptable ipred_cfl_ac_444_w32_tbl
+        .word L(ipred_cfl_ac_444_w32_wpad0) - ipred_cfl_ac_444_w32_tbl
+        .word L(ipred_cfl_ac_444_w32_wpad2) - ipred_cfl_ac_444_w32_tbl
+        .word L(ipred_cfl_ac_444_w32_wpad4) - ipred_cfl_ac_444_w32_tbl
+        .word L(ipred_cfl_ac_444_w32_wpad6) - ipred_cfl_ac_444_w32_tbl
+endjumptable
diff --git a/src/arm/64/ipred16.S b/src/arm/64/ipred16.S
index 3f8cff9..2292a85 100644
--- a/src/arm/64/ipred16.S
+++ b/src/arm/64/ipred16.S
@@ -36,17 +36,18 @@
 function ipred_dc_128_16bpc_neon, export=1
         ldr             w8,  [sp]
         clz             w3,  w3
-        adr             x5,  L(ipred_dc_128_tbl)
+        movrel          x5,  ipred_dc_128_tbl
         sub             w3,  w3,  #25
-        ldrh            w3,  [x5, w3, uxtw #1]
+        ldrsw           x3,  [x5, w3, uxtw #2]
         dup             v0.8h,   w8
-        sub             x5,  x5,  w3, uxtw
+        add             x5,  x5,  x3
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         urshr           v0.8h,   v0.8h,  #1
         br              x5
-4:
+40:
         AARCH64_VALID_JUMP_TARGET
+4:
         st1             {v0.4h},  [x0], x1
         st1             {v0.4h},  [x6], x1
         subs            w4,  w4,  #4
@@ -54,8 +55,9 @@ function ipred_dc_128_16bpc_neon, export=1
         st1             {v0.4h},  [x6], x1
         b.gt            4b
         ret
-8:
+80:
         AARCH64_VALID_JUMP_TARGET
+8:
         st1             {v0.8h},  [x0], x1
         st1             {v0.8h},  [x6], x1
         subs            w4,  w4,  #4
@@ -105,26 +107,27 @@ function ipred_dc_128_16bpc_neon, export=1
         st1             {v0.8h, v1.8h, v2.8h, v3.8h}, [x6], x1
         b.gt            64b
         ret
-
-L(ipred_dc_128_tbl):
-        .hword L(ipred_dc_128_tbl) - 640b
-        .hword L(ipred_dc_128_tbl) - 320b
-        .hword L(ipred_dc_128_tbl) - 160b
-        .hword L(ipred_dc_128_tbl) -   8b
-        .hword L(ipred_dc_128_tbl) -   4b
 endfunc
 
+jumptable ipred_dc_128_tbl
+        .word 640b - ipred_dc_128_tbl
+        .word 320b - ipred_dc_128_tbl
+        .word 160b - ipred_dc_128_tbl
+        .word 80b  - ipred_dc_128_tbl
+        .word 40b  - ipred_dc_128_tbl
+endjumptable
+
 // void ipred_v_16bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                         const pixel *const topleft,
 //                         const int width, const int height, const int a,
 //                         const int max_width, const int max_height);
 function ipred_v_16bpc_neon, export=1
         clz             w3,  w3
-        adr             x5,  L(ipred_v_tbl)
+        movrel          x5,  ipred_v_tbl
         sub             w3,  w3,  #25
-        ldrh            w3,  [x5, w3, uxtw #1]
+        ldrsw           x3,  [x5, w3, uxtw #2]
         add             x2,  x2,  #2
-        sub             x5,  x5,  w3, uxtw
+        add             x5,  x5,  x3
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
@@ -189,32 +192,34 @@ function ipred_v_16bpc_neon, export=1
         st1             {v4.8h, v5.8h, v6.8h, v7.8h}, [x6], x1
         b.gt            64b
         ret
-
-L(ipred_v_tbl):
-        .hword L(ipred_v_tbl) - 640b
-        .hword L(ipred_v_tbl) - 320b
-        .hword L(ipred_v_tbl) - 160b
-        .hword L(ipred_v_tbl) -  80b
-        .hword L(ipred_v_tbl) -  40b
 endfunc
 
+jumptable ipred_v_tbl
+        .word 640b - ipred_v_tbl
+        .word 320b - ipred_v_tbl
+        .word 160b - ipred_v_tbl
+        .word 80b  - ipred_v_tbl
+        .word 40b  - ipred_v_tbl
+endjumptable
+
 // void ipred_h_16bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                         const pixel *const topleft,
 //                         const int width, const int height, const int a,
 //                         const int max_width, const int max_height);
 function ipred_h_16bpc_neon, export=1
         clz             w3,  w3
-        adr             x5,  L(ipred_h_tbl)
+        movrel          x5,  ipred_h_tbl
         sub             w3,  w3,  #25
-        ldrh            w3,  [x5, w3, uxtw #1]
+        ldrsw           x3,  [x5, w3, uxtw #2]
         sub             x2,  x2,  #8
-        sub             x5,  x5,  w3, uxtw
+        add             x5,  x5,  x3
         mov             x7,  #-8
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
-4:
+40:
         AARCH64_VALID_JUMP_TARGET
+4:
         ld4r            {v0.8h, v1.8h, v2.8h, v3.8h},  [x2], x7
         st1             {v3.4h},  [x0], x1
         st1             {v2.4h},  [x6], x1
@@ -223,8 +228,9 @@ function ipred_h_16bpc_neon, export=1
         st1             {v0.4h},  [x6], x1
         b.gt            4b
         ret
-8:
+80:
         AARCH64_VALID_JUMP_TARGET
+8:
         ld4r            {v0.8h, v1.8h, v2.8h, v3.8h},  [x2], x7
         st1             {v3.8h},  [x0], x1
         st1             {v2.8h},  [x6], x1
@@ -233,8 +239,9 @@ function ipred_h_16bpc_neon, export=1
         st1             {v0.8h},  [x6], x1
         b.gt            8b
         ret
-16:
+160:
         AARCH64_VALID_JUMP_TARGET
+16:
         ld4r            {v0.8h, v1.8h, v2.8h, v3.8h},  [x2], x7
         str             q3,  [x0, #16]
         str             q2,  [x6, #16]
@@ -247,8 +254,9 @@ function ipred_h_16bpc_neon, export=1
         st1             {v0.8h}, [x6], x1
         b.gt            16b
         ret
-32:
+320:
         AARCH64_VALID_JUMP_TARGET
+32:
         ld4r            {v0.8h, v1.8h, v2.8h, v3.8h},  [x2], x7
         str             q3,  [x0, #16]
         str             q2,  [x6, #16]
@@ -265,8 +273,9 @@ function ipred_h_16bpc_neon, export=1
         st1             {v0.8h}, [x6], x1
         b.gt            32b
         ret
-64:
+640:
         AARCH64_VALID_JUMP_TARGET
+64:
         ld4r            {v0.8h, v1.8h, v2.8h, v3.8h},  [x2], x7
         str             q3,  [x0, #16]
         str             q2,  [x6, #16]
@@ -291,26 +300,27 @@ function ipred_h_16bpc_neon, export=1
         st1             {v0.8h}, [x6], x1
         b.gt            64b
         ret
-
-L(ipred_h_tbl):
-        .hword L(ipred_h_tbl) - 64b
-        .hword L(ipred_h_tbl) - 32b
-        .hword L(ipred_h_tbl) - 16b
-        .hword L(ipred_h_tbl) -  8b
-        .hword L(ipred_h_tbl) -  4b
 endfunc
 
+jumptable ipred_h_tbl
+        .word 640b - ipred_h_tbl
+        .word 320b - ipred_h_tbl
+        .word 160b - ipred_h_tbl
+        .word 80b  - ipred_h_tbl
+        .word 40b  - ipred_h_tbl
+endjumptable
+
 // void ipred_dc_top_16bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                              const pixel *const topleft,
 //                              const int width, const int height, const int a,
 //                              const int max_width, const int max_height);
 function ipred_dc_top_16bpc_neon, export=1
         clz             w3,  w3
-        adr             x5,  L(ipred_dc_top_tbl)
+        movrel          x5,  ipred_dc_top_tbl
         sub             w3,  w3,  #25
-        ldrh            w3,  [x5, w3, uxtw #1]
+        ldrsw           x3,  [x5, w3, uxtw #2]
         add             x2,  x2,  #2
-        sub             x5,  x5,  w3, uxtw
+        add             x5,  x5,  x3
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
@@ -408,15 +418,16 @@ function ipred_dc_top_16bpc_neon, export=1
         st1             {v0.8h, v1.8h, v2.8h, v3.8h}, [x6], x1
         b.gt            64b
         ret
-
-L(ipred_dc_top_tbl):
-        .hword L(ipred_dc_top_tbl) - 640b
-        .hword L(ipred_dc_top_tbl) - 320b
-        .hword L(ipred_dc_top_tbl) - 160b
-        .hword L(ipred_dc_top_tbl) -  80b
-        .hword L(ipred_dc_top_tbl) -  40b
 endfunc
 
+jumptable ipred_dc_top_tbl
+        .word 640b - ipred_dc_top_tbl
+        .word 320b - ipred_dc_top_tbl
+        .word 160b - ipred_dc_top_tbl
+        .word 80b  - ipred_dc_top_tbl
+        .word 40b  - ipred_dc_top_tbl
+endjumptable
+
 // void ipred_dc_left_16bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                               const pixel *const topleft,
 //                               const int width, const int height, const int a,
@@ -425,13 +436,13 @@ function ipred_dc_left_16bpc_neon, export=1
         sub             x2,  x2,  w4, uxtw #1
         clz             w3,  w3
         clz             w7,  w4
-        adr             x5,  L(ipred_dc_left_tbl)
+        movrel          x5,  ipred_dc_left_tbl
         sub             w3,  w3,  #20 // 25 leading bits, minus table offset 5
         sub             w7,  w7,  #25
-        ldrh            w3,  [x5, w3, uxtw #1]
-        ldrh            w7,  [x5, w7, uxtw #1]
-        sub             x3,  x5,  w3, uxtw
-        sub             x5,  x5,  w7, uxtw
+        ldrsw           x3,  [x5, w3, uxtw #2]
+        ldrsw           x7,  [x5, w7, uxtw #2]
+        add             x3,  x5,  x3
+        add             x5,  x5,  x7
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
@@ -445,12 +456,13 @@ L(ipred_dc_left_h4):
         br              x3
 L(ipred_dc_left_w4):
         AARCH64_VALID_JUMP_TARGET
+1:
         st1             {v0.4h},  [x0], x1
         st1             {v0.4h},  [x6], x1
         subs            w4,  w4,  #4
         st1             {v0.4h},  [x0], x1
         st1             {v0.4h},  [x6], x1
-        b.gt            L(ipred_dc_left_w4)
+        b.gt            1b
         ret
 
 L(ipred_dc_left_h8):
@@ -462,12 +474,13 @@ L(ipred_dc_left_h8):
         br              x3
 L(ipred_dc_left_w8):
         AARCH64_VALID_JUMP_TARGET
+1:
         st1             {v0.8h},  [x0], x1
         st1             {v0.8h},  [x6], x1
         subs            w4,  w4,  #4
         st1             {v0.8h},  [x0], x1
         st1             {v0.8h},  [x6], x1
-        b.gt            L(ipred_dc_left_w8)
+        b.gt            1b
         ret
 
 L(ipred_dc_left_h16):
@@ -549,20 +562,21 @@ L(ipred_dc_left_w64):
         st1             {v0.8h, v1.8h, v2.8h, v3.8h}, [x6], x1
         b.gt            1b
         ret
-
-L(ipred_dc_left_tbl):
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_h64)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_h32)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_h16)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_h8)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_h4)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_w64)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_w32)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_w16)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_w8)
-        .hword L(ipred_dc_left_tbl) - L(ipred_dc_left_w4)
 endfunc
 
+jumptable ipred_dc_left_tbl
+        .word L(ipred_dc_left_h64) - ipred_dc_left_tbl
+        .word L(ipred_dc_left_h32) - ipred_dc_left_tbl
+        .word L(ipred_dc_left_h16) - ipred_dc_left_tbl
+        .word L(ipred_dc_left_h8)  - ipred_dc_left_tbl
+        .word L(ipred_dc_left_h4)  - ipred_dc_left_tbl
+        .word L(ipred_dc_left_w64) - ipred_dc_left_tbl
+        .word L(ipred_dc_left_w32) - ipred_dc_left_tbl
+        .word L(ipred_dc_left_w16) - ipred_dc_left_tbl
+        .word L(ipred_dc_left_w8)  - ipred_dc_left_tbl
+        .word L(ipred_dc_left_w4)  - ipred_dc_left_tbl
+endjumptable
+
 // void ipred_dc_16bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                          const pixel *const topleft,
 //                          const int width, const int height, const int a,
@@ -573,16 +587,16 @@ function ipred_dc_16bpc_neon, export=1
         clz             w3,  w3
         clz             w6,  w4
         dup             v16.4s, w7               // width + height
-        adr             x5,  L(ipred_dc_tbl)
+        movrel          x5,  ipred_dc_tbl
         rbit            w7,  w7                  // rbit(width + height)
         sub             w3,  w3,  #20            // 25 leading bits, minus table offset 5
         sub             w6,  w6,  #25
         clz             w7,  w7                  // ctz(width + height)
-        ldrh            w3,  [x5, w3, uxtw #1]
-        ldrh            w6,  [x5, w6, uxtw #1]
+        ldrsw           x3,  [x5, w3, uxtw #2]
+        ldrsw           x6,  [x5, w6, uxtw #2]
         neg             w7,  w7                  // -ctz(width + height)
-        sub             x3,  x5,  w3, uxtw
-        sub             x5,  x5,  w6, uxtw
+        add             x3,  x5,  x3
+        add             x5,  x5,  x6
         ushr            v16.4s,  v16.4s,  #1     // (width + height) >> 1
         dup             v17.4s,  w7              // -ctz(width + height)
         add             x6,  x0,  x1
@@ -794,33 +808,34 @@ L(ipred_dc_w64):
         st1             {v0.8h, v1.8h, v2.8h, v3.8h}, [x6], x1
         b.gt            2b
         ret
-
-L(ipred_dc_tbl):
-        .hword L(ipred_dc_tbl) - L(ipred_dc_h64)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_h32)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_h16)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_h8)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_h4)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_w64)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_w32)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_w16)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_w8)
-        .hword L(ipred_dc_tbl) - L(ipred_dc_w4)
 endfunc
 
+jumptable ipred_dc_tbl
+        .word L(ipred_dc_h64) - ipred_dc_tbl
+        .word L(ipred_dc_h32) - ipred_dc_tbl
+        .word L(ipred_dc_h16) - ipred_dc_tbl
+        .word L(ipred_dc_h8)  - ipred_dc_tbl
+        .word L(ipred_dc_h4)  - ipred_dc_tbl
+        .word L(ipred_dc_w64) - ipred_dc_tbl
+        .word L(ipred_dc_w32) - ipred_dc_tbl
+        .word L(ipred_dc_w16) - ipred_dc_tbl
+        .word L(ipred_dc_w8)  - ipred_dc_tbl
+        .word L(ipred_dc_w4)  - ipred_dc_tbl
+endjumptable
+
 // void ipred_paeth_16bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                             const pixel *const topleft,
 //                             const int width, const int height, const int a,
 //                             const int max_width, const int max_height);
 function ipred_paeth_16bpc_neon, export=1
         clz             w9,  w3
-        adr             x5,  L(ipred_paeth_tbl)
+        movrel          x5,  ipred_paeth_tbl
         sub             w9,  w9,  #25
-        ldrh            w9,  [x5, w9, uxtw #1]
+        ldrsw           x9,  [x5, w9, uxtw #2]
         ld1r            {v4.8h},  [x2]
         add             x8,  x2,  #2
         sub             x2,  x2,  #8
-        sub             x5,  x5,  w9, uxtw
+        add             x5,  x5,  x9
         mov             x7,  #-8
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
@@ -933,15 +948,16 @@ function ipred_paeth_16bpc_neon, export=1
         b               1b
 9:
         ret
-
-L(ipred_paeth_tbl):
-        .hword L(ipred_paeth_tbl) - 640b
-        .hword L(ipred_paeth_tbl) - 320b
-        .hword L(ipred_paeth_tbl) - 160b
-        .hword L(ipred_paeth_tbl) -  80b
-        .hword L(ipred_paeth_tbl) -  40b
 endfunc
 
+jumptable ipred_paeth_tbl
+        .word 640b - ipred_paeth_tbl
+        .word 320b - ipred_paeth_tbl
+        .word 160b - ipred_paeth_tbl
+        .word 80b  - ipred_paeth_tbl
+        .word 40b  - ipred_paeth_tbl
+endjumptable
+
 // void ipred_smooth_16bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                              const pixel *const topleft,
 //                              const int width, const int height, const int a,
@@ -951,13 +967,13 @@ function ipred_smooth_16bpc_neon, export=1
         add             x11, x10, w4, uxtw
         add             x10, x10, w3, uxtw
         clz             w9,  w3
-        adr             x5,  L(ipred_smooth_tbl)
+        movrel          x5,  ipred_smooth_tbl
         sub             x12, x2,  w4, uxtw #1
         sub             w9,  w9,  #25
-        ldrh            w9,  [x5, w9, uxtw #1]
+        ldrsw           x9,  [x5, w9, uxtw #2]
         ld1r            {v4.8h},  [x12] // bottom
         add             x8,  x2,  #2
-        sub             x5,  x5,  w9, uxtw
+        add             x5,  x5,  x9
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
@@ -1137,15 +1153,16 @@ function ipred_smooth_16bpc_neon, export=1
         b               1b
 9:
         ret
-
-L(ipred_smooth_tbl):
-        .hword L(ipred_smooth_tbl) - 640b
-        .hword L(ipred_smooth_tbl) - 320b
-        .hword L(ipred_smooth_tbl) - 160b
-        .hword L(ipred_smooth_tbl) -  80b
-        .hword L(ipred_smooth_tbl) -  40b
 endfunc
 
+jumptable ipred_smooth_tbl
+        .word 640b - ipred_smooth_tbl
+        .word 320b - ipred_smooth_tbl
+        .word 160b - ipred_smooth_tbl
+        .word 80b  - ipred_smooth_tbl
+        .word 40b  - ipred_smooth_tbl
+endjumptable
+
 // void ipred_smooth_v_16bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                                const pixel *const topleft,
 //                                const int width, const int height, const int a,
@@ -1154,13 +1171,13 @@ function ipred_smooth_v_16bpc_neon, export=1
         movrel          x7,  X(sm_weights)
         add             x7,  x7,  w4, uxtw
         clz             w9,  w3
-        adr             x5,  L(ipred_smooth_v_tbl)
+        movrel          x5,  ipred_smooth_v_tbl
         sub             x8,  x2,  w4, uxtw #1
         sub             w9,  w9,  #25
-        ldrh            w9,  [x5, w9, uxtw #1]
+        ldrsw           x9,  [x5, w9, uxtw #2]
         ld1r            {v4.8h},  [x8] // bottom
         add             x2,  x2,  #2
-        sub             x5,  x5,  w9, uxtw
+        add             x5,  x5,  x9
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
@@ -1264,15 +1281,16 @@ function ipred_smooth_v_16bpc_neon, export=1
         b               1b
 9:
         ret
-
-L(ipred_smooth_v_tbl):
-        .hword L(ipred_smooth_v_tbl) - 640b
-        .hword L(ipred_smooth_v_tbl) - 320b
-        .hword L(ipred_smooth_v_tbl) - 160b
-        .hword L(ipred_smooth_v_tbl) -  80b
-        .hword L(ipred_smooth_v_tbl) -  40b
 endfunc
 
+jumptable ipred_smooth_v_tbl
+        .word 640b - ipred_smooth_v_tbl
+        .word 320b - ipred_smooth_v_tbl
+        .word 160b - ipred_smooth_v_tbl
+        .word 80b  - ipred_smooth_v_tbl
+        .word 40b  - ipred_smooth_v_tbl
+endjumptable
+
 // void ipred_smooth_h_16bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                                const pixel *const topleft,
 //                                const int width, const int height, const int a,
@@ -1281,12 +1299,12 @@ function ipred_smooth_h_16bpc_neon, export=1
         movrel          x8,  X(sm_weights)
         add             x8,  x8,  w3, uxtw
         clz             w9,  w3
-        adr             x5,  L(ipred_smooth_h_tbl)
+        movrel          x5,  ipred_smooth_h_tbl
         add             x12, x2,  w3, uxtw #1
         sub             w9,  w9,  #25
-        ldrh            w9,  [x5, w9, uxtw #1]
+        ldrsw           x9,  [x5, w9, uxtw #2]
         ld1r            {v5.8h},  [x12] // right
-        sub             x5,  x5,  w9, uxtw
+        add             x5,  x5,  x9
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         br              x5
@@ -1396,15 +1414,16 @@ function ipred_smooth_h_16bpc_neon, export=1
         b               1b
 9:
         ret
-
-L(ipred_smooth_h_tbl):
-        .hword L(ipred_smooth_h_tbl) - 640b
-        .hword L(ipred_smooth_h_tbl) - 320b
-        .hword L(ipred_smooth_h_tbl) - 160b
-        .hword L(ipred_smooth_h_tbl) -  80b
-        .hword L(ipred_smooth_h_tbl) -  40b
 endfunc
 
+jumptable ipred_smooth_h_tbl
+        .word 640b - ipred_smooth_h_tbl
+        .word 320b - ipred_smooth_h_tbl
+        .word 160b - ipred_smooth_h_tbl
+        .word 80b  - ipred_smooth_h_tbl
+        .word 40b  - ipred_smooth_h_tbl
+endjumptable
+
 const padding_mask_buf
         .byte 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
         .byte 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
@@ -1728,11 +1747,11 @@ endfunc
 //                                const int dx, const int max_base_x);
 function ipred_z1_fill1_16bpc_neon, export=1
         clz             w9,  w3
-        adr             x8,  L(ipred_z1_fill1_tbl)
+        movrel          x8,  ipred_z1_fill1_tbl
         sub             w9,  w9,  #25
-        ldrh            w9,  [x8, w9, uxtw #1]
+        ldrsw           x9,  [x8, w9, uxtw #2]
         add             x10, x2,  w6,  uxtw #1    // top[max_base_x]
-        sub             x8,  x8,  w9,  uxtw
+        add             x8,  x8,  x9
         ld1r            {v31.8h}, [x10]           // padding
         mov             w7,  w5
         mov             w15, #64
@@ -1916,15 +1935,16 @@ function ipred_z1_fill1_16bpc_neon, export=1
         add             x13, x13, x1
         mov             w3,  w12
         b               169b
-
-L(ipred_z1_fill1_tbl):
-        .hword L(ipred_z1_fill1_tbl) - 640b
-        .hword L(ipred_z1_fill1_tbl) - 320b
-        .hword L(ipred_z1_fill1_tbl) - 160b
-        .hword L(ipred_z1_fill1_tbl) -  80b
-        .hword L(ipred_z1_fill1_tbl) -  40b
 endfunc
 
+jumptable ipred_z1_fill1_tbl
+        .word 640b - ipred_z1_fill1_tbl
+        .word 320b - ipred_z1_fill1_tbl
+        .word 160b - ipred_z1_fill1_tbl
+        .word 80b  - ipred_z1_fill1_tbl
+        .word 40b  - ipred_z1_fill1_tbl
+endjumptable
+
 function ipred_z1_fill2_16bpc_neon, export=1
         cmp             w3,  #8
         add             x10, x2,  w6,  uxtw       // top[max_base_x]
@@ -2050,11 +2070,11 @@ endconst
 //                                const int dx, const int dy);
 function ipred_z2_fill1_16bpc_neon, export=1
         clz             w10, w4
-        adr             x9,  L(ipred_z2_fill1_tbl)
+        movrel          x9,  ipred_z2_fill1_tbl
         sub             w10, w10, #25
-        ldrh            w10, [x9, w10, uxtw #1]
+        ldrsw           x10, [x9, w10, uxtw #2]
         mov             w8,  #(1 << 6)            // xpos = 1 << 6
-        sub             x9,  x9,  w10, uxtw
+        add             x9,  x9,  x10
         sub             w8,  w8,  w6              // xpos -= dx
 
         movrel          x11, increments
@@ -2814,15 +2834,16 @@ function ipred_z2_fill1_16bpc_neon, export=1
         ldp             d10, d11, [sp, #0x10]
         ldp             d8,  d9,  [sp], 0x40
         ret
-
-L(ipred_z2_fill1_tbl):
-        .hword L(ipred_z2_fill1_tbl) - 640b
-        .hword L(ipred_z2_fill1_tbl) - 320b
-        .hword L(ipred_z2_fill1_tbl) - 160b
-        .hword L(ipred_z2_fill1_tbl) -  80b
-        .hword L(ipred_z2_fill1_tbl) -  40b
 endfunc
 
+jumptable ipred_z2_fill1_tbl
+        .word 640b - ipred_z2_fill1_tbl
+        .word 320b - ipred_z2_fill1_tbl
+        .word 160b - ipred_z2_fill1_tbl
+        .word 80b  - ipred_z2_fill1_tbl
+        .word 40b  - ipred_z2_fill1_tbl
+endjumptable
+
 function ipred_z2_fill2_16bpc_neon, export=1
         cmp             w4,  #8
         mov             w8,  #(2 << 6)            // xpos = 2 << 6
@@ -3432,11 +3453,11 @@ endfunc
 //                                const int dy, const int max_base_y);
 function ipred_z3_fill1_16bpc_neon, export=1
         clz             w9,  w4
-        adr             x8,  L(ipred_z3_fill1_tbl)
+        movrel          x8,  ipred_z3_fill1_tbl
         sub             w9,  w9,  #25
-        ldrh            w9,  [x8, w9, uxtw #1]
+        ldrsw           x9,  [x8, w9, uxtw #2]
         add             x10, x2,  w6,  uxtw #1    // left[max_base_y]
-        sub             x8,  x8,  w9,  uxtw
+        add             x8,  x8,  x9
         ld1r            {v31.8h}, [x10]           // padding
         mov             w7,  w5
         mov             w15, #64
@@ -3637,19 +3658,20 @@ function ipred_z3_fill1_16bpc_neon, export=1
         b               1b
 9:
         ret
-
-L(ipred_z3_fill1_tbl):
-        .hword L(ipred_z3_fill1_tbl) - 640b
-        .hword L(ipred_z3_fill1_tbl) - 320b
-        .hword L(ipred_z3_fill1_tbl) - 160b
-        .hword L(ipred_z3_fill1_tbl) -  80b
-        .hword L(ipred_z3_fill1_tbl) -  40b
 endfunc
 
+jumptable ipred_z3_fill1_tbl
+        .word 640b - ipred_z3_fill1_tbl
+        .word 320b - ipred_z3_fill1_tbl
+        .word 160b - ipred_z3_fill1_tbl
+        .word 80b  - ipred_z3_fill1_tbl
+        .word 40b  - ipred_z3_fill1_tbl
+endjumptable
+
 function ipred_z3_fill_padding_neon, export=0
         cmp             w3,  #8
-        adr             x8,  L(ipred_z3_fill_padding_tbl)
-        b.gt            L(ipred_z3_fill_padding_wide)
+        movrel          x8,  ipred_z3_fill_padding_tbl
+        b.gt            ipred_z3_fill_padding_wide
         // w3 = remaining width, w4 = constant height
         mov             w12, w4
 
@@ -3659,12 +3681,13 @@ function ipred_z3_fill_padding_neon, export=0
         // power of two in the remaining width, and repeating.
         clz             w9,  w3
         sub             w9,  w9,  #25
-        ldrh            w9,  [x8, w9, uxtw #1]
-        sub             x9,  x8,  w9,  uxtw
+        ldrsw           x9,  [x8, w9, uxtw #2]
+        add             x9,  x8,  x9
         br              x9
 
-2:
+20:
         AARCH64_VALID_JUMP_TARGET
+2:
         st1             {v31.s}[0], [x0],  x1
         subs            w4,  w4,  #4
         st1             {v31.s}[0], [x13], x1
@@ -3682,8 +3705,9 @@ function ipred_z3_fill_padding_neon, export=0
         mov             w4,  w12
         b               1b
 
-4:
+40:
         AARCH64_VALID_JUMP_TARGET
+4:
         st1             {v31.4h}, [x0],  x1
         subs            w4,  w4,  #4
         st1             {v31.4h}, [x13], x1
@@ -3701,17 +3725,18 @@ function ipred_z3_fill_padding_neon, export=0
         mov             w4,  w12
         b               1b
 
-8:
-16:
-32:
-64:
+80:
+160:
+320:
+640:
         AARCH64_VALID_JUMP_TARGET
+8:
         st1             {v31.8h}, [x0],  x1
         subs            w4,  w4,  #4
         st1             {v31.8h}, [x13], x1
         st1             {v31.8h}, [x0],  x1
         st1             {v31.8h}, [x13], x1
-        b.gt            4b
+        b.gt            8b
         subs            w3,  w3,  #8
         lsr             x1,  x1,  #1
         msub            x0,  x1,  x12, x0         // ptr -= h * stride
@@ -3725,16 +3750,18 @@ function ipred_z3_fill_padding_neon, export=0
 
 9:
         ret
+endfunc
 
-L(ipred_z3_fill_padding_tbl):
-        .hword L(ipred_z3_fill_padding_tbl) - 64b
-        .hword L(ipred_z3_fill_padding_tbl) - 32b
-        .hword L(ipred_z3_fill_padding_tbl) - 16b
-        .hword L(ipred_z3_fill_padding_tbl) -  8b
-        .hword L(ipred_z3_fill_padding_tbl) -  4b
-        .hword L(ipred_z3_fill_padding_tbl) -  2b
+jumptable ipred_z3_fill_padding_tbl
+        .word 640b - ipred_z3_fill_padding_tbl
+        .word 320b - ipred_z3_fill_padding_tbl
+        .word 160b - ipred_z3_fill_padding_tbl
+        .word 80b  - ipred_z3_fill_padding_tbl
+        .word 40b  - ipred_z3_fill_padding_tbl
+        .word 20b  - ipred_z3_fill_padding_tbl
+endjumptable
 
-L(ipred_z3_fill_padding_wide):
+function ipred_z3_fill_padding_wide
         // Fill a WxH rectangle with padding, with W > 8.
         lsr             x1,  x1,  #1
         mov             w12, w3
@@ -3883,13 +3910,13 @@ function ipred_filter_\bpc\()bpc_neon
         add             x6,  x6,  w5, uxtw
         ld1             {v16.8b, v17.8b, v18.8b, v19.8b}, [x6], #32
         clz             w9,  w3
-        adr             x5,  L(ipred_filter\bpc\()_tbl)
+        movrel          x5,  ipred_filter\bpc\()_tbl
         ld1             {v20.8b, v21.8b, v22.8b}, [x6]
         sub             w9,  w9,  #26
-        ldrh            w9,  [x5, w9, uxtw #1]
+        ldrsw           x9,  [x5, w9, uxtw #2]
         sxtl            v16.8h,  v16.8b
         sxtl            v17.8h,  v17.8b
-        sub             x5,  x5,  w9, uxtw
+        add             x5,  x5,  x9
         sxtl            v18.8h,  v18.8b
         sxtl            v19.8h,  v19.8b
         add             x6,  x0,  x1
@@ -4162,13 +4189,14 @@ function ipred_filter_\bpc\()bpc_neon
         b               1b
 9:
         ret
-
-L(ipred_filter\bpc\()_tbl):
-        .hword L(ipred_filter\bpc\()_tbl) - 320b
-        .hword L(ipred_filter\bpc\()_tbl) - 160b
-        .hword L(ipred_filter\bpc\()_tbl) -  80b
-        .hword L(ipred_filter\bpc\()_tbl) -  40b
 endfunc
+
+jumptable ipred_filter\bpc\()_tbl
+        .word 320b - ipred_filter\bpc\()_tbl
+        .word 160b - ipred_filter\bpc\()_tbl
+        .word 80b  - ipred_filter\bpc\()_tbl
+        .word 40b  - ipred_filter\bpc\()_tbl
+endjumptable
 .endm
 
 filter_fn 10
@@ -4187,12 +4215,12 @@ endfunc
 function pal_pred_16bpc_neon, export=1
         ld1             {v30.8h}, [x2]
         clz             w9,  w4
-        adr             x6,  L(pal_pred_tbl)
+        movrel          x6,  pal_pred_tbl
         sub             w9,  w9,  #25
         movi            v29.16b, #7
-        ldrh            w9,  [x6, w9, uxtw #1]
+        ldrsw           x9,  [x6, w9, uxtw #2]
         movi            v31.8h,  #1, lsl #8
-        sub             x6,  x6,  w9, uxtw
+        add             x6,  x6,  x9
         br              x6
 40:
         AARCH64_VALID_JUMP_TARGET
@@ -4391,15 +4419,16 @@ function pal_pred_16bpc_neon, export=1
         st1             {v4.8h, v5.8h, v6.8h, v7.8h}, [x2], x1
         b.gt            64b
         ret
-
-L(pal_pred_tbl):
-        .hword L(pal_pred_tbl) - 640b
-        .hword L(pal_pred_tbl) - 320b
-        .hword L(pal_pred_tbl) - 160b
-        .hword L(pal_pred_tbl) -  80b
-        .hword L(pal_pred_tbl) -  40b
 endfunc
 
+jumptable pal_pred_tbl
+        .word 640b - pal_pred_tbl
+        .word 320b - pal_pred_tbl
+        .word 160b - pal_pred_tbl
+        .word 80b  - pal_pred_tbl
+        .word 40b  - pal_pred_tbl
+endjumptable
+
 // void ipred_cfl_128_16bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                               const pixel *const topleft,
 //                               const int width, const int height,
@@ -4408,18 +4437,19 @@ endfunc
 function ipred_cfl_128_16bpc_neon, export=1
         dup             v31.8h,  w7   // bitdepth_max
         clz             w9,  w3
-        adr             x7,  L(ipred_cfl_128_tbl)
+        movrel          x7,  ipred_cfl_128_tbl
         sub             w9,  w9,  #26
-        ldrh            w9,  [x7, w9, uxtw #1]
+        ldrsw           x9,  [x7, w9, uxtw #2]
         urshr           v0.8h,   v31.8h,  #1
         dup             v1.8h,   w6   // alpha
-        sub             x7,  x7,  w9, uxtw
+        add             x7,  x7,  x9
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         movi            v30.8h,  #0
         br              x7
 L(ipred_cfl_splat_w4):
         AARCH64_VALID_JUMP_TARGET
+1:
         ld1             {v4.8h, v5.8h}, [x5], #32
         subs            w4,  w4,  #4
         smull           v2.4s,   v4.4h,   v1.4h  // diff = ac * alpha
@@ -4448,10 +4478,11 @@ L(ipred_cfl_splat_w4):
         st1             {v2.d}[1],  [x6], x1
         st1             {v3.d}[0],  [x0], x1
         st1             {v3.d}[1],  [x6], x1
-        b.gt            L(ipred_cfl_splat_w4)
+        b.gt            1b
         ret
 L(ipred_cfl_splat_w8):
         AARCH64_VALID_JUMP_TARGET
+1:
         ld1             {v4.8h, v5.8h}, [x5], #32
         subs            w4,  w4,  #2
         smull           v2.4s,   v4.4h,   v1.4h  // diff = ac * alpha
@@ -4478,7 +4509,7 @@ L(ipred_cfl_splat_w8):
         smin            v3.8h,   v3.8h,   v31.8h
         st1             {v2.8h},  [x0], x1
         st1             {v3.8h},  [x6], x1
-        b.gt            L(ipred_cfl_splat_w8)
+        b.gt            1b
         ret
 L(ipred_cfl_splat_w16):
         AARCH64_VALID_JUMP_TARGET
@@ -4544,15 +4575,16 @@ L(ipred_cfl_splat_w16):
         mov             w3,  w9
         b.gt            1b
         ret
-
-L(ipred_cfl_128_tbl):
-L(ipred_cfl_splat_tbl):
-        .hword L(ipred_cfl_128_tbl) - L(ipred_cfl_splat_w16)
-        .hword L(ipred_cfl_128_tbl) - L(ipred_cfl_splat_w16)
-        .hword L(ipred_cfl_128_tbl) - L(ipred_cfl_splat_w8)
-        .hword L(ipred_cfl_128_tbl) - L(ipred_cfl_splat_w4)
 endfunc
 
+jumptable ipred_cfl_128_tbl
+ipred_cfl_splat_tbl:
+        .word L(ipred_cfl_splat_w16) - ipred_cfl_128_tbl
+        .word L(ipred_cfl_splat_w16) - ipred_cfl_128_tbl
+        .word L(ipred_cfl_splat_w8) - ipred_cfl_128_tbl
+        .word L(ipred_cfl_splat_w4) - ipred_cfl_128_tbl
+endjumptable
+
 // void ipred_cfl_top_16bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                               const pixel *const topleft,
 //                               const int width, const int height,
@@ -4561,12 +4593,12 @@ endfunc
 function ipred_cfl_top_16bpc_neon, export=1
         dup             v31.8h,  w7   // bitdepth_max
         clz             w9,  w3
-        adr             x7,  L(ipred_cfl_top_tbl)
+        movrel          x7,  ipred_cfl_top_tbl
         sub             w9,  w9,  #26
-        ldrh            w9,  [x7, w9, uxtw #1]
+        ldrsw           x9,  [x7, w9, uxtw #2]
         dup             v1.8h,   w6   // alpha
         add             x2,  x2,  #2
-        sub             x7,  x7,  w9, uxtw
+        add             x7,  x7,  x9
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         movi            v30.8h,  #0
@@ -4603,14 +4635,15 @@ function ipred_cfl_top_16bpc_neon, export=1
         rshrn           v0.4h,   v0.4s,   #5
         dup             v0.8h,   v0.h[0]
         b               L(ipred_cfl_splat_w16)
-
-L(ipred_cfl_top_tbl):
-        .hword L(ipred_cfl_top_tbl) - 32b
-        .hword L(ipred_cfl_top_tbl) - 16b
-        .hword L(ipred_cfl_top_tbl) -  8b
-        .hword L(ipred_cfl_top_tbl) -  4b
 endfunc
 
+jumptable ipred_cfl_top_tbl
+        .word 32b - ipred_cfl_top_tbl
+        .word 16b - ipred_cfl_top_tbl
+        .word 8b  - ipred_cfl_top_tbl
+        .word 4b  - ipred_cfl_top_tbl
+endjumptable
+
 // void ipred_cfl_left_16bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                                const pixel *const topleft,
 //                                const int width, const int height,
@@ -4621,15 +4654,15 @@ function ipred_cfl_left_16bpc_neon, export=1
         sub             x2,  x2,  w4, uxtw #1
         clz             w9,  w3
         clz             w8,  w4
-        adr             x10, L(ipred_cfl_splat_tbl)
-        adr             x7,  L(ipred_cfl_left_tbl)
+        movrel          x10, ipred_cfl_splat_tbl
+        movrel          x7,  ipred_cfl_left_tbl
         sub             w9,  w9,  #26
         sub             w8,  w8,  #26
-        ldrh            w9,  [x10, w9, uxtw #1]
-        ldrh            w8,  [x7,  w8, uxtw #1]
+        ldrsw           x9,  [x10, w9, uxtw #2]
+        ldrsw           x8,  [x7,  w8, uxtw #2]
         dup             v1.8h,   w6   // alpha
-        sub             x9,  x10, w9, uxtw
-        sub             x7,  x7,  w8, uxtw
+        add             x9,  x10, x9
+        add             x7,  x7,  x8
         add             x6,  x0,  x1
         lsl             x1,  x1,  #1
         movi            v30.8h,  #0
@@ -4670,14 +4703,15 @@ L(ipred_cfl_left_h32):
         rshrn           v0.4h,   v0.4s,   #5
         dup             v0.8h,   v0.h[0]
         br              x9
-
-L(ipred_cfl_left_tbl):
-        .hword L(ipred_cfl_left_tbl) - L(ipred_cfl_left_h32)
-        .hword L(ipred_cfl_left_tbl) - L(ipred_cfl_left_h16)
-        .hword L(ipred_cfl_left_tbl) - L(ipred_cfl_left_h8)
-        .hword L(ipred_cfl_left_tbl) - L(ipred_cfl_left_h4)
 endfunc
 
+jumptable ipred_cfl_left_tbl
+        .word L(ipred_cfl_left_h32) - ipred_cfl_left_tbl
+        .word L(ipred_cfl_left_h16) - ipred_cfl_left_tbl
+        .word L(ipred_cfl_left_h8)  - ipred_cfl_left_tbl
+        .word L(ipred_cfl_left_h4)  - ipred_cfl_left_tbl
+endjumptable
+
 // void ipred_cfl_16bpc_neon(pixel *dst, const ptrdiff_t stride,
 //                           const pixel *const topleft,
 //                           const int width, const int height,
@@ -4691,16 +4725,16 @@ function ipred_cfl_16bpc_neon, export=1
         clz             w9,  w3
         clz             w6,  w4
         dup             v16.4s, w8               // width + height
-        adr             x7,  L(ipred_cfl_tbl)
+        movrel          x7,  ipred_cfl_tbl
         rbit            w8,  w8                  // rbit(width + height)
         sub             w9,  w9,  #22            // 26 leading bits, minus table offset 4
         sub             w6,  w6,  #26
         clz             w8,  w8                  // ctz(width + height)
-        ldrh            w9,  [x7, w9, uxtw #1]
-        ldrh            w6,  [x7, w6, uxtw #1]
+        ldrsw           x9,  [x7, w9, uxtw #2]
+        ldrsw           x6,  [x7, w6, uxtw #2]
         neg             w8,  w8                  // -ctz(width + height)
-        sub             x9,  x7,  w9, uxtw
-        sub             x7,  x7,  w6, uxtw
+        add             x9,  x7,  x9
+        add             x7,  x7,  x6
         ushr            v16.4s,  v16.4s,  #1     // (width + height) >> 1
         dup             v17.4s,  w8              // -ctz(width + height)
         add             x6,  x0,  x1
@@ -4823,32 +4857,33 @@ L(ipred_cfl_w32):
 1:
         dup             v0.8h,   v0.h[0]
         b               L(ipred_cfl_splat_w16)
-
-L(ipred_cfl_tbl):
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_h32)
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_h16)
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_h8)
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_h4)
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_w32)
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_w16)
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_w8)
-        .hword L(ipred_cfl_tbl) - L(ipred_cfl_w4)
 endfunc
 
+jumptable ipred_cfl_tbl
+        .word L(ipred_cfl_h32) - ipred_cfl_tbl
+        .word L(ipred_cfl_h16) - ipred_cfl_tbl
+        .word L(ipred_cfl_h8)  - ipred_cfl_tbl
+        .word L(ipred_cfl_h4)  - ipred_cfl_tbl
+        .word L(ipred_cfl_w32) - ipred_cfl_tbl
+        .word L(ipred_cfl_w16) - ipred_cfl_tbl
+        .word L(ipred_cfl_w8)  - ipred_cfl_tbl
+        .word L(ipred_cfl_w4)  - ipred_cfl_tbl
+endjumptable
+
 // void cfl_ac_420_16bpc_neon(int16_t *const ac, const pixel *const ypx,
 //                            const ptrdiff_t stride, const int w_pad,
 //                            const int h_pad, const int cw, const int ch);
 function ipred_cfl_ac_420_16bpc_neon, export=1
         clz             w8,  w5
         lsl             w4,  w4,  #2
-        adr             x7,  L(ipred_cfl_ac_420_tbl)
+        movrel          x7,  ipred_cfl_ac_420_tbl
         sub             w8,  w8,  #27
-        ldrh            w8,  [x7, w8, uxtw #1]
+        ldrsw           x8,  [x7, w8, uxtw #2]
         movi            v24.4s,  #0
         movi            v25.4s,  #0
         movi            v26.4s,  #0
         movi            v27.4s,  #0
-        sub             x7,  x7,  w8, uxtw
+        add             x7,  x7,  x8
         sub             w8,  w6,  w4         // height - h_pad
         rbit            w9,  w5              // rbit(width)
         rbit            w10, w6              // rbit(height)
@@ -4980,9 +5015,9 @@ L(ipred_cfl_ac_420_w8_hpad):
 
 L(ipred_cfl_ac_420_w16):
         AARCH64_VALID_JUMP_TARGET
-        adr             x7,  L(ipred_cfl_ac_420_w16_tbl)
-        ldrh            w3,  [x7, w3, uxtw #1]
-        sub             x7,  x7,  w3, uxtw
+        movrel          x7,  ipred_cfl_ac_420_w16_tbl
+        ldrsw           x3,  [x7, w3, uxtw #2]
+        add             x7,  x7,  x3
         br              x7
 
 L(ipred_cfl_ac_420_w16_wpad0):
@@ -5158,34 +5193,35 @@ L(ipred_cfl_ac_420_w16_hpad):
         // Quadruple the height and reuse the w4 summing/subtracting
         lsl             w6,  w6,  #2
         b               L(ipred_cfl_ac_420_w4_calc_subtract_dc)
-
-L(ipred_cfl_ac_420_tbl):
-        .hword L(ipred_cfl_ac_420_tbl) - L(ipred_cfl_ac_420_w16)
-        .hword L(ipred_cfl_ac_420_tbl) - L(ipred_cfl_ac_420_w8)
-        .hword L(ipred_cfl_ac_420_tbl) - L(ipred_cfl_ac_420_w4)
-        .hword 0
-
-L(ipred_cfl_ac_420_w16_tbl):
-        .hword L(ipred_cfl_ac_420_w16_tbl) - L(ipred_cfl_ac_420_w16_wpad0)
-        .hword L(ipred_cfl_ac_420_w16_tbl) - L(ipred_cfl_ac_420_w16_wpad1)
-        .hword L(ipred_cfl_ac_420_w16_tbl) - L(ipred_cfl_ac_420_w16_wpad2)
-        .hword L(ipred_cfl_ac_420_w16_tbl) - L(ipred_cfl_ac_420_w16_wpad3)
 endfunc
 
+jumptable ipred_cfl_ac_420_tbl
+        .word L(ipred_cfl_ac_420_w16) - ipred_cfl_ac_420_tbl
+        .word L(ipred_cfl_ac_420_w8)  - ipred_cfl_ac_420_tbl
+        .word L(ipred_cfl_ac_420_w4)  - ipred_cfl_ac_420_tbl
+endjumptable
+
+jumptable ipred_cfl_ac_420_w16_tbl
+        .word L(ipred_cfl_ac_420_w16_wpad0) - ipred_cfl_ac_420_w16_tbl
+        .word L(ipred_cfl_ac_420_w16_wpad1) - ipred_cfl_ac_420_w16_tbl
+        .word L(ipred_cfl_ac_420_w16_wpad2) - ipred_cfl_ac_420_w16_tbl
+        .word L(ipred_cfl_ac_420_w16_wpad3) - ipred_cfl_ac_420_w16_tbl
+endjumptable
+
 // void cfl_ac_422_16bpc_neon(int16_t *const ac, const pixel *const ypx,
 //                            const ptrdiff_t stride, const int w_pad,
 //                            const int h_pad, const int cw, const int ch);
 function ipred_cfl_ac_422_16bpc_neon, export=1
         clz             w8,  w5
         lsl             w4,  w4,  #2
-        adr             x7,  L(ipred_cfl_ac_422_tbl)
+        movrel          x7,  ipred_cfl_ac_422_tbl
         sub             w8,  w8,  #27
-        ldrh            w8,  [x7, w8, uxtw #1]
+        ldrsw           x8,  [x7, w8, uxtw #2]
         movi            v24.4s,  #0
         movi            v25.4s,  #0
         movi            v26.4s,  #0
         movi            v27.4s,  #0
-        sub             x7,  x7,  w8, uxtw
+        add             x7,  x7,  x8
         sub             w8,  w6,  w4         // height - h_pad
         rbit            w9,  w5              // rbit(width)
         rbit            w10, w6              // rbit(height)
@@ -5286,9 +5322,9 @@ L(ipred_cfl_ac_422_w8_wpad):
 
 L(ipred_cfl_ac_422_w16):
         AARCH64_VALID_JUMP_TARGET
-        adr             x7,  L(ipred_cfl_ac_422_w16_tbl)
-        ldrh            w3,  [x7, w3, uxtw #1]
-        sub             x7,  x7,  w3, uxtw
+        movrel          x7,  ipred_cfl_ac_422_w16_tbl
+        ldrsw           x3,  [x7, w3, uxtw #2]
+        add             x7,  x7,  x3
         br              x7
 
 L(ipred_cfl_ac_422_w16_wpad0):
@@ -5406,34 +5442,35 @@ L(ipred_cfl_ac_422_w16_wpad3):
         mov             v0.16b,  v2.16b
         mov             v1.16b,  v3.16b
         b               L(ipred_cfl_ac_420_w16_hpad)
-
-L(ipred_cfl_ac_422_tbl):
-        .hword L(ipred_cfl_ac_422_tbl) - L(ipred_cfl_ac_422_w16)
-        .hword L(ipred_cfl_ac_422_tbl) - L(ipred_cfl_ac_422_w8)
-        .hword L(ipred_cfl_ac_422_tbl) - L(ipred_cfl_ac_422_w4)
-        .hword 0
-
-L(ipred_cfl_ac_422_w16_tbl):
-        .hword L(ipred_cfl_ac_422_w16_tbl) - L(ipred_cfl_ac_422_w16_wpad0)
-        .hword L(ipred_cfl_ac_422_w16_tbl) - L(ipred_cfl_ac_422_w16_wpad1)
-        .hword L(ipred_cfl_ac_422_w16_tbl) - L(ipred_cfl_ac_422_w16_wpad2)
-        .hword L(ipred_cfl_ac_422_w16_tbl) - L(ipred_cfl_ac_422_w16_wpad3)
 endfunc
 
+jumptable ipred_cfl_ac_422_tbl
+        .word L(ipred_cfl_ac_422_w16) - ipred_cfl_ac_422_tbl
+        .word L(ipred_cfl_ac_422_w8)  - ipred_cfl_ac_422_tbl
+        .word L(ipred_cfl_ac_422_w4)  - ipred_cfl_ac_422_tbl
+endjumptable
+
+jumptable ipred_cfl_ac_422_w16_tbl
+        .word L(ipred_cfl_ac_422_w16_wpad0) - ipred_cfl_ac_422_w16_tbl
+        .word L(ipred_cfl_ac_422_w16_wpad1) - ipred_cfl_ac_422_w16_tbl
+        .word L(ipred_cfl_ac_422_w16_wpad2) - ipred_cfl_ac_422_w16_tbl
+        .word L(ipred_cfl_ac_422_w16_wpad3) - ipred_cfl_ac_422_w16_tbl
+endjumptable
+
 // void cfl_ac_444_16bpc_neon(int16_t *const ac, const pixel *const ypx,
 //                            const ptrdiff_t stride, const int w_pad,
 //                            const int h_pad, const int cw, const int ch);
 function ipred_cfl_ac_444_16bpc_neon, export=1
         clz             w8,  w5
         lsl             w4,  w4,  #2
-        adr             x7,  L(ipred_cfl_ac_444_tbl)
+        movrel          x7,  ipred_cfl_ac_444_tbl
         sub             w8,  w8,  #26
-        ldrh            w8,  [x7, w8, uxtw #1]
+        ldrsw           x8,  [x7, w8, uxtw #2]
         movi            v24.4s,  #0
         movi            v25.4s,  #0
         movi            v26.4s,  #0
         movi            v27.4s,  #0
-        sub             x7,  x7,  w8, uxtw
+        add             x7,  x7,  x8
         sub             w8,  w6,  w4         // height - h_pad
         rbit            w9,  w5              // rbit(width)
         rbit            w10, w6              // rbit(height)
@@ -5542,10 +5579,11 @@ L(ipred_cfl_ac_444_w16_wpad):
 
 L(ipred_cfl_ac_444_w32):
         AARCH64_VALID_JUMP_TARGET
-        adr             x7,  L(ipred_cfl_ac_444_w32_tbl)
-        ldrh            w3,  [x7, w3, uxtw] // (w3>>1) << 1
+        movrel          x7,  ipred_cfl_ac_444_w32_tbl
+        lsr             w3,  w3,  #1
+        ldrsw           x3,  [x7, w3, uxtw #2]
         lsr             x2,  x2,  #1 // Restore the stride to one line increments
-        sub             x7,  x7,  w3, uxtw
+        add             x7,  x7,  x3
         br              x7
 
 L(ipred_cfl_ac_444_w32_wpad0):
@@ -5659,16 +5697,18 @@ L(ipred_cfl_ac_444_w32_hpad):
         //  Multiply the height by eight and reuse the w4 subtracting
         lsl             w6,  w6,  #3
         b               L(ipred_cfl_ac_420_w4_calc_subtract_dc)
-
-L(ipred_cfl_ac_444_tbl):
-        .hword L(ipred_cfl_ac_444_tbl) - L(ipred_cfl_ac_444_w32)
-        .hword L(ipred_cfl_ac_444_tbl) - L(ipred_cfl_ac_444_w16)
-        .hword L(ipred_cfl_ac_444_tbl) - L(ipred_cfl_ac_444_w8)
-        .hword L(ipred_cfl_ac_444_tbl) - L(ipred_cfl_ac_444_w4)
-
-L(ipred_cfl_ac_444_w32_tbl):
-        .hword L(ipred_cfl_ac_444_w32_tbl) - L(ipred_cfl_ac_444_w32_wpad0)
-        .hword L(ipred_cfl_ac_444_w32_tbl) - L(ipred_cfl_ac_444_w32_wpad2)
-        .hword L(ipred_cfl_ac_444_w32_tbl) - L(ipred_cfl_ac_444_w32_wpad4)
-        .hword L(ipred_cfl_ac_444_w32_tbl) - L(ipred_cfl_ac_444_w32_wpad6)
 endfunc
+
+jumptable ipred_cfl_ac_444_tbl
+        .word L(ipred_cfl_ac_444_w32) - ipred_cfl_ac_444_tbl
+        .word L(ipred_cfl_ac_444_w16) - ipred_cfl_ac_444_tbl
+        .word L(ipred_cfl_ac_444_w8)  - ipred_cfl_ac_444_tbl
+        .word L(ipred_cfl_ac_444_w4)  - ipred_cfl_ac_444_tbl
+endjumptable
+
+jumptable ipred_cfl_ac_444_w32_tbl
+        .word L(ipred_cfl_ac_444_w32_wpad0) - ipred_cfl_ac_444_w32_tbl
+        .word L(ipred_cfl_ac_444_w32_wpad2) - ipred_cfl_ac_444_w32_tbl
+        .word L(ipred_cfl_ac_444_w32_wpad4) - ipred_cfl_ac_444_w32_tbl
+        .word L(ipred_cfl_ac_444_w32_wpad6) - ipred_cfl_ac_444_w32_tbl
+endjumptable
diff --git a/src/arm/64/looprestoration_common.S b/src/arm/64/looprestoration_common.S
index 745f6c2..c10a9f3 100644
--- a/src/arm/64/looprestoration_common.S
+++ b/src/arm/64/looprestoration_common.S
@@ -28,14 +28,77 @@
 #include "src/arm/asm.S"
 #include "util.S"
 
+// Series of LUTs for efficiently computing sgr's 1 - x/(x+1) table.
+// In the comments, let RefTable denote the original, reference table.
+const x_by_x_tables
+// RangeMins
+//
+// Min(RefTable[i*8:i*8+8])
+// First two values are zeroed.
+//
+// Lookup using RangeMins[(x >> 3)]
+        .byte 0,  0, 11,  8,  6,  5,  5,  4,  4,  3,  3,  3,  2,  2,  2,  2
+        .byte 2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0
+
+// DiffMasks
+//
+// This contains a bit pattern, indicating at which index positions the value of RefTable changes. For each range
+// in the RangeMins table (covering 8 RefTable entries), we have one byte; each bit indicates whether the value of
+// RefTable changes at that particular index.
+// Using popcount, we can integrate the diff bit field. By shifting away bits in a byte, we can refine the range of
+// the integral. Finally, adding the integral to RangeMins[(x>>3)] reconstructs RefTable (for x > 15).
+//
+// Lookup using DiffMasks[(x >> 3)]
+        .byte 0x00, 0x00, 0xD4, 0x44
+        .byte 0x42, 0x04, 0x00, 0x00
+        .byte 0x00, 0x80, 0x00, 0x00
+        .byte 0x04, 0x00, 0x00, 0x00
+        .byte 0x00, 0x00, 0x00, 0x00
+        .byte 0x00, 0x40, 0x00, 0x00
+        .byte 0x00, 0x00, 0x00, 0x00
+        .byte 0x00, 0x00, 0x00, 0x02
+// Binary form:
+// 0b00000000, 0b00000000, 0b11010100, 0b01000100
+// 0b01000010, 0b00000100, 0b00000000, 0b00000000
+// 0b00000000, 0b10000000, 0b00000000, 0b00000000
+// 0b00000100, 0b00000000, 0b00000000, 0b00000000
+// 0b00000000, 0b00000000, 0b00000000, 0b00000000
+// 0b00000000, 0b01000000, 0b00000000, 0b00000000
+// 0b00000000, 0b00000000, 0b00000000, 0b00000000
+// 0b00000000, 0b00000000, 0b00000000, 0b00000010
+
+// RefLo
+//
+// RefTable[0:16]
+//      i.e. First 16 elements of the original table.
+// Add to the sum obtained in the rest of the other lut logic to include the first 16 bytes of RefTable.
+//
+// Lookup using RangeMins[x] (tbl will replace x > 15 with 0)
+        .byte 255, 128,  85,  64,  51,  43,  37,  32, 28,  26,  23,  21,  20,  18,  17,  16
+
+// Pseudo assembly
+//
+// hi_bits = x >> 3
+// tbl             ref,    {RefLo}, x
+// tbl             diffs,  {DiffMasks[0:16], DiffMasks[16:32]}, hi_bits
+// tbl             min,    {RangeMins[0:16], RangeMins[16:32]}, hi_bits
+// lo_bits = x & 0x7
+// diffs = diffs << lo_bits
+// ref = ref + min
+// integral = popcnt(diffs)
+// ref = ref + integral
+// return ref
+endconst
+
 // void dav1d_sgr_box3_vert_neon(int32_t **sumsq, int16_t **sum,
 //                               int32_t *AA, int16_t *BB,
 //                               const int w, const int s,
 //                               const int bitdepth_max);
 function sgr_box3_vert_neon, export=1
-        stp             d8,  d9,  [sp, #-0x30]!
+        stp             d8,  d9,  [sp, #-0x40]!
         stp             d10, d11, [sp, #0x10]
         stp             d12, d13, [sp, #0x20]
+        stp             d14, d15, [sp, #0x30]
 
         add             w4,  w4,  #2
         clz             w9,  w6        // bitdepth_max
@@ -49,93 +112,112 @@ function sgr_box3_vert_neon, export=1
         movi            v31.4s,   #9   // n
 
         sub             w9,  w9,  #24  // -bitdepth_min_8
-        movrel          x12, X(sgr_x_by_x)
+        movrel          x12, x_by_x_tables
         mov             w13, #455      // one_by_x
-        ld1             {v16.16b, v17.16b, v18.16b}, [x12]
+        ld1             {v24.16b, v25.16b, v26.16b, v27.16b}, [x12] // RangeMins, DiffMasks
+        movi            v22.16b, #0x7
+        ldr             q23, [x12, #64] //RefLo
         dup             v6.8h,    w9   // -bitdepth_min_8
-        movi            v19.16b,  #5
-        movi            v20.8b,   #55  // idx of last 5
-        movi            v21.8b,   #72  // idx of last 4
-        movi            v22.8b,   #101 // idx of last 3
-        movi            v23.8b,   #169 // idx of last 2
-        movi            v24.8b,   #254 // idx of last 1
         saddl           v7.4s,    v6.4h,   v6.4h  // -2*bitdepth_min_8
         movi            v29.8h,   #1, lsl #8
         dup             v30.4s,   w13  // one_by_x
 
-        sub             v16.16b, v16.16b, v19.16b
-        sub             v17.16b, v17.16b, v19.16b
-        sub             v18.16b, v18.16b, v19.16b
-
-        ld1             {v8.4s,  v9.4s},  [x5], #32
-        ld1             {v10.4s, v11.4s}, [x6], #32
-        ld1             {v12.8h},         [x7], #16
-        ld1             {v13.8h},         [x8], #16
-        ld1             {v0.4s, v1.4s},   [x0], #32
-        ld1             {v2.8h},          [x1], #16
+        ld1             {v8.4s,  v9.4s,  v10.4s, v11.4s}, [x5], #64
+        ld1             {v12.4s, v13.4s, v14.4s, v15.4s}, [x6], #64
+        ld1             {v16.4s, v17.4s, v18.4s, v19.4s}, [x0], #64
+        ld1             {v20.8h, v21.8h}, [x8], #32
+        ld1             {v0.8h,  v1.8h},  [x7], #32
 1:
-
-        add             v8.4s,   v8.4s,   v10.4s
-        add             v9.4s,   v9.4s,   v11.4s
-
-        add             v12.8h,  v12.8h,  v13.8h
-
-        subs            w4,  w4,  #8
-        add             v0.4s,   v0.4s,   v8.4s
-        add             v1.4s,   v1.4s,   v9.4s
-        add             v2.8h,   v2.8h,   v12.8h
-
-        srshl           v0.4s,   v0.4s,   v7.4s
-        srshl           v1.4s,   v1.4s,   v7.4s
-        srshl           v4.8h,   v2.8h,   v6.8h
-        mul             v0.4s,   v0.4s,   v31.4s // a * n
-        mul             v1.4s,   v1.4s,   v31.4s // a * n
-        umull           v3.4s,   v4.4h,   v4.4h  // b * b
-        umull2          v4.4s,   v4.8h,   v4.8h  // b * b
-        uqsub           v0.4s,   v0.4s,   v3.4s  // imax(a * n - b * b, 0)
-        uqsub           v1.4s,   v1.4s,   v4.4s  // imax(a * n - b * b, 0)
-        mul             v0.4s,   v0.4s,   v28.4s // p * s
-        mul             v1.4s,   v1.4s,   v28.4s // p * s
-        ld1             {v8.4s,  v9.4s},  [x5], #32
-        uqshrn          v0.4h,   v0.4s,   #16
-        uqshrn2         v0.8h,   v1.4s,   #16
-        ld1             {v10.4s, v11.4s}, [x6], #32
-        uqrshrn         v0.8b,   v0.8h,   #4     // imin(z, 255)
-
-        ld1             {v12.8h},         [x7], #16
-
-        cmhi            v25.8b,  v0.8b,   v20.8b // = -1 if sgr_x_by_x[v0] < 5
-        cmhi            v26.8b,  v0.8b,   v21.8b // = -1 if sgr_x_by_x[v0] < 4
-        tbl             v1.8b, {v16.16b,  v17.16b, v18.16b}, v0.8b
-        cmhi            v27.8b,  v0.8b,   v22.8b // = -1 if sgr_x_by_x[v0] < 3
-        cmhi            v4.8b,   v0.8b,   v23.8b // = -1 if sgr_x_by_x[v0] < 2
-        add             v25.8b,  v25.8b,  v26.8b
-        cmhi            v5.8b,   v0.8b,   v24.8b // = -1 if sgr_x_by_x[v0] < 1
-        add             v27.8b,  v27.8b,  v4.8b
-        add             v5.8b,   v5.8b,   v19.8b
-        add             v25.8b,  v25.8b,  v27.8b
-        add             v5.8b,   v1.8b,   v5.8b
-        ld1             {v13.8h},         [x8], #16
-        add             v5.8b,   v5.8b,   v25.8b
-        ld1             {v0.4s, v1.4s},   [x0], #32
-        uxtl            v5.8h,   v5.8b           // x
-
-        umull           v3.4s,   v5.4h,   v2.4h  // x * BB[i]
-        umull2          v4.4s,   v5.8h,   v2.8h  // x * BB[i]
-        mul             v3.4s,   v3.4s,   v30.4s // x * BB[i] * sgr_one_by_x
-        mul             v4.4s,   v4.4s,   v30.4s // x * BB[i] * sgr_one_by_x
-        srshr           v3.4s,   v3.4s,   #12    // AA[i]
-        srshr           v4.4s,   v4.4s,   #12    // AA[i]
-        sub             v5.8h,   v29.8h,  v5.8h  // 256 - x
-        ld1             {v2.8h},          [x1], #16
-
-        st1             {v3.4s, v4.4s}, [x2], #32
-        st1             {v5.8h}, [x3], #16
+        ld1             {v2.8h,  v3.8h},   [x1], #32
+        add             v8.4s,   v8.4s,   v12.4s
+        add             v9.4s,   v9.4s,   v13.4s
+        add             v10.4s,  v10.4s,  v14.4s
+        add             v11.4s,  v11.4s,  v15.4s
+        add             v0.8h,   v0.8h,   v20.8h
+        add             v1.8h,   v1.8h,   v21.8h
+
+        add             v16.4s,  v16.4s,  v8.4s
+        add             v17.4s,  v17.4s,  v9.4s
+        add             v18.4s,  v18.4s,  v10.4s
+        add             v19.4s,  v19.4s,  v11.4s
+        add             v4.8h,   v2.8h,   v0.8h
+        add             v5.8h,   v3.8h,   v1.8h
+
+        srshl           v16.4s,  v16.4s,  v7.4s
+        srshl           v17.4s,  v17.4s,  v7.4s
+        srshl           v18.4s,  v18.4s,  v7.4s
+        srshl           v19.4s,  v19.4s,  v7.4s
+        srshl           v9.8h,   v4.8h,   v6.8h
+        srshl           v13.8h,  v5.8h,   v6.8h
+        mul             v16.4s,  v16.4s,  v31.4s // a * n
+        mul             v17.4s,  v17.4s,  v31.4s // a * n
+        mul             v18.4s,  v18.4s,  v31.4s // a * n
+        mul             v19.4s,  v19.4s,  v31.4s // a * n
+        umull           v8.4s,   v9.4h,   v9.4h  // b * b
+        umull2          v9.4s,   v9.8h,   v9.8h  // b * b
+        umull           v12.4s,  v13.4h,  v13.4h // b * b
+        umull2          v13.4s,  v13.8h,  v13.8h // b * b
+        uqsub           v16.4s,  v16.4s,  v8.4s  // imax(a * n - b * b, 0)
+        uqsub           v17.4s,  v17.4s,  v9.4s  // imax(a * n - b * b, 0)
+        uqsub           v18.4s,  v18.4s,  v12.4s // imax(a * n - b * b, 0)
+        uqsub           v19.4s,  v19.4s,  v13.4s // imax(a * n - b * b, 0)
+        mul             v16.4s,  v16.4s,  v28.4s // p * s
+        mul             v17.4s,  v17.4s,  v28.4s // p * s
+        mul             v18.4s,  v18.4s,  v28.4s // p * s
+        mul             v19.4s,  v19.4s,  v28.4s // p * s
+        uqshrn          v16.4h,  v16.4s,  #16
+        uqshrn2         v16.8h,  v17.4s,  #16
+        uqshrn          v18.4h,  v18.4s,  #16
+        uqshrn2         v18.8h,  v19.4s,  #16
+        uqrshrn         v1.8b,   v16.8h,  #4     // imin(z, 255)
+        uqrshrn2        v1.16b,  v18.8h,  #4     // imin(z, 255)
+
+        ld1             {v16.4s, v17.4s}, [x0], #32
+        subs            w4,  w4,  #16
+
+        ushr            v0.16b,  v1.16b,  #3
+        ld1             {v8.4s,  v9.4s}, [x5], #32
+        tbl             v2.16b,  {v26.16b, v27.16b}, v0.16b // RangeMins
+        tbl             v0.16b,  {v24.16b, v25.16b}, v0.16b // DiffMasks
+        tbl             v3.16b,  {v23.16b}, v1.16b          // RefLo
+        and             v1.16b,  v1.16b,   v22.16b
+        ld1             {v12.4s, v13.4s}, [x6], #32
+        ushl            v1.16b,  v2.16b,  v1.16b
+        ld1             {v20.8h, v21.8h}, [x8], #32
+        add             v3.16b,  v3.16b,  v0.16b
+        cnt             v1.16b,  v1.16b
+        ld1             {v18.4s, v19.4s}, [x0], #32
+        add             v3.16b,  v3.16b,  v1.16b
+        ld1             {v10.4s, v11.4s}, [x5], #32
+        uxtl            v0.8h,   v3.8b           // x
+        uxtl2           v1.8h,   v3.16b          // x
+
+        ld1             {v14.4s, v15.4s}, [x6], #32
+
+        umull           v2.4s,   v0.4h,   v4.4h // x * BB[i]
+        umull2          v3.4s,   v0.8h,   v4.8h // x * BB[i]
+        umull           v4.4s,   v1.4h,   v5.4h // x * BB[i]
+        umull2          v5.4s,   v1.8h,   v5.8h // x * BB[i]
+        sub             v0.8h,   v29.8h,  v0.8h // 256 - x
+        sub             v1.8h,   v29.8h,  v1.8h // 256 - x
+        mul             v2.4s,   v2.4s,  v30.4s // x * BB[i] * sgr_one_by_x
+        mul             v3.4s,   v3.4s,  v30.4s // x * BB[i] * sgr_one_by_x
+        mul             v4.4s,   v4.4s,  v30.4s // x * BB[i] * sgr_one_by_x
+        mul             v5.4s,   v5.4s,  v30.4s // x * BB[i] * sgr_one_by_x
+        st1             {v0.8h, v1.8h}, [x3], #32
+        ld1             {v0.8h, v1.8h}, [x7], #32
+        srshr           v2.4s,   v2.4s,  #12    // AA[i]
+        srshr           v3.4s,   v3.4s,  #12    // AA[i]
+        srshr           v4.4s,   v4.4s,  #12    // AA[i]
+        srshr           v5.4s,   v5.4s,  #12    // AA[i]
+
+        st1             {v2.4s, v3.4s, v4.4s, v5.4s}, [x2], #64
         b.gt            1b
 
+        ldp             d14, d15, [sp, #0x30]
         ldp             d12, d13, [sp, #0x20]
         ldp             d10, d11, [sp, #0x10]
-        ldp             d8,  d9,  [sp], 0x30
+        ldp             d8,  d9,  [sp], 0x40
         ret
 endfunc
 
@@ -144,10 +226,9 @@ endfunc
 //                               const int w, const int s,
 //                               const int bitdepth_max);
 function sgr_box5_vert_neon, export=1
-        stp             d8,  d9,  [sp, #-0x40]!
+        stp             d8,  d9,  [sp, #-0x30]!
         stp             d10, d11, [sp, #0x10]
         stp             d12, d13, [sp, #0x20]
-        stp             d14, d15, [sp, #0x30]
 
         add             w4,  w4,  #2
         clz             w15, w6        // bitdepth_max
@@ -163,24 +244,19 @@ function sgr_box5_vert_neon, export=1
         movi            v31.4s,   #25   // n
 
         sub             w15, w15, #24  // -bitdepth_min_8
-        movrel          x13, X(sgr_x_by_x)
-        mov             w14, #164      // one_by_x
-        ld1             {v16.16b, v17.16b, v18.16b}, [x13]
+        movrel          x13, x_by_x_tables
+        movi            v30.4s,  #164
+        ld1             {v24.16b, v25.16b, v26.16b, v27.16b}, [x13] // RangeMins, DiffMasks
         dup             v6.8h,   w15  // -bitdepth_min_8
-        movi            v19.16b, #5
-        movi            v24.8b,  #254 // idx of last 1
+        movi            v19.8b,  #0x7
+        ldr             q18, [x13, #64] // RefLo
         saddl           v7.4s,   v6.4h,   v6.4h  // -2*bitdepth_min_8
         movi            v29.8h,  #1, lsl #8
-        dup             v30.4s,  w14  // one_by_x
-
-        sub             v16.16b, v16.16b, v19.16b
-        sub             v17.16b, v17.16b, v19.16b
-        sub             v18.16b, v18.16b, v19.16b
 
         ld1             {v8.4s,  v9.4s},  [x5], #32
         ld1             {v10.4s, v11.4s}, [x6], #32
         ld1             {v12.4s, v13.4s}, [x7], #32
-        ld1             {v14.4s, v15.4s}, [x8], #32
+        ld1             {v16.4s, v17.4s}, [x8], #32
         ld1             {v20.8h},         [x9], #16
         ld1             {v21.8h},         [x10], #16
         ld1             {v22.8h},         [x11], #16
@@ -191,8 +267,8 @@ function sgr_box5_vert_neon, export=1
 1:
         add             v8.4s,   v8.4s,   v10.4s
         add             v9.4s,   v9.4s,   v11.4s
-        add             v12.4s,  v12.4s,  v14.4s
-        add             v13.4s,  v13.4s,  v15.4s
+        add             v12.4s,  v12.4s,  v16.4s
+        add             v13.4s,  v13.4s,  v17.4s
 
         add             v20.8h,  v20.8h,  v21.8h
         add             v22.8h,  v22.8h,  v23.8h
@@ -207,11 +283,6 @@ function sgr_box5_vert_neon, export=1
 
         subs            w4,  w4,  #8
 
-        movi            v20.8b,  #55  // idx of last 5
-        movi            v21.8b,  #72  // idx of last 4
-        movi            v22.8b,  #101 // idx of last 3
-        movi            v23.8b,  #169 // idx of last 2
-
         srshl           v0.4s,   v0.4s,   v7.4s
         srshl           v1.4s,   v1.4s,   v7.4s
         srshl           v4.8h,   v2.8h,   v6.8h
@@ -231,22 +302,19 @@ function sgr_box5_vert_neon, export=1
 
         ld1             {v12.4s, v13.4s}, [x7], #32
 
-        cmhi            v25.8b,  v0.8b,   v20.8b // = -1 if sgr_x_by_x[v0] < 5
-        cmhi            v26.8b,  v0.8b,   v21.8b // = -1 if sgr_x_by_x[v0] < 4
-        tbl             v1.8b, {v16.16b,  v17.16b, v18.16b}, v0.8b
-        cmhi            v27.8b,  v0.8b,   v22.8b // = -1 if sgr_x_by_x[v0] < 3
-        cmhi            v4.8b,   v0.8b,   v23.8b // = -1 if sgr_x_by_x[v0] < 2
-        ld1             {v14.4s, v15.4s}, [x8], #32
-        add             v25.8b,  v25.8b,  v26.8b
-        cmhi            v5.8b,   v0.8b,   v24.8b // = -1 if sgr_x_by_x[v0] < 1
-        add             v27.8b,  v27.8b,  v4.8b
+        ushr            v1.8b,   v0.8b,  #3
+        ld1             {v16.4s, v17.4s}, [x8], #32
+        tbl             v5.8b,   {v26.16b, v27.16b}, v1.8b // RangeMins
+        tbl             v1.8b,   {v24.16b, v25.16b}, v1.8b // DiffMasks
+        tbl             v4.8b,   {v18.16b}, v0.8b          // RefLo
+        and             v0.8b,   v0.8b,  v19.8b
         ld1             {v20.8h},         [x9], #16
-        add             v5.8b,   v5.8b,   v19.8b
-        add             v25.8b,  v25.8b,  v27.8b
+        ushl            v5.8b,   v5.8b,  v0.8b
+        add             v4.8b,   v4.8b,  v1.8b
         ld1             {v21.8h},         [x10], #16
-        add             v5.8b,   v1.8b,   v5.8b
+        cnt             v5.8b,   v5.8b
         ld1             {v22.8h},         [x11], #16
-        add             v5.8b,   v5.8b,   v25.8b
+        add             v5.8b,   v4.8b,  v5.8b
         ld1             {v23.8h},         [x12], #16
         uxtl            v5.8h,   v5.8b           // x
 
@@ -264,9 +332,8 @@ function sgr_box5_vert_neon, export=1
         st1             {v5.8h}, [x3], #16
         b.gt            1b
 
-        ldp             d14, d15, [sp, #0x30]
         ldp             d12, d13, [sp, #0x20]
         ldp             d10, d11, [sp, #0x10]
-        ldp             d8,  d9,  [sp], 0x40
+        ldp             d8,  d9,  [sp], 0x30
         ret
 endfunc
diff --git a/src/arm/64/mc.S b/src/arm/64/mc.S
index 736b2bb..24ef4d2 100644
--- a/src/arm/64/mc.S
+++ b/src/arm/64/mc.S
@@ -79,11 +79,11 @@ function \type\()_8bpc_neon, export=1
 .ifc \type, mask
         movi            v31.16b, #256-2
 .endif
-        adr             x7,  L(\type\()_tbl)
+        movrel          x7,  \type\()_tbl
         sub             w4,  w4,  #24
-        ldrh            w4,  [x7, x4, lsl #1]
+        ldrsw           x4,  [x7, x4, lsl #2]
         \type           v4,  v0,  v1,  v2,  v3
-        sub             x7,  x7,  w4, uxtw
+        add             x7,  x7,  x4
         br              x7
 40:
         AARCH64_VALID_JUMP_TARGET
@@ -119,17 +119,18 @@ function \type\()_8bpc_neon, export=1
         add             x7,  x0,  x1
         lsl             x1,  x1,  #1
 8:
-        st1             {v4.d}[0],  [x0], x1
+        st1             {v4.8b},    [x0], x1
         \type           v5,  v0,  v1,  v2,  v3
         st1             {v4.d}[1],  [x7], x1
-        st1             {v5.d}[0],  [x0], x1
+        st1             {v5.8b},    [x0], x1
         subs            w5,  w5,  #4
         st1             {v5.d}[1],  [x7], x1
         b.le            0f
         \type           v4,  v0,  v1,  v2,  v3
         b               8b
-16:
+160:
         AARCH64_VALID_JUMP_TARGET
+16:
         \type           v5,  v0,  v1,  v2,  v3
         st1             {v4.16b}, [x0], x1
         \type           v6,  v0,  v1,  v2,  v3
@@ -192,14 +193,16 @@ function \type\()_8bpc_neon, export=1
         b               128b
 0:
         ret
-L(\type\()_tbl):
-        .hword L(\type\()_tbl) - 1280b
-        .hword L(\type\()_tbl) -  640b
-        .hword L(\type\()_tbl) -  320b
-        .hword L(\type\()_tbl) -   16b
-        .hword L(\type\()_tbl) -   80b
-        .hword L(\type\()_tbl) -   40b
 endfunc
+
+jumptable \type\()_tbl
+        .word 1280b - \type\()_tbl
+        .word 640b  - \type\()_tbl
+        .word 320b  - \type\()_tbl
+        .word 160b  - \type\()_tbl
+        .word 80b   - \type\()_tbl
+        .word 40b   - \type\()_tbl
+endjumptable
 .endm
 
 bidir_fn avg
@@ -210,10 +213,10 @@ bidir_fn mask
 .macro w_mask_fn type
 function w_mask_\type\()_8bpc_neon, export=1
         clz             w8,  w4
-        adr             x9,  L(w_mask_\type\()_tbl)
+        movrel          x9,  w_mask_\type\()_tbl
         sub             w8,  w8,  #24
-        ldrh            w8,  [x9,  x8,  lsl #1]
-        sub             x9,  x9,  w8,  uxtw
+        ldrsw           x8,  [x9,  x8,  lsl #2]
+        add             x9,  x9,  x8
         mov             w10, #6903
         dup             v0.8h,   w10
 .if \type == 444
@@ -230,8 +233,9 @@ function w_mask_\type\()_8bpc_neon, export=1
         add             x12,  x0,  x1
         lsl             x1,   x1,  #1
         br              x9
-4:
+40:
         AARCH64_VALID_JUMP_TARGET
+4:
         ld1             {v4.8h,   v5.8h},   [x2],  #32  // tmp1 (four rows at once)
         ld1             {v6.8h,   v7.8h},   [x3],  #32  // tmp2 (four rows at once)
         subs            w5,  w5,  #4
@@ -267,7 +271,7 @@ function w_mask_\type\()_8bpc_neon, export=1
         addp            v18.8h,   v24.8h,  v24.8h
         sub             v18.4h,   v3.4h,   v18.4h
         rshrn           v18.8b,   v18.8h,  #2
-        st1             {v18.s}[0],  [x6],  #4
+        str             s18,         [x6],  #4
 .endif
         st1             {v22.s}[0],  [x0],  x1
         st1             {v22.s}[1],  [x12], x1
@@ -275,8 +279,9 @@ function w_mask_\type\()_8bpc_neon, export=1
         st1             {v23.s}[1],  [x12], x1
         b.gt            4b
         ret
-8:
+80:
         AARCH64_VALID_JUMP_TARGET
+8:
         ld1             {v4.8h,   v5.8h},   [x2],  #32
         ld1             {v6.8h,   v7.8h},   [x3],  #32
         subs            w5,  w5,  #2
@@ -310,7 +315,7 @@ function w_mask_\type\()_8bpc_neon, export=1
         addp            v18.8h,  v18.8h,  v18.8h
         sub             v18.4h,  v3.4h,   v18.4h
         rshrn           v18.8b,  v18.8h,  #2
-        st1             {v18.s}[0],  [x6],  #4
+        str             s18,       [x6],  #4
 .endif
         st1             {v22.8b},  [x0],  x1
         st1             {v23.8b},  [x12], x1
@@ -413,14 +418,16 @@ function w_mask_\type\()_8bpc_neon, export=1
         add             x12, x12, x1
         b.gt            161b
         ret
-L(w_mask_\type\()_tbl):
-        .hword L(w_mask_\type\()_tbl) - 1280b
-        .hword L(w_mask_\type\()_tbl) -  640b
-        .hword L(w_mask_\type\()_tbl) -  320b
-        .hword L(w_mask_\type\()_tbl) -  160b
-        .hword L(w_mask_\type\()_tbl) -    8b
-        .hword L(w_mask_\type\()_tbl) -    4b
 endfunc
+
+jumptable w_mask_\type\()_tbl
+        .word 1280b - w_mask_\type\()_tbl
+        .word 640b  - w_mask_\type\()_tbl
+        .word 320b  - w_mask_\type\()_tbl
+        .word 160b  - w_mask_\type\()_tbl
+        .word 80b   - w_mask_\type\()_tbl
+        .word 40b   - w_mask_\type\()_tbl
+endjumptable
 .endm
 
 w_mask_fn 444
@@ -429,20 +436,21 @@ w_mask_fn 420
 
 
 function blend_8bpc_neon, export=1
-        adr             x6,  L(blend_tbl)
+        movrel          x6,  blend_tbl
         clz             w3,  w3
         sub             w3,  w3,  #26
-        ldrh            w3,  [x6,  x3,  lsl #1]
-        sub             x6,  x6,  w3,  uxtw
+        ldrsw           x3,  [x6,  x3,  lsl #2]
+        add             x6,  x6,  x3
         movi            v4.16b,  #64
         add             x8,  x0,  x1
         lsl             x1,  x1,  #1
         br              x6
-4:
+40:
         AARCH64_VALID_JUMP_TARGET
-        ld1             {v2.8b},     [x5],  #8
-        ld1             {v1.d}[0],   [x2],  #8
-        ld1             {v0.s}[0],   [x0]
+4:
+        ld1             {v2.8b},  [x5],  #8
+        ldr             d1,       [x2],  #8
+        ldr             s0,       [x0]
         subs            w4,  w4,  #2
         ld1             {v0.s}[1],   [x8]
         sub             v3.8b,   v4.8b,   v2.8b
@@ -453,12 +461,13 @@ function blend_8bpc_neon, export=1
         st1             {v6.s}[1],   [x8],  x1
         b.gt            4b
         ret
-8:
+80:
         AARCH64_VALID_JUMP_TARGET
+8:
         ld1             {v2.16b},  [x5],  #16
         ld1             {v1.16b},  [x2],  #16
-        ld1             {v0.d}[0],   [x0]
-        ld1             {v0.d}[1],   [x8]
+        ldr             d0,        [x0]
+        ld1             {v0.d}[1], [x8]
         sub             v3.16b,  v4.16b,  v2.16b
         subs            w4,  w4,  #2
         umull           v5.8h,   v1.8b,   v2.8b
@@ -466,13 +475,14 @@ function blend_8bpc_neon, export=1
         umull2          v6.8h,   v1.16b,  v2.16b
         umlal2          v6.8h,   v0.16b,  v3.16b
         rshrn           v7.8b,   v5.8h,   #6
-        rshrn2          v7.16b,  v6.8h,   #6
-        st1             {v7.d}[0],   [x0],  x1
-        st1             {v7.d}[1],   [x8],  x1
+        rshrn           v16.8b,  v6.8h,   #6
+        st1             {v7.8b},   [x0],  x1
+        st1             {v16.8b},  [x8],  x1
         b.gt            8b
         ret
-16:
+160:
         AARCH64_VALID_JUMP_TARGET
+16:
         ld1             {v1.16b,  v2.16b},  [x5],  #32
         ld1             {v5.16b,  v6.16b},  [x2],  #32
         ld1             {v0.16b},  [x0]
@@ -496,8 +506,9 @@ function blend_8bpc_neon, export=1
         st1             {v19.16b}, [x8],  x1
         b.gt            16b
         ret
-32:
+320:
         AARCH64_VALID_JUMP_TARGET
+32:
         ld1             {v0.16b,  v1.16b,  v2.16b,  v3.16b},  [x5],  #64
         ld1             {v16.16b, v17.16b, v18.16b, v19.16b}, [x2],  #64
         ld1             {v20.16b, v21.16b}, [x0]
@@ -535,15 +546,17 @@ function blend_8bpc_neon, export=1
         st1             {v27.16b, v28.16b}, [x8],  x1
         b.gt            32b
         ret
-L(blend_tbl):
-        .hword L(blend_tbl) - 32b
-        .hword L(blend_tbl) - 16b
-        .hword L(blend_tbl) -  8b
-        .hword L(blend_tbl) -  4b
 endfunc
 
+jumptable blend_tbl
+        .word 320b - blend_tbl
+        .word 160b - blend_tbl
+        .word 80b  - blend_tbl
+        .word 40b  - blend_tbl
+endjumptable
+
 function blend_h_8bpc_neon, export=1
-        adr             x6,  L(blend_h_tbl)
+        movrel          x6,  blend_h_tbl
         movrel          x5,  X(obmc_masks)
         add             x5,  x5,  w4,  uxtw
         sub             w4,  w4,  w4,  lsr #2
@@ -552,15 +565,16 @@ function blend_h_8bpc_neon, export=1
         add             x8,  x0,  x1
         lsl             x1,  x1,  #1
         sub             w7,  w7,  #24
-        ldrh            w7,  [x6,  x7,  lsl #1]
-        sub             x6,  x6,  w7, uxtw
+        ldrsw           x7,  [x6,  x7,  lsl #2]
+        add             x6,  x6,  x7
         br              x6
-2:
+20:
         AARCH64_VALID_JUMP_TARGET
-        ld1             {v0.h}[0],   [x5],  #2
-        ld1             {v1.s}[0],   [x2],  #4
+2:
+        ldr             h0,  [x5],  #2
+        ldr             s1,  [x2],  #4
         subs            w4,  w4,  #2
-        ld1             {v2.h}[0],   [x0]
+        ldr             h2,  [x0]
         zip1            v0.8b,   v0.8b,   v0.8b
         sub             v3.8b,   v4.8b,   v0.8b
         ld1             {v2.h}[1],   [x8]
@@ -571,13 +585,14 @@ function blend_h_8bpc_neon, export=1
         st1             {v5.h}[1],   [x8],  x1
         b.gt            2b
         ret
-4:
+40:
         AARCH64_VALID_JUMP_TARGET
+4:
         ld2r            {v0.8b,   v1.8b},   [x5],  #2
         ld1             {v2.8b},   [x2],  #8
         subs            w4,  w4,  #2
         ext             v0.8b,   v0.8b,   v1.8b,   #4
-        ld1             {v3.s}[0],   [x0]
+        ldr             s3,          [x0]
         sub             v5.8b,   v4.8b,   v0.8b
         ld1             {v3.s}[1],   [x8]
         umull           v6.8h,   v2.8b,   v0.8b
@@ -587,27 +602,29 @@ function blend_h_8bpc_neon, export=1
         st1             {v6.s}[1],   [x8],  x1
         b.gt            4b
         ret
-8:
+80:
         AARCH64_VALID_JUMP_TARGET
+8:
         ld2r            {v0.16b,  v1.16b},  [x5],  #2
         ld1             {v2.16b},  [x2],  #16
-        ld1             {v3.d}[0],   [x0]
+        ldr             d3,        [x0]
         ext             v0.16b,  v0.16b,  v1.16b,  #8
         sub             v5.16b,  v4.16b,  v0.16b
-        ld1             {v3.d}[1],   [x8]
+        ld1             {v3.d}[1], [x8]
         subs            w4,  w4,  #2
         umull           v6.8h,   v0.8b,   v2.8b
         umlal           v6.8h,   v3.8b,   v5.8b
         umull2          v7.8h,   v0.16b,  v2.16b
         umlal2          v7.8h,   v3.16b,  v5.16b
         rshrn           v16.8b,  v6.8h,   #6
-        rshrn2          v16.16b, v7.8h,   #6
-        st1             {v16.d}[0],  [x0],  x1
-        st1             {v16.d}[1],  [x8],  x1
+        rshrn           v17.8b,  v7.8h,   #6
+        st1             {v16.8b},  [x0],  x1
+        st1             {v17.8b},  [x8],  x1
         b.gt            8b
         ret
-16:
+160:
         AARCH64_VALID_JUMP_TARGET
+16:
         ld2r            {v0.16b,  v1.16b},  [x5],  #2
         ld1             {v2.16b,  v3.16b},  [x2],  #32
         ld1             {v5.16b},  [x0]
@@ -682,18 +699,20 @@ function blend_h_8bpc_neon, export=1
         add             x7,  x7,  w3,  uxtw
         b.gt            321b
         ret
-L(blend_h_tbl):
-        .hword L(blend_h_tbl) - 1280b
-        .hword L(blend_h_tbl) -  640b
-        .hword L(blend_h_tbl) -  320b
-        .hword L(blend_h_tbl) -   16b
-        .hword L(blend_h_tbl) -    8b
-        .hword L(blend_h_tbl) -    4b
-        .hword L(blend_h_tbl) -    2b
 endfunc
 
+jumptable blend_h_tbl
+        .word 1280b - blend_h_tbl
+        .word 640b  - blend_h_tbl
+        .word 320b  - blend_h_tbl
+        .word 160b  - blend_h_tbl
+        .word 80b   - blend_h_tbl
+        .word 40b   - blend_h_tbl
+        .word 20b   - blend_h_tbl
+endjumptable
+
 function blend_v_8bpc_neon, export=1
-        adr             x6,  L(blend_v_tbl)
+        movrel          x6,  blend_v_tbl
         movrel          x5,  X(obmc_masks)
         add             x5,  x5,  w3,  uxtw
         clz             w3,  w3
@@ -701,16 +720,16 @@ function blend_v_8bpc_neon, export=1
         add             x8,  x0,  x1
         lsl             x1,  x1,  #1
         sub             w3,  w3,  #26
-        ldrh            w3,  [x6,  x3,  lsl #1]
-        sub             x6,  x6,  w3,  uxtw
+        ldrsw           x3,  [x6,  x3,  lsl #2]
+        add             x6,  x6,  x3
         br              x6
 20:
         AARCH64_VALID_JUMP_TARGET
         ld1r            {v0.8b},   [x5]
         sub             v1.8b,   v4.8b,   v0.8b
 2:
-        ld1             {v2.h}[0],   [x2],  #2
-        ld1             {v3.b}[0],   [x0]
+        ldr             h2,          [x2],  #2
+        ldr             b3,          [x0]
         subs            w4,  w4,  #2
         ld1             {v2.b}[1],   [x2]
         ld1             {v3.b}[1],   [x8]
@@ -729,13 +748,13 @@ function blend_v_8bpc_neon, export=1
         sub             v1.8b,   v4.8b,   v0.8b
 4:
         ld1             {v2.8b},   [x2],  #8
-        ld1             {v3.s}[0],   [x0]
+        ldr             s3,          [x0]
         ld1             {v3.s}[1],   [x8]
         subs            w4,  w4,  #2
         umull           v5.8h,   v2.8b,   v0.8b
         umlal           v5.8h,   v3.8b,   v1.8b
         rshrn           v5.8b,   v5.8h,   #6
-        st1             {v5.h}[0],   [x0],  #2
+        str             h5,          [x0],  #2
         st1             {v5.h}[2],   [x8],  #2
         st1             {v5.b}[2],   [x0],  x1
         st1             {v5.b}[6],   [x8],  x1
@@ -746,21 +765,22 @@ function blend_v_8bpc_neon, export=1
         ld1r            {v0.2d},   [x5]
         sub             x1,  x1,  #4
         sub             v1.16b,  v4.16b,  v0.16b
+        zip2            v16.2d,  v1.2d,   v1.2d
 8:
         ld1             {v2.16b},  [x2],  #16
-        ld1             {v3.d}[0],   [x0]
-        ld1             {v3.d}[1],   [x8]
+        ldr             d3,          [x0]
+        ldr             d4,          [x8]
         subs            w4,  w4,  #2
         umull           v5.8h,  v0.8b,  v2.8b
         umlal           v5.8h,  v3.8b,  v1.8b
         umull2          v6.8h,  v0.16b, v2.16b
-        umlal2          v6.8h,  v3.16b, v1.16b
+        umlal           v6.8h,  v4.8b,  v16.8b
         rshrn           v7.8b,  v5.8h,  #6
-        rshrn2          v7.16b, v6.8h,  #6
-        st1             {v7.s}[0],   [x0],  #4
-        st1             {v7.s}[2],   [x8],  #4
+        rshrn           v17.8b, v6.8h,  #6
+        str             s7,          [x0],  #4
+        str             s17,         [x8],  #4
         st1             {v7.h}[2],   [x0],  x1
-        st1             {v7.h}[6],   [x8],  x1
+        st1             {v17.h}[2],  [x8],  x1
         b.gt            8b
         ret
 160:
@@ -826,21 +846,23 @@ function blend_v_8bpc_neon, export=1
         st1             {v27.8b},  [x8],  x1
         b.gt            32b
         ret
-L(blend_v_tbl):
-        .hword L(blend_v_tbl) - 320b
-        .hword L(blend_v_tbl) - 160b
-        .hword L(blend_v_tbl) -  80b
-        .hword L(blend_v_tbl) -  40b
-        .hword L(blend_v_tbl) -  20b
 endfunc
 
+jumptable blend_v_tbl
+        .word 320b - blend_v_tbl
+        .word 160b - blend_v_tbl
+        .word 80b  - blend_v_tbl
+        .word 40b  - blend_v_tbl
+        .word 20b  - blend_v_tbl
+endjumptable
+
 
 // This has got the same signature as the put_8tap functions,
 // and assumes that x8 is set to (clz(w)-24).
 function put_neon, export=1
-        adr             x9,  L(put_tbl)
-        ldrh            w8,  [x9, x8, lsl #1]
-        sub             x9,  x9,  x8
+        movrel          x9,  put_tbl
+        ldrsw           x8,  [x9, x8, lsl #2]
+        add             x9,  x9,  x8
         br              x9
 
 20:
@@ -933,34 +955,39 @@ function put_neon, export=1
         add             x0, x0, x1
         b.gt            128b
         ret
-
-L(put_tbl):
-        .hword L(put_tbl) - 1280b
-        .hword L(put_tbl) -  640b
-        .hword L(put_tbl) -  320b
-        .hword L(put_tbl) -  160b
-        .hword L(put_tbl) -   80b
-        .hword L(put_tbl) -   40b
-        .hword L(put_tbl) -   20b
 endfunc
 
+jumptable put_tbl
+        .word 1280b - put_tbl
+        .word 640b  - put_tbl
+        .word 320b  - put_tbl
+        .word 160b  - put_tbl
+        .word 80b   - put_tbl
+        .word 40b   - put_tbl
+        .word 20b   - put_tbl
+endjumptable
+
 
 // This has got the same signature as the prep_8tap functions,
 // and assumes that x8 is set to (clz(w)-24), and x7 to w*2.
 function prep_neon, export=1
-        adr             x9,  L(prep_tbl)
-        ldrh            w8,  [x9, x8, lsl #1]
+        movrel          x9,  prep_tbl
+        ldrsw           x8,  [x9, x8, lsl #2]
         movi            v24.16b, #16
-        sub             x9,  x9,  x8
+        add             x9,  x9,  x8
         br              x9
 
 40:
         AARCH64_VALID_JUMP_TARGET
 4:
-        ld1             {v0.s}[0], [x1], x2
-        ld1             {v0.s}[1], [x1], x2
-        ld1             {v1.s}[0], [x1], x2
-        ld1             {v1.s}[1], [x1], x2
+        ldr             s0, [x1]
+        ldr             s2, [x1, x2]
+        add             x1, x1, x2, lsl #1
+        ldr             s1, [x1]
+        ldr             s3, [x1, x2]
+        add             x1, x1, x2, lsl #1
+        mov             v0.s[1], v2.s[0]
+        mov             v1.s[1], v3.s[0]
         ushll           v0.8h, v0.8b, #4
         ushll           v1.8h, v1.8b, #4
         subs            w4, w4, #4
@@ -1092,16 +1119,17 @@ function prep_neon, export=1
         add             x0, x0, #256
         b.gt            128b
         ret
-
-L(prep_tbl):
-        .hword L(prep_tbl) - 1280b
-        .hword L(prep_tbl) -  640b
-        .hword L(prep_tbl) -  320b
-        .hword L(prep_tbl) -  160b
-        .hword L(prep_tbl) -   80b
-        .hword L(prep_tbl) -   40b
 endfunc
 
+jumptable prep_tbl
+        .word 1280b - prep_tbl
+        .word 640b  - prep_tbl
+        .word 320b  - prep_tbl
+        .word 160b  - prep_tbl
+        .word 80b   - prep_tbl
+        .word 40b   - prep_tbl
+endjumptable
+
 
 .macro load_slice s0, s1, strd, wd, d0, d1, d2, d3, d4, d5, d6
         ld1             {\d0\wd}[0], [\s0], \strd
@@ -1335,10 +1363,10 @@ endfunc
 .endif
 .endm
 .macro st_d strd, r0, r1
-        st1             {\r0\().d}[0], [x0], \strd
+        st1             {\r0\().8b},   [x0], \strd
         st1             {\r0\().d}[1], [x8], \strd
 .ifnb \r1
-        st1             {\r1\().d}[0], [x0], \strd
+        st1             {\r1\().8b},   [x0], \strd
         st1             {\r1\().d}[1], [x8], \strd
 .endif
 .endm
@@ -1439,16 +1467,15 @@ L(\type\()_\taps\()_h):
         add             \xmx, x10, \mx, uxtw #3
         b.ne            L(\type\()_\taps\()_hv)
 
-        adr             x9,  L(\type\()_\taps\()_h_tbl)
-        ldrh            w8,  [x9, x8, lsl #1]
-        sub             x9,  x9,  w8, uxtw
+        movrel          x9,  \type\()_\taps\()_h_tbl
+        ldrsw           x8,  [x9, x8, lsl #2]
+        add             x9,  x9,  x8
         br              x9
 
 20:     // 2xN h
         AARCH64_VALID_JUMP_TARGET
 .ifc \type, put
-        add             \xmx,  \xmx,  #2
-        ld1             {v0.s}[0], [\xmx]
+        ldur            s0,  [\xmx, #2]
         sub             \src,  \src,  #1
         add             \ds2,  \dst,  \d_strd
         add             \sr2,  \src,  \s_strd
@@ -1481,8 +1508,7 @@ L(\type\()_\taps\()_h):
 
 40:     // 4xN h
         AARCH64_VALID_JUMP_TARGET
-        add             \xmx,  \xmx,  #2
-        ld1             {v0.s}[0], [\xmx]
+        ldur            s0,  [\xmx, #2]
         sub             \src,  \src,  #1
         add             \ds2,  \dst,  \d_strd
         add             \sr2,  \src,  \s_strd
@@ -1514,8 +1540,10 @@ L(\type\()_\taps\()_h):
 .ifc \type, put
         sqrshrun        v16.8b,  v16.8h,  #4
         sqrshrun        v20.8b,  v20.8h,  #4
-        st1             {v16.s}[0], [\dst], \d_strd
-        st1             {v20.s}[0], [\ds2], \d_strd
+        str             s16,  [\dst]
+        str             s20,  [\ds2]
+        add             \dst, \dst, \d_strd
+        add             \ds2, \ds2, \d_strd
 .else
         st1             {v16.4h}, [\dst], \d_strd
         st1             {v20.4h}, [\ds2], \d_strd
@@ -1526,7 +1554,11 @@ L(\type\()_\taps\()_h):
 80:     // 8xN h
         AARCH64_VALID_JUMP_TARGET
         ld1             {v0.8b}, [\xmx]
+.ifc \taps, 6tap
+        sub             \src,  \src,  #2
+.else
         sub             \src,  \src,  #3
+.endif
         add             \ds2,  \dst,  \d_strd
         add             \sr2,  \src,  \s_strd
         lsl             \d_strd,  \d_strd,  #1
@@ -1541,25 +1573,23 @@ L(\type\()_\taps\()_h):
         uxtl            v21.8h,  v21.8b
 
 .ifc \taps, 6tap
-        ext             v19.16b, v16.16b, v17.16b, #2
-        ext             v23.16b, v20.16b, v21.16b, #2
-        mul             v18.8h,  v19.8h,  v0.h[1]
-        mul             v22.8h,  v23.8h,  v0.h[1]
-.irpc i, 23456
-        ext             v19.16b, v16.16b, v17.16b, #(2*\i)
-        ext             v23.16b, v20.16b, v21.16b, #(2*\i)
+        mul             v18.8h,  v16.8h,  v0.h[1]
+        mul             v22.8h,  v20.8h,  v0.h[1]
+    .irpc i, 23456
+        ext             v19.16b, v16.16b, v17.16b, #(2*\i-2)
+        ext             v23.16b, v20.16b, v21.16b, #(2*\i-2)
         mla             v18.8h,  v19.8h,  v0.h[\i]
         mla             v22.8h,  v23.8h,  v0.h[\i]
-.endr
+    .endr
 .else   // 8tap
         mul             v18.8h,  v16.8h,  v0.h[0]
         mul             v22.8h,  v20.8h,  v0.h[0]
-.irpc i, 1234567
+    .irpc i, 1234567
         ext             v19.16b, v16.16b, v17.16b, #(2*\i)
         ext             v23.16b, v20.16b, v21.16b, #(2*\i)
         mla             v18.8h,  v19.8h,  v0.h[\i]
         mla             v22.8h,  v23.8h,  v0.h[\i]
-.endr
+    .endr
 .endif
         subs            \h,  \h,  #2
         srshr           v18.8h,  v18.8h, #2
@@ -1581,7 +1611,11 @@ L(\type\()_\taps\()_h):
 1280:   // 16xN, 32xN, ... h
         AARCH64_VALID_JUMP_TARGET
         ld1             {v0.8b}, [\xmx]
+.ifc \taps, 6tap
+        sub             \src,  \src,  #2
+.else
         sub             \src,  \src,  #3
+.endif
         add             \ds2,  \dst,  \d_strd
         add             \sr2,  \src,  \s_strd
         lsl             \s_strd,  \s_strd,  #1
@@ -1606,30 +1640,26 @@ L(\type\()_\taps\()_h):
 
 16:
 .ifc \taps, 6tap
-        ext             v28.16b, v16.16b, v17.16b, #2
-        ext             v29.16b, v17.16b, v18.16b, #2
-        ext             v30.16b, v20.16b, v21.16b, #2
-        ext             v31.16b, v21.16b, v22.16b, #2
-        mul             v24.8h,  v28.8h,  v0.h[1]
-        mul             v25.8h,  v29.8h,  v0.h[1]
-        mul             v26.8h,  v30.8h,  v0.h[1]
-        mul             v27.8h,  v31.8h,  v0.h[1]
-.irpc i, 23456
-        ext             v28.16b, v16.16b, v17.16b, #(2*\i)
-        ext             v29.16b, v17.16b, v18.16b, #(2*\i)
-        ext             v30.16b, v20.16b, v21.16b, #(2*\i)
-        ext             v31.16b, v21.16b, v22.16b, #(2*\i)
+        mul             v24.8h,  v16.8h,  v0.h[1]
+        mul             v25.8h,  v17.8h,  v0.h[1]
+        mul             v26.8h,  v20.8h,  v0.h[1]
+        mul             v27.8h,  v21.8h,  v0.h[1]
+    .irpc i, 23456
+        ext             v28.16b, v16.16b, v17.16b, #(2*\i-2)
+        ext             v29.16b, v17.16b, v18.16b, #(2*\i-2)
+        ext             v30.16b, v20.16b, v21.16b, #(2*\i-2)
+        ext             v31.16b, v21.16b, v22.16b, #(2*\i-2)
         mla             v24.8h,  v28.8h,  v0.h[\i]
         mla             v25.8h,  v29.8h,  v0.h[\i]
         mla             v26.8h,  v30.8h,  v0.h[\i]
         mla             v27.8h,  v31.8h,  v0.h[\i]
-.endr
+    .endr
 .else   // 8tap
         mul             v24.8h,  v16.8h,  v0.h[0]
         mul             v25.8h,  v17.8h,  v0.h[0]
         mul             v26.8h,  v20.8h,  v0.h[0]
         mul             v27.8h,  v21.8h,  v0.h[0]
-.irpc i, 1234567
+    .irpc i, 1234567
         ext             v28.16b, v16.16b, v17.16b, #(2*\i)
         ext             v29.16b, v17.16b, v18.16b, #(2*\i)
         ext             v30.16b, v20.16b, v21.16b, #(2*\i)
@@ -1638,7 +1668,7 @@ L(\type\()_\taps\()_h):
         mla             v25.8h,  v29.8h,  v0.h[\i]
         mla             v26.8h,  v30.8h,  v0.h[\i]
         mla             v27.8h,  v31.8h,  v0.h[\i]
-.endr
+    .endr
 .endif
         srshr           v24.8h,  v24.8h, #2
         srshr           v25.8h,  v25.8h, #2
@@ -1677,19 +1707,19 @@ L(\type\()_\taps\()_h):
         subs            \h,  \h,  #2
         b.gt            161b
         ret
+endfunc
 
-L(\type\()_\taps\()_h_tbl):
-        .hword L(\type\()_\taps\()_h_tbl) - 1280b
-        .hword L(\type\()_\taps\()_h_tbl) -  640b
-        .hword L(\type\()_\taps\()_h_tbl) -  320b
-        .hword L(\type\()_\taps\()_h_tbl) -  160b
-        .hword L(\type\()_\taps\()_h_tbl) -   80b
-        .hword L(\type\()_\taps\()_h_tbl) -   40b
-        .hword L(\type\()_\taps\()_h_tbl) -   20b
-        .hword 0
-
-
-L(\type\()_\taps\()_v):
+jumptable \type\()_\taps\()_h_tbl
+        .word 1280b - \type\()_\taps\()_h_tbl
+        .word 640b  - \type\()_\taps\()_h_tbl
+        .word 320b  - \type\()_\taps\()_h_tbl
+        .word 160b  - \type\()_\taps\()_h_tbl
+        .word 80b   - \type\()_\taps\()_h_tbl
+        .word 40b   - \type\()_\taps\()_h_tbl
+        .word 20b   - \type\()_\taps\()_h_tbl
+endjumptable
+
+function L(\type\()_\taps\()_v)
         cmp             \h,  #4
         ubfx            w9,  \my, #7, #7
         and             \my, \my, #0x7f
@@ -1698,9 +1728,9 @@ L(\type\()_\taps\()_v):
 4:
         add             \xmy, x10, \my, uxtw #3
 
-        adr             x9,  L(\type\()_\taps\()_v_tbl)
-        ldrh            w8,  [x9, x8, lsl #1]
-        sub             x9,  x9,  w8, uxtw
+        movrel          x9,  \type\()_\taps\()_v_tbl
+        ldrsw           x8,  [x9, x8, lsl #2]
+        add             x9,  x9,  x8
         br              x9
 
 20:     // 2xN v
@@ -1709,8 +1739,7 @@ L(\type\()_\taps\()_v):
         b.gt            28f
 
         cmp             \h,  #2
-        add             \xmy, \xmy, #2
-        ld1             {v0.s}[0], [\xmy]
+        ldur            s0,  [\xmy, #2]
         sub             \src,  \src,  \s_strd
         add             \ds2,  \dst,  \d_strd
         add             \sr2,  \src,  \s_strd
@@ -1789,8 +1818,7 @@ L(\type\()_\taps\()_v):
 
         // 4x2, 4x4 v
         cmp             \h,  #2
-        add             \xmy, \xmy, #2
-        ld1             {v0.s}[0], [\xmy]
+        ldur            s0,  [\xmy, #2]
         sub             \src, \src, \s_strd
         add             \ds2, \dst, \d_strd
         add             \sr2, \src, \s_strd
@@ -1865,8 +1893,7 @@ L(\type\()_\taps\()_v):
 
         // 8x2, 8x4 v
         cmp             \h,  #2
-        add             \xmy, \xmy, #2
-        ld1             {v0.s}[0], [\xmy]
+        ldur            s0,  [\xmy, #2]
         sub             \src, \src, \s_strd
         add             \ds2, \dst, \d_strd
         add             \sr2, \src, \s_strd
@@ -1964,8 +1991,7 @@ L(\type\()_\taps\()_v):
         b.gt            1680b
 
         // 16x2, 16x4 v
-        add             \xmy, \xmy, #2
-        ld1             {v0.s}[0], [\xmy]
+        ldur            s0,  [\xmy, #2]
         sub             \src, \src, \s_strd
         add             \ds2, \dst, \d_strd
         add             \sr2, \src, \s_strd
@@ -2003,18 +2029,19 @@ L(\type\()_\taps\()_v):
         shift_store_16  \type, \d_strd, v1, v2, v3, v4
 0:
         ret
+endfunc
 
-L(\type\()_\taps\()_v_tbl):
-        .hword L(\type\()_\taps\()_v_tbl) - 1280b
-        .hword L(\type\()_\taps\()_v_tbl) -  640b
-        .hword L(\type\()_\taps\()_v_tbl) -  320b
-        .hword L(\type\()_\taps\()_v_tbl) -  160b
-        .hword L(\type\()_\taps\()_v_tbl) -   80b
-        .hword L(\type\()_\taps\()_v_tbl) -   40b
-        .hword L(\type\()_\taps\()_v_tbl) -   20b
-        .hword 0
-
-L(\type\()_\taps\()_hv):
+jumptable \type\()_\taps\()_v_tbl
+        .word 1280b - \type\()_\taps\()_v_tbl
+        .word 640b  - \type\()_\taps\()_v_tbl
+        .word 320b  - \type\()_\taps\()_v_tbl
+        .word 160b  - \type\()_\taps\()_v_tbl
+        .word 80b   - \type\()_\taps\()_v_tbl
+        .word 40b   - \type\()_\taps\()_v_tbl
+        .word 20b   - \type\()_\taps\()_v_tbl
+endjumptable
+
+function L(\type\()_\taps\()_hv)
         cmp             \h,  #4
         ubfx            w9,  \my, #7, #7
         and             \my, \my, #0x7f
@@ -2023,19 +2050,17 @@ L(\type\()_\taps\()_hv):
 4:
         add             \xmy,  x10, \my, uxtw #3
 
-        adr             x9,  L(\type\()_\taps\()_hv_tbl)
-        ldrh            w8,  [x9, x8, lsl #1]
-        sub             x9,  x9,  w8, uxtw
+        movrel          x9,  \type\()_\taps\()_hv_tbl
+        ldrsw           x8,  [x9, x8, lsl #2]
+        add             x9,  x9,  x8
         br              x9
 
 20:
         AARCH64_VALID_JUMP_TARGET
 .ifc \type, put
-        add             \xmx,  \xmx,  #2
-        ld1             {v0.s}[0],  [\xmx]
+        ldur            s0,  [\xmx, #2]
         b.gt            280f
-        add             \xmy,  \xmy,  #2
-        ld1             {v1.s}[0],  [\xmy]
+        ldur            s1,  [\xmy, #2]
 
         // 2x2, 2x4 hv
         sub             \sr2, \src, #1
@@ -2169,11 +2194,9 @@ L(\type\()_\taps\()_filter_2):
 
 40:
         AARCH64_VALID_JUMP_TARGET
-        add             \xmx, \xmx, #2
-        ld1             {v0.s}[0],  [\xmx]
+        ldur            s0,  [\xmx, #2]
         b.gt            480f
-        add             \xmy, \xmy,  #2
-        ld1             {v1.s}[0],  [\xmy]
+        ldur            s1,  [\xmy, #2]
         sub             \sr2, \src, #1
         sub             \src, \sr2, \s_strd
         add             \ds2, \dst, \d_strd
@@ -2218,8 +2241,10 @@ L(\type\()_\taps\()_filter_2):
 .ifc \type, put
         sqxtun          v2.8b,  v2.8h
         sqxtun          v3.8b,  v3.8h
-        st1             {v2.s}[0], [\dst], \d_strd
-        st1             {v3.s}[0], [\ds2], \d_strd
+        str             s2,  [\dst]
+        str             s3,  [\ds2]
+        add             \dst, \dst, \d_strd
+        add             \ds2, \ds2, \d_strd
 .else
         st1             {v2.4h}, [\dst], \d_strd
         st1             {v3.4h}, [\ds2], \d_strd
@@ -2311,8 +2336,10 @@ L(\type\()_\taps\()_filter_2):
 .ifc \type, put
         sqxtun          v2.8b,  v2.8h
         sqxtun          v3.8b,  v3.8h
-        st1             {v2.s}[0], [\dst], \d_strd
-        st1             {v3.s}[0], [\ds2], \d_strd
+        str             s2,  [\dst]
+        str             s3,  [\ds2]
+        add             \dst, \dst, \d_strd
+        add             \ds2, \ds2, \d_strd
 .else
         st1             {v2.4h}, [\dst], \d_strd
         st1             {v3.4h}, [\ds2], \d_strd
@@ -2359,10 +2386,13 @@ L(\type\()_\taps\()_filter_4):
 320:
         AARCH64_VALID_JUMP_TARGET
         b.gt            880f
-        add             \xmy,  \xmy,  #2
         ld1             {v0.8b},  [\xmx]
-        ld1             {v1.s}[0],  [\xmy]
+        ldur            s1,  [\xmy, #2]
+.ifc \taps, 6tap
+        sub             \src,  \src,  #2
+.else
         sub             \src,  \src,  #3
+.endif
         sub             \src,  \src,  \s_strd
         sxtl            v0.8h,  v0.8b
         sxtl            v1.8h,  v1.8b
@@ -2440,8 +2470,10 @@ L(\type\()_\taps\()_filter_4):
         AARCH64_VALID_JUMP_TARGET
         ld1             {v0.8b},  [\xmx]
         ld1             {v1.8b},  [\xmy]
+.ifc \taps, 6tap
+        sub             \src,  \src,  #2
+.else
         sub             \src,  \src,  #3
-.ifc \taps, 8tap
         sub             \src,  \src,  \s_strd
 .endif
         sub             \src,  \src,  \s_strd, lsl #1
@@ -2585,17 +2617,15 @@ L(\type\()_\taps\()_filter_8_first):
         uxtl            v28.8h,  v28.8b
         uxtl            v29.8h,  v29.8b
 .ifc \taps, 6tap
-        ext             v24.16b, v28.16b, v29.16b, #(2*1)
-        ext             v25.16b, v28.16b, v29.16b, #(2*2)
-        ext             v26.16b, v28.16b, v29.16b, #(2*3)
-        ext             v27.16b, v28.16b, v29.16b, #(2*4)
-        mul             v16.8h,  v24.8h,  v0.h[1]
+        mul             v16.8h,  v28.8h,  v0.h[1]
+        ext             v25.16b, v28.16b, v29.16b, #(2*1)
+        ext             v26.16b, v28.16b, v29.16b, #(2*2)
+        ext             v27.16b, v28.16b, v29.16b, #(2*3)
         mla             v16.8h,  v25.8h,  v0.h[2]
         mla             v16.8h,  v26.8h,  v0.h[3]
         mla             v16.8h,  v27.8h,  v0.h[4]
-        ext             v24.16b, v28.16b, v29.16b, #(2*5)
-        ext             v25.16b, v28.16b, v29.16b, #(2*6)
-        ext             v26.16b, v28.16b, v29.16b, #(2*7)
+        ext             v24.16b, v28.16b, v29.16b, #(2*4)
+        ext             v25.16b, v28.16b, v29.16b, #(2*5)
         mla             v16.8h,  v24.8h,  v0.h[5]
         mla             v16.8h,  v25.8h,  v0.h[6]
 .else   // 8tap
@@ -2626,40 +2656,38 @@ L(\type\()_\taps\()_filter_8):
         uxtl            v30.8h,  v30.8b
         uxtl            v31.8h,  v31.8b
 .ifc \taps, 6tap
-        ext             v26.16b, v28.16b, v29.16b, #2
-        ext             v27.16b, v30.16b, v31.16b, #2
-        mul             v24.8h,  v26.8h,  v0.h[1]
-        mul             v25.8h,  v27.8h,  v0.h[1]
-.irpc i, 23456
-        ext             v26.16b, v28.16b, v29.16b, #(2*\i)
-        ext             v27.16b, v30.16b, v31.16b, #(2*\i)
+        mul             v24.8h,  v28.8h,  v0.h[1]
+        mul             v25.8h,  v30.8h,  v0.h[1]
+    .irpc i, 23456
+        ext             v26.16b, v28.16b, v29.16b, #(2*\i-2)
+        ext             v27.16b, v30.16b, v31.16b, #(2*\i-2)
         mla             v24.8h,  v26.8h,  v0.h[\i]
         mla             v25.8h,  v27.8h,  v0.h[\i]
-.endr
+    .endr
 .else   // 8tap
         mul             v24.8h,  v28.8h,  v0.h[0]
         mul             v25.8h,  v30.8h,  v0.h[0]
-.irpc i, 1234567
+    .irpc i, 1234567
         ext             v26.16b, v28.16b, v29.16b, #(2*\i)
         ext             v27.16b, v30.16b, v31.16b, #(2*\i)
         mla             v24.8h,  v26.8h,  v0.h[\i]
         mla             v25.8h,  v27.8h,  v0.h[\i]
-.endr
+    .endr
 .endif
         srshr           v24.8h,  v24.8h, #2
         srshr           v25.8h,  v25.8h, #2
         ret
-
-L(\type\()_\taps\()_hv_tbl):
-        .hword L(\type\()_\taps\()_hv_tbl) - 1280b
-        .hword L(\type\()_\taps\()_hv_tbl) -  640b
-        .hword L(\type\()_\taps\()_hv_tbl) -  320b
-        .hword L(\type\()_\taps\()_hv_tbl) -  160b
-        .hword L(\type\()_\taps\()_hv_tbl) -   80b
-        .hword L(\type\()_\taps\()_hv_tbl) -   40b
-        .hword L(\type\()_\taps\()_hv_tbl) -   20b
-        .hword 0
 endfunc
+
+jumptable \type\()_\taps\()_hv_tbl
+        .word 1280b - \type\()_\taps\()_hv_tbl
+        .word 640b  - \type\()_\taps\()_hv_tbl
+        .word 320b  - \type\()_\taps\()_hv_tbl
+        .word 160b  - \type\()_\taps\()_hv_tbl
+        .word 80b   - \type\()_\taps\()_hv_tbl
+        .word 40b   - \type\()_\taps\()_hv_tbl
+        .word 20b   - \type\()_\taps\()_hv_tbl
+endjumptable
 .endm
 
 
@@ -2686,9 +2714,9 @@ function \type\()_bilin_8bpc_neon, export=1
 L(\type\()_bilin_h):
         cbnz            \my, L(\type\()_bilin_hv)
 
-        adr             x9,  L(\type\()_bilin_h_tbl)
-        ldrh            w8,  [x9, x8, lsl #1]
-        sub             x9,  x9,  w8, uxtw
+        movrel          x9,  \type\()_bilin_h_tbl
+        ldrsw           x8,  [x9, x8, lsl #2]
+        add             x9,  x9,  x8
         br              x9
 
 20:     // 2xN h
@@ -2699,8 +2727,8 @@ L(\type\()_bilin_h):
         lsl             \d_strd,  \d_strd,  #1
         lsl             \s_strd,  \s_strd,  #1
 2:
-        ld1             {v4.s}[0],  [\src], \s_strd
-        ld1             {v6.s}[0],  [\sr2], \s_strd
+        ld1r            {v4.4s},  [\src], \s_strd
+        ld1r            {v6.4s},  [\sr2], \s_strd
         ext             v5.8b,  v4.8b,  v4.8b, #1
         ext             v7.8b,  v6.8b,  v6.8b, #1
         trn1            v4.4h,  v4.4h,  v6.4h
@@ -2736,7 +2764,7 @@ L(\type\()_bilin_h):
         st1             {v4.s}[0], [\dst], \d_strd
         st1             {v4.s}[1], [\ds2], \d_strd
 .else
-        st1             {v4.d}[0], [\dst], \d_strd
+        st1             {v4.8b},   [\dst], \d_strd
         st1             {v4.d}[1], [\ds2], \d_strd
 .endif
         b.gt            4b
@@ -2831,23 +2859,24 @@ L(\type\()_bilin_h):
         subs            \h,  \h,  #2
         b.gt            161b
         ret
+endfunc
 
-L(\type\()_bilin_h_tbl):
-        .hword L(\type\()_bilin_h_tbl) - 1280b
-        .hword L(\type\()_bilin_h_tbl) -  640b
-        .hword L(\type\()_bilin_h_tbl) -  320b
-        .hword L(\type\()_bilin_h_tbl) -  160b
-        .hword L(\type\()_bilin_h_tbl) -   80b
-        .hword L(\type\()_bilin_h_tbl) -   40b
-        .hword L(\type\()_bilin_h_tbl) -   20b
-        .hword 0
+jumptable \type\()_bilin_h_tbl
+        .word 1280b - \type\()_bilin_h_tbl
+        .word 640b  - \type\()_bilin_h_tbl
+        .word 320b  - \type\()_bilin_h_tbl
+        .word 160b  - \type\()_bilin_h_tbl
+        .word 80b   - \type\()_bilin_h_tbl
+        .word 40b   - \type\()_bilin_h_tbl
+        .word 20b   - \type\()_bilin_h_tbl
+endjumptable
 
 
-L(\type\()_bilin_v):
+function L(\type\()_bilin_v)
         cmp             \h,  #4
-        adr             x9,  L(\type\()_bilin_v_tbl)
-        ldrh            w8,  [x9, x8, lsl #1]
-        sub             x9,  x9,  w8, uxtw
+        movrel          x9,  \type\()_bilin_v_tbl
+        ldrsw           x8,  [x9, x8, lsl #2]
+        add             x9,  x9,  x8
         br              x9
 
 20:     // 2xN v
@@ -2860,24 +2889,24 @@ L(\type\()_bilin_v):
         lsl             \d_strd,  \d_strd,  #1
 
         // 2x2 v
-        ld1             {v16.h}[0], [\src], \s_strd
+        ld1r            {v16.8h}, [\src], \s_strd
         b.gt            24f
 22:
-        ld1             {v17.h}[0], [\sr2], \s_strd
-        ld1             {v18.h}[0], [\src], \s_strd
+        ld1r            {v17.8h}, [\sr2], \s_strd
+        ld1r            {v18.8h}, [\src], \s_strd
         trn1            v16.4h, v16.4h, v17.4h
         trn1            v17.4h, v17.4h, v18.4h
         umull           v4.8h,  v16.8b,  v2.8b
         umlal           v4.8h,  v17.8b,  v3.8b
         uqrshrn         v4.8b,  v4.8h,  #4
-        st1             {v4.h}[0], [\dst]
+        str             h4,        [\dst]
         st1             {v4.h}[1], [\ds2]
         ret
 24:     // 2x4, 2x6, 2x8, ... v
-        ld1             {v17.h}[0], [\sr2], \s_strd
-        ld1             {v18.h}[0], [\src], \s_strd
-        ld1             {v19.h}[0], [\sr2], \s_strd
-        ld1             {v20.h}[0], [\src], \s_strd
+        ld1r            {v17.8h}, [\sr2], \s_strd
+        ld1r            {v18.8h}, [\src], \s_strd
+        ld1r            {v19.8h}, [\sr2], \s_strd
+        ld1r            {v20.8h}, [\src], \s_strd
         sub             \h,  \h,  #4
         trn1            v16.4h, v16.4h, v17.4h
         trn1            v17.4h, v17.4h, v18.4h
@@ -2907,10 +2936,10 @@ L(\type\()_bilin_v):
         add             \sr2,  \src,  \s_strd
         lsl             \s_strd,  \s_strd,  #1
         lsl             \d_strd,  \d_strd,  #1
-        ld1             {v16.s}[0], [\src], \s_strd
+        ld1r            {v16.4s}, [\src], \s_strd
 4:
-        ld1             {v17.s}[0], [\sr2], \s_strd
-        ld1             {v18.s}[0], [\src], \s_strd
+        ld1r            {v17.4s}, [\sr2], \s_strd
+        ld1r            {v18.4s}, [\src], \s_strd
         trn1            v16.2s, v16.2s, v17.2s
         trn1            v17.2s, v17.2s, v18.2s
         umull           v4.8h,  v16.8b,  v2.8b
@@ -2921,7 +2950,7 @@ L(\type\()_bilin_v):
         st1             {v4.s}[0], [\dst], \d_strd
         st1             {v4.s}[1], [\ds2], \d_strd
 .else
-        st1             {v4.d}[0], [\dst], \d_strd
+        st1             {v4.8b},   [\dst], \d_strd
         st1             {v4.d}[1], [\ds2], \d_strd
 .endif
         b.le            0f
@@ -3017,23 +3046,24 @@ L(\type\()_bilin_v):
         b               1b
 0:
         ret
+endfunc
 
-L(\type\()_bilin_v_tbl):
-        .hword L(\type\()_bilin_v_tbl) - 1280b
-        .hword L(\type\()_bilin_v_tbl) -  640b
-        .hword L(\type\()_bilin_v_tbl) -  320b
-        .hword L(\type\()_bilin_v_tbl) -  160b
-        .hword L(\type\()_bilin_v_tbl) -   80b
-        .hword L(\type\()_bilin_v_tbl) -   40b
-        .hword L(\type\()_bilin_v_tbl) -   20b
-        .hword 0
-
-L(\type\()_bilin_hv):
+jumptable \type\()_bilin_v_tbl
+        .word 1280b - \type\()_bilin_v_tbl
+        .word 640b  - \type\()_bilin_v_tbl
+        .word 320b  - \type\()_bilin_v_tbl
+        .word 160b  - \type\()_bilin_v_tbl
+        .word 80b   - \type\()_bilin_v_tbl
+        .word 40b   - \type\()_bilin_v_tbl
+        .word 20b   - \type\()_bilin_v_tbl
+endjumptable
+
+function L(\type\()_bilin_hv)
         uxtl            v2.8h, v2.8b
         uxtl            v3.8h, v3.8b
-        adr             x9,  L(\type\()_bilin_hv_tbl)
-        ldrh            w8,  [x9, x8, lsl #1]
-        sub             x9,  x9,  w8, uxtw
+        movrel          x9,  \type\()_bilin_hv_tbl
+        ldrsw           x8,  [x9, x8, lsl #2]
+        add             x9,  x9,  x8
         br              x9
 
 20:     // 2xN hv
@@ -3044,14 +3074,14 @@ L(\type\()_bilin_hv):
         lsl             \s_strd, \s_strd, #1
         lsl             \d_strd, \d_strd, #1
 
-        ld1             {v28.s}[0],  [\src], \s_strd
+        ld1r            {v28.4s},  [\src], \s_strd
         ext             v29.8b, v28.8b, v28.8b, #1
         umull           v16.8h, v28.8b, v0.8b
         umlal           v16.8h, v29.8b, v1.8b
 
 2:
-        ld1             {v28.s}[0],  [\sr2], \s_strd
-        ld1             {v30.s}[0],  [\src], \s_strd
+        ld1r            {v28.4s},  [\sr2], \s_strd
+        ld1r            {v30.4s},  [\src], \s_strd
         ext             v29.8b, v28.8b, v28.8b, #1
         ext             v31.8b, v30.8b, v30.8b, #1
         trn1            v28.4h, v28.4h, v30.4h
@@ -3107,7 +3137,7 @@ L(\type\()_bilin_hv):
         st1             {v4.s}[1], [\ds2], \d_strd
 .else
         urshr           v4.8h,  v4.8h,  #4
-        st1             {v4.d}[0], [\dst], \d_strd
+        st1             {v4.8b},   [\dst], \d_strd
         st1             {v4.d}[1], [\ds2], \d_strd
 .endif
         b.le            0f
@@ -3182,17 +3212,17 @@ L(\type\()_bilin_hv):
         b               1b
 0:
         ret
-
-L(\type\()_bilin_hv_tbl):
-        .hword L(\type\()_bilin_hv_tbl) - 1280b
-        .hword L(\type\()_bilin_hv_tbl) -  640b
-        .hword L(\type\()_bilin_hv_tbl) -  320b
-        .hword L(\type\()_bilin_hv_tbl) -  160b
-        .hword L(\type\()_bilin_hv_tbl) -   80b
-        .hword L(\type\()_bilin_hv_tbl) -   40b
-        .hword L(\type\()_bilin_hv_tbl) -   20b
-        .hword 0
 endfunc
+
+jumptable \type\()_bilin_hv_tbl
+        .word 1280b - \type\()_bilin_hv_tbl
+        .word 640b  - \type\()_bilin_hv_tbl
+        .word 320b  - \type\()_bilin_hv_tbl
+        .word 160b  - \type\()_bilin_hv_tbl
+        .word 80b   - \type\()_bilin_hv_tbl
+        .word 40b   - \type\()_bilin_hv_tbl
+        .word 20b   - \type\()_bilin_hv_tbl
+endjumptable
 .endm
 
 make_8tap_fn    put,  regular_sharp,  REGULAR, SHARP,   8tap
diff --git a/src/arm/64/mc16.S b/src/arm/64/mc16.S
index 576fab1..66cdfff 100644
--- a/src/arm/64/mc16.S
+++ b/src/arm/64/mc16.S
@@ -145,11 +145,11 @@ function \type\()_16bpc_neon, export=1
         dup             v27.4s,  w6
         neg             v27.4s,  v27.4s
 .endif
-        adr             x7,  L(\type\()_tbl)
+        movrel          x7,  \type\()_tbl
         sub             w4,  w4,  #24
         \type           v4,  v5,  v0,  v1,  v2,  v3
-        ldrh            w4,  [x7, x4, lsl #1]
-        sub             x7,  x7,  w4, uxtw
+        ldrsw           x4,  [x7, x4, lsl #2]
+        add             x7,  x7,  x4
         br              x7
 40:
         AARCH64_VALID_JUMP_TARGET
@@ -157,9 +157,9 @@ function \type\()_16bpc_neon, export=1
         lsl             x1,  x1,  #1
 4:
         subs            w5,  w5,  #4
-        st1             {v4.d}[0],  [x0], x1
+        st1             {v4.8b},    [x0], x1
         st1             {v4.d}[1],  [x7], x1
-        st1             {v5.d}[0],  [x0], x1
+        st1             {v5.8b},    [x0], x1
         st1             {v5.d}[1],  [x7], x1
         b.le            0f
         \type           v4,  v5,  v0,  v1,  v2,  v3
@@ -175,8 +175,9 @@ function \type\()_16bpc_neon, export=1
         b.le            0f
         \type           v4,  v5,  v0,  v1,  v2,  v3
         b               8b
-16:
+160:
         AARCH64_VALID_JUMP_TARGET
+16:
         \type           v6,  v7,  v0,  v1,  v2,  v3
         st1             {v4.8h, v5.8h}, [x0], x1
         subs            w5,  w5,  #2
@@ -184,8 +185,9 @@ function \type\()_16bpc_neon, export=1
         b.le            0f
         \type           v4,  v5,  v0,  v1,  v2,  v3
         b               16b
-32:
+320:
         AARCH64_VALID_JUMP_TARGET
+32:
         \type           v6,  v7,  v0,  v1,  v2,  v3
         subs            w5,  w5,  #1
         st1             {v4.8h, v5.8h, v6.8h, v7.8h},  [x0], x1
@@ -228,14 +230,16 @@ function \type\()_16bpc_neon, export=1
         b               128b
 0:
         ret
-L(\type\()_tbl):
-        .hword L(\type\()_tbl) - 1280b
-        .hword L(\type\()_tbl) -  640b
-        .hword L(\type\()_tbl) -   32b
-        .hword L(\type\()_tbl) -   16b
-        .hword L(\type\()_tbl) -   80b
-        .hword L(\type\()_tbl) -   40b
 endfunc
+
+jumptable \type\()_tbl
+        .word 1280b - \type\()_tbl
+        .word 640b  - \type\()_tbl
+        .word 320b  - \type\()_tbl
+        .word 160b  - \type\()_tbl
+        .word 80b   - \type\()_tbl
+        .word 40b   - \type\()_tbl
+endjumptable
 .endm
 
 bidir_fn avg, w6
@@ -247,12 +251,12 @@ bidir_fn mask, w7
 function w_mask_\type\()_16bpc_neon, export=1
         ldr             w8,  [sp]
         clz             w9,  w4
-        adr             x10, L(w_mask_\type\()_tbl)
+        movrel          x10, w_mask_\type\()_tbl
         dup             v31.8h,  w8   // bitdepth_max
         sub             w9,  w9,  #24
         clz             w8,  w8       // clz(bitdepth_max)
-        ldrh            w9,  [x10,  x9,  lsl #1]
-        sub             x10, x10, w9,  uxtw
+        ldrsw           x9,  [x10,  x9,  lsl #2]
+        add             x10, x10, x9
         sub             w8,  w8,  #12 // sh = intermediate_bits + 6 = clz(bitdepth_max) - 12
         mov             w9,  #PREP_BIAS*64
         neg             w8,  w8       // -sh
@@ -274,8 +278,9 @@ function w_mask_\type\()_16bpc_neon, export=1
         add             x12,  x0,  x1
         lsl             x1,   x1,  #1
         br              x10
-4:
+40:
         AARCH64_VALID_JUMP_TARGET
+4:
         ld1             {v4.8h, v5.8h}, [x2], #32 // tmp1 (four rows at once)
         ld1             {v6.8h, v7.8h}, [x3], #32 // tmp2 (four rows at once)
         subs            w5,  w5,  #4
@@ -331,16 +336,17 @@ function w_mask_\type\()_16bpc_neon, export=1
         addp            v20.8h,  v24.8h,  v24.8h  // (128 - m) + (128 - n) (column wise addition)
         sub             v20.4h,  v3.4h,   v20.4h  // (256 - sign) - ((128 - m) + (128 - n))
         rshrn           v20.8b,  v20.8h,  #2      // ((256 - sign) - ((128 - m) + (128 - n)) + 2) >> 2
-        st1             {v20.s}[0], [x6], #4
+        str             s20,        [x6],  #4
 .endif
-        st1             {v4.d}[0],  [x0],  x1
+        st1             {v4.8b},    [x0],  x1
         st1             {v4.d}[1],  [x12], x1
-        st1             {v5.d}[0],  [x0],  x1
+        st1             {v5.8b},    [x0],  x1
         st1             {v5.d}[1],  [x12], x1
         b.gt            4b
         ret
-8:
+80:
         AARCH64_VALID_JUMP_TARGET
+8:
         ld1             {v4.8h, v5.8h}, [x2], #32 // tmp1
         ld1             {v6.8h, v7.8h}, [x3], #32 // tmp2
         subs            w5,  w5,  #2
@@ -394,7 +400,7 @@ function w_mask_\type\()_16bpc_neon, export=1
         addp            v20.8h,  v20.8h,  v20.8h  // (128 - m) + (128 - n) (column wise addition)
         sub             v20.4h,  v3.4h,   v20.4h  // (256 - sign) - ((128 - m) + (128 - n))
         rshrn           v20.8b,  v20.8h,  #2      // ((256 - sign) - ((128 - m) + (128 - n)) + 2) >> 2
-        st1             {v20.s}[0], [x6], #4
+        str             s20,     [x6],  #4
 .endif
         st1             {v4.8h}, [x0],  x1
         st1             {v5.8h}, [x12], x1
@@ -541,14 +547,16 @@ function w_mask_\type\()_16bpc_neon, export=1
         add             x12, x12, x1
         b.gt            161b
         ret
-L(w_mask_\type\()_tbl):
-        .hword L(w_mask_\type\()_tbl) - 1280b
-        .hword L(w_mask_\type\()_tbl) -  640b
-        .hword L(w_mask_\type\()_tbl) -  320b
-        .hword L(w_mask_\type\()_tbl) -  160b
-        .hword L(w_mask_\type\()_tbl) -    8b
-        .hword L(w_mask_\type\()_tbl) -    4b
 endfunc
+
+jumptable w_mask_\type\()_tbl
+        .word 1280b - w_mask_\type\()_tbl
+        .word 640b  - w_mask_\type\()_tbl
+        .word 320b  - w_mask_\type\()_tbl
+        .word 160b  - w_mask_\type\()_tbl
+        .word 80b   - w_mask_\type\()_tbl
+        .word 40b   - w_mask_\type\()_tbl
+endjumptable
 .endm
 
 w_mask_fn 444
@@ -557,11 +565,11 @@ w_mask_fn 420
 
 
 function blend_16bpc_neon, export=1
-        adr             x6,  L(blend_tbl)
+        movrel          x6,  blend_tbl
         clz             w3,  w3
         sub             w3,  w3,  #26
-        ldrh            w3,  [x6,  x3,  lsl #1]
-        sub             x6,  x6,  w3,  uxtw
+        ldrsw           x3,  [x6,  x3,  lsl #2]
+        add             x6,  x6,  x3
         add             x8,  x0,  x1
         br              x6
 40:
@@ -570,7 +578,7 @@ function blend_16bpc_neon, export=1
 4:
         ld1             {v2.8b},   [x5], #8
         ld1             {v1.8h},   [x2], #16
-        ld1             {v0.d}[0], [x0]
+        ldr             d0,        [x0]
         neg             v2.8b,   v2.8b            // -m
         subs            w4,  w4,  #2
         ld1             {v0.d}[1], [x8]
@@ -579,7 +587,7 @@ function blend_16bpc_neon, export=1
         sub             v1.8h,   v0.8h,   v1.8h   // a - b
         sqrdmulh        v1.8h,   v1.8h,   v2.8h   // ((a-b)*-m + 32) >> 6
         add             v0.8h,   v0.8h,   v1.8h
-        st1             {v0.d}[0], [x0], x1
+        st1             {v0.8b},   [x0], x1
         st1             {v0.d}[1], [x8], x1
         b.gt            4b
         ret
@@ -642,8 +650,9 @@ function blend_16bpc_neon, export=1
         st1             {v2.8h, v3.8h}, [x8], x1
         b.gt            16b
         ret
-32:
+320:
         AARCH64_VALID_JUMP_TARGET
+32:
         ld1             {v16.16b, v17.16b},           [x5], #32
         ld1             {v4.8h, v5.8h, v6.8h, v7.8h}, [x2], #64
         subs            w4,  w4,  #1
@@ -673,15 +682,17 @@ function blend_16bpc_neon, export=1
         st1             {v0.8h, v1.8h, v2.8h, v3.8h}, [x0], x1
         b.gt            32b
         ret
-L(blend_tbl):
-        .hword L(blend_tbl) -  32b
-        .hword L(blend_tbl) - 160b
-        .hword L(blend_tbl) -  80b
-        .hword L(blend_tbl) -  40b
 endfunc
 
+jumptable blend_tbl
+        .word 320b - blend_tbl
+        .word 160b - blend_tbl
+        .word 80b  - blend_tbl
+        .word 40b  - blend_tbl
+endjumptable
+
 function blend_h_16bpc_neon, export=1
-        adr             x6,  L(blend_h_tbl)
+        movrel          x6,  blend_h_tbl
         movrel          x5,  X(obmc_masks)
         add             x5,  x5,  w4,  uxtw
         sub             w4,  w4,  w4,  lsr #2
@@ -689,17 +700,18 @@ function blend_h_16bpc_neon, export=1
         add             x8,  x0,  x1
         lsl             x1,  x1,  #1
         sub             w7,  w7,  #24
-        ldrh            w7,  [x6,  x7,  lsl #1]
-        sub             x6,  x6,  w7, uxtw
+        ldrsw           x7,  [x6,  x7,  lsl #2]
+        add             x6,  x6,  x7
         br              x6
-2:
+20:
         AARCH64_VALID_JUMP_TARGET
+2:
         ld2r            {v2.8b, v3.8b}, [x5], #2
         ld1             {v1.4h},        [x2], #8
         ext             v2.8b,   v2.8b,   v3.8b,   #6
         subs            w4,  w4,  #2
         neg             v2.8b,   v2.8b            // -m
-        ld1             {v0.s}[0], [x0]
+        ldr             s0,        [x0]
         ld1             {v0.s}[1], [x8]
         sxtl            v2.8h,   v2.8b
         shl             v2.4h,   v2.4h,   #9      // -m << 9
@@ -710,26 +722,28 @@ function blend_h_16bpc_neon, export=1
         st1             {v0.s}[1], [x8], x1
         b.gt            2b
         ret
-4:
+40:
         AARCH64_VALID_JUMP_TARGET
+4:
         ld2r            {v2.8b, v3.8b}, [x5], #2
         ld1             {v1.8h},        [x2], #16
         ext             v2.8b,   v2.8b,   v3.8b,   #4
         subs            w4,  w4,  #2
         neg             v2.8b,   v2.8b            // -m
-        ld1             {v0.d}[0],   [x0]
+        ldr             d0,          [x0]
         ld1             {v0.d}[1],   [x8]
         sxtl            v2.8h,   v2.8b
         shl             v2.8h,   v2.8h,   #9      // -m << 9
         sub             v1.8h,   v0.8h,   v1.8h   // a - b
         sqrdmulh        v1.8h,   v1.8h,   v2.8h   // ((a-b)*-m + 32) >> 6
         add             v0.8h,   v0.8h,   v1.8h
-        st1             {v0.d}[0], [x0], x1
+        st1             {v0.8b},   [x0], x1
         st1             {v0.d}[1], [x8], x1
         b.gt            4b
         ret
-8:
+80:
         AARCH64_VALID_JUMP_TARGET
+8:
         ld2r            {v4.8b, v5.8b}, [x5], #2
         ld1             {v2.8h, v3.8h}, [x2], #32
         neg             v4.8b,   v4.8b            // -m
@@ -751,8 +765,9 @@ function blend_h_16bpc_neon, export=1
         st1             {v1.8h}, [x8], x1
         b.gt            8b
         ret
-16:
+160:
         AARCH64_VALID_JUMP_TARGET
+16:
         ld2r            {v16.8b, v17.8b}, [x5], #2
         ld1             {v4.8h, v5.8h, v6.8h, v7.8h}, [x2], #64
         neg             v16.8b,  v16.8b           // -m
@@ -835,26 +850,28 @@ function blend_h_16bpc_neon, export=1
         add             x7,  x7,  w3,  uxtw #1
         b.gt            321b
         ret
-L(blend_h_tbl):
-        .hword L(blend_h_tbl) - 1280b
-        .hword L(blend_h_tbl) -  640b
-        .hword L(blend_h_tbl) -  320b
-        .hword L(blend_h_tbl) -   16b
-        .hword L(blend_h_tbl) -    8b
-        .hword L(blend_h_tbl) -    4b
-        .hword L(blend_h_tbl) -    2b
 endfunc
 
+jumptable blend_h_tbl
+        .word 1280b - blend_h_tbl
+        .word 640b  - blend_h_tbl
+        .word 320b  - blend_h_tbl
+        .word 160b  - blend_h_tbl
+        .word 80b   - blend_h_tbl
+        .word 40b   - blend_h_tbl
+        .word 20b   - blend_h_tbl
+endjumptable
+
 function blend_v_16bpc_neon, export=1
-        adr             x6,  L(blend_v_tbl)
+        movrel          x6,  blend_v_tbl
         movrel          x5,  X(obmc_masks)
         add             x5,  x5,  w3,  uxtw
         clz             w3,  w3
         add             x8,  x0,  x1
         lsl             x1,  x1,  #1
         sub             w3,  w3,  #26
-        ldrh            w3,  [x6,  x3,  lsl #1]
-        sub             x6,  x6,  w3,  uxtw
+        ldrsw           x3,  [x6,  x3,  lsl #2]
+        add             x6,  x6,  x3
         br              x6
 20:
         AARCH64_VALID_JUMP_TARGET
@@ -863,8 +880,8 @@ function blend_v_16bpc_neon, export=1
         sxtl            v2.8h,   v2.8b
         shl             v2.4h,   v2.4h,   #9      // -m << 9
 2:
-        ld1             {v1.s}[0], [x2], #4
-        ld1             {v0.h}[0], [x0]
+        ldr             s1,  [x2],  #4
+        ldr             h0,  [x0]
         subs            w4,  w4,  #2
         ld1             {v1.h}[1], [x2]
         ld1             {v0.h}[1], [x8]
@@ -885,13 +902,13 @@ function blend_v_16bpc_neon, export=1
         shl             v2.8h,   v2.8h,   #9      // -m << 9
 4:
         ld1             {v1.8h},   [x2], #16
-        ld1             {v0.d}[0], [x0]
+        ldr             d0,        [x0]
         ld1             {v0.d}[1], [x8]
         subs            w4,  w4,  #2
         sub             v1.8h,   v0.8h,   v1.8h   // a - b
         sqrdmulh        v1.8h,   v1.8h,   v2.8h   // ((a-b)*-m + 32) >> 6
         add             v0.8h,   v0.8h,   v1.8h
-        st1             {v0.s}[0], [x0], #4
+        str             s0,        [x0], #4
         st1             {v0.s}[2], [x8], #4
         st1             {v0.h}[2], [x0], x1
         st1             {v0.h}[6], [x8], x1
@@ -915,8 +932,8 @@ function blend_v_16bpc_neon, export=1
         sqrdmulh        v3.8h,   v3.8h,   v4.8h
         add             v0.8h,   v0.8h,   v2.8h
         add             v1.8h,   v1.8h,   v3.8h
-        st1             {v0.d}[0], [x0], #8
-        st1             {v1.d}[0], [x8], #8
+        str             d0,        [x0], #8
+        str             d1,        [x8], #8
         st1             {v0.s}[2], [x0], x1
         st1             {v1.s}[2], [x8], x1
         b.gt            8b
@@ -992,34 +1009,38 @@ function blend_v_16bpc_neon, export=1
         st1             {v4.8h, v5.8h, v6.8h}, [x8], x1
         b.gt            32b
         ret
-L(blend_v_tbl):
-        .hword L(blend_v_tbl) - 320b
-        .hword L(blend_v_tbl) - 160b
-        .hword L(blend_v_tbl) -  80b
-        .hword L(blend_v_tbl) -  40b
-        .hword L(blend_v_tbl) -  20b
 endfunc
 
+jumptable blend_v_tbl
+        .word 320b - blend_v_tbl
+        .word 160b - blend_v_tbl
+        .word 80b  - blend_v_tbl
+        .word 40b  - blend_v_tbl
+        .word 20b  - blend_v_tbl
+endjumptable
+
 
 // This has got the same signature as the put_8tap functions,
 // and assumes that x9 is set to (clz(w)-24).
-function put_neon
-        adr             x10, L(put_tbl)
-        ldrh            w9, [x10, x9, lsl #1]
-        sub             x10, x10, w9, uxtw
+function put_16bpc_neon, export=1
+        movrel          x10, put_16bpc_tbl
+        ldrsw           x9, [x10, x9, lsl #2]
+        add             x10, x10, x9
         br              x10
 
-2:
+20:
         AARCH64_VALID_JUMP_TARGET
-        ld1             {v0.s}[0], [x2], x3
-        ld1             {v1.s}[0], [x2], x3
+2:
+        ld1r            {v0.4s},   [x2], x3
+        ld1r            {v1.4s},   [x2], x3
         subs            w5,  w5,  #2
         st1             {v0.s}[0], [x0], x1
         st1             {v1.s}[0], [x0], x1
         b.gt            2b
         ret
-4:
+40:
         AARCH64_VALID_JUMP_TARGET
+4:
         ld1             {v0.4h}, [x2], x3
         ld1             {v1.4h}, [x2], x3
         subs            w5,  w5,  #2
@@ -1041,8 +1062,9 @@ function put_neon
         st1             {v1.8h}, [x8], x1
         b.gt            8b
         ret
-16:
+160:
         AARCH64_VALID_JUMP_TARGET
+16:
         ldp             x6,  x7,  [x2]
         ldp             x8,  x9,  [x2, #16]
         stp             x6,  x7,  [x0]
@@ -1052,8 +1074,9 @@ function put_neon
         add             x0,  x0,  x1
         b.gt            16b
         ret
-32:
+320:
         AARCH64_VALID_JUMP_TARGET
+32:
         ldp             x6,  x7,  [x2]
         ldp             x8,  x9,  [x2, #16]
         stp             x6,  x7,  [x0]
@@ -1067,8 +1090,9 @@ function put_neon
         add             x0,  x0,  x1
         b.gt            32b
         ret
-64:
+640:
         AARCH64_VALID_JUMP_TARGET
+64:
         ldp             q0,  q1,  [x2]
         ldp             q2,  q3,  [x2, #32]
         stp             q0,  q1,  [x0]
@@ -1082,8 +1106,9 @@ function put_neon
         add             x0,  x0,  x1
         b.gt            64b
         ret
-128:
+1280:
         AARCH64_VALID_JUMP_TARGET
+128:
         ldp             q0,  q1,  [x2]
         ldp             q2,  q3,  [x2, #32]
         stp             q0,  q1,  [x0]
@@ -1105,27 +1130,28 @@ function put_neon
         add             x0,  x0,  x1
         b.gt            128b
         ret
-
-L(put_tbl):
-        .hword L(put_tbl) - 128b
-        .hword L(put_tbl) -  64b
-        .hword L(put_tbl) -  32b
-        .hword L(put_tbl) -  16b
-        .hword L(put_tbl) -  80b
-        .hword L(put_tbl) -   4b
-        .hword L(put_tbl) -   2b
 endfunc
 
+jumptable put_16bpc_tbl
+        .word 1280b - put_16bpc_tbl
+        .word 640b  - put_16bpc_tbl
+        .word 320b  - put_16bpc_tbl
+        .word 160b  - put_16bpc_tbl
+        .word 80b   - put_16bpc_tbl
+        .word 40b   - put_16bpc_tbl
+        .word 20b   - put_16bpc_tbl
+endjumptable
+
 
 // This has got the same signature as the prep_8tap functions,
 // and assumes that x9 is set to (clz(w)-24), w7 to intermediate_bits and
 // x8 to w*2.
-function prep_neon
-        adr             x10, L(prep_tbl)
-        ldrh            w9, [x10, x9, lsl #1]
+function prep_16bpc_neon
+        movrel          x10, prep_16bpc_tbl
+        ldrsw           x9, [x10, x9, lsl #2]
         dup             v31.8h,  w7   // intermediate_bits
         movi            v30.8h,  #(PREP_BIAS >> 8), lsl #8
-        sub             x10, x10, w9, uxtw
+        add             x10, x10, x9
         br              x10
 
 40:
@@ -1133,7 +1159,7 @@ function prep_neon
         add             x9,  x1,  x2
         lsl             x2,  x2,  #1
 4:
-        ld1             {v0.d}[0], [x1], x2
+        ld1             {v0.8b},   [x1], x2
         ld1             {v0.d}[1], [x9], x2
         subs            w4,  w4,  #2
         sshl            v0.8h,   v0.8h,   v31.8h
@@ -1156,8 +1182,9 @@ function prep_neon
         st1             {v0.8h, v1.8h}, [x0], #32
         b.gt            8b
         ret
-16:
+160:
         AARCH64_VALID_JUMP_TARGET
+16:
         ldp             q0,  q1,  [x1]
         add             x1,  x1,  x2
         sshl            v0.8h,   v0.8h,   v31.8h
@@ -1174,8 +1201,9 @@ function prep_neon
         st1             {v0.8h, v1.8h, v2.8h, v3.8h}, [x0], #64
         b.gt            16b
         ret
-32:
+320:
         AARCH64_VALID_JUMP_TARGET
+32:
         ldp             q0,  q1,  [x1]
         sshl            v0.8h,   v0.8h,   v31.8h
         ldp             q2,  q3,  [x1, #32]
@@ -1191,8 +1219,9 @@ function prep_neon
         st1             {v0.8h, v1.8h, v2.8h, v3.8h}, [x0], #64
         b.gt            32b
         ret
-64:
+640:
         AARCH64_VALID_JUMP_TARGET
+64:
         ldp             q0,  q1,  [x1]
         subs            w4,  w4,  #1
         sshl            v0.8h,   v0.8h,   v31.8h
@@ -1222,8 +1251,9 @@ function prep_neon
         add             x0,  x0,  x8
         b.gt            64b
         ret
-128:
+1280:
         AARCH64_VALID_JUMP_TARGET
+128:
         ldp             q0,  q1,  [x1]
         subs            w4,  w4,  #1
         sshl            v0.8h,   v0.8h,   v31.8h
@@ -1277,16 +1307,17 @@ function prep_neon
         add             x0,  x0,  x8
         b.gt            128b
         ret
-
-L(prep_tbl):
-        .hword L(prep_tbl) - 128b
-        .hword L(prep_tbl) -  64b
-        .hword L(prep_tbl) -  32b
-        .hword L(prep_tbl) -  16b
-        .hword L(prep_tbl) -  80b
-        .hword L(prep_tbl) -  40b
 endfunc
 
+jumptable prep_16bpc_tbl
+        .word 1280b - prep_16bpc_tbl
+        .word 640b  - prep_16bpc_tbl
+        .word 320b  - prep_16bpc_tbl
+        .word 160b  - prep_16bpc_tbl
+        .word 80b   - prep_16bpc_tbl
+        .word 40b   - prep_16bpc_tbl
+endjumptable
+
 
 .macro load_slice s0, s1, strd, wd, d0, d1, d2, d3, d4, d5, d6
         ld1             {\d0\wd}[0], [\s0], \strd
@@ -1455,10 +1486,10 @@ endfunc
 .endif
 .endm
 .macro st_d strd, r0, r1
-        st1             {\r0\().d}[0], [x0], \strd
+        st1             {\r0\().8b},   [x0], \strd
         st1             {\r0\().d}[1], [x9], \strd
 .ifnb \r1
-        st1             {\r1\().d}[0], [x0], \strd
+        st1             {\r1\().8b},   [x0], \strd
         st1             {\r1\().d}[1], [x9], \strd
 .endif
 .endm
@@ -1556,7 +1587,7 @@ function \type\()_\taps\()_neon
         b.ne            L(\type\()_\taps\()_h)
         tst             \my, #(0x7f << 14)
         b.ne            L(\type\()_\taps\()_v)
-        b               \type\()_neon
+        b               \type\()_16bpc_neon
 
 L(\type\()_\taps\()_h):
         cmp             \w,   #4
@@ -1569,26 +1600,25 @@ L(\type\()_\taps\()_h):
         add             \xmx, x11, \mx, uxtw #3
         b.ne            L(\type\()_\taps\()_hv)
 
-        adr             x10, L(\type\()_\taps\()_h_tbl)
-        dup             v30.4s,  w12           // 6 - intermediate_bits
-        ldrh            w9,  [x10, x9, lsl #1]
-        neg             v30.4s,  v30.4s        // -(6-intermediate_bits)
+        movrel          x10, \type\()_\taps\()_h_tbl
+        ldrsw           x9,  [x10, x9, lsl #2]
 .ifc \type, put
-        dup             v29.8h,  \bdmax        // intermediate_bits
+        mov             w12,  #34              // rounding for 10-bit
+        mov             w13,  #40              // rounding for 12-bit
+        cmp             \bdmax, #2             // 10-bit: 4, 12-bit: 2
+        csel            w12,  w12,  w13,  ne   // select rounding based on \bdmax
 .else
+        neg             w12,  w12              // -(6 - intermediate_bits)
         movi            v28.8h,  #(PREP_BIAS >> 8), lsl #8
 .endif
-        sub             x10, x10, w9, uxtw
-.ifc \type, put
-        neg             v29.8h,  v29.8h        // -intermediate_bits
-.endif
+        add             x10, x10, x9
+        dup             v30.4s,  w12           // rounding or shift amount
         br              x10
 
 20:     // 2xN h
         AARCH64_VALID_JUMP_TARGET
 .ifc \type, put
-        add             \xmx,  \xmx,  #2
-        ld1             {v0.s}[0], [\xmx]
+        ldur            s0,  [\xmx, #2]
         sub             \src,  \src,  #2
         add             \ds2,  \dst,  \d_strd
         add             \sr2,  \src,  \s_strd
@@ -1598,6 +1628,7 @@ L(\type\()_\taps\()_h):
 2:
         ld1             {v4.8h},  [\src], \s_strd
         ld1             {v6.8h},  [\sr2], \s_strd
+        mov             v2.16b,  v30.16b
         ext             v5.16b,  v4.16b,  v4.16b,  #2
         ext             v7.16b,  v6.16b,  v6.16b,  #2
         subs            \h,  \h,  #2
@@ -1605,24 +1636,21 @@ L(\type\()_\taps\()_h):
         trn2            v6.2s,   v4.2s,   v6.2s
         trn1            v4.2s,   v5.2s,   v7.2s
         trn2            v7.2s,   v5.2s,   v7.2s
-        smull           v3.4s,   v3.4h,   v0.h[0]
-        smlal           v3.4s,   v4.4h,   v0.h[1]
-        smlal           v3.4s,   v6.4h,   v0.h[2]
-        smlal           v3.4s,   v7.4h,   v0.h[3]
-        srshl           v3.4s,   v3.4s,   v30.4s // -(6-intermediate_bits)
-        sqxtun          v3.4h,   v3.4s
-        srshl           v3.4h,   v3.4h,   v29.4h // -intermediate_bits
-        umin            v3.4h,   v3.4h,   v31.4h
-        st1             {v3.s}[0], [\dst], \d_strd
-        st1             {v3.s}[1], [\ds2], \d_strd
+        smlal           v2.4s,   v3.4h,   v0.h[0]
+        smlal           v2.4s,   v4.4h,   v0.h[1]
+        smlal           v2.4s,   v6.4h,   v0.h[2]
+        smlal           v2.4s,   v7.4h,   v0.h[3]
+        sqshrun         v2.4h,   v2.4s,   #6
+        umin            v2.4h,   v2.4h,   v31.4h
+        st1             {v2.s}[0], [\dst], \d_strd
+        st1             {v2.s}[1], [\ds2], \d_strd
         b.gt            2b
         ret
 .endif
 
 40:     // 4xN h
         AARCH64_VALID_JUMP_TARGET
-        add             \xmx,  \xmx,  #2
-        ld1             {v0.s}[0], [\xmx]
+        ldur            s0,  [\xmx, #2]
         sub             \src,  \src,  #2
         add             \ds2,  \dst,  \d_strd
         add             \sr2,  \src,  \s_strd
@@ -1632,6 +1660,10 @@ L(\type\()_\taps\()_h):
 4:
         ld1             {v16.8h}, [\src], \s_strd
         ld1             {v20.8h}, [\sr2], \s_strd
+.ifc \type, put
+        mov             v2.16b,  v30.16b
+        mov             v3.16b,  v30.16b
+.endif
         ext             v17.16b, v16.16b, v16.16b, #2
         ext             v18.16b, v16.16b, v16.16b, #4
         ext             v19.16b, v16.16b, v16.16b, #6
@@ -1639,26 +1671,33 @@ L(\type\()_\taps\()_h):
         ext             v22.16b, v20.16b, v20.16b, #4
         ext             v23.16b, v20.16b, v20.16b, #6
         subs            \h,  \h,  #2
-        smull           v16.4s,  v16.4h,  v0.h[0]
-        smlal           v16.4s,  v17.4h,  v0.h[1]
-        smlal           v16.4s,  v18.4h,  v0.h[2]
-        smlal           v16.4s,  v19.4h,  v0.h[3]
-        smull           v20.4s,  v20.4h,  v0.h[0]
-        smlal           v20.4s,  v21.4h,  v0.h[1]
-        smlal           v20.4s,  v22.4h,  v0.h[2]
-        smlal           v20.4s,  v23.4h,  v0.h[3]
-        srshl           v16.4s,  v16.4s,  v30.4s // -(6-intermediate_bits)
-        srshl           v20.4s,  v20.4s,  v30.4s // -(6-intermediate_bits)
 .ifc \type, put
-        sqxtun          v16.4h,  v16.4s
-        sqxtun2         v16.8h,  v20.4s
-        srshl           v16.8h,  v16.8h,  v29.8h // -intermediate_bits
+        smlal           v2.4s,   v16.4h,  v0.h[0]
+.else
+        smull           v2.4s,   v16.4h,  v0.h[0]
+.endif
+        smlal           v2.4s,   v17.4h,  v0.h[1]
+        smlal           v2.4s,   v18.4h,  v0.h[2]
+        smlal           v2.4s,   v19.4h,  v0.h[3]
+.ifc \type, put
+        smlal           v3.4s,   v20.4h,  v0.h[0]
+.else
+        smull           v3.4s,   v20.4h,  v0.h[0]
+.endif
+        smlal           v3.4s,   v21.4h,  v0.h[1]
+        smlal           v3.4s,   v22.4h,  v0.h[2]
+        smlal           v3.4s,   v23.4h,  v0.h[3]
+.ifc \type, put
+        sqshrun         v16.4h,  v2.4s,   #6
+        sqshrun2        v16.8h,  v3.4s,   #6
         umin            v16.8h,  v16.8h,  v31.8h
 .else
+        srshl           v16.4s,  v2.4s,   v30.4s // -(6-intermediate_bits)
+        srshl           v20.4s,  v3.4s,   v30.4s // -(6-intermediate_bits)
         uzp1            v16.8h,  v16.8h,  v20.8h // Same as xtn, xtn2
         sub             v16.8h,  v16.8h,  v28.8h // PREP_BIAS
 .endif
-        st1             {v16.d}[0], [\dst], \d_strd
+        st1             {v16.8b},   [\dst], \d_strd
         st1             {v16.d}[1], [\ds2], \d_strd
         b.gt            4b
         ret
@@ -1670,7 +1709,11 @@ L(\type\()_\taps\()_h):
 1280:   // 8xN, 16xN, 32xN, ... h
         AARCH64_VALID_JUMP_TARGET
         ld1             {v0.8b}, [\xmx]
+.ifc \taps, 6tap
+        sub             \src,  \src,  #4
+.else
         sub             \src,  \src,  #6
+.endif
         add             \ds2,  \dst,  \d_strd
         add             \sr2,  \src,  \s_strd
         lsl             \s_strd,  \s_strd,  #1
@@ -1689,49 +1732,67 @@ L(\type\()_\taps\()_h):
 
 8:
 .ifc \taps, 6tap
-        ext             v24.16b, v16.16b, v17.16b, #2
-        ext             v25.16b, v20.16b, v21.16b, #2
-        smull           v18.4s,  v24.4h,  v0.h[1]
-        smull2          v19.4s,  v24.8h,  v0.h[1]
-        smull           v22.4s,  v25.4h,  v0.h[1]
-        smull2          v23.4s,  v25.8h,  v0.h[1]
-.irpc i, 23456
-        ext             v24.16b, v16.16b, v17.16b, #(2*\i)
-        ext             v25.16b, v20.16b, v21.16b, #(2*\i)
+    .ifc \type, put
+        mov             v18.16b, v30.16b
+        mov             v19.16b, v30.16b
+        smlal           v18.4s,  v16.4h,  v0.h[1]
+        smlal2          v19.4s,  v16.8h,  v0.h[1]
+        mov             v22.16b, v30.16b
+        mov             v23.16b, v30.16b
+        smlal           v22.4s,  v20.4h,  v0.h[1]
+        smlal2          v23.4s,  v20.8h,  v0.h[1]
+    .else
+        smull           v18.4s,  v16.4h,  v0.h[1]
+        smull2          v19.4s,  v16.8h,  v0.h[1]
+        smull           v22.4s,  v20.4h,  v0.h[1]
+        smull2          v23.4s,  v20.8h,  v0.h[1]
+    .endif
+    .irpc i, 23456
+        ext             v24.16b, v16.16b, v17.16b, #(2*\i-2)
+        ext             v25.16b, v20.16b, v21.16b, #(2*\i-2)
         smlal           v18.4s,  v24.4h,  v0.h[\i]
         smlal2          v19.4s,  v24.8h,  v0.h[\i]
         smlal           v22.4s,  v25.4h,  v0.h[\i]
         smlal2          v23.4s,  v25.8h,  v0.h[\i]
-.endr
+    .endr
 .else   // 8tap
+    .ifc \type, put
+        mov             v18.16b, v30.16b
+        mov             v19.16b, v30.16b
+        smlal           v18.4s,  v16.4h,  v0.h[0]
+        smlal2          v19.4s,  v16.8h,  v0.h[0]
+        mov             v22.16b, v30.16b
+        mov             v23.16b, v30.16b
+        smlal           v22.4s,  v20.4h,  v0.h[0]
+        smlal2          v23.4s,  v20.8h,  v0.h[0]
+    .else
         smull           v18.4s,  v16.4h,  v0.h[0]
         smull2          v19.4s,  v16.8h,  v0.h[0]
         smull           v22.4s,  v20.4h,  v0.h[0]
         smull2          v23.4s,  v20.8h,  v0.h[0]
-.irpc i, 1234567
+    .endif
+    .irpc i, 1234567
         ext             v24.16b, v16.16b, v17.16b, #(2*\i)
         ext             v25.16b, v20.16b, v21.16b, #(2*\i)
         smlal           v18.4s,  v24.4h,  v0.h[\i]
         smlal2          v19.4s,  v24.8h,  v0.h[\i]
         smlal           v22.4s,  v25.4h,  v0.h[\i]
         smlal2          v23.4s,  v25.8h,  v0.h[\i]
-.endr
+    .endr
 .endif
         subs            \mx, \mx, #8
-        srshl           v18.4s,  v18.4s,  v30.4s // -(6-intermediate_bits)
-        srshl           v19.4s,  v19.4s,  v30.4s // -(6-intermediate_bits)
-        srshl           v22.4s,  v22.4s,  v30.4s // -(6-intermediate_bits)
-        srshl           v23.4s,  v23.4s,  v30.4s // -(6-intermediate_bits)
 .ifc \type, put
-        sqxtun          v18.4h,  v18.4s
-        sqxtun2         v18.8h,  v19.4s
-        sqxtun          v22.4h,  v22.4s
-        sqxtun2         v22.8h,  v23.4s
-        srshl           v18.8h,  v18.8h,  v29.8h // -intermediate_bits
-        srshl           v22.8h,  v22.8h,  v29.8h // -intermediate_bits
+        sqshrun         v18.4h,  v18.4s,  #6
+        sqshrun2        v18.8h,  v19.4s,  #6
+        sqshrun         v22.4h,  v22.4s,  #6
+        sqshrun2        v22.8h,  v23.4s,  #6
         umin            v18.8h,  v18.8h,  v31.8h
         umin            v22.8h,  v22.8h,  v31.8h
 .else
+        srshl           v18.4s,  v18.4s,  v30.4s // -(6-intermediate_bits)
+        srshl           v19.4s,  v19.4s,  v30.4s // -(6-intermediate_bits)
+        srshl           v22.4s,  v22.4s,  v30.4s // -(6-intermediate_bits)
+        srshl           v23.4s,  v23.4s,  v30.4s // -(6-intermediate_bits)
         uzp1            v18.8h,  v18.8h,  v19.8h // Same as xtn, xtn2
         uzp1            v22.8h,  v22.8h,  v23.8h // Ditto
         sub             v18.8h,  v18.8h,  v28.8h // PREP_BIAS
@@ -1756,19 +1817,20 @@ L(\type\()_\taps\()_h):
         subs            \h,  \h,  #2
         b.gt            81b
         ret
+endfunc
 
-L(\type\()_\taps\()_h_tbl):
-        .hword L(\type\()_\taps\()_h_tbl) - 1280b
-        .hword L(\type\()_\taps\()_h_tbl) -  640b
-        .hword L(\type\()_\taps\()_h_tbl) -  320b
-        .hword L(\type\()_\taps\()_h_tbl) -  160b
-        .hword L(\type\()_\taps\()_h_tbl) -   80b
-        .hword L(\type\()_\taps\()_h_tbl) -   40b
-        .hword L(\type\()_\taps\()_h_tbl) -   20b
-        .hword 0
+jumptable \type\()_\taps\()_h_tbl
+        .word 1280b - \type\()_\taps\()_h_tbl
+        .word 640b  - \type\()_\taps\()_h_tbl
+        .word 320b  - \type\()_\taps\()_h_tbl
+        .word 160b  - \type\()_\taps\()_h_tbl
+        .word 80b   - \type\()_\taps\()_h_tbl
+        .word 40b   - \type\()_\taps\()_h_tbl
+        .word 20b   - \type\()_\taps\()_h_tbl
+endjumptable
 
 
-L(\type\()_\taps\()_v):
+function L(\type\()_\taps\()_v)
         cmp             \h,  #4
         ubfx            w10, \my, #7, #7
         and             \my, \my, #0x7f
@@ -1781,12 +1843,12 @@ L(\type\()_\taps\()_v):
         dup             v30.4s,  w12           // 6 - intermediate_bits
         movi            v29.8h,  #(PREP_BIAS >> 8), lsl #8
 .endif
-        adr             x10, L(\type\()_\taps\()_v_tbl)
-        ldrh            w9,  [x10, x9, lsl #1]
+        movrel          x10, \type\()_\taps\()_v_tbl
+        ldrsw           x9,  [x10, x9, lsl #2]
 .ifc \type, prep
         neg             v30.4s,  v30.4s        // -(6-intermediate_bits)
 .endif
-        sub             x10, x10, w9, uxtw
+        add             x10, x10, x9
         br              x10
 
 20:     // 2xN v
@@ -1795,8 +1857,7 @@ L(\type\()_\taps\()_v):
         b.gt            28f
 
         cmp             \h,  #2
-        add             \xmy, \xmy, #2
-        ld1             {v0.s}[0], [\xmy]
+        ldur            s0,  [\xmy, #2]
         sub             \src,  \src,  \s_strd
         add             \ds2,  \dst,  \d_strd
         add             \sr2,  \src,  \s_strd
@@ -1873,8 +1934,7 @@ L(\type\()_\taps\()_v):
 
         // 4x2, 4x4 v
         cmp             \h,  #2
-        add             \xmy, \xmy, #2
-        ld1             {v0.s}[0], [\xmy]
+        ldur            s0,  [\xmy, #2]
         sub             \src, \src, \s_strd
         add             \ds2, \dst, \d_strd
         add             \sr2, \src, \s_strd
@@ -1938,8 +1998,7 @@ L(\type\()_\taps\()_v):
 
         // 8x2, 8x4 v
         cmp             \h,  #2
-        add             \xmy, \xmy, #2
-        ld1             {v0.s}[0], [\xmy]
+        ldur            s0,  [\xmy, #2]
         sub             \src, \src, \s_strd
         add             \ds2, \dst, \d_strd
         add             \sr2, \src, \s_strd
@@ -2027,8 +2086,7 @@ L(\type\()_\taps\()_v):
         b.gt            1680b
 
         // 16x2, 16x4 v
-        add             \xmy, \xmy, #2
-        ld1             {v0.s}[0], [\xmy]
+        ldur            s0,  [\xmy, #2]
         sub             \src, \src, \s_strd
         sxtl            v0.8h,   v0.8b
 
@@ -2051,18 +2109,19 @@ L(\type\()_\taps\()_v):
         b               16b
 0:
         ret
+endfunc
 
-L(\type\()_\taps\()_v_tbl):
-        .hword L(\type\()_\taps\()_v_tbl) - 1280b
-        .hword L(\type\()_\taps\()_v_tbl) -  640b
-        .hword L(\type\()_\taps\()_v_tbl) -  320b
-        .hword L(\type\()_\taps\()_v_tbl) -  160b
-        .hword L(\type\()_\taps\()_v_tbl) -   80b
-        .hword L(\type\()_\taps\()_v_tbl) -   40b
-        .hword L(\type\()_\taps\()_v_tbl) -   20b
-        .hword 0
-
-L(\type\()_\taps\()_hv):
+jumptable \type\()_\taps\()_v_tbl
+        .word 1280b - \type\()_\taps\()_v_tbl
+        .word 640b  - \type\()_\taps\()_v_tbl
+        .word 320b  - \type\()_\taps\()_v_tbl
+        .word 160b  - \type\()_\taps\()_v_tbl
+        .word 80b   - \type\()_\taps\()_v_tbl
+        .word 40b   - \type\()_\taps\()_v_tbl
+        .word 20b   - \type\()_\taps\()_v_tbl
+endjumptable
+
+function L(\type\()_\taps\()_hv)
         cmp             \h,  #4
         ubfx            w10, \my, #7, #7
         and             \my, \my, #0x7f
@@ -2071,16 +2130,16 @@ L(\type\()_\taps\()_hv):
 4:
         add             \xmy, x11, \my, uxtw #3
 
-        adr             x10, L(\type\()_\taps\()_hv_tbl)
+        movrel          x10, \type\()_\taps\()_hv_tbl
         dup             v30.4s,  w12           // 6 - intermediate_bits
-        ldrh            w9,  [x10, x9, lsl #1]
+        ldrsw           x9,  [x10, x9, lsl #2]
         neg             v30.4s,  v30.4s        // -(6-intermediate_bits)
 .ifc \type, put
         dup             v29.4s,  w13           // 6 + intermediate_bits
 .else
         movi            v29.8h,  #(PREP_BIAS >> 8), lsl #8
 .endif
-        sub             x10, x10, w9, uxtw
+        add             x10, x10, x9
 .ifc \type, put
         neg             v29.4s,  v29.4s        // -(6+intermediate_bits)
 .endif
@@ -2089,11 +2148,9 @@ L(\type\()_\taps\()_hv):
 20:
         AARCH64_VALID_JUMP_TARGET
 .ifc \type, put
-        add             \xmx,  \xmx,  #2
-        ld1             {v0.s}[0],  [\xmx]
+        ldur            s0,  [\xmx, #2]
         b.gt            280f
-        add             \xmy,  \xmy,  #2
-        ld1             {v1.s}[0],  [\xmy]
+        ldur            s1,  [\xmy, #2]
 
         // 2x2, 2x4 hv
         sub             \sr2, \src, #2
@@ -2236,11 +2293,9 @@ L(\type\()_\taps\()_filter_2):
 
 40:
         AARCH64_VALID_JUMP_TARGET
-        add             \xmx, \xmx, #2
-        ld1             {v0.s}[0],  [\xmx]
+        ldur            s0,  [\xmx, #2]
         b.gt            480f
-        add             \xmy, \xmy,  #2
-        ld1             {v1.s}[0],  [\xmy]
+        ldur            s1,  [\xmy, #2]
         sub             \sr2, \src, #2
         sub             \src, \sr2, \s_strd
         add             \ds2, \dst, \d_strd
@@ -2293,7 +2348,7 @@ L(\type\()_\taps\()_filter_2):
 .endif
         subs            \h,  \h,  #2
 
-        st1             {v2.d}[0], [\dst], \d_strd
+        st1             {v2.8b},   [\dst], \d_strd
         st1             {v2.d}[1], [\ds2], \d_strd
         b.le            0f
         mov             v16.8b,  v18.8b
@@ -2392,7 +2447,7 @@ L(\type\()_\taps\()_filter_2):
         sub             v3.8h,   v3.8h,   v29.8h // PREP_BIAS
 .endif
         subs            \h,  \h,  #2
-        st1             {v3.d}[0], [\dst], \d_strd
+        st1             {v3.8b},   [\dst], \d_strd
         st1             {v3.d}[1], [\ds2], \d_strd
         b.le            0f
 .ifc \taps, 8tap
@@ -2436,10 +2491,13 @@ L(\type\()_\taps\()_filter_4):
 320:
         AARCH64_VALID_JUMP_TARGET
         b.gt            880f
-        add             \xmy,  \xmy,  #2
         ld1             {v0.8b},  [\xmx]
-        ld1             {v1.s}[0],  [\xmy]
+        ldur            s1,  [\xmy, #2]
+.ifc \taps, 6tap
+        sub             \src,  \src,  #4
+.else
         sub             \src,  \src,  #6
+.endif
         sub             \src,  \src,  \s_strd
         sxtl            v0.8h,   v0.8b
         sxtl            v1.8h,   v1.8b
@@ -2453,13 +2511,23 @@ L(\type\()_\taps\()_filter_4):
         lsl             \s_strd, \s_strd, #1
 
         ld1             {v27.8h, v28.8h},  [\src], \s_strd
+.ifc \taps, 6tap
+        smull           v24.4s,  v27.4h,  v0.h[1]
+        smull2          v25.4s,  v27.8h,  v0.h[1]
+    .irpc i, 23456
+        ext             v26.16b, v27.16b, v28.16b, #(2*\i-2)
+        smlal           v24.4s,  v26.4h,  v0.h[\i]
+        smlal2          v25.4s,  v26.8h,  v0.h[\i]
+    .endr
+.else
         smull           v24.4s,  v27.4h,  v0.h[0]
         smull2          v25.4s,  v27.8h,  v0.h[0]
-.irpc i, 1234567
+    .irpc i, 1234567
         ext             v26.16b, v27.16b, v28.16b, #(2*\i)
         smlal           v24.4s,  v26.4h,  v0.h[\i]
         smlal2          v25.4s,  v26.8h,  v0.h[\i]
-.endr
+    .endr
+.endif
         srshl           v24.4s,  v24.4s,  v30.4s // -(6-intermediate_bits)
         srshl           v25.4s,  v25.4s,  v30.4s // -(6-intermediate_bits)
         // The intermediates from the horizontal pass fit in 16 bit without
@@ -2537,8 +2605,10 @@ L(\type\()_\taps\()_filter_4):
         AARCH64_VALID_JUMP_TARGET
         ld1             {v0.8b},  [\xmx]
         ld1             {v1.8b},  [\xmy]
+.ifc \taps, 6tap
+        sub             \src,  \src,  #4
+.else
         sub             \src,  \src,  #6
-.ifc \taps, 8tap
         sub             \src,  \src,  \s_strd
 .endif
         sub             \src,  \src,  \s_strd, lsl #1
@@ -2555,22 +2625,21 @@ L(\type\()_\taps\()_filter_4):
 
         ld1             {v27.8h, v28.8h},  [\src], \s_strd
 .ifc \taps, 6tap
-        ext             v26.16b, v27.16b, v28.16b, #2
-        smull           v24.4s,  v26.4h,  v0.h[1]
-        smull2          v25.4s,  v26.8h,  v0.h[1]
-.irpc i, 23456
-        ext             v26.16b, v27.16b, v28.16b, #(2*\i)
+        smull           v24.4s,  v27.4h,  v0.h[1]
+        smull2          v25.4s,  v27.8h,  v0.h[1]
+    .irpc i, 23456
+        ext             v26.16b, v27.16b, v28.16b, #(2*\i-2)
         smlal           v24.4s,  v26.4h,  v0.h[\i]
         smlal2          v25.4s,  v26.8h,  v0.h[\i]
-.endr
+    .endr
 .else   // 8tap
         smull           v24.4s,  v27.4h,  v0.h[0]
         smull2          v25.4s,  v27.8h,  v0.h[0]
-.irpc i, 1234567
+    .irpc i, 1234567
         ext             v26.16b, v27.16b, v28.16b, #(2*\i)
         smlal           v24.4s,  v26.4h,  v0.h[\i]
         smlal2          v25.4s,  v26.8h,  v0.h[\i]
-.endr
+    .endr
 .endif
         srshl           v24.4s,  v24.4s,  v30.4s // -(6-intermediate_bits)
         srshl           v25.4s,  v25.4s,  v30.4s // -(6-intermediate_bits)
@@ -2712,15 +2781,13 @@ L(\type\()_\taps\()_filter_8):
         ld1             {v4.8h, v5.8h},  [\sr2], \s_strd
         ld1             {v6.8h, v7.8h},  [\src], \s_strd
 .ifc \taps, 6tap
-        ext             v23.16b, v4.16b,  v5.16b,  #2
-        ext             v24.16b, v6.16b,  v7.16b,  #2
-        smull           v25.4s,  v23.4h,  v0.h[1]
-        smull2          v26.4s,  v23.8h,  v0.h[1]
-        smull           v27.4s,  v24.4h,  v0.h[1]
-        smull2          v28.4s,  v24.8h,  v0.h[1]
+        smull           v25.4s,  v4.4h,   v0.h[1]
+        smull2          v26.4s,  v4.8h,   v0.h[1]
+        smull           v27.4s,  v6.4h,   v0.h[1]
+        smull2          v28.4s,  v6.8h,   v0.h[1]
 .irpc i, 23456
-        ext             v23.16b, v4.16b,  v5.16b,  #(2*\i)
-        ext             v24.16b, v6.16b,  v7.16b,  #(2*\i)
+        ext             v23.16b, v4.16b,  v5.16b,  #(2*\i-2)
+        ext             v24.16b, v6.16b,  v7.16b,  #(2*\i-2)
         smlal           v25.4s,  v23.4h,  v0.h[\i]
         smlal2          v26.4s,  v23.8h,  v0.h[\i]
         smlal           v27.4s,  v24.4h,  v0.h[\i]
@@ -2747,17 +2814,17 @@ L(\type\()_\taps\()_filter_8):
         uzp1            v23.8h,  v25.8h,  v26.8h // Same as xtn, xtn2
         uzp1            v24.8h,  v27.8h,  v28.8h // Ditto
         ret
-
-L(\type\()_\taps\()_hv_tbl):
-        .hword L(\type\()_\taps\()_hv_tbl) - 1280b
-        .hword L(\type\()_\taps\()_hv_tbl) -  640b
-        .hword L(\type\()_\taps\()_hv_tbl) -  320b
-        .hword L(\type\()_\taps\()_hv_tbl) -  160b
-        .hword L(\type\()_\taps\()_hv_tbl) -   80b
-        .hword L(\type\()_\taps\()_hv_tbl) -   40b
-        .hword L(\type\()_\taps\()_hv_tbl) -   20b
-        .hword 0
 endfunc
+
+jumptable \type\()_\taps\()_hv_tbl
+        .word 1280b - \type\()_\taps\()_hv_tbl
+        .word 640b  - \type\()_\taps\()_hv_tbl
+        .word 320b  - \type\()_\taps\()_hv_tbl
+        .word 160b  - \type\()_\taps\()_hv_tbl
+        .word 80b   - \type\()_\taps\()_hv_tbl
+        .word 40b   - \type\()_\taps\()_hv_tbl
+        .word 20b   - \type\()_\taps\()_hv_tbl
+endjumptable
 .endm
 
 
@@ -2787,21 +2854,21 @@ function \type\()_bilin_16bpc_neon, export=1
         add             w12, \bdmax, #4   // 4 + intermediate_bits
         cbnz            \mx, L(\type\()_bilin_h)
         cbnz            \my, L(\type\()_bilin_v)
-        b               \type\()_neon
+        b               \type\()_16bpc_neon
 
 L(\type\()_bilin_h):
         cbnz            \my, L(\type\()_bilin_hv)
 
-        adr             x10, L(\type\()_bilin_h_tbl)
+        movrel          x10, \type\()_bilin_h_tbl
         dup             v31.8h,  w11      // 4 - intermediate_bits
-        ldrh            w9,  [x10, x9, lsl #1]
+        ldrsw           x9,  [x10, x9, lsl #2]
         neg             v31.8h,  v31.8h   // -(4-intermediate_bits)
 .ifc \type, put
         dup             v30.8h,  \bdmax   // intermediate_bits
 .else
         movi            v29.8h,  #(PREP_BIAS >> 8), lsl #8
 .endif
-        sub             x10, x10, w9, uxtw
+        add             x10, x10, x9
 .ifc \type, put
         neg             v30.8h,  v30.8h   // -intermediate_bits
 .endif
@@ -2854,7 +2921,7 @@ L(\type\()_bilin_h):
 .else
         sub             v4.8h,   v4.8h,   v29.8h
 .endif
-        st1             {v4.d}[0], [\dst], \d_strd
+        st1             {v4.8b},   [\dst], \d_strd
         st1             {v4.d}[1], [\ds2], \d_strd
         b.gt            4b
         ret
@@ -2958,30 +3025,31 @@ L(\type\()_bilin_h):
         subs            \h,  \h,  #2
         b.gt            161b
         ret
+endfunc
 
-L(\type\()_bilin_h_tbl):
-        .hword L(\type\()_bilin_h_tbl) - 1280b
-        .hword L(\type\()_bilin_h_tbl) -  640b
-        .hword L(\type\()_bilin_h_tbl) -  320b
-        .hword L(\type\()_bilin_h_tbl) -  160b
-        .hword L(\type\()_bilin_h_tbl) -   80b
-        .hword L(\type\()_bilin_h_tbl) -   40b
-        .hword L(\type\()_bilin_h_tbl) -   20b
-        .hword 0
+jumptable \type\()_bilin_h_tbl
+        .word 1280b - \type\()_bilin_h_tbl
+        .word 640b  - \type\()_bilin_h_tbl
+        .word 320b  - \type\()_bilin_h_tbl
+        .word 160b  - \type\()_bilin_h_tbl
+        .word 80b   - \type\()_bilin_h_tbl
+        .word 40b   - \type\()_bilin_h_tbl
+        .word 20b   - \type\()_bilin_h_tbl
+endjumptable
 
 
-L(\type\()_bilin_v):
+function L(\type\()_bilin_v)
         cmp             \h,  #4
-        adr             x10, L(\type\()_bilin_v_tbl)
+        movrel          x10, \type\()_bilin_v_tbl
 .ifc \type, prep
         dup             v31.8h,  w11      // 4 - intermediate_bits
 .endif
-        ldrh            w9,  [x10, x9, lsl #1]
+        ldrsw           x9,  [x10, x9, lsl #2]
 .ifc \type, prep
         movi            v29.8h,  #(PREP_BIAS >> 8), lsl #8
         neg             v31.8h,  v31.8h   // -(4-intermediate_bits)
 .endif
-        sub             x10, x10, w9, uxtw
+        add             x10, x10, x9
         br              x10
 
 20:     // 2xN v
@@ -2994,24 +3062,24 @@ L(\type\()_bilin_v):
         lsl             \d_strd,  \d_strd,  #1
 
         // 2x2 v
-        ld1             {v16.s}[0], [\src], \s_strd
+        ld1r            {v16.4s}, [\src], \s_strd
         b.gt            24f
 22:
-        ld1             {v17.s}[0], [\sr2], \s_strd
-        ld1             {v18.s}[0], [\src], \s_strd
+        ld1r            {v17.4s}, [\sr2], \s_strd
+        ld1r            {v18.4s}, [\src], \s_strd
         trn1            v16.2s,  v16.2s,  v17.2s
         trn1            v17.2s,  v17.2s,  v18.2s
         mul             v4.4h,   v16.4h,  v2.4h
         mla             v4.4h,   v17.4h,  v3.4h
         urshr           v4.8h,   v4.8h,   #4
-        st1             {v4.s}[0], [\dst]
+        str             s4,        [\dst]
         st1             {v4.s}[1], [\ds2]
         ret
 24:     // 2x4, 2x6, 2x8, ... v
-        ld1             {v17.s}[0], [\sr2], \s_strd
-        ld1             {v18.s}[0], [\src], \s_strd
-        ld1             {v19.s}[0], [\sr2], \s_strd
-        ld1             {v20.s}[0], [\src], \s_strd
+        ld1r            {v17.4s}, [\sr2], \s_strd
+        ld1r            {v18.4s}, [\src], \s_strd
+        ld1r            {v19.4s}, [\sr2], \s_strd
+        ld1r            {v20.4s}, [\src], \s_strd
         sub             \h,  \h,  #4
         trn1            v16.2s,  v16.2s,  v17.2s
         trn1            v17.2s,  v17.2s,  v18.2s
@@ -3056,7 +3124,7 @@ L(\type\()_bilin_v):
         urshl           v4.8h,   v4.8h,   v31.8h
         sub             v4.8h,   v4.8h,   v29.8h
 .endif
-        st1             {v4.d}[0], [\dst], \d_strd
+        st1             {v4.8b},   [\dst], \d_strd
         st1             {v4.d}[1], [\ds2], \d_strd
         b.le            0f
         mov             v16.8b,  v18.8b
@@ -3156,28 +3224,29 @@ L(\type\()_bilin_v):
         b               1b
 0:
         ret
+endfunc
 
-L(\type\()_bilin_v_tbl):
-        .hword L(\type\()_bilin_v_tbl) - 1280b
-        .hword L(\type\()_bilin_v_tbl) -  640b
-        .hword L(\type\()_bilin_v_tbl) -  320b
-        .hword L(\type\()_bilin_v_tbl) -  160b
-        .hword L(\type\()_bilin_v_tbl) -   80b
-        .hword L(\type\()_bilin_v_tbl) -   40b
-        .hword L(\type\()_bilin_v_tbl) -   20b
-        .hword 0
-
-L(\type\()_bilin_hv):
-        adr             x10, L(\type\()_bilin_hv_tbl)
+jumptable \type\()_bilin_v_tbl
+        .word 1280b - \type\()_bilin_v_tbl
+        .word 640b  - \type\()_bilin_v_tbl
+        .word 320b  - \type\()_bilin_v_tbl
+        .word 160b  - \type\()_bilin_v_tbl
+        .word 80b   - \type\()_bilin_v_tbl
+        .word 40b   - \type\()_bilin_v_tbl
+        .word 20b   - \type\()_bilin_v_tbl
+endjumptable
+
+function L(\type\()_bilin_hv)
+        movrel          x10, \type\()_bilin_hv_tbl
         dup             v31.8h,  w11      // 4 - intermediate_bits
-        ldrh            w9,  [x10, x9, lsl #1]
+        ldrsw           x9,  [x10, x9, lsl #2]
         neg             v31.8h,  v31.8h   // -(4-intermediate_bits)
 .ifc \type, put
         dup             v30.4s,  w12      // 4 + intermediate_bits
 .else
         movi            v29.8h,  #(PREP_BIAS >> 8), lsl #8
 .endif
-        sub             x10, x10, w9, uxtw
+        add             x10, x10, x9
 .ifc \type, put
         neg             v30.4s,  v30.4s   // -(4+intermediate_bits)
 .endif
@@ -3264,7 +3333,7 @@ L(\type\()_bilin_hv):
         sub             v4.8h,   v4.8h,   v29.8h
 .endif
         subs            \h,  \h,  #2
-        st1             {v4.d}[0], [\dst], \d_strd
+        st1             {v4.8b},   [\dst], \d_strd
         st1             {v4.d}[1], [\ds2], \d_strd
         b.le            0f
         trn2            v16.2d,  v17.2d,  v17.2d
@@ -3350,17 +3419,17 @@ L(\type\()_bilin_hv):
         b               1b
 0:
         ret
-
-L(\type\()_bilin_hv_tbl):
-        .hword L(\type\()_bilin_hv_tbl) - 1280b
-        .hword L(\type\()_bilin_hv_tbl) -  640b
-        .hword L(\type\()_bilin_hv_tbl) -  320b
-        .hword L(\type\()_bilin_hv_tbl) -  160b
-        .hword L(\type\()_bilin_hv_tbl) -   80b
-        .hword L(\type\()_bilin_hv_tbl) -   40b
-        .hword L(\type\()_bilin_hv_tbl) -   20b
-        .hword 0
 endfunc
+
+jumptable \type\()_bilin_hv_tbl
+        .word 1280b - \type\()_bilin_hv_tbl
+        .word 640b  - \type\()_bilin_hv_tbl
+        .word 320b  - \type\()_bilin_hv_tbl
+        .word 160b  - \type\()_bilin_hv_tbl
+        .word 80b   - \type\()_bilin_hv_tbl
+        .word 40b   - \type\()_bilin_hv_tbl
+        .word 20b   - \type\()_bilin_hv_tbl
+endjumptable
 .endm
 
 make_8tap_fn    put,  regular_sharp,  REGULAR, SHARP,   8tap
diff --git a/src/arm/64/mc16_sve.S b/src/arm/64/mc16_sve.S
new file mode 100644
index 0000000..9ebdb21
--- /dev/null
+++ b/src/arm/64/mc16_sve.S
@@ -0,0 +1,1649 @@
+/*
+ * Copyright © 2024, Arm Limited
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "src/arm/asm.S"
+#include "util.S"
+
+#define PREP_BIAS 32, lsl #8        // 8192
+#define PREP_BIAS_NEG 224, lsl #8   // -8192
+
+#if HAVE_SVE2
+ENABLE_SVE
+ENABLE_SVE2
+
+// No spaces in these expressions, due to gas-preprocessor. It is translated by
+// -1 to save the negative offset when getting the address of `mc_subpel_filters`.
+#define REGULAR1        (((0*15-1)<<7)|(3*15-1))
+#define SMOOTH1         (((1*15-1)<<7)|(4*15-1))
+#define SHARP1          (((2*15-1)<<7)|(3*15-1))
+
+#define FUNC_ALIGN      2
+#define JUMP_ALIGN      2
+#define LOOP_ALIGN      2
+
+
+// Shuffle indices to permute horizontal samples in preparation for input to
+// 16-bit SDOT instructions. The 8-tap horizontal convolution uses sample
+// indices in the interval of [-3, 4] relative to the current sample position.
+const h_tbl_sve, align=4
+        .byte  0,  1,  2,  3,  4,  5,  6,  7,   2,  3,  4,  5,  6,  7,  8,  9
+        .byte  4,  5,  6,  7,  8,  9, 10, 11,   6,  7,  8,  9, 10, 11, 12, 13
+endconst
+
+// Vertical convolutions also use 16-bit SDOT instructions, where two 128-bit
+// registers contain a transposed 4x4 matrix of values. Subsequent iterations
+// of the vertical convolution can reuse the 3x4 sub-matrix from the previous
+// loop iteration. These shuffle indices shift and merge this 4x4 matrix with
+// the values of a new line.
+const v_tbl_sve, align=4
+        .byte  2,  3,  4,  5,  6,  7, 16, 17,  10, 11, 12, 13, 14, 15, 24, 25
+        .byte  2,  3,  4,  5,  6,  7, 16, 17,  10, 11, 12, 13, 14, 15, 18, 19
+        .byte  2,  3,  4,  5,  6,  7, 20, 21,  10, 11, 12, 13, 14, 15, 22, 23
+        .byte  2,  3,  4,  5,  6,  7, 24, 25,  10, 11, 12, 13, 14, 15, 26, 27
+        .byte  2,  3,  4,  5,  6,  7, 28, 29,  10, 11, 12, 13, 14, 15, 30, 31
+endconst
+
+
+.macro make_8tap_fn op, type, type_h, type_v, isa, jump=1
+function \op\()_8tap_\type\()_16bpc_\isa, export=1, align=FUNC_ALIGN
+        mov             x9,  \type_h
+        mov             x10, \type_v
+    .if \jump
+        b               \op\()_8tap_\isa
+    .endif
+endfunc
+.endm
+
+.macro filter_8tap_fn type, isa, dst, d_strd, src, s_strd, w, h, mx, my, bdmax, xmx, xmy, ldst, lsrc, wd_strd, ws_strd
+make_8tap_fn \type, sharp,          SHARP1,   SHARP1,   \isa
+make_8tap_fn \type, sharp_smooth,   SHARP1,   SMOOTH1,  \isa
+make_8tap_fn \type, sharp_regular,  SHARP1,   REGULAR1, \isa
+make_8tap_fn \type, smooth_sharp,   SMOOTH1,  SHARP1,   \isa
+make_8tap_fn \type, smooth,         SMOOTH1,  SMOOTH1,  \isa
+make_8tap_fn \type, smooth_regular, SMOOTH1,  REGULAR1, \isa
+make_8tap_fn \type, regular_sharp,  REGULAR1, SHARP1,   \isa
+make_8tap_fn \type, regular_smooth, REGULAR1, SMOOTH1,  \isa
+make_8tap_fn \type, regular,        REGULAR1, REGULAR1, \isa, jump=0
+
+function \type\()_8tap_\isa, align=FUNC_ALIGN
+        clz             w8, \w
+        mov             w11, #0x4081                    // (1<<14) | (1<<7) | 1
+        ptrue           p0.b, vl16
+        sub             w8, w8, #24                     // for jump tables
+        movrel          x12, X(mc_subpel_filters)
+        cbnz            \mx, L(\type\()_8tap_h_hv_\isa)
+.ifc \type, prep
+        cbz             \my, prep_sve
+.else   // put
+        cbnz            \my, L(\type\()_8tap_v_\isa)
+        mov             w9, w8
+        b               X(put_16bpc_neon)
+
+        .align JUMP_ALIGN
+.endif
+
+L(\type\()_8tap_v_\isa):
+        madd            \my, \my, w11, w10
+        movrel          x13, v_tbl_sve
+.ifc \bdmax, w8                                         // put case, but skip
+        ld1r            {v5.8h}, [sp]                   // loading into w8
+.endif
+        sub             \src, \src, \s_strd             // src - s_strd
+        ubfx            w11, \my, #7, #7
+        and             \my, \my, #0x7F
+        ldr             q6, [x13]
+        cmp             \h, #4
+        csel            \my, \my, w11, le
+        sub             \src, \src, \s_strd, lsl #1     // src - 3 * s_strd
+        add             \xmy, x12, \xmy, lsl #3         // subpel V filter address
+        ldp             q28, q29, [x13, #16]
+        ld1sb           {z7.h}, p0/z, [\xmy]
+.ifc \type, prep
+        clz             \bdmax, \bdmax
+        sub             \bdmax, \bdmax, #24
+        dup             v5.4s, \bdmax
+.endif
+        cmp             \w, #8
+        b.lt            40f
+
+        // .align JUMP_ALIGN   // fallthrough
+80:     // V - 8xN+
+        ldp             q30, q31, [x13, #48]
+.ifc \type, prep
+        add             \wd_strd, \w, \w                // d_strd = 2 * w
+.endif
+        .align LOOP_ALIGN
+81:
+        add             \lsrc, \src, \s_strd, lsl #1
+
+        ldr             q16, [\src]
+        ldr             q17, [\src, \s_strd]
+        ldr             q18, [\lsrc]
+        ldr             q19, [\lsrc, \s_strd]
+        add             \lsrc, \lsrc, \s_strd, lsl #1
+        mov             \ldst, \dst
+
+        ldr             q20, [\lsrc]
+        ldr             q21, [\lsrc, \s_strd]
+        add             \lsrc, \lsrc, \s_strd, lsl #1
+        ldr             q22, [\lsrc]
+        ldr             q23, [\lsrc, \s_strd]
+        add             \lsrc, \lsrc, \s_strd, lsl #1
+        sub             w8, \h, #1
+
+        zip1            v0.8h, v16.8h, v17.8h
+        zip2            v1.8h, v16.8h, v17.8h
+        zip1            v2.8h, v18.8h, v19.8h
+        zip2            v3.8h, v18.8h, v19.8h
+
+        zip1            v18.8h, v20.8h, v21.8h
+        zip2            v21.8h, v20.8h, v21.8h
+        zip1            v24.8h, v22.8h, v23.8h
+        zip2            v27.8h, v22.8h, v23.8h
+
+        zip1            v16.4s, v0.4s, v2.4s
+        zip2            v19.4s, v0.4s, v2.4s
+        zip1            v22.4s, v1.4s, v3.4s
+        zip2            v25.4s, v1.4s, v3.4s
+
+        zip1            v17.4s, v18.4s, v24.4s
+        zip2            v20.4s, v18.4s, v24.4s
+        zip1            v23.4s, v21.4s, v27.4s
+        zip2            v26.4s, v21.4s, v27.4s
+
+        .align LOOP_ALIGN
+8:
+        ld1             {v18.16b}, [\lsrc], \s_strd
+
+        movi            v0.2d, #0
+        movi            v1.2d, #0
+        movi            v2.2d, #0
+        movi            v3.2d, #0
+        mov             v21.16b, v18.16b
+        mov             v24.16b, v18.16b
+        mov             v27.16b, v18.16b
+
+        sdot            z0.d, z16.h, z7.h[0]
+        tbl             v16.16b, {v16.16b, v17.16b}, v6.16b
+        sdot            z1.d, z19.h, z7.h[0]
+        tbl             v19.16b, {v19.16b, v20.16b}, v6.16b
+        sdot            z2.d, z22.h, z7.h[0]
+        tbl             v22.16b, {v22.16b, v23.16b}, v6.16b
+        subs            w8, w8, #1
+        sdot            z3.d, z25.h, z7.h[0]
+        tbl             v25.16b, {v25.16b, v26.16b}, v6.16b
+
+        sdot            z0.d, z17.h, z7.h[1]
+        tbl             v17.16b, {v17.16b, v18.16b}, v28.16b
+        sdot            z1.d, z20.h, z7.h[1]
+        tbl             v20.16b, {v20.16b, v21.16b}, v29.16b
+        sdot            z2.d, z23.h, z7.h[1]
+        tbl             v23.16b, {v23.16b, v24.16b}, v30.16b
+        sdot            z3.d, z26.h, z7.h[1]
+        tbl             v26.16b, {v26.16b, v27.16b}, v31.16b
+
+        uzp1            v0.4s, v0.4s, v1.4s
+        uzp1            v1.4s, v2.4s, v3.4s
+.ifc \type, prep
+        srshl           v0.4s, v0.4s, v5.4s
+        srshl           v1.4s, v1.4s, v5.4s
+        uzp1            v0.8h, v0.8h, v1.8h
+        sub             z0.h, z0.h, #PREP_BIAS
+.else   // put
+        sqrshrun        v0.4h, v0.4s, #6
+        sqrshrun2       v0.8h, v1.4s, #6
+        umin            v0.8h, v0.8h, v5.8h
+.endif
+        st1             {v0.16b}, [\ldst], \d_strd
+        b.gt            8b
+
+        movi            v0.2d, #0
+        movi            v1.2d, #0
+        movi            v2.2d, #0
+        movi            v3.2d, #0
+
+        sdot            z0.d, z16.h, z7.h[0]
+        sdot            z1.d, z19.h, z7.h[0]
+        sdot            z2.d, z22.h, z7.h[0]
+        sdot            z3.d, z25.h, z7.h[0]
+
+        sdot            z0.d, z17.h, z7.h[1]
+        sdot            z1.d, z20.h, z7.h[1]
+        sdot            z2.d, z23.h, z7.h[1]
+        sdot            z3.d, z26.h, z7.h[1]
+        subs            \w, \w, #8
+
+        uzp1            v0.4s, v0.4s, v1.4s
+        uzp1            v1.4s, v2.4s, v3.4s
+.ifc \type, prep
+        srshl           v0.4s, v0.4s, v5.4s
+        srshl           v1.4s, v1.4s, v5.4s
+        uzp1            v0.8h, v0.8h, v1.8h
+        sub             z0.h, z0.h, #PREP_BIAS
+.else   // put
+        sqrshrun        v0.4h, v0.4s, #6
+        sqrshrun2       v0.8h, v1.4s, #6
+        umin            v0.8h, v0.8h, v5.8h
+.endif
+        str             q0, [\ldst]
+
+        add             \dst, \dst, #16
+        add             \src, \src, #16
+        b.gt            81b
+        ret
+
+        .align JUMP_ALIGN
+40:     // V - 4xN, put only: 2xN
+.ifc \type, put
+        lsr             \d_strd, \d_strd, #1        // hword index for `st1h`
+        whilelt         p1.h, wzr, \w               // masking for writes
+.endif
+        cmp             \h, #4
+        b.le            44f
+
+        ldr             d16, [\src]
+        ldr             d17, [\src, \s_strd]
+        add             \src, \src, \s_strd, lsl #1
+        ldr             d18, [\src]
+        ldr             d19, [\src, \s_strd]
+        add             \src, \src, \s_strd, lsl #1
+
+        ldr             d20, [\src]
+        ldr             d21, [\src, \s_strd]
+        add             \src, \src, \s_strd, lsl #1
+        ldr             d22, [\src]
+        ldr             d23, [\src, \s_strd]
+        add             \src, \src, \s_strd, lsl #1
+        sub             \h, \h, #2
+
+        zip1            v0.8h, v16.8h, v17.8h
+        zip1            v2.8h, v18.8h, v19.8h
+        zip1            v18.8h, v20.8h, v21.8h
+        zip1            v24.8h, v22.8h, v23.8h
+
+        zip1            v16.4s, v0.4s, v2.4s
+        zip2            v19.4s, v0.4s, v2.4s
+        zip1            v17.4s, v18.4s, v24.4s
+        zip2            v20.4s, v18.4s, v24.4s
+
+        .align LOOP_ALIGN
+4:
+        ldr             d18, [\src]
+        ldr             d24, [\src, \s_strd]
+        add             \src, \src, \s_strd, lsl #1
+
+        movi            v0.2d, #0
+        movi            v1.2d, #0
+        movi            v2.2d, #0
+        movi            v3.2d, #0
+        mov             v21.16b, v18.16b
+        mov             v27.16b, v24.16b
+
+        sdot            z0.d, z16.h, z7.h[0]
+        tbl             v22.16b, {v16.16b, v17.16b}, v6.16b
+        sdot            z1.d, z19.h, z7.h[0]
+        tbl             v25.16b, {v19.16b, v20.16b}, v6.16b
+        sdot            z0.d, z17.h, z7.h[1]
+        tbl             v23.16b, {v17.16b, v18.16b}, v28.16b
+        sdot            z1.d, z20.h, z7.h[1]
+        tbl             v26.16b, {v20.16b, v21.16b}, v29.16b
+        subs            \h, \h, #2
+
+        sdot            z2.d, z22.h, z7.h[0]
+        tbl             v16.16b, {v22.16b, v23.16b}, v6.16b
+        sdot            z3.d, z25.h, z7.h[0]
+        tbl             v19.16b, {v25.16b, v26.16b}, v6.16b
+        sdot            z2.d, z23.h, z7.h[1]
+        tbl             v17.16b, {v23.16b, v24.16b}, v28.16b
+        sdot            z3.d, z26.h, z7.h[1]
+        tbl             v20.16b, {v26.16b, v27.16b}, v29.16b
+
+        uzp1            v0.4s, v0.4s, v1.4s
+        uzp1            v1.4s, v2.4s, v3.4s
+.ifc \type, prep
+        srshl           v0.4s, v0.4s, v5.4s
+        srshl           v1.4s, v1.4s, v5.4s
+        uzp1            v0.8h, v0.8h, v1.8h
+        sub             z0.h, z0.h, #PREP_BIAS
+        str             q0, [\dst], #16
+.else   // put
+        sqrshrun        v0.4h, v0.4s, #6
+        sqrshrun        v1.4h, v1.4s, #6
+        umin            v0.4h, v0.4h, v5.4h
+        umin            v1.4h, v1.4h, v5.4h
+        st1h            {z0.h}, p1, [\dst]
+        st1h            {z1.h}, p1, [\dst, \d_strd, lsl #1]
+        add             \dst, \dst, \d_strd, lsl #2
+.endif
+        b.gt            4b
+
+        ldr             d18, [\src]
+
+        movi            v0.2d, #0
+        movi            v1.2d, #0
+        movi            v2.2d, #0
+        movi            v3.2d, #0
+        mov             v21.16b, v18.16b
+
+        sdot            z0.d, z16.h, z7.h[0]
+        tbl             v22.16b, {v16.16b, v17.16b}, v6.16b
+        sdot            z1.d, z19.h, z7.h[0]
+        tbl             v25.16b, {v19.16b, v20.16b}, v6.16b
+        sdot            z0.d, z17.h, z7.h[1]
+        tbl             v23.16b, {v17.16b, v18.16b}, v28.16b
+        sdot            z1.d, z20.h, z7.h[1]
+        tbl             v26.16b, {v20.16b, v21.16b}, v29.16b
+
+        sdot            z2.d, z22.h, z7.h[0]
+        sdot            z3.d, z25.h, z7.h[0]
+        sdot            z2.d, z23.h, z7.h[1]
+        sdot            z3.d, z26.h, z7.h[1]
+
+        uzp1            v0.4s, v0.4s, v1.4s
+        uzp1            v1.4s, v2.4s, v3.4s
+.ifc \type, prep
+        srshl           v0.4s, v0.4s, v5.4s
+        srshl           v1.4s, v1.4s, v5.4s
+        uzp1            v0.8h, v0.8h, v1.8h
+        sub             z0.h, z0.h, #PREP_BIAS
+        str             q0, [\dst]
+.else   // put
+        sqrshrun        v0.4h, v0.4s, #6
+        sqrshrun        v1.4h, v1.4s, #6
+        umin            v0.4h, v0.4h, v5.4h
+        umin            v1.4h, v1.4h, v5.4h
+        st1h            {z0.h}, p1, [\dst]
+        st1h            {z1.h}, p1, [\dst, \d_strd, lsl #1]
+.endif
+        ret
+
+        .align JUMP_ALIGN
+44:     // V - 4x4, put only: 4x2, 2x4, 2x2
+        add             \src, \src, \s_strd, lsl #1     // src - s_strd
+        subs            \h, \h, #2
+
+        ldr             d16, [\src]
+        ldr             d17, [\src, \s_strd]
+        add             \src, \src, \s_strd, lsl #1
+        ldr             d18, [\src]
+        ldr             d19, [\src, \s_strd]
+        add             \src, \src, \s_strd, lsl #1
+
+        ext             v7.16b, v7.16b, v7.16b, #4      // [\xmy + 2 * 2]
+
+        zip1            v0.8h, v16.8h, v17.8h
+        zip1            v2.8h, v18.8h, v19.8h
+        zip1            v16.4s, v0.4s, v2.4s
+        zip2            v19.4s, v0.4s, v2.4s
+
+.ifc \type, put
+        b.eq            42f
+.endif
+        ldr             d17, [\src]
+        ldr             d23, [\src, \s_strd]
+        add             \src, \src, \s_strd, lsl #1
+
+        movi            v0.2d, #0
+        movi            v1.2d, #0
+        movi            v2.2d, #0
+        movi            v3.2d, #0
+        mov             v20.16b, v17.16b
+        mov             v26.16b, v23.16b
+
+        sdot            z0.d, z16.h, z7.h[0]
+        tbl             v22.16b, {v16.16b, v17.16b}, v28.16b
+        sdot            z1.d, z19.h, z7.h[0]
+        tbl             v25.16b, {v19.16b, v20.16b}, v29.16b
+        sdot            z2.d, z22.h, z7.h[0]
+        tbl             v16.16b, {v22.16b, v23.16b}, v28.16b
+        sdot            z3.d, z25.h, z7.h[0]
+        tbl             v19.16b, {v25.16b, v26.16b}, v29.16b
+
+        uzp1            v0.4s, v0.4s, v1.4s
+        uzp1            v1.4s, v2.4s, v3.4s
+.ifc \type, prep
+        srshl           v0.4s, v0.4s, v5.4s
+        srshl           v1.4s, v1.4s, v5.4s
+        uzp1            v0.8h, v0.8h, v1.8h
+        sub             z0.h, z0.h, #PREP_BIAS
+        str             q0, [\dst], #16
+.else   // put
+        sqrshrun        v0.4h, v0.4s, #6
+        sqrshrun        v1.4h, v1.4s, #6
+        umin            v0.4h, v0.4h, v5.4h
+        umin            v1.4h, v1.4h, v5.4h
+        st1h            {z0.h}, p1, [\dst]
+        st1h            {z1.h}, p1, [\dst, \d_strd, lsl #1]
+        add             \dst, \dst, \d_strd, lsl #2
+.endif
+
+.ifc \type, put
+        .align JUMP_ALIGN
+42:
+.endif
+        ldr             d17, [\src]
+
+        movi            v0.2d, #0
+        movi            v1.2d, #0
+        movi            v2.2d, #0
+        movi            v3.2d, #0
+        mov             v20.16b, v17.16b
+
+        sdot            z0.d, z16.h, z7.h[0]
+        tbl             v22.16b, {v16.16b, v17.16b}, v28.16b
+        sdot            z1.d, z19.h, z7.h[0]
+        tbl             v25.16b, {v19.16b, v20.16b}, v29.16b
+
+        sdot            z2.d, z22.h, z7.h[0]
+        sdot            z3.d, z25.h, z7.h[0]
+
+        uzp1            v0.4s, v0.4s, v1.4s
+        uzp1            v1.4s, v2.4s, v3.4s
+.ifc \type, prep
+        srshl           v0.4s, v0.4s, v5.4s
+        srshl           v1.4s, v1.4s, v5.4s
+        uzp1            v0.8h, v0.8h, v1.8h
+        sub             z0.h, z0.h, #PREP_BIAS
+        str             q0, [\dst]
+.else   // put
+        sqrshrun        v0.4h, v0.4s, #6
+        sqrshrun        v1.4h, v1.4s, #6
+        umin            v0.4h, v0.4h, v5.4h
+        umin            v1.4h, v1.4h, v5.4h
+        st1h            {z0.h}, p1, [\dst]
+        st1h            {z1.h}, p1, [\dst, \d_strd, lsl #1]
+.endif
+        ret
+
+        .align JUMP_ALIGN
+L(\type\()_8tap_h_hv_\isa):
+        madd            \mx, \mx, w11, w9
+        movrel          x13, h_tbl_sve
+        sub             \src, \src, #6              // src - 3 * 2
+        ubfx            w9, \mx, #7, #7
+        and             \mx, \mx, #0x7F
+        cmp             \w, #4
+        csel            \mx, \mx, w9, le
+        ldp             q30, q31, [x13]
+        add             \xmx, x12, \xmx, lsl #3     // subpel H filter address
+        cbz             \my, L(\type\()_8tap_h_\isa)
+
+        // HV cases
+        madd            w14, \my, w11, w10
+.ifc \bdmax, w8
+        ldr             \bdmax, [sp]
+.endif
+        ubfx            w11, w14, #7, #7
+        and             w14, w14, #0x7F
+        ld1sb           {z4.h}, p0/z, [\xmx]
+        cmp             \h, #4
+        csel            w14, w14, w11, le
+.ifc \type, put
+        dup             v29.8h, \bdmax
+.endif
+        clz             \bdmax, \bdmax
+        add             \xmy, x12, x14, lsl #3      // subpel V filter address
+        ld1sb           {z7.h}, p0/z, [\xmy]
+.ifc \type, put
+        mov             w9, #12
+        sub             w9, w9, \bdmax
+        dup             v6.4s, w9
+.endif
+        sub             \bdmax, \bdmax, #24
+        mov             x15, x30
+        sub             \src, \src, \s_strd         // src - s_strd - 3 * 2
+        dup             v5.4s, \bdmax
+        cmp             w10, SHARP1
+        b.ne            L(\type\()_6tap_hv_\isa)    // vertical != SHARP1
+
+        // HV 8-tap cases
+        cmp             \w, #4
+        b.le            40f
+
+        // .align JUMP_ALIGN    // fallthrough
+80:     // HV8 - 8xN+
+.ifc \type, prep
+        add             \wd_strd, \w, \w                // d_strd = 2 * w
+.endif
+        cmp             \h, #4
+        b.le            84f
+        sub             \src, \src, \s_strd, lsl #1     // src - 3 * s_strd - 3 * 2
+
+        .align LOOP_ALIGN
+81:
+        mov             \lsrc, \src
+        mov             \ldst, \dst
+        mov             w8, \h
+
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v16.8h, v23.8h, v24.8h
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v17.8h, v23.8h, v24.8h
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v18.8h, v23.8h, v24.8h
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v19.8h, v23.8h, v24.8h
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v20.8h, v23.8h, v24.8h
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v21.8h, v23.8h, v24.8h
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v22.8h, v23.8h, v24.8h
+
+        .align LOOP_ALIGN
+8:
+        ldp             q24, q28, [\lsrc]
+        smull           v0.4s, v16.4h, v7.h[0]
+        smull2          v1.4s, v16.8h, v7.h[0]
+        mov             v16.16b, v17.16b
+
+        movi            v2.2d, #0
+        movi            v3.2d, #0
+        tbl             v23.16b, {v24.16b}, v30.16b
+        tbl             v24.16b, {v24.16b}, v31.16b
+
+        ldur            q26, [\lsrc, #8]
+        smlal           v0.4s, v17.4h, v7.h[1]
+        smlal2          v1.4s, v17.8h, v7.h[1]
+        mov             v17.16b, v18.16b
+        add             \lsrc, \lsrc, \s_strd
+
+        sdot            z2.d, z23.h, z4.h[0]
+        sdot            z3.d, z24.h, z4.h[0]
+        movi            v23.2d, #0
+        movi            v24.2d, #0
+        tbl             v25.16b, {v26.16b}, v30.16b
+        tbl             v26.16b, {v26.16b}, v31.16b
+        smlal           v0.4s, v18.4h, v7.h[2]
+        smlal2          v1.4s, v18.8h, v7.h[2]
+        mov             v18.16b, v19.16b
+
+        sdot            z23.d, z25.h, z4.h[0]
+        sdot            z24.d, z26.h, z4.h[0]
+        tbl             v27.16b, {v28.16b}, v30.16b
+        tbl             v28.16b, {v28.16b}, v31.16b
+        smlal           v0.4s, v19.4h, v7.h[3]
+        smlal2          v1.4s, v19.8h, v7.h[3]
+        mov             v19.16b, v20.16b
+
+        subs            w8, w8, #1
+        sdot            z2.d, z25.h, z4.h[1]
+        sdot            z3.d, z26.h, z4.h[1]
+        sdot            z23.d, z27.h, z4.h[1]
+        sdot            z24.d, z28.h, z4.h[1]
+
+        smlal           v0.4s, v20.4h, v7.h[4]
+        smlal2          v1.4s, v20.8h, v7.h[4]
+        mov             v20.16b, v21.16b
+
+        uzp1            v3.4s, v2.4s, v3.4s
+        uzp1            v24.4s, v23.4s, v24.4s
+        smlal           v0.4s, v21.4h, v7.h[5]
+        smlal2          v1.4s, v21.8h, v7.h[5]
+        mov             v21.16b, v22.16b
+
+        srshl           v23.4s, v3.4s, v5.4s
+        srshl           v24.4s, v24.4s, v5.4s
+        smlal           v0.4s, v22.4h, v7.h[6]
+        smlal2          v1.4s, v22.8h, v7.h[6]
+
+        uzp1            v22.8h, v23.8h, v24.8h
+        smlal           v0.4s, v22.4h, v7.h[7]
+        smlal2          v1.4s, v22.8h, v7.h[7]
+
+.ifc \type, prep
+        rshrn           v0.4h, v0.4s, #6
+        rshrn2          v0.8h, v1.4s, #6
+        sub             z0.h, z0.h, #PREP_BIAS
+.else   // put
+        srshl           v0.4s, v0.4s, v6.4s
+        srshl           v1.4s, v1.4s, v6.4s
+        sqxtun          v0.4h, v0.4s
+        sqxtun2         v0.8h, v1.4s
+        umin            v0.8h, v0.8h, v29.8h
+.endif
+        st1             {v0.8h}, [\ldst], \d_strd
+        b.gt            8b
+
+        subs            \w, \w, #8
+        add             \src, \src, #16
+        add             \dst, \dst, #16
+        b.gt            81b
+        ret             x15
+
+        .align JUMP_ALIGN
+40:     // HV8 - 4xN, put only: 2xN
+.ifc \type, put
+        lsr             \d_strd, \d_strd, #1        // hword index for `st1h`
+        whilelt         p1.h, wzr, \w               // masking for writes
+.endif
+        ext             v4.16b, v4.16b, v4.16b, #4  // [\xmy + 2 * 2]
+        add             \src, \src, #4
+
+        cmp             \h, #4
+        b.le            44f
+
+        sub             \src, \src, \s_strd, lsl #1 // src - 3 * s_strd - 3 * 2
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v16.4h, v0.4s
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v17.4h, v0.4s
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v18.4h, v0.4s
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v19.4h, v0.4s
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v20.4h, v0.4s
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v21.4h, v0.4s
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v22.4h, v0.4s
+
+        .align LOOP_ALIGN
+4:
+        ld1             {v3.16b}, [\src], \s_strd
+
+        smull           v24.4s, v16.4h, v7.h[0]
+        smlal           v24.4s, v17.4h, v7.h[1]
+        tbl             v2.16b, {v3.16b}, v30.16b
+        tbl             v3.16b, {v3.16b}, v31.16b
+        movi            v0.2d, #0
+        movi            v1.2d, #0
+        mov             v16.16b, v17.16b
+        mov             v17.16b, v18.16b
+
+        smlal           v24.4s, v18.4h, v7.h[2]
+        smlal           v24.4s, v19.4h, v7.h[3]
+        sdot            z0.d, z2.h, z4.h[0]
+        sdot            z1.d, z3.h, z4.h[0]
+        mov             v18.16b, v19.16b
+        mov             v19.16b, v20.16b
+        uzp1            v0.4s, v0.4s, v1.4s
+
+        smlal           v24.4s, v20.4h, v7.h[4]
+        smlal           v24.4s, v21.4h, v7.h[5]
+        srshl           v0.4s, v0.4s, v5.4s
+        mov             v20.16b, v21.16b
+        mov             v21.16b, v22.16b
+
+        subs            \h, \h, #1
+        smlal           v24.4s, v22.4h, v7.h[6]
+        xtn             v22.4h, v0.4s
+        smlal           v24.4s, v22.4h, v7.h[7]
+
+.ifc \type, prep
+        rshrn           v0.4h, v24.4s, #6
+        sub             z0.h, z0.h, #PREP_BIAS
+        str             d0, [\dst], #8
+.else   // put
+        srshl           v0.4s, v24.4s, v6.4s
+        sqxtun          v0.4h, v0.4s
+        umin            v0.4h, v0.4h, v29.4h
+        st1h            {z0.h}, p1, [\dst]
+        add             \dst, \dst, \d_strd, lsl #1
+.endif
+        b.gt            4b
+        ret             x15
+
+        .align JUMP_ALIGN
+L(\type\()_6tap_hv_\isa):
+        cmp             \w, #4
+        b.le            46f
+
+        // .align JUMP_ALIGN    // fallthrough
+80:     // HV6 - 8xN+
+.ifc \type, prep
+        add             \wd_strd, \w, \w        // d_strd = 2 * w
+.endif
+        cmp             \h, #4
+        b.le            84f
+        sub             \src, \src, \s_strd     // src - 2 * s_strd - 3 * 2
+
+        .align LOOP_ALIGN
+81:
+        mov             \lsrc, \src
+        mov             \ldst, \dst
+        mov             w8, \h
+
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v16.8h, v23.8h, v24.8h
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v17.8h, v23.8h, v24.8h
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v18.8h, v23.8h, v24.8h
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v19.8h, v23.8h, v24.8h
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v20.8h, v23.8h, v24.8h
+
+        .align LOOP_ALIGN
+8:
+        ldp             q24, q28, [\lsrc]
+
+        smull           v0.4s, v16.4h, v7.h[1]
+        smull2          v1.4s, v16.8h, v7.h[1]
+        mov             v16.16b, v17.16b
+
+        tbl             v23.16b, {v24.16b}, v30.16b
+        tbl             v24.16b, {v24.16b}, v31.16b
+        movi            v2.2d, #0
+        movi            v3.2d, #0
+
+        ldur            q26, [\lsrc, #8]
+        add             \lsrc, \lsrc, \s_strd
+
+        sdot            z2.d, z23.h, z4.h[0]
+        sdot            z3.d, z24.h, z4.h[0]
+        tbl             v25.16b, {v26.16b}, v30.16b
+        tbl             v26.16b, {v26.16b}, v31.16b
+        movi            v23.2d, #0
+        movi            v24.2d, #0
+
+        sdot            z23.d, z25.h, z4.h[0]
+        sdot            z24.d, z26.h, z4.h[0]
+        tbl             v27.16b, {v28.16b}, v30.16b
+        tbl             v28.16b, {v28.16b}, v31.16b
+        smlal           v0.4s, v17.4h, v7.h[2]
+        smlal2          v1.4s, v17.8h, v7.h[2]
+        mov             v17.16b, v18.16b
+
+        sdot            z2.d, z25.h, z4.h[1]
+        sdot            z3.d, z26.h, z4.h[1]
+        sdot            z23.d, z27.h, z4.h[1]
+        sdot            z24.d, z28.h, z4.h[1]
+
+        smlal           v0.4s, v18.4h, v7.h[3]
+        smlal2          v1.4s, v18.8h, v7.h[3]
+        mov             v18.16b, v19.16b
+
+        uzp1            v3.4s, v2.4s, v3.4s
+        uzp1            v24.4s, v23.4s, v24.4s
+        smlal           v0.4s, v19.4h, v7.h[4]
+        smlal2          v1.4s, v19.8h, v7.h[4]
+        mov             v19.16b, v20.16b
+
+        srshl           v23.4s, v3.4s, v5.4s
+        srshl           v24.4s, v24.4s, v5.4s
+        smlal           v0.4s, v20.4h, v7.h[5]
+        smlal2          v1.4s, v20.8h, v7.h[5]
+
+        subs            w8, w8, #1
+        uzp1            v20.8h, v23.8h, v24.8h
+        smlal           v0.4s, v20.4h, v7.h[6]
+        smlal2          v1.4s, v20.8h, v7.h[6]
+
+.ifc \type, prep
+        rshrn           v0.4h, v0.4s, #6
+        rshrn2          v0.8h, v1.4s, #6
+        sub             z0.h, z0.h, #PREP_BIAS
+.else   // put
+        srshl           v0.4s, v0.4s, v6.4s
+        srshl           v1.4s, v1.4s, v6.4s
+        sqxtun          v0.4h, v0.4s
+        sqxtun2         v0.8h, v1.4s
+        umin            v0.8h, v0.8h, v29.8h
+.endif
+        st1             {v0.8h}, [\ldst], \d_strd
+        b.gt            8b
+
+        add             \dst, \dst, #16
+        subs            \w, \w, #8
+        add             \src, \src, #16
+        b.gt            81b
+        ret             x15
+
+        .align LOOP_ALIGN
+84:     // HV4 - 8x4, 8x2
+        mov             \lsrc, \src
+        mov             \ldst, \dst
+        mov             w8, \h
+
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v17.8h, v23.8h, v24.8h
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v18.8h, v23.8h, v24.8h
+        bl              L(\type\()_hv_filter8_\isa)
+        uzp1            v19.8h, v23.8h, v24.8h
+
+        .align LOOP_ALIGN
+81:
+        ldp             q24, q28, [\lsrc]
+        ldur            q26, [\lsrc, #8]
+        add             \lsrc, \lsrc, \s_strd
+
+        tbl             v23.16b, {v24.16b}, v30.16b
+        tbl             v24.16b, {v24.16b}, v31.16b
+        movi            v2.2d, #0
+        movi            v3.2d, #0
+        sdot            z2.d, z23.h, z4.h[0]
+        sdot            z3.d, z24.h, z4.h[0]
+
+        tbl             v25.16b, {v26.16b}, v30.16b
+        tbl             v26.16b, {v26.16b}, v31.16b
+        movi            v23.2d, #0
+        movi            v24.2d, #0
+        sdot            z23.d, z25.h, z4.h[0]
+        sdot            z24.d, z26.h, z4.h[0]
+
+        tbl             v27.16b, {v28.16b}, v30.16b
+        tbl             v28.16b, {v28.16b}, v31.16b
+        sdot            z2.d, z25.h, z4.h[1]
+        sdot            z3.d, z26.h, z4.h[1]
+        sdot            z23.d, z27.h, z4.h[1]
+        sdot            z24.d, z28.h, z4.h[1]
+
+        smull           v0.4s, v17.4h, v7.h[2]
+        smull2          v1.4s, v17.8h, v7.h[2]
+        mov             v17.16b, v18.16b
+
+        subs            w8, w8, #1
+        uzp1            v3.4s, v2.4s, v3.4s
+        uzp1            v24.4s, v23.4s, v24.4s
+        smlal           v0.4s, v18.4h, v7.h[3]
+        smlal2          v1.4s, v18.8h, v7.h[3]
+        mov             v18.16b, v19.16b
+
+        srshl           v23.4s, v3.4s, v5.4s
+        srshl           v24.4s, v24.4s, v5.4s
+        smlal           v0.4s, v19.4h, v7.h[4]
+        smlal2          v1.4s, v19.8h, v7.h[4]
+
+        uzp1            v19.8h, v23.8h, v24.8h
+        smlal           v0.4s, v19.4h, v7.h[5]
+        smlal2          v1.4s, v19.8h, v7.h[5]
+
+.ifc \type, prep
+        rshrn           v0.4h, v0.4s, #6
+        rshrn2          v0.8h, v1.4s, #6
+        sub             z0.h, z0.h, #PREP_BIAS
+.else   // put
+        srshl           v0.4s, v0.4s, v6.4s
+        srshl           v1.4s, v1.4s, v6.4s
+        sqxtun          v0.4h, v0.4s
+        sqxtun2         v0.8h, v1.4s
+        umin            v0.8h, v0.8h, v29.8h
+.endif
+        st1             {v0.8h}, [\ldst], \d_strd
+        b.gt            81b
+
+        subs            \w, \w, #8
+        add             \dst, \dst, #16
+        add             \src, \src, #16
+        b.gt            84b
+        ret             x15
+
+        .align FUNC_ALIGN
+L(\type\()_hv_filter8_\isa):
+        ldp             q24, q28, [\lsrc]
+        ldur            q26, [\lsrc, #8]
+        add             \lsrc, \lsrc, \s_strd
+
+        tbl             v23.16b, {v24.16b}, v30.16b
+        tbl             v24.16b, {v24.16b}, v31.16b
+        movi            v2.2d, #0
+        movi            v3.2d, #0
+        sdot            z2.d, z23.h, z4.h[0]
+        sdot            z3.d, z24.h, z4.h[0]
+
+        tbl             v25.16b, {v26.16b}, v30.16b
+        tbl             v26.16b, {v26.16b}, v31.16b
+        movi            v23.2d, #0
+        movi            v24.2d, #0
+        sdot            z23.d, z25.h, z4.h[0]
+        sdot            z24.d, z26.h, z4.h[0]
+
+        tbl             v27.16b, {v28.16b}, v30.16b
+        tbl             v28.16b, {v28.16b}, v31.16b
+        sdot            z2.d, z25.h, z4.h[1]
+        sdot            z3.d, z26.h, z4.h[1]
+        sdot            z23.d, z27.h, z4.h[1]
+        sdot            z24.d, z28.h, z4.h[1]
+
+        uzp1            v3.4s, v2.4s, v3.4s
+        uzp1            v24.4s, v23.4s, v24.4s
+        srshl           v23.4s, v3.4s, v5.4s
+        srshl           v24.4s, v24.4s, v5.4s
+        ret
+
+        .align FUNC_ALIGN
+L(\type\()_hv_filter4_\isa):
+        ld1             {v3.16b}, [\src], \s_strd
+
+        tbl             v2.16b, {v3.16b}, v30.16b
+        tbl             v3.16b, {v3.16b}, v31.16b
+        movi            v0.2d, #0
+        movi            v1.2d, #0
+        sdot            z0.d, z2.h, z4.h[0]
+        sdot            z1.d, z3.h, z4.h[0]
+
+        uzp1            v0.4s, v0.4s, v1.4s
+        srshl           v0.4s, v0.4s, v5.4s
+        ret
+
+        .align JUMP_ALIGN
+46:     // H4V6 - 4xN, put only: 2xN
+.ifc \type, put
+        lsr             \d_strd, \d_strd, #1        // hword index for `st1h`
+        whilelt         p1.h, wzr, \w               // masking for writes
+.endif
+        ext             v4.16b, v4.16b, v4.16b, #4  // [\xmy + 2 * 2]
+        add             \src, \src, #4
+
+        cmp             \h, #4
+        b.le            44f
+
+        sub             \src, \src, \s_strd         // src - 2 * s_strd - 3 * 2
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v16.4h, v0.4s
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v17.4h, v0.4s
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v18.4h, v0.4s
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v19.4h, v0.4s
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v20.4h, v0.4s
+
+        .align LOOP_ALIGN
+4:
+        ld1             {v3.16b}, [\src], \s_strd
+        smull           v24.4s, v16.4h, v7.h[1]
+        smlal           v24.4s, v17.4h, v7.h[2]
+
+        tbl             v2.16b, {v3.16b}, v30.16b
+        tbl             v3.16b, {v3.16b}, v31.16b
+        movi            v0.2d, #0
+        movi            v1.2d, #0
+        sdot            z0.d, z2.h, z4.h[0]
+        sdot            z1.d, z3.h, z4.h[0]
+
+        mov             v16.16b, v17.16b
+        mov             v17.16b, v18.16b
+        smlal           v24.4s, v18.4h, v7.h[3]
+        smlal           v24.4s, v19.4h, v7.h[4]
+        uzp1            v0.4s, v0.4s, v1.4s
+
+        mov             v18.16b, v19.16b
+        mov             v19.16b, v20.16b
+        subs            \h, \h, #1
+        srshl           v0.4s, v0.4s, v5.4s
+        smlal           v24.4s, v20.4h, v7.h[5]
+        xtn             v20.4h, v0.4s
+        smlal           v24.4s, v20.4h, v7.h[6]
+
+.ifc \type, prep
+        rshrn           v0.4h, v24.4s, #6
+        sub             z0.h, z0.h, #PREP_BIAS
+        str             d0, [\dst], #8
+.else   // put
+        srshl           v0.4s, v24.4s, v6.4s
+        sqxtun          v0.4h, v0.4s
+        umin            v0.4h, v0.4h, v29.4h
+        st1h            {z0.h}, p1, [\dst]
+        add             \dst, \dst, \d_strd, lsl #1
+.endif
+        b.gt            4b
+        ret             x15
+
+        .align JUMP_ALIGN
+44:     // H4V4 - 4x4, put only: 4x2, 2x4, 2x2
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v17.4h, v0.4s
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v18.4h, v0.4s
+        bl              L(\type\()_hv_filter4_\isa)
+        xtn             v19.4h, v0.4s
+
+        .align LOOP_ALIGN
+4:
+        ld1             {v3.16b}, [\src], \s_strd
+        smull           v24.4s, v17.4h, v7.h[2]
+        smlal           v24.4s, v18.4h, v7.h[3]
+
+        tbl             v2.16b, {v3.16b}, v30.16b
+        tbl             v3.16b, {v3.16b}, v31.16b
+        movi            v0.2d, #0
+        movi            v1.2d, #0
+        sdot            z0.d, z2.h, z4.h[0]
+        sdot            z1.d, z3.h, z4.h[0]
+        uzp1            v0.4s, v0.4s, v1.4s
+
+        mov             v17.16b, v18.16b
+        mov             v18.16b, v19.16b
+        subs            \h, \h, #1
+        srshl           v0.4s, v0.4s, v5.4s
+        smlal           v24.4s, v19.4h, v7.h[4]
+        xtn             v19.4h, v0.4s
+        smlal           v24.4s, v19.4h, v7.h[5]
+
+.ifc \type, prep
+        rshrn           v0.4h, v24.4s, #6
+        sub             z0.h, z0.h, #PREP_BIAS
+        str             d0, [\dst], #8
+.else   // put
+        srshl           v0.4s, v24.4s, v6.4s
+        sqxtun          v0.4h, v0.4s
+        umin            v0.4h, v0.4h, v29.4h
+        st1h            {z0.h}, p1, [\dst]
+        add             \dst, \dst, \d_strd, lsl #1
+.endif
+        b.gt            4b
+        ret             x15
+
+        .align JUMP_ALIGN
+L(\type\()_8tap_h_\isa):
+        movrel          x11, \type\()_8tap_h_\isa\()_tbl
+        ldrsw           x12, [x11, x8, lsl #2]
+.ifc \bdmax, w8
+        ldr             \bdmax, [sp]
+.endif
+.ifc \type, prep
+        clz             \bdmax, \bdmax
+        sub             \bdmax, \bdmax, #24
+        dup             v5.4s, \bdmax
+.else   // put
+        mov             w9, #34             // rounding for 10-bit case
+        mov             w10, #40            // rounding for 12-bit case
+        cmp             \bdmax, #0xFFF
+        csel            w9, w9, w10, ne     // select rounding based on \bdmax
+        dup             v5.8h, \bdmax
+        dup             v6.2d, x9
+.endif
+        add             x11, x11, x12
+        ld1sb           {z4.h}, p0/z, [\xmx]
+        br              x11
+
+        .align JUMP_ALIGN
+20:     // H - 4xN, put only: 2xN
+40:
+        AARCH64_VALID_JUMP_TARGET
+        add             \src, \src, #4              // src - 1 * 2
+        ext             v4.16b, v4.16b, v4.16b, #4  // [\xmy + 2 * 2]
+.ifc \type, put
+        lsr             \d_strd, \d_strd, #1        // hword index for `st1h`
+        whilelt         p1.h, wzr, \w               // masking for writes
+.endif
+        .align LOOP_ALIGN
+4:
+        ldr             q17, [\src]
+        ldr             q19, [\src, \s_strd]
+        add             \src, \src, \s_strd, lsl #1
+
+.ifc \type, prep
+        movi            v0.2d, #0
+        movi            v1.2d, #0
+        movi            v2.2d, #0
+        movi            v3.2d, #0
+.else
+        mov             v0.16b, v6.16b
+        mov             v1.16b, v6.16b
+        mov             v2.16b, v6.16b
+        mov             v3.16b, v6.16b
+.endif
+        tbl             v16.16b, {v17.16b}, v30.16b
+        tbl             v17.16b, {v17.16b}, v31.16b
+        sdot            z0.d, z16.h, z4.h[0]
+        sdot            z1.d, z17.h, z4.h[0]
+        subs            \h, \h, #2
+        tbl             v18.16b, {v19.16b}, v30.16b
+        tbl             v19.16b, {v19.16b}, v31.16b
+        sdot            z2.d, z18.h, z4.h[0]
+        sdot            z3.d, z19.h, z4.h[0]
+
+        uzp1            v0.4s, v0.4s, v1.4s
+        uzp1            v1.4s, v2.4s, v3.4s
+.ifc \type, prep
+        srshl           v0.4s, v0.4s, v5.4s
+        srshl           v1.4s, v1.4s, v5.4s
+        uzp1            v0.8h, v0.8h, v1.8h
+        sub             z0.h, z0.h, #PREP_BIAS
+        str             q0, [\dst], #16
+.else   // put
+        sqshrun         v0.4h, v0.4s, #6
+        sqshrun         v1.4h, v1.4s, #6
+        umin            v0.4h, v0.4h, v5.4h
+        umin            v1.4h, v1.4h, v5.4h
+        st1h            {z0.h}, p1, [\dst]
+        st1h            {z1.h}, p1, [\dst, \d_strd, lsl #1]
+        add             \dst, \dst, \d_strd, lsl #2
+.endif
+        b.gt            4b
+        ret
+
+        .align JUMP_ALIGN
+80:     // H - 8xN
+        AARCH64_VALID_JUMP_TARGET
+
+        .align LOOP_ALIGN
+8:
+        ldp             q17, q21, [\src]
+        ldur            q19, [\src, #8]
+
+.ifc \type, prep
+        movi            v0.2d, #0
+        movi            v2.2d, #0
+.else
+        mov             v0.16b, v6.16b
+        mov             v2.16b, v6.16b
+.endif
+        tbl             v16.16b, {v17.16b}, v30.16b
+        tbl             v17.16b, {v17.16b}, v31.16b
+        add             \src, \src, \s_strd
+        sdot            z0.d, z16.h, z4.h[0]
+        sdot            z2.d, z17.h, z4.h[0]
+
+        tbl             v18.16b, {v19.16b}, v30.16b
+        tbl             v19.16b, {v19.16b}, v31.16b
+.ifc \type, prep
+        movi            v16.2d, #0
+        movi            v17.2d, #0
+.else
+        mov             v16.16b, v6.16b
+        mov             v17.16b, v6.16b
+.endif
+        ldp             q23, q27, [\src]
+        ldur            q25, [\src, #8]
+
+        sdot            z16.d, z18.h, z4.h[0]
+        sdot            z17.d, z19.h, z4.h[0]
+
+        tbl             v22.16b, {v23.16b}, v30.16b
+        tbl             v23.16b, {v23.16b}, v31.16b
+.ifc \type, prep
+        movi            v1.2d, #0
+        movi            v3.2d, #0
+.else
+        mov             v1.16b, v6.16b
+        mov             v3.16b, v6.16b
+.endif
+        add             \src, \src, \s_strd
+        sdot            z1.d, z22.h, z4.h[0]
+        sdot            z3.d, z23.h, z4.h[0]
+
+        tbl             v24.16b, {v25.16b}, v30.16b
+        tbl             v25.16b, {v25.16b}, v31.16b
+.ifc \type, prep
+        movi            v22.2d, #0
+        movi            v23.2d, #0
+.else
+        mov             v22.16b, v6.16b
+        mov             v23.16b, v6.16b
+.endif
+        sdot            z22.d, z24.h, z4.h[0]
+        sdot            z23.d, z25.h, z4.h[0]
+
+        tbl             v20.16b, {v21.16b}, v30.16b
+        tbl             v21.16b, {v21.16b}, v31.16b
+        sdot            z0.d, z18.h, z4.h[1]
+        sdot            z2.d, z19.h, z4.h[1]
+        tbl             v26.16b, {v27.16b}, v30.16b
+        tbl             v27.16b, {v27.16b}, v31.16b
+        sdot            z16.d, z20.h, z4.h[1]
+        sdot            z17.d, z21.h, z4.h[1]
+
+        sdot            z1.d, z24.h, z4.h[1]
+        sdot            z3.d, z25.h, z4.h[1]
+
+        sdot            z22.d, z26.h, z4.h[1]
+        sdot            z23.d, z27.h, z4.h[1]
+
+        subs            \h, \h, #2
+        uzp1            v0.4s, v0.4s, v2.4s
+        uzp1            v2.4s, v16.4s, v17.4s
+        uzp1            v1.4s, v1.4s, v3.4s
+        uzp1            v3.4s, v22.4s, v23.4s
+.ifc \type, prep
+        srshl           v0.4s, v0.4s, v5.4s
+        srshl           v2.4s, v2.4s, v5.4s
+        srshl           v1.4s, v1.4s, v5.4s
+        srshl           v3.4s, v3.4s, v5.4s
+        uzp1            v0.8h, v0.8h, v2.8h
+        uzp1            v1.8h, v1.8h, v3.8h
+        sub             z0.h, z0.h, #PREP_BIAS
+        sub             z1.h, z1.h, #PREP_BIAS
+        stp             q0, q1, [\dst], #32
+.else   // put
+        sqshrun         v0.4h, v0.4s, #6
+        sqshrun2        v0.8h, v2.4s, #6
+        sqshrun         v1.4h, v1.4s, #6
+        sqshrun2        v1.8h, v3.4s, #6
+        umin            v0.8h, v0.8h, v5.8h
+        umin            v1.8h, v1.8h, v5.8h
+        st1             {v0.16b}, [\dst], \d_strd
+        st1             {v1.16b}, [\dst], \d_strd
+.endif
+        b.gt            8b
+        ret
+
+        .align JUMP_ALIGN
+160:    // H - 16xN
+        AARCH64_VALID_JUMP_TARGET
+
+        .align LOOP_ALIGN
+16:
+        ldp             q17, q21, [\src]
+        ldur            q19, [\src, #8]
+
+.ifc \type, prep
+        movi            v0.2d, #0
+        movi            v2.2d, #0
+.else
+        mov             v0.16b, v6.16b
+        mov             v2.16b, v6.16b
+.endif
+        tbl             v16.16b, {v17.16b}, v30.16b
+        tbl             v17.16b, {v17.16b}, v31.16b
+        sdot            z0.d, z16.h, z4.h[0]
+        sdot            z2.d, z17.h, z4.h[0]
+
+        tbl             v18.16b, {v19.16b}, v30.16b
+        tbl             v19.16b, {v19.16b}, v31.16b
+.ifc \type, prep
+        movi            v16.2d, #0
+        movi            v17.2d, #0
+.else
+        mov             v16.16b, v6.16b
+        mov             v17.16b, v6.16b
+.endif
+        ldur            q25, [\src, #24]
+        ldr             q27, [\src, #32]
+
+        sdot            z16.d, z18.h, z4.h[0]
+        sdot            z17.d, z19.h, z4.h[0]
+
+        tbl             v22.16b, {v21.16b}, v30.16b
+        tbl             v23.16b, {v21.16b}, v31.16b
+.ifc \type, prep
+        movi            v1.2d, #0
+        movi            v3.2d, #0
+.else
+        mov             v1.16b, v6.16b
+        mov             v3.16b, v6.16b
+.endif
+        add             \src, \src, \s_strd
+        sdot            z1.d, z22.h, z4.h[0]
+        sdot            z3.d, z23.h, z4.h[0]
+
+        tbl             v24.16b, {v25.16b}, v30.16b
+        tbl             v25.16b, {v25.16b}, v31.16b
+.ifc \type, prep
+        movi            v22.2d, #0
+        movi            v23.2d, #0
+.else
+        mov             v22.16b, v6.16b
+        mov             v23.16b, v6.16b
+.endif
+        sdot            z22.d, z24.h, z4.h[0]
+        sdot            z23.d, z25.h, z4.h[0]
+
+        tbl             v20.16b, {v21.16b}, v30.16b
+        tbl             v21.16b, {v21.16b}, v31.16b
+        sdot            z0.d, z18.h, z4.h[1]
+        sdot            z2.d, z19.h, z4.h[1]
+        tbl             v26.16b, {v27.16b}, v30.16b
+        tbl             v27.16b, {v27.16b}, v31.16b
+        sdot            z16.d, z20.h, z4.h[1]
+        sdot            z17.d, z21.h, z4.h[1]
+
+        sdot            z1.d, z24.h, z4.h[1]
+        sdot            z3.d, z25.h, z4.h[1]
+
+        sdot            z22.d, z26.h, z4.h[1]
+        sdot            z23.d, z27.h, z4.h[1]
+
+        subs            \h, \h, #1
+        uzp1            v0.4s, v0.4s, v2.4s
+        uzp1            v2.4s, v16.4s, v17.4s
+        uzp1            v1.4s, v1.4s, v3.4s
+        uzp1            v3.4s, v22.4s, v23.4s
+.ifc \type, prep
+        srshl           v0.4s, v0.4s, v5.4s
+        srshl           v2.4s, v2.4s, v5.4s
+        srshl           v1.4s, v1.4s, v5.4s
+        srshl           v3.4s, v3.4s, v5.4s
+        uzp1            v0.8h, v0.8h, v2.8h
+        uzp1            v1.8h, v1.8h, v3.8h
+        sub             z0.h, z0.h, #PREP_BIAS
+        sub             z1.h, z1.h, #PREP_BIAS
+        stp             q0, q1, [\dst], #32
+.else   // put
+        sqshrun         v0.4h, v0.4s, #6
+        sqshrun2        v0.8h, v2.4s, #6
+        sqshrun         v1.4h, v1.4s, #6
+        sqshrun2        v1.8h, v3.4s, #6
+        umin            v0.8h, v0.8h, v5.8h
+        umin            v1.8h, v1.8h, v5.8h
+        st1             {v0.16b, v1.16b}, [\dst], \d_strd
+.endif
+        b.gt            16b
+        ret
+
+        .align JUMP_ALIGN
+320:    // H - 32xN+
+640:
+1280:
+        AARCH64_VALID_JUMP_TARGET
+.ifc \type, put
+        sub             \d_strd, \d_strd, \w, uxtw #1
+.endif
+        sub             \s_strd, \s_strd, \w, uxtw #1
+        mov             w8, \w
+
+        .align LOOP_ALIGN
+32:
+        ldp             q17, q21, [\src]
+        ldur            q19, [\src, #8]
+
+.ifc \type, prep
+        movi            v0.2d, #0
+        movi            v2.2d, #0
+.else
+        mov             v0.16b, v6.16b
+        mov             v2.16b, v6.16b
+.endif
+        tbl             v16.16b, {v17.16b}, v30.16b
+        tbl             v17.16b, {v17.16b}, v31.16b
+        sdot            z0.d, z16.h, z4.h[0]
+        sdot            z2.d, z17.h, z4.h[0]
+
+        tbl             v18.16b, {v19.16b}, v30.16b
+        tbl             v19.16b, {v19.16b}, v31.16b
+.ifc \type, prep
+        movi            v16.2d, #0
+        movi            v17.2d, #0
+.else
+        mov             v16.16b, v6.16b
+        mov             v17.16b, v6.16b
+.endif
+        ldur            q25, [\src, #24]
+
+        sdot            z16.d, z18.h, z4.h[0]
+        sdot            z17.d, z19.h, z4.h[0]
+
+        ldr             q27, [\src, #32]!
+
+        tbl             v22.16b, {v21.16b}, v30.16b
+        tbl             v23.16b, {v21.16b}, v31.16b
+.ifc \type, prep
+        movi            v1.2d, #0
+        movi            v3.2d, #0
+.else
+        mov             v1.16b, v6.16b
+        mov             v3.16b, v6.16b
+.endif
+        sdot            z1.d, z22.h, z4.h[0]
+        sdot            z3.d, z23.h, z4.h[0]
+
+        tbl             v24.16b, {v25.16b}, v30.16b
+        tbl             v25.16b, {v25.16b}, v31.16b
+.ifc \type, prep
+        movi            v22.2d, #0
+        movi            v23.2d, #0
+.else
+        mov             v22.16b, v6.16b
+        mov             v23.16b, v6.16b
+.endif
+        sdot            z22.d, z24.h, z4.h[0]
+        sdot            z23.d, z25.h, z4.h[0]
+
+        tbl             v20.16b, {v21.16b}, v30.16b
+        tbl             v21.16b, {v21.16b}, v31.16b
+        sdot            z0.d, z18.h, z4.h[1]
+        sdot            z2.d, z19.h, z4.h[1]
+        tbl             v26.16b, {v27.16b}, v30.16b
+        tbl             v27.16b, {v27.16b}, v31.16b
+        sdot            z16.d, z20.h, z4.h[1]
+        sdot            z17.d, z21.h, z4.h[1]
+
+        sdot            z1.d, z24.h, z4.h[1]
+        sdot            z3.d, z25.h, z4.h[1]
+
+        sdot            z22.d, z26.h, z4.h[1]
+        sdot            z23.d, z27.h, z4.h[1]
+
+        subs            w8, w8, #16
+        uzp1            v0.4s, v0.4s, v2.4s
+        uzp1            v2.4s, v16.4s, v17.4s
+        uzp1            v1.4s, v1.4s, v3.4s
+        uzp1            v3.4s, v22.4s, v23.4s
+.ifc \type, prep
+        srshl           v0.4s, v0.4s, v5.4s
+        srshl           v2.4s, v2.4s, v5.4s
+        srshl           v1.4s, v1.4s, v5.4s
+        srshl           v3.4s, v3.4s, v5.4s
+        uzp1            v0.8h, v0.8h, v2.8h
+        uzp1            v1.8h, v1.8h, v3.8h
+        sub             z0.h, z0.h, #PREP_BIAS
+        sub             z1.h, z1.h, #PREP_BIAS
+.else   // put
+        sqshrun         v0.4h, v0.4s, #6
+        sqshrun2        v0.8h, v2.4s, #6
+        sqshrun         v1.4h, v1.4s, #6
+        sqshrun2        v1.8h, v3.4s, #6
+        umin            v0.8h, v0.8h, v5.8h
+        umin            v1.8h, v1.8h, v5.8h
+.endif
+        stp             q0, q1, [\dst], #32
+        b.gt            32b
+
+        add             \src, \src, \s_strd
+.ifc \type, put
+        add             \dst, \dst, \d_strd
+.endif
+        subs            \h, \h, #1
+        mov             w8, \w
+        b.gt            32b
+        ret
+endfunc
+
+jumptable \type\()_8tap_h_\isa\()_tbl
+        .word 1280b - \type\()_8tap_h_\isa\()_tbl
+        .word 640b  - \type\()_8tap_h_\isa\()_tbl
+        .word 320b  - \type\()_8tap_h_\isa\()_tbl
+        .word 160b  - \type\()_8tap_h_\isa\()_tbl
+        .word 80b   - \type\()_8tap_h_\isa\()_tbl
+        .word 40b   - \type\()_8tap_h_\isa\()_tbl
+.ifc \type, put
+        .word 20b   - \type\()_8tap_h_\isa\()_tbl
+.endif
+endjumptable
+.endm
+
+
+function prep_sve
+        movrel          x9, prep_tbl
+        mov             w6, #19
+        ldrsw           x8, [x9, x8, lsl #2]
+        sub             w6, w6, w7, lsr #8          // 19 - bdmax / 256
+        add             x9, x9, x8
+        movi            v30.8h, #PREP_BIAS_NEG
+        dup             v29.8h, w6                  // 10b: 1 << 4, 12b: 1 << 2
+        br              x9
+
+        .align JUMP_ALIGN
+40:     // prep - 4xN
+        AARCH64_VALID_JUMP_TARGET
+
+        .align LOOP_ALIGN
+4:
+        ldr             d0, [x1]
+        ldr             d1, [x1, x2]
+        add             x1, x1, x2, lsl #1
+        subs            w4, w4, #2
+        mad             z0.h, p0/m, z29.h, z30.h
+        mad             z1.h, p0/m, z29.h, z30.h
+        stp             d0, d1, [x0], #16
+        b.gt            4b
+        ret
+
+        .align JUMP_ALIGN
+80:     // prep - 8xN
+        AARCH64_VALID_JUMP_TARGET
+
+        .align LOOP_ALIGN
+8:
+        ld1             {v0.8h}, [x1], x2
+        ld1             {v1.8h}, [x1], x2
+        subs            w4, w4, #2
+        mad             z0.h, p0/m, z29.h, z30.h
+        mad             z1.h, p0/m, z29.h, z30.h
+        stp             q0, q1, [x0], #32
+        b.gt            8b
+        ret
+
+        .align JUMP_ALIGN
+160:    // prep - 16xN
+        AARCH64_VALID_JUMP_TARGET
+
+        .align LOOP_ALIGN
+16:
+        ld1             {v0.8h, v1.8h}, [x1], x2
+        mad             z0.h, p0/m, z29.h, z30.h
+        mad             z1.h, p0/m, z29.h, z30.h
+        subs            w4, w4, #2
+        ld1             {v2.8h, v3.8h}, [x1], x2
+        mad             z2.h, p0/m, z29.h, z30.h
+        mad             z3.h, p0/m, z29.h, z30.h
+        stp             q0, q1, [x0]
+        stp             q2, q3, [x0, #32]
+        add             x0, x0, #64
+        b.gt            16b
+        ret
+
+        .align JUMP_ALIGN
+320:    // prep - 32xN
+        AARCH64_VALID_JUMP_TARGET
+
+        .align LOOP_ALIGN
+32:
+        ldp             q0, q1, [x1]
+        mad             z0.h, p0/m, z29.h, z30.h
+        mad             z1.h, p0/m, z29.h, z30.h
+        ldp             q2, q3, [x1, #32]
+        subs            w4, w4, #1
+        mad             z2.h, p0/m, z29.h, z30.h
+        mad             z3.h, p0/m, z29.h, z30.h
+        add             x1, x1, x2
+        stp             q0, q1, [x0]
+        stp             q2, q3, [x0, #32]
+        add             x0, x0, #64
+        b.gt            32b
+        ret
+
+        .align JUMP_ALIGN
+640:    // prep - 64xN
+        AARCH64_VALID_JUMP_TARGET
+
+        .align LOOP_ALIGN
+64:
+        ldp             q0, q1, [x1]
+        mad             z0.h, p0/m, z29.h, z30.h
+        mad             z1.h, p0/m, z29.h, z30.h
+        ldp             q2, q3, [x1, #32]
+        mad             z2.h, p0/m, z29.h, z30.h
+        mad             z3.h, p0/m, z29.h, z30.h
+        ldp             q4, q5, [x1, #64]
+        mad             z4.h, p0/m, z29.h, z30.h
+        mad             z5.h, p0/m, z29.h, z30.h
+        ldp             q6, q7, [x1, #96]
+        add             x1, x1, x2
+        subs            w4, w4, #1
+        mad             z6.h, p0/m, z29.h, z30.h
+        mad             z7.h, p0/m, z29.h, z30.h
+        stp             q0, q1, [x0]
+        stp             q2, q3, [x0, #32]
+        stp             q4, q5, [x0, #64]
+        stp             q6, q7, [x0, #96]
+        add             x0, x0, #128
+        b.gt            64b
+        ret
+
+        .align JUMP_ALIGN
+1280:   // prep - 128xN
+        AARCH64_VALID_JUMP_TARGET
+
+        .align LOOP_ALIGN
+128:
+        ldp             q0, q1, [x1]
+        mad             z0.h, p0/m, z29.h, z30.h
+        mad             z1.h, p0/m, z29.h, z30.h
+        ldp             q2, q3, [x1, #32]
+        mad             z2.h, p0/m, z29.h, z30.h
+        mad             z3.h, p0/m, z29.h, z30.h
+        ldp             q4, q5, [x1, #64]
+        mad             z4.h, p0/m, z29.h, z30.h
+        mad             z5.h, p0/m, z29.h, z30.h
+        ldp             q6, q7, [x1, #96]
+        mad             z6.h, p0/m, z29.h, z30.h
+        mad             z7.h, p0/m, z29.h, z30.h
+        ldp             q16, q17, [x1, #128]
+        mad             z16.h, p0/m, z29.h, z30.h
+        mad             z17.h, p0/m, z29.h, z30.h
+        ldp             q18, q19, [x1, #160]
+        mad             z18.h, p0/m, z29.h, z30.h
+        mad             z19.h, p0/m, z29.h, z30.h
+        ldp             q20, q21, [x1, #192]
+        mad             z20.h, p0/m, z29.h, z30.h
+        mad             z21.h, p0/m, z29.h, z30.h
+        ldp             q22, q23, [x1, #224]
+        add             x1, x1, x2
+        mad             z22.h, p0/m, z29.h, z30.h
+        mad             z23.h, p0/m, z29.h, z30.h
+        subs            w4, w4, #1
+        stp             q0, q1, [x0]
+        stp             q2, q3, [x0, #32]
+        stp             q4, q5, [x0, #64]
+        stp             q6, q7, [x0, #96]
+        stp             q16, q17, [x0, #128]
+        stp             q18, q19, [x0, #160]
+        stp             q20, q21, [x0, #192]
+        stp             q22, q23, [x0, #224]
+        add             x0, x0, #256
+        b.gt            128b
+        ret
+endfunc
+
+jumptable prep_tbl
+        .word 1280b - prep_tbl
+        .word 640b  - prep_tbl
+        .word 320b  - prep_tbl
+        .word 160b  - prep_tbl
+        .word 80b   - prep_tbl
+        .word 40b   - prep_tbl
+endjumptable
+
+
+// dst(x0), d_strd(x9), src(x1), s_strd(x2), w(w3), h(w4), mx(w5), my(w6), bdmax(w7)
+// xmx(x5), xmy(x6), ldst(x5), lsrc(x6), wd_strd(w9), ws_strd(w2)
+filter_8tap_fn prep, sve2, x0, x9, x1, x2, w3, w4, w5, w6, w7, x5, x6, x5, x6, w9, w2
+
+// dst(x0) d_strd(x1) src(x2) s_strd(x3) w(w4) h(w5) mx(w6) my(w7), bdmax(w8)
+// xmx(x6), xmy(x7), ldst(x6), lsrc(x7), wd_strd(w1), ws_strd(w3)
+filter_8tap_fn  put, sve2, x0, x1, x2, x3, w4, w5, w6, w7, w8, x6, x7, x6, x7, w1, w3
+
+DISABLE_SVE2
+DISABLE_SVE
+#endif  // HAVE_SVE2
diff --git a/src/arm/64/mc_dotprod.S b/src/arm/64/mc_dotprod.S
index a4f98a2..079ff9e 100644
--- a/src/arm/64/mc_dotprod.S
+++ b/src/arm/64/mc_dotprod.S
@@ -45,34 +45,39 @@ ENABLE_DOTPROD
 #define LOOP_ALIGN      2
 
 
-// Lookup table used to help conversion of shifted 32-bit values to 8-bit.
-        .align 4
-L(hv_tbl_neon_dotprod):
+const h_tbl_neon_dotprod, align=4
+        // Shuffle indices to permute horizontal samples in preparation for
+        // input to SDOT instructions. The 8-tap horizontal convolution uses
+        // sample indices in the interval of [-3, 4] relative to the current
+        // sample position.
+        .byte  0,  1,  2,  3,   1,  2,  3,  4,   2,  3,  4,  5,   3,  4,  5,  6
+        .byte  4,  5,  6,  7,   5,  6,  7,  8,   6,  7,  8,  9,   7,  8,  9, 10
+        .byte  8,  9, 10, 11,   9, 10, 11, 12,  10, 11, 12, 13,  11, 12, 13, 14
+
+        // Shuffle indices to permute horizontal samples in preparation for
+        // input to USMMLA instructions.
+#define OFFSET_USMMLA 48
+        .byte  0,  1,  2,  3,   4,  5,  6,  7,   2,  3,  4,  5,   6,  7,  8,  9
+        .byte  4,  5,  6,  7,   8,  9, 10, 11,   6,  7,  8,  9,  10, 11, 12, 13
+
+        // Lookup table used to help conversion of shifted 32-bit values to 8-bit.
+#define OFFSET_CVT_32_8 80
         .byte  1,  2,  5,  6,   9, 10, 13, 14,  17, 18, 21, 22,  25, 26, 29, 30
-
-// Shuffle indices to permute horizontal samples in preparation for input to
-// SDOT instructions. The 8-tap horizontal convolution uses sample indices in the
-// interval of [-3, 4] relative to the current sample position. We load samples
-// from index value -4 to keep loads word aligned, so the shuffle bytes are
-// translated by 1 to handle this.
-        .align 4
-L(h_tbl_neon_dotprod):
-        .byte  1,  2,  3,  4,   2,  3,  4,  5,   3,  4,  5,  6,   4,  5,  6,  7
-        .byte  5,  6,  7,  8,   6,  7,  8,  9,   7,  8,  9, 10,   8,  9, 10, 11
-        .byte  9, 10, 11, 12,  10, 11, 12, 13,  11, 12, 13, 14,  12, 13, 14, 15
-
-// Vertical convolutions are also using SDOT instructions, where a 128-bit
-// register contains a transposed 4x4 matrix of values. Subsequent iterations of
-// the vertical convolution can reuse the 3x4 sub-matrix from the previous loop
-// iteration. These shuffle indices shift and merge this 4x4 matrix with the
-// values of a new line.
-        .align 4
-L(v_tbl_neon_dotprod):
+endconst
+
+const v_tbl_neon_dotprod, align=4
+        // Vertical convolutions are also using SDOT instructions, where a
+        // 128-bit register contains a transposed 4x4 matrix of values.
+        // Subsequent iterations of the vertical convolution can reuse the
+        // 3x4 sub-matrix from the previous loop iteration. These shuffle
+        // indices shift and merge this 4x4 matrix with the values of a new
+        // line.
         .byte  1,  2,  3, 16,   5,  6,  7, 20,   9, 10, 11, 24,  13, 14, 15, 28
         .byte  1,  2,  3, 16,   5,  6,  7, 17,   9, 10, 11, 18,  13, 14, 15, 19
         .byte  1,  2,  3, 20,   5,  6,  7, 21,   9, 10, 11, 22,  13, 14, 15, 23
         .byte  1,  2,  3, 24,   5,  6,  7, 25,   9, 10, 11, 26,  13, 14, 15, 27
         .byte  1,  2,  3, 28,   5,  6,  7, 29,   9, 10, 11, 30,  13, 14, 15, 31
+endconst
 
 
 .macro make_8tap_fn op, type, type_h, type_v, isa, jump=1
@@ -111,24 +116,24 @@ function \type\()_8tap_\isa, align=FUNC_ALIGN
         .align JUMP_ALIGN
 L(\type\()_8tap_v_\isa):
         madd            \my, \my, w11, w10
-        ldr             q6, L(v_tbl_neon_dotprod)
+        movrel          x13, v_tbl_neon_dotprod
         sub             \src, \src, \s_strd
 .ifc \isa, neon_dotprod
     .ifc \type, prep
-        mov             w8, 0x2002          // FILTER_WEIGHT * 128 + rounding
+        mov             w8, #0x2002         // FILTER_WEIGHT * 128 + rounding
         dup             v4.4s, w8
     .else
-        movi            v4.4s, #32, lsl 8   // FILTER_WEIGHT * 128, bias for SDOT
+        movi            v4.4s, #32, lsl #8  // FILTER_WEIGHT * 128, bias for SDOT
     .endif
 .endif
         ubfx            w11, \my, #7, #7
         and             \my, \my, #0x7F
-        ldr             q28, L(v_tbl_neon_dotprod) + 16
+        ldp             q6, q28, [x13]
         cmp             \h, #4
         csel            \my, \my, w11, le
         sub             \src, \src, \s_strd, lsl #1     // src - s_strd * 3
         add             \xmy, x12, \xmy, lsl #3         // subpel V filter address
-        ldr             q29, L(v_tbl_neon_dotprod) + 32
+        ldr             q29, [x13, #32]
 .ifc \isa, neon_dotprod
         movi            v5.16b, #128
 .endif
@@ -139,8 +144,7 @@ L(\type\()_8tap_v_\isa):
 
         // .align JUMP_ALIGN    // fallthrough
 160:    // V - 16xN+
-        ldr             q30, L(v_tbl_neon_dotprod) + 48
-        ldr             q31, L(v_tbl_neon_dotprod) + 64
+        ldp             q30, q31, [x13, #48]
 .ifc \type, prep
         add             \wd_strd, \w, \w
 .endif
@@ -678,18 +682,19 @@ L(\type\()_8tap_v_\isa):
 L(\type\()_8tap_h_hv_\isa):
         madd            \mx, \mx, w11, w9
         madd            w14, \my, w11, w10      // for HV
-        ldr             q28, L(h_tbl_neon_dotprod)
 .ifc \isa, neon_dotprod
-        mov             w13, 0x2002             // FILTER_WEIGHT * 128 + rounding
+        mov             w13, #0x2002            // FILTER_WEIGHT * 128 + rounding
         dup             v27.4s, w13             // put H overrides this
 .endif
-        sub             \src, \src, #4          // src - 4
-        ubfx            w9, \mx, #7, #7
+        movrel          x13, h_tbl_neon_dotprod
+        sub             \src, \src, #3          // src - 3
+        ldr             q28, [x13]              // for 4-tap & 8-tap H filters
+        ubfx            w15, \mx, #7, #7
         and             \mx, \mx, #0x7F
         ubfx            w11, w14, #7, #7        // for HV
         and             w14, w14, #0x7F         // for HV
         cmp             \w, #4
-        csel            \mx, \mx, w9, le
+        csel            \mx, \mx, w15, le
         add             \xmx, x12, \xmx, lsl #3 // subpel H filter address
 .ifc \isa, neon_dotprod
         movi            v24.16b, #128
@@ -699,19 +704,19 @@ L(\type\()_8tap_h_hv_\isa):
         // HV cases
         cmp             \h, #4
         csel            w14, w14, w11, le
-        sub             \src, \src, \s_strd, lsl #1 // src - s_strd * 2 - 4
+        sub             \src, \src, \s_strd, lsl #1 // src - s_strd * 2 - 3
         add             \xmy, x12, x14, lsl #3      // subpel V filter address
         mov             x15, x30
         ldr             d7, [\xmy]
 .ifc \type, put
-        ldr             q25, L(hv_tbl_neon_dotprod)
-.endif
+        ldr             q25, [x13, #(OFFSET_CVT_32_8)] // LUT to help conversion
+.endif                                                 // of 32b values to 8b
         sxtl            v7.8h, v7.8b
-        cmp             w10, SHARP1
+        cmp             w10, #SHARP1
         b.ne            L(\type\()_6tap_hv_\isa)    // vertical != SHARP1
 
         // HV 8-tap cases
-        sub             \src, \src, \s_strd         // src - s_strd * 3 - 4
+        sub             \src, \src, \s_strd         // src - s_strd * 3 - 3
         cmp             \w, #4
         b.eq            40f
 .ifc \type, put
@@ -720,8 +725,7 @@ L(\type\()_8tap_h_hv_\isa):
 
         // .align JUMP_ALIGN    // fallthrough
 80:     // HV8 - 8xN+
-        ldr             q29, L(h_tbl_neon_dotprod) + 16
-        ldr             q30, L(h_tbl_neon_dotprod) + 32
+        ldp             q29, q30, [x13, #16]
         ldr             d26, [\xmx]
 .ifc \type, prep
         add             \wd_strd, \w, \w
@@ -862,7 +866,7 @@ L(\type\()_8tap_h_hv_\isa):
 
         .align JUMP_ALIGN
 40:     // HV8 - 4xN
-        ldr             s26, [\xmx, #2]
+        ldur            s26, [\xmx, #2]
         add             \src, \src, #2
 
         bl              L(\type\()_hv_filter4_\isa)
@@ -932,7 +936,7 @@ L(\type\()_8tap_h_hv_\isa):
 .ifc \type, put
         .align JUMP_ALIGN
 20:     // HV8 - 2xN
-        ldr             s26, [\xmx, #2]
+        ldur            s26, [\xmx, #2]
         add             \src, \src, #2
 
         bl              L(\type\()_hv_filter4_\isa)
@@ -1007,12 +1011,91 @@ L(\type\()_6tap_hv_\isa):
 
         // .align JUMP_ALIGN    // fallthrough
 80:     // HV6 - 8xN+
-        ldr             q29, L(h_tbl_neon_dotprod) + 16
-        ldr             q30, L(h_tbl_neon_dotprod) + 32
         ldr             d26, [\xmx]
 .ifc \type, prep
         add             \wd_strd, \w, \w
 .endif
+.ifc \isa, neon_i8mm
+        cmp             w9, #SHARP1
+        b.eq            88f             // horizontal == SHARP1
+
+        ldp             q29, q30, [x13, #(OFFSET_USMMLA)]
+        ext             v0.8b, v26.8b, v26.8b, #7
+        ins             v26.d[1], v0.d[0]
+
+        .align LOOP_ALIGN
+81:
+        mov             \lsrc, \src
+        mov             \ldst, \dst
+        mov             w8, \h
+
+        bl              L(\type\()_hv_filter6_neon_i8mm)
+        srshr           v16.8h, v22.8h, #2
+        bl              L(\type\()_hv_filter6_neon_i8mm)
+        srshr           v17.8h, v22.8h, #2
+        bl              L(\type\()_hv_filter6_neon_i8mm)
+        srshr           v18.8h, v22.8h, #2
+        bl              L(\type\()_hv_filter6_neon_i8mm)
+        srshr           v19.8h, v22.8h, #2
+        bl              L(\type\()_hv_filter6_neon_i8mm)
+        srshr           v20.8h, v22.8h, #2
+
+        .align LOOP_ALIGN
+8:
+        ld1             {v23.16b}, [\lsrc], \s_strd
+
+        smull           v0.4s, v16.4h, v7.h[1]
+        smull2          v1.4s, v16.8h, v7.h[1]
+        mov             v16.16b, v17.16b
+        movi            v5.4s, #0
+        movi            v6.4s, #0
+        tbl             v2.16b, {v23.16b}, v29.16b
+        tbl             v3.16b, {v23.16b}, v30.16b
+
+        smlal           v0.4s, v17.4h, v7.h[2]
+        smlal2          v1.4s, v17.8h, v7.h[2]
+        mov             v17.16b, v18.16b
+
+        usmmla          v5.4s, v2.16b, v26.16b
+        usmmla          v6.4s, v3.16b, v26.16b
+
+        smlal           v0.4s, v18.4h, v7.h[3]
+        smlal2          v1.4s, v18.8h, v7.h[3]
+        mov             v18.16b, v19.16b
+        subs            w8, w8, #1
+
+        smlal           v0.4s, v19.4h, v7.h[4]
+        smlal2          v1.4s, v19.8h, v7.h[4]
+        uzp1            v23.8h, v5.8h, v6.8h
+        mov             v19.16b, v20.16b
+
+        smlal           v0.4s, v20.4h, v7.h[5]
+        smlal2          v1.4s, v20.8h, v7.h[5]
+        srshr           v20.8h, v23.8h, #2
+        smlal           v0.4s, v20.4h, v7.h[6]
+        smlal2          v1.4s, v20.8h, v7.h[6]
+    .ifc \type, prep
+        rshrn           v0.4h, v0.4s, #6
+        rshrn2          v0.8h, v1.4s, #6
+        st1             {v0.8h}, [\ldst], \d_strd
+        b.gt            8b
+        add             \dst, \dst, #16
+    .else
+        tbl             v0.16b, {v0.16b, v1.16b}, v25.16b
+        sqrshrun        v0.8b, v0.8h, #2
+        st1             {v0.8b}, [\ldst], \d_strd
+        b.gt            8b
+        add             \dst, \dst, #8
+    .endif
+        add             \src, \src, #8
+        subs            \w, \w, #8
+        b.gt            81b
+        ret             x15
+
+        .align JUMP_ALIGN
+88:
+.endif  // neon_i8mm
+        ldp             q29, q30, [x13, #16]
 
         .align LOOP_ALIGN
 81:
@@ -1044,8 +1127,8 @@ L(\type\()_6tap_hv_\isa):
 .endif
         .align LOOP_ALIGN
 8:
-        ldr             q23, [\xmy]
-        add             \xmy, \xmy, \s_strd
+        ldr             q23, [\lsrc]
+        add             \lsrc, \lsrc, \s_strd
 
         smull           v0.4s, v16.4h, v7.h[1]
         smull2          v1.4s, v16.8h, v7.h[1]
@@ -1132,6 +1215,20 @@ L(\type\()_hv_filter8_\isa):
         uzp1            v22.8h, v22.8h, v23.8h
         ret
 
+.ifc \isa, neon_i8mm
+        .align FUNC_ALIGN
+L(\type\()_hv_filter6_neon_i8mm):
+        ld1             {v4.16b}, [\lsrc], \s_strd
+        movi            v22.4s, #0
+        movi            v23.4s, #0
+        tbl             v2.16b, {v4.16b}, v29.16b
+        tbl             v3.16b, {v4.16b}, v30.16b
+        usmmla          v22.4s, v2.16b, v26.16b
+        usmmla          v23.4s, v3.16b, v26.16b
+        uzp1            v22.8h, v22.8h, v23.8h
+        ret
+.endif
+
         .align FUNC_ALIGN
 L(\type\()_hv_filter4_\isa):
         ld1             {v4.8b}, [\src], \s_strd
@@ -1147,7 +1244,7 @@ L(\type\()_hv_filter4_\isa):
 
         .align JUMP_ALIGN
 40:     // HV6 - 4xN
-        ldr             s26, [\xmx, #2]
+        ldur            s26, [\xmx, #2]
         add             \src, \src, #2
 
         bl              L(\type\()_hv_filter4_\isa)
@@ -1208,7 +1305,7 @@ L(\type\()_hv_filter4_\isa):
 .ifc \type, put
         .align JUMP_ALIGN
 20:     // HV6 - 2xN
-        ldr             s26, [\xmx, #2]
+        ldur            s26, [\xmx, #2]
         add             \src, \src, #2
 
         bl              L(\type\()_hv_filter4_\isa)
@@ -1268,8 +1365,8 @@ L(\type\()_hv_filter4_\isa):
 
         .align JUMP_ALIGN
 L(\type\()_8tap_h_\isa):
-        adr             x9, L(\type\()_8tap_h_\isa\()_tbl)
-        ldrh            w8, [x9, x8, lsl #1]
+        movrel          x11, \type\()_8tap_h_\isa\()_tbl
+        ldrsw           x8, [x11, x8, lsl #2]
 .ifc \type, put
     .ifc \isa, neon_i8mm
         movi            v27.4s, #34     // special rounding
@@ -1278,15 +1375,15 @@ L(\type\()_8tap_h_\isa):
         dup             v27.4s, w10
     .endif
 .endif
-        sub             x9, x9, x8
-        br              x9
+        add             x11, x11, x8
+        br              x11
 
 .ifc \type, put
         .align JUMP_ALIGN
 20:     // H - 2xN
         AARCH64_VALID_JUMP_TARGET
         add             \src, \src, #2
-        ldr             s26, [\xmx, #2]
+        ldur            s26, [\xmx, #2]
 
         .align LOOP_ALIGN
 2:
@@ -1323,7 +1420,7 @@ L(\type\()_8tap_h_\isa):
 40:     // H - 4xN
         AARCH64_VALID_JUMP_TARGET
         add             \src, \src, #2
-        ldr             s26, [\xmx, #2]
+        ldur            s26, [\xmx, #2]
 
         .align LOOP_ALIGN
 4:
@@ -1372,9 +1469,63 @@ L(\type\()_8tap_h_\isa):
         .align JUMP_ALIGN
 80:     // H - 8xN
         AARCH64_VALID_JUMP_TARGET
-        ldr             q29, L(h_tbl_neon_dotprod) + 16
-        ldr             q30, L(h_tbl_neon_dotprod) + 32
         ldr             d26, [\xmx]
+.ifc \isa, neon_i8mm
+        cmp             w9, #SHARP1
+        b.eq            88f             // horizontal == SHARP1
+
+        ldp             q29, q30, [x13, #(OFFSET_USMMLA)]
+        ext             v0.8b, v26.8b, v26.8b, #7
+        ins             v26.d[1], v0.d[0]
+
+        .align LOOP_ALIGN
+8:
+        ldr             q0, [\src]
+        ldr             q16, [\src, \s_strd]
+        add             \src, \src, \s_strd, lsl #1
+    .ifc \type, prep
+        movi            v4.4s, #0
+        movi            v5.4s, #0
+        movi            v20.4s, #0
+        movi            v21.4s, #0
+    .else
+        mov             v4.16b, v27.16b
+        mov             v5.16b, v27.16b
+        mov             v20.16b, v27.16b
+        mov             v21.16b, v27.16b
+    .endif
+        tbl             v1.16b, {v0.16b}, v29.16b
+        tbl             v2.16b, {v0.16b}, v30.16b
+        tbl             v17.16b, {v16.16b}, v29.16b
+        tbl             v18.16b, {v16.16b}, v30.16b
+
+        usmmla          v4.4s, v1.16b, v26.16b
+        usmmla          v5.4s, v2.16b, v26.16b
+        usmmla          v20.4s, v17.16b, v26.16b
+        usmmla          v21.4s, v18.16b, v26.16b
+
+        uzp1            v4.8h, v4.8h, v5.8h
+        uzp1            v20.8h, v20.8h, v21.8h
+    .ifc \type, prep
+        srshr           v4.8h, v4.8h, #2
+        srshr           v20.8h, v20.8h, #2
+        subs            \h, \h, #2
+        stp             q4, q20, [\dst], #32
+    .else   // put
+        sqshrun         v4.8b, v4.8h, #6
+        sqshrun         v20.8b, v20.8h, #6
+        subs            \h, \h, #2
+        str             d4, [\dst]
+        str             d20, [\dst, \d_strd]
+        add             \dst, \dst, \d_strd, lsl #1
+    .endif
+        b.gt            8b
+        ret
+
+        .align JUMP_ALIGN
+88:
+.endif  // neon_i8mm
+        ldp             q29, q30, [x13, #16]
 
         .align LOOP_ALIGN
 8:
@@ -1438,14 +1589,66 @@ L(\type\()_8tap_h_\isa):
         .align JUMP_ALIGN
 160:    // H - 16xN
         AARCH64_VALID_JUMP_TARGET
-        ldr             q29, L(h_tbl_neon_dotprod) + 16
-        ldr             q30, L(h_tbl_neon_dotprod) + 32
         ldr             d26, [\xmx]
+.ifc \isa, neon_i8mm
+        cmp             w9, #SHARP1
+        b.eq            168f            // horizontal == SHARP1
+
+        ldp             q29, q30, [x13, #(OFFSET_USMMLA)]
+        ext             v0.8b, v26.8b, v26.8b, #7
+        ins             v26.d[1], v0.d[0]
+
+        .align LOOP_ALIGN
+16:
+        ldr             q16, [\src]
+        ldur            q17, [\src, #8] // avoid 2 register TBL for small cores
+        add             \src, \src, \s_strd
+    .ifc \type, prep
+        movi            v6.4s, #0
+        movi            v7.4s, #0
+        movi            v22.4s, #0
+        movi            v23.4s, #0
+    .else
+        mov             v6.16b, v27.16b
+        mov             v7.16b, v27.16b
+        mov             v22.16b, v27.16b
+        mov             v23.16b, v27.16b
+    .endif
+        tbl             v0.16b, {v16.16b}, v29.16b
+        tbl             v1.16b, {v16.16b}, v30.16b
+        tbl             v2.16b, {v17.16b}, v29.16b
+        tbl             v3.16b, {v17.16b}, v30.16b
+
+        usmmla          v6.4s, v0.16b, v26.16b
+        usmmla          v7.4s, v1.16b, v26.16b
+        usmmla          v22.4s, v2.16b, v26.16b
+        usmmla          v23.4s, v3.16b, v26.16b
+
+        uzp1            v6.8h, v6.8h, v7.8h
+        uzp1            v22.8h, v22.8h, v23.8h
+    .ifc \type, prep
+        srshr           v6.8h, v6.8h, #2
+        srshr           v22.8h, v22.8h, #2
+        subs            \h, \h, #1
+        stp             q6, q22, [\dst], #32
+    .else   // put
+        sqshrun         v6.8b, v6.8h, #6
+        sqshrun2        v6.16b, v22.8h, #6
+        subs            \h, \h, #1
+        st1             {v6.16b}, [\dst], \d_strd
+    .endif
+        b.gt            16b
+        ret
+
+        .align JUMP_ALIGN
+168:
+.endif  // neon_i8mm
+        ldp             q29, q30, [x13, #16]
 
         .align LOOP_ALIGN
 16:
         ldr             q16, [\src]
-        ldr             q17, [\src, #12]  // avoid 2 register TBL for small cores
+        ldur            q17, [\src, #12]  // avoid 2 register TBL for small cores
         add             \src, \src, \s_strd
 .ifc \type\()_\isa, prep_neon_i8mm
         movi            v6.4s, #0
@@ -1503,8 +1706,6 @@ L(\type\()_8tap_h_\isa):
 640:
 1280:
         AARCH64_VALID_JUMP_TARGET
-        ldr             q29, L(h_tbl_neon_dotprod) + 16
-        ldr             q30, L(h_tbl_neon_dotprod) + 32
         ldr             d26, [\xmx]
 .ifc \type, put
         sub             \d_strd, \d_strd, \w, uxtw
@@ -1512,10 +1713,73 @@ L(\type\()_8tap_h_\isa):
         sub             \s_strd, \s_strd, \w, uxtw
         mov             w8, \w
 
+.ifc \isa, neon_i8mm
+        cmp             w9, #SHARP1
+        b.eq            328f            // horizontal == SHARP1
+
+        ldp             q29, q30, [x13, #(OFFSET_USMMLA)]
+        ext             v0.8b, v26.8b, v26.8b, #7
+        ins             v26.d[1], v0.d[0]
+
+        .align LOOP_ALIGN
+32:
+        ldr             q16, [\src]
+        ldur            q17, [\src, #8] // avoid 2 register TBL for small cores
+        add             \src, \src, #16
+    .ifc \type, prep
+        movi            v6.4s, #0
+        movi            v7.4s, #0
+        movi            v22.4s, #0
+        movi            v23.4s, #0
+    .else
+        mov             v6.16b, v27.16b
+        mov             v7.16b, v27.16b
+        mov             v22.16b, v27.16b
+        mov             v23.16b, v27.16b
+    .endif
+        tbl             v0.16b, {v16.16b}, v29.16b
+        tbl             v1.16b, {v16.16b}, v30.16b
+        tbl             v2.16b, {v17.16b}, v29.16b
+        tbl             v3.16b, {v17.16b}, v30.16b
+
+        usmmla          v6.4s, v0.16b, v26.16b
+        usmmla          v7.4s, v1.16b, v26.16b
+        usmmla          v22.4s, v2.16b, v26.16b
+        usmmla          v23.4s, v3.16b, v26.16b
+
+        uzp1            v6.8h, v6.8h, v7.8h
+        uzp1            v22.8h, v22.8h, v23.8h
+    .ifc \type, prep
+        srshr           v6.8h, v6.8h, #2
+        srshr           v22.8h, v22.8h, #2
+        subs            w8, w8, #16
+        stp             q6, q22, [\dst], #32
+    .else   // put
+        sqshrun         v6.8b, v6.8h, #6
+        sqshrun2        v6.16b, v22.8h, #6
+        subs            w8, w8, #16
+        str             q6, [\dst], #16
+    .endif
+        b.gt            32b
+
+        add             \src, \src, \s_strd
+    .ifc \type, put
+        add             \dst, \dst, \d_strd
+    .endif
+        mov             w8, \w
+        subs            \h, \h, #1
+        b.gt            32b
+        ret
+
+        .align JUMP_ALIGN
+328:
+.endif  // neon_i8mm
+        ldp             q29, q30, [x13, #16]
+
         .align LOOP_ALIGN
 32:
         ldr             q16, [\src]
-        ldr             q17, [\src, #12]  // avoid 2 register TBL for small cores
+        ldur            q17, [\src, #12]  // avoid 2 register TBL for small cores
         add             \src, \src, #16
 .ifc \type\()_\isa, prep_neon_i8mm
         movi            v6.4s, #0
@@ -1575,19 +1839,19 @@ L(\type\()_8tap_h_\isa):
         subs            \h, \h, #1
         b.gt            32b
         ret
+endfunc
 
-L(\type\()_8tap_h_\isa\()_tbl):
-        .hword (L(\type\()_8tap_h_\isa\()_tbl) - 1280b)
-        .hword (L(\type\()_8tap_h_\isa\()_tbl) - 640b)
-        .hword (L(\type\()_8tap_h_\isa\()_tbl) - 320b)
-        .hword (L(\type\()_8tap_h_\isa\()_tbl) - 160b)
-        .hword (L(\type\()_8tap_h_\isa\()_tbl) - 80b)
-        .hword (L(\type\()_8tap_h_\isa\()_tbl) - 40b)
+jumptable \type\()_8tap_h_\isa\()_tbl
+        .word 1280b - \type\()_8tap_h_\isa\()_tbl
+        .word 640b  - \type\()_8tap_h_\isa\()_tbl
+        .word 320b  - \type\()_8tap_h_\isa\()_tbl
+        .word 160b  - \type\()_8tap_h_\isa\()_tbl
+        .word 80b   - \type\()_8tap_h_\isa\()_tbl
+        .word 40b   - \type\()_8tap_h_\isa\()_tbl
 .ifc \type, put
-        .hword (L(\type\()_8tap_h_\isa\()_tbl) - 20b)
-        .hword 0
+        .word 20b   - \type\()_8tap_h_\isa\()_tbl
 .endif
-endfunc
+endjumptable
 .endm
 
 // dst(x0), d_strd(x7), src(x1), s_strd(x2), w(w3), h(w4), mx(w5), my(w6)
diff --git a/src/arm/64/refmvs.S b/src/arm/64/refmvs.S
index e905682..c75c478 100644
--- a/src/arm/64/refmvs.S
+++ b/src/arm/64/refmvs.S
@@ -34,13 +34,13 @@
 function splat_mv_neon, export=1
         ld1             {v3.16b},  [x1]
         clz             w3,  w3
-        adr             x5,  L(splat_tbl)
+        movrel          x5,  splat_tbl
         sub             w3,  w3,  #26
         ext             v2.16b,  v3.16b,  v3.16b,  #12
-        ldrh            w3,  [x5, w3, uxtw #1]
+        ldrsw           x3,  [x5, w3, uxtw #2]
         add             w2,  w2,  w2,  lsl #1
         ext             v0.16b,  v2.16b,  v3.16b,  #4
-        sub             x3,  x5,  w3, uxtw
+        add             x3,  x5,  x3
         ext             v1.16b,  v2.16b,  v3.16b,  #8
         lsl             w2,  w2,  #2
         ext             v2.16b,  v2.16b,  v3.16b,  #12
@@ -80,16 +80,17 @@ function splat_mv_neon, export=1
         st1             {v0.16b, v1.16b, v2.16b}, [x1]
         b.gt            1b
         ret
-
-L(splat_tbl):
-        .hword L(splat_tbl) -  320b
-        .hword L(splat_tbl) -  160b
-        .hword L(splat_tbl) -   80b
-        .hword L(splat_tbl) -   40b
-        .hword L(splat_tbl) -   20b
-        .hword L(splat_tbl) -   10b
 endfunc
 
+jumptable splat_tbl
+        .word 320b  - splat_tbl
+        .word 160b  - splat_tbl
+        .word 80b   - splat_tbl
+        .word 40b   - splat_tbl
+        .word 20b   - splat_tbl
+        .word 10b   - splat_tbl
+endjumptable
+
 const mv_tbls, align=4
         .byte           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255
         .byte           0, 1, 2, 3, 8, 0, 1, 2, 3, 8, 0, 1, 2, 3, 8, 0
@@ -112,7 +113,7 @@ function save_tmvs_neon, export=1
 
         movi            v30.8b,  #0
         ld1             {v31.8b}, [x3]
-        adr             x8,  L(save_tmvs_tbl)
+        movrel          x8,  save_tmvs_tbl
         movrel          x16, mask_mult
         movrel          x13, mv_tbls
         ld1             {v29.8b}, [x16]
@@ -137,9 +138,9 @@ function save_tmvs_neon, export=1
 2:
         ldrb            w11, [x9, #10]            // cand_b->bs
         ld1             {v0.16b}, [x9]            // cand_b->mv
-        add             x11, x8,  w11, uxtw #2
+        add             x11, x8,  w11, uxtw #3
         ldr             h1,  [x9, #8]             // cand_b->ref
-        ldrh            w12, [x11]                // bw8
+        ldr             w12, [x11]                // bw8
         mov             x15, x8
         add             x9,  x9,  w12, uxtw #1    // cand_b += bw8*2
         cmp             x9,  x10
@@ -149,9 +150,9 @@ function save_tmvs_neon, export=1
         ldrb            w15, [x9, #10]            // cand_b->bs
         add             x16, x9,  #8
         ld1             {v4.16b}, [x9]            // cand_b->mv
-        add             x15, x8,  w15, uxtw #2
+        add             x15, x8,  w15, uxtw #3
         ld1             {v1.h}[1], [x16]          // cand_b->ref
-        ldrh            w12, [x15]                // bw8
+        ldr             w12, [x15]                // bw8
         add             x9,  x9,  w12, uxtw #1    // cand_b += bw8*2
         trn1            v2.2d,   v0.2d,   v4.2d
 
@@ -166,12 +167,12 @@ function save_tmvs_neon, export=1
         addp            v1.4h,   v1.4h,   v1.4h   // Combine condition for [1] and [0]
         umov            w16, v1.h[0]              // Extract case for first block
         umov            w17, v1.h[1]
-        ldrh            w11, [x11, #2]            // Fetch jump table entry
-        ldrh            w15, [x15, #2]
+        ldrsw           x11, [x11, #4]            // Fetch jump table entry
+        ldrsw           x15, [x15, #4]
         ldr             q1, [x13, w16, uxtw #4]   // Load permutation table base on case
         ldr             q5, [x13, w17, uxtw #4]
-        sub             x11, x8,  w11, uxtw       // Find jump table target
-        sub             x15, x8,  w15, uxtw
+        add             x11, x8,  x11             // Find jump table target
+        add             x15, x8,  x15
         tbl             v0.16b, {v0.16b}, v1.16b  // Permute cand_b to output refmvs_temporal_block
         tbl             v4.16b, {v4.16b}, v5.16b
 
@@ -243,50 +244,51 @@ function save_tmvs_neon, export=1
         str             q2, [x3, #(16*5-16)]
         add             x3,  x3,  #16*5
         ret
-
-L(save_tmvs_tbl):
-        .hword 16 * 12
-        .hword L(save_tmvs_tbl) - 160b
-        .hword 16 * 12
-        .hword L(save_tmvs_tbl) - 160b
-        .hword 8 * 12
-        .hword L(save_tmvs_tbl) -  80b
-        .hword 8 * 12
-        .hword L(save_tmvs_tbl) -  80b
-        .hword 8 * 12
-        .hword L(save_tmvs_tbl) -  80b
-        .hword 8 * 12
-        .hword L(save_tmvs_tbl) -  80b
-        .hword 4 * 12
-        .hword L(save_tmvs_tbl) -  40b
-        .hword 4 * 12
-        .hword L(save_tmvs_tbl) -  40b
-        .hword 4 * 12
-        .hword L(save_tmvs_tbl) -  40b
-        .hword 4 * 12
-        .hword L(save_tmvs_tbl) -  40b
-        .hword 2 * 12
-        .hword L(save_tmvs_tbl) -  20b
-        .hword 2 * 12
-        .hword L(save_tmvs_tbl) -  20b
-        .hword 2 * 12
-        .hword L(save_tmvs_tbl) -  20b
-        .hword 2 * 12
-        .hword L(save_tmvs_tbl) -  20b
-        .hword 2 * 12
-        .hword L(save_tmvs_tbl) -  20b
-        .hword 1 * 12
-        .hword L(save_tmvs_tbl) -  10b
-        .hword 1 * 12
-        .hword L(save_tmvs_tbl) -  10b
-        .hword 1 * 12
-        .hword L(save_tmvs_tbl) -  10b
-        .hword 1 * 12
-        .hword L(save_tmvs_tbl) -  10b
-        .hword 1 * 12
-        .hword L(save_tmvs_tbl) -  10b
-        .hword 1 * 12
-        .hword L(save_tmvs_tbl) -  10b
-        .hword 1 * 12
-        .hword L(save_tmvs_tbl) -  10b
 endfunc
+
+jumptable save_tmvs_tbl
+        .word 16 * 12
+        .word 160b - save_tmvs_tbl
+        .word 16 * 12
+        .word 160b - save_tmvs_tbl
+        .word 8 * 12
+        .word 80b  - save_tmvs_tbl
+        .word 8 * 12
+        .word 80b  - save_tmvs_tbl
+        .word 8 * 12
+        .word 80b  - save_tmvs_tbl
+        .word 8 * 12
+        .word 80b  - save_tmvs_tbl
+        .word 4 * 12
+        .word 40b  - save_tmvs_tbl
+        .word 4 * 12
+        .word 40b  - save_tmvs_tbl
+        .word 4 * 12
+        .word 40b  - save_tmvs_tbl
+        .word 4 * 12
+        .word 40b  - save_tmvs_tbl
+        .word 2 * 12
+        .word 20b  - save_tmvs_tbl
+        .word 2 * 12
+        .word 20b  - save_tmvs_tbl
+        .word 2 * 12
+        .word 20b  - save_tmvs_tbl
+        .word 2 * 12
+        .word 20b  - save_tmvs_tbl
+        .word 2 * 12
+        .word 20b  - save_tmvs_tbl
+        .word 1 * 12
+        .word 10b  - save_tmvs_tbl
+        .word 1 * 12
+        .word 10b  - save_tmvs_tbl
+        .word 1 * 12
+        .word 10b  - save_tmvs_tbl
+        .word 1 * 12
+        .word 10b  - save_tmvs_tbl
+        .word 1 * 12
+        .word 10b  - save_tmvs_tbl
+        .word 1 * 12
+        .word 10b  - save_tmvs_tbl
+        .word 1 * 12
+        .word 10b  - save_tmvs_tbl
+endjumptable
diff --git a/src/arm/arm-arch.h b/src/arm/arm-arch.h
new file mode 100644
index 0000000..f00b9b2
--- /dev/null
+++ b/src/arm/arm-arch.h
@@ -0,0 +1,68 @@
+/*
+ * Copyright © 2024, VideoLAN and dav1d authors
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef ARM_ARM_ARCH_H
+#define ARM_ARM_ARCH_H
+
+/* Compatibility header to define __ARM_ARCH with older compilers */
+#ifndef __ARM_ARCH
+
+#ifdef _M_ARM
+#define __ARM_ARCH _M_ARM
+
+#elif defined(__ARM_ARCH_8A__) || defined(_M_ARM64)
+#define __ARM_ARCH 8
+
+#elif defined(__ARM_ARCH_7__) || defined(__ARM_ARCH_7A__) || \
+      defined(__ARM_ARCH_7EM__) || defined(__ARM_ARCH_7R__) || \
+      defined(__ARM_ARCH_7M__) || defined(__ARM_ARCH_7S__)
+#define __ARM_ARCH 7
+
+#elif defined(__ARM_ARCH_6__) || defined(__ARM_ARCH_6J__) || \
+      defined(__ARM_ARCH_6K__) || defined(__ARM_ARCH_6T2__) || \
+      defined(__ARM_ARCH_6Z__) || defined(__ARM_ARCH_6ZK__)
+#define __ARM_ARCH 6
+
+#elif defined(__ARM_ARCH_5__) || defined(__ARM_ARCH_5T__) || \
+      defined(__ARM_ARCH_5E__) || defined(__ARM_ARCH_5TE__)
+#define __ARM_ARCH 5
+
+#elif defined(__ARM_ARCH_4__) || defined(__ARM_ARCH_4T__)
+#define __ARM_ARCH 4
+
+#elif defined(__ARM_ARCH_3__) || defined(__ARM_ARCH_3M__)
+#define __ARM_ARCH 3
+
+#elif defined(__ARM_ARCH_2__)
+#define __ARM_ARCH 2
+
+#else
+#error Unknown ARM architecture version
+#endif
+
+#endif /* !__ARM_ARCH */
+
+#endif /* ARM_ARM_ARCH_H */
diff --git a/src/arm/asm.S b/src/arm/asm.S
index fed73b3..e3731fe 100644
--- a/src/arm/asm.S
+++ b/src/arm/asm.S
@@ -323,6 +323,32 @@ EXTERN\name:
 \name:
 .endm
 
+.macro jumptable name
+#ifdef _WIN32
+// MS armasm64 doesn't seem to be able to create relocations for subtraction
+// of labels in different sections; for armasm64 (and all of Windows for
+// simplicity), write the jump table in the text section, to allow calculating
+// differences at assembly time. See
+// https://developercommunity.visualstudio.com/t/armasm64-unable-to-create-cross-section/10722340
+// for reference. (LLVM can create such relocations, but checking for _WIN32
+// for simplicity, as execute-only memory isn't relevant on Windows at the
+// moment.)
+        function \name
+#else
+// For other platforms, write jump tables in a const data section, to allow
+// working in environments where executable memory isn't readable.
+        const \name
+#endif
+.endm
+
+.macro endjumptable
+#ifdef _WIN32
+        endfunc
+#else
+        endconst
+#endif
+.endm
+
 #ifdef __APPLE__
 #define L(x) L ## x
 #else
diff --git a/src/arm/cpu.c b/src/arm/cpu.c
index d9b1751..5275b74 100644
--- a/src/arm/cpu.c
+++ b/src/arm/cpu.c
@@ -29,9 +29,10 @@
 
 #include "common/attributes.h"
 
+#include "src/cpu.h"
 #include "src/arm/cpu.h"
 
-#if defined(HAVE_GETAUXVAL) || defined(HAVE_ELF_AUX_INFO)
+#if HAVE_GETAUXVAL || HAVE_ELF_AUX_INFO
 #include <sys/auxv.h>
 
 #if ARCH_AARCH64
@@ -42,7 +43,7 @@
 #define HWCAP2_AARCH64_I8MM   (1 << 13)
 
 COLD unsigned dav1d_get_cpu_flags_arm(void) {
-#ifdef HAVE_GETAUXVAL
+#if HAVE_GETAUXVAL
     unsigned long hw_cap = getauxval(AT_HWCAP);
     unsigned long hw_cap2 = getauxval(AT_HWCAP2);
 #else
@@ -52,7 +53,7 @@ COLD unsigned dav1d_get_cpu_flags_arm(void) {
     elf_aux_info(AT_HWCAP2, &hw_cap2, sizeof(hw_cap2));
 #endif
 
-    unsigned flags = DAV1D_ARM_CPU_FLAG_NEON;
+    unsigned flags = dav1d_get_default_cpu_flags();
     flags |= (hw_cap & HWCAP_AARCH64_ASIMDDP) ? DAV1D_ARM_CPU_FLAG_DOTPROD : 0;
     flags |= (hw_cap2 & HWCAP2_AARCH64_I8MM) ? DAV1D_ARM_CPU_FLAG_I8MM : 0;
     flags |= (hw_cap & HWCAP_AARCH64_SVE) ? DAV1D_ARM_CPU_FLAG_SVE : 0;
@@ -68,14 +69,15 @@ COLD unsigned dav1d_get_cpu_flags_arm(void) {
 #define HWCAP_ARM_I8MM    (1 << 27)
 
 COLD unsigned dav1d_get_cpu_flags_arm(void) {
-#ifdef HAVE_GETAUXVAL
+#if HAVE_GETAUXVAL
     unsigned long hw_cap = getauxval(AT_HWCAP);
 #else
     unsigned long hw_cap = 0;
     elf_aux_info(AT_HWCAP, &hw_cap, sizeof(hw_cap));
 #endif
 
-    unsigned flags = (hw_cap & HWCAP_ARM_NEON) ? DAV1D_ARM_CPU_FLAG_NEON : 0;
+    unsigned flags = dav1d_get_default_cpu_flags();
+    flags |= (hw_cap & HWCAP_ARM_NEON) ? DAV1D_ARM_CPU_FLAG_NEON : 0;
     flags |= (hw_cap & HWCAP_ARM_ASIMDDP) ? DAV1D_ARM_CPU_FLAG_DOTPROD : 0;
     flags |= (hw_cap & HWCAP_ARM_I8MM) ? DAV1D_ARM_CPU_FLAG_I8MM : 0;
     return flags;
@@ -95,7 +97,7 @@ static int have_feature(const char *feature) {
 }
 
 COLD unsigned dav1d_get_cpu_flags_arm(void) {
-    unsigned flags = DAV1D_ARM_CPU_FLAG_NEON;
+    unsigned flags = dav1d_get_default_cpu_flags();
     if (have_feature("hw.optional.arm.FEAT_DotProd"))
         flags |= DAV1D_ARM_CPU_FLAG_DOTPROD;
     if (have_feature("hw.optional.arm.FEAT_I8MM"))
@@ -104,17 +106,67 @@ COLD unsigned dav1d_get_cpu_flags_arm(void) {
     return flags;
 }
 
+#elif defined(__OpenBSD__) && ARCH_AARCH64
+#include <machine/armreg.h>
+#include <machine/cpu.h>
+#include <sys/types.h>
+#include <sys/sysctl.h>
+
+COLD unsigned dav1d_get_cpu_flags_arm(void) {
+     unsigned flags = dav1d_get_default_cpu_flags();
+
+#ifdef CPU_ID_AA64ISAR0
+     int mib[2];
+     uint64_t isar0;
+     uint64_t isar1;
+     size_t len;
+
+     mib[0] = CTL_MACHDEP;
+     mib[1] = CPU_ID_AA64ISAR0;
+     len = sizeof(isar0);
+     if (sysctl(mib, 2, &isar0, &len, NULL, 0) != -1) {
+         if (ID_AA64ISAR0_DP(isar0) >= ID_AA64ISAR0_DP_IMPL)
+             flags |= DAV1D_ARM_CPU_FLAG_DOTPROD;
+     }
+
+     mib[0] = CTL_MACHDEP;
+     mib[1] = CPU_ID_AA64ISAR1;
+     len = sizeof(isar1);
+     if (sysctl(mib, 2, &isar1, &len, NULL, 0) != -1) {
+#ifdef ID_AA64ISAR1_I8MM_IMPL
+         if (ID_AA64ISAR1_I8MM(isar1) >= ID_AA64ISAR1_I8MM_IMPL)
+             flags |= DAV1D_ARM_CPU_FLAG_I8MM;
+#endif
+     }
+#endif
+
+     return flags;
+}
+
 #elif defined(_WIN32)
 #include <windows.h>
 
 COLD unsigned dav1d_get_cpu_flags_arm(void) {
-    unsigned flags = DAV1D_ARM_CPU_FLAG_NEON;
+    unsigned flags = dav1d_get_default_cpu_flags();
 #ifdef PF_ARM_V82_DP_INSTRUCTIONS_AVAILABLE
     if (IsProcessorFeaturePresent(PF_ARM_V82_DP_INSTRUCTIONS_AVAILABLE))
         flags |= DAV1D_ARM_CPU_FLAG_DOTPROD;
 #endif
-    /* No I8MM or SVE feature detection available on Windows at the time of
-     * writing. */
+#ifdef PF_ARM_SVE_INSTRUCTIONS_AVAILABLE
+    if (IsProcessorFeaturePresent(PF_ARM_SVE_INSTRUCTIONS_AVAILABLE))
+        flags |= DAV1D_ARM_CPU_FLAG_SVE;
+#endif
+#ifdef PF_ARM_SVE2_INSTRUCTIONS_AVAILABLE
+    if (IsProcessorFeaturePresent(PF_ARM_SVE2_INSTRUCTIONS_AVAILABLE))
+        flags |= DAV1D_ARM_CPU_FLAG_SVE2;
+#endif
+#ifdef PF_ARM_SVE_I8MM_INSTRUCTIONS_AVAILABLE
+    /* There's no PF_* flag that indicates whether plain I8MM is available
+     * or not. But if SVE_I8MM is available, that also implies that
+     * regular I8MM is available. */
+    if (IsProcessorFeaturePresent(PF_ARM_SVE_I8MM_INSTRUCTIONS_AVAILABLE))
+        flags |= DAV1D_ARM_CPU_FLAG_I8MM;
+#endif
     return flags;
 }
 
@@ -160,7 +212,8 @@ static unsigned parse_proc_cpuinfo(const char *flag) {
 }
 
 COLD unsigned dav1d_get_cpu_flags_arm(void) {
-    unsigned flags = parse_proc_cpuinfo("neon") ? DAV1D_ARM_CPU_FLAG_NEON : 0;
+    unsigned flags = dav1d_get_default_cpu_flags();
+    flags |= parse_proc_cpuinfo("neon") ? DAV1D_ARM_CPU_FLAG_NEON : 0;
     flags |= parse_proc_cpuinfo("asimd") ? DAV1D_ARM_CPU_FLAG_NEON : 0;
     flags |= parse_proc_cpuinfo("asimddp") ? DAV1D_ARM_CPU_FLAG_DOTPROD : 0;
     flags |= parse_proc_cpuinfo("i8mm") ? DAV1D_ARM_CPU_FLAG_I8MM : 0;
@@ -174,7 +227,7 @@ COLD unsigned dav1d_get_cpu_flags_arm(void) {
 #else  /* Unsupported OS */
 
 COLD unsigned dav1d_get_cpu_flags_arm(void) {
-    return 0;
+    return dav1d_get_default_cpu_flags();
 }
 
 #endif
diff --git a/src/arm/itx.h b/src/arm/itx.h
index 2a58a31..657f85e 100644
--- a/src/arm/itx.h
+++ b/src/arm/itx.h
@@ -49,7 +49,9 @@ decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_64x16, neon));
 decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_64x32, neon));
 decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_64x64, neon));
 
-static ALWAYS_INLINE void itx_dsp_init_arm(Dav1dInvTxfmDSPContext *const c, int bpc) {
+static ALWAYS_INLINE void itx_dsp_init_arm(Dav1dInvTxfmDSPContext *const c, int bpc,
+                                           int *const all_simd)
+{
     const unsigned flags = dav1d_get_cpu_flags();
 
     if (!(flags & DAV1D_ARM_CPU_FLAG_NEON)) return;
@@ -77,4 +79,5 @@ static ALWAYS_INLINE void itx_dsp_init_arm(Dav1dInvTxfmDSPContext *const c, int
     assign_itx1_fn (R, 64, 16, neon);
     assign_itx1_fn (R, 64, 32, neon);
     assign_itx1_fn ( , 64, 64, neon);
+    *all_simd = 1;
 }
diff --git a/src/arm/mc.h b/src/arm/mc.h
index dabdab3..bb6f7f8 100644
--- a/src/arm/mc.h
+++ b/src/arm/mc.h
@@ -63,6 +63,7 @@
 decl_8tap_fns(neon);
 decl_8tap_fns(neon_dotprod);
 decl_8tap_fns(neon_i8mm);
+decl_8tap_fns(sve2);
 
 decl_mc_fn(BF(dav1d_put_bilin, neon));
 decl_mct_fn(BF(dav1d_prep_bilin, neon));
@@ -110,17 +111,27 @@ static ALWAYS_INLINE void mc_dsp_init_arm(Dav1dMCDSPContext *const c) {
     c->warp8x8t = BF(dav1d_warp_affine_8x8t, neon);
     c->emu_edge = BF(dav1d_emu_edge, neon);
 
-#if ARCH_AARCH64 && BITDEPTH == 8
+#if ARCH_AARCH64
+#if BITDEPTH == 8
 #if HAVE_DOTPROD
-    if (!(flags & DAV1D_ARM_CPU_FLAG_DOTPROD)) return;
-
-    init_8tap_fns(neon_dotprod);
+    if (flags & DAV1D_ARM_CPU_FLAG_DOTPROD) {
+        init_8tap_fns(neon_dotprod);
+    }
 #endif  // HAVE_DOTPROD
 
 #if HAVE_I8MM
-    if (!(flags & DAV1D_ARM_CPU_FLAG_I8MM)) return;
-
-    init_8tap_fns(neon_i8mm);
+    if (flags & DAV1D_ARM_CPU_FLAG_I8MM) {
+        init_8tap_fns(neon_i8mm);
+    }
 #endif  // HAVE_I8MM
-#endif  // ARCH_AARCH64 && BITDEPTH == 8
+#endif  // BITDEPTH == 8
+
+#if BITDEPTH == 16
+#if HAVE_SVE2
+    if (flags & DAV1D_ARM_CPU_FLAG_SVE2) {
+        init_8tap_fns(sve2);
+    }
+#endif  // HAVE_SVE2
+#endif  // BITDEPTH == 16
+#endif  // ARCH_AARCH64
 }
diff --git a/src/cdef_tmpl.c b/src/cdef_tmpl.c
index 5943945..4efea45 100644
--- a/src/cdef_tmpl.c
+++ b/src/cdef_tmpl.c
@@ -308,8 +308,12 @@ static int cdef_find_dir_c(const pixel *img, const ptrdiff_t stride,
 #include "src/arm/cdef.h"
 #elif ARCH_PPC64LE
 #include "src/ppc/cdef.h"
+#elif ARCH_RISCV
+#include "src/riscv/cdef.h"
 #elif ARCH_X86
 #include "src/x86/cdef.h"
+#elif ARCH_LOONGARCH64
+#include "src/loongarch/cdef.h"
 #endif
 #endif
 
@@ -324,8 +328,12 @@ COLD void bitfn(dav1d_cdef_dsp_init)(Dav1dCdefDSPContext *const c) {
     cdef_dsp_init_arm(c);
 #elif ARCH_PPC64LE
     cdef_dsp_init_ppc(c);
+#elif ARCH_RISCV
+    cdef_dsp_init_riscv(c);
 #elif ARCH_X86
     cdef_dsp_init_x86(c);
+#elif ARCH_LOONGARCH64
+    cdef_dsp_init_loongarch(c);
 #endif
 #endif
 }
diff --git a/src/cpu.c b/src/cpu.c
index 9bb85f1..4152667 100644
--- a/src/cpu.c
+++ b/src/cpu.c
@@ -33,20 +33,24 @@
 
 #ifdef _WIN32
 #include <windows.h>
-#elif defined(__APPLE__)
+#endif
+#ifdef __APPLE__
 #include <sys/sysctl.h>
 #include <sys/types.h>
-#else
-#include <pthread.h>
+#endif
+#if HAVE_UNISTD_H
 #include <unistd.h>
 #endif
 
-#ifdef HAVE_PTHREAD_NP_H
+#if HAVE_PTHREAD_GETAFFINITY_NP
+#include <pthread.h>
+#if HAVE_PTHREAD_NP_H
 #include <pthread_np.h>
 #endif
 #if defined(__FreeBSD__)
 #define cpu_set_t cpuset_t
 #endif
+#endif
 
 unsigned dav1d_cpu_flags = 0U;
 unsigned dav1d_cpu_flags_mask = ~0U;
@@ -87,7 +91,7 @@ COLD int dav1d_num_logical_processors(Dav1dContext *const c) {
     GetNativeSystemInfo(&system_info);
     return system_info.dwNumberOfProcessors;
 #endif
-#elif defined(HAVE_PTHREAD_GETAFFINITY_NP) && defined(CPU_COUNT)
+#elif HAVE_PTHREAD_GETAFFINITY_NP && defined(CPU_COUNT)
     cpu_set_t affinity;
     if (!pthread_getaffinity_np(pthread_self(), sizeof(affinity), &affinity))
         return CPU_COUNT(&affinity);
diff --git a/src/cpu.h b/src/cpu.h
index 7205e8e..c18b7ff 100644
--- a/src/cpu.h
+++ b/src/cpu.h
@@ -54,12 +54,9 @@ void dav1d_init_cpu(void);
 DAV1D_API void dav1d_set_cpu_flags_mask(unsigned mask);
 int dav1d_num_logical_processors(Dav1dContext *c);
 
-static ALWAYS_INLINE unsigned dav1d_get_cpu_flags(void) {
-    unsigned flags = dav1d_cpu_flags & dav1d_cpu_flags_mask;
+static ALWAYS_INLINE unsigned dav1d_get_default_cpu_flags(void) {
+    unsigned flags = 0;
 
-#if TRIM_DSP_FUNCTIONS
-/* Since this function is inlined, unconditionally setting a flag here will
- * enable dead code elimination in the calling function. */
 #if ARCH_AARCH64 || ARCH_ARM
 #if defined(__ARM_NEON) || defined(__APPLE__) || defined(_WIN32) || ARCH_AARCH64
     flags |= DAV1D_ARM_CPU_FLAG_NEON;
@@ -119,6 +116,17 @@ static ALWAYS_INLINE unsigned dav1d_get_cpu_flags(void) {
     flags |= DAV1D_X86_CPU_FLAG_SSE2;
 #endif
 #endif
+
+    return flags;
+}
+
+static ALWAYS_INLINE unsigned dav1d_get_cpu_flags(void) {
+    unsigned flags = dav1d_cpu_flags & dav1d_cpu_flags_mask;
+
+#if TRIM_DSP_FUNCTIONS
+/* Since this function is inlined, unconditionally setting a flag here will
+ * enable dead code elimination in the calling function. */
+    flags |= dav1d_get_default_cpu_flags();
 #endif
 
     return flags;
diff --git a/src/ctx.c b/src/ctx.c
new file mode 100644
index 0000000..0a0fe54
--- /dev/null
+++ b/src/ctx.c
@@ -0,0 +1,65 @@
+/*
+ * Copyright © 2024, VideoLAN and dav1d authors
+ * Copyright © 2024, Two Orioles, LLC
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "config.h"
+
+#include <string.h>
+
+#include "ctx.h"
+
+static void memset_w1(void *const ptr, const int value) {
+    set_ctx1((uint8_t *) ptr, 0, value);
+}
+
+static void memset_w2(void *const ptr, const int value) {
+    set_ctx2((uint8_t *) ptr, 0, value);
+}
+
+static void memset_w4(void *const ptr, const int value) {
+    set_ctx4((uint8_t *) ptr, 0, value);
+}
+
+static void memset_w8(void *const ptr, const int value) {
+    set_ctx8((uint8_t *) ptr, 0, value);
+}
+
+static void memset_w16(void *const ptr, const int value) {
+    set_ctx16((uint8_t *) ptr, 0, value);
+}
+
+static void memset_w32(void *const ptr, const int value) {
+    set_ctx32((uint8_t *) ptr, 0, value);
+}
+
+const dav1d_memset_pow2_fn dav1d_memset_pow2[6] = {
+    memset_w1,
+    memset_w2,
+    memset_w4,
+    memset_w8,
+    memset_w16,
+    memset_w32
+};
diff --git a/src/ctx.h b/src/ctx.h
index d0e1f31..7dea8b6 100644
--- a/src/ctx.h
+++ b/src/ctx.h
@@ -31,61 +31,59 @@
 #include <stdint.h>
 
 #include "common/attributes.h"
+#include "common/intops.h"
 
 union alias64 { uint64_t u64; uint8_t u8[8]; } ATTR_ALIAS;
 union alias32 { uint32_t u32; uint8_t u8[4]; } ATTR_ALIAS;
 union alias16 { uint16_t u16; uint8_t u8[2]; } ATTR_ALIAS;
 union alias8 { uint8_t u8; } ATTR_ALIAS;
 
-#define set_ctx_rep4(type, var, off, val) do { \
-        const uint64_t const_val = val; \
-        ((union alias64 *) &var[off +  0])->u64 = const_val; \
-        ((union alias64 *) &var[off +  8])->u64 = const_val; \
-        ((union alias64 *) &var[off + 16])->u64 = const_val; \
-        ((union alias64 *) &var[off + 24])->u64 = const_val; \
+typedef void (*dav1d_memset_pow2_fn)(void *ptr, int value);
+EXTERN const dav1d_memset_pow2_fn dav1d_memset_pow2[6];
+
+static inline void dav1d_memset_likely_pow2(void *const ptr, const int value, const int n) {
+    assert(n >= 1 && n <= 32);
+    if ((n&(n-1)) == 0) {
+        dav1d_memset_pow2[ulog2(n)](ptr, value);
+    } else {
+        memset(ptr, value, n);
+    }
+}
+
+// For smaller sizes use multiplication to broadcast bytes. memset misbehaves on the smaller sizes.
+// For the larger sizes, we want to use memset to get access to vector operations.
+#define set_ctx1(var, off, val) \
+    ((union alias8 *) &(var)[off])->u8 = (val) * 0x01
+#define set_ctx2(var, off, val) \
+    ((union alias16 *) &(var)[off])->u16 = (val) * 0x0101
+#define set_ctx4(var, off, val) \
+    ((union alias32 *) &(var)[off])->u32 = (val) * 0x01010101U
+#define set_ctx8(var, off, val) \
+    ((union alias64 *) &(var)[off])->u64 = (val) * 0x0101010101010101ULL
+#define set_ctx16(var, off, val) do { \
+        memset(&(var)[off], val, 16); \
     } while (0)
-#define set_ctx_rep2(type, var, off, val) do { \
-        const uint64_t const_val = val; \
-        ((union alias64 *) &var[off + 0])->u64 = const_val; \
-        ((union alias64 *) &var[off + 8])->u64 = const_val; \
+#define set_ctx32(var, off, val) do { \
+        memset(&(var)[off], val, 32); \
     } while (0)
-#define set_ctx_rep1(typesz, var, off, val) \
-    ((union alias##typesz *) &var[off])->u##typesz = val
-#define case_set(var, dir, diridx, off) \
-    switch (var) { \
-    case  1: set_ctx( 8, dir, diridx, off, 0x01, set_ctx_rep1); break; \
-    case  2: set_ctx(16, dir, diridx, off, 0x0101, set_ctx_rep1); break; \
-    case  4: set_ctx(32, dir, diridx, off, 0x01010101U, set_ctx_rep1); break; \
-    case  8: set_ctx(64, dir, diridx, off, 0x0101010101010101ULL, set_ctx_rep1); break; \
-    case 16: set_ctx(  , dir, diridx, off, 0x0101010101010101ULL, set_ctx_rep2); break; \
-    case 32: set_ctx(  , dir, diridx, off, 0x0101010101010101ULL, set_ctx_rep4); break; \
-    }
-#define case_set_upto16(var, dir, diridx, off) \
-    switch (var) { \
-    case  1: set_ctx( 8, dir, diridx, off, 0x01, set_ctx_rep1); break; \
-    case  2: set_ctx(16, dir, diridx, off, 0x0101, set_ctx_rep1); break; \
-    case  4: set_ctx(32, dir, diridx, off, 0x01010101U, set_ctx_rep1); break; \
-    case  8: set_ctx(64, dir, diridx, off, 0x0101010101010101ULL, set_ctx_rep1); break; \
-    case 16: set_ctx(  , dir, diridx, off, 0x0101010101010101ULL, set_ctx_rep2); break; \
-    }
-#define case_set_upto32_with_default(var, dir, diridx, off) \
+#define case_set(var) \
     switch (var) { \
-    case  1: set_ctx( 8, dir, diridx, off, 0x01, set_ctx_rep1); break; \
-    case  2: set_ctx(16, dir, diridx, off, 0x0101, set_ctx_rep1); break; \
-    case  4: set_ctx(32, dir, diridx, off, 0x01010101U, set_ctx_rep1); break; \
-    case  8: set_ctx(64, dir, diridx, off, 0x0101010101010101ULL, set_ctx_rep1); break; \
-    case 16: set_ctx(  , dir, diridx, off, 0x0101010101010101ULL, set_ctx_rep2); break; \
-    case 32: set_ctx(  , dir, diridx, off, 0x0101010101010101ULL, set_ctx_rep4); break; \
-    default: default_memset(dir, diridx, off, var); break; \
+    case 0: set_ctx(set_ctx1); break; \
+    case 1: set_ctx(set_ctx2); break; \
+    case 2: set_ctx(set_ctx4); break; \
+    case 3: set_ctx(set_ctx8); break; \
+    case 4: set_ctx(set_ctx16); break; \
+    case 5: set_ctx(set_ctx32); break; \
+    default: assert(0); \
     }
-#define case_set_upto16_with_default(var, dir, diridx, off) \
+#define case_set_upto16(var) \
     switch (var) { \
-    case  1: set_ctx( 8, dir, diridx, off, 0x01, set_ctx_rep1); break; \
-    case  2: set_ctx(16, dir, diridx, off, 0x0101, set_ctx_rep1); break; \
-    case  4: set_ctx(32, dir, diridx, off, 0x01010101U, set_ctx_rep1); break; \
-    case  8: set_ctx(64, dir, diridx, off, 0x0101010101010101ULL, set_ctx_rep1); break; \
-    case 16: set_ctx(  , dir, diridx, off, 0x0101010101010101ULL, set_ctx_rep2); break; \
-    default: default_memset(dir, diridx, off, var); break; \
+    case 0: set_ctx(set_ctx1); break; \
+    case 1: set_ctx(set_ctx2); break; \
+    case 2: set_ctx(set_ctx4); break; \
+    case 3: set_ctx(set_ctx8); break; \
+    case 4: set_ctx(set_ctx16); break; \
+    default: assert(0); \
     }
 
 #endif /* DAV1D_SRC_CTX_H */
diff --git a/src/decode.c b/src/decode.c
index ea37132..f5b6db9 100644
--- a/src/decode.c
+++ b/src/decode.c
@@ -161,14 +161,8 @@ static void read_tx_tree(Dav1dTaskContext *const t,
         }
         t->by -= txsh;
     } else {
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-        rep_macro(type, t->dir tx, off, is_split ? TX_4X4 : mul * txh)
-        case_set_upto16(t_dim->h, l., 1, by4);
-#undef set_ctx
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-        rep_macro(type, t->dir tx, off, is_split ? TX_4X4 : mul * txw)
-        case_set_upto16(t_dim->w, a->, 0, bx4);
-#undef set_ctx
+        dav1d_memset_pow2[t_dim->lw](&t->a->tx[bx4], is_split ? TX_4X4 : txw);
+        dav1d_memset_pow2[t_dim->lh](&t->l.tx[by4], is_split ? TX_4X4 : txh);
     }
 }
 
@@ -464,19 +458,13 @@ static void read_vartx_tree(Dav1dTaskContext *const t,
     {
         b->max_ytx = b->uvtx = TX_4X4;
         if (f->frame_hdr->txfm_mode == DAV1D_TX_SWITCHABLE) {
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-            rep_macro(type, t->dir tx, off, TX_4X4)
-            case_set(bh4, l., 1, by4);
-            case_set(bw4, a->, 0, bx4);
-#undef set_ctx
+            dav1d_memset_pow2[b_dim[2]](&t->a->tx[bx4], TX_4X4);
+            dav1d_memset_pow2[b_dim[3]](&t->l.tx[by4], TX_4X4);
         }
     } else if (f->frame_hdr->txfm_mode != DAV1D_TX_SWITCHABLE || b->skip) {
         if (f->frame_hdr->txfm_mode == DAV1D_TX_SWITCHABLE) {
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-            rep_macro(type, t->dir tx, off, mul * b_dim[2 + diridx])
-            case_set(bh4, l., 1, by4);
-            case_set(bw4, a->, 0, bx4);
-#undef set_ctx
+            dav1d_memset_pow2[b_dim[2]](&t->a->tx[bx4], b_dim[2 + 0]);
+            dav1d_memset_pow2[b_dim[3]](&t->l.tx[by4], b_dim[2 + 1]);
         }
         b->uvtx = dav1d_max_txfm_size_for_bs[bs][f->cur.p.layout];
     } else {
@@ -696,8 +684,7 @@ static int decode_b(Dav1dTaskContext *const t,
                     const enum BlockLevel bl,
                     const enum BlockSize bs,
                     const enum BlockPartition bp,
-                    const enum EdgeFlags intra_edge_flags)
-{
+                    const enum EdgeFlags intra_edge_flags) {
     Dav1dTileState *const ts = t->ts;
     const Dav1dFrameContext *const f = t->f;
     Av1Block b_mem, *const b = t->frame_thread.pass ?
@@ -722,11 +709,13 @@ static int decode_b(Dav1dTaskContext *const t,
 
             const enum IntraPredMode y_mode_nofilt =
                 b->y_mode == FILTER_PRED ? DC_PRED : b->y_mode;
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-            rep_macro(type, t->dir mode, off, mul * y_mode_nofilt); \
-            rep_macro(type, t->dir intra, off, mul)
-            case_set(bh4, l., 1, by4);
-            case_set(bw4, a->, 0, bx4);
+#define set_ctx(rep_macro) \
+            rep_macro(edge->mode, off, y_mode_nofilt); \
+            rep_macro(edge->intra, off, 1)
+            BlockContext *edge = t->a;
+            for (int i = 0, off = bx4; i < 2; i++, off = by4, edge = &t->l) {
+                case_set(b_dim[2 + i]);
+            }
 #undef set_ctx
             if (IS_INTER_OR_SWITCH(f->frame_hdr)) {
                 refmvs_block *const r = &t->rt.r[(t->by & 31) + 5 + bh4 - 1][t->bx];
@@ -742,11 +731,9 @@ static int decode_b(Dav1dTaskContext *const t,
             }
 
             if (has_chroma) {
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-                rep_macro(type, t->dir uvmode, off, mul * b->uv_mode)
-                case_set(cbh4, l., 1, cby4);
-                case_set(cbw4, a->, 0, cbx4);
-#undef set_ctx
+                uint8_t uv_mode = b->uv_mode;
+                dav1d_memset_pow2[ulog2(cbw4)](&t->a->uvmode[cbx4], uv_mode);
+                dav1d_memset_pow2[ulog2(cbh4)](&t->l.uvmode[cby4], uv_mode);
             }
         } else {
             if (IS_INTER_OR_SWITCH(f->frame_hdr) /* not intrabc */ &&
@@ -784,13 +771,15 @@ static int decode_b(Dav1dTaskContext *const t,
             if (f->bd_fn.recon_b_inter(t, bs, b)) return -1;
 
             const uint8_t *const filter = dav1d_filter_dir[b->filter2d];
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-            rep_macro(type, t->dir filter[0], off, mul * filter[0]); \
-            rep_macro(type, t->dir filter[1], off, mul * filter[1]); \
-            rep_macro(type, t->dir intra, off, 0)
-            case_set(bh4, l., 1, by4);
-            case_set(bw4, a->, 0, bx4);
+            BlockContext *edge = t->a;
+            for (int i = 0, off = bx4; i < 2; i++, off = by4, edge = &t->l) {
+#define set_ctx(rep_macro) \
+                rep_macro(edge->filter[0], off, filter[0]); \
+                rep_macro(edge->filter[1], off, filter[1]); \
+                rep_macro(edge->intra, off, 0)
+                case_set(b_dim[2 + i]);
 #undef set_ctx
+            }
 
             if (IS_INTER_OR_SWITCH(f->frame_hdr)) {
                 refmvs_block *const r = &t->rt.r[(t->by & 31) + 5 + bh4 - 1][t->bx];
@@ -808,11 +797,8 @@ static int decode_b(Dav1dTaskContext *const t,
             }
 
             if (has_chroma) {
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-                rep_macro(type, t->dir uvmode, off, mul * DC_PRED)
-                case_set(cbh4, l., 1, cby4);
-                case_set(cbw4, a->, 0, cbx4);
-#undef set_ctx
+                dav1d_memset_pow2[ulog2(cbw4)](&t->a->uvmode[cbx4], DC_PRED);
+                dav1d_memset_pow2[ulog2(cbh4)](&t->l.uvmode[cby4], DC_PRED);
             }
         }
         return 0;
@@ -1240,39 +1226,39 @@ static int decode_b(Dav1dTaskContext *const t,
                                        has_chroma ? &t->a->tx_lpf_uv[cbx4] : NULL,
                                        has_chroma ? &t->l.tx_lpf_uv[cby4] : NULL);
         }
-
         // update contexts
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-        rep_macro(type, t->dir tx_intra, off, mul * (((uint8_t *) &t_dim->lw)[diridx])); \
-        rep_macro(type, t->dir tx, off, mul * (((uint8_t *) &t_dim->lw)[diridx])); \
-        rep_macro(type, t->dir mode, off, mul * y_mode_nofilt); \
-        rep_macro(type, t->dir pal_sz, off, mul * b->pal_sz[0]); \
-        rep_macro(type, t->dir seg_pred, off, mul * seg_pred); \
-        rep_macro(type, t->dir skip_mode, off, 0); \
-        rep_macro(type, t->dir intra, off, mul); \
-        rep_macro(type, t->dir skip, off, mul * b->skip); \
-        /* see aomedia bug 2183 for why we use luma coordinates here */ \
-        rep_macro(type, t->pal_sz_uv[diridx], off, mul * (has_chroma ? b->pal_sz[1] : 0)); \
-        if (IS_INTER_OR_SWITCH(f->frame_hdr)) { \
-            rep_macro(type, t->dir comp_type, off, mul * COMP_INTER_NONE); \
-            rep_macro(type, t->dir ref[0], off, mul * ((uint8_t) -1)); \
-            rep_macro(type, t->dir ref[1], off, mul * ((uint8_t) -1)); \
-            rep_macro(type, t->dir filter[0], off, mul * DAV1D_N_SWITCHABLE_FILTERS); \
-            rep_macro(type, t->dir filter[1], off, mul * DAV1D_N_SWITCHABLE_FILTERS); \
-        }
         const enum IntraPredMode y_mode_nofilt =
             b->y_mode == FILTER_PRED ? DC_PRED : b->y_mode;
-        case_set(bh4, l., 1, by4);
-        case_set(bw4, a->, 0, bx4);
+        BlockContext *edge = t->a;
+        for (int i = 0, off = bx4; i < 2; i++, off = by4, edge = &t->l) {
+            int t_lsz = ((uint8_t *) &t_dim->lw)[i]; // lw then lh
+#define set_ctx(rep_macro) \
+            rep_macro(edge->tx_intra, off, t_lsz); \
+            rep_macro(edge->tx, off, t_lsz); \
+            rep_macro(edge->mode, off, y_mode_nofilt); \
+            rep_macro(edge->pal_sz, off, b->pal_sz[0]); \
+            rep_macro(edge->seg_pred, off, seg_pred); \
+            rep_macro(edge->skip_mode, off, 0); \
+            rep_macro(edge->intra, off, 1); \
+            rep_macro(edge->skip, off, b->skip); \
+            /* see aomedia bug 2183 for why we use luma coordinates here */ \
+            rep_macro(t->pal_sz_uv[i], off, (has_chroma ? b->pal_sz[1] : 0)); \
+            if (IS_INTER_OR_SWITCH(f->frame_hdr)) { \
+                rep_macro(edge->comp_type, off, COMP_INTER_NONE); \
+                rep_macro(edge->ref[0], off, ((uint8_t) -1)); \
+                rep_macro(edge->ref[1], off, ((uint8_t) -1)); \
+                rep_macro(edge->filter[0], off, DAV1D_N_SWITCHABLE_FILTERS); \
+                rep_macro(edge->filter[1], off, DAV1D_N_SWITCHABLE_FILTERS); \
+            }
+            case_set(b_dim[2 + i]);
 #undef set_ctx
+        }
         if (b->pal_sz[0])
             f->bd_fn.copy_pal_block_y(t, bx4, by4, bw4, bh4);
         if (has_chroma) {
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-                rep_macro(type, t->dir uvmode, off, mul * b->uv_mode)
-                case_set(cbh4, l., 1, cby4);
-                case_set(cbw4, a->, 0, cbx4);
-#undef set_ctx
+            uint8_t uv_mode = b->uv_mode;
+            dav1d_memset_pow2[ulog2(cbw4)](&t->a->uvmode[cbx4], uv_mode);
+            dav1d_memset_pow2[ulog2(cbh4)](&t->l.uvmode[cby4], uv_mode);
             if (b->pal_sz[1])
                 f->bd_fn.copy_pal_block_uv(t, bx4, by4, bw4, bh4);
         }
@@ -1374,26 +1360,24 @@ static int decode_b(Dav1dTaskContext *const t,
         }
 
         splat_intrabc_mv(f->c, t, bs, b, bw4, bh4);
-
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-        rep_macro(type, t->dir tx_intra, off, mul * b_dim[2 + diridx]); \
-        rep_macro(type, t->dir mode, off, mul * DC_PRED); \
-        rep_macro(type, t->dir pal_sz, off, 0); \
-        /* see aomedia bug 2183 for why this is outside if (has_chroma) */ \
-        rep_macro(type, t->pal_sz_uv[diridx], off, 0); \
-        rep_macro(type, t->dir seg_pred, off, mul * seg_pred); \
-        rep_macro(type, t->dir skip_mode, off, 0); \
-        rep_macro(type, t->dir intra, off, 0); \
-        rep_macro(type, t->dir skip, off, mul * b->skip)
-        case_set(bh4, l., 1, by4);
-        case_set(bw4, a->, 0, bx4);
+        BlockContext *edge = t->a;
+        for (int i = 0, off = bx4; i < 2; i++, off = by4, edge = &t->l) {
+#define set_ctx(rep_macro) \
+            rep_macro(edge->tx_intra, off, b_dim[2 + i]); \
+            rep_macro(edge->mode, off, DC_PRED); \
+            rep_macro(edge->pal_sz, off, 0); \
+            /* see aomedia bug 2183 for why this is outside if (has_chroma) */ \
+            rep_macro(t->pal_sz_uv[i], off, 0); \
+            rep_macro(edge->seg_pred, off, seg_pred); \
+            rep_macro(edge->skip_mode, off, 0); \
+            rep_macro(edge->intra, off, 0); \
+            rep_macro(edge->skip, off, b->skip)
+            case_set(b_dim[2 + i]);
 #undef set_ctx
+        }
         if (has_chroma) {
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-            rep_macro(type, t->dir uvmode, off, mul * DC_PRED)
-            case_set(cbh4, l., 1, cby4);
-            case_set(cbw4, a->, 0, cbx4);
-#undef set_ctx
+            dav1d_memset_pow2[ulog2(cbw4)](&t->a->uvmode[cbx4], DC_PRED);
+            dav1d_memset_pow2[ulog2(cbh4)](&t->l.uvmode[cby4], DC_PRED);
         }
     } else {
         // inter-specific mode/mv coding
@@ -1922,32 +1906,29 @@ static int decode_b(Dav1dTaskContext *const t,
             splat_tworef_mv(f->c, t, bs, b, bw4, bh4);
         else
             splat_oneref_mv(f->c, t, bs, b, bw4, bh4);
-
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-        rep_macro(type, t->dir seg_pred, off, mul * seg_pred); \
-        rep_macro(type, t->dir skip_mode, off, mul * b->skip_mode); \
-        rep_macro(type, t->dir intra, off, 0); \
-        rep_macro(type, t->dir skip, off, mul * b->skip); \
-        rep_macro(type, t->dir pal_sz, off, 0); \
-        /* see aomedia bug 2183 for why this is outside if (has_chroma) */ \
-        rep_macro(type, t->pal_sz_uv[diridx], off, 0); \
-        rep_macro(type, t->dir tx_intra, off, mul * b_dim[2 + diridx]); \
-        rep_macro(type, t->dir comp_type, off, mul * b->comp_type); \
-        rep_macro(type, t->dir filter[0], off, mul * filter[0]); \
-        rep_macro(type, t->dir filter[1], off, mul * filter[1]); \
-        rep_macro(type, t->dir mode, off, mul * b->inter_mode); \
-        rep_macro(type, t->dir ref[0], off, mul * b->ref[0]); \
-        rep_macro(type, t->dir ref[1], off, mul * ((uint8_t) b->ref[1]))
-        case_set(bh4, l., 1, by4);
-        case_set(bw4, a->, 0, bx4);
+        BlockContext *edge = t->a;
+        for (int i = 0, off = bx4; i < 2; i++, off = by4, edge = &t->l) {
+#define set_ctx(rep_macro) \
+            rep_macro(edge->seg_pred, off, seg_pred); \
+            rep_macro(edge->skip_mode, off, b->skip_mode); \
+            rep_macro(edge->intra, off, 0); \
+            rep_macro(edge->skip, off, b->skip); \
+            rep_macro(edge->pal_sz, off, 0); \
+            /* see aomedia bug 2183 for why this is outside if (has_chroma) */ \
+            rep_macro(t->pal_sz_uv[i], off, 0); \
+            rep_macro(edge->tx_intra, off, b_dim[2 + i]); \
+            rep_macro(edge->comp_type, off, b->comp_type); \
+            rep_macro(edge->filter[0], off, filter[0]); \
+            rep_macro(edge->filter[1], off, filter[1]); \
+            rep_macro(edge->mode, off, b->inter_mode); \
+            rep_macro(edge->ref[0], off, b->ref[0]); \
+            rep_macro(edge->ref[1], off, ((uint8_t) b->ref[1]))
+            case_set(b_dim[2 + i]);
 #undef set_ctx
-
+        }
         if (has_chroma) {
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-            rep_macro(type, t->dir uvmode, off, mul * DC_PRED)
-            case_set(cbh4, l., 1, cby4);
-            case_set(cbw4, a->, 0, cbx4);
-#undef set_ctx
+            dav1d_memset_pow2[ulog2(cbw4)](&t->a->uvmode[cbx4], DC_PRED);
+            dav1d_memset_pow2[ulog2(cbh4)](&t->l.uvmode[cby4], DC_PRED);
         }
     }
 
@@ -1956,12 +1937,12 @@ static int decode_b(Dav1dTaskContext *const t,
         f->frame_hdr->segmentation.update_map)
     {
         uint8_t *seg_ptr = &f->cur_segmap[t->by * f->b4_stride + t->bx];
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
+#define set_ctx(rep_macro) \
         for (int y = 0; y < bh4; y++) { \
-            rep_macro(type, seg_ptr, 0, mul * b->seg_id); \
+            rep_macro(seg_ptr, 0, b->seg_id); \
             seg_ptr += f->b4_stride; \
         }
-        case_set(bw4, NULL, 0, 0);
+        case_set(b_dim[2]);
 #undef set_ctx
     }
     if (!b->skip) {
@@ -2398,10 +2379,10 @@ static int decode_sb(Dav1dTaskContext *const t, const enum BlockLevel bl,
     }
 
     if (t->frame_thread.pass != 2 && (bp != PARTITION_SPLIT || bl == BL_8X8)) {
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-        rep_macro(type, t->a->partition, bx8, mul * dav1d_al_part_ctx[0][bl][bp]); \
-        rep_macro(type, t->l.partition, by8, mul * dav1d_al_part_ctx[1][bl][bp])
-        case_set_upto16(hsz,,,);
+#define set_ctx(rep_macro) \
+        rep_macro(t->a->partition, bx8, dav1d_al_part_ctx[0][bl][bp]); \
+        rep_macro(t->l.partition, by8, dav1d_al_part_ctx[1][bl][bp])
+        case_set_upto16(ulog2(hsz));
 #undef set_ctx
     }
 
diff --git a/src/ipred_tmpl.c b/src/ipred_tmpl.c
index 9975816..9cad40e 100644
--- a/src/ipred_tmpl.c
+++ b/src/ipred_tmpl.c
@@ -732,8 +732,12 @@ static void pal_pred_c(pixel *dst, const ptrdiff_t stride,
 #if HAVE_ASM
 #if ARCH_AARCH64 || ARCH_ARM
 #include "src/arm/ipred.h"
+#elif ARCH_RISCV
+#include "src/riscv/ipred.h"
 #elif ARCH_X86
 #include "src/x86/ipred.h"
+#elif ARCH_LOONGARCH64
+#include "src/loongarch/ipred.h"
 #endif
 #endif
 
@@ -767,8 +771,12 @@ COLD void bitfn(dav1d_intra_pred_dsp_init)(Dav1dIntraPredDSPContext *const c) {
 #if HAVE_ASM
 #if ARCH_AARCH64 || ARCH_ARM
     intra_pred_dsp_init_arm(c);
+#elif ARCH_RISCV
+    intra_pred_dsp_init_riscv(c);
 #elif ARCH_X86
     intra_pred_dsp_init_x86(c);
+#elif ARCH_LOONGARCH64
+    intra_pred_dsp_init_loongarch(c);
 #endif
 #endif
 }
diff --git a/src/itx_1d.c b/src/itx_1d.c
index 8f75c65..14e89ca 100644
--- a/src/itx_1d.c
+++ b/src/itx_1d.c
@@ -89,8 +89,8 @@ inv_dct4_1d_internal_c(int32_t *const c, const ptrdiff_t stride,
     c[3 * stride] = CLIP(t0 - t3);
 }
 
-void dav1d_inv_dct4_1d_c(int32_t *const c, const ptrdiff_t stride,
-                         const int min, const int max)
+static void inv_dct4_1d_c(int32_t *const c, const ptrdiff_t stride,
+                          const int min, const int max)
 {
     inv_dct4_1d_internal_c(c, stride, min, max, 0);
 }
@@ -142,8 +142,8 @@ inv_dct8_1d_internal_c(int32_t *const c, const ptrdiff_t stride,
     c[7 * stride] = CLIP(t0 - t7);
 }
 
-void dav1d_inv_dct8_1d_c(int32_t *const c, const ptrdiff_t stride,
-                         const int min, const int max)
+static void inv_dct8_1d_c(int32_t *const c, const ptrdiff_t stride,
+                          const int min, const int max)
 {
     inv_dct8_1d_internal_c(c, stride, min, max, 0);
 }
@@ -237,8 +237,8 @@ inv_dct16_1d_internal_c(int32_t *const c, const ptrdiff_t stride,
     c[15 * stride] = CLIP(t0 - t15a);
 }
 
-void dav1d_inv_dct16_1d_c(int32_t *const c, const ptrdiff_t stride,
-                          const int min, const int max)
+static void inv_dct16_1d_c(int32_t *const c, const ptrdiff_t stride,
+                           const int min, const int max)
 {
     inv_dct16_1d_internal_c(c, stride, min, max, 0);
 }
@@ -427,14 +427,14 @@ inv_dct32_1d_internal_c(int32_t *const c, const ptrdiff_t stride,
     c[31 * stride] = CLIP(t0  - t31);
 }
 
-void dav1d_inv_dct32_1d_c(int32_t *const c, const ptrdiff_t stride,
-                          const int min, const int max)
+static void inv_dct32_1d_c(int32_t *const c, const ptrdiff_t stride,
+                           const int min, const int max)
 {
     inv_dct32_1d_internal_c(c, stride, min, max, 0);
 }
 
-void dav1d_inv_dct64_1d_c(int32_t *const c, const ptrdiff_t stride,
-                          const int min, const int max)
+static void inv_dct64_1d_c(int32_t *const c, const ptrdiff_t stride,
+                           const int min, const int max)
 {
     assert(stride > 0);
     inv_dct32_1d_internal_c(c, stride << 1, min, max, 1);
@@ -962,13 +962,13 @@ inv_adst16_1d_internal_c(const int32_t *const in, const ptrdiff_t in_s,
 }
 
 #define inv_adst_1d(sz) \
-void dav1d_inv_adst##sz##_1d_c(int32_t *const c, const ptrdiff_t stride, \
-                               const int min, const int max) \
+static void inv_adst##sz##_1d_c(int32_t *const c, const ptrdiff_t stride, \
+                                const int min, const int max) \
 { \
     inv_adst##sz##_1d_internal_c(c, stride, min, max, c, stride); \
 } \
-void dav1d_inv_flipadst##sz##_1d_c(int32_t *const c, const ptrdiff_t stride, \
-                                   const int min, const int max) \
+static void inv_flipadst##sz##_1d_c(int32_t *const c, const ptrdiff_t stride, \
+                                          const int min, const int max) \
 { \
     inv_adst##sz##_1d_internal_c(c, stride, min, max, \
                                  &c[(sz - 1) * stride], -stride); \
@@ -980,8 +980,8 @@ inv_adst_1d(16)
 
 #undef inv_adst_1d
 
-void dav1d_inv_identity4_1d_c(int32_t *const c, const ptrdiff_t stride,
-                              const int min, const int max)
+static void inv_identity4_1d_c(int32_t *const c, const ptrdiff_t stride,
+                               const int min, const int max)
 {
     assert(stride > 0);
     for (int i = 0; i < 4; i++) {
@@ -990,16 +990,16 @@ void dav1d_inv_identity4_1d_c(int32_t *const c, const ptrdiff_t stride,
     }
 }
 
-void dav1d_inv_identity8_1d_c(int32_t *const c, const ptrdiff_t stride,
-                              const int min, const int max)
+static void inv_identity8_1d_c(int32_t *const c, const ptrdiff_t stride,
+                               const int min, const int max)
 {
     assert(stride > 0);
     for (int i = 0; i < 8; i++)
         c[stride * i] *= 2;
 }
 
-void dav1d_inv_identity16_1d_c(int32_t *const c, const ptrdiff_t stride,
-                               const int min, const int max)
+static void inv_identity16_1d_c(int32_t *const c, const ptrdiff_t stride,
+                                const int min, const int max)
 {
     assert(stride > 0);
     for (int i = 0; i < 16; i++) {
@@ -1008,14 +1008,57 @@ void dav1d_inv_identity16_1d_c(int32_t *const c, const ptrdiff_t stride,
     }
 }
 
-void dav1d_inv_identity32_1d_c(int32_t *const c, const ptrdiff_t stride,
-                               const int min, const int max)
+static void inv_identity32_1d_c(int32_t *const c, const ptrdiff_t stride,
+                                const int min, const int max)
 {
     assert(stride > 0);
     for (int i = 0; i < 32; i++)
         c[stride * i] *= 4;
 }
 
+const itx_1d_fn dav1d_tx1d_fns[N_TX_SIZES][N_TX_1D_TYPES] = {
+    [TX_4X4] = {
+        [DCT] = inv_dct4_1d_c,
+        [ADST] = inv_adst4_1d_c,
+        [FLIPADST] = inv_flipadst4_1d_c,
+        [IDENTITY] = inv_identity4_1d_c,
+    }, [TX_8X8] = {
+        [DCT] = inv_dct8_1d_c,
+        [ADST] = inv_adst8_1d_c,
+        [FLIPADST] = inv_flipadst8_1d_c,
+        [IDENTITY] = inv_identity8_1d_c,
+    }, [TX_16X16] = {
+        [DCT] = inv_dct16_1d_c,
+        [ADST] = inv_adst16_1d_c,
+        [FLIPADST] = inv_flipadst16_1d_c,
+        [IDENTITY] = inv_identity16_1d_c,
+    }, [TX_32X32] = {
+        [DCT] = inv_dct32_1d_c,
+        [IDENTITY] = inv_identity32_1d_c,
+    }, [TX_64X64] = {
+        [DCT] = inv_dct64_1d_c,
+    },
+};
+
+const uint8_t /* enum Tx1dType */ dav1d_tx1d_types[N_TX_TYPES][2] = {
+    [DCT_DCT]           = { DCT, DCT },
+    [ADST_DCT]          = { ADST, DCT },
+    [DCT_ADST]          = { DCT, ADST },
+    [ADST_ADST]         = { ADST, ADST },
+    [FLIPADST_DCT]      = { FLIPADST, DCT },
+    [DCT_FLIPADST]      = { DCT, FLIPADST },
+    [FLIPADST_FLIPADST] = { FLIPADST, FLIPADST },
+    [ADST_FLIPADST]     = { ADST, FLIPADST },
+    [FLIPADST_ADST]     = { FLIPADST, ADST },
+    [IDTX]              = { IDENTITY, IDENTITY },
+    [V_DCT]             = { DCT, IDENTITY },
+    [H_DCT]             = { IDENTITY, DCT },
+    [V_ADST]            = { ADST, IDENTITY },
+    [H_ADST]            = { IDENTITY, ADST },
+    [V_FLIPADST]        = { FLIPADST, IDENTITY },
+    [H_FLIPADST]        = { IDENTITY, FLIPADST },
+};
+
 #if !(HAVE_ASM && TRIM_DSP_FUNCTIONS && ( \
   ARCH_AARCH64 || \
   (ARCH_ARM && (defined(__ARM_NEON) || defined(__APPLE__) || defined(_WIN32))) \
diff --git a/src/itx_1d.h b/src/itx_1d.h
index b63d71b..880ac99 100644
--- a/src/itx_1d.h
+++ b/src/itx_1d.h
@@ -28,31 +28,25 @@
 #include <stddef.h>
 #include <stdint.h>
 
+#include "src/levels.h"
+
 #ifndef DAV1D_SRC_ITX_1D_H
 #define DAV1D_SRC_ITX_1D_H
 
+enum Tx1dType {
+    DCT,
+    ADST,
+    IDENTITY,
+    FLIPADST,
+    N_TX_1D_TYPES,
+};
+
 #define decl_itx_1d_fn(name) \
 void (name)(int32_t *c, ptrdiff_t stride, int min, int max)
 typedef decl_itx_1d_fn(*itx_1d_fn);
 
-decl_itx_1d_fn(dav1d_inv_dct4_1d_c);
-decl_itx_1d_fn(dav1d_inv_dct8_1d_c);
-decl_itx_1d_fn(dav1d_inv_dct16_1d_c);
-decl_itx_1d_fn(dav1d_inv_dct32_1d_c);
-decl_itx_1d_fn(dav1d_inv_dct64_1d_c);
-
-decl_itx_1d_fn(dav1d_inv_adst4_1d_c);
-decl_itx_1d_fn(dav1d_inv_adst8_1d_c);
-decl_itx_1d_fn(dav1d_inv_adst16_1d_c);
-
-decl_itx_1d_fn(dav1d_inv_flipadst4_1d_c);
-decl_itx_1d_fn(dav1d_inv_flipadst8_1d_c);
-decl_itx_1d_fn(dav1d_inv_flipadst16_1d_c);
-
-decl_itx_1d_fn(dav1d_inv_identity4_1d_c);
-decl_itx_1d_fn(dav1d_inv_identity8_1d_c);
-decl_itx_1d_fn(dav1d_inv_identity16_1d_c);
-decl_itx_1d_fn(dav1d_inv_identity32_1d_c);
+EXTERN const itx_1d_fn dav1d_tx1d_fns[N_TX_SIZES][N_TX_1D_TYPES];
+EXTERN const uint8_t /* enum Tx1dType */ dav1d_tx1d_types[N_TX_TYPES][2];
 
 void dav1d_inv_wht4_1d_c(int32_t *c, ptrdiff_t stride);
 
diff --git a/src/itx_tmpl.c b/src/itx_tmpl.c
index a226223..bafe0a8 100644
--- a/src/itx_tmpl.c
+++ b/src/itx_tmpl.c
@@ -29,6 +29,7 @@
 
 #include <stddef.h>
 #include <stdint.h>
+#include <stdlib.h>
 #include <string.h>
 
 #include "common/attributes.h"
@@ -36,13 +37,17 @@
 
 #include "src/itx.h"
 #include "src/itx_1d.h"
+#include "src/scan.h"
+#include "src/tables.h"
 
 static NOINLINE void
 inv_txfm_add_c(pixel *dst, const ptrdiff_t stride, coef *const coeff,
-               const int eob, const int w, const int h, const int shift,
-               const itx_1d_fn first_1d_fn, const itx_1d_fn second_1d_fn,
-               const int has_dconly HIGHBD_DECL_SUFFIX)
+               const int eob, const /*enum RectTxfmSize*/ int tx, const int shift,
+               const enum TxfmType txtp HIGHBD_DECL_SUFFIX)
 {
+    const TxfmInfo *const t_dim = &dav1d_txfm_dimensions[tx];
+    const int w = 4 * t_dim->w, h = 4 * t_dim->h;
+    const int has_dconly = txtp == DCT_DCT;
     assert(w >= 4 && w <= 64);
     assert(h >= 4 && h <= 64);
     assert(eob >= 0);
@@ -64,6 +69,9 @@ inv_txfm_add_c(pixel *dst, const ptrdiff_t stride, coef *const coeff,
         return;
     }
 
+    const uint8_t *const txtps = dav1d_tx1d_types[txtp];
+    const itx_1d_fn first_1d_fn = dav1d_tx1d_fns[t_dim->lw][txtps[0]];
+    const itx_1d_fn second_1d_fn = dav1d_tx1d_fns[t_dim->lh][txtps[1]];
     const int sh = imin(h, 32), sw = imin(w, 32);
 #if BITDEPTH == 8
     const int row_clip_min = INT16_MIN;
@@ -76,7 +84,16 @@ inv_txfm_add_c(pixel *dst, const ptrdiff_t stride, coef *const coeff,
     const int col_clip_max = ~col_clip_min;
 
     int32_t tmp[64 * 64], *c = tmp;
-    for (int y = 0; y < sh; y++, c += w) {
+    int last_nonzero_col; // in first 1d itx
+    if (txtps[1] == IDENTITY && txtps[0] != IDENTITY) {
+        last_nonzero_col = imin(sh - 1, eob);
+    } else if (txtps[0] == IDENTITY && txtps[1] != IDENTITY) {
+        last_nonzero_col = eob >> (t_dim->lw + 2);
+    } else {
+        last_nonzero_col = dav1d_last_nonzero_col_from_eob[tx][eob];
+    }
+    assert(last_nonzero_col < sh);
+    for (int y = 0; y <= last_nonzero_col; y++, c += w) {
         if (is_rect2)
             for (int x = 0; x < sw; x++)
                 c[x] = (coeff[y + x * sh] * 181 + 128) >> 8;
@@ -85,6 +102,8 @@ inv_txfm_add_c(pixel *dst, const ptrdiff_t stride, coef *const coeff,
                 c[x] = coeff[y + x * sh];
         first_1d_fn(c, 1, row_clip_min, row_clip_max);
     }
+    if (last_nonzero_col + 1 < sh)
+        memset(c, 0, sizeof(*c) * (sh - last_nonzero_col - 1) * w);
 
     memset(coeff, 0, sizeof(*coeff) * sw * sh);
     for (int i = 0; i < w * sh; i++)
@@ -99,7 +118,7 @@ inv_txfm_add_c(pixel *dst, const ptrdiff_t stride, coef *const coeff,
             dst[x] = iclip_pixel(dst[x] + ((*c++ + 8) >> 4));
 }
 
-#define inv_txfm_fn(type1, type2, w, h, shift, has_dconly) \
+#define inv_txfm_fn(type1, type2, type, pfx, w, h, shift) \
 static void \
 inv_txfm_add_##type1##_##type2##_##w##x##h##_c(pixel *dst, \
                                                const ptrdiff_t stride, \
@@ -107,57 +126,56 @@ inv_txfm_add_##type1##_##type2##_##w##x##h##_c(pixel *dst, \
                                                const int eob \
                                                HIGHBD_DECL_SUFFIX) \
 { \
-    inv_txfm_add_c(dst, stride, coeff, eob, w, h, shift, \
-                   dav1d_inv_##type1##w##_1d_c, dav1d_inv_##type2##h##_1d_c, \
-                   has_dconly HIGHBD_TAIL_SUFFIX); \
+    inv_txfm_add_c(dst, stride, coeff, eob, pfx##TX_##w##X##h, shift, type \
+                   HIGHBD_TAIL_SUFFIX); \
 }
 
-#define inv_txfm_fn64(w, h, shift) \
-inv_txfm_fn(dct, dct, w, h, shift, 1)
+#define inv_txfm_fn64(pfx, w, h, shift) \
+inv_txfm_fn(dct, dct, DCT_DCT, pfx, w, h, shift)
 
-#define inv_txfm_fn32(w, h, shift) \
-inv_txfm_fn64(w, h, shift) \
-inv_txfm_fn(identity, identity, w, h, shift, 0)
+#define inv_txfm_fn32(pfx, w, h, shift) \
+inv_txfm_fn64(pfx, w, h, shift) \
+inv_txfm_fn(identity, identity, IDTX, pfx, w, h, shift)
 
-#define inv_txfm_fn16(w, h, shift) \
-inv_txfm_fn32(w, h, shift) \
-inv_txfm_fn(adst,     dct,      w, h, shift, 0) \
-inv_txfm_fn(dct,      adst,     w, h, shift, 0) \
-inv_txfm_fn(adst,     adst,     w, h, shift, 0) \
-inv_txfm_fn(dct,      flipadst, w, h, shift, 0) \
-inv_txfm_fn(flipadst, dct,      w, h, shift, 0) \
-inv_txfm_fn(adst,     flipadst, w, h, shift, 0) \
-inv_txfm_fn(flipadst, adst,     w, h, shift, 0) \
-inv_txfm_fn(flipadst, flipadst, w, h, shift, 0) \
-inv_txfm_fn(identity, dct,      w, h, shift, 0) \
-inv_txfm_fn(dct,      identity, w, h, shift, 0) \
+#define inv_txfm_fn16(pfx, w, h, shift) \
+inv_txfm_fn32(pfx, w, h, shift) \
+inv_txfm_fn(adst,     dct,      ADST_DCT,          pfx,  w, h, shift) \
+inv_txfm_fn(dct,      adst,     DCT_ADST,          pfx, w, h, shift) \
+inv_txfm_fn(adst,     adst,     ADST_ADST,         pfx, w, h, shift) \
+inv_txfm_fn(dct,      flipadst, DCT_FLIPADST,      pfx, w, h, shift) \
+inv_txfm_fn(flipadst, dct,      FLIPADST_DCT,      pfx, w, h, shift) \
+inv_txfm_fn(adst,     flipadst, ADST_FLIPADST,     pfx, w, h, shift) \
+inv_txfm_fn(flipadst, adst,     FLIPADST_ADST,     pfx, w, h, shift) \
+inv_txfm_fn(flipadst, flipadst, FLIPADST_FLIPADST, pfx, w, h, shift) \
+inv_txfm_fn(identity, dct,      H_DCT,             pfx, w, h, shift) \
+inv_txfm_fn(dct,      identity, V_DCT,             pfx, w, h, shift) \
 
-#define inv_txfm_fn84(w, h, shift) \
-inv_txfm_fn16(w, h, shift) \
-inv_txfm_fn(identity, flipadst, w, h, shift, 0) \
-inv_txfm_fn(flipadst, identity, w, h, shift, 0) \
-inv_txfm_fn(identity, adst,     w, h, shift, 0) \
-inv_txfm_fn(adst,     identity, w, h, shift, 0) \
+#define inv_txfm_fn84(pfx, w, h, shift) \
+inv_txfm_fn16(pfx, w, h, shift) \
+inv_txfm_fn(identity, flipadst, H_FLIPADST, pfx, w, h, shift) \
+inv_txfm_fn(flipadst, identity, V_FLIPADST, pfx, w, h, shift) \
+inv_txfm_fn(identity, adst,     H_ADST,     pfx, w, h, shift) \
+inv_txfm_fn(adst,     identity, V_ADST,     pfx, w, h, shift) \
 
-inv_txfm_fn84( 4,  4, 0)
-inv_txfm_fn84( 4,  8, 0)
-inv_txfm_fn84( 4, 16, 1)
-inv_txfm_fn84( 8,  4, 0)
-inv_txfm_fn84( 8,  8, 1)
-inv_txfm_fn84( 8, 16, 1)
-inv_txfm_fn32( 8, 32, 2)
-inv_txfm_fn84(16,  4, 1)
-inv_txfm_fn84(16,  8, 1)
-inv_txfm_fn16(16, 16, 2)
-inv_txfm_fn32(16, 32, 1)
-inv_txfm_fn64(16, 64, 2)
-inv_txfm_fn32(32,  8, 2)
-inv_txfm_fn32(32, 16, 1)
-inv_txfm_fn32(32, 32, 2)
-inv_txfm_fn64(32, 64, 1)
-inv_txfm_fn64(64, 16, 2)
-inv_txfm_fn64(64, 32, 1)
-inv_txfm_fn64(64, 64, 2)
+inv_txfm_fn84( ,  4,  4, 0)
+inv_txfm_fn84(R,  4,  8, 0)
+inv_txfm_fn84(R,  4, 16, 1)
+inv_txfm_fn84(R,  8,  4, 0)
+inv_txfm_fn84( ,  8,  8, 1)
+inv_txfm_fn84(R,  8, 16, 1)
+inv_txfm_fn32(R,  8, 32, 2)
+inv_txfm_fn84(R, 16,  4, 1)
+inv_txfm_fn84(R, 16,  8, 1)
+inv_txfm_fn16( , 16, 16, 2)
+inv_txfm_fn32(R, 16, 32, 1)
+inv_txfm_fn64(R, 16, 64, 2)
+inv_txfm_fn32(R, 32,  8, 2)
+inv_txfm_fn32(R, 32, 16, 1)
+inv_txfm_fn32( , 32, 32, 2)
+inv_txfm_fn64(R, 32, 64, 1)
+inv_txfm_fn64(R, 64, 16, 2)
+inv_txfm_fn64(R, 64, 32, 1)
+inv_txfm_fn64( , 64, 64, 2)
 
 #if !(HAVE_ASM && TRIM_DSP_FUNCTIONS && ( \
   ARCH_AARCH64 || \
@@ -190,6 +208,8 @@ static void inv_txfm_add_wht_wht_4x4_c(pixel *dst, const ptrdiff_t stride,
 #include "src/arm/itx.h"
 #elif ARCH_LOONGARCH64
 #include "src/loongarch/itx.h"
+#elif ARCH_PPC64LE
+#include "src/ppc/itx.h"
 #elif ARCH_RISCV
 #include "src/riscv/itx.h"
 #elif ARCH_X86
@@ -267,18 +287,25 @@ COLD void bitfn(dav1d_itx_dsp_init)(Dav1dInvTxfmDSPContext *const c, int bpc) {
     assign_itx_all_fn64(64, 32, R);
     assign_itx_all_fn64(64, 64, );
 
+    int all_simd = 0;
 #if HAVE_ASM
 #if ARCH_AARCH64 || ARCH_ARM
-    itx_dsp_init_arm(c, bpc);
+    itx_dsp_init_arm(c, bpc, &all_simd);
 #endif
 #if ARCH_LOONGARCH64
     itx_dsp_init_loongarch(c, bpc);
 #endif
+#if ARCH_PPC64LE
+    itx_dsp_init_ppc(c, bpc);
+#endif
 #if ARCH_RISCV
     itx_dsp_init_riscv(c, bpc);
 #endif
 #if ARCH_X86
-    itx_dsp_init_x86(c, bpc);
+    itx_dsp_init_x86(c, bpc, &all_simd);
 #endif
 #endif
+
+    if (!all_simd)
+        dav1d_init_last_nonzero_col_from_eob_tables();
 }
diff --git a/src/lf_mask.c b/src/lf_mask.c
index 09a5c53..c81bd9b 100644
--- a/src/lf_mask.c
+++ b/src/lf_mask.c
@@ -64,18 +64,15 @@ static void decomp_tx(uint8_t (*const txa)[2 /* txsz, step */][32 /* y */][32 /*
     } else {
         const int lw = imin(2, t_dim->lw), lh = imin(2, t_dim->lh);
 
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
+#define set_ctx(rep_macro) \
         for (int y = 0; y < t_dim->h; y++) { \
-            rep_macro(type, txa[0][0][y], off, mul * lw); \
-            rep_macro(type, txa[1][0][y], off, mul * lh); \
+            rep_macro(txa[0][0][y], 0, lw); \
+            rep_macro(txa[1][0][y], 0, lh); \
             txa[0][1][y][0] = t_dim->w; \
         }
-        case_set_upto16(t_dim->w,,, 0);
-#undef set_ctx
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-        rep_macro(type, txa[1][1][0], off, mul * t_dim->h)
-        case_set_upto16(t_dim->w,,, 0);
+        case_set_upto16(t_dim->lw);
 #undef set_ctx
+        dav1d_memset_pow2[t_dim->lw](txa[1][1][0], t_dim->h);
     }
 }
 
@@ -196,20 +193,8 @@ static inline void mask_edges_intra(uint16_t (*const masks)[32][3][2],
         if (inner2) masks[1][by4 + y][thl4c][1] |= inner2;
     }
 
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-    rep_macro(type, a, off, mul * thl4c)
-#define default_memset(dir, diridx, off, var) \
-    memset(a, thl4c, var)
-    case_set_upto32_with_default(w4,,, 0);
-#undef default_memset
-#undef set_ctx
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-    rep_macro(type, l, off, mul * twl4c)
-#define default_memset(dir, diridx, off, var) \
-    memset(l, twl4c, var)
-    case_set_upto32_with_default(h4,,, 0);
-#undef default_memset
-#undef set_ctx
+    dav1d_memset_likely_pow2(a, thl4c, w4);
+    dav1d_memset_likely_pow2(l, twl4c, h4);
 }
 
 static void mask_edges_chroma(uint16_t (*const masks)[32][2][2],
@@ -267,20 +252,8 @@ static void mask_edges_chroma(uint16_t (*const masks)[32][2][2],
         }
     }
 
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-    rep_macro(type, a, off, mul * thl4c)
-#define default_memset(dir, diridx, off, var) \
-    memset(a, thl4c, var)
-    case_set_upto32_with_default(cw4,,, 0);
-#undef default_memset
-#undef set_ctx
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-    rep_macro(type, l, off, mul * twl4c)
-#define default_memset(dir, diridx, off, var) \
-    memset(l, twl4c, var)
-    case_set_upto32_with_default(ch4,,, 0);
-#undef default_memset
-#undef set_ctx
+    dav1d_memset_likely_pow2(a, thl4c, cw4);
+    dav1d_memset_likely_pow2(l, twl4c, ch4);
 }
 
 void dav1d_create_lf_mask_intra(Av1Filter *const lflvl,
diff --git a/src/lib.c b/src/lib.c
index 3807efd..6d2d80d 100644
--- a/src/lib.c
+++ b/src/lib.c
@@ -31,7 +31,7 @@
 #include <errno.h>
 #include <string.h>
 
-#if defined(__linux__) && defined(HAVE_DLSYM)
+#if defined(__linux__) && HAVE_DLSYM
 #include <dlfcn.h>
 #endif
 
@@ -90,7 +90,7 @@ static void close_internal(Dav1dContext **const c_out, int flush);
 
 NO_SANITIZE("cfi-icall") // CFI is broken with dlsym()
 static COLD size_t get_stack_size_internal(const pthread_attr_t *const thread_attr) {
-#if defined(__linux__) && defined(HAVE_DLSYM) && defined(__GLIBC__)
+#if defined(__linux__) && HAVE_DLSYM && defined(__GLIBC__)
     /* glibc has an issue where the size of the TLS is subtracted from the stack
      * size instead of allocated separately. As a result the specified stack
      * size may be insufficient when used in an application with large amounts
@@ -263,7 +263,6 @@ COLD int dav1d_open(Dav1dContext **const c_out, const Dav1dSettings *const s) {
         f->c = c;
         f->task_thread.ttd = &c->task_thread;
         f->lf.last_sharpness = -1;
-        dav1d_refmvs_init(&f->rf);
     }
 
     for (unsigned m = 0; m < c->n_tc; m++) {
@@ -664,7 +663,7 @@ static COLD void close_internal(Dav1dContext **const c_out, int flush) {
         dav1d_free(f->lf.lr_mask);
         dav1d_free(f->lf.tx_lpf_right_edge[0]);
         dav1d_free(f->lf.start_of_tile_row);
-        dav1d_refmvs_clear(&f->rf);
+        dav1d_free_aligned(f->rf.r);
         dav1d_free_aligned(f->lf.cdef_line_buf);
         dav1d_free_aligned(f->lf.lr_line_buf);
     }
diff --git a/src/loongarch/cdef.S b/src/loongarch/cdef.S
new file mode 100644
index 0000000..d069f66
--- /dev/null
+++ b/src/loongarch/cdef.S
@@ -0,0 +1,2249 @@
+/*
+ * Copyright © 2024, VideoLAN and dav1d authors
+ * Copyright © 2024, Loongson Technology Corporation Limited
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "src/loongarch/loongson_asm.S"
+
+// static int cdef_find_dir_lsx(const pixel *img, const ptrdiff_t stride,
+//                            unsigned *const var HIGHBD_DECL_SUFFIX)
+// param: img: a0, stride: a1, var: a2
+function cdef_find_dir_8bpc_lsx
+    addi.d         sp,    sp,    -64
+    fst.d          f24,   sp,    0
+    fst.d          f25,   sp,    8
+    fst.d          f26,   sp,    16
+    fst.d          f27,   sp,    24
+    fst.d          f28,   sp,    32
+    fst.d          f29,   sp,    40
+    fst.d          f30,   sp,    48
+    fst.d          f31,   sp,    56
+
+    li.d           a3,    128
+    vreplgr2vr.w   vr31,  a3
+
+    // hv: vr0-vr3  diag: vr4-vr11  alt: vr12-vr23
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, vr8, vr9, vr10, \
+        vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, vr19, \
+        vr20, vr21, vr22, vr23
+    vxor.v      \i,       \i,       \i
+.endr
+
+.CFDL01:  // 8
+    // 0
+    fld.d          f24,   a0,    0  //img
+    vpermi.w       vr25,  vr24,  0x01
+
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+
+    vsub.w         vr24,  vr24,  vr31  //px
+    vsub.w         vr25,  vr25,  vr31
+
+    vadd.w         vr4,   vr4,   vr24  //diag[0][y+x]
+    vadd.w         vr5,   vr5,   vr25
+
+    vpackev.w      vr26,  vr25,  vr24
+    vpackod.w      vr27,  vr25,  vr24
+    vpermi.w       vr26,  vr26,  0xd8 //px0246
+    vpermi.w       vr27,  vr27,  0xd8 //px1357
+    vadd.w         vr12,  vr12,  vr26
+    vadd.w         vr12,  vr12,  vr27  //alt[0][y+(x>>1)]
+
+    vhaddw.d.w     vr28,  vr24,  vr24
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a3,    vr28,  0
+    vhaddw.d.w     vr28,  vr25,  vr25
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    add.d          a3,    a3,    a4
+    vinsgr2vr.w    vr0,   a3,    0    //hv[0][y]
+
+    vadd.w         vr15,  vr15,  vr26
+    vadd.w         vr15,  vr15,  vr27  //alt[1][3+y-(x>>1)]
+    vpermi.w       vr15,  vr15,  0x1b
+
+    vadd.w         vr9,   vr9,   vr24
+    vadd.w         vr8,   vr8,   vr25
+    vpermi.w       vr8,   vr8,   0x1b
+    vpermi.w       vr9,   vr9,   0x1b  //diag[1][7+y-x]
+
+    vxor.v         vr28,  vr28,  vr28
+    vxor.v         vr29,  vr29,  vr29
+    vadd.w         vr28,  vr28,  vr24
+    vadd.w         vr29,  vr29,  vr25
+    vextrins.w     vr18,  vr28,  0x30
+    vshuf4i.w      vr19,  vr28,  0x39
+    vextrins.w     vr19,  vr29,  0x30
+    vshuf4i.w      vr20,  vr29,  0x39  //alt[2][3-(y>>1)+7]
+    vinsgr2vr.w    vr20,  zero,  3
+
+    vadd.w         vr2,   vr2,   vr24
+    vadd.w         vr3,   vr3,   vr25  //hv[1][x]
+
+    vadd.w         vr21,  vr21,  vr24
+    vadd.w         vr22,  vr22,  vr25  //alt[3][(y>>1)+x]
+
+    add.d          a0,    a0,    a1
+
+    // 1
+    fld.d          f24,   a0,    0  //img
+    vpermi.w       vr25,  vr24,  0x01
+
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+
+    vsub.w         vr24,  vr24,  vr31  //px
+    vsub.w         vr25,  vr25,  vr31
+
+    vbsrl.v        vr28,  vr4,   4  //1-4
+    vbsrl.v        vr29,  vr5,   4  //5-8
+    vextrins.w     vr28,  vr5,   0x30
+    vadd.w         vr28,  vr28,  vr24  //diag[0][y+x]
+    vadd.w         vr29,  vr29,  vr25
+    vbsll.v        vr5,   vr29,  4
+    vextrins.w     vr5,   vr28,  0x03
+    vextrins.w     vr6,   vr29,  0x03
+    vextrins.w     vr28,  vr4,   0x30
+    vshuf4i.w      vr4,   vr28,  0x93
+
+    vbsrl.v        vr28,  vr12,  4
+    vextrins.w     vr28,  vr13,  0x30
+    vpackev.w      vr26,  vr25,  vr24
+    vpackod.w      vr27,  vr25,  vr24
+    vpermi.w       vr26,  vr26,  0xd8 //px0246
+    vpermi.w       vr27,  vr27,  0xd8 //px1357
+    vadd.w         vr28,  vr28,  vr26
+    vadd.w         vr28,  vr28,  vr27  //alt[0][y+(x>>1)]
+    vextrins.w     vr13,  vr28,  0x03
+    vextrins.w     vr28,  vr12,  0x30
+    vshuf4i.w      vr12,  vr28,  0x93
+
+    vhaddw.d.w     vr28,  vr24,  vr24
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a3,    vr28,  0
+    vhaddw.d.w     vr28,  vr25,  vr25
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    add.d          a3,    a3,    a4
+    vinsgr2vr.w    vr0,   a3,    1    //hv[0][y]
+
+    vbsrl.v        vr28,  vr15,  4
+    vextrins.w     vr28,  vr16,  0x30
+    vpermi.w       vr28,  vr28,  0x1b
+    vadd.w         vr28,  vr28,  vr26
+    vadd.w         vr28,  vr28,  vr27  //alt[1][3+y-(x>>1)]
+    vextrins.w     vr16,  vr28,  0x00
+    vextrins.w     vr28,  vr15,  0x00
+    vshuf4i.w      vr15,  vr28,  0x6c
+
+    vbsrl.v        vr28,  vr8,   4     //4321
+    vbsrl.v        vr29,  vr9,   4     //8765
+    vextrins.w     vr28,  vr9,   0x30
+    vpermi.w       vr28,  vr28,  0x1b
+    vpermi.w       vr29,  vr29,  0x1b
+    vadd.w         vr29,  vr29,  vr24
+    vadd.w         vr28,  vr28,  vr25  //diag[1][7+y-x]
+    vextrins.w     vr10,  vr29,  0x00
+    vextrins.w     vr29,  vr28,  0x00
+    vshuf4i.w      vr9,   vr29,  0x6c
+    vextrins.w     vr28,  vr8,   0x00
+    vshuf4i.w      vr8,   vr28,  0x6c
+
+    vbsll.v        vr28,  vr19,  4
+    vextrins.w     vr28,  vr18,  0x03
+    vbsll.v        vr29,  vr20,  4
+    vextrins.w     vr29,  vr19,  0x03
+    vadd.w         vr28,  vr28,  vr24
+    vadd.w         vr29,  vr29,  vr25  //alt[2][3-(y>>1)+7]
+    vextrins.w     vr18,  vr28,  0x30
+    vextrins.w     vr28,  vr29,  0x00
+    vshuf4i.w      vr19,  vr28,  0x39
+    vbsrl.v        vr20,  vr29,  4
+
+    vadd.w         vr2,   vr2,   vr24
+    vadd.w         vr3,   vr3,   vr25  //hv[1][x]
+
+    vadd.w         vr21,  vr21,  vr24
+    vadd.w         vr22,  vr22,  vr25  //alt[3][(y>>1)+x]
+
+    add.d          a0,    a0,    a1
+
+    // 2
+    fld.d          f24,   a0,    0  //img
+    vpermi.w       vr25,  vr24,  0x01
+
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+
+    vsub.w         vr24,  vr24,  vr31  //px
+    vsub.w         vr25,  vr25,  vr31
+
+    vbsrl.v        vr28,  vr4,   8
+    vbsrl.v        vr29,  vr5,   8
+    vextrins.d     vr28,  vr5,   0x10  //2-5
+    vextrins.d     vr29,  vr6,   0x10  //6-9
+    vadd.w         vr28,  vr28,  vr24  //diag[0][y+x]
+    vadd.w         vr29,  vr29,  vr25
+    vextrins.d     vr4,   vr28,  0x10
+    vextrins.d     vr5,   vr28,  0x01
+    vextrins.d     vr5,   vr29,  0x10
+    vextrins.d     vr6,   vr29,  0x01
+
+    vbsrl.v        vr28,  vr12,  8
+    vextrins.d     vr28,  vr13,  0x10
+    vpackev.w      vr26,  vr25,  vr24
+    vpackod.w      vr27,  vr25,  vr24
+    vpermi.w       vr26,  vr26,  0xd8 //px0246
+    vpermi.w       vr27,  vr27,  0xd8 //px1357
+    vadd.w         vr28,  vr28,  vr26
+    vadd.w         vr28,  vr28,  vr27  //alt[0][y+(x>>1)]
+    vextrins.d     vr12,  vr28,  0x10
+    vextrins.d     vr13,  vr28,  0x01
+
+    vhaddw.d.w     vr28,  vr24,  vr24
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a3,    vr28,  0
+    vhaddw.d.w     vr28,  vr25,  vr25
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    add.d          a3,    a3,    a4
+    vinsgr2vr.w    vr0,   a3,    2    //hv[0][y]
+
+    vbsrl.v        vr28,  vr15,  8
+    vextrins.d     vr28,  vr16,  0x10
+    vpermi.w       vr28,  vr28,  0x1b
+    vadd.w         vr28,  vr28,  vr26
+    vadd.w         vr28,  vr28,  vr27  //alt[1][3+y-(x>>1)]
+    vpermi.w       vr28,  vr28,  0x1b
+    vextrins.d     vr15,  vr28,  0x10
+    vextrins.d     vr16,  vr28,  0x01
+
+    vbsrl.v        vr28,  vr8,   8
+    vextrins.d     vr28,  vr9,   0x10
+    vbsrl.v        vr29,  vr9,   8
+    vextrins.d     vr29,  vr10,  0x10
+    vpermi.w       vr28,  vr28,  0x1b  //5432
+    vpermi.w       vr29,  vr29,  0x1b  //9876
+    vadd.w         vr29,  vr29,  vr24
+    vadd.w         vr28,  vr28,  vr25
+    vpermi.w       vr28,  vr28,  0x1b
+    vpermi.w       vr29,  vr29,  0x1b
+    vextrins.d     vr8,   vr28,  0x10
+    vextrins.d     vr9,   vr28,  0x01
+    vextrins.d     vr9,   vr29,  0x10
+    vextrins.d     vr10,  vr29,  0x01  //diag[1][7+y-x]
+
+    vbsrl.v        vr28,  vr18,  8
+    vextrins.d     vr28,  vr19,  0x10 //2345
+    vbsrl.v        vr29,  vr19,  8
+    vextrins.d     vr29,  vr20,  0x10 //6789
+    vadd.w         vr28,  vr28,  vr24
+    vadd.w         vr29,  vr29,  vr25
+    vextrins.d     vr18,  vr28,  0x10
+    vextrins.d     vr19,  vr28,  0x01
+    vextrins.d     vr19,  vr29,  0x10
+    vextrins.d     vr20,  vr29,  0x01   //alt[2][3-(y>>1)+7]
+
+    vadd.w         vr2,   vr2,   vr24
+    vadd.w         vr3,   vr3,   vr25  //hv[1][x]
+
+    vbsrl.v        vr28,  vr21,  4
+    vextrins.w     vr28,  vr22,  0x30  //1234
+    vbsrl.v        vr29,  vr22,  4     //5678
+    vadd.w         vr28,  vr28,  vr24
+    vadd.w         vr29,  vr29,  vr25  //alt[3][(y>>1)+x]
+    vextrins.w     vr23,  vr29,  0x03
+    vextrins.w     vr29,  vr28,  0x33
+    vshuf4i.w      vr22,  vr29,  0x93
+    vextrins.w     vr28,  vr21,  0x30
+    vshuf4i.w      vr21,  vr28,  0x93
+
+    add.d          a0,    a0,    a1
+
+    // 3
+    fld.d          f24,   a0,    0  //img
+    vpermi.w       vr25,  vr24,  0x01
+
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+
+    vsub.w         vr24,  vr24,  vr31  //px
+    vsub.w         vr25,  vr25,  vr31
+
+    vbsll.v        vr28,  vr5,   4
+    vextrins.w     vr28,  vr4,   0x03 //3456
+    vbsll.v        vr29,  vr6,   4
+    vextrins.w     vr29,  vr5,   0x03 //78910
+    vadd.w         vr28,  vr28,  vr24  //diag[0][y+x]
+    vadd.w         vr29,  vr29,  vr25
+    vextrins.w     vr4,   vr28,  0x30
+    vextrins.w     vr28,  vr29,  0x00
+    vshuf4i.w      vr5,   vr28,  0x39
+    vbsrl.v        vr6,   vr29,  4
+
+    vbsll.v        vr28,  vr13,  4
+    vextrins.w     vr28,  vr12,  0x03
+    vpackev.w      vr26,  vr25,  vr24
+    vpackod.w      vr27,  vr25,  vr24
+    vpermi.w       vr26,  vr26,  0xd8 //px0246
+    vpermi.w       vr27,  vr27,  0xd8 //px1357
+    vadd.w         vr28,  vr28,  vr26
+    vadd.w         vr28,  vr28,  vr27  //alt[0][y+(x>>1)]
+    vextrins.w     vr12,  vr28,  0x30
+    vbsrl.v        vr13,  vr28,  4
+
+    vhaddw.d.w     vr28,  vr24,  vr24
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a3,    vr28,  0
+    vhaddw.d.w     vr28,  vr25,  vr25
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    add.d          a3,    a3,    a4
+    vinsgr2vr.w    vr0,   a3,    3    //hv[0][y]
+
+    vbsll.v        vr28,  vr16,  4
+    vextrins.w     vr28,  vr15,  0x03
+    vpermi.w       vr28,  vr28,  0x1b  //6543
+    vadd.w         vr28,  vr28,  vr26
+    vadd.w         vr28,  vr28,  vr27  //alt[1][3+y-(x>>1)]
+    vextrins.w     vr15,  vr28,  0x33
+    vshuf4i.w      vr16,  vr28,  0xc6
+    vinsgr2vr.w    vr16,  zero,  3
+
+    vbsll.v        vr28,  vr9,   4
+    vextrins.w     vr28,  vr8,   0x03  //3456
+    vbsll.v        vr29,  vr10,  4
+    vextrins.w     vr29,  vr9,   0x03  //78910
+    vpermi.w       vr28,  vr28,  0x1b  //6543
+    vpermi.w       vr29,  vr29,  0x1b  //10987
+    vadd.w         vr29,  vr29,  vr24
+    vadd.w         vr28,  vr28,  vr25  //diag[1][7+y-x]
+    vextrins.w     vr8,   vr28,  0x33
+    vextrins.w     vr28,  vr29,  0x33
+    vshuf4i.w      vr9,   vr28,  0xc6
+    vshuf4i.w      vr10,  vr29,  0xc6
+    vinsgr2vr.w    vr10,  zero,  3
+
+    vbsrl.v        vr28,  vr18,  8
+    vextrins.d     vr28,  vr19,  0x10 //2345
+    vbsrl.v        vr29,  vr19,  8
+    vextrins.d     vr29,  vr20,  0x10 //6789
+    vadd.w         vr28,  vr28,  vr24
+    vadd.w         vr29,  vr29,  vr25
+    vextrins.d     vr18,  vr28,  0x10
+    vextrins.d     vr19,  vr28,  0x01
+    vextrins.d     vr19,  vr29,  0x10
+    vextrins.d     vr20,  vr29,  0x01   //alt[2][3-(y>>1)+7]
+
+    vadd.w         vr2,   vr2,   vr24
+    vadd.w         vr3,   vr3,   vr25  //hv[1][x]
+
+    vbsrl.v        vr28,  vr21,  4
+    vextrins.w     vr28,  vr22,  0x30  //1234
+    vbsrl.v        vr29,  vr22,  4     //5678
+    vextrins.w     vr29,  vr23,  0x30
+    vadd.w         vr28,  vr28,  vr24
+    vadd.w         vr29,  vr29,  vr25  //alt[3][(y>>1)+x]
+    vextrins.w     vr23,  vr29,  0x03
+    vextrins.w     vr29,  vr28,  0x33
+    vshuf4i.w      vr22,  vr29,  0x93
+    vextrins.w     vr28,  vr21,  0x30
+    vshuf4i.w      vr21,  vr28,  0x93
+
+    add.d          a0,    a0,    a1
+
+    // 4
+    fld.d          f24,   a0,    0  //img
+    vpermi.w       vr25,  vr24,  0x01
+
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+
+    vsub.w         vr24,  vr24,  vr31  //px
+    vsub.w         vr25,  vr25,  vr31
+
+    vadd.w         vr5,   vr5,   vr24  //diag[0][y+x]
+    vadd.w         vr6,   vr6,   vr25
+
+    vpackev.w      vr26,  vr25,  vr24
+    vpackod.w      vr27,  vr25,  vr24
+    vpermi.w       vr26,  vr26,  0xd8 //px0246
+    vpermi.w       vr27,  vr27,  0xd8 //px1357
+    vadd.w         vr13,  vr13,  vr26
+    vadd.w         vr13,  vr13,  vr27  //alt[0][y+(x>>1)]
+
+    vhaddw.d.w     vr28,  vr24,  vr24
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a3,    vr28,  0
+    vhaddw.d.w     vr28,  vr25,  vr25
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    add.d          a3,    a3,    a4
+    vinsgr2vr.w    vr1,   a3,    0    //hv[0][y]
+
+    vpermi.w       vr16,  vr16,  0x1b
+    vadd.w         vr16,  vr16,  vr26
+    vadd.w         vr16,  vr16,  vr27  //alt[1][3+y-(x>>1)]
+    vpermi.w       vr16,  vr16,  0x1b
+
+    vpermi.w       vr9,   vr9,   0x1b
+    vpermi.w       vr10,  vr10,  0x1b
+    vadd.w         vr10,  vr10,  vr24
+    vadd.w         vr9,   vr9,   vr25
+    vpermi.w       vr9,   vr9,   0x1b
+    vpermi.w       vr10,  vr10,  0x1b  //diag[1][7+y-x]
+
+    vbsrl.v        vr28,  vr18,  4
+    vextrins.w     vr28,  vr19,  0x30  //1234
+    vbsrl.v        vr29,  vr19,  4
+    vextrins.w     vr29,  vr20,  0x30  //5678
+    vadd.w         vr28,  vr28,  vr24
+    vadd.w         vr29,  vr29,  vr25  //alt[2][3-(y>>1)+7]
+    vextrins.w     vr20,  vr29,  0x03
+    vextrins.w     vr29,  vr28,  0x33
+    vshuf4i.w      vr19,  vr29,  0x93
+    vbsll.v        vr18,  vr28,  4
+
+    vadd.w         vr2,   vr2,   vr24
+    vadd.w         vr3,   vr3,   vr25  //hv[1][x]
+
+    vbsrl.v        vr28,  vr21,  8
+    vextrins.d     vr28,  vr22,  0x10
+    vbsrl.v        vr29,  vr22,  8
+    vextrins.d     vr29,  vr23,  0x10
+    vadd.w         vr28,  vr28,  vr24
+    vadd.w         vr29,  vr29,  vr25
+    vextrins.d     vr21,  vr28,  0x10
+    vextrins.d     vr22,  vr28,  0x01
+    vextrins.d     vr22,  vr29,  0x10
+    vextrins.d     vr23,  vr29,  0x01  //alt[3][(y>>1)+x]
+
+    add.d          a0,    a0,    a1
+
+    // 5
+    fld.d          f24,   a0,    0  //img
+    vpermi.w       vr25,  vr24,  0x01
+
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+
+    vsub.w         vr24,  vr24,  vr31  //px
+    vsub.w         vr25,  vr25,  vr31
+
+    vbsrl.v        vr28,  vr5,   4  //5-8
+    vbsrl.v        vr29,  vr6,   4  //9-12
+    vextrins.w     vr28,  vr6,   0x30
+    vadd.w         vr28,  vr28,  vr24  //diag[0][y+x]
+    vadd.w         vr29,  vr29,  vr25
+    vextrins.w     vr7,   vr29,  0x03
+    vextrins.w     vr29,  vr28,  0x33
+    vshuf4i.w      vr6,   vr29,  0x93
+    vextrins.w     vr28,  vr5,   0x30
+    vshuf4i.w      vr5,   vr28,  0x93
+
+    vbsrl.v        vr28,  vr13,  4
+    vextrins.w     vr28,  vr14,  0x30
+    vpackev.w      vr26,  vr25,  vr24
+    vpackod.w      vr27,  vr25,  vr24
+    vpermi.w       vr26,  vr26,  0xd8 //px0246
+    vpermi.w       vr27,  vr27,  0xd8 //px1357
+    vadd.w         vr28,  vr28,  vr26
+    vadd.w         vr28,  vr28,  vr27  //alt[0][y+(x>>1)]
+    vextrins.w     vr14,  vr28,  0x03
+    vextrins.w     vr28,  vr13,  0x30
+    vshuf4i.w      vr13,  vr28,  0x93
+
+    vhaddw.d.w     vr28,  vr24,  vr24
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a3,    vr28,  0
+    vhaddw.d.w     vr28,  vr25,  vr25
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    add.d          a3,    a3,    a4
+    vinsgr2vr.w    vr1,   a3,    1    //hv[0][y]
+
+    vbsrl.v        vr28,  vr16,  4
+    vextrins.w     vr28,  vr17,  0x30
+    vpermi.w       vr28,  vr28,  0x1b
+    vadd.w         vr28,  vr28,  vr26
+    vadd.w         vr28,  vr28,  vr27  //alt[1][3+y-(x>>1)]
+    vextrins.w     vr17,  vr28,  0x00
+    vextrins.w     vr28,  vr16,  0x00
+    vshuf4i.w      vr16,  vr28,  0x6c
+
+    vbsrl.v        vr28,  vr9,   4
+    vbsrl.v        vr29,  vr10,  4
+    vextrins.w     vr28,  vr10,  0x30
+    vpermi.w       vr28,  vr28,  0x1b  //8-5
+    vpermi.w       vr29,  vr29,  0x1b  //12-9
+    vadd.w         vr29,  vr29,  vr24
+    vadd.w         vr28,  vr28,  vr25  //diag[1][7+y-x]
+    vextrins.w     vr11,  vr29,  0x00
+    vextrins.w     vr29,  vr28,  0x00
+    vshuf4i.w      vr10,  vr29,  0x6c
+    vextrins.w     vr28,  vr9,   0x00
+    vshuf4i.w      vr9,   vr28,  0x6c
+
+    vbsrl.v        vr28,  vr18,  4
+    vextrins.w     vr28,  vr19,  0x30  //1234
+    vbsrl.v        vr29,  vr19,  4
+    vextrins.w     vr29,  vr20,  0x30  //5678
+    vadd.w         vr28,  vr28,  vr24
+    vadd.w         vr29,  vr29,  vr25  //alt[2][3-(y>>1)+7]
+    vextrins.w     vr20,  vr29,  0x03
+    vextrins.w     vr29,  vr28,  0x33
+    vshuf4i.w      vr19,  vr29,  0x93
+    vbsll.v        vr18,  vr28,  4
+
+    vadd.w         vr2,   vr2,   vr24
+    vadd.w         vr3,   vr3,   vr25  //hv[1][x]
+
+    vbsrl.v        vr28,  vr21,  8
+    vextrins.d     vr28,  vr22,  0x10
+    vbsrl.v        vr29,  vr22,  8
+    vextrins.d     vr29,  vr23,  0x10
+    vadd.w         vr28,  vr28,  vr24
+    vadd.w         vr29,  vr29,  vr25
+    vextrins.d     vr21,  vr28,  0x10
+    vextrins.d     vr22,  vr28,  0x01
+    vextrins.d     vr22,  vr29,  0x10
+    vextrins.d     vr23,  vr29,  0x01  //alt[3][(y>>1)+x]
+
+    add.d          a0,    a0,    a1
+
+    // 6
+    fld.d          f24,   a0,    0  //img
+    vpermi.w       vr25,  vr24,  0x01
+
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+
+    vsub.w         vr24,  vr24,  vr31  //px
+    vsub.w         vr25,  vr25,  vr31
+
+    vbsrl.v        vr28,  vr5,   8
+    vbsrl.v        vr29,  vr6,   8
+    vextrins.d     vr28,  vr6,   0x10  //6-9
+    vextrins.d     vr29,  vr7,   0x10  //10-13
+    vadd.w         vr28,  vr28,  vr24  //diag[0][y+x]
+    vadd.w         vr29,  vr29,  vr25
+    vextrins.d     vr5,   vr28,  0x10
+    vextrins.d     vr6,   vr28,  0x01
+    vextrins.d     vr6,   vr29,  0x10
+    vextrins.d     vr7,   vr29,  0x01
+
+    vbsrl.v        vr28,  vr13,  8
+    vextrins.d     vr28,  vr14,  0x10
+    vpackev.w      vr26,  vr25,  vr24
+    vpackod.w      vr27,  vr25,  vr24
+    vpermi.w       vr26,  vr26,  0xd8 //px0246
+    vpermi.w       vr27,  vr27,  0xd8 //px1357
+    vadd.w         vr28,  vr28,  vr26
+    vadd.w         vr28,  vr28,  vr27  //alt[0][y+(x>>1)]
+    vextrins.d     vr13,  vr28,  0x10
+    vextrins.d     vr14,  vr28,  0x01
+
+    vhaddw.d.w     vr28,  vr24,  vr24
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a3,    vr28,  0
+    vhaddw.d.w     vr28,  vr25,  vr25
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    add.d          a3,    a3,    a4
+    vinsgr2vr.w    vr1,   a3,    2    //hv[0][y]
+
+    vbsrl.v        vr28,  vr16,  8
+    vextrins.d     vr28,  vr17,  0x10
+    vpermi.w       vr28,  vr28,  0x1b
+    vadd.w         vr28,  vr28,  vr26
+    vadd.w         vr28,  vr28,  vr27  //alt[1][3+y-(x>>1)]
+    vpermi.w       vr28,  vr28,  0x1b
+    vextrins.d     vr16,  vr28,  0x10
+    vextrins.d     vr17,  vr28,  0x01
+
+    vbsrl.v        vr28,  vr9,   8
+    vextrins.d     vr28,  vr10,  0x10
+    vbsrl.v        vr29,  vr10,  8
+    vextrins.d     vr29,  vr11,  0x10
+    vpermi.w       vr28,  vr28,  0x1b  //9876
+    vpermi.w       vr29,  vr29,  0x1b  //13-10
+    vadd.w         vr29,  vr29,  vr24
+    vadd.w         vr28,  vr28,  vr25
+    vpermi.w       vr28,  vr28,  0x1b
+    vpermi.w       vr29,  vr29,  0x1b
+    vextrins.d     vr9,   vr28,  0x10
+    vextrins.d     vr10,  vr28,  0x01
+    vextrins.d     vr10,  vr29,  0x10
+    vextrins.d     vr11,  vr29,  0x01  //diag[1][7+y-x]
+
+    vadd.w         vr18,  vr18,  vr24 //0123
+    vadd.w         vr19,  vr19,  vr25 //4567 alt[2][3-(y>>1)+7]
+
+    vadd.w         vr2,   vr2,   vr24
+    vadd.w         vr3,   vr3,   vr25  //hv[1][x]
+
+    vbsll.v        vr28,  vr22,  4
+    vextrins.w     vr28,  vr21,  0x03  //3456
+    vbsll.v        vr29,  vr23,  4
+    vextrins.w     vr29,  vr22,  0x03  //78910
+    vadd.w         vr28,  vr28,  vr24
+    vadd.w         vr29,  vr29,  vr25  //alt[3][(y>>1)+x]
+    vextrins.w     vr21,  vr28,  0x30
+    vextrins.w     vr28,  vr29,  0x00
+    vshuf4i.w      vr22,  vr28,  0x39
+    vbsrl.v        vr23,  vr29,  4
+
+    add.d          a0,    a0,    a1
+
+    // 7
+    fld.d          f24,   a0,    0  //img
+    vpermi.w       vr25,  vr24,  0x01
+
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr24,  vr24,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+    vsllwil.hu.bu  vr25,  vr25,  0
+
+    vsub.w         vr24,  vr24,  vr31  //px
+    vsub.w         vr25,  vr25,  vr31
+
+    vbsll.v        vr28,  vr6,   4
+    vextrins.w     vr28,  vr5,   0x03 //78910
+    vbsll.v        vr29,  vr7,   4
+    vextrins.w     vr29,  vr6,   0x03 //11-14
+    vadd.w         vr28,  vr28,  vr24  //diag[0][y+x]
+    vadd.w         vr29,  vr29,  vr25
+    vextrins.w     vr5,   vr28,  0x30
+    vextrins.w     vr28,  vr29,  0x00
+    vshuf4i.w      vr6,   vr28,  0x39
+    vbsrl.v        vr7,   vr29,  4
+
+    vbsll.v        vr28,  vr14,  4
+    vextrins.w     vr28,  vr13,  0x03
+    vpackev.w      vr26,  vr25,  vr24
+    vpackod.w      vr27,  vr25,  vr24
+    vpermi.w       vr26,  vr26,  0xd8 //px0246
+    vpermi.w       vr27,  vr27,  0xd8 //px1357
+    vadd.w         vr28,  vr28,  vr26
+    vadd.w         vr28,  vr28,  vr27  //alt[0][y+(x>>1)]
+    vextrins.w     vr13,  vr28,  0x30
+    vbsrl.v        vr14,  vr28,  4
+
+    vhaddw.d.w     vr28,  vr24,  vr24
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a3,    vr28,  0
+    vhaddw.d.w     vr28,  vr25,  vr25
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    add.d          a3,    a3,    a4
+    vinsgr2vr.w    vr1,   a3,    3    //hv[0][y]
+
+    vbsll.v        vr28,  vr17,  4
+    vextrins.w     vr28,  vr16,  0x03
+    vpermi.w       vr28,  vr28,  0x1b  //10987
+    vadd.w         vr28,  vr28,  vr26
+    vadd.w         vr28,  vr28,  vr27  //alt[1][3+y-(x>>1)]
+    vextrins.w     vr16,  vr28,  0x33
+    vshuf4i.w      vr17,  vr28,  0xc6
+    vinsgr2vr.w    vr17,  zero,  3
+
+    vbsll.v        vr28,  vr10,  4
+    vextrins.w     vr28,  vr9,   0x03  //7-10
+    vbsll.v        vr29,  vr11,  4
+    vextrins.w     vr29,  vr10,  0x03  //11-14
+    vpermi.w       vr28,  vr28,  0x1b  //10-7
+    vpermi.w       vr29,  vr29,  0x1b  //14-11
+    vadd.w         vr29,  vr29,  vr24
+    vadd.w         vr28,  vr28,  vr25  //diag[1][7+y-x]
+    vextrins.w     vr9,   vr28,  0x33
+    vextrins.w     vr28,  vr29,  0x33
+    vshuf4i.w      vr10,  vr28,  0xc6
+    vshuf4i.w      vr11,  vr29,  0xc6
+    vinsgr2vr.w    vr11,  zero,  3
+
+    vadd.w         vr18,  vr18,  vr24 //0123
+    vadd.w         vr19,  vr19,  vr25 //4567 alt[2][3-(y>>1)+7]
+
+    vadd.w         vr2,   vr2,   vr24
+    vadd.w         vr3,   vr3,   vr25  //hv[1][x]
+
+    vbsll.v        vr28,  vr22,  4
+    vextrins.w     vr28,  vr21,  0x03  //3456
+    vbsll.v        vr29,  vr23,  4
+    vextrins.w     vr29,  vr22,  0x03  //78910
+    vadd.w         vr28,  vr28,  vr24
+    vadd.w         vr29,  vr29,  vr25  //alt[3][(y>>1)+x]
+    vextrins.w     vr21,  vr28,  0x30
+    vextrins.w     vr28,  vr29,  0x00
+    vshuf4i.w      vr22,  vr28,  0x39
+    vbsrl.v        vr23,  vr29,  4
+
+    add.d          a0,    a0,    a1
+
+    vxor.v         vr24,  vr24,  vr24  //unsigned cost[8]
+    vxor.v         vr25,  vr25,  vr25
+
+    vmul.w         vr26,  vr0,   vr0
+    vmul.w         vr27,  vr1,   vr1
+    vhaddw.d.w     vr28,  vr26,  vr26
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a3,    vr28,  0
+    vhaddw.d.w     vr28,  vr27,  vr27
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    add.d          a3,    a3,    a4
+
+    vmul.w         vr26,  vr2,   vr2
+    vmul.w         vr27,  vr3,   vr3
+    vhaddw.d.w     vr28,  vr26,  vr26
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    vhaddw.d.w     vr28,  vr27,  vr27
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a5,    vr28,  0
+    add.d          a4,    a4,    a5
+
+    li.d           a6,    105
+    mul.w          a3,    a3,    a6
+    mul.w          a4,    a4,    a6
+    vinsgr2vr.w    vr24,  a3,    2
+    vinsgr2vr.w    vr25,  a4,    2
+
+    vxor.v         vr30,  vr30,  vr30  //div_table
+    vxor.v         vr31,  vr31,  vr31
+    li.d           t0,    840
+    vinsgr2vr.w    vr30,  t0,    0
+    li.d           t0,    420
+    vinsgr2vr.w    vr30,  t0,    1
+    li.d           t0,    280
+    vinsgr2vr.w    vr30,  t0,    2
+    li.d           t0,    210
+    vinsgr2vr.w    vr30,  t0,    3
+    li.d           t0,    168
+    vinsgr2vr.w    vr31,  t0,    0
+    li.d           t0,    140
+    vinsgr2vr.w    vr31,  t0,    1
+    li.d           t0,    120
+    vinsgr2vr.w    vr31,  t0,    2
+
+    vbsll.v        vr27,  vr7,   4
+    vextrins.w     vr27,  vr6,   0x03
+    vpermi.w       vr27,  vr27,  0x1b
+    vmul.w         vr26,  vr4,   vr4
+    vmadd.w        vr26,  vr27,  vr27
+    vmul.w         vr26,  vr26,  vr30
+    vhaddw.d.w     vr28,  vr26,  vr26
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a3,    vr28,  0
+    vbsll.v        vr27,  vr6,   4
+    vpermi.w       vr27,  vr27,  0x1b
+    vmul.w         vr26,  vr5,   vr5
+    vmadd.w        vr26,  vr27,  vr27
+    vmul.w         vr26,  vr26,  vr31
+    vextrins.w     vr26,  vr31,  0x33
+    vhaddw.d.w     vr28,  vr26,  vr26
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    add.d          a3,    a3,    a4   //cost[0]
+
+    vbsll.v        vr27,  vr11,  4
+    vextrins.w     vr27,  vr10,  0x03
+    vpermi.w       vr27,  vr27,  0x1b
+    vmul.w         vr26,  vr8,   vr8
+    vmadd.w        vr26,  vr27,  vr27
+    vmul.w         vr26,  vr26,  vr30
+    vhaddw.d.w     vr28,  vr26,  vr26
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    vbsll.v        vr27,  vr10,  4
+    vpermi.w       vr27,  vr27,  0x1b
+    vmul.w         vr26,  vr9,   vr9
+    vmadd.w        vr26,  vr27,  vr27
+    vmul.w         vr26,  vr26,  vr31
+    vextrins.w     vr26,  vr31,  0x33
+    vhaddw.d.w     vr28,  vr26,  vr26
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a5,    vr28,  0
+    add.d          a4,    a4,    a5   //cost[4]
+
+    vpickve2gr.w   a5,    vr5,   3
+    mul.w          a5,    a5,    a5
+    mul.w          a5,    a5,    a6
+    add.w          a3,    a3,    a5
+    vinsgr2vr.w    vr24,  a3,    0
+    vpickve2gr.w   a5,    vr9,   3
+    mul.w          a5,    a5,    a5
+    mul.w          a5,    a5,    a6
+    add.w          a4,    a4,    a5
+    vinsgr2vr.w    vr25,  a4,    0
+
+    //n=0
+    vpickve2gr.w   a3,    vr24,  1
+    vmul.w         vr26,  vr13,  vr13
+    vhaddw.d.w     vr28,  vr26,  vr26
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    vpickve2gr.w   a5,    vr12,  3
+    mul.w          a5,    a5,    a5
+    add.d          a3,    a3,    a4
+    add.d          a3,    a3,    a5
+    mul.w          a3,    a3,    a6  //*cost_ptr
+
+    vextrins.w     vr29,  vr30,  0x01
+    vextrins.w     vr29,  vr30,  0x13
+    vextrins.w     vr29,  vr31,  0x21
+    vextrins.w     vr29,  vr31,  0x33
+    vbsll.v        vr27,  vr14,  4
+    vpermi.w       vr27,  vr27,  0x1b
+    vmul.w         vr28,  vr12,  vr12
+    vextrins.w     vr28,  vr31,  0x33
+    vmadd.w        vr28,  vr27,  vr27
+    vmul.w         vr26,  vr28,  vr29
+    vhaddw.d.w     vr28,  vr26,  vr26
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    add.d          a3,    a3,    a4
+    vinsgr2vr.w    vr24,  a3,    1
+
+    //n=1
+    vpickve2gr.w   a3,    vr24,  3
+    vmul.w         vr26,  vr16,  vr16
+    vhaddw.d.w     vr28,  vr26,  vr26
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    vpickve2gr.w   a5,    vr15,  3
+    mul.w          a5,    a5,    a5
+    add.d          a3,    a3,    a4
+    add.d          a3,    a3,    a5
+    mul.w          a3,    a3,    a6  //*cost_ptr
+
+    vbsll.v        vr27,  vr17,  4
+    vpermi.w       vr27,  vr27,  0x1b
+    vmul.w         vr28,  vr15,  vr15
+    vextrins.w     vr28,  vr31,  0x33
+    vmadd.w        vr28,  vr27,  vr27
+    vmul.w         vr26,  vr28,  vr29
+    vhaddw.d.w     vr28,  vr26,  vr26
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    add.d          a3,    a3,    a4
+    vinsgr2vr.w    vr24,  a3,    3
+
+    //n=2
+    vpickve2gr.w   a3,    vr25,  1
+    vmul.w         vr26,  vr19,  vr19
+    vhaddw.d.w     vr28,  vr26,  vr26
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    vpickve2gr.w   a5,    vr18,  3
+    mul.w          a5,    a5,    a5
+    add.d          a3,    a3,    a4
+    add.d          a3,    a3,    a5
+    mul.w          a3,    a3,    a6  //*cost_ptr
+
+    vbsll.v        vr27,  vr20,  4
+    vpermi.w       vr27,  vr27,  0x1b
+    vmul.w         vr28,  vr18,  vr18
+    vextrins.w     vr28,  vr31,  0x33
+    vmadd.w        vr28,  vr27,  vr27
+    vmul.w         vr26,  vr28,  vr29
+    vhaddw.d.w     vr28,  vr26,  vr26
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    add.d          a3,    a3,    a4
+    vinsgr2vr.w    vr25,  a3,    1
+
+    //n=3
+    vpickve2gr.w   a3,    vr25,  3
+    vmul.w         vr26,  vr22,  vr22
+    vhaddw.d.w     vr28,  vr26,  vr26
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    vpickve2gr.w   a5,    vr21,  3
+    mul.w          a5,    a5,    a5
+    add.d          a3,    a3,    a4
+    add.d          a3,    a3,    a5
+    mul.w          a3,    a3,    a6  //*cost_ptr
+
+    vbsll.v        vr27,  vr23,  4
+    vpermi.w       vr27,  vr27,  0x1b
+    vmul.w         vr28,  vr21,  vr21
+    vextrins.w     vr28,  vr31,  0x33
+    vmadd.w        vr28,  vr27,  vr27
+    vmul.w         vr26,  vr28,  vr29
+    vhaddw.d.w     vr28,  vr26,  vr26
+    vhaddw.q.d     vr28,  vr28,  vr28
+    vpickve2gr.d   a4,    vr28,  0
+    add.d          a3,    a3,    a4
+    vinsgr2vr.w    vr25,  a3,    3
+
+    xor            a3,    a3,    a3  //best_dir
+    vpickve2gr.w   a4,    vr24,  0   //best_cost
+.BSETDIR01:
+    vpickve2gr.w   a5,    vr24,  1
+    bge            a4,    a5,    .BSETDIR02
+    or             a4,    a5,    a5
+    ori            a3,    zero,  1
+.BSETDIR02:
+    vpickve2gr.w   a5,    vr24,  2
+    bge            a4,    a5,    .BSETDIR03
+    or             a4,    a5,    a5
+    ori            a3,    zero,  2
+.BSETDIR03:
+    vpickve2gr.w   a5,    vr24,  3
+    bge            a4,    a5,    .BSETDIR04
+    or             a4,    a5,    a5
+    ori            a3,    zero,  3
+.BSETDIR04:
+    vpickve2gr.w   a5,    vr25,  0
+    bge            a4,    a5,    .BSETDIR05
+    or             a4,    a5,    a5
+    ori            a3,    zero,  4
+.BSETDIR05:
+    vpickve2gr.w   a5,    vr25,  1
+    bge            a4,    a5,    .BSETDIR06
+    or             a4,    a5,    a5
+    ori            a3,    zero,  5
+.BSETDIR06:
+    vpickve2gr.w   a5,    vr25,  2
+    bge            a4,    a5,    .BSETDIR07
+    or             a4,    a5,    a5
+    ori            a3,    zero,  6
+.BSETDIR07:
+    vpickve2gr.w   a5,    vr25,  3
+    bge            a4,    a5,    .BSETDIREND
+    or             a4,    a5,    a5
+    ori            a3,    zero,  7
+.BSETDIREND:
+    xori           a5,    a3,    4
+    li.d           a1,    4
+    bge            a5,    a1,    .GETCOST01
+    vreplve.w      vr26,  vr24,  a5
+    b              .GETCOST02
+.GETCOST01:
+    vreplve.w      vr26,  vr25,  a5
+.GETCOST02:
+    vpickve2gr.w   a5,    vr26,  0
+    sub.w          a5,    a4,    a5
+    srai.d         a5,    a5,    10
+    st.w           a5,    a2,    0
+    or             a0,    a3,    a3
+
+    fld.d          f24,   sp,    0
+    fld.d          f25,   sp,    8
+    fld.d          f26,   sp,    16
+    fld.d          f27,   sp,    24
+    fld.d          f28,   sp,    32
+    fld.d          f29,   sp,    40
+    fld.d          f30,   sp,    48
+    fld.d          f31,   sp,    56
+    addi.d         sp,    sp,    64
+
+endfunc
+
+.macro cdef_fill tmp, stride, w, h
+    beqz          \h,     700f         //h
+    or            t0,     zero,  zero  //y
+100:
+    or            t1,     zero,  zero  //xx
+    srai.d        s6,     \w,    3     //x
+    beqz          s6,     300f
+200:
+    vstx          vr18,   \tmp,    t1
+    addi.d        t1,     t1,    16
+    addi.d        s6,     s6,    -1
+    bnez          s6,     200b
+300:
+    andi          s6,     \w,    4
+    beqz          s6,     400f
+    fstx.d        f18,    \tmp,    t1
+    addi.d        t1,     t1,    8
+400:
+    andi          s6,     \w,    2
+    beqz          s6,     500f
+    fstx.s        f18,    \tmp,    t1
+    addi.d        t1,     t1,    4
+500:
+    andi          s6,     \w,    1
+    beqz          s6,     600f
+    li.w          s6,     -16384
+    stx.h         s6,     \tmp,    t1
+    addi.d        t1,     t1,    2
+600:
+    add.d         \tmp,     \tmp,    \stride
+    add.d         \tmp,     \tmp,    \stride
+    addi.d        t0,     t0,    1
+    blt           t0,     \h,    100b
+700:
+.endm
+
+const dav1d_cdef_directions
+.byte   1 * 12 + 0,  2 * 12 + 0
+.byte   1 * 12 + 0,  2 * 12 - 1
+.byte   -1 * 12 + 1, -2 * 12 + 2
+.byte   0 * 12 + 1, -1 * 12 + 2
+.byte   0 * 12 + 1,  0 * 12 + 2
+.byte   0 * 12 + 1,  1 * 12 + 2
+.byte   1 * 12 + 1,  2 * 12 + 2
+.byte   1 * 12 + 0,  2 * 12 + 1
+.byte   1 * 12 + 0,  2 * 12 + 0
+.byte   1 * 12 + 0,  2 * 12 - 1
+.byte   -1 * 12 + 1, -2 * 12 + 2
+.byte   0 * 12 + 1, -1 * 12 + 2
+endconst
+
+.macro constrain_vrh in0, in1, in2, tmp0, tmp1, out
+    vabsd.h        \tmp0, \in0,  vr23   //adiff
+    vsra.h         \tmp1, \tmp0, \in2
+    vsub.h         \tmp1, \in1,  \tmp1
+    vmax.h         \tmp1, vr23,  \tmp1  //imax
+    vmin.h         \tmp0, \tmp0, \tmp1  //imin
+
+    //apply_sign
+    vslt.h         \tmp1, \in0,  vr23
+    vandn.v        \in0,  \tmp1, \tmp0
+    vsigncov.h     \tmp0, \tmp1, \tmp0
+    vor.v          \out,  \in0,  \tmp0
+.endm
+
+.macro iclip_vrh in0, in1, in2, tmp0, tmp1, out
+    vmin.h         \tmp0, \in2,  \in0
+    vslt.h         \in0,  \in0,  \in1
+    vand.v         \tmp1, \in0,  \in1
+    vandn.v        \tmp0, \in0,  \tmp0
+    vor.v          \out,  \tmp1, \tmp0
+.endm
+
+.macro cdef_padding_data
+    //y < 0
+    beqz          t7,     90f
+4:
+    or            t4,     t5,    t5  //data index xx
+    slli.d        t0,     t4,    1
+    mul.w         t2,     t7,    s5
+    slli.d        t2,     t2,    1
+    add.d         t2,     s4,    t2
+
+    sub.d         t3,     t6,    t5  //loop param x
+    srai.d        t3,     t3,    3
+    add.d         t3,     t3,    t5
+    beq           t5,     t3,    6f
+5:  // /8
+    fldx.d        f18,    a3,    t4
+    vsllwil.hu.bu vr18,   vr18,  0
+    vstx          vr18,   t2,    t0
+    addi.d        t0,     t0,    16
+    addi.d        t4,     t4,    8
+
+    addi.d        t3,     t3,    -1
+    bne           t5,     t3,    5b
+6:  // &4
+    sub.d         t1,     t6,    t5
+    andi          t1,     t1,    4
+    beqz          t1,     7f
+
+    fldx.s        f18,    a3,    t4
+    vsllwil.hu.bu vr18,   vr18,  0
+    fstx.d        f18,    t2,    t0
+    addi.d        t0,     t0,    8
+    addi.d        t4,     t4,    4
+7:  // &2
+    sub.d         t1,     t6,    t5
+    andi          t1,     t1,    2
+    beqz          t1,     9f
+
+    ldx.bu        t1,     a3,    t4
+    stx.h         t1,     t2,    t0
+    addi.d        t0,     t0,    2
+    addi.d        t4,     t4,    1
+    ldx.bu        t1,     a3,    t4
+    stx.h         t1,     t2,    t0
+    addi.d        t0,     t0,    2
+    addi.d        t4,     t4,    1
+9:
+    add.d         a3,     a3,    a1
+    addi.d        t7,     t7,    1
+    bnez          t7,     4b
+
+90:
+    // y < h
+    beqz          s1,     12f
+    beqz          t5,     12f
+    or            t7,     zero,  zero  //y
+10:
+    or            t4,     t5,    t5  //data index x
+11:
+    slli.d        t3,     t7,    1
+    addi.d        t3,     t3,    2
+    add.d         t3,     t3,    t4
+    ldx.bu        t1,     a2,    t3
+
+    mul.w         t3,     t7,    s5
+    add.d         t3,     t3,    t4
+    slli.d        t3,     t3,    1
+    stx.h         t1,     s4,    t3
+
+    addi.d        t4,     t4,    1
+    bnez          t4,     11b
+
+    addi.d        t7,     t7,    1
+    bne           t7,     s1,    10b
+
+12:
+    // y = 0 ; y < h
+    or            s0,     s4,    s4
+    beqz          s1,     20f
+    or            s6,     a0,    a0
+    or            t7,     zero,  zero  //y
+    srai.d        t4,     t6,    3    //loop max
+13:
+    or            t0,     zero,  zero //loop param
+    or            t3,     t0,    t0   //data index src
+    or            t1,     t0,    t0   //data index tmp
+    beqz          t4,     16f
+15:  // /8
+    fldx.d        f18,    s6,    t3
+    vsllwil.hu.bu vr18,   vr18,  0
+    vstx          vr18,   s0,    t1
+    addi.d        t3,     t3,    8
+    addi.d        t1,     t1,    16
+
+    addi.d        t0,     t0,    1
+    blt           t0,     t4,    15b
+16:  // &4
+    andi          t0,     t6,    4
+    beqz          t0,     17f
+
+    fldx.s        f18,    s6,    t3
+    vsllwil.hu.bu vr18,   vr18,  0
+    fstx.d        f18,    s0,    t1
+    addi.d        t3,     t3,    4
+    addi.d        t1,     t1,    8
+17:  // &2
+    andi          t0,     t6,    2
+    beqz          t0,     19f
+
+    ldx.bu        t2,     s6,    t3
+    stx.h         t2,     s0,    t1
+    addi.d        t3,     t3,    1
+    addi.d        t1,     t1,    2
+    ldx.bu        t2,     s6,    t3
+    stx.h         t2,     s0,    t1
+    addi.d        t3,     t3,    1
+    addi.d        t1,     t1,    2
+19: // src+ tmp+
+    add.d         s6,     s6,    a1
+    add.d         s0,     s0,    s5
+    add.d         s0,     s0,    s5
+
+    addi.d        t7,     t7,    1
+    blt           t7,     s1,    13b
+
+    // y = h ; y < y_end
+20:
+    beq           s1,     t8,    27f
+    or            t7,     s1,    s1  //y
+    sub.d         t4,     t6,    t5
+    srai.d        t4,     t4,    3
+    add.d         t4,     t4,    t5   //8 loop max
+21:
+    or            t0,     t5,    t5   //xx
+    or            t3,     t0,    t0   //data index bottom
+    slli.d        t1,     t0,    1    //data index tmp
+    beq           t5,     t4,    23f
+22:  // /8
+    fldx.d        f18,    a4,    t3
+    vsllwil.hu.bu vr18,   vr18,  0
+    vstx          vr18,   s0,    t1
+    addi.d        t3,     t3,    8
+    addi.d        t1,     t1,    16
+
+    addi.d        t0,     t0,    1
+    blt           t0,     t4,    22b
+23:  // &4
+    sub.d         t0,     t6,    t5
+    andi          t0,     t0,    4
+    beqz          t0,     24f
+
+    fldx.s        f18,    a4,    t3
+    vsllwil.hu.bu vr18,   vr18,  0
+    fstx.d        f18,    s0,    t1
+    addi.d        t3,     t3,    4
+    addi.d        t1,     t1,    8
+24:  // &2
+    sub.d         t0,     t6,    t5
+    andi          t0,     t0,    2
+    beqz          t0,     26f
+
+    ldx.bu        t2,     a4,    t3
+    stx.h         t2,     s0,    t1
+    addi.d        t3,     t3,    1
+    addi.d        t1,     t1,    2
+    ldx.bu        t2,     a4,    t3
+    stx.h         t2,     s0,    t1
+    addi.d        t3,     t3,    1
+    addi.d        t1,     t1,    2
+26: // bottom+ tmp+
+    add.d         a4,     a4,    a1
+    add.d         s0,     s0,    s5
+    add.d         s0,     s0,    s5
+
+    addi.d        t7,     t7,    1
+    blt           t7,     t8,    21b
+27:
+    // padding end
+.endm
+
+.macro cdef_pri_sec_init
+    clz.w          t3,    a6
+    sub.w          t3,    t2,    t3
+    sub.w          t3,    s7,    t3  //sec_shift
+
+    vreplgr2vr.h   vr4,   t0         //pri_tap_k
+    vreplgr2vr.h   vr9,   a5         //pri_strength
+    vreplgr2vr.h   vr10,  t1         //pri_shift
+    vreplgr2vr.h   vr18,  a6         //sec_strength
+    vreplgr2vr.h   vr19,  t3         //sec_shift
+
+    or             t2,    s1,    s1  //dowhile loop param
+    addi.d         s1,    a7,    2
+    slli.d         s1,    s1,    1   //directions dir+2
+    addi.d         s2,    a7,    4
+    slli.d         s2,    s2,    1   //directions dir+4
+    slli.d         s3,    a7,    1   //directions dir+0
+
+    la.local       t0,    dav1d_cdef_directions
+    add.d          s1,    t0,    s1
+    ld.b           a2,    s1,    0  //off01
+    ld.b           a3,    s1,    1  //off11
+    add.d          s2,    t0,    s2
+    ld.b           s1,    s2,    0  //off02
+    ld.b           s2,    s2,    1  //off12
+    add.d          s3,    t0,    s3
+    ld.b           t0,    s3,    0  //off03
+    ld.b           s3,    s3,    1  //off13
+
+    slli.d         a2,    a2,    1
+    slli.d         a3,    a3,    1
+    slli.d         s1,    s1,    1
+    slli.d         s2,    s2,    1
+    slli.d         t0,    t0,    1
+    slli.d         s3,    s3,    1
+.endm
+
+.macro cdef_pri_init
+    vreplgr2vr.h   vr4,   t0         //pri_tap_k
+    vreplgr2vr.h   vr9,   a5         //pri_strength
+    vreplgr2vr.h   vr10,  t1         //pri_shift
+
+    or             t2,    s1,    s1  //dowhile loop param
+    addi.d         s1,    a7,    2
+    slli.d         s1,    s1,    1   //directions dir+2
+
+    la.local       t0,    dav1d_cdef_directions
+    add.d          s1,    t0,    s1
+    ld.b           a2,    s1,    0  //off01
+    ld.b           a3,    s1,    1  //off11
+
+    slli.d         a2,    a2,    1
+    slli.d         a3,    a3,    1
+.endm
+
+.macro cdef_sec_init
+    clz.w          t3,    a6
+    li.w           t2,    31
+    sub.w          t3,    t2,    t3
+    sub.w          t3,    s7,    t3  //sec_shift
+
+    vreplgr2vr.h   vr18,  a6         //sec_strength
+    vreplgr2vr.h   vr19,  t3         //sec_shift
+
+    or             t2,    s1,    s1  //dowhile loop param
+    addi.d         s2,    a7,    4
+    slli.d         s2,    s2,    1   //directions dir+4
+    slli.d         s3,    a7,    1   //directions dir+0
+
+    la.local       t0,    dav1d_cdef_directions
+    add.d          s1,    t0,    s1
+    add.d          s2,    t0,    s2
+    ld.b           s1,    s2,    0  //off02
+    ld.b           s2,    s2,    1  //off12
+    add.d          s3,    t0,    s3
+    ld.b           t0,    s3,    0  //off03
+    ld.b           s3,    s3,    1  //off13
+
+    slli.d         s1,    s1,    1
+    slli.d         s2,    s2,    1
+    slli.d         t0,    t0,    1
+    slli.d         s3,    s3,    1
+.endm
+
+.macro cdef_process_data_w8 in0, in1
+    vsub.h       vr11,   vr5,   vr0
+    vsub.h       vr12,   vr6,   vr0
+    vsub.h       vr13,   vr7,   vr0
+    vsub.h       vr14,   vr8,   vr0
+
+    constrain_vrh   vr11,  \in0,   \in1,  vr16,  vr17,  vr11
+    constrain_vrh   vr12,  \in0,   \in1,  vr16,  vr17,  vr12
+    constrain_vrh   vr13,  \in0,   \in1,  vr16,  vr17,  vr13
+    constrain_vrh   vr14,  \in0,   \in1,  vr16,  vr17,  vr14
+.endm
+
+.macro cdef_process_data_w4 in0, in1
+    vpermi.w       vr6,  vr5,  0x44
+    vpermi.w       vr8,  vr7,  0x44
+
+    vsub.h         vr12,  vr6,   vr0
+    vsub.h         vr14,  vr8,   vr0
+
+    constrain_vrh   vr12,  \in0,   \in1,  vr16,  vr17,  vr12
+    constrain_vrh   vr14,  \in0,   \in1,  vr16,  vr17,  vr14
+.endm
+
+.macro cdef_calc_sum_tapchange_w8
+    vmul.h         vr1,   vr15,  vr11  //sum
+    vmadd.h        vr1,   vr15,  vr12  //sum
+    vand.v         vr15,  vr15,  vr21
+    vor.v          vr15,  vr15,  vr22
+    vmadd.h        vr1,   vr15,  vr13  //sum
+    vmadd.h        vr1,   vr15,  vr14  //sum
+.endm
+
+.macro cdef_calc_sum_tapchange_w4
+    vmul.h         vr1,   vr15,  vr12  //sum
+    vand.v         vr15,  vr15,  vr21
+    vor.v          vr15,  vr15,  vr22
+    vmadd.h        vr1,   vr15,  vr14  //sum
+.endm
+
+.macro cdef_calc_sum_no_tapchange_w4 in0
+    vmadd.h        vr1,   \in0,  vr12
+    vmadd.h        vr1,   \in0,  vr14
+.endm
+
+.macro cdef_calc_sum_no_tapchange_w8 in0
+    vmadd.h        vr1,   \in0,  vr11  //sum
+    vmadd.h        vr1,   \in0,  vr12
+    vmadd.h        vr1,   \in0,  vr13
+    vmadd.h        vr1,   \in0,  vr14
+.endm
+
+.macro cdef_calc_maxmin_w4
+    vmin.hu        vr3,   vr6,   vr3
+    vmax.h         vr2,   vr6,   vr2
+    vmin.hu        vr3,   vr8,   vr3  //min
+    vmax.h         vr2,   vr8,   vr2  //max
+.endm
+
+.macro cdef_calc_maxmin_w8
+    vmin.hu        vr3,   vr5,   vr3
+    vmax.h         vr2,   vr5,   vr2
+    vmin.hu        vr3,   vr6,   vr3
+    vmax.h         vr2,   vr6,   vr2
+    vmin.hu        vr3,   vr7,   vr3
+    vmax.h         vr2,   vr7,   vr2
+    vmin.hu        vr3,   vr8,   vr3  //min
+    vmax.h         vr2,   vr8,   vr2  //max
+.endm
+
+.macro cdef_calc_dst
+    vslti.h        vr5,   vr1,   0
+    vand.v         vr5,   vr5,   vr20
+    vsub.h         vr5,   vr1,   vr5
+    vaddi.hu       vr5,   vr5,   8
+    vsrai.h        vr5,   vr5,   4
+    vadd.h         vr5,   vr0,   vr5
+.endm
+
+//static NOINLINE void cdef_filter_block_lsx
+//                    (pixel *dst, const ptrdiff_t dst_stride,
+//                     const pixel (*left)[2], const pixel *const top,
+//                     const int pri_strength, const int sec_strength,
+//                     const int dir, const int damping, const int w, int h,
+//                     const enum CdefEdgeFlags edges HIGHBD_DECL_SUFFIX)
+// w=4 h=4
+//param: dst:a0, dst_stride:a1, left:a2, top:a3, bottom:a4, pri_strength:a5
+//sec_strength:a6, dir:a7, damping:s7, w:s0, h:s1, edges:s2
+function cdef_filter_block_4x4_8bpc_lsx
+    ld.w           t0,    sp,    0
+    ld.w           t1,    sp,    8
+    addi.d         sp,    sp,    -(64+288)
+    st.d           s0,    sp,    0
+    st.d           s1,    sp,    8
+    st.d           s2,    sp,    16
+    st.d           s3,    sp,    24
+    st.d           s4,    sp,    32
+    st.d           s5,    sp,    40
+    st.d           s6,    sp,    48
+    st.d           s7,    sp,    56
+
+    li.w           s0,    4         //w
+    li.w           s1,    4         //h
+    or             s2,    t1,    t1 //edges
+    or             s7,    t0,    t0 //damping
+
+    li.d           s5,    12         //tmp_stride
+    addi.d         s4,    sp,    64
+    slli.d         t0,    s5,    1
+    addi.d         t0,    t0,    2
+    slli.d         t0,    t0,    1
+    add.d          s4,    s4,    t0  //ptr tmp
+    vxor.v         vr23,  vr23,  vr23
+    li.w           t2,    1
+    vreplgr2vr.h   vr20,  t2
+    vaddi.hu       vr21,  vr20,  2
+    vaddi.hu       vr22,  vr20,  1
+
+    li.w          t0,     -16384
+    vreplgr2vr.h  vr18,   t0
+
+    //padding
+    li.w          t5,     -2        //x_start
+    addi.d        t6,     s0,    2  //x_end
+    li.w          t7,     -2        //y_start
+    addi.d        t8,     s1,    2  //y_end
+    li.w          t2,     2
+
+    andi          t4,     s2,    4
+    bnez          t4,     1f
+
+    //CDEF_HAVE_TOP
+    slli.d        t3,     s5,    2
+    addi.d        t4,     s4,    -4
+    sub.d         t4,     t4,    t3
+    addi.d        t3,     s0,    4
+
+    cdef_fill     t4,     s5,    t3,     t2
+
+    or            t7,     zero,  zero
+
+1:  //CDEF_HAVE_BOTTOM
+    andi          t4,     s2,8
+    bnez          t4,     2f
+
+    mul.w         t3,     s1,    s5
+    slli.d        t3,     t3,  1
+    add.d         t4,     s4,  t3
+    addi.d        t4,     t4,    -4
+    li.d          t3,     8
+
+    cdef_fill     t4,     s5,    t3,     t2
+
+    addi.d        t8,     t8,    -2
+
+2:  //CDEF_HAVE_LEFT
+    andi          t4,     s2,1
+    bnez          t4,     3f
+
+    mul.w         t3,     t7,    s5
+    slli.d        t3,     t3,    1
+    add.d         t4,     s4,    t3
+    addi.d        t4,     t4,    -4
+    sub.d         t3,     t8,    t7
+
+    cdef_fill     t4,     s5,    t2,     t3
+
+    or            t5,     zero,  zero
+
+3:  //CDEF_HAVE_RIGHT
+    andi          t4,     s2,2
+    bnez          t4,     40f
+
+    mul.w         t3,     t7,    s5
+    slli.d        t3,     t3,    1
+    add.d         t4,     s4,    t3
+    addi.d        t4,     t4,    8
+    sub.d         t3,     t8,    t7
+
+    cdef_fill     t4,     s5,    t2,     t3
+
+    addi.d        t6,     t6,    -2
+
+40:
+    cdef_padding_data
+
+    beqz           a5,    33f
+
+28:  //if (pri_strength)
+    li.w           t0,    4
+    andi           t1,    a5,    1
+    sub.d          t0,    t0,    t1  //pri_tap
+
+    clz.w          t1,    a5
+    li.d           t2,    31
+    sub.w          t1,    t2,    t1
+    sub.w          t1,    s7,    t1
+
+    blt            t1,    zero,  281f
+    or             t1,    t1,    t1
+    b              282f
+281:
+    or             t1,    zero,  zero   //t1: pri_shift
+282:
+
+    beqz           a6,    31f
+
+29:  //if (sec_strength)
+    cdef_pri_sec_init
+
+30:
+    fld.s          f0,    a0,    0     //px
+    vsllwil.hu.bu  vr0,   vr0,   0
+    vpermi.w       vr0,   vr0,   0x44
+
+    vxor.v         vr1,   vr1,   vr1   //sum
+    vor.v          vr2,   vr0,   vr0   //max
+    vor.v          vr3,   vr0,   vr0   //min
+    vor.v          vr15,  vr4,   vr4   //pri_tap_k
+
+    sub.d          t4,    s4,    a2
+    sub.d          t5,    s4,    a3
+
+    fldx.d         f5,    s4,    a2   //p0_00
+    fld.d          f6,    t4,    0    //p0_01
+    fldx.d         f7,    s4,    a3   //p0_10
+    fld.d          f8,    t5,    0    //p0_11
+
+    cdef_process_data_w4 vr9,   vr10
+    cdef_calc_sum_tapchange_w4
+    cdef_calc_maxmin_w4
+
+    sub.d          t4,    s4,    s1  //tmp[-off02]
+    sub.d          t5,    s4,    t0  //tmp[-off03]
+
+    fldx.d         f5,    s4,    s1   //s0_00
+    fld.d          f6,    t4,    0    //s0_01
+    fldx.d         f7,    s4,    t0   //s0_02
+    fld.d          f8,    t5,    0    //s0_03
+
+    cdef_process_data_w4 vr18, vr19
+    cdef_calc_sum_no_tapchange_w4 vr22
+    cdef_calc_maxmin_w4
+
+    sub.d          t4,    s4,    s2  //tmp[-off12]
+    sub.d          t5,    s4,    s3  //tmp[-off13]
+
+    fldx.d         f5,    s4,    s2   //s0_10
+    fld.d          f6,    t4,    0    //s0_11
+    fldx.d         f7,    s4,    s3   //s0_12
+    fld.d          f8,    t5,    0    //s0_13
+
+    cdef_process_data_w4 vr18, vr19
+    cdef_calc_sum_no_tapchange_w4 vr20
+    cdef_calc_maxmin_w4
+
+    vshuf4i.w      vr5,   vr1,   0x0e
+    vshuf4i.w      vr6,   vr3,   0x0e
+    vshuf4i.w      vr7,   vr2,   0x0e
+    vadd.h         vr1,   vr1,   vr5
+    vmin.hu        vr3,   vr6,   vr3
+    vmax.h         vr2,   vr7,   vr2
+
+    cdef_calc_dst
+    iclip_vrh       vr5,   vr3,   vr2,  vr16,  vr17,  vr5
+
+    vsrlni.b.h     vr5,   vr5,   0
+    fst.s          f5,    a0,    0
+
+    add.d          a0,    a0,    a1
+    add.d          s4,    s4,    s5
+    add.d          s4,    s4,    s5
+
+    addi.d         t2,    t2,    -1
+    blt            zero,  t2,    30b
+    b              35f
+
+31:  // pri_strength only
+    cdef_pri_init
+
+32:
+    fld.s          f0,    a0,    0     //px
+    vsllwil.hu.bu  vr0,   vr0,   0
+    vpermi.w       vr0,   vr0,   0x44
+
+    vxor.v         vr1,   vr1,   vr1   //sum
+    vor.v          vr15,  vr4,   vr4   //pri_tap_k
+
+    sub.d          t4,    s4,    a2
+    sub.d          t5,    s4,    a3
+
+    fldx.d         f5,    s4,    a2   //p0_00
+    fld.d          f6,    t4,    0    //p0_01
+    fldx.d         f7,    s4,    a3   //p0_10
+    fld.d          f8,    t5,    0    //p0_11
+
+    cdef_process_data_w4 vr9,   vr10
+    cdef_calc_sum_tapchange_w4
+
+    vshuf4i.w      vr5,   vr1,   0x0e
+    vadd.h         vr1,   vr1,   vr5
+
+    cdef_calc_dst
+
+    vsrlni.b.h     vr5,   vr5,   0
+    fst.s          f5,    a0,    0
+
+    add.d          a0,    a0,    a1
+    add.d          s4,    s4,    s5
+    add.d          s4,    s4,    s5
+
+    addi.d         t2,    t2,    -1
+    blt            zero,  t2,    32b
+    b              35f
+
+33:   // sec_strength only
+    cdef_sec_init
+
+34:
+    fld.s          f0,    a0,    0     //px
+    vsllwil.hu.bu  vr0,   vr0,   0
+    vpermi.w       vr0,   vr0,   0x44
+
+    vxor.v         vr1,   vr1,   vr1   //sum
+
+    sub.d          t4,    s4,    s1  //tmp[-off02]
+    sub.d          t5,    s4,    t0  //tmp[-off03]
+
+    fldx.d         f5,    s4,    s1   //s0_00
+    fld.d          f6,    t4,    0    //s0_01
+    fldx.d         f7,    s4,    t0   //s0_02
+    fld.d          f8,    t5,    0    //s0_03
+
+    cdef_process_data_w4 vr18, vr19
+    cdef_calc_sum_no_tapchange_w4 vr22
+
+    sub.d          t4,    s4,    s2  //tmp[-off12]
+    sub.d          t5,    s4,    s3  //tmp[-off13]
+
+    fldx.d         f5,    s4,    s2   //s0_10
+    fld.d          f6,    t4,    0    //s0_11
+    fldx.d         f7,    s4,    s3   //s0_12
+    fld.d          f8,    t5,    0    //s0_13
+
+    cdef_process_data_w4 vr18, vr19
+    cdef_calc_sum_no_tapchange_w4 vr20
+
+    vshuf4i.w      vr5,   vr1,   0x0e
+    vadd.h         vr1,   vr1,   vr5
+
+    cdef_calc_dst
+
+    vsrlni.b.h     vr5,   vr5,   0
+    fst.s          f5,    a0,    0
+
+    add.d          a0,    a0,    a1
+    add.d          s4,    s4,    s5
+    add.d          s4,    s4,    s5
+
+    addi.d         t2,    t2,    -1
+    blt            zero,  t2,    34b
+
+35:
+    ld.d           s0,    sp,    0
+    ld.d           s1,    sp,    8
+    ld.d           s2,    sp,    16
+    ld.d           s3,    sp,    24
+    ld.d           s4,    sp,    32
+    ld.d           s5,    sp,    40
+    ld.d           s6,    sp,    48
+    ld.d           s7,    sp,    56
+    addi.d         sp,    sp,    (64+288)
+endfunc
+
+function cdef_filter_block_4x8_8bpc_lsx
+    ld.w           t0,    sp,    0
+    ld.w           t1,    sp,    8
+    addi.d         sp,    sp,    -(64+288)
+    st.d           s0,    sp,    0
+    st.d           s1,    sp,    8
+    st.d           s2,    sp,    16
+    st.d           s3,    sp,    24
+    st.d           s4,    sp,    32
+    st.d           s5,    sp,    40
+    st.d           s6,    sp,    48
+    st.d           s7,    sp,    56
+
+    li.w           s0,    4         //w
+    li.w           s1,    8         //h
+    or             s2,    t1,    t1 //edges
+    or             s7,    t0,    t0 //damping
+
+    li.d           s5,    12         //tmp_stride
+    addi.d         s4,    sp,    64
+    slli.d         t0,    s5,    1
+    addi.d         t0,    t0,    2
+    slli.d         t0,    t0,    1
+    add.d          s4,    s4,    t0  //ptr tmp
+    vxor.v         vr23,  vr23,  vr23
+    li.w           t2,    1
+    vreplgr2vr.h   vr20,  t2
+    vaddi.hu       vr21,  vr20,  2
+    vaddi.hu       vr22,  vr20,  1
+
+    li.w          t0,     -16384
+    vreplgr2vr.h  vr18,   t0
+
+    //padding
+    li.w          t5,     -2        //x_start
+    addi.d        t6,     s0,    2  //x_end
+    li.w          t7,     -2        //y_start
+    addi.d        t8,     s1,    2  //y_end
+    li.w          t2,     2
+
+    andi          t4,     s2,    4
+    bnez          t4,     1f
+
+    //CDEF_HAVE_TOP
+    slli.d        t3,     s5,    2
+    addi.d        t4,     s4,    -4
+    sub.d         t4,     t4,    t3
+    addi.d        t3,     s0,    4
+
+    cdef_fill     t4,     s5,    t3,     t2
+
+    or            t7,     zero,  zero
+
+1:  //CDEF_HAVE_BOTTOM
+    andi          t4,     s2,8
+    bnez          t4,     2f
+
+    mul.w         t3,     s1,    s5
+    slli.d        t3,     t3,  1
+    add.d         t4,     s4,  t3
+    addi.d        t4,     t4,    -4
+    li.d          t3,     8
+
+    cdef_fill     t4,     s5,    t3,     t2
+
+    addi.d        t8,     t8,    -2
+
+2:  //CDEF_HAVE_LEFT
+    andi          t4,     s2,1
+    bnez          t4,     3f
+
+    mul.w         t3,     t7,    s5
+    slli.d        t3,     t3,    1
+    add.d         t4,     s4,    t3
+    addi.d        t4,     t4,    -4
+    sub.d         t3,     t8,    t7
+
+    cdef_fill     t4,     s5,    t2,     t3
+
+    or            t5,     zero,  zero
+
+3:  //CDEF_HAVE_RIGHT
+    andi          t4,     s2,2
+    bnez          t4,     40f
+
+    mul.w         t3,     t7,    s5
+    slli.d        t3,     t3,    1
+    add.d         t4,     s4,    t3
+    addi.d        t4,     t4,    8
+    sub.d         t3,     t8,    t7
+
+    cdef_fill     t4,     s5,    t2,     t3
+
+    addi.d        t6,     t6,    -2
+
+40:
+    cdef_padding_data
+
+    beqz           a5,    33f
+
+28:  //if (pri_strength)
+    li.w           t0,    4
+    andi           t1,    a5,    1
+    sub.d          t0,    t0,    t1  //pri_tap
+
+    clz.w          t1,    a5
+    li.d           t2,    31
+    sub.w          t1,    t2,    t1
+    sub.w          t1,    s7,    t1
+
+    blt            t1,    zero,  281f
+    or             t1,    t1,    t1
+    b              282f
+281:
+    or             t1,    zero,  zero   //t1: pri_shift
+282:
+
+    beqz           a6,    31f
+
+29:  //if (sec_strength)
+    cdef_pri_sec_init
+
+30:
+    fld.s          f0,    a0,    0     //px
+    vsllwil.hu.bu  vr0,   vr0,   0
+    vpermi.w       vr0,   vr0,   0x44
+
+    vxor.v         vr1,   vr1,   vr1   //sum
+    vor.v          vr2,   vr0,   vr0   //max
+    vor.v          vr3,   vr0,   vr0   //min
+    vor.v          vr15,  vr4,   vr4   //pri_tap_k
+
+    sub.d          t4,    s4,    a2
+    sub.d          t5,    s4,    a3
+
+    fldx.d         f5,    s4,    a2   //p0_00
+    fld.d          f6,    t4,    0    //p0_01
+    fldx.d         f7,    s4,    a3   //p0_10
+    fld.d          f8,    t5,    0    //p0_11
+
+    cdef_process_data_w4 vr9,   vr10
+    cdef_calc_sum_tapchange_w4
+    cdef_calc_maxmin_w4
+
+    sub.d          t4,    s4,    s1  //tmp[-off02]
+    sub.d          t5,    s4,    t0  //tmp[-off03]
+
+    fldx.d         f5,    s4,    s1   //s0_00
+    fld.d          f6,    t4,    0    //s0_01
+    fldx.d         f7,    s4,    t0   //s0_02
+    fld.d          f8,    t5,    0    //s0_03
+
+    cdef_process_data_w4 vr18, vr19
+    cdef_calc_sum_no_tapchange_w4 vr22
+    cdef_calc_maxmin_w4
+
+    sub.d          t4,    s4,    s2  //tmp[-off12]
+    sub.d          t5,    s4,    s3  //tmp[-off13]
+
+    fldx.d         f5,    s4,    s2   //s0_10
+    fld.d          f6,    t4,    0    //s0_11
+    fldx.d         f7,    s4,    s3   //s0_12
+    fld.d          f8,    t5,    0    //s0_13
+
+    cdef_process_data_w4 vr18, vr19
+    cdef_calc_sum_no_tapchange_w4 vr20
+    cdef_calc_maxmin_w4
+
+    vshuf4i.w      vr5,   vr1,   0x0e
+    vshuf4i.w      vr6,   vr3,   0x0e
+    vshuf4i.w      vr7,   vr2,   0x0e
+    vadd.h         vr1,   vr1,   vr5
+    vmin.hu        vr3,   vr6,   vr3
+    vmax.h         vr2,   vr7,   vr2
+
+    cdef_calc_dst
+    iclip_vrh       vr5,   vr3,   vr2,  vr16,  vr17,  vr5
+
+    vsrlni.b.h     vr5,   vr5,   0
+    fst.s          f5,    a0,    0
+
+    add.d          a0,    a0,    a1
+    add.d          s4,    s4,    s5
+    add.d          s4,    s4,    s5
+
+    addi.d         t2,    t2,    -1
+    blt            zero,  t2,    30b
+    b              35f
+
+31:  // pri_strength only
+    cdef_pri_init
+
+32:
+    fld.s          f0,    a0,    0     //px
+    vsllwil.hu.bu  vr0,   vr0,   0
+    vpermi.w       vr0,   vr0,   0x44
+
+    vxor.v         vr1,   vr1,   vr1   //sum
+    vor.v          vr15,  vr4,   vr4   //pri_tap_k
+
+    sub.d          t4,    s4,    a2
+    sub.d          t5,    s4,    a3
+
+    fldx.d         f5,    s4,    a2   //p0_00
+    fld.d          f6,    t4,    0    //p0_01
+    fldx.d         f7,    s4,    a3   //p0_10
+    fld.d          f8,    t5,    0    //p0_11
+
+    cdef_process_data_w4 vr9,   vr10
+    cdef_calc_sum_tapchange_w4
+
+    vshuf4i.w      vr5,   vr1,   0x0e
+    vadd.h         vr1,   vr1,   vr5
+
+    cdef_calc_dst
+
+    vsrlni.b.h     vr5,   vr5,   0
+    fst.s          f5,    a0,    0
+
+    add.d          a0,    a0,    a1
+    add.d          s4,    s4,    s5
+    add.d          s4,    s4,    s5
+
+    addi.d         t2,    t2,    -1
+    blt            zero,  t2,    32b
+    b              35f
+
+33:   // sec_strength only
+    cdef_sec_init
+
+34:
+    fld.s          f0,    a0,    0     //px
+    vsllwil.hu.bu  vr0,   vr0,   0
+    vpermi.w       vr0,   vr0,   0x44
+
+    vxor.v         vr1,   vr1,   vr1   //sum
+
+    sub.d          t4,    s4,    s1  //tmp[-off02]
+    sub.d          t5,    s4,    t0  //tmp[-off03]
+
+    fldx.d         f5,    s4,    s1   //s0_00
+    fld.d          f6,    t4,    0    //s0_01
+    fldx.d         f7,    s4,    t0   //s0_02
+    fld.d          f8,    t5,    0    //s0_03
+
+    cdef_process_data_w4 vr18, vr19
+    cdef_calc_sum_no_tapchange_w4 vr22
+
+    sub.d          t4,    s4,    s2  //tmp[-off12]
+    sub.d          t5,    s4,    s3  //tmp[-off13]
+
+    fldx.d         f5,    s4,    s2   //s0_10
+    fld.d          f6,    t4,    0    //s0_11
+    fldx.d         f7,    s4,    s3   //s0_12
+    fld.d          f8,    t5,    0    //s0_13
+
+    cdef_process_data_w4 vr18, vr19
+    cdef_calc_sum_no_tapchange_w4 vr20
+
+    vshuf4i.w      vr5,   vr1,   0x0e
+    vadd.h         vr1,   vr1,   vr5
+
+    cdef_calc_dst
+
+    vsrlni.b.h     vr5,   vr5,   0
+    fst.s          f5,    a0,    0
+
+    add.d          a0,    a0,    a1
+    add.d          s4,    s4,    s5
+    add.d          s4,    s4,    s5
+
+    addi.d         t2,    t2,    -1
+    blt            zero,  t2,    34b
+
+35:
+    ld.d           s0,    sp,    0
+    ld.d           s1,    sp,    8
+    ld.d           s2,    sp,    16
+    ld.d           s3,    sp,    24
+    ld.d           s4,    sp,    32
+    ld.d           s5,    sp,    40
+    ld.d           s6,    sp,    48
+    ld.d           s7,    sp,    56
+    addi.d         sp,    sp,    (64+288)
+endfunc
+
+function cdef_filter_block_8x8_8bpc_lsx
+    ld.w           t0,    sp,    0
+    ld.w           t1,    sp,    8
+    addi.d         sp,    sp,    -(64+288)
+    st.d           s0,    sp,    0
+    st.d           s1,    sp,    8
+    st.d           s2,    sp,    16
+    st.d           s3,    sp,    24
+    st.d           s4,    sp,    32
+    st.d           s5,    sp,    40
+    st.d           s6,    sp,    48
+    st.d           s7,    sp,    56
+
+    li.w           s0,    8         //w
+    li.w           s1,    8         //h
+    or             s2,    t1,    t1 //edges
+    or             s7,    t0,    t0 //damping
+
+    // cdef_filter_block_kernel
+    li.d           s5,    12         //tmp_stride
+    addi.d         s4,    sp,    64
+    slli.d         t0,    s5,    1
+    addi.d         t0,    t0,    2
+    slli.d         t0,    t0,    1
+    add.d          s4,    s4,    t0  //ptr tmp
+    vxor.v         vr23,  vr23,  vr23
+    li.w           t2,    1
+    vreplgr2vr.h   vr20,  t2
+    vaddi.hu       vr21,  vr20,  2
+    vaddi.hu       vr22,  vr20,  1
+
+    li.w          t0,     -16384
+    vreplgr2vr.h  vr18,   t0
+
+    //padding
+    li.w          t5,     -2        //x_start
+    addi.d        t6,     s0,    2  //x_end
+    li.w          t7,     -2        //y_start
+    addi.d        t8,     s1,    2  //y_end
+    li.w          t2,     2
+
+    andi          t4,     s2,    4
+    bnez          t4,     1f
+
+    //CDEF_HAVE_TOP
+    slli.d        t3,     s5,    2
+    addi.d        t4,     s4,    -4
+    sub.d         t4,     t4,    t3
+    addi.d        t3,     s0,    4
+
+    cdef_fill     t4,     s5,    t3,     t2
+
+    or            t7,     zero,  zero
+
+1:  //CDEF_HAVE_BOTTOM
+    andi          t4,     s2,8
+    bnez          t4,     2f
+
+    mul.w         t3,     s1,    s5
+    slli.d        t3,     t3,  1
+    add.d         t4,     s4,  t3
+    addi.d        t4,     t4,    -4
+    li.d          t3,     12
+
+    cdef_fill     t4,     s5,    t3,    t2
+
+    addi.d        t8,     t8,    -2
+
+2:  //CDEF_HAVE_LEFT
+    andi          t4,     s2,1
+    bnez          t4,     3f
+
+    mul.w         t3,     t7,    s5
+    slli.d        t3,     t3,    1
+    add.d         t4,     s4,    t3
+    addi.d        t4,     t4,    -4
+    sub.d         t3,     t8,    t7
+    li.d          t2,     2
+
+    cdef_fill     t4,     s5,    t2,    t3
+
+    or            t5,     zero,  zero
+
+3:  //CDEF_HAVE_RIGHT
+    andi          t4,     s2,2
+    bnez          t4,     40f
+
+    mul.w         t3,     t7,    s5
+    slli.d        t3,     t3,    1
+    add.d         t4,     s4,    t3
+    addi.d        t4,     t4,    16
+    sub.d         t3,     t8,    t7
+    li.d          t2,     2
+
+    cdef_fill     t4,     s5,    t2,    t3
+
+    addi.d        t6,     t6,    -2
+
+40:
+    cdef_padding_data
+
+    beqz           a5,    33f
+
+28:  //if (pri_strength)
+    li.w           t0,    4
+    andi           t1,    a5,    1
+    sub.d          t0,    t0,    t1  //pri_tap
+
+    //edit
+    clz.w          t1,    a5
+    li.d           t2,    31
+    sub.w          t3,    t2,    t1
+    sub.w          t3,    s7,    t3
+
+    or             t1,    zero,  zero   //t1: pri_shift
+    blt            t3,    zero,  281f
+    or             t1,    t3,    t3
+281:
+
+    beqz           a6,    31f
+
+29:  //if (sec_strength)
+    cdef_pri_sec_init
+
+301:
+    fld.d          f0,    a0,    0     //px
+    vsllwil.hu.bu  vr0,   vr0,   0
+
+    vxor.v         vr1,   vr1,   vr1   //sum
+    vor.v          vr2,   vr0,   vr0   //max
+    vor.v          vr3,   vr0,   vr0   //min
+    vor.v          vr15,  vr4,   vr4   //pri_tap_k
+
+    sub.d          t4,    s4,    a2
+    sub.d          t5,    s4,    a3
+
+    vldx           vr5,  s4,    a2
+    vld            vr6,  t4,    0
+    vldx           vr7,  s4,    a3
+    vld            vr8,  t5,    0
+
+    cdef_process_data_w8 vr9, vr10
+    cdef_calc_sum_tapchange_w8
+    cdef_calc_maxmin_w8
+
+    //s 00-03
+    sub.d          t4,    s4,    s1  //tmp[-off02]
+    sub.d          t5,    s4,    t0  //tmp[-off03]
+
+    vldx           vr5,  s4,    s1
+    vld            vr6,  t4,    0
+    vldx           vr7,  s4,    t0
+    vld            vr8,  t5,    0
+
+    cdef_process_data_w8 vr18, vr19
+    cdef_calc_sum_no_tapchange_w8 vr22
+    cdef_calc_maxmin_w8
+
+    //s 10-13
+    sub.d          t4,    s4,    s2  //tmp[-off12]
+    sub.d          t5,    s4,    s3  //tmp[-off13]
+
+    vldx           vr5,  s4,    s2
+    vld            vr6,  t4,    0
+    vldx           vr7,  s4,    s3
+    vld            vr8,  t5,    0
+
+    cdef_process_data_w8 vr18, vr19
+    cdef_calc_sum_no_tapchange_w8 vr20
+
+    cdef_calc_maxmin_w8
+    cdef_calc_dst
+
+    iclip_vrh       vr5,   vr3,   vr2,  vr16,  vr17,  vr5
+
+    vsrlni.b.h     vr5,   vr5,   0
+    fst.d          f5,    a0,    0
+
+    add.d          a0,    a0,    a1
+    add.d          s4,    s4,    s5
+    add.d          s4,    s4,    s5
+
+    addi.d         t2,    t2,    -1
+    blt            zero,  t2,    301b
+    b              35f
+
+31:  // pri_strength only
+    cdef_pri_init
+
+32:
+    fld.d          f0,    a0,    0     //px
+    vsllwil.hu.bu  vr0,   vr0,   0
+
+    vxor.v         vr1,   vr1,   vr1   //sum
+    vor.v          vr15,  vr4,   vr4   //pri_tap_k
+
+    sub.d          t4,    s4,    a2
+    sub.d          t5,    s4,    a3
+
+    vldx           vr5,  s4,    a2
+    vld            vr6,  t4,    0
+    vldx           vr7,  s4,    a3
+    vld            vr8,  t5,    0
+
+    cdef_process_data_w8 vr9, vr10
+    cdef_calc_sum_tapchange_w8
+    cdef_calc_dst
+
+    vsrlni.b.h     vr5,   vr5,   0
+    fst.d          f5,    a0,    0
+
+    add.d          a0,    a0,    a1
+    add.d          s4,    s4,    s5
+    add.d          s4,    s4,    s5
+
+    addi.d         t2,    t2,    -1
+    blt            zero,  t2,    32b
+    b              35f
+
+33:   // sec_strength only
+    cdef_sec_init
+
+34:
+    fld.d          f0,    a0,    0     //px
+    vsllwil.hu.bu  vr0,   vr0,   0
+
+    vxor.v         vr1,   vr1,   vr1   //sum
+
+    sub.d          t4,    s4,    s1  //tmp[-off02]
+    sub.d          t5,    s4,    t0  //tmp[-off03]
+
+    vldx           vr5,  s4,    s1
+    vld            vr6,  t4,    0
+    vldx           vr7,  s4,    t0
+    vld            vr8,  t5,    0
+
+    cdef_process_data_w8 vr18,  vr19
+    cdef_calc_sum_no_tapchange_w8 vr22
+
+    sub.d          t4,    s4,    s2  //tmp[-off12]
+    sub.d          t5,    s4,    s3  //tmp[-off13]
+
+    vldx           vr5,  s4,    s2
+    vld            vr6,  t4,    0
+    vldx           vr7,  s4,    s3
+    vld            vr8,  t5,    0
+
+    cdef_process_data_w8 vr18,  vr19
+    cdef_calc_sum_no_tapchange_w8 vr20
+    cdef_calc_dst
+
+    vsrlni.b.h     vr5,   vr5,   0
+    fst.d          f5,    a0,    0
+
+    add.d          a0,    a0,    a1
+    add.d          s4,    s4,    s5
+    add.d          s4,    s4,    s5
+
+    addi.d         t2,    t2,    -1
+    blt            zero,  t2,    34b
+
+35:
+    ld.d           s0,    sp,    0
+    ld.d           s1,    sp,    8
+    ld.d           s2,    sp,    16
+    ld.d           s3,    sp,    24
+    ld.d           s4,    sp,    32
+    ld.d           s5,    sp,    40
+    ld.d           s6,    sp,    48
+    ld.d           s7,    sp,    56
+    addi.d         sp,    sp,    (64+288)
+endfunc
+
diff --git a/src/loongarch/cdef.h b/src/loongarch/cdef.h
new file mode 100644
index 0000000..f9e3974
--- /dev/null
+++ b/src/loongarch/cdef.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright © 2024, VideoLAN and dav1d authors
+ * Copyright © 2024, Loongson Technology Corporation Limited
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef DAV1D_SRC_LOONGARCH_CDEF_H
+#define DAV1D_SRC_LOONGARCH_CDEF_H
+
+#include "config.h"
+#include "src/cdef.h"
+#include "src/cpu.h"
+
+decl_cdef_dir_fn(BF(dav1d_cdef_find_dir, lsx));
+decl_cdef_fn(BF(dav1d_cdef_filter_block_4x4, lsx));
+decl_cdef_fn(BF(dav1d_cdef_filter_block_4x8, lsx));
+decl_cdef_fn(BF(dav1d_cdef_filter_block_8x8, lsx));
+
+static ALWAYS_INLINE void cdef_dsp_init_loongarch(Dav1dCdefDSPContext *const c) {
+    const unsigned flags = dav1d_get_cpu_flags();
+
+    if (!(flags & DAV1D_LOONGARCH_CPU_FLAG_LSX)) return;
+
+#if BITDEPTH == 8
+    c->dir = BF(dav1d_cdef_find_dir, lsx);
+    c->fb[0] = BF(dav1d_cdef_filter_block_8x8, lsx);
+    c->fb[1] = BF(dav1d_cdef_filter_block_4x8, lsx);
+    c->fb[2] = BF(dav1d_cdef_filter_block_4x4, lsx);
+#endif
+}
+
+#endif /* DAV1D_SRC_LOONGARCH_CDEF_H */
diff --git a/src/loongarch/cpu.c b/src/loongarch/cpu.c
index a79ade5..383aa01 100644
--- a/src/loongarch/cpu.c
+++ b/src/loongarch/cpu.c
@@ -26,9 +26,11 @@
 
 #include "config.h"
 #include "common/attributes.h"
+
+#include "src/cpu.h"
 #include "src/loongarch/cpu.h"
 
-#if defined(HAVE_GETAUXVAL)
+#if HAVE_GETAUXVAL
 #include <sys/auxv.h>
 
 #define LA_HWCAP_LSX    ( 1 << 4 )
@@ -36,8 +38,8 @@
 #endif
 
 COLD unsigned dav1d_get_cpu_flags_loongarch(void) {
-    unsigned flags = 0;
-#if defined(HAVE_GETAUXVAL)
+    unsigned flags = dav1d_get_default_cpu_flags();
+#if HAVE_GETAUXVAL
     unsigned long hw_cap = getauxval(AT_HWCAP);
     flags |= (hw_cap & LA_HWCAP_LSX) ? DAV1D_LOONGARCH_CPU_FLAG_LSX : 0;
     flags |= (hw_cap & LA_HWCAP_LASX) ? DAV1D_LOONGARCH_CPU_FLAG_LASX : 0;
diff --git a/src/loongarch/ipred.S b/src/loongarch/ipred.S
new file mode 100644
index 0000000..ddc7738
--- /dev/null
+++ b/src/loongarch/ipred.S
@@ -0,0 +1,4260 @@
+/*
+ * Copyright © 2024, VideoLAN and dav1d authors
+ * Copyright © 2024, Loongson Technology Corporation Limited
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "src/loongarch/loongson_asm.S"
+
+.macro ipred_dc_gen topleft, width, height
+    add.d          t0,      \width,  \height //dc
+    srai.d         t0,      t0,      1
+    addi.d         t3,      \topleft,1
+
+    or             t1,      zero,    zero  //data index
+    srai.d         t2,      \width,  4     //loop param
+    beqz           t2,      2f
+
+1:  // width/16
+    vldx           vr0,     t3,      t1
+    vhaddw.hu.bu   vr0,     vr0,     vr0
+    vhaddw.wu.hu   vr0,     vr0,     vr0
+    vhaddw.du.wu   vr0,     vr0,     vr0
+    vhaddw.qu.du   vr0,     vr0,     vr0
+
+    vpickve2gr.du  t4,      vr0,     0
+    add.d          t0,      t0,      t4
+
+    addi.d         t1,      t1,      16
+    addi.d         t2,      t2,      -1
+    bnez           t2,      1b
+    b              4f
+
+2:  // &8
+    andi           t2,      \width,  8
+    beqz           t2,      3f
+
+    vxor.v         vr0,     vr0,     vr0
+    fldx.d         f0,      t3,      t1
+
+    vhaddw.hu.bu   vr0,     vr0,     vr0
+    vhaddw.wu.hu   vr0,     vr0,     vr0
+    vhaddw.du.wu   vr0,     vr0,     vr0
+
+    vpickve2gr.du  t4,      vr0,     0
+    add.d          t0,      t0,      t4
+    addi.d         t1,      t1,      8
+    b              4f
+
+3:  // &4
+    andi           t2,      \width,  4
+    beqz           t2,      4f
+
+    vxor.v         vr0,     vr0,     vr0
+    fldx.s         f0,      t3,      t1
+
+    vhaddw.hu.bu   vr0,     vr0,     vr0
+    vhaddw.wu.hu   vr0,     vr0,     vr0
+
+    vpickve2gr.wu  t4,      vr0,     0
+    add.d          t0,      t0,      t4
+    addi.d         t1,      t1,      4
+
+4:
+    addi.d         t3,      \topleft,0
+    srai.d         t2,      \height, 4     //loop param
+    beqz           t2,      8f
+
+7:  // height/16
+    addi.d         t3,      t3,      -16
+    vld            vr0,     t3,      0
+
+    vhaddw.hu.bu   vr0,     vr0,     vr0
+    vhaddw.wu.hu   vr0,     vr0,     vr0
+    vhaddw.du.wu   vr0,     vr0,     vr0
+    vhaddw.qu.du   vr0,     vr0,     vr0
+
+    vpickve2gr.du  t4,      vr0,     0
+    add.d          t0,      t0,      t4
+
+    addi.d         t2,      t2,      -1
+    bnez           t2,      7b
+    b              10f
+
+8:  // &8
+    andi           t2,      \height, 8
+    beqz           t2,      9f
+
+    addi.d         t3,      t3,      -8
+    vxor.v         vr0,     vr0,     vr0
+    fld.d          f0,      t3,      0
+
+    vhaddw.hu.bu   vr0,     vr0,     vr0
+    vhaddw.wu.hu   vr0,     vr0,     vr0
+    vhaddw.du.wu   vr0,     vr0,     vr0
+
+    vpickve2gr.du  t4,      vr0,     0
+    add.d          t0,      t0,      t4
+    b              10f
+
+9:  // &4
+    andi           t2,      \height, 4
+    beqz           t2,      10f
+
+    addi.d         t3,      t3,      -4
+    vxor.v         vr0,     vr0,     vr0
+    fld.s          f0,      t3,      0
+
+    vhaddw.hu.bu   vr0,     vr0,     vr0
+    vhaddw.wu.hu   vr0,     vr0,     vr0
+
+    vpickve2gr.wu  t4,      vr0,     0
+    add.d          t0,      t0,      t4
+
+10:
+    add.d          t1,      \width,  \height
+    ctz.w          t1,      t1
+    sra.w          t0,      t0,      t1
+
+    // w != h
+    beq            \width,  \height, 16f
+    add.d          t2,      \height, \height
+    add.d          t3,      \width,  \width
+    slt            t2,      t2,      \width
+    slt            t3,      t3,      \height
+    or             t2,      t2,      t3
+    li.w           t3,      0x3334
+    maskeqz        t1,      t3,      t2
+    li.w           t3,      0x5556
+    masknez        t2,      t3,      t2
+    or             t1,      t1,      t2
+    mul.w          t0,      t0,      t1
+    srai.w         t0,      t0,      16
+
+16:
+.endm
+
+.macro ipred_splat_dc dst, stride, width, height, dc
+    li.w           t1,      4
+    blt            t1,      \width,  2f
+
+    li.w           t1,      0x01010101
+    mulw.d.wu      t1,      \dc,     t1
+    beqz           \height, 7f
+    or             t2,      \dst,    \dst
+1:  // width <= 4
+    st.w           t1,      t2,      0
+    add.d          t2,      t2,      \stride
+    addi.d         \height, \height, -1
+    bnez           \height, 1b
+    b              7f
+
+2:  //width > 4
+    li.d           t1,      0x0101010101010101
+    mul.d          t1,      \dc,     t1
+    vreplgr2vr.d   vr0,     t1
+    or             t4,      \dst,    \dst
+    beqz           \height, 7f
+
+3:
+    andi           t5,      \width,  64
+    beqz           t5,      4f
+    vst            vr0,     t4,      0
+    vst            vr0,     t4,      16
+    vst            vr0,     t4,      32
+    vst            vr0,     t4,      48
+    b              6f
+
+4:
+    andi           t5,      \width,  32
+    beqz           t5,      41f
+    vst            vr0,     t4,      0
+    vst            vr0,     t4,      16
+    b              6f
+
+41:
+    andi           t5,      \width,  16
+    beqz           t5,      5f
+    vst            vr0,     t4,      0
+    b              6f
+
+5:
+    fst.d          f0,      t4,      0
+
+6:
+    add.d          t4,      t4,      \stride
+    addi.d         \height, \height, -1
+    bnez           \height, 3b
+
+7:
+.endm
+
+.macro ipred_dc_gen_top topleft, width
+    srai.d         t0,      \width,  1
+    addi.d         t1,      \topleft,1
+
+    srai.d         t2,      \width,  4
+    beqz           t2,      2f
+1:
+    vld            vr0,     t1,      0
+    vhaddw.hu.bu   vr0,     vr0,     vr0
+    vhaddw.wu.hu   vr0,     vr0,     vr0
+    vhaddw.du.wu   vr0,     vr0,     vr0
+    vhaddw.qu.du   vr0,     vr0,     vr0
+
+    vpickve2gr.du  t3,      vr0,     0
+    add.d          t0,      t0,      t3
+
+    addi.d         t1,      t1,      16
+    addi.d         t2,      t2,      -1
+    bnez           t2,      1b
+    b              4f
+
+2:  // &8
+    andi           t2,      \width,  8
+    beqz           t2,      3f
+
+    vxor.v         vr0,     vr0,     vr0
+    fld.d          f0,      t1,      0
+
+    vhaddw.hu.bu   vr0,     vr0,     vr0
+    vhaddw.wu.hu   vr0,     vr0,     vr0
+    vhaddw.du.wu   vr0,     vr0,     vr0
+
+    vpickve2gr.du  t2,      vr0,     0
+    add.d          t0,      t0,      t2
+
+    addi.d         t1,      t1,      8
+    b              4f
+
+3:  // &4
+    andi           t2,      \width,  4
+    beqz           t2,      4f
+
+    vxor.v         vr0,     vr0,     vr0
+    fld.s          f0,      t1,      0
+
+    vhaddw.hu.bu   vr0,     vr0,     vr0
+    vhaddw.wu.hu   vr0,     vr0,     vr0
+
+    vpickve2gr.du  t2,      vr0,     0
+    add.d          t0,      t0,      t2
+    addi.d         t1,      t1,      4
+
+4:
+    ctz.w          t1,      \width
+    sra.w          t0,      t0,      t1
+.endm
+
+.macro ipred_dc_gen_left topleft, height
+    srai.d         t0,      \height, 1
+    srai.d         t2,      \height, 4     //loop param
+    beqz           t2,      8f
+
+7:  // height/16
+    addi.d         \topleft,\topleft,-16
+    vld            vr0,     \topleft,0
+
+    vhaddw.hu.bu   vr0,     vr0,     vr0
+    vhaddw.wu.hu   vr0,     vr0,     vr0
+    vhaddw.du.wu   vr0,     vr0,     vr0
+    vhaddw.qu.du   vr0,     vr0,     vr0
+
+    vpickve2gr.du  t4,      vr0,     0
+    add.d          t0,      t0,      t4
+
+    addi.d         t2,      t2,      -1
+    bnez           t2,      7b
+    b              10f
+
+8:  // &8
+    andi           t2,      \height, 8
+    beqz           t2,      9f
+
+    addi.d         \topleft,\topleft,-8
+    vxor.v         vr0,     vr0,     vr0
+    fld.d          f0,      \topleft,0
+
+    vhaddw.hu.bu   vr0,     vr0,     vr0
+    vhaddw.wu.hu   vr0,     vr0,     vr0
+    vhaddw.du.wu   vr0,     vr0,     vr0
+
+    vpickve2gr.du  t4,      vr0,     0
+    add.d          t0,      t0,      t4
+    b              10f
+
+9:  // &4
+    andi           t2,      \height, 4
+    beqz           t2,      10f
+
+    addi.d         \topleft,\topleft,-4
+    vxor.v         vr0,     vr0,     vr0
+    fld.s          f0,      \topleft,0
+
+    vhaddw.hu.bu   vr0,     vr0,     vr0
+    vhaddw.wu.hu   vr0,     vr0,     vr0
+
+    vpickve2gr.wu  t4,      vr0,     0
+    add.d          t0,      t0,      t4
+
+10:
+    ctz.w          t1,      \height
+    sra.w          t0,      t0,      t1
+
+.endm
+
+// void ipred_dc_lsx(pixel *dst, const ptrdiff_t stride,
+//                   const pixel *const topleft,
+//                   const int width, const int height, const int a,
+//                   const int max_width, const int max_height
+//                   HIGHBD_DECL_SUFFIX)
+function ipred_dc_8bpc_lsx
+    ipred_dc_gen   a2, a3, a4
+    ipred_splat_dc a0, a1, a3, a4, t0
+
+endfunc
+
+// void ipred_dc_128_lsx(pixel *dst, const ptrdiff_t stride,
+//                       const pixel *const topleft,
+//                       const int width, const int height, const int a,
+//                       const int max_width, const int max_height
+//                       HIGHBD_DECL_SUFFIX)
+function ipred_dc_128_8bpc_lsx
+    li.w           t0,      128
+    ipred_splat_dc a0, a1, a3, a4, t0
+
+endfunc
+
+// void ipred_dc_top_c(pixel *dst, const ptrdiff_t stride,
+//                     const pixel *const topleft,
+//                     const int width, const int height, const int a,
+//                     const int max_width, const int max_height
+//                     HIGHBD_DECL_SUFFIX)
+function ipred_dc_top_8bpc_lsx
+    ipred_dc_gen_top a2, a3
+    ipred_splat_dc   a0, a1, a3, a4, t0
+
+endfunc
+
+// void ipred_dc_left_c(pixel *dst, const ptrdiff_t stride,
+//                      const pixel *const topleft,
+//                      const int width, const int height, const int a,
+//                      const int max_width, const int max_height
+//                      HIGHBD_DECL_SUFFIX)
+function ipred_dc_left_8bpc_lsx
+    ipred_dc_gen_left a2, a4
+    ipred_splat_dc    a0, a1, a3, a4, t0
+
+endfunc
+
+.macro pixel_set_8bpc dst_ptr, src_ptr, width
+    vldrepl.b      vr0,     \src_ptr, 0
+1:
+    andi           a5,      \width,   64
+    beqz           a5,      2f
+
+    vst            vr0,     \dst_ptr, 0
+    vst            vr0,     \dst_ptr, 16
+    vst            vr0,     \dst_ptr, 32
+    vst            vr0,     \dst_ptr, 48
+    b              6f
+2:
+    andi           a5,      \width,   32
+    beqz           a5,      3f
+
+    vst            vr0,     \dst_ptr, 0
+    vst            vr0,     \dst_ptr, 16
+    b              6f
+3:
+    andi           a5,      \width,   16
+    beqz           a5,      4f
+
+    vst            vr0,     \dst_ptr, 0
+    b              6f
+4:
+    andi           a5,      \width,   8
+    beqz           a5,      5f
+
+    fst.d          f0,      \dst_ptr, 0
+    b              6f
+5:
+    andi           a5,      \width,   4
+    beqz           a5,      6f
+
+    fst.s          f0,      \dst_ptr, 0
+6:
+.endm
+
+// void ipred_h_c(pixel *dst, const ptrdiff_t stride,
+//                const pixel *const topleft,
+//                const int width, const int height, const int a,
+//                const int max_width, const int max_height
+//                HIGHBD_DECL_SUFFIX)
+function ipred_h_8bpc_lsx
+    beqz           a4,      .IPRED_H_END
+.IPRED_H_LOOP:
+    addi.d         a2,      a2,      -1
+
+    pixel_set_8bpc a0, a2, a3
+
+    add.d          a0,      a0,      a1
+    addi.d         a4,      a4,      -1
+    bnez           a4,      .IPRED_H_LOOP
+
+.IPRED_H_END:
+endfunc
+
+.macro pixel_copy_8bpc dst_ptr, src_ptr, width
+1:
+    andi           a5,      \width,   64
+    beqz           a5,      2f
+
+    vld            vr0,     \src_ptr, 0
+    vld            vr1,     \src_ptr, 16
+    vld            vr2,     \src_ptr, 32
+    vld            vr3,     \src_ptr, 48
+
+    vst            vr0,     \dst_ptr, 0
+    vst            vr1,     \dst_ptr, 16
+    vst            vr2,     \dst_ptr, 32
+    vst            vr3,     \dst_ptr, 48
+
+    b              6f
+2:
+    andi           a5,      \width,   32
+    beqz           a5,      3f
+
+    vld            vr0,     \src_ptr, 0
+    vld            vr1,     \src_ptr, 16
+
+    vst            vr0,     \dst_ptr, 0
+    vst            vr1,     \dst_ptr, 16
+
+    b              6f
+3:
+    andi           a5,      \width,   16
+    beqz           a5,      4f
+
+    vld            vr0,     \src_ptr, 0
+    vst            vr0,     \dst_ptr, 0
+
+    b              6f
+4:
+    andi           a5,      \width,   8
+    beqz           a5,      5f
+
+    fld.d          f0,      \src_ptr, 0
+    fst.d          f0,      \dst_ptr, 0
+
+    b              6f
+5:
+    andi           a5,      \width,   4
+    beqz           a5,      6f
+
+    fld.s          f0,      \src_ptr, 0
+    fst.s          f0,      \dst_ptr, 0
+6:
+.endm
+
+// void ipred_v_lsx(pixel *dst, const ptrdiff_t stride,
+//                  const pixel *const topleft,
+//                  const int width, const int height, const int a,
+//                  const int max_width, const int max_height
+//                  HIGHBD_DECL_SUFFIX)
+function ipred_v_8bpc_lsx
+    beqz           a4,      .IPRED_V_END
+    addi.d         a2,      a2,      1
+.IPRED_V_LOOP:
+    pixel_copy_8bpc  a0, a2, a3
+
+    add.d          a0,      a0,      a1
+    addi.d         a4,      a4,      -1
+    bnez           a4,      .IPRED_V_LOOP
+
+.IPRED_V_END:
+endfunc
+
+// void ipred_paeth_lsx(pixel *dst, const ptrdiff_t stride,
+//                      const pixel *const tl_ptr,
+//                      const int width, const int height, const int a,
+//                      const int max_width, const int max_height
+//                      HIGHBD_DECL_SUFFIX)
+function ipred_paeth_8bpc_lsx
+    vldrepl.b      vr0,     a2,      0    //topleft
+    vsllwil.hu.bu  vr0,     vr0,     0
+    or             a6,      a2,      a2
+    addi.d         a7,      a2,      1
+
+.IPRED_PAETH_H_LOOP:
+    addi.d         a6,      a6,      -1
+    vldrepl.b      vr1,     a6,      0   //left
+    vsllwil.hu.bu  vr1,     vr1,     0
+
+.IPRED_PAETH_W_LOOP64:
+    andi           a5,      a3,      64
+    beqz           a5,      .IPRED_PAETH_W_LOOP32
+
+    vld            vr2,     a7,      0   //top
+    vpermi.w       vr9,     vr2,     0x0e
+    vsllwil.hu.bu  vr2,     vr2,     0
+    vsllwil.hu.bu  vr9,     vr9,     0
+
+    vabsd.hu       vr5,     vr0,     vr1  //tdiff
+    vabsd.hu       vr4,     vr0,     vr2  //ldiff
+    vabsd.hu       vr10,    vr0,     vr9
+
+    vadd.h         vr3,     vr0,     vr0
+    vadd.h         vr6,     vr1,     vr2
+    vadd.h         vr11,    vr1,     vr9
+    vabsd.hu       vr6,     vr3,     vr6  //tldiff
+    vabsd.hu       vr11,    vr3,     vr11 //tldiff
+
+    vsle.hu        vr3,     vr5,     vr6
+    vbitsel.v      vr7,     vr0,     vr2,    vr3
+    vsle.hu        vr3,     vr4,     vr5
+    vsle.hu        vr8,     vr4,     vr6
+    vand.v         vr3,     vr3,     vr8
+    vbitsel.v      vr3,     vr7,     vr1,    vr3
+    vsrlni.b.h     vr3,     vr3,     0
+
+    vsle.hu        vr12,    vr5,     vr11
+    vbitsel.v      vr7,     vr0,     vr9,    vr12
+    vsle.hu        vr12,    vr10,    vr5
+    vsle.hu        vr8,     vr10,    vr11
+    vand.v         vr12,    vr12,    vr8
+    vbitsel.v      vr12,    vr7,     vr1,    vr12
+    vsrlni.b.h     vr12,    vr12,    0
+
+    vpermi.w       vr12,    vr3,     0x44
+
+    vst            vr12,    a0,      0
+
+    vld            vr2,     a7,      16   //top
+    vpermi.w       vr9,     vr2,     0x0e
+    vsllwil.hu.bu  vr2,     vr2,     0
+    vsllwil.hu.bu  vr9,     vr9,     0
+
+    vabsd.hu       vr5,     vr0,     vr1  //tdiff
+    vabsd.hu       vr4,     vr0,     vr2  //ldiff
+    vabsd.hu       vr10,    vr0,     vr9
+
+    vadd.h         vr3,     vr0,     vr0
+    vadd.h         vr6,     vr1,     vr2
+    vadd.h         vr11,    vr1,     vr9
+    vabsd.hu       vr6,     vr3,     vr6  //tldiff
+    vabsd.hu       vr11,    vr3,     vr11 //tldiff
+
+    vsle.hu        vr3,     vr5,     vr6
+    vbitsel.v      vr7,     vr0,     vr2,    vr3
+    vsle.hu        vr3,     vr4,     vr5
+    vsle.hu        vr8,     vr4,     vr6
+    vand.v         vr3,     vr3,     vr8
+    vbitsel.v      vr3,     vr7,     vr1,    vr3
+    vsrlni.b.h     vr3,     vr3,     0
+
+    vsle.hu        vr12,    vr5,     vr11
+    vbitsel.v      vr7,     vr0,     vr9,    vr12
+    vsle.hu        vr12,    vr10,    vr5
+    vsle.hu        vr8,     vr10,    vr11
+    vand.v         vr12,    vr12,    vr8
+    vbitsel.v      vr12,    vr7,     vr1,    vr12
+    vsrlni.b.h     vr12,    vr12,    0
+
+    vpermi.w       vr12,    vr3,     0x44
+
+    vst            vr12,    a0,      16
+
+    vld            vr2,     a7,      32   //top
+    vpermi.w       vr9,     vr2,     0x0e
+    vsllwil.hu.bu  vr2,     vr2,     0
+    vsllwil.hu.bu  vr9,     vr9,     0
+
+    vabsd.hu       vr5,     vr0,     vr1  //tdiff
+    vabsd.hu       vr4,     vr0,     vr2  //ldiff
+    vabsd.hu       vr10,    vr0,     vr9
+
+    vadd.h         vr3,     vr0,     vr0
+    vadd.h         vr6,     vr1,     vr2
+    vadd.h         vr11,    vr1,     vr9
+    vabsd.hu       vr6,     vr3,     vr6  //tldiff
+    vabsd.hu       vr11,    vr3,     vr11 //tldiff
+
+    vsle.hu        vr3,     vr5,     vr6
+    vbitsel.v      vr7,     vr0,     vr2,    vr3
+    vsle.hu        vr3,     vr4,     vr5
+    vsle.hu        vr8,     vr4,     vr6
+    vand.v         vr3,     vr3,     vr8
+    vbitsel.v      vr3,     vr7,     vr1,    vr3
+    vsrlni.b.h     vr3,     vr3,     0
+
+    vsle.hu        vr12,    vr5,     vr11
+    vbitsel.v      vr7,     vr0,     vr9,    vr12
+    vsle.hu        vr12,    vr10,    vr5
+    vsle.hu        vr8,     vr10,    vr11
+    vand.v         vr12,    vr12,    vr8
+    vbitsel.v      vr12,    vr7,     vr1,    vr12
+    vsrlni.b.h     vr12,    vr12,    0
+
+    vpermi.w       vr12,    vr3,     0x44
+
+    vst            vr12,    a0,      32
+
+    vld            vr2,     a7,      48   //top
+    vpermi.w       vr9,     vr2,     0x0e
+    vsllwil.hu.bu  vr2,     vr2,     0
+    vsllwil.hu.bu  vr9,     vr9,     0
+
+    vabsd.hu       vr5,     vr0,     vr1  //tdiff
+    vabsd.hu       vr4,     vr0,     vr2  //ldiff
+    vabsd.hu       vr10,    vr0,     vr9
+
+    vadd.h         vr3,     vr0,     vr0
+    vadd.h         vr6,     vr1,     vr2
+    vadd.h         vr11,    vr1,     vr9
+    vabsd.hu       vr6,     vr3,     vr6  //tldiff
+    vabsd.hu       vr11,    vr3,     vr11 //tldiff
+
+    vsle.hu        vr3,     vr5,     vr6
+    vbitsel.v      vr7,     vr0,     vr2,    vr3
+    vsle.hu        vr3,     vr4,     vr5
+    vsle.hu        vr8,     vr4,     vr6
+    vand.v         vr3,     vr3,     vr8
+    vbitsel.v      vr3,     vr7,     vr1,    vr3
+    vsrlni.b.h     vr3,     vr3,     0
+
+    vsle.hu        vr12,    vr5,     vr11
+    vbitsel.v      vr7,     vr0,     vr9,    vr12
+    vsle.hu        vr12,    vr10,    vr5
+    vsle.hu        vr8,     vr10,    vr11
+    vand.v         vr12,    vr12,    vr8
+    vbitsel.v      vr12,    vr7,     vr1,    vr12
+    vsrlni.b.h     vr12,    vr12,    0
+
+    vpermi.w       vr12,    vr3,     0x44
+
+    vst            vr12,    a0,      48
+
+    b              .IPRED_PAETH_W_LOOPEND
+
+.IPRED_PAETH_W_LOOP32:
+    andi           a5,      a3,      32
+    beqz           a5,      .IPRED_PAETH_W_LOOP16
+
+    vld            vr2,     a7,      0   //top
+    vpermi.w       vr9,     vr2,     0x0e
+    vsllwil.hu.bu  vr2,     vr2,     0
+    vsllwil.hu.bu  vr9,     vr9,     0
+
+    vabsd.hu       vr5,     vr0,     vr1  //tdiff
+    vabsd.hu       vr4,     vr0,     vr2  //ldiff
+    vabsd.hu       vr10,    vr0,     vr9
+
+    vadd.h         vr3,     vr0,     vr0
+    vadd.h         vr6,     vr1,     vr2
+    vadd.h         vr11,    vr1,     vr9
+    vabsd.hu       vr6,     vr3,     vr6  //tldiff
+    vabsd.hu       vr11,    vr3,     vr11 //tldiff
+
+    vsle.hu        vr3,     vr5,     vr6
+    vbitsel.v      vr7,     vr0,     vr2,    vr3
+    vsle.hu        vr3,     vr4,     vr5
+    vsle.hu        vr8,     vr4,     vr6
+    vand.v         vr3,     vr3,     vr8
+    vbitsel.v      vr3,     vr7,     vr1,    vr3
+    vsrlni.b.h     vr3,     vr3,     0
+
+    vsle.hu        vr12,    vr5,     vr11
+    vbitsel.v      vr7,     vr0,     vr9,    vr12
+    vsle.hu        vr12,    vr10,    vr5
+    vsle.hu        vr8,     vr10,    vr11
+    vand.v         vr12,    vr12,    vr8
+    vbitsel.v      vr12,    vr7,     vr1,    vr12
+    vsrlni.b.h     vr12,    vr12,    0
+
+    vpermi.w       vr12,    vr3,     0x44
+
+    vst            vr12,    a0,      0
+
+    vld            vr2,     a7,      16   //top
+    vpermi.w       vr9,     vr2,     0x0e
+    vsllwil.hu.bu  vr2,     vr2,     0
+    vsllwil.hu.bu  vr9,     vr9,     0
+
+    vabsd.hu       vr5,     vr0,     vr1  //tdiff
+    vabsd.hu       vr4,     vr0,     vr2  //ldiff
+    vabsd.hu       vr10,    vr0,     vr9
+
+    vadd.h         vr3,     vr0,     vr0
+    vadd.h         vr6,     vr1,     vr2
+    vadd.h         vr11,    vr1,     vr9
+    vabsd.hu       vr6,     vr3,     vr6  //tldiff
+    vabsd.hu       vr11,    vr3,     vr11 //tldiff
+
+    vsle.hu        vr3,     vr5,     vr6
+    vbitsel.v      vr7,     vr0,     vr2,    vr3
+    vsle.hu        vr3,     vr4,     vr5
+    vsle.hu        vr8,     vr4,     vr6
+    vand.v         vr3,     vr3,     vr8
+    vbitsel.v      vr3,     vr7,     vr1,    vr3
+    vsrlni.b.h     vr3,     vr3,     0
+
+    vsle.hu        vr12,    vr5,     vr11
+    vbitsel.v      vr7,     vr0,     vr9,    vr12
+    vsle.hu        vr12,    vr10,    vr5
+    vsle.hu        vr8,     vr10,    vr11
+    vand.v         vr12,    vr12,    vr8
+    vbitsel.v      vr12,    vr7,     vr1,    vr12
+    vsrlni.b.h     vr12,    vr12,    0
+
+    vpermi.w       vr12,    vr3,     0x44
+
+    vst            vr12,    a0,      16
+
+    b              .IPRED_PAETH_W_LOOPEND
+
+.IPRED_PAETH_W_LOOP16:
+    andi           a5,      a3,      16
+    beqz           a5,      .IPRED_PAETH_W_LOOP8
+
+    vld            vr2,     a7,      0   //top
+    vpermi.w       vr9,     vr2,     0x0e
+    vsllwil.hu.bu  vr2,     vr2,     0
+    vsllwil.hu.bu  vr9,     vr9,     0
+
+    vabsd.hu       vr5,     vr0,     vr1  //tdiff
+    vabsd.hu       vr4,     vr0,     vr2  //ldiff
+    vabsd.hu       vr10,    vr0,     vr9
+
+    vadd.h         vr3,     vr0,     vr0
+    vadd.h         vr6,     vr1,     vr2
+    vadd.h         vr11,    vr1,     vr9
+    vabsd.hu       vr6,     vr3,     vr6  //tldiff
+    vabsd.hu       vr11,    vr3,     vr11 //tldiff
+
+    vsle.hu        vr3,     vr5,     vr6
+    vbitsel.v      vr7,     vr0,     vr2,    vr3
+    vsle.hu        vr3,     vr4,     vr5
+    vsle.hu        vr8,     vr4,     vr6
+    vand.v         vr3,     vr3,     vr8
+    vbitsel.v      vr3,     vr7,     vr1,    vr3
+    vsrlni.b.h     vr3,     vr3,     0
+
+    vsle.hu        vr12,    vr5,     vr11
+    vbitsel.v      vr7,     vr0,     vr9,    vr12
+    vsle.hu        vr12,    vr10,    vr5
+    vsle.hu        vr8,     vr10,    vr11
+    vand.v         vr12,    vr12,    vr8
+    vbitsel.v      vr12,    vr7,     vr1,    vr12
+    vsrlni.b.h     vr12,    vr12,    0
+
+    vpermi.w       vr12,    vr3,     0x44
+
+    vst            vr12,    a0,      0
+
+    b              .IPRED_PAETH_W_LOOPEND
+
+.IPRED_PAETH_W_LOOP8:
+    andi           a5,      a3,      8
+    beqz           a5,      .IPRED_PAETH_W_LOOP4
+
+    fld.d          f2,      a7,      0   //top
+    vsllwil.hu.bu  vr2,     vr2,     0
+
+    vabsd.hu       vr5,     vr0,     vr1  //tdiff
+    vabsd.hu       vr4,     vr0,     vr2  //ldiff
+
+    vadd.h         vr3,     vr0,     vr0
+    vadd.h         vr6,     vr1,     vr2
+    vabsd.hu       vr6,     vr3,     vr6 //tldiff
+
+    vsle.hu        vr3,     vr5,     vr6
+    vbitsel.v      vr7,     vr0,     vr2,    vr3
+    vsle.hu        vr3,     vr4,     vr5
+    vsle.hu        vr8,     vr4,     vr6
+    vand.v         vr3,     vr3,     vr8
+    vbitsel.v      vr3,     vr7,     vr1,    vr3
+    vsrlni.b.h     vr3,     vr3,     0
+    fst.d          f3,      a0,      0
+
+    b              .IPRED_PAETH_W_LOOPEND
+
+.IPRED_PAETH_W_LOOP4:
+    andi           a5,      a3,      4
+    beqz           a5,      .IPRED_PAETH_W_LOOPEND
+
+    fld.s          f2,      a7,      0   //top
+    vsllwil.hu.bu  vr2,     vr2,     0
+
+    vabsd.hu       vr5,     vr0,     vr1  //tdiff
+    vabsd.hu       vr4,     vr0,     vr2  //ldiff
+
+    vadd.h         vr3,     vr0,     vr0
+    vadd.h         vr6,     vr1,     vr2
+    vabsd.hu       vr6,     vr3,     vr6 //tldiff
+
+    vsle.hu        vr3,     vr5,     vr6
+    vbitsel.v      vr7,     vr0,     vr2,    vr3
+    vsle.hu        vr3,     vr4,     vr5
+    vsle.hu        vr8,     vr4,     vr6
+    vand.v         vr3,     vr3,     vr8
+    vbitsel.v      vr3,     vr7,     vr1,    vr3
+    vsrlni.b.h     vr3,     vr3,     0
+    fst.s          f3,      a0,      0
+
+    b              .IPRED_PAETH_W_LOOPEND
+
+.IPRED_PAETH_W_LOOPEND:
+    add.d         a0,       a0,      a1
+    addi.d        a4,       a4,      -1
+    bnez          a4,       .IPRED_PAETH_H_LOOP
+endfunc
+
+const dav1d_sm_weights
+    .byte  0,   0
+    // bs = 2
+    .byte  255, 128
+    // bs = 4
+    .byte  255, 149,  85,  64
+    // bs = 8
+    .byte  255, 197, 146, 105,  73,  50,  37,  32
+    // bs = 16
+    .byte  255, 225, 196, 170, 145, 123, 102,  84
+    .byte  68,  54,  43,  33,  26,  20,  17,  16
+    // bs = 32
+    .byte  255, 240, 225, 210, 196, 182, 169, 157
+    .byte  145, 133, 122, 111, 101,  92,  83,  74
+    .byte  66,  59,  52,  45,  39,  34,  29,  25
+    .byte  21,  17,  14,  12,  10,   9,   8,   8
+    // bs = 64
+    .byte  255, 248, 240, 233, 225, 218, 210, 203
+    .byte  196, 189, 182, 176, 169, 163, 156, 150
+    .byte  144, 138, 133, 127, 121, 116, 111, 106
+    .byte  101,  96,  91,  86,  82,  77,  73,  69
+    .byte  65,  61,  57,  54,  50,  47,  44,  41
+    .byte  38,  35,  32,  29,  27,  25,  22,  20
+    .byte  18,  16,  15,  13,  12,  10,   9,   8
+    .byte  7,   6,   6,   5,   5,   4,   4,   4
+endconst
+
+// void ipred_smooth_lsx(pixel *dst, const ptrdiff_t stride,
+//                       const pixel *const topleft,
+//                       const int width, const int height, const int a,
+//                       const int max_width, const int max_height
+//                       HIGHBD_DECL_SUFFIX)
+function ipred_smooth_8bpc_lsx
+    la.local       a5,      dav1d_sm_weights
+    add.d          a6,      a5,      a3  //hor
+    add.d          a5,      a5,      a4  //ver
+
+    add.d          a7,      a2,      a3
+    sub.d          t0,      a2,      a4
+
+    vldrepl.b      vr0,     a7,      0  //right
+    vldrepl.b      vr1,     t0,      0  //bottom
+
+    vsllwil.hu.bu  vr0,     vr0,     0
+    vsllwil.wu.hu  vr0,     vr0,     0
+    vsllwil.hu.bu  vr1,     vr1,     0
+    vsllwil.wu.hu  vr1,     vr1,     0
+
+    li.w           t0,      256
+    vreplgr2vr.w   vr6,     t0
+
+    addi.d         t0,      a2,      1   //ptr topleft[x]
+    addi.d         t3,      a2,      -1  //ptr topleft[y]
+
+.IPRED_SMOOTH_H_LOOP:
+    vldrepl.b      vr2,     a5,      0  //ver[y]
+    vldrepl.b      vr3,     t3,      0  //topleft[y]
+
+    vsllwil.hu.bu  vr2,     vr2,     0
+    vsllwil.wu.hu  vr2,     vr2,     0
+    vsllwil.hu.bu  vr3,     vr3,     0
+    vsllwil.wu.hu  vr3,     vr3,     0
+
+    vsub.w         vr7,     vr6,     vr2  //256-ver[y]
+
+    or             t1,      zero,    zero  //xx
+    srai.d         t2,      a3,      2     //loop max
+
+.IPRED_SMOOTH_W_LOOP:
+    fldx.s         f4,      t0,      t1   //topleft[x]
+    fldx.s         f5,      a6,      t1   //hor[x]
+
+    vsllwil.hu.bu  vr4,     vr4,     0
+    vsllwil.wu.hu  vr4,     vr4,     0
+    vsllwil.hu.bu  vr5,     vr5,     0
+    vsllwil.wu.hu  vr5,     vr5,     0
+
+    vsub.w         vr8,     vr6,     vr5  //256-hor[x]
+
+    vmul.w         vr9,     vr8,     vr0
+    vmadd.w        vr9,     vr5,     vr3
+    vmadd.w        vr9,     vr7,     vr1
+    vmadd.w        vr9,     vr2,     vr4  //pred
+
+    vadd.w         vr9,     vr9,     vr6
+    vsrlni.h.w     vr9,     vr9,     9
+    vsrlni.b.h     vr9,     vr9,     0
+
+    fstx.s         f9,      a0,      t1
+
+    addi.d         t1,      t1,      4
+    addi.d         t2,      t2,      -1
+    bnez           t2,      .IPRED_SMOOTH_W_LOOP
+
+.IPRED_SMOOTH_W_LOOP_END:
+    addi.d         t3,      t3,      -1
+    addi.d         a5,      a5,      1
+    add.d          a0,      a0,      a1
+    addi.d         a4,      a4,      -1
+    bnez           a4,      .IPRED_SMOOTH_H_LOOP
+
+endfunc
+
+// void ipred_smooth_v_lsx(pixel *dst, const ptrdiff_t stride,
+//                         const pixel *const topleft,
+//                         const int width, const int height, const int a,
+//                         const int max_width, const int max_height
+//                         HIGHBD_DECL_SUFFIX)
+function ipred_smooth_v_8bpc_lsx
+    la.local       a5,      dav1d_sm_weights
+    add.d          a5,      a5,      a4  //ver
+
+    sub.d          t0,      a2,      a4
+    vldrepl.b      vr0,     t0,      0  //bottom
+    vsllwil.hu.bu  vr0,     vr0,     0
+
+    li.w           t0,      256
+    vreplgr2vr.h   vr2,     t0
+    li.w           t0,      128
+    vreplgr2vr.h   vr3,     t0
+
+    addi.d         t0,      a2,      1   //ptr topleft[x]
+
+.IPRED_SMOOTH_V_H_LOOP:
+    vldrepl.b      vr1,     a5,      0  //ver[y]
+    vsllwil.hu.bu  vr1,     vr1,     0
+    vsub.h         vr5,     vr2,     vr1  //256-ver[y]
+
+    or             t1,      zero,    zero  //xx
+    srai.d         t2,      a3,      3     //loop max
+    beqz           t2,      .IPRED_SMOOTH_V_W_LOOP4
+
+.IPRED_SMOOTH_V_W_LOOP8:
+    fldx.d         f4,      t0,      t1   //topleft[x]
+    vsllwil.hu.bu  vr4,     vr4,     0
+
+    vmul.h         vr6,     vr5,     vr0
+    vmadd.h        vr6,     vr1,     vr4  //pred
+    vadd.h         vr6,     vr6,     vr3
+    vsrlni.b.h     vr6,     vr6,     8
+
+    fstx.d         f6,      a0,      t1
+
+    addi.d         t1,      t1,      8
+    addi.d         t2,      t2,      -1
+    bnez           t2,      .IPRED_SMOOTH_V_W_LOOP8
+    b              .IPRED_SMOOTH_V_W_LOOP_END
+
+.IPRED_SMOOTH_V_W_LOOP4:
+    fldx.s         f4,      t0,      t1   //topleft[x]
+    vsllwil.hu.bu  vr4,     vr4,     0
+
+    vmul.h         vr6,     vr5,     vr0
+    vmadd.h        vr6,     vr1,     vr4  //pred
+    vadd.h         vr6,     vr6,     vr3
+    vsrai.h        vr6,     vr6,     8
+    vsrlni.b.h     vr6,     vr6,     0
+
+    fstx.s         f6,      a0,      t1
+
+    addi.d         t1,      t1,      4
+
+.IPRED_SMOOTH_V_W_LOOP_END:
+    addi.d         a5,      a5,      1
+    add.d          a0,      a0,      a1
+    addi.d         a4,      a4,      -1
+    bnez           a4,      .IPRED_SMOOTH_V_H_LOOP
+
+endfunc
+
+// void ipred_smooth_h_lsx(pixel *dst, const ptrdiff_t stride,
+//                         const pixel *const topleft,
+//                         const int width, const int height, const int a,
+//                         const int max_width, const int max_height
+//                         HIGHBD_DECL_SUFFIX)
+function ipred_smooth_h_8bpc_lsx
+    la.local       a5,      dav1d_sm_weights
+    add.d          a6,      a5,      a3  //hor
+
+    add.d          a7,      a2,      a3
+    vldrepl.b      vr0,     a7,      0  //right
+    vsllwil.hu.bu  vr0,     vr0,     0
+
+    li.w           t0,      256
+    vreplgr2vr.h   vr1,     t0
+    li.w           t0,      128
+    vreplgr2vr.h   vr2,     t0
+
+    addi.d         t3,      a2,      -1  //ptr topleft[y]
+
+.IPRED_SMOOTH_H_H_LOOP:
+    vldrepl.b      vr3,     t3,      0  //topleft[y]
+    vsllwil.hu.bu  vr3,     vr3,     0
+
+    or             t1,      zero,    zero  //xx
+    srai.d         t2,      a3,      3     //loop max
+    beqz           t2,      .IPRED_SMOOTH_H_W_LOOP4
+
+.IPRED_SMOOTH_H_W_LOOP8:
+    fldx.d         f5,      a6,      t1   //hor[x]
+    vsllwil.hu.bu  vr5,     vr5,     0
+    vsub.h         vr4,     vr1,     vr5  //256-hor[x]
+
+    vmul.h         vr6,     vr4,     vr0
+    vmadd.h        vr6,     vr5,     vr3  //pred
+    vadd.h         vr6,     vr6,     vr2
+    vsrlni.b.h     vr6,     vr6,     8
+
+    fstx.d         f6,      a0,      t1
+
+    addi.d         t1,      t1,      8
+    addi.d         t2,      t2,      -1
+    bnez           t2,      .IPRED_SMOOTH_H_W_LOOP8
+    b              .IPRED_SMOOTH_W_H_LOOP_END
+
+.IPRED_SMOOTH_H_W_LOOP4:
+    fldx.s         f5,      a6,      t1   //hor[x]
+    vsllwil.hu.bu  vr5,     vr5,     0
+    vsub.h         vr4,     vr1,     vr5  //256-hor[x]
+
+    vmul.h         vr6,     vr4,     vr0
+    vmadd.h        vr6,     vr5,     vr3  //pred
+    vadd.h         vr6,     vr6,     vr2
+    vsrai.h        vr6,     vr6,     8
+    vsrlni.b.h     vr6,     vr6,     0
+
+    fstx.s         f6,      a0,      t1
+
+    addi.d         t1,      t1,      4
+
+.IPRED_SMOOTH_W_H_LOOP_END:
+    addi.d         t3,      t3,      -1
+    add.d          a0,      a0,      a1
+    addi.d         a4,      a4,      -1
+    bnez           a4,      .IPRED_SMOOTH_H_H_LOOP
+
+endfunc
+
+// void pal_pred_lsx(pixel *dst, const ptrdiff_t stride,
+//                   const pixel *const pal, const uint8_t *idx,
+//                   const int w, const int h)
+function pal_pred_8bpc_lsx
+    srai.d         a7,      a5,      2
+
+.PAL_PRED_WLOOP4:
+    andi           a6,      a4,      4
+    beqz           a6,      .PAL_PRED_WLOOP8
+    fld.d          f0,      a3,      0
+    vsrli.b        vr1,     vr0,     4
+    vandi.b        vr2,     vr0,     7
+    vilvl.b        vr0,     vr1,     vr2
+    fld.d          f1,      a2,      0
+    vshuf.b        vr2,     vr1,     vr1,    vr0
+
+    vstelm.w       vr2,     a0,      0,      0
+    add.d          a0,      a0,      a1
+    vstelm.w       vr2,     a0,      0,      1
+    add.d          a0,      a0,      a1
+    vstelm.w       vr2,     a0,      0,      2
+    add.d          a0,      a0,      a1
+    vstelm.w       vr2,     a0,      0,      3
+    add.d          a0,      a0,      a1
+
+    addi.d         a3,      a3,      8
+    addi.d         a7,      a7,      -1
+    bnez           a7,      .PAL_PRED_WLOOP4
+    b              .PAL_PRED_END
+
+.PAL_PRED_WLOOP8:
+    andi           a6,      a4,      8
+    beqz           a6,      .PAL_PRED_WLOOP16
+
+    vld            vr0,     a3,      0
+    vsrli.b        vr1,     vr0,     4
+    vandi.b        vr2,     vr0,     7
+    vilvl.b        vr0,     vr1,     vr2
+    vilvh.b        vr3,     vr1,     vr2
+    fld.d          f1,      a2,      0
+    vshuf.b        vr0,     vr1,     vr1,    vr0
+    vshuf.b        vr3,     vr1,     vr1,    vr3
+
+    vstelm.d       vr0,     a0,      0,      0
+    add.d          a0,      a0,      a1
+    vstelm.d       vr0,     a0,      0,      1
+    add.d          a0,      a0,      a1
+
+    vstelm.d       vr3,     a0,      0,      0
+    add.d          a0,      a0,      a1
+    vstelm.d       vr3,     a0,      0,      1
+    add.d          a0,      a0,      a1
+
+    addi.d         a3,      a3,      16
+    addi.d         a7,      a7,      -1
+    bnez           a7,      .PAL_PRED_WLOOP8
+    b              .PAL_PRED_END
+
+.PAL_PRED_WLOOP16:
+    andi           a6,      a4,      16
+    beqz           a6,      .PAL_PRED_WLOOP32
+
+    vld            vr0,     a3,      0
+    vld            vr1,     a3,      16
+    fld.d          f6,      a2,      0
+    vsrli.b        vr2,     vr0,     4
+    vandi.b        vr3,     vr0,     7
+    vsrli.b        vr4,     vr1,     4
+    vandi.b        vr5,     vr1,     7
+    vilvl.b        vr0,     vr2,     vr3
+    vilvh.b        vr1,     vr2,     vr3
+    vilvl.b        vr2,     vr4,     vr5
+    vilvh.b        vr3,     vr4,     vr5
+    vshuf.b        vr0,     vr6,     vr6,    vr0
+    vshuf.b        vr1,     vr6,     vr6,    vr1
+    vshuf.b        vr2,     vr6,     vr6,    vr2
+    vshuf.b        vr3,     vr6,     vr6,    vr3
+
+    vst            vr0,     a0,      0
+    add.d          a0,      a0,      a1
+    vst            vr1,     a0,      0
+    add.d          a0,      a0,      a1
+    vst            vr2,     a0,      0
+    add.d          a0,      a0,      a1
+    vst            vr3,     a0,      0
+    add.d          a0,      a0,      a1
+
+    addi.d         a3,      a3,      32
+    addi.d         a7,      a7,      -1
+    bnez           a7,      .PAL_PRED_WLOOP16
+    b              .PAL_PRED_END
+
+.PAL_PRED_WLOOP32:
+    andi           a6,      a4,      32
+    beqz           a6,      .PAL_PRED_WLOOP64
+
+    vld            vr0,     a3,      0
+    vld            vr1,     a3,      16
+    vld            vr2,     a3,      32
+    vld            vr3,     a3,      48
+    fld.d          f4,      a2,      0
+    vsrli.b        vr5,     vr0,     4
+    vandi.b        vr6,     vr0,     7
+    vsrli.b        vr7,     vr1,     4
+    vandi.b        vr8,     vr1,     7
+    vsrli.b        vr9,     vr2,     4
+    vandi.b        vr10,    vr2,     7
+    vsrli.b        vr11,    vr3,     4
+    vandi.b        vr12,    vr3,     7
+    vilvl.b        vr0,     vr5,     vr6
+    vilvh.b        vr1,     vr5,     vr6
+    vilvl.b        vr2,     vr7,     vr8
+    vilvh.b        vr3,     vr7,     vr8
+    vilvl.b        vr5,     vr9,     vr10
+    vilvh.b        vr6,     vr9,     vr10
+    vilvl.b        vr7,     vr11,    vr12
+    vilvh.b        vr8,     vr11,    vr12
+    vshuf.b        vr0,     vr4,     vr4,    vr0
+    vshuf.b        vr1,     vr4,     vr4,    vr1
+    vshuf.b        vr2,     vr4,     vr4,    vr2
+    vshuf.b        vr3,     vr4,     vr4,    vr3
+    vshuf.b        vr5,     vr4,     vr4,    vr5
+    vshuf.b        vr6,     vr4,     vr4,    vr6
+    vshuf.b        vr7,     vr4,     vr4,    vr7
+    vshuf.b        vr8,     vr4,     vr4,    vr8
+
+    vst            vr0,     a0,      0
+    vst            vr1,     a0,      16
+    add.d          a0,      a0,      a1
+    vst            vr2,     a0,      0
+    vst            vr3,     a0,      16
+    add.d          a0,      a0,      a1
+    vst            vr5,     a0,      0
+    vst            vr6,     a0,      16
+    add.d          a0,      a0,      a1
+    vst            vr7,     a0,      0
+    vst            vr8,     a0,      16
+    add.d          a0,      a0,      a1
+
+    addi.d         a3,      a3,      64
+    addi.d         a7,      a7,      -1
+    bnez           a7,      .PAL_PRED_WLOOP32
+    b              .PAL_PRED_END
+
+.PAL_PRED_WLOOP64:
+    vld            vr0,     a3,      0
+    vld            vr1,     a3,      16
+    fld.d          f2,      a2,      0
+    vsrli.b        vr3,     vr0,     4
+    vandi.b        vr4,     vr0,     7
+    vsrli.b        vr5,     vr1,     4
+    vandi.b        vr6,     vr1,     7
+    vilvl.b        vr0,     vr3,     vr4
+    vilvh.b        vr1,     vr3,     vr4
+    vilvl.b        vr3,     vr5,     vr6
+    vilvh.b        vr4,     vr5,     vr6
+    vshuf.b        vr0,     vr2,     vr2,    vr0
+    vshuf.b        vr1,     vr2,     vr2,    vr1
+    vshuf.b        vr3,     vr2,     vr2,    vr3
+    vshuf.b        vr4,     vr2,     vr2,    vr4
+
+    vst            vr0,     a0,      0
+    vst            vr1,     a0,      16
+    vst            vr3,     a0,      32
+    vst            vr4,     a0,      48
+
+    add.d          a0,      a0,      a1
+    addi.d         a3,      a3,      32
+    addi.d         a5,      a5,      -1
+    bnez           a5,      .PAL_PRED_WLOOP64
+
+.PAL_PRED_END:
+endfunc
+
+.macro apply_sign_vrh v, s, vrzero, vrt0 ,out
+    vslt.h         \vrt0,   \s,      \vrzero
+    vandn.v        \s,      \vrt0,   \v
+    vsigncov.h     \v,      \vrt0,   \v
+    vor.v          \out,    \s,      \v
+.endm
+
+.macro iclip_pixel_vrh in0, in1, in2, tmp0, tmp1, out
+    vmin.h         \tmp0,   \in2,    \in0
+    vslt.h         \in0,    \in0,    \in1
+    vand.v         \tmp1,   \in0,    \in1
+    vandn.v        \tmp0,   \in0,    \tmp0
+    vor.v          \out,    \tmp1,   \tmp0
+.endm
+
+.macro ipred_cfl_pred dst, stride, w, h, dc, ac, alpha
+    vreplgr2vr.h   vr2,     \alpha
+    vreplgr2vr.h   vr7,     \dc
+    li.w           t1,      32
+    vreplgr2vr.h   vr3,     t1
+    vxor.v         vr4,     vr4,     vr4
+    li.w           t1,      255
+    vreplgr2vr.h   vr6,     t1
+    add.d          t4,      \w,      \w
+
+1:
+    or             t1,      zero,    zero
+    or             t2,      zero,    zero
+    srai.d         t3,      \w,      3
+    beqz           t3,      3f
+
+2:
+    vldx           vr0,     \ac,     t1
+    vmul.h         vr1,     vr2,     vr0
+    vadda.h        vr0,     vr1,     vr3
+    vsrai.h        vr0,     vr0,     6
+    apply_sign_vrh vr0, vr1, vr4, vr5, vr0
+    vadd.h         vr1,     vr0,     vr7
+    iclip_pixel_vrh vr1, vr4, vr6, vr5, vr8, vr0
+    vsrlni.b.h     vr0,     vr0,     0
+    fstx.d         f0,      \dst,    t2
+
+    addi.d         t1,      t1,      16
+    addi.d         t2,      t2,      8
+    addi.d         t3,      t3,      -1
+    bnez           t3,      2b
+    b              4f
+
+3:
+    fld.d          f0,      \ac,     0
+    vmul.h         vr1,     vr2,     vr0
+    vadda.h        vr0,     vr1,     vr3
+    vsrai.h        vr0,     vr0,     6
+    apply_sign_vrh vr0, vr1, vr4, vr5, vr0
+    vadd.h         vr1,     vr0,     vr7
+    iclip_pixel_vrh vr1, vr4, vr6, vr5, vr8, vr0
+    vsrlni.b.h     vr0,     vr0,     0
+    fst.s          f0,      \dst,    0
+
+4:
+    add.d          \ac,     \ac,     t4
+    add.d          \dst,    \dst,    \stride
+    addi.d         \h,      \h,      -1
+    bnez           \h,      1b
+.endm
+
+function ipred_cfl_8bpc_lsx
+    ipred_dc_gen   a2, a3, a4
+    ipred_cfl_pred a0, a1, a3, a4, t0, a5, a6
+endfunc
+
+function ipred_cfl_top_8bpc_lsx
+    ipred_dc_gen_top   a2, a3
+    ipred_cfl_pred a0, a1, a3, a4, t0, a5, a6
+endfunc
+
+function ipred_cfl_left_8bpc_lsx
+    ipred_dc_gen_left   a2, a4
+    ipred_cfl_pred a0, a1, a3, a4, t0, a5, a6
+endfunc
+
+function ipred_cfl_128_8bpc_lsx
+    li.w           t0,      128
+    ipred_cfl_pred a0, a1, a3, a4, t0, a5, a6
+endfunc
+
+const dav1d_filter_intra_taps_lsx
+    //arr0  8*7
+.byte    -6, -5, -3, -3, -4, -3, -3, -3
+.byte    10,  2,  1,  1,  6,  2,  2,  1
+.byte    0, 10,  1,  1,  0,  6,  2,  2
+.byte    0,  0, 10,  2,  0,  0,  6,  2
+.byte    0,  0,  0, 10,  0,  0,  0,  6
+.byte    12,  9,  7,  5,  2,  2,  2,  3
+.byte    0,  0,  0,  0, 12,  9,  7,  5
+    //arr1
+.byte    -10,  -6,  -4,  -2, -10,  -6,  -4,  -2
+.byte    16,   0,   0,   0,  16,   0,   0,   0
+.byte    0,  16,   0,   0,   0,  16,   0,   0
+.byte    0,   0,  16,   0,   0,   0,  16,   0
+.byte    0,   0,   0,  16,   0,   0,   0,  16
+.byte    10,   6,   4,   2,   0,   0,   0,   0
+.byte    0,   0,   0,   0,  10,   6,   4,   2
+    //arr2
+.byte    -8,  -8,  -8,  -8,  -4,  -4,  -4,  -4
+.byte    8,   0,   0,   0,   4,   0,   0,   0
+.byte    0,   8,   0,   0,   0,   4,   0,   0
+.byte    0,   0,   8,   0,   0,   0,   4,   0
+.byte    0,   0,   0,   8,   0,   0,   0,   4
+.byte    16,  16,  16,  16,   0,   0,   0,   0
+.byte    0,   0,   0,   0,  16,  16,  16,  16
+    //arr3
+.byte    -2,  -1,  -1,   0,  -1,  -1,  -1,  -1
+.byte    8,   3,   2,   1,   4,   3,   2,   2
+.byte    0,   8,   3,   2,   0,   4,   3,   2
+.byte    0,   0,   8,   3,   0,   0,   4,   3
+.byte    0,   0,   0,   8,   0,   0,   0,   4
+.byte    10,   6,   4,   2,   3,   4,   4,   3
+.byte    0,   0,   0,   0,  10,   6,   4,   3
+    //arr4
+.byte    -12, -10,  -9,  -8, -10,  -9,  -8,  -7
+.byte    14,   0,   0,   0,  12,   1,   0,   0
+.byte    0,  14,   0,   0,   0,  12,   0,   0
+.byte    0,   0,  14,   0,   0,   0,  12,   1
+.byte    0,   0,   0,  14,   0,   0,   0,  12
+.byte    14,  12,  11,  10,   0,   0,   1,   1
+.byte    0,   0,   0,   0,  14,  12,  11,   9
+endconst
+
+.macro ipred_filter_load_p
+    vldrepl.b      vr0,     t0,      0
+    vldrepl.b      vr1,     a7,      0
+    vldrepl.b      vr2,     a7,      1
+    vldrepl.b      vr3,     a7,      2
+    vldrepl.b      vr4,     a7,      3
+    vldrepl.b      vr5,     t1,      0
+    vldrepl.b      vr6,     t1,      -1
+
+    vsllwil.hu.bu  vr0,     vr0,     0
+    vsllwil.hu.bu  vr1,     vr1,     0
+    vsllwil.hu.bu  vr2,     vr2,     0
+    vsllwil.hu.bu  vr3,     vr3,     0
+    vsllwil.hu.bu  vr4,     vr4,     0
+    vsllwil.hu.bu  vr5,     vr5,     0
+    vsllwil.hu.bu  vr6,     vr6,     0
+.endm
+
+.macro ipred_filter_loadx_p
+    vldrepl.b      vr0,     t0,      0
+    vldrepl.b      vr1,     a7,      0
+    vldrepl.b      vr2,     a7,      1
+    vldrepl.b      vr3,     a7,      2
+    vldrepl.b      vr4,     a7,      3
+    vldrepl.b      vr5,     t1,      0
+    ldx.bu         t3,      t1,      a1
+    vreplgr2vr.b   vr6,     t3
+
+    vsllwil.hu.bu  vr0,     vr0,     0
+    vsllwil.hu.bu  vr1,     vr1,     0
+    vsllwil.hu.bu  vr2,     vr2,     0
+    vsllwil.hu.bu  vr3,     vr3,     0
+    vsllwil.hu.bu  vr4,     vr4,     0
+    vsllwil.hu.bu  vr5,     vr5,     0
+    vsllwil.hu.bu  vr6,     vr6,     0
+.endm
+
+.macro ipred_filter_load_fltptr
+    fld.d          f7,      a6,      0
+    fld.d          f8,      a6,      8
+    fld.d          f9,      a6,      16
+    fld.d          f10,     a6,      24
+    fld.d          f11,     a6,      32
+    fld.d          f12,     a6,      40
+    fld.d          f13,     a6,      48
+
+    vsllwil.h.b    vr7,     vr7,     0
+    vsllwil.h.b    vr8,     vr8,     0
+    vsllwil.h.b    vr9,     vr9,     0
+    vsllwil.h.b    vr10,    vr10,    0
+    vsllwil.h.b    vr11,    vr11,    0
+    vsllwil.h.b    vr12,    vr12,    0
+    vsllwil.h.b    vr13,    vr13,    0
+.endm
+
+.macro ipred_filter_calc_acc
+    vmul.h         vr7,     vr7,     vr0
+    vmadd.h        vr7,     vr8,     vr1
+    vmadd.h        vr7,     vr9,     vr2
+    vmadd.h        vr7,     vr10,    vr3
+    vmadd.h        vr7,     vr11,    vr4
+    vmadd.h        vr7,     vr12,    vr5
+    vmadd.h        vr7,     vr13,    vr6
+    vaddi.hu       vr7,     vr7,     8
+    vsrai.h        vr7,     vr7,     4
+    iclip_pixel_vrh vr7, vr14, vr15, vr9, vr10, vr8
+    vsrlni.b.h     vr8,     vr8,     0
+.endm
+
+// void ipred_filter_lsx(pixel *dst, const ptrdiff_t stride,
+//                       const pixel *const topleft_in,
+//                       const int width, const int height, int filt_idx,
+//                       const int max_width, const int max_height
+//                       HIGHBD_DECL_SUFFIX)
+function ipred_filter_8bpc_lsx
+    andi           a5,      a5,      511
+    la.local       a6,      dav1d_filter_intra_taps_lsx
+    li.w           a7,      56
+    mul.w          a7,      a7,      a5
+    add.d          a6,      a6,      a7   //*filter
+    addi.d         a7,      a2,      1    //*top
+    or             a5,      zero,    zero //y
+    vxor.v         vr14,    vr14,    vr14
+    li.w           t0,      255
+    vreplgr2vr.h   vr15,    t0
+
+.FILTER_LOOP_H:
+    sub.d          t0,      a2,      a5   //*topleft
+    addi.d         t1,      t0,      -1   //left
+
+    ctz.w          t2,      a3
+    addi.d         t3,      t2,      -2
+    beqz           t3,      .FILTER_LOOP_W4
+    addi.d         t3,      t2,      -3
+    beqz           t3,      .FILTER_LOOP_W8
+    addi.d         t3,      t2,      -4
+    beqz           t3,      .FILTER_LOOP_W16
+    addi.d         t3,      t2,      -5
+    beqz           t3,      .FILTER_LOOP_W32
+
+.FILTER_LOOP_W4:
+    ipred_filter_load_p
+
+    or             t3,      a0,      a0  //*ptr
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+    b              .FILTER_LOOP_W_END
+
+.FILTER_LOOP_W8:
+    ipred_filter_load_p
+
+    or             t3,      a0,      a0
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+    addi.d         t1,      a0,      3
+    addi.d         a7,      a7,      4
+    addi.d         t0,      a7,      -1
+
+    ipred_filter_loadx_p
+
+    addi.d         t3,      a0,      4
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+    b              .FILTER_LOOP_W_END
+
+.FILTER_LOOP_W16:
+    ipred_filter_load_p
+
+    or             t3,      a0,      a0
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+    addi.d         t1,      a0,      3
+    addi.d         a7,      a7,      4
+    addi.d         t0,      a7,      -1
+
+    ipred_filter_loadx_p
+
+    addi.d         t3,      a0,      4
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+    addi.d         t1,      a0,      7
+    addi.d         a7,      a7,      4
+    addi.d         t0,      a7,      -1
+
+    ipred_filter_loadx_p
+
+    addi.d         t3,      a0,      8
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+    addi.d         t1,      a0,      11
+    addi.d         a7,      a7,      4
+    addi.d         t0,      a7,      -1
+
+    ipred_filter_loadx_p
+
+    addi.d         t3,      a0,      12
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+    b              .FILTER_LOOP_W_END
+
+.FILTER_LOOP_W32:
+    ipred_filter_load_p
+
+    or             t3,      a0,      a0
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+    addi.d         t1,      a0,      3
+    addi.d         a7,      a7,      4
+    addi.d         t0,      a7,      -1
+
+    ipred_filter_loadx_p
+
+    addi.d         t3,      a0,      4
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+    addi.d         t1,      a0,      7
+    addi.d         a7,      a7,      4
+    addi.d         t0,      a7,      -1
+
+    ipred_filter_loadx_p
+
+    addi.d         t3,      a0,      8
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+    addi.d         t1,      a0,      11
+    addi.d         a7,      a7,      4
+    addi.d         t0,      a7,      -1
+
+    ipred_filter_loadx_p
+
+    addi.d         t3,      a0,      12
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+    addi.d         t1,      a0,      15
+    addi.d         a7,      a7,      4
+    addi.d         t0,      a7,      -1
+
+    ipred_filter_loadx_p
+
+    addi.d         t3,      a0,      16
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+    addi.d         t1,      a0,      19
+    addi.d         a7,      a7,      4
+    addi.d         t0,      a7,      -1
+
+    ipred_filter_loadx_p
+
+    addi.d         t3,      a0,      20
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+    addi.d         t1,      a0,      23
+    addi.d         a7,      a7,      4
+    addi.d         t0,      a7,      -1
+
+    ipred_filter_loadx_p
+
+    addi.d         t3,      a0,      24
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+    addi.d         t1,      a0,      27
+    addi.d         a7,      a7,      4
+    addi.d         t0,      a7,      -1
+
+    ipred_filter_loadx_p
+
+    addi.d         t3,      a0,      28
+
+    ipred_filter_load_fltptr
+    ipred_filter_calc_acc
+
+    fst.s          f8,      t3,      0
+    add.d          t3,      t3,      a1
+    vstelm.w       vr8,     t3,      0,      1
+    add.d          t3,      t3,      a1
+
+.FILTER_LOOP_W_END:
+    add.d          a7,      a0,      a1
+    add.d          t2,      a1,      a1
+    add.d          a0,      a0,      t2
+    addi.d         a5,      a5,      2
+    blt            a5,      a4,      .FILTER_LOOP_H
+endfunc
+
+const dav1d_dr_intra_derivative
+    // Values that are 0 will never be used
+    .short  0         // Angles:
+    .short  1023, 0   //  3,  93, 183
+    .short  547       //  6,  96, 186
+    .short  372, 0, 0 //  9,  99, 189
+    .short  273       // 14, 104, 194
+    .short  215, 0    // 17, 107, 197
+    .short  178       // 20, 110, 200
+    .short  151, 0    // 23, 113, 203 (113 & 203 are base angles)
+    .short  132       // 26, 116, 206
+    .short  116, 0    // 29, 119, 209
+    .short  102, 0    // 32, 122, 212
+    .short  90        // 36, 126, 216
+    .short  80, 0     // 39, 129, 219
+    .short  71        // 42, 132, 222
+    .short  64, 0     // 45, 135, 225 (45 & 135 are base angles)
+    .short  57        // 48, 138, 228
+    .short  51, 0     // 51, 141, 231
+    .short  45, 0     // 54, 144, 234
+    .short  40        // 58, 148, 238
+    .short  35, 0     // 61, 151, 241
+    .short  31        // 64, 154, 244
+    .short  27, 0     // 67, 157, 247 (67 & 157 are base angles)
+    .short  23        // 70, 160, 250
+    .short  19, 0     // 73, 163, 253
+    .short  15, 0     // 76, 166, 256
+    .short  11, 0     // 81, 171, 261
+    .short  7         // 84, 174, 264
+    .short  3         // 87, 177, 267
+endconst
+
+const z1_upsample_edge_kernel
+    .short  -1, 9, 9, -1, -1, 9, 9, -1
+endconst
+
+const ipred_filter_edge_kernel1
+    .short  0, 4, 8, 4, 0, 4, 8, 4
+    .short  0, 5, 6, 5, 0, 5, 6, 5
+    .short  2, 4, 4, 4, 2, 4, 4, 4
+endconst
+
+const ipred_filter_edge_kernel2
+    .short  0, 0, 0, 0, 0, 0, 0, 0
+    .short  0, 0, 0, 0, 0, 0, 0, 0
+    .short  2, 2, 2, 2, 2, 2, 2, 2
+endconst
+
+.macro z1_upsample_edge_calc_loop
+    vsllwil.hu.bu  vr10,    vr7,     0
+    vsllwil.hu.bu  vr11,    vr11,    0
+    vsllwil.hu.bu  vr12,    vr12,    0
+    vsllwil.hu.bu  vr13,    vr13,    0
+
+    vmul.h         vr10,    vr10,    vr0
+    vmul.h         vr11,    vr11,    vr0
+    vmul.h         vr12,    vr12,    vr0
+    vmul.h         vr13,    vr13,    vr0
+
+    vhaddw.w.h     vr10,    vr10,    vr10
+    vhaddw.w.h     vr11,    vr11,    vr11
+    vhaddw.w.h     vr12,    vr12,    vr12
+    vhaddw.w.h     vr13,    vr13,    vr13
+    vhaddw.d.w     vr10,    vr10,    vr10
+    vhaddw.d.w     vr11,    vr11,    vr11
+    vhaddw.d.w     vr12,    vr12,    vr12
+    vhaddw.d.w     vr13,    vr13,    vr13
+
+    vpackev.h      vr10,    vr11,    vr10
+    vpackev.h      vr11,    vr13,    vr12
+    vpackev.w      vr12,    vr11,    vr10  //s:01234567
+    vsrari.h       vr12,    vr12,    4
+    iclip_pixel_vrh vr12, vr15, vr16, vr10, vr11, vr12
+    vsrlni.b.h     vr12,    vr12,    0  //out: 13579...
+    vbsrl.v        vr11,    vr7,     1  //out:02468...
+    vilvl.b        vr13,    vr12,    vr11
+.endm
+
+.macro z1_upsample_edge_data_init1
+    vbsrl.v        vr11,    vr7,     1
+    vbsrl.v        vr12,    vr7,     2
+    vbsrl.v        vr13,    vr7,     3
+    z1_upsample_edge_calc_loop
+.endm
+
+.macro z1_upsample_edge_data_init2
+    vbsrl.v        vr11,    vr7,     1
+    vbsrl.v        vr12,    vr7,     2
+    vextrins.b     vr12,    vr12,    0x76
+    vbsrl.v        vr13,    vr7,     3
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_upsample_edge_calc_loop
+.endm
+
+.macro z1_upsample_edge_calc_other
+    vsllwil.hu.bu  vr10,    vr7,     0
+    vmul.h         vr10,    vr10,    vr0
+    vhaddw.w.h     vr10,    vr10,    vr10
+    vhaddw.d.w     vr10,    vr10,    vr10
+    vreplvei.h     vr12,    vr10,    0   //s0-s7
+    vsrari.h       vr12,    vr12,    4
+
+    iclip_pixel_vrh vr12, vr15, vr16, vr10, vr11, vr12
+    vsrlni.b.h     vr12,    vr12,    0
+    vilvl.b        vr13,    vr12,    vr7
+.endm
+
+.macro z1_filter_edge_calc_loop1
+    vmul.h         vr10,    vr10,    vr1
+    vmul.h         vr11,    vr11,    vr1
+    vmul.h         vr12,    vr12,    vr1
+    vmul.h         vr13,    vr13,    vr1
+
+    vhaddw.w.h     vr10,    vr10,    vr10
+    vhaddw.w.h     vr11,    vr11,    vr11
+    vhaddw.w.h     vr12,    vr12,    vr12
+    vhaddw.w.h     vr13,    vr13,    vr13
+    vhaddw.d.w     vr10,    vr10,    vr10
+    vhaddw.d.w     vr11,    vr11,    vr11
+    vhaddw.d.w     vr12,    vr12,    vr12
+    vhaddw.d.w     vr13,    vr13,    vr13
+
+    vpackev.h      vr10,    vr11,    vr10
+    vpackev.h      vr11,    vr13,    vr12
+    vpackev.w      vr10,    vr11,    vr10  //s:01234567
+.endm
+
+.macro z1_filter_edge_calc_loop2
+    vsllwil.hu.bu  vr13,    vr13,    0
+    vmadd.h        vr10,    vr13,    vr6
+    vsrari.h       vr12,    vr10,    4
+    vsrlni.b.h     vr12,    vr12,    0  //out: 0-7
+.endm
+
+.macro z1_filter_edge_calc_other
+    vsllwil.hu.bu  vr10,    vr10,    0
+    vmul.h         vr11,    vr10,    vr1
+    vhaddw.w.h     vr11,    vr11,    vr11
+    vhaddw.d.w     vr11,    vr11,    vr11
+    vreplvei.h     vr12,    vr11,    4
+    vextrins.h     vr12,    vr11,    0x00
+
+    vreplvei.h     vr13,    vr10,    1
+    vmadd.h        vr12,    vr13,    vr6
+    vsrari.h       vr12,    vr12,    4
+    vsrlni.b.h     vr12,    vr12,    0  //out: 0-7
+.endm
+
+.macro z1_filter_edge_data_init1
+    vbsll.v        vr10,    vr7,     1
+    vextrins.b     vr10,    vr10,    0x01
+    vbsrl.v        vr12,    vr7,     1
+    vbsrl.v        vr13,    vr7,     2
+    vsllwil.hu.bu  vr10,    vr10,    0
+    vsllwil.hu.bu  vr11,    vr7,     0
+    vsllwil.hu.bu  vr12,    vr12,    0
+    vsllwil.hu.bu  vr13,    vr13,    0
+    z1_filter_edge_calc_loop1
+.endm
+
+.macro z1_filter_edge_data_init2
+    vbsrl.v        vr11,    vr7,     1
+    vbsrl.v        vr12,    vr7,     2
+    vbsrl.v        vr13,    vr7,     3
+    vsllwil.hu.bu  vr10,    vr7,     0
+    vsllwil.hu.bu  vr11,    vr11,    0
+    vsllwil.hu.bu  vr12,    vr12,    0
+    vsllwil.hu.bu  vr13,    vr13,    0
+    z1_filter_edge_calc_loop1
+.endm
+
+.macro z1_filter_edge_data_init3
+    vbsrl.v        vr11,    vr7,     1
+    vbsrl.v        vr12,    vr7,     2
+    vbsrl.v        vr13,    vr7,     3
+    vextrins.b     vr13,    vr13,    0x76
+    vsllwil.hu.bu  vr10,    vr7,     0
+    vsllwil.hu.bu  vr11,    vr11,    0
+    vsllwil.hu.bu  vr12,    vr12,    0
+    vsllwil.hu.bu  vr13,    vr13,    0
+    z1_filter_edge_calc_loop1
+.endm
+
+.macro z1_filter_edge_data_init4
+    vbsll.v        vr10,    vr7,     1
+    vextrins.b     vr10,    vr10,    0x01
+    vbsrl.v        vr12,    vr7,     1
+    vbsrl.v        vr13,    vr7,     2
+    vextrins.b     vr13,    vr13,    0x76
+    vsllwil.hu.bu  vr10,    vr10,    0
+    vsllwil.hu.bu  vr11,    vr7,     0
+    vsllwil.hu.bu  vr12,    vr12,    0
+    vsllwil.hu.bu  vr13,    vr13,    0
+    z1_filter_edge_calc_loop1
+.endm
+
+.macro pixel_set_8bpc_allw dst_ptr, src_ptr, width, tmp0, tmp1
+    vldrepl.b      vr10,    \src_ptr, 0
+    or             \tmp1,   zero,     zero
+    srai.d         \tmp0,   \width,   4
+    beqz           \tmp0,   2f
+1:
+    vstx           vr10,    \dst_ptr, \tmp1
+    addi.d         \tmp1,   \tmp1,    16
+    addi.d         \tmp0,   \tmp0,    -1
+    bnez           \tmp0,   1b
+2:
+    andi           \tmp0,   \width,   8
+    beqz           \tmp0,   3f
+    fstx.d         f10,     \dst_ptr, \tmp1
+    addi.d         \tmp1,   \tmp1,    8
+3:
+    andi           \tmp0,   \width,   4
+    beqz           \tmp0,   4f
+    fstx.s         f10,     \dst_ptr, \tmp1
+    addi.d         \tmp1,   \tmp1,    4
+4:
+    andi           \tmp0,   \width,   2
+    beqz           \tmp0,   5f
+    ldx.bu         \tmp0,   \src_ptr, zero
+    stx.b          \tmp0,   \dst_ptr, \tmp1
+    addi.d         \tmp1,   \tmp1,    1
+    stx.b          \tmp0,   \dst_ptr, \tmp1
+    addi.d         \tmp1,   \tmp1,    1
+5:
+    andi           \tmp0,   \width,   1
+    beqz           \tmp0,   6f
+    ldx.bu         \tmp0,   \src_ptr, zero
+    stx.b          \tmp0,   \dst_ptr, \tmp1
+6:
+.endm
+
+// void ipred_z1_lsx(pixel *dst, const ptrdiff_t stride,
+//                   const pixel *const topleft_in,
+//                   const int width, const int height, int angle,
+//                   const int max_width, const int max_height
+//                   HIGHBD_DECL_SUFFIX)
+function ipred_z1_8bpc_lsx
+    addi.d         a2,      a2,      1   //&topleft_in[1]
+    addi.d         sp,      sp,      -128
+    or             t2,      sp,      sp  //top_out
+    srai.d         a6,      a5,      9
+    andi           a6,      a6,      1   //is_sum
+    srai.d         a7,      a5,      10  //enable_intra_edge_filter
+    andi           a5,      a5,      511
+
+    la.local       t0,      dav1d_dr_intra_derivative
+    andi           t1,      a5,      0xFFE
+    ldx.hu         t1,      t0,      t1  //dx
+
+    beqz           a7,      .IPRED_Z1_NOTUA
+    add.d          t3,      a3,      a4
+    li.w           t4,      90
+    sub.w          t4,      t4,      a5
+    // ipred_get_upsample t5:upsample_above
+    li.w           t6,      16
+    sra.d          t6,      t6,      a6
+    bge            t6,      t3,      .Z1_GETUS1
+    addi.d         t5,      zero,    0
+    b              .Z1_GETUS2
+.Z1_GETUS1:
+    addi.d         t5,      zero,    1
+.Z1_GETUS2:
+    li.w           t6,      40
+    blt            t4,      t6,      .Z1_GETUS3
+    addi.d         t6,      zero,    0
+    b              .Z1_GETUS4
+.Z1_GETUS3:
+    addi.d         t6,      zero,    1
+.Z1_GETUS4:
+    and            t5,      t5,      t6
+
+    beqz           t5,      .IPRED_Z1_NOTUA
+
+    la.local       t0,      z1_upsample_edge_kernel
+    vld            vr0,     t0,      0   //kernel
+    vxor.v         vr15,    vr15,    vr15
+    li.w           t0,      255
+    vreplgr2vr.h   vr16,    t0
+
+.Z1_UEDGE_W4:
+    andi           t6,      a3,     4
+    beqz           t6,      .Z1_UEDGE_W8
+.Z1_UEDGE_W4_H4:
+    andi           t6,      a4,     4
+    beqz           t6,      .Z1_UEDGE_W4_H8
+
+    //0-6
+    vld            vr7,     a2,      -1
+    vbsrl.v        vr11,    vr7,     1
+    vbsrl.v        vr12,    vr7,     2
+    vextrins.b     vr12,    vr12,    0x76
+    vbsrl.v        vr13,    vr7,     3
+    z1_upsample_edge_calc_loop
+
+    fst.d          f13,     t2,     0
+    vstelm.w       vr13,    t2,     8,    2
+    vstelm.h       vr13,    t2,     12,   6
+
+    ld.bu          t7,      a2,     7
+    st.b           t7,      t2,     14
+
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W4_H8:
+    andi           t6,      a4,     8
+    beqz           t6,      .Z1_UEDGE_W4_H16
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init2
+    vst            vr13,    t2,     0
+
+    //8-10
+    vldrepl.b      vr7,     a2,     7
+    z1_upsample_edge_calc_other
+
+    vstelm.w       vr13,    t2,     16,   0
+    vstelm.h       vr13,    t2,     20,   2
+
+    ld.bu          t7,      a2,     7
+    st.b           t7,      t2,     22
+
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W4_H16:
+    andi           t6,      a4,     16
+    beqz           t6,      .Z1_UEDGE_W4_H32
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init2
+    vst            vr13,    t2,     0
+
+    //8-15
+    vldrepl.b      vr7,     a2,     7
+    z1_upsample_edge_calc_other
+    vst            vr13,    t2,     16
+
+    //16-18
+    vstelm.w       vr13,    t2,     32,   0
+    vstelm.h       vr13,    t2,     36,   2
+
+    ld.bu          t7,      a2,     7
+    st.b           t7,      t2,     38
+
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W4_H32:
+    andi           t6,      a4,     32
+    beqz           t6,      .Z1_UEDGE_W4_H64
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init2
+    vst            vr13,    t2,     0
+
+    //8-15
+    vldrepl.b      vr7,     a2,     7
+    z1_upsample_edge_calc_other
+    vst            vr13,    t2,     16
+
+    vst            vr13,    t2,     32 //16-23
+    vst            vr13,    t2,     48 //24-31
+
+    //32-34
+    vstelm.w       vr13,    t2,     64,   0
+    vstelm.h       vr13,    t2,     68,   2
+
+    ld.bu          t7,      a2,     7
+    st.b           t7,      t2,     70
+
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W4_H64:
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init2
+    vst            vr13,    t2,     0
+
+    //8-15
+    vldrepl.b      vr7,     a2,     7
+    z1_upsample_edge_calc_other
+    vst            vr13,    t2,     16
+
+    vst            vr13,    t2,     32 //16-23
+    vst            vr13,    t2,     48 //24-31
+    vst            vr13,    t2,     64 //32-39
+    vst            vr13,    t2,     80 //40-47
+    vst            vr13,    t2,     96 //48-55
+    vst            vr13,    t2,     112 //56-63
+
+    //64-66
+    vstelm.w       vr13,    t2,     128,   0
+    vstelm.h       vr13,    t2,     132,   2
+
+    ld.bu          t7,      a2,     7
+    st.b           t7,      t2,     134
+
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W8:
+    andi           t6,      a3,     8
+    beqz           t6,      .Z1_UEDGE_W16
+.Z1_UEDGE_W8_H4:
+    andi           t6,      a4,     4
+    beqz           t6,      .Z1_UEDGE_W8_H8
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    vbsrl.v        vr11,    vr7,     1
+    vbsrl.v        vr12,    vr7,     2
+    vextrins.b     vr12,    vr12,    0x32
+    vbsrl.v        vr13,    vr7,     3
+    vextrins.b     vr13,    vr13,    0x21
+    vextrins.b     vr13,    vr13,    0x31
+    z1_upsample_edge_calc_loop
+    vstelm.w       vr13,    t2,     16,    0
+    vstelm.h       vr13,    t2,     20,    2
+
+    ld.bu          t7,      a2,     11
+    st.b           t7,      t2,     22
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W8_H8:
+    andi           t6,      a4,     8
+    beqz           t6,      .Z1_UEDGE_W8_H16
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-14
+    vld            vr7,     a2,      7
+    vbsrl.v        vr11,    vr7,     1
+    vbsrl.v        vr12,    vr7,     2
+    vextrins.b     vr12,    vr12,    0x76
+    vbsrl.v        vr13,    vr7,     3
+    z1_upsample_edge_calc_loop
+    fst.d          f13,     t2,     16
+    vstelm.w       vr13,    t2,     24,    2
+    vstelm.h       vr13,    t2,     28,    6
+
+    ld.bu          t7,      a2,     15
+    st.b           t7,      t2,     30
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W8_H16:
+    andi           t6,      a4,     16
+    beqz           t6,      .Z1_UEDGE_W8_H32
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init2
+    vst            vr13,    t2,     16
+
+    //16-22
+    vldrepl.b      vr7,     a2,     15
+    z1_upsample_edge_calc_other
+    fst.d          f13,     t2,     32
+    vstelm.w       vr13,    t2,     40,   2
+    vstelm.h       vr13,    t2,     44,   6
+
+    ld.bu          t7,      a2,     15
+    st.b           t7,      t2,     46
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W8_H32:
+    andi           t6,      a4,     32
+    beqz           t6,      .Z1_UEDGE_W8_H64
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init2
+    vst            vr13,    t2,     16
+
+    //16-23
+    vldrepl.b      vr7,     a2,     15
+    z1_upsample_edge_calc_other
+    vst            vr13,    t2,     32
+
+    vst            vr13,    t2,     48 //24-31
+
+    //32-38
+    fst.d          f13,     t2,     64
+    vstelm.w       vr13,    t2,     72,   2
+    vstelm.h       vr13,    t2,     76,   6
+
+    ld.bu          t7,      a2,     15
+    st.b           t7,      t2,     78
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W8_H64:
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init2
+    vst            vr13,    t2,     16
+
+    //16-23
+    vldrepl.b      vr7,     a2,     15
+    z1_upsample_edge_calc_other
+    vst            vr13,    t2,     32
+
+    vst            vr13,    t2,     48 //24-31
+    vst            vr13,    t2,     64 //32-39
+    vst            vr13,    t2,     80 //40-47
+    vst            vr13,    t2,     96 //48-55
+    vst            vr13,    t2,     112 //56-63
+
+    //64-70
+    fst.d          f13,     t2,     128
+    vstelm.w       vr13,    t2,     136,   2
+    vstelm.h       vr13,    t2,     140,   6
+
+    ld.bu          t7,      a2,     15
+    st.b           t7,      t2,     142
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W16:
+    andi           t6,      a3,     16
+    beqz           t6,      .Z1_UEDGE_W32
+.Z1_UEDGE_W16_H4:
+    andi           t6,      a4,     4
+    beqz           t6,      .Z1_UEDGE_W16_H8
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     16
+
+    //16-18
+    vld            vr7,     a2,      15
+    z1_upsample_edge_data_init1
+    vstelm.w       vr13,    t2,     32,    0
+    vstelm.h       vr13,    t2,     36,    2
+
+    ld.bu          t7,      a2,     19
+    st.b           t7,      t2,     38
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W16_H8:
+    andi           t6,      a4,     8
+    beqz           t6,      .Z1_UEDGE_W16_H16
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      16
+
+    //16-22
+    vld            vr7,     a2,      15
+    vbsrl.v        vr11,    vr7,     1
+    vbsrl.v        vr12,    vr7,     2
+    vextrins.b     vr12,    vr12,    0x76
+    vbsrl.v        vr13,    vr7,     3
+    z1_upsample_edge_calc_loop
+    fst.d          f13,     t2,     32
+    vstelm.w       vr13,    t2,     40,    2
+    vstelm.h       vr13,    t2,     44,    6
+
+    ld.bu          t7,      a2,     23
+    st.b           t7,      t2,     46
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W16_H16:
+    andi           t6,      a4,     16
+    beqz           t6,      .Z1_UEDGE_W16_H32
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     16
+
+    //16-23
+    vld            vr7,     a2,      15
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      32
+
+    //24-30
+    vld            vr7,     a2,      23
+    vbsrl.v        vr11,    vr7,     1
+    vbsrl.v        vr12,    vr7,     2
+    vextrins.b     vr12,    vr12,    0x76
+    vbsrl.v        vr13,    vr7,     3
+    z1_upsample_edge_calc_loop
+    fst.d          f13,     t2,     48
+    vstelm.w       vr13,    t2,     56,    2
+    vstelm.h       vr13,    t2,     60,    6
+
+    ld.bu          t7,      a2,     31
+    st.b           t7,      t2,     62
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W16_H32:
+    andi           t6,      a4,     32
+    beqz           t6,      .Z1_UEDGE_W16_H64
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     16
+
+    //16-23
+    vld            vr7,     a2,      15
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      32
+
+    //24-31
+    vld            vr7,     a2,      23
+    z1_upsample_edge_data_init2
+    vst            vr13,    t2,      48
+
+    //32-39
+    vldrepl.b      vr7,     a2,      31
+    z1_upsample_edge_calc_other
+    vst            vr13,    t2,      64
+
+    //40-46
+    fst.d          f13,     t2,     80
+    vstelm.w       vr13,    t2,     88,    2
+    vstelm.h       vr13,    t2,     92,    6
+
+    ld.bu          t7,      a2,     31
+    st.b           t7,      t2,     94
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W16_H64:
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     16
+
+    //16-23
+    vld            vr7,     a2,      15
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      32
+
+    //24-31
+    vld            vr7,     a2,      23
+    z1_upsample_edge_data_init2
+    vst            vr13,    t2,      48
+
+    //32-39
+    vldrepl.b      vr7,     a2,      31
+    z1_upsample_edge_calc_other
+    vst            vr13,    t2,      64
+
+    vst            vr13,    t2,      80  //40-47
+    vst            vr13,    t2,      96  //48-55
+    vst            vr13,    t2,      112 //56-63
+    vst            vr13,    t2,      128 //64-71
+
+    //72-78
+    fst.d          f13,     t2,     144
+    vstelm.w       vr13,    t2,     152,    2
+    vstelm.h       vr13,    t2,     156,    6
+
+    ld.bu          t7,      a2,     31
+    st.b           t7,      t2,     158
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W32:
+    andi           t6,      a3,     32
+    beqz           t6,      .Z1_UEDGE_W64
+.Z1_UEDGE_W32_H8:
+    andi           t6,      a4,     8
+    beqz           t6,      .Z1_UEDGE_W32_H16
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      16
+
+    //16-23
+    vld            vr7,     a2,      15
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      32
+
+    //24-31
+    vld            vr7,     a2,      23
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      48
+
+    //32-38
+    vld            vr7,     a2,      31
+    vbsrl.v        vr11,    vr7,     1
+    vbsrl.v        vr12,    vr7,     2
+    vextrins.b     vr12,    vr12,    0x76
+    vbsrl.v        vr13,    vr7,     3
+    z1_upsample_edge_calc_loop
+    fst.d          f13,     t2,      64
+    vstelm.w       vr13,    t2,      72,    2
+    vstelm.h       vr13,    t2,      76,    6
+
+    ld.bu          t7,      a2,     39
+    st.b           t7,      t2,     78
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W32_H16:
+    andi           t6,      a4,     16
+    beqz           t6,      .Z1_UEDGE_W32_H32
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      16
+
+    //16-23
+    vld            vr7,     a2,      15
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      32
+
+    //24-31
+    vld            vr7,     a2,      23
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      48
+
+    //32-39
+    vld            vr7,     a2,      31
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      64
+
+    //40-46
+    vld            vr7,     a2,      39
+    vbsrl.v        vr11,    vr7,     1
+    vbsrl.v        vr12,    vr7,     2
+    vextrins.b     vr12,    vr12,    0x76
+    vbsrl.v        vr13,    vr7,     3
+    z1_upsample_edge_calc_loop
+    fst.d          f13,     t2,      80
+    vstelm.w       vr13,    t2,      88,    2
+    vstelm.h       vr13,    t2,      92,    6
+
+    ld.bu          t7,      a2,     47
+    st.b           t7,      t2,     94
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W32_H32:
+    andi           t6,      a4,     32
+    beqz           t6,      .Z1_UEDGE_W32_H64
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      16
+
+    //16-23
+    vld            vr7,     a2,      15
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      32
+
+    //24-31
+    vld            vr7,     a2,      23
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      48
+
+    //32-39
+    vld            vr7,     a2,      31
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      64
+
+    //40-47
+    vld            vr7,     a2,      39
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      80
+
+    //48-55
+    vld            vr7,     a2,      47
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      96
+
+    //56-62
+    vld            vr7,     a2,      55
+    vbsrl.v        vr11,    vr7,     1
+    vbsrl.v        vr12,    vr7,     2
+    vextrins.b     vr12,    vr12,    0x76
+    vbsrl.v        vr13,    vr7,     3
+    z1_upsample_edge_calc_loop
+    fst.d          f13,     t2,      112
+    vstelm.w       vr13,    t2,      120,   2
+    vstelm.h       vr13,    t2,      124,   6
+
+    ld.bu          t7,      a2,     63
+    st.b           t7,      t2,     126
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W32_H64:
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      16
+
+    //16-23
+    vld            vr7,     a2,      15
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      32
+
+    //24-31
+    vld            vr7,     a2,      23
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      48
+
+    //32-39
+    vld            vr7,     a2,      31
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      64
+
+    //40-47
+    vld            vr7,     a2,      39
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      80
+
+    //48-55
+    vld            vr7,     a2,      47
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      96
+
+    //56-63
+    vld            vr7,     a2,      55
+    z1_upsample_edge_data_init2
+    vst            vr13,    t2,      112
+
+    //64-71
+    vldrepl.b      vr7,     a2,      63
+    z1_upsample_edge_calc_other
+    vst            vr13,    t2,      128
+
+    vst            vr13,    t2,      144 //72-79
+    vst            vr13,    t2,      160 //80-87
+
+    //88-94
+    fst.d          f13,     t2,     176
+    vstelm.w       vr13,    t2,     184,    2
+    vstelm.h       vr13,    t2,     188,    6
+
+    ld.bu          t7,      a2,     63
+    st.b           t7,      t2,     190
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W64:
+.Z1_UEDGE_W64_H16:
+    andi           t6,      a4,     16
+    beqz           t6,      .Z1_UEDGE_W64_H32
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      16
+
+    //16-23
+    vld            vr7,     a2,      15
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      32
+
+    //24-31
+    vld            vr7,     a2,      23
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      48
+
+    //32-39
+    vld            vr7,     a2,      31
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      64
+
+    //40-47
+    vld            vr7,     a2,      39
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      80
+
+    //48-55
+    vld            vr7,     a2,      47
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      96
+
+    //56-63
+    vld            vr7,     a2,      55
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      112
+
+    //64-71
+    vld            vr7,     a2,      63
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      128
+
+    //72-78
+    vld            vr7,     a2,      71
+    z1_upsample_edge_data_init2
+    fst.d          f13,     t2,     144
+    vstelm.w       vr13,    t2,     152,    2
+    vstelm.h       vr13,    t2,     156,    6
+
+    ld.bu          t7,      a2,     79
+    st.b           t7,      t2,     158
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W64_H32:
+    andi           t6,      a4,     32
+    beqz           t6,      .Z1_UEDGE_W64_H64
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      16
+
+    //16-23
+    vld            vr7,     a2,      15
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      32
+
+    //24-31
+    vld            vr7,     a2,      23
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      48
+
+    //32-39
+    vld            vr7,     a2,      31
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      64
+
+    //40-47
+    vld            vr7,     a2,      39
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      80
+
+    //48-55
+    vld            vr7,     a2,      47
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      96
+
+    //56-63
+    vld            vr7,     a2,      55
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      112
+
+    //64-71
+    vld            vr7,     a2,      63
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      128
+
+    //72-79
+    vld            vr7,     a2,      71
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      144
+
+    //80-87
+    vld            vr7,     a2,      79
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      160
+
+    //88-94
+    vld            vr7,     a2,      87
+    z1_upsample_edge_data_init2
+    fst.d          f13,     t2,     176
+    vstelm.w       vr13,    t2,     184,    2
+    vstelm.h       vr13,    t2,     188,    6
+
+    ld.bu          t7,      a2,     95
+    st.b           t7,      t2,     190
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_W64_H64:
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,     0
+
+    //8-15
+    vld            vr7,     a2,      7
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      16
+
+    //16-23
+    vld            vr7,     a2,      15
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      32
+
+    //24-31
+    vld            vr7,     a2,      23
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      48
+
+    //32-39
+    vld            vr7,     a2,      31
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      64
+
+    //40-47
+    vld            vr7,     a2,      39
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      80
+
+    //48-55
+    vld            vr7,     a2,      47
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      96
+
+    //56-63
+    vld            vr7,     a2,      55
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      112
+
+    //64-71
+    vld            vr7,     a2,      63
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      128
+
+    //72-79
+    vld            vr7,     a2,      71
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      144
+
+    //80-87
+    vld            vr7,     a2,      79
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      160
+
+    //88-95
+    vld            vr7,     a2,      87
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      176
+
+    //96-103
+    vld            vr7,     a2,      95
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      192
+
+    //104-111
+    vld            vr7,     a2,      103
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      208
+
+    //112-119
+    vld            vr7,     a2,      111
+    z1_upsample_edge_data_init1
+    vst            vr13,    t2,      224
+
+    //120-126
+    vld            vr7,     a2,      119
+    z1_upsample_edge_data_init2
+    fst.d          f13,     t2,      240
+    vstelm.w       vr13,    t2,      248,    2
+    vstelm.h       vr13,    t2,      252,    6
+
+    ld.bu          t7,      a2,      127
+    st.b           t7,      t2,      254
+    b              .Z1_UEDGE_END
+
+.Z1_UEDGE_END:
+    //upsample_edge end
+
+    or             a7,      t2,      t2   //top
+    add.d          t0,      a3,      a4
+    slli.d         t0,      t0,      1
+    addi.d         t0,      t0,      -2   //max_base_x
+    slli.d         t1,      t1,      1
+    b              .IPRED_Z1_UA_END
+
+.IPRED_Z1_NOTUA:
+    or             t5,      zero,    zero  //upsample_above=0
+    beqz           a7,      .IPRED_Z1_NOTFS
+    add.d          a7,      a3,      a4  //w+h
+    li.w           t4,      90
+    sub.d          t4,      t4,      a5
+    // ipred_get_filter_strength a6:filter_strength
+    beqz           a6,      .Z1_GETFS20
+.Z1_GETFS10:  //wh<=8
+    addi.d         t6,      a7,      -8
+    blt            zero,    t6,      .Z1_GETFS11
+    addi.d         t6,      t4,      -64
+    blt            t6,      zero,    .Z1_GETFS101
+    ori            a6,      zero,    2
+    b              .Z1_GETFS40
+.Z1_GETFS101:
+    addi.d         t6,      t4,      -40
+    blt            t6,      zero,    .Z1_GETFS30
+    ori            a6,      zero,    1
+    b              .Z1_GETFS40
+.Z1_GETFS11:  //wh<=16
+    addi.d         t6,      a7,      -16
+    blt            zero,    t6,      .Z1_GETFS12
+    addi.d         t6,      t4,      -48
+    blt            t6,      zero,    .Z1_GETFS111
+    ori            a6,      zero,    2
+    b              .Z1_GETFS40
+.Z1_GETFS111:
+    addi.d         t6,      t4,      -20
+    blt            t6,      zero,    .Z1_GETFS30
+    ori            a6,      zero,    1
+    b              .Z1_GETFS40
+.Z1_GETFS12:  //wh<=24
+    addi.d         t6,      a7,      -24
+    blt            zero,    t6,      .Z1_GETFS13
+    addi.d         t6,      t4,      -4
+    blt            t6,      zero,    .Z1_GETFS30
+    ori            a6,      zero,    3
+    b              .Z1_GETFS40
+.Z1_GETFS13:
+    ori            a6,      zero,    3
+    b              .Z1_GETFS40
+
+.Z1_GETFS20:  //wh<=8
+    addi.d         t6,      a7,      -8
+    blt            zero,    t6,      .Z1_GETFS21
+    addi.d         t6,      t4,      -56
+    blt            t6,      zero,    .Z1_GETFS30
+    ori            a6,      zero,    1
+    b              .Z1_GETFS40
+.Z1_GETFS21:  //wh<=16
+    addi.d         t6,      a7,      -16
+    blt            zero,    t6,      .Z1_GETFS22
+    addi.d         t6,      t4,      -40
+    blt            t6,      zero,    .Z1_GETFS30
+    ori            a6,      zero,    1
+    b              .Z1_GETFS40
+.Z1_GETFS22:  //wh<=24
+    addi.d         t6,      a7,      -24
+    blt            zero,    t6,      .Z1_GETFS23
+    addi.d         t6,      t4,      -32
+    blt            t6,      zero,    .Z1_GETFS221
+    ori            a6,      zero,    3
+    b              .Z1_GETFS40
+.Z1_GETFS221:
+    addi.d         t6,      t4,      -16
+    blt            t6,      zero,    .Z1_GETFS222
+    ori            a6,      zero,    2
+    b              .Z1_GETFS40
+.Z1_GETFS222:
+    addi.d         t6,      t4,      -8
+    blt            t6,      zero,    .Z1_GETFS30
+    ori            a6,      zero,    1
+    b              .Z1_GETFS40
+.Z1_GETFS23:  //wh<=32
+    addi.d         t6,      a7,      -32
+    blt            zero,    t6,      .Z1_GETFS24
+    addi.d         t6,      t4,      -32
+    blt            t6,      zero,    .Z1_GETFS231
+    ori            a6,      zero,    3
+    b              .Z1_GETFS40
+.Z1_GETFS231:
+    addi.d         t6,      t4,      -4
+    blt            t6,      zero,    .Z1_GETFS232
+    ori            a6,      zero,    2
+    b              .Z1_GETFS40
+.Z1_GETFS232:
+    ori            a6,      zero,    1
+    b              .Z1_GETFS40
+.Z1_GETFS24:
+    ori            a6,      zero,    3
+    b              .Z1_GETFS40
+.Z1_GETFS30:
+   or              a6,      zero,    zero
+.Z1_GETFS40:
+
+    beqz           a6,      .IPRED_Z1_NOTFS
+
+.IPRED_Z1_IFFS:
+    // filter_edge
+    addi.d         a6,      a6,      -1
+    slli.d         a6,      a6,      4
+    la.local       t0,      ipred_filter_edge_kernel1
+    vldx           vr1,     t0,      a6    //kernel[0-3]
+
+    la.local       t0,      ipred_filter_edge_kernel2
+    vldx           vr6,     t0,      a6    //kernel[4]
+
+.IPRED_Z1_FS_W4:
+    andi           t0,      a3,      4
+    beqz           t0,      .IPRED_Z1_FS_W8
+.IPRED_Z1_FS_W4_H4:
+    andi           t0,      a4,      4
+    beqz           t0,      .IPRED_Z1_FS_W4_H8
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init4
+    vbsrl.v        vr13,    vr7,     3
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W4_H8:
+    andi           t0,      a4,      8
+    beqz           t0,      .IPRED_Z1_FS_W4_H16
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init4
+    vbsrl.v        vr13,    vr7,     3
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-11
+    vreplvei.b     vr10,    vr7,     8
+    vextrins.b     vr10,    vr7,     0x07
+    z1_filter_edge_calc_other
+    fst.s          f12,     t2,      8
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W4_H16:
+    andi           t0,      a4,      16
+    beqz           t0,      .IPRED_Z1_FS_W4_H32
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init4
+    vbsrl.v        vr13,    vr7,     3
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vreplvei.b     vr10,    vr7,     8
+    vextrins.b     vr10,    vr7,     0x07
+    z1_filter_edge_calc_other
+    fst.d          f12,     t2,      8
+
+    //16-19
+    vreplvei.b     vr12,    vr12,    1
+    fst.s          f12,     t2,      16
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W4_H32:
+    andi           t0,      a4,      32
+    beqz           t0,      .IPRED_Z1_FS_W4_H64
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init4
+    vbsrl.v        vr13,    vr7,     3
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vreplvei.b     vr10,    vr7,     8
+    vextrins.b     vr10,    vr7,     0x07
+    z1_filter_edge_calc_other
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vreplvei.b     vr12,    vr12,    1
+    fst.d          f12,     t2,      16
+
+    fst.d          f12,     t2,      24 //24-31
+    fst.s          f12,     t2,      32 //32-35
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W4_H64:
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init4
+    vbsrl.v        vr13,    vr7,     3
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vreplvei.b     vr10,    vr7,     8
+    vextrins.b     vr10,    vr7,     0x07
+    z1_filter_edge_calc_other
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vreplvei.b     vr12,    vr12,    1
+    fst.d          f12,     t2,      16
+
+    fst.d          f12,     t2,      24 //24-31
+    fst.d          f12,     t2,      32 //32-39
+    fst.d          f12,     t2,      40 //40-47
+    fst.d          f12,     t2,      48 //48-55
+    fst.d          f12,     t2,      56 //56-63
+    fst.s          f12,     t2,      64 //64-67
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W8:
+    andi           t0,      a3,      8
+    beqz           t0,      .IPRED_Z1_FS_W16
+.IPRED_Z1_FS_W8_H4:
+    andi           t0,      a4,      4
+    beqz           t0,      .IPRED_Z1_FS_W8_H8
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-11
+    vld            vr7,     a2,      6
+    vbsrl.v        vr11,    vr7,     1
+    vbsrl.v        vr12,    vr7,     2
+    vbsrl.v        vr13,    vr7,     3
+    vextrins.b     vr13,    vr13,    0x32
+    vsllwil.hu.bu  vr10,    vr7,     0
+    vsllwil.hu.bu  vr11,    vr11,    0
+    vsllwil.hu.bu  vr12,    vr12,    0
+    vsllwil.hu.bu  vr13,    vr13,    0
+    z1_filter_edge_calc_loop1
+
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x21
+    vextrins.b     vr13,    vr13,    0x31
+    z1_filter_edge_calc_loop2
+    fst.s          f12,     t2,      8
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W8_H8:
+    andi           t0,      a4,      8
+    beqz           t0,      .IPRED_Z1_FS_W8_H16
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W8_H16:
+    andi           t0,      a4,      16
+    beqz           t0,      .IPRED_Z1_FS_W8_H32
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vreplvei.b     vr10,    vr7,     9
+    vextrins.b     vr10,    vr7,     0x08
+    z1_filter_edge_calc_other
+    fst.d          f12,     t2,      16
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W8_H32:
+    andi           t0,      a4,      32
+    beqz           t0,      .IPRED_Z1_FS_W8_H64
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vreplvei.b     vr10,    vr7,     9
+    vextrins.b     vr10,    vr7,     0x08
+    z1_filter_edge_calc_other
+    fst.d          f12,     t2,      16
+
+    //24-31
+    vreplvei.b     vr12,    vr12,    1
+    fst.d          f12,     t2,      24
+
+    //32-39
+    fst.d          f12,     t2,      32
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W8_H64:
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vreplvei.b     vr10,    vr7,     9
+    vextrins.b     vr10,    vr7,     0x08
+    z1_filter_edge_calc_other
+    fst.d          f12,     t2,      16
+
+    //24-31
+    vreplvei.b     vr12,    vr12,    1
+    fst.d          f12,     t2,      24
+
+    fst.d          f12,     t2,      32  //32-39
+    fst.d          f12,     t2,      40  //40-47
+    fst.d          f12,     t2,      48  //48-55
+    fst.d          f12,     t2,      56  //56-63
+    fst.d          f12,     t2,      64  //64-71
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W16:
+    andi           t0,      a3,      16
+    beqz           t0,      .IPRED_Z1_FS_W32
+.IPRED_Z1_FS_W16_H4:
+    andi           t0,      a4,      4
+    beqz           t0,      .IPRED_Z1_FS_W16_H8
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-19
+    vld            vr7,     a2,      14
+    vbsrl.v        vr11,    vr7,     1
+    vbsrl.v        vr12,    vr7,     2
+    vbsrl.v        vr13,    vr7,     3
+    vextrins.b     vr13,    vr13,    0x32
+    vsllwil.hu.bu  vr10,    vr7,     0
+    vsllwil.hu.bu  vr11,    vr11,    0
+    vsllwil.hu.bu  vr12,    vr12,    0
+    vsllwil.hu.bu  vr13,    vr13,    0
+    z1_filter_edge_calc_loop1
+
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x21
+    vextrins.b     vr13,    vr13,    0x31
+    z1_filter_edge_calc_loop2
+    fst.s          f12,     t2,      16
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W16_H8:
+    andi           t0,      a4,      8
+    beqz           t0,      .IPRED_Z1_FS_W16_H16
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vld            vr7,     a2,      14
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      16
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W16_H16:
+    andi           t0,      a4,      16
+    beqz           t0,      .IPRED_Z1_FS_W16_H32
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vld            vr7,     a2,      14
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      16
+
+    //24-31
+    vld            vr7,     a2,      22
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      24
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W16_H32:
+    andi           t0,      a4,      32
+    beqz           t0,      .IPRED_Z1_FS_W16_H64
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vld            vr7,     a2,      14
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      16
+
+    //24-31
+    vld            vr7,     a2,      22
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      24
+
+    //32-39
+    vreplvei.b     vr10,    vr7,     9
+    vextrins.b     vr10,    vr7,     0x08
+    z1_filter_edge_calc_other
+    fst.d          f12,     t2,      32
+
+    //40-47
+    vreplvei.b     vr12,    vr12,    1
+    fst.d          f12,     t2,      40
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W16_H64:
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vld            vr7,     a2,      14
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      16
+
+    //24-31
+    vld            vr7,     a2,      22
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      24
+
+    //32-39
+    vreplvei.b     vr10,    vr7,     9
+    vextrins.b     vr10,    vr7,     0x08
+    z1_filter_edge_calc_other
+    fst.d          f12,     t2,      32
+
+    //40-47
+    vreplvei.b     vr12,    vr12,    1
+    fst.d          f12,     t2,      40
+
+    fst.d          f12,     t2,      48 //48-55
+    fst.d          f12,     t2,      56 //56-63
+    fst.d          f12,     t2,      64 //64-71
+    fst.d          f12,     t2,      72 //72-81
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W32:
+    andi           t0,      a3,      32
+    beqz           t0,      .IPRED_Z1_FS_W64
+.IPRED_Z1_FS_W32_H8:
+    andi           t0,      a4,      8
+    beqz           t0,      .IPRED_Z1_FS_W32_H16
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vld            vr7,     a2,      14
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      16
+
+    //24-31
+    vld            vr7,     a2,      22
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      24
+
+    //32-39
+    vld            vr7,     a2,      30
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      32
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W32_H16:
+    andi           t0,      a4,      16
+    beqz           t0,      .IPRED_Z1_FS_W32_H32
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vld            vr7,     a2,      14
+    z1_filter_edge_data_init2
+
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      16
+
+    //24-31
+    vld            vr7,     a2,      22
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      24
+
+    //32-39
+    vld            vr7,     a2,      30
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      32
+
+    //40-47
+    vld            vr7,     a2,      38
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      40
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W32_H32:
+    andi           t0,      a4,      32
+    beqz           t0,      .IPRED_Z1_FS_W32_H64
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vld            vr7,     a2,      14
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      16
+
+    //24-31
+    vld            vr7,     a2,      22
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      24
+
+    //32-39
+    vld            vr7,     a2,      30
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      32
+
+    //40-47
+    vld            vr7,     a2,      38
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      40
+
+    //48-55
+    vld            vr7,     a2,      46
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      48
+
+    //56-63
+    vld            vr7,     a2,      54
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      56
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W32_H64:
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vld            vr7,     a2,      14
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      16
+
+    //24-31
+    vld            vr7,     a2,      22
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      24
+
+    //32-39
+    vld            vr7,     a2,      30
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      32
+
+    //40-47
+    vld            vr7,     a2,      38
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      40
+
+    //48-55
+    vld            vr7,     a2,      46
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      48
+
+    //56-63
+    vld            vr7,     a2,      54
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      56
+
+    //64-71
+    vreplvei.b     vr10,    vr7,     9
+    vextrins.b     vr10,    vr7,     0x08
+    z1_filter_edge_calc_other
+    fst.d          f12,     t2,      64
+
+    //72-89
+    vreplvei.b     vr12,    vr12,    1
+    fst.d          f12,     t2,      72
+
+    fst.d          f12,     t2,      80 //80-87
+    fst.d          f12,     t2,      88 //88-95
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W64:
+.IPRED_Z1_FS_W64_H16:
+    andi           t0,      a4,      16
+    beqz           t0,      .IPRED_Z1_FS_W64_H32
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vld            vr7,     a2,      14
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      16
+
+    //24-31
+    vld            vr7,     a2,      22
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      24
+
+    //32-39
+    vld            vr7,     a2,      30
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      32
+
+    //40-47
+    vld            vr7,     a2,      38
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      40
+
+    //48-55
+    vld            vr7,     a2,      46
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      48
+
+    //56-63
+    vld            vr7,     a2,      54
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      56
+
+    //64-71
+    vld            vr7,     a2,      62
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      64
+
+    //72-79
+    vld            vr7,     a2,      70
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      72
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W64_H32:
+    andi           t0,      a4,      32
+    beqz           t0,      .IPRED_Z1_FS_W64_H64
+
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vld            vr7,     a2,      14
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      16
+
+    //24-31
+    vld            vr7,     a2,      22
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      24
+
+    //32-39
+    vld            vr7,     a2,      30
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      32
+
+    //40-47
+    vld            vr7,     a2,      38
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      40
+
+    //48-55
+    vld            vr7,     a2,      46
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      48
+
+    //56-63
+    vld            vr7,     a2,      54
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      56
+
+    //64-71
+    vld            vr7,     a2,      62
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      64
+
+    //72-79
+    vld            vr7,     a2,      70
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      72
+
+    //80-87
+    vld            vr7,     a2,      78
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      80
+
+    //88-95
+    vld            vr7,     a2,      86
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      88
+
+    b              .IPRED_Z1_FS_END
+
+.IPRED_Z1_FS_W64_H64:
+    //0-7
+    vld            vr7,     a2,      -1
+    z1_filter_edge_data_init1
+    vbsrl.v        vr13,    vr7,     3
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      0
+
+    //8-15
+    vld            vr7,     a2,      6
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      8
+
+    //16-23
+    vld            vr7,     a2,      14
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      16
+
+    //24-31
+    vld            vr7,     a2,      22
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      24
+
+    //32-39
+    vld            vr7,     a2,      30
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      32
+
+    //40-47
+    vld            vr7,     a2,      38
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      40
+
+    //48-55
+    vld            vr7,     a2,      46
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      48
+
+    //56-63
+    vld            vr7,     a2,      54
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      56
+
+    //64-71
+    vld            vr7,     a2,      62
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      64
+
+    //72-79
+    vld            vr7,     a2,      70
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      72
+
+    //80-87
+    vld            vr7,     a2,      78
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      80
+
+    //88-95
+    vld            vr7,     a2,      86
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      88
+
+    //96-103
+    vld            vr7,     a2,      94
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      96
+
+    //104-111
+    vld            vr7,     a2,      102
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      104
+
+    //112-119
+    vld            vr7,     a2,      110
+    z1_filter_edge_data_init2
+    vbsrl.v        vr13,    vr7,     4
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      112
+
+    //120-127
+    vld            vr7,     a2,      118
+    z1_filter_edge_data_init3
+    vbsrl.v        vr13,    vr7,     4
+    vextrins.b     vr13,    vr13,    0x65
+    vextrins.b     vr13,    vr13,    0x75
+    z1_filter_edge_calc_loop2
+    fst.d          f12,     t2,      120
+
+.IPRED_Z1_FS_END:
+    addi.d         t0,      a7,      -1   //max_base_x
+    or             a7,      t2,      t2   //top
+    b              .IPRED_Z1_UA_END
+
+.IPRED_Z1_NOTFS:
+    or             a7,      a2,      a2   //top
+    // imin_gr
+    blt            a3,      a4,      .Z1_IMIN1
+    or             t0,      a4,      a4
+    b              .Z1_IMIN2
+.Z1_IMIN1:
+    or             t0,      a3,      a3
+.Z1_IMIN2:
+
+    add.d          t0,      a3,      t0
+    addi.d         t0,      t0,      -1   //max_base_x
+
+.IPRED_Z1_UA_END:
+    //st dst, t1:dx  a2 a6 t6 t7
+    beqz           t5,      .Z1_UA0
+
+    li.w           a5,      64
+    vreplgr2vr.h   vr0,     a5
+    vsrai.h        vr7,     vr0,     1
+    or             t2,      zero,    zero  //y
+    or             t3,      t1,      t1    //xpos
+.Z1_LOOPY:
+    andi           t4,      t3,      0x3e  //frac
+    vreplgr2vr.h   vr1,     t4
+    vsub.h         vr2,     vr0,     vr1
+    or             a6,      zero,    zero  //x
+    or             a2,      zero,    zero  //base_num
+    srai.d         t6,      t3,      6     //base
+
+    or             t7,      t6,      t6
+    bge            t7,      t0,      .Z1_LOOPX
+.Z1_BASENUM:
+    addi.d         a2,      a2,      1
+    addi.d         t7,      t7,      2
+    blt            t7,      t0,      .Z1_BASENUM
+
+.Z1_LOOPX:
+    blt            a2,      a3,      .Z1_LOOPX_BASEMAX
+
+    srai.d         t8,      a3,      3  //loop param
+    beqz           t8,      .Z1_LOOPX_W4
+.Z1_LOOPX_W8:
+    add.d          t5,      a7,      t6
+    vld            vr3,     t5,      0
+    vpickev.b      vr5,     vr3,     vr3  //0 2 4 6...
+    vpickod.b      vr6,     vr3,     vr3  //1 3 5 7...
+    vsllwil.hu.bu  vr5,     vr5,     0
+    vsllwil.hu.bu  vr6,     vr6,     0
+
+    vmul.h         vr3,     vr5,     vr2
+    vmadd.h        vr3,     vr6,     vr1
+    vadd.h         vr3,     vr3,     vr7
+    vsrai.h        vr3,     vr3,     6
+    vsrlni.b.h     vr3,     vr3,     0
+    fstx.d         f3,      a0,      a6
+
+    addi.d         a6,      a6,      8
+    addi.d         t6,      t6,      16
+    addi.d         t8,      t8,      -1
+    bnez           t8,      .Z1_LOOPX_W8
+    b              .Z1_LOOPY_END
+.Z1_LOOPX_W4:
+    vldx           vr3,     a7,      t6
+    vsllwil.hu.bu  vr3,     vr3,     0
+    vpickev.h      vr5,     vr3,     vr3  //0 2 4 6...
+    vpickod.h      vr6,     vr3,     vr3  //1 3 5 7...
+
+    vmul.h         vr3,     vr5,     vr2
+    vmadd.h        vr3,     vr6,     vr1
+    vadd.h         vr3,     vr3,     vr7
+    vsrai.h        vr3,     vr3,     6
+    vsrlni.b.h     vr3,     vr3,     0
+    fstx.s         f3,      a0,      a6
+    b              .Z1_LOOPY_END
+.Z1_LOOPX_BASEMAX:
+    srai.d         t8,      a2,      3  //loop param
+    beqz           t8,      .Z1_LOOPX_BASEMAX4
+.Z1_LOOPX_BASEMAX8:
+    add.d          t5,      a7,      t6
+    vld            vr3,     t5,      0
+    vpickev.b      vr5,     vr3,     vr3  //0 2 4 6...
+    vpickod.b      vr6,     vr3,     vr3  //1 3 5 7...
+    vsllwil.hu.bu  vr5,     vr5,     0
+    vsllwil.hu.bu  vr6,     vr6,     0
+
+    vmul.h         vr3,     vr5,     vr2
+    vmadd.h        vr3,     vr6,     vr1
+    vadd.h         vr3,     vr3,     vr7
+    vsrai.h        vr3,     vr3,     6
+    vsrlni.b.h     vr3,     vr3,     0
+    fstx.d         f3,      a0,      a6
+
+    addi.d         a6,      a6,      8
+    addi.d         t6,      t6,      16
+    addi.d         t8,      t8,      -1
+    bnez           t8,      .Z1_LOOPX_BASEMAX8
+.Z1_LOOPX_BASEMAX4:
+    andi           t8,      a2,      4
+    beqz           t8,      .Z1_LOOPX_BASEMAX2
+
+    vldx           vr3,     a7,      t6
+    vsllwil.hu.bu  vr3,     vr3,     0
+    vpickev.h      vr5,     vr3,     vr3  //0 2 4 6...
+    vpickod.h      vr6,     vr3,     vr3  //1 3 5 7...
+
+    vmul.h         vr3,     vr5,     vr2
+    vmadd.h        vr3,     vr6,     vr1
+    vadd.h         vr3,     vr3,     vr7
+    vsrai.h        vr3,     vr3,     6
+    vsrlni.b.h     vr3,     vr3,     0
+    fstx.s         f3,      a0,      a6
+
+    addi.d         a6,      a6,      4
+    addi.d         t6,      t6,      8
+.Z1_LOOPX_BASEMAX2:
+    andi           t8,      a2,     2
+    beqz           t8,      .Z1_LOOPX_BASEMAX1
+
+    vldx           vr3,     a7,      t6
+    vsllwil.hu.bu  vr3,     vr3,     0
+    vpickev.h      vr5,     vr3,     vr3  //0 2 4 6...
+    vpickod.h      vr6,     vr3,     vr3  //1 3 5 7...
+
+    vmul.h         vr3,     vr5,     vr2
+    vmadd.h        vr3,     vr6,     vr1
+    vadd.h         vr3,     vr3,     vr7
+    vsrai.h        vr3,     vr3,     6
+    vsrlni.b.h     vr3,     vr3,     0
+    vpickve2gr.bu  t7,      vr3,     0
+    vpickve2gr.bu  t8,      vr3,     1
+    stx.b          t7,      a0,      a6
+    addi.d         a6,      a6,      1
+    stx.b          t8,      a0,      a6
+    addi.d         a6,      a6,      1
+    addi.d         t6,      t6,      4
+.Z1_LOOPX_BASEMAX1:
+    andi           t8,      a2,     1
+    beqz           t8,      .Z1_LOOPX_BASEMAX_MSET
+
+    add.d          a2,      a7,      t6
+    sub.d          t7,      a5,      t4
+    ld.bu          t8,      a2,      0
+    mul.w          t7,      t7,      t8
+    ld.bu          t8,      a2,      1
+    mul.w          t8,      t8,      t4
+    add.d          t7,      t7,      t8
+    addi.d         t7,      t7,      32
+    srai.d         t7,      t7,      6
+    stx.b          t7,      a0,      a6
+
+    addi.d         a6,      a6,      1
+.Z1_LOOPX_BASEMAX_MSET:  //memset
+    add.d          t6,      a0,      a6  //dst
+    add.d          t7,      a7,      t0  //src
+    sub.d          a2,      a3,      a6  //size
+    pixel_set_8bpc_allw t6, t7, a2, t8, t4
+.Z1_LOOPY_END:
+    addi.d         t2,      t2,      1
+    add.d          a0,      a0,      a1
+    add.d          t3,      t3,      t1
+    blt            t2,      a4,      .Z1_LOOPY
+    b              .Z1_END
+
+.Z1_UA0:
+    li.w           a5,      64
+    vreplgr2vr.h   vr0,     a5
+    vsrai.h        vr7,     vr0,     1
+    or             t2,      zero,    zero  //y
+    or             t3,      t1,      t1    //xpos
+.Z1_UA0_LOOPY:
+    andi           t4,      t3,      0x3e  //frac
+    vreplgr2vr.h   vr1,     t4
+    vsub.h         vr2,     vr0,     vr1
+    or             a6,      zero,    zero  //x
+    srai.d         t6,      t3,      6     //base
+
+    sub.d          a2,      t0,      t6     //a2:base_num
+    blt            a2,      zero,    .Z1_UA0_BASENUM
+    b              .Z1_UA0_LOOPX
+.Z1_UA0_BASENUM:
+    or             a2,      zero,    zero
+
+.Z1_UA0_LOOPX:
+    blt            a2,      a3,      .Z1_UA0_LOOPX_BASEMAX
+
+    srai.d         t8,      a3,      3  //loop param
+    beqz           t8,      .Z1_UA0_LOOPX_W4
+.Z1_UA0_LOOPX_W8:
+    add.d          t5,      a7,      t6
+    vld            vr5,     t5,      0
+    vld            vr6,     t5,      1
+    vsllwil.hu.bu  vr5,     vr5,     0
+    vsllwil.hu.bu  vr6,     vr6,     0
+
+    vmul.h         vr3,     vr5,     vr2
+    vmadd.h        vr3,     vr6,     vr1
+    vadd.h         vr3,     vr3,     vr7
+    vsrai.h        vr3,     vr3,     6
+    vsrlni.b.h     vr3,     vr3,     0
+    fstx.d         f3,      a0,      a6
+
+    addi.d         a6,      a6,      8
+    addi.d         t6,      t6,      8
+    addi.d         t8,      t8,      -1
+    bnez           t8,      .Z1_UA0_LOOPX_W8
+    b              .Z1_UA0_LOOPY_END
+.Z1_UA0_LOOPX_W4:
+    vldx           vr5,     a7,      t6
+    vsllwil.hu.bu  vr5,     vr5,     0
+    vbsrl.v        vr6,     vr5,     2
+
+    vmul.h         vr3,     vr5,     vr2
+    vmadd.h        vr3,     vr6,     vr1
+    vadd.h         vr3,     vr3,     vr7
+    vsrai.h        vr3,     vr3,     6
+    vsrlni.b.h     vr3,     vr3,     0
+    fstx.s         f3,      a0,      a6
+    b              .Z1_UA0_LOOPY_END
+.Z1_UA0_LOOPX_BASEMAX:
+    srai.d         t8,      a2,      3  //loop param
+    beqz           t8,      .Z1_UA0_LOOPX_BASEMAX4
+.Z1_UA0_LOOPX_BASEMAX8:
+    add.d          t5,      a7,      t6
+    vld            vr5,     t5,      0
+    vld            vr6,     t5,      1
+    vsllwil.hu.bu  vr5,     vr5,     0
+    vsllwil.hu.bu  vr6,     vr6,     0
+
+    vmul.h         vr3,     vr5,     vr2
+    vmadd.h        vr3,     vr6,     vr1
+    vadd.h         vr3,     vr3,     vr7
+    vsrai.h        vr3,     vr3,     6
+    vsrlni.b.h     vr3,     vr3,     0
+    fstx.d         f3,      a0,      a6
+
+    addi.d         a6,      a6,      8
+    addi.d         t6,      t6,      8
+    addi.d         t8,      t8,      -1
+    bnez           t8,      .Z1_UA0_LOOPX_BASEMAX8
+.Z1_UA0_LOOPX_BASEMAX4:
+    andi           t8,      a2,      4
+    beqz           t8,      .Z1_UA0_LOOPX_BASEMAX2
+
+    vldx           vr5,     a7,      t6
+    vsllwil.hu.bu  vr5,     vr5,     0
+    vbsrl.v        vr6,     vr5,     2
+
+    vmul.h         vr3,     vr5,     vr2
+    vmadd.h        vr3,     vr6,     vr1
+    vadd.h         vr3,     vr3,     vr7
+    vsrai.h        vr3,     vr3,     6
+    vsrlni.b.h     vr3,     vr3,     0
+    fstx.s         f3,      a0,      a6
+
+    addi.d         a6,      a6,      4
+    addi.d         t6,      t6,      4
+.Z1_UA0_LOOPX_BASEMAX2:
+    andi           t8,      a2,     2
+    beqz           t8,      .Z1_UA0_LOOPX_BASEMAX1
+
+    vldx           vr5,     a7,      t6
+    vsllwil.hu.bu  vr5,     vr5,     0
+    vbsrl.v        vr6,     vr5,     2
+
+    vmul.h         vr3,     vr5,     vr2
+    vmadd.h        vr3,     vr6,     vr1
+    vadd.h         vr3,     vr3,     vr7
+    vsrai.h        vr3,     vr3,     6
+    vsrlni.b.h     vr3,     vr3,     0
+    vpickve2gr.bu  t7,      vr3,     0
+    vpickve2gr.bu  t8,      vr3,     1
+    stx.b          t7,      a0,      a6
+    addi.d         a6,      a6,      1
+    stx.b          t8,      a0,      a6
+    addi.d         a6,      a6,      1
+    addi.d         t6,      t6,      2
+.Z1_UA0_LOOPX_BASEMAX1:
+    andi           t8,      a2,     1
+    beqz           t8,      .Z1_UA0_LOOPX_BASEMAX_MSET
+
+    add.d          a2,      a7,      t6
+    sub.d          t7,      a5,      t4
+    ld.bu          t8,      a2,      0
+    mul.w          t7,      t7,      t8
+    ld.bu          t8,      a2,      1
+    mul.w          t8,      t8,      t4
+    add.d          t7,      t7,      t8
+    addi.d         t7,      t7,      32
+    srai.d         t7,      t7,      6
+    stx.b          t7,      a0,      a6
+
+    addi.d         a6,      a6,      1
+.Z1_UA0_LOOPX_BASEMAX_MSET:  //memset
+    add.d          t6,      a0,      a6  //dst
+    add.d          t7,      a7,      t0  //src
+    sub.d          a2,      a3,      a6  //size
+    pixel_set_8bpc_allw t6, t7, a2, t8, t4
+.Z1_UA0_LOOPY_END:
+    addi.d         t2,      t2,      1
+    add.d          a0,      a0,      a1
+    add.d          t3,      t3,      t1
+    blt            t2,      a4,      .Z1_UA0_LOOPY
+
+.Z1_END:
+    addi.d         sp,      sp,      128
+endfunc
+
diff --git a/src/loongarch/ipred.h b/src/loongarch/ipred.h
new file mode 100644
index 0000000..c9f4bc1
--- /dev/null
+++ b/src/loongarch/ipred.h
@@ -0,0 +1,96 @@
+/*
+ * Copyright © 2024, VideoLAN and dav1d authors
+ * Copyright © 2024, Loongson Technology Corporation Limited
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef DAV1D_SRC_LOONGARCH_IPRED_H
+#define DAV1D_SRC_LOONGARCH_IPRED_H
+
+#include "config.h"
+#include "src/ipred.h"
+#include "src/cpu.h"
+#include "src/tables.h"
+
+#define MULTIPLIER_1x2 0x5556
+#define MULTIPLIER_1x4 0x3334
+#define BASE_SHIFT 16
+
+#define init_fn(type0, type1, name, suffix) \
+    c->type0[type1] = BF(dav1d_##name, suffix)
+
+#define init_angular_ipred_fn(type, name, suffix) \
+    init_fn(intra_pred, type, name, suffix)
+#define init_cfl_pred_fn(type, name, suffix) \
+    init_fn(cfl_pred, type, name, suffix)
+
+decl_angular_ipred_fn(BF(dav1d_ipred_dc, lsx));
+decl_angular_ipred_fn(BF(dav1d_ipred_dc_128, lsx));
+decl_angular_ipred_fn(BF(dav1d_ipred_dc_top, lsx));
+decl_angular_ipred_fn(BF(dav1d_ipred_dc_left, lsx));
+decl_angular_ipred_fn(BF(dav1d_ipred_h, lsx));
+decl_angular_ipred_fn(BF(dav1d_ipred_v, lsx));
+decl_angular_ipred_fn(BF(dav1d_ipred_paeth, lsx));
+decl_angular_ipred_fn(BF(dav1d_ipred_smooth, lsx));
+decl_angular_ipred_fn(BF(dav1d_ipred_smooth_v, lsx));
+decl_angular_ipred_fn(BF(dav1d_ipred_smooth_h, lsx));
+decl_angular_ipred_fn(BF(dav1d_ipred_filter, lsx));
+decl_angular_ipred_fn(BF(dav1d_ipred_z1, lsx));
+
+decl_cfl_pred_fn(BF(dav1d_ipred_cfl, lsx));
+decl_cfl_pred_fn(BF(dav1d_ipred_cfl_128, lsx));
+decl_cfl_pred_fn(BF(dav1d_ipred_cfl_top, lsx));
+decl_cfl_pred_fn(BF(dav1d_ipred_cfl_left, lsx));
+
+decl_pal_pred_fn(BF(dav1d_pal_pred, lsx));
+
+static ALWAYS_INLINE void intra_pred_dsp_init_loongarch(Dav1dIntraPredDSPContext *const c) {
+    const unsigned flags = dav1d_get_cpu_flags();
+
+    if (!(flags & DAV1D_LOONGARCH_CPU_FLAG_LSX)) return;
+
+#if BITDEPTH == 8
+    init_angular_ipred_fn(DC_PRED,       ipred_dc,       lsx);
+    init_angular_ipred_fn(DC_128_PRED,   ipred_dc_128,   lsx);
+    init_angular_ipred_fn(TOP_DC_PRED,   ipred_dc_top,   lsx);
+    init_angular_ipred_fn(LEFT_DC_PRED,  ipred_dc_left,  lsx);
+    init_angular_ipred_fn(HOR_PRED,      ipred_h,        lsx);
+    init_angular_ipred_fn(VERT_PRED,     ipred_v,        lsx);
+    init_angular_ipred_fn(PAETH_PRED,    ipred_paeth,    lsx);
+    init_angular_ipred_fn(SMOOTH_PRED,   ipred_smooth,   lsx);
+    init_angular_ipred_fn(SMOOTH_V_PRED, ipred_smooth_v, lsx);
+    init_angular_ipred_fn(SMOOTH_H_PRED, ipred_smooth_h, lsx);
+    init_angular_ipred_fn(FILTER_PRED,   ipred_filter,   lsx);
+    init_angular_ipred_fn(Z1_PRED,       ipred_z1,       lsx);
+
+    init_cfl_pred_fn(DC_PRED,      ipred_cfl,      lsx);
+    init_cfl_pred_fn(DC_128_PRED,  ipred_cfl_128,  lsx);
+    init_cfl_pred_fn(TOP_DC_PRED,  ipred_cfl_top,  lsx);
+    init_cfl_pred_fn(LEFT_DC_PRED, ipred_cfl_left, lsx);
+
+    c->pal_pred = BF(dav1d_pal_pred, lsx);
+#endif
+}
+
+#endif /* DAV1D_SRC_LOONGARCH_IPRED_H */
diff --git a/src/loongarch/itx.S b/src/loongarch/itx.S
index fc0c79e..9cd5968 100644
--- a/src/loongarch/itx.S
+++ b/src/loongarch/itx.S
@@ -26,6531 +26,2045 @@
  */
 
 #include "src/loongarch/loongson_asm.S"
+#include "src/loongarch/loongson_util.S"
+
+.macro PUSH_REG
+    addi.d           sp,     sp,    -64
+    fst.d            f24,    sp,     0
+    fst.d            f25,    sp,     8
+    fst.d            f26,    sp,     16
+    fst.d            f27,    sp,     24
+    fst.d            f28,    sp,     32
+    fst.d            f29,    sp,     40
+    fst.d            f30,    sp,     48
+    fst.d            f31,    sp,     56
+.endm
+
+.macro POP_REG
+    fld.d            f24,    sp,     0
+    fld.d            f25,    sp,     8
+    fld.d            f26,    sp,     16
+    fld.d            f27,    sp,     24
+    fld.d            f28,    sp,     32
+    fld.d            f29,    sp,     40
+    fld.d            f30,    sp,     48
+    fld.d            f31,    sp,     56
+    addi.d           sp,     sp,     64
+.endm
+
+.macro malloc_space number
+    li.w          t0,       \number
+    sub.d         sp,       sp,       t0
+    addi.d        sp,       sp,       -64
+    PUSH_REG
+.endm
+
+.macro free_space number
+    POP_REG
+    li.w          t0,       \number
+    add.d         sp,       sp,       t0
+    addi.d        sp,       sp,       64
+.endm
+
+.macro iwht4
+    vadd.h        vr0,       vr0,     vr1
+    vsub.h        vr4,       vr2,     vr3
+    vsub.h        vr5,       vr0,     vr4
+    vsrai.h       vr5,       vr5,     1
+    vsub.h        vr2,       vr5,     vr1
+    vsub.h        vr1,       vr5,     vr3
+    vadd.h        vr3,       vr4,     vr2
+    vsub.h        vr0,       vr0,     vr1
+.endm
+
+.macro DST_ADD_W4 in0, in1, in2, in3, in4, in5
+    vilvl.w       \in0,     \in1,     \in0  // 0 1  2  3  4  5  6  7 x ...
+    vilvl.w       \in2,     \in3,     \in2  // 8 9 10 11 12 13 14 15 x ...
+    vsllwil.hu.bu \in0,     \in0,     0
+    vsllwil.hu.bu \in2,     \in2,     0
+    vadd.h        \in0,     \in4,     \in0
+    vadd.h        \in2,     \in5,     \in2
+    vssrani.bu.h  \in2,     \in0,     0
+    vstelm.w      \in2,     a0,       0,    0
+    vstelmx.w     \in2,     a0,       a1,   1
+    vstelmx.w     \in2,     a0,       a1,   2
+    vstelmx.w     \in2,     a0,       a1,   3
+.endm
+
+.macro VLD_DST_ADD_W4 in0, in1
+    vld           vr0,      a0,       0
+    vldx          vr1,      a0,       a1
+    vld           vr2,      t2,       0
+    vldx          vr3,      t2,       a1
+
+    DST_ADD_W4    vr0, vr1, vr2, vr3, \in0, \in1
+.endm
 
-/*
-void inv_txfm_add_wht_wht_4x4_c(pixel *dst, const ptrlowff_t stride,
-                                coef *const coeff, const int eob
-                                HIGHBD_DECL_SUFFIX)
-*/
 function inv_txfm_add_wht_wht_4x4_8bpc_lsx
     vld           vr0,       a2,      0
     vld           vr2,       a2,      16
 
-    vreplgr2vr.h  vr20,      zero
-
+    vxor.v        vr20,      vr20,    vr20
     vsrai.h       vr0,       vr0,     2
     vsrai.h       vr2,       vr2,     2
-
     vst           vr20,      a2,      0
-
     vpickod.d     vr1,       vr0,     vr0
     vpickod.d     vr3,       vr2,     vr2
+    vst           vr20,      a2,      16
 
-    vadd.h        vr4,       vr0,     vr1
-    vsub.h        vr5,       vr2,     vr3
-    vsub.h        vr6,       vr4,     vr5
-    vsrai.h       vr6,       vr6,     1
-    vsub.h        vr0,       vr6,     vr3
-    vsub.h        vr2,       vr6,     vr1
-    vsub.h        vr1,       vr4,     vr0
-    vadd.h        vr3,       vr5,     vr2
+    iwht4
 
-    vst           vr20,      a2,      16
+    LSX_TRANSPOSE4x4_H vr0, vr1, vr2, vr3, vr0, vr1, vr2, vr3, vr4, vr5
+
+    iwht4
 
-    vilvl.h       vr4,       vr0,     vr1
-    vilvl.h       vr5,       vr3,     vr2
-    vilvl.w       vr0,       vr5,     vr4
-    vilvh.w       vr2,       vr5,     vr4
-    vilvh.d       vr1,       vr0,     vr0
-    vilvh.d       vr3,       vr2,     vr2
-
-    vadd.h        vr4,       vr0,     vr1
-    vsub.h        vr5,       vr2,     vr3
-    vsub.h        vr6,       vr4,     vr5
-    vsrai.h       vr6,       vr6,     1
-    vsub.h        vr0,       vr6,     vr3
-    vsub.h        vr2,       vr6,     vr1
-    vsub.h        vr1,       vr4,     vr0
-    vadd.h        vr3,       vr5,     vr2
-
-    vld           vr4,       a0,      0
-    vldx          vr5,       a0,      a1
-    alsl.d        t0,        a1,      a0,    1
-    vld           vr6,       t0,      0
-    vldx          vr7,       t0,      a1
-
-    vsllwil.hu.bu vr4,       vr4,     0
-    vsllwil.hu.bu vr5,       vr5,     0
-    vsllwil.hu.bu vr6,       vr6,     0
-    vsllwil.hu.bu vr7,       vr7,     0
-    vilvl.d       vr1,       vr0,     vr1
-    vilvl.d       vr2,       vr3,     vr2
-    vilvl.d       vr4,       vr5,     vr4
-    vilvl.d       vr6,       vr7,     vr6
-    vadd.h        vr1,       vr1,     vr4
-    vadd.h        vr2,       vr2,     vr6
-    vssrani.bu.h  vr2,       vr1,     0
-
-    vstelm.w      vr2,       a0,      0,     0
-    add.d         a0,        a0,      a1
-    vstelm.w      vr2,       a0,      0,     1
-    add.d         a0,        a0,      a1
-    vstelm.w      vr2,       a0,      0,     2
-    add.d         a0,        a0,      a1
-    vstelm.w      vr2,       a0,      0,     3
+    vilvl.d       vr4,       vr1,     vr0
+    vilvl.d       vr5,       vr3,     vr2
+    alsl.d        t2,        a1,      a0,    1
+    VLD_DST_ADD_W4 vr4, vr5
 endfunc
 
 const idct_coeffs, align=4
-    // idct4
     .word          2896, 2896*8, 1567, 3784
-    // idct8
     .word          799, 4017, 3406, 2276
-    // idct16
     .word          401, 4076, 3166, 2598
     .word          1931, 3612, 3920, 1189
-    // idct32
     .word          201, 4091, 3035, 2751
     .word          1751, 3703, 3857, 1380
     .word          995, 3973, 3513, 2106
     .word          2440, 3290, 4052, 601
 endconst
 
-.macro vld_x8 src, start, stride, in0, in1, in2, in3, in4, in5, in6, in7
-    vld           \in0,     \src,     \start
-    vld           \in1,     \src,     \start+(\stride*1)
-    vld           \in2,     \src,     \start+(\stride*2)
-    vld           \in3,     \src,     \start+(\stride*3)
-    vld           \in4,     \src,     \start+(\stride*4)
-    vld           \in5,     \src,     \start+(\stride*5)
-    vld           \in6,     \src,     \start+(\stride*6)
-    vld           \in7,     \src,     \start+(\stride*7)
+.macro vsrari_h_x4 in0, in1, in2, in3, out0, out1, out2, out3, shift
+    vsrari.h      \out0,    \in0,     \shift
+    vsrari.h      \out1,    \in1,     \shift
+    vsrari.h      \out2,    \in2,     \shift
+    vsrari.h      \out3,    \in3,     \shift
 .endm
 
-.macro vst_x8 src, start, stride, in0, in1, in2, in3, in4, in5, in6, in7
-    vst           \in0,     \src,     \start
-    vst           \in1,     \src,     \start+(\stride*1)
-    vst           \in2,     \src,     \start+(\stride*2)
-    vst           \in3,     \src,     \start+(\stride*3)
-    vst           \in4,     \src,     \start+(\stride*4)
-    vst           \in5,     \src,     \start+(\stride*5)
-    vst           \in6,     \src,     \start+(\stride*6)
-    vst           \in7,     \src,     \start+(\stride*7)
+.macro vsrari_h_x8 in0, in1, in2, in3, in4, in5, in6, in7, out0, \
+                   out1, out2, out3, out4, out5, out6, out7, shift
+    vsrari.h      \out0,    \in0,     \shift
+    vsrari.h      \out1,    \in1,     \shift
+    vsrari.h      \out2,    \in2,     \shift
+    vsrari.h      \out3,    \in3,     \shift
+    vsrari.h      \out4,    \in4,     \shift
+    vsrari.h      \out5,    \in5,     \shift
+    vsrari.h      \out6,    \in6,     \shift
+    vsrari.h      \out7,    \in7,     \shift
 .endm
 
-.macro vld_x16 src, start, stride, in0, in1, in2, in3, in4, in5, in6, in7, \
-               in8, in9, in10, in11, in12, in13, in14, in15
-
-    vld_x8 \src, \start, \stride, \in0, \in1, \in2, \in3, \in4, \in5, \in6, \in7
-
-    vld           \in8,     \src,     \start+(\stride*8)
-    vld           \in9,     \src,     \start+(\stride*9)
-    vld           \in10,    \src,     \start+(\stride*10)
-    vld           \in11,    \src,     \start+(\stride*11)
-    vld           \in12,    \src,     \start+(\stride*12)
-    vld           \in13,    \src,     \start+(\stride*13)
-    vld           \in14,    \src,     \start+(\stride*14)
-    vld           \in15,    \src,     \start+(\stride*15)
+.macro vmulev_vmaddod_lsx in0, in1, in2, in3, out0, out1, sz
+    vmulwev.w.h   \out0,    \in0,     \in2
+    vmulwod.w.h   \out1,    \in0,     \in2
+    vmaddwev.w.h  \out0,    \in1,     \in3
+    vmaddwod.w.h  \out1,    \in1,     \in3
+.ifc \sz, .4h
+    vilvl.w       \out0,    \out1,    \out0
+.else
+    vilvl.w       vr22,     \out1,    \out0
+    vilvh.w       \out1,    \out1,    \out0
+    vor.v         \out0,    vr22,     vr22
+.endif
 .endm
 
-.macro vst_x16 src, start, stride, in0, in1, in2, in3, in4, in5, in6, in7, \
-               in8, in9, in10, in11, in12, in13, in14, in15
+const idct_coeffs_h, align=4
+    .short          2896, 2896*8, 1567, 3784
+    .short          799, 4017, 3406, 2276
+    .short          401, 4076, 3166, 2598
+    .short          1931, 3612, 3920, 1189
+    .short          201, 4091, 3035, 2751
+    .short          1751, 3703, 3857, 1380
+    .short          995, 3973, 3513, 2106
+    .short          2440, 3290, 4052, 601
+endconst
 
-    vst_x8 \src, \start, \stride, \in0, \in1, \in2, \in3, \in4, \in5, \in6, \in7
+const iadst4_coeffs, align=4
+    .word          1321, 3803, 2482, 3344
+endconst
 
-    vst           \in8,     \src,     \start+(\stride*8)
-    vst           \in9,     \src,     \start+(\stride*9)
-    vst           \in10,    \src,     \start+(\stride*10)
-    vst           \in11,    \src,     \start+(\stride*11)
-    vst           \in12,    \src,     \start+(\stride*12)
-    vst           \in13,    \src,     \start+(\stride*13)
-    vst           \in14,    \src,     \start+(\stride*14)
-    vst           \in15,    \src,     \start+(\stride*15)
+.macro inv_dct4_lsx in0, in1, in2, in3, out0, out1, out2, out3, sz
+    la.local      t0,       idct_coeffs_h
+
+    vldrepl.h     vr20,     t0,       0    // 2896
+    vmulev_vmaddod_lsx \in0, \in2, vr20, vr20, vr16, vr18, \sz
+    vneg.h        vr21,     vr20
+    vmulev_vmaddod_lsx \in0, \in2, vr20, vr21, vr17, vr19, \sz
+    vssrarni.h.w  vr18,     vr16,     12   // t0
+    vssrarni.h.w  vr19,     vr17,     12   // t1
+
+    vldrepl.h     vr20,     t0,       4    // 1567
+    vldrepl.h     vr21,     t0,       6    // 3784
+    vmulev_vmaddod_lsx \in1, \in3, vr21, vr20, \in0, vr16, \sz
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx \in1, \in3, vr20, vr21, \in2, vr17, \sz
+    vssrarni.h.w  vr16,     \in0,     12   // t3
+    vssrarni.h.w  vr17,     \in2,     12   // t2
+
+    vsadd.h       \out0,    vr18,     vr16
+    vsadd.h       \out1,    vr19,     vr17
+    vssub.h       \out2,    vr19,     vr17
+    vssub.h       \out3,    vr18,     vr16
 .endm
 
-.macro DST_ADD_W4 in0, in1, in2, in3, in4, in5
-    vilvl.w       vr10,     \in1,     \in0  // 0 1  2  3  4  5  6  7 x ...
-    vilvl.w       vr12,     \in3,     \in2  // 8 9 10 11 12 13 14 15 x ...
-    vsllwil.hu.bu vr10,     vr10,     0
-    vsllwil.hu.bu vr12,     vr12,     0
-    vadd.h        vr10,     \in4,     vr10
-    vadd.h        vr12,     \in5,     vr12
-    vssrani.bu.h  vr12,     vr10,     0
-    vstelm.w      vr12,     a0,       0,    0
-    add.d         t8,       a0,       a1
-    vstelm.w      vr12,     t8,       0,    1
-    vstelm.w      vr12,     t2,       0,    2
-    add.d         t8,       t2,       a1
-    vstelm.w      vr12,     t8,       0,    3
+functionl inv_dct_4h_x4_lsx
+    inv_dct4_lsx vr0, vr1, vr2, vr3, vr0, vr1, vr2, vr3, .4h
+endfuncl
+
+functionl inv_dct_8h_x4_lsx
+    inv_dct4_lsx vr0, vr1, vr2, vr3, vr0, vr1, vr2, vr3, .8h
+endfuncl
+
+.macro inv_adst4_core_lsx in0, in1, in2, in3, out0, out1, out2, out3
+    vsub.w        vr16,     \in0,    \in2  // in0-in2
+    vmul.w        vr17,     \in0,    vr20  // in0*1321
+    vmul.w        vr19,     \in0,    vr22  // in0*2482
+    vmul.w        vr18,     \in1,    vr23  // in1*3344
+    vmadd.w       vr17,     \in2,    vr21  // in0*1321+in2*3803
+    vmsub.w       vr19,     \in2,    vr20  // in2*1321
+    vadd.w        vr16,     vr16,    \in3  // in0-in2+in3
+    vmadd.w       vr17,     \in3,    vr22  // in0*1321+in2*3803+in3*2482
+    vmsub.w       vr19,     \in3,    vr21  // in0*2482-in2*1321-in3*3803
+    vadd.w        vr15,     vr17,    vr19
+    vmul.w        \out2,    vr16,    vr23  // out[2] 8  9  10 11
+    vadd.w        \out0,    vr17,    vr18  // out[0] 0  1  2  3
+    vadd.w        \out1,    vr19,    vr18  // out[1] 4  5  6  7
+    vsub.w        \out3,    vr15,    vr18  // out[3] 12 13 14 15
 .endm
 
-.macro VLD_DST_ADD_W4 in0, in1
-    vld           vr0,      a0,       0
-    vldx          vr1,      a0,       a1
-    vld           vr2,      t2,       0
-    vldx          vr3,      t2,       a1
+.macro inv_adst4_lsx in0, in1, in2, in3, out0, out1, out2, out3
+    la.local      t0,       iadst4_coeffs
 
-    DST_ADD_W4    vr0, vr1, vr2, vr3, \in0, \in1
+    vldrepl.w     vr20,     t0,      0     // 1321
+    vldrepl.w     vr21,     t0,      4     // 3803
+    vldrepl.w     vr22,     t0,      8     // 2482
+    vldrepl.w     vr23,     t0,      12    // 3344
+
+    vsllwil.w.h   vr0,      \in0,    0
+    vsllwil.w.h   vr1,      \in1,    0
+    vsllwil.w.h   vr2,      \in2,    0
+    vsllwil.w.h   vr3,      \in3,    0
+    inv_adst4_core_lsx vr0, vr1, vr2, vr3, \out0, \out1, \out2, \out3
+    vssrarni.h.w  \out0,    \out0,   12
+    vssrarni.h.w  \out1,    \out1,   12
+    vssrarni.h.w  \out2,    \out2,   12
+    vssrarni.h.w  \out3,    \out3,   12
 .endm
 
-.macro dct_4x4_core_lsx in0, in1, in2, in3, in4, in5, in6, in7, out0, out1
-    vexth.w.h     vr4,      \in0            // in1
-    vexth.w.h     vr5,      \in1            // in3
-    vmul.w        vr6,      vr4,      \in4
-    vmul.w        vr7,      vr4,      \in5
-    vmadd.w       vr6,      vr5,      \in5  // t3
-    vmsub.w       vr7,      vr5,      \in4  // t2
-    vsllwil.w.h   vr4,      \in2,     0     // in0
-    vsllwil.w.h   vr5,      \in3,     0     // in2
-    vmul.w        vr9,      vr4,      \in6
-    vmul.w        vr10,     vr4,      \in7
-    vmadd.w       vr9,      vr5,      \in7  // t0
-    vmsub.w       vr10,     vr5,      \in6  // t1
-    vssrarni.h.w  vr10,     vr9,      12    // t0 t1
-    vssrarni.h.w  vr7,      vr6,      12    // t3 t2
-    vsadd.h       \out0,    vr10,     vr7   // 0 4  8 12 1 5  9 13  c[0] c[1]
-    vssub.h       \out1,    vr10,     vr7   // 3 7 11 15 2 6 10 14  c[3] c[2]
-.endm
+functionl inv_adst_4h_x4_lsx
+    inv_adst4_lsx vr0, vr1, vr2, vr3, vr0, vr1, vr2, vr3
+endfuncl
 
-.macro inv_dct_dct_4x4_lsx
-    la.local      t0,       idct_coeffs
+functionl inv_flipadst_4h_x4_lsx
+    inv_adst4_lsx vr0, vr1, vr2, vr3, vr3, vr2, vr1, vr0
+endfuncl
 
-    vld           vr0,      a2,       0    // 0 1  2  3  4  5  6  7
-    vld           vr1,      a2,       16   // 8 9 10 11 12 13 14 15
+.macro inv_adst_8x4_lsx in0, in1, in2, in3, out0, out1, out2, out3
+    la.local      t0,       iadst4_coeffs
+    vldrepl.w     vr20,     t0,      0     // 1321
+    vldrepl.w     vr21,     t0,      4     // 3803
+    vldrepl.w     vr22,     t0,      8     // 2482
+    vldrepl.w     vr23,     t0,      12    // 3344
+
+    vsllwil.w.h   vr10,     \in0,     0     // in0
+    vsllwil.w.h   vr11,     \in1,     0     // in1
+    vsllwil.w.h   vr12,     \in2,     0     // in2
+    vsllwil.w.h   vr13,     \in3,     0     // in3
+    inv_adst4_core_lsx vr10, vr11, vr12, vr13, vr10, vr11, vr12, vr13
+
+    vexth.w.h     \in0,      \in0           // in0
+    vexth.w.h     \in1,      \in1           // in1
+    vexth.w.h     \in2,      \in2           // in2
+    vexth.w.h     \in3,      \in3           // in3
+    inv_adst4_core_lsx \in0, \in1, \in2, \in3, \out0, \out1, \out2, \out3
+
+    vssrarni.h.w  \out0,     vr10,    12
+    vssrarni.h.w  \out1,     vr11,    12
+    vssrarni.h.w  \out2,     vr12,    12
+    vssrarni.h.w  \out3,     vr13,    12
+.endm
 
-    vldrepl.w     vr2,      t0,       8    // 1567
-    vldrepl.w     vr3,      t0,       12   // 3784
-    vldrepl.w     vr8,      t0,       0    // 2896
+functionl inv_adst_8h_x4_lsx
+    inv_adst_8x4_lsx vr0, vr1, vr2, vr3, vr0, vr1, vr2, vr3
+endfuncl
 
-    dct_4x4_core_lsx vr0, vr1, vr0, vr1, vr3, vr2, vr8, vr8, vr11, vr12
+functionl inv_flipadst_8h_x4_lsx
+    inv_adst_8x4_lsx vr0, vr1, vr2, vr3, vr3, vr2, vr1, vr0
+endfuncl
 
-    vreplgr2vr.h  vr15,     zero
-    vshuf4i.d     vr12,     vr12,     0x01 // 2 6 10 14 3 7 11 15
-    vst           vr15,     a2,       0
-    vst           vr15,     a2,       16
+functionl inv_identity_4h_x4_lsx
+    li.w          t0,       1697
+    vreplgr2vr.h  vr20,     t0
+
+    vilvl.d       vr0,      vr1,      vr0
+    vilvl.d       vr2,      vr3,      vr2
+    vmulwev.w.h   vr16,     vr0,      vr20
+    vmulwod.w.h   vr17,     vr0,      vr20
+    vmulwev.w.h   vr18,     vr2,      vr20
+    vmulwod.w.h   vr19,     vr2,      vr20
+    vilvl.w       vr1,      vr17,     vr16
+    vilvh.w       vr3,      vr17,     vr16
+    vilvl.w       vr22,     vr19,     vr18
+    vilvh.w       vr23,     vr19,     vr18
+    vssrarni.h.w  vr3,      vr1,      12
+    vssrarni.h.w  vr23,     vr22,     12
+    vsadd.h       vr0,      vr3,      vr0  // t0
+    vsadd.h       vr2,      vr23,     vr2  // t2
+    vilvh.d       vr1,      vr0,      vr0  // t1
+    vilvh.d       vr3,      vr2,      vr2  // t3
+endfuncl
+
+.macro inv_identity4_lsx1 in0, in1, in2, out0, out1
+    vsllwil.w.h   vr16,     \in0,     0
+    vexth.w.h     vr17,     \in1
+    vmul.w        vr18,     vr16,     \in2
+    vmul.w        vr19,     vr17,     \in2
+    vsrari.w      vr18,     vr18,     12
+    vsrari.w      vr19,     vr19,     12
+    vadd.w        \out0,    vr18,     vr16
+    vadd.w        \out1,    vr19,     vr17
+    vssrarni.h.w  \out1,    \out0,    1
+.endm
 
-    vilvl.h       vr4,      vr12,     vr11 // 0 2 4 6 8 10 12 14
-    vilvh.h       vr5,      vr12,     vr11 // 1 3 5 7 9 11 13 15
-    vilvl.h       vr0,      vr5,      vr4  // 0 1  2  3  4  5  6  7
-    vilvh.h       vr1,      vr5,      vr4  // 8 9 10 11 12 13 14 15
+functionl inv_identity_8h_x4_lsx
+    li.w          t0,        1697
+    vreplgr2vr.h  vr20,      t0
+    vmulwev.w.h   vr16,      vr0,     vr20
+    vmulwod.w.h   vr17,      vr0,     vr20
+    vmulwev.w.h   vr18,      vr1,     vr20
+    vmulwod.w.h   vr19,      vr1,     vr20
+    vilvl.w       vr21,      vr17,    vr16
+    vilvh.w       vr22,      vr17,    vr16
+    vilvl.w       vr23,      vr19,    vr18
+    vilvh.w       vr16,      vr19,    vr18
+    vssrarni.h.w  vr22,      vr21,    12
+    vssrarni.h.w  vr16,      vr23,    12
+    vsadd.h       vr0,       vr22,    vr0  // t0
+    vsadd.h       vr1,       vr16,    vr1  // t1
+    vmulwev.w.h   vr16,      vr2,     vr20
+    vmulwod.w.h   vr17,      vr2,     vr20
+    vmulwev.w.h   vr18,      vr3,     vr20
+    vmulwod.w.h   vr19,      vr3,     vr20
+    vilvl.w       vr21,      vr17,    vr16
+    vilvh.w       vr22,      vr17,    vr16
+    vilvl.w       vr23,      vr19,    vr18
+    vilvh.w       vr16,      vr19,    vr18
+    vssrarni.h.w  vr22,      vr21,    12
+    vssrarni.h.w  vr16,      vr23,    12
+    vsadd.h       vr2,       vr22,    vr2  // t2
+    vsadd.h       vr3,       vr16,    vr3  // t3
+endfuncl
+
+functionl inv_identity_8h_x4_lsx1
+    li.w          t0,        1697
+    vreplgr2vr.w  vr20,      t0
+.irp i, vr0, vr1, vr2, vr3
+    inv_identity4_lsx1 \i, \i vr20, vr21, \i
+.endr
+endfuncl
 
-    dct_4x4_core_lsx vr0, vr1, vr0, vr1, vr3, vr2, vr8, vr8, vr13, vr14
-    vsrari.h      vr13,     vr13,     4
-    vsrari.h      vr14,     vr14,     4
-    vshuf4i.d     vr14,     vr14,     0x01
+functionl inv_txfm_add_4x4_lsx
+    vxor.v        vr23,     vr23,     vr23
+    vld           vr0,      a2,       0
+    vld           vr2,      a2,       16
+    vilvh.d       vr1,      vr0,      vr0
+    vilvh.d       vr3,      vr2,      vr2
+    vst           vr23,     a2,       0
+    vst           vr23,     a2,       16
 
-    alsl.d        t2,       a1,       a0,     1
+    move          t6,       ra
+    jirl          ra,       t7,       0
+    move          ra,       t6
 
-    VLD_DST_ADD_W4 vr13, vr14
-.endm
+    LSX_TRANSPOSE4x4_H vr0, vr1, vr2, vr3, vr0, vr1, vr2, vr3, vr4, vr5
 
-.macro identity_4x4_lsx in0, in1, in2, in3, out0
-    vsllwil.w.h   vr2,      \in0,    0
-    vexth.w.h     vr3,      \in1
-    vmul.w        vr4,      vr2,     \in2
-    vmul.w        vr5,      vr3,     \in2
-    vssrarni.h.w  vr5,      vr4,     12
-    vsadd.h       \out0,    vr5,     \in3
-.endm
+    move          t6,       ra
+    jirl          ra,       t8,       0
+    move          ra,       t6
 
-.macro inv_identity_identity_4x4_lsx
-    vld           vr0,      a2,       0    // 0 1  2  3  4  5  6  7
-    vld           vr1,      a2,       16   // 8 9 10 11 12 13 14 15
+    vilvl.d       vr4,      vr1,      vr0
+    vilvl.d       vr5,      vr3,      vr2
+    vsrari.h      vr4,      vr4,      4
+    vsrari.h      vr5,      vr5,      4
+    alsl.d        t2,       a1,       a0,    1
+    VLD_DST_ADD_W4 vr4, vr5
+endfuncl
 
-    li.w          t0,       1697
-    vreplgr2vr.w  vr20,     t0
+.macro idct_dc w, h, shift
+    ld.h          t2,       a2,       0      // dc
+    vldi          vr0,      0x8b5            // 181
+    vreplgr2vr.w  vr1,      t2
+    vldi          vr20,     0x880            // 128
+    vmul.w        vr2,      vr0,      vr1    // dc * 181
+    st.h          zero,     a2,       0
+    vsrari.w      vr2,      vr2,      8      // (dc * 181 + 128) >> 8
+    vld           vr10,     a0,       0      // 0 1 2 3 4 5 6 7
 
-    identity_4x4_lsx vr0, vr0, vr20, vr0, vr0
-    identity_4x4_lsx vr1, vr1, vr20, vr1, vr1
-    vreplgr2vr.h  vr15,     zero
-    vst           vr15,     a2,      0
-    vst           vr15,     a2,      16
-    identity_4x4_lsx vr0, vr0, vr20, vr0, vr6
-    identity_4x4_lsx vr1, vr1, vr20, vr1, vr7
-
-    vsrari.h      vr6,      vr6,     4
-    vsrari.h      vr7,      vr7,     4
-    vilvh.d       vr8,      vr6,     vr6
-    vilvh.d       vr9,      vr7,     vr7
-    vilvl.h       vr4,      vr8,     vr6
-    vilvl.h       vr5,      vr9,     vr7
-    vilvl.w       vr6,      vr5,     vr4
-    vilvh.w       vr7,      vr5,     vr4
-
-    alsl.d        t2,       a1,      a0,   1
-    VLD_DST_ADD_W4 vr6, vr7
+.if (2*\w == \h) || (2*\h == \w)
+    vmul.w        vr2,      vr0,      vr2
+    vsrari.w      vr2,      vr2,      8      // (dc * 181 + 128) >> 8
+.endif
+.if \shift>0
+    vsrari.w      vr2,      vr2,      \shift      // (dc + rnd) >> shift
+.endif
+    vldx          vr11,     a0,       a1     // 8 9 10 11 12 13 14 15
+    alsl.d        t2,       a1,       a0,    1
+    vmadd.w       vr20,     vr2,      vr0
+    vld           vr12,     t2,       0      // 16 17 18 19 20 21 22 23
+    vssrarni.h.w  vr20,     vr20,     12
+    vldx          vr13,     t2,       a1     // 24 25 26 27 28 29 30 31
 .endm
 
-const iadst4_coeffs, align=4
-    .word          1321, 3803, 2482, 3344
-endconst
+.macro fun4x4 txfm1, txfm2
+function inv_txfm_add_\txfm1\()_\txfm2\()_4x4_8bpc_lsx
+.ifc \txfm1\()_\txfm2, dct_dct
+    bnez          a3,       1f
 
-.macro adst4x4_1d_lsx in0, in1, in2, in3, out0, out1, out2, out3
-    vsub.w        vr6,      \in0,   \in2  // in0-in2
-    vmul.w        vr7,      \in0,   vr20  // in0*1321
-    vmadd.w       vr7,      \in2,   vr21  // in0*1321+in2*3803
-    vmadd.w       vr7,      \in3,   vr22  // in0*1321+in2*3803+in3*2482
-    vmul.w        vr8,      \in1,   vr23  // in1*3344
-    vadd.w        vr6,      vr6,    \in3  // in0-in2+in3
-    vmul.w        vr9,      \in0,   vr22  // in0*2482
-    vmsub.w       vr9,      \in2,   vr20  // in2*1321
-    vmsub.w       vr9,      \in3,   vr21  // in0*2482-in2*1321-in3*3803
-    vadd.w        vr5,      vr7,    vr9
-    vmul.w        \out2,    vr6,    vr23  // out[2] 8  9  10 11
-    vadd.w        \out0,    vr7,    vr8   // out[0] 0  1  2  3
-    vadd.w        \out1,    vr9,    vr8   // out[1] 4  5  6  7
-    vsub.w        \out3,    vr5,    vr8   // out[3] 12 13 14 15
-.endm
+    idct_dc 4, 4, 0
+
+    DST_ADD_W4    vr10, vr11, vr12, vr13, vr20, vr20
+    b             .\txfm1\()_\txfm2\()_4X4_END
+1:
+.endif
 
-.macro inv_adst_dct_4x4_lsx
-    vld           vr0,      a2,     0
-    vld           vr1,      a2,     16
+    la.local     t7,    inv_\txfm1\()_4h_x4_lsx
+    la.local     t8,    inv_\txfm2\()_4h_x4_lsx
 
-    la.local      t0,       iadst4_coeffs
-    vsllwil.w.h   vr2,      vr0,    0     // in0
-    vexth.w.h     vr3,      vr0           // in1
-    vsllwil.w.h   vr4,      vr1,    0     // in2
-    vexth.w.h     vr5,      vr1           // in3
-    vldrepl.w     vr20,     t0,     0     // 1321
-    vldrepl.w     vr21,     t0,     4     // 3803
-    vldrepl.w     vr22,     t0,     8     // 2482
-    vldrepl.w     vr23,     t0,     12    // 3344
-
-    adst4x4_1d_lsx vr2, vr3, vr4, vr5, vr0, vr1, vr2, vr3
-
-    LSX_TRANSPOSE4x4_W vr0, vr1, vr2, vr3, vr11, vr13, vr12, vr14, vr6, vr7
-    vssrarni.h.w  vr13,     vr11,    12
-    vssrarni.h.w  vr14,     vr12,    12
-
-    vreplgr2vr.h  vr15,     zero
-    la.local      t0,       idct_coeffs
-    vst           vr15,     a2,      0
-    vst           vr15,     a2,      16
-    vldrepl.w     vr20,     t0,      8    // 1567
-    vldrepl.w     vr21,     t0,      12   // 3784
-    vldrepl.w     vr22,     t0,      0    // 2896
+    b            inv_txfm_add_4x4_lsx
+.\txfm1\()_\txfm2\()_4X4_END:
+endfunc
+.endm
 
-    dct_4x4_core_lsx vr13, vr14, vr13, vr14, vr21, vr20, vr22, vr22, vr13, vr14
+fun4x4 dct, dct
+fun4x4 identity, identity
+fun4x4 adst, dct
+fun4x4 dct, adst
+fun4x4 adst, adst
+fun4x4 dct, flipadst
+fun4x4 flipadst, adst
+fun4x4 adst, flipadst
+fun4x4 flipadst, dct
+fun4x4 flipadst, flipadst
+fun4x4 dct, identity
+fun4x4 identity, dct
+fun4x4 flipadst, identity
+fun4x4 identity, flipadst
+fun4x4 identity, adst
+fun4x4 adst, identity
 
-    vshuf4i.d     vr14,     vr14,    0x01
-    vsrari.h      vr13,     vr13,    4
-    vsrari.h      vr14,     vr14,    4
+const iadst8_coeffs_h, align=4
+    .short          4076, 401, 3612, 1931
+    .short          2598, 3166, 1189, 3920
+    .short          2896, 0, 1567, 3784, 0, 0, 0, 0
+endconst
 
-    alsl.d        t2,       a1,      a0,   1
-    VLD_DST_ADD_W4 vr13, vr14
+.macro inv_adst8_lsx out0, out1, out2, out3, out4, out5, out6, out7, sz
+    la.local      t0,       iadst8_coeffs_h
+
+    vldrepl.h     vr20,     t0,       0     // 4076
+    vldrepl.h     vr21,     t0,       2     // 401
+    vmulev_vmaddod_lsx vr7, vr0, vr20, vr21, vr16, vr17, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr7, vr0, vr21, vr20, vr18, vr19, \sz
+    vssrarni.h.w  vr17,     vr16,     12    // t0a
+    vssrarni.h.w  vr19,     vr18,     12    // t1a
+
+    vldrepl.h     vr20,     t0,       4     // 3612
+    vldrepl.h     vr21,     t0,       6     // 1931
+    vmulev_vmaddod_lsx vr5, vr2, vr20, vr21, vr0, vr16, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr5, vr2, vr21, vr20, vr7, vr18, \sz
+    vssrarni.h.w  vr16,     vr0,      12    // t2a
+    vssrarni.h.w  vr18,     vr7,      12    // t3a
+
+    vldrepl.h     vr20,     t0,       8     // 2598
+    vldrepl.h     vr21,     t0,       10    // 3166
+    vmulev_vmaddod_lsx vr3, vr4, vr20, vr21, vr2, vr0, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr3, vr4, vr21, vr20, vr5, vr7, \sz
+    vssrarni.h.w  vr0,      vr2,      12    // t4a
+    vssrarni.h.w  vr7,      vr5,      12    // t5a
+
+    vldrepl.h     vr20,     t0,       12    // 1189
+    vldrepl.h     vr21,     t0,       14    // 3920
+    vmulev_vmaddod_lsx vr1, vr6, vr20, vr21, vr3, vr2, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr1, vr6, vr21, vr20, vr4, vr5, \sz
+    vssrarni.h.w  vr2,      vr3,      12    // t6a
+    vssrarni.h.w  vr5,      vr4,      12    // t7a
+
+    vsadd.h       vr3,      vr17,     vr0   // t0
+    vssub.h       vr4,      vr17,     vr0   // t4
+    vsadd.h       vr1,      vr19,     vr7   // t1
+    vssub.h       vr6,      vr19,     vr7   // t5
+    vsadd.h       vr17,     vr16,     vr2   // t2
+    vssub.h       vr19,     vr16,     vr2   // t6
+    vsadd.h       vr0,      vr18,     vr5   // t3
+    vssub.h       vr7,      vr18,     vr5   // t7
+
+    la.local      t0,       idct_coeffs_h
+
+    vldrepl.h     vr20,     t0,       4     // 1567
+    vldrepl.h     vr21,     t0,       6     // 3784
+    vmulev_vmaddod_lsx vr4, vr6, vr21, vr20, vr16, vr5, \sz
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr4, vr6, vr20, vr21, vr18, vr2, \sz
+    vssrarni.h.w  vr5,      vr16,     12    // t4a
+    vssrarni.h.w  vr2,      vr18,     12    // t5a
+
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr7, vr19, vr20, vr21, vr4, vr16, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr7, vr19, vr21, vr20, vr6, vr18, \sz
+    vssrarni.h.w  vr16,     vr4,      12    // t7a
+    vssrarni.h.w  vr18,     vr6,      12    // t6a
+
+    vsadd.h       vr4,      vr5,      vr18  // out1
+    vssub.h       vr19,     vr5,      vr18  // t6
+    vsadd.h       vr20,     vr1,      vr0   // out7
+    vssub.h       vr18,     vr1,      vr0   // t3
+    vsadd.h       \out0,    vr3,      vr17  // out0
+    vssub.h       vr5,      vr3,      vr17  // t2
+    vsadd.h       \out6,    vr2,      vr16  // out6
+    vssub.h       vr23,     vr2,      vr16  // t7
+
+    vsllwil.w.h   vr3,      vr20,     0     // out7
+    vexth.w.h     \out7,    vr20            // out7
+    vsllwil.w.h   vr21,     vr4,      0     // out1
+    vexth.w.h     \out1,    vr4             // out1
+    vneg.w        vr3,      vr3
+    vneg.w        \out7,    \out7
+    vneg.w        vr21,     vr21
+    vneg.w        \out1,    \out1
+    vssrarni.h.w  \out7,    vr3,      0
+    vssrarni.h.w  \out1,    vr21,     0
+
+    la.local      t0,       idct_coeffs_h
+
+    vldrepl.h     vr20,     t0,       0     // 2896
+    vmulev_vmaddod_lsx vr5, vr18, vr20, vr20, vr16, \out3, \sz
+    vneg.h        vr21,     vr20
+    vmulev_vmaddod_lsx vr5, vr18, vr20, vr21, vr17, \out4, \sz
+    vsrari.w      vr16,     vr16,     12
+    vsrari.w      \out3,    \out3,    12
+    vneg.w        vr16,     vr16
+    vneg.w        \out3,    \out3
+    vssrarni.h.w  \out3,    vr16,     0     // out3
+    vssrarni.h.w  \out4,    vr17,     12    // out4
+
+    vmulev_vmaddod_lsx vr19, vr23, vr20, vr20, vr16, \out2, \sz
+    vmulev_vmaddod_lsx vr19, vr23, vr20, vr21, vr17, \out5, \sz
+    vssrarni.h.w  \out2,    vr16,     12    // out2
+    vsrari.w      vr17,     vr17,     12
+    vsrari.w      \out5,    \out5,    12
+    vneg.w        vr17,     vr17
+    vneg.w        \out5,    \out5
+    vssrarni.h.w  \out5,    vr17,     0     // out5
 .endm
 
-.macro inv_adst_adst_4x4_lsx
-    vld           vr0,      a2,     0
-    vld           vr1,      a2,     16
+functionl inv_adst_8h_x8_lsx
+    inv_adst8_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, .8h
+endfuncl
+
+functionl inv_flipadst_8h_x8_lsx
+    inv_adst8_lsx vr7, vr6, vr5, vr4, vr3, vr2, vr1, vr0, .8h
+endfuncl
+
+functionl inv_adst_4h_x8_lsx
+    inv_adst8_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, .8h
+endfuncl
+
+functionl inv_flipadst_4h_x8_lsx
+    inv_adst8_lsx vr7, vr6, vr5, vr4, vr3, vr2, vr1, vr0, .8h
+endfuncl
+
+.macro inv_dct8_lsx in0, in1, in2, in3, in4, in5, in6, in7, sz
+    inv_dct4_lsx \in0, \in2, \in4, \in6, \in0, \in2, \in4, \in6, \sz
+
+    la.local      t0,       idct_coeffs_h
+
+    vldrepl.h     vr20,     t0,       8        // 799
+    vldrepl.h     vr21,     t0,       10       // 4017
+    vmulev_vmaddod_lsx  \in1, \in7, vr21, vr20, vr16, vr17, \sz
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx  \in1, \in7, vr20, vr21, vr18, vr19, \sz
+    vssrarni.h.w  vr17,     vr16,     12       // t7a
+    vssrarni.h.w  vr19,     vr18,     12       // t4a
+
+    vldrepl.h     vr20,     t0,       12       // 3406
+    vldrepl.h     vr21,     t0,       14       // 2276
+    vmulev_vmaddod_lsx  \in5, \in3, vr21, vr20, \in1, vr16, \sz
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx  \in5, \in3, vr20, vr21, \in7, vr18, \sz
+    vssrarni.h.w  vr16,     \in1,       12      // t6a
+    vssrarni.h.w  vr18,     \in7,       12      // t5a
+
+    vssub.h       \in7,     vr19,      vr18     // t5a
+    vsadd.h       vr18,     vr19,      vr18     // t4
+    vssub.h       \in5,     vr17,      vr16     // t6a
+    vsadd.h       vr16,     vr17,      vr16     // t7
+
+    vldrepl.h     vr20,     t0,        0        // 2896
+    vmulev_vmaddod_lsx  \in5, \in7, vr20, vr20, \in1, vr17, \sz
+    vneg.h        vr21,     vr20
+    vmulev_vmaddod_lsx  \in5, \in7, vr20, vr21, vr23, vr19, \sz
+    vssrarni.h.w  vr17,     \in1,      12       // t6
+    vssrarni.h.w  vr19,     vr23,      12       // t5
+
+    vssub.h       \in7,      \in0,     vr16     //c[7]
+    vsadd.h       \in0,      \in0,     vr16     //c[0]
+    vssub.h       \in5,      \in4,     vr19     //c[5]
+    vsadd.h       vr23,      \in4,     vr19     //c[2]
+    vssub.h       \in4,      \in6,     vr18     //c[4]
+    vsadd.h       \in3,      \in6,     vr18     //c[3]
+    vssub.h       \in6,      \in2,     vr17     //c[6]
+    vsadd.h       \in1,      \in2,     vr17     //c[1]
+    vor.v         \in2,      vr23,     vr23
+.endm
 
-    la.local      t0,       iadst4_coeffs
-    vsllwil.w.h   vr2,      vr0,    0     // in0
-    vexth.w.h     vr3,      vr0           // in1
-    vsllwil.w.h   vr4,      vr1,    0     // in2
-    vexth.w.h     vr5,      vr1           // in3
-    vldrepl.w     vr20,     t0,     0     // 1321
-    vldrepl.w     vr21,     t0,     4     // 3803
-    vldrepl.w     vr22,     t0,     8     // 2482
-    vldrepl.w     vr23,     t0,     12    // 3344
+functionl inv_dct_8h_x8_lsx
+    inv_dct8_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, .8h
+endfuncl
 
-    adst4x4_1d_lsx vr2, vr3, vr4, vr5, vr0, vr1, vr2, vr3
+functionl inv_dct_4h_x8_lsx
+    inv_dct8_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, .4h
+endfuncl
 
-    LSX_TRANSPOSE4x4_W vr0, vr1, vr2, vr3, vr11, vr13, vr12, vr14, vr6, vr7
+.macro DST_ADD_W8 in0, in1, in2, in3, in4, in5, in6, in7
+    vsllwil.hu.bu vr0,      \in0,     0
+    vsllwil.hu.bu vr1,      \in1,     0
+    vsllwil.hu.bu vr2,      \in2,     0
+    vsllwil.hu.bu vr3,      \in3,     0
+    vadd.h        vr0,      \in4,     vr0
+    vadd.h        vr1,      \in5,     vr1
+    vadd.h        vr2,      \in6,     vr2
+    vadd.h        vr3,      \in7,     vr3
+    vssrani.bu.h  vr1,      vr0,      0
+    vssrani.bu.h  vr3,      vr2,      0
+    vstelm.d      vr1,      a0,       0,    0
+    vstelmx.d     vr1,      a0,       a1,   1
+    vstelmx.d     vr3,      a0,       a1,   0
+    vstelmx.d     vr3,      a0,       a1,   1
+.endm
 
-    vsrari.w      vr11,     vr11,    12
-    vsrari.w      vr13,     vr13,    12
-    vsrari.w      vr12,     vr12,    12
-    vsrari.w      vr14,     vr14,    12
+.macro VLD_DST_ADD_W8 in0, in1, in2, in3
+    vld           vr0,      a0,       0
+    vldx          vr1,      a0,       a1
+    vld           vr2,      t2,       0
+    vldx          vr3,      t2,       a1
 
-    vreplgr2vr.h  vr15,     zero
-    vst           vr15,     a2,      0
-    vst           vr15,     a2,      16
+    DST_ADD_W8 vr0, vr1, vr2, vr3, \in0, \in1, \in2, \in3
+.endm
 
-    adst4x4_1d_lsx vr11, vr13, vr12, vr14, vr11, vr13, vr12, vr14
+functionl inv_identity_8h_x8_lsx
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    vsadd.h       \i,       \i,       \i
+.endr
+endfuncl
 
-    vssrarni.h.w  vr13,     vr11,    12
-    vssrarni.h.w  vr14,     vr12,    12
-    vsrari.h      vr13,     vr13,    4
-    vsrari.h      vr14,     vr14,    4
+functionl inv_identity_4h_x8_lsx
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    vsadd.h       \i,       \i,       \i
+.endr
+endfuncl
 
-    alsl.d        t2,       a1,      a0,   1
-    VLD_DST_ADD_W4 vr13, vr14
-.endm
+.macro def_fn_8x8_base variant
+functionl inv_txfm_\variant\()add_8x8_lsx
+    vxor.v  vr23, vr23, vr23
+    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+.irp i, 0, 16, 32, 48, 64, 80, 96, 112
+    vst           vr23,     a2,       \i
+.endr
 
-.macro inv_dct_adst_4x4_lsx
-    la.local      t0,       idct_coeffs
+.ifc \variant, identity_
+    // The identity shl #1 and downshift srshr #1 cancel out
+    b             .itx_8x8_epilog
+.else
 
-    vld           vr0,      a2,       0    // 0 1  2  3  4  5  6  7
-    vld           vr1,      a2,       16   // 8 9 10 11 12 13 14 15
+    move          t6,       ra
+    jirl          ra,       t7,       0
+    move          ra,       t6
 
-    vldrepl.w     vr20,     t0,       8    // 1567
-    vldrepl.w     vr21,     t0,       12   // 3784
-    vldrepl.w     vr22,     t0,       0    // 2896
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    vsrari.h      \i,       \i,       1
+.endr
 
-    dct_4x4_core_lsx  vr0, vr1, vr0, vr1, vr21, vr20, vr22, vr22, vr11, vr12
+.itx_8x8_epilog:
+    LSX_TRANSPOSE8x8_H vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                       vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
 
-    vreplgr2vr.h  vr15,     zero
-    vst           vr15,     a2,       0
-    vst           vr15,     a2,       16
+    move          t6,       ra
+    jirl          ra,       t8,       0
+    move          ra,       t6
 
-    vshuf4i.d     vr12,     vr12,     0x01 // 3 7 11 15 2 6 10 14
+    vsrari_h_x8 vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23, 4
 
-    vilvl.h       vr4,      vr12,     vr11 // 0 2 4 6 8 10 12 14
-    vilvh.h       vr5,      vr12,     vr11 // 1 3 5 7 9 11 13 15
-    vilvl.h       vr11,     vr5,      vr4  // 0 1  2  3  4  5  6  7
-    vilvh.h       vr12,     vr5,      vr4  // 8 9 10 11 12 13 14 15
+    alsl.d        t2,       a1,       a0,     1
+    VLD_DST_ADD_W8 vr16, vr17, vr18, vr19
+    add.d         a0,       a0,       a1
+    alsl.d        t2,       a1,       a0,     1
+    VLD_DST_ADD_W8 vr20, vr21, vr22, vr23
+.endif
+endfuncl
+.endm
 
-    vsllwil.w.h   vr2,      vr11,     0     // in0
-    vexth.w.h     vr3,      vr11            // in1
-    vsllwil.w.h   vr4,      vr12,     0     // in2
-    vexth.w.h     vr5,      vr12            // in3
+def_fn_8x8_base identity_
+def_fn_8x8_base
 
-    la.local      t0,       iadst4_coeffs
+.macro fn8x8 txfm1, txfm2
+function inv_txfm_add_\txfm1\()_\txfm2\()_8x8_8bpc_lsx
+.ifc \txfm1\()_\txfm2, dct_dct
+    bnez          a3,       .NO_HAS_DCONLY_8x8
 
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
+    idct_dc 8, 8, 1
 
-    adst4x4_1d_lsx vr2, vr3, vr4, vr5, vr11, vr13, vr12, vr14
+    DST_ADD_W8 vr10, vr11, vr12, vr13, vr20, vr20, vr20, vr20
 
-    vssrarni.h.w  vr13,     vr11,     12
-    vssrarni.h.w  vr14,     vr12,     12
-    vsrari.h      vr13,     vr13,     4
-    vsrari.h      vr14,     vr14,     4
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       a0,     1
+    VLD_DST_ADD_W8 vr20, vr20, vr20, vr20
 
-    alsl.d        t2,       a1,       a0,   1
-    VLD_DST_ADD_W4 vr13, vr14
+    b             .\txfm1\()_\txfm2\()_8X8_END
+.NO_HAS_DCONLY_8x8:
+.endif
+    la.local      t8,       inv_\txfm2\()_8h_x8_lsx
+.ifc \txfm1, identity
+    b             inv_txfm_identity_add_8x8_lsx
+.else
+    la.local      t7,       inv_\txfm1\()_8h_x8_lsx
+    b             inv_txfm_add_8x8_lsx
+.endif
+.\txfm1\()_\txfm2\()_8X8_END:
+endfunc
 .endm
 
-.macro inv_dct_flipadst_4x4_lsx
-    la.local      t0,       idct_coeffs
-
-    vld           vr0,      a2,       0    // 0 1  2  3  4  5  6  7
-    vld           vr1,      a2,       16   // 8 9 10 11 12 13 14 15
+fn8x8 dct, dct
+fn8x8 identity, identity
+fn8x8 dct, adst
+fn8x8 dct, flipadst
+fn8x8 dct, identity
+fn8x8 adst, dct
+fn8x8 adst, adst
+fn8x8 adst, flipadst
+fn8x8 flipadst, dct
+fn8x8 flipadst, adst
+fn8x8 flipadst, flipadst
+fn8x8 identity, dct
+fn8x8 adst, identity
+fn8x8 flipadst, identity
+fn8x8 identity, adst
+fn8x8 identity, flipadst
 
-    vldrepl.w     vr20,     t0,       8    // 1567
-    vldrepl.w     vr21,     t0,       12   // 3784
-    vldrepl.w     vr22,     t0,       0    // 2896
+.macro rect2_lsx in0, in1, out0
+    vsllwil.w.h   vr22,     \in0,     0     // in1
+    vexth.w.h     \in0,     \in0            // in1
+    vmul.w        vr22,     vr22,     \in1
+    vmul.w        \out0,    \in0,     \in1
+    vssrarni.h.w  \out0,    vr22,     12
+.endm
 
-    dct_4x4_core_lsx  vr0, vr1, vr0, vr1, vr21, vr20, vr22, vr22, vr11, vr12
+.macro LSX_TRANSPOSE8x4_H in0, in1, in2, in3, in4, in5, in6, in7, out0, out1, \
+                          out2, out3, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5
+    vilvl.h       \tmp0,    \in1,     \in0
+    vilvl.h       \tmp1,    \in3,     \in2
+    vilvl.w       \tmp2,    \tmp1,    \tmp0
+    vilvh.w       \tmp3,    \tmp1,    \tmp0
+    vilvl.h       \tmp0,    \in5,     \in4
+    vilvl.h       \tmp1,    \in7,     \in6
+    vilvl.w       \tmp4,    \tmp1,    \tmp0
+    vilvh.w       \tmp5,    \tmp1,    \tmp0
+    vilvl.d       \out0,    \tmp4,    \tmp2
+    vilvh.d       \out1,    \tmp4,    \tmp2
+    vilvl.d       \out2,    \tmp5,    \tmp3
+    vilvh.d       \out3,    \tmp5,    \tmp3
+.endm
 
-    vreplgr2vr.h  vr15,     zero
-    vst           vr15,     a2,       0
-    vst           vr15,     a2,       16
+functionl inv_txfm_add_8x4_lsx
+    vxor.v        vr23,     vr23,     vr23
+    vld           vr0,      a2,       0
+    vld           vr2,      a2,       16
+    vld           vr4,      a2,       32
+    vld           vr6,      a2,       48
+.irp i, 0, 16, 32, 48
+    vst           vr23,     a2,       \i
+.endr
 
-    vshuf4i.d     vr12,     vr12,     0x01 // 3 7 11 15 2 6 10 14
+    li.w          t0,       2896
+    vreplgr2vr.w  vr23,     t0
+    rect2_lsx     vr0,      vr23,     vr0
+    rect2_lsx     vr2,      vr23,     vr2
+    rect2_lsx     vr4,      vr23,     vr4
+    rect2_lsx     vr6,      vr23,     vr6
 
-    vilvl.h       vr4,      vr12,     vr11 // 0 2 4 6 8 10 12 14
-    vilvh.h       vr5,      vr12,     vr11 // 1 3 5 7 9 11 13 15
-    vilvl.h       vr11,     vr5,      vr4  // 0 1  2  3  4  5  6  7
-    vilvh.h       vr12,     vr5,      vr4  // 8 9 10 11 12 13 14 15
-    vsllwil.w.h   vr2,      vr11,     0    // in0
-    vexth.w.h     vr3,      vr11           // in1
-    vsllwil.w.h   vr4,      vr12,     0    // in2
-    vexth.w.h     vr5,      vr12           // in3
+    vilvh.d       vr1,      vr0,      vr0
+    vilvh.d       vr3,      vr2,      vr2
+    vilvh.d       vr5,      vr4,      vr4
+    vilvh.d       vr7,      vr6,      vr6
 
-    la.local      t0,       iadst4_coeffs
+    move          t6,       ra
+    jirl          ra,       t7,       0
+    move          ra,       t6
 
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
+    LSX_TRANSPOSE8x4_H vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, vr0, vr1, \
+                       vr2, vr3, vr16, vr17, vr18, vr19, vr20, vr21
 
-    adst4x4_1d_lsx vr2, vr3, vr4, vr5, vr11, vr12, vr13, vr14
+    move          t6,       ra
+    jirl          ra,       t8,       0
+    move          ra,       t6
 
-    vssrarni.h.w  vr11,     vr12,     12    // 0 1  2  3  4  5  6  7
-    vssrarni.h.w  vr13,     vr14,     12    // 8 9 10 11 12 13 14 15
-    vsrari.h      vr11,     vr11,     4
-    vsrari.h      vr13,     vr13,     4
+    vsrari_h_x4 vr0, vr1, vr2, vr3, vr16, vr17, vr18, vr19, 4
 
-    alsl.d        t2,       a1,       a0,   1
-    VLD_DST_ADD_W4 vr13, vr11
+    alsl.d        t2,       a1,       a0,     1
+    VLD_DST_ADD_W8 vr16, vr17, vr18, vr19
+endfuncl
+
+.macro LSX_TRANSPOSE4x8_H in0, in1, in2, in3, out0, out1, out2, out3, out4, \
+                          out5, out6, out7, tmp0, tmp1, tmp2, tmp3
+    vilvl.h       \tmp0,    \in1,     \in0
+    vilvl.h       \tmp1,    \in3,     \in2
+    vilvh.h       \tmp2,    \in1,     \in0
+    vilvh.h       \tmp3,    \in3,     \in2
+    vilvl.w       \out0,    \tmp1,    \tmp0
+    vilvh.w       \out2,    \tmp1,    \tmp0
+    vilvl.w       \out4,    \tmp3,    \tmp2
+    vilvh.w       \out6,    \tmp3,    \tmp2
+
+    vbsrl.v       \out1,    \out0,    8
+    vbsrl.v       \out3,    \out2,    8
+    vbsrl.v       \out5,    \out4,    8
+    vbsrl.v       \out7,    \out6,    8
+    vinsgr2vr.d   \out0,    zero,     1
+    vinsgr2vr.d   \out2,    zero,     1
+    vinsgr2vr.d   \out4,    zero,     1
+    vinsgr2vr.d   \out6,    zero,     1
 .endm
 
-.macro inv_flipadst_adst_4x4_lsx
+functionl inv_txfm_add_4x8_lsx
+    vxor.v        vr23,     vr23,     vr23
     vld           vr0,      a2,       0
     vld           vr1,      a2,       16
+    vld           vr2,      a2,       32
+    vld           vr3,      a2,       48
+.irp i, 0, 16, 32, 48
+    vst           vr23,     a2,       \i
+.endr
 
-    la.local      t0,       iadst4_coeffs
-    vsllwil.w.h   vr2,      vr0,      0     // in0
-    vexth.w.h     vr3,      vr0             // in1
-    vsllwil.w.h   vr4,      vr1,      0     // in2
-    vexth.w.h     vr5,      vr1             // in3
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
-
-    adst4x4_1d_lsx vr2, vr3, vr4, vr5, vr0, vr1, vr2, vr3
-
-    vsrari.w      vr0,      vr0,      12
-    vsrari.w      vr1,      vr1,      12
-    vsrari.w      vr2,      vr2,      12
-    vsrari.w      vr3,      vr3,      12
-
-    vilvl.w       vr4,      vr0,      vr1
-    vilvh.w       vr5,      vr0,      vr1
-    vilvl.w       vr6,      vr2,      vr3
-    vilvh.w       vr7,      vr2,      vr3
-    vilvl.d       vr11,     vr4,      vr6
-    vilvh.d       vr12,     vr4,      vr6
-    vilvl.d       vr13,     vr5,      vr7
-    vilvh.d       vr14,     vr5,      vr7
-
-    vreplgr2vr.h  vr15,     zero
-    vst           vr15,     a2,       0
-    vst           vr15,     a2,       16
-
-    adst4x4_1d_lsx vr11, vr12, vr13, vr14, vr11, vr13, vr12, vr14
-
-    vssrarni.h.w  vr13,     vr11,     12
-    vssrarni.h.w  vr14,     vr12,     12
-    vsrari.h      vr13,     vr13,     4
-    vsrari.h      vr14,     vr14,     4
+    li.w          t0,       2896
+    vreplgr2vr.w  vr23,     t0
+    rect2_lsx     vr0,      vr23,     vr0
+    rect2_lsx     vr1,      vr23,     vr1
+    rect2_lsx     vr2,      vr23,     vr2
+    rect2_lsx     vr3,      vr23,     vr3
 
-    alsl.d        t2,       a1,       a0,   1
-    VLD_DST_ADD_W4 vr13, vr14
-.endm
+    move          t6,       ra
+    jirl          ra,       t7,       0
+    move          ra,       t6
 
-.macro inv_adst_flipadst_4x4_lsx
-    vld           vr0,      a2,      0
-    vld           vr1,      a2,      16
+    LSX_TRANSPOSE4x8_H vr0, vr1, vr2, vr3, vr0, vr1, vr2, vr3, vr4, vr5, \
+                       vr6, vr7, vr16, vr17, vr18, vr19
 
-    la.local      t0,       iadst4_coeffs
-    vsllwil.w.h   vr2,      vr0,      0     // in0
-    vexth.w.h     vr3,      vr0             // in1
-    vsllwil.w.h   vr4,      vr1,      0     // in2
-    vexth.w.h     vr5,      vr1             // in3
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
-
-    adst4x4_1d_lsx vr2, vr3, vr4, vr5, vr0, vr1, vr2, vr3
-    LSX_TRANSPOSE4x4_W vr0, vr1, vr2, vr3, vr11, vr13, vr12, vr14, vr6, vr7
-    vsrari.w      vr11,     vr11,     12
-    vsrari.w      vr12,     vr12,     12
-    vsrari.w      vr13,     vr13,     12
-    vsrari.w      vr14,     vr14,     12
-
-    vreplgr2vr.h  vr15,     zero
-    vst           vr15,     a2,       0
-    vst           vr15,     a2,       16
-
-    adst4x4_1d_lsx vr11, vr13, vr12, vr14, vr11, vr12, vr13, vr14
-
-    vssrarni.h.w  vr11,     vr12,     12
-    vssrarni.h.w  vr13,     vr14,     12
-    vsrari.h      vr11,     vr11,     4
-    vsrari.h      vr13,     vr13,     4
+    move          t6,       ra
+    jirl          ra,       t8,       0
+    move          ra,       t6
 
-    alsl.d        t2,       a1,       a0,   1
-    VLD_DST_ADD_W4 vr13, vr11
-.endm
+    vilvl.d       vr0,      vr1,      vr0
+    vilvl.d       vr1,      vr3,      vr2
+    vilvl.d       vr2,      vr5,      vr4
+    vilvl.d       vr3,      vr7,      vr6
 
-.macro inv_flipadst_dct_4x4_lsx
-    vld           vr0,      a2,       0
-    vld           vr1,      a2,       16
+    vsrari_h_x4 vr0, vr1, vr2, vr3, vr16, vr17, vr18, vr19, 4
 
-    la.local      t0,       iadst4_coeffs
-    vsllwil.w.h   vr2,      vr0,      0     // in0
-    vexth.w.h     vr3,      vr0             // in1
-    vsllwil.w.h   vr4,      vr1,      0     // in2
-    vexth.w.h     vr5,      vr1             // in3
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
-
-    adst4x4_1d_lsx vr2, vr3, vr4, vr5, vr0, vr1, vr2, vr3
-
-    vilvl.w       vr4,      vr0,      vr1
-    vilvh.w       vr5,      vr0,      vr1
-    vilvl.w       vr6,      vr2,      vr3
-    vilvh.w       vr7,      vr2,      vr3
-
-    vilvl.d       vr11,     vr4,      vr6
-    vilvh.d       vr12,     vr4,      vr6
-    vilvl.d       vr13,     vr5,      vr7
-    vilvh.d       vr14,     vr5,      vr7
-
-    vssrarni.h.w  vr12,     vr11,     12
-    vssrarni.h.w  vr14,     vr13,     12
-
-    vreplgr2vr.h  vr15,     zero
-    la.local      t0,       idct_coeffs
-    vst           vr15,     a2,       0
-    vst           vr15,     a2,       16
-    vldrepl.w     vr20,     t0,       8    // 1567
-    vldrepl.w     vr21,     t0,       12   // 3784
-    vldrepl.w     vr22,     t0,       0    // 2896
+    alsl.d        t2,       a1,       a0,    1
+    VLD_DST_ADD_W4 vr16, vr17
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       a0,    1
+    VLD_DST_ADD_W4 vr18, vr19
+endfuncl
+
+.macro fn8x4 txfm1, txfm2
+function inv_txfm_add_\txfm1\()_\txfm2\()_8x4_8bpc_lsx
+.ifc \txfm1()_\txfm2, dct_dct
+    bnez          a3,       .NO_HAS_DCONLY_8x4
 
-    dct_4x4_core_lsx vr12, vr14, vr12, vr14, vr21, vr20, vr22, vr22, vr13, vr14
+    idct_dc 8, 4, 0
 
-    vshuf4i.d     vr14,     vr14,     0x01
-    vsrari.h      vr13,     vr13,     4
-    vsrari.h      vr14,     vr14,     4
+    DST_ADD_W8 vr10, vr11, vr12, vr13, vr5, vr5, vr5, vr5
 
-    alsl.d        t2,       a1,       a0,   1
-    VLD_DST_ADD_W4 vr13, vr14
+    b             .\txfm1\()_\txfm2\()_8X4_END
+.NO_HAS_DCONLY_8x4:
+.endif
+    la.local      t7,       inv_\txfm1\()_4h_x8_lsx
+    la.local      t8,       inv_\txfm2\()_8h_x4_lsx
+    b             inv_txfm_add_8x4_lsx
+.\txfm1\()_\txfm2\()_8X4_END:
+endfunc
 .endm
 
-.macro inv_flipadst_flipadst_4x4_lsx
-    vld           vr0,      a2,       0
-    vld           vr1,      a2,       16
+fn8x4 dct, dct
+fn8x4 identity, identity
+fn8x4 dct, adst
+fn8x4 dct, flipadst
+fn8x4 dct, identity
+fn8x4 adst, dct
+fn8x4 adst, adst
+fn8x4 adst, flipadst
+fn8x4 flipadst, dct
+fn8x4 flipadst, adst
+fn8x4 flipadst, flipadst
+fn8x4 identity, dct
+fn8x4 adst, identity
+fn8x4 flipadst, identity
+fn8x4 identity, adst
+fn8x4 identity, flipadst
+
+.macro fn4x8 txfm1, txfm2
+function inv_txfm_add_\txfm1\()_\txfm2\()_4x8_8bpc_lsx
+.ifc \txfm1()_\txfm2, dct_dct
+    bnez          a3,       .NO_HAS_DCONLY_4x8
 
-    la.local      t0,       iadst4_coeffs
-    vsllwil.w.h   vr2,      vr0,      0     // in0
-    vexth.w.h     vr3,      vr0             // in1
-    vsllwil.w.h   vr4,      vr1,      0     // in2
-    vexth.w.h     vr5,      vr1             // in3
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
-
-    adst4x4_1d_lsx vr2, vr3, vr4, vr5, vr0, vr1, vr2, vr3
-
-    vilvl.w       vr4,      vr0,      vr1
-    vilvh.w       vr5,      vr0,      vr1
-    vilvl.w       vr6,      vr2,      vr3
-    vilvh.w       vr7,      vr2,      vr3
-    vilvl.d       vr11,     vr4,      vr6
-    vilvh.d       vr12,     vr4,      vr6
-    vilvl.d       vr13,     vr5,      vr7
-    vilvh.d       vr14,     vr5,      vr7
-
-    vsrari.w      vr11,     vr11,     12
-    vsrari.w      vr12,     vr12,     12
-    vsrari.w      vr13,     vr13,     12
-    vsrari.w      vr14,     vr14,     12
-
-    vreplgr2vr.h  vr15,     zero
-    vst           vr15,     a2,       0
-    vst           vr15,     a2,       16
-
-    adst4x4_1d_lsx vr11, vr12, vr13, vr14, vr11, vr12, vr13, vr14
-
-    vssrarni.h.w  vr11,     vr12,     12
-    vssrarni.h.w  vr13,     vr14,     12
-    vsrari.h      vr11,     vr11,     4
-    vsrari.h      vr13,     vr13,     4
+    idct_dc 4, 8, 0
 
+    DST_ADD_W4 vr10, vr11, vr12, vr13, vr20, vr20
+
+    add.d         a0,       a0,       a1
     alsl.d        t2,       a1,       a0,   1
-    VLD_DST_ADD_W4 vr13, vr11
+    VLD_DST_ADD_W4 vr5, vr5
+    b             .\txfm1\()_\txfm2\()_4X8_END
+.NO_HAS_DCONLY_4x8:
+.endif
+    la.local      t7,       inv_\txfm1\()_8h_x4_lsx
+    la.local      t8,       inv_\txfm2\()_4h_x8_lsx
+    b             inv_txfm_add_4x8_lsx
+.\txfm1\()_\txfm2\()_4X8_END:
+endfunc
 .endm
 
-.macro inv_dct_identity_4x4_lsx
-    la.local      t0,       idct_coeffs
-
-    vld           vr0,      a2,       0
-    vld           vr1,      a2,       16
+fn4x8 dct, dct
+fn4x8 identity, identity
+fn4x8 dct, adst
+fn4x8 dct, flipadst
+fn4x8 dct, identity
+fn4x8 adst, dct
+fn4x8 adst, adst
+fn4x8 adst, flipadst
+fn4x8 flipadst, dct
+fn4x8 flipadst, adst
+fn4x8 flipadst, flipadst
+fn4x8 identity, dct
+fn4x8 adst, identity
+fn4x8 flipadst, identity
+fn4x8 identity, adst
+fn4x8 identity, flipadst
+
+.macro inv_identity4_lsx_x2 in0, in1, in2, in3, in4, out0, out1
+    vsllwil.w.h   vr4,      \in0,    0
+    vexth.w.h     vr5,      \in0
+    vsllwil.w.h   vr6,      \in1,    0
+    vexth.w.h     vr7,      \in1
+    vmul.w        vr4,      vr4,     \in2
+    vmul.w        vr5,      vr5,     \in2
+    vmul.w        vr6,      vr6,     \in2
+    vmul.w        vr7,      vr7,     \in2
+    vssrarni.h.w  vr5,      vr4,     12
+    vssrarni.h.w  vr7,      vr6,     12
+    vsadd.h       \out0,    vr5,     \in3
+    vsadd.h       \out1,    vr7,     \in4
+.endm
 
-    vldrepl.w     vr2,      t0,       8    // 1567
-    vldrepl.w     vr3,      t0,       12   // 3784
-    vldrepl.w     vr8,      t0,       0    // 2896
+.macro vmul_vmadd_w in0, in1, in2, in3, out0, out1
+    vsllwil.w.h   vr22,     \in0,     0
+    vexth.w.h     vr23,     \in0
+    vmul.w        \out0,    vr22,     \in2
+    vmul.w        \out1,    vr23,     \in2
+    vsllwil.w.h   vr22,     \in1,     0
+    vexth.w.h     vr23,     \in1
+    vmadd.w       \out0,    vr22,     \in3
+    vmadd.w       \out1,    vr23,     \in3
+.endm
 
-    dct_4x4_core_lsx vr0, vr1, vr0, vr1, vr3, vr2, vr8, vr8, vr11, vr12
-    vshuf4i.d     vr12,     vr12,     0x01 // 2 6 10 14 3 7 11 15
+.macro vmul_vmsub_w in0, in1, in2, in3, out0, out1
+    vsllwil.w.h   vr22,     \in0,     0
+    vexth.w.h     vr23,     \in0
+    vmul.w        \out0,    vr22,     \in2
+    vmul.w        \out1,    vr23,     \in2
+    vsllwil.w.h   vr22,     \in1,     0
+    vexth.w.h     vr23,     \in1
+    vmsub.w       \out0,    vr22,     \in3
+    vmsub.w       \out1,    vr23,     \in3
+.endm
 
-    vreplgr2vr.h  vr15,     zero
-    li.w          t0,       1697
+.macro inv_dct16_lsx sz
+    inv_dct8_lsx vr0, vr2, vr4, vr6, vr8, vr10, vr12, vr14, \sz
+
+    la.local      t0,       idct_coeffs_h
+    vldrepl.h     vr20,     t0,       16        // 401
+    vldrepl.h     vr21,     t0,       18        // 4076
+    vmulev_vmaddod_lsx vr1, vr15, vr21, vr20, vr16, vr17, \sz
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr1, vr15, vr20, vr21, vr18, vr19, \sz
+    vssrarni.h.w  vr17,     vr16,     12        // t15a
+    vssrarni.h.w  vr19,     vr18,     12        // t8a
+    vldrepl.h     vr20,     t0,       20        // 3166 -> 1583
+    vldrepl.h     vr21,     t0,       22        // 2598 -> 1299
+    vmulev_vmaddod_lsx vr9, vr7, vr21, vr20, vr1, vr16, \sz
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr9, vr7, vr20, vr21, vr15, vr18, \sz
+    vssrarni.h.w  vr16,     vr1,      12        // t14a
+    vssrarni.h.w  vr18,     vr15,     12        // t9a
+    vldrepl.h     vr20,     t0,       24        // 1931
+    vldrepl.h     vr21,     t0,       26        // 3612
+    vmulev_vmaddod_lsx vr5, vr11, vr21, vr20, vr7, vr1, \sz
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr5, vr11, vr20, vr21, vr9, vr15, \sz
+    vssrarni.h.w  vr1,      vr7,      12        // t13a
+    vssrarni.h.w  vr15,     vr9,      12        // t10a
+    vldrepl.h     vr20,     t0,       28        // 3920
+    vldrepl.h     vr21,     t0,       30        // 1189
+    vmulev_vmaddod_lsx vr13, vr3, vr21, vr20, vr5, vr7, \sz
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr13, vr3, vr20, vr21, vr11, vr9, \sz
+    vssrarni.h.w  vr7,      vr5,      12        // t12a
+    vssrarni.h.w  vr9,      vr11,     12        // t11a
+
+    vsadd.h       vr5,      vr19,     vr18     // t8
+    vssub.h       vr11,     vr19,     vr18     // t9
+    vssub.h       vr3,      vr9,      vr15     // t10
+    vsadd.h       vr13,     vr9,      vr15     // t11
+    vsadd.h       vr18,     vr7,      vr1      // t12
+    vssub.h       vr19,     vr7,      vr1      // t13
+    vssub.h       vr9,      vr17,     vr16     // t14
+    vsadd.h       vr15,     vr17,     vr16     // t15
+
+    vldrepl.h     vr20,     t0,       4        // 1567
+    vldrepl.h     vr21,     t0,       6        // 3784
+    vmulev_vmaddod_lsx vr9, vr11, vr21, vr20, vr1, vr16, \sz
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr9, vr11, vr20, vr21, vr7, vr17, \sz
+    vssrarni.h.w  vr16,     vr1,      12       // t14a
+    vssrarni.h.w  vr17,     vr7,      12       // t9a
+
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr19, vr3, vr21, vr20, vr9, vr1, \sz
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr19, vr3, vr20, vr21, vr11, vr7, \sz
+    vneg.w        vr1,      vr1
+    vneg.w        vr9,      vr9
+    vssrarni.h.w  vr7,      vr11,     12       // t13a
+    vssrarni.h.w  vr1,      vr9,      12       // t10a
+    vsadd.h       vr9,      vr5,      vr13     // t8a
+    vssub.h       vr11,     vr5,      vr13     // t11a
+    vssub.h       vr3,      vr15,     vr18     // t12a
+    vsadd.h       vr19,     vr15,     vr18     // t15a
+    vsadd.h       vr5,      vr17,     vr1      // t9
+    vssub.h       vr13,     vr17,     vr1      // t10
+    vssub.h       vr15,     vr16,     vr7      // t13
+    vsadd.h       vr18,     vr16,     vr7      // t14
+
+    vldrepl.h     vr20,     t0,       0        // 2896
+    vmulev_vmaddod_lsx vr15, vr13, vr20, vr20, vr1, vr7, \sz
+    vneg.h        vr21,     vr20
+    vmulev_vmaddod_lsx vr15, vr13, vr20, vr21, vr17, vr16, \sz
+    vssrarni.h.w  vr7,      vr1,      12       // t13a
+    vssrarni.h.w  vr16,     vr17,     12       // t10a
+
+    vmulev_vmaddod_lsx vr3, vr11, vr20, vr20, vr13, vr23, \sz
+    vmulev_vmaddod_lsx vr3, vr11, vr20, vr21, vr15, vr17, \sz
+    vssrarni.h.w  vr23,     vr13,     12       // t12
+    vssrarni.h.w  vr17,     vr15,     12       // t11
+
+    vssub.h       vr15,     vr0,     vr19      // c[15]
+    vsadd.h       vr0,      vr0,     vr19      // c[0]
+    vsadd.h       vr1,      vr2,     vr18      // c[1]
+    vssub.h       vr20,     vr2,     vr18      // c[14]
+    vsadd.h       vr2,      vr4,     vr7       // c[2]
+    vssub.h       vr13,     vr4,     vr7       // c[13]
+    vsadd.h       vr3,      vr6,     vr23      // c[3]
+    vssub.h       vr21,     vr6,     vr23      // c[12]
+    vsadd.h       vr4,      vr8,     vr17      // c[4]
+    vssub.h       vr11,     vr8,     vr17      // c[11]
+    vsadd.h       vr7,      vr14,    vr9       // c[7]
+    vssub.h       vr8,      vr14,    vr9       // c[8]
+    vsadd.h       vr6,      vr12,    vr5       // c[6]
+    vssub.h       vr9,      vr12,    vr5       // c[9]
+    vsadd.h       vr5,      vr10,    vr16      // c[5]
+    vssub.h       vr10,     vr10,    vr16      // c[10]
+    vor.v         vr14,     vr20,    vr20
+    vor.v         vr12,     vr21,    vr21
+.endm
 
-    vilvl.h       vr4,      vr12,     vr11 // 0 2 4 6 8 10 12 14
-    vilvh.h       vr5,      vr12,     vr11 // 1 3 5 7 9 11 13 15
-    vilvl.h       vr10,     vr5,      vr4  // 0 1  2  3  4  5  6  7
-    vilvh.h       vr12,     vr5,      vr4  // 8 9 10 11 12 13 14 15
+functionl inv_dct_8h_x16_lsx
+    inv_dct16_lsx .8h
+endfuncl
 
-    vst           vr15,     a2,       0
-    vst           vr15,     a2,       16
-    vreplgr2vr.w  vr20,     t0
+functionl inv_dct_4h_x16_lsx
+    inv_dct16_lsx .4h
+endfuncl
 
-    identity_4x4_lsx vr10, vr10, vr20, vr10, vr6
-    identity_4x4_lsx vr12, vr12, vr20, vr12, vr7
-    vsrari.h      vr11,      vr6,     4
-    vsrari.h      vr13,      vr7,     4
+.macro VLD_DST_ADD_W4_x4 in0, in1, in2, in3, in4, in5, in6 ,in7
+    alsl.d        t2,       a1,       a0,    1
 
-    alsl.d        t2,       a1,       a0,   1
-    VLD_DST_ADD_W4 vr11, vr13
-.endm
+    VLD_DST_ADD_W4 \in0, \in1
 
-.macro inv_identity_dct_4x4_lsx
-    vld           vr0,      a2,       0
-    vld           vr1,      a2,       16
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       a0,    1
+    VLD_DST_ADD_W4 \in2, \in3
 
-    li.w          t0,       1697
-    vreplgr2vr.w  vr20,     t0
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       a0,    1
+    VLD_DST_ADD_W4 \in4, \in5
 
-    identity_4x4_lsx vr0, vr0, vr20, vr0, vr0
-    identity_4x4_lsx vr1, vr1, vr20, vr1, vr1
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       a0,    1
+    VLD_DST_ADD_W4 \in6, \in7
+.endm
 
-    vreplgr2vr.h  vr15,     zero
+.macro def_fn_4x16_base txfm
+functionl inv_txfm_\txfm\()add_4x16_lsx
+    PUSH_REG
+    blt           a3,       t5,       416f
+    vld           vr0,      a2,       16
+    vld           vr1,      a2,       48
+    vld           vr2,      a2,       80
+    vld           vr3,      a2,       112
+    vxor.v        vr23,     vr23,     vr23
+.irp i, 16, 48, 80, 112
+    vst           vr23,     a2,       \i
+.endr
 
-    vilvl.h       vr4,      vr1,      vr0  // 0 2 4 6 8 10 12 14
-    vilvh.h       vr5,      vr1,      vr0  // 1 3 5 7 9 11 13 15
-    vilvl.h       vr13,     vr5,      vr4  // 0 1  2  3  4  5  6  7
-    vilvh.h       vr14,     vr5,      vr4  // 8 9 10 11 12 13 14 15
+    move          t6,       ra
+    jirl          ra,       t7,       0
+    move          ra,       t6
 
-    vst           vr15,     a2,       0
-    vst           vr15,     a2,       16
+.ifnc \txfm, identity_
+    vsrari.h      vr0,      vr0,      1
+    vsrari.h      vr1,      vr1,      1
+    vsrari.h      vr2,      vr2,      1
+    vsrari.h      vr3,      vr3,      1
+.endif
 
-    la.local      t0,       idct_coeffs
+    LSX_TRANSPOSE4x8_H vr0, vr1, vr2, vr3, vr8, vr9, vr24, vr25, vr26, \
+                       vr27, vr14, vr28, vr10, vr11, vr12, vr13
 
-    vldrepl.w     vr20,     t0,       8    // 1567
-    vldrepl.w     vr21,     t0,       12   // 3784
-    vldrepl.w     vr22,     t0,       0    // 2896
+416:
+    ble           t5,       a3,       416416f
+.irp i, vr8, vr9, vr24, vr25, vr26, vr27, vr14, vr28
+    vxor.v        \i,       \i,       \i
+.endr
 
-    dct_4x4_core_lsx vr13, vr14, vr13, vr14, vr21, vr20, vr22, vr22, vr13, vr14
+416416:
+    vld           vr0,      a2,       0
+    vld           vr1,      a2,       32
+    vld           vr2,      a2,       64
+    vld           vr3,      a2,       96
+    vxor.v        vr23,     vr23,     vr23
+.irp i, 0, 32, 64, 96
+    vst           vr23,     a2,       \i
+.endr
 
-    vshuf4i.d     vr14,     vr14,     0x01
-    vsrari.h      vr13,     vr13,     4
-    vsrari.h      vr14,     vr14,     4
+    move          t6,       ra
+    jirl          ra,       t7,       0
+    move          ra,       t6
 
-    alsl.d        t2,       a1,       a0,   1
-    VLD_DST_ADD_W4 vr13, vr14
-.endm
+.ifnc \txfm, identity_
+    vsrari.h      vr0,      vr0,      1
+    vsrari.h      vr1,      vr1,      1
+    vsrari.h      vr2,      vr2,      1
+    vsrari.h      vr3,      vr3,      1
+.endif
 
-.macro inv_flipadst_identity_4x4_lsx
-    vld           vr0,      a2,       0
-    vld           vr1,      a2,       16
+    LSX_TRANSPOSE4x8_H vr0, vr1, vr2, vr3, vr0, vr1, vr2, vr3, vr4, vr5, \
+                       vr6, vr7, vr16, vr17, vr18, vr19
 
-    la.local      t0,       iadst4_coeffs
-    vsllwil.w.h   vr2,      vr0,      0     // in0
-    vexth.w.h     vr3,      vr0             // in1
-    vsllwil.w.h   vr4,      vr1,      0     // in2
-    vexth.w.h     vr5,      vr1             // in3
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
-
-    adst4x4_1d_lsx vr2, vr3, vr4, vr5, vr10, vr11, vr12, vr13
-
-    vssrarni.h.w  vr12,     vr13,     12
-    vssrarni.h.w  vr10,     vr11,     12
-
-    vilvl.h       vr4,      vr10,     vr12  // 0 2 4 6 8 10 12 14
-    vilvh.h       vr5,      vr10,     vr12  // 1 3 5 7 9 11 13 15
-    vilvl.h       vr11,     vr5,      vr4   // 0 1  2  3  4  5  6  7
-    vilvh.h       vr13,     vr5,      vr4   // 8 9 10 11 12 13 14 15
-
-    vreplgr2vr.h  vr15,     zero
-    li.w          t0,       1697
+    vor.v         vr10,     vr24,     vr24
+    vor.v         vr11,     vr25,     vr25
+    vor.v         vr12,     vr26,     vr26
+    vor.v         vr13,     vr27,     vr27
+    vor.v         vr15,     vr28,     vr28
 
-    vst           vr15,     a2,       0
-    vst           vr15,     a2,       16
-    vreplgr2vr.w  vr20,     t0
+    move          t6,       ra
+    jirl          ra,       t8,       0
+    move          ra,       t6
+
+    vilvl.d       vr16,     vr1,      vr0
+    vilvl.d       vr17,     vr3,      vr2
+    vilvl.d       vr18,     vr5,      vr4
+    vilvl.d       vr19,     vr7,      vr6
+    vilvl.d       vr20,     vr9,      vr8
+    vilvl.d       vr21,     vr11,     vr10
+    vilvl.d       vr22,     vr13,     vr12
+    vilvl.d       vr23,     vr15,     vr14
 
-    identity_4x4_lsx vr11, vr11, vr20, vr11, vr6
-    identity_4x4_lsx vr13, vr13, vr20, vr13, vr7
-    vsrari.h      vr11,     vr6,     4
-    vsrari.h      vr13,     vr7,     4
+.irp i, vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+    vsrari.h     \i,       \i,       4
+.endr
 
-    alsl.d        t2,       a1,      a0,   1
-    VLD_DST_ADD_W4 vr11, vr13
+    VLD_DST_ADD_W4_x4 vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+    POP_REG
+endfuncl
 .endm
 
-.macro inv_identity_flipadst_4x4_lsx
-    vld           vr0,      a2,       0
-    vld           vr1,      a2,       16
+def_fn_4x16_base identity_
+def_fn_4x16_base
 
-    li.w          t0,       1697
-    vreplgr2vr.w  vr20,     t0
+.macro fn4x16 txfm1, txfm2, eob_half
+function inv_txfm_add_\txfm1\()_\txfm2\()_4x16_8bpc_lsx
+.ifc \txfm1()_\txfm2, dct_dct
+    bnez          a3,       .NO_HAS_DCONLY_4x16
 
-    identity_4x4_lsx vr0, vr0, vr20, vr0, vr0
-    identity_4x4_lsx vr1, vr1, vr20, vr1, vr1
+    idct_dc 4, 16, 1
 
-    vilvl.h       vr4,      vr1,      vr0
-    vilvh.h       vr5,      vr1,      vr0
-    vilvl.h       vr11,     vr5,      vr4
-    vilvh.h       vr13,     vr5,      vr4
+    DST_ADD_W4 vr10, vr11, vr12, vr13, vr5, vr5
 
-    vreplgr2vr.h  vr15,     zero
-    vst           vr15,     a2,       0
-    vst           vr15,     a2,       16
+.rept 3
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       a0,   1
 
-    la.local      t0,       iadst4_coeffs
-    vsllwil.w.h   vr2,      vr11,     0   // in0
-    vexth.w.h     vr3,      vr11          // in1
-    vsllwil.w.h   vr4,      vr13,     0   // in2
-    vexth.w.h     vr5,      vr13          // in3
-    vldrepl.w     vr20,     t0,       0   // 1321
-    vldrepl.w     vr21,     t0,       4   // 3803
-    vldrepl.w     vr22,     t0,       8   // 2482
-    vldrepl.w     vr23,     t0,       12  // 3344
-
-    adst4x4_1d_lsx vr2, vr3, vr4, vr5, vr0, vr1, vr2, vr3
-
-    vssrarni.h.w  vr0,      vr1,      12  // 8 9 10 11 12 13 14 15
-    vssrarni.h.w  vr2,      vr3,      12  // 0 1  2  3  4  5  6  7
-    vsrari.h      vr11,     vr0,      4
-    vsrari.h      vr13,     vr2,      4
+    VLD_DST_ADD_W4 vr5, vr5
+.endr
+    b             .\txfm1\()_\txfm2\()_4X16_END
 
-    alsl.d        t2,       a1,       a0,   1
-    VLD_DST_ADD_W4 vr13, vr11
+.NO_HAS_DCONLY_4x16:
+.endif
+    li.w          t5,       \eob_half
+    la.local      t7,       inv_\txfm1\()_8h_x4_lsx
+.ifc \txfm1, identity
+    la.local      t7,       inv_\txfm1\()_8h_x4_lsx1
+.endif
+    la.local      t8,       inv_\txfm2\()_4h_x16_lsx
+
+.ifc \txfm1, identity
+    b             inv_txfm_identity_add_4x16_lsx
+.else
+    b             inv_txfm_add_4x16_lsx
+.endif
+.\txfm1\()_\txfm2\()_4X16_END:
+endfunc
 .endm
 
-.macro inv_identity_adst_4x4_lsx
-    vld           vr0,      a2,       0
-    vld           vr1,      a2,       16
+fn4x16 dct, dct, 29
+fn4x16 identity, identity, 29
+fn4x16 dct, adst, 29
+fn4x16 dct, flipadst, 29
+fn4x16 dct, identity, 8
+fn4x16 adst, dct, 29
+fn4x16 adst, adst, 29
+fn4x16 adst, flipadst, 29
+fn4x16 flipadst, dct, 29
+fn4x16 flipadst, adst, 29
+fn4x16 flipadst, flipadst, 29
+fn4x16 identity, dct, 32
+fn4x16 adst, identity, 8
+fn4x16 flipadst, identity, 8
+fn4x16 identity, adst, 32
+fn4x16 identity, flipadst, 32
+
+.macro inv_identity16_lsx in0, in1, in2, out0, sz
+.ifc \sz, .8h
+    vsllwil.w.h   vr16,     \in0,     0
+    vexth.w.h     vr17,     \in0
+    vmul.w        vr16,     vr16,     \in1
+    vmul.w        vr17,     vr17,     \in1
+    vsadd.h       \in2,     \in2,     \in2
+    vssrarni.h.w  vr17,     vr16,     11
+    vsadd.h       \out0,    vr17,     \in2
+.else
+    vsllwil.w.h   vr16,     \in0,     0
+    vmul.w        vr16,     vr16,     \in1
+    vsadd.h       \in2,     \in2,     \in2
+    vssrarni.h.w  vr16,     vr16,     11
+    vsadd.h       \out0,    vr16,     \in2
+.endif
+.endm
+
+.macro inv_identity16_lsx1 in0, in1, in2, out0
+    vsllwil.w.h   vr16,     \in0,     0
+    vexth.w.h     vr17,     \in1
+    vmul.w        vr18,     vr16,     \in2
+    vmul.w        vr19,     vr17,     \in2
+    vsrari.w      vr18,     vr18,     11
+    vsrari.w      vr19,     vr19,     11
+    vslli.w       vr16,     vr16,     1
+    vslli.w       vr17,     vr17,     1
+    vadd.w        vr16,     vr18,     vr16
+    vadd.w        \out0,    vr19,     vr17
+    vssrarni.h.w  \out0,    vr16,     1
+.endm
 
+functionl inv_identity_8h_x16_lsx
     li.w          t0,       1697
     vreplgr2vr.w  vr20,     t0
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, vr8, \
+    vr9, vr10, vr11, vr12, vr13, vr14, vr15
+    inv_identity16_lsx \i, vr20, \i, \i, .8h
+.endr
+endfuncl
 
-    identity_4x4_lsx vr0, vr0, vr20, vr0, vr0
-    identity_4x4_lsx vr1, vr1, vr20, vr1, vr1
+functionl inv_identity_4h_x16_lsx
+    li.w          t0,       1697
+    vreplgr2vr.w  vr20,     t0
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, vr8, \
+    vr9, vr10, vr11, vr12, vr13, vr14, vr15
+    inv_identity16_lsx \i, vr20, \i, \i, .4h
+.endr
+endfuncl
 
-    vilvl.h       vr4,      vr1,      vr0
-    vilvh.h       vr5,      vr1,      vr0
-    vilvl.h       vr11,     vr5,      vr4
-    vilvh.h       vr13,     vr5,      vr4
+functionl inv_identity_8h_x16_lsx1
+    li.w          t0,       1697
+    vreplgr2vr.w  vr20,     t0
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, vr8, \
+    vr9, vr10, vr11, vr12, vr13, vr14, vr15
+    inv_identity16_lsx1 \i, \i, vr20, \i
+.endr
+endfuncl
 
-    vreplgr2vr.h  vr15,     zero
-    vst           vr15,     a2,       0
-    vst           vr15,     a2,       16
+const iadst16_coeffs_h, align=4
+    .short         4091, 201, 3973, 995
+    .short         3703, 1751, 3290, 2440
+    .short         2751, 3035, 2106, 3513
+    .short         1380, 3857, 601, 4052
+endconst
 
-    la.local      t0,       iadst4_coeffs
-    vsllwil.w.h   vr2,      vr11,     0     // in0
-    vexth.w.h     vr3,      vr11            // in1
-    vsllwil.w.h   vr4,      vr13,     0     // in2
-    vexth.w.h     vr5,      vr13            // in3
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
-
-    adst4x4_1d_lsx vr2, vr3, vr4, vr5, vr0, vr1, vr2, vr3
-
-    vssrarni.h.w  vr1,      vr0,      12
-    vssrarni.h.w  vr3,      vr2,      12
-    vsrari.h      vr11,     vr1,      4
-    vsrari.h      vr13,     vr3,      4
+.macro inv_adst16_lsx txfm, sz
+    la.local      t0,       iadst16_coeffs_h
+    vldrepl.h     vr20,     t0,        0        // 4091
+    vldrepl.h     vr21,     t0,        2        // 201
+    vmulev_vmaddod_lsx vr15, vr0, vr20, vr21, vr16, vr18, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr15, vr0, vr21, vr20, vr17, vr19, \sz
+    vssrarni.h.w  vr18,     vr16,      12       // t0
+    vssrarni.h.w  vr19,     vr17,      12       // t1
+    vldrepl.h     vr20,     t0,        4        // 3973
+    vldrepl.h     vr21,     t0,        6        // 995
+    vmulev_vmaddod_lsx vr13, vr2, vr20, vr21, vr16, vr0, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr13, vr2, vr21, vr20, vr17, vr15, \sz
+    vssrarni.h.w  vr0,      vr16,      12       // t2
+    vssrarni.h.w  vr15,     vr17,      12       // t3
+    vldrepl.h     vr20,     t0,        8       // 3703
+    vldrepl.h     vr21,     t0,        10       // 1751
+    vmulev_vmaddod_lsx vr11, vr4, vr20, vr21, vr16, vr2, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr11, vr4, vr21, vr20, vr17, vr13, \sz
+    vssrarni.h.w  vr2,      vr16,      12       // t4
+    vssrarni.h.w  vr13,     vr17,      12       // t5
+    vldrepl.h     vr20,     t0,        12       // 3290 -> 1645
+    vldrepl.h     vr21,     t0,        14       // 2440 -> 1220
+    vmulev_vmaddod_lsx vr9, vr6, vr20, vr21, vr16, vr4, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr9, vr6, vr21, vr20, vr17, vr11, \sz
+    vssrarni.h.w  vr4,      vr16,      12       // t6
+    vssrarni.h.w  vr11,     vr17,      12       // t7
+    vldrepl.h     vr20,     t0,        16       // 2751
+    vldrepl.h     vr21,     t0,        18       // 3035
+    vmulev_vmaddod_lsx vr7, vr8, vr20, vr21, vr16, vr6, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr7, vr8, vr21, vr20, vr17, vr9, \sz
+    vssrarni.h.w  vr6,      vr16,      12       // t8
+    vssrarni.h.w  vr9,      vr17,      12       // t9
+    vldrepl.h     vr20,     t0,        20       // 2106
+    vldrepl.h     vr21,     t0,        22       // 3513
+    vmulev_vmaddod_lsx vr5, vr10, vr20, vr21, vr16, vr7, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr5, vr10, vr21, vr20, vr17, vr8, \sz
+    vssrarni.h.w  vr7,      vr16,      12       // t10
+    vssrarni.h.w  vr8,      vr17,      12       // t11
+    vldrepl.h     vr20,     t0,        24       // 1380
+    vldrepl.h     vr21,     t0,        26       // 3857
+    vmulev_vmaddod_lsx vr3, vr12, vr20, vr21, vr16, vr5, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr3, vr12, vr21, vr20, vr17, vr10, \sz
+    vssrarni.h.w  vr5,      vr16,      12       // t12
+    vssrarni.h.w  vr10,     vr17,      12       // t13
+    vldrepl.h     vr20,     t0,        28       // 601
+    vldrepl.h     vr21,     t0,        30       // 4052
+    vmulev_vmaddod_lsx vr1, vr14, vr20, vr21, vr16, vr3, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr1, vr14, vr21, vr20, vr17, vr12, \sz
+    vssrarni.h.w  vr3,      vr16,      12       // t14
+    vssrarni.h.w  vr12,     vr17,      12       // t15
 
-    alsl.d        t2,       a1,       a0,   1
-    VLD_DST_ADD_W4 vr11, vr13
-.endm
+    vsadd.h       vr1,      vr18,      vr6      // t0a
+    vssub.h       vr14,     vr18,      vr6      // t8a
+    vsadd.h       vr16,     vr19,      vr9      // t1a
+    vssub.h       vr17,     vr19,      vr9      // t9a
+    vsadd.h       vr6,      vr0,       vr7      // t2a
+    vssub.h       vr18,     vr0,       vr7      // t10a
+    vsadd.h       vr9,      vr15,      vr8      // t3a
+    vssub.h       vr19,     vr15,      vr8      // t11a
+    vsadd.h       vr0,      vr2,       vr5      // t4a
+    vssub.h       vr7,      vr2,       vr5      // t12a
+    vsadd.h       vr8,      vr13,      vr10     // t5a
+    vssub.h       vr15,     vr13,      vr10     // t13a
+    vsadd.h       vr2,      vr4,       vr3      // t6a
+    vssub.h       vr5,      vr4,       vr3      // t14a
+    vsadd.h       vr10,     vr11,      vr12     // t7a
+    vssub.h       vr13,     vr11,      vr12     // t15a
 
-.macro inv_adst_identity_4x4_lsx
-    vld           vr0,      a2,       0
-    vld           vr1,      a2,       16
+    la.local      t0,       idct_coeffs_h
 
-    la.local      t0,       iadst4_coeffs
-    vsllwil.w.h   vr2,      vr0,      0     // in0
-    vexth.w.h     vr3,      vr0             // in1
-    vsllwil.w.h   vr4,      vr1,      0     // in2
-    vexth.w.h     vr5,      vr1             // in3
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
+    vldrepl.h     vr20,     t0,        8        // 799
+    vldrepl.h     vr21,     t0,        10       // 4017
+    vmulev_vmaddod_lsx vr14, vr17, vr21, vr20, vr3, vr11, \sz
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr14, vr17, vr20, vr21, vr4, vr12, \sz
+    vssrarni.h.w  vr11,     vr3,       12       // t8
+    vssrarni.h.w  vr12,     vr4,       12       // t9
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr15, vr7, vr20, vr21, vr3, vr14, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr15, vr7, vr21, vr20, vr4, vr17, \sz
+    vssrarni.h.w  vr14,     vr3,       12       // t13
+    vssrarni.h.w  vr17,     vr4,       12       // t12
+    vldrepl.h     vr20,     t0,        12       // 3406
+    vldrepl.h     vr21,     t0,        14       // 2276
+    vmulev_vmaddod_lsx vr18, vr19, vr21, vr20, vr3, vr7, \sz
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr18, vr19, vr20, vr21, vr4, vr15, \sz
+    vssrarni.h.w  vr7,      vr3,       12       // t10
+    vssrarni.h.w  vr15,     vr4,       12       // t11
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr13, vr5, vr20, vr21, vr3, vr18, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr13, vr5, vr21, vr20, vr4, vr19, \sz
+    vssrarni.h.w  vr18,     vr3,       12       // t15
+    vssrarni.h.w  vr19,     vr4,       12       // t14
 
-    adst4x4_1d_lsx vr2, vr3, vr4, vr5, vr0, vr1, vr2, vr3
+    vsadd.h       vr5,      vr1,       vr0      // t0
+    vssub.h       vr13,     vr1,       vr0      // t4
+    vsadd.h       vr3,      vr16,      vr8      // t1
+    vssub.h       vr4,      vr16,      vr8      // t5
+    vsadd.h       vr0,      vr6,       vr2      // t2
+    vssub.h       vr1,      vr6,       vr2      // t6
+    vsadd.h       vr8,      vr9,       vr10     // t3
+    vssub.h       vr16,     vr9,       vr10     // t7
+    vsadd.h       vr2,      vr11,      vr17     // t8a
+    vssub.h       vr6,      vr11,      vr17     // t12a
+    vsadd.h       vr9,      vr12,      vr14     // t9a
+    vssub.h       vr10,     vr12,      vr14     // t13a
+    vsadd.h       vr11,     vr7,       vr19     // t10a
+    vssub.h       vr17,     vr7,       vr19     // t14a
+    vsadd.h       vr12,     vr15,      vr18     // t11a
+    vssub.h       vr14,     vr15,      vr18     // t15a
 
-    LSX_TRANSPOSE4x4_W vr0, vr1, vr2, vr3, vr11, vr13, vr12, vr14, vr6, vr7
+    vldrepl.h     vr20,     t0,        4        // 1567
+    vldrepl.h     vr21,     t0,        6       // 3784
+    vmulev_vmaddod_lsx vr13, vr4, vr21, vr20, vr7, vr18, \sz
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr13, vr4, vr20, vr21, vr15, vr19, \sz
+    vssrarni.h.w  vr18,     vr7,       12       // t4a
+    vssrarni.h.w  vr19,     vr15,      12       // t5a
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr16, vr1, vr20, vr21, vr7, vr4, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr16, vr1, vr21, vr20, vr15, vr13, \sz
+    vssrarni.h.w  vr4,      vr7,       12       // t7a
+    vssrarni.h.w  vr13,     vr15,      12       // t6a
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr6, vr10, vr21, vr20, vr7, vr1, \sz
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr6, vr10, vr20, vr21, vr15, vr16, \sz
+    vssrarni.h.w  vr1,      vr7,       12       // t12
+    vssrarni.h.w  vr16,     vr15,      12       // t13
+    vneg.h        vr21,     vr21
+    vmulev_vmaddod_lsx vr14, vr17, vr20, vr21, vr7, vr6, \sz
+    vneg.h        vr20,     vr20
+    vmulev_vmaddod_lsx vr14, vr17, vr21, vr20, vr15, vr10, \sz
+    vssrarni.h.w  vr6,      vr7,       12       // t15
+    vssrarni.h.w  vr10,     vr15,      12       // t14
 
-    vssrarni.h.w  vr13,     vr11,     12
-    vssrarni.h.w  vr14,     vr12,     12
+    vssub.h       vr17,     vr5,       vr0      // t2a
+    vsadd.h       vr14,     vr5,       vr0      // out[0]
+    vssub.h       vr7,      vr3,       vr8      // t3a
+    vsadd.h       vr15,     vr3,       vr8      // out[15]
+    vsllwil.w.h   vr22,     vr15,      0
+    vexth.w.h     vr15,     vr15
+    vneg.w        vr22,     vr22
+    vneg.w        vr15,     vr15
+    vssrarni.h.w  vr15,     vr22,      0        // out[15]
 
-    vreplgr2vr.h  vr15,     zero
-    li.w          t0,       1697
+    vsadd.h       vr3,      vr19,      vr4      // out[12]
+    vssub.h       vr8,      vr19,      vr4      // t7
+    vssub.h       vr0,      vr18,      vr13     // t6
+    vsadd.h       vr5,      vr18,      vr13     // out[3]
+    vsllwil.w.h   vr22,     vr5,       0
+    vexth.w.h     vr5,      vr5
+    vneg.w        vr22,     vr22
+    vneg.w        vr5,      vr5
+    vssrarni.h.w  vr5,      vr22,      0        // out[3]
 
-    vst           vr15,     a2,       0
-    vst           vr15,     a2,       16
-    vreplgr2vr.w  vr20,     t0
+    vsadd.h       vr13,     vr9,       vr12     // out[14]
+    vssub.h       vr19,     vr9,       vr12     // t11
+    vssub.h       vr4,      vr2,       vr11     // t10
+    vsadd.h       vr18,     vr2,       vr11     // out[1]
+    vsllwil.w.h   vr22,     vr18,      0
+    vexth.w.h     vr18,     vr18
+    vneg.w        vr22,     vr22
+    vneg.w        vr18,     vr18
+    vssrarni.h.w  vr18,     vr22,      0        // out[1]
 
-    identity_4x4_lsx vr13, vr13, vr20, vr13, vr6
-    identity_4x4_lsx vr14, vr14, vr20, vr14, vr7
-    vsrari.h      vr11,     vr6,      4
-    vsrari.h      vr13,     vr7,      4
+    vsadd.h       vr2,      vr1,       vr10     // out[2]
+    vssub.h       vr11,     vr1,       vr10     // t14a
+    vssub.h       vr12,     vr16,      vr6      // t15a
+    vsadd.h       vr9,      vr16,      vr6      // out[13]
+    vsllwil.w.h   vr22,     vr9,       0
+    vexth.w.h     vr9,      vr9
+    vneg.w        vr22,     vr22
+    vneg.w        vr9,      vr9
+    vssrarni.h.w  vr9,      vr22,      0        // out[13]
 
-    alsl.d        t2,       a1,       a0,   1
-    VLD_DST_ADD_W4 vr11, vr13
-.endm
+    vldrepl.h     vr20,     t0,        0        // 2896
+    vmulev_vmaddod_lsx vr17, vr7, vr20, vr20, vr6, vr10, \sz
+    vneg.h        vr21,     vr20
+    vmulev_vmaddod_lsx vr17, vr7, vr20, vr21, vr16, vr1, \sz
+    vssrarni.h.w  vr1,      vr16,      12       // out[8]
+    vsrari.w      vr6,      vr6,       12
+    vsrari.w      vr10,     vr10,      12
+    vneg.w        vr6,      vr6
+    vneg.w        vr10,     vr10
+    vssrarni.h.w  vr10,     vr6,       0        // out[7]
+    vmulev_vmaddod_lsx vr0, vr8, vr20, vr21, vr16, vr17, \sz
+    vmulev_vmaddod_lsx vr0, vr8, vr20, vr20, vr6, vr7, \sz
+    vssrarni.h.w  vr7,      vr6,       12       // out[4]
+    vsrari.w      vr16,     vr16,      12
+    vsrari.w      vr17,     vr17,      12
+    vneg.w        vr16,     vr16
+    vneg.w        vr17,     vr17
+    vssrarni.h.w  vr17,     vr16,       0        // out[11]
 
-.macro fun4x4 type1, type2
-function inv_txfm_add_\type1\()_\type2\()_4x4_8bpc_lsx
-.ifc \type1\()_\type2, dct_dct
-    bnez          a3,       .LLL
+    vmulev_vmaddod_lsx vr4, vr19, vr20, vr21, vr16, vr0, \sz
+    vmulev_vmaddod_lsx vr4, vr19, vr20, vr20, vr6, vr8, \sz
+    vssrarni.h.w  vr8,      vr6,       12       // out[6]
+    vsrari.w      vr16,     vr16,      12
+    vsrari.w      vr0,      vr0,       12
+    vneg.w        vr16,     vr16
+    vneg.w        vr0,      vr0
+    vssrarni.h.w  vr0,      vr16,      0    // out[9]
 
-    vldi          vr0,      0x8b5            // 181
-    ld.h          t2,       a2,       0      // dc
-    st.h          zero,     a2,       0
-    vreplgr2vr.w  vr1,      t2
-    vldi          vr3,      0x880            // 128
-    vmul.w        vr2,      vr0,      vr1
-    vld           vr10,     a0,       0
-    vsrari.w      vr2,      vr2,      8
-    vldx          vr11,     a0,       a1
-    vmadd.w       vr3,      vr2,      vr0
-    alsl.d        t2,       a1,       a0,    1
-    vssrarni.h.w  vr3,      vr3,      12
-    vld           vr12,     t2,       0
-    vldx          vr13,     t2,       a1
+    vmulev_vmaddod_lsx vr11, vr12, vr20, vr20, vr6, vr4, \sz
+    vmulev_vmaddod_lsx vr11, vr12, vr20, vr21, vr16, vr19, \sz
+    vssrarni.h.w  vr19,     vr16,      12       // out[10]
+    vsrari.w      vr6,      vr6,       12
+    vsrari.w      vr4,      vr4,       12
+    vneg.w        vr6,      vr6
+    vneg.w        vr4,      vr4
+    vssrarni.h.w  vr4,      vr6,       0        // out[5]
+
+.ifc \txfm, adst
+    vor.v         vr12,     vr3,       vr3
+    vor.v         vr3,      vr5,       vr5
+    vor.v         vr5,      vr4,       vr4
+    vor.v         vr4,      vr7,       vr7
+    vor.v         vr7,      vr10,      vr10
+    vor.v         vr10,     vr19,      vr19
+    vor.v         vr6,      vr8,       vr8
+    vor.v         vr8,      vr1,       vr1
+    vor.v         vr11,     vr17,      vr17
+    vor.v         vr20,     vr13,      vr13
+    vor.v         vr13,     vr9,       vr9
+    vor.v         vr9,      vr0,       vr0
+    vor.v         vr0,      vr14,      vr14
+    vor.v         vr14,     vr20,      vr20
+    vor.v         vr1,      vr18,      vr18
+.else
+    vor.v         vr6,      vr0,       vr0
+    vor.v         vr0,      vr15,      vr15
+    vor.v         vr15,     vr14,      vr14
+    vor.v         vr14,     vr18,      vr18
+    vor.v         vr11,     vr7,       vr7
+    vor.v         vr7,      vr1,       vr1
+    vor.v         vr1,      vr13,      vr13
+    vor.v         vr13,     vr2,       vr2
+    vor.v         vr2,      vr9,       vr9
+    vor.v         vr9,      vr8,       vr8
+    vor.v         vr8,      vr10,      vr10
+    vor.v         vr10,     vr4,       vr4
+    vor.v         vr4,      vr17,      vr17
+    vor.v         vr12,     vr5,       vr5
+    vor.v         vr5,      vr19,      vr19
+.endif
+.endm // inv_adst16_lsx
 
-    DST_ADD_W4    vr10, vr11, vr12, vr13, vr3, vr3
+functionl inv_adst_8h_x16_lsx
+    inv_adst16_lsx adst, 8h
+endfuncl
 
-    b             .IDST_\type1\()_\type2\()_4X4_END
-.LLL:
-.endif
+functionl inv_flipadst_8h_x16_lsx
+    inv_adst16_lsx flipadst, 8h
+endfuncl
 
-    inv_\type1\()_\type2\()_4x4_lsx
-.IDST_\type1\()_\type2\()_4X4_END:
-endfunc
-.endm
+functionl inv_adst_4h_x16_lsx
+    inv_adst16_lsx adst, 4h
+endfuncl
 
-fun4x4 dct, dct
-fun4x4 identity, identity
-fun4x4 adst, dct
-fun4x4 dct, adst
-fun4x4 adst, adst
-fun4x4 dct, flipadst
-fun4x4 flipadst, adst
-fun4x4 adst, flipadst
-fun4x4 flipadst, dct
-fun4x4 flipadst, flipadst
-fun4x4 dct, identity
-fun4x4 identity, dct
-fun4x4 flipadst, identity
-fun4x4 identity, flipadst
-fun4x4 identity, adst
-fun4x4 adst, identity
+functionl inv_flipadst_4h_x16_lsx
+    inv_adst16_lsx flipadst, 4h
+endfuncl
 
-function inv_txfm_add_dct_dct_4x8_8bpc_lsx
-    bnez          a3,       .NO_HAS_DCONLY_4x8
+.macro VLD_DST_ADD_W8_x4 in0, in1, in2, in3, in4, in5, in6, in7, in8, \
+                         in9, in10, in11, in12, in13, in14, in15
 
-    ld.h          t2,       a2,       0      // dc
-    vldi          vr0,      0x8b5            // 181
-    vreplgr2vr.w  vr1,      t2
-    vldi          vr5,      0x880            // 128
-    vmul.w        vr2,      vr0,      vr1
-    st.h          zero,     a2,       0
-    vsrari.w      vr2,      vr2,      8
-    vld           vr10,     a0,       0
-    vmul.w        vr2,      vr2,      vr0
-    vldx          vr11,     a0,       a1
-    vsrari.w      vr2,      vr2,      8
     alsl.d        t2,       a1,       a0,    1
-    vmadd.w       vr5,      vr2,      vr0
-    vld           vr12,     t2,       0
-    vssrarni.h.w  vr5,      vr5,      12
-    vldx          vr13,     t2,       a1
-
-    DST_ADD_W4 vr10, vr11, vr12, vr13, vr5, vr5
+    VLD_DST_ADD_W8 \in0, \in1, \in2, \in3
 
-    alsl.d        a0,       a1,       a0,   2
-    alsl.d        t2,       a1,       t2,   2
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       a0,    1
+    VLD_DST_ADD_W8 \in4, \in5, \in6, \in7
 
-    VLD_DST_ADD_W4 vr5, vr5
-    b             .DCT_DCT_4x8_END
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       a0,    1
+    VLD_DST_ADD_W8 \in8, \in9, \in10, \in11
 
-.NO_HAS_DCONLY_4x8:
-    // sh=8 sw=4
-    la.local      t0,       idct_coeffs
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       a0,    1
+    VLD_DST_ADD_W8 \in12, \in13, \in14, \in15
+.endm
 
-    vld           vr0,      a2,       0    //  0  1  2  3  4  5  6  7  in0
-    vld           vr1,      a2,       16   //  8  9 10 11 12 13 14 15  in1
-    vld           vr20,     a2,       32   // 16 17 18 19 20 21 22 23  in2
-    vld           vr21,     a2,       48   // 24 25 26 27 28 29 30 31  in3
+.macro def_base_8x16 txfm1
+functionl inv_txfm_\txfm1\()add_8x16_lsx
+    blt     a3,    t5,   816f
+    vld_x8 a2, 16, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    vxor.v        vr23,     vr23,     vr23
+.irp i, 16, 48, 80, 112, 144, 176, 208, 240
+    vst           vr23,     a2,       \i
+.endr
 
-    vldrepl.w     vr2,      t0,       8    // 1567
-    vldrepl.w     vr3,      t0,       12   // 3784
-    vldrepl.w     vr8,      t0,       0    // 2896
+    li.w          t0,       2896
+    vreplgr2vr.w  vr23,     t0
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    rect2_lsx     \i,       vr23,     \i
+.endr
 
-.macro DCT4_4Wx8H_1D_LSX
-    // in1 in3
-    vsllwil.w.h   vr4,      vr1,      0    // in1
-    vsllwil.w.h   vr5,      vr21,     0    // in3
-    vmul.w        vr4,      vr4,      vr8
-    vmul.w        vr5,      vr5,      vr8
-    vsrari.w      vr4,      vr4,      12
-    vsrari.w      vr5,      vr5,      12
-    vmul.w        vr6,      vr4,      vr3
-    vmul.w        vr7,      vr4,      vr2
-    vmadd.w       vr6,      vr5,      vr2  // t3 0 1 2 3
-    vmsub.w       vr7,      vr5,      vr3  // t2 0 1 2 3
-    vexth.w.h     vr4,      vr1            // in1
-    vexth.w.h     vr5,      vr21           // in3
-    vmul.w        vr4,      vr4,      vr8
-    vmul.w        vr5,      vr5,      vr8
-    vsrari.w      vr4,      vr4,      12
-    vsrari.w      vr5,      vr5,      12
-    vmul.w        vr9,      vr4,      vr3
-    vmul.w        vr10,     vr4,      vr2
-    vmadd.w       vr9,      vr5,      vr2  // t3 4 5 6 7
-    vmsub.w       vr10,     vr5,      vr3  // t2 4 5 6 7
-
-    // in0 in2
-    vsllwil.w.h   vr4,      vr0,      0    // in0
-    vsllwil.w.h   vr5,      vr20,     0    // in2
-    vmul.w        vr4,      vr4,      vr8
-    vmul.w        vr5,      vr5,      vr8
-    vsrari.w      vr4,      vr4,      12
-    vsrari.w      vr5,      vr5,      12
-    vmul.w        vr11,     vr4,      vr8
-    vmul.w        vr12,     vr4,      vr8
-    vmadd.w       vr11,     vr5,      vr8  // t0 0 1 2 3
-    vmsub.w       vr12,     vr5,      vr8  // t1 0 1 2 3
-    vexth.w.h     vr4,      vr0            // in0
-    vexth.w.h     vr5,      vr20           // in2
-    vmul.w        vr4,      vr4,      vr8
-    vmul.w        vr5,      vr5,      vr8
-    vsrari.w      vr4,      vr4,      12
-    vsrari.w      vr5,      vr5,      12
-    vmul.w        vr13,     vr4,      vr8
-    vmul.w        vr14,     vr4,      vr8
-    vmadd.w       vr13,     vr5,      vr8  // t0 4 5 6 7
-    vmsub.w       vr14,     vr5,      vr8  // t1 4 5 6 7
-    vssrarni.h.w  vr9,      vr6,      12   // t3
-    vssrarni.h.w  vr10,     vr7,      12   // t2
-    vssrarni.h.w  vr14,     vr12,     12   // t1
-    vssrarni.h.w  vr13,     vr11,     12   // t0
-    vsadd.h       vr4,      vr13,     vr9  // c[0] 0 4  8 12 16 20 24 28
-    vsadd.h       vr5,      vr14,     vr10 // c[1] 1 5  9 13 17 21 25 29
-    vssub.h       vr20,     vr14,     vr10 // c[2] 2 6 10 14 18 22 26 30
-    vssub.h       vr21,     vr13,     vr9  // c[3] 3 7 11 15 19 23 27 31
-.endm
+.ifc \txfm1, identity_
+    LSX_TRANSPOSE8x8_H vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                       vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, \
+                       vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+.else
+    move          t6,       ra
+    jirl          ra,       t7,       0
+    move          ra,       t6
 
-    DCT4_4Wx8H_1D_LSX
-
-    vreplgr2vr.h  vr22,     zero
-    vst           vr22,     a2,       0
-    vst           vr22,     a2,       16
-    vst           vr22,     a2,       32
-    vst           vr22,     a2,       48
-
-    vilvl.h       vr0,      vr5,      vr4   // 0 1 4 5  8  9 12 13
-    vilvl.h       vr1,      vr21,     vr20  // 2 3 6 7 10 11 14 15
-    vilvh.h       vr6,      vr5,      vr4   // 16 17 20 21 24 25 28 29
-    vilvh.h       vr7,      vr21,     vr20  // 18 19 22 23 26 27 30 31
-    vilvl.w       vr9,      vr1,      vr0   //  0  1  2  3  4  5  6  7  in0
-    vilvh.w       vr10,     vr1,      vr0   //  8  9 10 11 12 13 14 15  in1
-    vilvl.w       vr11,     vr7,      vr6   // 16 17 18 19 20 21 22 23  in2
-    vilvh.w       vr12,     vr7,      vr6   // 24 25 26 27 28 29 30 31  in3
-
-    vilvl.d       vr0,      vr10,     vr9
-    vilvl.d       vr1,      vr12,     vr11
-    vilvh.d       vr20,     vr9,      vr11  // in5 in1
-    vilvh.d       vr21,     vr12,     vr10  // in3 in7
-
-.macro DCT8_4Wx8H_1D_LSX
-    dct_4x4_core_lsx vr0, vr1, vr0, vr1, vr3, vr2, vr8, vr8, vr13, vr14
-
-    vldrepl.w     vr17,     t0,       16    // 799
-    vldrepl.w     vr18,     t0,       20    // 4017
-    vldrepl.w     vr11,     t0,       24    // 3406
-    vldrepl.w     vr12,     t0,       28    // 2276
-
-    vexth.w.h     vr4,      vr20
-    vexth.w.h     vr5,      vr21
-    vmul.w        vr6,      vr4,      vr18  // in1 * 4017
-    vmul.w        vr7,      vr4,      vr17  // in1 * 799
-    vmadd.w       vr6,      vr5,      vr17  // in7 * 799
-    vmsub.w       vr7,      vr5,      vr18  // in7 * 4017
-    vsllwil.w.h   vr4,      vr20,     0
-    vsllwil.w.h   vr5,      vr21,     0
-    vmul.w        vr9,      vr4,      vr12
-    vmul.w        vr10,     vr4,      vr11
-    vmadd.w       vr9,      vr5,      vr11
-    vmsub.w       vr10,     vr5,      vr12
-    vssrarni.h.w  vr10,     vr9,      12    // t6a t5a
-    vssrarni.h.w  vr7,      vr6,      12    // t7a t4a
-    vsadd.h       vr15,     vr7,      vr10  // t7  t4
-    vssub.h       vr16,     vr7,      vr10  // t6a t5a
-
-    vexth.w.h     vr4,      vr16            // t5a
-    vsllwil.w.h   vr5,      vr16,     0     // t6a
-    vldi          vr2,      0x8b5           // 181
-    vsub.w        vr6,      vr5,      vr4
-    vadd.w        vr7,      vr5,      vr4
-    vmul.w        vr6,      vr6,      vr2
-    vmul.w        vr7,      vr7,      vr2
-    vssrarni.h.w  vr7,      vr6,      8     // t5 t6
-    vaddi.hu      vr18,     vr7,      0
-    vshuf4i.d     vr7,      vr15,     0x06  // t7 t6
-    vshuf4i.d     vr15,     vr18,     0x09  // t4 t5
-
-    // vr17 -> vr7 vr18 -> vr15
-    vsadd.h       vr4,      vr13,     vr7
-    vsadd.h       vr5,      vr14,     vr15
-    vssub.h       vr6,      vr14,     vr15
-    vssub.h       vr7,      vr13,     vr7
-.endm
+    vsrari_h_x8 vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, 1
 
-    DCT8_4Wx8H_1D_LSX
+    LSX_TRANSPOSE8x8_H vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, \
+                       vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, \
+                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+.endif
 
-    vshuf4i.d     vr5,      vr5,      0x01
-    vshuf4i.d     vr7,      vr7,      0x01
+816:
+    ble       t5,    a3,  816816f
+.irp i, vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+    vxor.v    \i,  \i,  \i
+.endr
 
-    vsrari.h      vr4,      vr4,      4
-    vsrari.h      vr5,      vr5,      4
-    vsrari.h      vr6,      vr6,      4
-    vsrari.h      vr7,      vr7,      4
+816816:
+    vld_x8 a2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    vxor.v        vr23,     vr23,     vr23
+.irp i, 0, 32, 64, 96, 128, 160, 192, 224
+    vst           vr23,     a2,       \i
+.endr
 
-    alsl.d        t2,       a1,       a0,    1
+    li.w          t0,       2896
+    vreplgr2vr.w  vr23,     t0
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    rect2_lsx     \i,       vr23,     \i
+.endr
 
-    VLD_DST_ADD_W4 vr4, vr5
+.ifc \txfm1, identity_
 
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       t2,    2
+.else
+    move          t6,       ra
+    jirl          ra,       t7,       0
+    move          ra,       t6
 
-    VLD_DST_ADD_W4 vr6, vr7
-.DCT_DCT_4x8_END:
-endfunc
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    vsrari.h      \i,       \i,       1
+.endr
+.endif
 
-.macro rect2_w4_lsx in0, in1, in2, out0, out1
-    vsllwil.w.h   vr22,     \in0,     0
-    vexth.w.h     vr23,     \in1
-    vmul.w        vr22,     vr22,     \in2
-    vmul.w        vr23,     vr23,     \in2
-    vsrari.w      \out0,    vr22,     12
-    vsrari.w      \out1,    vr23,     12
-.endm
+    LSX_TRANSPOSE8x8_H vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                       vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
 
-.macro dct_8x4_core_lsx1 out0, out1, out2, out3
-    // dct4 stride=1<<1
-    vmul.w        vr0,      vr6,      vr21
-    vmul.w        vr1,      vr6,      vr20
-    vmadd.w       vr0,      vr10,     vr20  // t3
-    vmsub.w       vr1,      vr10,     vr21  // t2
-    vmul.w        vr2,      vr18,     vr22
-    vmul.w        vr3,      vr18,     vr22
-    vmadd.w       vr2,      vr8,      vr22  // t0
-    vmsub.w       vr3,      vr8,      vr22  // t1
-    vssrarni.h.w  vr1,      vr0,      12    // t3 t2
-    vssrarni.h.w  vr3,      vr2,      12    // t0 t1
-    vsadd.h       vr8,      vr3,      vr1   // t0 t1
-    vssub.h       vr10,     vr3,      vr1   // t3 t2
-
-    vldrepl.w     vr20,     t0,       16    // 799
-    vldrepl.w     vr21,     t0,       20    // 4017
-    vldrepl.w     vr22,     t0,       24    // 3406
-    vldrepl.w     vr23,     t0,       28    // 2276
-
-    vmul.w        vr0,      vr19,     vr21  // in1 * 4017
-    vmul.w        vr1,      vr19,     vr20  // in1 * 799
-    vmadd.w       vr0,      vr11,     vr20  // in7 * 799   // t7a
-    vmsub.w       vr1,      vr11,     vr21  // in7 * 4017  // t4a
-    vmul.w        vr2,      vr9,      vr23  // in5 * 1138
-    vmul.w        vr3,      vr9,      vr22  // in5 * 1703
-    vmadd.w       vr2,      vr7,      vr22  // in3 * 1703  // t6a
-    vmsub.w       vr3,      vr7,      vr23  // in3 * 1138  // t5a
-    vssrarni.h.w  vr0,      vr1,      12    // t4a t7a
-    vssrarni.h.w  vr2,      vr3,      12    // t5a t6a
-    vsadd.h       vr9,      vr0,      vr2   // t4  t7
-    vssub.h       vr11,     vr0,      vr2   // t5a t6a
-
-    vldrepl.w     vr22,     t0,       0     // 2896
-    vexth.w.h     vr18,     vr11            // t6a
-    vsllwil.w.h   vr19,     vr11,     0     // t5a
-    vmul.w        vr6,      vr18,     vr22
-    vmul.w        vr7,      vr18,     vr22
-    vmadd.w       vr6,      vr19,     vr22  // t6
-    vmsub.w       vr7,      vr19,     vr22  // t5
-    vssrarni.h.w  vr6,      vr7,      12    // t5 t6
-
-    vilvh.d       vr11,     vr6,      vr9   // t7 t6
-    vilvl.d       vr9,      vr6,      vr9   // t4 t5
-
-    vsadd.h       \out0,    vr8,      vr11  // c[0] c[1]
-    vsadd.h       \out1,    vr10,     vr9   // c[3] c[2]
-    vssub.h       \out2,    vr10,     vr9   // c[4] c[5]
-    vssub.h       \out3,    vr8,      vr11  // c[7] c[6]
-.endm
+    move          t6,       ra
+    jirl          ra,       t8,       0
+    move          ra,       t6
 
-.macro dct_8x4_core_lsx2 in0, in1, in2, in3, in4, in5, in6, in7, \
-                         out0, out1, out2, out3
-    vexth.w.h     vr4,      \in0            // in1
-    vexth.w.h     vr5,      \in1            // in3
-    vmul.w        vr6,      vr4,      \in4
-    vmul.w        vr7,      vr4,      \in5
-    vmadd.w       vr6,      vr5,      \in5  // t3
-    vmsub.w       vr7,      vr5,      \in4  // t2
-    vexth.w.h     vr4,      \in2            // in1
-    vexth.w.h     vr5,      \in3            // in3
-    vmul.w        vr8,      vr4,      \in4
-    vmul.w        vr9,      vr4,      \in5
-    vmadd.w       vr8,      vr5,      \in5  // t3
-    vmsub.w       vr9,      vr5,      \in4  // t2
-    vssrarni.h.w  vr8,      vr6,      12    // t3
-    vssrarni.h.w  vr9,      vr7,      12    // t2
-
-    vsllwil.w.h   vr4,      \in0,     0
-    vsllwil.w.h   vr5,      \in1,     0
-    vmul.w        vr11,     vr4,      \in6
-    vmul.w        vr12,     vr4,      \in7
-    vmadd.w       vr11,     vr5,      \in7  // t0
-    vmsub.w       vr12,     vr5,      \in6  // t1
-    vsllwil.w.h   vr4,      \in2,     0
-    vsllwil.w.h   vr5,      \in3,     0
-    vmul.w        vr13,     vr4,      \in6
-    vmul.w        vr14,     vr4,      \in7
-    vmadd.w       vr13,     vr5,      \in7  // t0
-    vmsub.w       vr14,     vr5,      \in6  // t1
-    vssrarni.h.w  vr13,     vr11,     12    // t0
-    vssrarni.h.w  vr14,     vr12,     12    // t1
-
-    vsadd.h       \out0,    vr13,     vr8
-    vsadd.h       \out1,    vr14,     vr9
-    vssub.h       \out2,    vr14,     vr9
-    vssub.h       \out3,    vr13,     vr8
-.endm
+    vor.v   vr0, vr0, vr0
+    vsrari_h_x8 vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, \
+                vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23, 4
+    vsrari_h_x8 vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, 4
 
-.macro DST_ADD_W8 in0, in1, in2, in3, in4, in5, in6, in7
-    vsllwil.hu.bu vr10,     \in0,     0
-    vsllwil.hu.bu vr11,     \in1,     0
-    vsllwil.hu.bu vr12,     \in2,     0
-    vsllwil.hu.bu vr13,     \in3,     0
-    vadd.h        vr10,     \in4,     vr10
-    vadd.h        vr11,     \in5,     vr11
-    vadd.h        vr12,     \in6,     vr12
-    vadd.h        vr13,     \in7,     vr13
-    vssrani.bu.h  vr11,     vr10,     0
-    vssrani.bu.h  vr13,     vr12,     0
-    vstelm.d      vr11,     a0,       0,    0
-    add.d         t8,       a0,       a1
-    vstelm.d      vr11,     t8,       0,    1
-    vstelm.d      vr13,     t2,       0,    0
-    add.d         t8,       t2,       a1
-    vstelm.d      vr13,     t8,       0,    1
+    VLD_DST_ADD_W8_x4 vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, \
+                      vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+endfuncl
 .endm
 
-.macro VLD_DST_ADD_W8 in0, in1, in2, in3
-    vld           vr0,      a0,       0
-    vldx          vr1,      a0,       a1
-    vld           vr2,      t2,       0
-    vldx          vr3,      t2,       a1
-
-    DST_ADD_W8 vr0, vr1, vr2, vr3, \in0, \in1, \in2, \in3
-.endm
+def_base_8x16 identity_
+def_base_8x16
 
-function inv_txfm_add_dct_dct_8x4_8bpc_lsx
-    bnez          a3,       .NO_HAS_DCONLY_8x4
-
-    ld.h          t2,       a2,       0      // dc
-    vldi          vr0,      0x8b5            // 181
-    vreplgr2vr.w  vr1,      t2
-    vldi          vr5,      0x880            // 128
-    vmul.w        vr2,      vr0,      vr1
-    st.h          zero,     a2,       0
-    vsrari.w      vr2,      vr2,      8
-    vld           vr10,     a0,       0
-    vmul.w        vr2,      vr2,      vr0
-    vldx          vr11,     a0,       a1
-    vsrari.w      vr2,      vr2,      8
-    alsl.d        t2,       a1,       a0,    1
-    vmadd.w       vr5,      vr2,      vr0
-    vld           vr12,     t2,       0
-    vssrarni.h.w  vr5,      vr5,      12
-    vldx          vr13,     t2,       a1
-
-    DST_ADD_W8 vr10, vr11, vr12, vr13, vr5, vr5, vr5, vr5
-
-    b             .DCT_DCT_8X4_END
-
-.NO_HAS_DCONLY_8x4:
-    la.local      t0,       idct_coeffs
-
-    vld           vr0,      a2,       0
-    vld           vr1,      a2,       16
-    vld           vr2,      a2,       32
-    vld           vr3,      a2,       48
-
-    vldrepl.w     vr20,     t0,       0     // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11
-
-    vldrepl.w     vr20,     t0,       8     // 1567
-    vldrepl.w     vr21,     t0,       12    // 3784
-    vldrepl.w     vr22,     t0,       0     // 2896
-
-    dct_8x4_core_lsx1 vr0, vr1, vr2, vr3
-
-    vshuf4i.d     vr1,      vr1,      0x01
-    vshuf4i.d     vr3,      vr3,      0x01
-
-    vilvl.h       vr4,      vr1,      vr0   // 0 2 4 6 8 10 12 14
-    vilvh.h       vr5,      vr1,      vr0   // 1 3 5 7 9 11 13 15
-    vilvl.h       vr0,      vr5,      vr4   // 0 1  2  3  4  5  6  7 in0
-    vilvh.h       vr1,      vr5,      vr4   // 8 9 10 11 12 13 14 15 in1
-    vilvl.h       vr4,      vr3,      vr2   // 0 2 4 6 8 10 12 14
-    vilvh.h       vr5,      vr3,      vr2   // 1 3 5 7 9 11 13 15
-    vilvl.h       vr2,      vr5,      vr4   // 16 - 23  in2
-    vilvh.h       vr3,      vr5,      vr4   // 24 - 31  in3
-
-    la.local      t0,       idct_coeffs
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    vldrepl.w     vr20,     t0,       8     // 1567
-    vldrepl.w     vr21,     t0,       12    // 3784
-
-    dct_8x4_core_lsx2 vr0, vr1, vr2, vr3, vr21, vr20, vr22, \
-                      vr22, vr15, vr16, vr17, vr18
-
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-    vsrari.h      vr18,     vr18,     4
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr15, vr16, vr17, vr18
-
-.DCT_DCT_8X4_END:
-endfunc
-
-.macro identity8_lsx in0, in1, in2, in3, in4, in5, in6, in7, \
-                     out0, out1, out2, out3
-    vssrarni.h.w  \in1,     \in0,     0
-    vssrarni.h.w  \in3,     \in2,     0
-    vssrarni.h.w  \in5,     \in4,     0
-    vssrarni.h.w  \in7,     \in6,     0
-    vsadd.h       \out0,    \in1,     \in1
-    vsadd.h       \out1,    \in3,     \in3
-    vsadd.h       \out2,    \in5,     \in5
-    vsadd.h       \out3,    \in7,     \in7
-.endm
-
-function inv_txfm_add_identity_identity_8x4_8bpc_lsx
-    la.local      t0,       idct_coeffs
-
-    vld           vr0,      a2,       0    //  0  1  2  3  4  5  6  7  in0
-    vld           vr1,      a2,       16   //  8  9 10 11 12 13 14 15  in1
-    vld           vr2,      a2,       32   // 16 17 18 19 20 21 22 23  in2
-    vld           vr3,      a2,       48   // 24 25 26 27 28 29 30 31  in3
-
-    vldrepl.w     vr20,     t0,       0    // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11
-
-    identity8_lsx vr18, vr19, vr6, vr7, vr8, vr9, vr10, vr11, \
-                  vr19, vr7, vr9, vr11
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    li.w          t0,       1697
-    vreplgr2vr.w  vr20,     t0
-    identity_4x4_lsx vr19, vr19, vr20, vr19, vr19
-    identity_4x4_lsx vr7, vr7, vr20, vr7, vr7
-    identity_4x4_lsx vr9, vr9, vr20, vr9, vr9
-    identity_4x4_lsx vr11, vr11, vr20, vr11, vr11
-
-    vsrari.h      vr15,     vr19,     4
-    vsrari.h      vr16,     vr7,      4
-    vsrari.h      vr17,     vr9,      4
-    vsrari.h      vr18,     vr11,     4
-
-    vilvl.h       vr4,      vr16,     vr15
-    vilvh.h       vr5,      vr16,     vr15
-    vilvl.h       vr11,     vr5,      vr4
-    vilvh.h       vr12,     vr5,      vr4
-    vilvl.h       vr4,      vr18,     vr17
-    vilvh.h       vr5,      vr18,     vr17
-    vilvl.h       vr13,     vr5,      vr4
-    vilvh.h       vr14,     vr5,      vr4
-    vilvl.d       vr15,     vr13,     vr11
-    vilvh.d       vr16,     vr13,     vr11
-    vilvl.d       vr17,     vr14,     vr12
-    vilvh.d       vr18,     vr14,     vr12
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr15, vr16, vr17, vr18
-endfunc
-
-const iadst8_coeffs, align=4
-    .word          4076, 401, 3612, 1931
-    .word          2598, 3166, 1189, 3920
-    // idct_coeffs
-    .word          2896, 0, 1567, 3784, 0, 0, 0, 0
-endconst
-
-.macro vmadd_vmsub_vssrarni_hw_12 in0, in1, in2, in3, in4, in5, in6, in7, \
-                                  in8, in9, in10, in11, out0, out1, out2, out3
-    vmul.w        \out0,    \in0,     \in4
-    vmul.w        \out1,    \in0,     \in5
-    vmadd.w       \out0,    \in1,     \in6   // t0a
-    vmsub.w       \out1,    \in1,     \in7   // t1a
-    vmul.w        \out2,    \in2,     \in8
-    vmul.w        \out3,    \in2,     \in9
-    vmadd.w       \out2,    \in3,     \in10  // t2a
-    vmsub.w       \out3,    \in3,     \in11  // t3a
-    vssrarni.h.w  \out1,    \out0,    12     // t0a t1a
-    vssrarni.h.w  \out3,    \out2,    12     // t2a t3a
-.endm
-
-.macro adst8x4_1d_lsx
-    la.local      t0,       iadst8_coeffs
-
-    vldrepl.w     vr20,     t0,       0     // 4076
-    vldrepl.w     vr21,     t0,       4     // 401
-    vldrepl.w     vr22,     t0,       8     // 3612
-    vldrepl.w     vr23,     t0,       12    // 1931
-
-    // vr13 t0a t1a    vr15 t2a t3a
-    vmadd_vmsub_vssrarni_hw_12 vr11, vr18, vr9, vr6, vr20, vr21, vr21, vr20, \
-                               vr22, vr23, vr23, vr22, vr12, vr13, vr14, vr15
-    vldrepl.w     vr20,     t0,       16    // 2598
-    vldrepl.w     vr21,     t0,       20    // 3166
-    vldrepl.w     vr22,     t0,       24    // 1189
-    vldrepl.w     vr23,     t0,       28    // 3920
-
-    // vr18 t4a t5a     vr6 t6a t7a
-    vmadd_vmsub_vssrarni_hw_12 vr7, vr8, vr19, vr10, vr20, vr21, vr21, vr20, \
-                               vr22, vr23, vr23, vr22, vr11, vr18, vr9, vr6
-
-    vsadd.h       vr12,     vr13,     vr18  // t0 t1
-    vsadd.h       vr14,     vr15,     vr6   // t2 t3
-    vssub.h       vr16,     vr13,     vr18  // t4 t5
-    vssub.h       vr18,     vr15,     vr6   // t6 t7
-
-    la.local      t0,       idct_coeffs
-
-    vldrepl.w     vr20,     t0,       8     // 1567
-    vldrepl.w     vr21,     t0,       12    // 3784
-    vldrepl.w     vr22,     t0,       0     // 2896
-
-    vsllwil.w.h   vr7,      vr16,     0     // t4
-    vexth.w.h     vr8,      vr16            // t5
-    vsllwil.w.h   vr10,     vr18,     0     // t6
-    vexth.w.h     vr11,     vr18            // t7
-
-    // vr13 out0 out7   vr17 out1 out6
-    vmadd_vmsub_vssrarni_hw_12 vr7, vr8, vr11, vr10, vr21, vr20, vr20, vr21, \
-                               vr20, vr21, vr21, vr20, vr13, vr15, vr17, vr19
-    vshuf4i.d     vr19,     vr19,     0x01
-
-    vsadd.h       vr13,     vr12,     vr14  // out0 out7
-    vssub.h       vr16,     vr12,     vr14  // t2 t3
-    vsadd.h       vr17,     vr15,     vr19  // out1 out6
-    vssub.h       vr18,     vr15,     vr19  // t6 t7
-
-    vexth.w.h     vr20,     vr13            // out7
-    vsllwil.w.h   vr21,     vr17,     0     // out1
-    vneg.w        vr20,     vr20
-    vneg.w        vr21,     vr21
-    vssrarni.h.w  vr21,     vr20,     0     // out7 out1
-    vilvl.d       vr13,     vr21,     vr13  // out0 out7
-    vilvh.d       vr17,     vr17,     vr21  // out1 out6
-
-    vsllwil.w.h   vr7,      vr16,     0     // t2
-    vexth.w.h     vr8,      vr16            // t3
-    vsllwil.w.h   vr10,     vr18,     0     // t6
-    vexth.w.h     vr11,     vr18            // t7
-
-    // vr15 out[3] out[4]    vr18 out[2] out[5]
-    vmadd_vmsub_vssrarni_hw_12 vr7, vr8, vr10, vr11, vr22, vr22, vr22, vr22, \
-                               vr22, vr22, vr22, vr22, vr14, vr15, vr19, vr18
-
-    vexth.w.h     vr20,     vr18            // out5
-    vsllwil.w.h   vr21,     vr15,     0     // out3
-    vneg.w        vr20,     vr20
-    vneg.w        vr21,     vr21
-    vssrarni.h.w  vr21,     vr20,     0     // out5 out3
-    vilvl.d       vr18,     vr21,     vr18  // out2 out5
-    vilvh.d       vr15,     vr15,     vr21  // out3 out4
-.endm
-
-function inv_txfm_add_adst_dct_8x4_8bpc_lsx
-    vld           vr0,      a2,       0     //  0  1  2  3  4  5  6  7  in0
-    vld           vr1,      a2,       16    //  8  9 10 11 12 13 14 15  in1
-    vld           vr2,      a2,       32    // 16 17 18 19 20 21 22 23  in2
-    vld           vr3,      a2,       48    // 24 25 26 27 28 29 30 31  in3
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       0     // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11
-
-    adst8x4_1d_lsx
-
-    vilvl.h       vr4,      vr17,     vr13
-    vilvl.h       vr5,      vr15,     vr18
-    vilvl.w       vr0,      vr5,      vr4
-    vilvh.w       vr1,      vr5,      vr4
-    vilvh.h       vr4,      vr18,     vr15
-    vilvh.h       vr5,      vr13,     vr17
-    vilvl.w       vr2,      vr5,      vr4
-    vilvh.w       vr3,      vr5,      vr4
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    la.local      t0,       idct_coeffs
-
-    vldrepl.w     vr20,     t0,       8     // 1567
-    vldrepl.w     vr21,     t0,       12    // 3784
-    vldrepl.w     vr22,     t0,       0     // 2896
-
-    dct_8x4_core_lsx2 vr0, vr1, vr2, vr3, vr21, vr20, vr22, \
-                      vr22, vr15, vr16, vr17, vr18
-
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-    vsrari.h      vr18,     vr18,     4
-
-    alsl.d        t2,       a1,       a0,    1
-
-    VLD_DST_ADD_W8 vr15, vr16, vr17, vr18
-endfunc
-
-function inv_txfm_add_dct_adst_8x4_8bpc_lsx
-    vld           vr0,      a2,       0     //  0  1  2  3  4  5  6  7  in0
-    vld           vr1,      a2,       16    //  8  9 10 11 12 13 14 15  in1
-    vld           vr2,      a2,       32    // 16 17 18 19 20 21 22 23  in2
-    vld           vr3,      a2,       48    // 24 25 26 27 28 29 30 31  in3
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       0     // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11
-
-    vldrepl.w     vr20,      t0,       8    // 1567
-    vldrepl.w     vr21,      t0,       12   // 3784
-    vldrepl.w     vr22,      t0,       0    // 2896
-
-    dct_8x4_core_lsx1 vr0, vr1, vr2, vr3
-
-    vshuf4i.d     vr1,      vr1,      0x01
-    vshuf4i.d     vr3,      vr3,      0x01
-
-    vilvl.h       vr4,      vr1,      vr0
-    vilvh.h       vr5,      vr1,      vr0
-    vilvl.h       vr0,      vr5,      vr4
-    vilvh.h       vr1,      vr5,      vr4
-    vilvl.h       vr4,      vr3,      vr2
-    vilvh.h       vr5,      vr3,      vr2
-    vilvl.h       vr2,      vr5,      vr4
-    vilvh.h       vr3,      vr5,      vr4
-
-    la.local      t0,       iadst4_coeffs
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
-
-    vsllwil.w.h   vr10,     vr0,      0
-    vexth.w.h     vr11,     vr0
-    vsllwil.w.h   vr12,     vr1,      0
-    vexth.w.h     vr13,     vr1
-
-    adst4x4_1d_lsx vr10, vr11, vr12, vr13, vr10, vr11, vr12, vr13
-
-    vsllwil.w.h   vr14,     vr2,      0
-    vexth.w.h     vr15,     vr2
-    vsllwil.w.h   vr16,     vr3,      0
-    vexth.w.h     vr17,     vr3
-
-    adst4x4_1d_lsx vr14, vr15, vr16, vr17, vr14, vr15, vr16, vr17
-
-    vssrarni.h.w  vr14,     vr10,     12
-    vssrarni.h.w  vr15,     vr11,     12
-    vssrarni.h.w  vr16,     vr12,     12
-    vssrarni.h.w  vr17,     vr13,     12
-
-    vsrari.h      vr14,     vr14,     4
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr14, vr15, vr16, vr17
-endfunc
-
-function inv_txfm_add_adst_adst_8x4_8bpc_lsx
-    vld           vr0,      a2,       0     //  0  1  2  3  4  5  6  7  in0
-    vld           vr1,      a2,       16    //  8  9 10 11 12 13 14 15  in1
-    vld           vr2,      a2,       32    // 16 17 18 19 20 21 22 23  in2
-    vld           vr3,      a2,       48    // 24 25 26 27 28 29 30 31  in3
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       0     // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11
-
-    adst8x4_1d_lsx
-
-    vilvl.h       vr4,      vr17,     vr13
-    vilvl.h       vr5,      vr15,     vr18
-    vilvl.w       vr0,      vr5,      vr4
-    vilvh.w       vr1,      vr5,      vr4
-    vilvh.h       vr4,      vr18,     vr15
-    vilvh.h       vr5,      vr13,     vr17
-    vilvl.w       vr2,      vr5,      vr4
-    vilvh.w       vr3,      vr5,      vr4
-
-    la.local      t0,       iadst4_coeffs
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
-
-    vsllwil.w.h   vr10,     vr0,      0
-    vexth.w.h     vr11,     vr0
-    vsllwil.w.h   vr12,     vr1,      0
-    vexth.w.h     vr13,     vr1
-
-    adst4x4_1d_lsx vr10, vr11, vr12, vr13, vr10, vr11, vr12, vr13
-
-    vsllwil.w.h   vr14,     vr2,      0
-    vexth.w.h     vr15,     vr2
-    vsllwil.w.h   vr16,     vr3,      0
-    vexth.w.h     vr17,     vr3
-
-    adst4x4_1d_lsx vr14, vr15, vr16, vr17, vr14, vr15, vr16, vr17
-
-    vssrarni.h.w  vr14,     vr10,     12
-    vssrarni.h.w  vr15,     vr11,     12
-    vssrarni.h.w  vr16,     vr12,     12
-    vssrarni.h.w  vr17,     vr13,     12
-
-    vsrari.h      vr14,     vr14,     4
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr14, vr15, vr16, vr17
-endfunc
-
-function inv_txfm_add_flipadst_adst_8x4_8bpc_lsx
-    vld           vr0,      a2,       0    //  0  1  2  3  4  5  6  7  in0
-    vld           vr1,      a2,       16   //  8  9 10 11 12 13 14 15  in1
-    vld           vr2,      a2,       32   // 16 17 18 19 20 21 22 23  in2
-    vld           vr3,      a2,       48   // 24 25 26 27 28 29 30 31  in3
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       0    // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11
-
-    adst8x4_1d_lsx
-
-    vilvl.h       vr20,     vr15,     vr13
-    vilvl.h       vr21,     vr18,     vr17
-    vilvl.w       vr0,      vr21,     vr20
-    vilvh.w       vr1,      vr21,     vr20
-    vilvh.h       vr20,     vr15,     vr13
-    vilvh.h       vr21,     vr18,     vr17
-    vilvl.w       vr2,      vr21,     vr20
-    vilvh.w       vr3,      vr21,     vr20
-    vshuf4i.h     vr0,      vr0,      0x2d
-    vshuf4i.h     vr1,      vr1,      0x2d
-    vshuf4i.h     vr2,      vr2,      0x78
-    vshuf4i.h     vr3,      vr3,      0x78
-
-    la.local      t0,       iadst4_coeffs
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
-
-    vsllwil.w.h   vr10,     vr2,      0
-    vexth.w.h     vr11,     vr2
-    vsllwil.w.h   vr12,     vr3,      0
-    vexth.w.h     vr13,     vr3
-
-    adst4x4_1d_lsx vr10, vr11, vr12, vr13, vr10, vr11, vr12, vr13
-
-    vsllwil.w.h   vr14,     vr0,      0
-    vexth.w.h     vr15,     vr0
-    vsllwil.w.h   vr16,     vr1,      0
-    vexth.w.h     vr17,     vr1
-
-    adst4x4_1d_lsx vr14, vr15, vr16, vr17, vr14, vr15, vr16, vr17
-
-    vssrarni.h.w  vr14,     vr10,     12
-    vssrarni.h.w  vr15,     vr11,     12
-    vssrarni.h.w  vr16,     vr12,     12
-    vssrarni.h.w  vr17,     vr13,     12
-
-    vsrari.h      vr14,     vr14,     4
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr14, vr15, vr16, vr17
-endfunc
-
-function inv_txfm_add_adst_flipadst_8x4_8bpc_lsx
-    vld           vr0,      a2,       0      // in0
-    vld           vr1,      a2,       16     // in1
-    vld           vr2,      a2,       32     // in2
-    vld           vr3,      a2,       48     // in3
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       0      // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19  // 0  8 16 24 1  9 17 25 in0 in1
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7    // 2 10 18 26 3 11 19 27 in2 in3
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9    // 4 12 20 28 5 13 21 29 in4 in5
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11  // 6 14 22 30 7 15 23 31 in6 in7
-
-    adst8x4_1d_lsx
-
-    vilvl.h       vr4,      vr17,     vr13
-    vilvl.h       vr5,      vr15,     vr18
-    vilvl.w       vr0,      vr5,      vr4
-    vilvh.w       vr1,      vr5,      vr4
-    vilvh.h       vr4,      vr18,     vr15
-    vilvh.h       vr5,      vr13,     vr17
-    vilvl.w       vr2,      vr5,      vr4
-    vilvh.w       vr3,      vr5,      vr4
-
-    la.local      t0,       iadst4_coeffs
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
-
-    vsllwil.w.h   vr10,     vr0,      0
-    vexth.w.h     vr11,     vr0
-    vsllwil.w.h   vr12,     vr1,      0
-    vexth.w.h     vr13,     vr1
-
-    adst4x4_1d_lsx vr10, vr11, vr12, vr13, vr10, vr11, vr12, vr13
-
-    vsllwil.w.h   vr14,     vr2,      0
-    vexth.w.h     vr15,     vr2
-    vsllwil.w.h   vr16,     vr3,      0
-    vexth.w.h     vr17,     vr3
-
-    adst4x4_1d_lsx vr14, vr15, vr16, vr17, vr14, vr15, vr16, vr17
-
-    vssrarni.h.w  vr14,     vr10,     12
-    vssrarni.h.w  vr15,     vr11,     12
-    vssrarni.h.w  vr16,     vr12,     12
-    vssrarni.h.w  vr17,     vr13,     12
-
-    vsrari.h      vr14,     vr14,     4
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr17, vr16, vr15, vr14
-endfunc
-
-function inv_txfm_add_flipadst_dct_8x4_8bpc_lsx
-    vld           vr0,      a2,       0      // in0
-    vld           vr1,      a2,       16     // in1
-    vld           vr2,      a2,       32     // in2
-    vld           vr3,      a2,       48     // in3
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       0      // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19  // 0  8 16 24 1  9 17 25 in0 in1
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7    // 2 10 18 26 3 11 19 27 in2 in3
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9    // 4 12 20 28 5 13 21 29 in4 in5
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11  // 6 14 22 30 7 15 23 31 in6 in7
-
-    adst8x4_1d_lsx
-
-    vilvl.h       vr20,     vr15,     vr13
-    vilvl.h       vr21,     vr18,     vr17
-    vilvl.w       vr0,      vr21,     vr20
-    vilvh.w       vr1,      vr21,     vr20
-    vilvh.h       vr20,     vr15,     vr13
-    vilvh.h       vr21,     vr18,     vr17
-    vilvl.w       vr2,      vr21,     vr20
-    vilvh.w       vr3,      vr21,     vr20
-    vshuf4i.h     vr0,      vr0,      0x2d
-    vshuf4i.h     vr1,      vr1,      0x2d
-    vshuf4i.h     vr2,      vr2,      0x78
-    vshuf4i.h     vr3,      vr3,      0x78
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    la.local      t0,       idct_coeffs
-
-    vldrepl.w     vr20,     t0,       8     // 1567
-    vldrepl.w     vr21,     t0,       12    // 3784
-    vldrepl.w     vr22,     t0,       0     // 2896
-
-    dct_8x4_core_lsx2 vr2, vr3, vr0, vr1, vr21, vr20, vr22, \
-                      vr22, vr15, vr16, vr17, vr18
-
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-    vsrari.h      vr18,     vr18,     4
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr15, vr16, vr17, vr18
-endfunc
-
-function inv_txfm_add_dct_flipadst_8x4_8bpc_lsx
-    la.local      t0,       idct_coeffs
-
-    vld           vr0,      a2,       0     // in0
-    vld           vr1,      a2,       16    // in1
-    vld           vr2,      a2,       32    // in2
-    vld           vr3,      a2,       48    // in3
-
-    vldrepl.w     vr20,     t0,       0     // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19  // in0 0 - 7
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7    // in1 8 - 15
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9    // in2 16 - 23
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11  // in3 24 - 31
-
-    vldrepl.w     vr20,     t0,       8      // 1567
-    vldrepl.w     vr21,     t0,       12     // 3784
-    vldrepl.w     vr22,     t0,       0      // 2896
-
-    dct_8x4_core_lsx1 vr0, vr1, vr2, vr3
-
-    vshuf4i.d     vr1,      vr1,      0x01
-    vshuf4i.d     vr3,      vr3,      0x01
-
-    vilvl.h       vr4,      vr1,      vr0
-    vilvh.h       vr5,      vr1,      vr0
-    vilvl.h       vr0,      vr5,      vr4
-    vilvh.h       vr1,      vr5,      vr4
-    vilvl.h       vr4,      vr3,      vr2
-    vilvh.h       vr5,      vr3,      vr2
-    vilvl.h       vr2,      vr5,      vr4
-    vilvh.h       vr3,      vr5,      vr4
-
-    la.local      t0,       iadst4_coeffs
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
-
-    vsllwil.w.h   vr10,     vr0,      0     // in0
-    vexth.w.h     vr11,     vr0             // in1
-    vsllwil.w.h   vr12,     vr1,      0     // in2
-    vexth.w.h     vr13,     vr1             // in3
-    adst4x4_1d_lsx vr10, vr11, vr12, vr13, vr10, vr11, vr12, vr13
-
-    vsllwil.w.h   vr14,     vr2,      0
-    vexth.w.h     vr15,     vr2
-    vsllwil.w.h   vr16,     vr3,      0
-    vexth.w.h     vr17,     vr3
-    adst4x4_1d_lsx vr14, vr15, vr16, vr17, vr14, vr15, vr16, vr17
-
-    vssrarni.h.w  vr14,     vr10,     12
-    vssrarni.h.w  vr15,     vr11,     12
-    vssrarni.h.w  vr16,     vr12,     12
-    vssrarni.h.w  vr17,     vr13,     12
-    vsrari.h      vr14,     vr14,     4
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr17, vr16, vr15, vr14
-endfunc
-
-function inv_txfm_add_flipadst_flipadst_8x4_8bpc_lsx
-    vld           vr0,      a2,       0      // in0
-    vld           vr1,      a2,       16     // in1
-    vld           vr2,      a2,       32     // in2
-    vld           vr3,      a2,       48     // in3
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       0      // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19  // 0  8 16 24 1  9 17 25 in0 in1
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7    // 2 10 18 26 3 11 19 27 in2 in3
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9    // 4 12 20 28 5 13 21 29 in4 in5
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11  // 6 14 22 30 7 15 23 31 in6 in7
-
-    adst8x4_1d_lsx
-
-    vilvl.h       vr20,     vr15,     vr13
-    vilvl.h       vr21,     vr18,     vr17
-    vilvl.w       vr0,      vr21,     vr20
-    vilvh.w       vr1,      vr21,     vr20
-    vilvh.h       vr20,     vr15,     vr13
-    vilvh.h       vr21,     vr18,     vr17
-    vilvl.w       vr2,      vr21,     vr20
-    vilvh.w       vr3,      vr21,     vr20
-    vshuf4i.h     vr0,      vr0,      0x2d
-    vshuf4i.h     vr1,      vr1,      0x2d
-    vshuf4i.h     vr2,      vr2,      0x78
-    vshuf4i.h     vr3,      vr3,      0x78
-
-    la.local      t0,       iadst4_coeffs
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
-
-    vsllwil.w.h   vr10,     vr2,      0     // in0
-    vexth.w.h     vr11,     vr2             // in1
-    vsllwil.w.h   vr12,     vr3,      0     // in2
-    vexth.w.h     vr13,     vr3             // in3
-    adst4x4_1d_lsx vr10, vr11, vr12, vr13, vr10, vr11, vr12, vr13
-
-    vsllwil.w.h   vr14,     vr0,      0
-    vexth.w.h     vr15,     vr0
-    vsllwil.w.h   vr16,     vr1,      0
-    vexth.w.h     vr17,     vr1
-    adst4x4_1d_lsx vr14, vr15, vr16, vr17, vr14, vr15, vr16, vr17
-
-    vssrarni.h.w  vr14,     vr10,     12
-    vssrarni.h.w  vr15,     vr11,     12
-    vssrarni.h.w  vr16,     vr12,     12
-    vssrarni.h.w  vr17,     vr13,     12
-
-    vsrari.h      vr14,     vr14,     4
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr17, vr16, vr15, vr14
-endfunc
-
-function inv_txfm_add_dct_identity_8x4_8bpc_lsx
-    vld           vr0,      a2,       0     // in0
-    vld           vr1,      a2,       16    // in1
-    vld           vr2,      a2,       32    // in2
-    vld           vr3,      a2,       48    // in3
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       0     // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19 // in0 0 - 7
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7   // in1 8 - 15
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9   // in2 16 - 23
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11 // in3 24 - 31
-
-    vldrepl.w     vr20,     t0,       8     // 1567
-    vldrepl.w     vr21,     t0,       12    // 3784
-    vldrepl.w     vr22,     t0,       0     // 2896
-
-    dct_8x4_core_lsx1 vr0, vr1, vr2, vr3
-
-    vshuf4i.d     vr1,      vr1,      0x01
-    vshuf4i.d     vr3,      vr3,      0x01
-
-    vilvl.h       vr4,      vr1,      vr0
-    vilvh.h       vr5,      vr1,      vr0
-    vilvl.h       vr0,      vr5,      vr4
-    vilvh.h       vr1,      vr5,      vr4
-    vilvl.h       vr4,      vr3,      vr2
-    vilvh.h       vr5,      vr3,      vr2
-    vilvl.h       vr2,      vr5,      vr4
-    vilvh.h       vr3,      vr5,      vr4
-    vilvl.d       vr14,     vr2,      vr0
-    vilvh.d       vr15,     vr2,      vr0
-    vilvl.d       vr16,     vr3,      vr1
-    vilvh.d       vr17,     vr3,      vr1
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    li.w          t0,       1697
-    vreplgr2vr.w  vr20,     t0
-
-    identity_4x4_lsx vr14, vr14, vr20, vr14, vr14
-    identity_4x4_lsx vr15, vr15, vr20, vr15, vr15
-    identity_4x4_lsx vr16, vr16, vr20, vr16, vr16
-    identity_4x4_lsx vr17, vr17, vr20, vr17, vr17
-
-    vsrari.h      vr14,     vr14,     4
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr14, vr15, vr16, vr17
-endfunc
-
-function inv_txfm_add_identity_dct_8x4_8bpc_lsx
-    vld           vr0,      a2,       0     // in0
-    vld           vr1,      a2,       16    // in1
-    vld           vr2,      a2,       32    // in2
-    vld           vr3,      a2,       48    // in3
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       0     // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19 // in0 0 - 7
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7   // in1 8 - 15
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9   // in2 16 - 23
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11 // in3 24 - 31
-
-    identity8_lsx vr18, vr19, vr6, vr7, vr8, vr9, vr10, vr11, \
-                  vr19, vr7, vr9, vr11
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    vilvl.h       vr4,      vr7,      vr19
-    vilvh.h       vr5,      vr7,      vr19
-    vilvl.h       vr0,      vr5,      vr4
-    vilvh.h       vr1,      vr5,      vr4
-    vilvl.h       vr4,      vr11,     vr9
-    vilvh.h       vr5,      vr11,     vr9
-    vilvl.h       vr2,      vr5,      vr4
-    vilvh.h       vr3,      vr5,      vr4
-
-    la.local      t0,       idct_coeffs
-
-    vldrepl.w     vr20,     t0,       8    // 1567
-    vldrepl.w     vr21,     t0,       12   // 3784
-    vldrepl.w     vr22,     t0,       0    // 2896
-
-    dct_8x4_core_lsx2 vr0, vr1, vr2, vr3, vr21, vr20, vr22, \
-                      vr22, vr15, vr16, vr17, vr18
-
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-    vsrari.h      vr18,     vr18,     4
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr15, vr16, vr17, vr18
-endfunc
-
-function inv_txfm_add_flipadst_identity_8x4_8bpc_lsx
-    vld           vr0,      a2,       0      // in0
-    vld           vr1,      a2,       16     // in1
-    vld           vr2,      a2,       32     // in2
-    vld           vr3,      a2,       48     // in3
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       0      // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19  // 0  8 16 24 1  9 17 25 in0 in1
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7    // 2 10 18 26 3 11 19 27 in2 in3
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9    // 4 12 20 28 5 13 21 29 in4 in5
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11  // 6 14 22 30 7 15 23 31 in6 in7
-
-    adst8x4_1d_lsx
-
-    vilvl.h       vr20,     vr15,     vr13
-    vilvl.h       vr21,     vr18,     vr17
-    vilvl.w       vr0,      vr21,     vr20
-    vilvh.w       vr1,      vr21,     vr20
-    vilvh.h       vr20,     vr15,     vr13
-    vilvh.h       vr21,     vr18,     vr17
-    vilvl.w       vr2,      vr21,     vr20
-    vilvh.w       vr3,      vr21,     vr20
-    vshuf4i.h     vr0,      vr0,      0x2d
-    vshuf4i.h     vr1,      vr1,      0x2d
-    vshuf4i.h     vr2,      vr2,      0x78
-    vshuf4i.h     vr3,      vr3,      0x78
-    vilvl.d       vr14,     vr0,      vr2    // in0
-    vilvh.d       vr15,     vr0,      vr2    // in1
-    vilvl.d       vr16,     vr1,      vr3    // in2
-    vilvh.d       vr17,     vr1,      vr3    // in3
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    li.w          t0,       1697
-    vreplgr2vr.w  vr20,     t0
-
-    identity_4x4_lsx vr14, vr14, vr20, vr14, vr14
-    identity_4x4_lsx vr15, vr15, vr20, vr15, vr15
-    identity_4x4_lsx vr16, vr16, vr20, vr16, vr16
-    identity_4x4_lsx vr17, vr17, vr20, vr17, vr17
-
-    vsrari.h      vr14,     vr14,     4
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr14, vr15, vr16, vr17
-endfunc
-
-function inv_txfm_add_identity_flipadst_8x4_8bpc_lsx
-    vld           vr0,      a2,       0     // in0
-    vld           vr1,      a2,       16    // in1
-    vld           vr2,      a2,       32    // in2
-    vld           vr3,      a2,       48    // in3
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       0     // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19 // in0 0 - 7
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7   // in1 8 - 15
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9   // in2 16 - 23
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11 // in3 24 - 31
-
-    identity8_lsx vr18, vr19, vr6, vr7, vr8, vr9, vr10, vr11, \
-                  vr19, vr7, vr9, vr11
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    vilvl.h       vr4,      vr7,      vr19
-    vilvh.h       vr5,      vr7,      vr19
-    vilvl.h       vr0,      vr5,      vr4
-    vilvh.h       vr1,      vr5,      vr4
-    vilvl.h       vr4,      vr11,     vr9
-    vilvh.h       vr5,      vr11,     vr9
-    vilvl.h       vr2,      vr5,      vr4
-    vilvh.h       vr3,      vr5,      vr4
-
-    la.local      t0,       iadst4_coeffs
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
-
-    vsllwil.w.h   vr10,     vr0,      0     // in0
-    vexth.w.h     vr11,     vr0             // in1
-    vsllwil.w.h   vr12,     vr1,      0     // in2
-    vexth.w.h     vr13,     vr1             // in3
-    adst4x4_1d_lsx vr10, vr11, vr12, vr13, vr10, vr11, vr12, vr13
-
-    vsllwil.w.h   vr14,     vr2,      0
-    vexth.w.h     vr15,     vr2
-    vsllwil.w.h   vr16,     vr3,      0
-    vexth.w.h     vr17,     vr3
-    adst4x4_1d_lsx vr14, vr15, vr16, vr17, vr14, vr15, vr16, vr17
-
-    vssrarni.h.w  vr14,     vr10,     12
-    vssrarni.h.w  vr15,     vr11,     12
-    vssrarni.h.w  vr16,     vr12,     12
-    vssrarni.h.w  vr17,     vr13,     12
-
-    vsrari.h      vr14,     vr14,     4
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr17, vr16, vr15, vr14
-endfunc
-
-function inv_txfm_add_adst_identity_8x4_8bpc_lsx
-    vld           vr0,      a2,       0     // in0
-    vld           vr1,      a2,       16    // in1
-    vld           vr2,      a2,       32    // in2
-    vld           vr3,      a2,       48    // in3
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       0     // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19 // 0  8 16 24 1  9 17 25 in0 in1
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7   // 2 10 18 26 3 11 19 27 in2 in3
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9   // 4 12 20 28 5 13 21 29 in4 in5
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11 // 6 14 22 30 7 15 23 31 in6 in7
-
-    adst8x4_1d_lsx
-
-    vilvl.h       vr4,      vr17,     vr13
-    vilvl.h       vr5,      vr15,     vr18
-    vilvl.w       vr14,     vr5,      vr4   // in0 in1
-    vilvh.w       vr16,     vr5,      vr4   // in2 in3
-    vilvh.h       vr4,      vr18,     vr15
-    vilvh.h       vr5,      vr13,     vr17
-    vilvl.w       vr17,     vr5,      vr4
-    vilvh.w       vr18,     vr5,      vr4
-    vilvl.d       vr10,     vr17,     vr14  // in0
-    vilvh.d       vr11,     vr17,     vr14  // in1
-    vilvl.d       vr12,     vr18,     vr16  // in2
-    vilvh.d       vr13,     vr18,     vr16  // in3
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    li.w          t0,       1697
-    vreplgr2vr.w  vr20,     t0
-
-    identity_4x4_lsx vr10, vr10, vr20, vr10, vr15
-    identity_4x4_lsx vr11, vr11, vr20, vr11, vr16
-    identity_4x4_lsx vr12, vr12, vr20, vr12, vr17
-    identity_4x4_lsx vr13, vr13, vr20, vr13, vr18
-
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-    vsrari.h      vr18,     vr18,     4
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr15, vr16, vr17, vr18
-endfunc
-
-function inv_txfm_add_identity_adst_8x4_8bpc_lsx
-    vld           vr0,      a2,       0     // in0
-    vld           vr1,      a2,       16    // in1
-    vld           vr2,      a2,       32    // in2
-    vld           vr3,      a2,       48    // in3
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       0     // 2896
-
-    rect2_w4_lsx vr0, vr0, vr20, vr18, vr19 // in0 0 - 7
-    rect2_w4_lsx vr1, vr1, vr20, vr6, vr7   // in1 8 - 15
-    rect2_w4_lsx vr2, vr2, vr20, vr8, vr9   // in2 16 - 23
-    rect2_w4_lsx vr3, vr3, vr20, vr10, vr11 // in3 24 - 31
-
-    identity8_lsx vr18, vr19, vr6, vr7, vr8, vr9, vr10, vr11, \
-                  vr0, vr1, vr2, vr3
-
-    vilvl.h       vr4,      vr1,      vr0   // 0 2 4 6 8 10 12 14
-    vilvh.h       vr5,      vr1,      vr0   // 1 3 5 7 9 11 13 15
-    vilvl.h       vr0,      vr5,      vr4   // 0 1  2  3  4  5  6  7
-    vilvh.h       vr1,      vr5,      vr4   // 8 9 10 11 12 13 14 15
-    vilvl.h       vr4,      vr3,      vr2   // 0 2 4 6 8 10 12 14
-    vilvh.h       vr5,      vr3,      vr2   // 1 3 5 7 9 11 13 15
-    vilvl.h       vr2,      vr5,      vr4   // 0 1  2  3  4  5  6  7
-    vilvh.h       vr3,      vr5,      vr4   // 8 9 10 11 12 13 14 15
-
-    vreplgr2vr.h  vr23,     zero
-    vst           vr23,     a2,       0
-    vst           vr23,     a2,       16
-    vst           vr23,     a2,       32
-    vst           vr23,     a2,       48
-
-    la.local      t0,       iadst4_coeffs
-
-    vldrepl.w     vr20,     t0,       0     // 1321
-    vldrepl.w     vr21,     t0,       4     // 3803
-    vldrepl.w     vr22,     t0,       8     // 2482
-    vldrepl.w     vr23,     t0,       12    // 3344
-
-    vsllwil.w.h   vr10,     vr0,      0
-    vexth.w.h     vr11,     vr0
-    vsllwil.w.h   vr12,     vr1,      0
-    vexth.w.h     vr13,     vr1
-
-    adst4x4_1d_lsx vr10, vr11, vr12, vr13, vr10, vr11, vr12, vr13
-
-    vsllwil.w.h   vr14,     vr2,      0
-    vexth.w.h     vr15,     vr2
-    vsllwil.w.h   vr16,     vr3,      0
-    vexth.w.h     vr17,     vr3
-
-    adst4x4_1d_lsx vr14, vr15, vr16, vr17, vr14, vr15, vr16, vr17
-
-    vssrarni.h.w  vr14,     vr10,     12
-    vssrarni.h.w  vr15,     vr11,     12
-    vssrarni.h.w  vr16,     vr12,     12
-    vssrarni.h.w  vr17,     vr13,     12
-
-    vsrari.h      vr14,     vr14,     4
-    vsrari.h      vr15,     vr15,     4
-    vsrari.h      vr16,     vr16,     4
-    vsrari.h      vr17,     vr17,     4
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr14, vr15, vr16, vr17
-endfunc
-
-function inv_txfm_add_identity_identity_8x8_8bpc_lsx
-
-    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr14, vr15
-
-    // identity8
-    vsllwil.w.h   vr6,      vr0,      1
-    vsllwil.w.h   vr7,      vr1,      1
-    vsllwil.w.h   vr8,      vr2,      1
-    vsllwil.w.h   vr9,      vr3,      1
-    vsllwil.w.h   vr10,     vr4,      1
-    vsllwil.w.h   vr11,     vr5,      1
-    vsllwil.w.h   vr12,     vr14,     1
-    vsllwil.w.h   vr13,     vr15,     1
-
-.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr14, vr15
-    vexth.w.h     \i,       \i
-.endr
-
-.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr14, vr15
-    vslli.w       \i,       \i,       1
-.endr
-
-    vssrarni.h.w  vr0,      vr6,      1     // in0
-    vssrarni.h.w  vr1,      vr7,      1     // in1
-    vssrarni.h.w  vr2,      vr8,      1     // in2
-    vssrarni.h.w  vr3,      vr9,      1     // in3
-    vssrarni.h.w  vr4,      vr10,     1     // in4
-    vssrarni.h.w  vr5,      vr11,     1     // in5
-    vssrarni.h.w  vr14,     vr12,     1     // in6
-    vssrarni.h.w  vr15,     vr13,     1     // in7
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-    LSX_TRANSPOSE8x8_H vr0, vr1, vr2, vr3, vr4, vr5, vr14, vr15, \
-                       vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23, \
-                       vr6, vr7, vr8, vr9, vr10, vr11, vr12 vr13
-
-    vsllwil.w.h   vr6,      vr16,     1
-    vsllwil.w.h   vr7,      vr17,     1
-    vsllwil.w.h   vr8,      vr18,     1
-    vsllwil.w.h   vr9,      vr19,     1
-    vsllwil.w.h   vr10,     vr20,     1
-    vsllwil.w.h   vr11,     vr21,     1
-    vsllwil.w.h   vr12,     vr22,     1
-    vsllwil.w.h   vr13,     vr23,     1
-
-.irp i, vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
-    vexth.w.h     \i,       \i
-.endr
-
-.irp i, vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
-    vslli.w       \i,       \i,       1
-.endr
-
-    vssrarni.h.w  vr16,     vr6,      4     // in0
-    vssrarni.h.w  vr17,     vr7,      4     // in1
-    vssrarni.h.w  vr18,     vr8,      4     // in2
-    vssrarni.h.w  vr19,     vr9,      4     // in3
-    vssrarni.h.w  vr20,     vr10,     4     // in4
-    vssrarni.h.w  vr21,     vr11,     4     // in5
-    vssrarni.h.w  vr22,     vr12,     4     // in6
-    vssrarni.h.w  vr23,     vr13,     4     // in7
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr16, vr17, vr18, vr19
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr20, vr21, vr22, vr23
-
-endfunc
-
-.macro adst8x8_1d_lsx out0, out1, out2, out3
-    la.local      t0,       iadst8_coeffs
-
-    vldrepl.w     vr20,     t0,       0     // 4076
-    vldrepl.w     vr21,     t0,       4     // 401
-    vldrepl.w     vr22,     t0,       8     // 3612
-    vldrepl.w     vr23,     t0,       12    // 1931
-
-    // vr13 t0a t1a    vr15 t2a t3a
-    vmadd_vmsub_vssrarni_hw_12 vr11, vr18, vr9, vr6, vr20, vr21, vr21, vr20, \
-                               vr22, vr23, vr23, vr22, vr12, vr13, vr14, vr15
-    vldrepl.w     vr20,     t0,       16    // 2598
-    vldrepl.w     vr21,     t0,       20    // 3166
-    vldrepl.w     vr22,     t0,       24    // 1189
-    vldrepl.w     vr23,     t0,       28    // 3920
-
-    // vr18 t4a t5a     vr6 t6a t7a
-    vmadd_vmsub_vssrarni_hw_12 vr7, vr8, vr19, vr10, vr20, vr21, vr21, vr20, \
-                               vr22, vr23, vr23, vr22, vr11, vr18, vr9, vr6
-
-    vsadd.h       vr12,     vr13,     vr18  // t0 t1
-    vsadd.h       vr14,     vr15,     vr6   // t2 t3
-    vssub.h       vr9,      vr13,     vr18  // t4 t5
-    vssub.h       vr18,     vr15,     vr6   // t6 t7
-
-    la.local      t0,       idct_coeffs
-
-    vldrepl.w     vr20,     t0,       8     // 1567
-    vldrepl.w     vr21,     t0,       12    // 3784
-    vldrepl.w     vr22,     t0,       0     // 2896
-
-    vsllwil.w.h   vr7,      vr9,      0     // t4
-    vexth.w.h     vr8,      vr9             // t5
-    vsllwil.w.h   vr10,     vr18,     0     // t6
-    vexth.w.h     vr11,     vr18            // t7
-
-    // vr13 out0 out7   vr17 out1 out6
-    vmadd_vmsub_vssrarni_hw_12 vr7, vr8, vr11, vr10, vr21, vr20, vr20, vr21, \
-                               vr20, vr21, vr21, vr20, vr13, vr15, vr18, vr19
-    vshuf4i.d     vr19,     vr19,     0x01
-
-    vsadd.h       vr13,     vr12,     vr14  // out0 out7
-    vssub.h       vr6,      vr12,     vr14  // t2 t3
-    vsadd.h       vr7,      vr15,     vr19  // out1 out6
-    vssub.h       vr18,     vr15,     vr19  // t6 t7
-
-    vexth.w.h     vr20,     vr13            // out7
-    vsllwil.w.h   vr21,     vr7,      0     // out1
-    vneg.w        vr20,     vr20
-    vneg.w        vr21,     vr21
-    vssrarni.h.w  vr21,     vr20,     0     // out7 out1
-    vilvl.d       \out0,    vr21,     vr13  // out0 out7
-    vilvh.d       \out1,    vr7,      vr21  // out1 out6
-
-    vsllwil.w.h   vr7,      vr6,      0     // t2
-    vexth.w.h     vr8,      vr6             // t3
-    vsllwil.w.h   vr10,     vr18,     0     // t6
-    vexth.w.h     vr11,     vr18            // t7
-
-    // vr15 out[3] out[4]    vr18 out[2] out[5]
-    vmadd_vmsub_vssrarni_hw_12 vr7, vr8, vr10, vr11, vr22, vr22, vr22, vr22, \
-                               vr22, vr22, vr22, vr22, vr14, vr15, vr19, vr18
-
-    vexth.w.h     vr20,     vr18            // out5
-    vsllwil.w.h   vr21,     vr15,     0     // out3
-    vneg.w        vr20,     vr20
-    vneg.w        vr21,     vr21
-    vssrarni.h.w  vr21,     vr20,     0     // out5 out3
-    vilvl.d       \out2,    vr21,     vr18  // out2 out5
-    vilvh.d       \out3,    vr15,     vr21  // out3 out4
-.endm
-
-function inv_txfm_add_adst_dct_8x8_8bpc_lsx
-    addi.d        sp,       sp,       -32
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-
-    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr16, vr17
-
-    vsllwil.w.h   vr18,     vr0,      0
-    vsllwil.w.h   vr19,     vr1,      0
-    vsllwil.w.h   vr6,      vr2,      0
-    vsllwil.w.h   vr7,      vr3,      0
-    vsllwil.w.h   vr8,      vr4,      0
-    vsllwil.w.h   vr9,      vr5,      0
-    vsllwil.w.h   vr10,     vr16,     0
-    vsllwil.w.h   vr11,     vr17,     0
-    adst8x8_1d_lsx vr24, vr25, vr26, vr27
-
-    vexth.w.h     vr18,     vr0
-    vexth.w.h     vr19,     vr1
-    vexth.w.h     vr6,      vr2
-    vexth.w.h     vr7,      vr3
-    vexth.w.h     vr8,      vr4
-    vexth.w.h     vr9,      vr5
-    vexth.w.h     vr10,     vr16
-    vexth.w.h     vr11,     vr17
-    adst8x8_1d_lsx vr0, vr1, vr2, vr3
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-.irp i, vr24, vr25, vr26, vr27, vr0, vr1, vr2, vr3
-    vsrari.h        \i,       \i,     1
-.endr
-
-    LSX_TRANSPOSE8x8_H vr24, vr25, vr26, vr27, vr0, vr1, vr2, vr3, \
-                       vr4, vr5, vr12, vr13, vr14, vr15, vr24, vr25, \
-                       vr6, vr7, vr8, vr9, vr10, vr11, vr16, vr17
-
-    vshuf4i.h     vr14,     vr14,     0x1b
-    vshuf4i.h     vr15,     vr15,     0x1b
-    vshuf4i.h     vr24,     vr24,     0x1b
-    vshuf4i.h     vr25,     vr25,     0x1b
-
-    vsllwil.w.h   vr18,     vr4,      0
-    vsllwil.w.h   vr19,     vr5,      0
-    vsllwil.w.h   vr6,      vr12,     0
-    vsllwil.w.h   vr7,      vr13,     0
-    vexth.w.h     vr8,      vr4
-    vexth.w.h     vr9,      vr5
-    vexth.w.h     vr10,     vr12
-    vexth.w.h     vr11,     vr13
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       8    // 1567
-    vldrepl.w     vr21,     t0,       12   // 3784
-    vldrepl.w     vr22,     t0,       0    // 2896
-
-    dct_8x4_core_lsx1 vr4, vr5, vr12, vr13
-
-    vshuf4i.d     vr5,      vr5,      0x01
-    vshuf4i.d     vr13,     vr13,     0x01
-
-    vsllwil.w.h   vr18,     vr14,     0
-    vsllwil.w.h   vr19,     vr15,     0
-    vsllwil.w.h   vr6,      vr24,     0
-    vsllwil.w.h   vr7,      vr25,     0
-    vexth.w.h     vr8,      vr14
-    vexth.w.h     vr9,      vr15
-    vexth.w.h     vr10,     vr24
-    vexth.w.h     vr11,     vr25
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       8     // 1567
-    vldrepl.w     vr21,     t0,       12    // 3784
-    vldrepl.w     vr22,     t0,       0     // 2896
-
-    dct_8x4_core_lsx1 vr14, vr15, vr24, vr25
-
-    vshuf4i.d     vr15,     vr15,     0x01
-    vshuf4i.d     vr25,     vr25,     0x01
-
-    vilvl.d       vr20,     vr14,     vr4
-    vilvh.d       vr21,     vr14,     vr4
-    vilvl.d       vr22,     vr15,     vr5
-    vilvh.d       vr23,     vr15,     vr5
-    vilvl.d       vr16,     vr24,     vr12
-    vilvh.d       vr17,     vr24,     vr12
-    vilvl.d       vr18,     vr25,     vr13
-    vilvh.d       vr19,     vr25,     vr13
-
-.irp i, vr20, vr21, vr22, vr23, vr16, vr17, vr18, vr19
-    vsrari.h      \i,       \i,       4
-.endr
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr20, vr21, vr22, vr23
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr16, vr17, vr18, vr19
-
-    fld.d            f24,     sp,    0
-    fld.d            f25,     sp,    8
-    fld.d            f26,     sp,    16
-    fld.d            f27,     sp,    24
-    addi.d           sp,      sp,    32
-endfunc
-
-function inv_txfm_add_dct_adst_8x8_8bpc_lsx
-    addi.d        sp,       sp,       -48
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-    fst.d         f28,      sp,       32
-    fst.d         f29,      sp,       40
-
-    vld_x8 a2, 0, 16, vr4, vr5, vr12, vr13, vr14, vr15, vr24, vr25
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       8    // 1567
-    vldrepl.w     vr21,     t0,       12   // 3784
-    vldrepl.w     vr22,     t0,       0    // 2896
-
-    vsllwil.w.h   vr18,     vr4,      0
-    vsllwil.w.h   vr19,     vr5,      0
-    vsllwil.w.h   vr6,      vr12,     0
-    vsllwil.w.h   vr7,      vr13,     0
-    vsllwil.w.h   vr8,      vr14,     0
-    vsllwil.w.h   vr9,      vr15,     0
-    vsllwil.w.h   vr10,     vr24,     0
-    vsllwil.w.h   vr11,     vr25,     0
-
-    dct_8x4_core_lsx1 vr26, vr27, vr28, vr29
-
-    vshuf4i.d     vr27,     vr27,     0x01
-    vshuf4i.d     vr29,     vr29,     0x01
-
-    vilvl.h       vr8,      vr27,     vr26  // 0 2 4 6 8 10 12 14
-    vilvh.h       vr9,      vr27,     vr26  // 1 3 5 7 9 11 13 15
-    vilvl.h       vr26,     vr9,      vr8   // 0 - 7 in0
-    vilvh.h       vr27,     vr9,      vr8   // 8 - 15 in1
-    vilvl.h       vr8,      vr29,     vr28  // 0 2 4 6 8 10 12 14
-    vilvh.h       vr9,      vr29,     vr28  // 1 3 5 7 9 11 13 15
-    vilvl.h       vr28,     vr9,      vr8   // 16 - 23  in2
-    vilvh.h       vr29,     vr9,      vr8   // 24 - 31  in3
-
-    vsrari.h      vr26,     vr26,     1     // in0low in1low
-    vsrari.h      vr27,     vr27,     1     // in2low in3low
-    vsrari.h      vr28,     vr28,     1     // in0high in1high
-    vsrari.h      vr29,     vr29,     1     // in2high in3high
-
-    vexth.w.h     vr18,     vr4
-    vexth.w.h     vr19,     vr5
-    vexth.w.h     vr6,      vr12
-    vexth.w.h     vr7,      vr13
-    vexth.w.h     vr8,      vr14
-    vexth.w.h     vr9,      vr15
-    vexth.w.h     vr10,     vr24
-    vexth.w.h     vr11,     vr25
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       8     // 1567
-    vldrepl.w     vr21,     t0,       12    // 3784
-    vldrepl.w     vr22,     t0,       0     // 2896
-
-    dct_8x4_core_lsx1 vr12, vr13, vr14, vr15
-
-    vshuf4i.d     vr13,     vr13,     0x01
-    vshuf4i.d     vr15,     vr15,     0x01
-
-    vilvl.h       vr8,      vr13,     vr12  // 0 2 4 6 8 10 12 14
-    vilvh.h       vr9,      vr13,     vr12  // 1 3 5 7 9 11 13 15
-    vilvl.h       vr12,     vr9,      vr8   // 0 - 7 in0
-    vilvh.h       vr13,     vr9,      vr8   // 8 - 15 in1
-    vilvl.h       vr8,      vr15,     vr14  // 0 2 4 6 8 10 12 14
-    vilvh.h       vr9,      vr15,     vr14  // 1 3 5 7 9 11 13 15
-    vilvl.h       vr14,     vr9,      vr8   // 16 - 23  in2
-    vilvh.h       vr15,     vr9,      vr8   // 24 - 31  in3
-
-    vsrari.h      vr0,      vr12,     1     // in4low in5low
-    vsrari.h      vr1,      vr13,     1     // in6low in7low
-    vsrari.h      vr2,      vr14,     1     // in4high in5high
-    vsrari.h      vr3,      vr15,     1     // in6high in7high
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-    vsllwil.w.h   vr18,     vr26,     0     // in0
-    vexth.w.h     vr19,     vr26            // in1
-    vsllwil.w.h   vr6,      vr27,     0     // in2
-    vexth.w.h     vr7,      vr27            // in3
-    vsllwil.w.h   vr8,      vr0,      0     // in3
-    vexth.w.h     vr9,      vr0             // in4
-    vsllwil.w.h   vr10,     vr1,      0     // in5
-    vexth.w.h     vr11,     vr1             // in6
-    adst8x8_1d_lsx vr26, vr27, vr0, vr1
-
-    vsllwil.w.h   vr18,     vr28,     0     // in0
-    vexth.w.h     vr19,     vr28            // in1
-    vsllwil.w.h   vr6,      vr29,     0     // in2
-    vexth.w.h     vr7,      vr29            // in3
-    vsllwil.w.h   vr8,      vr2,      0     // in4
-    vexth.w.h     vr9,      vr2             // in5
-    vsllwil.w.h   vr10,     vr3,      0     // in6
-    vexth.w.h     vr11,     vr3             // in7
-    adst8x8_1d_lsx vr28, vr29, vr16, vr17
-
-    vilvl.d       vr4,      vr28,     vr26  // 0 ... 7
-    vilvl.d       vr5,      vr29,     vr27  // 8 ... 15
-    vilvl.d       vr6,      vr16,     vr0   // 16 ... 23
-    vilvl.d       vr7,      vr17,     vr1   // 24 ... 31
-    vilvh.d       vr14,     vr17,     vr1   // 32 ... 39
-    vilvh.d       vr15,     vr16,     vr0   // 40 ... 47
-    vilvh.d       vr16,     vr29,     vr27  // 48 ... 55
-    vilvh.d       vr17,     vr28,     vr26  // 56 ... 63
-
-.irp i, vr4, vr5, vr6, vr7, vr14, vr15, vr16, vr17
-    vsrari.h      \i,       \i,       4
-.endr
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr14, vr15, vr16, vr17
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    fld.d         f28,      sp,       32
-    fld.d         f29,      sp,       40
-    addi.d        sp,       sp,       48
-endfunc
-
-function inv_txfm_add_adst_adst_8x8_8bpc_lsx
-    addi.d        sp,       sp,       -32
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-
-    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr16, vr17
-
-    vsllwil.w.h   vr18,     vr0,      0
-    vsllwil.w.h   vr19,     vr1,      0
-    vsllwil.w.h   vr6,      vr2,      0
-    vsllwil.w.h   vr7,      vr3,      0
-    vsllwil.w.h   vr8,      vr4,      0
-    vsllwil.w.h   vr9,      vr5,      0
-    vsllwil.w.h   vr10,     vr16,     0
-    vsllwil.w.h   vr11,     vr17,     0
-    adst8x8_1d_lsx vr24, vr25, vr26, vr27
-
-    vexth.w.h     vr18,     vr0            // in0
-    vexth.w.h     vr19,     vr1            // in1
-    vexth.w.h     vr6,      vr2            // in2
-    vexth.w.h     vr7,      vr3            // in3
-    vexth.w.h     vr8,      vr4            // in3
-    vexth.w.h     vr9,      vr5            // in4
-    vexth.w.h     vr10,     vr16           // in5
-    vexth.w.h     vr11,     vr17           // in6
-    adst8x8_1d_lsx vr0, vr1, vr2, vr3
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-.irp i, vr24, vr25, vr26, vr27, vr0, vr1, vr2, vr3
-    vsrari.h        \i,       \i,     1
-.endr
-
-    LSX_TRANSPOSE8x8_H vr24, vr25, vr26, vr27, vr0, vr1, vr2, vr3, \
-                       vr14, vr15, vr12, vr13, vr4, vr5, vr24, vr25, \
-                       vr6, vr7, vr8, vr9, vr10, vr11, vr16, vr17
-
-    vshuf4i.h     vr4,      vr4,      0x1b
-    vshuf4i.h     vr5,      vr5,      0x1b
-    vshuf4i.h     vr24,     vr24,     0x1b
-    vshuf4i.h     vr25,     vr25,     0x1b
-
-    vsllwil.w.h   vr18,     vr14,     0
-    vsllwil.w.h   vr19,     vr15,     0
-    vsllwil.w.h   vr6,      vr12,     0
-    vsllwil.w.h   vr7,      vr13,     0
-    vexth.w.h     vr8,      vr14            // in3
-    vexth.w.h     vr9,      vr15            // in4
-    vexth.w.h     vr10,     vr12            // in5
-    vexth.w.h     vr11,     vr13            // in6
-
-    adst8x8_1d_lsx vr26, vr27, vr0, vr1
-
-    vsllwil.w.h   vr18,     vr4,     0
-    vsllwil.w.h   vr19,     vr5,     0
-    vsllwil.w.h   vr6,      vr24,    0
-    vsllwil.w.h   vr7,      vr25,    0
-    vexth.w.h     vr8,      vr4             // in3
-    vexth.w.h     vr9,      vr5             // in4
-    vexth.w.h     vr10,     vr24            // in5
-    vexth.w.h     vr11,     vr25            // in6
-
-    adst8x8_1d_lsx vr24, vr25, vr16, vr17
-
-    vilvl.d       vr4,      vr24,     vr26  // 0 ... 7
-    vilvl.d       vr5,      vr25,     vr27  // 8 ... 15
-    vilvl.d       vr6,      vr16,     vr0   // 16 ... 23
-    vilvl.d       vr7,      vr17,     vr1   // 24 ... 31
-    vilvh.d       vr14,     vr17,     vr1   // 32 ... 39
-    vilvh.d       vr15,     vr16,     vr0   // 40 ... 47
-    vilvh.d       vr16,     vr25,     vr27  // 48 ... 55
-    vilvh.d       vr17,     vr24,     vr26  // 56 ... 63
-
-.irp i, vr4, vr5, vr6, vr7, vr14, vr15, vr16, vr17
-    vsrari.h        \i,       \i,     4
-.endr
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr14, vr15, vr16, vr17
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    addi.d        sp,       sp,       32
-endfunc
-
-function inv_txfm_add_flipadst_adst_8x8_8bpc_lsx
-    addi.d        sp,       sp,       -32
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-
-    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr16, vr17
-
-    vsllwil.w.h   vr18,     vr0,      0
-    vsllwil.w.h   vr19,     vr1,      0
-    vsllwil.w.h   vr6,      vr2,      0
-    vsllwil.w.h   vr7,      vr3,      0
-    vsllwil.w.h   vr8,      vr4,      0
-    vsllwil.w.h   vr9,      vr5,      0
-    vsllwil.w.h   vr10,     vr16,     0
-    vsllwil.w.h   vr11,     vr17,     0
-    adst8x8_1d_lsx vr12, vr13, vr14, vr15
-
-    vilvl.h       vr20,     vr12,     vr13
-    vilvl.h       vr21,     vr14,     vr15
-    vilvl.w       vr24,     vr20,     vr21
-    vilvh.w       vr25,     vr20,     vr21
-    vilvh.h       vr20,     vr12,     vr13
-    vilvh.h       vr21,     vr14,     vr15
-    vilvl.w       vr26,     vr20,     vr21
-    vilvh.w       vr27,     vr20,     vr21
-    vshuf4i.h     vr26,     vr26,     0x1b
-    vshuf4i.h     vr27,     vr27,     0x1b
-
-    vexth.w.h     vr18,     vr0
-    vexth.w.h     vr19,     vr1
-    vexth.w.h     vr6,      vr2
-    vexth.w.h     vr7,      vr3
-    vexth.w.h     vr8,      vr4
-    vexth.w.h     vr9,      vr5
-    vexth.w.h     vr10,     vr16
-    vexth.w.h     vr11,     vr17
-    adst8x8_1d_lsx vr12, vr13, vr14, vr15
-
-    vilvl.h       vr20,     vr12,     vr13
-    vilvl.h       vr21,     vr14,     vr15
-    vilvl.w       vr0,      vr20,     vr21
-    vilvh.w       vr1,      vr20,     vr21
-    vilvh.h       vr20,     vr12,     vr13
-    vilvh.h       vr21,     vr14,     vr15
-    vilvl.w       vr2,      vr20,     vr21
-    vilvh.w       vr3,      vr20,     vr21
-    vshuf4i.h     vr2,      vr2,      0x1b
-    vshuf4i.h     vr3,      vr3,      0x1b
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-.irp i, vr24, vr25, vr26, vr27, vr0, vr1, vr2, vr3
-    vsrari.h      \i,       \i,       1
-.endr
-
-    vsllwil.w.h   vr18,     vr26,     0    // in0
-    vexth.w.h     vr19,     vr26           // in1
-    vsllwil.w.h   vr6,      vr27,     0    // in2
-    vexth.w.h     vr7,      vr27           // in3
-    vsllwil.w.h   vr8,      vr2,      0    // in4
-    vexth.w.h     vr9,      vr2            // in5
-    vsllwil.w.h   vr10,     vr3,      0    // in6
-    vexth.w.h     vr11,     vr3            // in7
-    adst8x8_1d_lsx vr4, vr5, vr16, vr17
-
-    vsllwil.w.h   vr18,     vr24,     0    // in0
-    vexth.w.h     vr19,     vr24           // in1
-    vsllwil.w.h   vr6,      vr25,     0    // in2
-    vexth.w.h     vr7,      vr25           // in3
-    vsllwil.w.h   vr8,      vr0,      0    // in4
-    vexth.w.h     vr9,      vr0            // in5
-    vsllwil.w.h   vr10,     vr1,      0    // in6
-    vexth.w.h     vr11,     vr1            // in7
-    adst8x8_1d_lsx vr0, vr1, vr2, vr3
-
-    vilvl.d       vr20,     vr0,     vr4   // 0 ... 7
-    vilvl.d       vr21,     vr1,     vr5   // 8 ... 15
-    vilvl.d       vr22,     vr2,     vr16  // 16 ... 23
-    vilvl.d       vr23,     vr3,     vr17  // 24 ... 31
-    vilvh.d       vr14,     vr3,     vr17  // 32 ... 39
-    vilvh.d       vr15,     vr2,     vr16  // 40 ... 47
-    vilvh.d       vr16,     vr1,     vr5   // 48 ... 55
-    vilvh.d       vr17,     vr0,     vr4   // 56 ... 63
-
-.irp i, vr20, vr21, vr22, vr23, vr14, vr15, vr16, vr17
-    vsrari.h      \i,       \i,      4
-.endr
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr20, vr21, vr22, vr23
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr14, vr15, vr16, vr17
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    addi.d        sp,       sp,       32
-endfunc
-
-function inv_txfm_add_adst_flipadst_8x8_8bpc_lsx
-    addi.d        sp,       sp,       -32
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-
-    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr16, vr17
-
-    vsllwil.w.h   vr18,     vr0,      0
-    vsllwil.w.h   vr19,     vr1,      0
-    vsllwil.w.h   vr6,      vr2,      0
-    vsllwil.w.h   vr7,      vr3,      0
-    vsllwil.w.h   vr8,      vr4,      0
-    vsllwil.w.h   vr9,      vr5,      0
-    vsllwil.w.h   vr10,     vr16,     0
-    vsllwil.w.h   vr11,     vr17,     0
-    adst8x8_1d_lsx vr24, vr25, vr26, vr27
-
-    vexth.w.h     vr18,     vr0
-    vexth.w.h     vr19,     vr1
-    vexth.w.h     vr6,      vr2
-    vexth.w.h     vr7,      vr3
-    vexth.w.h     vr8,      vr4
-    vexth.w.h     vr9,      vr5
-    vexth.w.h     vr10,     vr16
-    vexth.w.h     vr11,     vr17
-    adst8x8_1d_lsx vr0, vr1, vr2, vr3
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-.irp i, vr24, vr25, vr26, vr27, vr0, vr1, vr2, vr3
-    vsrari.h      \i,       \i,       1
-.endr
-
-    LSX_TRANSPOSE8x8_H vr24, vr25, vr26, vr27, vr0, vr1, vr2, vr3, \
-                       vr24, vr25, vr26, vr27, vr0, vr1, vr2, vr3, \
-                       vr6, vr7, vr8, vr9, vr10, vr11, vr16, vr17
-
-    vshuf4i.h     vr0,      vr0,      0x1b
-    vshuf4i.h     vr1,      vr1,      0x1b
-    vshuf4i.h     vr2,      vr2,      0x1b
-    vshuf4i.h     vr3,      vr3,      0x1b
-
-    vsllwil.w.h   vr18,     vr0,      0    // in0
-    vsllwil.w.h   vr19,     vr1,      0    // in1
-    vsllwil.w.h   vr6,      vr2,      0    // in2
-    vsllwil.w.h   vr7,      vr3,      0    // in3
-    vexth.w.h     vr8,      vr0            // in4
-    vexth.w.h     vr9,      vr1            // in5
-    vexth.w.h     vr10,     vr2            // in6
-    vexth.w.h     vr11,     vr3            // in7
-    adst8x8_1d_lsx vr4, vr5, vr16, vr17
-
-    vsllwil.w.h   vr18,     vr24,     0    // in0
-    vsllwil.w.h   vr19,     vr25,     0    // in1
-    vsllwil.w.h   vr6,      vr26,     0    // in2
-    vsllwil.w.h   vr7,      vr27,     0    // in3
-    vexth.w.h     vr8,      vr24           // in4
-    vexth.w.h     vr9,      vr25           // in5
-    vexth.w.h     vr10,     vr26           // in6
-    vexth.w.h     vr11,     vr27           // in7
-    adst8x8_1d_lsx vr0, vr1, vr2, vr3
-
-    vilvh.d       vr20,     vr4,      vr0
-    vilvh.d       vr21,     vr5,      vr1
-    vilvh.d       vr22,     vr16,     vr2
-    vilvh.d       vr23,     vr17,     vr3
-    vilvl.d       vr14,     vr17,     vr3
-    vilvl.d       vr15,     vr16,     vr2
-    vilvl.d       vr18,     vr5,      vr1
-    vilvl.d       vr19,     vr4,      vr0
-
-.irp i, vr20, vr21, vr22, vr23, vr14, vr15, vr18, vr19
-    vsrari.h      \i,       \i,       4
-.endr
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr20, vr21, vr22, vr23
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr14, vr15, vr18, vr19
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    addi.d        sp,       sp,       32
-endfunc
-
-function inv_txfm_add_flipadst_dct_8x8_8bpc_lsx
-    addi.d        sp,       sp,       -32
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-
-    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr16, vr17
-
-    vsllwil.w.h   vr18,     vr0,      0
-    vsllwil.w.h   vr19,     vr1,      0
-    vsllwil.w.h   vr6,      vr2,      0
-    vsllwil.w.h   vr7,      vr3,      0
-    vsllwil.w.h   vr8,      vr4,      0
-    vsllwil.w.h   vr9,      vr5,      0
-    vsllwil.w.h   vr10,     vr16,     0
-    vsllwil.w.h   vr11,     vr17,     0
-    adst8x8_1d_lsx vr12, vr13, vr14, vr15
-
-    vilvl.h       vr20,     vr12,     vr13
-    vilvl.h       vr21,     vr14,     vr15
-    vilvl.w       vr24,     vr20,     vr21
-    vilvh.w       vr25,     vr20,     vr21
-    vilvh.h       vr20,     vr12,     vr13
-    vilvh.h       vr21,     vr14,     vr15
-    vilvl.w       vr26,     vr20,     vr21
-    vilvh.w       vr27,     vr20,     vr21
-
-    vexth.w.h     vr18,     vr0
-    vexth.w.h     vr19,     vr1
-    vexth.w.h     vr6,      vr2
-    vexth.w.h     vr7,      vr3
-    vexth.w.h     vr8,      vr4
-    vexth.w.h     vr9,      vr5
-    vexth.w.h     vr10,     vr16
-    vexth.w.h     vr11,     vr17
-    adst8x8_1d_lsx vr12, vr13, vr14, vr15
-
-    vilvl.h       vr20,     vr12,     vr13
-    vilvl.h       vr21,     vr14,     vr15
-    vilvl.w       vr0,      vr20,     vr21
-    vilvh.w       vr1,      vr20,     vr21
-    vilvh.h       vr20,     vr12,     vr13
-    vilvh.h       vr21,     vr14,     vr15
-    vilvl.w       vr2,      vr20,     vr21
-    vilvh.w       vr3,      vr20,     vr21
-
-    vreplgr2vr.h  vr23,     zero
-
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-    vsrari.h      vr24,     vr24,     1
-    vsrari.h      vr25,     vr25,     1
-    vsrari.h      vr26,     vr26,     1
-    vsrari.h      vr27,     vr27,     1
-    vsrari.h      vr14,     vr0,      1
-    vsrari.h      vr15,     vr1,      1
-    vsrari.h      vr16,     vr2,      1
-    vsrari.h      vr17,     vr3,      1
-
-    vsllwil.w.h   vr18,     vr26,     0
-    vexth.w.h     vr19,     vr26
-    vsllwil.w.h   vr6,      vr27,     0
-    vexth.w.h     vr7,      vr27
-    vsllwil.w.h   vr8,      vr16,     0
-    vexth.w.h     vr9,      vr16
-    vsllwil.w.h   vr10,     vr17,     0
-    vexth.w.h     vr11,     vr17
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       8    // 1567
-    vldrepl.w     vr21,     t0,       12   // 3784
-    vldrepl.w     vr22,     t0,       0    // 2896
-
-    dct_8x4_core_lsx1 vr26, vr27, vr16, vr17
-
-    vshuf4i.h     vr26,     vr26,     0x1b
-    vshuf4i.h     vr27,     vr27,     0x1b
-    vshuf4i.h     vr16,     vr16,     0x1b
-    vshuf4i.h     vr17,     vr17,     0x1b
-
-    vsllwil.w.h   vr18,     vr24,     0
-    vexth.w.h     vr19,     vr24
-    vsllwil.w.h   vr6,      vr25,     0
-    vexth.w.h     vr7,      vr25
-    vsllwil.w.h   vr8,      vr14,     0
-    vexth.w.h     vr9,      vr14
-    vsllwil.w.h   vr10,     vr15,     0
-    vexth.w.h     vr11,     vr15
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       8    // 1567
-    vldrepl.w     vr21,     t0,       12   // 3784
-    vldrepl.w     vr22,     t0,       0    // 2896
-
-    dct_8x4_core_lsx1 vr24, vr25, vr14, vr15
-
-    vilvl.d       vr4,      vr24,     vr26
-    vilvh.d       vr5,      vr24,     vr26
-    vilvh.d       vr6,      vr25,     vr27
-    vilvl.d       vr7,      vr25,     vr27
-    vilvl.d       vr24,     vr14,     vr16
-    vilvh.d       vr25,     vr14,     vr16
-    vilvh.d       vr26,     vr15,     vr17
-    vilvl.d       vr27,     vr15,     vr17
-
-.irp i, vr4, vr5, vr6, vr7, vr24, vr25, vr26, vr27
-    vsrari.h      \i,       \i,       4
-.endr
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr24, vr25, vr26, vr27
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    addi.d        sp,       sp,       32
-endfunc
-
-function inv_txfm_add_dct_flipadst_8x8_8bpc_lsx
-    addi.d        sp,       sp,       -48
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-    fst.d         f28,      sp,       32
-    fst.d         f29,      sp,       40
-
-    vld_x8 a2, 0, 16, vr4, vr5, vr12, vr13, vr14, vr15, vr24, vr25
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       8    // 1567
-    vldrepl.w     vr21,     t0,       12   // 3784
-    vldrepl.w     vr22,     t0,       0    // 2896
-
-    vsllwil.w.h   vr18,     vr4,      0
-    vsllwil.w.h   vr19,     vr5,      0
-    vsllwil.w.h   vr6,      vr12,     0
-    vsllwil.w.h   vr7,      vr13,     0
-    vsllwil.w.h   vr8,      vr14,     0
-    vsllwil.w.h   vr9,      vr15,     0
-    vsllwil.w.h   vr10,     vr24,     0
-    vsllwil.w.h   vr11,     vr25,     0
-    dct_8x4_core_lsx1 vr26, vr27, vr28, vr29
-    vshuf4i.d     vr27,     vr27,     0x01
-    vshuf4i.d     vr29,     vr29,     0x01
-
-    vilvl.h       vr8,      vr27,     vr26
-    vilvh.h       vr9,      vr27,     vr26
-    vilvl.h       vr26,     vr9,      vr8
-    vilvh.h       vr27,     vr9,      vr8
-    vilvl.h       vr8,      vr29,     vr28
-    vilvh.h       vr9,      vr29,     vr28
-    vilvl.h       vr28,     vr9,      vr8
-    vilvh.h       vr29,     vr9,      vr8
-
-    vsrari.h      vr26,     vr26,     1     // in0low in1low
-    vsrari.h      vr27,     vr27,     1     // in2low in3low
-    vsrari.h      vr28,     vr28,     1     // in0high in1high
-    vsrari.h      vr29,     vr29,     1     // in2high in3high
-
-    vexth.w.h     vr18,     vr4
-    vexth.w.h     vr19,     vr5
-    vexth.w.h     vr6,      vr12
-    vexth.w.h     vr7,      vr13
-    vexth.w.h     vr8,      vr14
-    vexth.w.h     vr9,      vr15
-    vexth.w.h     vr10,     vr24
-    vexth.w.h     vr11,     vr25
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       8    // 1567
-    vldrepl.w     vr21,     t0,       12   // 3784
-    vldrepl.w     vr22,     t0,       0    // 2896
-    dct_8x4_core_lsx1 vr12, vr13, vr14, vr15
-    vshuf4i.d     vr13,     vr13,     0x01
-    vshuf4i.d     vr15,     vr15,     0x01
-
-    vilvl.h       vr8,      vr13,     vr12
-    vilvh.h       vr9,      vr13,     vr12
-    vilvl.h       vr12,     vr9,      vr8
-    vilvh.h       vr13,     vr9,      vr8
-    vilvl.h       vr8,      vr15,     vr14
-    vilvh.h       vr9,      vr15,     vr14
-    vilvl.h       vr14,     vr9,      vr8
-    vilvh.h       vr15,     vr9,      vr8
-
-    vsrari.h      vr0,      vr12,     1
-    vsrari.h      vr1,      vr13,     1
-    vsrari.h      vr2,      vr14,     1
-    vsrari.h      vr3,      vr15,     1
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-    vsllwil.w.h   vr18,     vr28,     0    // in0
-    vexth.w.h     vr19,     vr28           // in1
-    vsllwil.w.h   vr6,      vr29,     0    // in2
-    vexth.w.h     vr7,      vr29           // in3
-    vsllwil.w.h   vr8,      vr2,      0    // in4
-    vexth.w.h     vr9,      vr2            // in5
-    vsllwil.w.h   vr10,     vr3,      0    // in6
-    vexth.w.h     vr11,     vr3            // in7
-    adst8x8_1d_lsx vr4, vr5, vr16, vr17
-
-    vsllwil.w.h   vr18,     vr26,     0    // in0
-    vexth.w.h     vr19,     vr26           // in1
-    vsllwil.w.h   vr6,      vr27,     0    // in2
-    vexth.w.h     vr7,      vr27           // in3
-    vsllwil.w.h   vr8,      vr0,      0    // in4
-    vexth.w.h     vr9,      vr0            // in5
-    vsllwil.w.h   vr10,     vr1,      0    // in6
-    vexth.w.h     vr11,     vr1            // in7
-    adst8x8_1d_lsx vr0, vr1, vr2, vr3
-
-    vilvh.d       vr26,     vr4,      vr0
-    vilvh.d       vr27,     vr5,      vr1
-    vilvh.d       vr28,     vr16,     vr2
-    vilvh.d       vr29,     vr17,     vr3
-    vilvl.d       vr20,     vr17,     vr3
-    vilvl.d       vr21,     vr16,     vr2
-    vilvl.d       vr22,     vr5,      vr1
-    vilvl.d       vr23,     vr4,      vr0
-
-.irp i, vr26, vr27, vr28, vr29, vr20, vr21, vr22, vr23
-    vsrari.h      \i,       \i,       4
-.endr
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr26, vr27, vr28, vr29
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr20, vr21, vr22, vr23
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    fld.d         f28,      sp,       32
-    fld.d         f29,      sp,       40
-    addi.d        sp,       sp,       48
-endfunc
-
-function inv_txfm_add_flipadst_flipadst_8x8_8bpc_lsx
-    addi.d        sp,       sp,       -32
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-
-    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr16, vr17
-
-    vsllwil.w.h   vr18,     vr0,      0
-    vsllwil.w.h   vr19,     vr1,      0
-    vsllwil.w.h   vr6,      vr2,      0
-    vsllwil.w.h   vr7,      vr3,      0
-    vsllwil.w.h   vr8,      vr4,      0
-    vsllwil.w.h   vr9,      vr5,      0
-    vsllwil.w.h   vr10,     vr16,     0
-    vsllwil.w.h   vr11,     vr17,     0
-    adst8x8_1d_lsx vr12, vr13, vr14, vr15
-
-    vilvl.h       vr20,     vr12,     vr13
-    vilvl.h       vr21,     vr14,     vr15
-    vilvl.w       vr24,     vr20,     vr21
-    vilvh.w       vr25,     vr20,     vr21
-    vilvh.h       vr20,     vr12,     vr13
-    vilvh.h       vr21,     vr14,     vr15
-    vilvl.w       vr26,     vr20,     vr21
-    vilvh.w       vr27,     vr20,     vr21
-    vshuf4i.h     vr26,     vr26,     0x1b
-    vshuf4i.h     vr27,     vr27,     0x1b
-
-    vexth.w.h     vr18,     vr0
-    vexth.w.h     vr19,     vr1
-    vexth.w.h     vr6,      vr2
-    vexth.w.h     vr7,      vr3
-    vexth.w.h     vr8,      vr4
-    vexth.w.h     vr9,      vr5
-    vexth.w.h     vr10,     vr16
-    vexth.w.h     vr11,     vr17
-    adst8x8_1d_lsx vr12, vr13, vr14, vr15
-
-    vilvl.h       vr20,     vr12,     vr13
-    vilvl.h       vr21,     vr14,     vr15
-    vilvl.w       vr0,      vr20,     vr21
-    vilvh.w       vr1,      vr20,     vr21
-    vilvh.h       vr20,     vr12,     vr13
-    vilvh.h       vr21,     vr14,     vr15
-    vilvl.w       vr2,      vr20,     vr21
-    vilvh.w       vr3,      vr20,     vr21
-    vshuf4i.h     vr2,      vr2,      0x1b
-    vshuf4i.h     vr3,      vr3,      0x1b
-
-.irp i, vr24, vr25, vr26, vr27, vr0, vr1, vr2, vr3
-    vsrari.h      \i,       \i,       1
-.endr
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-    vsllwil.w.h   vr18,     vr26,     0    // in0
-    vexth.w.h     vr19,     vr26           // in1
-    vsllwil.w.h   vr6,      vr27,     0    // in2
-    vexth.w.h     vr7,      vr27           // in3
-    vsllwil.w.h   vr8,      vr2,      0    // in4
-    vexth.w.h     vr9,      vr2            // in5
-    vsllwil.w.h   vr10,     vr3,      0    // in6
-    vexth.w.h     vr11,     vr3            // in7
-    adst8x8_1d_lsx vr4, vr5, vr16, vr17
-
-    vsllwil.w.h   vr18,     vr24,     0    // in0
-    vexth.w.h     vr19,     vr24           // in1
-    vsllwil.w.h   vr6,      vr25,     0    // in2
-    vexth.w.h     vr7,      vr25           // in3
-    vsllwil.w.h   vr8,      vr0,      0    // in4
-    vexth.w.h     vr9,      vr0            // in5
-    vsllwil.w.h   vr10,     vr1,      0    // in6
-    vexth.w.h     vr11,     vr1            // in7
-    adst8x8_1d_lsx vr0, vr1, vr2, vr3
-
-    vilvh.d       vr24,     vr0,      vr4
-    vilvh.d       vr25,     vr1,      vr5
-    vilvh.d       vr26,     vr2,      vr16
-    vilvh.d       vr27,     vr3,      vr17
-    vilvl.d       vr20,     vr3,      vr17
-    vilvl.d       vr21,     vr2,      vr16
-    vilvl.d       vr22,     vr1,      vr5
-    vilvl.d       vr23,     vr0,      vr4
-
-.irp i, vr24, vr25, vr26, vr27, vr20, vr21, vr22, vr23
-    vsrari.h      \i,       \i,       4
-.endr
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr24, vr25, vr26, vr27
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr20, vr21, vr22, vr23
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    addi.d        sp,       sp,       32
-endfunc
-
-function inv_txfm_add_dct_identity_8x8_8bpc_lsx
-    addi.d        sp,       sp,       -48
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-    fst.d         f28,      sp,       32
-    fst.d         f29,      sp,       40
-
-    vld_x8 a2, 0, 16, vr4, vr5, vr12, vr13, vr14, vr15, vr24, vr25
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       8    // 1567
-    vldrepl.w     vr21,     t0,       12   // 3784
-    vldrepl.w     vr22,     t0,       0    // 2896
-
-    vsllwil.w.h   vr18,     vr4,      0
-    vsllwil.w.h   vr19,     vr5,      0
-    vsllwil.w.h   vr6,      vr12,     0
-    vsllwil.w.h   vr7,      vr13,     0
-    vsllwil.w.h   vr8,      vr14,     0
-    vsllwil.w.h   vr9,      vr15,     0
-    vsllwil.w.h   vr10,     vr24,     0
-    vsllwil.w.h   vr11,     vr25,     0
-    dct_8x4_core_lsx1 vr26, vr27, vr28, vr29
-    vshuf4i.d     vr27,     vr27,     0x01
-    vshuf4i.d     vr29,     vr29,     0x01
-
-    vilvl.h       vr8,      vr27,     vr26
-    vilvh.h       vr9,      vr27,     vr26
-    vilvl.h       vr26,     vr9,      vr8
-    vilvh.h       vr27,     vr9,      vr8
-    vilvl.h       vr8,      vr29,     vr28
-    vilvh.h       vr9,      vr29,     vr28
-    vilvl.h       vr28,     vr9,      vr8
-    vilvh.h       vr29,     vr9,      vr8
-
-    vsrari.h      vr26,     vr26,     1     // in0low in1low
-    vsrari.h      vr27,     vr27,     1     // in2low in3low
-    vsrari.h      vr28,     vr28,     1     // in0high in1high
-    vsrari.h      vr29,     vr29,     1     // in2high in3high
-
-    vexth.w.h     vr18,     vr4
-    vexth.w.h     vr19,     vr5
-    vexth.w.h     vr6,      vr12
-    vexth.w.h     vr7,      vr13
-    vexth.w.h     vr8,      vr14
-    vexth.w.h     vr9,      vr15
-    vexth.w.h     vr10,     vr24
-    vexth.w.h     vr11,     vr25
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       8    // 1567
-    vldrepl.w     vr21,     t0,       12   // 3784
-    vldrepl.w     vr22,     t0,       0    // 2896
-
-    dct_8x4_core_lsx1 vr12, vr13, vr14, vr15
-
-    vshuf4i.d     vr13,     vr13,     0x01
-    vshuf4i.d     vr15,     vr15,     0x01
-
-    vilvl.h       vr8,      vr13,     vr12
-    vilvh.h       vr9,      vr13,     vr12
-    vilvl.h       vr12,     vr9,      vr8
-    vilvh.h       vr13,     vr9,      vr8
-    vilvl.h       vr8,      vr15,     vr14
-    vilvh.h       vr9,      vr15,     vr14
-    vilvl.h       vr14,     vr9,      vr8
-    vilvh.h       vr15,     vr9,      vr8
-
-    vsrari.h      vr20,     vr12,     1
-    vsrari.h      vr21,     vr13,     1
-    vsrari.h      vr22,     vr14,     1
-    vsrari.h      vr23,     vr15,     1
-
-    vreplgr2vr.h  vr19,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr19,     a2,       \i
-.endr
-    // identity8
-    vsllwil.w.h   vr10,     vr26,     1
-    vsllwil.w.h   vr11,     vr27,     1
-    vsllwil.w.h   vr16,     vr28,     1
-    vsllwil.w.h   vr17,     vr29,     1
-    vsllwil.w.h   vr6,      vr20,     1
-    vsllwil.w.h   vr7,      vr21,     1
-    vsllwil.w.h   vr18,     vr22,     1
-    vsllwil.w.h   vr19,     vr23,     1
-
-.irp i, vr26, vr27, vr28, vr29, vr20, vr21, vr22, vr23
-    vexth.w.h     \i,       \i
-.endr
-
-.irp i, vr26, vr27, vr28, vr29, vr20, vr21, vr22, vr23
-    vslli.w       \i,       \i,       1
-.endr
-
-    vssrarni.h.w  vr16,     vr10,     4   // in0
-    vssrarni.h.w  vr28,     vr26,     4   // in1
-    vssrarni.h.w  vr17,     vr11,     4   // in2
-    vssrarni.h.w  vr29,     vr27,     4   // in3
-    vssrarni.h.w  vr18,     vr6,      4   // in4
-    vssrarni.h.w  vr22,     vr20,     4   // in5
-    vssrarni.h.w  vr19,     vr7,      4   // in6
-    vssrarni.h.w  vr23,     vr21,     4   // in7
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr16, vr28, vr17, vr29
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr18, vr22, vr19, vr23
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    fld.d         f28,      sp,       32
-    fld.d         f29,      sp,       40
-    addi.d        sp,       sp,       48
-endfunc
-
-function inv_txfm_add_identity_dct_8x8_8bpc_lsx
-    addi.d        sp,       sp,       -48
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-    fst.d         f28,      sp,       32
-    fst.d         f29,      sp,       40
-
-    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr24, vr25
-
-    // identity8
-    vsllwil.w.h   vr6,      vr0,      1
-    vsllwil.w.h   vr7,      vr1,      1
-    vsllwil.w.h   vr8,      vr2,      1
-    vsllwil.w.h   vr9,      vr3,      1
-    vsllwil.w.h   vr10,     vr4,      1
-    vsllwil.w.h   vr11,     vr5,      1
-    vsllwil.w.h   vr12,     vr24,     1
-    vsllwil.w.h   vr13,     vr25,     1
-
-.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr24, vr25
-    vexth.w.h     \i,       \i
-.endr
-
-.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr24, vr25
-    vslli.w       \i,       \i,       1
-.endr
-    vssrarni.h.w  vr0,      vr6,      1   // in0
-    vssrarni.h.w  vr1,      vr7,      1   // in1
-    vssrarni.h.w  vr2,      vr8,      1   // in2
-    vssrarni.h.w  vr3,      vr9,      1   // in3
-    vssrarni.h.w  vr4,      vr10,     1   // in4
-    vssrarni.h.w  vr5,      vr11,     1   // in5
-    vssrarni.h.w  vr24,     vr12,     1   // in6
-    vssrarni.h.w  vr25,     vr13,     1   // in7
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-    LSX_TRANSPOSE8x8_H vr0, vr1, vr2, vr3, vr4, vr5, vr24, vr25, \
-                       vr4, vr5, vr12, vr13, vr14, vr15, vr24, vr25, \
-                       vr6, vr7, vr8, vr9, vr10, vr11, vr16, vr17
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       8    // 1567
-    vldrepl.w     vr21,     t0,       12   // 3784
-    vldrepl.w     vr22,     t0,       0    // 2896
-
-    // dct4 in0 in2 in4 in6
-    vsllwil.w.h   vr18,     vr4,      0
-    vsllwil.w.h   vr19,     vr5,      0
-    vsllwil.w.h   vr6,      vr12,     0
-    vsllwil.w.h   vr7,      vr13,     0
-    vsllwil.w.h   vr8,      vr14,     0
-    vsllwil.w.h   vr9,      vr15,     0
-    vsllwil.w.h   vr10,     vr24,     0
-    vsllwil.w.h   vr11,     vr25,     0
-    dct_8x4_core_lsx1 vr16, vr17, vr26, vr27
-
-    vexth.w.h     vr18,     vr4
-    vexth.w.h     vr19,     vr5
-    vexth.w.h     vr6,      vr12
-    vexth.w.h     vr7,      vr13
-    vexth.w.h     vr8,      vr14
-    vexth.w.h     vr9,      vr15
-    vexth.w.h     vr10,     vr24
-    vexth.w.h     vr11,     vr25
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       8     // 1567
-    vldrepl.w     vr21,     t0,       12    // 3784
-    vldrepl.w     vr22,     t0,       0     // 2896
-    dct_8x4_core_lsx1 vr4, vr5, vr24, vr25
-
-    vilvl.d       vr8,      vr4,      vr16
-    vilvh.d       vr9,      vr4,      vr16
-    vilvh.d       vr6,      vr5,      vr17
-    vilvl.d       vr7,      vr5,      vr17
-    vilvl.d       vr16,     vr24,     vr26
-    vilvh.d       vr17,     vr24,     vr26
-    vilvh.d       vr18,     vr25,     vr27
-    vilvl.d       vr19,     vr25,     vr27
-
-.irp i, vr8, vr9, vr6, vr7, vr16, vr17, vr18, vr19
-    vsrari.h      \i,       \i,       4
-.endr
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr8, vr9, vr6, vr7
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr16, vr17, vr18, vr19
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    fld.d         f28,      sp,       32
-    fld.d         f29,      sp,       40
-    addi.d        sp,       sp,       48
-endfunc
-
-function inv_txfm_add_flipadst_identity_8x8_8bpc_lsx
-    addi.d        sp,       sp,       -32
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-
-    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr16, vr17
-
-    vsllwil.w.h   vr18,     vr0,      0
-    vsllwil.w.h   vr19,     vr1,      0
-    vsllwil.w.h   vr6,      vr2,      0
-    vsllwil.w.h   vr7,      vr3,      0
-    vsllwil.w.h   vr8,      vr4,      0
-    vsllwil.w.h   vr9,      vr5,      0
-    vsllwil.w.h   vr10,     vr16,     0
-    vsllwil.w.h   vr11,     vr17,     0
-    adst8x8_1d_lsx vr12, vr13, vr14, vr15
-
-    vilvl.h       vr20,     vr12,     vr13
-    vilvl.h       vr21,     vr14,     vr15
-    vilvl.w       vr24,     vr20,     vr21
-    vilvh.w       vr25,     vr20,     vr21
-    vilvh.h       vr20,     vr12,     vr13
-    vilvh.h       vr21,     vr14,     vr15
-    vilvl.w       vr26,     vr20,     vr21
-    vilvh.w       vr27,     vr20,     vr21
-    vshuf4i.h     vr26,     vr26,     0x1b
-    vshuf4i.h     vr27,     vr27,     0x1b
-
-    vexth.w.h     vr18,     vr0            // in0
-    vexth.w.h     vr19,     vr1            // in1
-    vexth.w.h     vr6,      vr2            // in2
-    vexth.w.h     vr7,      vr3            // in3
-    vexth.w.h     vr8,      vr4            // in3
-    vexth.w.h     vr9,      vr5            // in4
-    vexth.w.h     vr10,     vr16           // in5
-    vexth.w.h     vr11,     vr17           // in6
-    adst8x8_1d_lsx vr12, vr13, vr14, vr15
-
-    vilvl.h       vr20,     vr12,     vr13
-    vilvl.h       vr21,     vr14,     vr15
-    vilvl.w       vr16,     vr20,     vr21
-    vilvh.w       vr17,     vr20,     vr21
-    vilvh.h       vr20,     vr12,     vr13
-    vilvh.h       vr21,     vr14,     vr15
-    vilvl.w       vr18,     vr20,     vr21
-    vilvh.w       vr19,     vr20,     vr21
-    vshuf4i.h     vr18,     vr18,     0x1b
-    vshuf4i.h     vr19,     vr19,     0x1b
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-.irp i, vr24, vr25, vr26, vr27, vr16, vr17, vr18, vr19
-    vsrari.h      \i,       \i,       1
-.endr
-
-    // identity8
-    vsllwil.w.h   vr20,     vr24,     1
-    vsllwil.w.h   vr21,     vr25,     1
-    vsllwil.w.h   vr12,     vr26,     1
-    vsllwil.w.h   vr13,     vr27,     1
-    vsllwil.w.h   vr22,     vr16,     1
-    vsllwil.w.h   vr23,     vr17,     1
-    vsllwil.w.h   vr14,     vr18,     1
-    vsllwil.w.h   vr15,     vr19,     1
-
-.irp i, vr24, vr25, vr26, vr27, vr16, vr17, vr18, vr19
-    vexth.w.h     \i,       \i
-.endr
-
-.irp i, vr24, vr25, vr26, vr27, vr16, vr17, vr18, vr19
-     vslli.w      \i,       \i,       1
-.endr
-
-    vssrarni.h.w  vr20,     vr12,     4   // in0
-    vssrarni.h.w  vr24,     vr26,     4   // in1
-    vssrarni.h.w  vr21,     vr13,     4   // in2
-    vssrarni.h.w  vr25,     vr27,     4   // in3
-    vssrarni.h.w  vr22,     vr14,     4   // in4
-    vssrarni.h.w  vr16,     vr18,     4   // in5
-    vssrarni.h.w  vr23,     vr15,     4   // in6
-    vssrarni.h.w  vr17,     vr19,     4   // in7
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr20, vr24, vr21, vr25
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr22, vr16, vr23, vr17
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    addi.d        sp,       sp,       32
-endfunc
-
-function inv_txfm_add_identity_flipadst_8x8_8bpc_lsx
-    addi.d        sp,       sp,       -48
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-    fst.d         f28,      sp,       32
-    fst.d         f29,      sp,       40
-
-    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr24, vr25
-
-    // identity8
-    vsllwil.w.h   vr6,      vr0,      1
-    vsllwil.w.h   vr7,      vr1,      1
-    vsllwil.w.h   vr8,      vr2,      1
-    vsllwil.w.h   vr9,      vr3,      1
-    vsllwil.w.h   vr10,     vr4,      1
-    vsllwil.w.h   vr11,     vr5,      1
-    vsllwil.w.h   vr12,     vr24,     1
-    vsllwil.w.h   vr13,     vr25,     1
-
-.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr24, vr25
-    vexth.w.h     \i,       \i
-.endr
-
-.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr24, vr25
-    vslli.w       \i,       \i,       1
-.endr
-
-    vssrarni.h.w  vr0,      vr6,      1   // in0
-    vssrarni.h.w  vr1,      vr7,      1   // in1
-    vssrarni.h.w  vr2,      vr8,      1   // in2
-    vssrarni.h.w  vr3,      vr9,      1   // in3
-    vssrarni.h.w  vr4,      vr10,     1   // in4
-    vssrarni.h.w  vr5,      vr11,     1   // in5
-    vssrarni.h.w  vr24,     vr12,     1   // in6
-    vssrarni.h.w  vr25,     vr13,     1   // in7
-
-    LSX_TRANSPOSE8x8_H vr0, vr1, vr2, vr3, vr4, vr5, vr24, vr25, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr24, vr25, \
-                       vr6, vr7, vr8, vr9, vr10, vr11, vr12, vr13
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-    vsllwil.w.h   vr18,     vr0,      0    // in0
-    vsllwil.w.h   vr19,     vr1,      0    // in1
-    vsllwil.w.h   vr6,      vr2,      0    // in2
-    vsllwil.w.h   vr7,      vr3,      0    // in3
-    vsllwil.w.h   vr8,      vr4,      0    // in3
-    vsllwil.w.h   vr9,      vr5,      0    // in4
-    vsllwil.w.h   vr10,     vr24,     0    // in5
-    vsllwil.w.h   vr11,     vr25,     0    // in6
-    adst8x8_1d_lsx vr26, vr27, vr28, vr29
-
-    vexth.w.h     vr18,     vr0            // in0
-    vexth.w.h     vr19,     vr1            // in1
-    vexth.w.h     vr6,      vr2            // in2
-    vexth.w.h     vr7,      vr3            // in3
-    vexth.w.h     vr8,      vr4            // in3
-    vexth.w.h     vr9,      vr5            // in4
-    vexth.w.h     vr10,     vr24           // in5
-    vexth.w.h     vr11,     vr25           // in6
-    adst8x8_1d_lsx vr0, vr1, vr2, vr3
-
-    vilvh.d       vr4,      vr0,      vr26
-    vilvh.d       vr5,      vr1,      vr27
-    vilvh.d       vr6,      vr2,      vr28
-    vilvh.d       vr7,      vr3,      vr29
-    vilvl.d       vr14,     vr3,      vr29
-    vilvl.d       vr15,     vr2,      vr28
-    vilvl.d       vr16,     vr1,      vr27
-    vilvl.d       vr17,     vr0,      vr26
-
-.irp i, vr4, vr5, vr6, vr7, vr14, vr15, vr16, vr17
-    vsrari.h      \i,        \i,      4
-.endr
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr14, vr15, vr16, vr17
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    fld.d         f28,      sp,       32
-    fld.d         f29,      sp,       40
-    addi.d        sp,       sp,       48
-
-endfunc
-
-function inv_txfm_add_adst_identity_8x8_8bpc_lsx
-    addi.d        sp,       sp,       -32
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-
-    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr16, vr17
-
-    vsllwil.w.h   vr18,     vr0,      0
-    vsllwil.w.h   vr19,     vr1,      0
-    vsllwil.w.h   vr6,      vr2,      0
-    vsllwil.w.h   vr7,      vr3,      0
-    vsllwil.w.h   vr8,      vr4,      0
-    vsllwil.w.h   vr9,      vr5,      0
-    vsllwil.w.h   vr10,     vr16,     0
-    vsllwil.w.h   vr11,     vr17,     0
-    adst8x8_1d_lsx vr24, vr25, vr26, vr27
-
-    vexth.w.h     vr18,     vr0
-    vexth.w.h     vr19,     vr1
-    vexth.w.h     vr6,      vr2
-    vexth.w.h     vr7,      vr3
-    vexth.w.h     vr8,      vr4
-    vexth.w.h     vr9,      vr5
-    vexth.w.h     vr10,     vr16
-    vexth.w.h     vr11,     vr17
-    adst8x8_1d_lsx vr0, vr1, vr2, vr3
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-.irp i, vr24, vr25, vr26, vr27, vr0, vr1, vr2, vr3
-    vsrari.h      \i,       \i,       1
-.endr
-
-    LSX_TRANSPOSE8x8_H vr24, vr25, vr26, vr27, vr0, vr1, vr2, vr3, \
-                       vr24, vr25, vr20, vr21, vr26, vr27, vr22, vr23, \
-                       vr6, vr7, vr8, vr9, vr10, vr11, vr16, vr17
-
-    vshuf4i.h     vr26,     vr26,     0x1b
-    vshuf4i.h     vr27,     vr27,     0x1b
-    vshuf4i.h     vr22,     vr22,     0x1b
-    vshuf4i.h     vr23,     vr23,     0x1b
-
-    // identity8
-    vsllwil.w.h   vr16,     vr24,     1
-    vsllwil.w.h   vr17,     vr25,     1
-    vsllwil.w.h   vr10,     vr20,     1
-    vsllwil.w.h   vr11,     vr21,     1
-    vsllwil.w.h   vr18,     vr26,     1
-    vsllwil.w.h   vr19,     vr27,     1
-    vsllwil.w.h   vr14,     vr22,     1
-    vsllwil.w.h   vr15,     vr23,     1
-
-.irp i, vr24, vr25, vr20, vr21, vr26, vr27, vr22, vr23
-    vexth.w.h     \i,       \i
-.endr
-
-.irp i, vr24, vr25, vr20, vr21, vr26, vr27, vr22, vr23
-    vslli.w       \i,       \i,       1
-.endr
-
-    vssrarni.h.w  vr18,     vr16,     4    // in0
-    vssrarni.h.w  vr19,     vr17,     4    // in1
-    vssrarni.h.w  vr14,     vr10,     4    // in2
-    vssrarni.h.w  vr15,     vr11,     4    // in3
-    vssrarni.h.w  vr26,     vr24,     4    // in4
-    vssrarni.h.w  vr27,     vr25,     4    // in5
-    vssrarni.h.w  vr22,     vr20,     4    // in6
-    vssrarni.h.w  vr23,     vr21,     4    // in7
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr18, vr19, vr14, vr15
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr26, vr27, vr22, vr23
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    addi.d        sp,       sp,       32
-endfunc
-
-function inv_txfm_add_identity_adst_8x8_8bpc_lsx
-    addi.d        sp,       sp,       -48
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-    fst.d         f28,      sp,       32
-    fst.d         f29,      sp,       40
-
-    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr24, vr25
-
-    // identity8
-    vsllwil.w.h   vr6,      vr0,      1
-    vsllwil.w.h   vr7,      vr1,      1
-    vsllwil.w.h   vr8,      vr2,      1
-    vsllwil.w.h   vr9,      vr3,      1
-    vsllwil.w.h   vr10,     vr4,      1
-    vsllwil.w.h   vr11,     vr5,      1
-    vsllwil.w.h   vr12,     vr24,     1
-    vsllwil.w.h   vr13,     vr25,     1
-
-.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr24, vr25
-    vexth.w.h     \i,       \i
-.endr
-
-.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr24, vr25
-    vslli.w       \i,       \i,       1
-.endr
-
-    vssrarni.h.w  vr0,      vr6,      1   // in0
-    vssrarni.h.w  vr1,      vr7,      1   // in1
-    vssrarni.h.w  vr2,      vr8,      1   // in2
-    vssrarni.h.w  vr3,      vr9,      1   // in3
-    vssrarni.h.w  vr4,      vr10,     1   // in4
-    vssrarni.h.w  vr5,      vr11,     1   // in5
-    vssrarni.h.w  vr24,     vr12,     1   // in6
-    vssrarni.h.w  vr25,     vr13,     1   // in7
-
-    LSX_TRANSPOSE8x8_H vr0, vr1, vr2, vr3, vr4, vr5, vr24, vr25, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr24, vr25, \
-                       vr6, vr7, vr8, vr9, vr10, vr11, vr12, vr13
-
-    vreplgr2vr.h  vr23,     zero
-
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-    vsllwil.w.h   vr18,     vr0,      0
-    vsllwil.w.h   vr19,     vr1,      0
-    vsllwil.w.h   vr6,      vr2,      0
-    vsllwil.w.h   vr7,      vr3,      0
-    vsllwil.w.h   vr8,      vr4,      0
-    vsllwil.w.h   vr9,      vr5,      0
-    vsllwil.w.h   vr10,     vr24,     0
-    vsllwil.w.h   vr11,     vr25,     0
-    adst8x8_1d_lsx vr26, vr27, vr28, vr29
-
-    vexth.w.h     vr18,     vr0
-    vexth.w.h     vr19,     vr1
-    vexth.w.h     vr6,      vr2
-    vexth.w.h     vr7,      vr3
-    vexth.w.h     vr8,      vr4
-    vexth.w.h     vr9,      vr5
-    vexth.w.h     vr10,     vr24
-    vexth.w.h     vr11,     vr25
-
-    adst8x8_1d_lsx vr0, vr1, vr2, vr3
-
-    vilvl.d       vr4,      vr0,      vr26  // 0 ... 7
-    vilvl.d       vr5,      vr1,      vr27  // 8 ... 15
-    vilvl.d       vr6,      vr2,      vr28  // 16 ... 23
-    vilvl.d       vr7,      vr3,      vr29  // 24 ... 31
-    vilvh.d       vr14,     vr3,      vr29  // 32 ... 39
-    vilvh.d       vr15,     vr2,      vr28  // 40 ... 47
-    vilvh.d       vr16,     vr1,      vr27  // 48 ... 55
-    vilvh.d       vr17,     vr0,      vr26  // 56 ... 63
-
-.irp i, vr4, vr5, vr6, vr7, vr14, vr15, vr16, vr17
-    vsrari.h      \i,       \i,       4
-.endr
-
-    alsl.d        t2,       a1,       a0,    1
-
-    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr14, vr15, vr16, vr17
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    fld.d         f28,      sp,       32
-    fld.d         f29,      sp,       40
-    addi.d        sp,       sp,       48
-endfunc
-
-.macro vmul_vmadd_w in0, in1, in2, in3, out0, out1
-    vsllwil.w.h   vr22,     \in0,     0
-    vexth.w.h     vr23,     \in0
-    vmul.w        \out0,    vr22,     \in2
-    vmul.w        \out1,    vr23,     \in2
-    vsllwil.w.h   vr22,     \in1,     0
-    vexth.w.h     vr23,     \in1
-    vmadd.w       \out0,    vr22,     \in3
-    vmadd.w       \out1,    vr23,     \in3
-.endm
-
-.macro vmul_vmsub_w in0, in1, in2, in3, out0, out1
-    vsllwil.w.h   vr22,     \in0,     0
-    vexth.w.h     vr23,     \in0
-    vmul.w        \out0,    vr22,     \in2
-    vmul.w        \out1,    vr23,     \in2
-    vsllwil.w.h   vr22,     \in1,     0
-    vexth.w.h     vr23,     \in1
-    vmsub.w       \out0,    vr22,     \in3
-    vmsub.w       \out1,    vr23,     \in3
-.endm
-
-.macro rect2_lsx in0, in1, out0
-    vsllwil.w.h   vr22,     \in0,     0     // in1
-    vexth.w.h     \in0,     \in0            // in1
-    vmul.w        vr22,     vr22,     \in1
-    vmul.w        \out0,    \in0,     \in1
-    vssrarni.h.w  \out0,    vr22,     12
-.endm
-
-.macro dct_8x8_core_lsx in0, in1, in2, in3, in4, in5, in6, in7, out0, \
-                        out1, out2, out3, out4, out5, out6, out7, rect2
-
-    la.local      t0,       idct_coeffs
-
-.ifc \rect2, rect2_lsx
-    vldrepl.w     vr23,      t0,       0        // 2896
-.irp i, \in0, \in1, \in2, \in3, \in4, \in5, \in6, \in7
-    rect2_lsx \i, vr23, \i
-.endr
-.endif
-    vldrepl.w     vr20,      t0,       8        // 1567
-    vldrepl.w     vr21,      t0,       12       // 3784
-
-    vmul_vmadd_w  \in2, \in6, vr21, vr20, vr8, vr9
-    vssrarni.h.w  vr9,       vr8,      12       // t3
-    vmul_vmsub_w  \in2, \in6, vr20, vr21, vr8, vr10
-    vssrarni.h.w  vr10,      vr8,      12       // t2
-
-    vldrepl.w     vr20,      t0,       0        // 2896
-    vmul_vmadd_w  \in0, \in4, vr20, vr20, vr8, \in2
-    vssrarni.h.w  \in2,      vr8,      12       // t0
-    vmul_vmsub_w  \in0, \in4, vr20, vr20, vr8, \in6
-    vssrarni.h.w  \in6,      vr8,      12       // t1
-
-    vsadd.h       vr8,       \in2,     vr9      // c[0]
-    vssub.h       vr9,       \in2,     vr9      // c[3]
-    vsadd.h       \in0,      \in6,     vr10     // c[1]
-    vssub.h       vr10,      \in6,     vr10     // c[2]
-
-    vldrepl.w     vr20,     t0,        16       // 799
-    vldrepl.w     vr21,     t0,        20       // 4017
-    vmul_vmadd_w  \in1, \in7, vr21, vr20, \in2, \in4
-    vssrarni.h.w  \in4,     \in2,      12       // t7a
-    vmul_vmsub_w  \in1, \in7, vr20, vr21, \in2, \in6
-    vssrarni.h.w  \in6,     \in2,      12       // t4a
-
-    vldrepl.w     vr20,     t0,        24       // 3406
-    vldrepl.w     vr21,     t0,        28       // 2276
-    vmul_vmadd_w  \in5, \in3, vr21, vr20, \in2, \in1
-    vssrarni.h.w  \in1,     \in2,      12       // t6a
-    vmul_vmsub_w  \in5, \in3, vr20, vr21, \in2, \in7
-    vssrarni.h.w  \in7,     \in2,      12       // t5a
-
-    vsadd.h       \in3,     \in6,      \in7     // t4
-    vssub.h       \in6,     \in6,      \in7     // t5a
-    vsadd.h       \in5,     \in4,      \in1     // t7
-    vssub.h       \in4,     \in4,      \in1     // t6a
-
-    vldrepl.w     vr20,     t0,        0        // 2896
-    vmul_vmadd_w  \in4, \in6, vr20, vr20, \in2, \in1
-    vssrarni.h.w  \in1,     \in2,      12       // t6
-    vmul_vmsub_w  \in4, \in6, vr20, vr20, \in2, \in7
-    vssrarni.h.w  \in7,     \in2,      12       // t5
-
-    vsadd.h       \out0,    vr8,       \in5     // c[0]
-    vssub.h       \out7,    vr8,       \in5     // c[7]
-    vsadd.h       \out1,    \in0,      \in1     // c[1]
-    vssub.h       \out6,    \in0,      \in1     // c[6]
-    vsadd.h       \out2,    vr10,      \in7     // c[2]
-    vssub.h       \out5,    vr10,      \in7     // c[5]
-    vsadd.h       \out3,    vr9,       \in3     // c[3]
-    vssub.h       \out4,    vr9,       \in3     // c[4]
-.endm
-
-function inv_txfm_add_dct_dct_8x8_8bpc_lsx
-    bnez          a3,       .NO_HAS_DCONLY_8x8
-
-    ld.h          t2,       a2,       0      // dc
-    vldi          vr0,      0x8b5            // 181
-    vreplgr2vr.w  vr1,      t2
-    vldi          vr5,      0x880            // 128
-    vmul.w        vr2,      vr0,      vr1    // dc * 181
-    st.h          zero,     a2,       0
-    vsrari.w      vr2,      vr2,      8      // (dc * 181 + 128) >> 8
-    vld           vr10,     a0,       0      // 0 1 2 3 4 5 6 7
-    vsrari.w      vr2,      vr2,      1      // (dc + rnd) >> shift
-    vldx          vr11,     a0,       a1     // 8 9 10 11 12 13 14 15
-    alsl.d        t2,       a1,       a0,    1
-    vmadd.w       vr5,      vr2,      vr0
-    vld           vr12,     t2,       0      // 16 17 18 19 20 21 22 23
-    vssrarni.h.w  vr5,      vr5,      12
-    vldx          vr13,     t2,       a1     // 24 25 26 27 28 29 30 31
-
-    DST_ADD_W8 vr10, vr11, vr12, vr13, vr5, vr5, vr5, vr5
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr5, vr5, vr5, vr5
-
-    b             .DCT_DCT_8X8_END
-
-.NO_HAS_DCONLY_8x8:
-
-    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    la.local      t0,       idct_coeffs
-
-    dct_8x8_core_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-                     vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, no_rect2
-
-    LSX_TRANSPOSE8x8_H vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-                       vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-.irp i, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
-    vsrari.h      \i,       \i,       1
-.endr
-
-    vreplgr2vr.h  vr23,     zero
-
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112
-    vst           vr23,     a2,       \i
-.endr
-
-    dct_8x8_core_lsx vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-                     vr4, vr5, vr6, vr7, vr20, vr21, vr22, vr23,  no_rect2
-
-.irp i, vr4, vr5, vr6, vr7, vr20, vr21, vr22, vr23
-    vsrari.h      \i,       \i,       4
-.endr
-
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr20, vr21, vr22, vr23
-
-.DCT_DCT_8X8_END:
-
-endfunc
-
-.macro dct_8x16_core_lsx
-    dct_8x8_core_lsx vr0, vr2, vr4, vr6, vr19, vr25, vr27, vr29, \
-                     vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, no_rect2
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       32        // 401
-    vldrepl.w     vr21,     t0,       36        // 4076
-    vmul_vmadd_w vr1, vr30, vr21, vr20, vr0, vr10
-    vssrarni.h.w  vr10,     vr0,      12        // t15a
-    vmul_vmsub_w vr1, vr30, vr20, vr21, vr0, vr29
-    vssrarni.h.w  vr29,     vr0,      12        // t8a
-
-    vldrepl.w     vr20,     t0,       40        // 3166 -> 1583
-    vldrepl.w     vr21,     t0,       44        // 2598 -> 1299
-    vmul_vmadd_w vr24, vr7, vr21, vr20, vr0, vr30
-    vssrarni.h.w  vr30,     vr0,      12        // t14a
-    vmul_vmsub_w vr24, vr7, vr20, vr21, vr0, vr31
-    vssrarni.h.w  vr31,     vr0,      12        // t9a
-
-    vldrepl.w     vr20,     t0,       48        // 1931
-    vldrepl.w     vr21,     t0,       52        // 3612
-    vmul_vmadd_w vr5, vr26, vr21, vr20, vr0, vr24
-    vssrarni.h.w  vr24,     vr0,      12        // t13a
-    vmul_vmsub_w vr5, vr26, vr20, vr21, vr0, vr25
-    vssrarni.h.w  vr25,     vr0,      12        // t10a
-
-    vldrepl.w     vr20,     t0,       56        // 3920
-    vldrepl.w     vr21,     t0,       60        // 1189
-    vmul_vmadd_w vr28, vr3, vr21, vr20, vr0, vr26
-    vssrarni.h.w  vr26,     vr0,      12        // t12a
-    vmul_vmsub_w vr28, vr3, vr20, vr21, vr0, vr27
-    vssrarni.h.w  vr27,     vr0,      12        // t11a
-
-    // vr22 vr23 vr30 vr31 vr24 vr25 vr26 vr27
-    vsadd.h       vr28,     vr29,      vr31     // t8
-    vssub.h       vr19,     vr29,      vr31     // t9
-    vssub.h       vr29,     vr27,      vr25     // t10
-    vsadd.h       vr9,      vr27,      vr25     // t11
-    vsadd.h       vr31,     vr26,      vr24     // t12
-    vssub.h       vr25,     vr26,      vr24     // t13
-    vssub.h       vr27,     vr10,      vr30     // t14
-    vsadd.h       vr24,     vr10,      vr30     // t15
-
-    vldrepl.w     vr20,     t0,       8         // 1567
-    vldrepl.w     vr21,     t0,       12        // 3784
-    vmul_vmadd_w vr27, vr19, vr21, vr20, vr0, vr26
-    vssrarni.h.w  vr26,     vr0,       12       // t14a
-    vmul_vmsub_w vr27, vr19, vr20, vr21, vr0, vr30
-    vssrarni.h.w  vr30,     vr0,       12       // t9a
-
-    vmul_vmadd_w vr25, vr29, vr21, vr20, vr0, vr19
-    vneg.w        vr0,      vr0
-    vneg.w        vr19,     vr19
-    vssrarni.h.w  vr19,     vr0,       12       // t10a
-    vmul_vmsub_w vr25, vr29, vr20, vr21, vr0, vr27
-    vssrarni.h.w  vr27,     vr0,       12       // t13a
-
-    vsadd.h       vr25,     vr28,     vr9       // t8a
-    vssub.h       vr29,     vr28,     vr9       // t11a
-    vssub.h       vr28,     vr24,     vr31      // t12a
-    vsadd.h       vr10,     vr24,     vr31      // t15a
-    vsadd.h       vr9,      vr30,     vr19      // t9
-    vssub.h       vr31,     vr30,     vr19      // t10
-    vssub.h       vr30,     vr26,     vr27      // t13
-    vsadd.h       vr24,     vr26,     vr27      // t14
-
-    vldrepl.w     vr20,     t0,       0         // 2896
-    vmul_vmadd_w vr30, vr31, vr20, vr20, vr0, vr26
-    vssrarni.h.w  vr26,     vr0,      12        // t13a
-    vmul_vmsub_w vr30, vr31, vr20, vr20, vr0, vr27
-    vssrarni.h.w  vr27,     vr0,      12        // t10a
-
-    vmul_vmadd_w vr28, vr29, vr20, vr20, vr0, vr31
-    vssrarni.h.w  vr31,     vr0,      12        // t12
-    vmul_vmsub_w vr28, vr29, vr20, vr20, vr0, vr30
-    vssrarni.h.w  vr30,     vr0,      12        // t11
-
-    // vr11 vr12 ... vr18
-    vsadd.h       vr28,     vr14,     vr31      // c[3]
-    vssub.h       vr29,     vr14,     vr31      // c[12]
-    vsadd.h       vr20,     vr15,     vr30      // c[4]
-    vssub.h       vr21,     vr15,     vr30      // c[11]
-    vsadd.h       vr14,     vr16,     vr27      // c[5]
-    vssub.h       vr23,     vr16,     vr27      // c[10]
-    vsadd.h       vr15,     vr17,     vr9       // c[6]
-    vssub.h       vr30,     vr17,     vr9       // c[9]
-    vsadd.h       vr16,     vr18,     vr25      // c[7]
-    vssub.h       vr27,     vr18,     vr25      // c[8]
-    vsadd.h       vr17,     vr13,     vr26      // c[2]
-    vssub.h       vr26,     vr13,     vr26      // c[13]
-    vsadd.h       vr18,     vr12,     vr24      // c[1]
-    vssub.h       vr25,     vr12,     vr24      // c[14]
-    vsadd.h       vr22,     vr11,     vr10      // c[0]
-    vssub.h       vr24,     vr11,     vr10      // c[15]
-.endm
-
-function inv_txfm_add_dct_dct_8x16_8bpc_lsx
-    bnez          a3,       .NO_HAS_DCONLY_8x16
-
-    ld.h          t2,       a2,       0      // dc
-    vldi          vr0,      0x8b5            // 181
-    vreplgr2vr.w  vr1,      t2
-    vldi          vr5,      0x880            // 128
-    vmul.w        vr2,      vr0,      vr1    // dc * 181
-    st.h          zero,     a2,       0
-    vsrari.w      vr2,      vr2,      8      // (dc * 181 + 128) >> 8
-    vld           vr10,     a0,       0      // 0 1 2 3 4 5 6 7
-    vmul.w        vr2,      vr0,      vr2
-    vsrari.w      vr2,      vr2,      8      // (dc * 181 + 128) >> 8
-    vsrari.w      vr2,      vr2,      1      // (dc + rnd) >> shift
-    vldx          vr11,     a0,       a1     // 8 9 10 11 12 13 14 15
-    alsl.d        t2,       a1,       a0,    1
-    vmadd.w       vr5,      vr2,      vr0
-    vld           vr12,     t2,       0      // 16 17 18 19 20 21 22 23
-    vssrarni.h.w  vr5,      vr5,      12
-    vldx          vr13,     t2,       a1     // 24 25 26 27 28 29 30 31
-
-    DST_ADD_W8 vr10, vr11, vr12, vr13, vr5, vr5, vr5, vr5
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr5, vr5, vr5, vr5
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr5, vr5, vr5, vr5
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr5, vr5, vr5, vr5
-
-    b             .DCT_DCT_8X16_END
-
-.NO_HAS_DCONLY_8x16:
-    addi.d        sp,       sp,       -64
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-    fst.d         f28,      sp,       32
-    fst.d         f29,      sp,       40
-    fst.d         f30,      sp,       48
-    fst.d         f31,      sp,       56
-
-    vld_x8 a2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    la.local      t0,       idct_coeffs
-
-    dct_8x8_core_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-                     vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, rect2_lsx
-
-    vld_x8 a2, 16, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    dct_8x8_core_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-                     vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30, rect2_lsx
-
-.irp i, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-        vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-    vsrari.h      \i,       \i,       1
-.endr
-
-    vreplgr2vr.h  vr23,     zero
-
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240
-    vst           vr23,     a2,       \i
-.endr
-
-    LSX_TRANSPOSE8x8_H vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-                       vr8, vr9, vr10, vr20, vr21, vr22, vr23, vr31
-
-    LSX_TRANSPOSE8x8_H vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30, \
-                       vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30, \
-                       vr8, vr9, vr10, vr20, vr21, vr22, vr23, vr31
-
-    dct_8x16_core_lsx
-
-.irp i, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-        vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-    vsrari.h     \i,       \i,       4
-.endr
-
-    alsl.d        t2,       a1,       a0,    1
-
-    VLD_DST_ADD_W8 vr22, vr18, vr17, vr28
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-
-    VLD_DST_ADD_W8 vr20, vr14, vr15, vr16
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-
-    VLD_DST_ADD_W8 vr27, vr30, vr23, vr21
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-
-    VLD_DST_ADD_W8 vr29, vr26, vr25, vr24
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    fld.d         f28,      sp,       32
-    fld.d         f29,      sp,       40
-    fld.d         f30,      sp,       48
-    fld.d         f31,      sp,       56
-    addi.d        sp,       sp,       64
-.DCT_DCT_8X16_END:
-endfunc
-
-.macro identity_8x8_core_lsx in0, in1, in2, in3, in4, in5, in6, in7, rect2
-
-    la.local      t0,       idct_coeffs
-
-.ifc \rect2, rect2_lsx
-    vldrepl.w     vr23,      t0,       0       // 2896
-.irp i, \in0, \in1, \in2, \in3, \in4, \in5, \in6, \in7
-    rect2_lsx \i, vr23, \i
-.endr
-.endif
-    vsllwil.w.h   vr8,      \in0,     1
-    vsllwil.w.h   vr9,      \in1,     1
-    vsllwil.w.h   vr10,     \in2,     1
-    vsllwil.w.h   vr11,     \in3,     1
-    vsllwil.w.h   vr12,     \in4,     1
-    vsllwil.w.h   vr13,     \in5,     1
-    vsllwil.w.h   vr14,     \in6,     1
-    vsllwil.w.h   vr15,     \in7,     1
-
-.irp i, \in0, \in1, \in2, \in3, \in4, \in5, \in6, \in7
-    vexth.w.h     \i,       \i
-.endr
-
-.irp i, \in0, \in1, \in2, \in3, \in4, \in5, \in6, \in7
-    vslli.w       \i,       \i,       1
-.endr
-
-    vssrarni.h.w  \in0,     vr8,      1
-    vssrarni.h.w  \in1,     vr9,      1
-    vssrarni.h.w  \in2,     vr10,     1
-    vssrarni.h.w  \in3,     vr11,     1
-    vssrarni.h.w  \in4,     vr12,     1
-    vssrarni.h.w  \in5,     vr13,     1
-    vssrarni.h.w  \in6,     vr14,     1
-    vssrarni.h.w  \in7,     vr15,     1
-.endm
-
-.macro identity_8x16_core_lsx in0, out0
-    vsadd.h       vr10,     \in0,     \in0
-    vsllwil.w.h   vr8,      \in0,     0
-    vexth.w.h     \out0,    \in0
-    vmul.w        vr8,      vr8,      vr20
-    vmul.w        \out0,    \out0,    vr20
-    vssrarni.h.w  \out0,    vr8,      11
-    vsadd.h       \out0,    \out0,    vr10
-.endm
-
-function inv_txfm_add_identity_identity_8x16_8bpc_lsx
-    addi.d        sp,       sp,       -64
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-    fst.d         f28,      sp,       32
-    fst.d         f29,      sp,       40
-    fst.d         f30,      sp,       48
-    fst.d         f31,      sp,       56
-
-    vld_x8 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    identity_8x8_core_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, rect2_lsx
-
-    vld_x8 a2, 128, 16, vr16, vr17, vr18, vr19, vr24, vr25, vr26, vr27
-
-    identity_8x8_core_lsx vr16, vr17, vr18, vr19, vr24, vr25, vr26, vr27, rect2_lsx
-
-    vreplgr2vr.h  vr23,     zero
-
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240
-    vst           vr23,     a2,       \i
-.endr
-
-
-    LSX_TRANSPOSE8x8_H vr0, vr2, vr4, vr6, vr16, vr18, vr24, vr26, \
-                       vr14, vr15, vr22, vr23, vr16, vr18, vr24, vr26, \
-                       vr8, vr9, vr10, vr11, vr12, vr13, vr20, vr21
-
-    LSX_TRANSPOSE8x8_H vr1, vr3, vr5, vr7, vr17, vr19, vr25, vr27, \
-                       vr28, vr29, vr30, vr31, vr17, vr19, vr25, vr27, \
-                       vr8, vr9, vr10, vr11, vr12, vr13, vr20, vr21
-
-    li.w          t0,       1697
-    vreplgr2vr.w  vr20,     t0
-
-.irp i, vr14, vr15, vr22, vr23, vr16, vr18, vr24, vr26, \
-        vr28, vr29, vr30, vr31, vr17, vr19, vr25, vr27
-    identity_8x16_core_lsx \i, \i
-    vsrari.h      \i,       \i,       4
-.endr
-
-    alsl.d        t2,       a1,       a0,    1
-
-    VLD_DST_ADD_W8 vr14, vr15, vr22, vr23
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-
-    VLD_DST_ADD_W8 vr16, vr18, vr24, vr26
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-
-    VLD_DST_ADD_W8 vr28, vr29, vr30, vr31
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-
-    VLD_DST_ADD_W8 vr17, vr19, vr25, vr27
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    fld.d         f28,      sp,       32
-    fld.d         f29,      sp,       40
-    fld.d         f30,      sp,       48
-    fld.d         f31,      sp,       56
-    addi.d        sp,       sp,       64
-endfunc
-
-.macro adst_8x8_core_lsx in0, in1, in2, in3, in4, in5, in6, in7, out0, out1, \
-                         out2, out3, out4, out5, out6, out7, rect2
-
-    la.local      t0,       iadst8_coeffs
-
-.ifc \rect2, rect2_lsx
-    vldrepl.w     vr23,      t0,       32       // 2896
-.irp i, \in0, \in1, \in2, \in3, \in4, \in5, \in6, \in7
-    rect2_lsx \i, vr23, \i
-.endr
-.endif
-
-    vldrepl.w     vr20,     t0,       0         // 4076
-    vldrepl.w     vr21,     t0,       4         // 401
-
-    vmul_vmadd_w vr7, vr0, vr20, vr21, vr8, vr9
-    vssrarni.h.w  vr9,      vr8,      12        // t0a low
-    vmul_vmsub_w vr7, vr0, vr21, vr20, vr8, vr10
-    vssrarni.h.w  vr10,     vr8,      12        // t1a low
-
-    vldrepl.w     vr20,     t0,       8         // 3612
-    vldrepl.w     vr21,     t0,       12        // 1931
-    vmul_vmadd_w vr5, vr2, vr20, vr21, vr8, vr0
-    vssrarni.h.w  vr0,      vr8,      12        // t2a low
-    vmul_vmsub_w vr5, vr2, vr21, vr20, vr8, vr7
-    vssrarni.h.w  vr7,      vr8,      12        // t3a low
-
-    vldrepl.w     vr20,     t0,       16        // 2598 -> 1299
-    vldrepl.w     vr21,     t0,       20        // 3166 -> 1583
-    vmul_vmadd_w vr3, vr4, vr20, vr21, vr8, vr2
-    vssrarni.h.w  vr2,      vr8,      12        // t4a low
-    vmul_vmsub_w vr3, vr4, vr21, vr20, vr8, vr5
-    vssrarni.h.w  vr5,      vr8,      12        // t5a low
-
-    vldrepl.w     vr20,     t0,       24        // 1189
-    vldrepl.w     vr21,     t0,       28        // 3920
-    vmul_vmadd_w vr1, vr6, vr20, vr21, vr8, vr3
-    vssrarni.h.w  vr3,      vr8,      12        // t6a low
-    vmul_vmsub_w vr1, vr6, vr21, vr20, vr8, vr4
-    vssrarni.h.w  vr4,      vr8,      12        // t7a low
-
-    vsadd.h       vr1,      vr9,      vr2       // t0
-    vssub.h       vr6,      vr9,      vr2       // t4
-    vsadd.h       vr8,      vr10,     vr5       // t1
-    vssub.h       vr2,      vr10,     vr5       // t5
-    vsadd.h       vr9,      vr0,      vr3       // t2
-    vssub.h       vr5,      vr0,      vr3       // t6
-    vsadd.h       vr10,     vr7,      vr4       // t3
-    vssub.h       vr0,      vr7,      vr4       // t7
-
-    vldrepl.w     vr20,     t0,       40        // 1567
-    vldrepl.w     vr21,     t0,       44        // 3784
-    vmul_vmadd_w vr6, vr2, vr21, vr20, vr3, vr4
-    vssrarni.h.w  vr4,      vr3,      12        // t4a low
-    vmul_vmsub_w vr6, vr2, vr20, vr21, vr3, vr7
-    vssrarni.h.w  vr7,      vr3,      12        // t5a low
-
-    vmul_vmadd_w vr0, vr5, vr20, vr21, vr3, vr2
-    vssrarni.h.w  vr2,      vr3,      12        // t7a low
-    vmul_vmsub_w vr0, vr5, vr21, vr20, vr3, vr6
-    vssrarni.h.w  vr6,      vr3,      12        // t6a low
-
-    vsadd.h       \out0,    vr1,      vr9       // out[0]
-    vssub.h       vr5,      vr1,      vr9       // t2
-    vsadd.h       vr3,      vr8,      vr10      // out[7]
-    vssub.h       vr1,      vr8,      vr10      // t3
-    vexth.w.h     vr9,      vr3
-    vsllwil.w.h   vr21,     vr3,      0
-    vneg.w        \out7,    vr9
-    vneg.w        vr21,     vr21
-    vssrarni.h.w  \out7,    vr21,     0         // out[7]
-
-    vsadd.h       vr8,      vr4,      vr6       // out[1]
-    vssub.h       vr10,     vr4,      vr6       // t6
-    vexth.w.h     vr20,     vr8
-    vsllwil.w.h   vr21,     vr8,      0
-    vneg.w        \out1,    vr20
-    vneg.w        vr21,     vr21
-    vssrarni.h.w  \out1,    vr21,     0         // out[1]
-    vsadd.h       \out6,    vr7,      vr2       // out[6]
-    vssub.h       vr4,      vr7,      vr2       // t7
-
-    vldrepl.w     vr20,     t0,       32        // 2896
-    vmul_vmadd_w vr5, vr1, vr20, vr20, vr9, vr6
-    vssrarni.h.w  vr6,      vr9,      12        // out[3]
-    vmul_vmsub_w vr5, vr1, vr20, vr20, vr9, \out4
-    vssrarni.h.w  \out4,    vr9,      12        // out[4]
-
-    vmul_vmadd_w vr10, vr4, vr20, vr20, vr9, \out2
-    vssrarni.h.w  \out2,    vr9,      12        // out[2]
-    vmul_vmsub_w vr10, vr4, vr20, vr20, vr9, vr5
-    vssrarni.h.w  vr5,      vr9,      12        // out[5]
-
-    vexth.w.h     vr20,     vr6
-    vsllwil.w.h   vr21,     vr6,      0
-    vneg.w        \out3,    vr20
-    vneg.w        vr21,     vr21
-    vssrarni.h.w  \out3,    vr21,     0         // out[3]
-
-    vexth.w.h     vr20,     vr5
-    vsllwil.w.h   vr21,     vr5,      0
-    vneg.w        \out5,    vr20
-    vneg.w        vr21,     vr21
-    vssrarni.h.w  \out5,    vr21,     0         // out[5]
-.endm
-
-function inv_txfm_add_adst_dct_8x16_8bpc_lsx
-    addi.d        sp,       sp,       -64
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-    fst.d         f28,      sp,       32
-    fst.d         f29,      sp,       40
-    fst.d         f30,      sp,       48
-    fst.d         f31,      sp,       56
-
-    vld_x8 a2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    adst_8x8_core_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-                      vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, rect2_lsx
-
-    vld_x8 a2, 16, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    adst_8x8_core_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-                      vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30, rect2_lsx
-
-.irp i, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-        vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-    vsrari.h     \i,       \i,       1
-.endr
-
-    vreplgr2vr.h  vr23,     zero
-
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240
-    vst           vr23,     a2,       \i
-.endr
-
-    LSX_TRANSPOSE8x8_H vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-                       vr8, vr9, vr10, vr20, vr21, vr22, vr23, vr31
-
-    LSX_TRANSPOSE8x8_H vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30, \
-                       vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30, \
-                       vr8, vr9, vr10, vr20, vr21, vr22, vr23, vr31
-
-    dct_8x8_core_lsx vr0, vr2, vr4, vr6, vr19, vr25, vr27, vr29, \
-                     vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, no_rect2
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       32        // 401
-    vldrepl.w     vr21,     t0,       36        // 4076
-    vmul_vmadd_w vr1, vr30, vr21, vr20, vr0, vr10
-    vssrarni.h.w  vr10,     vr0,      12        // t15a
-    vmul_vmsub_w vr1, vr30, vr20, vr21, vr0, vr29
-    vssrarni.h.w  vr29,     vr0,      12        // t8a
-
-    vldrepl.w     vr20,     t0,       40        // 3166 -> 1583
-    vldrepl.w     vr21,     t0,       44        // 2598 -> 1299
-    vmul_vmadd_w vr24, vr7, vr21, vr20, vr0, vr30
-    vssrarni.h.w  vr30,     vr0,      12        // t14a
-    vmul_vmsub_w vr24, vr7, vr20, vr21, vr0, vr31
-    vssrarni.h.w  vr31,     vr0,      12        // t9a
-
-    vldrepl.w     vr20,     t0,       48        // 1931
-    vldrepl.w     vr21,     t0,       52        // 3612
-    vmul_vmadd_w vr5, vr26, vr21, vr20, vr0, vr24
-    vssrarni.h.w  vr24,     vr0,      12        // t13a
-    vmul_vmsub_w vr5, vr26, vr20, vr21, vr0, vr25
-    vssrarni.h.w  vr25,     vr0,      12        // t10a
-
-    vldrepl.w     vr20,     t0,       56        // 3920
-    vldrepl.w     vr21,     t0,       60        // 1189
-    vmul_vmadd_w vr28, vr3, vr21, vr20, vr0, vr26
-    vssrarni.h.w  vr26,     vr0,      12        // t12a
-    vmul_vmsub_w vr28, vr3, vr20, vr21, vr0, vr27
-    vssrarni.h.w  vr27,     vr0,      12        // t11a
-
-    // vr22 vr23 vr30 vr31 vr24 vr25 vr26 vr27
-    vsadd.h       vr28,     vr29,      vr31     // t8
-    vssub.h       vr19,     vr29,      vr31     // t9
-    vssub.h       vr29,     vr27,      vr25     // t10
-    vsadd.h       vr9,      vr27,      vr25     // t11
-    vsadd.h       vr31,     vr26,      vr24     // t12
-    vssub.h       vr25,     vr26,      vr24     // t13
-    vssub.h       vr27,     vr10,      vr30     // t14
-    vsadd.h       vr24,     vr10,      vr30     // t15
-
-    vldrepl.w     vr20,     t0,       8         // 1567
-    vldrepl.w     vr21,     t0,       12        // 3784
-    vmul_vmadd_w vr27, vr19, vr21, vr20, vr0, vr26
-    vssrarni.h.w  vr26,     vr0,       12       // t14a
-    vmul_vmsub_w vr27, vr19, vr20, vr21, vr0, vr30
-    vssrarni.h.w  vr30,     vr0,       12       // t9a
-
-    vmul_vmadd_w vr25, vr29, vr21, vr20, vr0, vr19
-    vneg.w        vr0,      vr0
-    vneg.w        vr19,     vr19
-    vssrarni.h.w  vr19,     vr0,       12       // t10a
-    vmul_vmsub_w vr25, vr29, vr20, vr21, vr0, vr27
-    vssrarni.h.w  vr27,     vr0,       12       // t13a
-
-    vsadd.h       vr25,     vr28,     vr9       // t8a
-    vssub.h       vr29,     vr28,     vr9       // t11a
-    vssub.h       vr28,     vr24,     vr31      // t12a
-    vsadd.h       vr10,     vr24,     vr31      // t15a
-    vsadd.h       vr9,      vr30,     vr19      // t9
-    vssub.h       vr31,     vr30,     vr19      // t10
-    vssub.h       vr30,     vr26,     vr27      // t13
-    vsadd.h       vr24,     vr26,     vr27      // t14
-
-    vldrepl.w     vr20,     t0,       0         // 2896
-    vmul_vmadd_w vr30, vr31, vr20, vr20, vr0, vr26
-    vssrarni.h.w  vr26,     vr0,      12        // t13a
-    vmul_vmsub_w vr30, vr31, vr20, vr20, vr0, vr27
-    vssrarni.h.w  vr27,     vr0,      12        // t10a
-
-    vmul_vmadd_w vr28, vr29, vr20, vr20, vr0, vr31
-    vssrarni.h.w  vr31,     vr0,      12        // t12
-    vmul_vmsub_w vr28, vr29, vr20, vr20, vr0, vr30
-    vssrarni.h.w  vr30,     vr0,      12        // t11
-
-    // vr11 vr12 ... vr18
-    vsadd.h       vr28,     vr14,     vr31      // c[3]
-    vssub.h       vr29,     vr14,     vr31      // c[12]
-    vsadd.h       vr20,     vr15,     vr30      // c[4]
-    vssub.h       vr21,     vr15,     vr30      // c[11]
-    vsadd.h       vr14,     vr16,     vr27      // c[5]
-    vssub.h       vr23,     vr16,     vr27      // c[10]
-    vsadd.h       vr15,     vr17,     vr9       // c[6]
-    vssub.h       vr30,     vr17,     vr9       // c[9]
-    vsadd.h       vr16,     vr18,     vr25      // c[7]
-    vssub.h       vr27,     vr18,     vr25      // c[8]
-    vsadd.h       vr17,     vr13,     vr26      // c[2]
-    vssub.h       vr26,     vr13,     vr26      // c[13]
-    vsadd.h       vr18,     vr12,     vr24      // c[1]
-    vssub.h       vr25,     vr12,     vr24      // c[14]
-    vsadd.h       vr22,     vr11,     vr10      // c[0]
-    vssub.h       vr24,     vr11,     vr10      // c[15]
-
-.irp i, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-        vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-    vsrari.h     \i,       \i,       4
-.endr
-
-    alsl.d        t2,       a1,       a0,    1
-
-    VLD_DST_ADD_W8 vr22, vr18, vr17, vr28
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-
-    VLD_DST_ADD_W8 vr20, vr14, vr15, vr16
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-
-    VLD_DST_ADD_W8 vr27, vr30, vr23, vr21
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-
-    VLD_DST_ADD_W8 vr29, vr26, vr25, vr24
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    fld.d         f28,      sp,       32
-    fld.d         f29,      sp,       40
-    fld.d         f30,      sp,       48
-    fld.d         f31,      sp,       56
-    addi.d        sp,       sp,       64
-endfunc
-
-const iadst16_coeffs, align=4
-    .word         4091, 201, 3973, 995
-    .word         3703, 1751, 3290, 2440
-    .word         2751, 3035, 2106, 3513
-    .word         1380, 3857, 601, 4052
-endconst
-
-.macro adst16_core_lsx transpose8x8, shift, vst
-    la.local      t0,       iadst16_coeffs
-    vldrepl.w     vr20,     t0,        0        // 4091
-    vldrepl.w     vr21,     t0,        4        // 201
-
-    vmul_vmadd_w vr15, vr0, vr20, vr21, vr16, vr18
-    vmul_vmsub_w vr15, vr0, vr21, vr20, vr17, vr19
-    vssrarni.h.w  vr18,     vr16,      12       // t0
-    vssrarni.h.w  vr19,     vr17,      12       // t1
-
-    vldrepl.w     vr20,     t0,        8        // 3973
-    vldrepl.w     vr21,     t0,        12       // 995
-    vmul_vmadd_w vr13, vr2, vr20, vr21, vr16, vr0
-    vmul_vmsub_w vr13, vr2, vr21, vr20, vr17, vr15
-    vssrarni.h.w  vr0,      vr16,      12       // t2
-    vssrarni.h.w  vr15,     vr17,      12       // t3
-
-    vldrepl.w     vr20,     t0,        16       // 3703
-    vldrepl.w     vr21,     t0,        20       // 1751
-    vmul_vmadd_w vr11, vr4, vr20, vr21, vr16, vr2
-    vmul_vmsub_w vr11, vr4, vr21, vr20, vr17, vr13
-    vssrarni.h.w  vr2,      vr16,      12       // t4
-    vssrarni.h.w  vr13,     vr17,      12       // t5
-
-    vldrepl.w     vr20,     t0,        24       // 3290 -> 1645
-    vldrepl.w     vr21,     t0,        28       // 2440 -> 1220
-    vmul_vmadd_w vr9, vr6, vr20, vr21, vr16, vr4
-    vmul_vmsub_w vr9, vr6, vr21, vr20, vr17, vr11
-    vssrarni.h.w  vr4,      vr16,      12       // t6
-    vssrarni.h.w  vr11,     vr17,      12       // t7
-
-    vldrepl.w     vr20,     t0,        32       // 2751
-    vldrepl.w     vr21,     t0,        36       // 3035
-    vmul_vmadd_w vr7, vr8, vr20, vr21, vr16, vr6
-    vmul_vmsub_w vr7, vr8, vr21, vr20, vr17, vr9
-    vssrarni.h.w  vr6,      vr16,      12       // t8
-    vssrarni.h.w  vr9,      vr17,      12       // t9
-
-    vldrepl.w     vr20,     t0,        40       // 2106
-    vldrepl.w     vr21,     t0,        44       // 3513
-    vmul_vmadd_w vr5, vr10, vr20, vr21, vr16, vr7
-    vmul_vmsub_w vr5, vr10, vr21, vr20, vr17, vr8
-    vssrarni.h.w  vr7,      vr16,      12       // t10
-    vssrarni.h.w  vr8,      vr17,      12       // t11
-
-    vldrepl.w     vr20,     t0,        48       // 1380
-    vldrepl.w     vr21,     t0,        52       // 3857
-    vmul_vmadd_w vr3, vr12, vr20, vr21, vr16, vr5
-    vmul_vmsub_w vr3, vr12, vr21, vr20, vr17, vr10
-    vssrarni.h.w  vr5,      vr16,      12       // t12
-    vssrarni.h.w  vr10,     vr17,      12       // t13
-
-    vldrepl.w     vr20,     t0,        56       // 601
-    vldrepl.w     vr21,     t0,        60       // 4052
-    vmul_vmadd_w vr1, vr14, vr20, vr21, vr16, vr3
-    vmul_vmsub_w vr1, vr14, vr21, vr20, vr17, vr12
-    vssrarni.h.w  vr3,      vr16,      12       // t14
-    vssrarni.h.w  vr12,     vr17,      12       // t15
-
-    vsadd.h       vr1,      vr18,      vr6      // t0a
-    vssub.h       vr14,     vr18,      vr6      // t8a
-    vsadd.h       vr16,     vr19,      vr9      // t1a
-    vssub.h       vr17,     vr19,      vr9      // t9a
-    vsadd.h       vr6,      vr0,       vr7      // t2a
-    vssub.h       vr18,     vr0,       vr7      // t10a
-    vsadd.h       vr9,      vr15,      vr8      // t3a
-    vssub.h       vr19,     vr15,      vr8      // t11a
-    vsadd.h       vr0,      vr2,       vr5      // t4a
-    vssub.h       vr7,      vr2,       vr5      // t12a
-    vsadd.h       vr8,      vr13,      vr10     // t5a
-    vssub.h       vr15,     vr13,      vr10     // t13a
-    vsadd.h       vr2,      vr4,       vr3      // t6a
-    vssub.h       vr5,      vr4,       vr3      // t14a
-    vsadd.h       vr10,     vr11,      vr12     // t7a
-    vssub.h       vr13,     vr11,      vr12     // t15a
-
-    la.local      t0,       idct_coeffs
-
-    vldrepl.w     vr20,     t0,        16       // 799
-    vldrepl.w     vr21,     t0,        20       // 4017
-    vmul_vmadd_w vr14, vr17, vr21, vr20, vr3, vr11
-    vmul_vmsub_w vr14, vr17, vr20, vr21, vr4, vr12
-    vssrarni.h.w  vr11,     vr3,       12       // t8
-    vssrarni.h.w  vr12,     vr4,       12       // t9
-
-    vmul_vmadd_w vr15, vr7, vr20, vr21, vr3, vr14
-    vmul_vmsub_w vr15, vr7, vr21, vr20, vr4, vr17
-    vssrarni.h.w  vr14,     vr3,       12       // t13
-    vssrarni.h.w  vr17,     vr4,       12       // t12
-
-    vldrepl.w     vr20,     t0,        24       // 3406
-    vldrepl.w     vr21,     t0,        28       // 2276
-    vmul_vmadd_w vr18, vr19, vr21, vr20, vr3, vr7
-    vmul_vmsub_w vr18, vr19, vr20, vr21, vr4, vr15
-    vssrarni.h.w  vr7,      vr3,       12       // t10
-    vssrarni.h.w  vr15,     vr4,       12       // t11
-
-    vmul_vmadd_w vr13, vr5, vr20, vr21, vr3, vr18
-    vmul_vmsub_w vr13, vr5, vr21, vr20, vr4, vr19
-    vssrarni.h.w  vr18,     vr3,       12       // t15
-    vssrarni.h.w  vr19,     vr4,       12       // t14
-
-    vsadd.h       vr5,      vr1,       vr0      // t0
-    vssub.h       vr13,     vr1,       vr0      // t4
-    vsadd.h       vr3,      vr16,      vr8      // t1
-    vssub.h       vr4,      vr16,      vr8      // t5
-    vsadd.h       vr0,      vr6,       vr2      // t2
-    vssub.h       vr1,      vr6,       vr2      // t6
-    vsadd.h       vr8,      vr9,       vr10     // t3
-    vssub.h       vr16,     vr9,       vr10     // t7
-    vsadd.h       vr2,      vr11,      vr17     // t8a
-    vssub.h       vr6,      vr11,      vr17     // t12a
-    vsadd.h       vr9,      vr12,      vr14     // t9a
-    vssub.h       vr10,     vr12,      vr14     // t13a
-    vsadd.h       vr11,     vr7,       vr19     // t10a
-    vssub.h       vr17,     vr7,       vr19     // t14a
-    vsadd.h       vr12,     vr15,      vr18     // t11a
-    vssub.h       vr14,     vr15,      vr18     // t15a
-
-    la.local      t0,       idct_coeffs
-
-    vldrepl.w     vr20,     t0,        8        // 1567
-    vldrepl.w     vr21,     t0,        12       // 3784
-    vmul_vmadd_w vr13, vr4, vr21, vr20, vr7, vr18
-    vmul_vmsub_w vr13, vr4, vr20, vr21, vr15, vr19
-    vssrarni.h.w  vr18,     vr7,       12       // t4a
-    vssrarni.h.w  vr19,     vr15,      12       // t5a
-
-    vmul_vmadd_w vr16, vr1, vr20, vr21, vr7, vr4
-    vmul_vmsub_w vr16, vr1, vr21, vr20, vr15, vr13
-    vssrarni.h.w  vr4,      vr7,       12       // t7a
-    vssrarni.h.w  vr13,     vr15,      12       // t6a
-
-    vmul_vmadd_w vr6, vr10, vr21, vr20, vr7, vr1
-    vmul_vmsub_w vr6, vr10, vr20, vr21, vr15, vr16
-    vssrarni.h.w  vr1,      vr7,       12       // t12
-    vssrarni.h.w  vr16,     vr15,      12       // t13
-
-    vmul_vmadd_w vr14, vr17, vr20, vr21, vr7, vr6
-    vmul_vmsub_w vr14, vr17, vr21, vr20, vr15, vr10
-    vssrarni.h.w  vr6,      vr7,       12       // t15
-    vssrarni.h.w  vr10,     vr15,      12       // t14
-
-    vsadd.h       vr14,     vr5,       vr0      // out[0]
-    vssub.h       vr17,     vr5,       vr0      // t2a
-    vssub.h       vr7,      vr3,       vr8      // t3a
-    vsadd.h       vr15,     vr3,       vr8      // out[15]
-    vsllwil.w.h   vr22,     vr15,      0
-    vexth.w.h     vr15,     vr15
-    vneg.w        vr22,     vr22
-    vneg.w        vr15,     vr15
-    vssrarni.h.w  vr15,     vr22,      0        // out[15]
-    vsadd.h       vr14,     vr5,       vr0      // out[0]
-    vssub.h       vr17,     vr5,       vr0      // t2a
-    vssub.h       vr7,      vr3,       vr8      // t3a
-
-    vsadd.h       vr3,      vr19,      vr4      // out[12]
-    vssub.h       vr8,      vr19,      vr4      // t7
-    vssub.h       vr0,      vr18,      vr13     // t6
-    vsadd.h       vr5,      vr18,      vr13     // out[3]
-    vsllwil.w.h   vr22,     vr5,       0
-    vexth.w.h     vr5,      vr5
-    vneg.w        vr22,     vr22
-    vneg.w        vr5,      vr5
-    vssrarni.h.w  vr5,      vr22,      0        // out[3]
-
-    vsadd.h       vr13,     vr9,       vr12     // out[14]
-    vssub.h       vr19,     vr9,       vr12     // t11
-    vssub.h       vr4,      vr2,       vr11     // t10
-    vsadd.h       vr18,     vr2,       vr11     // out[1]
-    vsllwil.w.h   vr22,     vr18,      0
-    vexth.w.h     vr18,     vr18
-    vneg.w        vr22,     vr22
-    vneg.w        vr18,     vr18
-    vssrarni.h.w  vr18,     vr22,      0        // out[1]
-
-    vsadd.h       vr2,      vr1,       vr10     // out[2]
-    vssub.h       vr11,     vr1,       vr10     // t14a
-    vssub.h       vr12,     vr16,      vr6      // t15a
-    vsadd.h       vr9,      vr16,      vr6      // out[13]
-    vsllwil.w.h   vr22,     vr9,       0
-    vexth.w.h     vr9,      vr9
-    vneg.w        vr22,     vr22
-    vneg.w        vr9,      vr9
-    vssrarni.h.w  vr9,      vr22,      0        // out[13]
-
-    vldrepl.w     vr20,     t0,        0        // 2896
-    vmul_vmadd_w vr17, vr7, vr20, vr20, vr6, vr10
-    vmul_vmsub_w vr17, vr7, vr20, vr20, vr16, vr1
-    vssrarni.h.w  vr10,     vr6,       12       // out[7]
-
-    vsllwil.w.h   vr7,      vr10,      0
-    vexth.w.h     vr10,     vr10
-    vneg.w        vr7,      vr7
-    vneg.w        vr10,     vr10
-    vssrarni.h.w  vr10,     vr7,       0
-    vssrarni.h.w  vr1,      vr16,      12       // out[8]
-
-    vmul_vmsub_w vr0, vr8, vr20, vr20, vr16, vr17
-    vmul_vmadd_w vr0, vr8, vr20, vr20, vr6, vr7
-    vssrarni.h.w  vr17,     vr16,      12       // out[11]
-
-    vsllwil.w.h   vr0,      vr17,      0
-    vexth.w.h     vr17,     vr17
-    vneg.w        vr0,      vr0
-    vneg.w        vr17,     vr17
-    vssrarni.h.w  vr17,     vr0,       0
-    vssrarni.h.w  vr7,      vr6,       12       // out[4]
-
-    vmul_vmsub_w vr4, vr19, vr20, vr20, vr16, vr0
-    vmul_vmadd_w vr4, vr19, vr20, vr20, vr6, vr8
-    vssrarni.h.w  vr0,      vr16,      12       // out[9]
-
-    vsllwil.w.h   vr4,      vr0,       0
-    vexth.w.h     vr0,      vr0
-    vneg.w        vr4,      vr4
-    vneg.w        vr0,      vr0
-    vssrarni.h.w  vr0,      vr4,       0
-    vssrarni.h.w  vr8,      vr6,       12       // out[6]
-
-    vmul_vmadd_w vr11, vr12, vr20, vr20, vr6, vr4
-    vmul_vmsub_w vr11, vr12, vr20, vr20, vr16, vr19
-    vssrarni.h.w  vr4,      vr6,       12       // out[5]
-
-    vsllwil.w.h   vr24,     vr4,       0
-    vexth.w.h     vr4,      vr4
-    vneg.w        vr24,     vr24
-    vneg.w        vr4,      vr4
-    vssrarni.h.w  vr4,      vr24,      0
-    vssrarni.h.w  vr19,     vr16,      12       // out[10]
-
-.ifnb \transpose8x8
-    LSX_TRANSPOSE8x8_H vr14, vr18, vr2, vr5, vr7, vr4, vr8, vr10, \
-                       vr14, vr18, vr2, vr5, vr7, vr4, vr8, vr10, \
-                       vr6, vr11, vr12, vr16, vr20, vr21, vr22, vr23
-
-    LSX_TRANSPOSE8x8_H vr1, vr0, vr19, vr17, vr3, vr9, vr13, vr15, \
-                       vr1, vr0, vr19, vr17, vr3, vr9, vr13, vr15, \
-                       vr6, vr11, vr12, vr16, vr20, vr21, vr22, vr23
-.endif
-
-.ifnb \shift
-.irp i, vr14, vr18, vr2, vr5, vr7, vr4, vr8, vr10, \
-    vr1, vr0, vr19, vr17, vr3, vr9, vr13, vr15
-    vsrari.h      \i,       \i,       \shift
-.endr
-.endif
-
-.ifnb \vst
-    vst_x16 t1, 0, 16, vr14, vr18, vr2, vr5, vr7, vr4, vr8, vr10, \
-            vr1, vr0, vr19, vr17, vr3, vr9, vr13, vr15
-.endif
-// out0 out1 out2 out3 out4 out5 out6 out7
-// vr14 vr18 vr2  vr5  vr7  vr4  vr8  vr10
-// out8 out9 out10 out11 out12 out13 out14 out15
-// vr1  vr0  vr19  vr17  vr3   vr9   vr13  vr15
-.endm // adst16_core_lsx
-
-.macro adst16_core_finish_lsx in0, in1, in2, in3, in4, in5, in6, in7
-    fld.d         f20,      t2,       0
-    fldx.d        f21,      t2,       a1
-    fld.d         f22,      t3,       0
-    fldx.d        f23,      t3,       a1
-
-    alsl.d        t2,       a1,       t2,     2
-    alsl.d        t3,       a1,       t3,     2
-
-    fld.d         f24,      t2,       0
-    fldx.d        f25,      t2,       a1
-    fld.d         f26,      t3,       0
-    fldx.d        f27,      t3,       a1
-
-.irp i, vr20, vr21, vr22, vr23, vr24, vr25, vr26, vr27
-    vsllwil.hu.bu \i,       \i,       0
-.endr
-
-.irp i, \in0, \in1, \in2, \in3, \in4, \in5, \in6, \in7
-    vsrari.h      \i,       \i,       4
-.endr
-
-    vadd.h        vr20,     vr20,     \in0
-    vadd.h        vr21,     vr21,     \in1
-    vadd.h        vr22,     vr22,     \in2
-    vadd.h        vr23,     vr23,     \in3
-    vadd.h        vr24,     vr24,     \in4
-    vadd.h        vr25,     vr25,     \in5
-    vadd.h        vr26,     vr26,     \in6
-    vadd.h        vr27,     vr27,     \in7
-
-    vssrani.bu.h  vr21,     vr20,     0
-    vssrani.bu.h  vr23,     vr22,     0
-    vssrani.bu.h  vr25,     vr24,     0
-    vssrani.bu.h  vr27,     vr26,     0
-
-    vstelm.d      vr21,     t4,       0,     0
-    vstelm.d      vr21,     t5,       0,     1
-
-    alsl.d        t4,       a1,       t4,    1
-    alsl.d        t5,       a1,       t5,    1
-    vstelm.d      vr23,     t4,       0,     0
-    vstelm.d      vr23,     t5,       0,     1
-
-    alsl.d        t4,       a1,       t4,    1
-    alsl.d        t5,       a1,       t5,    1
-    vstelm.d      vr25,     t4,       0,     0
-    vstelm.d      vr25,     t5,       0,     1
-
-    alsl.d        t4,       a1,       t4,    1
-    alsl.d        t5,       a1,       t5,    1
-    vstelm.d      vr27,     t4,       0,     0
-    vstelm.d      vr27,     t5,       0,     1
-
-.endm // adst16_core_finish_lsx
-
-function inv_txfm_add_dct_adst_8x16_8bpc_lsx
-    addi.d        sp,       sp,       -64
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-    fst.d         f28,      sp,       32
-    fst.d         f29,      sp,       40
-    fst.d         f30,      sp,       48
-    fst.d         f31,      sp,       56
-
-    vld_x8 a2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    la.local      t0,       idct_coeffs
-
-    dct_8x8_core_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-                     vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, rect2_lsx
-
-    vld_x8 a2, 16, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    dct_8x8_core_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-                     vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30, rect2_lsx
-
-.irp i, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-        vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-    vsrari.h      \i,       \i,       1
-.endr
-
-    vreplgr2vr.h  vr23,     zero
-
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240
-    vst           vr23,     a2,       \i
-.endr
-
-    LSX_TRANSPOSE8x8_H vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-                       vr8, vr9, vr10, vr20, vr21, vr22, vr23, vr31
-
-    LSX_TRANSPOSE8x8_H vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30, \
-                       vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, \
-                       vr16, vr17, vr18, vr20, vr21, vr22, vr23, vr31
-
-    adst16_core_lsx , ,
-
-    addi.d        t2,       a0,       0
-    alsl.d        t3,       a1,       a0,     1
-    addi.d        t4,       a0,       0
-    add.d         t5,       a1,       a0
-
-    adst16_core_finish_lsx vr14, vr18, vr2, vr5, vr7, vr4, vr8, vr10
-
-    alsl.d        t2,       a1,       t2,    2
-    alsl.d        t3,       a1,       t3,    2
-
-    alsl.d        t4,       a1,       t4,    1
-    alsl.d        t5,       a1,       t5,    1
-
-    adst16_core_finish_lsx vr1, vr0, vr19, vr17, vr3, vr9, vr13, vr15
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    fld.d         f28,      sp,       32
-    fld.d         f29,      sp,       40
-    fld.d         f30,      sp,       48
-    fld.d         f31,      sp,       56
-    addi.d        sp,       sp,       64
-endfunc
-
-.macro malloc_space number
-    li.w          t0,       \number
-    sub.d         sp,       sp,       t0
-    addi.d        sp,       sp,       -64
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-    fst.d         f28,      sp,       32
-    fst.d         f29,      sp,       40
-    fst.d         f30,      sp,       48
-    fst.d         f31,      sp,       56
-.endm
-
-.macro free_space number
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    fld.d         f28,      sp,       32
-    fld.d         f29,      sp,       40
-    fld.d         f30,      sp,       48
-    fld.d         f31,      sp,       56
-    li.w          t0,       \number
-    add.d         sp,       sp,       t0
-    addi.d        sp,       sp,       64
-.endm
-
-.macro DST_ADD_W16 in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, in11
-    vsllwil.hu.bu vr10,     \in0,     0
-    vexth.hu.bu   vr0,      \in0
-    vsllwil.hu.bu vr11,     \in1,     0
-    vexth.hu.bu   vr1,      \in1
-    vsllwil.hu.bu vr12,     \in2,     0
-    vexth.hu.bu   vr2,      \in2
-    vsllwil.hu.bu vr13,     \in3,     0
-    vexth.hu.bu   vr3,      \in3
-    vadd.h        vr10,     vr10,     \in4
-    vadd.h        vr0,      vr0,      \in5
-    vadd.h        vr11,     vr11,     \in6
-    vadd.h        vr1,      vr1,      \in7
-    vadd.h        vr12,     vr12,     \in8
-    vadd.h        vr2,      vr2,      \in9
-    vadd.h        vr13,     vr13,     \in10
-    vadd.h        vr3,      vr3,      \in11
-    vssrani.bu.h  vr0,      vr10,     0
-    vssrani.bu.h  vr1,      vr11,     0
-    vssrani.bu.h  vr2,      vr12,     0
-    vssrani.bu.h  vr3,      vr13,     0
-    vst           vr0,      a0,       0
-    vstx          vr1,      a0,       a1
-    vst           vr2,      t2,       0
-    vstx          vr3,      t2,       a1
-.endm
-
-.macro VLD_DST_ADD_W16 in0, in1, in2, in3, in4, in5, in6, in7, shift
-
-.ifnb \shift
-.irp i, \in0, \in1, \in2, \in3, \in4, \in5, \in6, \in7
-    vsrari.h      \i,       \i,       \shift
-.endr
-.endif
-
-    vld           vr0,      a0,       0
-    vldx          vr1,      a0,       a1
-    vld           vr2,      t2,       0
-    vldx          vr3,      t2,       a1
-    DST_ADD_W16 vr0, vr1, vr2, vr3, \in0, \in1, \in2, \in3, \
-                \in4, \in5, \in6, \in7
-.endm
-
-function inv_txfm_add_dct_dct_16x8_8bpc_lsx
-    bnez          a3,       .NO_HAS_DCONLY_16x8
-
-    ld.h          t2,       a2,       0      // dc
-    vldi          vr0,      0x8b5            // 181
-    vreplgr2vr.w  vr1,      t2
-    vldi          vr5,      0x880            // 128
-    vmul.w        vr2,      vr0,      vr1    // dc * 181
-    st.h          zero,     a2,       0
-    vsrari.w      vr2,      vr2,      8      // (dc * 181 + 128) >> 8
-    alsl.d        t2,       a1,       a0,    1
-    vmul.w        vr2,      vr2,      vr0
-    vldx          vr1,      a0,       a1
-    vsrari.w      vr2,      vr2,      8
-    vldx          vr3,      t2,       a1
-    vsrari.w      vr2,      vr2,      1      // (dc + rnd) >> shift
-    vmadd.w       vr5,      vr2,      vr0
-    vld           vr0,      a0,       0
-    vssrarni.h.w  vr5,      vr5,      12
-    vld           vr2,      t2,       0
-
-    DST_ADD_W16 vr0, vr1, vr2, vr3, vr5, vr5, vr5, vr5, vr5, vr5, vr5, vr5
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W16 vr5, vr5, vr5, vr5, vr5, vr5, vr5, vr5,
-
-    b             .DCT_DCT_16x8_END
-
-.NO_HAS_DCONLY_16x8:
-    malloc_space 512
-
-    vld_x16 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    la.local      t0,       idct_coeffs
-
-    vldrepl.w     vr23,     t0,       0   //2896
-.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-    vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-    rect2_lsx \i, vr23, \i
-.endr
-
-    dct_8x16_core_lsx
-
-    LSX_TRANSPOSE8x8_H vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-                       vr0, vr18, vr17, vr28, vr11, vr14, vr15, vr16, \
-                       vr13, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    LSX_TRANSPOSE8x8_H vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24, \
-                       vr27, vr30, vr1, vr12, vr29, vr26, vr25, vr24, \
-                       vr13, vr31, vr2, vr3, vr4, vr5, vr6, vr7
-
-.irp i, vr0, vr18, vr17, vr28, vr11, vr14, vr15, vr16, \
-        vr27, vr30, vr1, vr12, vr29, vr26, vr25, vr24
-    vsrari.h       \i,       \i,       1
-.endr
-
-    vst_x16 sp, 64, 16, vr13, vr18, vr17, vr28, vr11, vr14, vr15, vr16, \
-            vr27, vr30, vr23, vr12, vr29, vr26, vr25, vr24
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240
-    vst           vr23,     a2,       \i
-.endr
-
-    dct_8x8_core_lsx vr0, vr18, vr17, vr28, vr11, vr14, vr15, vr16,  \
-                     vr4, vr5, vr6, vr16, vr7, vr18, vr19, vr31, no_rect2
-
-    dct_8x8_core_lsx vr27, vr30, vr1, vr12, vr29, vr26, vr25, vr24, \
-                     vr14, vr15, vr17, vr20, vr21, vr22, vr23, vr28, no_rect2
-
-    alsl.d        t2,       a1,       a0,    1
-    VLD_DST_ADD_W16 vr4, vr14, vr5, vr15, vr6, vr17, vr16, vr20, 4
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-    VLD_DST_ADD_W16 vr7, vr21, vr18, vr22, vr19, vr23, vr31, vr28, 4
-
-    free_space 512
-
-.DCT_DCT_16x8_END:
-
-endfunc
-
-function inv_txfm_add_adst_dct_16x8_8bpc_lsx
-    addi.d        sp,       sp,       -64
-    fst.d         f24,      sp,       0
-    fst.d         f25,      sp,       8
-    fst.d         f26,      sp,       16
-    fst.d         f27,      sp,       24
-    fst.d         f28,      sp,       32
-    fst.d         f29,      sp,       40
-    fst.d         f30,      sp,       48
-    fst.d         f31,      sp,       56
-
-    addi.d        t1,       sp,       64
-    addi.d        t2,       a2,       0
-
-    vld_x16 t2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
-
-    la.local      t0,       idct_coeffs
-
-    vldrepl.w     vr23,     t0,       0         //2896
-.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-     vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
-    rect2_lsx     \i,       vr23,     \i
-.endr
-
-    adst16_core_lsx , 1,
-
-    // out0 out1 out2 out3 out4 out5 out6 out7
-    // vr14 vr18 vr2  vr5  vr7  vr4  vr8  vr10
-    // out8 out9 out10 out11 out12 out13 out14 out15
-    // vr1  vr0  vr19  vr17  vr3   vr9   vr13  vr15
-
-    LSX_TRANSPOSE8x8_H vr14, vr18, vr2, vr5, vr7, vr4, vr8, vr10, \
-                       vr14, vr18, vr2, vr5, vr7, vr4, vr24, vr25, \
-                       vr6, vr11, vr12, vr16, vr20, vr21, vr22, vr23
-
-    LSX_TRANSPOSE8x8_H vr1, vr0, vr19, vr17, vr3, vr9, vr13, vr15, \
-                       vr1, vr0, vr19, vr17, vr3, vr26, vr13, vr15, \
-                       vr6, vr11, vr12, vr16, vr20, vr21, vr22, vr23
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240
-    vst           vr23,     a2,       \i
-.endr
-
-    dct_8x8_core_lsx vr14, vr18, vr2, vr5, vr7, vr4, vr24, vr25, \
-                     vr27, vr28, vr29, vr25, vr30, vr31, vr6, vr16, no_rect2
-
-    dct_8x8_core_lsx vr1, vr0, vr19, vr17, vr3, vr26, vr13, vr15, \
-                     vr5, vr7, vr18, vr20, vr21, vr22, vr23, vr24, no_rect2
-
-    alsl.d        t2,       a1,       a0,    1
-    VLD_DST_ADD_W16 vr27, vr5, vr28, vr7, vr29, vr18, vr25, vr20, 4
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-    VLD_DST_ADD_W16 vr30, vr21, vr31, vr22, vr6, vr23, vr16, vr24, 4
-
-    fld.d         f24,      sp,       0
-    fld.d         f25,      sp,       8
-    fld.d         f26,      sp,       16
-    fld.d         f27,      sp,       24
-    fld.d         f28,      sp,       32
-    fld.d         f29,      sp,       40
-    fld.d         f30,      sp,       48
-    fld.d         f31,      sp,       56
-    addi.d        sp,       sp,       64
-endfunc
-
-function inv_txfm_add_dct_dct_16x16_8bpc_lsx
-    bnez          a3,       .NO_HAS_DCONLY_16x16
-
-    ld.h          t2,       a2,       0      // dc
-    vldi          vr0,      0x8b5            // 181
-    vreplgr2vr.w  vr1,      t2
-    vldi          vr5,      0x880            // 128
-    vmul.w        vr2,      vr0,      vr1    // dc * 181
-    st.h          zero,     a2,       0
-    vsrari.w      vr2,      vr2,      8      // (dc * 181 + 128) >> 8
-    alsl.d        t2,       a1,       a0,    1
-    vsrari.w      vr2,      vr2,      2      // (dc + rnd) >> shift
-    vldx          vr1,      a0,       a1
-    vmadd.w       vr5,      vr2,      vr0
-    vldx          vr3,      t2,       a1
-    vssrarni.h.w  vr5,      vr5,      12
-    vld           vr0,      a0,       0
-    vld           vr2,      t2,       0
-
-    DST_ADD_W16 vr0, vr1, vr2, vr3, vr5, vr5, vr5, vr5, vr5, vr5, vr5, vr5
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W16 vr5, vr5, vr5, vr5, vr5, vr5, vr5, vr5,
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W16 vr5, vr5, vr5, vr5, vr5, vr5, vr5, vr5,
-
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W16 vr5, vr5, vr5, vr5, vr5, vr5, vr5, vr5,
-
-    b             .DCT_DCT_16x16_END
-
-.NO_HAS_DCONLY_16x16:
-
-    malloc_space 512
-
-    vld_x16 a2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    dct_8x16_core_lsx
-
-    LSX_TRANSPOSE8x8_H vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-                       vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    LSX_TRANSPOSE8x8_H vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24, \
-                       vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-.irp i, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-        vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-    vsrari.h       \i,       \i,       2
-.endr
-
-    vst_x16 sp, 64, 16, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-            vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-
-    vld_x16 a2, 16, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    dct_8x16_core_lsx
-
-    LSX_TRANSPOSE8x8_H vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-                       vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    LSX_TRANSPOSE8x8_H vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24, \
-                       vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-.irp i, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-        vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-    vsrari.h      \i,       \i,       2
-.endr
-
-    vst_x16 sp, 320, 16, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-            vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-
-    vreplgr2vr.h  vr31,     zero
-
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, \
-        240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, \
-        464, 480, 496
-    vst           vr31,     a2,       \i
-.endr
-
-    vld_x8 sp, 64, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-    vld_x8 sp, 320, 16, vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    dct_8x16_core_lsx
-
-    vst_x8 sp, 64, 16, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16
-    vst_x8 sp, 320, 16, vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-
-    vld_x8 sp, 192, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-    vld_x8 sp, 448, 16, vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    dct_8x16_core_lsx
-
-    alsl.d        t2,       a1,       a0,    1
-    vld           vr4,      sp,       64
-    vld           vr5,      sp,       80
-    vld           vr6,      sp,       96
-    vld           vr7,      sp,       112
-    VLD_DST_ADD_W16 vr4, vr22, vr5, vr18, vr6, vr17, vr7, vr28, 4
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-    vld           vr4,      sp,       128
-    vld           vr5,      sp,       144
-    vld           vr6,      sp,       160
-    vld           vr7,      sp,       176
-    VLD_DST_ADD_W16 vr4, vr20, vr5, vr14, vr6, vr15, vr7, vr16, 4
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-    vld           vr4,      sp,       320
-    vld           vr5,      sp,       336
-    vld           vr6,      sp,       352
-    vld           vr7,      sp,       368
-    VLD_DST_ADD_W16 vr4, vr27, vr5, vr30, vr6, vr23, vr7, vr21, 4
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-    vld           vr4,      sp,       384
-    vld           vr5,      sp,       400
-    vld           vr6,      sp,       416
-    vld           vr7,      sp,       432
-    VLD_DST_ADD_W16 vr4, vr29, vr5, vr26, vr6, vr25, vr7, vr24, 4
-
-    free_space 512
-
-.DCT_DCT_16x16_END:
-endfunc
-
-function inv_txfm_add_adst_adst_16x16_8bpc_lsx
-
-    malloc_space 256+256
-
-    addi.d        t1,       sp,        64
-    addi.d        t2,       a2,        0
-
-    vld_x16 t2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
-
-    adst16_core_lsx transpose8x8, 2, vst_x16
-
-    addi.d        t2,       a2,        16
-    addi.d        t1,       t1,        256
-
-    vld_x16 t2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
-
-    adst16_core_lsx transpose8x8, 2, vst_x16
-
-    vreplgr2vr.h  vr23,     zero
-
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, \
-        240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, \
-        464, 480, 496
-    vst           vr23,     a2,       \i
-.endr
-
-    addi.d        t2,       sp,       64
-
-    vld_x8 t2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-    vld_x8 t2, 256, 16, vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
-
-    adst16_core_lsx , ,
-
-    // out0 out1 out2 out3 out4 out5 out6 out7
-    // vr14 vr18 vr2  vr5  vr7  vr4  vr8  vr10
-    // out8 out9 out10 out11 out12 out13 out14 out15
-    // vr1  vr0  vr19  vr17  vr3   vr9   vr13  vr15
-
-    addi.d        t2,       a0,       0
-    alsl.d        t3,       a1,       a0,     1
-    addi.d        t4,       a0,       0
-    add.d         t5,       a1,       a0
-
-    adst16_core_finish_lsx vr14, vr18, vr2, vr5, vr7, vr4, vr8, vr10
-
-    alsl.d        t2,       a1,       t2,    2
-    alsl.d        t3,       a1,       t3,    2
-
-    alsl.d        t4,       a1,       t4,    1
-    alsl.d        t5,       a1,       t5,    1
-
-    adst16_core_finish_lsx vr1, vr0, vr19, vr17, vr3, vr9, vr13, vr15
-
-    addi.d        t2,       sp,       64+128
-
-    vld_x8 t2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-    vld_x8 t2, 256, 16, vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
-
-    adst16_core_lsx , ,
-
-    addi.d        a0,       a0,       8
-
-    addi.d        t2,       a0,       0
-    alsl.d        t3,       a1,       a0,    1
-    addi.d        t4,       a0,       0
-    add.d         t5,       a1,       a0
-
-    adst16_core_finish_lsx vr14, vr18, vr2, vr5, vr7, vr4, vr8, vr10
-
-    alsl.d        t2,       a1,       t2,    2
-    alsl.d        t3,       a1,       t3,    2
-
-    alsl.d        t4,       a1,       t4,    1
-    alsl.d        t5,       a1,       t5,    1
-
-    adst16_core_finish_lsx vr1, vr0, vr19, vr17, vr3, vr9, vr13, vr15
-
-    free_space 256+256
-endfunc
-
-function inv_txfm_add_adst_dct_16x16_8bpc_lsx
-    malloc_space 256+256
-
-    addi.d        t1,       sp,        64
-    addi.d        t2,       a2,        0
-
-    vld_x16 t2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
-
-    adst16_core_lsx transpose8x8, 2, vst_x16
-
-    addi.d        t2,       a2,        16
-    addi.d        t1,       t1,        256
-
-    vld_x16 t2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
-
-    adst16_core_lsx transpose8x8, 2, vst_x16
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, \
-        240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, \
-        464, 480, 496
-    vst           vr23,     a2,       \i
-.endr
-
-    addi.d        t2,       sp,       64
-
-    vld_x8 t2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-    vld_x8 t2, 256, 16, vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    dct_8x16_core_lsx
-
-    vst_x8 t2, 0, 16, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16
-    vst_x8 t2, 256, 16, vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-
-    addi.d        t2,       sp,       64+128
-
-    vld_x8 t2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-    vld_x8 t2, 256, 16, vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    dct_8x16_core_lsx
-
-    alsl.d        t2,       a1,       a0,    1
-    vld           vr4,      sp,       64
-    vld           vr5,      sp,       80
-    vld           vr6,      sp,       96
-    vld           vr7,      sp,       112
-    VLD_DST_ADD_W16 vr4, vr22, vr5, vr18, vr6, vr17, vr7, vr28, 4
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-    vld           vr4,      sp,       128
-    vld           vr5,      sp,       144
-    vld           vr6,      sp,       160
-    vld           vr7,      sp,       176
-    VLD_DST_ADD_W16 vr4, vr20, vr5, vr14, vr6, vr15, vr7, vr16, 4
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-    vld           vr4,      sp,       320
-    vld           vr5,      sp,       336
-    vld           vr6,      sp,       352
-    vld           vr7,      sp,       368
-    VLD_DST_ADD_W16 vr4, vr27, vr5, vr30, vr6, vr23, vr7, vr21, 4
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-    vld           vr4,      sp,       384
-    vld           vr5,      sp,       400
-    vld           vr6,      sp,       416
-    vld           vr7,      sp,       432
-    VLD_DST_ADD_W16 vr4, vr29, vr5, vr26, vr6, vr25, vr7, vr24, 4
-
-    free_space 256+256
-endfunc
-
-function inv_txfm_add_dct_adst_16x16_8bpc_lsx
-    malloc_space 256+256
-
-    vld_x16 a2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-           vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    dct_8x16_core_lsx
-
-    LSX_TRANSPOSE8x8_H vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-                       vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    LSX_TRANSPOSE8x8_H vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24, \
-                       vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-.irp i, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-        vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-    vsrari.h       \i,       \i,       2
-.endr
-
-    vst_x16 sp, 64, 16, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-            vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-
-    vld_x16 a2, 16, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-           vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    dct_8x16_core_lsx
-
-    LSX_TRANSPOSE8x8_H vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-                       vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    LSX_TRANSPOSE8x8_H vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24, \
-                       vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-.irp i, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-        vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-    vsrari.h      \i,       \i,       2
-.endr
-
-    vst_x16 sp, 320, 16, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-            vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-
-    vreplgr2vr.h  vr31,     zero
-
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, \
-        240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, \
-        464, 480, 496
-    vst           vr31,     a2,       \i
-.endr
-
-    addi.d        t2,       sp,       64
-
-    vld_x8 t2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-    vld_x8 t2, 256, 16, vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
-
-    adst16_core_lsx , ,
-
-    // out0 out1 out2 out3 out4 out5 out6 out7
-    // vr14 vr18 vr2  vr5  vr7  vr4  vr8  vr10
-    // out8 out9 out10 out11 out12 out13 out14 out15
-    // vr1  vr0  vr19  vr17  vr3   vr9   vr13  vr15
-
-    addi.d        t2,       a0,       0
-    alsl.d        t3,       a1,       a0,     1
-    addi.d        t4,       a0,       0
-    add.d         t5,       a1,       a0
-
-    adst16_core_finish_lsx vr14, vr18, vr2, vr5, vr7, vr4, vr8, vr10
-
-    alsl.d        t2,       a1,       t2,    2
-    alsl.d        t3,       a1,       t3,    2
-
-    alsl.d        t4,       a1,       t4,    1
-    alsl.d        t5,       a1,       t5,    1
-
-    adst16_core_finish_lsx vr1, vr0, vr19, vr17, vr3, vr9, vr13, vr15
-
-    addi.d        t2,       sp,       64+128
-
-    vld_x8 t2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-    vld_x8 t2, 256, 16, vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
-
-    adst16_core_lsx , ,
-
-    addi.d        a0,       a0,       8
-
-    addi.d        t2,       a0,       0
-    alsl.d        t3,       a1,       a0,    1
-    addi.d        t4,       a0,       0
-    add.d         t5,       a1,       a0
-
-    adst16_core_finish_lsx vr14, vr18, vr2, vr5, vr7, vr4, vr8, vr10
-
-    alsl.d        t2,       a1,       t2,    2
-    alsl.d        t3,       a1,       t3,    2
-
-    alsl.d        t4,       a1,       t4,    1
-    alsl.d        t5,       a1,       t5,    1
-
-    adst16_core_finish_lsx vr1, vr0, vr19, vr17, vr3, vr9, vr13, vr15
-
-    free_space 256+256
-endfunc
-
-const shufb
-    .byte 14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1
-endconst
-
-function inv_txfm_add_flipadst_dct_16x16_8bpc_lsx
-    malloc_space 256+256
-
-    addi.d        t1,       sp,        64
-    addi.d        t2,       a2,        0
-
-    vld_x16 t2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+.macro DST_ADD_W16 in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, in11
+    vsllwil.hu.bu vr4,      \in0,     0
+    vexth.hu.bu   vr0,      \in0
+    vsllwil.hu.bu vr5,      \in1,     0
+    vexth.hu.bu   vr1,      \in1
+    vsllwil.hu.bu vr6,      \in2,     0
+    vexth.hu.bu   vr2,      \in2
+    vsllwil.hu.bu vr7,      \in3,     0
+    vexth.hu.bu   vr3,      \in3
+    vadd.h        vr4,      vr4,      \in4
+    vadd.h        vr0,      vr0,      \in5
+    vadd.h        vr5,      vr5,      \in6
+    vadd.h        vr1,      vr1,      \in7
+    vadd.h        vr6,      vr6,      \in8
+    vadd.h        vr2,      vr2,      \in9
+    vadd.h        vr7,      vr7,      \in10
+    vadd.h        vr3,      vr3,      \in11
+    vssrani.bu.h  vr0,      vr4,      0
+    vssrani.bu.h  vr1,      vr5,      0
+    vssrani.bu.h  vr2,      vr6,      0
+    vssrani.bu.h  vr3,      vr7,      0
+    vst           vr0,      a0,       0
+    vstx          vr1,      a0,       a1
+    vst           vr2,      t2,       0
+    vstx          vr3,      t2,       a1
+.endm
 
-    adst16_core_lsx transpose8x8, 2, vst_x16
+.macro VLD_DST_ADD_W16 in0, in1, in2, in3, in4, in5, in6, in7
+    vld           vr0,      a0,       0
+    vldx          vr1,      a0,       a1
+    vld           vr2,      t2,       0
+    vldx          vr3,      t2,       a1
+    DST_ADD_W16 vr0, vr1, vr2, vr3, \in0, \in1, \in2, \in3, \
+                \in4, \in5, \in6, \in7
+.endm
 
-    addi.d        t2,       a2,        16
-    addi.d        t1,       t1,        256
+.macro def_fn_16x8 txfm1
+functionl inv_txfm_\txfm1\()add_16x8_lsx
+    PUSH_REG
 
-    vld_x16 t2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+    vld_x16 a2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
             vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
-
-    adst16_core_lsx transpose8x8, 2, vst_x16
-
-    vreplgr2vr.h  vr23,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, \
-        240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, \
-        464, 480, 496
+    vxor.v        vr23,     vr23,     vr23
+.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, \
+    176, 192, 208, 224, 240
     vst           vr23,     a2,       \i
 .endr
 
-    addi.d        t2,       sp,       64
-
-    vld_x8 t2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-    vld_x8 t2, 256, 16, vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    dct_8x16_core_lsx
-
-    la.local      t0,       shufb
-    vld           vr0,      t0,       0
-
-.irp i, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-     vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-    vshuf.b       \i,       \i,       \i,    vr0
-.endr
-
-    vst_x8 t2, 0, 16, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16
-    vst_x8 t2, 256, 16, vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-
-    addi.d        t2,       sp,       64+128
-
-    vld_x8 t2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-    vld_x8 t2, 256, 16, vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    dct_8x16_core_lsx
-
-    la.local      t0,       shufb
-    vld           vr0,      t0,       0
-
-.irp i, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-     vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-    vshuf.b       \i,       \i,       \i,    vr0
-.endr
-
-    alsl.d        t2,       a1,       a0,    1
-    vld           vr4,      sp,       64
-    vld           vr5,      sp,       80
-    vld           vr6,      sp,       96
-    vld           vr7,      sp,       112
-    VLD_DST_ADD_W16 vr22, vr4, vr18, vr5, vr17, vr6, vr28, vr7, 4
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-    vld           vr4,      sp,       128
-    vld           vr5,      sp,       144
-    vld           vr6,      sp,       160
-    vld           vr7,      sp,       176
-    VLD_DST_ADD_W16 vr20, vr4, vr14, vr5, vr15, vr6, vr16, vr7, 4
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-    vld           vr4,      sp,       320
-    vld           vr5,      sp,       336
-    vld           vr6,      sp,       352
-    vld           vr7,      sp,       368
-    VLD_DST_ADD_W16 vr27, vr4, vr30, vr5, vr23, vr6, vr21, vr7, 4
-
-    alsl.d        a0,       a1,       a0,    2
-    alsl.d        t2,       a1,       a0,    1
-    vld           vr4,      sp,       384
-    vld           vr5,      sp,       400
-    vld           vr6,      sp,       416
-    vld           vr7,      sp,       432
-    VLD_DST_ADD_W16 vr29, vr4, vr26, vr5, vr25, vr6, vr24, vr7, 4
-
-    free_space 256+256
-endfunc
-
-function inv_txfm_add_dct_flipadst_16x16_8bpc_lsx
-    malloc_space 256+256
-
-    vld_x16 a2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-           vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    dct_8x16_core_lsx
-
-    LSX_TRANSPOSE8x8_H vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-                       vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    LSX_TRANSPOSE8x8_H vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24, \
-                       vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-.irp i, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-        vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-    vsrari.h       \i,       \i,       2
-.endr
-
-    vst_x16 sp, 64, 16, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-            vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-
-    vld_x16 a2, 16, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-           vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    dct_8x16_core_lsx
-
-    LSX_TRANSPOSE8x8_H vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-                       vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    LSX_TRANSPOSE8x8_H vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24, \
-                       vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24, \
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-.irp i, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-        vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-    vsrari.h      \i,       \i,       2
-.endr
-
-    vst_x16 sp, 320, 16, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-            vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-
-    vreplgr2vr.h  vr31,     zero
-
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, \
-        240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, \
-        464, 480, 496
-    vst           vr31,     a2,       \i
-.endr
-
-    addi.d        t2,       sp,       64
-
-    vld_x8 t2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-    vld_x8 t2, 256, 16, vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
-
-    adst16_core_lsx , ,
-
-    // out0 out1 out2 out3 out4 out5 out6 out7
-    // vr14 vr18 vr2  vr5  vr7  vr4  vr8  vr10
-    // out8 out9 out10 out11 out12 out13 out14 out15
-    // vr1  vr0  vr19  vr17  vr3   vr9   vr13  vr15
-
-    la.local      t0,       shufb
-    vld           vr31,     t0,       0
-
-    addi.d        t2,       a0,       0
-    alsl.d        t3,       a1,       a0,     1
-    addi.d        t4,       a0,       0
-    add.d         t5,       a1,       a0
-
-    adst16_core_finish_lsx vr15, vr13, vr9, vr3, vr17, vr19, vr0, vr1
-
-    alsl.d        t2,       a1,       t2,    2
-    alsl.d        t3,       a1,       t3,    2
-
-    alsl.d        t4,       a1,       t4,    1
-    alsl.d        t5,       a1,       t5,    1
-
-    adst16_core_finish_lsx vr10, vr8, vr4, vr7, vr5, vr2, vr18, vr14
-
-    addi.d        t2,       sp,       64+128
-
-    vld_x8 t2, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-    vld_x8 t2, 256, 16, vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
-
-    adst16_core_lsx , ,
-
-    addi.d        a0,       a0,       8
-
-    la.local      t0,       shufb
-    vld           vr31,     t0,       0
-
-    addi.d        t2,       a0,       0
-    alsl.d        t3,       a1,       a0,    1
-    addi.d        t4,       a0,       0
-    add.d         t5,       a1,       a0
-
-    adst16_core_finish_lsx vr15, vr13, vr9, vr3, vr17, vr19, vr0, vr1
-
-    alsl.d        t2,       a1,       t2,    2
-    alsl.d        t3,       a1,       t3,    2
-
-    alsl.d        t4,       a1,       t4,    1
-    alsl.d        t5,       a1,       t5,    1
-
-    adst16_core_finish_lsx vr10, vr8, vr4, vr7, vr5, vr2, vr18, vr14
-
-    free_space 256+256
-
-endfunc
-
-function inv_txfm_add_dct_dct_8x32_8bpc_lsx
-    bnez          a3,       .NO_HAS_DCONLY_8x32
-
-    ld.h          t2,       a2,       0      // dc
-    vldi          vr0,      0x8b5            // 181
-    vreplgr2vr.w  vr1,      t2
-    vldi          vr5,      0x880            // 128
-    vmul.w        vr2,      vr0,      vr1    // dc * 181
-    st.h          zero,     a2,       0
-    vsrari.w      vr2,      vr2,      8      // (dc * 181 + 128) >> 8
-    vld           vr10,     a0,       0      // 0 1 2 3 4 5 6 7
-    vsrari.w      vr2,      vr2,      2      // (dc + rnd) >> shift
-    vldx          vr11,     a0,       a1     // 8 9 10 11 12 13 14 15
-    alsl.d        t2,       a1,       a0,    1
-    vmadd.w       vr5,      vr2,      vr0
-    vld           vr12,     t2,       0      // 16 17 18 19 20 21 22 23
-    vssrarni.h.w  vr5,      vr5,      12
-    vldx          vr13,     t2,       a1     // 24 25 26 27 28 29 30 31
-
-    DST_ADD_W8 vr10, vr11, vr12, vr13, vr5, vr5, vr5, vr5
-
-.rept 7
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       a0,     1
-
-    VLD_DST_ADD_W8 vr5, vr5, vr5, vr5
-.endr
-
-    b             .DCT_DCT_8X32_END
-
-.NO_HAS_DCONLY_8x32:
-    malloc_space 512
-
-    vld_x8 a2, 0, 64, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    la.local      t0,       idct_coeffs
-
-    dct_8x8_core_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-                     vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, no_rect2
-
-.irp i, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
-    vsrari.h      \i,       \i,       2
-.endr
-
-    LSX_TRANSPOSE8x8_H vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-                       vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-                       vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    vst_x8 sp, 64, 16, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
-
-    vld_x8 a2, 16, 64, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    dct_8x8_core_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-                     vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, no_rect2
-
-.irp i, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
-    vsrari.h      \i,       \i,       2
-.endr
-
-    LSX_TRANSPOSE8x8_H vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-                       vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-                       vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    vst_x8 sp, 192, 16, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
-
-    vld_x8 a2, 32, 64, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    la.local      t0,       idct_coeffs
-
-    dct_8x8_core_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-                     vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, no_rect2
-
-.irp i, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
-    vsrari.h      \i,       \i,       2
-.endr
-
-    LSX_TRANSPOSE8x8_H vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-                       vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-                       vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    vst_x8 sp, 320, 16, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
-
-    vld_x8 a2, 48, 64, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-
-    dct_8x8_core_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-                     vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, no_rect2
-
-.irp i, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
-    vsrari.h      \i,       \i,       2
-.endr
-
-    LSX_TRANSPOSE8x8_H vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-                       vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18, \
-                       vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    vst_x8 sp, 448, 16, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
-
-    vreplgr2vr.h  vr31,     zero
-
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, \
-        240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, \
-        464, 480, 496
-    vst           vr31,     a2,       \i
+    li.w          t0,       2896
+    vreplgr2vr.w  vr23,     t0
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+    vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+    rect2_lsx     \i,       vr23,     \i
 .endr
 
-    addi.d       t2,   sp, 64
-    addi.d       t3,   sp, 64
-
-    vld_x16 t2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    dct_8x16_core_lsx
-
-    vst_x16 t3, 0, 32, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-            vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
-
-    vld_x16 t2, 16, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-
-    // vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-    // in1  in3  in5  in7  in9 in11 in13 in15
-    // vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-    // in17  in19  in21  in23  in25  in27  in29  in31
-
-    la.local      t0,       idct_coeffs
-    vldrepl.w     vr20,     t0,       64           // 201
-    vldrepl.w     vr21,     t0,       68           // 4091
-
-    vmul_vmadd_w vr0, vr30, vr21, vr20, vr8, vr9
-    vssrarni.h.w  vr9,      vr8,      12           // t31a
-    vmul_vmsub_w vr0, vr30, vr20, vr21, vr11, vr10
-    vssrarni.h.w  vr10,     vr11,      12          // t16a
-
-    vldrepl.w     vr20,     t0,       72           // 3035
-    vldrepl.w     vr21,     t0,       76           // 2751
-    vmul_vmadd_w vr19, vr7, vr21, vr20, vr11, vr0
-    vssrarni.h.w  vr0,      vr11,      12          // t30a
-    vmul_vmsub_w vr19, vr7, vr20, vr21, vr11, vr30
-    vssrarni.h.w  vr30,     vr11,      12          // t17a
-
-    vldrepl.w     vr20,     t0,       80           // 1751
-    vldrepl.w     vr21,     t0,       84           // 3703
-    vmul_vmadd_w vr4, vr26, vr21, vr20, vr8, vr7
-    vssrarni.h.w  vr7,      vr8,      12           // t29a
-    vmul_vmsub_w vr4, vr26, vr20, vr21, vr8, vr19
-    vssrarni.h.w  vr19,     vr8,      12           // t18a
-
-    vldrepl.w     vr20,     t0,       88           // 3857
-    vldrepl.w     vr21,     t0,       92           // 1380
-    vmul_vmadd_w vr27, vr3, vr21, vr20, vr8, vr4
-    vssrarni.h.w  vr4,      vr8,      12           // t28a
-    vmul_vmsub_w vr27, vr3, vr20, vr21, vr8, vr26
-    vssrarni.h.w  vr26,     vr8,      12           // t19a
-
-    vldrepl.w     vr20,     t0,       96           // 995
-    vldrepl.w     vr21,     t0,       100          // 3973
-    vmul_vmadd_w vr2, vr28, vr21, vr20, vr8, vr3
-    vssrarni.h.w  vr3,      vr8,      12           // t27a
-    vmul_vmsub_w vr2, vr28, vr20, vr21, vr8, vr27
-    vssrarni.h.w  vr27,     vr8,      12           // t20a
-
-    vldrepl.w     vr20,     t0,       104          // 3513
-    vldrepl.w     vr21,     t0,       108          // 2106
-    vmul_vmadd_w vr25, vr5, vr21, vr20, vr8, vr2
-    vssrarni.h.w  vr2,      vr8,      12           // t26a
-    vmul_vmsub_w vr25, vr5, vr20, vr21, vr8, vr28
-    vssrarni.h.w  vr28,     vr8,      12           // t21a
-
-    vldrepl.w     vr20,     t0,       112          // 2440 -> 1220
-    vldrepl.w     vr21,     t0,       116          // 3290 -> 1645
-    vmul_vmadd_w vr6, vr24, vr21, vr20, vr8, vr5
-    vssrarni.h.w  vr5,      vr8,      12           // t25a
-    vmul_vmsub_w vr6, vr24, vr20, vr21, vr8, vr25
-    vssrarni.h.w  vr25,     vr8,      12           // t22a
-
-    vldrepl.w     vr20,     t0,       120          // 4052
-    vldrepl.w     vr21,     t0,       124          // 601
-    vmul_vmadd_w vr29, vr1, vr21, vr20, vr8, vr6
-    vssrarni.h.w  vr6,      vr8,      12           // t24a
-    vmul_vmsub_w vr29, vr1, vr20, vr21, vr8, vr24
-    vssrarni.h.w  vr24,     vr8,      12           // t23a
-
-    vsadd.h       vr1,      vr10,     vr30         // t16
-    vssub.h       vr29,     vr10,     vr30         // t17
-    vssub.h       vr8,      vr26,     vr19         // t18
-    vsadd.h       vr31,     vr26,     vr19         // t19
-    vsadd.h       vr10,     vr27,     vr28         // t20
-    vssub.h       vr30,     vr27,     vr28         // t21
-    vssub.h       vr19,     vr24,     vr25         // t22
-    vsadd.h       vr26,     vr24,     vr25         // t23
-    vsadd.h       vr27,     vr6,      vr5          // t24
-    vssub.h       vr28,     vr6,      vr5          // t25
-    vssub.h       vr24,     vr3,      vr2          // t26
-    vsadd.h       vr25,     vr3,      vr2          // t27
-    vsadd.h       vr5,      vr4,      vr7          // t28
-    vssub.h       vr6,      vr4,      vr7          // t29
-    vssub.h       vr2,      vr9,      vr0          // t30
-    vsadd.h       vr3,      vr9,      vr0          // t31
+    move          t6,       ra
+    jirl          ra,       t7,       0
+    move          ra,       t6
 
-    vldrepl.w     vr20,     t0,       16           // 799
-    vldrepl.w     vr21,     t0,       20           // 4017
-    vmul_vmadd_w vr2, vr29, vr21, vr20, vr4, vr7
-    vssrarni.h.w  vr7,      vr4,      12           // t30a
-    vmul_vmsub_w vr2, vr29, vr20, vr21, vr4, vr0
-    vssrarni.h.w  vr0,      vr4,      12           // t17a
-    vmul_vmadd_w vr6, vr8, vr21, vr20, vr4, vr9
-    vneg.w        vr4,      vr4
-    vneg.w        vr9,      vr9
-    vssrarni.h.w  vr9,      vr4,      12           // t18a
-    vmul_vmsub_w vr6, vr8, vr20, vr21, vr4, vr2
-    vssrarni.h.w  vr2,      vr4,      12           // t29a
+.ifnc \txfm1, identity_
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+    vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+    vsrari.h       \i,       \i,       1
+.endr
+.endif
 
-    vldrepl.w     vr20,     t0,       24           // 3406 -> 1703
-    vldrepl.w     vr21,     t0,       28           // 2276 -> 1138
-    vmul_vmadd_w vr24, vr30, vr21, vr20, vr4, vr29
-    vssrarni.h.w  vr29,     vr4,      12           // t26a
-    vmul_vmsub_w vr24, vr30, vr20, vr21, vr4, vr6
-    vssrarni.h.w  vr6,      vr4,      12           // t21a
+    LSX_TRANSPOSE8x8_H vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                       vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
 
-    vmul_vmadd_w vr28, vr19, vr21, vr20, vr4, vr8
-    vneg.w        vr4,      vr4
-    vneg.w        vr8,      vr8
-    vssrarni.h.w  vr8,      vr4,      12           // t22a
-    vmul_vmsub_w vr28, vr19, vr20, vr21, vr4, vr24
-    vssrarni.h.w  vr24,     vr4,      12           // t25a
+    move          t6,       ra
+    jirl          ra,       t8,       0
+    move          ra,       t6
 
-    vsadd.h       vr4,      vr1,      vr31         // t16a
-    vssub.h       vr30,     vr1,      vr31         // t19a
-    vsadd.h       vr19,     vr0,      vr9          // t17
-    vssub.h       vr28,     vr0,      vr9          // t18
-    vssub.h       vr1,      vr26,     vr10         // t20a
-    vsadd.h       vr31,     vr26,     vr10         // t23a
-    vssub.h       vr0,      vr8,      vr6          // t21
-    vsadd.h       vr9,      vr8,      vr6          // t22
-    vsadd.h       vr10,     vr27,     vr25         // t24a
-    vssub.h       vr26,     vr27,     vr25         // t27a
-    vsadd.h       vr6,      vr24,     vr29         // t25
-    vssub.h       vr8,      vr24,     vr29         // t26
-    vssub.h       vr25,     vr3,      vr5          // t28a
-    vsadd.h       vr27,     vr3,      vr5          // t31a
-    vssub.h       vr24,     vr7,      vr2          // t29
-    vsadd.h       vr29,     vr7,      vr2          // t30
+    vsrari_h_x8 vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                vr24, vr25, vr26, vr27, vr28, vr29, vr30, vr31, 4
 
-    vldrepl.w     vr20,     t0,       8            // 1567
-    vldrepl.w     vr21,     t0,       12           // 3784
-    vmul_vmadd_w vr24, vr28, vr21, vr20, vr3, vr5
-    vssrarni.h.w  vr5,      vr3,      12           // t29a
-    vmul_vmsub_w vr24, vr28, vr20, vr21, vr3, vr2
-    vssrarni.h.w  vr2,      vr3,      12           // 18a
+    LSX_TRANSPOSE8x8_H vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, \
+                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                       vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+    move          t6,       ra
+    jirl          ra,       t8,       0
+    move          ra,       t6
 
-    vmul_vmadd_w vr25, vr30, vr21, vr20, vr3, vr7
-    vssrarni.h.w  vr7,      vr3,      12           // t28
-    vmul_vmsub_w vr25, vr30, vr20, vr21, vr3, vr24
-    vssrarni.h.w  vr24,     vr3,      12           // t19
+    vsrari_h_x8 vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, 4
 
-    vmul_vmadd_w vr26, vr1, vr21, vr20, vr3, vr28
-    vneg.w        vr3,      vr3
-    vneg.w        vr28,     vr28
-    vssrarni.h.w  vr28,     vr3,      12           // t20
-    vmul_vmsub_w vr26, vr1, vr20, vr21, vr3, vr25
-    vssrarni.h.w  vr25,     vr3,      12           // t27
+    alsl.d        t2,       a1,       a0,    1
+    VLD_DST_ADD_W16 vr24, vr8, vr25, vr9, vr26, vr10, vr27, vr11
 
-    vmul_vmadd_w vr8, vr0, vr21, vr20, vr3, vr30
-    vneg.w        vr3,      vr3
-    vneg.w        vr30,     vr30
-    vssrarni.h.w  vr30,     vr3,      12           // t21a
-    vmul_vmsub_w vr8, vr0, vr20, vr21, vr3, vr1
-    vssrarni.h.w  vr1,      vr3,      12           // t26a
+    alsl.d        a0,       a1,       a0,    2
+    alsl.d        t2,       a1,       a0,    1
+    VLD_DST_ADD_W16 vr28, vr12, vr29, vr13, vr30, vr14, vr31, vr15
 
-    vsadd.h       vr3,      vr4,      vr31         // t16
-    vssub.h       vr26,     vr4,      vr31         // t23
-    vsadd.h       vr0,      vr19,     vr9          // t17a
-    vssub.h       vr8,      vr19,     vr9          // t22a
-    vsadd.h       vr4,      vr2,      vr30         // t18
-    vssub.h       vr31,     vr2,      vr30         // t21
-    vsadd.h       vr9,      vr24,     vr28         // t19a
-    vssub.h       vr19,     vr24,     vr28         // t20a
-    vssub.h       vr2,      vr27,     vr10         // t24
-    vsadd.h       vr30,     vr27,     vr10         // t31
-    vssub.h       vr24,     vr29,     vr6          // t25a
-    vsadd.h       vr28,     vr29,     vr6          // t30a
-    vssub.h       vr10,     vr5,      vr1          // t26
-    vsadd.h       vr27,     vr5,      vr1          // t29
-    vssub.h       vr6,      vr7,      vr25         // t27a
-    vsadd.h       vr29,     vr7,      vr25         // t28a
+    POP_REG
+endfuncl
+.endm
 
-    vldrepl.w     vr20,     t0,       0            // 2896
-    vmul_vmsub_w vr6, vr19, vr20, vr20, vr1, vr5
-    vssrarni.h.w  vr5,      vr1,      12           // t20
-    vmul_vmadd_w vr6, vr19, vr20, vr20, vr1, vr7
-    vssrarni.h.w  vr7,      vr1,      12           // t27
+def_fn_16x8 identity_
+def_fn_16x8
 
-    vmul_vmsub_w vr10, vr31, vr20, vr20, vr1, vr25
-    vssrarni.h.w  vr25,     vr1,      12           // t21a
-    vmul_vmadd_w vr10, vr31, vr20, vr20, vr1, vr6
-    vssrarni.h.w  vr6,      vr1,      12           // t26a
+.macro fun16x8 txfm1, txfm2
+function inv_txfm_add_\txfm1\()_\txfm2\()_16x8_8bpc_lsx
+.ifc \txfm1\()_\txfm2, dct_dct
+    bnez          a3,       .NO_HAS_DCONLY_16x8
 
-    vmul_vmsub_w vr24, vr8, vr20, vr20, vr1, vr19
-    vssrarni.h.w  vr19,     vr1,      12           // t22
-    vmul_vmadd_w vr24, vr8, vr20, vr20, vr1, vr10
-    vssrarni.h.w  vr10,     vr1,      12           // t25
+    idct_dc 16, 8, 1
 
-    vmul_vmsub_w vr2, vr26, vr20, vr20, vr1, vr31
-    vssrarni.h.w  vr31,     vr1,      12           // t23a
-    vmul_vmadd_w vr2, vr26, vr20, vr20, vr1, vr8
-    vssrarni.h.w  vr8,      vr1,      12           // t24a
+    DST_ADD_W16 vr10, vr11, vr12, vr13, vr20, vr20, vr20, \
+                vr20, vr20, vr20, vr20, vr20
 
-    // t31 t30a t29 t28a t27 t26a t25 t24a t23a t22 t21a t20 t19a t18 t17a t16
-    // vr30 vr28 vr27 vr29 vr7 vr6 vr10 vr8 vr31 vr19 vr25 vr5 vr9 vr4 vr0 vr3
+    alsl.d        a0,       a1,       a0,     2
+    alsl.d        t2,       a1,       a0,     1
+    VLD_DST_ADD_W16 vr20, vr20, vr20, vr20, vr20, vr20, vr20, vr20,
+    b             .\txfm1\()_\txfm2\()_16x8_END
+.NO_HAS_DCONLY_16x8:
+.endif
 
-    vld_x8 t3, 0, 32, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
+    la.local     t7,    inv_\txfm1\()_8h_x16_lsx
+.ifc \txfm1, identity
+    la.local     t7,    inv_identity_8h_x16_lsx1
+.endif
 
-    vsadd.h       vr1,      vr11,     vr30         // c[0]
-    vssub.h       vr2,      vr11,     vr30         // c[31]
-    vsadd.h       vr24,     vr12,     vr28         // c[1]
-    vssub.h       vr26,     vr12,     vr28         // c[30]
-    vsadd.h       vr11,     vr13,     vr27         // c[2]
-    vssub.h       vr30,     vr13,     vr27         // c[29]
-    vsadd.h       vr12,     vr14,     vr29         // c[3]
-    vssub.h       vr28,     vr14,     vr29         // c[28]
-    vsadd.h       vr13,     vr15,     vr7          // c[4]
-    vssub.h       vr27,     vr15,     vr7          // c[27]
-    vsadd.h       vr14,     vr16,     vr6          // c[5]
-    vssub.h       vr29,     vr16,     vr6          // c[26]
-    vsadd.h       vr7,      vr17,     vr10         // c[6]
-    vssub.h       vr15,     vr17,     vr10         // c[25]
-    vsadd.h       vr6,      vr18,     vr8          // c[7]
-    vssub.h       vr16,     vr18,     vr8          // c[24]
+    la.local     t8,    inv_\txfm2\()_8h_x8_lsx
 
-.irp i, vr1, vr24, vr11, vr12, vr13, vr14, vr7, vr6, \
-        vr16, vr15, vr29, vr27, vr28, vr30, vr26, vr2
-    vsrari.h      \i,       \i,       4
+.ifc \txfm1, identity
+    b            inv_txfm_identity_add_16x8_lsx
+.else
+    b            inv_txfm_add_16x8_lsx
+.endif
+
+.\txfm1\()_\txfm2\()_16x8_END:
+endfunc
+.endm
+
+fun16x8 dct, dct
+fun16x8 identity, identity
+fun16x8 dct, adst
+fun16x8 dct, flipadst
+fun16x8 dct, identity
+fun16x8 adst, dct
+fun16x8 adst, adst
+fun16x8 adst, flipadst
+fun16x8 flipadst, dct
+fun16x8 flipadst, adst
+fun16x8 flipadst, flipadst
+fun16x8 identity, dct
+fun16x8 adst, identity
+fun16x8 flipadst, identity
+fun16x8 identity, adst
+fun16x8 identity, flipadst
+
+.macro fun8x16 txfm1, txfm2, eob_half
+function inv_txfm_add_\txfm1\()_\txfm2\()_8x16_8bpc_lsx
+.ifc \txfm1\()_\txfm2, dct_dct
+    bnez          a3,       .NO_HAS_DCONLY_8x16
+
+    idct_dc 8, 16, 1
+
+    DST_ADD_W8 vr10, vr11, vr12, vr13, vr20, vr20, vr20, vr20
+.rept 3
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       a0,     1
+    VLD_DST_ADD_W8 vr20, vr20, vr20, vr20
 .endr
 
-    vst_x8 t2, 0, 16, vr1, vr24, vr11, vr12, vr13, vr14, vr7, vr6
+    b             .\txfm1\()_\txfm2\()_8x16_END
+.NO_HAS_DCONLY_8x16:
+.endif
+    li.w         t5,    \eob_half
+.ifnc \txfm1, identity
+    la.local     t7,    inv_\txfm1\()_8h_x8_lsx
+.endif
 
-    vst_x8 t2, 128, 16, vr16, vr15, vr29, vr27, vr28, vr30, vr26, vr2
+    la.local     t8,    inv_\txfm2\()_8h_x16_lsx
+.ifc \txfm1, identity
+    b            inv_txfm_identity_add_8x16_lsx
+.else
+    b            inv_txfm_add_8x16_lsx
+.endif
+.\txfm1\()_\txfm2\()_8x16_END:
+endfunc
+.endm
 
-    vld_x8 t3, 256, 32, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
+fun8x16 dct, dct, 43
+fun8x16 identity, identity, 43
+fun8x16 dct, adst, 43
+fun8x16 dct, flipadst, 43
+fun8x16 dct, identity, 8
+fun8x16 adst, dct, 43
+fun8x16 adst, adst, 43
+fun8x16 adst, flipadst, 43
+fun8x16 flipadst, dct, 43
+fun8x16 flipadst, adst, 43
+fun8x16 flipadst, flipadst, 43
+fun8x16 identity, dct, 64
+fun8x16 adst, identity, 8
+fun8x16 flipadst, identity, 8
+fun8x16 identity, adst, 64
+fun8x16 identity, flipadst, 64
+
+functionl inv_txfm_add_16x16_lsx
+    malloc_space 512
 
-    vsadd.h       vr1,      vr11,     vr31         // c[8]
-    vssub.h       vr2,      vr11,     vr31         // c[23]
-    vsadd.h       vr24,     vr12,     vr19         // c[9]
-    vssub.h       vr26,     vr12,     vr19         // c[22]
-    vsadd.h       vr11,     vr13,     vr25         // c[10]
-    vssub.h       vr30,     vr13,     vr25         // c[21]
-    vsadd.h       vr12,     vr14,     vr5          // c[11]
-    vssub.h       vr28,     vr14,     vr5          // c[20]
-    vsadd.h       vr13,     vr15,     vr9          // c[12]
-    vssub.h       vr27,     vr15,     vr9          // c[19]
-    vsadd.h       vr14,     vr16,     vr4          // c[13]
-    vssub.h       vr29,     vr16,     vr4          // c[18]
-    vsadd.h       vr7,      vr17,     vr0          // c[14]
-    vssub.h       vr15,     vr17,     vr0          // c[17]
-    vsadd.h       vr6,      vr18,     vr3          // c[15]
-    vssub.h       vr16,     vr18,     vr3          // c[16]
+    addi.d        t1,       sp,       64
+    addi.d        t2,       a2,       0
+.rept 2
+    vld_x16 a2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
 
-.irp i, vr1, vr24, vr11, vr12, vr13, vr14, vr7, vr6, \
-        vr16, vr15, vr29, vr27, vr28, vr30, vr26, vr2
-    vsrari.h      \i,       \i,       4
+    vxor.v        vr23,     vr23,     vr23
+.irp i, 0, 32, 64, 96, 128, 160, 192, 224, 256, 288, 320, 352, \
+    384, 416, 448, 480
+    vst           vr23,     a2,       \i
 .endr
 
-    vst_x8 t2, 256, 16, vr1, vr24, vr11, vr12, vr13, vr14, vr7, vr6
+    move          t6,       ra
+    jirl          ra,       t7,       0
+    move          ra,       t6
 
-    vst_x8 t2, 384, 16, vr16, vr15, vr29, vr27, vr28, vr30, vr26, vr2
+    LSX_TRANSPOSE8x8_H vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                       vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
 
-    alsl.d        t2,       a1,       a0,     1
-    addi.d        t3,       sp,       64
+    LSX_TRANSPOSE8x8_H vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, \
+                       vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, \
+                       vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
 
-    vld           vr4,      t3,       0
-    vld           vr5,      t3,       16
-    vld           vr6,      t3,       32
-    vld           vr7,      t3,       48
-    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+    vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+    vsrari.h       \i,       \i,       2
+.endr
+    vst_x8 t1, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    vst_x8 t1, 16, 32, vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+    addi.d         t1,       t1,       256
+    addi.d         a2,       a2,       16
+    blt            a3,       t5,       1616f
+.endr
 
-    addi.d        t3,       sp,       64+64
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       t2,     2
-    vld           vr4,      t3,       0
-    vld           vr5,      t3,       16
-    vld           vr6,      t3,       32
-    vld           vr7,      t3,       48
-    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
+1616:
+    ble           t5,       a3,       16161616f
+    addi.d        t1,       sp,       320
+    vxor.v        vr23,     vr23,     vr23
+.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, \
+    240
+    vst           vr23,     t1,       \i
+.endr
 
-    addi.d        t3,       sp,       64+256
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       t2,     2
-    vld           vr4,      t3,       0
-    vld           vr5,      t3,       16
-    vld           vr6,      t3,       32
-    vld           vr7,      t3,       48
-    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
+16161616:
+    addi.d        t1,       sp,       64
+.rept 2
+    vld_x16 t1, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
 
-    addi.d        t3,       t3,       64
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       t2,     2
-    vld           vr4,      t3,       0
-    vld           vr5,      t3,       16
-    vld           vr6,      t3,       32
-    vld           vr7,      t3,       48
-    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
+    move          t6,       ra
+    jirl          ra,       t8,       0
+    move          ra,       t6
 
-    addi.d        t3,       sp,       64+384
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       t2,     2
-    vld           vr4,      t3,       0
-    vld           vr5,      t3,       16
-    vld           vr6,      t3,       32
-    vld           vr7,      t3,       48
-    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
+    vst_x16 t1, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
 
-    addi.d        t3,       t3,       64
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       t2,     2
-    vld           vr4,      t3,       0
-    vld           vr5,      t3,       16
-    vld           vr6,      t3,       32
-    vld           vr7,      t3,       48
-    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
+    addi.d        t1,       t1,       16
+.endr
+    alsl.d        t2,       a1,       a0,    1
+    addi.d        t1,       sp,       64
+.rept 4
+    vld_x8 t1, 0, 16, vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+    vsrari_h_x8 vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23, \
+                vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23 4
+    VLD_DST_ADD_W16 vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+    alsl.d        a0,       a1,       a0,    2
+    alsl.d        t2,       a1,       a0,    1
+    addi.d        t1,       t1,       128
+.endr
+    free_space 512
+endfuncl
 
-    addi.d        t3,       sp,       64+128
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       t2,     2
-    vld           vr4,      t3,       0
-    vld           vr5,      t3,       16
-    vld           vr6,      t3,       32
-    vld           vr7,      t3,       48
-    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
+.macro fun16x16 txfm1, txfm2, eob_half
+function inv_txfm_add_\txfm1\()_\txfm2\()_16x16_8bpc_lsx
+.ifc \txfm1\()_\txfm2, dct_dct
+    bnez          a3,       .NO_HAS_DCONLY_16x16
 
-    addi.d        t3,       t3,       64
-    alsl.d        a0,       a1,       a0,     2
-    alsl.d        t2,       a1,       t2,     2
-    vld           vr4,      t3,       0
-    vld           vr5,      t3,       16
-    vld           vr6,      t3,       32
-    vld           vr7,      t3,       48
-    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
+    idct_dc 16, 16, 2
 
-    free_space 512
-.DCT_DCT_8X32_END:
-endfunc
+    DST_ADD_W16 vr10, vr11, vr12, vr13, vr20, vr20, vr20, \
+                    vr20, vr20, vr20, vr20, vr20
+.rept 3
+    alsl.d        a0,       a1,       a0,     2
+    alsl.d        t2,       a1,       a0,     1
 
-.macro dct_8x32_core_lsx in1, in2, vst_start0, vst_start1, vst_start2, \
-                         vst_start3, transpose8x8, shift
+    VLD_DST_ADD_W16 vr20, vr20, vr20, vr20, vr20, vr20, vr20, vr20
+.endr
+    b             .\txfm1\()_\txfm2\()_16x16_END
+.NO_HAS_DCONLY_16x16:
+.endif
+    li.w         t5,    \eob_half
+    la.local     t7,    inv_\txfm1\()_8h_x16_lsx
+    la.local     t8,    inv_\txfm2\()_8h_x16_lsx
 
-    // vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
-    // in1  in3  in5  in7  in9 in11 in13 in15
-    // vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
-    // in17  in19  in21  in23  in25  in27  in29  in31
+    b            inv_txfm_add_16x16_lsx
+.\txfm1\()_\txfm2\()_16x16_END:
+endfunc
+.endm
 
+fun16x16 dct, dct, 36
+fun16x16 adst, adst, 36
+fun16x16 adst, dct, 36
+fun16x16 dct, adst, 36
+fun16x16 flipadst, dct, 36
+fun16x16 dct, flipadst, 36
+fun16x16 adst, flipadst, 36
+fun16x16 flipadst, adst, 36
+
+.macro dct_8x32_core_lsx in1, in2, vld_st0, vld_st1, vld_stride, \
+                         vst_st0, vst_st1, vst_st2, vst_st3, vst_stride, \
+                         transpose8x8, shift
     la.local      t0,       idct_coeffs
     vldrepl.w     vr20,     t0,       64           // 201
     vldrepl.w     vr21,     t0,       68           // 4091
-
     vmul_vmadd_w vr0, vr30, vr21, vr20, vr8, vr9
     vmul_vmsub_w vr0, vr30, vr20, vr21, vr11, vr10
     vssrarni.h.w  vr9,      vr8,      12           // t31a
     vssrarni.h.w  vr10,     vr11,     12           // t16a
-
     vldrepl.w     vr20,     t0,       72           // 3035
     vldrepl.w     vr21,     t0,       76           // 2751
     vmul_vmadd_w vr19, vr7, vr21, vr20, vr8, vr0
     vmul_vmsub_w vr19, vr7, vr20, vr21, vr11, vr30
     vssrarni.h.w  vr0,      vr8,      12           // t30a
     vssrarni.h.w  vr30,     vr11,     12           // t17a
-
     vldrepl.w     vr20,     t0,       80           // 1751
     vldrepl.w     vr21,     t0,       84           // 3703
     vmul_vmadd_w vr4, vr26, vr21, vr20, vr8, vr7
     vmul_vmsub_w vr4, vr26, vr20, vr21, vr11, vr19
     vssrarni.h.w  vr7,      vr8,      12           // t29a
     vssrarni.h.w  vr19,     vr11,     12           // t18a
-
     vldrepl.w     vr20,     t0,       88           // 3857
     vldrepl.w     vr21,     t0,       92           // 1380
     vmul_vmadd_w vr27, vr3, vr21, vr20, vr8, vr4
     vmul_vmsub_w vr27, vr3, vr20, vr21, vr11, vr26
     vssrarni.h.w  vr4,      vr8,      12           // t28a
     vssrarni.h.w  vr26,     vr11,     12           // t19a
-
     vldrepl.w     vr20,     t0,       96           // 995
     vldrepl.w     vr21,     t0,       100          // 3973
     vmul_vmadd_w vr2, vr28, vr21, vr20, vr8, vr3
     vmul_vmsub_w vr2, vr28, vr20, vr21, vr11, vr27
     vssrarni.h.w  vr3,      vr8,      12           // t27a
     vssrarni.h.w  vr27,     vr11,     12           // t20a
-
     vldrepl.w     vr20,     t0,       104          // 3513
     vldrepl.w     vr21,     t0,       108          // 2106
     vmul_vmadd_w vr25, vr5, vr21, vr20, vr8, vr2
     vmul_vmsub_w vr25, vr5, vr20, vr21, vr11, vr28
     vssrarni.h.w  vr2,      vr8,      12           // t26a
     vssrarni.h.w  vr28,     vr11,     12           // t21a
-
     vldrepl.w     vr20,     t0,       112          // 2440 -> 1220
     vldrepl.w     vr21,     t0,       116          // 3290 -> 1645
     vmul_vmadd_w vr6, vr24, vr21, vr20, vr8, vr5
     vmul_vmsub_w vr6, vr24, vr20, vr21, vr11, vr25
     vssrarni.h.w  vr5,      vr8,      12           // t25a
     vssrarni.h.w  vr25,     vr11,     12           // t22a
-
     vldrepl.w     vr20,     t0,       120          // 4052
     vldrepl.w     vr21,     t0,       124          // 601
     vmul_vmadd_w vr29, vr1, vr21, vr20, vr8, vr6
@@ -6587,14 +2101,12 @@ endfunc
     vmul_vmsub_w vr6, vr8, vr20, vr21, vr11, vr2
     vssrarni.h.w  vr9,      vr4,      12           // t18a
     vssrarni.h.w  vr2,      vr11,     12           // t29a
-
     vldrepl.w     vr20,     t0,       24           // 3406 -> 1703
     vldrepl.w     vr21,     t0,       28           // 2276 -> 1138
     vmul_vmadd_w vr24, vr30, vr21, vr20, vr4, vr29
     vmul_vmsub_w vr24, vr30, vr20, vr21, vr11, vr6
     vssrarni.h.w  vr29,     vr4,      12           // t26a
     vssrarni.h.w  vr6,      vr11,     12           // t21a
-
     vmul_vmadd_w vr28, vr19, vr21, vr20, vr4, vr8
     vneg.w        vr4,      vr4
     vneg.w        vr8,      vr8
@@ -6625,19 +2137,16 @@ endfunc
     vmul_vmsub_w vr24, vr28, vr20, vr21, vr11, vr2
     vssrarni.h.w  vr5,      vr3,      12           // t29a
     vssrarni.h.w  vr2,      vr11,     12           // 18a
-
     vmul_vmadd_w vr25, vr30, vr21, vr20, vr3, vr7
     vmul_vmsub_w vr25, vr30, vr20, vr21, vr11, vr24
     vssrarni.h.w  vr7,      vr3,      12           // t28
     vssrarni.h.w  vr24,     vr11,     12           // t19
-
     vmul_vmadd_w vr26, vr1, vr21, vr20, vr3, vr28
     vneg.w        vr3,      vr3
     vneg.w        vr28,     vr28
     vmul_vmsub_w vr26, vr1, vr20, vr21, vr11, vr25
     vssrarni.h.w  vr28,     vr3,      12           // t20
     vssrarni.h.w  vr25,     vr11,     12           // t27
-
     vmul_vmadd_w vr8, vr0, vr21, vr20, vr3, vr30
     vneg.w        vr3,      vr3
     vneg.w        vr30,     vr30
@@ -6667,17 +2176,14 @@ endfunc
     vmul_vmadd_w vr6, vr19, vr20, vr20, vr11, vr7
     vssrarni.h.w  vr5,      vr1,      12           // t20
     vssrarni.h.w  vr7,      vr11,     12           // t27
-
     vmul_vmsub_w vr10, vr31, vr20, vr20, vr1, vr25
     vmul_vmadd_w vr10, vr31, vr20, vr20, vr11, vr6
     vssrarni.h.w  vr25,     vr1,      12           // t21a
     vssrarni.h.w  vr6,      vr11,     12           // t26a
-
     vmul_vmsub_w vr24, vr8, vr20, vr20, vr1, vr19
     vmul_vmadd_w vr24, vr8, vr20, vr20, vr11, vr10
     vssrarni.h.w  vr19,     vr1,      12           // t22
     vssrarni.h.w  vr10,     vr11,     12           // t25
-
     vmul_vmsub_w vr2, vr26, vr20, vr20, vr1, vr31
     vmul_vmadd_w vr2, vr26, vr20, vr20, vr11, vr8
     vssrarni.h.w  vr31,     vr1,      12           // t23a
@@ -6685,8 +2191,7 @@ endfunc
 
     // t31 t30a t29 t28a t27 t26a t25 t24a t23a t22 t21a t20 t19a t18 t17a t16
     // vr30 vr28 vr27 vr29 vr7 vr6 vr10 vr8 vr31 vr19 vr25 vr5 vr9 vr4 vr0 vr3
-
-    vld_x8 \in2, 0, 16, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
+    vld_x8 \in2, \vld_st0, \vld_stride, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
 
     vsadd.h       vr1,      vr11,     vr30         // c[0]
     vssub.h       vr2,      vr11,     vr30         // c[31]
@@ -6717,7 +2222,7 @@ endfunc
 .endr
 .endif
 
-    vst_x8 \in1, \vst_start0, 64, vr1, vr24, vr11, vr12, vr13, vr14, vr7, vr6
+    vst_x8 \in1, \vst_st0, \vst_stride, vr1, vr24, vr11, vr12, vr13, vr14, vr7, vr6
 
 .ifnb \transpose8x8
     LSX_TRANSPOSE8x8_H vr16, vr15, vr29, vr27, vr28, vr30, vr26, vr2, \
@@ -6731,9 +2236,9 @@ endfunc
 .endr
 .endif
 
-    vst_x8 \in1, \vst_start3, 64, vr16, vr15, vr29, vr27, vr28, vr30, vr26, vr2
+    vst_x8 \in1, \vst_st1, \vst_stride, vr16, vr15, vr29, vr27, vr28, vr30, vr26, vr2
 
-    vld_x8 \in2, 128, 16, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
+    vld_x8 \in2, \vld_st1, \vld_stride, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
 
     vsadd.h       vr1,      vr11,     vr31         // c[8]
     vssub.h       vr2,      vr11,     vr31         // c[23]
@@ -6764,7 +2269,7 @@ endfunc
 .endr
 .endif
 
-    vst_x8 \in1, \vst_start1, 64, vr1, vr24, vr11, vr12, vr13, vr14, vr7, vr6
+    vst_x8 \in1, \vst_st2, \vst_stride, vr1, vr24, vr11, vr12, vr13, vr14, vr7, vr6
 
 .ifnb \transpose8x8
     LSX_TRANSPOSE8x8_H vr16, vr15, vr29, vr27, vr28, vr30, vr26, vr2, \
@@ -6778,216 +2283,383 @@ endfunc
 .endr
 .endif
 
-    vst_x8 \in1, \vst_start2, 64, vr16, vr15, vr29, vr27, vr28, vr30, vr26, vr2
-.endm
+    vst_x8 \in1, \vst_st3, \vst_stride, vr16, vr15, vr29, vr27, vr28, vr30, vr26, vr2
+.endm
+
+const eob_32x32
+        .short 36, 136, 300, 1024
+endconst
+
+const eob_8x32
+        .short 43, 107, 171, 256
+endconst
 
-function inv_txfm_add_dct_dct_32x32_8bpc_lsx
-    bnez          a3,       .NO_HAS_DCONLY_32x32
+const eob_16x32
+        .short 36, 151, 279, 512
+endconst
+
+.macro DST_ADD_W32 in0, in1, in2, in3, in4, in5, in6, in7
+    vsllwil.hu.bu vr4,      vr10,     0
+    vsllwil.hu.bu vr5,      vr11,     0
+    vsllwil.hu.bu vr6,      vr12,     0
+    vsllwil.hu.bu vr7,      vr13,     0
+    vexth.hu.bu   vr10,     vr10
+    vexth.hu.bu   vr11,     vr11
+    vexth.hu.bu   vr12,     vr12
+    vexth.hu.bu   vr13,     vr13
+    vadd.h        vr4,      vr4,      \in0
+    vadd.h        vr10,     vr10,     \in1
+    vadd.h        vr5,      vr5,      \in2
+    vadd.h        vr11,     vr11,     \in3
+    vadd.h        vr6,      vr6,      \in4
+    vadd.h        vr12,     vr12,     \in5
+    vadd.h        vr7,      vr7,      \in6
+    vadd.h        vr13,     vr13,     \in7
+    vssrani.bu.h  vr10,     vr4,      0
+    vssrani.bu.h  vr11,     vr5,      0
+    vssrani.bu.h  vr12,     vr6,      0
+    vssrani.bu.h  vr13,     vr7,      0
+    vst           vr10,     a0,       0
+    vst           vr11,     a0,       16
+    vst           vr12,     t2,       0
+    vst           vr13,     t2,       16
+.endm
 
+.macro idct_dc_w32 w, h, shift
     ld.h          t2,       a2,       0      // dc
     vldi          vr0,      0x8b5            // 181
     vreplgr2vr.w  vr1,      t2
     vldi          vr20,     0x880            // 128
     vmul.w        vr2,      vr0,      vr1    // dc * 181
     st.h          zero,     a2,       0
-    add.d         t0,       a0,       a1
+    add.d         t2,       a0,       a1
     vsrari.w      vr2,      vr2,      8      // (dc * 181 + 128) >> 8
-    vld           vr3,      t0,       16
-    vsrari.w      vr2,      vr2,      2      // (dc + rnd) >> shift
-    vld           vr1,      a0,       16
+    vld           vr13,     t2,       16
+
+.if (2*\w == \h) || (2*\h == \w)
+    vmul.w        vr2,      vr2,      vr0
+    vsrari.w      vr2,      vr2,      8
+.endif
+
+.if \shift>0
+    vsrari.w      vr2,      vr2,      \shift      // (dc + rnd) >> shift
+.endif
+    vld           vr11,     a0,       16
     vmadd.w       vr20,     vr2,      vr0
-    vld           vr2,      t0,       0
+    vld           vr12,     t2,       0
     vssrarni.h.w  vr20,     vr20,     12
-    vld           vr0,      a0,       0
+    vld           vr10,     a0,       0
+.endm
 
-    vsllwil.hu.bu vr4,      vr0,      0
-    vsllwil.hu.bu vr5,      vr1,      0
-    vsllwil.hu.bu vr6,      vr2,      0
-    vsllwil.hu.bu vr7,      vr3,      0
-    vexth.hu.bu   vr0,      vr0
-    vexth.hu.bu   vr1,      vr1
-    vexth.hu.bu   vr2,      vr2
-    vexth.hu.bu   vr3,      vr3
-    vadd.h        vr8,      vr4,      vr20
-    vadd.h        vr9,      vr0,      vr20
-    vadd.h        vr10,     vr5,      vr20
-    vadd.h        vr11,     vr1,      vr20
-    vadd.h        vr12,     vr6,      vr20
-    vadd.h        vr13,     vr2,      vr20
-    vadd.h        vr14,     vr7,      vr20
-    vadd.h        vr15,     vr3,      vr20
-    vssrani.bu.h  vr9,      vr8,      0
-    vssrani.bu.h  vr11,     vr10,     0
-    vssrani.bu.h  vr13,     vr12,     0
-    vssrani.bu.h  vr15,     vr14,     0
-    vst           vr9,      a0,       0
-    vst           vr11,     a0,       16
-    vst           vr13,     t0,       0
-    vst           vr15,     t0,       16
+function inv_txfm_add_dct_dct_32x8_8bpc_lsx
+    bnez          a3,       .NO_HAS_DCONLY_32x8
 
-.rept 15
+    idct_dc_w32 32, 8, 2
+
+    DST_ADD_W32 vr20, vr20, vr20, vr20, vr20, vr20, vr20, vr20
+
+.rept 3
     alsl.d        a0,       a1,       a0,     1
-    add.d         t0,       a0,       a1
+    add.d         t2,       a0,       a1
+    vld           vr10,     a0,       0
+    vld           vr11,     a0,       16
+    vld           vr12,     t2,       0
+    vld           vr13,     t2,       16
+    DST_ADD_W32 vr20, vr20, vr20, vr20, vr20, vr20, vr20, vr20
+.endr
+    b             .DCT_DCT_32X8_END
+.NO_HAS_DCONLY_32x8:
+    malloc_space 512+256
 
-    vld           vr0,      a0,       0
-    vld           vr1,      a0,       16
-    vld           vr2,      t0,       0
-    vld           vr3,      t0,       16
-    vsllwil.hu.bu vr4,      vr0,      0
-    vsllwil.hu.bu vr5,      vr1,      0
-    vsllwil.hu.bu vr6,      vr2,      0
-    vsllwil.hu.bu vr7,      vr3,      0
-    vexth.hu.bu   vr0,      vr0
-    vexth.hu.bu   vr1,      vr1
-    vexth.hu.bu   vr2,      vr2
-    vexth.hu.bu   vr3,      vr3
-    vadd.h        vr8,      vr4,      vr20
-    vadd.h        vr9,      vr0,      vr20
-    vadd.h        vr10,     vr5,      vr20
-    vadd.h        vr11,     vr1,      vr20
-    vadd.h        vr12,     vr6,      vr20
-    vadd.h        vr13,     vr2,      vr20
-    vadd.h        vr14,     vr7,      vr20
-    vadd.h        vr15,     vr3,      vr20
-    vssrani.bu.h  vr9,      vr8,      0
-    vssrani.bu.h  vr11,     vr10,     0
-    vssrani.bu.h  vr13,     vr12,     0
-    vssrani.bu.h  vr15,     vr14,     0
-    vst           vr9,      a0,       0
-    vst           vr11,     a0,       16
-    vst           vr13,     t0,       0
-    vst           vr15,     t0,       16
+    addi.d        t1,       sp,       64
+    addi.d        t2,       a2,       0
+    addi.d        t3,       sp,       64
+    addi.d        t3,       t3,       512
+
+    vld_x16 t2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+
+    vxor.v        vr31,     vr31,     vr31
+    vst_x16 t2, 0, 32, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+            vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+    inv_dct16_lsx .8h
+
+    vst_x16 t3, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+
+    vld_x16 t2, 16, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
+
+    vxor.v        vr31,     vr31,     vr31
+
+    vst_x16 t2, 16, 32, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+            vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+    dct_8x32_core_lsx t1, t3, 0, 128, 16, 0, 48, 16, 32, 64, transpose8x8, 2
+
+    addi.d        t2,       sp,       64
+.rept 4
+    vld_x8 t2, 0, 64, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+
+    inv_dct8_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, .8h
+
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    vsrari.h      \i,       \i,       4
 .endr
 
-    b             .DCT_DCT_32X32_END
-.NO_HAS_DCONLY_32x32:
+    vst_x8 t2, 0, 64, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
 
-    malloc_space 2560                              // 32*32*2+512
+    addi.d        t2,       t2,       16
+.endr
+
+    addi.d        t0,       sp,       64
+.rept 4
+    add.d         t2,       a0,       a1
+    vld           vr10,     a0,       0
+    vld           vr11,     a0,       16
+    vld           vr12,     t2,       0
+    vld           vr13,     t2,       16
+    vld_x8 t0, 0, 16, vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+    DST_ADD_W32 vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+    alsl.d        a0,       a1,       a0,     1
+    addi.d        t0,       t0,       128
+.endr
+    free_space 512+256
+.DCT_DCT_32X8_END:
+endfunc
 
+function inv_txfm_add_dct_dct_32x16_8bpc_lsx
+    bnez          a3,       .NO_HAS_DCONLY_32x16
+
+    idct_dc_w32 32, 16, 1
+
+    DST_ADD_W32 vr20, vr20, vr20, vr20, vr20, vr20, vr20, vr20
+
+.rept 7
+    alsl.d        a0,       a1,       a0,     1
+    add.d         t2,       a0,       a1
+    vld           vr10,     a0,       0
+    vld           vr11,     a0,       16
+    vld           vr12,     t2,       0
+    vld           vr13,     t2,       16
+    DST_ADD_W32 vr20, vr20, vr20, vr20, vr20, vr20, vr20, vr20
+.endr
+    b             .DCT_DCT_32X16_END
+.NO_HAS_DCONLY_32x16:
+    malloc_space 1024+256                            // 32*32*2+512
     addi.d        t1,       sp,       64
     addi.d        t2,       a2,       0
-    addi.d        t3,       sp,       1024
+    addi.d        t3,       sp,       64
     addi.d        t3,       t3,       1024
-    addi.d        t3,       t3,       64
+.rept 2
+    vld_x16 t2, 0, 64, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
 
-    vld_x16 t2, 0, 128, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
+    vxor.v        vr31,     vr31,     vr31
+    vst_x16 t2, 0, 64, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+            vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+    li.w          t0,       2896
+    vreplgr2vr.w  vr23,     t0
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+     vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+    rect2_lsx   \i, vr23, \i
+.endr
 
-    dct_8x16_core_lsx
+    inv_dct16_lsx .8h
 
-    vst_x16 t3, 0, 16, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-            vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
+    vst_x16 t3, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
 
-    vld_x16 t2, 64, 128, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+    vld_x16 t2, 32, 64, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
             vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
 
-    dct_8x32_core_lsx t1, t3, 0, 16, 32, 48, transpose8x8, 2
+    la.local      t0,       idct_coeffs
+    vldrepl.w     vr23,     t0,       0        // 2896
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+    vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
+    rect2_lsx \i, vr23, \i
+.endr
+    vxor.v        vr31,     vr31,     vr31
+    vst_x16 t2, 32, 64, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+            vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+    dct_8x32_core_lsx t1, t3, 0, 128, 16, 0, 48, 16, 32, 64, transpose8x8, 1
 
-.rept 3
     addi.d        t2,       t2,       16
     addi.d        t1,       t1,       512
+.endr
 
-    vld_x16 t2, 0, 128, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
+    addi.d        t2,       sp,       64
+.rept 4
+    vld_x16 t2, 0, 64, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
 
-    dct_8x16_core_lsx
+    inv_dct16_lsx .8h
 
-    vst_x16 t3, 0, 16, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-            vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+    vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+    vsrari.h      \i,       \i,       4
+.endr
 
-    vld_x16 t2, 64, 128, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
+    vst_x16 t2, 0, 64, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
 
-    dct_8x32_core_lsx t1, t3, 0, 16, 32, 48, transpose8x8, 2
+    addi.d        t2,       t2,       16
 .endr
 
-    vreplgr2vr.h     vr31,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512, 528, 544, 560, 576, 592, 608, 624, 640, 656, 672, 688, 704, 720, 736, 752, 768, 784, 800, 816, 832, 848, 864, 880, 896, 912, 928, 944, 960, 976, 992, 1008, 1024, 1040, 1056, 1072, 1088, 1104, 1120, 1136, 1152, 1168, 1184, 1200, 1216, 1232, 1248, 1264, 1280, 1296, 1312, 1328, 1344, 1360, 1376, 1392, 1408, 1424, 1440, 1456, 1472, 1488, 1504, 1520, 1536, 1552, 1568, 1584, 1600, 1616, 1632, 1648, 1664, 1680, 1696, 1712, 1728, 1744, 1760, 1776, 1792, 1808, 1824, 1840, 1856, 1872, 1888, 1904, 1920, 1936, 1952, 1968, 1984, 2000, 2016, 2032
-    vst           vr31,     a2,       \i
+    addi.d        t0,       sp,       64
+.rept 8
+    add.d         t2,       a0,       a1
+    vld           vr10,     a0,       0
+    vld           vr11,     a0,       16
+    vld           vr12,     t2,       0
+    vld           vr13,     t2,       16
+    vld_x8 t0, 0, 16, vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+    DST_ADD_W32 vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+
+    alsl.d        a0,       a1,       a0,     1
+    addi.d        t0,       t0,       128
 .endr
+    free_space 1024+256
+.DCT_DCT_32X16_END:
+endfunc
+
+function inv_txfm_add_dct_dct_32x32_8bpc_lsx
+    bnez          a3,       .NO_HAS_DCONLY_32x32
+
+    idct_dc_w32 32, 32, 2
+
+    DST_ADD_W32 vr20, vr20, vr20, vr20, vr20, vr20, vr20, vr20
+.rept 15
+    alsl.d        a0,       a1,       a0,     1
+    add.d         t2,       a0,       a1
+    vld           vr10,     a0,       0
+    vld           vr11,     a0,       16
+    vld           vr12,     t2,       0
+    vld           vr13,     t2,       16
+    DST_ADD_W32 vr20, vr20, vr20, vr20, vr20, vr20, vr20, vr20
+.endr
+    b             .DCT_DCT_32X32_END
+.NO_HAS_DCONLY_32x32:
+    malloc_space 2560                              // 32*32*2+512
 
-    addi.d        t2,       sp,       64
     addi.d        t1,       sp,       64
+    addi.d        t2,       a2,       0
+    addi.d        t3,       sp,       1024
+    addi.d        t3,       t3,       1024
+    addi.d        t3,       t3,       64
+
+    la.local      t8,       eob_32x32
+.DCT_DCT_EOB_32x32:
+    ld.h          t7,       t8,       0
+    addi.d        t8,       t8,       2
 
     vld_x16 t2, 0, 128, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
 
-    dct_8x16_core_lsx
+    vxor.v        vr31,     vr31,     vr31
+    vst_x16 t2, 0, 128, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+            vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
 
-    vst_x16 t3, 0, 16, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-            vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
+    inv_dct16_lsx .8h
+
+    vst_x16 t3, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
 
     vld_x16 t2, 64, 128, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
             vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
 
-    dct_8x32_core_lsx t1, t3, 0, 512, 1024, 1536, , 4
+    vxor.v        vr31,     vr31,     vr31
 
-.rept 3
-    addi.d        t2,       t2,       16
-    addi.d        t1,       t1,       16
+    vst_x16 t2, 64, 128, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+            vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+    dct_8x32_core_lsx t1, t3, 0, 128, 16, 0, 48, 16, 32, 64, transpose8x8, 2
 
+    addi.d        t2,       t2,       16
+    addi.d        t1,       t1,       512
+    bge           a3,       t7,       .DCT_DCT_EOB_32x32
+
+    la.local      t8,       eob_32x32
+    vxor.v        vr31,     vr31,     vr31
+    ld.h          t7,       t8,       4
+    bge           a3,       t7,       .DCT_DCT_EOB_32x32_END   // a3>=t7
+    vst_x16 sp, 64+1536, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+        vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+    addi.d        t1,       sp,       256+64
+    vst_x16 t1, 1536, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+        vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+    ld.h          t7,       t8,       2
+    bge           a3,       t7,       .DCT_DCT_EOB_32x32_END
+    vst_x16 sp, 64+1024, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+        vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+    vst_x16 t1, 1024, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+        vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+    ld.h          t7,       t8,       0
+    bge           a3,       t7,       .DCT_DCT_EOB_32x32_END
+    vst_x16 sp, 64+512, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+        vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+    vst_x16 t1, 512, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+        vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+.DCT_DCT_EOB_32x32_END:
+    addi.d        t2,       sp,       64
+    addi.d        t1,       sp,       64
+.rept 4
     vld_x16 t2, 0, 128, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
-            vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
 
-    dct_8x16_core_lsx
+    inv_dct16_lsx .8h
 
-    vst_x16 t3, 0, 16, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
-            vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
+    vst_x16 t3, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
 
     vld_x16 t2, 64, 128, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
             vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
 
-    dct_8x32_core_lsx t1, t3, 0, 512, 1024, 1536, , 4
-.endr
+    dct_8x32_core_lsx t1, t3, 0, 128, 16, 0, 1536, 512, 1024, 64, , 4
 
-    addi.d        t2,       sp,       64
+    addi.d        t2,       t2,       16
+    addi.d        t1,       t1,       16
+.endr
 
+    addi.d        t0,       sp,       64
 .rept 16
-    add.d         t0,       a0,       a1
-    vld           vr0,      a0,       0
-    vld           vr1,      a0,       16
-    vld           vr2,      t0,       0
-    vld           vr3,      t0,       16
-    vsllwil.hu.bu vr4,      vr0,      0
-    vsllwil.hu.bu vr5,      vr1,      0
-    vsllwil.hu.bu vr6,      vr2,      0
-    vsllwil.hu.bu vr7,      vr3,      0
-    vexth.hu.bu   vr0,      vr0
-    vexth.hu.bu   vr1,      vr1
-    vexth.hu.bu   vr2,      vr2
-    vexth.hu.bu   vr3,      vr3
-    vld_x8 t2, 0, 16, vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
-    vadd.h        vr8,      vr4,      vr8
-    vadd.h        vr9,      vr0,      vr9
-    vadd.h        vr10,     vr5,      vr10
-    vadd.h        vr11,     vr1,      vr11
-    vadd.h        vr12,     vr6,      vr12
-    vadd.h        vr13,     vr2,      vr13
-    vadd.h        vr14,     vr7,      vr14
-    vadd.h        vr15,     vr3,      vr15
-    vssrani.bu.h  vr9,      vr8,      0
-    vssrani.bu.h  vr11,     vr10,     0
-    vssrani.bu.h  vr13,     vr12,     0
-    vssrani.bu.h  vr15,     vr14,     0
-    vst           vr9,      a0,       0
-    vst           vr11,     a0,       16
-    vst           vr13,     t0,       0
-    vst           vr15,     t0,       16
-
+    add.d         t2,       a0,       a1
+    vld           vr10,     a0,       0
+    vld           vr11,     a0,       16
+    vld           vr12,     t2,       0
+    vld           vr13,     t2,       16
+    vld_x8 t0, 0, 16, vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+    DST_ADD_W32 vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
     alsl.d        a0,       a1,       a0,     1
-    addi.d        t2,       t2,       128
+    addi.d        t0,       t0,       128
 .endr
 
     free_space 2560                                // 32*32*2+512
-
 .DCT_DCT_32X32_END:
 endfunc
 
-.macro dct_8x8_tx64_core_lsx in0, in1, in2, in3, in4, in5, in6, in7, \
-                             out0, out1, out2, out3, out4, out5, out6, out7
+/*
+ * temp: vr8, vr9, vr10, vr12, vr20, vr21, vr22, vr23
+ */
+.macro dct_8x8_tx64_core_lsx in0, in1, in2, in3, in4, in5, in6, in7, out0, \
+                             out1, out2, out3, out4, out5, out6, out7, rect2
+
+    la.local      t0,       idct_coeffs
+
+.ifc \rect2, rect2_lsx
+    vldrepl.w     vr23,      t0,       0        // 2896
+.irp i, \in0, \in1, \in2, \in3, \in4, \in5, \in6, \in7
+    rect2_lsx \i, vr23, \i
+.endr
+.endif
 
-    // in0 in1 in2 in3
-    // dct4 in0 in2
     la.local      t0,       idct_coeffs
 
     vldrepl.w     vr20,     t0,       8            // 1567
@@ -7061,14 +2733,31 @@ endfunc
     vssub.h       \out4,    vr9,      \in3         // c[4]
 .endm
 
-.macro dct_8x16_tx64_core_lsx
+/*
+ * input:  in0,  in1,  in2,  in3,  in4,  in5,  in6,  in7       (fixed)
+ *         vr0,  vr1,  vr2,  vr3,  vr4,  vr5,  vr6,  vr7
+ *         in8,  in9,  in10, in11, in12, in13, in14, in15
+ *         vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
+ * output: out0, out1, out2, out3, out4, out5, out6, out7      (fixed)
+ *         vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16
+ *         out8, out9, out10, out11, out12, out13, out14, out15
+ *         vr27, vr30, vr23,  vr21,  vr29,  vr26,  vr25,  vr24
+ */
+.macro dct_8x16_tx64_core_lsx rect2
     dct_8x8_tx64_core_lsx vr0, vr2, vr4, vr6, vr19, vr25, vr27, vr29, vr11, \
-                          vr12, vr13, vr14, vr15, vr16, vr17, vr18
+                          vr12, vr13, vr14, vr15, vr16, vr17, vr18, \rect2
 
     // in1 in3 in5 in7 in9  in11 in13 in15
     // vr1 vr3 vr5 vr7 vr24 vr26 vr28 vr30
     la.local      t0,       idct_coeffs
 
+.ifc \rect2, rect2_lsx
+    vldrepl.w     vr23,      t0,       0        // 2896
+.irp i, vr1, vr3, vr5, vr7, vr24, vr26, vr28, vr30
+    rect2_lsx \i, vr23, \i
+.endr
+.endif
+
     vldrepl.w     vr20,     t0,       32           // 401
     vldrepl.w     vr21,     t0,       36           // 4076
     vsllwil.w.h   vr22,     vr1,      0
@@ -7193,39 +2882,27 @@ const idct64_coeffs, align=4
     .word         101, 4095, 2967, -2824
     .word         1660, 3745, 3822, -1474
     .word         4076, 401, 4017, 799
-
     .word         4036, -700, 2359, 3349
     .word         3461, -2191, 897, 3996
     .word         -3166, -2598, -799, -4017
-
     .word         501, 4065, 3229, -2520
     .word         2019, 3564, 3948, -1092
     .word         3612, 1931, 2276, 3406
-
     .word         4085, -301, 2675, 3102
     .word         3659, -1842, 1285, 3889
     .word         -3920, -1189, -3406, -2276
 endconst
 
-// in1/31/17/15 -> t32a/33/34a/35/60/61a/62/63a
-// in7/25/23/ 9 -> t56a/57/58a/59/36/37a/38/39a
-// in5/27/21/11 -> t40a/41/42a/43/52/53a/54/55a
-// in3/29/19/13 -> t48a/49/50a/51/44/45a/46/47a
-
 .macro dct64_step1_lsx
-
     vldrepl.w     vr20,     t0,       0            // 101
     vldrepl.w     vr21,     t0,       4            // 4095
     vmul_vssrarni_hw vr0, vr20, vr21, vr16, vr0, vr8, vr9    // vr8 t32a vr9 t63a
-
     vldrepl.w     vr20,     t0,       8            // 2967
     vldrepl.w     vr21,     t0,       12           // -2824
     vmul_vssrarni_hw vr1, vr20, vr21, vr16, vr1, vr10, vr11  // vr10 t62a vr11 t33a
-
     vldrepl.w     vr20,     t0,       16           // 1660
     vldrepl.w     vr21,     t0,       20           // 3745
     vmul_vssrarni_hw vr2, vr20, vr21, vr16, vr2, vr12, vr13  // vr12 t34a vr13 t61a
-
     vldrepl.w     vr20,     t0,       24           // 3822
     vldrepl.w     vr21,     t0,       28           // -1474
     vmul_vssrarni_hw vr3, vr20, vr21, vr16, vr3, vr14, vr15  // vr14 t60a vr15 t35a
@@ -7264,7 +2941,6 @@ endconst
 
     vldrepl.w     vr20,     t0,       40           // 4017
     vldrepl.w     vr21,     t0,       44           // 799
-
     vmul_vmadd_w vr1, vr13, vr20, vr21, vr8, vr4
     vmul_vmsub_w vr1, vr13, vr21, vr20, vr12, vr7
     vssrarni.h.w  vr4,      vr8,      12           // t61a
@@ -7300,21 +2976,18 @@ endconst
     vsadd.h       vr13,     vr5,      vr4          // t47
     vsadd.h       vr14,     vr7,      vr6          // t48
     vssub.h       vr15,     vr7,      vr6          // t55
-
     vldrepl.w     vr20,     t0,       8            // 1567
     vldrepl.w     vr21,     t0,       12           // 3784
     vmul_vmadd_w  vr11, vr9, vr21, vr20, vr0, vr2
     vmul_vmsub_w  vr11, vr9, vr20, vr21, vr1, vr3
     vssrarni.h.w  vr2,      vr0,      12           // t56a
     vssrarni.h.w  vr3,      vr1,      12           // t39a
-
     vmul_vmadd_w  vr15, vr12, vr21, vr20, vr0, vr4
     vmul_vmsub_w  vr15, vr12, vr20, vr21, vr1, vr5
     vneg.w        vr0,      vr0
     vneg.w        vr4,      vr4
     vssrarni.h.w  vr5,      vr1,      12           // t55a
     vssrarni.h.w  vr4,      vr0,      12           // t40a
-
     vsadd.h       vr9,      vr8,      vr13         // t32a
     vssub.h       vr11,     vr8,      vr13         // t47a
     vsadd.h       vr6,      vr3,      vr4          // t39
@@ -7349,7 +3022,6 @@ endconst
 .macro dct64_step3_lsx
     //                t0   t1   t2   t3   t4    t5    t6    t7
     vld_x8 t3, 0, 16, vr2, vr3, vr7, vr8, vr11, vr12, vr16, vr17
-
     vld           vr9,      t5,       16*24    // t56
     vld           vr6,      t5,       16*24+16 // t57a
     vld           vr13,     t5,       16*24+32 // t58
@@ -7358,7 +3030,6 @@ endconst
     vld           vr4,      t4,       16*24-32 // t61a
     vld           vr1,      t4,       16*24-16 // t62
     vld           vr15,     t4,       16*24    // t63a
-
     vsadd.h       vr20,     vr2,      vr15     // c[0]
     vssub.h       vr21,     vr2,      vr15     // c[63]
     vsadd.h       vr22,     vr3,      vr1      // c[1]
@@ -7367,7 +3038,6 @@ endconst
     vssub.h       vr25,     vr7,      vr4      // c[61]
     vsadd.h       vr26,     vr8,      vr14     // c[3]
     vssub.h       vr27,     vr8,      vr14     // c[60]
-
     vsadd.h       vr28,     vr11,     vr10     // c[4]
     vssub.h       vr29,     vr11,     vr10     // c[59]
     vsadd.h       vr30,     vr12,     vr13     // c[5]
@@ -7379,7 +3049,6 @@ endconst
 .endm // dct64_step3_lsx
 
 .macro dct64_step4_lsx transpose8x8, shift, start0, stride0, start1, stride1
-
     dct64_step3_lsx
 
 .ifnb \transpose8x8
@@ -7402,11 +3071,9 @@ endconst
     vst_x8 t7, \start0, \stride0, vr20, vr22, vr24, vr26, vr28, vr30, vr2, vr1
 
     vst_x8 t7, \start1, \stride1, vr3, vr15, vr31, vr29, vr27, vr25, vr23, vr21
-
 .endm // dct64_step4_lsx
 
 .macro dct64_step5_lsx in0, in1, in2, in3, in4, in5, in6, in7
-
     fld.d         f4,       t0,       0
     fldx.d        f5,       t0,       a1
     fld.d         f6,       t6,       0
@@ -7417,11 +3084,9 @@ endconst
     fldx.d        f9,       t0,       a1
     fld.d         f10,      t6,       0
     fldx.d        f11,      t6,       a1
-
 .irp i, vr4, vr5, vr6, vr7, vr8, vr9, vr10, vr11
     vsllwil.hu.bu   \i,      \i,       0
 .endr
-
     vsrari.h      vr20,     \in0,     4
     vsrari.h      vr22,     \in1,     4
     vsrari.h      vr24,     \in2,     4
@@ -7430,7 +3095,6 @@ endconst
     vsrari.h      vr30,     \in5,     4
     vsrari.h      vr2,      \in6,     4
     vsrari.h      vr1,      \in7,     4
-
     vadd.h        vr4,      vr4,      vr20
     vadd.h        vr5,      vr5,      vr22
     vadd.h        vr6,      vr6,      vr24
@@ -7439,7 +3103,6 @@ endconst
     vadd.h        vr9,      vr9,      vr30
     vadd.h        vr10,     vr10,     vr2
     vadd.h        vr11,     vr11,     vr1
-
     vssrani.bu.h  vr5,      vr4,      0
     vssrani.bu.h  vr7,      vr6,      0
     vssrani.bu.h  vr9,      vr8,      0
@@ -7447,35 +3110,44 @@ endconst
 
     vstelm.d      vr5,      t1,       0,     0
     vstelm.d      vr5,      t2,       0,     1
-
     alsl.d        t1,       a1,       t1,    1
     alsl.d        t2,       a1,       t2,    1
     vstelm.d      vr7,      t1,       0,     0
     vstelm.d      vr7,      t2,       0,     1
-
     alsl.d        t1,       a1,       t1,    1
     alsl.d        t2,       a1,       t2,    1
     vstelm.d      vr9,      t1,       0,     0
     vstelm.d      vr9,      t2,       0,     1
-
     alsl.d        t1,       a1,       t1,    1
     alsl.d        t2,       a1,       t2,    1
     vstelm.d      vr11,     t1,       0,     0
     vstelm.d      vr11,     t2,       0,     1
 .endm // dct64_step5_lsx
 
-.macro dct_8x32_tx64_new_lsx vld_loc0, stride0, vld_loc1, stride1
+.macro dct_8x32_tx64_new_lsx vld_loc0, stride0, vld_loc1, stride1, rect2
     vld_x8 t2, \vld_loc0, \stride0, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
 
-    dct_8x16_tx64_core_lsx
+    dct_8x16_tx64_core_lsx \rect2
 
     vst_x16 t3, 0, 16, vr22, vr18, vr17, vr28, vr20, vr14, vr15, vr16, \
             vr27, vr30, vr23, vr21, vr29, vr26, vr25, vr24
 
+    vxor.v        vr31,     vr31,     vr31
+    vst_x8 t2, \vld_loc0, \stride0, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
     vld_x8 t2, \vld_loc1, \stride1, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
 
+    vst_x8 t2, \vld_loc1, \stride1, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
     la.local      t0,       idct_coeffs
 
+.ifc \rect2, rect2_lsx
+    vldrepl.w     vr23,      t0,       0        // 2896
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    rect2_lsx \i, vr23, \i
+.endr
+.endif
+
     vldrepl.w     vr20,     t0,       64           // 201
     vldrepl.w     vr21,     t0,       68           // 4091
     vsllwil.w.h   vr22,     vr0,      0
@@ -7695,7 +3367,6 @@ endconst
 
     // t31 t30a t29 t28a t27 t26a t25 t24a t23a t22 t21a t20 t19a t18 t17a t16
     // vr30 vr28 vr27 vr29 vr7 vr6 vr10 vr8 vr31 vr19 vr25 vr5 vr9 vr4 vr0 vr3
-
     vld_x8 t3, 0, 16, vr11, vr12, vr13, vr14, vr15, vr16, vr17, vr18
 
     vsadd.h       vr1,      vr11,     vr30         // c[0]
@@ -7743,9 +3414,34 @@ endconst
     vst_x8 t3, 256, 16, vr16, vr15, vr29, vr27, vr28, vr30, vr26, vr2
 .endm // dct_8x32_tx64_new_lsx
 
-function inv_txfm_add_dct_dct_64x64_8bpc_lsx
-    bnez          a3,       .NO_HAS_DCONLY_64x64
+.macro DST_ADD_W64 in0, in1, in2, in3, in4, in5, in6, in7
+    vsllwil.hu.bu vr4,      vr10,     0
+    vsllwil.hu.bu vr5,      vr11,     0
+    vsllwil.hu.bu vr6,      vr12,     0
+    vsllwil.hu.bu vr7,      vr13,     0
+    vexth.hu.bu   vr10,     vr10
+    vexth.hu.bu   vr11,     vr11
+    vexth.hu.bu   vr12,     vr12
+    vexth.hu.bu   vr13,     vr13
+    vadd.h        vr4,      vr4,      \in0
+    vadd.h        vr10,     vr10,     \in1
+    vadd.h        vr5,      vr5,      \in2
+    vadd.h        vr11,     vr11,     \in3
+    vadd.h        vr6,      vr6,      \in4
+    vadd.h        vr12,     vr12,     \in5
+    vadd.h        vr7,      vr7,      \in6
+    vadd.h        vr13,     vr13,     \in7
+    vssrani.bu.h  vr10,     vr4,      0
+    vssrani.bu.h  vr11,     vr5,      0
+    vssrani.bu.h  vr12,     vr6,      0
+    vssrani.bu.h  vr13,     vr7,      0
+    vst           vr10,     a0,       0
+    vst           vr11,     a0,       16
+    vst           vr12,     a0,       32
+    vst           vr13,     a0,       48
+.endm
 
+.macro idct_dc_w64 w, h, shift
     ld.h          t2,       a2,       0
     vldi          vr0,      0x8b5
     vreplgr2vr.w  vr1,      t2
@@ -7753,122 +3449,135 @@ function inv_txfm_add_dct_dct_64x64_8bpc_lsx
     vmul.w        vr2,      vr0,      vr1
     st.h          zero,     a2,       0
     vsrari.w      vr2,      vr2,      8
-    vld           vr3,      a0,       48
-    vsrari.w      vr2,      vr2,      2
-    vld           vr1,      a0,       16
+    vld           vr13,     a0,       48
+
+.if (2*\w == \h) || (2*\h == \w)
+    vmul.w        vr2,      vr2,      vr0
+    vsrari.w      vr2,      vr2,      8
+.endif
+
+.if \shift>0
+    vsrari.w      vr2,      vr2,      \shift
+.endif
+    vld           vr11,     a0,       16
     vmadd.w       vr20,     vr2,      vr0
-    vld           vr2,      a0,       32
+    vld           vr12,     a0,       32
     vssrarni.h.w  vr20,     vr20,     12
-    vld           vr0,      a0,       0
+    vld           vr10,     a0,       0
+.endm
 
-    vsllwil.hu.bu vr4,      vr0,      0
-    vsllwil.hu.bu vr5,      vr1,      0
-    vsllwil.hu.bu vr6,      vr2,      0
-    vsllwil.hu.bu vr7,      vr3,      0
-    vexth.hu.bu   vr0,      vr0
-    vexth.hu.bu   vr1,      vr1
-    vexth.hu.bu   vr2,      vr2
-    vexth.hu.bu   vr3,      vr3
-    vadd.h        vr8,      vr4,      vr20
-    vadd.h        vr9,      vr0,      vr20
-    vadd.h        vr10,     vr5,      vr20
-    vadd.h        vr11,     vr1,      vr20
-    vadd.h        vr12,     vr6,      vr20
-    vadd.h        vr13,     vr2,      vr20
-    vadd.h        vr14,     vr7,      vr20
-    vadd.h        vr15,     vr3,      vr20
-    vssrani.bu.h  vr9,      vr8,      0
-    vssrani.bu.h  vr11,     vr10,     0
-    vssrani.bu.h  vr13,     vr12,     0
-    vssrani.bu.h  vr15,     vr14,     0
-    vst           vr9,      a0,       0
-    vst           vr11,     a0,       16
-    vst           vr13,     a0,       32
-    vst           vr15,     a0,       48
+function inv_txfm_add_dct_dct_64x64_8bpc_lsx
+    bnez          a3,       .NO_HAS_DCONLY_64x64
 
-.rept 63
+    idct_dc_w64 64, 64, 2
+
+    DST_ADD_W64 vr20, vr20, vr20, vr20, vr20, vr20, vr20, vr20
+
+    li.w          t3,       63
+.loop63:
     add.d         a0,       a0,       a1
-    vld           vr0,      a0,       0
-    vld           vr1,      a0,       16
-    vld           vr2,      a0,       32
-    vld           vr3,      a0,       48
-    vsllwil.hu.bu vr4,      vr0,      0
-    vsllwil.hu.bu vr5,      vr1,      0
-    vsllwil.hu.bu vr6,      vr2,      0
-    vsllwil.hu.bu vr7,      vr3,      0
-    vexth.hu.bu   vr0,      vr0
-    vexth.hu.bu   vr1,      vr1
-    vexth.hu.bu   vr2,      vr2
-    vexth.hu.bu   vr3,      vr3
-    vadd.h        vr8,      vr4,      vr20
-    vadd.h        vr9,      vr0,      vr20
-    vadd.h        vr10,     vr5,      vr20
-    vadd.h        vr11,     vr1,      vr20
-    vadd.h        vr12,     vr6,      vr20
-    vadd.h        vr13,     vr2,      vr20
-    vadd.h        vr14,     vr7,      vr20
-    vadd.h        vr15,     vr3,      vr20
-    vssrani.bu.h  vr9,      vr8,      0
-    vssrani.bu.h  vr11,     vr10,     0
-    vssrani.bu.h  vr13,     vr12,     0
-    vssrani.bu.h  vr15,     vr14,     0
-    vst           vr9,      a0,       0
-    vst           vr11,     a0,       16
-    vst           vr13,     a0,       32
-    vst           vr15,     a0,       48
-.endr
+    vld           vr10,     a0,       0
+    vld           vr11,     a0,       16
+    vld           vr12,     a0,       32
+    vld           vr13,     a0,       48
+    DST_ADD_W64 vr20, vr20, vr20, vr20, vr20, vr20, vr20, vr20
+    addi.d        t3,       t3,       -1
+    blt           zero,     t3,       .loop63
     b             .DCT_DCT_64X64_END
 .NO_HAS_DCONLY_64x64:
 
     malloc_space  64*32*2+512+512
 
-    addi.d        t7,       sp,       64
-
-.macro dct64x64_core1_lsx in0, in1, in2
-    addi.d        t2,       a2,       \in0
-    addi.d        t7,       t7,       \in1
+.macro dct64x64_core1_lsx shift, rect2
+    //addi.d        t2,       a2,       \in0
+    //addi.d        t7,       t7,       \in1
     li.w          t4,       64*32*2+64
     add.d         t3,       sp,       t4
     addi.d        t6,       t3,       512
     add.d         t5,       t6,       zero
 
-    dct_8x32_tx64_new_lsx 0, 256, 128, 256
+    dct_8x32_tx64_new_lsx 0, 256, 128, 256, \rect2
 
     la.local      t0,       idct64_coeffs
+    vxor.v        vr31,     vr31,     vr31
 
-    addi.d        t2,       a2,       \in2         // 32 ...
+    //addi.d        a4,       a2,       \in2         // 32 ...
     // in1/31/17/15 -> t32a/33/34a/35/60/61a/62/63a
-    vld           vr0,      t2,       128*0        // in1
-    vld           vr1,      t2,       128*15       // in31
-    vld           vr2,      t2,       128*8        // in17
-    vld           vr3,      t2,       128*7        // in15
+    vld           vr0,      a4,       128*0        // in1
+    vld           vr1,      a4,       128*15       // in31
+    vld           vr2,      a4,       128*8        // in17
+    vld           vr3,      a4,       128*7        // in15
+    la.local      a6,       idct_coeffs
+.ifc \rect2, rect2_lsx
+    vldrepl.w     vr23,      a6,       0        // 2896
+.irp i, vr0, vr1, vr2, vr3
+    rect2_lsx \i, vr23, \i
+.endr
+.endif
+    vst           vr31,     a4,       128*0
+    vst           vr31,     a4,       128*15
+    vst           vr31,     a4,       128*8
+    vst           vr31,     a4,       128*7
     dct64_step1_lsx
 
     addi.d        t0,       t0,       48
     addi.d        t6,       t6,       128
     // in7/25/23/ 9 -> t56a/57/58a/59/36/37a/38/39a
-    vld           vr0,      t2,       128*3        // in7
-    vld           vr1,      t2,       128*12       // in25
-    vld           vr2,      t2,       128*11       // in23
-    vld           vr3,      t2,       128*4        // in9
+    vld           vr0,      a4,       128*3        // in7
+    vld           vr1,      a4,       128*12       // in25
+    vld           vr2,      a4,       128*11       // in23
+    vld           vr3,      a4,       128*4        // in9
+    la.local      a6,       idct_coeffs
+.ifc \rect2, rect2_lsx
+    vldrepl.w     vr23,      a6,       0        // 2896
+.irp i, vr0, vr1, vr2, vr3
+    rect2_lsx \i, vr23, \i
+.endr
+.endif
+    vst           vr31,     a4,       128*3
+    vst           vr31,     a4,       128*12
+    vst           vr31,     a4,       128*11
+    vst           vr31,     a4,       128*4
     dct64_step1_lsx
 
     addi.d        t0,       t0,       48
     addi.d        t6,       t6,       128
     // in5/27/21/11 -> t40a/41/42a/43/52/53a/54/55a
-    vld           vr0,      t2,       128*2        // in5
-    vld           vr1,      t2,       128*13       // in27
-    vld           vr2,      t2,       128*10       // in21
-    vld           vr3,      t2,       128*5        // in11
+    vld           vr0,      a4,       128*2        // in5
+    vld           vr1,      a4,       128*13       // in27
+    vld           vr2,      a4,       128*10       // in21
+    vld           vr3,      a4,       128*5        // in11
+    la.local      a6,       idct_coeffs
+.ifc \rect2, rect2_lsx
+    vldrepl.w     vr23,      a6,      0        // 2896
+.irp i, vr0, vr1, vr2, vr3
+    rect2_lsx \i, vr23, \i
+.endr
+.endif
+    vst           vr31,     a4,       128*2
+    vst           vr31,     a4,       128*13
+    vst           vr31,     a4,       128*10
+    vst           vr31,     a4,       128*5
     dct64_step1_lsx
 
     addi.d        t0,       t0,       48
     addi.d        t6,       t6,       128
     // in3/29/19/13 -> t48a/49/50a/51/44/45a/46/47a
-    vld           vr0,      t2,       128*1        // in3
-    vld           vr1,      t2,       128*14       // in29
-    vld           vr2,      t2,       128*9        // in19
-    vld           vr3,      t2,       128*6        // in13
+    vld           vr0,      a4,       128*1        // in3
+    vld           vr1,      a4,       128*14       // in29
+    vld           vr2,      a4,       128*9        // in19
+    vld           vr3,      a4,       128*6        // in13
+    la.local      a6,       idct_coeffs
+.ifc \rect2, rect2_lsx
+    vldrepl.w     vr23,      a6,       0        // 2896
+.irp i, vr0, vr1, vr2, vr3
+    rect2_lsx \i, vr23, \i
+.endr
+.endif
+    vst           vr31,     a4,       128*1
+    vst           vr31,     a4,       128*14
+    vst           vr31,     a4,       128*9
+    vst           vr31,     a4,       128*6
     dct64_step1_lsx
 
     la.local      t0,       idct_coeffs
@@ -7894,38 +3603,73 @@ function inv_txfm_add_dct_dct_64x64_8bpc_lsx
     li.w          t4,       64*32*2+64+512
     add.d         t5,       t4,       sp
     addi.d        t4,       t5,       16*7
-    dct64_step4_lsx transpose8x8, 2, 0, 128, 112, 128
+    dct64_step4_lsx transpose8x8, \shift, 0, 128, 112, 128
 
     addi.d        t3,       t3,       128
     addi.d        t4,       t4,       -16*8
     addi.d        t5,       t5,       -16*8
-    dct64_step4_lsx transpose8x8, 2, 16, 128, 96, 128
+    dct64_step4_lsx transpose8x8, \shift, 16, 128, 96, 128
 
     addi.d        t5,       t5,       -16*8
     addi.d        t4,       t4,       -16*8
     addi.d        t3,       t3,       128
-    dct64_step4_lsx transpose8x8, 2, 32, 128, 80, 128
+    dct64_step4_lsx transpose8x8, \shift, 32, 128, 80, 128
 
     addi.d        t5,       t5,       -16*8
     addi.d        t4,       t4,       -16*8
     addi.d        t3,       t3,       128
-    dct64_step4_lsx transpose8x8, 2, 48, 128, 64, 128
+    dct64_step4_lsx transpose8x8, \shift, 48, 128, 64, 128
 .endm
+    la.local      t8,       eob_32x32
+    addi.d        t2,       a2,       0
+    addi.d        t7,       sp,       64
+    addi.d        t7,       t7,       0
+    addi.d        a4,       a2,       64
+.DCT_DCT_EOB_64x64:
+    ld.h          a5,       t8,       0
+    addi.d        t8,       t8,       2
+    dct64x64_core1_lsx 2, no_rect2
+    addi.d        t2,       t2,       16
+    addi.d        t7,       t7,       128*8
+    addi.d        a4,       a4,       16
+    bge           a3,       a5,       .DCT_DCT_EOB_64x64
+
+    la.local      t8,       eob_32x32
+    vxor.v        vr31,     vr31,     vr31
+
+    ld.h          t7,       t8,       4
+    bge           a3,       t7,       .DCT_DCT_EOB_64x64_END
+    li.d          t1,       1024*3+64
+    add.d         t0,       sp,       t1
+.rept 4
+    vst_x16 t0, 0, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+            vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+    addi.d t0, t0, 256
+.endr
 
-    dct64x64_core1_lsx 0, 0, 64
-
-    dct64x64_core1_lsx 16, 128*8, 64+16
-
-    dct64x64_core1_lsx 32, 128*8, 64+16*2
-
-    dct64x64_core1_lsx 48, 128*8, 64+16*3
-
-    vreplgr2vr.h  vr31,     zero
-.irp i, 0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512, 528, 544, 560, 576, 592, 608, 624, 640, 656, 672, 688, 704, 720, 736, 752, 768, 784, 800, 816, 832, 848, 864, 880, 896, 912, 928, 944, 960, 976, 992, 1008, 1024, 1040, 1056, 1072, 1088, 1104, 1120, 1136, 1152, 1168, 1184, 1200, 1216, 1232, 1248, 1264, 1280, 1296, 1312, 1328, 1344, 1360, 1376, 1392, 1408, 1424, 1440, 1456, 1472, 1488, 1504, 1520, 1536, 1552, 1568, 1584, 1600, 1616, 1632, 1648, 1664, 1680, 1696, 1712, 1728, 1744, 1760, 1776, 1792, 1808, 1824, 1840, 1856, 1872, 1888, 1904, 1920, 1936, 1952, 1968, 1984, 2000, 2016, 2032
-    vst           vr31,     a2,       \i
+    ld.h          t7,       t8,       2
+    bge           a3,       t7,       .DCT_DCT_EOB_64x64_END
+    li.d          t1,       1024*2+64
+    add.d         t0,       sp,       t1
+.rept 4
+    vst_x16 t0, 0, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+            vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+    addi.d        t0,       t0,       256
+.endr
+    ld.h          t7,       t8,       0
+    bge           a3,       t7,       .DCT_DCT_EOB_64x64_END
+
+    li.d          t1,       1024*1+64
+    add.d         t0,       sp,       t1
+.rept 4
+    vst_x16 t0, 0, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+            vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+    addi.d        t0,       t0,       256
 .endr
 
-.macro dct64x64_core2_lsx in0, in1
+.DCT_DCT_EOB_64x64_END:
+
+.macro dct64x64_core2_lsx in0, in1, rect2
     addi.d        t2,       sp,       64+\in0
     addi.d        t7,       sp,       64+\in0
     li.w          t4,       64*32*2+64
@@ -7933,12 +3677,11 @@ function inv_txfm_add_dct_dct_64x64_8bpc_lsx
     addi.d        t6,       t3,       512
     add.d         t5,       t6,       zero
 
-    addi.d        t2, t2, 1024
-    addi.d        t2, t2, 1024
-    dct_8x32_tx64_new_lsx -2048, 512, 256-2048, 512
+    addi.d        t2,       t2,       1024
+    addi.d        t2,       t2,       1024
+    dct_8x32_tx64_new_lsx -2048, 512, 256-2048, 512, \rect2
 
     la.local      t0,       idct64_coeffs
-
     addi.d        t2,       sp,       64+64*2+\in0
     addi.d        t4,       t2,       256*7
     addi.d        t4,       t4,       256
@@ -7999,7 +3742,6 @@ function inv_txfm_add_dct_dct_64x64_8bpc_lsx
     addi.d        a0,       a0,       \in1
     // 0 - 7, 56 -63
     dct64_step3_lsx
-
     li.w          t8,       0
     mul.w         t0,       t8,       a1
     add.d         t0,       a0,       t0
@@ -8007,7 +3749,6 @@ function inv_txfm_add_dct_dct_64x64_8bpc_lsx
     addi.d        t1,       t0,       0
     add.d         t2,       t0,       a1
     dct64_step5_lsx vr20, vr22, vr24, vr26, vr28, vr30, vr2, vr1
-
     li.w          t8,       56
     mul.w         t0,       t8,       a1
     add.d         t0,       a0,       t0
@@ -8021,7 +3762,6 @@ function inv_txfm_add_dct_dct_64x64_8bpc_lsx
     addi.d        t4,       t4,       -16*8
     addi.d        t5,       t5,       -16*8
     dct64_step3_lsx
-
     li.w          t8,       8
     mul.w         t0,       t8,       a1
     add.d         t0,       t0,       a0
@@ -8029,7 +3769,6 @@ function inv_txfm_add_dct_dct_64x64_8bpc_lsx
     addi.d        t1,       t0,       0
     add.d         t2,       t0,       a1
     dct64_step5_lsx vr20, vr22, vr24, vr26, vr28, vr30, vr2, vr1
-
     li.w          t8,       48
     mul.w         t0,       t8,       a1
     add.d         t0,       t0,       a0
@@ -8043,7 +3782,6 @@ function inv_txfm_add_dct_dct_64x64_8bpc_lsx
     addi.d        t4,       t4,       -16*8
     addi.d        t5,       t5,       -16*8
     dct64_step3_lsx
-
     li.w          t8,       16
     mul.w         t0,       t8,       a1
     add.d         t0,       t0,       a0
@@ -8051,7 +3789,6 @@ function inv_txfm_add_dct_dct_64x64_8bpc_lsx
     addi.d        t1,       t0,       0
     add.d         t2,       t0,       a1
     dct64_step5_lsx vr20, vr22, vr24, vr26, vr28, vr30, vr2, vr1
-
     li.w          t8,       40
     mul.w         t0,       t8,       a1
     add.d         t0,       t0,       a0
@@ -8065,7 +3802,6 @@ function inv_txfm_add_dct_dct_64x64_8bpc_lsx
     addi.d        t4,       t4,       -16*8
     addi.d        t5,       t5,       -16*8
     dct64_step3_lsx
-
     li.w          t8,       24
     mul.w         t0,       t8,       a1
     add.d         t0,       t0,       a0
@@ -8073,7 +3809,6 @@ function inv_txfm_add_dct_dct_64x64_8bpc_lsx
     addi.d        t1,       t0,       0
     add.d         t2,       t0,       a1
     dct64_step5_lsx vr20, vr22, vr24, vr26, vr28, vr30, vr2, vr1
-
     li.w          t8,       32
     mul.w         t0,       t8,       a1
     add.d         t0,       t0,       a0
@@ -8082,23 +3817,965 @@ function inv_txfm_add_dct_dct_64x64_8bpc_lsx
     add.d         t2,       t0,       a1
     dct64_step5_lsx vr3, vr15, vr31, vr29, vr27, vr25, vr23, vr21
 .endm
+    dct64x64_core2_lsx 16*0, 0, no_rect2
+    dct64x64_core2_lsx 16*1, 8, no_rect2
+    dct64x64_core2_lsx 16*2, 8, no_rect2
+    dct64x64_core2_lsx 16*3, 8, no_rect2
+    dct64x64_core2_lsx 16*4, 8, no_rect2
+    dct64x64_core2_lsx 16*5, 8, no_rect2
+    dct64x64_core2_lsx 16*6, 8, no_rect2
+    dct64x64_core2_lsx 16*7, 8, no_rect2
+
+    free_space 64*32*2+512+512
+.DCT_DCT_64X64_END:
+endfunc
+
+function inv_txfm_add_dct_dct_64x32_8bpc_lsx
+    bnez          a3,       .NO_HAS_DCONLY_64x32
+
+    idct_dc_w64 64, 32, 1
 
-    dct64x64_core2_lsx 16*0, 0
+    DST_ADD_W64 vr20, vr20, vr20, vr20, vr20, vr20, vr20, vr20
 
-    dct64x64_core2_lsx 16*1, 8
+    li.w          t3,       31
+.loop31:
+    add.d         a0,       a0,       a1
+    vld           vr10,     a0,       0
+    vld           vr11,     a0,       16
+    vld           vr12,     a0,       32
+    vld           vr13,     a0,       48
+    DST_ADD_W64 vr20, vr20, vr20, vr20, vr20, vr20, vr20, vr20
+    addi.d        t3,       t3,       -1
+    blt           zero,     t3,       .loop31
+    b             .DCT_DCT_64X32_END
+.NO_HAS_DCONLY_64x32:
+    malloc_space  64*32*2+512+512
+
+    la.local      t8,       eob_32x32
+    addi.d        t2,       a2,       0
+    addi.d        t7,       sp,       64
+    addi.d        t7,       t7,       0
+    addi.d        a4,       a2,       64
+.DCT_DCT_EOB_64x32:
+    ld.h          a5,       t8,       0
+    addi.d        t8,       t8,       2
+    dct64x64_core1_lsx 1, rect2_lsx
+    addi.d        t2,       t2,       16
+    addi.d        t7,       t7,       128*8
+    addi.d        a4,       a4,       16
+    bge           a3,       a5,       .DCT_DCT_EOB_64x32
+
+    la.local      t8,       eob_32x32
+    vxor.v        vr31,     vr31,     vr31
+
+    ld.h          t7,       t8,       4
+    bge           a3,       t7,       .DCT_DCT_EOB_64x32_END
+    li.d          t1,       1024*3+64
+    add.d         t0,       sp,       t1
+.rept 4
+    vst_x16 t0, 0, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+            vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+    addi.d t0, t0, 256
+.endr
 
-    dct64x64_core2_lsx 16*2, 8
+    ld.h          t7,       t8,       2
+    bge           a3,       t7,       .DCT_DCT_EOB_64x32_END
+    li.d          t1,       1024*2+64
+    add.d         t0,       sp,       t1
+.rept 4
+    vst_x16 t0, 0, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+            vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+    addi.d        t0,       t0,       256
+.endr
 
-    dct64x64_core2_lsx 16*3, 8
+    ld.h          t7,       t8,       0
+    bge           a3,       t7,       .DCT_DCT_EOB_64x32_END
+    li.d          t1,       1024*1+64
+    add.d         t0,       sp,       t1
+.rept 4
+    vst_x16 t0, 0, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31, \
+            vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+    addi.d        t0,       t0,       256
+.endr
 
-    dct64x64_core2_lsx 16*4, 8
+.DCT_DCT_EOB_64x32_END:
+    addi.d        t2,       sp,       64
+    li.w          t4,       64*32*2+64
+    add.d         t3,       sp,       t4
+    addi.d        t5,       sp,       64
+    addi.d        t5,       t5,       1024
+    addi.d        t5,       t5,       1024
+.rept 8
+    vld_x8 t2, 0, 256, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
 
-    dct64x64_core2_lsx 16*5, 8
+    addi.d        t4,       t2,       1024
+    addi.d        t4,       t4,       1024
 
-    dct64x64_core2_lsx 16*6, 8
+    vld_x8 t4, 0, 256, vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
 
-    dct64x64_core2_lsx 16*7, 8
+    inv_dct16_lsx no_rect2
 
-    free_space 64*32*2+512+512
-.DCT_DCT_64X64_END:
+    vst_x16 t3, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+
+    addi.d        t4,       t2,       128
+    vld_x8 t4, 0, 256, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+
+    addi.d        t4,       t4,       1024
+    addi.d        t4,       t4,       1024
+
+    vld_x8 t4, 0, 256, vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
+
+    dct_8x32_core_lsx t5, t3, 0, 128, 16, -2048, 1024, -1024, 0, 128, , 4
+
+    addi.d        t2,       t2,       16
+    addi.d        t5,       t5,       16
+    addi.d        t1,       t1,       16
+.endr
+    addi.d        t2,       sp,       64
+    li.w          t3,       32
+.loop32:
+    vld           vr10,     a0,       0
+    vld           vr11,     a0,       16
+    vld           vr12,     a0,       32
+    vld           vr13,     a0,       48
+    vld_x8 t2, 0, 16, vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+    DST_ADD_W64 vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+    add.d         a0,       a0,       a1
+    addi.d        t2,       t2,       128
+    addi.d        t3,       t3,       -1
+    blt           zero,     t3,       .loop32
+
+    free_space  64*32*2+512+512
+.DCT_DCT_64X32_END:
+endfunc
+
+.macro VLD_DST_ADD_W8_H32 in0
+    vld           vr4,      t3,       0
+    vld           vr5,      t3,       16
+    vld           vr6,      t3,       32
+    vld           vr7,      t3,       48
+    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
+    addi.d        t3,       t3,       64
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       t2,     2
+    vld           vr4,      t3,       0
+    vld           vr5,      t3,       16
+    vld           vr6,      t3,       32
+    vld           vr7,      t3,       48
+    VLD_DST_ADD_W8 vr4, vr5, vr6, vr7
+    addi.d        t3,       sp,       \in0
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       t2,     2
+.endm
+
+function inv_txfm_add_dct_dct_8x32_8bpc_lsx
+    bnez          a3,       .NO_HAS_DCONLY_8x32
+
+    idct_dc 8, 32, 2
+
+    DST_ADD_W8 vr10, vr11, vr12, vr13, vr20, vr20, vr20, vr20
+.rept 7
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       a0,     1
+
+    VLD_DST_ADD_W8 vr20, vr20, vr20, vr20
+.endr
+    b             .DCT_DCT_8X32_END
+.NO_HAS_DCONLY_8x32:
+    malloc_space 512
+
+    la.local      t8,       eob_8x32
+    addi.d        t3,       sp,       64
+    addi.d        t2,       a2,       0
+.DCT_DCT_EOB_8x32:
+    ld.h          t7,       t8,       0
+    addi.d        t8,       t8,       2
+
+    vld_x8 a2, 0, 64, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+
+    inv_dct8_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, .8h
+
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    vsrari.h      \i,       \i,       2
+.endr
+
+    vxor.v        vr31,     vr31,     vr31
+    vst_x8 a2, 0, 64, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+    LSX_TRANSPOSE8x8_H vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                       vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+
+    vst_x8 t3, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+
+    addi.d        a2,       a2,       16
+    addi.d        t3,       t3,       128
+    bge           a3,       t7,       .DCT_DCT_EOB_8x32
+
+    la.local      t8,       eob_8x32
+    vxor.v        vr31,     vr31,     vr31
+    ld.h          t7,       t8,       4
+    bge           a3,       t7,       .DCT_DCT_EOB_8x32_END
+    vst_x8 sp, 64+384, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+    ld.h          t7,       t8,       2
+    bge           a3,       t7,       .DCT_DCT_EOB_8x32_END
+    vst_x8 sp, 64+256, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+    ld.h          t7,       t8,       0
+    bge           a3,       t7,       .DCT_DCT_EOB_8x32_END
+    vst_x8 sp, 64+128, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+.DCT_DCT_EOB_8x32_END:
+    addi.d        t2,       sp,       64
+    addi.d        t3,       sp,       64
+
+    vld_x16 t2, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+
+    inv_dct16_lsx .8h
+
+    vst_x16 t3, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+
+    vld_x16 t2, 16, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
+
+    dct_8x32_core_lsx t2, t3, 0, 256, 32, 0, 128, 256, 384, 16, , 4
+
+    alsl.d        t2,       a1,       a0,     1
+    addi.d        t3,       sp,       64
+
+    VLD_DST_ADD_W8_H32 320
+    VLD_DST_ADD_W8_H32 448
+    VLD_DST_ADD_W8_H32 192
+    VLD_DST_ADD_W8_H32 0
+
+    free_space 512
+.DCT_DCT_8X32_END:
+endfunc
+
+function inv_txfm_add_identity_identity_8x32_8bpc_lsx
+    la.local      t7,       eob_8x32
+    alsl.d        t2,       a1,       a0,     1
+
+.IDENTITY_IDENTITY_EOB_8x32:
+    ld.h          t6,       t7,       0
+    addi.d        t7,       t7,       2
+    vld_x8 a2, 0, 64, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+
+    vxor.v        vr23,     vr23,     vr23
+    vst_x8 a2, 0, 64, vr23, vr23, vr23, vr23, vr23, vr23, vr23, vr23
+
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+    vsrari.h       \i,       \i,       1
+.endr
+
+    LSX_TRANSPOSE8x8_H vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                   vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23, \
+                   vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+
+.irp i, vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+    vsrari.h       \i,       \i,       2
+.endr
+    VLD_DST_ADD_W8 vr16, vr17, vr18, vr19
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       a0,     1
+
+    VLD_DST_ADD_W8 vr20, vr21, vr22, vr23
+    add.d         a0,       a1,       a0
+    alsl.d        t2,       a1,       a0,     1
+
+    addi.d        a2,       a2,       16
+    bge           a3,       t6,       .IDENTITY_IDENTITY_EOB_8x32
+endfunc
+
+.macro def_fn_16x4_base txfm
+functionl inv_txfm_\txfm\()add_16x4_lsx
+    vld_x8 a2, 0, 16, vr0, vr2, vr4, vr6, vr8, vr10, vr12, vr14
+
+.ifc \txfm, identity_
+    li.w          t0,       1697
+    vreplgr2vr.w  vr20,     t0
+.irp i, vr0, vr2, vr4, vr6, vr8, vr10, vr12, vr14
+    inv_identity16_lsx \i, vr20, \i, \i, .8h
+.endr
+
+    vilvh.d       vr1,      vr0,      vr0
+    vilvh.d       vr3,      vr2,      vr2
+    vilvh.d       vr5,      vr4,      vr4
+    vilvh.d       vr7,      vr6,      vr6
+    vilvh.d       vr9,      vr8,      vr8
+    vilvh.d       vr11,     vr10,     vr10
+    vilvh.d       vr13,     vr12,     vr12
+    vilvh.d       vr15,     vr14,     vr14
+.else
+    vilvh.d       vr1,      vr0,      vr0
+    vilvh.d       vr3,      vr2,      vr2
+    vilvh.d       vr5,      vr4,      vr4
+    vilvh.d       vr7,      vr6,      vr6
+    vilvh.d       vr9,      vr8,      vr8
+    vilvh.d       vr11,     vr10,     vr10
+    vilvh.d       vr13,     vr12,     vr12
+    vilvh.d       vr15,     vr14,     vr14
+
+    move          t6,       ra
+    jirl          ra,       t7,       0
+    move          ra,       t6
+.endif
+
+    vxor.v        vr23,     vr23,     vr23
+    vst_x8 a2, 0, 16, vr23, vr23, vr23, vr23, vr23, vr23, vr23, vr23
+
+    LSX_TRANSPOSE8x4_H vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, vr0, vr1, \
+                       vr2, vr3, vr16, vr17, vr18, vr19, vr20, vr21
+
+    LSX_TRANSPOSE8x4_H vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, vr4, \
+                       vr5, vr6, vr7, vr16, vr17, vr18, vr19, vr20, vr21
+
+    vsrari.h      vr0,      vr0,      1
+    vsrari.h      vr1,      vr1,      1
+    vsrari.h      vr2,      vr2,      1
+    vsrari.h      vr3,      vr3,      1
+    move          t6,       ra
+    jirl          ra,       t8,       0
+    move          ra,       t6
+
+    vsrari.h      vr8,      vr0,      4
+    vsrari.h      vr9,      vr1,      4
+    vsrari.h      vr10,     vr2,      4
+    vsrari.h      vr11,     vr3,      4
+    vsrari.h      vr0,      vr4,      1
+    vsrari.h      vr1,      vr5,      1
+    vsrari.h      vr2,      vr6,      1
+    vsrari.h      vr3,      vr7,      1
+
+    move          t6,       ra
+    jirl          ra,       t8,       0
+    move          ra,       t6
+
+    vsrari.h      vr16,     vr0,      4
+    vsrari.h      vr17,     vr1,      4
+    vsrari.h      vr18,     vr2,      4
+    vsrari.h      vr19,     vr3,      4
+
+    alsl.d        t2,       a1,       a0,    1
+    VLD_DST_ADD_W16 vr8, vr16, vr9, vr17, vr10, vr18, vr11, vr19
+endfuncl
+.endm
+
+def_fn_16x4_base identity_
+def_fn_16x4_base
+
+.macro fn_16x4 txfm1, txfm2
+function inv_txfm_add_\txfm1\()_\txfm2\()_16x4_8bpc_lsx
+.ifc \txfm1\()_\txfm2, dct_dct
+    bnez          a3,       .NO_HAS_DCONLY_16x4
+
+    idct_dc 16, 4, 1
+
+    DST_ADD_W16 vr10, vr11, vr12, vr13, vr20, vr20, vr20, \
+                vr20, vr20, vr20, vr20, vr20
+    b             .\txfm1\()_\txfm2\()_16x4_END
+.NO_HAS_DCONLY_16x4:
+.endif
+
+.ifnc \txfm1, identity
+    la.local     t7,    inv_\txfm1\()_4h_x16_lsx
+.endif
+    la.local     t8,    inv_\txfm2\()_8h_x4_lsx
+
+.ifc \txfm1, identity
+    b            inv_txfm_identity_add_16x4_lsx
+.else
+    b            inv_txfm_add_16x4_lsx
+.endif
+.\txfm1\()_\txfm2\()_16x4_END:
+endfunc
+.endm
+
+fn_16x4 dct, dct
+fn_16x4 identity, identity
+fn_16x4 adst, dct
+
+.macro VLD_DST_ADD_W16_H32 in0
+    vld           vr14,     t3,       0
+    vld           vr15,     t3,       16
+    vld           vr16,     t3,       32
+    vld           vr17,     t3,       48
+    vld           vr18,     t5,       0
+    vld           vr19,     t5,       16
+    vld           vr20,     t5,       32
+    vld           vr21,     t5,       48
+    vsrari_h_x8 vr14, vr18, vr15, vr19, vr16, vr20, vr17, vr21, \
+                vr14, vr18, vr15, vr19, vr16, vr20, vr17, vr21, 4
+    VLD_DST_ADD_W16 vr14, vr18, vr15, vr19, vr16, vr20, vr17, vr21
+    alsl.d        a0,       a1,       a0,    2
+    alsl.d        t2,       a1,       t2,    2
+    addi.d        t3,       t3,       64
+    addi.d        t5,       t5,       64
+    vld           vr14,     t3,       0
+    vld           vr15,     t3,       16
+    vld           vr16,     t3,       32
+    vld           vr17,     t3,       48
+    vld           vr18,     t5,       0
+    vld           vr19,     t5,       16
+    vld           vr20,     t5,       32
+    vld           vr21,     t5,       48
+    vsrari_h_x8 vr14, vr18, vr15, vr19, vr16, vr20, vr17, vr21, \
+                vr14, vr18, vr15, vr19, vr16, vr20, vr17, vr21, 4
+    VLD_DST_ADD_W16 vr14, vr18, vr15, vr19, vr16, vr20, vr17, vr21
+    alsl.d        a0,       a1,       a0,    2
+    alsl.d        t2,       a1,       t2,    2
+    addi.d        t3,       sp,       \in0
+    addi.d        t5,       sp,       \in0+512
+.endm
+
+function inv_txfm_add_dct_dct_16x32_8bpc_lsx
+    bnez          a3,       .NO_HAS_DCONLY_16x32
+
+    idct_dc 16, 32, 1
+
+    DST_ADD_W16 vr10, vr11, vr12, vr13, vr20, vr20, vr20, \
+                    vr20, vr20, vr20, vr20, vr20
+.rept 7
+    alsl.d        a0,       a1,       a0,     2
+    alsl.d        t2,       a1,       a0,     1
+
+    VLD_DST_ADD_W16 vr20, vr20, vr20, vr20, vr20, vr20, vr20, vr20
+.endr
+    b             .DCT_DCT_16x32_END
+.NO_HAS_DCONLY_16x32:
+    malloc_space 512+512
+
+    addi.d        t3,       sp,       64
+    la.local      t8,       eob_16x32
+
+.DCT_DCT_EOB_16x32:
+    ld.h          t7,       t8,       0
+    addi.d        t8,       t8,       2
+    vld_x16 a2, 0, 64, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+
+    vxor.v        vr31,     vr31,     vr31
+.irp i, 0, 64, 128, 192, 256, 320, 384, 448, 512, 576, 640, 704, 768, 832, 896, 960
+    vst           vr31,     a2,       \i
+.endr
+
+    li.w          t0,       2896
+    vreplgr2vr.w  vr23,     t0
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+     vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+    rect2_lsx   \i, vr23, \i
+.endr
+
+    inv_dct16_lsx .8h
+
+    LSX_TRANSPOSE8x8_H vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+                       vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+
+    LSX_TRANSPOSE8x8_H vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, \
+                       vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, \
+                       vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+
+.irp i, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+    vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+    vsrari.h       \i,       \i,       1
+.endr
+
+    vst_x8 t3, 0, 16, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7,
+    vst_x8 t3, 512, 16, vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+
+    addi.d        a2,       a2,       16
+    addi.d        t3,       t3,       128
+    bge           a3,       t7,       .DCT_DCT_EOB_16x32
+
+    la.local      t8,       eob_16x32
+    vxor.v        vr31,     vr31,     vr31
+
+    ld.h          t7,       t8,       4
+    bge           a3,       t7,       .DCT_DCT_EOB_16x32_END
+    vst_x8 sp, 64+384, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+    vst_x8 sp, 64+896, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+    ld.h          t7,       t8,       2
+    bge           a3,       t7,       .DCT_DCT_EOB_16x32_END
+    vst_x8 sp, 64+256, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+    vst_x8 sp, 64+768, 16,  vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+    ld.h          t7,       t8,       0
+    bge           a3,       t7,       .DCT_DCT_EOB_16x32_END
+    vst_x8 sp, 64+128, 16, vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+    vst_x8 sp, 64+512+128, 16  vr31, vr31, vr31, vr31, vr31, vr31, vr31, vr31
+
+.DCT_DCT_EOB_16x32_END:
+    addi.d      t7,   sp,    64
+.rept 2
+    vld_x16 t7, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+
+    inv_dct16_lsx .8h
+
+    vst_x16 t7, 0, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15
+
+    vld_x16 t7, 16, 32, vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, \
+            vr19, vr24, vr25, vr26, vr27, vr28, vr29, vr30
+
+    dct_8x32_core_lsx t7, t7, 0, 256, 32, 0, 128, 256, 384, 16, ,
+
+    addi.d        t7,       t7,       512
+.endr
+    alsl.d        t2,       a1,       a0,    1
+    addi.d        t3,       sp,       64
+    addi.d        t5,       sp,       512+64
+
+    VLD_DST_ADD_W16_H32 320
+    VLD_DST_ADD_W16_H32 448
+    VLD_DST_ADD_W16_H32 192
+    VLD_DST_ADD_W16_H32 0
+
+    free_space 512+512
+.DCT_DCT_16x32_END:
+endfunc
+
+.macro xvmulev_xvmaddod_lasx in0, in1, in2, in3, out0, out1
+    xvmulwev.w.h   \out0,    \in0,     \in2
+    xvmulwod.w.h   \out1,    \in0,     \in2
+    xvmaddwev.w.h  \out0,    \in1,     \in3
+    xvmaddwod.w.h  \out1,    \in1,     \in3
+.endm
+
+.macro xvsrari_h_x16 in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, \
+                     in11, in12, in13, in14, in15, out0, out1, out2, out3, \
+                     out4, out5, out6, out7, out8, out9, out10, out11, out12, \
+                     out13, out14, out15, shift
+    xvsrari.h  \out0,       \in0,     \shift
+    xvsrari.h  \out1,       \in1,     \shift
+    xvsrari.h  \out2,       \in2,     \shift
+    xvsrari.h  \out3,       \in3,     \shift
+    xvsrari.h  \out4,       \in4,     \shift
+    xvsrari.h  \out5,       \in5,     \shift
+    xvsrari.h  \out6,       \in6,     \shift
+    xvsrari.h  \out7,       \in7,     \shift
+    xvsrari.h  \out8,       \in8,     \shift
+    xvsrari.h  \out9,       \in9,     \shift
+    xvsrari.h  \out10,      \in10,    \shift
+    xvsrari.h  \out11,      \in11,    \shift
+    xvsrari.h  \out12,      \in12,    \shift
+    xvsrari.h  \out13,      \in13,    \shift
+    xvsrari.h  \out14,      \in14,    \shift
+    xvsrari.h  \out15,      \in15,    \shift
+.endm
+
+.macro xvpermi_q_x2 in0, in1, in2, in3, out0, out1, out2, out3, tmp0, tmp1
+    xvor.v      \tmp0,      \in0,     \in0
+    xvor.v      \tmp1,      \in1,     \in1
+    xvpermi.q   \out0,      \in2,     0x02
+    xvpermi.q   \out1,      \in3,     0x02
+    xvpermi.q   \out2,      \tmp0,    0x31
+    xvpermi.q   \out3,      \tmp1,    0x31
+.endm
+
+.macro DST_ADD_W16_LASX in0, in1, in2, in3, in4, in5, in6, in7
+    vext2xv.hu.bu xr0,      \in0
+    vext2xv.hu.bu xr1,      \in1
+    vext2xv.hu.bu xr2,      \in2
+    vext2xv.hu.bu xr3,      \in3
+    xvadd.h       xr0,      xr0,      \in4
+    xvadd.h       xr1,      xr1,      \in5
+    xvadd.h       xr2,      xr2,      \in6
+    xvadd.h       xr3,      xr3,      \in7
+    xvssrani.bu.h xr1,      xr0,      0
+    xvssrani.bu.h xr3,      xr2,      0
+    xvpermi.d     xr0,      xr1,      0b11011000
+    xvpermi.d     xr2,      xr3,      0b11011000
+    xvpermi.d     xr1,      xr0,      0b00001110
+    xvpermi.d     xr3,      xr2,      0b00001110
+    vst           vr0,      a0,       0
+    vstx          vr1,      a0,       a1
+    vst           vr2,      t2,       0
+    vstx          vr3,      t2,       a1
+.endm
+
+.macro XVLD_DST_ADD_W16 in0, in1, in2, in3
+    vld           vr0,      a0,       0
+    vldx          vr1,      a0,       a1
+    vld           vr2,      t2,       0
+    vldx          vr3,      t2,       a1
+    DST_ADD_W16_LASX xr0, xr1, xr2, xr3, \in0, \in1, \in2, \in3
+.endm
+
+.macro inv_adst16_lasx
+    la.local      t0,       iadst16_coeffs_h
+
+    xvldrepl.h    xr20,     t0,       0        // 4091
+    xvldrepl.h    xr21,     t0,       2        // 201
+    xvmulev_xvmaddod_lasx xr15, xr0, xr20, xr21, xr16, xr18
+    xvneg.h       xr20,     xr20
+    xvmulev_xvmaddod_lasx xr15, xr0, xr21, xr20, xr17, xr19
+    xvilvl.w      xr15,     xr18,     xr16
+    xvilvl.w      xr0,      xr19,     xr17
+    xvilvh.w      xr18,     xr18,     xr16
+    xvilvh.w      xr19,     xr19,     xr17
+    xvssrarni.h.w xr18,     xr15,     12       // t0
+    xvssrarni.h.w xr19,     xr0,      12       // t1
+
+    xvldrepl.h    xr20,     t0,       4        // 3973
+    xvldrepl.h    xr21,     t0,       6        // 995
+    xvmulev_xvmaddod_lasx xr13, xr2, xr20, xr21, xr16, xr0
+    xvneg.h       xr20,     xr20
+    xvmulev_xvmaddod_lasx xr13, xr2, xr21, xr20, xr17, xr15
+    xvilvl.w      xr13,     xr0,      xr16
+    xvilvl.w      xr2,      xr15,     xr17
+    xvilvh.w      xr0,      xr0,      xr16
+    xvilvh.w      xr15,     xr15,     xr17
+    xvssrarni.h.w xr0,      xr13,     12       // t2
+    xvssrarni.h.w xr15,     xr2,      12       // t3
+
+    xvldrepl.h    xr20,     t0,        8       // 3703
+    xvldrepl.h    xr21,     t0,        10      // 1751
+    xvmulev_xvmaddod_lasx xr11, xr4, xr20, xr21, xr16, xr2
+    xvneg.h       xr20,     xr20
+    xvmulev_xvmaddod_lasx xr11, xr4, xr21, xr20, xr17, xr13
+    xvilvl.w      xr11,     xr2,       xr16
+    xvilvl.w      xr4,      xr13,      xr17
+    xvilvh.w      xr2,      xr2,       xr16
+    xvilvh.w      xr13,     xr13,      xr17
+    xvssrarni.h.w xr2,      xr11,      12       // t4
+    xvssrarni.h.w xr13,     xr4,       12       // t5
+
+    xvldrepl.h    xr20,     t0,        12       // 3290 -> 1645
+    xvldrepl.h    xr21,     t0,        14       // 2440 -> 1220
+    xvmulev_xvmaddod_lasx xr9, xr6, xr20, xr21, xr16, xr4
+    xvneg.h       xr20,     xr20
+    xvmulev_xvmaddod_lasx xr9, xr6, xr21, xr20, xr17, xr11
+    xvilvl.w      xr9,      xr4,       xr16
+    xvilvl.w      xr6,      xr11,      xr17
+    xvilvh.w      xr4,      xr4,       xr16
+    xvilvh.w      xr11,     xr11,      xr17
+    xvssrarni.h.w xr4,      xr9,       12       // t6
+    xvssrarni.h.w xr11,     xr6,       12       // t7
+
+    xvldrepl.h    xr20,     t0,        16       // 2751
+    xvldrepl.h    xr21,     t0,        18       // 3035
+    xvmulev_xvmaddod_lasx xr7, xr8, xr20, xr21, xr16, xr6
+    xvneg.h       xr20,     xr20
+    xvmulev_xvmaddod_lasx xr7, xr8, xr21, xr20, xr17, xr9
+    xvilvl.w      xr7,      xr6,       xr16
+    xvilvl.w      xr8,      xr9,       xr17
+    xvilvh.w      xr6,      xr6,       xr16
+    xvilvh.w      xr9,      xr9,       xr17
+    xvssrarni.h.w xr6,      xr7,       12       // t8
+    xvssrarni.h.w xr9,      xr8,       12       // t9
+
+    xvldrepl.h    xr20,     t0,        20       // 2106
+    xvldrepl.h    xr21,     t0,        22       // 3513
+    xvmulev_xvmaddod_lasx xr5, xr10, xr20, xr21, xr16, xr7
+    xvneg.h       xr20,     xr20
+    xvmulev_xvmaddod_lasx xr5, xr10, xr21, xr20, xr17, xr8
+    xvilvl.w      xr5,      xr7,       xr16
+    xvilvl.w      xr10,     xr8,       xr17
+    xvilvh.w      xr7,      xr7,       xr16
+    xvilvh.w      xr8,      xr8,       xr17
+    xvssrarni.h.w xr7,      xr5,       12       // t10
+    xvssrarni.h.w xr8,      xr10,      12       // t11
+
+    xvldrepl.h    xr20,     t0,        24       // 1380
+    xvldrepl.h    xr21,     t0,        26       // 3857
+    xvmulev_xvmaddod_lasx xr3, xr12, xr20, xr21, xr16, xr5
+    xvneg.h       xr20,     xr20
+    xvmulev_xvmaddod_lasx xr3, xr12, xr21, xr20, xr17, xr10
+    xvilvl.w      xr3,      xr5,       xr16
+    xvilvl.w      xr12,     xr10,      xr17
+    xvilvh.w      xr5,      xr5,       xr16
+    xvilvh.w      xr10,     xr10,      xr17
+    xvssrarni.h.w xr5,      xr3,       12       // t12
+    xvssrarni.h.w xr10,     xr12,      12       // t13
+
+    xvldrepl.h    xr20,     t0,        28       // 601
+    xvldrepl.h    xr21,     t0,        30       // 4052
+    xvmulev_xvmaddod_lasx xr1, xr14, xr20, xr21, xr16, xr3
+    xvneg.h       xr20,     xr20
+    xvmulev_xvmaddod_lasx xr1, xr14, xr21, xr20, xr17, xr12
+    xvilvl.w      xr1,      xr3,       xr16
+    xvilvl.w      xr14,     xr12,      xr17
+    xvilvh.w      xr3,      xr3,       xr16
+    xvilvh.w      xr12,     xr12,      xr17
+    xvssrarni.h.w xr3,      xr1,       12       // t14
+    xvssrarni.h.w xr12,     xr14,      12       // t15
+
+    xvsadd.h      xr1,      xr18,      xr6      // t0a
+    xvssub.h      xr14,     xr18,      xr6      // t8a
+    xvsadd.h      xr16,     xr19,      xr9      // t1a
+    xvssub.h      xr17,     xr19,      xr9      // t9a
+    xvsadd.h      xr6,      xr0,       xr7      // t2a
+    xvssub.h      xr18,     xr0,       xr7      // t10a
+    xvsadd.h      xr9,      xr15,      xr8      // t3a
+    xvssub.h      xr19,     xr15,      xr8      // t11a
+    xvsadd.h      xr0,      xr2,       xr5      // t4a
+    xvssub.h      xr7,      xr2,       xr5      // t12a
+    xvsadd.h      xr8,      xr13,      xr10     // t5a
+    xvssub.h      xr15,     xr13,      xr10     // t13a
+    xvsadd.h      xr2,      xr4,       xr3      // t6a
+    xvssub.h      xr5,      xr4,       xr3      // t14a
+    xvsadd.h      xr10,     xr11,      xr12     // t7a
+    xvssub.h      xr13,     xr11,      xr12     // t15a
+
+    la.local      t0,       idct_coeffs_h
+
+    xvldrepl.h    xr20,     t0,        8        // 799
+    xvldrepl.h    xr21,     t0,        10       // 4017
+    xvmulev_xvmaddod_lasx xr14, xr17, xr21, xr20, xr3, xr11
+    xvneg.h       xr21,     xr21
+    xvmulev_xvmaddod_lasx xr14, xr17, xr20, xr21, xr4, xr12
+    xvilvl.w      xr14,     xr11,      xr3
+    xvilvl.w      xr17,     xr12,      xr4
+    xvilvh.w      xr11,     xr11,      xr3
+    xvilvh.w      xr12,     xr12,      xr4
+    xvssrarni.h.w xr11,     xr14,      12       // t8
+    xvssrarni.h.w xr12,     xr17,      12       // t9
+
+    xvneg.h       xr21,     xr21
+    xvmulev_xvmaddod_lasx xr15, xr7, xr20, xr21, xr3, xr14
+    xvneg.h       xr20,     xr20
+    xvmulev_xvmaddod_lasx xr15, xr7, xr21, xr20, xr4, xr17
+    xvilvl.w      xr15,     xr14,      xr3
+    xvilvl.w      xr7,      xr17,      xr4
+    xvilvh.w      xr14,     xr14,      xr3
+    xvilvh.w      xr17,     xr17,      xr4
+    xvssrarni.h.w xr14,     xr15,      12       // t13
+    xvssrarni.h.w xr17,     xr7,       12       // t12
+
+    xvldrepl.h    xr20,     t0,        12       // 3406
+    xvldrepl.h    xr21,     t0,        14       // 2276
+    xvmulev_xvmaddod_lasx xr18, xr19, xr21, xr20, xr3, xr7
+    xvneg.h       xr21,     xr21
+    xvmulev_xvmaddod_lasx xr18, xr19, xr20, xr21, xr4, xr15
+    xvilvl.w      xr18,     xr7,       xr3
+    xvilvl.w      xr19,     xr15,      xr4
+    xvilvh.w      xr7,      xr7,       xr3
+    xvilvh.w      xr15,     xr15,      xr4
+    xvssrarni.h.w xr7,      xr18,      12       // t10
+    xvssrarni.h.w xr15,     xr19,      12       // t11
+
+    xvneg.h       xr21,     xr21
+    xvmulev_xvmaddod_lasx xr13, xr5, xr20, xr21, xr3, xr18
+    xvneg.h       xr20,     xr20
+    xvmulev_xvmaddod_lasx xr13, xr5, xr21, xr20, xr4, xr19
+    xvilvl.w      xr13,     xr18,      xr3
+    xvilvl.w      xr5,      xr19,      xr4
+    xvilvh.w      xr18,     xr18,      xr3
+    xvilvh.w      xr19,     xr19,      xr4
+    xvssrarni.h.w xr18,     xr13,      12       // t15
+    xvssrarni.h.w xr19,     xr5,       12       // t14
+
+    xvsadd.h      xr5,      xr1,       xr0      // t0
+    xvssub.h      xr13,     xr1,       xr0      // t4
+    xvsadd.h      xr3,      xr16,      xr8      // t1
+    xvssub.h      xr4,      xr16,      xr8      // t5
+    xvsadd.h      xr0,      xr6,       xr2      // t2
+    xvssub.h      xr1,      xr6,       xr2      // t6
+    xvsadd.h      xr8,      xr9,       xr10     // t3
+    xvssub.h      xr16,     xr9,       xr10     // t7
+    xvsadd.h      xr2,      xr11,      xr17     // t8a
+    xvssub.h      xr6,      xr11,      xr17     // t12a
+    xvsadd.h      xr9,      xr12,      xr14     // t9a
+    xvssub.h      xr10,     xr12,      xr14     // t13a
+    xvsadd.h      xr11,     xr7,       xr19     // t10a
+    xvssub.h      xr17,     xr7,       xr19     // t14a
+    xvsadd.h      xr12,     xr15,      xr18     // t11a
+    xvssub.h      xr14,     xr15,      xr18     // t15a
+
+    la.local      t0,       idct_coeffs_h
+
+    xvldrepl.h    xr20,     t0,        4        // 1567
+    xvldrepl.h    xr21,     t0,        6        // 3784
+    xvmulev_xvmaddod_lasx xr13, xr4, xr21, xr20, xr7, xr18
+    xvneg.h       xr21,     xr21
+    xvmulev_xvmaddod_lasx xr13, xr4, xr20, xr21, xr15, xr19
+    xvilvl.w      xr13,     xr18,      xr7
+    xvilvl.w      xr4,      xr19,      xr15
+    xvilvh.w      xr18,     xr18,      xr7
+    xvilvh.w      xr19,     xr19,      xr15
+    xvssrarni.h.w xr18,     xr13,      12       // t4a
+    xvssrarni.h.w xr19,     xr4,       12       // t5a
+
+    xvneg.h       xr21,     xr21
+    xvmulev_xvmaddod_lasx xr16, xr1, xr20, xr21, xr7, xr4
+    xvneg.h       xr20,     xr20
+    xvmulev_xvmaddod_lasx xr16, xr1, xr21, xr20, xr15, xr13
+    xvilvl.w      xr16,     xr4,       xr7
+    xvilvl.w      xr1,      xr13,      xr15
+    xvilvh.w      xr4,      xr4,       xr7
+    xvilvh.w      xr13,     xr13,      xr15
+    xvssrarni.h.w xr4,      xr16,      12       // t7a
+    xvssrarni.h.w xr13,     xr1,       12       // t6a
+
+    xvneg.h       xr20,     xr20
+    xvmulev_xvmaddod_lasx xr6, xr10, xr21, xr20, xr7, xr1
+    xvneg.h       xr21,     xr21
+    xvmulev_xvmaddod_lasx xr6, xr10, xr20, xr21, xr15, xr16
+    xvilvl.w      xr6,      xr1,       xr7
+    xvilvl.w      xr10,     xr16,      xr15
+    xvilvh.w      xr1,      xr1,       xr7
+    xvilvh.w      xr16,     xr16,      xr15
+    xvssrarni.h.w xr1,      xr6,       12       // t12
+    xvssrarni.h.w xr16,     xr10,      12       // t13
+
+    xvneg.h       xr21,     xr21
+    xvmulev_xvmaddod_lasx xr14, xr17, xr20, xr21, xr7, xr6
+    xvneg.h       xr20,     xr20
+    xvmulev_xvmaddod_lasx xr14, xr17, xr21, xr20, xr15, xr10
+    xvilvl.w      xr14,     xr6,       xr7
+    xvilvl.w      xr17,     xr10,      xr15
+    xvilvh.w      xr6,      xr6,       xr7
+    xvilvh.w      xr10,     xr10,      xr15
+    xvssrarni.h.w xr6,      xr14,      12       // t15
+    xvssrarni.h.w xr10,     xr17,      12       // t14
+
+    xvsadd.h       xr14,     xr5,       xr0      // out[0]
+    xvssub.h       xr17,     xr5,       xr0      // t2a
+    xvssub.h       xr7,      xr3,       xr8      // t3a
+    xvsadd.h       xr15,     xr3,       xr8      // out[15]
+    xvsllwil.w.h   xr22,     xr15,      0
+    xvexth.w.h     xr15,     xr15
+    xvneg.w        xr22,     xr22
+    xvneg.w        xr15,     xr15
+    xvssrarni.h.w  xr15,     xr22,      0        // out[15]
+    xvssub.h       xr7,      xr3,       xr8      // t3a
+
+    xvsadd.h       xr3,      xr19,      xr4      // out[12]
+    xvssub.h       xr8,      xr19,      xr4      // t7
+    xvssub.h       xr0,      xr18,      xr13     // t6
+    xvsadd.h       xr5,      xr18,      xr13     // out[3]
+    xvsllwil.w.h   xr22,     xr5,       0
+    xvexth.w.h     xr5,      xr5
+    xvneg.w        xr22,     xr22
+    xvneg.w        xr5,      xr5
+    xvssrarni.h.w  xr5,      xr22,      0        // out[3]
+
+    xvsadd.h       xr13,     xr9,       xr12     // out[14]
+    xvssub.h       xr19,     xr9,       xr12     // t11
+    xvssub.h       xr4,      xr2,       xr11     // t10
+    xvsadd.h       xr18,     xr2,       xr11     // out[1]
+    xvsllwil.w.h   xr22,     xr18,      0
+    xvexth.w.h     xr18,     xr18
+    xvneg.w        xr22,     xr22
+    xvneg.w        xr18,     xr18
+    xvssrarni.h.w  xr18,     xr22,      0        // out[1]
+
+    xvsadd.h       xr2,      xr1,       xr10     // out[2]
+    xvssub.h       xr11,     xr1,       xr10     // t14a
+    xvssub.h       xr12,     xr16,      xr6      // t15a
+    xvsadd.h       xr9,      xr16,      xr6      // out[13]
+    xvsllwil.w.h   xr22,     xr9,       0
+    xvexth.w.h     xr9,      xr9
+    xvneg.w        xr22,     xr22
+    xvneg.w        xr9,      xr9
+    xvssrarni.h.w  xr9,      xr22,      0        // out[13]
+
+    xvldrepl.h     xr20,     t0,        0        // 2896
+    xvmulev_xvmaddod_lasx xr17, xr7, xr20, xr20, xr6, xr10
+    xvneg.h        xr21,     xr20
+    xvmulev_xvmaddod_lasx xr17, xr7, xr20, xr21, xr16, xr1
+    xvilvl.w       xr17,     xr10,      xr6
+    xvilvl.w       xr7,      xr1,       xr16
+    xvilvh.w       xr10,     xr10,      xr6
+    xvilvh.w       xr1,      xr1,       xr16
+    xvssrarni.h.w  xr1,      xr7,       12       // out[8]
+    xvsrari.w      xr17,     xr17,      12
+    xvsrari.w      xr10,     xr10,      12
+    xvneg.w        xr17,     xr17
+    xvneg.w        xr10,     xr10
+    xvssrarni.h.w  xr10,     xr17,      0        // out[7]
+
+    xvmulev_xvmaddod_lasx xr0, xr8, xr20, xr21, xr16, xr17
+    xvmulev_xvmaddod_lasx xr0, xr8, xr20, xr20, xr6, xr7
+    xvilvl.w       xr0,      xr17,      xr16
+    xvilvl.w       xr8,      xr7,       xr6
+    xvilvh.w       xr17,     xr17,      xr16
+    xvilvh.w       xr7,      xr7,       xr6
+    xvssrarni.h.w  xr7,      xr8,       12       // out[4]
+    xvsrari.w      xr0,      xr0,       12
+    xvsrari.w      xr17,     xr17,      12
+    xvneg.w        xr0,      xr0
+    xvneg.w        xr17,     xr17
+    xvssrarni.h.w xr17,      xr0,       0        // out[11]
+
+    xvmulev_xvmaddod_lasx xr4, xr19, xr20, xr21, xr16, xr0
+    xvmulev_xvmaddod_lasx xr4, xr19, xr20, xr20, xr6, xr8
+    xvilvl.w       xr4,      xr0,       xr16
+    xvilvl.w       xr19,     xr8,       xr6
+    xvilvh.w       xr0,      xr0,       xr16
+    xvilvh.w       xr8,      xr8,       xr6
+    xvssrarni.h.w  xr8,      xr19,      12       // out[6]
+    xvsrari.w      xr4,      xr4,       12
+    xvsrari.w      xr0,      xr0,       12
+    xvneg.w        xr4,      xr4
+    xvneg.w        xr0,      xr0
+    xvssrarni.h.w  xr0,      xr4,       0        // out[9]
+    xvmulev_xvmaddod_lasx xr11, xr12, xr20, xr20, xr6, xr4
+    xvmulev_xvmaddod_lasx xr11, xr12, xr20, xr21, xr16, xr19
+    xvilvl.w       xr11,     xr4,       xr6
+    xvilvl.w       xr12,     xr19,      xr16
+    xvilvh.w       xr4,      xr4,       xr6
+    xvilvh.w       xr19,     xr19,      xr16
+    xvssrarni.h.w  xr19,     xr12,      12       // out[10]
+    xvsrari.w      xr11,     xr11,      12
+    xvsrari.w      xr4,      xr4,       12
+    xvneg.w        xr11,     xr11
+    xvneg.w        xr4,      xr4
+    xvssrarni.h.w  xr4,      xr11,      0        // out[5]
+.endm
+
+function inv_txfm_add_adst_adst_16x16_8bpc_lasx
+    PUSH_REG
+    xvld_x16 a2, 0, 32, xr0, xr1, xr2, xr3, xr4, xr5, xr6, xr7, \
+             xr8, xr9, xr10, xr11, xr12, xr13, xr14, xr15
+
+    inv_adst16_lasx
+
+    LASX_TRANSPOSE8x8_H xr14, xr18, xr2, xr5, xr7, xr4, xr8, xr10, \
+                        xr14, xr18, xr2, xr5, xr7, xr28, xr6, xr10, \
+                        xr20, xr21, xr22, xr23, xr24, xr25, xr26, xr27
+
+    LASX_TRANSPOSE8x8_H xr1,  xr0,  xr19, xr17, xr3, xr9, xr13, xr15, \
+                        xr29, xr30, xr11, xr17, xr31, xr19, xr16, xr15, \
+                        xr20, xr21, xr22, xr23, xr24, xr25, xr26, xr27
+
+    xvsrari_h_x16 xr14, xr18, xr2, xr5, xr7, xr28, xr6, xr10, \
+                  xr29, xr30, xr11, xr17, xr31, xr19, xr16, xr15, \
+                  xr0, xr1, xr2, xr3, xr4, xr5, xr6, xr7, \
+                  xr8, xr9, xr10, xr11, xr12, xr13, xr14, xr15, 2
+
+    xvpermi_q_x2 xr0, xr1, xr8, xr9, xr0, xr1, xr8, xr9, xr20, xr21
+    xvpermi_q_x2 xr2, xr3, xr10, xr11, xr2, xr3, xr10, xr11, xr20, xr21
+    xvpermi_q_x2 xr4, xr5, xr12, xr13, xr4, xr5, xr12, xr13, xr20, xr21
+    xvpermi_q_x2 xr6, xr7, xr14, xr15, xr6, xr7, xr14, xr15, xr20, xr21
+
+    inv_adst16_lasx
+
+    xvsrari_h_x16 xr14, xr18, xr2,  xr5,  xr7,  xr4, xr8,  xr10, \
+                  xr1,  xr0,  xr19, xr17, xr3,  xr9, xr13, xr15, \
+                  xr14, xr18, xr11, xr5,  xr7,  xr4, xr8,  xr10, \
+                  xr12, xr16, xr19, xr17, xr20, xr9, xr13, xr15, 4
+
+    xvxor.v       xr23,     xr23,     xr23
+.irp i, 0, 32, 64, 96, 128, 160, 192, 224, 256, 288, 320, 352, 384, 416, 448, 480
+    xvst          xr23,     a2,       \i
+.endr
+    alsl.d        t2,       a1,       a0,    1
+    XVLD_DST_ADD_W16 xr14, xr18, xr11, xr5
+    alsl.d        a0,       a1,       a0,    2
+    alsl.d        t2,       a1,       a0,    1
+    XVLD_DST_ADD_W16 xr7, xr4, xr8, xr10
+    alsl.d        a0,       a1,       a0,    2
+    alsl.d        t2,       a1,       a0,    1
+    XVLD_DST_ADD_W16 xr12, xr16, xr19, xr17
+    alsl.d        a0,       a1,       a0,    2
+    alsl.d        t2,       a1,       a0,    1
+    XVLD_DST_ADD_W16 xr20, xr9, xr13, xr15
+    POP_REG
 endfunc
diff --git a/src/loongarch/itx.h b/src/loongarch/itx.h
index 3ad444f..e0b81f3 100644
--- a/src/loongarch/itx.h
+++ b/src/loongarch/itx.h
@@ -31,67 +31,18 @@
 #include "src/cpu.h"
 #include "src/itx.h"
 
-decl_itx_fn(BF(dav1d_inv_txfm_add_wht_wht_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_identity_identity_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_adst_dct_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_adst_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_adst_adst_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_flipadst_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_flipadst_adst_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_adst_flipadst_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_flipadst_dct_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_flipadst_flipadst_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_identity_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_identity_dct_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_flipadst_identity_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_identity_flipadst_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_identity_adst_4x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_adst_identity_4x4, lsx));
-
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_4x8, lsx));
-
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_identity_identity_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_adst_dct_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_adst_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_adst_adst_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_flipadst_adst_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_adst_flipadst_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_flipadst_dct_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_flipadst_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_flipadst_flipadst_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_identity_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_identity_dct_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_flipadst_identity_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_identity_flipadst_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_adst_identity_8x4, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_identity_adst_8x4, lsx));
-
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_identity_identity_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_adst_dct_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_adst_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_adst_adst_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_flipadst_adst_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_adst_flipadst_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_flipadst_dct_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_flipadst_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_identity_adst_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_flipadst_identity_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_identity_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_identity_dct_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_identity_flipadst_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_adst_identity_8x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_flipadst_flipadst_8x8, lsx));
-
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_8x16, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_identity_identity_8x16, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_adst_dct_8x16, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_adst_8x16, lsx));
-
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_16x8, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_adst_dct_16x8, lsx));
+decl_itx17_fns( 4,  4, lsx);
+decl_itx16_fns( 4,  8, lsx);
+decl_itx16_fns( 4, 16, lsx);
+decl_itx16_fns( 8,  4, lsx);
+decl_itx16_fns( 8,  8, lsx);
+decl_itx16_fns( 8, 16, lsx);
+decl_itx2_fns ( 8, 32, lsx);
+decl_itx16_fns(16,  8, lsx);
+
+decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_16x4, lsx));
+decl_itx_fn(BF(dav1d_inv_txfm_add_identity_identity_16x4, lsx));
+decl_itx_fn(BF(dav1d_inv_txfm_add_adst_dct_16x4, lsx));
 
 decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_16x16, lsx));
 decl_itx_fn(BF(dav1d_inv_txfm_add_adst_adst_16x16, lsx));
@@ -99,14 +50,23 @@ decl_itx_fn(BF(dav1d_inv_txfm_add_adst_dct_16x16, lsx));
 decl_itx_fn(BF(dav1d_inv_txfm_add_dct_adst_16x16, lsx));
 decl_itx_fn(BF(dav1d_inv_txfm_add_flipadst_dct_16x16, lsx));
 decl_itx_fn(BF(dav1d_inv_txfm_add_dct_flipadst_16x16, lsx));
+decl_itx_fn(BF(dav1d_inv_txfm_add_adst_flipadst_16x16, lsx));
+decl_itx_fn(BF(dav1d_inv_txfm_add_flipadst_adst_16x16, lsx));
 
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_8x32, lsx));
-decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_32x32, lsx));
+decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_16x32, lsx));
+
+decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_32x8, lsx));
+
+decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_32x16, lsx));
 
 decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_32x32, lsx));
 
+decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_64x32, lsx));
+
 decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_64x64, lsx));
 
+decl_itx_fn(BF(dav1d_inv_txfm_add_adst_adst_16x16, lasx));
+
 static ALWAYS_INLINE void itx_dsp_init_loongarch(Dav1dInvTxfmDSPContext *const c, int bpc) {
 #if BITDEPTH == 8
     const unsigned flags = dav1d_get_cpu_flags();
@@ -115,67 +75,20 @@ static ALWAYS_INLINE void itx_dsp_init_loongarch(Dav1dInvTxfmDSPContext *const c
 
     if (BITDEPTH != 8 ) return;
 
-    c->itxfm_add[TX_4X4][WHT_WHT]  = dav1d_inv_txfm_add_wht_wht_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][DCT_DCT]  = dav1d_inv_txfm_add_dct_dct_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][IDTX] = dav1d_inv_txfm_add_identity_identity_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][DCT_ADST] = dav1d_inv_txfm_add_adst_dct_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][ADST_DCT] = dav1d_inv_txfm_add_dct_adst_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][ADST_ADST] = dav1d_inv_txfm_add_adst_adst_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][FLIPADST_DCT] = dav1d_inv_txfm_add_dct_flipadst_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][ADST_FLIPADST] = dav1d_inv_txfm_add_flipadst_adst_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][FLIPADST_ADST] = dav1d_inv_txfm_add_adst_flipadst_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][DCT_FLIPADST] = dav1d_inv_txfm_add_flipadst_dct_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][FLIPADST_FLIPADST] = dav1d_inv_txfm_add_flipadst_flipadst_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][H_DCT] = dav1d_inv_txfm_add_dct_identity_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][V_DCT] = dav1d_inv_txfm_add_identity_dct_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][H_FLIPADST] = dav1d_inv_txfm_add_flipadst_identity_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][V_FLIPADST] = dav1d_inv_txfm_add_identity_flipadst_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][V_ADST] = dav1d_inv_txfm_add_identity_adst_4x4_8bpc_lsx;
-    c->itxfm_add[TX_4X4][H_ADST] = dav1d_inv_txfm_add_adst_identity_4x4_8bpc_lsx;
-
-    c->itxfm_add[RTX_4X8][DCT_DCT] = dav1d_inv_txfm_add_dct_dct_4x8_8bpc_lsx;
-
-    c->itxfm_add[RTX_8X4][DCT_DCT] = dav1d_inv_txfm_add_dct_dct_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][IDTX] = dav1d_inv_txfm_add_identity_identity_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][DCT_ADST] = dav1d_inv_txfm_add_adst_dct_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][ADST_DCT] = dav1d_inv_txfm_add_dct_adst_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][ADST_ADST] = dav1d_inv_txfm_add_adst_adst_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][ADST_FLIPADST] = dav1d_inv_txfm_add_flipadst_adst_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][FLIPADST_ADST] = dav1d_inv_txfm_add_adst_flipadst_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][DCT_FLIPADST] = dav1d_inv_txfm_add_flipadst_dct_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][FLIPADST_DCT] = dav1d_inv_txfm_add_dct_flipadst_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][FLIPADST_FLIPADST] = dav1d_inv_txfm_add_flipadst_flipadst_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][H_DCT] = dav1d_inv_txfm_add_dct_identity_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][V_DCT] = dav1d_inv_txfm_add_identity_dct_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][H_FLIPADST] = dav1d_inv_txfm_add_flipadst_identity_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][V_FLIPADST] = dav1d_inv_txfm_add_identity_flipadst_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][H_ADST] = dav1d_inv_txfm_add_adst_identity_8x4_8bpc_lsx;
-    c->itxfm_add[RTX_8X4][V_ADST] = dav1d_inv_txfm_add_identity_adst_8x4_8bpc_lsx;
-
-    c->itxfm_add[TX_8X8][DCT_DCT] = dav1d_inv_txfm_add_dct_dct_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][IDTX] = dav1d_inv_txfm_add_identity_identity_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][DCT_ADST] = dav1d_inv_txfm_add_adst_dct_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][ADST_DCT] = dav1d_inv_txfm_add_dct_adst_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][ADST_ADST] = dav1d_inv_txfm_add_adst_adst_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][ADST_FLIPADST] = dav1d_inv_txfm_add_flipadst_adst_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][FLIPADST_ADST] = dav1d_inv_txfm_add_adst_flipadst_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][DCT_FLIPADST] = dav1d_inv_txfm_add_flipadst_dct_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][FLIPADST_DCT] = dav1d_inv_txfm_add_dct_flipadst_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][FLIPADST_FLIPADST] = dav1d_inv_txfm_add_flipadst_flipadst_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][H_DCT] = dav1d_inv_txfm_add_dct_identity_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][V_DCT] = dav1d_inv_txfm_add_identity_dct_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][H_FLIPADST] = dav1d_inv_txfm_add_flipadst_identity_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][V_FLIPADST] = dav1d_inv_txfm_add_identity_flipadst_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][H_ADST] = dav1d_inv_txfm_add_adst_identity_8x8_8bpc_lsx;
-    c->itxfm_add[TX_8X8][V_ADST] = dav1d_inv_txfm_add_identity_adst_8x8_8bpc_lsx;
-
-    c->itxfm_add[RTX_8X16][DCT_DCT] = dav1d_inv_txfm_add_dct_dct_8x16_8bpc_lsx;
-    c->itxfm_add[RTX_8X16][IDTX] = dav1d_inv_txfm_add_identity_identity_8x16_8bpc_lsx;
-    c->itxfm_add[RTX_8X16][DCT_ADST] = dav1d_inv_txfm_add_adst_dct_8x16_8bpc_lsx;
-    c->itxfm_add[RTX_8X16][ADST_DCT] = dav1d_inv_txfm_add_dct_adst_8x16_8bpc_lsx;
-
-    c->itxfm_add[RTX_16X8][DCT_DCT] = dav1d_inv_txfm_add_dct_dct_16x8_8bpc_lsx;
-    c->itxfm_add[RTX_16X8][DCT_ADST] = dav1d_inv_txfm_add_adst_dct_16x8_8bpc_lsx;
+    assign_itx17_fn( ,  4,  4, lsx);
+    assign_itx16_fn(R,  4,  8, lsx);
+    assign_itx16_fn(R,  4, 16, lsx);
+    assign_itx16_fn(R,  8,  4, lsx);
+    assign_itx16_fn( ,  8,  8, lsx);
+    assign_itx16_fn(R,  8, 16, lsx);
+    assign_itx2_fn (R,  8, 32, lsx);
+    assign_itx16_fn(R, 16,  8, lsx);
+    assign_itx1_fn (R, 64, 32, lsx);
+    assign_itx1_fn ( , 64, 64, lsx);
+
+    c->itxfm_add[RTX_16X4][DCT_DCT] = dav1d_inv_txfm_add_dct_dct_16x4_8bpc_lsx;
+    c->itxfm_add[RTX_16X4][IDTX] = dav1d_inv_txfm_add_identity_identity_16x4_8bpc_lsx;
+    c->itxfm_add[RTX_16X4][DCT_ADST] = dav1d_inv_txfm_add_adst_dct_16x4_8bpc_lsx;
 
     c->itxfm_add[TX_16X16][DCT_DCT] = dav1d_inv_txfm_add_dct_dct_16x16_8bpc_lsx;
     c->itxfm_add[TX_16X16][ADST_ADST] = dav1d_inv_txfm_add_adst_adst_16x16_8bpc_lsx;
@@ -183,12 +96,23 @@ static ALWAYS_INLINE void itx_dsp_init_loongarch(Dav1dInvTxfmDSPContext *const c
     c->itxfm_add[TX_16X16][ADST_DCT] = dav1d_inv_txfm_add_dct_adst_16x16_8bpc_lsx;
     c->itxfm_add[TX_16X16][DCT_FLIPADST] = dav1d_inv_txfm_add_flipadst_dct_16x16_8bpc_lsx;
     c->itxfm_add[TX_16X16][FLIPADST_DCT] = dav1d_inv_txfm_add_dct_flipadst_16x16_8bpc_lsx;
+    c->itxfm_add[TX_16X16][FLIPADST_ADST] = dav1d_inv_txfm_add_adst_flipadst_16x16_8bpc_lsx;
+    c->itxfm_add[TX_16X16][ADST_FLIPADST] = dav1d_inv_txfm_add_flipadst_adst_16x16_8bpc_lsx;
+
+    c->itxfm_add[RTX_16X32][DCT_DCT] = dav1d_inv_txfm_add_dct_dct_16x32_8bpc_lsx;
 
-    c->itxfm_add[RTX_8X32][DCT_DCT] = dav1d_inv_txfm_add_dct_dct_8x32_8bpc_lsx;
+    c->itxfm_add[RTX_32X8][DCT_DCT] = dav1d_inv_txfm_add_dct_dct_32x8_8bpc_lsx;
+
+    c->itxfm_add[RTX_32X16][DCT_DCT] = dav1d_inv_txfm_add_dct_dct_32x16_8bpc_lsx;
 
     c->itxfm_add[TX_32X32][DCT_DCT] = dav1d_inv_txfm_add_dct_dct_32x32_8bpc_lsx;
 
-    c->itxfm_add[TX_64X64][DCT_DCT] = dav1d_inv_txfm_add_dct_dct_64x64_8bpc_lsx;
+    if (!(flags & DAV1D_LOONGARCH_CPU_FLAG_LASX)) return;
+
+    if (BITDEPTH != 8 ) return;
+
+    c->itxfm_add[TX_16X16][ADST_ADST] = dav1d_inv_txfm_add_adst_adst_16x16_8bpc_lasx;
+
 #endif
 }
 
diff --git a/src/loongarch/loongson_util.S b/src/loongarch/loongson_util.S
new file mode 100644
index 0000000..0941cb7
--- /dev/null
+++ b/src/loongarch/loongson_util.S
@@ -0,0 +1,192 @@
+/******************************************************************************
+ * Copyright © 2024, VideoLAN and dav1d authors
+ * Copyright © 2024, Loongson Technology Corporation Limited
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *****************************************************************************/
+
+#ifndef DAV1D_SRC_LOONGSON_UTIL_S
+#define DAV1D_SRC_LOONGSON_UTIL_S
+
+#ifndef DEFAULT_ALIGN
+#define DEFAULT_ALIGN 5
+#endif
+
+//That l means local defines local functions
+.macro functionl name, align=DEFAULT_ALIGN
+.macro endfuncl
+    jirl    $r0, $r1, 0x0
+    .size \name, . - \name
+    .purgem endfuncl
+.endm
+.text ;
+.align \align ;
+.hidden \name ;
+.type \name, @function ;
+\name: ;
+.endm
+
+.macro TRANSPOSE_4x16B in0, in1 ,in2, in3, in4, in5, in6, in7
+    vpackev.b        \in4,  \in1,  \in0
+    vpackod.b        \in5,  \in1,  \in0
+    vpackev.b        \in6,  \in3,  \in2
+    vpackod.b        \in7,  \in3,  \in2
+
+    vpackev.h        \in0,  \in6,  \in4
+    vpackod.h        \in2,  \in6,  \in4
+    vpackev.h        \in1,  \in7,  \in5
+    vpackod.h        \in3,  \in7,  \in5
+.endm
+
+.macro TRANSPOSE_8x16B in0, in1, in2, in3, in4, in5, in6, in7, in8, in9
+    vpackev.b        \in8,  \in1,  \in0
+    vpackod.b        \in9,  \in1,  \in0
+    vpackev.b        \in1,  \in3,  \in2
+    vpackod.b        \in3,  \in3,  \in2
+    vpackev.b        \in0,  \in5,  \in4
+    vpackod.b        \in5,  \in5,  \in4
+    vpackev.b        \in2,  \in7,  \in6
+    vpackod.b        \in7,  \in7,  \in6
+
+    vpackev.h        \in4,  \in2,  \in0
+    vpackod.h        \in2,  \in2,  \in0
+    vpackev.h        \in6,  \in7,  \in5
+    vpackod.h        \in7,  \in7,  \in5
+    vpackev.h        \in5,  \in3,  \in9
+    vpackod.h        \in9,  \in3,  \in9
+    vpackev.h        \in3,  \in1,  \in8
+    vpackod.h        \in8,  \in1,  \in8
+
+    vpackev.w        \in0,  \in4,  \in3
+    vpackod.w        \in4,  \in4,  \in3
+    vpackev.w        \in1,  \in6,  \in5
+    vpackod.w        \in5,  \in6,  \in5
+    vpackod.w        \in6,  \in2,  \in8
+    vpackev.w        \in2,  \in2,  \in8
+    vpackev.w        \in3,  \in7,  \in9
+    vpackod.w        \in7,  \in7,  \in9
+.endm
+
+.macro vld_x8 src, start, stride, in0, in1, in2, in3, in4, in5, in6, in7
+    vld           \in0,     \src,     \start
+    vld           \in1,     \src,     \start+(\stride*1)
+    vld           \in2,     \src,     \start+(\stride*2)
+    vld           \in3,     \src,     \start+(\stride*3)
+    vld           \in4,     \src,     \start+(\stride*4)
+    vld           \in5,     \src,     \start+(\stride*5)
+    vld           \in6,     \src,     \start+(\stride*6)
+    vld           \in7,     \src,     \start+(\stride*7)
+.endm
+
+.macro vst_x8 src, start, stride, in0, in1, in2, in3, in4, in5, in6, in7
+    vst           \in0,     \src,     \start
+    vst           \in1,     \src,     \start+(\stride*1)
+    vst           \in2,     \src,     \start+(\stride*2)
+    vst           \in3,     \src,     \start+(\stride*3)
+    vst           \in4,     \src,     \start+(\stride*4)
+    vst           \in5,     \src,     \start+(\stride*5)
+    vst           \in6,     \src,     \start+(\stride*6)
+    vst           \in7,     \src,     \start+(\stride*7)
+.endm
+
+.macro vld_x16 src, start, stride, in0, in1, in2, in3, in4, in5, in6, in7, \
+               in8, in9, in10, in11, in12, in13, in14, in15
+
+    vld_x8 \src, \start, \stride, \in0, \in1, \in2, \in3, \in4, \in5, \in6, \in7
+
+    vld           \in8,     \src,     \start+(\stride*8)
+    vld           \in9,     \src,     \start+(\stride*9)
+    vld           \in10,    \src,     \start+(\stride*10)
+    vld           \in11,    \src,     \start+(\stride*11)
+    vld           \in12,    \src,     \start+(\stride*12)
+    vld           \in13,    \src,     \start+(\stride*13)
+    vld           \in14,    \src,     \start+(\stride*14)
+    vld           \in15,    \src,     \start+(\stride*15)
+.endm
+
+.macro vst_x16 src, start, stride, in0, in1, in2, in3, in4, in5, in6, in7, \
+               in8, in9, in10, in11, in12, in13, in14, in15
+
+    vst_x8 \src, \start, \stride, \in0, \in1, \in2, \in3, \in4, \in5, \in6, \in7
+
+    vst           \in8,     \src,     \start+(\stride*8)
+    vst           \in9,     \src,     \start+(\stride*9)
+    vst           \in10,    \src,     \start+(\stride*10)
+    vst           \in11,    \src,     \start+(\stride*11)
+    vst           \in12,    \src,     \start+(\stride*12)
+    vst           \in13,    \src,     \start+(\stride*13)
+    vst           \in14,    \src,     \start+(\stride*14)
+    vst           \in15,    \src,     \start+(\stride*15)
+.endm
+
+.macro xvld_x8 src, start, stride, in0, in1, in2, in3, in4, in5, in6, in7
+    xvld           \in0,     \src,     \start
+    xvld           \in1,     \src,     \start+(\stride)
+    xvld           \in2,     \src,     \start+(\stride<<1)
+    xvld           \in3,     \src,     \start+(\stride<<1)+(\stride)
+    xvld           \in4,     \src,     \start+(\stride<<2)
+    xvld           \in5,     \src,     \start+(\stride<<2)+(\stride)
+    xvld           \in6,     \src,     \start+(\stride*6)
+    xvld           \in7,     \src,     \start+(\stride<<3)-(\stride)
+.endm
+
+.macro xvst_x8 src, start, stride, in0, in1, in2, in3, in4, in5, in6, in7
+    xvst           \in0,     \src,     \start
+    xvst           \in1,     \src,     \start+(\stride)
+    xvst           \in2,     \src,     \start+(\stride<<1)
+    xvst           \in3,     \src,     \start+(\stride<<1)+(\stride)
+    xvst           \in4,     \src,     \start+(\stride<<2)
+    xvst           \in5,     \src,     \start+(\stride<<2)+(\stride)
+    xvst           \in6,     \src,     \start+(\stride*6)
+    xvst           \in7,     \src,     \start+(\stride<<3)-(\stride)
+.endm
+
+.macro xvld_x16 src, start, stride, in0, in1, in2, in3, in4, in5, in6, in7, \
+                in8, in9, in10, in11, in12, in13, in14, in15
+    xvld_x8 \src, \start, \stride, \in0, \in1, \in2, \in3, \in4, \in5, \in6, \in7
+
+    xvld           \in8,     \src,     \start+(\stride<<3)
+    xvld           \in9,     \src,     \start+(\stride<<3)+(\stride)
+    xvld           \in10,    \src,     \start+(\stride*10)
+    xvld           \in11,    \src,     \start+(\stride*11)
+    xvld           \in12,    \src,     \start+(\stride*12)
+    xvld           \in13,    \src,     \start+(\stride*13)
+    xvld           \in14,    \src,     \start+(\stride*14)
+    xvld           \in15,    \src,     \start+(\stride<<4)-(\stride)
+.endm
+
+.macro xvst_x16 src, start, stride, in0, in1, in2, in3, in4, in5, in6, in7, \
+               in8, in9, in10, in11, in12, in13, in14, in15
+    xvst_x8 \src, \start, \stride, \in0, \in1, \in2, \in3, \in4, \in5, \in6, \in7
+
+    xvst           \in8,     \src,     \start+(\stride<<3)
+    xvst           \in9,     \src,     \start+(\stride<<3)+(\stride)
+    xvst           \in10,    \src,     \start+(\stride*10)
+    xvst           \in11,    \src,     \start+(\stride*11)
+    xvst           \in12,    \src,     \start+(\stride*12)
+    xvst           \in13,    \src,     \start+(\stride*13)
+    xvst           \in14,    \src,     \start+(\stride*14)
+    xvst           \in15,    \src,     \start+(\stride<<4)-(\stride)
+.endm
+
+#endif /* DAV1D_SRC_LOONGSON_UTIL_S */
diff --git a/src/loongarch/loopfilter.S b/src/loongarch/loopfilter.S
index e71d5a7..85b942c 100644
--- a/src/loongarch/loopfilter.S
+++ b/src/loongarch/loopfilter.S
@@ -26,1078 +26,1185 @@
  */
 
 #include "src/loongarch/loongson_asm.S"
-
-.macro FILTER_W4 DIR, TYPE
-.ifc \DIR, h
-    addi.d           t5,     a0,    -2
-    fld.s            f6,     t5,     0  //p1 p0 q0 q1
-    fldx.s           f7,     t5,     a1
-    alsl.d           t5,     a1,     t5,    1
-    fld.s            f8,     t5,     0
-    fldx.s           f9,     t5,     a1
-
-    vilvl.b          vr6,    vr7,    vr6
-    vilvl.b          vr7,    vr9,    vr8
-    vilvl.h          vr6,    vr7,    vr6 //p1p1p1p1
-    vbsrl.v          vr7,    vr6,    4   //p0p0p0p0
-    vbsrl.v          vr8,    vr7,    4   //q0q0q0q0
-    vbsrl.v          vr9,    vr8,    4   //q1q1q1q1
-.else
-    sub.d            t5,     a0,     a1
-    fld.s            f7,     t5,     0
-    sub.d            t5,     t5,     a1
-    fld.s            f6,     t5,     0
-    fld.s            f8,     a0,     0
-    fldx.s           f9,     a0,     a1
+#include "src/loongarch/loongson_util.S"
+
+// depending on how many pixels need to be stored, returns:
+// t4 = (1 << 0) : 0 pixels
+// t4 = (1 << 4) : inner 4 pixels
+// t4 = (1 << 6) : inner 6 pixels
+// t4 = 0 : all pixels
+.macro FILTER wd
+functionl lpf_16_wd\wd\()_lsx
+    vabsd.bu         vr0,   vr22,  vr23 // abs(p1 - p0)
+    vabsd.bu         vr1,   vr25,  vr24 // abs(q1 - q0)
+    vabsd.bu         vr2,   vr23,  vr24 // abs(p0 - q0)
+    vabsd.bu         vr3,   vr22,  vr25 // abs(p1 - q1)
+.if \wd >= 6
+    vabsd.bu         vr4,   vr21,  vr22 // abs(p2 - p1)
+    vabsd.bu         vr5,   vr26,  vr25 // abs(q2 - q1)
 .endif
-
-    vabsd.bu         vr10,   vr6,    vr7 // (p1 - p0)
-    vabsd.bu         vr11,   vr9,    vr8 // (q1 - q0)
-    vabsd.bu         vr12,   vr7,    vr8 // (p0 - q0)
-    vabsd.bu         vr13,   vr6,    vr9 // (p1 - q1)
-
-    vmax.bu          vr14,   vr10,   vr11
-    vsle.bu          vr15,   vr14,   vr4  //abs(p1 - p0) <= I && abs(q1 - q0) <= I
-    vsadd.bu         vr16,   vr12,   vr12
-    vsrli.b          vr17,   vr13,   1
-    vsadd.bu         vr16,   vr16,   vr17 //abs(p0 - q0) * 2 + (abs(p1 - q1) >> 1)
-    vsle.bu          vr16,   vr16,   vr3
-    vand.v           vr20,   vr15,   vr16 //fm
-
-    vpickve2gr.wu    t5,     vr20,   0
-    beqz             t5,     .END_FILTER_\DIR\()\TYPE\()_W4
-
-    vslt.bu          vr16,   vr2,    vr14 //hev
-
-    vsllwil.h.b      vr30,   vr20,   0 //expand fm to w
-    vsllwil.w.h      vr30,   vr30,   0
-
-    vsllwil.hu.bu    vr17,   vr6,    0
-    vsllwil.hu.bu    vr18,   vr9,    0
-    vsub.h           vr17,   vr17,   vr18
-    vssrarni.b.h     vr17,   vr17,   0    //f = iclip_diff(p1 - q1)
-
-    vand.v           vr17,   vr17,   vr16
-    vsllwil.h.b      vr18,   vr17,   0
-
-    vsllwil.hu.bu    vr10,   vr8,    0
-    vsllwil.hu.bu    vr11,   vr7,    0
-    vsub.h           vr10,   vr10,   vr11
-
-    vsadd.h          vr11,   vr10,   vr10
-    vsadd.h          vr10,   vr10,   vr11 //3 * (q0 - p0)
-    vsadd.h          vr10,   vr10,   vr18 //f = iclip_diff(3 * (q0 - p0) + f);
-    vssrani.b.h      vr10,   vr10,   0
-    vsllwil.h.b      vr10,   vr10,   0
-
-    vaddi.hu         vr11,   vr10,   4
-    vaddi.hu         vr12,   vr10,   3
-    li.w             t5,     127
-    vreplgr2vr.h     vr13,   t5
-    vmin.h           vr11,   vr11,   vr13
-    vmin.h           vr12,   vr12,   vr13
-    vsrai.h          vr11,   vr11,   3 //f1
-    vsrai.h          vr12,   vr12,   3 //f2
-
-    vsllwil.hu.bu    vr13,   vr7,    0 //p0
-    vsllwil.hu.bu    vr14,   vr8,    0 //q0
-    vsadd.h          vr13,   vr13,   vr12
-    vssub.h          vr14,   vr14,   vr11
-    vssrani.bu.h     vr13,   vr13,   0 //dst-1
-    vssrani.bu.h     vr14,   vr14,   0 //dst+0
-
-    vsrari.h         vr15,   vr11,   1 //f
-    vsllwil.hu.bu    vr18,   vr6,    0 //p1
-    vsllwil.hu.bu    vr19,   vr9,    0 //q1
-    vsadd.h          vr18,   vr18,   vr15
-    vssub.h          vr19,   vr19,   vr15
-    vssrani.bu.h     vr18,   vr18,   0 //dst-2
-    vssrani.bu.h     vr19,   vr19,   0 //dst+1
-    vbitsel.v        vr26,   vr18,   vr6,    vr16
-    vbitsel.v        vr29,   vr19,   vr9,    vr16
-
-    vbitsel.v        vr6,    vr6,    vr26,   vr20
-    vbitsel.v        vr7,    vr7,    vr13,   vr20
-    vbitsel.v        vr8,    vr8,    vr14,   vr20
-    vbitsel.v        vr9,    vr9,    vr29,   vr20
-
-.ifc \DIR, h
-    vilvl.b          vr6,    vr7,    vr6
-    vilvl.b          vr9,    vr9,    vr8
-    vilvl.h          vr6,    vr9,    vr6
-
-    addi.d           t5,     a0,    -2
-    vstelm.w         vr6,    t5,     0,      0
-    add.d            t5,     t5,     a1
-    vstelm.w         vr6,    t5,     0,      1
-    add.d            t5,     t5,     a1
-    vstelm.w         vr6,    t5,     0,      2
-    add.d            t5,     t5,     a1
-    vstelm.w         vr6,    t5,     0,      3
-.else
-    fst.s            f8,     a0,     0
-    fstx.s           f9,     a0,     a1
-    sub.d            t5,     a0,     a1
-    fst.s            f7,     t5,     0
-    sub.d            t5,     t5,     a1
-    fst.s            f6,     t5,     0
+.if \wd >= 8
+    vabsd.bu         vr6,   vr20,  vr21 // abs(p3 - p2)
+    vabsd.bu         vr7,   vr27,  vr26 // abs(q3 - q3)
 .endif
-.END_FILTER_\DIR\()\TYPE\()_W4:
-.endm
-
-.macro FILTER_W6 DIR, TYPE
-.ifc \DIR, h
-    addi.d           t5,     a0,    -3
-    fld.d            f6,     t5,     0 //p2 p1 p0 q0 q1 q2
-    fldx.d           f7,     t5,     a1
-    alsl.d           t5,     a1,     t5,    1
-    fld.d            f8,     t5,     0
-    fldx.d           f9,     t5,     a1
-
-    vilvl.b          vr6,    vr7,    vr6
-    vilvl.b          vr7,    vr9,    vr8
-    vilvh.h          vr10,   vr7,    vr6
-    vilvl.h          vr6,    vr7,    vr6
-
-    vbsrl.v          vr7,    vr6,    4 //p1
-    vbsrl.v          vr8,    vr7,    4 //p0
-    vbsrl.v          vr9,    vr8,    4 //q0
-    vbsrl.v          vr11,   vr10,   4 //q2
+.if \wd >= 6
+    vmax.bu          vr4,   vr4,   vr5
+.endif
+    vsadd.bu         vr2,   vr2,   vr2  // abs(p0 - q0) * 2
+.if \wd >= 8
+    vmax.bu          vr6,   vr6,   vr7
+.endif
+    vsrli.b          vr3,   vr3,   1    // abs(p1 - q1) >> 1
+.if \wd >= 8
+    vmax.bu          vr4,   vr4,   vr6
+.endif
+.if \wd >= 6
+    vand.v           vr4,   vr4,   vr14
+.endif
+    vmax.bu          vr0,   vr0,   vr1  // max(abs(p1 - p0), abs(q1 - q0))
+    vsadd.bu         vr2,   vr2,   vr3  // abs(p0 - q0) * 2 + abs(p1 - q1) >> 1
+.if \wd >= 6
+    vmax.bu          vr4,   vr0,   vr4
+    vsle.bu          vr1,   vr4,   vr11 // max(abs(p1 - p0), abs(q1 - q0), abs(), abs(), ...) <= I
 .else
-    alsl.d           t5,     a1,     a1,    1
-    sub.d            t5,     a0,     t5
-    fld.d            f6,     t5,     0
-    fldx.d           f7,     t5,     a1
-    alsl.d           t5,     a1,     t5,    1
-    fld.d            f8,     t5,     0
-    fldx.d           f9,     t5,     a1
-    alsl.d           t5,     a1,     t5,    1
-    fld.d            f10,    t5,     0
-    fldx.d           f11,    t5,     a1
+    vsle.bu          vr1,   vr0,   vr11 // max(abs(p1 - p0), abs(q1 - q0)) <= I
+.endif
+    vsle.bu          vr2,   vr2,   vr10 // abs(p0 - q0) * 2 + abs(p1 - q1) >> 1 <= E
+    vand.v           vr1,   vr1,   vr2  // fm
+    vand.v           vr1,   vr1,   vr13 // fm && wd >= 4
+.if \wd >= 6
+    vand.v           vr14,  vr14,  vr1  // fm && wd > 4
+.endif
+.if \wd >= 16
+    vand.v           vr15,  vr15,  vr1  // fm && wd == 16
+.endif
+    vhaddw.qu.du     vr8,   vr1,   vr1
+    vpickve2gr.du    t6,    vr8,   0
+    bnez             t6,    9f          // if (!fm || wd < 4) return;
+    li.w             t4,    1 << 0
+    jirl             zero,  ra,    0x00
+
+9:
+.if \wd >= 6
+    vabsd.bu         vr2,   vr21,  vr23 // abs(p2 - p0)
+    vabsd.bu         vr3,   vr22,  vr23 // abs(p1 - p0)
+    vabsd.bu         vr4,   vr25,  vr24 // abs(q1 - q0)
+    vabsd.bu         vr5,   vr26,  vr24 // abs(q2 - q0)
+.if \wd >= 8
+    vabsd.bu         vr6,   vr20,  vr23 // abs(p3 - p0)
+    vabsd.bu         vr7,   vr27,  vr24 // abs(q3 - q0)
+.endif
+    vmax.bu          vr2,   vr2,   vr3
+    vmax.bu          vr4,   vr4,   vr5
+.if \wd >= 8
+    vmax.bu          vr6,   vr6,   vr7
+.endif
+    vmax.bu          vr2,   vr2,   vr4
+.if \wd >= 8
+    vmax.bu          vr2,   vr2,   vr6
 .endif
 
-    vabsd.bu         vr12,   vr7,    vr8 //abs(p1-p0)
-    vabsd.bu         vr13,   vr10,   vr9 //abs(q1-q0)
-    vmax.bu          vr14,   vr12,   vr13
-    vslt.bu          vr2,    vr2,    vr14 //hev
-    vabsd.bu         vr12,   vr6,    vr7 //abs(p2-p1)
-    vmax.bu          vr12,   vr12,   vr14
-    vabsd.bu         vr13,   vr11,   vr10 //abs(q2-q1)
-    vmax.bu          vr12,   vr12,   vr13
-    vsle.bu          vr0,    vr12,   vr4 // <=I
-
-    vabsd.bu         vr13,   vr8,    vr9 //abs(p0-q0)
-    vsadd.bu         vr13,   vr13,   vr13
-    vabsd.bu         vr15,   vr7,    vr10
-    vsrli.b          vr15,   vr15,   1
-    vsadd.bu         vr13,   vr13,   vr15
-    vsle.bu          vr13,   vr13,   vr3 //abs(p0 - q0) * 2 + (abs(p1 - q1) >> 1) <= E
-    vand.v           vr0,    vr0,    vr13 //fm
-
-    vpickve2gr.wu    t5,     vr0,    0
-    beqz             t5,     .END_FILTER_\DIR\()\TYPE\()_W6
-
-    vabsd.bu         vr12,   vr6,    vr8 //abs(p2-p0)
-    vabsd.bu         vr13,   vr11,   vr9 //abs(q2-q0)
-    vmax.bu          vr12,   vr12,   vr14
-    vmax.bu          vr12,   vr12,   vr13
-    vxor.v           vr13,   vr13,   vr13
-    vaddi.bu         vr13,   vr13,   1
-    vsle.bu          vr1,    vr12,   vr13 //flat8in
-
-    //6789 10 11 --expand to h
-    vsllwil.hu.bu    vr12,   vr6,    0
-    vsllwil.hu.bu    vr13,   vr7,    0
-    vsllwil.hu.bu    vr14,   vr8,    0
-    vsllwil.hu.bu    vr15,   vr9,    0
-    vsllwil.hu.bu    vr16,   vr10,   0
-    vsllwil.hu.bu    vr17,   vr11,   0
-
-    //dst-2
-    vsadd.hu         vr18,   vr12,   vr12
-    vsadd.hu         vr18,   vr18,   vr12
-    vsadd.hu         vr18,   vr18,   vr13
-    vsadd.hu         vr18,   vr18,   vr13
-    vsadd.hu         vr18,   vr18,   vr14
-    vsadd.hu         vr18,   vr18,   vr14
-    vsadd.hu         vr18,   vr18,   vr15
-
-    //dst-1
-    vsadd.hu         vr19,   vr18,   vr15
-    vsadd.hu         vr19,   vr19,   vr16
-    vssub.hu         vr19,   vr19,   vr12
-    vssub.hu         vr19,   vr19,   vr12
-
-    //dst+0
-    vsadd.hu         vr20,   vr19,   vr17
-    vsadd.hu         vr20,   vr20,   vr16
-    vssub.hu         vr20,   vr20,   vr12
-    vssub.hu         vr20,   vr20,   vr13
-
-    //dst+1
-    vsadd.hu         vr21,   vr20,   vr17
-    vsadd.hu         vr21,   vr21,   vr17
-    vssub.hu         vr21,   vr21,   vr13
-    vssub.hu         vr21,   vr21,   vr14
-
-    vsrari.h         vr18,   vr18,   3
-    vsrari.h         vr19,   vr19,   3
-    vsrari.h         vr20,   vr20,   3
-    vsrari.h         vr21,   vr21,   3
-
-    vsub.h           vr22,   vr13,   vr16
-    vssrani.b.h      vr22,   vr22,   0
-    vand.v           vr22,   vr22,   vr2
-    vsllwil.h.b      vr22,   vr22,   0 //f = iclip_diff(p1 - q1);
-
-    vsub.h           vr23,   vr15,   vr14
-    vsadd.h          vr24,   vr23,   vr23
-    vsadd.h          vr23,   vr23,   vr24
-    vsadd.h          vr23,   vr23,   vr22
-    vssrani.b.h      vr23,   vr23,   0
-    vsllwil.h.b      vr23,   vr23,   0 //f = iclip_diff(3 * (q0 - p0) + f);
-
-    vaddi.hu         vr24,   vr23,   4
-    vaddi.hu         vr25,   vr23,   3
-    li.w             t5,     127
-    vreplgr2vr.h     vr3,    t5
-    vmin.h           vr24,   vr24,   vr3
-    vmin.h           vr25,   vr25,   vr3
-    vsrai.h          vr24,   vr24,   3 //f1
-    vsrai.h          vr25,   vr25,   3 //f2
-
-    vsadd.h          vr26,   vr14,   vr25 //dst-1
-    vssub.h          vr27,   vr15,   vr24 //dst+0
-
-    vsrari.h         vr24,   vr24,   1
-    vsadd.h          vr28,   vr13,   vr24
-    vssub.h          vr29,   vr16,   vr24
-    vsllwil.h.b      vr2,    vr2,    0
-    vbitsel.v        vr28,   vr28,   vr13,   vr2 //dst-2
-    vbitsel.v        vr29,   vr29,   vr16,   vr2 //dst+1
-
-    //flat8in
-    vsllwil.h.b      vr1,    vr1,    0
-    vbitsel.v        vr18,   vr28,   vr18,   vr1
-    vbitsel.v        vr19,   vr26,   vr19,   vr1
-    vbitsel.v        vr20,   vr27,   vr20,   vr1
-    vbitsel.v        vr21,   vr29,   vr21,   vr1
-
-    vssrani.bu.h     vr18,   vr18,   0
-    vssrani.bu.h     vr19,   vr19,   0
-    vssrani.bu.h     vr20,   vr20,   0
-    vssrani.bu.h     vr21,   vr21,   0
-
-    vbitsel.v        vr7,    vr7,    vr18,   vr0 //p1
-    vbitsel.v        vr8,    vr8,    vr19,   vr0 //p0
-    vbitsel.v        vr9,    vr9,    vr20,   vr0 //q0
-    vbitsel.v        vr10,   vr10,   vr21,   vr0 //q1
-
-.ifc \DIR, h
-    vilvl.b          vr7,    vr8,    vr7
-    vilvl.b          vr9,    vr10,   vr9
-    vilvl.h          vr7,    vr9,    vr7
-
-    addi.d           t5,     a0,    -2
-    vstelm.w         vr7,    t5,     0,      0
-    add.d            t5,     t5,     a1
-    vstelm.w         vr7,    t5,     0,      1
-    add.d            t5,     t5,     a1
-    vstelm.w         vr7,    t5,     0,      2
-    add.d            t5,     t5,     a1
-    vstelm.w         vr7,    t5,     0,      3
+.if \wd == 16
+    vabsd.bu         vr3,   vr17,  vr23 // abs(p6 - p0)
+    vabsd.bu         vr4,   vr18,  vr23 // abs(p5 - p0)
+    vabsd.bu         vr5,   vr19,  vr23 // abs(p4 - p0)
+.endif
+    vslei.bu         vr2,   vr2,   1    // flat8in
+.if \wd == 16
+    vabsd.bu         vr6,   vr28,  vr24 // abs(q4 - q0)
+    vabsd.bu         vr7,   vr29,  vr24 // abs(q5 - q0)
+    vabsd.bu         vr8,   vr30,  vr24 // abs(q6 - q0)
+.endif
+    vand.v           vr14,  vr2,   vr14 // flat8in && fm && wd > 4
+    vandn.v          vr1,   vr14,  vr1  // fm && wd >= 4 && !flat8in
+.if \wd == 16
+    vmax.bu          vr3,   vr3,   vr4
+    vmax.bu          vr5,   vr5,   vr6
+.endif
+    vhaddw.qu.du     vr9,   vr1,   vr1
+.if \wd == 16
+    vmax.bu          vr7,   vr7,   vr8
+    vmax.bu          vr3,   vr3,   vr5
+    vmax.bu          vr3,   vr3,   vr7
+    vslei.bu         vr3,   vr3,   1    // flat8out
+.endif
+    vpickve2gr.du    t6,    vr9,   0
+.if \wd == 16
+    vand.v           vr15,  vr15,  vr3  // flat8out && fm && wd == 16
+    vand.v           vr15,  vr15,  vr14 // flat8out && flat8in && fm && wd == 16
+    vandn.v          vr14,  vr15,  vr14 // flat8in && fm && wd >= 4 && !flat8out
+.endif
+    beqz             t6,    1f          // skip wd == 4 case
+.endif
+    vxori.b          vr2,   vr22,  128  // p1 - 128
+    vxori.b          vr3,   vr25,  128  // q1 - 128
+    vslt.bu          vr0,   vr12,  vr0  // hev
+    vssub.b          vr2,   vr2,   vr3  // iclip_diff(p1 - q1)
+    vand.v           vr4,   vr2,   vr0  // if (hev) iclip_diff(p1 - q1)
+    vandn.v          vr0,   vr0,   vr1  // (fm && wd >= 4 && !hev)
+    vxor.v           vr5,   vr5,   vr5
+    vaddi.hu         vr5,   vr5,   3
+    vsubwev.h.bu     vr2,   vr24,  vr23
+    vsubwod.h.bu     vr3,   vr24,  vr23
+    vmul.h           vr2,   vr2,   vr5
+    vmul.h           vr3,   vr3,   vr5
+    vxor.v           vr6,   vr6,   vr6
+    vaddwev.h.b      vr7,   vr4,   vr6
+    vaddwod.h.b      vr6,   vr4,   vr6
+    vadd.h           vr2,   vr2,   vr7
+    vadd.h           vr3,   vr3,   vr6
+    vssrani.b.h      vr2,   vr2,   0
+    vssrani.b.h      vr3,   vr3,   0
+    vilvl.b          vr2,   vr3,   vr2 // f
+    vxor.v           vr6,   vr6,   vr6
+    vaddi.bu         vr5,   vr6,   3
+    vaddi.bu         vr6,   vr6,   4   // 4
+    vsadd.b          vr4,   vr6,   vr2 // imin(f + 4, 127)
+    vsadd.b          vr5,   vr5,   vr2 // imin(f + 3, 127)
+    vsrai.b          vr4,   vr4,   3   // f1
+    vsrai.b          vr5,   vr5,   3   // f2
+    vaddi.bu         vr2,   vr23,  0   // p0
+    vaddi.bu         vr3,   vr24,  0   // q0
+    vxori.b          vr2,   vr2,   128
+    vxori.b          vr3,   vr3,   128
+    vsadd.b          vr2,   vr2,   vr5 // p0 + f2 out p0
+    vssub.b          vr3,   vr3,   vr4 // q0 - f1 out q0
+    vxori.b          vr2,   vr2,   128
+    vxori.b          vr3,   vr3,   128
+    vsrari.b         vr4,   vr4,   1   // (f1 + 1) >> 1
+    vbitsel.v        vr23,  vr23,  vr2,   vr1 // if (fm && wd >= 4)
+    vbitsel.v        vr24,  vr24,  vr3,   vr1 // if (fm && wd >= 4)
+    vaddi.bu         vr2,   vr22,  0   // p1
+    vaddi.bu         vr3,   vr25,  0   // q1
+    vxori.b          vr2,   vr2,   128
+    vxori.b          vr3,   vr3,   128
+    vsadd.b          vr2,   vr2,   vr4 // out p1
+    vssub.b          vr3,   vr3,   vr4 // out q1
+    vxori.b          vr2,   vr2,   128
+    vxori.b          vr3,   vr3,   128
+    vbitsel.v        vr22,  vr22,  vr2,   vr0 // if (fm && wd >= 4 && !hev)
+    vbitsel.v        vr25,  vr25,  vr3,   vr0 // if (fm && wd >= 4 && !hev)
+1:
+
+.if \wd == 6
+    vhaddw.qu.du     vr0,   vr14,  vr14
+    vpickve2gr.du    t6,    vr0,   0
+    beqz             t6,    2f        // skip if there's no flat8in
+
+    vaddwev.h.bu     vr0,   vr21,  vr21
+    vaddwod.h.bu     vr1,   vr21,  vr21 // p2 * 2
+    vaddwev.h.bu     vr2,   vr21,  vr22
+    vaddwod.h.bu     vr3,   vr21,  vr22 // p2 + p1
+    vaddwev.h.bu     vr4,   vr22,  vr23
+    vaddwod.h.bu     vr5,   vr22,  vr23 // p1 + p0
+    vaddwev.h.bu     vr6,   vr23,  vr24
+    vaddwod.h.bu     vr7,   vr23,  vr24 // p0 + q0
+    vadd.h           vr8,   vr0,   vr2
+    vadd.h           vr9,   vr1,   vr3
+    vadd.h           vr10,  vr4,   vr6
+    vadd.h           vr11,  vr5,   vr7
+    vaddwev.h.bu     vr12,  vr24,  vr25
+    vaddwod.h.bu     vr13,  vr24,  vr25 // q0 + q1
+    vadd.h           vr8,   vr8,   vr10
+    vadd.h           vr9,   vr9,   vr11
+    vsub.h           vr12,  vr12,  vr0
+    vsub.h           vr13,  vr13,  vr1
+    vaddwev.h.bu     vr10,  vr25,  vr26
+    vaddwod.h.bu     vr11,  vr25,  vr26 // q1 + q2
+    vssrlrni.bu.h    vr0,   vr8,   3
+    vssrlrni.bu.h    vr1,   vr9,   3
+    vilvl.b          vr0,   vr1,   vr0  // out p1
+
+    vadd.h           vr8,   vr8,   vr12
+    vadd.h           vr9,   vr9,   vr13
+    vsub.h           vr10,  vr10,  vr2
+    vsub.h           vr11,  vr11,  vr3
+    vaddwev.h.bu     vr12,  vr26,  vr26 // q2 + q2
+    vaddwod.h.bu     vr13,  vr26,  vr26
+    vssrlrni.bu.h    vr1,   vr8,   3
+    vssrlrni.bu.h    vr2,   vr9,   3
+    vilvl.b          vr1,   vr2,   vr1  // out p0
+
+    vadd.h           vr8,   vr8,   vr10
+    vadd.h           vr9,   vr9,   vr11
+    vsub.h           vr12,  vr12,  vr4
+    vsub.h           vr13,  vr13,  vr5
+    vssrlrni.bu.h    vr2,   vr8,   3
+    vssrlrni.bu.h    vr3,   vr9,   3
+    vilvl.b          vr2,   vr3,   vr2  // out q0
+
+    vbitsel.v        vr22,  vr22,  vr0,  vr14
+    vadd.h           vr8,   vr8,   vr12
+    vadd.h           vr9,   vr9,   vr13
+    vbitsel.v        vr23,  vr23,  vr1,  vr14
+    vssrlrni.bu.h    vr3,   vr8,   3
+    vssrlrni.bu.h    vr4,   vr9,   3
+    vilvl.b          vr3,   vr4,   vr3
+    vbitsel.v        vr24,  vr24,  vr2,  vr14
+    vbitsel.v        vr25,  vr25,  vr3,  vr14
+.elseif \wd >= 8
+    vhaddw.qu.du     vr0,   vr14,  vr14
+    vpickve2gr.du    t6,    vr0,   0
+.if \wd == 8
+    beqz             t6,    8f  // skip if there's no flat8in
 .else
-    fst.s            f9,     a0,     0
-    fstx.s           f10,    a0,     a1
-    sub.d            t5,     a0,     a1
-    fst.s            f8,     t5,     0
-    sub.d            t5,     t5,     a1
-    fst.s            f7,     t5,     0
+    beqz             t6,    2f  // skip if there's no flat8in
 .endif
-.END_FILTER_\DIR\()\TYPE\()_W6:
-.endm
 
-.macro FILTER_W8 DIR, TYPE
-.ifc \DIR, h
-    addi.d           t5,     a0,    -4
-    fld.d            f6,     t5,     0 //p3 p2 p1 p0 q0 q1 q2 q3
-    fldx.d           f7,     t5,     a1
-    alsl.d           t5,     a1,     t5,     1
-    fld.d            f8,     t5,     0
-    fldx.d           f9,     t5,     a1
-
-    vilvl.b          vr6,    vr7,    vr6
-    vilvl.b          vr7,    vr9,    vr8
-    vilvh.h          vr10,   vr7,    vr6 //q0
-    vilvl.h          vr6,    vr7,    vr6 //p3
-    vbsrl.v          vr7,    vr6,    4   //p2
-    vbsrl.v          vr8,    vr6,    8   //p1
-    vbsrl.v          vr9,    vr6,    12  //p0
-    vbsrl.v          vr11,   vr10,   4   //q1
-    vbsrl.v          vr12,   vr10,   8   //q2
-    vbsrl.v          vr13,   vr10,   12  //q3
-.else
-    fld.s            f10,    a0,     0
-    fldx.s           f11,    a0,     a1
-    add.d            t5,     a0,     a1
-    fldx.s           f12,    t5,     a1
-    add.d            t5,     t5,     a1
-    fldx.s           f13,    t5,     a1
-    sub.d            t5,     a0,     a1
-    fld.s            f9,     t5,     0
-    sub.d            t5,     t5,     a1
-    fld.s            f8,     t5,     0
-    sub.d            t5,     t5,     a1
-    fld.s            f7,     t5,     0
-    sub.d            t5,     t5,     a1
-    fld.s            f6,     t5,     0
+    vaddwev.h.bu     vr0,   vr20,  vr21
+    vaddwod.h.bu     vr1,   vr20,  vr21 // p3 + p2
+    vaddwev.h.bu     vr2,   vr22,  vr25
+    vaddwod.h.bu     vr3,   vr22,  vr25 // p1 + q1
+    vaddwev.h.bu     vr4,   vr20,  vr22
+    vaddwod.h.bu     vr5,   vr20,  vr22 // p3 + p1
+    vaddwev.h.bu     vr6,   vr23,  vr26
+    vaddwod.h.bu     vr7,   vr23,  vr26 // p0 + q2
+    vadd.h           vr8,   vr0,   vr0
+    vadd.h           vr9,   vr1,   vr1  // 2 * (p3 + p2)
+    vxor.v           vr10,  vr10,  vr10
+    vaddwev.h.bu     vr11,  vr23,  vr10
+    vaddwod.h.bu     vr12,  vr23,  vr10
+    vaddwev.h.bu     vr13,  vr24,  vr10
+    vaddwod.h.bu     vr10,  vr24,  vr10
+    vadd.h           vr8,   vr8,   vr11 // + p0
+    vadd.h           vr9,   vr9,   vr12
+    vadd.h           vr8,   vr8,   vr13 // + q0
+    vadd.h           vr9,   vr9,   vr10
+    vadd.h           vr8,   vr8,   vr4
+    vadd.h           vr9,   vr9,   vr5  // + p3 + p1
+    vsub.h           vr2,   vr2,   vr0
+    vsub.h           vr3,   vr3,   vr1  // p1 + q1 - p3 - p2
+    vsub.h           vr6,   vr6,   vr4
+    vsub.h           vr7,   vr7,   vr5  // p0 + q2 - p3 - p1
+    vssrlrni.bu.h    vr10,  vr8,   3
+    vssrlrni.bu.h    vr11,  vr9,   3
+    vilvl.b          vr10,  vr11,  vr10 // out p2
+
+    vadd.h           vr8,   vr8,   vr2
+    vadd.h           vr9,   vr9,   vr3
+    vaddwev.h.bu     vr0,   vr20,  vr23
+    vaddwod.h.bu     vr1,   vr20,  vr23 // p3 + p0
+    vaddwev.h.bu     vr2,   vr24,  vr27
+    vaddwod.h.bu     vr3,   vr24,  vr27 // q0 + q3
+    vssrlrni.bu.h    vr11,  vr8,   3
+    vssrlrni.bu.h    vr12,  vr9,   3
+    vilvl.b          vr11,  vr12,  vr11 // out p1
+
+    vadd.h           vr8,   vr8,   vr6
+    vadd.h           vr9,   vr9,   vr7
+    vsub.h           vr2,   vr2,   vr0 // q0 + q3 - p3 - p0
+    vsub.h           vr3,   vr3,   vr1
+    vaddwev.h.bu     vr4,   vr21,  vr24 // p2 + q0
+    vaddwod.h.bu     vr5,   vr21,  vr24
+    vaddwev.h.bu     vr6,   vr25,  vr27 // q1 + q3
+    vaddwod.h.bu     vr7,   vr25,  vr27
+    vssrlrni.bu.h    vr12,  vr8,   3
+    vssrlrni.bu.h    vr13,  vr9,   3
+    vilvl.b          vr12,  vr13,  vr12 // out p0
+
+    vadd.h           vr8,   vr8,   vr2
+    vadd.h           vr9,   vr9,   vr3
+    vsub.h           vr6,   vr6,   vr4 // q1 + q3 - p2 - q0
+    vsub.h           vr7,   vr7,   vr5
+    vaddwev.h.bu     vr0,   vr22,  vr25 // p1 + q1
+    vaddwod.h.bu     vr1,   vr22,  vr25
+    vaddwev.h.bu     vr2,   vr26,  vr27
+    vaddwod.h.bu     vr3,   vr26,  vr27 // q2 + q3
+    vssrlrni.bu.h    vr13,  vr8,   3
+    vssrlrni.bu.h    vr4,   vr9,   3
+    vilvl.b          vr13,  vr4,   vr13 // out q0
+
+    vadd.h           vr8,   vr8,   vr6
+    vadd.h           vr9,   vr9,   vr7
+    vsub.h           vr2,   vr2,   vr0  // q2 + q3 - p1 - q1
+    vsub.h           vr3,   vr3,   vr1
+    vssrlrni.bu.h    vr0,   vr8,   3
+    vssrlrni.bu.h    vr1,   vr9,   3
+    vilvl.b          vr0,   vr1,   vr0 // out q1
+
+    vadd.h           vr8,   vr8,   vr2
+    vadd.h           vr9,   vr9,   vr3
+
+    vbitsel.v        vr21,  vr21,  vr10,  vr14
+    vbitsel.v        vr22,  vr22,  vr11,  vr14
+    vbitsel.v        vr23,  vr23,  vr12,  vr14
+    vbitsel.v        vr24,  vr24,  vr13,  vr14
+    vssrlrni.bu.h    vr1,   vr8,   3
+    vssrlrni.bu.h    vr2,   vr9,   3
+    vilvl.b          vr1,   vr2,  vr1 // out q2
+    vbitsel.v        vr25,  vr25,  vr0,   vr14
+    vbitsel.v        vr26,  vr26,  vr1,   vr14
+.endif
+2:
+.if \wd == 16
+    vhaddw.qu.du     vr2,   vr15,  vr15
+    vpickve2gr.du    t6,    vr2,   0
+    bnez             t6,    1f                 // check if flat8out is needed
+    vhaddw.qu.du     vr2,   vr14,  vr14
+    vpickve2gr.du    t6,    vr2,   0
+    beqz             t6,    8f                 // if there was no flat8in, just write the inner 4 pixels
+    b                7f                        // if flat8in was used, write the inner 6 pixels
+1:
+
+    vaddwev.h.bu     vr2,   vr17,  vr17  // p6 + p6
+    vaddwod.h.bu     vr3,   vr17,  vr17
+    vaddwev.h.bu     vr4,   vr17,  vr18
+    vaddwod.h.bu     vr5,   vr17,  vr18  // p6 + p5
+    vaddwev.h.bu     vr6,   vr17,  vr19
+    vaddwod.h.bu     vr7,   vr17,  vr19  // p6 + p4
+    vaddwev.h.bu     vr8,   vr17,  vr20
+    vaddwod.h.bu     vr9,   vr17,  vr20  // p6 + p3
+    vadd.h           vr12,  vr2,   vr4
+    vadd.h           vr13,  vr3,   vr5
+    vadd.h           vr10,  vr6,   vr8
+    vadd.h           vr11,  vr7,   vr9
+    vaddwev.h.bu     vr6,   vr17,  vr21
+    vaddwod.h.bu     vr7,   vr17,  vr21  // p6 + p2
+    vadd.h           vr12,  vr12,  vr10
+    vadd.h           vr13,  vr13,  vr11
+    vaddwev.h.bu     vr8,   vr17,  vr22
+    vaddwod.h.bu     vr9,   vr17,  vr22  // p6 + p1
+    vaddwev.h.bu     vr10,  vr18,  vr23
+    vaddwod.h.bu     vr11,  vr18,  vr23  // p5 + p0
+    vadd.h           vr6,   vr6,   vr8
+    vadd.h           vr7,   vr7,   vr9
+    vaddwev.h.bu     vr8,   vr19,  vr24
+    vaddwod.h.bu     vr9,   vr19,  vr24  // p4 + q0
+    vadd.h           vr12,  vr12,  vr6
+    vadd.h           vr13,  vr13,  vr7
+    vadd.h           vr10,  vr10,  vr8
+    vadd.h           vr11,  vr11,  vr9
+    vaddwev.h.bu     vr6,   vr20,  vr25
+    vaddwod.h.bu     vr7,   vr20,  vr25  // p3 + q1
+    vadd.h           vr12,  vr12,  vr10
+    vadd.h           vr13,  vr13,  vr11
+    vsub.h           vr6,   vr6,   vr2
+    vsub.h           vr7,   vr7,   vr3
+    vaddwev.h.bu     vr2,   vr21,  vr26
+    vaddwod.h.bu     vr3,   vr21,  vr26  // p2 + q2
+    vssrlrni.bu.h    vr0,   vr12,  4
+    vssrlrni.bu.h    vr1,   vr13,  4
+    vilvl.b          vr0,   vr1,   vr0   // out p5
+    vadd.h           vr12,  vr12,  vr6
+    vadd.h           vr13,  vr13,  vr7   // - (p6 + p6) + (p3 + q1)
+    vsub.h           vr2,   vr2,   vr4
+    vsub.h           vr3,   vr3,   vr5
+    vaddwev.h.bu     vr4,   vr22,  vr27
+    vaddwod.h.bu     vr5,   vr22,  vr27  // p1 + q3
+    vaddwev.h.bu     vr6,   vr17,  vr19
+    vaddwod.h.bu     vr7,   vr17,  vr19  // p6 + p4
+    vssrlrni.bu.h    vr1,   vr12,  4
+    vssrlrni.bu.h    vr8,   vr13,  4
+    vilvl.b          vr1,   vr8,   vr1   // out p4
+    vadd.h           vr12,  vr12,  vr2
+    vadd.h           vr13,  vr13,  vr3   // - (p6 + p5) + (p2 + q2)
+    vsub.h           vr4,   vr4,   vr6
+    vsub.h           vr5,   vr5,   vr7
+    vaddwev.h.bu     vr6,   vr23,  vr28
+    vaddwod.h.bu     vr7,   vr23,  vr28  // p0 + q4
+    vaddwev.h.bu     vr8,   vr17,  vr20
+    vaddwod.h.bu     vr9,   vr17,  vr20  // p6 + p3
+    vssrlrni.bu.h    vr2,   vr12,  4
+    vssrlrni.bu.h    vr10,  vr13,  4
+    vilvl.b          vr2,   vr10,  vr2   // out p3
+    vadd.h           vr12,  vr12,  vr4
+    vadd.h           vr13,  vr13,  vr5   // - (p6 + p4) + (p1 + q3)
+    vsub.h           vr6,   vr6,   vr8
+    vsub.h           vr7,   vr7,   vr9
+    vaddwev.h.bu     vr8,   vr24,  vr29
+    vaddwod.h.bu     vr9,   vr24,  vr29  // q0 + q5
+    vaddwev.h.bu     vr4,   vr17,  vr21
+    vaddwod.h.bu     vr5,   vr17,  vr21  // p6 + p2
+    vssrlrni.bu.h    vr3,   vr12,  4
+    vssrlrni.bu.h    vr11,  vr13,  4
+    vilvl.b          vr3,   vr11,  vr3   // out p2
+    vadd.h           vr12,  vr12,  vr6
+    vadd.h           vr13,  vr13,  vr7   // - (p6 + p3) + (p0 + q4)
+    vsub.h           vr8,   vr8,   vr4
+    vsub.h           vr9,   vr9,   vr5
+    vaddwev.h.bu     vr6,   vr25,  vr30
+    vaddwod.h.bu     vr7,   vr25,  vr30  // q1 + q6
+    vaddwev.h.bu     vr10,  vr17,  vr22
+    vaddwod.h.bu     vr11,  vr17,  vr22  // p6 + p1
+    vssrlrni.bu.h    vr4,   vr12,  4
+    vssrlrni.bu.h    vr5,   vr13,  4
+    vilvl.b          vr4,   vr5,   vr4   // out p1
+
+    vadd.h           vr12,  vr12,  vr8
+    vadd.h           vr13,  vr13,  vr9   // - (p6 + p2) + (q0 + q5)
+    vsub.h           vr6,   vr6,   vr10
+    vsub.h           vr7,   vr7,   vr11
+    vaddwev.h.bu     vr8,   vr26,  vr30
+    vaddwod.h.bu     vr9,   vr26,  vr30  // q2 + q6
+    vbitsel.v        vr0,   vr18,  vr0,  vr15  // out p5
+    vaddwev.h.bu     vr10,  vr18,  vr23
+    vaddwod.h.bu     vr11,  vr18,  vr23  // p5 + p0
+    vssrlrni.bu.h    vr5,   vr12,  4
+    vssrlrni.bu.h    vr18,  vr13,  4
+    vilvl.b          vr5,   vr18,  vr5   // out p0
+
+    vadd.h           vr12,  vr12,  vr6
+    vadd.h           vr13,  vr13,  vr7   // - (p6 + p1) + (q1 + q6)
+    vsub.h           vr8,   vr8,   vr10
+    vsub.h           vr9,   vr9,   vr11
+    vaddwev.h.bu     vr10,  vr27,  vr30
+    vaddwod.h.bu     vr11,  vr27,  vr30  // q3 + q6
+    vbitsel.v        vr1,   vr19,  vr1,  vr15  // out p4
+
+    vaddwev.h.bu     vr18,  vr19,  vr24
+    vaddwod.h.bu     vr19,  vr19,  vr24  // p4 + q0
+    vssrlrni.bu.h    vr6,   vr12,  4
+    vssrlrni.bu.h    vr7,   vr13,  4
+    vilvl.b          vr6,   vr7,   vr6   // out q0
+
+    vadd.h           vr12,  vr12,  vr8
+    vadd.h           vr13,  vr13,  vr9   // - (p5 + p0) + (q2 + q6)
+    vsub.h           vr10,  vr10,  vr18
+    vsub.h           vr11,  vr11,  vr19
+    vaddwev.h.bu     vr8,   vr28,  vr30
+    vaddwod.h.bu     vr9,   vr28,  vr30  // q4 + q6
+    vbitsel.v        vr2,   vr20,  vr2,  vr15  // out p3
+    vaddwev.h.bu     vr18,  vr20,  vr25
+    vaddwod.h.bu     vr19,  vr20,  vr25  // p3 + q1
+    vssrlrni.bu.h    vr7,   vr12,  4
+    vssrlrni.bu.h    vr20,  vr13,  4
+    vilvl.b          vr7,   vr20,  vr7   // out q1
+
+    vadd.h           vr12,  vr12,  vr10
+    vadd.h           vr13,  vr13,  vr11  // - (p4 + q0) + (q3 + q6)
+    vsub.h           vr18,  vr8,   vr18
+    vsub.h           vr19,  vr9,   vr19
+    vaddwev.h.bu     vr10,  vr29,  vr30
+    vaddwod.h.bu     vr11,  vr29,  vr30  // q5 + q6
+    vbitsel.v        vr3,   vr21,  vr3,  vr15  // out p2
+    vaddwev.h.bu     vr20,  vr21,  vr26
+    vaddwod.h.bu     vr21,  vr21,  vr26  // p2 + q2
+    vssrlrni.bu.h    vr8,   vr12,  4
+    vssrlrni.bu.h    vr9,   vr13,  4
+    vilvl.b          vr8,   vr9,   vr8   // out q2
+
+    vadd.h           vr12,  vr12,  vr18
+    vadd.h           vr13,  vr13,  vr19  // - (p3 + q1) + (q4 + q6)
+    vsub.h           vr10,  vr10,  vr20
+    vsub.h           vr11,  vr11,  vr21
+    vaddwev.h.bu     vr18,  vr30,  vr30
+    vaddwod.h.bu     vr19,  vr30,  vr30  // q6 + q6
+    vbitsel.v        vr4,   vr22,  vr4,  vr15  // out p1
+    vaddwev.h.bu     vr20,  vr22,  vr27
+    vaddwod.h.bu     vr21,  vr22,  vr27  // p1 + q3
+    vssrlrni.bu.h    vr9,   vr12,  4
+    vssrlrni.bu.h    vr22,  vr13,  4
+    vilvl.b          vr9,   vr22,  vr9   // out q3
+    vadd.h           vr12,  vr12,  vr10
+    vadd.h           vr13,  vr13,  vr11  // - (p2 + q2) + (q5 + q6)
+    vsub.h           vr18,  vr18,  vr20
+    vsub.h           vr19,  vr19,  vr21
+    vbitsel.v        vr5,   vr23,  vr5,  vr15  // out p0
+    vssrlrni.bu.h    vr10,  vr12,  4
+    vssrlrni.bu.h    vr23,  vr13,  4
+    vilvl.b          vr10,  vr23,  vr10  // out q4
+    vadd.h           vr12,  vr12,  vr18
+    vadd.h           vr13,  vr13,  vr19  // - (p1 + q3) + (q6 + q6)
+    vssrlrni.bu.h    vr11,  vr12,  4
+    vssrlrni.bu.h    vr12,  vr13,  4
+    vilvl.b          vr11,  vr12,  vr11  // out q5
+    vbitsel.v        vr6,   vr24,  vr6,  vr15
+    vbitsel.v        vr7,   vr25,  vr7,  vr15
+    vbitsel.v        vr8,   vr26,  vr8,  vr15
+    vbitsel.v        vr9,   vr27,  vr9,  vr15
+    vbitsel.v        vr10,  vr28,  vr10, vr15
+    vbitsel.v        vr11,  vr29,  vr11, vr15
 .endif
+    li.w             t4,    0
+    jirl             zero,  ra,    0x00
+.if \wd == 16
+7:
+    // Return to a shorter epilogue, writing only the inner 6 pixels
+    li.w             t4,    1 << 6
+    jirl             zero,  ra,    0x00
+.endif
+.if \wd >= 8
+8:
+    // Return to a shorter epilogue, writing only the inner 4 pixels
+    li.w             t4,    1 << 4
+    jirl             zero,  ra,    0x00
+.endif
+endfuncl
+.endm
 
-    vabsd.bu         vr14,   vr8,    vr9  //p1-p0
-    vabsd.bu         vr15,   vr11,   vr10 //q1-q0
-    vabsd.bu         vr16,   vr9,    vr10 //p0-q0
-    vabsd.bu         vr17,   vr8,    vr11 //p1-q1
-    vabsd.bu         vr18,   vr7,    vr8  //p2-p1
-    vabsd.bu         vr19,   vr12,   vr11 //q2-q1
-    vabsd.bu         vr20,   vr6,    vr7  //p3-p2
-    vabsd.bu         vr21,   vr13,   vr12 //q3-q2
-
-    vmax.bu          vr22,   vr14,   vr15
-    vsle.bu          vr23,   vr22,   vr4  //abs(p1 - p0) <= I && abs(q1 - q0) <= I
-    vsadd.bu         vr16,   vr16,   vr16
-    vsrli.b          vr17,   vr17,   1
-    vsadd.bu         vr16,   vr16,   vr17
-    vsle.bu          vr16,   vr16,   vr3  //abs(p0 - q0) * 2 + (abs(p1 - q1) >> 1) <= E
-    vand.v           vr16,   vr16,   vr23 //fm
-
-    vpickve2gr.wu    t5,     vr16,   0
-    beqz             t5,     .END_FILTER_\DIR\()\TYPE\()_W8
-
-    vmax.bu          vr23,   vr18,   vr19
-    vmax.bu          vr23,   vr23,   vr20
-    vmax.bu          vr23,   vr23,   vr21
-    vsle.bu          vr23,   vr23,   vr4
-    vand.v           vr16,   vr16,   vr23 //fm
-
-    vabsd.bu         vr17,   vr7,    vr9  //abs(p2-p0)
-    vabsd.bu         vr18,   vr12,   vr10 //abs(q2-q0)
-    vmax.bu          vr17,   vr17,   vr14
-    vmax.bu          vr17,   vr17,   vr15
-    vmax.bu          vr17,   vr17,   vr18
-    vabsd.bu         vr18,   vr6,    vr9  //abs(p3 - p0)
-    vabsd.bu         vr19,   vr13,   vr10 //abs(q3 - q0)
-    vmax.bu          vr17,   vr17,   vr18
-    vmax.bu          vr17,   vr17,   vr19
-
-    vxor.v           vr5,    vr5,    vr5
-    vaddi.bu         vr5,    vr5,    1    //F
-    vsle.bu          vr17,   vr17,   vr5  //flat8in
-
-    vsllwil.hu.bu    vr0,    vr6,    0 //p3
-    vsllwil.hu.bu    vr1,    vr7,    0 //p2
-    vsllwil.hu.bu    vr27,   vr8,    0 //p1
-    vsllwil.hu.bu    vr3,    vr9,    0 //p0
-    vsllwil.hu.bu    vr4,    vr10,   0 //q0
-    vsllwil.hu.bu    vr5,    vr11,   0 //q1
-    vsllwil.hu.bu    vr14,   vr12,   0 //q2
-    vsllwil.hu.bu    vr15,   vr13,   0 //q3
-
-    vsadd.hu         vr18,   vr0,    vr0  //p3+p3
-    vsadd.hu         vr19,   vr15,   vr15 //q3+q3
-    vsadd.hu         vr20,   vr0,    vr1  //p3+p2
-    vsadd.hu         vr21,   vr1,    vr27 //p2+p1
-    vsadd.hu         vr28,   vr27,   vr3  //p1+p0
-    vsadd.hu         vr23,   vr3,    vr4  //p0+q0
-    vsadd.hu         vr24,   vr4,    vr5  //q0+q1
-    vsadd.hu         vr25,   vr5,    vr14 //q1+q2
-    vsadd.hu         vr26,   vr14,   vr15 //q2+q3
-
-    // dst-3
-    vsadd.hu         vr29,   vr18,   vr20
-    vsadd.hu         vr29,   vr29,   vr21
-    vsadd.hu         vr29,   vr29,   vr23
-
-    // dst-2
-    vsadd.hu         vr30,   vr18,   vr21
-    vsadd.hu         vr30,   vr30,   vr28
-    vsadd.hu         vr30,   vr30,   vr24
-
-    // dst-1
-    vsadd.hu         vr31,   vr20,   vr28
-    vsadd.hu         vr31,   vr31,   vr23
-    vsadd.hu         vr31,   vr31,   vr25
-
-    // dst+0
-    vsadd.hu         vr18,   vr21,   vr23
-    vsadd.hu         vr18,   vr18,   vr24
-    vsadd.hu         vr18,   vr18,   vr26
-
-    //dst+1
-    vsadd.hu         vr20,   vr28,   vr24
-    vsadd.hu         vr20,   vr20,   vr25
-    vsadd.hu         vr20,   vr20,   vr19
-
-    //dst+2
-    vsadd.hu         vr21,   vr23,   vr25
-    vsadd.hu         vr21,   vr21,   vr26
-    vsadd.hu         vr21,   vr21,   vr19
-
-    vssrarni.bu.h    vr23,   vr29,   3
-    vssrarni.bu.h    vr24,   vr30,   3
-    vssrarni.bu.h    vr25,   vr31,   3
-    vssrarni.bu.h    vr19,   vr18,   3
-    vssrarni.bu.h    vr20,   vr20,   3
-    vssrarni.bu.h    vr21,   vr21,   3
-
-    // !flat8in
-    vslt.bu          vr2,    vr2,    vr22 //hev
-
-    vsub.h           vr30,   vr27,   vr5 //p1-q1
-    vssrani.b.h      vr30,   vr30,   0
-    vand.v           vr30,   vr30,   vr2
-    vsllwil.h.b      vr30,   vr30,   0
-
-    vsub.h           vr31,   vr4,    vr3
-    vsadd.h          vr0,    vr31,   vr31
-    vsadd.h          vr31,   vr31,   vr0
-    vsadd.h          vr31,   vr31,   vr30
-    vssrani.b.h      vr31,   vr31,   0
-    vsllwil.h.b      vr31,   vr31,   0 //f = iclip_diff(3 * (q0 - p0) + f);
-
-    vaddi.hu         vr14,   vr31,   4
-    vaddi.hu         vr15,   vr31,   3
-    li.w             t5,     127
-    vreplgr2vr.h     vr18,   t5
-    vmin.h           vr14,   vr14,   vr18
-    vmin.h           vr15,   vr15,   vr18
-    vsrai.h          vr14,   vr14,   3 //f1
-    vsrai.h          vr15,   vr15,   3 //f2
-
-    vsadd.h          vr3,    vr3,    vr15
-    vssub.h          vr4,    vr4,    vr14
-    vssrani.bu.h     vr3,    vr3,    0 //dst-1
-    vssrani.bu.h     vr4,    vr4,    0 //dst+0
-
-    vsrari.h         vr14,   vr14,   1
-    vsadd.h          vr18,   vr27,   vr14
-    vssub.h          vr26,   vr5,    vr14
-    vssrani.bu.h     vr18,   vr18,   0 //dst-2
-    vssrani.bu.h     vr26,   vr26,   0 //dst+1
-
-    vbitsel.v        vr27,   vr18,   vr8,   vr2 //dst-2
-    vbitsel.v        vr28,   vr26,   vr11,  vr2 //dst+1
-
-    vbitsel.v        vr23,   vr7,    vr23,  vr17 //dst-3 (p2)
-    vbitsel.v        vr24,   vr27,   vr24,  vr17 //dst-2
-    vbitsel.v        vr25,   vr3,    vr25,  vr17 //dst-1
-    vbitsel.v        vr19,   vr4,    vr19,  vr17 //dst+0
-    vbitsel.v        vr20,   vr28,   vr20,  vr17 //dst+1
-    vbitsel.v        vr21,   vr12,   vr21,  vr17 //dst+2
-
-    vbitsel.v        vr7,    vr7,    vr23,  vr16 //-3
-    vbitsel.v        vr8,    vr8,    vr24,  vr16 //-2
-    vbitsel.v        vr9,    vr9,    vr25,  vr16 //-1
-    vbitsel.v        vr10,   vr10,   vr19,  vr16 //+0
-    vbitsel.v        vr11,   vr11,   vr20,  vr16 //+1
-    vbitsel.v        vr12,   vr12,   vr21,  vr16 //+2
+FILTER 16
+FILTER 8
+FILTER 6
+FILTER 4
+
+.macro LPF_16_WD16
+    move             t7,    ra
+    bl               lpf_16_wd16_lsx
+    move             ra,    t7
+    beqz             t4,    1f
+    andi             t5,    t4,    1 << 6
+    bnez             t5,    7f
+    andi             t5,    t4,    1 << 4
+    bnez             t5,    8f
+    jirl             zero,  ra,    0x00
+1:
+.endm
 
-.ifc \DIR, h
-    vilvl.b          vr6,    vr7,    vr6
-    vilvl.b          vr8,    vr9,    vr8
-    vilvl.b          vr10,   vr11,   vr10
-    vilvl.b          vr12,   vr13,   vr12
-    vilvl.h          vr6,    vr8,    vr6  //p3p2p1p0 -- -- --
-    vilvl.h          vr10,   vr12,   vr10 //q0q1q2q3 -- -- --
-    vilvl.w          vr0,    vr10,   vr6  //p3p2p1p0q0q1q2q3 --
-    vilvh.w          vr1,    vr10,   vr6  //--
-
-    addi.d           t5,     a0,     -4
-    vstelm.d         vr0,    t5,     0,     0
-    add.d            t5,     t5,     a1
-    vstelm.d         vr0,    t5,     0,     1
-    add.d            t5,     t5,     a1
-    vstelm.d         vr1,    t5,     0,     0
-    add.d            t5,     t5,     a1
-    vstelm.d         vr1,    t5,     0,     1
-.else
-    alsl.d           t5,     a1,     a1,    1
-    sub.d            t5,     a0,     t5
-    fst.s            f7,     t5,     0
-    fstx.s           f8,     t5,     a1
-    add.d            t5,     t5,     a1
-    fstx.s           f9,     t5,     a1
-
-    fst.s            f10,    a0,     0
-    add.d            t5,     a0,     a1
-    fst.s            f11,    t5,     0
-    fstx.s           f12,    t5,     a1
-.endif
-.END_FILTER_\DIR\()\TYPE\()_W8:
+.macro LPF_16_WD8
+    move             t7,    ra
+    bl               lpf_16_wd8_lsx
+    move             ra,    t7
+    beqz             t4,    1f
+    andi             t5,    t4,    1 << 4
+    bnez             t5,    8f
+    jirl             zero,  ra,    0x00
+1:
 .endm
 
-.macro FILTER_W16 DIR, TYPE
-.ifc \DIR, h
-    addi.d           t5,     a0,    -7
-    vld              vr6,    t5,     0 //p6p5p4p3p2p1p0q0 q1q2q3q4q5q6
-    vldx             vr7,    t5,     a1
-    add.d            t5,     t5,     a1
-    vldx             vr8,    t5,     a1
-    add.d            t5,     t5,     a1
-    vldx             vr9,    t5,     a1
-
-    vilvl.b          vr10,   vr7,    vr6
-    vilvh.b          vr11,   vr7,    vr6
-    vilvl.b          vr12,   vr9,    vr8
-    vilvh.b          vr13,   vr9,    vr8
-    vilvl.h          vr6,    vr12,   vr10
-    vilvh.h          vr10,   vr12,   vr10 //p2---
-    vilvl.h          vr15,   vr13,   vr11 //q1---
-    vilvh.h          vr19,   vr13,   vr11
-
-    vbsrl.v          vr7,    vr6,    4    //p5---
-    vbsrl.v          vr8,    vr6,    8    //p4---
-    vbsrl.v          vr9,    vr6,    12   //p3---
-    vbsrl.v          vr12,   vr10,   4    //p1---
-    vbsrl.v          vr13,   vr10,   8    //p0---
-    vbsrl.v          vr14,   vr10,   12   //q0---
-    vbsrl.v          vr16,   vr15,   4    //q2---
-    vbsrl.v          vr17,   vr15,   8    //q3---
-    vbsrl.v          vr18,   vr15,   12   //q4---
-    vbsrl.v          vr20,   vr19,   4    //q6---
-.else
-    slli.d           t5,     a1,     3
-    sub.d            t5,     a0,     t5
-    fldx.s           f6,     t5,     a1  //p6
-    alsl.d           t5,     a1,     t5,    1
-    fld.s            f7,     t5,     0   //p5
-    fldx.s           f8,     t5,     a1  //p4
-    alsl.d           t5,     a1,     t5,    1
-    fld.s            f9,     t5,     0   //p3
-    fldx.s           f10,    t5,     a1  //p2
-    alsl.d           t5,     a1,     t5,    1
-    fld.s            f12,    t5,     0   //p1
-    fldx.s           f13,    t5,     a1  //p0
-    alsl.d           t5,     a1,     t5,    1
-    fld.s            f14,    t5,     0   //q0
-    fldx.s           f15,    t5,     a1  //q1
-    alsl.d           t5,     a1,     t5,    1
-    fld.s            f16,    t5,     0   //q2
-    fldx.s           f17,    t5,     a1  //q3
-    alsl.d           t5,     a1,     t5,    1
-    fld.s            f18,    t5,     0   //q4
-    fldx.s           f19,    t5,     a1  //q5
-    add.d            t5,     t5,     a1
-    fldx.s           f20,    t5,     a1  //q6
-
-    //temp store
-    addi.d           sp,     sp,    -96
-    fst.d            f7,     sp,     0
-    fst.d            f8,     sp,     8
-    fst.d            f9,     sp,     16
-    fst.d            f10,    sp,     24
-    fst.d            f12,    sp,     32
-    fst.d            f13,    sp,     40
-    fst.d            f14,    sp,     48
-    fst.d            f15,    sp,     56
-    fst.d            f16,    sp,     64
-    fst.d            f17,    sp,     72
-    fst.d            f18,    sp,     80
-    fst.d            f19,    sp,     88
-.endif
+.macro LPF_16_WD6
+    move             t7,    ra
+    bl               lpf_16_wd6_lsx
+    move             ra,    t7
+    beqz             t4,    1f
+    jirl             zero,  ra,    0x00
+1:
+.endm
 
-    vabsd.bu         vr21,   vr12,   vr13 //abs(p1-p0)
-    vabsd.bu         vr22,   vr15,   vr14 //abs(q1-q0)
-    vmax.bu          vr0,    vr21,   vr22
-    vslt.bu          vr2,    vr2,    vr0  //hev
-    vabsd.bu         vr1,    vr10,   vr12 //abs(p2-p1)
-    vmax.bu          vr0,    vr0,    vr1
-    vabsd.bu         vr1,    vr16,   vr15 //abs(q2-q1)
-    vmax.bu          vr0,    vr0,    vr1
-    vabsd.bu         vr1,    vr9,    vr10 //abs(p3-p2)
-    vmax.bu          vr0,    vr0,    vr1
-    vabsd.bu         vr1,    vr17,   vr16 //abs(q3-q2)
-    vmax.bu          vr0,    vr0,    vr1
-    vsle.bu          vr0,    vr0,    vr4  //vr4 released I
-    vabsd.bu         vr1,    vr13,   vr14 //abs(p0-q0)
-    vsadd.bu         vr1,    vr1,    vr1
-    vabsd.bu         vr4,    vr12,   vr15 //abs(p1-q1)
-    vsrli.b          vr4,    vr4,    1
-    vsadd.bu         vr1,    vr1,    vr4  //abs(p0 - q0) * 2 + (abs(p1 - q1) >> 1)
-    vsle.bu          vr1,    vr1,    vr3  //vr3 released E
-    vand.v           vr0,    vr0,    vr1  //fm
-
-    vpickve2gr.wu    t5,     vr0,    0
-    beqz             t5,     .END_FILTER_\DIR\()\TYPE\()_W16
-
-    vabsd.bu         vr1,    vr6,    vr13 //abs(p6-p0)
-    vabsd.bu         vr4,    vr7,    vr13 //abs(p5-p0)
-    vmax.bu          vr1,    vr1,    vr4
-    vabsd.bu         vr4,    vr8,    vr13 //abs(p4-p0)
-    vmax.bu          vr1,    vr1,    vr4
-    vabsd.bu         vr4,    vr18,   vr14 //abs(q4-q0)
-    vmax.bu          vr1,    vr1,    vr4
-    vabsd.bu         vr4,    vr19,   vr14 //abs(q5-q0)
-    vmax.bu          vr1,    vr1,    vr4
-    vabsd.bu         vr4,    vr20,   vr14
-    vmax.bu          vr1,    vr1,    vr4
-    vxor.v           vr5,    vr5,    vr5
-    vaddi.bu         vr5,    vr5,    1    //F
-    vsle.bu          vr1,    vr1,    vr5  //flat8out
-
-    vabsd.bu         vr3,    vr10,   vr13 //abs(p2-p0)
-    vmax.bu          vr3,    vr3,    vr21
-    vmax.bu          vr3,    vr3,    vr22
-    vabsd.bu         vr4,    vr16,   vr14 //abs(q2-q0)
-    vmax.bu          vr3,    vr3,    vr4
-    vabsd.bu         vr4,    vr9,    vr13 //abs(p3-p0)
-    vmax.bu          vr3,    vr3,    vr4
-    vabsd.bu         vr4,    vr17,   vr14 //abs(q3-q0)
-    vmax.bu          vr3,    vr3,    vr4
-    vsle.bu          vr3,    vr3,    vr5  //flatin released vr5
-
-    vsllwil.hu.bu    vr6,    vr6,    0    //p6
-    vsllwil.hu.bu    vr7,    vr7,    0    //p5
-    vsllwil.hu.bu    vr8,    vr8,    0    //p4
-    vsllwil.hu.bu    vr9,    vr9,    0    //p3
-    vsllwil.hu.bu    vr10,   vr10,   0    //p2
-    vsllwil.hu.bu    vr12,   vr12,   0    //p1
-    vsllwil.hu.bu    vr13,   vr13,   0    //p0
-    vsllwil.hu.bu    vr14,   vr14,   0    //q0
-    vsllwil.hu.bu    vr15,   vr15,   0    //q1
-    vsllwil.hu.bu    vr16,   vr16,   0    //q2
-    vsllwil.hu.bu    vr17,   vr17,   0    //q3
-    vsllwil.hu.bu    vr18,   vr18,   0    //q4
-    vsllwil.hu.bu    vr19,   vr19,   0    //q5
-    vsllwil.hu.bu    vr20,   vr20,   0    //q6
-
-    //dst-6
-    vslli.w          vr21,   vr6,    3
-    vssub.hu         vr21,   vr21,   vr6
-    vsadd.hu         vr21,   vr21,   vr7
-    vsadd.hu         vr21,   vr21,   vr7
-    vsadd.hu         vr21,   vr21,   vr8
-    vsadd.hu         vr21,   vr21,   vr8
-    vsadd.hu         vr21,   vr21,   vr9
-    vsadd.hu         vr21,   vr21,   vr10
-    vsadd.hu         vr21,   vr21,   vr12
-    vsadd.hu         vr21,   vr21,   vr13
-    vsadd.hu         vr21,   vr21,   vr14
-
-    //dst-5
-    vsadd.hu         vr22,   vr21,   vr15
-    vsadd.hu         vr22,   vr22,   vr9
-    vssub.hu         vr22,   vr22,   vr6
-    vssub.hu         vr22,   vr22,   vr6
-
-    //dst-4
-    vsadd.hu         vr23,   vr22,   vr16
-    vsadd.hu         vr23,   vr23,   vr10
-    vssub.hu         vr23,   vr23,   vr7
-    vssub.hu         vr23,   vr23,   vr6
-
-    //dst-3
-    vsadd.hu         vr24,   vr23,   vr12
-    vsadd.hu         vr24,   vr24,   vr17
-    vssub.hu         vr24,   vr24,   vr6
-    vssub.hu         vr24,   vr24,   vr8
-
-    //dst-2
-    vsadd.hu         vr25,   vr24,   vr18
-    vsadd.hu         vr25,   vr25,   vr13
-    vssub.hu         vr25,   vr25,   vr6
-    vssub.hu         vr25,   vr25,   vr9
-
-    //dst-1
-    vsadd.hu         vr26,   vr25,   vr19
-    vsadd.hu         vr26,   vr26,   vr14
-    vssub.hu         vr26,   vr26,   vr6
-    vssub.hu         vr26,   vr26,   vr10
-
-    //dst+0
-    vsadd.hu         vr27,   vr26,   vr20
-    vsadd.hu         vr27,   vr27,   vr15
-    vssub.hu         vr27,   vr27,   vr6
-    vssub.hu         vr27,   vr27,   vr12
-
-    //dst+1
-    vsadd.hu         vr28,   vr27,   vr20
-    vsadd.hu         vr28,   vr28,   vr16
-    vssub.hu         vr28,   vr28,   vr7
-    vssub.hu         vr28,   vr28,   vr13
-
-    //dst+2
-    vsadd.hu         vr29,   vr28,   vr20
-    vsadd.hu         vr29,   vr29,   vr17
-    vssub.hu         vr29,   vr29,   vr8
-    vssub.hu         vr29,   vr29,   vr14
-
-    //dst+3
-    vsadd.hu         vr30,   vr29,   vr20
-    vsadd.hu         vr30,   vr30,   vr18
-    vssub.hu         vr30,   vr30,   vr9
-    vssub.hu         vr30,   vr30,   vr15
-
-    //dst+4
-    vsadd.hu         vr31,   vr30,   vr20
-    vsadd.hu         vr31,   vr31,   vr19
-    vssub.hu         vr31,   vr31,   vr10
-    vssub.hu         vr31,   vr31,   vr16
-
-    //dst+5
-    vsadd.hu         vr11,   vr31,   vr20
-    vsadd.hu         vr11,   vr11,   vr20
-    vssub.hu         vr11,   vr11,   vr12
-    vssub.hu         vr11,   vr11,   vr17
-
-    vsrari.h         vr21,   vr21,   4
-    vsrari.h         vr22,   vr22,   4
-    vsrari.h         vr23,   vr23,   4
-    vsrari.h         vr24,   vr24,   4
-    vsrari.h         vr25,   vr25,   4
-    vsrari.h         vr26,   vr26,   4
-    vsrari.h         vr27,   vr27,   4
-    vsrari.h         vr28,   vr28,   4
-    vsrari.h         vr29,   vr29,   4
-    vsrari.h         vr30,   vr30,   4
-    vsrari.h         vr31,   vr31,   4
-    vsrari.h         vr11,   vr11,   4
-
-    vand.v           vr1,    vr1,    vr3
-    vsllwil.h.b      vr1,    vr1,    0 //expand to h
-    //(flat8out & flat8in)
-    vbitsel.v        vr21,   vr7,    vr21,    vr1  //dst-6
-    vbitsel.v        vr22,   vr8,    vr22,    vr1  //dst-5
-    vbitsel.v        vr23,   vr9,    vr23,    vr1  //dst-4
-    vbitsel.v        vr30,   vr17,   vr30,    vr1  //dst+3
-    vbitsel.v        vr31,   vr18,   vr31,    vr1  //dst+4
-    vbitsel.v        vr11,   vr19,   vr11,    vr1  //dst+5
-
-    //flat8in
-    //dst-3
-    vslli.h          vr4,    vr9,    1
-    vsadd.hu         vr4,    vr4,    vr9 //p3*3
-    vsadd.hu         vr4,    vr4,    vr10
-    vsadd.hu         vr4,    vr4,    vr10
-    vsadd.hu         vr4,    vr4,    vr12
-    vsadd.hu         vr4,    vr4,    vr13
-    vsadd.hu         vr4,    vr4,    vr14
-
-    //dst-2
-    vsadd.hu         vr5,    vr4,    vr12
-    vsadd.hu         vr5,    vr5,    vr15
-    vssub.hu         vr5,    vr5,    vr9
-    vssub.hu         vr5,    vr5,    vr10
-
-    //dst-1
-    vsadd.hu         vr18,   vr5,    vr13
-    vsadd.hu         vr18,   vr18,   vr16
-    vssub.hu         vr18,   vr18,   vr9
-    vssub.hu         vr18,   vr18,   vr12
-
-    //dst+0
-    vsadd.hu         vr7,    vr18,   vr14
-    vsadd.hu         vr7,    vr7,    vr17
-    vssub.hu         vr7,    vr7,    vr9
-    vssub.hu         vr7,    vr7,    vr13
-
-    //dst+1
-    vsadd.hu         vr8,    vr7,    vr15
-    vsadd.hu         vr8,    vr8,    vr17
-    vssub.hu         vr8,    vr8,    vr10
-    vssub.hu         vr8,    vr8,    vr14
-
-    //dst+2
-    vsadd.hu         vr9,    vr8,    vr16
-    vsadd.hu         vr9,    vr9,    vr17
-    vssub.hu         vr9,    vr9,    vr12
-    vssub.hu         vr9,    vr9,    vr15
-
-    vsrari.h         vr4,    vr4,    3
-    vsrari.h         vr5,    vr5,    3
-    vsrari.h         vr18,   vr18,   3
-    vsrari.h         vr7,    vr7,    3
-    vsrari.h         vr8,    vr8,    3
-    vsrari.h         vr9,    vr9,    3
-
-    //flat8out & flat8in
-    vbitsel.v        vr24,   vr4,    vr24,    vr1 //dst-3
-    vbitsel.v        vr25,   vr5,    vr25,    vr1 //dst-2
-    vbitsel.v        vr26,   vr18,   vr26,    vr1 //dst-1
-    vbitsel.v        vr27,   vr7,    vr27,    vr1 //dst+0
-    vbitsel.v        vr28,   vr8,    vr28,    vr1 //dst+1
-    vbitsel.v        vr29,   vr9,    vr29,    vr1 //dst+2
-
-    //!flat8in
-    vsub.h           vr17,   vr12,   vr15 //p1-q1
-    vsllwil.h.b      vr2,    vr2,    0
-    vand.v           vr17,   vr17,   vr2  //&hev
-    vssrani.b.h      vr17,   vr17,   0
-    vsllwil.h.b      vr17,   vr17,   0
-
-    vsub.h           vr7,    vr14,   vr13
-    vsadd.h          vr8,    vr7,    vr7
-    vsadd.h          vr7,    vr7,    vr8
-    vsadd.h          vr7,    vr7,    vr17
-    vssrani.b.h      vr7,    vr7,    0
-    vsllwil.h.b      vr17,   vr7,    0  //f = iclip_diff(3 * (q0 - p0) + f);
-
-    vaddi.hu         vr7,    vr17,   4
-    vaddi.hu         vr8,    vr17,   3
-    li.w             t5,     127
-    vreplgr2vr.h     vr9,    t5
-    vmin.h           vr7,    vr7,    vr9
-    vmin.h           vr8,    vr8,    vr9
-    vsrai.h          vr7,    vr7,    3  //f1
-    vsrai.h          vr8,    vr8,    3  //f2
-
-    vsadd.h          vr4,    vr13,   vr8  //dst-1
-    vssub.h          vr5,    vr14,   vr7  //dst+0
-
-    vsrari.h         vr7,    vr7,    1
-    vsadd.h          vr17,   vr12,   vr7
-    vssub.h          vr7,    vr15,   vr7
-    vbitsel.v        vr17,   vr17,   vr12,    vr2  //dst-2
-    vbitsel.v        vr7,    vr7,    vr15,    vr2  //dst+1
-
-    //flat8in or !flat8in
-    vsllwil.h.b      vr3,    vr3,    0
-    vbitsel.v        vr24,   vr10,   vr24,    vr3  //dst-3
-    vbitsel.v        vr25,   vr17,   vr25,    vr3  //dst-2
-    vbitsel.v        vr26,   vr4,    vr26,    vr3  //dst-1
-    vbitsel.v        vr27,   vr5,    vr27,    vr3  //dst+0
-    vbitsel.v        vr28,   vr7,    vr28,    vr3  //dst+1
-    vbitsel.v        vr29,   vr16,   vr29,    vr3  //dst+2
+.macro LPF_16_WD4
+    move             t7,    ra
+    bl               lpf_16_wd4_lsx
+    move             ra,    t7
+    beqz             t4,    1f
+    jirl             zero,  ra,    0x00
+1:
+.endm
 
-.ifc \DIR, h
-    //dst-6,dst-2,dst-5,dst-1
-    vssrani.bu.h     vr25,   vr21,   0
-    vssrani.bu.h     vr26,   vr22,   0
-    vpermi.w         vr25,   vr25,   0xd8
-    vpermi.w         vr26,   vr26,   0xd8
-    vilvl.b          vr6,    vr26,   vr25 //65656565 21212121
-
-    //dst-4,dst+0,dst-3,dst+1
-    vssrani.bu.h     vr27,   vr23,   0
-    vssrani.bu.h     vr28,   vr24,   0
-    vpermi.w         vr27,   vr27,   0xd8
-    vpermi.w         vr28,   vr28,   0xd8
-    vilvl.b          vr26,   vr28,   vr27 //43434343 01010101
-
-    vilvl.h          vr21,   vr26,   vr6  //6543 -- -- --
-    vilvh.h          vr22,   vr26,   vr6  //2101 -- -- --
-    vilvl.w          vr20,   vr22,   vr21 //65432101 --
-    vilvh.w          vr22,   vr22,   vr21 //65432101 --
-    vreplvei.d       vr21,   vr20,   1
-    vreplvei.d       vr23,   vr22,   1
-
-    //dst+2,dst+4,dst+3,dst+5
-    vssrani.bu.h     vr31,   vr29,   0
-    vssrani.bu.h     vr11,   vr30,   0
-    vpermi.w         vr31,   vr31,   0xd8
-    vpermi.w         vr11,   vr11,   0xd8
-    vilvl.b          vr11,   vr11,   vr31 //23232323 45454545
-    vshuf4i.w        vr11,   vr11,   0xd8
-    vshuf4i.h        vr11,   vr11,   0xd8 //2345 -- -- --
-
-    vextrins.w       vr20,   vr11,   0x20
-    vextrins.w       vr21,   vr11,   0x21
-    vextrins.w       vr22,   vr11,   0x22
-    vextrins.w       vr23,   vr11,   0x23
-
-    addi.d           t5,     a0,     -6
-    vld              vr6,    t5,     0  //p6p5p4p3p2p1p0q0 q1q2q3q4q5q6
-    vldx             vr7,    t5,     a1
-    add.d            t5,     t5,     a1
-    vldx             vr8,    t5,     a1
-    add.d            t5,     t5,     a1
-    vldx             vr9,    t5,     a1
-
-    //expand fm to 128
-    vreplvei.b       vr10,   vr0,    0
-    vreplvei.b       vr11,   vr0,    1
-    vreplvei.b       vr12,   vr0,    2
-    vreplvei.b       vr13,   vr0,    3
-
-    vbitsel.v        vr20,   vr6,    vr20,    vr10
-    vbitsel.v        vr21,   vr7,    vr21,    vr11
-    vbitsel.v        vr22,   vr8,    vr22,    vr12
-    vbitsel.v        vr23,   vr9,    vr23,    vr13
-
-    addi.d           t5,     a0,    -6
-    vstelm.d         vr20,   t5,     0,       0
-    vstelm.w         vr20,   t5,     8,       2
-    add.d            t5,     t5,     a1
-    vstelm.d         vr21,   t5,     0,       0
-    vstelm.w         vr21,   t5,     8,       2
-    add.d            t5,     t5,     a1
-    vstelm.d         vr22,   t5,     0,       0
-    vstelm.w         vr22,   t5,     8,       2
-    add.d            t5,     t5,     a1
-    vstelm.d         vr23,   t5,     0,       0
-    vstelm.w         vr23,   t5,     8,       2
-.else
-    //reload
-    fld.d            f7,     sp,     0
-    fld.d            f8,     sp,     8
-    fld.d            f9,     sp,     16
-    fld.d            f10,    sp,     24
-    fld.d            f12,    sp,     32
-    fld.d            f13,    sp,     40
-    fld.d            f14,    sp,     48
-    fld.d            f15,    sp,     56
-    fld.d            f16,    sp,     64
-    fld.d            f17,    sp,     72
-    fld.d            f18,    sp,     80
-    fld.d            f19,    sp,     88
-
-    vssrarni.bu.h    vr21,   vr21,   0
-    vssrarni.bu.h    vr22,   vr22,   0
-    vssrarni.bu.h    vr23,   vr23,   0
-    vssrarni.bu.h    vr24,   vr24,   0
-    vssrarni.bu.h    vr25,   vr25,   0
-    vssrarni.bu.h    vr26,   vr26,   0
-    vssrarni.bu.h    vr27,   vr27,   0
-    vssrarni.bu.h    vr28,   vr28,   0
-    vssrarni.bu.h    vr29,   vr29,   0
-    vssrarni.bu.h    vr30,   vr30,   0
-    vssrarni.bu.h    vr31,   vr31,   0
-    vssrarni.bu.h    vr11,   vr11,   0
-
-    vbitsel.v        vr7,    vr7,    vr21,   vr0 //p5
-    vbitsel.v        vr8,    vr8,    vr22,   vr0 //p4
-    vbitsel.v        vr9,    vr9,    vr23,   vr0 //p3
-    vbitsel.v        vr10,   vr10,   vr24,   vr0 //p2
-    vbitsel.v        vr12,   vr12,   vr25,   vr0 //p1
-    vbitsel.v        vr13,   vr13,   vr26,   vr0 //p0
-    vbitsel.v        vr14,   vr14,   vr27,   vr0 //q0
-    vbitsel.v        vr15,   vr15,   vr28,   vr0 //q1
-    vbitsel.v        vr16,   vr16,   vr29,   vr0 //q2
-    vbitsel.v        vr17,   vr17,   vr30,   vr0 //q3
-    vbitsel.v        vr18,   vr18,   vr31,   vr0 //q4
-    vbitsel.v        vr19,   vr19,   vr11,   vr0 //q5
-
-    fst.s            f14,    a0,     0
-    fstx.s           f15,    a0,     a1
-    alsl.d           t5,     a1,     a0,     1
-    fst.s            f16,    t5,     0
-    fstx.s           f17,    t5,     a1
-    alsl.d           t5,     a1,     t5,     1
-    fst.s            f18,    t5,     0
-    fstx.s           f19,    t5,     a1
-
-    slli.w           t5,     a1,     2
-    alsl.d           t5,     a1,     t5,     1
-    sub.d            t5,     a0,     t5
-    fst.s            f7,     t5,     0
-    fstx.s           f8,     t5,     a1
-    alsl.d           t5,     a1,     t5,     1
-    fst.s            f9,     t5,     0
-    fstx.s           f10,    t5,     a1
-    alsl.d           t5,     a1,     t5,     1
-    fst.s            f12,    t5,     0
-    fstx.s           f13,    t5,     a1
-.endif
-.END_FILTER_\DIR\()\TYPE\()_W16:
-.ifc \DIR, v
-    addi.d           sp,     sp,     96
-.endif
+functionl lpf_v_4_16_lsx
+    slli.d           t3,    a1,    1
+    sub.d            t3,    a0,    t3
+    vld              vr22,  t3,    0   // p1
+    vldx             vr23,  t3,    a1  // p0
+    vld              vr24,  a0,    0   // q0
+    vldx             vr25,  a0,    a1  // q1
+
+    LPF_16_WD4
+
+    vst              vr22,  t3,    0   // p1
+    vstx             vr23,  t3,    a1  // p0
+    vst              vr24,  a0,    0   // q0
+    vstx             vr25,  a0,    a1  // q1
+endfuncl
+
+functionl lpf_h_4_16_lsx
+    addi.d           t3,    a0,   -2
+    fld.s            f22,   t3,    0
+    fldx.s           f23,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.s            f24,   t3,    0
+    fldx.s           f25,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.s            f17,   t3,    0
+    fldx.s           f18,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.s            f19,   t3,    0
+    fldx.s           f20,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    vilvl.w          vr22,  vr17,  vr22
+    vilvl.w          vr23,  vr18,  vr23
+    vilvl.w          vr24,  vr19,  vr24
+    vilvl.w          vr25,  vr20,  vr25
+    fld.s            f17,   t3,    0
+    fldx.s           f18,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.s            f19,   t3,    0
+    fldx.s           f20,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.s            f26,   t3,    0
+    fldx.s           f27,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.s            f28,   t3,    0
+    fldx.s           f29,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    vilvl.w          vr17,  vr26,  vr17
+    vilvl.w          vr18,  vr27,  vr18
+    vilvl.w          vr19,  vr28,  vr19
+    vilvl.w          vr20,  vr29,  vr20
+    vilvl.d          vr22,  vr17,  vr22
+    vilvl.d          vr23,  vr18,  vr23
+    vilvl.d          vr24,  vr19,  vr24
+    vilvl.d          vr25,  vr20,  vr25
+    addi.d           a0,    t3,    2
+
+    TRANSPOSE_4x16B  vr22, vr23, vr24, vr25, vr26, vr27, vr28, vr29
+
+    LPF_16_WD4
+
+    slli.d           t3,    a1,    4
+    sub.d            a0,    a0,    t3
+
+    TRANSPOSE_4x16B  vr22, vr23, vr24, vr25, vr26, vr27, vr28, vr29
+
+    addi.d           a0,    a0,   -2
+.irp i, vr22, vr23, vr24, vr25
+    vstelm.w         \i,    a0,    0,    0
+    add.d            a0,    a0,    a1
+.endr
+.irp i, vr22, vr23, vr24, vr25
+    vstelm.w         \i,    a0,    0,    1
+    add.d            a0,    a0,    a1
+.endr
+.irp i, vr22, vr23, vr24, vr25
+    vstelm.w         \i,    a0,    0,    2
+    add.d            a0,    a0,    a1
+.endr
+.irp i, vr22, vr23, vr24, vr25
+    vstelm.w         \i,    a0,    0,    3
+    add.d            a0,    a0,    a1
+.endr
+    addi.d           a0,    a0,    2
+endfuncl
+
+functionl lpf_v_6_16_lsx
+    slli.d           t3,    a1,    1
+    sub.d            t3,    a0,    t3
+    sub.d            s0,    t3,    a1
+    vld              vr21,  s0,    0   // p2
+    vldx             vr22,  s0,    a1  // p1
+    alsl.d           s0,    a1,    s0,   1
+    vld              vr23,  s0,    0   // p0
+    vldx             vr24,  s0,    a1  // q0
+    alsl.d           s0,    a1,    s0,   1
+    vld              vr25,  s0,    0   // q1
+    vldx             vr26,  s0,    a1  // q2
+
+    LPF_16_WD6
+
+    vst              vr22,  t3,    0   // p1
+    vstx             vr23,  t3,    a1  // p0
+    vst              vr24,  a0,    0   // q0
+    vstx             vr25,  a0,    a1  // q1
+endfuncl
+
+functionl lpf_h_6_16_lsx
+    addi.d           t3,    a0,   -4
+    fld.d            f20,   t3,    0
+    fldx.d           f21,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.d            f22,   t3,    0
+    fldx.d           f23,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.d            f24,   t3,    0
+    fldx.d           f25,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.d            f26,   t3,    0
+    fldx.d           f27,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.d            f16,   t3,    0
+    fldx.d           f17,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.d            f18,   t3,    0
+    fldx.d           f19,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.d            f28,   t3,    0
+    fldx.d           f29,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.d            f30,   t3,    0
+    fldx.d           f31,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+
+    vilvl.d          vr20,  vr16,  vr20
+    vilvl.d          vr21,  vr17,  vr21
+    vilvl.d          vr22,  vr18,  vr22
+    vilvl.d          vr23,  vr19,  vr23
+    vilvl.d          vr24,  vr28,  vr24
+    vilvl.d          vr25,  vr29,  vr25
+    vilvl.d          vr26,  vr30,  vr26
+    vilvl.d          vr27,  vr31,  vr27
+    addi.d           a0,    t3,    4
+
+    TRANSPOSE_8x16B vr20, vr21, vr22, vr23, vr24, vr25, vr26, vr27, vr28, vr29
+
+    LPF_16_WD6
+
+    slli.d           t3,    a1,    4
+    sub.d            a0,    a0,    t3
+
+    TRANSPOSE_4x16b vr22, vr23, vr24, vr25, vr26, vr27, vr28, vr29
+
+    addi.d           a0,    a0,   -2
+.irp i, vr22, vr23, vr24, vr25
+    vstelm.w         \i,    a0,    0,    0
+    add.d            a0,    a0,    a1
+.endr
+.irp i, vr22, vr23, vr24, vr25
+    vstelm.w         \i,    a0,    0,    1
+    add.d            a0,    a0,    a1
+.endr
+.irp i, vr22, vr23, vr24, vr25
+    vstelm.w         \i,    a0,    0,    2
+    add.d            a0,    a0,    a1
+.endr
+.irp i, vr22, vr23, vr24, vr25
+    vstelm.w         \i,    a0,    0,    3
+    add.d            a0,    a0,    a1
+.endr
+    addi.d           a0,    a0,    2
+endfuncl
+
+functionl lpf_v_8_16_lsx
+    slli.d           t3,    a1,    2
+    sub.d            s0,    a0,    t3
+    vld              vr20,  s0,    0   // p3
+    vldx             vr21,  s0,    a1  // p2
+    alsl.d           s0,    a1,    s0,   1
+    vld              vr22,  s0,    0   // p1
+    vldx             vr23,  s0,    a1  // p0
+    alsl.d           s0,    a1,    s0,   1
+    vld              vr24,  s0,    0   // q0
+    vldx             vr25,  s0,    a1  // q1
+    alsl.d           s0,    a1,    s0,   1
+    vld              vr26,  s0,    0   // q2
+    vldx             vr27,  s0,    a1  // q3
+
+    LPF_16_WD8
+
+    sub.d            t3,    a0,    t3
+    add.d            t3,    t3,    a1  // -3
+    vst              vr21,  t3,    0   // p2
+    vstx             vr22,  t3,    a1  // p1
+    alsl.d           t3,    a1,    t3,   1
+    vst              vr23,  t3,    0   // p0
+    vstx             vr24,  t3,    a1  // q0
+    alsl.d           t3,    a1,    t3,   1
+    vst              vr25,  t3,    0   // q1
+    vstx             vr26,  t3,    a1  // q2
+    jirl             zero,  ra,    0x00
+8:
+    slli.d           t3,    a1,    1
+    sub.d            t3,    a0,    t3
+    vst              vr22,  t3,    0   // p1
+    vstx             vr23,  t3,    a1  // p0
+    alsl.d           t3,    a1,    t3,   1
+    vst              vr24,  t3,    0   // q0
+    vstx             vr25,  t3,    a1  // q1
+endfuncl
+
+functionl lpf_h_8_16_lsx
+    addi.d           t3,    a0,   -4
+    fld.d            f20,   t3,    0
+    fldx.d           f21,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.d            f22,   t3,    0
+    fldx.d           f23,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.d            f24,   t3,    0
+    fldx.d           f25,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.d            f26,   t3,    0
+    fldx.d           f27,   t3,    a1
+
+    alsl.d           t3,    a1,    t3,   1
+    fld.d            f16,   t3,    0
+    fldx.d           f17,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.d            f18,   t3,    0
+    fldx.d           f19,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.d            f28,   t3,    0
+    fldx.d           f29,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    fld.d            f30,   t3,    0
+    fldx.d           f31,   t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+
+    vilvl.d          vr20,  vr16,  vr20
+    vilvl.d          vr21,  vr17,  vr21
+    vilvl.d          vr22,  vr18,  vr22
+    vilvl.d          vr23,  vr19,  vr23
+    vilvl.d          vr24,  vr28,  vr24
+    vilvl.d          vr25,  vr29,  vr25
+    vilvl.d          vr26,  vr30,  vr26
+    vilvl.d          vr27,  vr31,  vr27
+    addi.d           a0,    t3,    4
+
+    TRANSPOSE_8x16B vr20, vr21, vr22, vr23, vr24, vr25, vr26, vr27, vr28, vr29
+
+    LPF_16_WD8
+
+    slli.d           t3,    a1,    4
+    sub.d            a0,    a0,    t3
+
+    TRANSPOSE_8x16b vr20, vr21, vr22, vr23, vr24, vr25, vr26, vr27, vr28, vr29
+
+    addi.d           a0,    a0,   -4
+.irp i, vr20, vr21, vr22, vr23, vr24, vr25, vr26, vr27
+    vstelm.d         \i,    a0,    0,    0
+    add.d            a0,    a0,    a1
+.endr
+.irp i, vr20, vr21, vr22, vr23, vr24, vr25, vr26, vr27
+    vstelm.d         \i,    a0,    0,    1
+    add.d            a0,    a0,    a1
+.endr
+    addi.d           a0,    a0,    4
+    jirl             zero,  ra,    0x00
+
+8:
+    slli.d           t3,    a1,    4
+    sub.d            a0,    a0,    t3
+
+    TRANSPOSE_4x16B vr22, vr23, vr24, vr25, vr26, vr27, vr28, vr29
+
+    addi.d           a0,    a0,   -2
+.irp i, vr22, vr23, vr24, vr25
+    vstelm.w         \i,    a0,    0,    0
+    add.d            a0,    a0,    a1
+.endr
+.irp i, vr22, vr23, vr24, vr25
+    vstelm.w         \i,    a0,    0,    1
+    add.d            a0,    a0,    a1
+.endr
+.irp i, vr22, vr23, vr24, vr25
+    vstelm.w         \i,    a0,    0,    2
+    add.d            a0,    a0,    a1
+.endr
+.irp i, vr22, vr23, vr24, vr25
+    vstelm.w         \i,    a0,    0,    3
+    add.d            a0,    a0,    a1
+.endr
+    addi.d           a0,    a0,    2
+endfuncl
+
+functionl lpf_v_16_16_lsx
+    slli.d           t3,    a1,    3
+    sub.d            s0,    a0,    t3
+    add.d            s0,    s0,    a1
+    vld              vr17,  s0,    0        // p6
+    vldx             vr18,  s0,    a1       // p5
+    alsl.d           s0,    a1,    s0,   1
+    vld              vr19,  s0,    0        // p4
+    vldx             vr20,  s0,    a1       // p3
+    alsl.d           s0,    a1,    s0,   1
+    vld              vr21,  s0,    0        // p2
+    vldx             vr22,  s0,    a1       // p1
+    alsl.d           s0,    a1,    s0,   1
+    vld              vr23,  s0,    0        // p0
+    vldx             vr24,  s0,    a1       // q0
+    alsl.d           s0,    a1,    s0,   1
+    vld              vr25,  s0,    0        // q1
+    vldx             vr26,  s0,    a1       // q2
+    alsl.d           s0,    a1,    s0,   1
+    vld              vr27,  s0,    0        // q3
+    vldx             vr28,  s0,    a1       // q4
+    alsl.d           s0,    a1,    s0,   1
+    vld              vr29,  s0,    0        // q5
+    vldx             vr30,  s0,    a1       // q6
+
+    LPF_16_WD16
+
+    sub.d            s0,    a0,    t3
+    alsl.d           s0,    a1,    s0,   1
+    vst              vr0,   s0,    0        // p5
+    vstx             vr1,   s0,    a1       // p4
+    alsl.d           s0,    a1,    s0,   1
+    vst              vr2,   s0,    0        // p3
+    vstx             vr3,   s0,    a1       // p2
+    alsl.d           s0,    a1,    s0,   1
+    vst              vr4,   s0,    0        // p1
+    vstx             vr5,   s0,    a1       // p0
+    alsl.d           s0,    a1,    s0,   1
+    vst              vr6,   s0,    0        // q0
+    vstx             vr7,   s0,    a1       // q1
+    alsl.d           s0,    a1,    s0,   1
+    vst              vr8,   s0,    0        // q2
+    vstx             vr9,   s0,    a1       // q3
+    alsl.d           s0,    a1,    s0,   1
+    vst              vr10,  s0,    0        // q4
+    vstx             vr11,  s0,    a1       // q5
+    jirl             zero,  ra,    0x00
+7:
+    slli.d           t3,    a1,    1
+    add.d            t3,    t3,    a1
+    sub.d            s0,    a0,    t3
+    vst              vr21,  s0,    0        // p2
+    vstx             vr22,  s0,    a1       // p1
+    alsl.d           s0,    a1,    s0,   1
+    vst              vr23,  s0,    0        // p0
+    vstx             vr24,  s0,    a1       // q0
+    alsl.d           s0,    a1,    s0,   1
+    vst              vr25,  s0,    0        // q1
+    vstx             vr26,  s0,    a1       // q2
+    jirl             zero,  ra,    0x00
+8:
+    slli.d           t3,    a1,    1
+    sub.d            s0,    a0,    t3
+    vst              vr22,  s0,    0        // p1
+    vstx             vr23,  s0,    a1       // p0
+    alsl.d           s0,    a1,    s0,   1
+    vst              vr24,  s0,    0        // q0
+    vstx             vr25,  s0,    a1       // q1
+endfuncl
+
+functionl lpf_h_16_16_lsx
+    addi.d           t3,    a0,   -8
+    vld              vr16,  t3,    0
+    vldx             vr17,  t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    vld              vr18,  t3,    0
+    vldx             vr19,  t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    vld              vr20,  t3,    0
+    vldx             vr21,  t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    vld              vr22,  t3,    0
+    vldx             vr23,  t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    vld              vr24,  t3,    0
+    vldx             vr25,  t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    vld              vr26,  t3,    0
+    vldx             vr27,  t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    vld              vr28,  t3,    0
+    vldx             vr29,  t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+    vld              vr30,  t3,    0
+    vldx             vr31,  t3,    a1
+    alsl.d           t3,    a1,    t3,   1
+.macro SWAPD in0, in1
+    vaddi.bu         vr0,   \in0,  0
+    vilvl.d          \in0,  \in1,  \in0
+    vilvh.d          \in1,  \in1,  vr0
 .endm
+    SWAPD            vr16,  vr24
+    SWAPD            vr17,  vr25
+    SWAPD            vr18,  vr26
+    SWAPD            vr19,  vr27
+    SWAPD            vr20,  vr28
+    SWAPD            vr21,  vr29
+    SWAPD            vr22,  vr30
+    SWAPD            vr23,  vr31
+    addi.d           a0,    t3,    8
+
+    TRANSPOSE_8x16B vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23, vr0, vr1
+    TRANSPOSE_8x16B vr24, vr25, vr26, vr27, vr28, vr29, vr30, vr31, vr0, vr1
+
+    LPF_16_WD16
+
+    slli.d           t3,    a1,    4
+    sub.d            a0,    a0,    t3
+
+    TRANSPOSE_8x16B vr16, vr17, vr0,  vr1,  vr2,  vr3,  vr4,  vr5,  vr18, vr19
+    TRANSPOSE_8x16B vr6,  vr7,  vr8,  vr9,  vr10, vr11, vr30, vr31, vr18, vr19
+
+    addi.d           t3,    a0,   -8
+.irp i, vr16, vr17, vr0, vr1, vr2, vr3, vr4, vr5
+    vstelm.d         \i,    t3,    0,    0
+    add.d            t3,    t3,    a1
+.endr
+.irp i, vr16, vr17, vr0, vr1, vr2, vr3, vr4, vr5
+    vstelm.d         \i,    t3,    0,    1
+    add.d            t3,    t3,    a1
+.endr
+.irp i, vr6, vr7, vr8, vr9, vr10, vr11, vr30, vr31
+    vstelm.d         \i,    a0,    0,    0
+    add.d            a0,    a0,    a1
+.endr
+.irp i, vr6, vr7, vr8, vr9, vr10, vr11, vr30, vr31
+    vstelm.d         \i,    a0,    0,    1
+    add.d            a0,    a0,    a1
+.endr
+    jirl             zero,  ra,    0x00
+
+7:
+    slli.d           t3,    a1,    4
+    sub.d            a0,    a0,    t3
+
+    TRANSPOSE_8x16B vr20, vr21, vr22, vr23, vr24, vr25, vr26, vr27, vr28, vr29
+
+    addi.d           a0,    a0,   -4
+.irp i, vr20, vr21, vr22, vr23, vr24, vr25, vr26, vr27
+    vstelm.d         \i,    a0,    0,    0
+    add.d            a0,    a0,    a1
+.endr
+.irp i, vr20, vr21, vr22, vr23, vr24, vr25, vr26, vr27
+    vstelm.d         \i,    a0,    0,    1
+    add.d            a0,    a0,    a1
+.endr
+    addi.d           a0,    a0,    4
+    jirl             zero,  ra,    0x00
+8:
+
+    slli.d           t3,    a1,    4
+    sub.d            a0,    a0,    t3
+
+    TRANSPOSE_4x16B vr22, vr23, vr24, vr25, vr26, vr27, vr28, vr29
+
+    addi.d           a0,    a0,   -2
+.irp i, 0, 1, 2, 3
+    vstelm.w         vr22,  a0,    0,    \i
+    add.d            a0,    a0,    a1
+    vstelm.w         vr23,  a0,    0,    \i
+    add.d            a0,    a0,    a1
+    vstelm.w         vr24,  a0,    0,    \i
+    add.d            a0,    a0,    a1
+    vstelm.w         vr25,  a0,    0,    \i
+    add.d            a0,    a0,    a1
+.endr
+    addi.d           a0,    a0,    2
+endfuncl
 
 .macro PUSH_REG
-    addi.d           sp,     sp,    -64
-    fst.d            f24,    sp,     0
-    fst.d            f25,    sp,     8
-    fst.d            f26,    sp,     16
-    fst.d            f27,    sp,     24
-    fst.d            f28,    sp,     32
-    fst.d            f29,    sp,     40
-    fst.d            f30,    sp,     48
-    fst.d            f31,    sp,     56
+    addi.d           sp,    sp,   -64-8
+    fst.d            f24,   sp,    0
+    fst.d            f25,   sp,    8
+    fst.d            f26,   sp,    16
+    fst.d            f27,   sp,    24
+    fst.d            f28,   sp,    32
+    fst.d            f29,   sp,    40
+    fst.d            f30,   sp,    48
+    fst.d            f31,   sp,    56
+    st.d             s0,    sp,    64
 .endm
 .macro POP_REG
-    fld.d            f24,    sp,     0
-    fld.d            f25,    sp,     8
-    fld.d            f26,    sp,     16
-    fld.d            f27,    sp,     24
-    fld.d            f28,    sp,     32
-    fld.d            f29,    sp,     40
-    fld.d            f30,    sp,     48
-    fld.d            f31,    sp,     56
-    addi.d           sp,     sp,     64
+    fld.d            f24,   sp,    0
+    fld.d            f25,   sp,    8
+    fld.d            f26,   sp,    16
+    fld.d            f27,   sp,    24
+    fld.d            f28,   sp,    32
+    fld.d            f29,   sp,    40
+    fld.d            f30,   sp,    48
+    fld.d            f31,   sp,    56
+    ld.d             s0,    sp,    64
+    addi.d           sp,    sp,    64+8
 .endm
 
+const mask_1248
+.word 1, 2, 4, 8
+endconst
+
 .macro LPF_FUNC DIR, TYPE
 function lpf_\DIR\()_sb_\TYPE\()_8bpc_lsx
     PUSH_REG
-    vld              vr0,    a2,     0 //vmask
-    vpickve2gr.wu    t0,     vr0,    0
-    vpickve2gr.wu    t1,     vr0,    1
-    vpickve2gr.wu    t2,     vr0,    2
-    li.w             t3,     1          //y
-    or               t0,     t0,     t1
+    move             t8,    ra
+    vld              vr0,   a2,    0 //vmask
+    vpickve2gr.wu    t0,    vr0,   0
+    vpickve2gr.wu    t1,    vr0,   1
 .ifc \TYPE, y
-    or               t0,     t0,     t2 //vm
+    vpickve2gr.wu    t2,    vr0,   2
 .endif
-    addi.w           t8,     t3,    -1
-    andn             t8,     t0,     t8
-    beqz             t0,     .\DIR\()\TYPE\()_END
-.\DIR\()\TYPE\()_LOOP:
-    and              t4,     t0,     t3 //vm & y
-    beqz             t4,     .\DIR\()\TYPE\()_LOOP_NEXT
-    vldrepl.b        vr1,    a3,     0 //l[0][0]
-.ifc \DIR, h
-    addi.d           t5,     a3,    -4
+    addi.d           a5,    a5,    128 // Move to sharp part of lut
+.ifc \TYPE, y
+    or               t1,    t1,    t2 // vmask[1] |= vmaks[2]
+.endif
+    slli.d           a4,    a4,    2
+.ifc \DIR, v
+    sub.d            a4,    a3,    a4
 .else
-    slli.d           t5,     a4,     2
-    sub.d            t5,     a3,     t5
+    addi.d           a3,    a3,   -4
 .endif
-    vldrepl.b        vr2,    t5,     0 //l[-1][0]
-    vseqi.b          vr3,    vr1,    0
-    vbitsel.v        vr1,    vr1,    vr2,    vr3 //L
-    vpickve2gr.b     t5,     vr1,    0
-    beqz             t5,     .\DIR\()\TYPE\()_LOOP_NEXT
-    vsrai.b          vr2,    vr1,    4 //H
-    add.d            t6,     a5,     t5
-    vldrepl.b        vr3,    t6,     0 //E
-    addi.d           t6,     t6,     64
-    vldrepl.b        vr4,    t6,     0 //I
+    or               t0,    t0,    t1 // vmaks[0] |= vmask[1]
+1:
+    andi             t3,    t0,    0x0f
+.ifc \DIR, v
+    vld              vr0,   a4,    0 // l[-b4_stride][]
+    addi.d           a4,    a4,    16
+    vld              vr1,   a3,    0 // l[0][]
+    addi.d           a3,    a3,    16
+.else
+    fld.d            f0,    a3,    0
+    fldx.d           f1,    a3,    a4
+    alsl.d           a3,    a4,    a3,   1
+    fld.d            f2,    a3,    0
+    fldx.d           f3,    a3,    a4
+    alsl.d           a3,    a4,    a3,   1
+    vilvl.w          vr1,   vr1,   vr0
+    vilvl.w          vr2,   vr3,   vr2
+    vilvl.d          vr0,   vr2,   vr1
+    vilvh.d          vr1,   vr2,   vr1
+.endif
+    beqz             t3,    7f
+    //l[0][] ? l[0][] : l[-b4_stride][]
+    vseqi.b          vr2,   vr1,    0
+    vbitsel.v        vr1,   vr1,   vr0,   vr2
+    li.w             t3,    0xff
+    vreplgr2vr.w     vr3,   t3
+    vand.v           vr1,   vr1,   vr3
+    vshuf4i.b        vr1,   vr1,   0x00 // L --       1  0  2   0
+    vseqi.w          vr2,   vr1,   0    //            0 -1  0  -1
+    vseqi.w          vr2,   vr2,   0    // L != 0 -- -1  0 -1   0
+    vhaddw.qu.du     vr3,   vr2,   vr2
+    vpickve2gr.du    t4,    vr3,   0
+    beqz             t4,    7f          // if (!L) continue
+    la.local         t3,    mask_1248   // bits x
+    vld              vr16,  t3,    0
+    vreplgr2vr.w     vr13,  t0          // vmask[0]
+    vreplgr2vr.w     vr14,  t1          // vmaks[1]
+    vand.v           vr13,  vr13,  vr16
+    vseqi.w          vr13,  vr13,  0
+    vseqi.w          vr13,  vr13,  0    // if (vmask[0] & x)
+    vand.v           vr13,  vr13,  vr2  // vmask[0] &= L != 0
+    vand.v           vr14,  vr14,  vr16
+    vseqi.w          vr14,  vr14,  0
+    vseqi.w          vr14,  vr14,  0    // if (vmask[1] & x)
+.ifc \TYPE, y
+    vreplgr2vr.w     vr15,  t2          // vmask[2]
+    vand.v           vr15,  vr15,  vr16
+    vseqi.w          vr15,  vr15,  0
+    vseqi.w          vr15,  vr15,  0    // if (vmask[2] & x)
+.endif
+    vldrepl.b        vr5,   a5,    0    // sharp[0]
+    addi.d           t5,    a5,    8
+    vldrepl.b        vr6,   t5,    0    // sharp[1]
+    vsrl.b           vr3,   vr1,   vr5  // L >> sharp[0]
+    vsrli.b          vr12,  vr1,   4    // H
+    vmin.bu          vr3,   vr3,   vr6  // imin(L >> sharp[0], sharp[1])
+    vaddi.bu         vr0,   vr1,   2    // L + 2
+    vmaxi.bu         vr11,  vr3,   1    // imax(imin(), 1) = limit = I
+    vslli.b          vr0,   vr0,   1    // 2*(L + 2)
+    vadd.b           vr10,  vr0,   vr11 // 2*(L + 2) + limit = E
 .ifc \TYPE, y
-    and              t5,     t2,     t3
-    bnez             t5,     .FILTER_\DIR\()\TYPE\()_16
+    andi             t3,    t2,    0x0f
+    beqz             t3,    2f
+    //wd16
+    bl               lpf_\DIR\()_16_16_lsx
+    b                8f
+2:
 .endif
-    and              t5,     t1,     t3
+    andi             t3,    t1,    0x0f
+    beqz             t3,    3f
 .ifc \TYPE, y
-    bnez             t5,     .FILTER_\DIR\()\TYPE\()_8
+    // wd8
+    bl               lpf_\DIR\()_8_16_lsx
 .else
-    bnez             t5,     .FILTER_\DIR\()\TYPE\()_6
+    // wd6
+    bl               lpf_\DIR\()_6_16_lsx
 .endif
-    FILTER_W4 \DIR, \TYPE
-    b                .\DIR\()\TYPE\()_LOOP_NEXT
-.ifc \TYPE, uv
-.FILTER_\DIR\()\TYPE\()_6:
-    FILTER_W6 \DIR, \TYPE
+    b                8f
+3:
+    // wd4
+    bl               lpf_\DIR\()_4_16_lsx
+.ifc \DIR, h
+    b                8f
+7:
+    // For dir h, the functions above increment a0.
+    // If the whole function is skipped, increment it here instead.
+    alsl.d           a0,    a1,    a0,    4
+.else
+7:
 .endif
+8:
+    srli.d           t0,    t0,    4
+    srli.d           t1,    t1,    4
 .ifc \TYPE, y
-.FILTER_\DIR\()\TYPE\()_8:
-    FILTER_W8 \DIR, \TYPE
-    b                .\DIR\()\TYPE\()_LOOP_NEXT
-.FILTER_\DIR\()\TYPE\()_16:
-    FILTER_W16 \DIR, \TYPE
+    srli.d           t2,    t2,    4
 .endif
-.\DIR\()\TYPE\()_LOOP_NEXT:
-    slli.w           t3,     t3,     1
-.ifc \DIR, h
-    alsl.d           a0,     a1,     a0,    2
-    slli.w           t8,     a4,     2
-    add.d            a3,     a3,     t8
+.ifc \DIR, v
+    addi.d           a0,    a0,    16
 .else
-    addi.d           a0,     a0,     4
-    addi.d           a3,     a3,     4
+    // For dir h, a0 is returned incremented
 .endif
-    addi.w           t8,     t3,    -1
-    andn             t8,     t0,     t8
-    bnez             t8,     .\DIR\()\TYPE\()_LOOP
-.\DIR\()\TYPE\()_END:
+    bnez             t0,    1b
+    move             ra,    t8
     POP_REG
 endfunc
 .endm
diff --git a/src/loongarch/looprestoration.S b/src/loongarch/looprestoration.S
index ab512d1..559e2be 100644
--- a/src/loongarch/looprestoration.S
+++ b/src/loongarch/looprestoration.S
@@ -92,13 +92,11 @@ function wiener_filter_h_8bpc_lsx
 
     vsllwil.hu.bu vr15,     vr8,      0    //  3  4  5  6  7  8  9 10
     vexth.hu.bu   vr16,     vr8            // 11 12 13 14 15 16 17 18
-    vsllwil.wu.hu vr17,     vr15,     0    //  3  4  5  6
+    vsllwil.wu.hu vr17,     vr15,     7    //  3  4  5  6
     vexth.wu.hu   vr18,     vr15           //  7  8  9 10
-    vsllwil.wu.hu vr19,     vr16,     0    // 11 12 13 14
+    vsllwil.wu.hu vr19,     vr16,     7    // 11 12 13 14
     vexth.wu.hu   vr20,     vr16           // 15 16 17 18
-    vslli.w       vr17,     vr17,     7
     vslli.w       vr18,     vr18,     7
-    vslli.w       vr19,     vr19,     7
     vslli.w       vr20,     vr20,     7
     vxor.v        vr15,     vr15,     vr15
     vxor.v        vr14,     vr14,     vr14
@@ -315,30 +313,24 @@ function boxsum3_h_8bpc_lsx
     vmulwod.h.bu   vr10,    vr3,     vr3
     vmulwev.h.bu   vr11,    vr5,     vr5
     vmulwod.h.bu   vr12,    vr5,     vr5
-    vmul.h         vr7,     vr7,     vr7
-    vmul.h         vr8,     vr8,     vr8
     vaddwev.w.hu   vr13,    vr10,    vr9
     vaddwod.w.hu   vr14,    vr10,    vr9
-    vilvl.w        vr3,     vr14,    vr13
-    vilvh.w        vr4,     vr14,    vr13
-    vaddwev.w.hu   vr13,    vr12,    vr11
-    vaddwod.w.hu   vr14,    vr12,    vr11
-    vilvl.w        vr15,    vr14,    vr13
-    vilvh.w        vr16,    vr14,    vr13
-    vsllwil.wu.hu  vr9,     vr7,     0
-    vexth.wu.hu    vr10,    vr7
-    vsllwil.wu.hu  vr11,    vr8,     0
-    vexth.wu.hu    vr12,    vr8
-    vadd.w         vr9,     vr9,     vr3
-    vadd.w         vr10,    vr10,    vr4
-    vadd.w         vr11,    vr11,    vr15
-    vadd.w         vr12,    vr12,    vr16
+    vaddwev.w.hu   vr15,    vr12,    vr11
+    vaddwod.w.hu   vr16,    vr12,    vr11
+    vmaddwev.w.hu  vr13,    vr7,     vr7
+    vmaddwod.w.hu  vr14,    vr7,     vr7
+    vmaddwev.w.hu  vr15,    vr8,     vr8
+    vmaddwod.w.hu  vr16,    vr8,     vr8
+    vilvl.w        vr9,     vr14,    vr13
+    vilvh.w        vr10,    vr14,    vr13
+    vilvl.w        vr11,    vr16,    vr15
+    vilvh.w        vr12,    vr16,    vr15
     vst            vr9,     t2,      REST_UNIT_STRIDE<<2
     vst            vr10,    t2,      (REST_UNIT_STRIDE<<2)+16
     vst            vr11,    t2,      (REST_UNIT_STRIDE<<2)+32
     vst            vr12,    t2,      (REST_UNIT_STRIDE<<2)+48
-    addi.d         t2,      t2,      64
 
+    addi.d         t2,      t2,      64
     addi.w         t5,      t5,      -16
     addi.d         t3,      t3,      16
     blt            zero,    t5,      .LBS3_H_W
@@ -348,8 +340,6 @@ function boxsum3_h_8bpc_lsx
     addi.d         a2,      a2,      REST_UNIT_STRIDE
     addi.d         a4,      a4,      -1
     blt            zero,    a4,      .LBS3_H_H
-
-.LBS3_H_END:
 endfunc
 
 /*
@@ -379,10 +369,10 @@ function boxsum3_v_8bpc_lsx
     vld            vr7,      t0,      20  //    4 5 6 7
     vld            vr8,      t0,      24  //    5 6 7 8
     vadd.h         vr9,      vr0,     vr1
-    vadd.h         vr9,      vr9,     vr2
     vadd.w         vr10,     vr3,     vr4
-    vadd.w         vr10,     vr10,    vr5
     vadd.w         vr11,     vr6,     vr7
+    vadd.h         vr9,      vr9,     vr2
+    vadd.w         vr10,     vr10,    vr5
     vadd.w         vr11,     vr11,    vr8
     vpickve2gr.h   t7,       vr2,     6
     vpickve2gr.w   t8,       vr8,     2
@@ -425,15 +415,13 @@ function boxsum3_v_8bpc_lsx
     addi.d         t0,       t0,      32
     addi.d         t5,       t5,      32
     addi.d         t6,       t6,      16
-    blt            zero,     t3,       .LBS3_V_W8
+    blt            zero,     t3,      .LBS3_V_W8
 
 .LBS3_V_H0:
     addi.d         a1,       a1,      REST_UNIT_STRIDE<<1
     addi.d         a0,       a0,      REST_UNIT_STRIDE<<2
     addi.w         a3,       a3,      -1
     bnez           a3,       .LBS3_V_H
-
-.LBS3_V_END:
 endfunc
 
 /*
@@ -559,14 +547,14 @@ function boxsum3_sgf_v_8bpc_lsx
 .LBS3SGF_V_W:
     vld           vr0,       t0,      0   // P[i - REST_UNIT_STRIDE]
     vld           vr1,       t0,      16
-    vld           vr2,       t1,      -4  // P[i-1]
-    vld           vr3,       t1,      12
+    vld           vr2,       t1,      -4  // P[i-1]  -1 0 1 2
+    vld           vr3,       t1,      12           // 3 4 5 6
     vld           vr4,       t2,      0   // P[i + REST_UNIT_STRIDE]
     vld           vr5,       t2,      16
-    vld           vr6,       t1,      0   // p[i]
-    vld           vr7,       t1,      16
-    vld           vr8,       t1,      4   // p[i+1]
-    vld           vr9,       t1,      20
+    vld           vr6,       t1,      0   // p[i]     0 1 2 3
+    vld           vr7,       t1,      16           // 4 5 6 7
+    vld           vr8,       t1,      4   // p[i+1]   1 2 3 4
+    vld           vr9,       t1,      20           // 5 6 7 8
 
     vld           vr10,      t0,      -4  // P[i - 1 - REST_UNIT_STRIDE]
     vld           vr11,      t0,      12
@@ -666,6 +654,144 @@ function boxsum3_sgf_v_8bpc_lsx
     bnez          a5,        .LBS3SGF_V_H
 endfunc
 
+function boxsum3_sgf_v_8bpc_lasx
+    addi.d        a1,        a1,      (3*REST_UNIT_STRIDE+3)   // src
+    addi.d        a2,        a2,      REST_UNIT_STRIDE<<2
+    addi.d        a2,        a2,      (REST_UNIT_STRIDE<<2)+12
+    addi.d        a3,        a3,      REST_UNIT_STRIDE<<2
+    addi.d        a3,        a3,      6
+.LBS3SGF_V_H_LASX:
+    // A int32_t *sumsq
+    addi.d        t0,        a2,      -(REST_UNIT_STRIDE<<2)   // -stride
+    addi.d        t1,        a2,      0    // sumsq
+    addi.d        t2,        a2,      REST_UNIT_STRIDE<<2      // +stride
+    addi.d        t6,        a1,      0
+    addi.w        t7,        a4,      0
+    addi.d        t8,        a0,      0
+    // B coef *sum
+    addi.d        t3,        a3,      -(REST_UNIT_STRIDE<<1)   // -stride
+    addi.d        t4,        a3,      0
+    addi.d        t5,        a3,      REST_UNIT_STRIDE<<1
+
+.LBS3SGF_V_W_LASX:
+    xvld           xr0,       t0,      0   // P[i - REST_UNIT_STRIDE]
+    xvld           xr1,       t0,      32
+    xvld           xr2,       t1,      -4  // P[i-1]  -1 0 1 2
+    xvld           xr3,       t1,      28           // 3 4 5 6
+    xvld           xr4,       t2,      0   // P[i + REST_UNIT_STRIDE]
+    xvld           xr5,       t2,      32
+    xvld           xr6,       t1,      0   // p[i]     0 1 2 3
+    xvld           xr7,       t1,      32           // 4 5 6 7
+    xvld           xr8,       t1,      4   // p[i+1]   1 2 3 4
+    xvld           xr9,       t1,      36           // 5 6 7 8
+
+    xvld           xr10,      t0,      -4  // P[i - 1 - REST_UNIT_STRIDE]
+    xvld           xr11,      t0,      28
+    xvld           xr12,      t2,      -4  // P[i - 1 + REST_UNIT_STRIDE]
+    xvld           xr13,      t2,      28
+    xvld           xr14,      t0,      4   // P[i + 1 - REST_UNIT_STRIDE]
+    xvld           xr15,      t0,      36
+    xvld           xr16,      t2,      4   // P[i + 1 + REST_UNIT_STRIDE]
+    xvld           xr17,      t2,      36
+
+    xvadd.w        xr0,       xr2,     xr0
+    xvadd.w        xr4,       xr6,     xr4
+    xvadd.w        xr0,       xr0,     xr8
+    xvadd.w        xr20,      xr0,     xr4
+    xvslli.w       xr20,      xr20,    2      // 0 1 2 3
+    xvadd.w        xr0,       xr1,     xr3
+    xvadd.w        xr4,       xr5,     xr7
+    xvadd.w        xr0,       xr0,     xr9
+    xvadd.w        xr21,      xr0,     xr4
+    xvslli.w       xr21,      xr21,    2      // 4 5 6 7
+    xvadd.w        xr12,      xr10,    xr12
+    xvadd.w        xr16,      xr14,    xr16
+    xvadd.w        xr22,      xr12,    xr16
+    xvslli.w       xr23,      xr22,    1
+    xvadd.w        xr22,      xr23,    xr22
+    xvadd.w        xr11,      xr11,    xr13
+    xvadd.w        xr15,      xr15,    xr17
+    xvadd.w        xr0,       xr11,    xr15
+    xvslli.w       xr23,      xr0,     1
+    xvadd.w        xr23,      xr23,    xr0
+    xvadd.w        xr20,      xr20,    xr22   // b
+    xvadd.w        xr21,      xr21,    xr23
+
+    // B coef *sum
+    xvld           xr0,       t3,      0   // P[i - REST_UNIT_STRIDE]
+    xvld           xr1,       t4,      -2  // p[i - 1]
+    xvld           xr2,       t4,      0   // p[i]
+    xvld           xr3,       t4,      2   // p[i + 1]
+    xvld           xr4,       t5,      0   // P[i + REST_UNIT_STRIDE]
+    xvld           xr5,       t3,      -2  // P[i - 1 - REST_UNIT_STRIDE]
+    xvld           xr6,       t5,      -2  // P[i - 1 + REST_UNIT_STRIDE]
+    xvld           xr7,       t3,      2   // P[i + 1 - REST_UNIT_STRIDE]
+    xvld           xr8,       t5,      2   // P[i + 1 + REST_UNIT_STRIDE]
+
+    xvaddwev.w.h   xr9,       xr0,     xr1
+    xvaddwod.w.h   xr10,      xr0,     xr1
+    xvaddwev.w.h   xr11,      xr2,     xr3
+    xvaddwod.w.h   xr12,      xr2,     xr3
+    xvadd.w        xr9,       xr11,    xr9   // 0 2 4 6 8 10 12 14
+    xvadd.w        xr10,      xr12,    xr10  // 1 3 5 7 9 11 13 15
+    xvilvl.w       xr11,      xr10,    xr9   // 0 1 2 3 8 9 10 11
+    xvilvh.w       xr12,      xr10,    xr9   // 4 5 6 7 12 13 14 15
+    xvsllwil.w.h   xr0,       xr4,     0     // 0 1 2 3 8 9 10 11
+    xvexth.w.h     xr1,       xr4            // 4 5 6 7 12 13 14 15
+
+    xvadd.w        xr0,       xr11,    xr0
+    xvadd.w        xr1,       xr12,    xr1
+    xvslli.w       xr0,       xr0,     2
+    xvslli.w       xr1,       xr1,     2
+
+    xvaddwev.w.h   xr9,       xr5,     xr6
+    xvaddwod.w.h   xr10,      xr5,     xr6
+    xvaddwev.w.h   xr11,      xr7,     xr8
+    xvaddwod.w.h   xr12,      xr7,     xr8
+    xvadd.w        xr9,       xr11,    xr9
+    xvadd.w        xr10,      xr12,    xr10
+    xvilvl.w       xr13,      xr10,    xr9   // 0 1 2 3 8 9 10 11
+    xvilvh.w       xr14,      xr10,    xr9   // 4 5 6 7 12 13 14 15
+
+    xvslli.w       xr15,      xr13,    1
+    xvslli.w       xr16,      xr14,    1
+    xvadd.w        xr15,      xr13,    xr15   // a
+    xvadd.w        xr16,      xr14,    xr16
+    xvadd.w        xr22,      xr0,     xr15   // A B
+    xvadd.w        xr23,      xr1,     xr16   // C D
+
+    vld            vr0,       t6,      0      // src
+    vilvh.d        vr2,       vr0,     vr0
+    vext2xv.wu.bu  xr1,       xr0
+    vext2xv.wu.bu  xr2,       xr2
+    xvor.v         xr15,      xr22,    xr22   // A B
+    xvpermi.q      xr22,      xr23,    0b00000010  // A C
+    xvpermi.q      xr23,      xr15,    0b00110001
+    xvmadd.w       xr20,      xr22,    xr1
+    xvmadd.w       xr21,      xr23,    xr2
+    xvssrlrni.h.w  xr21,      xr20,    9
+    xvpermi.d      xr22,      xr21,    0b11011000
+    xvst           xr22,      t8,      0
+    addi.d         t8,        t8,      32
+
+    addi.d        t0,        t0,      64
+    addi.d        t1,        t1,      64
+    addi.d        t2,        t2,      64
+    addi.d        t3,        t3,      32
+    addi.d        t4,        t4,      32
+    addi.d        t5,        t5,      32
+    addi.d        t6,        t6,      16
+    addi.w        t7,        t7,      -16
+    blt           zero,      t7,      .LBS3SGF_V_W_LASX
+
+    addi.w        a5,        a5,      -1
+    addi.d        a0,        a0,      384*2
+    addi.d        a1,        a1,      REST_UNIT_STRIDE
+    addi.d        a3,        a3,      REST_UNIT_STRIDE<<1
+    addi.d        a2,        a2,      REST_UNIT_STRIDE<<2
+    bnez          a5,        .LBS3SGF_V_H_LASX
+endfunc
+
 #define FILTER_OUT_STRIDE (384)
 
 /*
@@ -835,20 +961,15 @@ function boxsum5_h_8bpc_lsx
     vadd.w        vr6,     vr6,       vr20
     vadd.w        vr7,     vr7,       vr21
     vadd.w        vr8,     vr8,       vr22
+    vmaddwev.w.hu vr5,     vr11,      vr11
+    vmaddwod.w.hu vr6,     vr11,      vr11
+    vmaddwev.w.hu vr7,     vr12,      vr12
+    vmaddwod.w.hu vr8,     vr12,      vr12
     vilvl.w       vr19,    vr6,       vr5
     vilvh.w       vr20,    vr6,       vr5
     vilvl.w       vr21,    vr8,       vr7
     vilvh.w       vr22,    vr8,       vr7
-    vmul.h        vr11,    vr11,      vr11
-    vmul.h        vr12,    vr12,      vr12
-    vsllwil.wu.hu vr0,     vr11,      0
-    vexth.wu.hu   vr1,     vr11
-    vsllwil.wu.hu vr2,     vr12,      0
-    vexth.wu.hu   vr3,     vr12
-    vadd.w        vr19,    vr19,      vr0
-    vadd.w        vr20,    vr20,      vr1
-    vadd.w        vr21,    vr21,      vr2
-    vadd.w        vr22,    vr22,      vr3
+
     vst           vr19,    t0,        0
     vst           vr20,    t0,        16
     vst           vr21,    t0,        32
@@ -1405,3 +1526,431 @@ function sgr_mix_finish_8bpc_lsx
 
 .LSGR_MIX_END:
 endfunc
+
+.macro MADD_HU_BU_LASX in0, in1, out0, out1
+    xvsllwil.hu.bu xr12,     \in0,     0
+    xvexth.hu.bu   xr13,     \in0
+    xvmadd.h       \out0,    xr12,     \in1
+    xvmadd.h       \out1,    xr13,     \in1
+.endm
+
+const wiener_shuf_lasx
+.byte 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18
+.byte 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18
+endconst
+
+function wiener_filter_h_8bpc_lasx
+    addi.d         sp,       sp,       -40
+    fst.d          f24,      sp,       0
+    fst.d          f25,      sp,       8
+    fst.d          f26,      sp,       16
+    fst.d          f27,      sp,       24
+    fst.d          f28,      sp,       32
+    li.w           t7,       1<<14          // clip_limit
+
+    la.local       t1,       wiener_shuf_lasx
+    xvld           xr4,      t1,       0
+    vld            vr27,     a2,       0    // filter[0][k]
+    xvpermi.q      xr14,     xr27,     0b00000000
+    xvrepl128vei.h xr21,     xr14,     0
+    xvrepl128vei.h xr22,     xr14,     1
+    xvrepl128vei.h xr23,     xr14,     2
+    xvrepl128vei.h xr24,     xr14,     3
+    xvrepl128vei.h xr25,     xr14,     4
+    xvrepl128vei.h xr26,     xr14,     5
+    xvrepl128vei.h xr27,     xr14,     6
+    xvreplgr2vr.w  xr0,      t7
+
+.WIENER_FILTER_H_H_LASX:
+    addi.w         a4,       a4,       -1    // h
+    addi.w         t0,       a3,       0     // w
+    addi.d         t1,       a1,       0     // tmp_ptr
+    addi.d         t2,       a0,       0     // hor_ptr
+
+.WIENER_FILTER_H_W_LASX:
+    addi.w         t0,       t0,       -32
+    xvld           xr5,      t1,       0
+    xvld           xr13,     t1,       16
+
+    xvsubi.bu      xr14,     xr4,      2
+    xvsubi.bu      xr15,     xr4,      1
+    xvshuf.b       xr6,      xr13,     xr5,     xr14  // 1 ... 8, 9 ... 16
+    xvshuf.b       xr7,      xr13,     xr5,     xr15  // 2 ... 9, 10 ... 17
+    xvshuf.b       xr8,      xr13,     xr5,     xr4   // 3 ... 10, 11 ... 18
+    xvaddi.bu      xr14,     xr4,      1
+    xvaddi.bu      xr15,     xr4,      2
+    xvshuf.b       xr9,      xr13,     xr5,     xr14  // 4 ... 11, 12 ... 19
+    xvshuf.b       xr10,     xr13,     xr5,     xr15  // 5 ... 12, 13 ... 20
+    xvaddi.bu      xr14,     xr4,      3
+    xvshuf.b       xr11,     xr13,     xr5,     xr14  // 6 ... 13, 14 ... 21
+
+    xvsllwil.hu.bu xr15,     xr8,      0    //  3  4  5  6  7  8  9 10
+    xvexth.hu.bu   xr16,     xr8            // 11 12 13 14 15 16 17 18
+    xvsllwil.wu.hu xr17,     xr15,     7    //  3  4  5  6
+    xvexth.wu.hu   xr18,     xr15           //  7  8  9 10
+    xvsllwil.wu.hu xr19,     xr16,     7    // 11 12 13 14
+    xvexth.wu.hu   xr20,     xr16           // 15 16 17 18
+    xvslli.w       xr18,     xr18,     7
+    xvslli.w       xr20,     xr20,     7
+    xvxor.v        xr15,     xr15,     xr15
+    xvxor.v        xr14,     xr14,     xr14
+
+    MADD_HU_BU_LASX xr5,  xr21, xr14, xr15
+    MADD_HU_BU_LASX xr6,  xr22, xr14, xr15
+    MADD_HU_BU_LASX xr7,  xr23, xr14, xr15
+    MADD_HU_BU_LASX xr8,  xr24, xr14, xr15
+    MADD_HU_BU_LASX xr9,  xr25, xr14, xr15
+    MADD_HU_BU_LASX xr10, xr26, xr14, xr15
+    MADD_HU_BU_LASX xr11, xr27, xr14, xr15
+
+    xvsllwil.w.h   xr5,      xr14,     0   //  0  1  2  3
+    xvexth.w.h     xr6,      xr14          //  4  5  6  7
+    xvsllwil.w.h   xr7,      xr15,     0   //  8  9 10 11
+    xvexth.w.h     xr8,      xr15          // 12 13 14 15
+    xvadd.w        xr17,     xr17,     xr5
+    xvadd.w        xr18,     xr18,     xr6
+    xvadd.w        xr19,     xr19,     xr7
+    xvadd.w        xr20,     xr20,     xr8
+    xvadd.w        xr17,     xr17,     xr0
+    xvadd.w        xr18,     xr18,     xr0
+    xvadd.w        xr19,     xr19,     xr0
+    xvadd.w        xr20,     xr20,     xr0
+
+    xvsrli.w       xr1,      xr0,      1
+    xvsubi.wu      xr1,      xr1,      1
+    xvxor.v        xr3,      xr3,      xr3
+    xvsrari.w      xr17,     xr17,     3
+    xvsrari.w      xr18,     xr18,     3
+    xvsrari.w      xr19,     xr19,     3
+    xvsrari.w      xr20,     xr20,     3
+    xvclip.w       xr17,     xr17,     xr3,     xr1
+    xvclip.w       xr18,     xr18,     xr3,     xr1
+    xvclip.w       xr19,     xr19,     xr3,     xr1
+    xvclip.w       xr20,     xr20,     xr3,     xr1
+
+    xvor.v         xr5,      xr17,     xr17
+    xvor.v         xr6,      xr19,     xr19
+    xvpermi.q      xr17,     xr18,     0b00000010
+    xvpermi.q      xr19,     xr20,     0b00000010
+
+    xvst           xr17,     t2,       0
+    xvst           xr19,     t2,       32
+    xvpermi.q      xr18,     xr5,      0b00110001
+    xvpermi.q      xr20,     xr6,      0b00110001
+    xvst           xr18,     t2,       64
+    xvst           xr20,     t2,       96
+    addi.d         t1,       t1,       32
+    addi.d         t2,       t2,       128
+    blt            zero,     t0,       .WIENER_FILTER_H_W_LASX
+
+    addi.d         a1,       a1,       REST_UNIT_STRIDE
+    addi.d         a0,       a0,       (REST_UNIT_STRIDE << 2)
+    bnez           a4,       .WIENER_FILTER_H_H_LASX
+
+    fld.d          f24,      sp,       0
+    fld.d          f25,      sp,       8
+    fld.d          f26,      sp,       16
+    fld.d          f27,      sp,       24
+    fld.d          f28,      sp,       32
+    addi.d         sp,       sp,       40
+endfunc
+
+.macro APPLY_FILTER_LASX in0, in1, in2
+    alsl.d         t7,       \in0,     \in1,    2
+    xvld           xr10,     t7,       0
+    xvld           xr12,     t7,       32
+    xvmadd.w       xr14,     xr10,     \in2
+    xvmadd.w       xr16,     xr12,     \in2
+.endm
+
+.macro wiener_filter_v_8bpc_core_lasx
+    xvreplgr2vr.w  xr14,     t6
+    xvreplgr2vr.w  xr16,     t6
+
+    addi.w         t7,       t2,       0      // j + index k
+    mul.w          t7,       t7,       t8     // (j + index) * REST_UNIT_STRIDE
+    add.w          t7,       t7,       t4     // (j + index) * REST_UNIT_STRIDE + i
+
+    APPLY_FILTER_LASX  t7, a2, xr2
+    APPLY_FILTER_LASX  t8, t7, xr3
+    APPLY_FILTER_LASX  t8, t7, xr4
+    APPLY_FILTER_LASX  t8, t7, xr5
+    APPLY_FILTER_LASX  t8, t7, xr6
+    APPLY_FILTER_LASX  t8, t7, xr7
+    APPLY_FILTER_LASX  t8, t7, xr8
+    xvssrarni.hu.w xr16,     xr14,      11
+    xvpermi.d      xr17,     xr16,      0b11011000
+    xvssrlni.bu.h  xr17,     xr17,      0
+    xvpermi.d      xr17,     xr17,      0b00001000
+.endm
+
+function wiener_filter_v_8bpc_lasx
+    li.w          t6,       -(1 << 18)
+
+    li.w          t8,       REST_UNIT_STRIDE
+    ld.h          t0,       a3,       0
+    ld.h          t1,       a3,       2
+    xvreplgr2vr.w xr2,      t0
+    xvreplgr2vr.w xr3,      t1
+    ld.h          t0,       a3,       4
+    ld.h          t1,       a3,       6
+    xvreplgr2vr.w xr4,      t0
+    xvreplgr2vr.w xr5,      t1
+    ld.h          t0,       a3,       8
+    ld.h          t1,       a3,       10
+    xvreplgr2vr.w xr6,      t0
+    xvreplgr2vr.w xr7,      t1
+    ld.h          t0,       a3,       12
+    xvreplgr2vr.w xr8,      t0
+
+    andi          t1,       a4,       0xf
+    sub.w         t0,       a4,       t1    // w-w%16
+    or            t2,       zero,     zero  // j
+    or            t4,       zero,     zero
+    beqz          t0,       .WIENER_FILTER_V_W_LT16_LASX
+
+.WIENER_FILTER_V_H_LASX:
+    andi          t1,       a4,       0xf
+    add.d         t3,       zero,     a0     // p
+    or            t4,       zero,     zero   // i
+
+.WIENER_FILTER_V_W_LASX:
+
+    wiener_filter_v_8bpc_core_lasx
+
+    mul.w         t5,       t2,       a1   // j * stride
+    add.w         t5,       t5,       t4   // j * stride + i
+    add.d         t3,       a0,       t5
+    addi.w        t4,       t4,       16
+    vst           vr17,     t3,       0
+    bne           t0,       t4,       .WIENER_FILTER_V_W_LASX
+
+    beqz          t1,       .WIENER_FILTER_V_W_EQ16_LASX
+
+    wiener_filter_v_8bpc_core_lsx
+
+    addi.d        t3,       t3,       16
+    andi          t1,       a4,       0xf
+
+.WIENER_FILTER_V_ST_REM_LASX:
+    vstelm.b      vr17,     t3,       0,    0
+    vbsrl.v       vr17,     vr17,     1
+    addi.d        t3,       t3,       1
+    addi.w        t1,       t1,       -1
+    bnez          t1,       .WIENER_FILTER_V_ST_REM_LASX
+.WIENER_FILTER_V_W_EQ16_LASX:
+    addi.w        t2,       t2,       1
+    blt           t2,       a5,       .WIENER_FILTER_V_H_LASX
+    b              .WIENER_FILTER_V_LASX_END
+
+.WIENER_FILTER_V_W_LT16_LASX:
+    andi          t1,       a4,       0xf
+    add.d         t3,       zero,     a0
+
+    wiener_filter_v_8bpc_core_lsx
+
+    mul.w         t5,       t2,       a1   // j * stride
+    add.d         t3,       a0,       t5
+
+.WIENER_FILTER_V_ST_REM_1_LASX:
+    vstelm.b      vr17,     t3,       0,    0
+    vbsrl.v       vr17,     vr17,     1
+    addi.d        t3,       t3,       1
+    addi.w        t1,       t1,       -1
+    bnez          t1,       .WIENER_FILTER_V_ST_REM_1_LASX
+
+    addi.w        t2,       t2,       1
+    blt           t2,       a5,       .WIENER_FILTER_V_W_LT16_LASX
+
+.WIENER_FILTER_V_LASX_END:
+endfunc
+
+function boxsum3_sgf_h_8bpc_lasx
+    addi.d         a0,       a0,        (REST_UNIT_STRIDE<<2)+12  // AA
+    //addi.d        a0,       a0,        12   // AA
+    addi.d         a1,       a1,        (REST_UNIT_STRIDE<<1)+6   // BB
+    //addi.d        a1,       a1,        6    // BB
+    la.local       t8,       dav1d_sgr_x_by_x
+    li.w           t6,       455
+    xvreplgr2vr.w  xr20,     t6
+    li.w           t6,       255
+    xvreplgr2vr.w  xr22,     t6
+    xvaddi.wu      xr21,     xr22,      1  // 256
+    xvreplgr2vr.w  xr6,      a4
+    xvldi          xr19,     0x809
+    addi.w         a2,       a2,        2  // w + 2
+    addi.w         a3,       a3,        2  // h + 2
+
+.LBS3SGF_H_H_LASX:
+    addi.w         t2,       a2,        0
+    addi.d         t0,       a0,        -4
+    addi.d         t1,       a1,        -2
+
+.LBS3SGF_H_W_LASX:
+    addi.w         t2,       t2,        -16
+    xvld           xr0,      t0,        0   // AA[i]
+    xvld           xr1,      t0,        32
+    xvld           xr2,      t1,        0   // BB[i]
+
+    xvmul.w        xr4,      xr0,       xr19 // a * n
+    xvmul.w        xr5,      xr1,       xr19
+    vext2xv.w.h    xr9,      xr2
+    xvpermi.q      xr10,     xr2,       0b00000001
+    vext2xv.w.h    xr10,     xr10
+    xvmsub.w       xr4,      xr9,       xr9  // p
+    xvmsub.w       xr5,      xr10,      xr10
+    xvmaxi.w       xr4,      xr4,       0
+    xvmaxi.w       xr5,      xr5,       0
+    xvmul.w        xr4,      xr4,       xr6  // p * s
+    xvmul.w        xr5,      xr5,       xr6
+    xvsrlri.w      xr4,      xr4,       20
+    xvsrlri.w      xr5,      xr5,       20
+    xvmin.w        xr4,      xr4,       xr22
+    xvmin.w        xr5,      xr5,       xr22
+
+    vpickve2gr.w   t6,       vr4,       0
+    ldx.bu         t7,       t8,        t6
+    vinsgr2vr.w    vr7,      t7,        0
+    vpickve2gr.w   t6,       vr4,       1
+    ldx.bu         t7,       t8,        t6
+    vinsgr2vr.w    vr7,      t7,        1
+    vpickve2gr.w   t6,       vr4,       2
+    ldx.bu         t7,       t8,        t6
+    vinsgr2vr.w    vr7,      t7,        2
+    vpickve2gr.w   t6,       vr4,       3
+    ldx.bu         t7,       t8,        t6
+    vinsgr2vr.w    vr7,      t7,        3
+
+    xvpickve2gr.w  t6,       xr4,       4
+    ldx.bu         t7,       t8,        t6
+    xvinsgr2vr.w   xr7,      t7,        4
+    xvpickve2gr.w  t6,       xr4,       5
+    ldx.bu         t7,       t8,        t6
+    xvinsgr2vr.w   xr7,      t7,        5
+    xvpickve2gr.w  t6,       xr4,       6
+    ldx.bu         t7,       t8,        t6
+    xvinsgr2vr.w   xr7,      t7,        6
+    xvpickve2gr.w  t6,       xr4,       7
+    ldx.bu         t7,       t8,        t6
+    xvinsgr2vr.w   xr7,      t7,        7     // x
+
+    vpickve2gr.w   t6,       vr5,       0
+    ldx.bu         t7,       t8,        t6
+    vinsgr2vr.w    vr8,      t7,        0
+    vpickve2gr.w   t6,       vr5,       1
+    ldx.bu         t7,       t8,        t6
+    vinsgr2vr.w    vr8,      t7,        1
+    vpickve2gr.w   t6,       vr5,       2
+    ldx.bu         t7,       t8,        t6
+    vinsgr2vr.w    vr8,      t7,        2
+    vpickve2gr.w   t6,       vr5,       3
+    ldx.bu         t7,       t8,        t6
+    vinsgr2vr.w    vr8,      t7,        3
+
+    xvpickve2gr.w  t6,       xr5,       4
+    ldx.bu         t7,       t8,        t6
+    xvinsgr2vr.w   xr8,      t7,        4
+    xvpickve2gr.w  t6,       xr5,       5
+    ldx.bu         t7,       t8,        t6
+    xvinsgr2vr.w   xr8,      t7,        5
+    xvpickve2gr.w  t6,       xr5,       6
+    ldx.bu         t7,       t8,        t6
+    xvinsgr2vr.w   xr8,      t7,        6
+    xvpickve2gr.w  t6,       xr5,       7
+    ldx.bu         t7,       t8,        t6
+    xvinsgr2vr.w   xr8,      t7,        7     // x
+
+    xvmul.w        xr9,      xr7,       xr9   // x * BB[i]
+    xvmul.w        xr10,     xr8,       xr10
+    xvmul.w        xr9,      xr9,       xr20  // x * BB[i] * sgr_one_by_x
+    xvmul.w        xr10,     xr10,      xr20
+    xvsrlri.w      xr9,      xr9,       12
+    xvsrlri.w      xr10,     xr10,      12
+    xvsub.w        xr7,      xr21,      xr7
+    xvsub.w        xr8,      xr21,      xr8
+    xvpickev.h     xr12,     xr8,       xr7
+    xvpermi.d      xr11,     xr12,      0b11011000
+
+    xvst           xr9,      t0,        0
+    xvst           xr10,     t0,        32
+    xvst           xr11,     t1,        0
+    addi.d         t0,       t0,        64
+    addi.d         t1,       t1,        32
+    blt            zero,     t2,        .LBS3SGF_H_W_LASX
+
+    addi.d         a0,       a0,        REST_UNIT_STRIDE<<2
+    addi.d         a1,       a1,        REST_UNIT_STRIDE<<1
+    addi.w         a3,       a3,        -1
+    bnez           a3,       .LBS3SGF_H_H_LASX
+endfunc
+
+function boxsum3_h_8bpc_lasx
+    addi.d         a2,      a2,      REST_UNIT_STRIDE
+    li.w           t0,      1
+    addi.w         a3,      a3,      -2
+    addi.w         a4,      a4,      -4
+.LBS3_H_H_LASX:
+    alsl.d         t1,      t0,      a1,    1     // sum_v    *sum_v = sum + x
+    alsl.d         t2,      t0,      a0,    2     // sumsq_v  *sumsq_v = sumsq + x
+    add.d          t3,      t0,      a2           // s
+    addi.w         t5,      a3,      0
+
+.LBS3_H_W_LASX:
+    xvld           xr0,     t3,      0
+    xvld           xr1,     t3,      REST_UNIT_STRIDE
+    xvld           xr2,     t3,      (REST_UNIT_STRIDE<<1)
+
+    xvilvl.b       xr3,     xr1,     xr0
+    xvhaddw.hu.bu  xr4,     xr3,     xr3
+    xvilvh.b       xr5,     xr1,     xr0
+    xvhaddw.hu.bu  xr6,     xr5,     xr5
+    xvsllwil.hu.bu xr7,     xr2,     0
+    xvexth.hu.bu   xr8,     xr2
+    // sum_v
+    xvadd.h        xr4,     xr4,     xr7  // 0 2
+    xvadd.h        xr6,     xr6,     xr8  // 1 3
+    xvor.v         xr9,     xr4,     xr4
+    xvpermi.q      xr4,     xr6,     0b00000010
+    xvpermi.q      xr6,     xr9,     0b00110001
+    xvst           xr4,     t1,      REST_UNIT_STRIDE<<1
+    xvst           xr6,     t1,      (REST_UNIT_STRIDE<<1)+32
+    addi.d         t1,      t1,      64
+    // sumsq
+    xvmulwev.h.bu  xr9,     xr3,     xr3
+    xvmulwod.h.bu  xr10,    xr3,     xr3
+    xvmulwev.h.bu  xr11,    xr5,     xr5
+    xvmulwod.h.bu  xr12,    xr5,     xr5
+    xvaddwev.w.hu  xr13,    xr10,    xr9
+    xvaddwod.w.hu  xr14,    xr10,    xr9
+    xvaddwev.w.hu  xr15,    xr12,    xr11
+    xvaddwod.w.hu  xr16,    xr12,    xr11
+    xvmaddwev.w.hu xr13,    xr7,     xr7
+    xvmaddwod.w.hu xr14,    xr7,     xr7
+    xvmaddwev.w.hu xr15,    xr8,     xr8
+    xvmaddwod.w.hu xr16,    xr8,     xr8
+    xvilvl.w       xr9,     xr14,    xr13
+    xvilvh.w       xr10,    xr14,    xr13
+    xvilvl.w       xr11,    xr16,    xr15
+    xvilvh.w       xr12,    xr16,    xr15
+    xvor.v         xr7,     xr9,     xr9
+    xvor.v         xr8,     xr11,    xr11
+    xvpermi.q      xr9,     xr10,    0b00000010
+    xvpermi.q      xr10,    xr7,     0b00110001
+    xvpermi.q      xr11,    xr12,    0b00000010
+    xvpermi.q      xr12,    xr8,     0b00110001
+    xvst           xr9,     t2,      REST_UNIT_STRIDE<<2
+    xvst           xr11,    t2,      (REST_UNIT_STRIDE<<2)+32
+    xvst           xr10,    t2,      (REST_UNIT_STRIDE<<2)+64
+    xvst           xr12,    t2,      (REST_UNIT_STRIDE<<2)+96
+
+    addi.d         t2,      t2,      128
+    addi.w         t5,      t5,      -32
+    addi.d         t3,      t3,      32
+    blt            zero,    t5,      .LBS3_H_W_LASX
+
+    addi.d         a0,      a0,      REST_UNIT_STRIDE<<2
+    addi.d         a1,      a1,      REST_UNIT_STRIDE<<1
+    addi.d         a2,      a2,      REST_UNIT_STRIDE
+    addi.d         a4,      a4,      -1
+    blt            zero,    a4,      .LBS3_H_H_LASX
+endfunc
diff --git a/src/loongarch/looprestoration.h b/src/loongarch/looprestoration.h
index ac0cb06..2e05f86 100644
--- a/src/loongarch/looprestoration.h
+++ b/src/loongarch/looprestoration.h
@@ -39,6 +39,13 @@ void dav1d_wiener_filter_lsx(uint8_t *p, const ptrdiff_t stride,
                              const LooprestorationParams *const params,
                              const enum LrEdgeFlags edges HIGHBD_DECL_SUFFIX);
 
+void dav1d_wiener_filter_lasx(uint8_t *p, const ptrdiff_t stride,
+                             const uint8_t (*const left)[4],
+                             const uint8_t *lpf,
+                             const int w, const int h,
+                             const LooprestorationParams *const params,
+                             const enum LrEdgeFlags edges HIGHBD_DECL_SUFFIX);
+
 void dav1d_sgr_filter_3x3_lsx(pixel *p, const ptrdiff_t p_stride,
                               const pixel (*const left)[4],
                               const pixel *lpf,
@@ -46,6 +53,13 @@ void dav1d_sgr_filter_3x3_lsx(pixel *p, const ptrdiff_t p_stride,
                               const LooprestorationParams *const params,
                               const enum LrEdgeFlags edges HIGHBD_DECL_SUFFIX);
 
+void dav1d_sgr_filter_3x3_lasx(pixel *p, const ptrdiff_t p_stride,
+                              const pixel (*const left)[4],
+                              const pixel *lpf,
+                              const int w, const int h,
+                              const LooprestorationParams *const params,
+                              const enum LrEdgeFlags edges HIGHBD_DECL_SUFFIX);
+
 void dav1d_sgr_filter_5x5_lsx(pixel *p, const ptrdiff_t p_stride,
                               const pixel (*const left)[4],
                               const pixel *lpf,
@@ -60,6 +74,13 @@ void dav1d_sgr_filter_mix_lsx(pixel *p, const ptrdiff_t p_stride,
                               const LooprestorationParams *const params,
                               const enum LrEdgeFlags edges HIGHBD_DECL_SUFFIX);
 
+void dav1d_sgr_filter_mix_lasx(pixel *p, const ptrdiff_t p_stride,
+                               const pixel (*const left)[4],
+                               const pixel *lpf,
+                               const int w, const int h,
+                               const LooprestorationParams *const params,
+                               const enum LrEdgeFlags edges HIGHBD_DECL_SUFFIX);
+
 static ALWAYS_INLINE void loop_restoration_dsp_init_loongarch(Dav1dLoopRestorationDSPContext *const c, int bpc)
 {
     const unsigned flags = dav1d_get_cpu_flags();
@@ -73,6 +94,14 @@ static ALWAYS_INLINE void loop_restoration_dsp_init_loongarch(Dav1dLoopRestorati
     c->sgr[1] = dav1d_sgr_filter_3x3_lsx;
     c->sgr[2] = dav1d_sgr_filter_mix_lsx;
 #endif
+
+    if (!(flags & DAV1D_LOONGARCH_CPU_FLAG_LASX)) return;
+
+#if BITDEPTH == 8
+    c->wiener[0] = c->wiener[1] = dav1d_wiener_filter_lasx;
+
+    c->sgr[1] = dav1d_sgr_filter_3x3_lasx;
+#endif
 }
 
 #endif /* DAV1D_SRC_LOONGARCH_LOOPRESTORATION_H */
diff --git a/src/loongarch/looprestoration_tmpl.c b/src/loongarch/looprestoration_tmpl.c
index 66d0d63..926c204 100644
--- a/src/loongarch/looprestoration_tmpl.c
+++ b/src/loongarch/looprestoration_tmpl.c
@@ -36,12 +36,23 @@ void BF(dav1d_wiener_filter_h, lsx)(int32_t *hor_ptr,
                                     const int16_t filterh[8],
                                     const int w, const int h);
 
+void BF(dav1d_wiener_filter_h, lasx)(int32_t *hor_ptr,
+                                     uint8_t *tmp_ptr,
+                                     const int16_t filterh[8],
+                                     const int w, const int h);
+
 void BF(dav1d_wiener_filter_v, lsx)(uint8_t *p,
                                     const ptrdiff_t p_stride,
                                     const int32_t *hor,
                                     const int16_t filterv[8],
                                     const int w, const int h);
 
+void BF(dav1d_wiener_filter_v, lasx)(uint8_t *p,
+                                     const ptrdiff_t p_stride,
+                                     const int32_t *hor,
+                                     const int16_t filterv[8],
+                                     const int w, const int h);
+
 // This function refers to the function in the ppc/looprestoration_init_tmpl.c.
 static inline void padding(uint8_t *dst, const uint8_t *p,
                            const ptrdiff_t stride, const uint8_t (*left)[4],
@@ -156,20 +167,46 @@ void dav1d_wiener_filter_lsx(uint8_t *p, const ptrdiff_t p_stride,
     BF(dav1d_wiener_filter_v, lsx)(p, p_stride, hor, filter[1], w, h);
 }
 
+void dav1d_wiener_filter_lasx(uint8_t *p, const ptrdiff_t p_stride,
+                              const uint8_t (*const left)[4],
+                              const uint8_t *lpf,
+                              const int w, const int h,
+                              const LooprestorationParams *const params,
+                              const enum LrEdgeFlags edges HIGHBD_DECL_SUFFIX)
+{
+    const int16_t (*const filter)[8] = params->filter;
+
+    // Wiener filtering is applied to a maximum stripe height of 64 + 3 pixels
+    // of padding above and below
+    ALIGN_STK_16(uint8_t, tmp, 70 /*(64 + 3 + 3)*/ * REST_UNIT_STRIDE,);
+    padding(tmp, p, p_stride, left, lpf, w, h, edges);
+    ALIGN_STK_16(int32_t, hor, 70 /*(64 + 3 + 3)*/ * REST_UNIT_STRIDE + 64,);
+
+    BF(dav1d_wiener_filter_h, lasx)(hor, tmp, filter[0], w, h + 6);
+    BF(dav1d_wiener_filter_v, lasx)(p, p_stride, hor, filter[1], w, h);
+}
+
 void BF(dav1d_boxsum3_h, lsx)(int32_t *sumsq, int16_t *sum, pixel *src,
                               const int w, const int h);
 void BF(dav1d_boxsum3_v, lsx)(int32_t *sumsq, int16_t *sum,
                               const int w, const int h);
 
 void BF(dav1d_boxsum3_sgf_h, lsx)(int32_t *sumsq, int16_t *sum,
-                          const int w, const int h, const int w1);
+                                  const int w, const int h, const int w1);
 void BF(dav1d_boxsum3_sgf_v, lsx)(int16_t *dst, uint8_t *tmp,
-                          int32_t *sumsq, int16_t *sum,
-                          const int w, const int h);
+                                  int32_t *sumsq, int16_t *sum,
+                                  const int w, const int h);
 void BF(dav1d_sgr_3x3_finish, lsx)(pixel *p, const ptrdiff_t p_stride,
-                                    int16_t *dst, int w1,
-                                    const int w, const int h);
+                                   int16_t *dst, int w1,
+                                   const int w, const int h);
 
+void BF(dav1d_boxsum3_h, lasx)(int32_t *sumsq, int16_t *sum, pixel *src,
+                               const int w, const int h);
+void BF(dav1d_boxsum3_sgf_h, lasx)(int32_t *sumsq, int16_t *sum,
+                                   const int w, const int h, const int w1);
+void BF(dav1d_boxsum3_sgf_v, lasx)(int16_t *dst, uint8_t *tmp,
+                                   int32_t *sumsq, int16_t *sum,
+                                   const int w, const int h);
 
 static inline void boxsum3_lsx(int32_t *sumsq, coef *sum, pixel *src,
                                const int w, const int h)
@@ -178,6 +215,13 @@ static inline void boxsum3_lsx(int32_t *sumsq, coef *sum, pixel *src,
     BF(dav1d_boxsum3_v, lsx)(sumsq, sum, w + 6, h + 6);
 }
 
+static inline void boxsum3_lasx(int32_t *sumsq, coef *sum, pixel *src,
+                               const int w, const int h)
+{
+    BF(dav1d_boxsum3_h, lasx)(sumsq, sum, src, w + 6, h + 6);
+    BF(dav1d_boxsum3_v, lsx)(sumsq, sum, w + 6, h + 6);
+}
+
 void dav1d_sgr_filter_3x3_lsx(pixel *p, const ptrdiff_t p_stride,
                               const pixel (*const left)[4],
                               const pixel *lpf,
@@ -198,6 +242,26 @@ void dav1d_sgr_filter_3x3_lsx(pixel *p, const ptrdiff_t p_stride,
     BF(dav1d_sgr_3x3_finish, lsx)(p, p_stride, dst, params->sgr.w1, w, h);
 }
 
+void dav1d_sgr_filter_3x3_lasx(pixel *p, const ptrdiff_t p_stride,
+                              const pixel (*const left)[4],
+                              const pixel *lpf,
+                              const int w, const int h,
+                              const LooprestorationParams *const params,
+                              const enum LrEdgeFlags edges HIGHBD_DECL_SUFFIX)
+{
+    ALIGN_STK_16(uint8_t, tmp, 70 /*(64 + 3 + 3)*/ * REST_UNIT_STRIDE,);
+    padding(tmp, p, p_stride, left, lpf, w, h, edges);
+    coef dst[64 * 384];
+
+    ALIGN_STK_16(int32_t, sumsq, 68 * REST_UNIT_STRIDE + 8, );
+    ALIGN_STK_16(int16_t, sum, 68 * REST_UNIT_STRIDE + 16, );
+
+    boxsum3_lasx(sumsq, sum, tmp, w, h);
+    BF(dav1d_boxsum3_sgf_h, lasx)(sumsq, sum, w, h, params->sgr.s1);
+    BF(dav1d_boxsum3_sgf_v, lasx)(dst, tmp, sumsq, sum, w, h);
+    BF(dav1d_sgr_3x3_finish, lsx)(p, p_stride, dst, params->sgr.w1, w, h);
+}
+
 void BF(dav1d_boxsum5_h, lsx)(int32_t *sumsq, int16_t *sum,
                               const uint8_t *const src,
                               const int w, const int h);
diff --git a/src/loongarch/mc.S b/src/loongarch/mc.S
index 97887de..42c595d 100644
--- a/src/loongarch/mc.S
+++ b/src/loongarch/mc.S
@@ -33,64 +33,99 @@ static void warp_affine_8x8_c(pixel *dst, const ptrdiff_t dst_stride,
                               const int16_t *const abcd, int mx, int my
                               HIGHBD_DECL_SUFFIX)
 */
-.macro FILTER_WARP_RND_P_LSX in0, in1, in2, in3, out0, out1, out2, out3
-    vbsrl.v         vr2,    \in0,     \in1
-    vbsrl.v         vr20,   \in0,     \in2
-    addi.w          t4,     \in3,     512
-    srai.w          t4,     t4,       10
-    addi.w          t4,     t4,       64
-    slli.w          t4,     t4,       3
-    vldx            vr1,    t5,       t4
-    add.w           t3,     t3,       t0   // tmx += abcd[0]
-
-    addi.w          t4,     t3,       512
-    srai.w          t4,     t4,       10
-    addi.w          t4,     t4,       64
-    slli.w          t4,     t4,       3
-    vldx            vr29,   t5,       t4
-    add.w           t3,     t3,       t0   // tmx += abcd[0]
-
-    vilvl.d         vr2,    vr20,     vr2
-    vilvl.d         vr1,    vr29,     vr1
-    vmulwev.h.bu.b  vr3,    vr2,      vr1
-    vmulwod.h.bu.b  vr20,   vr2,      vr1
-    vilvl.d         vr2,    vr20,     vr3
-    vhaddw.w.h      vr2,    vr2,      vr2
-    vhaddw.d.w      vr2,    vr2,      vr2
-    vhaddw.q.d      vr2,    vr2,      vr2
-    vilvh.d         vr3,    vr20,     vr3
-    vhaddw.w.h      vr3,    vr3,      vr3
-    vhaddw.d.w      vr3,    vr3,      vr3
-    vhaddw.q.d      vr3,    vr3,      vr3
-    vextrins.w      \out0,  vr2,      \out1
-    vextrins.w      \out2,  vr3,      \out3
+.macro vld_filter_row dst, src, inc
+    addi.w          t3,       \src,     512
+    srai.w          t3,       t3,       10
+    add.w           \src,     \src,     \inc
+    addi.w          t3,       t3,       64
+    slli.w          t3,       t3,       3
+    fldx.d          \dst,     t4,       t3
 .endm
 
-.macro FILTER_WARP_CLIP_LSX in0, in1, in2, out0, out1
-    add.w           \in0,     \in0,    \in1
-    addi.w          t6,       \in0,    512
-    srai.w          t6,       t6,      10
-    addi.w          t6,       t6,      64
-    slli.w          t6,       t6,      3
-    fldx.d          f1,       t5,      t6
-    vsllwil.h.b     vr1,      vr1,     0
-    vmulwev.w.h     vr3,      \in2,    vr1
-    vmaddwod.w.h    vr3,      \in2,    vr1
-    vhaddw.d.w      vr3,      vr3,     vr3
-    vhaddw.q.d      vr3,      vr3,     vr3
-    vextrins.w      \out0,    vr3,     \out1
+.macro warp_filter_horz_lsx
+    addi.w          t5,       a5,       0
+    vld             vr10,     a2,       0
+    add.d           a2,       a2,       a3
+
+    vld_filter_row f0, t5, t0
+    vld_filter_row f1, t5, t0
+    vld_filter_row f2, t5, t0
+    vld_filter_row f3, t5, t0
+    vld_filter_row f4, t5, t0
+    vld_filter_row f5, t5, t0
+    vld_filter_row f6, t5, t0
+    vld_filter_row f7, t5, t0
+
+    vxor.v          vr10,     vr10,     vr20
+
+    vbsrl.v         vr8,      vr10,     1
+    vbsrl.v         vr9,      vr10,     2
+    vilvl.d         vr8,      vr8,      vr10
+    vilvl.d         vr0,      vr1,      vr0
+    vmulwev.h.b     vr11,     vr8,      vr0
+    vmulwod.h.b     vr12,     vr8,      vr0
+    vbsrl.v         vr8,      vr10,     3
+    vbsrl.v         vr19,     vr10,     4
+    vilvl.d         vr8,      vr8,      vr9
+    vilvl.d         vr2,      vr3,      vr2
+    vmulwev.h.b     vr13,     vr8,      vr2
+    vmulwod.h.b     vr14,     vr8,      vr2
+    vbsrl.v         vr8,      vr10,     5
+    vbsrl.v         vr9,      vr10,     6
+    vilvl.d         vr8,      vr8,      vr19
+    vilvl.d         vr4,      vr5,      vr4
+    vmulwev.h.b     vr15,     vr8,      vr4
+    vmulwod.h.b     vr16,     vr8,      vr4
+    vbsrl.v         vr8,      vr10,     7
+    vilvl.d         vr8,      vr8,      vr9
+    vilvl.d         vr6,      vr7,      vr6
+    vmulwev.h.b     vr17,     vr8,      vr6
+    vmulwod.h.b     vr18,     vr8,      vr6
+
+    vadd.h          vr11,     vr11,     vr12
+    vadd.h          vr13,     vr13,     vr14
+    vadd.h          vr15,     vr15,     vr16
+    vadd.h          vr17,     vr17,     vr18
+    vpickev.h       vr12,     vr13,     vr11
+    vpickod.h       vr14,     vr13,     vr11
+    vpickev.h       vr16,     vr17,     vr15
+    vpickod.h       vr18,     vr17,     vr15
+    vadd.h          vr11,     vr12,     vr14
+    vadd.h          vr15,     vr16,     vr18
+    vpickev.h       vr12,     vr15,     vr11
+    vpickod.h       vr14,     vr15,     vr11
+    vadd.h          vr11,     vr12,     vr14
+
+    add.d           a5,       a5,       t1
 .endm
 
-const warp_sh
-.rept 2
-.byte 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17
-.endr
-.rept 2
-.byte 18, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
-.endr
-endconst
+.macro transpose_8x8b_extend_lsx in0, in1, in2, in3, in4, in5, in6, in7
+    vilvl.b         \in0,     \in1,     \in0
+    vilvl.b         \in2,     \in3,     \in2
+    vilvl.b         \in4,     \in5,     \in4
+    vilvl.b         \in6,     \in7,     \in6
+
+    vpackev.h       \in1,     \in2,     \in0
+    vpackod.h       \in3,     \in2,     \in0
+    vpackev.h       \in5,     \in6,     \in4
+    vpackod.h       \in7,     \in6,     \in4
+
+    vpackev.w       \in0,     \in5,     \in1
+    vpackod.w       \in2,     \in5,     \in1
+    vpackev.w       \in1,     \in7,     \in3
+    vpackod.w       \in3,     \in7,     \in3
+
+    vexth.h.b       \in4,     \in0
+    vsllwil.h.b     \in0,     \in0,     0
+    vexth.h.b       \in5,     \in1
+    vsllwil.h.b     \in1,     \in1,     0
+    vexth.h.b       \in6,     \in2
+    vsllwil.h.b     \in2,     \in2,     0
+    vexth.h.b       \in7,     \in3
+    vsllwil.h.b     \in3,     \in3,     0
+.endm
 
-.macro warp_lsx t, shift
+.macro warp t, shift
 function warp_affine_8x8\t\()_8bpc_lsx
     addi.d          sp,       sp,      -64
     fst.d           f24,      sp,      0
@@ -102,417 +137,119 @@ function warp_affine_8x8\t\()_8bpc_lsx
     fst.d           f30,      sp,      48
     fst.d           f31,      sp,      56
 
-    la.local        t4,       warp_sh
-    ld.h            t0,       a4,      0   // abcd[0]
-    ld.h            t1,       a4,      2   // abcd[1]
+    ld.h            t0,       a4,      0
+    ld.h            t1,       a4,      2
+    ld.h            t2,       a4,      4
+    ld.h            a4,       a4,      6
 
-    alsl.w          t2,       a3,      a3,     1
-    addi.w          t3,       a5,      0
-    la.local        t5,       dav1d_mc_warp_filter
-    sub.d           a2,       a2,      t2
+    li.d            t7,       8
+    alsl.w          t3,       a3,      a3,     1
+    sub.d           a2,       a2,      t3
     addi.d          a2,       a2,      -3
-    vld             vr0,      a2,      0
-    vld             vr30,     t4,      0
-    vld             vr31,     t4,      32
-
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr4, 0x00, vr5, 0x00
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr6, 0x00, vr7, 0x00
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr8, 0x00, vr9, 0x00
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr10, 0x00, vr11, 0x00
-
-    add.w           a5,       a5,      t1
-    or              t3,       a5,      a5
-    add.d           a2,       a2,      a3
-    vld             vr0,      a2,      0
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr4, 0x10, vr5, 0x10
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr6, 0x10, vr7, 0x10
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr8, 0x10, vr9, 0x10
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr10, 0x10, vr11, 0x10
-
-    add.w           a5,       a5,      t1
-    or              t3,       a5,      a5
-    add.d           a2,       a2,      a3
-    vld             vr0,      a2,      0
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr4, 0x20, vr5, 0x20
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr6, 0x20, vr7, 0x20
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr8, 0x20, vr9, 0x20
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr10, 0x20, vr11, 0x20
-
-    add.w           a5,       a5,      t1
-    or              t3,       a5,      a5
-    add.d           a2,       a2,      a3
-    vld             vr0,      a2,      0
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr4, 0x30, vr5, 0x30
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr6, 0x30, vr7, 0x30
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr8, 0x30, vr9, 0x30
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr10, 0x30, vr11, 0x30
-
-    add.w           a5,       t1,      a5
-    or              t3,       a5,      a5
-    add.d           a2,       a2,      a3
-    vld             vr0,      a2,      0
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr12, 0x00, vr13, 0x00
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr14, 0x00, vr15, 0x00
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr16, 0x00, vr17, 0x00
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr18, 0x00, vr19, 0x00
-
-    add.w           a5,       a5,      t1
-    or              t3,       a5,      a5
-    add.d           a2,       a2,      a3
-    vld             vr0,      a2,      0
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr12, 0x10, vr13, 0x10
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr14, 0x10, vr15, 0x10
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr16, 0x10, vr17, 0x10
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr18, 0x10, vr19, 0x10
-
-    add.w           a5,       a5,      t1
-    or              t3,       a5,      a5
-    add.d           a2,       a2,      a3
-    vld             vr0,      a2,      0
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr12, 0x20, vr13, 0x20
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr14, 0x20, vr15, 0x20
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr16, 0x20, vr17, 0x20
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr18, 0x20, vr19, 0x20
-
-    add.w           a5,       a5,      t1
-    or              t3,       a5,      a5
-    add.d           a2,       a2,      a3
-    vld             vr0,      a2,      0
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr12, 0x30, vr13, 0x30
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr14, 0x30, vr15, 0x30
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr16, 0x30, vr17, 0x30
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr18, 0x30, vr19, 0x30
-
-    vsrarni.h.w       vr12,     vr4,     3
-    vsrarni.h.w       vr13,     vr5,     3
-    vsrarni.h.w       vr14,     vr6,     3
-    vsrarni.h.w       vr15,     vr7,     3
-    vsrarni.h.w       vr16,     vr8,     3
-    vsrarni.h.w       vr17,     vr9,     3
-    vsrarni.h.w       vr18,     vr10,    3
-    vsrarni.h.w       vr19,     vr11,    3
-
-    add.w           a5,       a5,      t1
-    or              t3,       a5,      a5
-    add.d           a2,       a2,      a3
-    vld             vr0,      a2,      0
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr4, 0x00, vr5, 0x00
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr6, 0x00, vr7, 0x00
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr8, 0x00, vr9, 0x00
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr10, 0x00, vr11, 0x00
-
-    add.w           a5,       a5,      t1
-    or              t3,       a5,      a5
-    add.d           a2,       a2,      a3
-    vld             vr0,      a2,      0
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr4, 0x10, vr5, 0x10
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr6, 0x10, vr7, 0x10
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr8, 0x10, vr9, 0x10
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr10, 0x10, vr11, 0x10
-
-    add.w           a5,       a5,      t1
-    or              t3,       a5,      a5
-    add.d           a2,       a2,      a3
-    vld             vr0,      a2,      0
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr4, 0x20, vr5, 0x20
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr6, 0x20, vr7, 0x20
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr8, 0x20, vr9, 0x20
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr10, 0x20, vr11, 0x20
-
-    add.w           a5,       a5,      t1
-    or              t3,       a5,      a5
-    add.d           a2,       a2,      a3
-    vld             vr0,      a2,      0
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr4, 0x30, vr5, 0x30
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr6, 0x30, vr7, 0x30
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr8, 0x30, vr9, 0x30
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr10, 0x30, vr11, 0x30
-
-    add.w           a5,       a5,      t1
-    or              t3,       a5,      a5
-    add.d           a2,       a2,      a3
-    vld             vr0,      a2,      0
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr21, 0x00, vr22, 0x00
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr23, 0x00, vr24, 0x00
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr25, 0x00, vr26, 0x00
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr27, 0x00, vr28, 0x00
-
-    add.w           a5,       a5,      t1
-    or              t3,       a5,      a5
-    add.d           a2,       a2,      a3
-    vld             vr0,      a2,      0
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr21, 0x10, vr22, 0x10
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr23, 0x10, vr24, 0x10
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr25, 0x10, vr26, 0x10
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr27, 0x10, vr28, 0x10
-
-    add.w           a5,       a5,      t1
-    or              t3,       a5,      a5
-    add.d           a2,       a2,      a3
-    vld             vr0,      a2,      0
-    FILTER_WARP_RND_P_LSX   vr0, 0, 1, a5, vr21, 0x20, vr22, 0x20
-    FILTER_WARP_RND_P_LSX   vr0, 2, 3, t3, vr23, 0x20, vr24, 0x20
-    FILTER_WARP_RND_P_LSX   vr0, 4, 5, t3, vr25, 0x20, vr26, 0x20
-    FILTER_WARP_RND_P_LSX   vr0, 6, 7, t3, vr27, 0x20, vr28, 0x20
-
-    vsrarni.h.w     vr21,     vr4,     3
-    vsrarni.h.w     vr22,     vr5,     3
-    vsrarni.h.w     vr23,     vr6,     3
-    vsrarni.h.w     vr24,     vr7,     3
-    vsrarni.h.w     vr25,     vr8,     3
-    vsrarni.h.w     vr26,     vr9,     3
-    vsrarni.h.w     vr27,     vr10,    3
-    vsrarni.h.w     vr28,     vr11,    3
-
-    addi.w          t2,       a6,      0   // my
-    ld.h            t7,       a4,      4   // abcd[2]
-    ld.h            t8,       a4,      6   // abcd[3]
+    la.local        t4,       dav1d_mc_warp_filter
 
 .ifnb \t
     slli.d          a1,       a1,      1
 .endif
 
-    FILTER_WARP_CLIP_LSX  t2, zero, vr12,  vr4, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr13,  vr4, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr14,  vr4, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr15,  vr4, 0x30
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr16,  vr5, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr17,  vr5, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr18,  vr5, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr19,  vr5, 0x30
-.ifnb \t
-    vssrarni.h.w    vr5,      vr4,     \shift
-    vst             vr5,      a0,      0
+    li.w            t3,       128
+    vreplgr2vr.b    vr20,     t3
+.ifb \t
+    vreplgr2vr.h    vr21,     t3
 .else
-    vssrarni.hu.w   vr5,      vr4,     \shift
-    vssrlni.bu.h    vr5,      vr5,     0
-    fst.d           f5,       a0,      0
+    li.w            t3,       2048
+    vreplgr2vr.h    vr21,     t3
 .endif
-
-    vshuf.b         vr12,     vr21,    vr12,   vr30
-    vshuf.b         vr13,     vr22,    vr13,   vr30
-    vshuf.b         vr14,     vr23,    vr14,   vr30
-    vshuf.b         vr15,     vr24,    vr15,   vr30
-    vshuf.b         vr16,     vr25,    vr16,   vr30
-    vshuf.b         vr17,     vr26,    vr17,   vr30
-    vshuf.b         vr18,     vr27,    vr18,   vr30
-    vshuf.b         vr19,     vr28,    vr19,   vr30
-    vextrins.h      vr30,     vr31,    0x70
-
-    add.w           a6,       a6,      t8
-    addi.w          t2,       a6,      0
-    FILTER_WARP_CLIP_LSX  t2, zero, vr12,  vr4, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr13,  vr4, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr14,  vr4, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr15,  vr4, 0x30
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr16,  vr5, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr17,  vr5, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr18,  vr5, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr19,  vr5, 0x30
-.ifnb \t
-    vssrarni.h.w    vr5,      vr4,     \shift
-    vstx            vr5,      a0,      a1
-.else
-    vssrarni.hu.w   vr5,      vr4,     \shift
-    vssrlni.bu.h    vr5,      vr5,     0
-    fstx.d          f5,       a0,      a1
+    warp_filter_horz_lsx
+    vsrari.h        vr24,     vr11,    3
+    warp_filter_horz_lsx
+    vsrari.h        vr25,     vr11,    3
+    warp_filter_horz_lsx
+    vsrari.h        vr26,     vr11,    3
+    warp_filter_horz_lsx
+    vsrari.h        vr27,     vr11,    3
+    warp_filter_horz_lsx
+    vsrari.h        vr28,     vr11,    3
+    warp_filter_horz_lsx
+    vsrari.h        vr29,     vr11,    3
+    warp_filter_horz_lsx
+    vsrari.h        vr30,     vr11,    3
+
+1:
+    addi.d          t6,       a6,      0
+    warp_filter_horz_lsx
+    vsrari.h        vr31,     vr11,    3
+
+    vld_filter_row f0, t6, t2
+    vld_filter_row f1, t6, t2
+    vld_filter_row f2, t6, t2
+    vld_filter_row f3, t6, t2
+    vld_filter_row f4, t6, t2
+    vld_filter_row f5, t6, t2
+    vld_filter_row f6, t6, t2
+    vld_filter_row f7, t6, t2
+
+    transpose_8x8b_extend_lsx vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7
+
+    vmulwev.w.h     vr16,     vr24,    vr0
+    vmulwod.w.h     vr17,     vr24,    vr0
+    vmaddwev.w.h    vr16,     vr25,    vr1
+    vmaddwod.w.h    vr17,     vr25,    vr1
+    vmaddwev.w.h    vr16,     vr26,    vr2
+    vmaddwod.w.h    vr17,     vr26,    vr2
+    vmaddwev.w.h    vr16,     vr27,    vr3
+    vmaddwod.w.h    vr17,     vr27,    vr3
+    vmaddwev.w.h    vr16,     vr28,    vr4
+    vmaddwod.w.h    vr17,     vr28,    vr4
+    vmaddwev.w.h    vr16,     vr29,    vr5
+    vmaddwod.w.h    vr17,     vr29,    vr5
+    vmaddwev.w.h    vr16,     vr30,    vr6
+    vmaddwod.w.h    vr17,     vr30,    vr6
+    vmaddwev.w.h    vr16,     vr31,    vr7
+    vmaddwod.w.h    vr17,     vr31,    vr7
+
+    vssrarni.h.w    vr16,     vr16,    \shift
+    vssrarni.h.w    vr17,     vr17,    \shift
+    vilvl.h         vr16,     vr17,    vr16
+    vadd.h          vr16,     vr16,    vr21
+
+    vor.v           vr24,     vr25,    vr25
+    vor.v           vr25,     vr26,    vr26
+    vor.v           vr26,     vr27,    vr27
+    vor.v           vr27,     vr28,    vr28
+    vor.v           vr28,     vr29,    vr29
+    vor.v           vr29,     vr30,    vr30
+    vor.v           vr30,     vr31,    vr31
+
+.ifb \t
+    vssrarni.bu.h   vr16,     vr16,    0
 .endif
 
-    vaddi.bu        vr31,     vr31,    2
-    vshuf.b         vr12,     vr21,    vr12,   vr30
-    vshuf.b         vr13,     vr22,    vr13,   vr30
-    vshuf.b         vr14,     vr23,    vr14,   vr30
-    vshuf.b         vr15,     vr24,    vr15,   vr30
-    vshuf.b         vr16,     vr25,    vr16,   vr30
-    vshuf.b         vr17,     vr26,    vr17,   vr30
-    vshuf.b         vr18,     vr27,    vr18,   vr30
-    vshuf.b         vr19,     vr28,    vr19,   vr30
-    vextrins.h      vr30,     vr31,    0x70
-
-    add.w           a6,       a6,      t8
-    addi.w          t2,       a6,      0
-    FILTER_WARP_CLIP_LSX  t2, zero, vr12,  vr4, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr13,  vr4, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr14,  vr4, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr15,  vr4, 0x30
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr16,  vr5, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr17,  vr5, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr18,  vr5, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr19,  vr5, 0x30
-    alsl.d          a0,       a1,      a0,   1
-.ifnb \t
-    vssrarni.h.w    vr5,      vr4,     \shift
-    vst             vr5,      a0,      0
-.else
-    vssrarni.hu.w   vr5,      vr4,     \shift
-    vssrlni.bu.h    vr5,      vr5,     0
-    fst.d           f5,       a0,      0
-.endif
-
-    vaddi.bu        vr31,     vr31,    2
-    vshuf.b         vr12,     vr21,    vr12,   vr30
-    vshuf.b         vr13,     vr22,    vr13,   vr30
-    vshuf.b         vr14,     vr23,    vr14,   vr30
-    vshuf.b         vr15,     vr24,    vr15,   vr30
-    vshuf.b         vr16,     vr25,    vr16,   vr30
-    vshuf.b         vr17,     vr26,    vr17,   vr30
-    vshuf.b         vr18,     vr27,    vr18,   vr30
-    vshuf.b         vr19,     vr28,    vr19,   vr30
-    vextrins.h      vr30,     vr31,    0x70
-
-    add.w           a6,       a6,       t8
-    addi.w          t2,       a6,       0
-    FILTER_WARP_CLIP_LSX  t2, zero, vr12,  vr4, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr13,  vr4, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr14,  vr4, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr15,  vr4, 0x30
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr16,  vr5, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr17,  vr5, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr18,  vr5, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr19,  vr5, 0x30
+    addi.d          t7,       t7,      -1
 .ifnb \t
-    vssrarni.h.w    vr5,      vr4,      \shift
-    vstx            vr5,      a0,       a1
+    vst             vr16,     a0,      0
 .else
-    vssrarni.hu.w   vr5,      vr4,      \shift
-    vssrlni.bu.h    vr5,      vr5,      0
-    fstx.d          f5,       a0,       a1
+    vstelm.d        vr16,     a0,      0,   0
 .endif
-
-    vaddi.bu        vr31,     vr31,    2
-    vshuf.b         vr12,     vr21,    vr12,   vr30
-    vshuf.b         vr13,     vr22,    vr13,   vr30
-    vshuf.b         vr14,     vr23,    vr14,   vr30
-    vshuf.b         vr15,     vr24,    vr15,   vr30
-    vshuf.b         vr16,     vr25,    vr16,   vr30
-    vshuf.b         vr17,     vr26,    vr17,   vr30
-    vshuf.b         vr18,     vr27,    vr18,   vr30
-    vshuf.b         vr19,     vr28,    vr19,   vr30
-    vextrins.h      vr30,     vr31,    0x70
-
-    add.w           a6,       a6,       t8
-    addi.w          t2,       a6,       0
-    FILTER_WARP_CLIP_LSX  t2, zero, vr12,  vr4, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr13,  vr4, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr14,  vr4, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr15,  vr4, 0x30
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr16,  vr5, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr17,  vr5, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr18,  vr5, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr19,  vr5, 0x30
-    alsl.d          a0,       a1,       a0,   1
-.ifnb \t
-    vssrarni.h.w    vr5,      vr4,      \shift
-    vst             vr5,      a0,       0
-.else
-    vssrarni.hu.w   vr5,      vr4,      \shift
-    vssrlni.bu.h    vr5,      vr5,      0
-    fst.d           f5,       a0,       0
-.endif
-
-    vaddi.bu        vr31,     vr31,    2
-    vshuf.b         vr12,     vr21,    vr12,   vr30
-    vshuf.b         vr13,     vr22,    vr13,   vr30
-    vshuf.b         vr14,     vr23,    vr14,   vr30
-    vshuf.b         vr15,     vr24,    vr15,   vr30
-    vshuf.b         vr16,     vr25,    vr16,   vr30
-    vshuf.b         vr17,     vr26,    vr17,   vr30
-    vshuf.b         vr18,     vr27,    vr18,   vr30
-    vshuf.b         vr19,     vr28,    vr19,   vr30
-    vextrins.h      vr30,     vr31,    0x70
-
-    add.w           a6,       a6,       t8
-    addi.w          t2,       a6,       0
-    FILTER_WARP_CLIP_LSX  t2, zero, vr12,  vr4, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr13,  vr4, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr14,  vr4, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr15,  vr4, 0x30
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr16,  vr5, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr17,  vr5, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr18,  vr5, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr19,  vr5, 0x30
-.ifnb \t
-    vssrarni.h.w    vr5,      vr4,      \shift
-    vstx            vr5,      a0,       a1
-.else
-    vssrarni.hu.w   vr5,      vr4,      \shift
-    vssrlni.bu.h    vr5,      vr5,      0
-    fstx.d          f5,       a0,       a1
-.endif
-
-    vaddi.bu        vr31,     vr31,    2
-    vshuf.b         vr12,     vr21,    vr12,   vr30
-    vshuf.b         vr13,     vr22,    vr13,   vr30
-    vshuf.b         vr14,     vr23,    vr14,   vr30
-    vshuf.b         vr15,     vr24,    vr15,   vr30
-    vshuf.b         vr16,     vr25,    vr16,   vr30
-    vshuf.b         vr17,     vr26,    vr17,   vr30
-    vshuf.b         vr18,     vr27,    vr18,   vr30
-    vshuf.b         vr19,     vr28,    vr19,   vr30
-    vextrins.h      vr30,     vr31,    0x70
-
-    add.w           a6,       a6,       t8
-    addi.w          t2,       a6,       0
-    FILTER_WARP_CLIP_LSX  t2, zero, vr12,  vr4, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr13,  vr4, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr14,  vr4, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr15,  vr4, 0x30
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr16,  vr5, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr17,  vr5, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr18,  vr5, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr19,  vr5, 0x30
-    alsl.d          a0,       a1,       a0,   1
-.ifnb \t
-    vssrarni.h.w    vr5,      vr4,      \shift
-    vst             vr5,      a0,       0
-.else
-    vssrarni.hu.w   vr5,      vr4,      \shift
-    vssrlni.bu.h    vr5,      vr5,      0
-    fst.d           f5,       a0,       0
-.endif
-
-    vshuf.b         vr12,     vr21,    vr12,   vr30
-    vshuf.b         vr13,     vr22,    vr13,   vr30
-    vshuf.b         vr14,     vr23,    vr14,   vr30
-    vshuf.b         vr15,     vr24,    vr15,   vr30
-    vshuf.b         vr16,     vr25,    vr16,   vr30
-    vshuf.b         vr17,     vr26,    vr17,   vr30
-    vshuf.b         vr18,     vr27,    vr18,   vr30
-    vshuf.b         vr19,     vr28,    vr19,   vr30
-
-    add.w           a6,       a6,       t8
-    addi.w          t2,       a6,       0
-    FILTER_WARP_CLIP_LSX  t2, zero, vr12,  vr4, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr13,  vr4, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr14,  vr4, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr15,  vr4, 0x30
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr16,  vr5, 0x00
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr17,  vr5, 0x10
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr18,  vr5, 0x20
-    FILTER_WARP_CLIP_LSX  t2, t7,   vr19,  vr5, 0x30
-.ifnb \t
-    vssrarni.h.w    vr5,      vr4,      \shift
-    vstx            vr5,      a0,       a1
-.else
-    vssrarni.hu.w   vr5,      vr4,      \shift
-    vssrlni.bu.h    vr5,      vr5,      0
-    fstx.d          f5,       a0,       a1
-.endif
-
-    fld.d           f24,      sp,       0
-    fld.d           f25,      sp,       8
-    fld.d           f26,      sp,       16
-    fld.d           f27,      sp,       24
-    fld.d           f28,      sp,       32
-    fld.d           f29,      sp,       40
-    fld.d           f30,      sp,       48
-    fld.d           f31,      sp,       56
-    addi.d          sp,       sp,       64
+    add.d           a0,       a1,      a0
+
+    add.d           a6,       a6,      a4
+    blt             zero,     t7,      1b
+
+    fld.d           f24,      sp,      0
+    fld.d           f25,      sp,      8
+    fld.d           f26,      sp,      16
+    fld.d           f27,      sp,      24
+    fld.d           f28,      sp,      32
+    fld.d           f29,      sp,      40
+    fld.d           f30,      sp,      48
+    fld.d           f31,      sp,      56
+    addi.d          sp,       sp,      64
 endfunc
 .endm
 
-warp_lsx , 11
-warp_lsx t, 7
+warp  , 11
+warp t, 7
 
 .macro FILTER_WARP_RND_P_LASX in0, in1, in2, out0, out1, out2, out3
     xvshuf.b        xr2,    \in0,     \in0,     \in2
@@ -593,6 +330,15 @@ const shuf0
 .byte  1, 2, 3, 4, 5, 6, 7, 8, 3, 4, 5, 6, 7, 8, 9, 10
 endconst
 
+const warp_sh
+.rept 2
+.byte 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17
+.endr
+.rept 2
+.byte 18, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
+.endr
+endconst
+
 .macro warp_lasx t, shift
 function warp_affine_8x8\t\()_8bpc_lasx
     addi.d          sp,       sp,      -16
@@ -2634,114 +2380,57 @@ endfunc
     vhaddw.q.d  \in0,  \in0,  \in0
 .endm
 .macro PUT_H_8W in0
-    vbsrl.v          vr2,    \in0,  1
-    vbsrl.v          vr3,    \in0,  2
-    vbsrl.v          vr4,    \in0,  3
-    vbsrl.v          vr5,    \in0,  4
-    vbsrl.v          vr6,    \in0,  5
-    vbsrl.v          vr7,    \in0,  6
-    vbsrl.v          vr10,   \in0,  7
-    vilvl.d          vr2,    vr2,   \in0
-    vilvl.d          vr3,    vr4,   vr3
-    vilvl.d          vr4,    vr6,   vr5
-    vilvl.d          vr5,    vr10,  vr7
-    vdp2.h.bu.b      \in0,   vr2,   vr8
-    vdp2.h.bu.b      vr2,    vr3,   vr8
-    vdp2.h.bu.b      vr3,    vr4,   vr8
-    vdp2.h.bu.b      vr4,    vr5,   vr8
-    vhaddw.d.h       \in0
-    vhaddw.d.h       vr2
-    vhaddw.d.h       vr3
-    vhaddw.d.h       vr4
-    vpickev.w        \in0,   vr2,   \in0
-    vpickev.w        vr2,    vr4,   vr3
-    vpickev.h        \in0,   vr2,   \in0
+    vshuf.b          vr2,    \in0,  \in0,   vr6
+    vshuf.b          vr3,    \in0,  \in0,   vr7
+    vshuf.b          vr4,    \in0,  \in0,   vr8
+    vmulwev.h.bu.b   vr12,   vr2,   vr10
+    vmulwev.h.bu.b   vr13,   vr3,   vr11
+    vmulwev.h.bu.b   vr14,   vr3,   vr10
+    vmulwev.h.bu.b   vr15,   vr4,   vr11
+    vmaddwod.h.bu.b  vr12,   vr2,   vr10
+    vmaddwod.h.bu.b  vr13,   vr3,   vr11
+    vmaddwod.h.bu.b  vr14,   vr3,   vr10
+    vmaddwod.h.bu.b  vr15,   vr4,   vr11
+    vadd.h           vr12,   vr12,  vr13
+    vadd.h           vr14,   vr14,  vr15
+    vhaddw.w.h       vr12,   vr12,  vr12
+    vhaddw.w.h       vr14,   vr14,  vr14
+    vpickev.h        \in0,   vr14,  vr12
     vadd.h           \in0,   \in0,  vr9
 .endm
-.macro FILTER_8TAP_4W in0
-    vbsrl.v          vr10,   \in0,  1
-    vbsrl.v          vr11,   \in0,  2
-    vbsrl.v          vr12,   \in0,  3
-    vilvl.d          vr10,   vr10, \in0
-    vilvl.d          vr11,   vr12,  vr11
-    vdp2.h.bu.b      vr7,    vr10,  vr8
-    vdp2.h.bu.b      vr10,   vr11,  vr8
-    vhaddw.d.h       vr7
-    vhaddw.d.h       vr10
-    vpickev.w        \in0,   vr10,  vr7
-.endm
+
+const subpel_h_shuf0
+.byte 0, 1, 2, 3, 1, 2, 3, 4, 16, 17, 18, 19, 17, 18, 19, 20
+endconst
+const subpel_h_shuf1
+.byte 0, 1, 2, 3, 1, 2, 3, 4, 2, 3, 4, 5, 3, 4, 5, 6
+endconst
+const subpel_h_shuf2
+.byte 0, 1, 2, 3, 1, 2, 3, 4,  8,  9, 10, 11,  9, 10, 11, 12
+.byte 2, 3, 4, 5, 3, 4, 5, 6, 10, 11, 12, 13, 11, 12, 13, 14
+endconst
+const subpel_h_shuf3
+.byte 0, 4, 1, 5, 2, 6, 3, 7, 4, 8, 5, 9, 6, 10, 7, 11
+.byte 0, 4, 1, 5, 2, 6, 3, 7, 4, 8, 5, 9, 6, 10, 7, 11
+endconst
+
 .macro FILTER_8TAP_8W in0
-    vbsrl.v         vr10,    \in0,  1
-    vbsrl.v         vr11,    \in0,  2
-    vbsrl.v         vr12,    \in0,  3
-    vbsrl.v         vr13,    \in0,  4
-    vbsrl.v         vr14,    \in0,  5
-    vbsrl.v         vr15,    \in0,  6
-    vbsrl.v         vr16,    \in0,  7
-    vilvl.d         vr10,    vr10,  \in0
-    vilvl.d         vr11,    vr12,  vr11
-    vilvl.d         vr12,    vr14,  vr13
-    vilvl.d         vr13,    vr16,  vr15
-    vdp2.h.bu.b     vr14,    vr10,  vr8
-    vdp2.h.bu.b     vr15,    vr11,  vr8
-    vdp2.h.bu.b     vr16,    vr12,  vr8
-    vdp2.h.bu.b     vr17,    vr13,  vr8
-    vhaddw.d.h      vr14
-    vhaddw.d.h      vr15
-    vhaddw.d.h      vr16
-    vhaddw.d.h      vr17
-    vpickev.w       vr13,    vr15,  vr14
-    vpickev.w       vr14,    vr17,  vr16
-    vpickev.h       \in0,    vr14,  vr13 //x0 ... x7
-    vsrari.h        \in0,    \in0,  2
-.endm
-.macro FILTER_8TAP_8W_CLIP_STORE
-    vdp2.w.h        vr12,    vr0,   vr9
-    vdp2.w.h        vr13,    vr1,   vr9
-    vdp2.w.h        vr14,    vr2,   vr9
-    vdp2.w.h        vr15,    vr3,   vr9
-    vdp2.w.h        vr16,    vr4,   vr9
-    vdp2.w.h        vr17,    vr5,   vr9
-    vdp2.w.h        vr18,    vr6,   vr9
-    vdp2.w.h        vr19,    vr7,   vr9
-    vhaddw.q.w      vr12
-    vhaddw.q.w      vr13
-    vhaddw.q.w      vr14
-    vhaddw.q.w      vr15
-    vhaddw.q.w      vr16
-    vhaddw.q.w      vr17
-    vhaddw.q.w      vr18
-    vhaddw.q.w      vr19
-    vpackev.w       vr12,    vr13,  vr12
-    vpackev.w       vr13,    vr15,  vr14
-    vpackev.d       vr12,    vr13,  vr12
-    vpackev.w       vr14,    vr17,  vr16
-    vpackev.w       vr15,    vr19,  vr18
-    vpackev.d       vr13,    vr15,  vr14
-    vssrarni.hu.w   vr13,    vr12,  10
-    vssrani.bu.h    vr13,    vr13,  0
-    vstelm.d        vr13,    a0,    0,   0
-    add.d           a0,      a0,    a1
-.endm
-.macro VEXTRINS_Hx8 in0
-    vextrins.h      vr0,     \in0,  0x70
-    vextrins.h      vr1,     \in0,  0x71
-    vextrins.h      vr2,     \in0,  0x72
-    vextrins.h      vr3,     \in0,  0x73
-    vextrins.h      vr4,     \in0,  0x74
-    vextrins.h      vr5,     \in0,  0x75
-    vextrins.h      vr6,     \in0,  0x76
-    vextrins.h      vr7,     \in0,  0x77
-.endm
-.macro VBSRL_Vx8
-    vbsrl.v         vr0,     vr0,   2
-    vbsrl.v         vr1,     vr1,   2
-    vbsrl.v         vr2,     vr2,   2
-    vbsrl.v         vr3,     vr3,   2
-    vbsrl.v         vr4,     vr4,   2
-    vbsrl.v         vr5,     vr5,   2
-    vbsrl.v         vr6,     vr6,   2
-    vbsrl.v         vr7,     vr7,   2
+    vshuf.b         vr13,    \in0,  \in0,  vr7
+    vshuf.b         vr14,    \in0,  \in0,  vr11
+    vshuf.b         vr15,    \in0,  \in0,  vr12
+    vmulwev.h.bu.b  vr16,    vr13,  vr8
+    vmulwev.h.bu.b  vr17,    vr14,  vr10
+    vmulwev.h.bu.b  vr18,    vr14,  vr8
+    vmulwev.h.bu.b  vr19,    vr15,  vr10
+    vmaddwod.h.bu.b vr16,    vr13,  vr8
+    vmaddwod.h.bu.b vr17,    vr14,  vr10
+    vmaddwod.h.bu.b vr18,    vr14,  vr8
+    vmaddwod.h.bu.b vr19,    vr15,  vr10
+    vadd.h          vr16,    vr16,  vr17
+    vadd.h          vr18,    vr18,  vr19
+    vhaddw.w.h      vr16,    vr16,  vr16
+    vhaddw.w.h      \in0,    vr18,  vr18
+    vssrarni.h.w    \in0,    vr16,  2
 .endm
 
 .macro PUT_8TAP_8BPC_LSX lable
@@ -2910,9 +2599,7 @@ endfunc
     addi.w           t5,     a6,    -1
     slli.w           t5,     t5,    3
     add.w            t1,     t1,    t5
-    add.d            t1,     t6,    t1 //fh's offset
-    vldrepl.d        vr8,    t1,    0
-    addi.d           a2,     a2,    -3
+    add.d            t7,     t6,    t1 //fh's offset
     li.w             t1,     34
     vreplgr2vr.h     vr9,    t1
 
@@ -2936,75 +2623,72 @@ endfunc
     .dword .l_\lable\()put_h_2w   - .l_\lable\()put_h_jtable
 
 .l_\lable\()put_h_2w:
+    addi.d           t7,     t7,    2
+    addi.d           a2,     a2,    -1
+    vldrepl.w        vr8,    t7,    0
+    la.local         t7,     subpel_h_shuf0
+    vld              vr7,    t7,    0
+.l_\lable\()put_h_2w_loop:
     vld              vr0,    a2,    0
     vldx             vr1,    a2,    a3
     add.d            a2,     a2,    t2
 
-    vbsrl.v          vr2,    vr0,   1
-    vilvl.d          vr0,    vr2,   vr0
-    vdp2.h.bu.b      vr2,    vr0,   vr8
-    vhaddw.w.h       vr0,    vr2,   vr2
-    vhaddw.d.w       vr0,    vr0,   vr0
-    vbsrl.v          vr2,    vr1,   1
-    vilvl.d          vr1,    vr2,   vr1
-    vdp2.h.bu.b      vr2,    vr1,   vr8
-    vhaddw.w.h       vr1,    vr2,   vr2
-    vhaddw.d.w       vr1,    vr1,   vr1
-    vpickev.w        vr0,    vr1,   vr0
+    vshuf.b          vr0,    vr1,   vr0,   vr7
+    vdp2.h.bu.b      vr1,    vr0,   vr8
+    vhaddw.w.h       vr0,    vr1,   vr1
     vpickev.h        vr0,    vr0,   vr0
     vadd.h           vr0,    vr0,   vr9
     vssrani.bu.h     vr0,    vr0,   6
 
-    vstelm.h         vr0,    a0,    0,   0
+    vstelm.h         vr0,    a0,    0,     0
     add.d            a0,     a0,    a1
-    vstelm.h         vr0,    a0,    0,   1
+    vstelm.h         vr0,    a0,    0,     1
     add.d            a0,     a0,    a1
     addi.w           a5,     a5,    -2
-    bnez             a5,     .l_\lable\()put_h_2w
+    bnez             a5,     .l_\lable\()put_h_2w_loop
     b                .l_\lable\()end_put_8tap
 
 .l_\lable\()put_h_4w:
+    addi.d           t7,     t7,    2
+    addi.d           a2,     a2,    -1
+    vldrepl.w        vr8,    t7,    0
+    la.local         t7,     subpel_h_shuf1
+    vld              vr7,    t7,    0
+.l_\lable\()put_h_4w_loop:
     vld              vr0,    a2,    0
     vldx             vr1,    a2,    a3
     add.d            a2,     a2,    t2
 
-    vbsrl.v          vr2,    vr0,   1
-    vbsrl.v          vr3,    vr0,   2
-    vbsrl.v          vr4,    vr0,   3
-    vilvl.d          vr0,    vr2,   vr0 //x0 x1
-    vilvl.d          vr2,    vr4,   vr3 //x2 x3
-    vdp2.h.bu.b      vr3,    vr0,   vr8
-    vdp2.h.bu.b      vr4,    vr2,   vr8
-    vhaddw.w.h       vr0,    vr3,   vr3
-    vhaddw.d.w       vr0,    vr0,   vr0
-    vhaddw.w.h       vr2,    vr4,   vr4
-    vhaddw.d.w       vr2,    vr2,   vr2
-    vpickev.w        vr5,    vr2,   vr0
-    vbsrl.v          vr2,    vr1,   1
-    vbsrl.v          vr3,    vr1,   2
-    vbsrl.v          vr4,    vr1,   3
-    vilvl.d          vr0,    vr2,   vr1 //x0 x1
-    vilvl.d          vr2,    vr4,   vr3 //x2 x3
-    vdp2.h.bu.b      vr3,    vr0,   vr8
-    vdp2.h.bu.b      vr4,    vr2,   vr8
-    vhaddw.w.h       vr0,    vr3,   vr3
-    vhaddw.d.w       vr0,    vr0,   vr0
-    vhaddw.w.h       vr2,    vr4,   vr4
-    vhaddw.d.w       vr2,    vr2,   vr2
-    vpickev.w        vr6,    vr2,   vr0
-    vpickev.h        vr0,    vr6,   vr5
+    vshuf.b          vr0,    vr0,   vr0,   vr7
+    vshuf.b          vr1,    vr1,   vr1,   vr7
+    vmulwev.h.bu.b   vr2,    vr0,   vr8
+    vmulwev.h.bu.b   vr3,    vr1,   vr8
+    vmaddwod.h.bu.b  vr2,    vr0,   vr8
+    vmaddwod.h.bu.b  vr3,    vr1,   vr8
+    vhaddw.w.h       vr0,    vr2,   vr2
+    vhaddw.w.h       vr1,    vr3,   vr3
+    vpickev.h        vr0,    vr1,   vr0
     vadd.h           vr0,    vr0,   vr9
     vssrani.bu.h     vr0,    vr0,   6
 
-    vstelm.w         vr0,    a0,    0,    0
+    vstelm.w         vr0,    a0,    0,     0
     add.d            a0,     a0,    a1
-    vstelm.w         vr0,    a0,    0,    1
+    vstelm.w         vr0,    a0,    0,     1
     add.d            a0,     a0,    a1
     addi.d           a5,     a5,    -2
-    bnez             a5,     .l_\lable\()put_h_4w
+    bnez             a5,     .l_\lable\()put_h_4w_loop
     b                .l_\lable\()end_put_8tap
 
 .l_\lable\()put_h_8w:
+    fld.d            f10,    t7,    0
+    vreplvei.w       vr11,   vr10,  1
+    vreplvei.w       vr10,   vr10,  0
+    la.local         t7,     subpel_h_shuf1
+    vld              vr6,    t7,    0
+    vaddi.bu         vr7,    vr6,   4
+    vaddi.bu         vr8,    vr6,   8
+    addi.d           a2,     a2,    -3
+.l_\lable\()put_h_8w_loop:
     vld              vr0,    a2,    0
     vldx             vr1,    a2,    a3
     add.d            a2,     a2,    t2
@@ -3016,35 +2700,41 @@ endfunc
     vstelm.d         vr1,    a0,    0,    1
     add.d            a0,     a0,    a1
     addi.w           a5,     a5,    -2
-    bnez             a5,     .l_\lable\()put_h_8w
+    bnez             a5,     .l_\lable\()put_h_8w_loop
     b                .l_\lable\()end_put_8tap
 
 .l_\lable\()put_h_16w:
 .l_\lable\()put_h_32w:
 .l_\lable\()put_h_64w:
 .l_\lable\()put_h_128w:
+    fld.d            f10,    t7,    0
+    vreplvei.w       vr11,   vr10,  1
+    vreplvei.w       vr10,   vr10,  0
+    la.local         t7,     subpel_h_shuf1
+    vld              vr6,    t7,    0
+    vaddi.bu         vr7,    vr6,   4
+    vaddi.bu         vr8,    vr6,   8
+    addi.d           a2,     a2,    -3
     addi.d           t0,     a2,    0 //src
     addi.w           t5,     a5,    0 //h
     addi.d           t8,     a0,    0 //dst
 .l_\lable\()put_h_16w_loop:
     vld              vr0,    a2,    0
-    vldx             vr1,    a2,    a3
-    add.d            a2,     a2,    t2
+    vld              vr1,    a2,    8
+    add.d            a2,     a2,    a3
     PUT_H_8W         vr0
     PUT_H_8W         vr1
     vssrani.bu.h     vr1,    vr0,   6
-    vstelm.d         vr1,    a0,    0,   0
-    add.d            a0,     a0,    a1
-    vstelm.d         vr1,    a0,    0,   1
+    vst              vr1,    a0,    0
     add.d            a0,     a0,    a1
-    addi.d           a5,     a5,    -2
+    addi.d           a5,     a5,    -1
     bnez             a5,     .l_\lable\()put_h_16w_loop
-    addi.d           a2,     t0,    8
-    addi.d           t0,     t0,    8
-    addi.d           a0,     t8,    8
-    addi.d           t8,     t8,    8
+    addi.d           a2,     t0,    16
+    addi.d           t0,     t0,    16
+    addi.d           a0,     t8,    16
+    addi.d           t8,     t8,    16
     addi.w           a5,     t5,    0
-    addi.w           a4,     a4,    -8
+    addi.w           a4,     a4,    -16
     bnez             a4,     .l_\lable\()put_h_16w_loop
     b                .l_\lable\()end_put_8tap
 
@@ -3065,6 +2755,12 @@ endfunc
     vldrepl.d        vr8,    t1,    0
     sub.d            a2,     a2,    t3
 
+    vilvl.h          vr8,    vr8,   vr8
+    vreplvei.w       vr9,    vr8,   1
+    vreplvei.w       vr10,   vr8,   2
+    vreplvei.w       vr11,   vr8,   3
+    vreplvei.w       vr8,    vr8,   0
+
     clz.w            t1,     a4
     li.w             t5,     24
     sub.w            t1,     t1,    t5
@@ -3094,36 +2790,43 @@ endfunc
     fldx.s           f5,     a2,    t2
     fldx.s           f6,     a2,    t3
     add.d            a2,     a2,    t4
-    vilvl.b          vr0,    vr1,   vr0
-    vilvl.b          vr1,    vr3,   vr2
-    vilvl.b          vr2,    vr5,   vr4
-    vilvl.b          vr3,    vr7,   vr6
-    vilvl.h          vr0,    vr1,   vr0
-    vilvl.h          vr1,    vr3,   vr2
-    vilvl.w          vr0,    vr1,   vr0
 
+    vilvl.h          vr0,    vr1,   vr0 //0 1
+    vilvl.h          vr1,    vr2,   vr1 //1 2
+    vilvl.b          vr0,    vr1,   vr0 //01 12
+    vilvl.h          vr2,    vr3,   vr2 //2 3
+    vilvl.h          vr3,    vr4,   vr3 //3 4
+    vilvl.b          vr1,    vr3,   vr2 //23 34
+    vilvl.h          vr2,    vr5,   vr4 //4 5
+    vilvl.h          vr3,    vr6,   vr5 //5 6
+    vilvl.b          vr2,    vr3,   vr2 //45 56
 .l_\lable\()put_v_2w_loop:
-    fld.s            f7,     a2,    0  //h0
-    fldx.s           f10,    a2,    a3 //h1
+    fld.s            f7,     a2,    0
+    vilvl.h          vr3,    vr7,   vr6 //6 7
+    fldx.s           f6,     a2,    a3
     add.d            a2,     a2,    t2
-
-    vextrins.b       vr0,    vr7,   0x70
-    vextrins.b       vr0,    vr7,   0xf1
-    vbsrl.v          vr1,    vr0,   1
-    vextrins.b       vr1,    vr10,  0x70
-    vextrins.b       vr1,    vr10,  0xf1
-    vdp2.h.bu.b      vr10,   vr0,   vr8
-    vdp2.h.bu.b      vr11,   vr1,   vr8
-    vbsrl.v          vr0,    vr1,   1
-    vhaddw.d.h       vr10
-    vhaddw.d.h       vr11
-    vpickev.w        vr10,   vr11,  vr10
-    vssrarni.hu.w    vr10,   vr10,  6
-    vssrani.bu.h     vr10,   vr10,  0
-
-    vstelm.h         vr10,   a0,    0,   0
+    vilvl.h          vr4,    vr6,   vr7 //7 8
+    vilvl.b          vr3,    vr4,   vr3 //67 78
+
+    vmulwev.h.bu.b   vr12,   vr0,   vr8
+    vmulwev.h.bu.b   vr13,   vr1,   vr9
+    vmulwev.h.bu.b   vr14,   vr2,   vr10
+    vmulwev.h.bu.b   vr15,   vr3,   vr11
+    vmaddwod.h.bu.b  vr12,   vr0,   vr8
+    vmaddwod.h.bu.b  vr13,   vr1,   vr9
+    vmaddwod.h.bu.b  vr14,   vr2,   vr10
+    vmaddwod.h.bu.b  vr15,   vr3,   vr11
+    vaddi.hu         vr0,    vr1,   0
+    vaddi.hu         vr1,    vr2,   0
+    vaddi.hu         vr2,    vr3,   0
+    vadd.h           vr12,   vr12,  vr13
+    vadd.h           vr12,   vr12,  vr14
+    vadd.h           vr12,   vr12,  vr15
+
+    vssrarni.bu.h    vr12,   vr12,  6
+    vstelm.h         vr12,   a0,    0,   0
     add.d            a0,     a0,    a1
-    vstelm.h         vr10,   a0,    0,   1
+    vstelm.h         vr12,   a0,    0,   1
     add.d            a0,     a0,    a1
     addi.w           a5,     a5,    -2
     bnez             a5,     .l_\lable\()put_v_2w_loop
@@ -3140,50 +2843,43 @@ endfunc
     fldx.s           f6,     a2,    t3
     add.d            a2,     a2,    t4
 
+    vilvl.w          vr0,    vr1,   vr0
+    vilvl.w          vr1,    vr2,   vr1
     vilvl.b          vr0,    vr1,   vr0
-    vilvl.b          vr1,    vr3,   vr2
-    vilvl.b          vr2,    vr5,   vr4
-    vilvl.b          vr3,    vr7,   vr6
-    vilvl.h          vr0,    vr1,   vr0
-    vilvl.h          vr1,    vr3,   vr2
-    vilvl.w          vr2,    vr1,   vr0
-    vilvh.w          vr3,    vr1,   vr0
-
+    vilvl.w          vr1,    vr3,   vr2
+    vilvl.w          vr2,    vr4,   vr3
+    vilvl.b          vr1,    vr2,   vr1
+    vilvl.w          vr2,    vr5,   vr4
+    vilvl.w          vr3,    vr6,   vr5
+    vilvl.b          vr2,    vr3,   vr2
 .l_\lable\()put_v_4w_loop:
     fld.s            f7,     a2,    0
-    fldx.s           f10,    a2,    a3
-    add.d            a2,     a2,    t2
 
-    vextrins.b       vr2,    vr7,   0x70
-    vextrins.b       vr2,    vr7,   0xf1 //x0x1(h0)
-    vbsrl.v          vr4,    vr2,   1
-    vextrins.b       vr4,    vr10,  0x70
-    vextrins.b       vr4,    vr10,  0xf1 //x0x1(h1)
-    vdp2.h.bu.b      vr11,   vr2,   vr8
-    vdp2.h.bu.b      vr12,   vr4,   vr8
-    vbsrl.v          vr2,    vr4,   1
-
-    vextrins.b       vr3,    vr7,   0x72
-    vextrins.b       vr3,    vr7,   0xf3 //x2x3(h0)
-    vbsrl.v          vr4,    vr3,   1
-    vextrins.b       vr4,    vr10,  0x72
-    vextrins.b       vr4,    vr10,  0xf3 //x2x3(h1)
-    vdp2.h.bu.b      vr13,   vr3,   vr8
-    vdp2.h.bu.b      vr14,   vr4,   vr8
-    vbsrl.v          vr3,    vr4,   1
-
-    vhaddw.d.h       vr11
-    vhaddw.d.h       vr12
-    vhaddw.d.h       vr13
-    vhaddw.d.h       vr14
-
-    vpickev.w        vr11,   vr13,  vr11
-    vpickev.w        vr12,   vr14,  vr12
-    vpickev.h        vr11,   vr12,  vr11
-    vssrarni.bu.h    vr11,   vr11,  6
-    vstelm.w         vr11,   a0,    0,   0
+    vilvl.w          vr3,    vr7,   vr6
+    fldx.s           f6,     a2,    a3
+    add.d            a2,     a2,    t2
+    vilvl.w          vr4,    vr6,   vr7
+    vilvl.b          vr3,    vr4,   vr3
+
+    vmulwev.h.bu.b   vr12,   vr0,   vr8
+    vmulwev.h.bu.b   vr13,   vr1,   vr9
+    vmulwev.h.bu.b   vr14,   vr2,   vr10
+    vmulwev.h.bu.b   vr15,   vr3,   vr11
+    vmaddwod.h.bu.b  vr12,   vr0,   vr8
+    vmaddwod.h.bu.b  vr13,   vr1,   vr9
+    vmaddwod.h.bu.b  vr14,   vr2,   vr10
+    vmaddwod.h.bu.b  vr15,   vr3,   vr11
+    vaddi.hu         vr0,    vr1,   0
+    vaddi.hu         vr1,    vr2,   0
+    vaddi.hu         vr2,    vr3,   0
+    vadd.h           vr12,   vr12,  vr13
+    vadd.h           vr12,   vr12,  vr14
+    vadd.h           vr12,   vr12,  vr15
+
+    vssrarni.bu.h    vr12,   vr12,  6
+    vstelm.w         vr12,   a0,    0,   0
     add.d            a0,     a0,    a1
-    vstelm.w         vr11,   a0,    0,   1
+    vstelm.w         vr12,   a0,    0,   1
     add.d            a0,     a0,    a1
     addi.w           a5,     a5,    -2
     bnez             a5,     .l_\lable\()put_v_4w_loop
@@ -3208,76 +2904,54 @@ endfunc
     fldx.d           f6,     a2,    t3
     add.d            a2,     a2,    t4
 
-    vilvl.b          vr0,    vr1,   vr0
-    vilvl.b          vr1,    vr3,   vr2
-    vilvl.b          vr2,    vr5,   vr4
-    vilvl.b          vr3,    vr7,   vr6
-    vilvl.h          vr4,    vr1,   vr0
-    vilvh.h          vr5,    vr1,   vr0
-    vilvl.h          vr6,    vr3,   vr2
-    vilvh.h          vr7,    vr3,   vr2
-    vilvl.w          vr0,    vr6,   vr4 // x0x1
-    vilvh.w          vr1,    vr6,   vr4 // x2x3
-    vilvl.w          vr2,    vr7,   vr5 // x4x5
-    vilvh.w          vr3,    vr7,   vr5 // x6x7
+    vilvl.b          vr0,    vr1,   vr0 //0 1
+    vilvl.b          vr1,    vr2,   vr1 //1 2
+    vilvl.b          vr2,    vr3,   vr2 //2 3
+    vilvl.b          vr3,    vr4,   vr3 //3 4
+    vilvl.b          vr4,    vr5,   vr4 //4 5
+    vilvl.b          vr5,    vr6,   vr5 //5 6
 .l_\lable\()put_v_8w_loop:
     fld.d            f7,     a2,    0
-    fldx.d           f10,    a2,    a3
+    vilvl.b          vr12,   vr7,   vr6 //6 7
+    fldx.d           f6,     a2,    a3
     add.d            a2,     a2,    t2
-    //h0
-    vextrins.b       vr0,    vr7,   0x70
-    vextrins.b       vr0,    vr7,   0xf1
-    vextrins.b       vr1,    vr7,   0x72
-    vextrins.b       vr1,    vr7,   0xf3
-    vextrins.b       vr2,    vr7,   0x74
-    vextrins.b       vr2,    vr7,   0xf5
-    vextrins.b       vr3,    vr7,   0x76
-    vextrins.b       vr3,    vr7,   0xf7
-    vdp2.h.bu.b      vr11,   vr0,   vr8
-    vdp2.h.bu.b      vr12,   vr1,   vr8
-    vdp2.h.bu.b      vr13,   vr2,   vr8
-    vdp2.h.bu.b      vr14,   vr3,   vr8
-    vhaddw.d.h       vr11
-    vhaddw.d.h       vr12
-    vhaddw.d.h       vr13
-    vhaddw.d.h       vr14
-    vpickev.w        vr11,   vr12,  vr11
-    vpickev.w        vr12,   vr14,  vr13
-    vpickev.h        vr11,   vr12,  vr11
-    vssrarni.bu.h    vr11,   vr11,  6
-    fst.d            f11,    a0,    0
+    vilvl.b          vr13,   vr6,   vr7 //7 8
+
+    vmulwev.h.bu.b   vr14,   vr0,   vr8
+    vmulwev.h.bu.b   vr15,   vr1,   vr8
+    vmulwev.h.bu.b   vr16,   vr2,   vr9
+    vmulwev.h.bu.b   vr17,   vr3,   vr9
+    vmulwev.h.bu.b   vr18,   vr4,   vr10
+    vmulwev.h.bu.b   vr19,   vr5,   vr10
+    vmulwev.h.bu.b   vr20,   vr12,  vr11
+    vmulwev.h.bu.b   vr21,   vr13,  vr11
+    vmaddwod.h.bu.b  vr14,   vr0,   vr8
+    vmaddwod.h.bu.b  vr15,   vr1,   vr8
+    vmaddwod.h.bu.b  vr16,   vr2,   vr9
+    vmaddwod.h.bu.b  vr17,   vr3,   vr9
+    vmaddwod.h.bu.b  vr18,   vr4,   vr10
+    vmaddwod.h.bu.b  vr19,   vr5,   vr10
+    vmaddwod.h.bu.b  vr20,   vr12,  vr11
+    vmaddwod.h.bu.b  vr21,   vr13,  vr11
+
+    vaddi.hu         vr0,    vr2,   0
+    vaddi.hu         vr1,    vr3,   0
+    vaddi.hu         vr2,    vr4,   0
+    vaddi.hu         vr3,    vr5,   0
+    vaddi.hu         vr4,    vr12,  0
+    vaddi.hu         vr5,    vr13,  0
+    vadd.h           vr14,   vr14,  vr16
+    vadd.h           vr14,   vr14,  vr18
+    vadd.h           vr14,   vr14,  vr20
+    vadd.h           vr15,   vr15,  vr17
+    vadd.h           vr15,   vr15,  vr19
+    vadd.h           vr15,   vr15,  vr21
+
+    vssrarni.bu.h    vr15,   vr14,  6
+    vstelm.d         vr15,   a0,    0,   0
     add.d            a0,     a0,    a1
-    //h1
-    vbsrl.v          vr0,    vr0,   1
-    vbsrl.v          vr1,    vr1,   1
-    vbsrl.v          vr2,    vr2,   1
-    vbsrl.v          vr3,    vr3,   1
-    vextrins.b       vr0,    vr10,  0x70
-    vextrins.b       vr0,    vr10,  0xf1
-    vextrins.b       vr1,    vr10,  0x72
-    vextrins.b       vr1,    vr10,  0xf3
-    vextrins.b       vr2,    vr10,  0x74
-    vextrins.b       vr2,    vr10,  0xf5
-    vextrins.b       vr3,    vr10,  0x76
-    vextrins.b       vr3,    vr10,  0xf7
-    vdp2.h.bu.b      vr11,   vr0,   vr8
-    vdp2.h.bu.b      vr12,   vr1,   vr8
-    vdp2.h.bu.b      vr13,   vr2,   vr8
-    vdp2.h.bu.b      vr14,   vr3,   vr8
-    vhaddw.d.h       vr11
-    vhaddw.d.h       vr12
-    vhaddw.d.h       vr13
-    vhaddw.d.h       vr14
-    vpickev.w        vr11,   vr12,  vr11
-    vpickev.w        vr12,   vr14,  vr13
-    vpickev.h        vr11,   vr12,  vr11
-    vssrarni.bu.h    vr11,   vr11,  6
-    fst.d            f11,    a0,    0
+    vstelm.d         vr15,   a0,    0,   1
     add.d            a0,     a0,    a1
-    vbsrl.v          vr0,    vr0,   1
-    vbsrl.v          vr1,    vr1,   1
-    vbsrl.v          vr2,    vr2,   1
-    vbsrl.v          vr3,    vr3,   1
     addi.w           a5,     a5,    -2
     bnez             a5,     .l_\lable\()put_v_8w_loop
     addi.d           a2,     t0,    8
@@ -3341,6 +3015,7 @@ endfunc
     .dword .l_\lable\()put_hv_2w   - .l_\lable\()put_hv_jtable
 
 .l_\lable\()put_hv_2w:
+    addi.d           a2,     a2,    2
     vld              vr0,    a2,    0
     vldx             vr1,    a2,    a3
     vldx             vr2,    a2,    t2
@@ -3351,86 +3026,71 @@ endfunc
     vldx             vr6,    a2,    t3
     add.d            a2,     a2,    t4
 
-    vbsrl.v          vr10,   vr0,   1
-    vbsrl.v          vr11,   vr1,   1
-    vbsrl.v          vr12,   vr2,   1
-    vbsrl.v          vr13,   vr3,   1
-    vbsrl.v          vr14,   vr4,   1
-    vbsrl.v          vr15,   vr5,   1
-    vbsrl.v          vr16,   vr6,   1
-    vilvl.d          vr0,    vr10,  vr0
-    vilvl.d          vr1,    vr11,  vr1
-    vilvl.d          vr2,    vr12,  vr2
-    vilvl.d          vr3,    vr13,  vr3
-    vilvl.d          vr4,    vr14,  vr4
-    vilvl.d          vr5,    vr15,  vr5
-    vilvl.d          vr6,    vr16,  vr6
-    vdp2.h.bu.b      vr10,   vr0,   vr8
-    vdp2.h.bu.b      vr11,   vr1,   vr8
-    vdp2.h.bu.b      vr12,   vr2,   vr8
-    vdp2.h.bu.b      vr13,   vr3,   vr8
-    vdp2.h.bu.b      vr14,   vr4,   vr8
-    vdp2.h.bu.b      vr15,   vr5,   vr8
-    vdp2.h.bu.b      vr16,   vr6,   vr8
-    vhaddw.d.h       vr10
-    vhaddw.d.h       vr11
-    vhaddw.d.h       vr12
-    vhaddw.d.h       vr13
-    vhaddw.d.h       vr14
-    vhaddw.d.h       vr15
-    vhaddw.d.h       vr16
-
-    vpackev.w        vr10,   vr11,  vr10
-    vpackev.w        vr12,   vr13,  vr12
-    vpackod.d        vr11,   vr12,  vr10
-    vpackev.d        vr10,   vr12,  vr10
-
-    vpackev.w        vr12,   vr15,  vr14
-    vpackev.w        vr16,   vr17,  vr16
-    vpackod.d        vr13,   vr16,  vr12
-    vpackev.d        vr12,   vr16,  vr12
-
-    vpickev.h        vr10,   vr12,  vr10 //0 1 2  3  4  5  6  * (h0)
-    vpickev.h        vr11,   vr13,  vr11 //8 9 10 11 12 13 14 * (h1)
-    vsrari.h         vr10,   vr10,  2
-    vsrari.h         vr11,   vr11,  2
+    la.local         t1,     subpel_h_shuf0
+    vld              vr7,    t1,    0
+    vbsrl.v          vr8,    vr8,   2
+    vreplvei.w       vr8,    vr8,   0
+
+    //fv
+    vreplvei.w       vr14,   vr9,   1
+    vreplvei.w       vr15,   vr9,   2
+    vreplvei.w       vr16,   vr9,   3
+    vreplvei.w       vr9,    vr9,   0
+
+    vshuf.b          vr0,    vr1,   vr0,  vr7
+    vshuf.b          vr1,    vr3,   vr2,  vr7
+    vshuf.b          vr2,    vr5,   vr4,  vr7
+    vshuf.b          vr3,    vr6,   vr6,  vr7
+    vmulwev.h.bu.b   vr10,   vr0,   vr8
+    vmulwev.h.bu.b   vr11,   vr1,   vr8
+    vmulwev.h.bu.b   vr12,   vr2,   vr8
+    vmulwev.h.bu.b   vr13,   vr3,   vr8
+    vmaddwod.h.bu.b  vr10,   vr0,   vr8
+    vmaddwod.h.bu.b  vr11,   vr1,   vr8
+    vmaddwod.h.bu.b  vr12,   vr2,   vr8
+    vmaddwod.h.bu.b  vr13,   vr3,   vr8
+    vhaddw.w.h       vr0,    vr10,  vr10
+    vhaddw.w.h       vr1,    vr11,  vr11
+    vssrarni.h.w     vr1,    vr0,   2 //h0 h1 h2 h3
+    vhaddw.w.h       vr2,    vr12,  vr12
+    vhaddw.w.h       vr3,    vr13,  vr13
+    vssrarni.h.w     vr3,    vr2,   2 //h4 h5 h6 ~
+    vbsrl.v          vr2,    vr1,   4
+    vextrins.w       vr2,    vr3,   0x30 //h1 h2 h3 h4
+    vilvl.h          vr4,    vr2,   vr1 //h0 h1 h1 h2 --
+    vilvh.h          vr5,    vr2,   vr1 //h2 h3 h3 h4 --
+    vbsrl.v          vr6,    vr3,   4
+    vilvl.h          vr6,    vr6,   vr3 //h4 h5 h5 h6 --
+    vbsrl.v          vr3,    vr3,   8  //h6 ~
 .l_\lable\()put_hv_2w_loop:
-    vld              vr7,    a2,    0
-    vldx             vr12,   a2,    a3
+    vld              vr0,    a2,    0
+    vldx             vr2,    a2,    a3
     add.d            a2,     a2,    t2
-
-    vbsrl.v          vr1,    vr7,   1
-    vbsrl.v          vr2,    vr12,  1
-    vilvl.d          vr0,    vr1,   vr7
-    vilvl.d          vr1,    vr2,   vr12
-    vdp2.h.bu.b      vr2,    vr0,   vr8
-    vdp2.h.bu.b      vr3,    vr1,   vr8
-    vhaddw.d.h       vr2
-    vhaddw.d.h       vr3
-    vpickev.w        vr2,    vr3,   vr2
-    vpickev.h        vr2,    vr2,   vr2
-    vsrari.h         vr2,    vr2,   2
-    vextrins.h       vr10,   vr2,   0x70 //0 1 2 3 4 5 6 7
-    vextrins.h       vr11,   vr2,   0x71
-    vbsrl.v          vr12,   vr10,  2
-    vbsrl.v          vr13,   vr11,  2
-    vextrins.h       vr12,   vr2,   0x72 //1 2 3 4 5 6 7 8
-    vextrins.h       vr13,   vr2,   0x73
-    vdp2.w.h         vr0,    vr10,  vr9
-    vdp2.w.h         vr1,    vr11,  vr9
-    vdp2.w.h         vr2,    vr12,  vr9
-    vdp2.w.h         vr3,    vr13,  vr9
-    vhaddw.q.w       vr0
-    vhaddw.q.w       vr1
-    vhaddw.q.w       vr2
-    vhaddw.q.w       vr3
-    vpackev.w        vr0,    vr1,   vr0
-    vpackev.w        vr1,    vr3,   vr2
-    vpackev.d        vr0,    vr1,   vr0
-    vssrarni.hu.w    vr0,    vr0,   10
+    vshuf.b          vr0,    vr2,   vr0,  vr7
+    vdp2.h.bu.b      vr17,   vr0,   vr8
+    vhaddw.w.h       vr17,   vr17,  vr17
+    vssrarni.h.w     vr17,   vr17,  2 //h7 h8
+    vextrins.w       vr3,    vr17,  0x10 //h6 h7
+    vilvl.h          vr3,    vr17,  vr3  //h6 h7 h7 h8 --
+
+    vmulwev.w.h      vr18,   vr4,   vr9
+    vmulwev.w.h      vr19,   vr5,   vr14
+    vmulwev.w.h      vr20,   vr6,   vr15
+    vmulwev.w.h      vr21,   vr3,   vr16
+    vmaddwod.w.h     vr18,   vr4,   vr9
+    vmaddwod.w.h     vr19,   vr5,   vr14
+    vmaddwod.w.h     vr20,   vr6,   vr15
+    vmaddwod.w.h     vr21,   vr3,   vr16
+    vaddi.hu         vr4,    vr5,   0
+    vaddi.hu         vr5,    vr6,   0
+    vaddi.hu         vr6,    vr3,   0
+    vbsrl.v          vr3,    vr17,  4 //h8 ~
+    vadd.w           vr18,   vr18,  vr19
+    vadd.w           vr18,   vr18,  vr20
+    vadd.w           vr18,   vr18,  vr21
+
+    vssrarni.hu.w    vr0,    vr18,  10
     vssrani.bu.h     vr0,    vr0,   0
-    vbsrl.v          vr10,   vr12,  2
-    vbsrl.v          vr11,   vr13,  2
     vstelm.h         vr0,    a0,    0,   0
     add.d            a0,     a0,    a1
     vstelm.h         vr0,    a0,    0,   1
@@ -3440,6 +3100,7 @@ endfunc
     b                .l_\lable\()end_put_8tap
 
 .l_\lable\()put_hv_4w:
+    addi.d           a2,     a2,    2 //ignore leading 0
     vld              vr0,    a2,    0
     vldx             vr1,    a2,    a3
     vldx             vr2,    a2,    t2
@@ -3449,81 +3110,125 @@ endfunc
     vldx             vr5,    a2,    t2
     vldx             vr6,    a2,    t3
     add.d            a2,     a2,    t4
-    FILTER_8TAP_4W   vr0 //x0 x1 x2 x3
-    FILTER_8TAP_4W   vr1
-    FILTER_8TAP_4W   vr2
-    FILTER_8TAP_4W   vr3
-    FILTER_8TAP_4W   vr4
-    FILTER_8TAP_4W   vr5
-    FILTER_8TAP_4W   vr6
-    vpackev.h        vr0,    vr1,   vr0
-    vpackev.h        vr1,    vr3,   vr2
-    vpackev.h        vr2,    vr5,   vr4
-    vpackev.h        vr3,    vr7,   vr6
-    vilvl.w          vr4,    vr1,   vr0
-    vilvh.w          vr5,    vr1,   vr0
-    vilvl.w          vr6,    vr3,   vr2
-    vilvh.w          vr7,    vr3,   vr2
-    vilvl.d          vr0,    vr6,   vr4 //0 1 2 3 4 5 6 *
-    vilvh.d          vr1,    vr6,   vr4
-    vilvl.d          vr2,    vr7,   vr5
-    vilvh.d          vr3,    vr7,   vr5
-    vsrari.h         vr0,    vr0,   2
-    vsrari.h         vr1,    vr1,   2
-    vsrari.h         vr2,    vr2,   2
-    vsrari.h         vr3,    vr3,   2
+
+    la.local         t1,     subpel_h_shuf1
+    vld              vr7,    t1,    0
+    vbsrl.v          vr8,    vr8,   2
+    vreplvei.w       vr8,    vr8,   0
+
+    //fv
+    vreplvei.w       vr17,   vr9,   0
+    vreplvei.w       vr18,   vr9,   1
+    vreplvei.w       vr19,   vr9,   2
+    vreplvei.w       vr20,   vr9,   3
+
+    //DAV1D_FILTER_8TAP_RND
+    vshuf.b          vr0,    vr0,   vr0,  vr7
+    vshuf.b          vr1,    vr1,   vr1,  vr7
+    vshuf.b          vr2,    vr2,   vr2,  vr7
+    vshuf.b          vr3,    vr3,   vr3,  vr7
+    vshuf.b          vr4,    vr4,   vr4,  vr7
+    vshuf.b          vr5,    vr5,   vr5,  vr7
+    vshuf.b          vr6,    vr6,   vr6,  vr7
+
+    vmulwev.h.bu.b   vr10,   vr0,   vr8
+    vmulwev.h.bu.b   vr11,   vr1,   vr8
+    vmulwev.h.bu.b   vr12,   vr2,   vr8
+    vmulwev.h.bu.b   vr13,   vr3,   vr8
+    vmulwev.h.bu.b   vr14,   vr4,   vr8
+    vmulwev.h.bu.b   vr15,   vr5,   vr8
+    vmulwev.h.bu.b   vr16,   vr6,   vr8
+    vmaddwod.h.bu.b  vr10,   vr0,   vr8
+    vmaddwod.h.bu.b  vr11,   vr1,   vr8
+    vmaddwod.h.bu.b  vr12,   vr2,   vr8
+    vmaddwod.h.bu.b  vr13,   vr3,   vr8
+    vmaddwod.h.bu.b  vr14,   vr4,   vr8
+    vmaddwod.h.bu.b  vr15,   vr5,   vr8
+    vmaddwod.h.bu.b  vr16,   vr6,   vr8
+
+    vhaddw.w.h       vr10,   vr10,  vr10
+    vhaddw.w.h       vr11,   vr11,  vr11
+    vhaddw.w.h       vr12,   vr12,  vr12
+    vhaddw.w.h       vr13,   vr13,  vr13
+    vhaddw.w.h       vr14,   vr14,  vr14
+    vhaddw.w.h       vr15,   vr15,  vr15
+    vhaddw.w.h       vr16,   vr16,  vr16
+
+    vssrarni.h.w     vr10,   vr10,  2 //h0
+    vssrarni.h.w     vr11,   vr11,  2 //h1
+    vssrarni.h.w     vr12,   vr12,  2 //h2
+    vssrarni.h.w     vr13,   vr13,  2 //h3
+    vssrarni.h.w     vr14,   vr14,  2 //h4
+    vssrarni.h.w     vr15,   vr15,  2 //h5
+    vssrarni.h.w     vr16,   vr16,  2 //h6
+
+    //h0
+    vilvl.h          vr0,    vr11,  vr10 //01
+    vilvl.h          vr1,    vr13,  vr12 //23
+    vilvl.h          vr2,    vr15,  vr14 //45
+    //h1
+    vilvl.h          vr4,    vr12,  vr11 //12
+    vilvl.h          vr5,    vr14,  vr13 //34
+    vilvl.h          vr6,    vr16,  vr15 //56
+
 .l_\lable\()put_hv_4w_loop:
-    vld              vr4,    a2,    0
-    vldx             vr5,    a2,    a3
+    vld              vr9,    a2,    0
+    vldx             vr10,   a2,    a3
     add.d            a2,     a2,    t2
-    FILTER_8TAP_4W   vr4
-    FILTER_8TAP_4W   vr5
-    vpickev.h        vr4,    vr5,   vr4
-    vsrari.h         vr4,    vr4,   2
-    vextrins.h       vr0,    vr4,   0x70
-    vextrins.h       vr1,    vr4,   0x71
-    vextrins.h       vr2,    vr4,   0x72
-    vextrins.h       vr3,    vr4,   0x73
-    vbsrl.v          vr5,    vr0,   2
-    vbsrl.v          vr6,    vr1,   2
-    vbsrl.v          vr7,    vr2,   2
-    vbsrl.v          vr10,   vr3,   2
-    vextrins.h       vr5,    vr4,   0x74
-    vextrins.h       vr6,    vr4,   0x75
-    vextrins.h       vr7,    vr4,   0x76
-    vextrins.h       vr10,   vr4,   0x77
-    vdp2.w.h         vr11,   vr0,   vr9
-    vdp2.w.h         vr12,   vr1,   vr9
-    vdp2.w.h         vr13,   vr2,   vr9
-    vdp2.w.h         vr14,   vr3,   vr9
-    vhaddw.q.w       vr11
-    vhaddw.q.w       vr12
-    vhaddw.q.w       vr13
-    vhaddw.q.w       vr14
-    vpackev.w        vr0,    vr12,  vr11
-    vpackev.w        vr1,    vr14,  vr13
-    vpackev.d        vr0,    vr1,   vr0
-    vdp2.w.h         vr11,   vr5,   vr9
-    vdp2.w.h         vr12,   vr6,   vr9
-    vdp2.w.h         vr13,   vr7,   vr9
-    vdp2.w.h         vr14,   vr10,  vr9
-    vhaddw.q.w       vr11
-    vhaddw.q.w       vr12
-    vhaddw.q.w       vr13
-    vhaddw.q.w       vr14
-    vpackev.w        vr1,    vr12,  vr11
-    vpackev.w        vr2,    vr14,  vr13
-    vpackev.d        vr1,    vr2,   vr1
-    vssrarni.hu.w    vr1,    vr0,   10
-    vssrani.bu.h     vr1,    vr1,   0
-    vstelm.w         vr1,    a0,    0,    0
+
+    //DAV1D_FILTER_8TAP_CLIP
+    vshuf.b          vr9,    vr9,   vr9,  vr7
+    vshuf.b          vr10,   vr10,  vr10, vr7
+    vmulwev.h.bu.b   vr11,   vr9,   vr8
+    vmulwev.h.bu.b   vr12,   vr10,  vr8
+    vmaddwod.h.bu.b  vr11,   vr9,   vr8
+    vmaddwod.h.bu.b  vr12,   vr10,  vr8
+    vhaddw.w.h       vr11,   vr11,  vr11
+    vhaddw.w.h       vr12,   vr12,  vr12
+    vssrarni.h.w     vr11,   vr11,  2 //h7
+    vssrarni.h.w     vr12,   vr12,  2 //h8
+    vilvl.h          vr3,    vr11,  vr16 //67
+    vilvl.h          vr13,   vr12,  vr11 //78
+
+    vmulwev.w.h      vr9,    vr0,   vr17
+    vmulwev.w.h      vr10,   vr1,   vr18
+    vmulwev.w.h      vr14,   vr2,   vr19
+    vmulwev.w.h      vr15,   vr3,   vr20
+    vmaddwod.w.h     vr9,    vr0,   vr17
+    vmaddwod.w.h     vr10,   vr1,   vr18
+    vmaddwod.w.h     vr14,   vr2,   vr19
+    vmaddwod.w.h     vr15,   vr3,   vr20
+    vadd.w           vr16,   vr9,   vr10
+    vadd.w           vr16,   vr16,  vr14
+    vadd.w           vr16,   vr16,  vr15
+
+    vmulwev.w.h      vr9,    vr4,   vr17
+    vmulwev.w.h      vr10,   vr5,   vr18
+    vmulwev.w.h      vr14,   vr6,   vr19
+    vmulwev.w.h      vr15,   vr13,  vr20
+    vmaddwod.w.h     vr9,    vr4,   vr17
+    vmaddwod.w.h     vr10,   vr5,   vr18
+    vmaddwod.w.h     vr14,   vr6,   vr19
+    vmaddwod.w.h     vr15,   vr13,  vr20
+    vadd.w           vr21,   vr9,   vr10
+    vadd.w           vr21,   vr21,  vr14
+    vadd.w           vr21,   vr21,  vr15
+
+    vssrarni.hu.w    vr21,   vr16,  10
+    vssrani.bu.h     vr21,   vr21,  0
+    //cache
+    vaddi.hu         vr0,    vr1,   0
+    vaddi.hu         vr1,    vr2,   0
+    vaddi.hu         vr2,    vr3,   0
+    vaddi.hu         vr4,    vr5,   0
+    vaddi.hu         vr5,    vr6,   0
+    vaddi.hu         vr6,    vr13,  0
+    vaddi.hu         vr16,   vr12,  0
+
+    vstelm.w         vr21,   a0,    0,    0
     add.d            a0,     a0,    a1
-    vstelm.w         vr1,    a0,    0,    1
+    vstelm.w         vr21,   a0,    0,    1
     add.d            a0,     a0,    a1
-    vbsrl.v          vr0,    vr5,   2
-    vbsrl.v          vr1,    vr6,   2
-    vbsrl.v          vr2,    vr7,   2
-    vbsrl.v          vr3,    vr10,  2
     addi.w           a5,     a5,    -2
     bnez             a5,     .l_\lable\()put_hv_4w_loop
     b                .l_\lable\()end_put_8tap
@@ -3533,9 +3238,28 @@ endfunc
 .l_\lable\()put_hv_32w:
 .l_\lable\()put_hv_64w:
 .l_\lable\()put_hv_128w:
+    addi.d          sp,      sp,    -8*8
+    fst.d           f24,     sp,    0
+    fst.d           f25,     sp,    8
+    fst.d           f26,     sp,    16
+    fst.d           f27,     sp,    24
+    fst.d           f28,     sp,    32
+    fst.d           f29,     sp,    40
+    fst.d           f30,     sp,    48
+    fst.d           f31,     sp,    56
     addi.d          t0,      a2,    0 //src
     addi.d          t5,      a5,    0 //h
     addi.d          t8,      a0,    0 //dst
+    la.local        t1,      subpel_h_shuf1
+    vld             vr7,     t1,    0
+    vaddi.bu        vr11,    vr7,   4
+    vaddi.bu        vr12,    vr7,   8
+    vreplvei.w      vr10,    vr8,   1
+    vreplvei.w      vr8,     vr8,   0
+    vreplvei.w      vr20,    vr9,   1
+    vreplvei.w      vr21,    vr9,   2
+    vreplvei.w      vr22,    vr9,   3
+    vreplvei.w      vr9,     vr9,   0
 .l_\lable\()put_hv_8w_loop0:
     vld             vr0,     a2,    0
     vldx            vr1,     a2,    a3
@@ -3546,28 +3270,123 @@ endfunc
     vldx            vr5,     a2,    t2
     vldx            vr6,     a2,    t3
     add.d           a2,      a2,    t4
-    FILTER_8TAP_8W  vr0
-    FILTER_8TAP_8W  vr1
-    FILTER_8TAP_8W  vr2
-    FILTER_8TAP_8W  vr3
-    FILTER_8TAP_8W  vr4
-    FILTER_8TAP_8W  vr5
-    FILTER_8TAP_8W  vr6
-    LSX_TRANSPOSE8x8_H vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7,\
-                       vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7,\
-                       vr10,vr11,vr12,vr13,vr14,vr15,vr16,vr17
+
+    FILTER_8TAP_8W  vr0 //h0
+    FILTER_8TAP_8W  vr1 //h1
+    FILTER_8TAP_8W  vr2 //h2
+    FILTER_8TAP_8W  vr3 //h3
+    FILTER_8TAP_8W  vr4 //h4
+    FILTER_8TAP_8W  vr5 //h5
+    FILTER_8TAP_8W  vr6 //h6
+
+    //h0' low part
+    vilvl.h         vr23,    vr1,   vr0 //01
+    vilvl.h         vr24,    vr3,   vr2 //23
+    vilvl.h         vr25,    vr5,   vr4 //45
+    //h0' high part
+    vilvh.h         vr26,    vr1,   vr0 //01
+    vilvh.h         vr27,    vr3,   vr2 //23
+    vilvh.h         vr28,    vr5,   vr4 //45
+
+    //h1' low part
+    vilvl.h         vr29,    vr2,   vr1 //12
+    vilvl.h         vr30,    vr4,   vr3 //34
+    vilvl.h         vr31,    vr6,   vr5 //56
+    //h1' high part
+    vilvh.h         vr0,     vr2,   vr1 //12
+    vilvh.h         vr1,     vr4,   vr3 //34
+    vilvh.h         vr2,     vr6,   vr5 //56
+
 .l_\lable\()put_hv_8w_loop:
-    vld             vr20,    a2,    0
-    vldx            vr21,    a2,    a3
+    vld             vr3,     a2,    0
+    vldx            vr4,     a2,    a3
     add.d           a2,      a2,    t2
-    FILTER_8TAP_8W  vr20
-    FILTER_8TAP_8W  vr21
-    VEXTRINS_Hx8    vr20
-    FILTER_8TAP_8W_CLIP_STORE
-    VBSRL_Vx8
-    VEXTRINS_Hx8    vr21
-    FILTER_8TAP_8W_CLIP_STORE
-    VBSRL_Vx8
+
+    FILTER_8TAP_8W  vr3 //h7
+    FILTER_8TAP_8W  vr4 //h8
+
+    //h0' low part
+    vilvl.h         vr16,    vr3,   vr6 //67 ~low
+    vmulwev.w.h     vr13,    vr23,  vr9
+    vmulwev.w.h     vr14,    vr24,  vr20
+    vmulwev.w.h     vr15,    vr25,  vr21
+    vmulwev.w.h     vr17,    vr16,  vr22
+    vmaddwod.w.h    vr13,    vr23,  vr9
+    vmaddwod.w.h    vr14,    vr24,  vr20
+    vmaddwod.w.h    vr15,    vr25,  vr21
+    vmaddwod.w.h    vr17,    vr16,  vr22
+    vadd.w          vr13,    vr13,  vr14
+    vadd.w          vr13,    vr13,  vr15
+    vadd.w          vr13,    vr13,  vr17
+    //cache
+    vaddi.hu        vr23,    vr24,  0
+    vaddi.hu        vr24,    vr25,  0
+    vaddi.hu        vr25,    vr16,  0
+
+    //h0' high part
+    vilvh.h         vr17,    vr3,   vr6 //67 ~high
+    vmulwev.w.h     vr14,    vr26,  vr9
+    vmulwev.w.h     vr15,    vr27,  vr20
+    vmulwev.w.h     vr16,    vr28,  vr21
+    vmulwev.w.h     vr18,    vr17,  vr22
+    vmaddwod.w.h    vr14,    vr26,  vr9
+    vmaddwod.w.h    vr15,    vr27,  vr20
+    vmaddwod.w.h    vr16,    vr28,  vr21
+    vmaddwod.w.h    vr18,    vr17,  vr22
+    vadd.w          vr14,    vr14,  vr15
+    vadd.w          vr14,    vr14,  vr16
+    vadd.w          vr14,    vr14,  vr18
+    vssrarni.hu.w   vr14,    vr13,  10
+    vssrarni.bu.h   vr5,     vr14,  0
+    vstelm.d        vr5,     a0,    0,   0
+    add.d           a0,      a0,    a1
+    //cache
+    vaddi.hu        vr26,    vr27,  0
+    vaddi.hu        vr27,    vr28,  0
+    vaddi.hu        vr28,    vr17,  0
+    vaddi.hu        vr6,     vr4,   0
+
+    vilvl.h         vr5,     vr4,   vr3 //78 ~low
+    vilvh.h         vr4,     vr4,   vr3 //78 ~high
+
+    //h1' low part
+    vmulwev.w.h     vr13,    vr29,  vr9
+    vmulwev.w.h     vr14,    vr30,  vr20
+    vmulwev.w.h     vr15,    vr31,  vr21
+    vmulwev.w.h     vr16,    vr5,   vr22
+    vmaddwod.w.h    vr13,    vr29,  vr9
+    vmaddwod.w.h    vr14,    vr30,  vr20
+    vmaddwod.w.h    vr15,    vr31,  vr21
+    vmaddwod.w.h    vr16,    vr5,   vr22
+    vadd.w          vr13,    vr13,  vr14
+    vadd.w          vr13,    vr13,  vr15
+    vadd.w          vr13,    vr13,  vr16
+    //cache
+    vaddi.hu        vr29,    vr30,  0
+    vaddi.hu        vr30,    vr31,  0
+    vaddi.hu        vr31,    vr5,   0
+
+    //h1' high part
+    vmulwev.w.h     vr14,    vr0,   vr9
+    vmulwev.w.h     vr15,    vr1,   vr20
+    vmulwev.w.h     vr16,    vr2,   vr21
+    vmulwev.w.h     vr17,    vr4,   vr22
+    vmaddwod.w.h    vr14,    vr0,   vr9
+    vmaddwod.w.h    vr15,    vr1,   vr20
+    vmaddwod.w.h    vr16,    vr2,   vr21
+    vmaddwod.w.h    vr17,    vr4,   vr22
+    vadd.w          vr14,    vr14,  vr15
+    vadd.w          vr14,    vr14,  vr16
+    vadd.w          vr14,    vr14,  vr17
+    vssrarni.hu.w   vr14,    vr13,  10
+    vssrarni.bu.h   vr5,     vr14,  0
+    vstelm.d        vr5,     a0,    0,   0
+    add.d           a0,      a0,    a1
+    //cache
+    vaddi.hu        vr0,     vr1,   0
+    vaddi.hu        vr1,     vr2,   0
+    vaddi.hu        vr2,     vr4,   0
+
     addi.w          a5,      a5,    -2
     bnez            a5,      .l_\lable\()put_hv_8w_loop
     addi.d          a2,      t0,    8
@@ -3577,6 +3396,15 @@ endfunc
     addi.d          a5,      t5,    0
     addi.w          a4,      a4,    -8
     bnez            a4,      .l_\lable\()put_hv_8w_loop0
+    fld.d           f24,     sp,    0
+    fld.d           f25,     sp,    8
+    fld.d           f26,     sp,    16
+    fld.d           f27,     sp,    24
+    fld.d           f28,     sp,    32
+    fld.d           f29,     sp,    40
+    fld.d           f30,     sp,    48
+    fld.d           f31,     sp,    56
+    addi.d          sp,      sp,    8*8
 .l_\lable\()end_put_8tap:
 .endm
 
@@ -3652,83 +3480,76 @@ function put_8tap_sharp_8bpc_lsx
 endfunc
 
 const shufb1
-.byte 0,1,2,3,4,5,6,7,1,2,3,4,5,6,7,8,0,1,2,3,4,5,6,7,1,2,3,4,5,6,7,8
+.byte 0,1,2,3,4,5,6,7,1,2,3,4,5,6,7,8
 endconst
 
-.macro SHUFB in0, in1, tmp, out
-    xvbsrl.v  \tmp, \in0, 2
-    xvpermi.q \tmp, \in0, 0x20
-    xvshuf.b  \out, \tmp, \tmp, \in1
-.endm
-
-.macro HADDWDH in0
-    xvhaddw.w.h \in0, \in0, \in0
-    xvhaddw.d.w \in0, \in0, \in0
+.macro PREP_H_8W in0
+    vshuf.b          vr2,    \in0,  \in0,   vr6
+    vshuf.b          vr3,    \in0,  \in0,   vr7
+    vshuf.b          vr4,    \in0,  \in0,   vr8
+    vmulwev.h.bu.b   vr12,   vr2,   vr22
+    vmulwev.h.bu.b   vr13,   vr3,   vr23
+    vmulwev.h.bu.b   vr14,   vr3,   vr22
+    vmulwev.h.bu.b   vr15,   vr4,   vr23
+    vmaddwod.h.bu.b  vr12,   vr2,   vr22
+    vmaddwod.h.bu.b  vr13,   vr3,   vr23
+    vmaddwod.h.bu.b  vr14,   vr3,   vr22
+    vmaddwod.h.bu.b  vr15,   vr4,   vr23
+    vadd.h           vr12,   vr12,  vr13
+    vadd.h           vr14,   vr14,  vr15
+    vhaddw.w.h       vr12,   vr12,  vr12
+    vhaddw.w.h       \in0,   vr14,  vr14
+    vssrarni.h.w     \in0,   vr12,  2
 .endm
 
-.macro HADDWQW in0
-    xvhaddw.d.w \in0, \in0, \in0
-    xvhaddw.q.d \in0, \in0, \in0
-.endm
-
-.macro PREP_W16_H in0
-    xvbsrl.v         xr4,    \in0,    4
-    xvbsrl.v         xr5,    \in0,    8
-    xvpermi.q        xr9,    \in0,    0x31
-    xvpackev.d       xr5,     xr9,    xr5
-    xvbsrl.v         xr6,     xr5,    4
-    SHUFB           \in0,     xr23,   xr9,   \in0
-    SHUFB            xr4,     xr23,   xr9,    xr4
-    SHUFB            xr5,     xr23,   xr9,    xr5
-    SHUFB            xr6,     xr23,   xr9,    xr6
-    xvdp2.h.bu.b     xr10,   \in0,    xr22
-    xvdp2.h.bu.b     xr11,    xr4,    xr22
-    xvdp2.h.bu.b     xr12,    xr5,    xr22
-    xvdp2.h.bu.b     xr13,    xr6,    xr22
-    HADDWDH          xr10
-    HADDWDH          xr11
-    HADDWDH          xr12
-    HADDWDH          xr13
-    xvpickev.w       xr10,    xr11,   xr10
-    xvpickev.w       xr11,    xr13,   xr12
-    xvpermi.d        xr10,    xr10,   0xd8
-    xvpermi.d        xr11,    xr11,   0xd8
-    xvpickev.h       xr10,    xr11,   xr10
-    xvpermi.d        xr10,    xr10,   0xd8
-    xvsrari.h       \in0,     xr10,   2
+.macro PREP_HV_8W_LASX in0
+    xvshuf.b         xr4,   \in0,  \in0,   xr19
+    xvshuf.b         xr5,   \in0,  \in0,   xr20
+    xvshuf.b         xr6,   \in0,  \in0,   xr21
+    xvmulwev.h.bu.b  xr7,   xr4,   xr22
+    xvmulwev.h.bu.b  xr9,   xr5,   xr23
+    xvmulwev.h.bu.b  xr10,  xr5,   xr22
+    xvmulwev.h.bu.b  xr11,  xr6,   xr23
+    xvmaddwod.h.bu.b xr7,   xr4,   xr22
+    xvmaddwod.h.bu.b xr9,   xr5,   xr23
+    xvmaddwod.h.bu.b xr10,  xr5,   xr22
+    xvmaddwod.h.bu.b xr11,  xr6,   xr23
+    xvadd.h          xr7,   xr7,   xr9
+    xvadd.h          xr9,   xr10,  xr11
+    xvhaddw.w.h      xr7,   xr7,   xr7
+    xvhaddw.w.h      \in0,  xr9,   xr9
+    xvssrarni.h.w    \in0,  xr7,   2
 .endm
 
 .macro PREP_8TAP_8BPC_LASX lable
     li.w             t0,     4
     la.local         t6,     dav1d_mc_subpel_filters
-    la.local         t7,     shufb1
-    xvld             xr23,   t7,    0
     slli.d           t2,     a2,    1  //src_stride*2
     add.d            t3,     t2,    a2 //src_stride*3
     slli.d           t4,     t2,    1
 
-    bnez             a5,     .l_\lable\()h //mx
-    bnez             a6,     .l_\lable\()v
+    bnez             a5,     .l_\lable\()h_lasx //mx
+    bnez             a6,     .l_\lable\()v_lasx
 
     clz.w            t1,     a3
     li.w             t5,     24
     sub.w            t1,     t1,    t5
-    la.local         t5,     .l_\lable\()prep_hv0_jtable
+    la.local         t5,     .l_\lable\()prep_hv0_jtable_lasx
     alsl.d           t1,     t1,    t5,   1
     ld.h             t8,     t1,    0
     add.d            t5,     t5,    t8
     jirl             $r0,    t5,    0
 
     .align   3
-.l_\lable\()prep_hv0_jtable:
-    .hword .l_\lable\()hv0_128w - .l_\lable\()prep_hv0_jtable
-    .hword .l_\lable\()hv0_64w  - .l_\lable\()prep_hv0_jtable
-    .hword .l_\lable\()hv0_32w  - .l_\lable\()prep_hv0_jtable
-    .hword .l_\lable\()hv0_16w  - .l_\lable\()prep_hv0_jtable
-    .hword .l_\lable\()hv0_8w   - .l_\lable\()prep_hv0_jtable
-    .hword .l_\lable\()hv0_4w   - .l_\lable\()prep_hv0_jtable
-
-.l_\lable\()hv0_4w:
+.l_\lable\()prep_hv0_jtable_lasx:
+    .hword .l_\lable\()hv0_128w_lasx - .l_\lable\()prep_hv0_jtable_lasx
+    .hword .l_\lable\()hv0_64w_lasx  - .l_\lable\()prep_hv0_jtable_lasx
+    .hword .l_\lable\()hv0_32w_lasx  - .l_\lable\()prep_hv0_jtable_lasx
+    .hword .l_\lable\()hv0_16w_lasx  - .l_\lable\()prep_hv0_jtable_lasx
+    .hword .l_\lable\()hv0_8w_lasx   - .l_\lable\()prep_hv0_jtable_lasx
+    .hword .l_\lable\()hv0_4w_lasx   - .l_\lable\()prep_hv0_jtable_lasx
+
+.l_\lable\()hv0_4w_lasx:
     fld.s            f0,     a1,    0
     fldx.s           f1,     a1,    a2
     fldx.s           f2,     a1,    t2
@@ -3741,9 +3562,9 @@ endconst
     xvst             xr0,    a0,    0
     addi.d           a0,     a0,    32
     addi.d           a4,     a4,    -4
-    bnez             a4,     .l_\lable\()hv0_4w
-    b                .l_\lable\()end_pre_8tap
-.l_\lable\()hv0_8w:
+    bnez             a4,     .l_\lable\()hv0_4w_lasx
+    b                .l_\lable\()end_pre_8tap_lasx
+.l_\lable\()hv0_8w_lasx:
     fld.d            f0,     a1,    0
     fldx.d           f1,     a1,    a2
     fldx.d           f2,     a1,    t2
@@ -3757,9 +3578,9 @@ endconst
     xvst             xr2,    a0,    32
     addi.d           a0,     a0,    64
     addi.d           a4,     a4,    -4
-    bnez             a4,     .l_\lable\()hv0_8w
-    b                .l_\lable\()end_pre_8tap
-.l_\lable\()hv0_16w:
+    bnez             a4,     .l_\lable\()hv0_8w_lasx
+    b                .l_\lable\()end_pre_8tap_lasx
+.l_\lable\()hv0_16w_lasx:
     vld              vr0,    a1,    0
     vldx             vr1,    a1,    a2
     vldx             vr2,    a1,    t2
@@ -3779,9 +3600,9 @@ endconst
     xvst             xr3,    a0,    96
     addi.d           a0,     a0,    128
     addi.d           a4,     a4,    -4
-    bnez             a4,     .l_\lable\()hv0_16w
-    b                .l_\lable\()end_pre_8tap
-.l_\lable\()hv0_32w:
+    bnez             a4,     .l_\lable\()hv0_16w_lasx
+    b                .l_\lable\()end_pre_8tap_lasx
+.l_\lable\()hv0_32w_lasx:
     xvld             xr0,    a1,    0
     xvldx            xr1,    a1,    a2
     xvldx            xr2,    a1,    t2
@@ -3813,16 +3634,16 @@ endconst
     xvst             xr7,    a0,    224
     addi.d           a0,     a0,    256
     addi.d           a4,     a4,    -4
-    bnez             a4,     .l_\lable\()hv0_32w
-    b                .l_\lable\()end_pre_8tap
-.l_\lable\()hv0_64w:
-.l_\lable\()hv0_128w:
+    bnez             a4,     .l_\lable\()hv0_32w_lasx
+    b                .l_\lable\()end_pre_8tap_lasx
+.l_\lable\()hv0_64w_lasx:
+.l_\lable\()hv0_128w_lasx:
     addi.d           t0,     a1,    0
     addi.d           t5,     a4,    0
     srli.w           t7,     a3,    5
     slli.w           t7,     t7,    6
     addi.d           t8,     a0,    0
-.l_\lable\()hv0_32_loop:
+.l_\lable\()hv0_32_loop_lasx:
     xvld             xr0,    a1,    0
     xvldx            xr1,    a1,    a2
     xvldx            xr2,    a1,    t2
@@ -3856,25 +3677,25 @@ endconst
     xvst             xr3,    t1,    0
     xvst             xr7,    t1,    32
     add.d            a0,     t1,    t7
-    addi.d           a4,     a4,    -4
-    bnez             a4,     .l_\lable\()hv0_32_loop
+    addi.d           a4,     a4,   -4
+    bnez             a4,     .l_\lable\()hv0_32_loop_lasx
     addi.d           a1,     t0,    32
     addi.d           t0,     t0,    32
     addi.d           a0,     t8,    64
     addi.d           t8,     t8,    64
     addi.d           a4,     t5,    0
-    addi.d           a3,     a3,    -32
-    bnez             a3,     .l_\lable\()hv0_32_loop
-    b                .l_\lable\()end_pre_8tap
+    addi.d           a3,     a3,   -32
+    bnez             a3,     .l_\lable\()hv0_32_loop_lasx
+    b                .l_\lable\()end_pre_8tap_lasx
 
-.l_\lable\()h:
-    bnez             a6,     .l_\lable\()hv //if(fh) && if (fv)
+.l_\lable\()h_lasx:
+    bnez             a6,     .l_\lable\()hv_lasx //if(fh) && if (fv)
 
     andi             t1,    a7,    3
-    blt              t0,    a3,    .l_\lable\()h_idx_fh
+    blt              t0,    a3,    .l_\lable\()h_idx_fh_lasx
     andi             t1,    a7,    1
     addi.w           t1,    t1,    3
-.l_\lable\()h_idx_fh:
+.l_\lable\()h_idx_fh_lasx:
     addi.w           t5,    zero,  120
     mul.w            t1,    t1,    t5
     addi.w           t5,    a5,    -1
@@ -3883,190 +3704,149 @@ endconst
     add.d            t1,    t6,    t1 //fh's offset
     xvldrepl.d       xr22,  t1,    0
 
-    addi.d           a1,     a1,    -3
-    clz.w            t1,     a3
-    li.w             t5,     24
-    sub.w            t1,     t1,    t5
-    la.local         t5,     .l_\lable\()prep_h_jtable
-    alsl.d           t1,     t1,    t5,   1
-    ld.h             t8,     t1,    0
-    add.d            t5,     t5,    t8
-    jirl             $r0,    t5,    0
+    addi.d           a1,    a1,    -3
+    clz.w            t1,    a3
+    li.w             t5,    24
+    sub.w            t1,    t1,    t5
+    la.local         t5,    .l_\lable\()prep_h_jtable_lasx
+    alsl.d           t1,    t1,    t5,   1
+    ld.h             t8,    t1,    0
+    add.d            t5,    t5,    t8
+    jirl             $r0,   t5,    0
 
     .align   3
-.l_\lable\()prep_h_jtable:
-    .hword .l_\lable\()h_128w - .l_\lable\()prep_h_jtable
-    .hword .l_\lable\()h_64w  - .l_\lable\()prep_h_jtable
-    .hword .l_\lable\()h_32w  - .l_\lable\()prep_h_jtable
-    .hword .l_\lable\()h_16w  - .l_\lable\()prep_h_jtable
-    .hword .l_\lable\()h_8w   - .l_\lable\()prep_h_jtable
-    .hword .l_\lable\()h_4w   - .l_\lable\()prep_h_jtable
-
-.l_\lable\()h_4w:
-    xvld             xr0,    a1,    0
-    xvldx            xr1,    a1,    a2
-    xvldx            xr2,    a1,    t2
-    xvldx            xr3,    a1,    t3
-    add.d            a1,     a1,    t4
-
-    SHUFB            xr0,    xr23,  xr9,   xr0
-    SHUFB            xr1,    xr23,  xr9,   xr1
-    SHUFB            xr2,    xr23,  xr9,   xr2
-    SHUFB            xr3,    xr23,  xr9,   xr3
-
-    xvdp2.h.bu.b     xr10,   xr0,   xr22
-    xvdp2.h.bu.b     xr12,   xr1,   xr22
-    xvdp2.h.bu.b     xr14,   xr2,   xr22
-    xvdp2.h.bu.b     xr16,   xr3,   xr22
-
-    HADDWDH          xr10    //h0 mid0 mid1 mid2 mid3
-    HADDWDH          xr12    //h1 mid4 mid5 mid6 mid7
-    HADDWDH          xr14    //h2
-    HADDWDH          xr16    //h3
-
-    xvpickev.w       xr10,   xr12,    xr10
-    xvpickev.w       xr14,   xr16,    xr14
-    xvpermi.d        xr10,   xr10,    0xd8
-    xvpermi.d        xr14,   xr14,    0xd8
-    xvpickev.h       xr10,   xr14,    xr10
-    xvpermi.d        xr10,   xr10,    0xd8
-    xvsrari.h        xr10,   xr10,    2
-
-    xvst             xr10,   a0,      0
-    addi.d           a0,     a0,      32
-    addi.w           a4,     a4,      -4
-    bnez             a4,     .l_\lable\()h_4w
-    b                .l_\lable\()end_pre_8tap
-
-.l_\lable\()h_8w:
-    xvld             xr0,    a1,      0
-    xvldx            xr2,    a1,      a2
-    xvldx            xr4,    a1,      t2
-    xvldx            xr6,    a1,      t3
-    add.d            a1,     a1,      t4
-
-    xvbsrl.v         xr1,    xr0,     4
-    xvbsrl.v         xr3,    xr2,     4
-    xvbsrl.v         xr5,    xr4,     4
-    xvbsrl.v         xr7,    xr6,     4
-
-    SHUFB            xr0,    xr23,    xr9,    xr10
-    SHUFB            xr1,    xr23,    xr9,    xr11
-    SHUFB            xr2,    xr23,    xr9,    xr12
-    SHUFB            xr3,    xr23,    xr9,    xr13
-    SHUFB            xr4,    xr23,    xr9,    xr14
-    SHUFB            xr5,    xr23,    xr9,    xr15
-    SHUFB            xr6,    xr23,    xr9,    xr16
-    SHUFB            xr7,    xr23,    xr9,    xr17
-
-    xvdp2.h.bu.b     xr0,    xr10,    xr22
-    xvdp2.h.bu.b     xr1,    xr11,    xr22
-    xvdp2.h.bu.b     xr2,    xr12,    xr22
-    xvdp2.h.bu.b     xr3,    xr13,    xr22
-    xvdp2.h.bu.b     xr4,    xr14,    xr22
-    xvdp2.h.bu.b     xr5,    xr15,    xr22
-    xvdp2.h.bu.b     xr6,    xr16,    xr22
-    xvdp2.h.bu.b     xr7,    xr17,    xr22
-
-    HADDWDH          xr0
-    HADDWDH          xr1
-    HADDWDH          xr2
-    HADDWDH          xr3
-    HADDWDH          xr4
-    HADDWDH          xr5
-    HADDWDH          xr6
-    HADDWDH          xr7
-
-    xvpickev.w       xr0,    xr1,    xr0
-    xvpickev.w       xr2,    xr3,    xr2
-    xvpermi.d        xr0,    xr0,    0xd8
-    xvpermi.d        xr2,    xr2,    0xd8
-    xvpickev.h       xr0,    xr2,    xr0
-    xvpermi.d        xr0,    xr0,    0xd8
-    xvsrari.h        xr0,    xr0,    2
-
-    xvpickev.w       xr4,    xr5,    xr4
-    xvpickev.w       xr6,    xr7,    xr6
-    xvpermi.d        xr4,    xr4,    0xd8
-    xvpermi.d        xr6,    xr6,    0xd8
-    xvpickev.h       xr4,    xr6,    xr4
-    xvpermi.d        xr4,    xr4,    0xd8
-    xvsrari.h        xr4,    xr4,    2
-
-    xvst             xr0,    a0,     0
-    xvst             xr4,    a0,     32
-    addi.d           a0,     a0,     64
-    addi.d           a4,     a4,     -4
-    bnez             a4,     .l_\lable\()h_8w
-    b                .l_\lable\()end_pre_8tap
-
-.l_\lable\()h_16w:
-    xvld             xr0,    a1,     0
-    xvldx            xr1,    a1,     a2
-    xvldx            xr2,    a1,     t2
-    xvldx            xr3,    a1,     t3
-    add.d            a1,     a1,     t4
-
-    PREP_W16_H       xr0
-    PREP_W16_H       xr1
-    PREP_W16_H       xr2
-    PREP_W16_H       xr3
-
-    xvst             xr0,    a0,     0
-    xvst             xr1,    a0,     32
-    xvst             xr2,    a0,     64
-    xvst             xr3,    a0,     96
-
-    addi.d           a0,     a0,     128
-    addi.w           a4,     a4,     -4
-    bnez             a4,     .l_\lable\()h_16w
-    b                .l_\lable\()end_pre_8tap
-
-.l_\lable\()h_32w:
-.l_\lable\()h_64w:
-.l_\lable\()h_128w:
-    addi.d           t0,     a1,     0 //src
-    addi.d           t5,     a4,     0 //h
-    srli.w           t7,     a3,     4 //w
-    slli.w           t7,     t7,     5 //store offset
-    addi.d           t8,     a0,     0 //dst
-.l_\lable\()h_16_loop:
-    xvld             xr0,    a1,     0
-    xvldx            xr1,    a1,     a2
-    xvldx            xr2,    a1,     t2
-    xvldx            xr3,    a1,     t3
-    add.d            a1,     a1,     t4
-
-    PREP_W16_H       xr0
-    PREP_W16_H       xr1
-    PREP_W16_H       xr2
-    PREP_W16_H       xr3
-
-    xvst             xr0,    a0,     0
-    xvstx            xr1,    a0,     t7
-    slli.w           t1,     t7,     1
-    xvstx            xr2,    a0,     t1
-    add.w            t1,     t1,     t7
-    xvstx            xr3,    a0,     t1
-    slli.w           t1,     t7,     2
-    add.d            a0,     a0,     t1
-    addi.d           a4,     a4,     -4
-    bnez             a4,     .l_\lable\()h_16_loop
-
-    addi.d           a1,     t0,     16
-    addi.d           t0,     t0,     16
-    addi.d           a0,     t8,     32
-    addi.d           t8,     t8,     32
-    addi.d           a4,     t5,     0
-    addi.d           a3,     a3,     -16
-    bnez             a3,     .l_\lable\()h_16_loop
-    b                .l_\lable\()end_pre_8tap
-.l_\lable\()hv:
+.l_\lable\()prep_h_jtable_lasx:
+    .hword .l_\lable\()h_128w_lasx - .l_\lable\()prep_h_jtable_lasx
+    .hword .l_\lable\()h_64w_lasx  - .l_\lable\()prep_h_jtable_lasx
+    .hword .l_\lable\()h_32w_lasx  - .l_\lable\()prep_h_jtable_lasx
+    .hword .l_\lable\()h_16w_lasx  - .l_\lable\()prep_h_jtable_lasx
+    .hword .l_\lable\()h_8w_lasx   - .l_\lable\()prep_h_jtable_lasx
+    .hword .l_\lable\()h_4w_lasx   - .l_\lable\()prep_h_jtable_lasx
+
+.l_\lable\()h_4w_lasx:
+    addi.d           a1,    a1,    2
+    la.local         t7,    subpel_h_shuf1
+    vld              vr7,   t7,    0
+    xvreplve0.q      xr7,   xr7
+    xvbsrl.v         xr22,  xr22,  2
+    xvreplve0.w      xr22,  xr22
+.l_\lable\()h_4w_loop_lasx:
+    vld              vr0,   a1,    0
+    vldx             vr1,   a1,    a2
+    vldx             vr2,   a1,    t2
+    vldx             vr3,   a1,    t3
+    add.d            a1,    a1,    t4
+    xvpermi.q        xr1,   xr0,   0x20
+    xvpermi.q        xr3,   xr2,   0x20
+    xvshuf.b         xr1,   xr1,   xr1,   xr7
+    xvshuf.b         xr3,   xr3,   xr3,   xr7
+    xvmulwev.h.bu.b  xr0,   xr1,   xr22
+    xvmulwev.h.bu.b  xr2,   xr3,   xr22
+    xvmaddwod.h.bu.b xr0,   xr1,   xr22
+    xvmaddwod.h.bu.b xr2,   xr3,   xr22
+    xvhaddw.w.h      xr0,   xr0,   xr0
+    xvhaddw.w.h      xr2,   xr2,   xr2
+    xvssrarni.h.w    xr2,   xr0,   2
+    xvpermi.d        xr2,   xr2,   0xd8
+    xvst             xr2,   a0,    0
+    addi.d           a0,    a0,    32
+    addi.w           a4,    a4,    -4
+    bnez             a4,    .l_\lable\()h_4w_loop_lasx
+    b                .l_\lable\()end_pre_8tap_lasx
+
+.l_\lable\()h_8w_lasx:
+    la.local         t7,    subpel_h_shuf1
+    vld              vr6,   t7,    0
+    vbsrl.v          vr23,  vr22,  4 //fh
+    xvreplve0.w      xr23,  xr23
+    xvreplve0.w      xr22,  xr22
+    xvreplve0.q      xr19,  xr6
+    xvaddi.bu        xr20,  xr19,  4
+    xvaddi.bu        xr21,  xr19,  8
+.l_\lable\()h_8w_loop_lasx:
+    xvld             xr0,   a1,    0
+    xvldx            xr1,   a1,    a2
+    add.d            a1,    a1,    t2
+    xvpermi.q        xr0,   xr1,   0x02
+    PREP_HV_8W_LASX  xr0
+    xvst             xr0,   a0,    0
+    addi.d           a0,    a0,    32
+    addi.d           a4,    a4,   -2
+    bnez             a4,    .l_\lable\()h_8w_loop_lasx
+    b                .l_\lable\()end_pre_8tap_lasx
+
+.l_\lable\()h_16w_lasx:
+    la.local         t7,    subpel_h_shuf1
+    vld              vr6,   t7,    0
+    vbsrl.v          vr23,  vr22,  4 //fh
+    xvreplve0.w      xr23,  xr23
+    xvreplve0.w      xr22,  xr22
+    xvreplve0.q      xr19,  xr6
+    xvaddi.bu        xr20,  xr19,  4
+    xvaddi.bu        xr21,  xr19,  8
+.l_\lable\()h_16w_loop_lasx:
+    xvld             xr0,   a1,    0
+    xvld             xr1,   a1,    8
+    add.d            a1,    a1,    a2
+    xvpermi.q        xr0,   xr1,   0x02
+    PREP_HV_8W_LASX  xr0
+    xvst             xr0,   a0,    0
+    xvld             xr0,   a1,    0
+    xvld             xr1,   a1,    8
+    add.d            a1,    a1,    a2
+    xvpermi.q        xr0,   xr1,   0x02
+    PREP_HV_8W_LASX  xr0
+    xvst             xr0,   a0,    32
+    addi.d           a0,    a0,    64
+    addi.w           a4,    a4,    -2
+    bnez             a4,     .l_\lable\()h_16w_loop_lasx
+    b                .l_\lable\()end_pre_8tap_lasx
+
+.l_\lable\()h_32w_lasx:
+.l_\lable\()h_64w_lasx:
+.l_\lable\()h_128w_lasx:
+    la.local         t7,    subpel_h_shuf1
+    vld              vr6,   t7,    0
+    vbsrl.v          vr23,  vr22,  4 //fh
+    xvreplve0.w      xr23,  xr23
+    xvreplve0.w      xr22,  xr22
+    xvreplve0.q      xr19,  xr6
+    xvaddi.bu        xr20,  xr19,  4
+    xvaddi.bu        xr21,  xr19,  8
+    addi.d           t5,    a1,    0 //src
+    addi.d           t6,    a3,    0 //w
+    slli.w           t7,    a3,    1 //store offset
+    addi.d           t8,    a0,    0 //dst
+.l_\lable\()h_16_loop_lasx:
+    xvld             xr0,   a1,    0
+    xvld             xr1,   a1,    8
+    xvpermi.q        xr0,   xr1,   0x02
+    PREP_HV_8W_LASX  xr0
+    xvst             xr0,   a0,    0
+    xvld             xr0,   a1,    16
+    xvld             xr1,   a1,    24
+    xvpermi.q        xr0,   xr1,   0x02
+    PREP_HV_8W_LASX  xr0
+    xvst             xr0,   a0,    32
+    addi.d           a0,    a0,    64
+    addi.d           a1,    a1,    32
+    addi.d           a3,    a3,   -32
+    bnez             a3,    .l_\lable\()h_16_loop_lasx
+    add.d            a1,    t5,    a2
+    add.d            t5,    t5,    a2
+    add.d            a0,    t8,    t7
+    add.d            t8,    t8,    t7
+    addi.d           a3,    t6,    0
+    addi.d           a4,    a4,    -1
+    bnez             a4,    .l_\lable\()h_16_loop_lasx
+    b                .l_\lable\()end_pre_8tap_lasx
+
+.l_\lable\()hv_lasx:
     andi             t1,    a7,    3
-    blt              t0,    a3,    .l_\lable\()hv_idx_fh
+    blt              t0,    a3,    .l_\lable\()hv_idx_fh_lasx
     andi             t1,    a7,    1
     addi.w           t1,    t1,    3
-.l_\lable\()hv_idx_fh:
+.l_\lable\()hv_idx_fh_lasx:
     addi.w           t5,    zero,  120
     mul.w            t1,    t1,    t5
     addi.w           t5,    a5,    -1
@@ -4075,641 +3855,465 @@ endconst
     add.d            t1,    t6,    t1 //fh's offset
     xvldrepl.d       xr22,  t1,    0
     srli.w           a7,    a7,    2
-    blt              t0,    a4,    .l_\lable\()hv_idx_fv
+    blt              t0,    a4,    .l_\lable\()hv_idx_fv_lasx
     andi             a7,    a7,    1
     addi.w           a7,    a7,    3
-.l_\lable\()hv_idx_fv:
-    addi.w           t5,     zero,  120
-    mul.w            a7,     a7,    t5
-    addi.w           t5,     a6,    -1
-    slli.w           t5,     t5,    3
-    add.w            a7,     a7,    t5
-    add.d            a7,     t6,    a7 //fv's offset
-    xvldrepl.d       xr8,    a7,    0
-    xvsllwil.h.b     xr8,    xr8,   0
-
-    sub.d            a1,     a1,     t3
-    addi.d           a1,     a1,     -3
-    beq              a3,     t0,     .l_\lable\()hv_4w
-    b                .l_\lable\()hv_8w
-.l_\lable\()hv_4w:
-    xvld             xr0,    a1,     0
-    xvldx            xr1,    a1,     a2
-    xvldx            xr2,    a1,     t2
-    xvldx            xr3,    a1,     t3
-    add.d            a1,     a1,     t4
-    xvld             xr4,    a1,     0
-    xvldx            xr5,    a1,     a2
-    xvldx            xr6,    a1,     t2
-
-    SHUFB            xr0,    xr23,   xr9,   xr0
-    SHUFB            xr1,    xr23,   xr9,   xr1
-    SHUFB            xr2,    xr23,   xr9,   xr2
-    SHUFB            xr3,    xr23,   xr9,   xr3
-
-    SHUFB            xr4,    xr23,   xr9,   xr4
-    SHUFB            xr5,    xr23,   xr9,   xr5
-    SHUFB            xr6,    xr23,   xr9,   xr6
-
-    xvdp2.h.bu.b     xr10,   xr0,    xr22
-    xvdp2.h.bu.b     xr11,   xr1,    xr22
-    xvdp2.h.bu.b     xr12,   xr2,    xr22
-    xvdp2.h.bu.b     xr13,   xr3,    xr22
-
-    xvdp2.h.bu.b     xr14,   xr4,    xr22
-    xvdp2.h.bu.b     xr15,   xr5,    xr22
-    xvdp2.h.bu.b     xr16,   xr6,    xr22
-
-    HADDWDH          xr10    //h0 mid0 mid1 mid2 mid3
-    HADDWDH          xr11    //h1 mid4 mid5 mid6 mid7
-    HADDWDH          xr12    //h2
-    HADDWDH          xr13    //h3
-
-    xvpackev.w       xr10,   xr11,   xr10
-    xvpackev.w       xr12,   xr13,   xr12
-    xvpackev.d       xr11,   xr12,   xr10
-    xvpackod.d       xr10,   xr12,   xr10
-    xvpickev.h       xr11,   xr10,   xr11
-    xvsrari.h        xr11,   xr11,   2
-
-    HADDWDH          xr14    //h4
-    HADDWDH          xr15    //h5
-    HADDWDH          xr16    //h6
-
-    xvpackev.w       xr14,   xr15,   xr14
-    xvpackev.w       xr16,   xr17,   xr16
-    xvpackev.d       xr17,   xr16,   xr14
-    xvpackod.d       xr14,   xr16,   xr14
-    xvpickev.h       xr13,   xr14,   xr17
-    xvsrari.h        xr13,   xr13,   2
-
-    xvpackev.d       xr18,   xr13,   xr11 //0 4 8 12 16 20 24 *  2 6 10 14 18 22 26 *
-    xvpackod.d       xr19,   xr13,   xr11 //1 5 9 13 17 21 25 *  3 7 11 15 19 23 27 *
-.l_\lable\()hv_w4_loop:
-    xvldx            xr0,    a1,     t3
-    add.d            a1,     a1,     t4
-    xvld             xr1,    a1,     0
-    xvldx            xr2,    a1,     a2
-    xvldx            xr3,    a1,     t2
-
-    SHUFB            xr0,    xr23,   xr9,   xr0
-    SHUFB            xr1,    xr23,   xr9,   xr1
-    SHUFB            xr2,    xr23,   xr9,   xr2
-    SHUFB            xr3,    xr23,   xr9,   xr3
-
-    xvdp2.h.bu.b     xr10,   xr0,    xr22
-    xvdp2.h.bu.b     xr12,   xr1,    xr22
-    xvdp2.h.bu.b     xr14,   xr2,    xr22
-    xvdp2.h.bu.b     xr16,   xr3,    xr22
-
-    HADDWDH          xr10    //h0 mid0 mid1 mid2 mid3
-    HADDWDH          xr12    //h1 mid4 mid5 mid6 mid7
-    HADDWDH          xr14    //h2
-    HADDWDH          xr16    //h3
-
-    xvpackev.w       xr10,   xr12,    xr10
-    xvpackev.w       xr14,   xr16,    xr14
-    xvpackev.d       xr12,   xr14,    xr10
-    xvpackod.d       xr10,   xr14,    xr10
-    xvpickev.h       xr12,   xr10,    xr12
-    xvsrari.h        xr12,   xr12,    2
-
-    xvextrins.h      xr18,   xr12,    0x70 //0 4 8 12 16 20 24  0(x0)   2 6 10 14 18 22 26  2(x2)
-    xvextrins.h      xr19,   xr12,    0x74 //1 5 9 13 17 21 25  0(x1)   3 7 11 15 19 23 27  2(x3)
-
-    xvdp2.w.h        xr0,    xr18,    xr8
-    xvdp2.w.h        xr2,    xr19,    xr8
-    HADDWQW          xr0
-    HADDWQW          xr2
-    xvpackev.w       xr0,    xr2,     xr0
-
-    xvbsrl.v         xr18,   xr18,    2
-    xvbsrl.v         xr19,   xr19,    2
-    xvextrins.h      xr18,   xr12,    0x71
-    xvextrins.h      xr19,   xr12,    0x75
-    xvdp2.w.h        xr2,    xr18,    xr8
-    xvdp2.w.h        xr4,    xr19,    xr8
-    HADDWQW          xr2
-    HADDWQW          xr4
-    xvpackev.w       xr2,    xr4,     xr2
-
-    xvbsrl.v         xr18,   xr18,    2
-    xvbsrl.v         xr19,   xr19,    2
-    xvextrins.h      xr18,   xr12,    0x72
-    xvextrins.h      xr19,   xr12,    0x76
-    xvdp2.w.h        xr4,    xr18,    xr8
-    xvdp2.w.h        xr9,    xr19,    xr8
-    HADDWQW          xr4
-    HADDWQW          xr9
-    xvpackev.w       xr4,    xr9,     xr4
-
-    xvbsrl.v         xr18,   xr18,    2
-    xvbsrl.v         xr19,   xr19,    2
-    xvextrins.h      xr18,   xr12,    0x73
-    xvextrins.h      xr19,   xr12,    0x77
-    xvdp2.w.h        xr9,    xr18,    xr8
-    xvdp2.w.h        xr11,   xr19,    xr8
-    HADDWQW          xr9
-    HADDWQW          xr11
-    xvpackev.w       xr9,    xr11,    xr9
-
-    xvpackev.d       xr0,    xr2,     xr0
-    xvpackev.d       xr4,    xr9,     xr4
-    xvsrari.w        xr0,    xr0,     6
-    xvsrari.w        xr4,    xr4,     6
-    xvpermi.d        xr0,    xr0,     0xd8
-    xvpermi.d        xr4,    xr4,     0xd8
-    xvpickev.h       xr0,    xr4,     xr0
-    xvpermi.d        xr0,    xr0,     0xd8
-    xvst             xr0,    a0,      0
-    addi.d           a0,     a0,      32
-
-    xvbsrl.v         xr18,   xr18,    2
-    xvbsrl.v         xr19,   xr19,    2
-
-    addi.d           a4,     a4,      -4
-    bnez             a4,     .l_\lable\()hv_w4_loop
-    b                .l_\lable\()end_pre_8tap
-
-.l_\lable\()hv_8w:
-    addi.d           t0,     a1,      0
-    addi.d           t5,     a4,      0
-    srli.w           t7,     a3,      3
-    slli.w           t7,     t7,      4 // store offset
-    addi.d           t8,     a0,      0
-.l_\lable\()hv_8w_loop0:
-    xvld             xr0,    a1,      0
-    xvldx            xr2,    a1,      a2
-    xvldx            xr4,    a1,      t2
-    xvldx            xr6,    a1,      t3
-
-    add.d            a1,     a1,      t4
-    xvld             xr10,   a1,      0
-    xvldx            xr11,   a1,      a2
-    xvldx            xr12,   a1,      t2
-
-    xvbsrl.v         xr1,    xr0,     4
-    xvbsrl.v         xr3,    xr2,     4
-    xvbsrl.v         xr5,    xr4,     4
-    xvbsrl.v         xr7,    xr6,     4
-
-    SHUFB            xr0,    xr23,    xr9,    xr13
-    SHUFB            xr1,    xr23,    xr9,    xr14
-    SHUFB            xr2,    xr23,    xr9,    xr15
-    SHUFB            xr3,    xr23,    xr9,    xr16
-    SHUFB            xr4,    xr23,    xr9,    xr17
-    SHUFB            xr5,    xr23,    xr9,    xr18
-    SHUFB            xr6,    xr23,    xr9,    xr19
-    SHUFB            xr7,    xr23,    xr9,    xr20
-
-    xvdp2.h.bu.b     xr0,    xr13,    xr22
-    xvdp2.h.bu.b     xr1,    xr14,    xr22
-    xvdp2.h.bu.b     xr2,    xr15,    xr22
-    xvdp2.h.bu.b     xr3,    xr16,    xr22
-    xvdp2.h.bu.b     xr4,    xr17,    xr22
-    xvdp2.h.bu.b     xr5,    xr18,    xr22
-    xvdp2.h.bu.b     xr6,    xr19,    xr22
-    xvdp2.h.bu.b     xr7,    xr20,    xr22
-
-    HADDWDH          xr0
-    HADDWDH          xr1
-    HADDWDH          xr2
-    HADDWDH          xr3
-    HADDWDH          xr4
-    HADDWDH          xr5
-    HADDWDH          xr6
-    HADDWDH          xr7
-
-    xvpackev.w       xr0,    xr2,    xr0
-    xvpackev.w       xr2,    xr6,    xr4
-    xvpackev.d       xr16,   xr2,    xr0
-    xvpackod.d       xr0,    xr2,    xr0
-    xvpickev.h       xr0,    xr0,    xr16
-    xvsrari.h        xr0,    xr0,    2   // 0 8 16 24  1 9 17 25  2 10 18 26  3 11 19 27
-
-    xvpackev.w       xr1,    xr3,    xr1
-    xvpackev.w       xr3,    xr7,    xr5
-    xvpackev.d       xr16,   xr3,    xr1
-    xvpackod.d       xr1,    xr3,    xr1
-    xvpickev.h       xr1,    xr1,    xr16
-    xvsrari.h        xr1,    xr1,    2   // 4 12 20 28  5 13 21 29  6 14 22 30  7 15 23 31
-
-    xvbsrl.v         xr13,   xr10,    4
-    xvbsrl.v         xr14,   xr11,    4
-    xvbsrl.v         xr15,   xr12,    4
-
-    SHUFB            xr10,   xr23,   xr9,    xr10
-    SHUFB            xr13,   xr23,   xr9,    xr13
-    SHUFB            xr11,   xr23,   xr9,    xr11
-    SHUFB            xr14,   xr23,   xr9,    xr14
-    SHUFB            xr12,   xr23,   xr9,    xr12
-    SHUFB            xr15,   xr23,   xr9,    xr15
-
-    xvdp2.h.bu.b     xr4,    xr10,   xr22
-    xvdp2.h.bu.b     xr5,    xr13,   xr22
-    xvdp2.h.bu.b     xr6,    xr11,   xr22
-    xvdp2.h.bu.b     xr7,    xr14,   xr22
-    xvdp2.h.bu.b     xr9,    xr12,   xr22
-    xvdp2.h.bu.b     xr10,   xr15,   xr22
-
-    HADDWDH          xr4
-    HADDWDH          xr5
-    HADDWDH          xr6
-    HADDWDH          xr7
-    HADDWDH          xr9
-    HADDWDH          xr10
-
-    xvpackev.w       xr4,    xr6,    xr4
-    xvpackev.w       xr9,    xr12,   xr9
-    xvpackev.d       xr16,   xr9,    xr4
-    xvpackod.d       xr11,   xr9,    xr4
-    xvpickev.h       xr2,    xr11,   xr16
-    xvsrari.h        xr2,    xr2,    2   // 32 40 48 *  33 41 49 *  34 42 50 *  35 43 51 *
-
-    xvpackev.w       xr5,    xr7,    xr5
-    xvpackev.w       xr10,   xr12,   xr10
-    xvpackev.d       xr16,   xr10,   xr5
-    xvpackod.d       xr11,   xr10,   xr5
-    xvpickev.h       xr3,    xr11,   xr16
-    xvsrari.h        xr3,    xr3,    2   // 36 44 52 *  37 45 53 *  38 46 54 *  39 47 56 *
-
-    xvpackev.d       xr18,   xr2,    xr0 // 0 8 16 24 32 40 48 *  2 10 18 26 34 42 50 *
-    xvpackod.d       xr19,   xr2,    xr0 // 1 9 17 25 33 41 49 *  3 11 19 27 35 43 51 *
-    xvpackev.d       xr20,   xr3,    xr1 // 4 12 20 28 36 44 52 *  6 14 22 30 38 46 54 *
-    xvpackod.d       xr21,   xr3,    xr1 // 5 13 21 29 37 45 53 *  7 15 23 31 39 47 55 *
-
-.l_\lable\()hv_8w_loop:
-    xvldx            xr0,    a1,     t3
-    add.d            a1,     a1,     t4
-    xvld             xr2,    a1,     0
-    xvldx            xr4,    a1,     a2
-    xvldx            xr6,    a1,     t2
-
-    xvbsrl.v         xr1,    xr0,    4
-    xvbsrl.v         xr3,    xr2,    4
-    xvbsrl.v         xr5,    xr4,    4
-    xvbsrl.v         xr7,    xr6,    4
-
-    SHUFB            xr0,    xr23,   xr9,   xr0
-    SHUFB            xr1,    xr23,   xr9,   xr1
-    SHUFB            xr2,    xr23,   xr9,   xr2
-    SHUFB            xr3,    xr23,   xr9,   xr3
-    SHUFB            xr4,    xr23,   xr9,   xr4
-    SHUFB            xr5,    xr23,   xr9,   xr5
-    SHUFB            xr6,    xr23,   xr9,   xr6
-    SHUFB            xr7,    xr23,   xr9,   xr7
-
-    xvdp2.h.bu.b     xr10,   xr0,    xr22
-    xvdp2.h.bu.b     xr11,   xr1,    xr22
-    xvdp2.h.bu.b     xr12,   xr2,    xr22
-    xvdp2.h.bu.b     xr13,   xr3,    xr22
-    xvdp2.h.bu.b     xr14,   xr4,    xr22
-    xvdp2.h.bu.b     xr15,   xr5,    xr22
-    xvdp2.h.bu.b     xr16,   xr6,    xr22
-    xvdp2.h.bu.b     xr17,   xr7,    xr22
-
-    HADDWDH          xr10
-    HADDWDH          xr11
-    HADDWDH          xr12
-    HADDWDH          xr13
-    HADDWDH          xr14
-    HADDWDH          xr15
-    HADDWDH          xr16
-    HADDWDH          xr17
-
-    xvpackev.w       xr0,    xr12,   xr10
-    xvpackev.w       xr2,    xr16,   xr14
-    xvpackev.d       xr9,    xr2,    xr0
-    xvpackod.d       xr0,    xr2,    xr0
-    xvpickev.h       xr0,    xr0,    xr9
-    xvsrari.h        xr0,    xr0,    2   // 56 64 72 80  57 65 73 81  58 66 74 82  59 67 75 83
-
-    xvpackev.w       xr1,    xr13,   xr11
-    xvpackev.w       xr3,    xr17,   xr15
-    xvpackev.d       xr9,    xr3,    xr1
-    xvpackod.d       xr1,    xr3,    xr1
-    xvpickev.h       xr1,    xr1,    xr9
-    xvsrari.h        xr1,    xr1,    2   // 60 68 76 84  61 69 77 85  62 70 78 86  63 71 79 87
-
-    xvextrins.h      xr18,   xr0,    0x70 // 0 8 16 24 32 40 48 (56)  2 10 18 26 34 42 50 (58)
-    xvextrins.h      xr19,   xr0,    0x74 // 1 9 17 25 33 41 49 (57)  3 11 19 27 35 43 51 (59)
-    xvextrins.h      xr20,   xr1,    0x70
-    xvextrins.h      xr21,   xr1,    0x74
-
-    //h - 1
-    xvdp2.w.h        xr10,   xr18,   xr8
-    xvdp2.w.h        xr11,   xr19,   xr8
-    xvdp2.w.h        xr12,   xr20,   xr8
-    xvdp2.w.h        xr13,   xr21,   xr8
-
-    HADDWQW          xr10
-    HADDWQW          xr11
-    HADDWQW          xr12
-    HADDWQW          xr13
-
-    xvpackev.w       xr2,    xr11,   xr10 //0 1 * * 2 3 * *
-    xvpackev.w       xr3,    xr13,   xr12 //4 5 * * 6 7 * *
-    xvpackev.d       xr2,    xr3,    xr2  //0 1 4 5  2 3 6 7
-    //h - 2
-    xvbsrl.v         xr4,    xr18,   2
-    xvbsrl.v         xr5,    xr19,   2
-    xvbsrl.v         xr6,    xr20,   2
-    xvbsrl.v         xr7,    xr21,   2
-    xvextrins.h      xr4,    xr0,    0x71
-    xvextrins.h      xr5,    xr0,    0x75
-    xvextrins.h      xr6,    xr1,    0x71
-    xvextrins.h      xr7,    xr1,    0x75
-
-    xvdp2.w.h        xr10,   xr4,    xr8
-    xvdp2.w.h        xr11,   xr5,    xr8
-    xvdp2.w.h        xr12,   xr6,    xr8
-    xvdp2.w.h        xr13,   xr7,    xr8
-
-    HADDWQW          xr10
-    HADDWQW          xr11
-    HADDWQW          xr12
-    HADDWQW          xr13
-
-    xvpackev.w       xr14,   xr11,   xr10
-    xvpackev.w       xr15,   xr13,   xr12
-    xvpackev.d       xr14,   xr15,   xr14 //8 9 12 13  10 11 14 15
-    //h - 3
-    xvbsrl.v         xr4,    xr4,    2
-    xvbsrl.v         xr5,    xr5,    2
-    xvbsrl.v         xr6,    xr6,    2
-    xvbsrl.v         xr7,    xr7,    2
-    xvextrins.h      xr4,    xr0,    0x72
-    xvextrins.h      xr5,    xr0,    0x76
-    xvextrins.h      xr6,    xr1,    0x72
-    xvextrins.h      xr7,    xr1,    0x76
-
-    xvdp2.w.h        xr10,   xr4,    xr8
-    xvdp2.w.h        xr11,   xr5,    xr8
-    xvdp2.w.h        xr12,   xr6,    xr8
-    xvdp2.w.h        xr13,   xr7,    xr8
-
-    HADDWQW          xr10
-    HADDWQW          xr11
-    HADDWQW          xr12
-    HADDWQW          xr13
-
-    xvpackev.w       xr15,   xr11,   xr10
-    xvpackev.w       xr16,   xr13,   xr12
-    xvpackev.d       xr15,   xr16,   xr15 //16 17 20 21  18 19 22 23
-    //h - 4
-    xvbsrl.v         xr4,    xr4,    2
-    xvbsrl.v         xr5,    xr5,    2
-    xvbsrl.v         xr6,    xr6,    2
-    xvbsrl.v         xr7,    xr7,    2
-    xvextrins.h      xr4,    xr0,    0x73
-    xvextrins.h      xr5,    xr0,    0x77
-    xvextrins.h      xr6,    xr1,    0x73
-    xvextrins.h      xr7,    xr1,    0x77
-
-    xvdp2.w.h        xr10,   xr4,    xr8
-    xvdp2.w.h        xr11,   xr5,    xr8
-    xvdp2.w.h        xr12,   xr6,    xr8
-    xvdp2.w.h        xr13,   xr7,    xr8
-
-    HADDWQW          xr10
-    HADDWQW          xr11
-    HADDWQW          xr12
-    HADDWQW          xr13
-
-    xvpackev.w       xr16,   xr11,   xr10
-    xvpackev.w       xr17,   xr13,   xr12
-    xvpackev.d       xr16,   xr17,   xr16 //24 25 28 29  26 27 30 31
-
-    xvsrari.w        xr2,    xr2,    6
-    xvsrari.w        xr14,   xr14,   6
-    xvsrari.w        xr15,   xr15,   6
-    xvsrari.w        xr16,   xr16,   6
-
-    xvpermi.d        xr2,    xr2,    0xd8
-    xvpermi.d        xr14,   xr14,   0xd8
-    xvpermi.d        xr15,   xr15,   0xd8
-    xvpermi.d        xr16,   xr16,   0xd8
-    xvpickev.h       xr2,    xr14,   xr2
-    xvpickev.h       xr3,    xr16,   xr15
-    xvpermi.d        xr2,    xr2,    0xd8
-    xvpermi.d        xr3,    xr3,    0xd8
-
-    xvpermi.q        xr10,   xr2,    0x31
-    xvpermi.q        xr11,   xr3,    0x31
-
-    vst              vr2,    a0,     0
-    vstx             vr10,   a0,     t7 //32
-    slli.w           t1,     t7,     1  //64
-    vstx             vr3,    a0,     t1
-    add.w            t1,     t1,     t7 //96
-    vstx             vr11,   a0,     t1
-    slli.w           t1,     t7,     2  //128
-    add.d            a0,     a0,     t1
-
-    xvbsrl.v         xr18,   xr4,    2
-    xvbsrl.v         xr19,   xr5,    2
-    xvbsrl.v         xr20,   xr6,    2
-    xvbsrl.v         xr21,   xr7,    2
-
-    addi.d           a4,     a4,     -4
-    bnez             a4,     .l_\lable\()hv_8w_loop
-
-    addi.d           a1,     t0,     8
-    addi.d           t0,     t0,     8
-    addi.d           a0,     t8,     16
-    addi.d           t8,     t8,     16
-    addi.d           a4,     t5,     0
-    addi.d           a3,     a3,    -8
-    bnez             a3,     .l_\lable\()hv_8w_loop0
-    b                .l_\lable\()end_pre_8tap
-.l_\lable\()v:
-
+.l_\lable\()hv_idx_fv_lasx:
+    addi.w           t5,    zero,  120
+    mul.w            a7,    a7,    t5
+    addi.w           t5,    a6,    -1
+    slli.w           t5,    t5,    3
+    add.w            a7,    a7,    t5
+    add.d            a7,    t6,    a7 //fv's offset
+    xvldrepl.d       xr8,   a7,    0
+    xvsllwil.h.b     xr8,   xr8,   0
+    sub.d            a1,    a1,    t3
+    addi.d           a1,    a1,    -1 //ignore leading 0s
+    beq              a3,    t0,    .l_\lable\()hv_4w_lasx
+    addi.d           a1,    a1,    -2
+    b                .l_\lable\()hv_8w_lasx
+.l_\lable\()hv_4w_lasx:
+    xvld             xr0,   a1,    0
+    xvldx            xr1,   a1,    a2
+    xvldx            xr2,   a1,    t2
+    xvldx            xr3,   a1,    t3
+    add.d            a1,    a1,    t4
+    xvld             xr4,   a1,    0
+    xvldx            xr5,   a1,    a2
+    xvldx            xr6,   a1,    t2
+    la.local         t1,    subpel_h_shuf2
+    xvld             xr7,   t1,    0
+    vbsrl.v          vr22,  vr22,  2
+    xvreplve0.w      xr22,  xr22
+    xvreplve0.q      xr8,   xr8
+    xvrepl128vei.w   xr12,  xr8,   0
+    xvrepl128vei.w   xr13,  xr8,   1
+    xvrepl128vei.w   xr14,  xr8,   2
+    xvrepl128vei.w   xr15,  xr8,   3
+    xvilvl.d         xr0,   xr1,   xr0
+    xvilvl.d         xr2,   xr3,   xr2
+    xvilvl.d         xr4,   xr5,   xr4
+    xvreplve0.q      xr0,   xr0
+    xvreplve0.q      xr2,   xr2
+    xvreplve0.q      xr4,   xr4
+    xvreplve0.q      xr6,   xr6
+    xvshuf.b         xr0,   xr0,   xr0,   xr7
+    xvshuf.b         xr2,   xr2,   xr2,   xr7
+    xvshuf.b         xr4,   xr4,   xr4,   xr7
+    xvshuf.b         xr6,   xr6,   xr6,   xr7
+    xvmulwev.h.bu.b  xr1,   xr0,   xr22
+    xvmulwev.h.bu.b  xr3,   xr2,   xr22
+    xvmulwev.h.bu.b  xr5,   xr4,   xr22
+    xvmulwev.h.bu.b  xr9,   xr6,   xr22
+    xvmaddwod.h.bu.b xr1,   xr0,   xr22
+    xvmaddwod.h.bu.b xr3,   xr2,   xr22
+    xvmaddwod.h.bu.b xr5,   xr4,   xr22
+    xvmaddwod.h.bu.b xr9,   xr6,   xr22
+    xvhaddw.w.h      xr1,   xr1,   xr1  // a0 b0 a1 b1  c0 d0 c1 d1
+    xvhaddw.w.h      xr3,   xr3,   xr3  // a2 b2 a3 b3  c2 d2 c3 d3
+    xvhaddw.w.h      xr5,   xr5,   xr5  // a4 b4 a5 b5  c4 d4 c5 d5
+    xvhaddw.w.h      xr9,   xr9,   xr9  // a6 b6 -  -   c6 d6 -  -
+    xvssrarni.h.w    xr3,   xr1,   2    // a0 b0 a1 b1  a2 b2 a3 b3  c0 d0 c1 d1  c2 d2 c3 d3
+    xvssrarni.h.w    xr9,   xr5,   2    // a4 b4 a5 b5  a6 b6 -  -   c4 d4 c5 d5  c6 d6 -  -
+    xvbsrl.v         xr4,   xr3,   4
+    xvextrins.w      xr4,   xr9,   0x30 // a1 b1 a2 b2  a3 b3 a4 b4  c1 d1 c2 d2  c3 d3 c4 d4
+    xvilvl.h         xr5,   xr4,   xr3  // a0 a1 b0 b1  a1 a2 b1 b2  c0 c1 d0 d1  c1 c2 d1 d2
+    xvilvh.h         xr6,   xr4,   xr3  // a2 a3 b2 b3  a3 a4 b3 b4  c2 c3 d2 d3  c3 c4 d3 d4
+    xvbsrl.v         xr10,  xr9,   4    // a5 b5 a6 b6  -  -  -  -   c5 d5 c6 d6  -  -  -  -
+    xvilvl.h         xr11,  xr10,  xr9  // a4 a5 b4 b5  a5 a6 b5 b6  c4 c5 d4 d5  c5 c6 d5 d6
+.l_\lable\()hv_w4_loop_lasx:
+    xvmulwev.w.h     xr16,  xr5,   xr12 //a0 a1 (h0)
+    xvmulwev.w.h     xr17,  xr6,   xr12 //a2 a3 (h1)
+    xvmulwev.w.h     xr18,  xr6,   xr13 //a2 a3 (h0)
+    xvmulwev.w.h     xr19,  xr11,  xr13 //a4 a5 (h1)
+    xvmulwev.w.h     xr20,  xr11,  xr14 //a4 a5 (h0)
+    xvmaddwod.w.h    xr16,  xr5,   xr12 //
+    xvmaddwod.w.h    xr17,  xr6,   xr12 //
+    xvmaddwod.w.h    xr18,  xr6,   xr13 //
+    xvmaddwod.w.h    xr19,  xr11,  xr13 //
+    xvmaddwod.w.h    xr20,  xr11,  xr14 //
+    xvaddi.wu        xr5,   xr11,   0
+    xvadd.w          xr16,  xr16,  xr18 //a0 a1 + a2 a3
+    xvldx            xr18,  a1,    t3   //a7 b7 c7 d7
+    add.d            a1,    a1,    t4
+    xvadd.w          xr17,  xr17,  xr19 //a2 a3 + a4 a5
+    xvld             xr19,  a1,    0    //a8 b8 c8 d8
+    xvadd.w          xr16,  xr16,  xr20 //a0 a1 + a2 a3 + a4 a5
+    xvldx            xr20,  a1,    a2   //a9 b9 c9 d9
+    xvilvl.d         xr18,  xr19,  xr18
+    xvreplve0.q      xr18,  xr18
+    xvldx            xr19,  a1,    t2   //aa ba ca da
+    xvilvl.d         xr20,  xr19,  xr20
+    xvreplve0.q      xr20,  xr20
+    xvshuf.b         xr18,  xr18,  xr18,  xr7
+    xvshuf.b         xr20,  xr20,  xr20,  xr7
+    xvmulwev.h.bu.b  xr21,  xr18,  xr22
+    xvmulwev.h.bu.b  xr23,  xr20,  xr22
+    xvmaddwod.h.bu.b xr21,  xr18,  xr22
+    xvmaddwod.h.bu.b xr23,  xr20,  xr22
+    xvhaddw.w.h      xr21,  xr21,  xr21 //a7 b7 a8 b8 c7 d7 c8 d8
+    xvhaddw.w.h      xr23,  xr23,  xr23 //a9 b9 aa ba c9 d9 ca da
+    xvssrarni.h.w    xr23,  xr21,  2    //a7 b7 a8 b8  a9 b9 aa ba  c7 d7 c8 d8  c9 d9 ca da
+    xvbsll.v         xr0,   xr23,  4
+    xvextrins.w      xr0,   xr9,   0x02 //a6 b6 a7 b7  a8 b8 a9 b9  c6 d6 c7 d7  c8 d8 c9 d9
+    xvilvl.h         xr6,   xr23,  xr0  //a6 a7 b6 b7  a7 a8 b7 b8  c6 c7 d6 d7  c7 c8 d7 d8
+    xvilvh.h         xr11,  xr23,  xr0  //a8 a9 b8 b9  a9 aa b9 ba  c8 c9 d8 d9  c9 ca d9 da
+    xvbsrl.v         xr9,   xr23,  4
+    xvmulwev.w.h     xr1 ,  xr6,   xr14 //a6 a7 (h0)
+    xvmulwev.w.h     xr2 ,  xr6,   xr15 //a6 a7 (h1)
+    xvmulwev.w.h     xr3 ,  xr11,  xr15 //a8 a9 (h1)
+    xvmaddwod.w.h    xr1 ,  xr6,   xr14
+    xvmaddwod.w.h    xr2 ,  xr6,   xr15
+    xvmaddwod.w.h    xr3 ,  xr11,  xr15
+    xvadd.w          xr17,  xr17,  xr1  //a2 a3 + a4 a5 + a6 a7
+    xvadd.w          xr16,  xr16,  xr2  //a0 a1 + a2 a3 + a4 a5 + a6 a7
+    xvadd.w          xr17,  xr17,  xr3  //a2 a3 + a4 a5 + a6 a7 + a8 a9
+    xvssrarni.h.w    xr17,  xr16,  6    //a01 b01 a12 b12  a23 b23 a34 b34  c01 d01 c12 d12  c23 d23 c34 d34
+    xvpermi.d        xr17,  xr17,  0xd8 //a01 b01 a12 b12  c01 d01 c12 d12  a23 b23 a34 b34  c23 d23 c34 d34
+    xvshuf4i.w       xr17,  xr17,  0xd8
+    xvst             xr17,  a0,    0
+    addi.d           a0,    a0,    32
+    addi.d           a4,    a4,    -4
+    bnez             a4,    .l_\lable\()hv_w4_loop_lasx
+    b                .l_\lable\()end_pre_8tap_lasx
+
+.l_\lable\()hv_8w_lasx:
+    addi.d           sp,    sp,   -4*8
+    fst.d            f24,   sp,    0
+    fst.d            f25,   sp,    8
+    fst.d            f26,   sp,    16
+    fst.d            f27,   sp,    24
+    la.local         t1,    subpel_h_shuf1
+    vld              vr19,  t1,    0
+    addi.d           t0,    a1,    0
+    addi.d           t5,    a4,    0
+    slli.w           t7,    a3,    1 // store offset
+    addi.d           t8,    a0,    0
+    xvreplve0.q      xr19,  xr19
+    xvaddi.bu        xr20,  xr19,  4
+    xvaddi.bu        xr21,  xr19,  8
+    vbsrl.v          vr23,  vr22,  4
+    xvreplve0.w      xr22,  xr22 //f0f1f2f3
+    xvreplve0.w      xr23,  xr23 //f4f5f6f7
+    xvreplve0.q      xr8,   xr8
+    xvrepl128vei.w   xr24,  xr8,   0
+    xvrepl128vei.w   xr25,  xr8,   1
+    xvrepl128vei.w   xr26,  xr8,   2
+    xvrepl128vei.w   xr27,  xr8,   3
+.l_\lable\()hv_8w_loop0_lasx:
+    xvld             xr0,   a1,    0
+    xvldx            xr1,   a1,    a2
+    xvldx            xr2,   a1,    t2
+    add.d            a1,    a1,    t3
+    xvld             xr3,   a1,    0
+    xvldx            xr4,   a1,    a2
+    xvldx            xr5,   a1,    t2
+    xvldx            xr6,   a1,    t3
+    add.d            a1,    a1,    t4
+    xvpermi.q        xr0,   xr3,   0x02 //0 3
+    xvpermi.q        xr1,   xr4,   0x02 //1 4
+    xvpermi.q        xr2,   xr5,   0x02 //2 5
+    xvpermi.q        xr3,   xr6,   0x02 //3 6
+    PREP_HV_8W_LASX  xr0 //a0b0c0d0 e0f0g0h0 a3b3c3d3 e3f3g3h3
+    PREP_HV_8W_LASX  xr1 //a1b1c1d1 e1f1g1h1 a4b4c4d4 e4f4g4h4
+    PREP_HV_8W_LASX  xr2 //a2b2c2d2 e2f2g2h2 a5b5c5d5 e5f5g5h5
+    PREP_HV_8W_LASX  xr3 //a3b3c3d3 e3f3g3h3 a6b6c6d6 e6f6g6h6
+    xvpermi.d        xr0,   xr0,   0xd8
+    xvpermi.d        xr1,   xr1,   0xd8
+    xvpermi.d        xr2,   xr2,   0xd8
+    xvpermi.d        xr18,  xr3,   0xd8
+    xvilvl.h         xr12,  xr1,   xr0 //a0a1b0b1c0c1d0d1 e0e1f0f1g0g1h0h1
+    xvilvh.h         xr13,  xr1,   xr0 //a3a4b3b4c3c4d3d4 e3e4f3f4g3g4h3h4
+    xvilvl.h         xr14,  xr2,   xr1 //a1a2b1b2c1c2d1d2 e1e2f1f2g1g2h1h2
+    xvilvh.h         xr15,  xr2,   xr1 //a4a5b4b5c4c5d4d5 e4e5f4f5g4g5h4h5
+    xvilvl.h         xr16,  xr18,  xr2 //a2a3b2b3c2c3d2d3 e2e3f2f3g2g3h2h3
+    xvilvh.h         xr17,  xr18,  xr2 //a5a6b5b6c5c6d5d6 e5e6f5f6g5g6h5h6
+.l_\lable\()hv_8w_loop_lasx:
+    xvld             xr0,   a1,    0
+    xvldx            xr1,   a1,    a2
+    add.d            a1,    a1,    t2
+    xvpermi.q        xr0,   xr1,   0x02 //7 8
+    PREP_HV_8W_LASX  xr0                //a7b7c7d7e7f7g7h7 a8b8c8d8e8f8g8h8
+    xvpermi.q        xr3,   xr0,   0x03 //a6b6c6d6e6f6g6h6 a7b7c7d7e7f7g7h7
+    xvpermi.d        xr3,   xr3,   0xd8 //a6b6c6d6a7b7c7d7 e6f6g6h6e7f7g7h7
+    xvpermi.d        xr1,   xr0,   0xd8 //a7b7c7d7a8b8c8d8 e7f7g7h7e8f8g8h8
+    xvilvl.h         xr18,  xr1,   xr3  //a6a7b6b7c6c7d6d7 e6e7f6f7g6g7h6h7
+    xvilvh.h         xr2,   xr1,   xr3  //a7a8b7b8c7c8d7d8 e7e8f7f8g7g8h7h8
+    xvaddi.hu        xr3,   xr0,   0
+    xvmulwev.w.h     xr4,   xr12,  xr24 //01
+    xvmulwev.w.h     xr5,   xr14,  xr24 //12
+    xvmulwev.w.h     xr6,   xr16,  xr25 //23
+    xvmulwev.w.h     xr7,   xr13,  xr25 //34
+    xvmulwev.w.h     xr8,   xr15,  xr26 //45
+    xvmulwev.w.h     xr9,   xr17,  xr26 //56
+    xvmulwev.w.h     xr10,  xr18,  xr27 //67
+    xvmulwev.w.h     xr11,  xr2,   xr27 //78
+    xvmaddwod.w.h    xr4,   xr12,  xr24 //01
+    xvmaddwod.w.h    xr5,   xr14,  xr24 //12
+    xvmaddwod.w.h    xr6,   xr16,  xr25 //23
+    xvmaddwod.w.h    xr7,   xr13,  xr25 //34
+    xvmaddwod.w.h    xr8,   xr15,  xr26 //45
+    xvmaddwod.w.h    xr9,   xr17,  xr26 //56
+    xvmaddwod.w.h    xr10,  xr18,  xr27 //67
+    xvmaddwod.w.h    xr11,  xr2,   xr27 //78
+    xvadd.w          xr4,   xr4,   xr6
+    xvadd.w          xr5,   xr5,   xr7
+    xvadd.w          xr4,   xr4,   xr8
+    xvadd.w          xr5,   xr5,   xr9
+    xvadd.w          xr4,   xr4,   xr10
+    xvadd.w          xr5,   xr5,   xr11
+    xvaddi.hu        xr12,  xr16,  0 //01 <-- 23
+    xvaddi.hu        xr14,  xr13,  0 //12 <-- 34
+    xvaddi.hu        xr16,  xr15,  0 //23 <-- 45
+    xvaddi.hu        xr13,  xr17,  0 //34 <-- 56
+    xvaddi.hu        xr15,  xr18,  0 //45 <-- 67
+    xvaddi.hu        xr17,  xr2,   0 //56 <-- 78
+    xvssrarni.h.w    xr5,   xr4,   6
+    xvpermi.d        xr5,   xr5,   0xd8
+    vst              vr5,   a0,    0
+    xvpermi.q        xr5,   xr5,   0x11
+    vstx             vr5,   a0,    t7
+    alsl.d           a0,    t7,    a0,  1
+    addi.d           a4,    a4,   -2
+    bnez             a4,    .l_\lable\()hv_8w_loop_lasx
+    addi.d           a1,    t0,    8
+    addi.d           t0,    t0,    8
+    addi.d           a0,    t8,    16
+    addi.d           t8,    t8,    16
+    addi.d           a4,    t5,    0
+    addi.d           a3,    a3,   -8
+    bnez             a3,    .l_\lable\()hv_8w_loop0_lasx
+    fld.d            f24,   sp,    0
+    fld.d            f25,   sp,    8
+    fld.d            f26,   sp,    16
+    fld.d            f27,   sp,    24
+    addi.d           sp,    sp,    4*8
+    b                .l_\lable\()end_pre_8tap_lasx
+
+.l_\lable\()v_lasx:
     srli.w           a7,    a7,    2
-    blt              t0,    a4,    .l_\lable\()v_idx_fv
+    blt              t0,    a4,    .l_\lable\()v_idx_fv_lasx
     andi             a7,    a7,    1
     addi.w           a7,    a7,    3
-.l_\lable\()v_idx_fv:
-    addi.w           t5,     zero,  120
-    mul.w            a7,     a7,    t5
-    addi.w           t5,     a6,    -1
-    slli.w           t5,     t5,    3
-    add.w            a7,     a7,    t5
-    add.d            a7,     t6,    a7 //fv's offset
-    xvldrepl.d       xr8,    a7,     0
-
-    sub.d            a1,     a1,     t3
-    beq              a3,     t0,     .l_\lable\()v_4w
-    blt              t0,     a3,     .l_\lable\()v_8w
-.l_\lable\()v_4w:
-    fld.s            f0,     a1,     0
-    fldx.s           f1,     a1,     a2
-    fldx.s           f2,     a1,     t2
-    add.d            a1,     a1,     t3
-    fld.s            f3,     a1,     0
-    fldx.s           f4,     a1,     a2
-    fldx.s           f5,     a1,     t2
-    fldx.s           f6,     a1,     t3
-
-    xvilvl.b         xr0,    xr1,    xr0 // 0 1  8 9  16 17 24 25
-    xvilvl.b         xr1,    xr3,    xr2 // 2 3 10 11 18 19 26 27
-    xvilvl.b         xr2,    xr5,    xr4 // 4 5 12 13 20 21 28 29
-    xvilvl.b         xr3,    xr7,    xr6 // 6 7 14 15 22 23 30 31
-    xvilvl.h         xr0,    xr1,    xr0 // 0 1 2 3  8  9  10 11  16 17 18 19  24 25 26 27
-    xvilvl.h         xr1,    xr3,    xr2 // 4 5 6 7  12 13 14 15  20 21 22 23  28 29 30 31
-    xvilvl.w         xr2,    xr1,    xr0
-    xvilvh.w         xr0,    xr1,    xr0
-    xvpermi.q        xr0,    xr2,    0x20
-
-.l_\lable\()v_4w_loop:
-    add.d            a1,     a1,     t4
-    fld.s            f7,     a1,     0  //h0
-    fldx.s           f10,    a1,     a2 //h1
-    fldx.s           f11,    a1,     t2 //h2
-    fldx.s           f12,    a1,     t3 //h3
-
-    xvbsrl.v         xr9,    xr7,    2
-    xvpermi.q        xr9,    xr7,    0x20
-    xvextrins.b      xr0,    xr9,    0x70
-    xvextrins.b      xr0,    xr9,    0xf1
-
-    xvbsrl.v         xr1,    xr0,    1
-    xvbsrl.v         xr7,    xr10,   2
-    xvpermi.q        xr7,    xr10,   0x20
-    xvextrins.b      xr1,    xr7,    0x70
-    xvextrins.b      xr1,    xr7,    0xf1
-
-    xvbsrl.v         xr2,    xr1,    1
-    xvbsrl.v         xr7,    xr11,   2
-    xvpermi.q        xr7,    xr11,   0x20
-    xvextrins.b      xr2,    xr7,    0x70
-    xvextrins.b      xr2,    xr7,    0xf1
-
-    xvbsrl.v         xr3,    xr2,    1
-    xvbsrl.v         xr7,    xr12,   2
-    xvpermi.q        xr7,    xr12,   0x20
-    xvextrins.b      xr3,    xr7,    0x70
-    xvextrins.b      xr3,    xr7,    0xf1
-    xvbsrl.v         xr4,    xr3,    1
-
-    xvdp2.h.bu.b     xr10,   xr0,    xr8
-    xvdp2.h.bu.b     xr11,   xr1,    xr8
-    xvdp2.h.bu.b     xr12,   xr2,    xr8
-    xvdp2.h.bu.b     xr13,   xr3,    xr8
-    HADDWDH          xr10
-    HADDWDH          xr11
-    HADDWDH          xr12
-    HADDWDH          xr13
-    xvpickev.w       xr10,   xr11,   xr10
-    xvpickev.w       xr11,   xr13,   xr12
-    xvpermi.d        xr10,   xr10,   0xd8
-    xvpermi.d        xr11,   xr11,   0xd8
-    xvpickev.h       xr10,   xr11,   xr10
-    xvpermi.d        xr10,   xr10,   0xd8
-    xvsrari.h        xr10,   xr10,   2
-
-    xvaddi.bu        xr0,    xr4,    0
-
-    xvst             xr10,   a0,     0
-    addi.d           a0,     a0,     32
-    addi.w           a4,     a4,     -4
-    bnez             a4,     .l_\lable\()v_4w_loop
-    b                .l_\lable\()end_pre_8tap
-
-.l_\lable\()v_8w:
-    addi.d           t0,     a1,     0
-    addi.d           t5,     a4,     0
-    srli.w           t7,     a3,     2
-    slli.w           t7,     t7,     3
-    addi.d           t8,     a0,     0
-.l_\lable\()v_8w_loop0:
-    fld.s            f0,     a1,     0
-    fldx.s           f1,     a1,     a2
-    fldx.s           f2,     a1,     t2
-    add.d            a1,     a1,     t3
-    fld.s            f3,     a1,     0
-    fldx.s           f4,     a1,     a2
-    fldx.s           f5,     a1,     t2
-    fldx.s           f6,     a1,     t3
-
-    xvilvl.b         xr0,    xr1,    xr0 // 0 1  8 9  16 17 24 25
-    xvilvl.b         xr1,    xr3,    xr2 // 2 3 10 11 18 19 26 27
-    xvilvl.b         xr2,    xr5,    xr4 // 4 5 12 13 20 21 28 29
-    xvilvl.b         xr3,    xr7,    xr6 // 6 7 14 15 22 23 30 31
-    xvilvl.h         xr0,    xr1,    xr0 // 0 1 2 3  8  9  10 11  16 17 18 19  24 25 26 27
-    xvilvl.h         xr1,    xr3,    xr2 // 4 5 6 7  12 13 14 15  20 21 22 23  28 29 30 31
-    xvilvl.w         xr2,    xr1,    xr0
-    xvilvh.w         xr0,    xr1,    xr0
-    xvpermi.q        xr0,    xr2,    0x20
-
-.l_\lable\()v_8w_loop:
-    add.d            a1,     a1,     t4
-    fld.s            f7,     a1,     0  //h0
-    fldx.s           f10,    a1,     a2 //h1
-    fldx.s           f11,    a1,     t2 //h2
-    fldx.s           f12,    a1,     t3 //h3
-
-    xvbsrl.v         xr9,    xr7,    2
-    xvpermi.q        xr9,    xr7,    0x20
-    xvextrins.b      xr0,    xr9,    0x70
-    xvextrins.b      xr0,    xr9,    0xf1
-
-    xvbsrl.v         xr1,    xr0,    1
-    xvbsrl.v         xr7,    xr10,   2
-    xvpermi.q        xr7,    xr10,   0x20
-    xvextrins.b      xr1,    xr7,    0x70
-    xvextrins.b      xr1,    xr7,    0xf1
-
-    xvbsrl.v         xr2,    xr1,    1
-    xvbsrl.v         xr7,    xr11,   2
-    xvpermi.q        xr7,    xr11,   0x20
-    xvextrins.b      xr2,    xr7,    0x70
-    xvextrins.b      xr2,    xr7,    0xf1
-
-    xvbsrl.v         xr3,    xr2,    1
-    xvbsrl.v         xr7,    xr12,   2
-    xvpermi.q        xr7,    xr12,   0x20
-    xvextrins.b      xr3,    xr7,    0x70
-    xvextrins.b      xr3,    xr7,    0xf1
-    xvbsrl.v         xr4,    xr3,    1
-
-    xvdp2.h.bu.b     xr10,   xr0,    xr8
-    xvdp2.h.bu.b     xr11,   xr1,    xr8
-    xvdp2.h.bu.b     xr12,   xr2,    xr8
-    xvdp2.h.bu.b     xr13,   xr3,    xr8
-    HADDWDH          xr10
-    HADDWDH          xr11
-    HADDWDH          xr12
-    HADDWDH          xr13
-    xvpickev.w       xr10,   xr11,   xr10
-    xvpickev.w       xr11,   xr13,   xr12
-    xvpermi.d        xr10,   xr10,   0xd8
-    xvpermi.d        xr11,   xr11,   0xd8
-    xvpickev.h       xr10,   xr11,   xr10
-    xvpermi.d        xr10,   xr10,   0xd8
-    xvsrari.h        xr10,   xr10,   2
-
-    xvaddi.bu        xr0,    xr4,    0
-
-    xvstelm.d        xr10,   a0,     0,    0
-    add.d            a0,     a0,     t7
-    xvstelm.d        xr10,   a0,     0,    1
-    add.d            a0,     a0,     t7
-    xvstelm.d        xr10,   a0,     0,    2
-    add.d            a0,     a0,     t7
-    xvstelm.d        xr10,   a0,     0,    3
-    add.d            a0,     a0,     t7
-    addi.w           a4,     a4,     -4
-    bnez             a4,     .l_\lable\()v_8w_loop
-
-    addi.d           a1,     t0,     4
-    addi.d           t0,     t0,     4
-    addi.d           a0,     t8,     8
-    addi.d           t8,     t8,     8
-    addi.d           a4,     t5,     0
-    addi.d           a3,     a3,     -4
-    bnez             a3,     .l_\lable\()v_8w_loop0
-
-.l_\lable\()end_pre_8tap:
+.l_\lable\()v_idx_fv_lasx:
+    addi.w           t5,    zero,  120
+    mul.w            a7,    a7,    t5
+    addi.w           t5,    a6,    -1
+    slli.w           t5,    t5,    3
+    add.w            a7,    a7,    t5
+    add.d            a7,    t6,    a7 //fv's offset
+    xvldrepl.d       xr8,   a7,    0
+    xvrepl128vei.h   xr12,  xr8,   0
+    xvrepl128vei.h   xr13,  xr8,   1
+    xvrepl128vei.h   xr14,  xr8,   2
+    xvrepl128vei.h   xr15,  xr8,   3
+    sub.d            a1,    a1,    t3
+    beq              a3,    t0,    .l_\lable\()v_4w_lasx
+    addi.w           t0,    t0,    4
+    beq              a3,    t0,    .l_\lable\()v_8w_lasx
+    blt              t0,    a3,    .l_\lable\()v_16w_lasx
+.l_\lable\()v_4w_lasx:
+    la.local         t6,    subpel_h_shuf3
+    xvld             xr11,  t6,    0
+    fld.s            f0,    a1,    0   //a0b0c0d0
+    fldx.s           f1,    a1,    a2  //a1b1c1d1
+    fldx.s           f2,    a1,    t2  //a2b2c2d2
+    add.d            a1,    a1,    t3
+    fld.s            f3,    a1,    0   //a3b3c3d3
+    fldx.s           f4,    a1,    a2  //a4b4c4d4
+    fldx.s           f5,    a1,    t2  //a5b5c5d5
+    fldx.s           f6,    a1,    t3  //a6b6c6d6
+    vilvl.w          vr0,   vr1,   vr0 //01
+    vilvl.w          vr1,   vr3,   vr2 //23
+    vilvl.d          vr0,   vr1,   vr0 //0123
+    vilvl.w          vr2,   vr5,   vr4 //45
+    vilvl.d          vr1,   vr2,   vr1 //2345
+    xvpermi.q        xr0,   xr1,   0x02 //0123 2345
+    xvbsrl.v         xr1,   xr0,   4    //123- 345-
+    xvpermi.q        xr4,   xr6,   0x02
+    xvextrins.w      xr1,   xr4,   0x30 //1234 3456
+    xvilvl.b         xr2,   xr1,   xr0  //0112 2334         //a0a1b0b1c0c1d0d1 a1a2b1b2c1c2d1d2 a2a3b2b3c2c3d2d3 a3a4b3b4c3c4d3d4
+    xvilvh.b         xr3,   xr1,   xr0  //2334 4556         //a2a3b2b3c2c3d2d3 a3a4b3b4c3c4d3d4 a4a5b4b5c4c5d4d5 a5a6b5b6c5c6d5d6
+.l_\lable\()v_4w_loop_lasx:
+    add.d            a1,    a1,    t4
+    fld.s            f0,    a1,    0  //a7b7c7d7
+    fldx.s           f1,    a1,    a2 //a8b8c8d8
+    fldx.s           f4,    a1,    t2 //a9b9c9d9
+    fldx.s           f5,    a1,    t3 //aabacada
+    vilvl.w          vr7,   vr0,   vr6 //67
+    vilvl.w          vr10,  vr4,   vr1 //89
+    vextrins.w       vr7,   vr1,   0x20//678-
+    vextrins.w       vr10,  vr5,   0x20//89a-
+    xvpermi.q        xr7,   xr10,  0x02//678- 89a-
+    xvshuf.b         xr4,   xr7,   xr7,  xr11 //67 78 89 9a //a6a7b6b7c6c7d6d7 a7a8b7b8c7c8d7d8 a8a9b8b9c8c9d8d9 a9aab9bac9cad9da
+    xvpermi.q        xr7,   xr3,   0x11 //4556
+    xvpermi.q        xr7,   xr4,   0x02 //45 56 67 78       //a4a5b4b5c4c5d4d5 a5a6b5b6c5c6d5d6 a6a7b6b7c6c7d6d7 a7a8b7b8c7c8d7d8
+    xvmulwev.h.bu.b  xr16,  xr2,   xr12
+    xvmulwev.h.bu.b  xr17,  xr3,   xr13
+    xvmulwev.h.bu.b  xr18,  xr7,   xr14
+    xvmulwev.h.bu.b  xr19,  xr4,   xr15
+    xvmaddwod.h.bu.b xr16,  xr2,   xr12
+    xvmaddwod.h.bu.b xr17,  xr3,   xr13
+    xvmaddwod.h.bu.b xr18,  xr7,   xr14
+    xvmaddwod.h.bu.b xr19,  xr4,   xr15
+    xvadd.h          xr16,  xr16,  xr17
+    xvadd.h          xr16,  xr16,  xr18
+    xvadd.h          xr16,  xr16,  xr19
+    xvsrari.h        xr16,  xr16,  2
+    xvaddi.bu        xr2,   xr7,   0
+    xvaddi.bu        xr3,   xr4,   0
+    xvaddi.bu        xr6,   xr5,   0
+    xvst             xr16,  a0,    0
+    addi.d           a0,    a0,    32
+    addi.w           a4,    a4,   -4
+    bnez             a4,    .l_\lable\()v_4w_loop_lasx
+    b                .l_\lable\()end_pre_8tap_lasx
+
+.l_\lable\()v_8w_lasx:
+    fld.d            f0,    a1,    0
+    fldx.d           f1,    a1,    a2
+    fldx.d           f2,    a1,    t2
+    add.d            a1,    a1,    t3
+    fld.d            f3,    a1,    0
+    fldx.d           f4,    a1,    a2
+    fldx.d           f5,    a1,    t2
+    fldx.d           f6,    a1,    t3
+    xvpermi.q        xr0,   xr1,   0x02
+    xvpermi.q        xr1,   xr2,   0x02
+    xvilvl.b         xr0,   xr1,   xr0 //01 12
+    xvpermi.q        xr2,   xr3,   0x02
+    xvpermi.q        xr3,   xr4,   0x02
+    xvilvl.b         xr2,   xr3,   xr2 //23 34
+    xvpermi.q        xr4,   xr5,   0x02
+    xvpermi.q        xr5,   xr6,   0x02
+    xvilvl.b         xr4,   xr5,   xr4 //45 56
+.l_\lable\()v_8w_loop_lasx:
+    add.d            a1,    a1,    t4
+    fld.d            f7,    a1,    0   //7
+    fldx.d           f10,   a1,    a2  //8
+    fldx.d           f11,   a1,    t2  //9
+    fldx.d           f18,   a1,    t3  //a
+    xvpermi.q        xr6,   xr7,   0x02
+    xvpermi.q        xr7,   xr10,  0x02
+    xvilvl.b         xr6,   xr7,   xr6  //67 78
+    xvpermi.q        xr10,  xr11,  0x02
+    xvpermi.q        xr11,  xr18,  0x02
+    xvilvl.b         xr10,  xr11,  xr10 //89 9a
+    xvmulwev.h.bu.b  xr1,   xr0,   xr12
+    xvmulwev.h.bu.b  xr3,   xr2,   xr13
+    xvmulwev.h.bu.b  xr5,   xr4,   xr14
+    xvmulwev.h.bu.b  xr7,   xr6,   xr15
+    xvmulwev.h.bu.b  xr9,   xr2,   xr12
+    xvmulwev.h.bu.b  xr11,  xr4,   xr13
+    xvmulwev.h.bu.b  xr16,  xr6,   xr14
+    xvmulwev.h.bu.b  xr17,  xr10,  xr15
+    xvmaddwod.h.bu.b xr1,   xr0,   xr12
+    xvmaddwod.h.bu.b xr3,   xr2,   xr13
+    xvmaddwod.h.bu.b xr5,   xr4,   xr14
+    xvmaddwod.h.bu.b xr7,   xr6,   xr15
+    xvmaddwod.h.bu.b xr9,   xr2,   xr12
+    xvmaddwod.h.bu.b xr11,  xr4,   xr13
+    xvmaddwod.h.bu.b xr16,  xr6,   xr14
+    xvmaddwod.h.bu.b xr17,  xr10,  xr15
+    xvadd.h          xr1,   xr1,   xr3
+    xvadd.h          xr1,   xr1,   xr5
+    xvadd.h          xr1,   xr1,   xr7
+    xvadd.h          xr9,   xr9,   xr11
+    xvadd.h          xr9,   xr9,   xr16
+    xvadd.h          xr9,   xr9,   xr17
+    xvaddi.bu        xr0,   xr4,   0
+    xvaddi.bu        xr2,   xr6,   0
+    xvaddi.bu        xr4,   xr10,  0
+    xvaddi.bu        xr6,   xr18,  0
+    xvsrari.h        xr1,   xr1,   2
+    xvsrari.h        xr9,   xr9,   2
+    xvst             xr1,   a0,    0
+    xvst             xr9,   a0,    32
+    addi.d           a0,    a0,    64
+    addi.w           a4,    a4,   -4
+    bnez             a4,    .l_\lable\()v_8w_loop_lasx
+    b                .l_\lable\()end_pre_8tap_lasx
+
+.l_\lable\()v_16w_lasx:
+    addi.d           t0,    a0,    0 //dst
+    addi.d           t5,    a1,    0 //src
+    slli.w           t7,    a3,    1 //w
+    addi.d           t8,    a4,    0 //h
+.l_\lable\()v_16w_loop0_lasx:
+    vld              vr0,   a1,    0
+    vldx             vr1,   a1,    a2
+    vldx             vr2,   a1,    t2
+    add.d            a1,    a1,    t3
+    vld              vr3,   a1,    0
+    vldx             vr4,   a1,    a2
+    vldx             vr5,   a1,    t2
+    vldx             vr6,   a1,    t3
+    add.d            a1,    a1,    t4
+    xvpermi.d        xr0,   xr0,   0xd8
+    xvpermi.d        xr1,   xr1,   0xd8
+    xvpermi.d        xr2,   xr2,   0xd8
+    xvpermi.d        xr3,   xr3,   0xd8
+    xvpermi.d        xr4,   xr4,   0xd8
+    xvpermi.d        xr5,   xr5,   0xd8
+    xvpermi.d        xr6,   xr6,   0xd8
+    xvilvl.b         xr0,   xr1,   xr0 //01
+    xvilvl.b         xr1,   xr2,   xr1 //12
+    xvilvl.b         xr2,   xr3,   xr2 //23
+    xvilvl.b         xr3,   xr4,   xr3 //34
+    xvilvl.b         xr4,   xr5,   xr4 //45
+    xvilvl.b         xr5,   xr6,   xr5 //56
+.l_\lable\()v_16w_loop_lasx:
+    vld              vr7,   a1,    0   //7
+    vldx             vr10,  a1,    a2  //8
+    add.d            a1,    a1,    t2
+    xvpermi.d        xr7,   xr7,   0xd8
+    xvpermi.d        xr10,  xr10,  0xd8
+    xvilvl.b         xr6,   xr7,   xr6 //67
+    xvilvl.b         xr7,   xr10,  xr7 //78
+    xvmulwev.h.bu.b  xr9,   xr0,   xr12
+    xvmulwev.h.bu.b  xr11,  xr2,   xr13
+    xvmulwev.h.bu.b  xr16,  xr4,   xr14
+    xvmulwev.h.bu.b  xr17,  xr6,   xr15
+    xvmulwev.h.bu.b  xr18,  xr1,   xr12
+    xvmulwev.h.bu.b  xr19,  xr3,   xr13
+    xvmulwev.h.bu.b  xr20,  xr5,   xr14
+    xvmulwev.h.bu.b  xr21,  xr7,   xr15
+    xvmaddwod.h.bu.b xr9,   xr0,   xr12
+    xvmaddwod.h.bu.b xr11,  xr2,   xr13
+    xvmaddwod.h.bu.b xr16,  xr4,   xr14
+    xvmaddwod.h.bu.b xr17,  xr6,   xr15
+    xvmaddwod.h.bu.b xr18,  xr1,   xr12
+    xvmaddwod.h.bu.b xr19,  xr3,   xr13
+    xvmaddwod.h.bu.b xr20,  xr5,   xr14
+    xvmaddwod.h.bu.b xr21,  xr7,   xr15
+    xvadd.h          xr9,   xr9,   xr11
+    xvadd.h          xr9,   xr9,   xr16
+    xvadd.h          xr9,   xr9,   xr17
+    xvadd.h          xr11,  xr18,  xr19
+    xvadd.h          xr11,  xr11,  xr20
+    xvadd.h          xr11,  xr11,  xr21
+    xvsrari.h        xr9,   xr9,   2
+    xvsrari.h        xr11,  xr11,  2
+    xvaddi.bu        xr0,   xr2,   0
+    xvaddi.bu        xr1,   xr3,   0
+    xvaddi.bu        xr2,   xr4,   0
+    xvaddi.bu        xr3,   xr5,   0
+    xvaddi.bu        xr4,   xr6,   0
+    xvaddi.bu        xr5,   xr7,   0
+    xvaddi.bu        xr6,   xr10,  0
+    xvst             xr9,   a0,    0
+    xvstx            xr11,  a0,    t7
+    alsl.d           a0,    t7,    a0,  1
+    addi.d           a4,    a4,   -2
+    bnez             a4,    .l_\lable\()v_16w_loop_lasx
+    addi.d           a3,    a3,   -16
+    addi.d           a0,    t0,    32
+    addi.d           t0,    t0,    32
+    addi.d           a1,    t5,    16
+    addi.d           t5,    t5,    16
+    addi.d           a4,    t8,    0
+    bnez             a3,    .l_\lable\()v_16w_loop0_lasx
+.l_\lable\()end_pre_8tap_lasx:
 .endm
 
 function prep_8tap_regular_8bpc_lasx
@@ -4756,3 +4360,1786 @@ function prep_8tap_sharp_8bpc_lasx
     addi.w a7, zero, 10
     PREP_8TAP_8BPC_LASX 10
 endfunc
+
+.macro PREP_8TAP_8BPC_LSX lable
+    li.w             t0,     4
+    la.local         t6,     dav1d_mc_subpel_filters
+    la.local         t7,     shufb1
+    vld              vr23,   t7,    0
+    slli.d           t2,     a2,    1  //src_stride*2
+    add.d            t3,     t2,    a2 //src_stride*3
+    slli.d           t4,     t2,    1
+
+    bnez             a5,     .l_\lable\()h_lsx //mx
+    bnez             a6,     .l_\lable\()v_lsx
+
+    clz.w            t1,     a3
+    li.w             t5,     24
+    sub.w            t1,     t1,    t5
+    la.local         t5,     .l_\lable\()prep_hv0_jtable_lsx
+    alsl.d           t1,     t1,    t5,   1
+    ld.h             t8,     t1,    0
+    add.d            t5,     t5,    t8
+    jirl             $r0,    t5,    0
+    .align   3
+.l_\lable\()prep_hv0_jtable_lsx:
+    .hword .l_\lable\()hv0_128w_lsx - .l_\lable\()prep_hv0_jtable_lsx
+    .hword .l_\lable\()hv0_64w_lsx  - .l_\lable\()prep_hv0_jtable_lsx
+    .hword .l_\lable\()hv0_32w_lsx  - .l_\lable\()prep_hv0_jtable_lsx
+    .hword .l_\lable\()hv0_16w_lsx  - .l_\lable\()prep_hv0_jtable_lsx
+    .hword .l_\lable\()hv0_8w_lsx   - .l_\lable\()prep_hv0_jtable_lsx
+    .hword .l_\lable\()hv0_4w_lsx   - .l_\lable\()prep_hv0_jtable_lsx
+
+.l_\lable\()hv0_4w_lsx:
+    fld.s            f0,     a1,    0
+    fldx.s           f1,     a1,    a2
+    add.d            a1,     a1,    t2
+    vilvl.w          vr0,    vr1,   vr0
+    vsllwil.hu.bu    vr0,    vr0,   4
+    vst              vr0,    a0,    0
+    addi.d           a0,     a0,    16
+    addi.d           a4,     a4,    -2
+    bnez             a4,     .l_\lable\()hv0_4w_lsx
+    b                .l_\lable\()end_pre_8tap_lsx
+.l_\lable\()hv0_8w_lsx:
+    fld.d            f0,     a1,    0
+    fldx.d           f1,     a1,    a2
+    add.d            a1,     a1,    t2
+    vsllwil.hu.bu    vr0,    vr0,   4
+    vsllwil.hu.bu    vr1,    vr1,   4
+    vst              vr0,    a0,    0
+    vst              vr1,    a0,    16
+    addi.d           a0,     a0,    32
+    addi.d           a4,     a4,    -2
+    bnez             a4,     .l_\lable\()hv0_8w_lsx
+    b                .l_\lable\()end_pre_8tap_lsx
+.l_\lable\()hv0_16w_lsx:
+    vld              vr0,    a1,    0
+    vldx             vr1,    a1,    a2
+    add.d            a1,     a1,    t2
+    vsllwil.hu.bu    vr2,    vr0,   4
+    vsllwil.hu.bu    vr4,    vr1,   4
+    vexth.hu.bu      vr3,    vr0
+    vexth.hu.bu      vr5,    vr1
+    vslli.h          vr3,    vr3,   4
+    vslli.h          vr5,    vr5,   4
+    vst              vr2,    a0,    0
+    vst              vr3,    a0,    16
+    vst              vr4,    a0,    32
+    vst              vr5,    a0,    48
+    addi.d           a0,     a0,    64
+    addi.d           a4,     a4,    -2
+    bnez             a4,     .l_\lable\()hv0_16w_lsx
+    b                .l_\lable\()end_pre_8tap_lsx
+.l_\lable\()hv0_32w_lsx:
+.l_\lable\()hv0_64w_lsx:
+.l_\lable\()hv0_128w_lsx:
+    addi.d           t0,     a1,    0
+    addi.d           t5,     a4,    0
+    srli.w           t7,     a3,    4
+    slli.w           t7,     t7,    5
+    addi.d           t8,     a0,    0
+.l_\lable\()hv0_16_loop_lsx:
+    vld              vr0,    a1,    0
+    vldx             vr1,    a1,    a2
+    add.d            a1,     a1,    t2
+    vsllwil.hu.bu    vr2,    vr0,   4
+    vsllwil.hu.bu    vr3,    vr1,   4
+    vexth.hu.bu      vr0,    vr0
+    vexth.hu.bu      vr1,    vr1
+    vslli.h          vr0,    vr0,   4
+    vslli.h          vr1,    vr1,   4
+    vst              vr2,    a0,    0
+    vst              vr0,    a0,    16
+    add.d            a0,     a0,    t7
+    vst              vr3,    a0,    0
+    vst              vr1,    a0,    16
+    add.d            a0,     a0,    t7
+    addi.d           a4,     a4,    -2
+    bnez             a4,     .l_\lable\()hv0_16_loop_lsx
+    addi.d           a1,     t0,    16
+    addi.d           t0,     t0,    16
+    addi.d           a0,     t8,    32
+    addi.d           t8,     t8,    32
+    addi.d           a4,     t5,    0
+    addi.d           a3,     a3,    -16
+    bnez             a3,     .l_\lable\()hv0_16_loop_lsx
+    b                .l_\lable\()end_pre_8tap_lsx
+.l_\lable\()h_lsx:
+    bnez             a6,     .l_\lable\()hv_lsx //if(fh) && if (fv)
+
+    andi             t1,     a7,    3
+    blt              t0,     a3,    .l_\lable\()h_idx_fh_lsx
+    andi             t1,     a7,    1
+    addi.w           t1,     t1,    3
+.l_\lable\()h_idx_fh_lsx:
+    addi.w           t5,     zero,  120
+    mul.w            t1,     t1,    t5
+    addi.w           t5,     a5,    -1
+    slli.w           t5,     t5,    3
+    add.w            t1,     t1,    t5
+    add.d            t1,     t6,    t1 //fh's offset
+    vldrepl.d        vr23,   t1,    0
+
+    addi.d           a1,     a1,    -3
+    clz.w            t1,     a3
+    li.w             t5,     24
+    sub.w            t1,     t1,    t5
+    la.local         t5,     .l_\lable\()prep_h_jtable_lsx
+    alsl.d           t1,     t1,    t5,   1
+    ld.h             t8,     t1,    0
+    add.d            t5,     t5,    t8
+    jirl             $r0,    t5,    0
+
+    .align   3
+.l_\lable\()prep_h_jtable_lsx:
+    .hword .l_\lable\()h_128w_lsx - .l_\lable\()prep_h_jtable_lsx
+    .hword .l_\lable\()h_64w_lsx  - .l_\lable\()prep_h_jtable_lsx
+    .hword .l_\lable\()h_32w_lsx  - .l_\lable\()prep_h_jtable_lsx
+    .hword .l_\lable\()h_16w_lsx  - .l_\lable\()prep_h_jtable_lsx
+    .hword .l_\lable\()h_8w_lsx   - .l_\lable\()prep_h_jtable_lsx
+    .hword .l_\lable\()h_4w_lsx   - .l_\lable\()prep_h_jtable_lsx
+
+.l_\lable\()h_4w_lsx:
+    addi.d           a1,     a1,    2
+    la.local         t7,     subpel_h_shuf1
+    vld              vr7,    t7,    0
+    vbsrl.v          vr23,   vr23,  2
+    vreplvei.w       vr23,   vr23,  0
+.l_\lable\()h_4w_loop_lsx:
+    vld              vr0,    a1,    0
+    vldx             vr1,    a1,    a2
+    add.d            a1,     a1,    t2
+    vshuf.b          vr0,    vr0,   vr0,   vr7
+    vshuf.b          vr1,    vr1,   vr1,   vr7
+    vmulwev.h.bu.b   vr2,    vr0,   vr23
+    vmulwev.h.bu.b   vr3,    vr1,   vr23
+    vmaddwod.h.bu.b  vr2,    vr0,   vr23
+    vmaddwod.h.bu.b  vr3,    vr1,   vr23
+    vhaddw.w.h       vr0,    vr2,   vr2
+    vhaddw.w.h       vr1,    vr3,   vr3
+    vssrarni.h.w     vr1,    vr0,   2
+    vst              vr1,    a0,    0
+    addi.d           a0,     a0,    16
+    addi.w           a4,     a4,    -2
+    bnez             a4,     .l_\lable\()h_4w_loop_lsx
+    b                .l_\lable\()end_pre_8tap_lsx
+
+.l_\lable\()h_8w_lsx:
+    vreplvei.w       vr22,   vr23,  0 //fh
+    vreplvei.w       vr23,   vr23,  1
+    la.local         t7,     subpel_h_shuf1
+    vld              vr6,    t7,    0
+    vaddi.bu         vr7,    vr6,   4
+    vaddi.bu         vr8,    vr6,   8
+.l_\lable\()h_8w_loop_lsx:
+    vld              vr0,    a1,    0
+    vldx             vr1,    a1,    a2
+    add.d            a1,     a1,    t2
+    PREP_H_8W        vr0
+    PREP_H_8W        vr1
+    vst              vr0,    a0,    0
+    vst              vr1,    a0,    16
+    addi.d           a0,     a0,    32
+    addi.d           a4,     a4,    -2
+    bnez             a4,     .l_\lable\()h_8w_loop_lsx
+    b                .l_\lable\()end_pre_8tap_lsx
+
+.l_\lable\()h_16w_lsx:
+.l_\lable\()h_32w_lsx:
+.l_\lable\()h_64w_lsx:
+.l_\lable\()h_128w_lsx:
+    vreplvei.w       vr22,   vr23,  0 //fh
+    vreplvei.w       vr23,   vr23,  1
+    la.local         t7,     subpel_h_shuf1
+    vld              vr6,    t7,    0
+    vaddi.bu         vr7,    vr6,   4
+    vaddi.bu         vr8,    vr6,   8
+    srli.w           t7,     a3,    4
+    slli.w           t6,     t7,    5
+.l_\lable\()h_16w_loop0_lsx:
+    addi.d           t0,     a1,    0 //src
+    addi.d           t5,     a4,    0 //h
+    addi.d           t8,     a0,    0 //dst
+.l_\lable\()h_16w_loop_lsx:
+    vld              vr0,    a1,    0
+    vld              vr1,    a1,    8
+    add.d            a1,     a1,    a2
+    PREP_H_8W        vr0
+    PREP_H_8W        vr1
+    vst              vr0,    a0,    0
+    vst              vr1,    a0,    16
+    add.d            a0,     a0,    t6
+    addi.d           t5,     t5,    -1
+    bnez             t5,     .l_\lable\()h_16w_loop_lsx
+    addi.d           a1,     t0,    16
+    addi.d           a0,     t8,    32
+    addi.w           t7,     t7,    -1
+    bnez             t7,     .l_\lable\()h_16w_loop0_lsx
+    b                .l_\lable\()end_pre_8tap_lsx
+
+.l_\lable\()hv_lsx:
+    andi             t1,     a7,    3
+    blt              t0,     a3,    .l_\lable\()hv_idx_fh_lsx
+    andi             t1,     a7,    1
+    addi.w           t1,     t1,    3
+.l_\lable\()hv_idx_fh_lsx:
+    addi.w           t5,     zero,  120
+    mul.w            t1,     t1,    t5
+    addi.w           t5,     a5,    -1
+    slli.w           t5,     t5,    3
+    add.w            t1,     t1,    t5
+    add.d            t1,     t6,    t1 //fh's offset
+    vldrepl.d        vr8,    t1,    0
+    srli.w           a7,     a7,    2
+    blt              t0,     a4,    .l_\lable\()hv_idx_fv_lsx
+    andi             a7,     a7,    1
+    addi.w           a7,     a7,    3
+.l_\lable\()hv_idx_fv_lsx:
+    addi.w           t5,     zero,  120
+    mul.w            a7,     a7,    t5
+    addi.w           t5,     a6,    -1
+    slli.w           t5,     t5,    3
+    add.w            a7,     a7,    t5
+    add.d            a7,     t6,    a7 //fv's offset
+    vldrepl.d        vr9,    a7,    0
+    vsllwil.h.b      vr9,    vr9,   0
+
+    sub.d            a1,     a1,    t3
+    addi.d           a1,     a1,    -3
+    beq              a3,     t0,    .l_\lable\()hv_4w_lsx
+    b                .l_\lable\()hv_8w_lsx
+.l_\lable\()hv_4w_lsx:
+    addi.d           a1,     a1,    2 //ignore leading 0s
+    vld              vr0,    a1,    0
+    vldx             vr1,    a1,    a2
+    vldx             vr2,    a1,    t2
+    add.d            a1,     a1,    t3
+    vld              vr3,    a1,    0
+    vldx             vr4,    a1,    a2
+    vldx             vr5,    a1,    t2
+    vldx             vr6,    a1,    t3
+    add.d            a1,     a1,    t4
+
+    la.local         t1,     subpel_h_shuf1
+    vld              vr7,    t1,    0
+    vbsrl.v          vr8,    vr8,   2
+    vreplvei.w       vr8,    vr8,   0
+
+    //fv
+    vreplvei.w       vr17,   vr9,   0
+    vreplvei.w       vr18,   vr9,   1
+    vreplvei.w       vr19,   vr9,   2
+    vreplvei.w       vr20,   vr9,   3
+
+    //DAV1D_FILTER_8TAP_RND
+    vshuf.b          vr0,    vr0,   vr0,  vr7
+    vshuf.b          vr1,    vr1,   vr1,  vr7
+    vshuf.b          vr2,    vr2,   vr2,  vr7
+    vshuf.b          vr3,    vr3,   vr3,  vr7
+    vshuf.b          vr4,    vr4,   vr4,  vr7
+    vshuf.b          vr5,    vr5,   vr5,  vr7
+    vshuf.b          vr6,    vr6,   vr6,  vr7
+
+    vmulwev.h.bu.b   vr10,   vr0,   vr8
+    vmulwev.h.bu.b   vr11,   vr1,   vr8
+    vmulwev.h.bu.b   vr12,   vr2,   vr8
+    vmulwev.h.bu.b   vr13,   vr3,   vr8
+    vmulwev.h.bu.b   vr14,   vr4,   vr8
+    vmulwev.h.bu.b   vr15,   vr5,   vr8
+    vmulwev.h.bu.b   vr16,   vr6,   vr8
+    vmaddwod.h.bu.b  vr10,   vr0,   vr8
+    vmaddwod.h.bu.b  vr11,   vr1,   vr8
+    vmaddwod.h.bu.b  vr12,   vr2,   vr8
+    vmaddwod.h.bu.b  vr13,   vr3,   vr8
+    vmaddwod.h.bu.b  vr14,   vr4,   vr8
+    vmaddwod.h.bu.b  vr15,   vr5,   vr8
+    vmaddwod.h.bu.b  vr16,   vr6,   vr8
+
+    vhaddw.w.h       vr10,   vr10,  vr10
+    vhaddw.w.h       vr11,   vr11,  vr11
+    vhaddw.w.h       vr12,   vr12,  vr12
+    vhaddw.w.h       vr13,   vr13,  vr13
+    vhaddw.w.h       vr14,   vr14,  vr14
+    vhaddw.w.h       vr15,   vr15,  vr15
+    vhaddw.w.h       vr16,   vr16,  vr16
+
+    vssrarni.h.w     vr10,   vr10,  2 //h0
+    vssrarni.h.w     vr11,   vr11,  2 //h1
+    vssrarni.h.w     vr12,   vr12,  2 //h2
+    vssrarni.h.w     vr13,   vr13,  2 //h3
+    vssrarni.h.w     vr14,   vr14,  2 //h4
+    vssrarni.h.w     vr15,   vr15,  2 //h5
+    vssrarni.h.w     vr16,   vr16,  2 //h6
+
+    //h0
+    vilvl.h          vr0,    vr11,  vr10 //01
+    vilvl.h          vr1,    vr13,  vr12 //23
+    vilvl.h          vr2,    vr15,  vr14 //45
+    //h1
+    vilvl.h          vr4,    vr12,  vr11 //12
+    vilvl.h          vr5,    vr14,  vr13 //34
+    vilvl.h          vr6,    vr16,  vr15 //56
+
+.l_\lable\()hv_w4_loop_lsx:
+    vld              vr9,    a1,    0
+    vldx             vr10,   a1,    a2
+    add.d            a1,     a1,    t2
+
+    //DAV1D_FILTER_8TAP_CLIP
+    vshuf.b          vr9,    vr9,   vr9,  vr7
+    vshuf.b          vr10,   vr10,  vr10, vr7
+    vmulwev.h.bu.b   vr11,   vr9,   vr8
+    vmulwev.h.bu.b   vr12,   vr10,  vr8
+    vmaddwod.h.bu.b  vr11,   vr9,   vr8
+    vmaddwod.h.bu.b  vr12,   vr10,  vr8
+    vhaddw.w.h       vr11,   vr11,  vr11
+    vhaddw.w.h       vr12,   vr12,  vr12
+    vssrarni.h.w     vr11,   vr11,  2 //7h
+    vssrarni.h.w     vr12,   vr12,  2 //h8
+    vilvl.h          vr3,    vr11,  vr16 //67
+    vilvl.h          vr13,   vr12,  vr11 //78
+
+    vmulwev.w.h      vr9,    vr0,   vr17
+    vmulwev.w.h      vr10,   vr1,   vr18
+    vmulwev.w.h      vr14,   vr2,   vr19
+    vmulwev.w.h      vr15,   vr3,   vr20
+    vmaddwod.w.h     vr9,    vr0,   vr17
+    vmaddwod.w.h     vr10,   vr1,   vr18
+    vmaddwod.w.h     vr14,   vr2,   vr19
+    vmaddwod.w.h     vr15,   vr3,   vr20
+    vadd.w           vr16,   vr9,   vr10
+    vadd.w           vr16,   vr16,  vr14
+    vadd.w           vr16,   vr16,  vr15
+
+    vmulwev.w.h      vr9,    vr4,   vr17
+    vmulwev.w.h      vr10,   vr5,   vr18
+    vmulwev.w.h      vr14,   vr6,   vr19
+    vmulwev.w.h      vr15,   vr13,  vr20
+    vmaddwod.w.h     vr9,    vr4,   vr17
+    vmaddwod.w.h     vr10,   vr5,   vr18
+    vmaddwod.w.h     vr14,   vr6,   vr19
+    vmaddwod.w.h     vr15,   vr13,  vr20
+    vadd.w           vr21,   vr9,   vr10
+    vadd.w           vr21,   vr21,  vr14
+    vadd.w           vr21,   vr21,  vr15
+
+    vssrarni.h.w     vr21,   vr16,  6
+    //cache
+    vaddi.hu         vr0,    vr1,   0
+    vaddi.hu         vr1,    vr2,   0
+    vaddi.hu         vr2,    vr3,   0
+    vaddi.hu         vr4,    vr5,   0
+    vaddi.hu         vr5,    vr6,   0
+    vaddi.hu         vr6,    vr13,  0
+    vaddi.hu         vr16,   vr12,  0
+
+    vst              vr21,   a0,    0
+    addi.d           a0,     a0,    16
+    addi.d           a4,     a4,    -2
+    bnez             a4,     .l_\lable\()hv_w4_loop_lsx
+    b                .l_\lable\()end_pre_8tap_lsx
+
+.l_\lable\()hv_8w_lsx:
+.l_\lable\()hv_16w_lsx:
+.l_\lable\()hv_32w_lsx:
+.l_\lable\()hv_64w_lsx:
+.l_\lable\()hv_128w_lsx:
+    addi.d          sp,      sp,    -8*8
+    fst.d           f24,     sp,    0
+    fst.d           f25,     sp,    8
+    fst.d           f26,     sp,    16
+    fst.d           f27,     sp,    24
+    fst.d           f28,     sp,    32
+    fst.d           f29,     sp,    40
+    fst.d           f30,     sp,    48
+    fst.d           f31,     sp,    56
+    addi.d          t0,      a1,    0 //src
+    addi.d          t5,      a4,    0 //h
+    addi.d          t8,      a0,    0 //dst
+    slli.w          t6,      a3,    1
+    la.local        t1,      subpel_h_shuf1
+    vld             vr7,     t1,    0
+    vaddi.bu        vr11,    vr7,   4
+    vaddi.bu        vr12,    vr7,   8
+    vreplvei.w      vr10,    vr8,   1
+    vreplvei.w      vr8,     vr8,   0
+    vreplvei.w      vr20,    vr9,   1
+    vreplvei.w      vr21,    vr9,   2
+    vreplvei.w      vr22,    vr9,   3
+    vreplvei.w      vr9,     vr9,   0
+.l_\lable\()prep_hv_8w_loop0_lsx:
+    vld             vr0,     a1,    0
+    vldx            vr1,     a1,    a2
+    vldx            vr2,     a1,    t2
+    add.d           a1,      a1,    t3
+    vld             vr3,     a1,    0
+    vldx            vr4,     a1,    a2
+    vldx            vr5,     a1,    t2
+    vldx            vr6,     a1,    t3
+    add.d           a1,      a1,    t4
+
+    FILTER_8TAP_8W  vr0 //h0
+    FILTER_8TAP_8W  vr1 //h1
+    FILTER_8TAP_8W  vr2 //h2
+    FILTER_8TAP_8W  vr3 //h3
+    FILTER_8TAP_8W  vr4 //h4
+    FILTER_8TAP_8W  vr5 //h5
+    FILTER_8TAP_8W  vr6 //h6
+
+    //h0' low part
+    vilvl.h         vr23,    vr1,   vr0 //01
+    vilvl.h         vr24,    vr3,   vr2 //23
+    vilvl.h         vr25,    vr5,   vr4 //45
+    //h0' high part
+    vilvh.h         vr26,    vr1,   vr0 //01
+    vilvh.h         vr27,    vr3,   vr2 //23
+    vilvh.h         vr28,    vr5,   vr4 //45
+
+    //h1' low part
+    vilvl.h         vr29,    vr2,   vr1 //12
+    vilvl.h         vr30,    vr4,   vr3 //34
+    vilvl.h         vr31,    vr6,   vr5 //56
+    //h1' high part
+    vilvh.h         vr0,     vr2,   vr1 //12
+    vilvh.h         vr1,     vr4,   vr3 //34
+    vilvh.h         vr2,     vr6,   vr5 //56
+
+.l_\lable\()prep_hv_8w_loop_lsx:
+    vld             vr3,     a1,    0
+    vldx            vr4,     a1,    a2
+    add.d           a1,      a1,    t2
+
+    FILTER_8TAP_8W  vr3 //h7
+    FILTER_8TAP_8W  vr4 //h8
+
+    //h0' low part
+    vilvl.h         vr16,    vr3,   vr6 //67 ~low
+    vmulwev.w.h     vr13,    vr23,  vr9
+    vmulwev.w.h     vr14,    vr24,  vr20
+    vmulwev.w.h     vr15,    vr25,  vr21
+    vmulwev.w.h     vr17,    vr16,  vr22
+    vmaddwod.w.h    vr13,    vr23,  vr9
+    vmaddwod.w.h    vr14,    vr24,  vr20
+    vmaddwod.w.h    vr15,    vr25,  vr21
+    vmaddwod.w.h    vr17,    vr16,  vr22
+    vadd.w          vr13,    vr13,  vr14
+    vadd.w          vr13,    vr13,  vr15
+    vadd.w          vr13,    vr13,  vr17
+    //cache
+    vaddi.hu        vr23,    vr24,  0
+    vaddi.hu        vr24,    vr25,  0
+    vaddi.hu        vr25,    vr16,  0
+
+    //h0' high part
+    vilvh.h         vr17,    vr3,   vr6 //67 ~high
+    vmulwev.w.h     vr14,    vr26,  vr9
+    vmulwev.w.h     vr15,    vr27,  vr20
+    vmulwev.w.h     vr16,    vr28,  vr21
+    vmulwev.w.h     vr18,    vr17,  vr22
+    vmaddwod.w.h    vr14,    vr26,  vr9
+    vmaddwod.w.h    vr15,    vr27,  vr20
+    vmaddwod.w.h    vr16,    vr28,  vr21
+    vmaddwod.w.h    vr18,    vr17,  vr22
+    vadd.w          vr14,    vr14,  vr15
+    vadd.w          vr14,    vr14,  vr16
+    vadd.w          vr14,    vr14,  vr18
+    vssrarni.h.w    vr14,    vr13,  6
+    vst             vr14,    a0,    0
+    add.d           a0,      a0,    t6
+    //cache
+    vaddi.hu        vr26,    vr27,  0
+    vaddi.hu        vr27,    vr28,  0
+    vaddi.hu        vr28,    vr17,  0
+    vaddi.hu        vr6,     vr4,   0
+
+    vilvl.h         vr5,     vr4,   vr3 //78 ~low
+    vilvh.h         vr4,     vr4,   vr3 //78 ~high
+
+    //h1' low part
+    vmulwev.w.h     vr13,    vr29,  vr9
+    vmulwev.w.h     vr14,    vr30,  vr20
+    vmulwev.w.h     vr15,    vr31,  vr21
+    vmulwev.w.h     vr16,    vr5,   vr22
+    vmaddwod.w.h    vr13,    vr29,  vr9
+    vmaddwod.w.h    vr14,    vr30,  vr20
+    vmaddwod.w.h    vr15,    vr31,  vr21
+    vmaddwod.w.h    vr16,    vr5,   vr22
+    vadd.w          vr13,    vr13,  vr14
+    vadd.w          vr13,    vr13,  vr15
+    vadd.w          vr13,    vr13,  vr16
+    //cache
+    vaddi.hu        vr29,    vr30,  0
+    vaddi.hu        vr30,    vr31,  0
+    vaddi.hu        vr31,    vr5,   0
+
+    //h1' high part
+    vmulwev.w.h     vr14,    vr0,   vr9
+    vmulwev.w.h     vr15,    vr1,   vr20
+    vmulwev.w.h     vr16,    vr2,   vr21
+    vmulwev.w.h     vr17,    vr4,   vr22
+    vmaddwod.w.h    vr14,    vr0,   vr9
+    vmaddwod.w.h    vr15,    vr1,   vr20
+    vmaddwod.w.h    vr16,    vr2,   vr21
+    vmaddwod.w.h    vr17,    vr4,   vr22
+    vadd.w          vr14,    vr14,  vr15
+    vadd.w          vr14,    vr14,  vr16
+    vadd.w          vr14,    vr14,  vr17
+    vssrarni.h.w    vr14,    vr13,  6
+    vst             vr14,    a0,    0
+    add.d           a0,      a0,    t6
+    //cache
+    vaddi.hu        vr0,     vr1,   0
+    vaddi.hu        vr1,     vr2,   0
+    vaddi.hu        vr2,     vr4,   0
+    addi.w          a4,      a4,    -2
+    bnez            a4,      .l_\lable\()prep_hv_8w_loop_lsx
+    addi.d          a1,      t0,    8
+    addi.d          t0,      t0,    8
+    addi.d          a0,      t8,    16
+    addi.d          t8,      t8,    16
+    addi.d          a4,      t5,    0
+    addi.w          a3,      a3,    -8
+    bnez            a3,      .l_\lable\()prep_hv_8w_loop0_lsx
+    fld.d           f24,     sp,    0
+    fld.d           f25,     sp,    8
+    fld.d           f26,     sp,    16
+    fld.d           f27,     sp,    24
+    fld.d           f28,     sp,    32
+    fld.d           f29,     sp,    40
+    fld.d           f30,     sp,    48
+    fld.d           f31,     sp,    56
+    addi.d          sp,      sp,    8*8
+    b                .l_\lable\()end_pre_8tap_lsx
+
+.l_\lable\()v_lsx:
+    srli.w           a7,    a7,     2
+    blt              t0,    a4,     .l_\lable\()v_idx_fv_lsx
+    andi             a7,    a7,     1
+    addi.w           a7,    a7,     3
+.l_\lable\()v_idx_fv_lsx:
+    addi.w           t5,     zero,  120
+    mul.w            a7,     a7,    t5
+    addi.w           t5,     a6,    -1
+    slli.w           t5,     t5,    3
+    add.w            a7,     a7,    t5
+    add.d            a7,     t6,    a7 //fv's offset
+    vldrepl.d        vr8,    a7,    0
+
+    vilvl.h          vr8,    vr8,   vr8
+    vreplvei.w       vr9,    vr8,   1
+    vreplvei.w       vr10,   vr8,   2
+    vreplvei.w       vr11,   vr8,   3
+    vreplvei.w       vr8,    vr8,   0
+
+    sub.d            a1,     a1,    t3
+    beq              a3,     t0,    .l_\lable\()v_4w_lsx
+    blt              t0,     a3,    .l_\lable\()v_8w_lsx
+.l_\lable\()v_4w_lsx:
+    fld.s            f0,     a1,    0
+    fldx.s           f1,     a1,    a2
+    fldx.s           f2,     a1,    t2
+    add.d            a1,     a1,    t3
+    fld.s            f3,     a1,    0
+    fldx.s           f4,     a1,    a2
+    fldx.s           f5,     a1,    t2
+    fldx.s           f6,     a1,    t3
+    add.d            a1,     a1,    t4
+
+    vilvl.w          vr0,    vr1,   vr0
+    vilvl.w          vr1,    vr2,   vr1
+    vilvl.b          vr0,    vr1,   vr0 //0 1 1 2
+    vilvl.w          vr1,    vr3,   vr2
+    vilvl.w          vr2,    vr4,   vr3
+    vilvl.b          vr1,    vr2,   vr1 //2 3 3 4
+    vilvl.w          vr2,    vr5,   vr4
+    vilvl.w          vr3,    vr6,   vr5
+    vilvl.b          vr2,    vr3,   vr2 //4 5 5 6
+.l_\lable\()v_4w_loop_lsx:
+    fld.s            f7,     a1,     0
+
+    vilvl.w          vr3,    vr7,   vr6
+    fldx.s           f6,     a1,    a2
+    add.d            a1,     a1,    t2
+    vilvl.w          vr4,    vr6,   vr7
+    vilvl.b          vr3,    vr4,   vr3 //6 7 7 8
+
+    vmulwev.h.bu.b   vr12,   vr0,   vr8
+    vmulwev.h.bu.b   vr13,   vr1,   vr9
+    vmulwev.h.bu.b   vr14,   vr2,   vr10
+    vmulwev.h.bu.b   vr15,   vr3,   vr11
+    vmaddwod.h.bu.b  vr12,   vr0,   vr8
+    vmaddwod.h.bu.b  vr13,   vr1,   vr9
+    vmaddwod.h.bu.b  vr14,   vr2,   vr10
+    vmaddwod.h.bu.b  vr15,   vr3,   vr11
+    vaddi.hu         vr0,    vr1,   0
+    vaddi.hu         vr1,    vr2,   0
+    vaddi.hu         vr2,    vr3,   0
+    vadd.h           vr12,   vr12,  vr13
+    vadd.h           vr12,   vr12,  vr14
+    vadd.h           vr12,   vr12,  vr15
+
+    vsrari.h         vr12,   vr12,  2
+    vst              vr12,   a0,    0
+    addi.d           a0,     a0,    16
+    addi.w           a4,     a4,    -2
+    bnez             a4,     .l_\lable\()v_4w_loop_lsx
+    b                .l_\lable\()end_pre_8tap_lsx
+
+.l_\lable\()v_8w_lsx:
+    addi.d           t0,     a1,    0
+    addi.d           t5,     a4,    0
+    addi.d           t8,     a0,    0
+    slli.w           t6,     a3,    1
+.l_\lable\()v_8w_loop0_lsx:
+    fld.d            f0,     a1,    0
+    fldx.d           f1,     a1,    a2
+    fldx.d           f2,     a1,    t2
+    add.d            a1,     a1,    t3
+    fld.d            f3,     a1,    0
+    fldx.d           f4,     a1,    a2
+    fldx.d           f5,     a1,    t2
+    fldx.d           f6,     a1,    t3
+    add.d            a1,     a1,    t4
+
+    vilvl.b          vr0,    vr1,   vr0 //0 1
+    vilvl.b          vr1,    vr2,   vr1 //1 2
+    vilvl.b          vr2,    vr3,   vr2 //2 3
+    vilvl.b          vr3,    vr4,   vr3 //3 4
+    vilvl.b          vr4,    vr5,   vr4 //4 5
+    vilvl.b          vr5,    vr6,   vr5 //5 6
+.l_\lable\()v_8w_loop_lsx:
+    fld.d            f7,     a1,    0
+    vilvl.b          vr12,   vr7,   vr6 //6 7
+    fldx.d           f6,     a1,    a2
+    add.d            a1,     a1,    t2
+    vilvl.b          vr13,   vr6,   vr7 //7 8
+
+    vmulwev.h.bu.b   vr14,   vr0,   vr8
+    vmulwev.h.bu.b   vr15,   vr1,   vr8
+    vmulwev.h.bu.b   vr16,   vr2,   vr9
+    vmulwev.h.bu.b   vr17,   vr3,   vr9
+    vmulwev.h.bu.b   vr18,   vr4,   vr10
+    vmulwev.h.bu.b   vr19,   vr5,   vr10
+    vmulwev.h.bu.b   vr20,   vr12,  vr11
+    vmulwev.h.bu.b   vr21,   vr13,  vr11
+    vmaddwod.h.bu.b  vr14,   vr0,   vr8
+    vmaddwod.h.bu.b  vr15,   vr1,   vr8
+    vmaddwod.h.bu.b  vr16,   vr2,   vr9
+    vmaddwod.h.bu.b  vr17,   vr3,   vr9
+    vmaddwod.h.bu.b  vr18,   vr4,   vr10
+    vmaddwod.h.bu.b  vr19,   vr5,   vr10
+    vmaddwod.h.bu.b  vr20,   vr12,  vr11
+    vmaddwod.h.bu.b  vr21,   vr13,  vr11
+
+    vaddi.hu         vr0,    vr2,   0
+    vaddi.hu         vr1,    vr3,   0
+    vaddi.hu         vr2,    vr4,   0
+    vaddi.hu         vr3,    vr5,   0
+    vaddi.hu         vr4,    vr12,  0
+    vaddi.hu         vr5,    vr13,  0
+    vadd.h           vr14,   vr14,  vr16
+    vadd.h           vr14,   vr14,  vr18
+    vadd.h           vr14,   vr14,  vr20
+    vadd.h           vr15,   vr15,  vr17
+    vadd.h           vr15,   vr15,  vr19
+    vadd.h           vr15,   vr15,  vr21
+
+    vsrari.h         vr14,   vr14,  2
+    vsrari.h         vr15,   vr15,  2
+    vst              vr14,   a0,    0
+    add.d            a0,     a0,    t6
+    vst              vr15,   a0,    0
+    add.d            a0,     a0,    t6
+    addi.w           a4,     a4,    -2
+    bnez             a4,     .l_\lable\()v_8w_loop_lsx
+    addi.d           a1,     t0,    8
+    addi.d           t0,     t0,    8
+    addi.d           a0,     t8,    16
+    addi.d           t8,     t8,    16
+    addi.d           a4,     t5,    0
+    addi.d           a3,     a3,    -8
+    bnez             a3,     .l_\lable\()v_8w_loop0_lsx
+.l_\lable\()end_pre_8tap_lsx:
+.endm
+
+function prep_8tap_regular_8bpc_lsx
+    addi.w a7, zero, 0
+    PREP_8TAP_8BPC_LSX 0
+endfunc
+
+function prep_8tap_smooth_regular_8bpc_lsx
+    addi.w a7, zero, 1
+    PREP_8TAP_8BPC_LSX 1
+endfunc
+
+function prep_8tap_sharp_regular_8bpc_lsx
+    addi.w a7, zero, 2
+    PREP_8TAP_8BPC_LSX 2
+endfunc
+
+function prep_8tap_regular_smooth_8bpc_lsx
+    addi.w a7, zero, 4
+    PREP_8TAP_8BPC_LSX 4
+endfunc
+
+function prep_8tap_smooth_8bpc_lsx
+    addi.w a7, zero, 5
+    PREP_8TAP_8BPC_LSX 5
+endfunc
+
+function prep_8tap_sharp_smooth_8bpc_lsx
+    addi.w a7, zero, 6
+    PREP_8TAP_8BPC_LSX 6
+endfunc
+
+function prep_8tap_regular_sharp_8bpc_lsx
+    addi.w a7, zero, 8
+    PREP_8TAP_8BPC_LSX 8
+endfunc
+
+function prep_8tap_smooth_sharp_8bpc_lsx
+    addi.w a7, zero, 9
+    PREP_8TAP_8BPC_LSX 9
+endfunc
+
+function prep_8tap_sharp_8bpc_lsx
+    addi.w a7, zero, 10
+    PREP_8TAP_8BPC_LSX 10
+endfunc
+
+/*
+ * static void blend_lsx(pixel *dst, const ptrdiff_t dst_stride, const pixel *tmp,
+                         const int w, int h, const uint8_t *mask)
+ */
+function blend_8bpc_lsx
+    addi.d        t8,     zero,    64
+    vreplgr2vr.b  vr23,   t8
+
+    clz.w         t0,     a3
+    li.w          t1,     26
+    sub.w         t0,     t0,      t1
+    la.local      t1,     .BLEND_LSX_JRTABLE
+    alsl.d        t0,     t0,      t1,    1
+    ld.h          t2,     t0,      0  // The jump addresses are relative to JRTABLE
+    add.d         t1,     t1,      t2 // Get absolute address
+    jirl          $r0,    t1,      0
+
+    .align   3
+.BLEND_LSX_JRTABLE:
+    .hword .BLEND_W32_LSX  - .BLEND_LSX_JRTABLE
+    .hword .BLEND_W16_LSX  - .BLEND_LSX_JRTABLE
+    .hword .BLEND_W8_LSX   - .BLEND_LSX_JRTABLE
+    .hword .BLEND_W4_LSX   - .BLEND_LSX_JRTABLE
+
+.BLEND_W4_LSX:
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+    vld             vr2,    a5,      0
+
+    vsllwil.hu.bu   vr1,    vr1,     0
+    vsllwil.hu.bu   vr4,    vr2,     0
+    vmul.h          vr1,    vr1,     vr4  //b*m
+    vsub.b          vr3,    vr23,    vr2
+    vsllwil.hu.bu   vr0,    vr0,     0
+    vsllwil.hu.bu   vr3,    vr3,     0
+    vmadd.h         vr1,    vr0,     vr3
+    vssrarni.bu.h   vr1,    vr1,     6
+
+    vstelm.w        vr1,    a0,      0,   0
+    addi.w          a4,     a4,      -1
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      4
+    addi.d          a5,     a5,      4
+
+    blt             zero,   a4,     .BLEND_W4_LSX
+    b              .BLEND_END_LSX
+.BLEND_W8_LSX:
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+    vld             vr2,    a5,      0
+
+    vsllwil.hu.bu   vr1,    vr1,     0
+    vsllwil.hu.bu   vr4,    vr2,     0
+    vmul.h          vr1,    vr1,     vr4  //b*m
+    vsub.b          vr3,    vr23,    vr2
+    vsllwil.hu.bu   vr0,    vr0,     0
+    vsllwil.hu.bu   vr3,    vr3,     0
+    vmadd.h         vr1,    vr0,     vr3
+    vssrarni.bu.h   vr1,    vr1,     6
+
+    vstelm.d        vr1,    a0,      0,   0
+    addi.w          a4,     a4,      -1
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      8
+    addi.d          a5,     a5,      8
+
+    blt             zero,   a4,     .BLEND_W8_LSX
+    b               .BLEND_END_LSX
+.BLEND_W16_LSX:
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+    vld             vr2,    a5,      0
+
+    vexth.hu.bu     vr5,    vr1
+    vsllwil.hu.bu   vr1,    vr1,     0
+    vexth.hu.bu     vr6,    vr2
+    vsllwil.hu.bu   vr4,    vr2,     0
+    vmul.h          vr1,    vr1,     vr4  //b*m
+    vmul.h          vr5,    vr5,     vr6  //b*m
+    vsub.b          vr3,    vr23,    vr2
+    vexth.hu.bu     vr7,    vr0
+    vexth.hu.bu     vr8,    vr3
+    vmadd.h         vr5,    vr7,     vr8
+    vsllwil.hu.bu   vr0,    vr0,     0
+    vsllwil.hu.bu   vr3,    vr3,     0
+    vmadd.h         vr1,    vr0,     vr3
+    vssrarni.bu.h   vr5,    vr1,     6
+
+    vst             vr5,    a0,      0
+    addi.w          a4,     a4,      -1
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      16
+    addi.d          a5,     a5,      16
+
+    blt             zero,   a4,     .BLEND_W16_LSX
+    b               .BLEND_END_LSX
+.BLEND_W32_LSX:
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+    vld             vr2,    a5,      0
+
+    vexth.hu.bu     vr5,    vr1
+    vsllwil.hu.bu   vr1,    vr1,     0
+    vexth.hu.bu     vr6,    vr2
+    vsllwil.hu.bu   vr4,    vr2,     0
+    vmul.h          vr1,    vr1,     vr4  //b*m
+    vmul.h          vr5,    vr5,     vr6  //b*m
+    vsub.b          vr3,    vr23,    vr2
+    vexth.hu.bu     vr7,    vr0
+    vexth.hu.bu     vr8,    vr3
+    vmadd.h         vr5,    vr7,     vr8
+    vsllwil.hu.bu   vr0,    vr0,     0
+    vsllwil.hu.bu   vr3,    vr3,     0
+    vmadd.h         vr1,    vr0,     vr3
+    vssrarni.bu.h   vr5,    vr1,     6
+
+    vst             vr5,    a0,      0
+
+    /* sencond */
+    vld             vr0,    a0,      16
+    vld             vr1,    a2,      16
+    vld             vr2,    a5,      16
+
+    vexth.hu.bu     vr5,    vr1
+    vsllwil.hu.bu   vr1,    vr1,     0
+    vexth.hu.bu     vr6,    vr2
+    vsllwil.hu.bu   vr4,    vr2,     0
+    vmul.h          vr1,    vr1,     vr4  //b*m
+    vmul.h          vr5,    vr5,     vr6  //b*m
+    vsub.b          vr3,    vr23,    vr2
+    vexth.hu.bu     vr7,    vr0
+    vexth.hu.bu     vr8,    vr3
+    vmadd.h         vr5,    vr7,     vr8
+    vsllwil.hu.bu   vr0,    vr0,     0
+    vsllwil.hu.bu   vr3,    vr3,     0
+    vmadd.h         vr1,    vr0,     vr3
+    vssrarni.bu.h   vr5,    vr1,     6
+
+    vst             vr5,    a0,      16
+    addi.w          a4,     a4,      -1
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      32
+    addi.d          a5,     a5,      32
+
+    blt             zero,   a4,     .BLEND_W32_LSX
+.BLEND_END_LSX:
+
+endfunc
+
+const obmc_masks_la
+/* Unused */
+.byte 0,  0,  0,  0
+/* 2 */
+.byte 45, 19, 64, 0
+/* 4 */
+.byte 39, 25, 50, 14, 59,  5, 64,  0
+/* 8 */
+.byte 36, 28, 42, 22, 48, 16, 53, 11, 57,  7, 61,  3, 64,  0, 64,  0
+/* 16 */
+.byte 34, 30, 37, 27, 40, 24, 43, 21, 46, 18, 49, 15, 52, 12, 54, 10
+.byte 56,  8, 58,  6, 60,  4, 61,  3, 64,  0, 64,  0, 64,  0, 64,  0
+/* 32 */
+.byte 33, 31, 35, 29, 36, 28, 38, 26, 40, 24, 41, 23, 43, 21, 44, 20
+.byte 45, 19, 47, 17, 48, 16, 50, 14, 51, 13, 52, 12, 53, 11, 55,  9
+.byte 56,  8, 57,  7, 58,  6, 59,  5, 60,  4, 60,  4, 61,  3, 62,  2
+endconst
+
+/*
+ * static void blend_v_lsx(pixel *dst, const ptrdiff_t dst_stride, const pixel *tmp,
+                           const int w, int h)
+ */
+function blend_v_8bpc_lsx
+    la.local      t8,     obmc_masks_la
+
+    clz.w         t0,     a3
+    li.w          t1,     26
+    sub.w         t0,     t0,      t1
+    la.local      t1,     .BLEND_V_LSX_JRTABLE
+    alsl.d        t0,     t0,      t1,    1
+    ld.h          t2,     t0,      0  // The jump addresses are relative to JRTABLE
+    add.d         t1,     t1,      t2 // Get absolute address
+    jirl          $r0,    t1,      0
+
+    .align   3
+.BLEND_V_LSX_JRTABLE:
+    .hword .BLEND_V_W32_LSX  - .BLEND_V_LSX_JRTABLE
+    .hword .BLEND_V_W16_LSX  - .BLEND_V_LSX_JRTABLE
+    .hword .BLEND_V_W8_LSX   - .BLEND_V_LSX_JRTABLE
+    .hword .BLEND_V_W4_LSX   - .BLEND_V_LSX_JRTABLE
+    .hword .BLEND_V_W2_LSX   - .BLEND_V_LSX_JRTABLE
+    .hword .BLEND_V_W2_LSX_1 - .BLEND_V_LSX_JRTABLE  //Instructions must be 4-byte aligned
+
+.BLEND_V_W2_LSX:
+    ld.bu           t6,     t8,      4
+    ld.bu           t7,     t8,      5
+
+.BLEND_V_W2_LSX_1:
+    ld.bu           t0,     a0,      0
+    ld.bu           t1,     a2,      0
+    mul.d           t0,     t0,      t6
+    mul.d           t1,     t1,      t7
+    addi.d          t0,     t0,      32
+    add.d           t0,     t0,      t1
+    srli.d          t0,     t0,      6
+    st.b            t0,     a0,      0
+
+    addi.w          a4,     a4,      -1
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      2
+    addi.d          a5,     a5,      2
+
+    blt             zero,   a4,     .BLEND_V_W2_LSX_1
+    b               .BLEND_V_END_LSX
+
+.BLEND_V_W4_LSX:
+    vld             vr20,   t8,      8
+
+.BLEND_V_W4_LSX_1:
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+
+    vilvl.b         vr0,    vr1,     vr0
+    vdp2.h.bu       vr1,    vr0,     vr20
+    vssrarni.bu.h   vr1,    vr1,     6
+
+    vstelm.h        vr1,    a0,      0,   0
+    vstelm.b        vr1,    a0,      2,   2
+    addi.w          a4,     a4,      -1
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      4
+
+    blt             zero,   a4,     .BLEND_V_W4_LSX_1
+    b              .BLEND_V_END_LSX
+
+.BLEND_V_W8_LSX:
+    vld             vr20,   t8,      16
+
+.BLEND_V_W8_LSX_1:
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+
+    vilvl.b         vr0,    vr1,     vr0
+    vdp2.h.bu       vr1,    vr0,     vr20
+    vssrarni.bu.h   vr1,    vr1,     6
+
+    vstelm.w        vr1,    a0,      0,   0
+    vstelm.h        vr1,    a0,      4,   2
+    addi.w          a4,     a4,      -1
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      8
+
+    blt             zero,   a4,     .BLEND_V_W8_LSX_1
+    b              .BLEND_V_END_LSX
+
+.BLEND_V_W16_LSX:
+    vld             vr20,   t8,      32
+    vld             vr21,   t8,      48
+
+.BLEND_V_W16_LSX_1:
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+
+    vilvl.b         vr2,    vr1,     vr0
+    vilvh.b         vr3,    vr1,     vr0
+    vmulwev.h.bu    vr4,    vr2,     vr20
+    vmulwev.h.bu    vr5,    vr3,     vr21
+    vmaddwod.h.bu   vr4,    vr2,     vr20
+    vmaddwod.h.bu   vr5,    vr3,     vr21
+    vssrarni.bu.h   vr5,    vr4,     6
+
+    vstelm.d        vr5,    a0,      0,   0
+    vstelm.w        vr5,    a0,      8,   2
+    addi.w          a4,     a4,      -1
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      16
+
+    blt             zero,   a4,     .BLEND_V_W16_LSX_1
+    b              .BLEND_V_END_LSX
+
+.BLEND_V_W32_LSX:
+    vld             vr20,   t8,      64
+    vld             vr21,   t8,      80
+    vld             vr22,   t8,      96
+
+.BLEND_V_W32_LSX_1:
+    vld             vr0,    a0,      0
+    vld             vr1,    a0,      16
+    vld             vr2,    a2,      0
+    vld             vr3,    a2,      16
+
+    vilvl.b         vr4,    vr2,     vr0
+    vmulwev.h.bu    vr7,    vr4,     vr20
+    vilvh.b         vr5,    vr2,     vr0
+    vmulwev.h.bu    vr8,    vr5,     vr21
+    vilvl.b         vr6,    vr3,     vr1
+    vmulwev.h.bu    vr9,    vr6,     vr22
+    vmaddwod.h.bu   vr7,    vr4,     vr20
+    vmaddwod.h.bu   vr8,    vr5,     vr21
+    vmaddwod.h.bu   vr9,    vr6,     vr22
+    vssrarni.bu.h   vr8,    vr7,     6
+    vssrarni.bu.h   vr9,    vr9,     6
+
+    vst             vr8,    a0,      0
+    vstelm.d        vr9,    a0,      16,   0
+    addi.w          a4,     a4,      -1
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      32
+
+    blt             zero,   a4,     .BLEND_V_W32_LSX_1
+
+.BLEND_V_END_LSX:
+
+endfunc
+
+/*
+ * static void blend_h_lsx(pixel *dst, const ptrdiff_t dst_stride, const pixel *tmp,
+                           const int w, int h)
+ */
+function blend_h_8bpc_lsx
+    la.local      t8,     obmc_masks_la
+    alsl.d        t8,     a4,      t8,    1
+    srli.d        t0,     a4,      1
+    srli.d        t1,     a4,      2
+    add.d         a4,     t0,      t1  // h = (h * 3) >> 2;
+    slli.d        a4,     a4,      1
+    add.d         a4,     a4,      t8
+
+    clz.w         t0,     a3
+    li.w          t1,     24
+    sub.w         t0,     t0,      t1
+    la.local      t1,     .BLEND_H_LSX_JRTABLE
+    alsl.d        t0,     t0,      t1,    1
+    ld.h          t2,     t0,      0  // The jump addresses are relative to JRTABLE
+    add.d         t1,     t1,      t2 // Get absolute address
+    jirl          $r0,    t1,      0
+
+    .align   3
+.BLEND_H_LSX_JRTABLE:
+    .hword .BLEND_H_W128_LSX - .BLEND_H_LSX_JRTABLE
+    .hword .BLEND_H_W64_LSX  - .BLEND_H_LSX_JRTABLE
+    .hword .BLEND_H_W32_LSX  - .BLEND_H_LSX_JRTABLE
+    .hword .BLEND_H_W16_LSX  - .BLEND_H_LSX_JRTABLE
+    .hword .BLEND_H_W8_LSX   - .BLEND_H_LSX_JRTABLE
+    .hword .BLEND_H_W4_LSX   - .BLEND_H_LSX_JRTABLE
+    .hword .BLEND_H_W2_LSX   - .BLEND_H_LSX_JRTABLE
+    .hword .BLEND_H_END_LSX  - .BLEND_H_LSX_JRTABLE  //Instructions must be 4-byte aligned
+
+.BLEND_H_W2_LSX:
+    vldrepl.h       vr20,   t8,      0
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+
+    vilvl.b         vr0,    vr1,     vr0
+    vdp2.h.bu       vr1,    vr0,     vr20
+    vssrarni.bu.h   vr1,    vr1,     6
+
+    vstelm.h        vr1,    a0,      0,   0
+    addi.d          t8,     t8,      2
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      2
+
+    blt             t8,     a4,     .BLEND_H_W2_LSX
+    b               .BLEND_H_END_LSX
+
+.BLEND_H_W4_LSX:
+    vldrepl.h       vr20,   t8,      0
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+
+    vilvl.b         vr0,    vr1,     vr0
+    vdp2.h.bu       vr1,    vr0,     vr20
+    vssrarni.bu.h   vr1,    vr1,     6
+
+    vstelm.w        vr1,    a0,      0,   0
+    addi.d          t8,     t8,      2
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      4
+
+    blt             t8,     a4,     .BLEND_H_W4_LSX
+    b               .BLEND_H_END_LSX
+
+.BLEND_H_W8_LSX:
+    vldrepl.h       vr20,   t8,      0
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+
+    vilvl.b         vr0,    vr1,     vr0
+    vdp2.h.bu       vr1,    vr0,     vr20
+    vssrarni.bu.h   vr1,    vr1,     6
+
+    vstelm.d        vr1,    a0,      0,   0
+    addi.d          t8,     t8,      2
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      8
+
+    blt             t8,     a4,     .BLEND_H_W8_LSX
+    b               .BLEND_H_END_LSX
+
+.BLEND_H_W16_LSX:
+    vldrepl.h       vr20,   t8,      0
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+
+    vilvl.b         vr2,    vr1,     vr0
+    vilvh.b         vr3,    vr1,     vr0
+    vmulwev.h.bu    vr4,    vr2,     vr20
+    vmulwev.h.bu    vr5,    vr3,     vr20
+    vmaddwod.h.bu   vr4,    vr2,     vr20
+    vmaddwod.h.bu   vr5,    vr3,     vr20
+    vssrarni.bu.h   vr5,    vr4,     6
+
+    vst             vr5,    a0,      0
+    addi.d          t8,     t8,      2
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      16
+
+    blt             t8,     a4,     .BLEND_H_W16_LSX
+    b               .BLEND_H_END_LSX
+
+.BLEND_H_W32_LSX:
+    vldrepl.h       vr20,   t8,      0
+
+    vld             vr0,    a0,      0
+    vld             vr1,    a0,      16
+    vld             vr2,    a2,      0
+    vld             vr3,    a2,      16
+
+    vilvl.b         vr4,    vr2,     vr0
+    vilvh.b         vr5,    vr2,     vr0
+    vilvl.b         vr6,    vr3,     vr1
+    vilvh.b         vr3,    vr3,     vr1
+    vmulwev.h.bu    vr7,    vr4,     vr20
+    vmulwev.h.bu    vr8,    vr5,     vr20
+    vmulwev.h.bu    vr9,    vr6,     vr20
+    vmulwev.h.bu    vr0,    vr3,     vr20
+    vmaddwod.h.bu   vr7,    vr4,     vr20
+    vmaddwod.h.bu   vr8,    vr5,     vr20
+    vmaddwod.h.bu   vr9,    vr6,     vr20
+    vmaddwod.h.bu   vr0,    vr3,     vr20
+    vssrarni.bu.h   vr8,    vr7,     6
+    vssrarni.bu.h   vr0,    vr9,     6
+
+    vst             vr8,    a0,      0
+    vst             vr0,    a0,      16
+    addi.d          t8,     t8,      2
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      32
+
+    blt             t8,     a4,     .BLEND_H_W32_LSX
+    b               .BLEND_H_END_LSX
+
+.BLEND_H_W64_LSX:
+    vldrepl.h       vr20,   t8,      0
+
+    vld             vr0,    a0,      0
+    vld             vr1,    a0,      16
+    vld             vr2,    a0,      32
+    vld             vr3,    a0,      48
+    vld             vr4,    a2,      0
+    vld             vr5,    a2,      16
+    vld             vr6,    a2,      32
+    vld             vr7,    a2,      48
+
+    vilvl.b         vr8,    vr4,     vr0
+    vilvh.b         vr9,    vr4,     vr0
+    vilvl.b         vr10,   vr5,     vr1
+    vilvh.b         vr11,   vr5,     vr1
+    vilvl.b         vr12,   vr6,     vr2
+    vilvh.b         vr13,   vr6,     vr2
+    vilvl.b         vr14,   vr7,     vr3
+    vilvh.b         vr15,   vr7,     vr3
+    vmulwev.h.bu    vr0,    vr8,     vr20
+    vmulwev.h.bu    vr1,    vr9,     vr20
+    vmulwev.h.bu    vr2,    vr10,    vr20
+    vmulwev.h.bu    vr3,    vr11,    vr20
+    vmulwev.h.bu    vr4,    vr12,    vr20
+    vmulwev.h.bu    vr5,    vr13,    vr20
+    vmulwev.h.bu    vr6,    vr14,    vr20
+    vmulwev.h.bu    vr7,    vr15,    vr20
+
+    vmaddwod.h.bu   vr0,    vr8,     vr20
+    vmaddwod.h.bu   vr1,    vr9,     vr20
+    vmaddwod.h.bu   vr2,    vr10,    vr20
+    vmaddwod.h.bu   vr3,    vr11,    vr20
+    vmaddwod.h.bu   vr4,    vr12,    vr20
+    vmaddwod.h.bu   vr5,    vr13,    vr20
+    vmaddwod.h.bu   vr6,    vr14,    vr20
+    vmaddwod.h.bu   vr7,    vr15,    vr20
+
+    vssrarni.bu.h   vr1,    vr0,     6
+    vssrarni.bu.h   vr3,    vr2,     6
+    vssrarni.bu.h   vr5,    vr4,     6
+    vssrarni.bu.h   vr7,    vr6,     6
+
+    vst             vr1,    a0,      0
+    vst             vr3,    a0,      16
+    vst             vr5,    a0,      32
+    vst             vr7,    a0,      48
+    addi.d          t8,     t8,      2
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      64
+
+    blt             t8,     a4,     .BLEND_H_W64_LSX
+    b               .BLEND_H_END_LSX
+
+.BLEND_H_W128_LSX:
+    vldrepl.h       vr20,   t8,      0
+
+    vld             vr0,    a0,      0
+    vld             vr1,    a0,      16
+    vld             vr2,    a0,      32
+    vld             vr3,    a0,      48
+    vld             vr4,    a2,      0
+    vld             vr5,    a2,      16
+    vld             vr6,    a2,      32
+    vld             vr7,    a2,      48
+
+    vilvl.b         vr8,    vr4,     vr0
+    vilvh.b         vr9,    vr4,     vr0
+    vilvl.b         vr10,   vr5,     vr1
+    vilvh.b         vr11,   vr5,     vr1
+    vilvl.b         vr12,   vr6,     vr2
+    vilvh.b         vr13,   vr6,     vr2
+    vilvl.b         vr14,   vr7,     vr3
+    vilvh.b         vr15,   vr7,     vr3
+    vmulwev.h.bu    vr0,    vr8,     vr20
+    vmulwev.h.bu    vr1,    vr9,     vr20
+    vmulwev.h.bu    vr2,    vr10,    vr20
+    vmulwev.h.bu    vr3,    vr11,    vr20
+    vmulwev.h.bu    vr4,    vr12,    vr20
+    vmulwev.h.bu    vr5,    vr13,    vr20
+    vmulwev.h.bu    vr6,    vr14,    vr20
+    vmulwev.h.bu    vr7,    vr15,    vr20
+
+    vmaddwod.h.bu   vr0,    vr8,     vr20
+    vmaddwod.h.bu   vr1,    vr9,     vr20
+    vmaddwod.h.bu   vr2,    vr10,    vr20
+    vmaddwod.h.bu   vr3,    vr11,    vr20
+    vmaddwod.h.bu   vr4,    vr12,    vr20
+    vmaddwod.h.bu   vr5,    vr13,    vr20
+    vmaddwod.h.bu   vr6,    vr14,    vr20
+    vmaddwod.h.bu   vr7,    vr15,    vr20
+
+    vssrarni.bu.h   vr1,    vr0,     6
+    vssrarni.bu.h   vr3,    vr2,     6
+    vssrarni.bu.h   vr5,    vr4,     6
+    vssrarni.bu.h   vr7,    vr6,     6
+
+    vst             vr1,    a0,      0
+    vst             vr3,    a0,      16
+    vst             vr5,    a0,      32
+    vst             vr7,    a0,      48
+
+    /* second */
+    vld             vr0,    a0,      64
+    vld             vr1,    a0,      80
+    vld             vr2,    a0,      96
+    vld             vr3,    a0,      112
+    vld             vr4,    a2,      64
+    vld             vr5,    a2,      80
+    vld             vr6,    a2,      96
+    vld             vr7,    a2,      112
+
+    vilvl.b         vr8,    vr4,     vr0
+    vilvh.b         vr9,    vr4,     vr0
+    vilvl.b         vr10,   vr5,     vr1
+    vilvh.b         vr11,   vr5,     vr1
+    vilvl.b         vr12,   vr6,     vr2
+    vilvh.b         vr13,   vr6,     vr2
+    vilvl.b         vr14,   vr7,     vr3
+    vilvh.b         vr15,   vr7,     vr3
+    vmulwev.h.bu    vr0,    vr8,     vr20
+    vmulwev.h.bu    vr1,    vr9,     vr20
+    vmulwev.h.bu    vr2,    vr10,    vr20
+    vmulwev.h.bu    vr3,    vr11,    vr20
+    vmulwev.h.bu    vr4,    vr12,    vr20
+    vmulwev.h.bu    vr5,    vr13,    vr20
+    vmulwev.h.bu    vr6,    vr14,    vr20
+    vmulwev.h.bu    vr7,    vr15,    vr20
+
+    vmaddwod.h.bu   vr0,    vr8,     vr20
+    vmaddwod.h.bu   vr1,    vr9,     vr20
+    vmaddwod.h.bu   vr2,    vr10,    vr20
+    vmaddwod.h.bu   vr3,    vr11,    vr20
+    vmaddwod.h.bu   vr4,    vr12,    vr20
+    vmaddwod.h.bu   vr5,    vr13,    vr20
+    vmaddwod.h.bu   vr6,    vr14,    vr20
+    vmaddwod.h.bu   vr7,    vr15,    vr20
+
+    vssrarni.bu.h   vr1,    vr0,     6
+    vssrarni.bu.h   vr3,    vr2,     6
+    vssrarni.bu.h   vr5,    vr4,     6
+    vssrarni.bu.h   vr7,    vr6,     6
+
+    vst             vr1,    a0,      64
+    vst             vr3,    a0,      80
+    vst             vr5,    a0,      96
+    vst             vr7,    a0,      112
+
+    addi.d          t8,     t8,      2
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      128
+
+    blt             t8,     a4,     .BLEND_H_W128_LSX
+    b               .BLEND_H_END_LSX
+
+.BLEND_H_END_LSX:
+
+endfunc
+
+/*
+ * static void blend_h_lsx(pixel *dst, const ptrdiff_t dst_stride, const pixel *tmp,
+                           const int w, int h)
+ */
+function blend_h_8bpc_lasx
+    la.local      t8,     obmc_masks_la
+    alsl.d        t8,     a4,      t8,    1
+    srli.d        t0,     a4,      1
+    srli.d        t1,     a4,      2
+    add.d         a4,     t0,      t1  // h = (h * 3) >> 2;
+    slli.d        a4,     a4,      1
+    add.d         a4,     a4,      t8
+
+    clz.w         t0,     a3
+    li.w          t1,     24
+    sub.w         t0,     t0,      t1
+    la.local      t1,     .BLEND_H_LASX_JRTABLE
+    alsl.d        t0,     t0,      t1,    1
+    ld.h          t2,     t0,      0  // The jump addresses are relative to JRTABLE
+    add.d         t1,     t1,      t2 // Get absolute address
+    jirl          $r0,    t1,      0
+
+    .align   3
+.BLEND_H_LASX_JRTABLE:
+    .hword .BLEND_H_W128_LASX - .BLEND_H_LASX_JRTABLE
+    .hword .BLEND_H_W64_LASX  - .BLEND_H_LASX_JRTABLE
+    .hword .BLEND_H_W32_LASX  - .BLEND_H_LASX_JRTABLE
+    .hword .BLEND_H_W16_LASX  - .BLEND_H_LASX_JRTABLE
+    .hword .BLEND_H_W8_LASX   - .BLEND_H_LASX_JRTABLE
+    .hword .BLEND_H_W4_LASX   - .BLEND_H_LASX_JRTABLE
+    .hword .BLEND_H_W2_LASX   - .BLEND_H_LASX_JRTABLE
+    .hword .BLEND_H_END_LASX  - .BLEND_H_LASX_JRTABLE  //Instructions must be 4-byte aligned
+
+.BLEND_H_W2_LASX:
+    vldrepl.h       vr20,   t8,      0
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+
+    vilvl.b         vr0,    vr1,     vr0
+    vdp2.h.bu       vr1,    vr0,     vr20
+    vssrarni.bu.h   vr1,    vr1,     6
+
+    vstelm.h        vr1,    a0,      0,   0
+    addi.d          t8,     t8,      2
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      2
+
+    blt             t8,     a4,     .BLEND_H_W2_LASX
+    b               .BLEND_H_END_LASX
+
+.BLEND_H_W4_LASX:
+    vldrepl.h       vr20,   t8,      0
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+
+    vilvl.b         vr0,    vr1,     vr0
+    vdp2.h.bu       vr1,    vr0,     vr20
+    vssrarni.bu.h   vr1,    vr1,     6
+
+    vstelm.w        vr1,    a0,      0,   0
+    addi.d          t8,     t8,      2
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      4
+
+    blt             t8,     a4,     .BLEND_H_W4_LASX
+    b               .BLEND_H_END_LASX
+
+.BLEND_H_W8_LASX:
+    vldrepl.h       vr20,   t8,      0
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+
+    vilvl.b         vr0,    vr1,     vr0
+    vdp2.h.bu       vr1,    vr0,     vr20
+    vssrarni.bu.h   vr1,    vr1,     6
+
+    vstelm.d        vr1,    a0,      0,   0
+    addi.d          t8,     t8,      2
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      8
+
+    blt             t8,     a4,     .BLEND_H_W8_LASX
+    b               .BLEND_H_END_LASX
+
+.BLEND_H_W16_LASX:
+    vldrepl.h       vr20,   t8,      0
+    vld             vr0,    a0,      0
+    vld             vr1,    a2,      0
+
+    vilvl.b         vr2,    vr1,     vr0
+    vilvh.b         vr3,    vr1,     vr0
+    vmulwev.h.bu    vr4,    vr2,     vr20
+    vmulwev.h.bu    vr5,    vr3,     vr20
+    vmaddwod.h.bu   vr4,    vr2,     vr20
+    vmaddwod.h.bu   vr5,    vr3,     vr20
+    vssrarni.bu.h   vr5,    vr4,     6
+
+    vst             vr5,    a0,      0
+    addi.d          t8,     t8,      2
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      16
+
+    blt             t8,     a4,     .BLEND_H_W16_LSX
+    b               .BLEND_H_END_LSX
+
+.BLEND_H_W32_LASX:
+    xvldrepl.h      xr20,   t8,      0
+
+    xvld            xr0,    a0,      0
+    xvld            xr1,    a2,      0
+
+    xvilvl.b        xr2,    xr1,     xr0
+    xvilvh.b        xr3,    xr1,     xr0
+
+    xvmulwev.h.bu   xr4,    xr2,     xr20
+    xvmulwev.h.bu   xr5,    xr3,     xr20
+    xvmaddwod.h.bu  xr4,    xr2,     xr20
+    xvmaddwod.h.bu  xr5,    xr3,     xr20
+    xvssrarni.bu.h  xr5,    xr4,     6
+
+    xvst            xr5,    a0,      0
+    addi.d          t8,     t8,      2
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      32
+
+    blt             t8,     a4,     .BLEND_H_W32_LASX
+    b               .BLEND_H_END_LASX
+
+.BLEND_H_W64_LASX:
+    xvldrepl.h      xr20,   t8,      0
+
+    xvld            xr0,    a0,      0
+    xvld            xr1,    a0,      32
+    xvld            xr2,    a2,      0
+    xvld            xr3,    a2,      32
+
+    xvilvl.b        xr4,    xr2,     xr0
+    xvilvh.b        xr5,    xr2,     xr0
+    xvilvl.b        xr6,    xr3,     xr1
+    xvilvh.b        xr7,    xr3,     xr1
+
+    xvmulwev.h.bu   xr0,    xr4,     xr20
+    xvmulwev.h.bu   xr1,    xr5,     xr20
+    xvmulwev.h.bu   xr2,    xr6,     xr20
+    xvmulwev.h.bu   xr3,    xr7,     xr20
+    xvmaddwod.h.bu  xr0,    xr4,     xr20
+    xvmaddwod.h.bu  xr1,    xr5,     xr20
+    xvmaddwod.h.bu  xr2,    xr6,     xr20
+    xvmaddwod.h.bu  xr3,    xr7,     xr20
+    xvssrarni.bu.h  xr1,    xr0,     6
+    xvssrarni.bu.h  xr3,    xr2,     6
+
+    xvst            xr1,    a0,      0
+    xvst            xr3,    a0,      32
+    addi.d          t8,     t8,      2
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      64
+
+    blt             t8,     a4,     .BLEND_H_W64_LASX
+    b               .BLEND_H_END_LASX
+
+.BLEND_H_W128_LASX:
+    xvldrepl.h      xr20,   t8,      0
+
+    xvld            xr0,    a0,      0
+    xvld            xr1,    a0,      32
+    xvld            xr2,    a0,      64
+    xvld            xr3,    a0,      96
+    xvld            xr4,    a2,      0
+    xvld            xr5,    a2,      32
+    xvld            xr6,    a2,      64
+    xvld            xr7,    a2,      96
+
+    xvilvl.b        xr8,    xr4,     xr0
+    xvilvh.b        xr9,    xr4,     xr0
+    xvilvl.b        xr10,   xr5,     xr1
+    xvilvh.b        xr11,   xr5,     xr1
+    xvilvl.b        xr12,   xr6,     xr2
+    xvilvh.b        xr13,   xr6,     xr2
+    xvilvl.b        xr14,   xr7,     xr3
+    xvilvh.b        xr15,   xr7,     xr3
+
+    xvmulwev.h.bu   xr0,    xr8,     xr20
+    xvmulwev.h.bu   xr1,    xr9,     xr20
+    xvmulwev.h.bu   xr2,    xr10,    xr20
+    xvmulwev.h.bu   xr3,    xr11,    xr20
+    xvmulwev.h.bu   xr4,    xr12,    xr20
+    xvmulwev.h.bu   xr5,    xr13,    xr20
+    xvmulwev.h.bu   xr6,    xr14,    xr20
+    xvmulwev.h.bu   xr7,    xr15,    xr20
+    xvmaddwod.h.bu  xr0,    xr8,     xr20
+    xvmaddwod.h.bu  xr1,    xr9,     xr20
+    xvmaddwod.h.bu  xr2,    xr10,    xr20
+    xvmaddwod.h.bu  xr3,    xr11,    xr20
+    xvmaddwod.h.bu  xr4,    xr12,    xr20
+    xvmaddwod.h.bu  xr5,    xr13,    xr20
+    xvmaddwod.h.bu  xr6,    xr14,    xr20
+    xvmaddwod.h.bu  xr7,    xr15,    xr20
+    xvssrarni.bu.h  xr1,    xr0,     6
+    xvssrarni.bu.h  xr3,    xr2,     6
+    xvssrarni.bu.h  xr5,    xr4,     6
+    xvssrarni.bu.h  xr7,    xr6,     6
+
+    xvst            xr1,    a0,      0
+    xvst            xr3,    a0,      32
+    xvst            xr5,    a0,      64
+    xvst            xr7,    a0,      96
+    addi.d          t8,     t8,      2
+    add.d           a0,     a0,      a1
+    addi.d          a2,     a2,      128
+
+    blt             t8,     a4,     .BLEND_H_W128_LASX
+    b               .BLEND_H_END_LASX
+
+.BLEND_H_END_LASX:
+
+endfunc
+
+/*
+ *  a1=16 | a2=8 | a3=4
+ *  temp reg: a4
+ */
+.macro PIXEL_COPY_LSX _dst, _src, _size
+    blt             \_size,  a1,     8f
+16:
+    vld             vr0,     \_src,  0
+    vst             vr0,     \_dst,  0
+    addi.d          \_size,  \_size, -16
+    addi.d          \_dst,   \_dst,  16
+    addi.d          \_src,   \_src,  16
+    blt             a1,      \_size, 16b
+8:
+    blt             \_size,  a2,     14f
+    ld.d            a4,      \_src,  0
+    st.d            a4,      \_dst,  0
+    addi.d          \_size,  \_size, -8
+    addi.d          \_dst,   \_dst,  8
+    addi.d          \_src,   \_src,  8
+14:
+    blt             \_size,  a3,     11f
+    ld.w            a4,      \_src,  0
+    st.w            a4,      \_dst,  0
+    addi.d          \_size,  \_size, -4
+    addi.d          \_dst,   \_dst,  4
+    addi.d          \_src,   \_src,  4
+11:
+    beqz            \_size,  110f
+111:
+    ld.b            a4,      \_src,  0
+    st.b            a4,      \_dst,  0
+    addi.d          \_size,  \_size, -1
+    addi.d          \_dst,   \_dst,  1
+    addi.d          \_src,   \_src,  1
+    bnez            \_size,  111b
+110:
+.endm
+
+/*
+ *  a1=16 | a2=8 | a3=4
+ */
+.macro PIXEL_SET_LSX _dst, _vsrc, _size
+    blt             \_size,  a1,     8f
+16:
+    vst             \_vsrc,  \_dst,  0
+    addi.d          \_size,  \_size, -16
+    addi.d          \_dst,   \_dst,  16
+    blt             a1,      \_size, 16b
+8:
+    blt             \_size,  a2,     14f
+    vstelm.d        \_vsrc,  \_dst,  0,   0
+    addi.d          \_size,  \_size, -8
+    addi.d          \_dst,   \_dst,  8
+14:
+    blt             \_size,  a3,     11f
+    vstelm.w        \_vsrc,  \_dst,  0,   0
+    addi.d          \_size,  \_size, -4
+    addi.d          \_dst,   \_dst,  4
+11:
+    beqz            \_size,  110f
+111:
+    vstelm.b        \_vsrc,  \_dst,  0,   0
+    addi.d          \_size,  \_size, -1
+    addi.d          \_dst,   \_dst,  1
+    bnez            \_size,  111b
+110:
+.endm
+
+/*
+ *  temp reg: a4 a5 t2 t3 vr0
+ */
+.macro DEGE_LOOP need_left, need_right
+0:
+    addi.d          t2,      t6,     0   // dst
+    addi.d          t3,      t7,     0   // src
+.if \need_left
+    vldrepl.b       vr0,     t3,     0
+    addi.d          a5,      t0,     0
+    PIXEL_SET_LSX t2, vr0, a5
+.endif
+
+    addi.d          a5,      t4,     0
+    PIXEL_COPY_LSX t2, t3, a5
+
+.if \need_right
+    vldrepl.b       vr0,     t3,     -1
+    addi.d          a5,      t1,     0
+    PIXEL_SET_LSX t2, vr0, a5
+.endif
+
+    addi.d          t5,      t5,     -1
+    add.d           t7,      t7,     t8
+    add.d           t6,      t6,     a7
+    bnez            t5,      0b
+.endm
+
+/*
+ * static void emu_edge_c(const intptr_t bw, const intptr_t bh,
+ *                        const intptr_t iw, const intptr_t ih,
+ *                        const intptr_t x, const intptr_t y,
+ *                        pixel *dst, const ptrdiff_t dst_stride,
+ *                        const pixel *ref, const ptrdiff_t ref_stride)
+ */
+function emu_edge_8bpc_lsx
+    vxor.v          vr23,   vr23,    vr23   // zero
+    addi.d          t0,     a3,      -1     // ih - 1
+    addi.d          t1,     a2,      -1     // iw - 1
+    vreplgr2vr.w    vr22,   t0
+    vinsgr2vr.w     vr22,   t1,        1
+    vreplgr2vr.w    vr0,    a5
+    vinsgr2vr.w     vr0,    a4,        1     // [0] - h | [1] - w
+
+    vclip.w         vr2,    vr0,      vr23,    vr22
+    vpickve2gr.w    t0,     vr2,      0
+    ld.d            t2,     sp,       0
+    ld.d            t8,     sp,       8     // ref_stride
+    mul.w           t0,     t0,       t8
+    vpickve2gr.w    t1,     vr2,      1
+    add.d           t2,     t2,       t1
+    add.d           t7,     t0,       t2    // ref
+
+    addi.d          t0,     a0,       -1     // bw - 1
+    addi.d          t1,     a1,       -1     // bh - 1
+    vreplgr2vr.w    vr21,   t0
+    vreplgr2vr.w    vr22,   t1
+    vilvl.d         vr21,   vr22,      vr21
+    sub.d           t2,     zero,      a4    // -x
+    add.d           t3,     a0,        a4
+    sub.d           t3,     t3,        a2    // x + bw - iw
+    sub.d           t4,     zero,      a5    // -y
+    add.d           t5,     a1,        a5
+    sub.d           t5,     t5,        a3    // y + bh - ih
+    vreplgr2vr.w    vr0,    t2
+    vinsgr2vr.w     vr0,    t3,        1
+    vinsgr2vr.w     vr0,    t4,        2
+    vinsgr2vr.w     vr0,    t5,        3
+    vclip.w         vr2,    vr0,       vr23,    vr21
+    vpickve2gr.w    t0,     vr2,       0     // left_ext
+    vpickve2gr.w    t1,     vr2,       1     // right_ext
+    vpickve2gr.w    t2,     vr2,       2     // top_ext
+    vpickve2gr.w    t3,     vr2,       3     // bottom_ext
+
+    mul.w           t6,     t2,        a7
+    add.d           t4,     t0,        t1
+    add.d           t5,     t2,        t3
+    sub.d           t4,     a0,        t4    // center_w
+    sub.d           t5,     a1,        t5    // center_h
+
+    addi.d          a1,     zero,      16
+    addi.d          a2,     zero,      8
+    addi.d          a3,     zero,      4
+    add.d           t6,     t6,        a6    // blk
+
+    beqz            t0,     2f
+    // need_left
+    beqz            t1,     3f
+    // need_left + need_right
+    DEGE_LOOP       1,   1
+    b               5f
+
+2:
+    // !need_left
+    beqz            t1,     4f
+    // !need_left + need_right
+    DEGE_LOOP       0,   1
+    b               5f
+
+3:
+    // need_left + !need_right
+    DEGE_LOOP       1,   0
+    b               5f
+
+4:
+    // !need_left + !need_right
+    DEGE_LOOP       0,   0
+
+5:
+    vpickve2gr.w    t2,     vr2,       2     // top_ext
+    vpickve2gr.w    t3,     vr2,       3     // bottom_ext
+    sub.d           t7,     a7,        a0    // dst_stride - bw
+    mul.w           t8,     t2,        a7
+
+    beqz            t3,     2f
+    // need_bottom
+    sub.d           t0,     t6,        a7    //  &dst[-PXSTRIDE(dst_stride)]
+1:
+    addi.d          t1,     t0,        0
+    addi.d          a5,     a0,        0
+    PIXEL_COPY_LSX t6, t1, a5
+    add.d           t6,     t6,        t7
+    addi.d          t3,     t3,   -1
+    bnez            t3,     1b
+2:
+    beqz            t2,     3f
+    // need_top
+    add.d           t8,     t8,        a6    // blk
+1:
+    addi.d          t1,     t8,        0
+    addi.d          a5,     a0,        0
+    PIXEL_COPY_LSX a6, t1, a5
+    add.d           a6,     a6,        t7
+    addi.d          t2,     t2,   -1
+    bnez            t2,     1b
+3:
+
+endfunc
diff --git a/src/loongarch/mc.h b/src/loongarch/mc.h
index c64b7ef..678b2c9 100644
--- a/src/loongarch/mc.h
+++ b/src/loongarch/mc.h
@@ -43,6 +43,10 @@ decl_mask_fn(BF(dav1d_mask, lsx));
 decl_warp8x8_fn(BF(dav1d_warp_affine_8x8, lsx));
 decl_warp8x8t_fn(BF(dav1d_warp_affine_8x8t, lsx));
 decl_w_mask_fn(BF(dav1d_w_mask_420, lsx));
+decl_blend_fn(BF(dav1d_blend, lsx));
+decl_blend_dir_fn(BF(dav1d_blend_v, lsx));
+decl_blend_dir_fn(BF(dav1d_blend_h, lsx));
+decl_emu_edge_fn(BF(dav1d_emu_edge, lsx));
 
 decl_mc_fn(BF(dav1d_put_8tap_regular,          lsx));
 decl_mc_fn(BF(dav1d_put_8tap_regular_smooth,   lsx));
@@ -60,6 +64,17 @@ decl_mask_fn(BF(dav1d_mask, lasx));
 decl_warp8x8_fn(BF(dav1d_warp_affine_8x8, lasx));
 decl_warp8x8t_fn(BF(dav1d_warp_affine_8x8t, lasx));
 decl_w_mask_fn(BF(dav1d_w_mask_420, lasx));
+decl_blend_dir_fn(BF(dav1d_blend_h, lasx));
+
+decl_mct_fn(BF(dav1d_prep_8tap_regular,        lsx));
+decl_mct_fn(BF(dav1d_prep_8tap_regular_smooth, lsx));
+decl_mct_fn(BF(dav1d_prep_8tap_regular_sharp,  lsx));
+decl_mct_fn(BF(dav1d_prep_8tap_smooth,         lsx));
+decl_mct_fn(BF(dav1d_prep_8tap_smooth_regular, lsx));
+decl_mct_fn(BF(dav1d_prep_8tap_smooth_sharp,   lsx));
+decl_mct_fn(BF(dav1d_prep_8tap_sharp,          lsx));
+decl_mct_fn(BF(dav1d_prep_8tap_sharp_regular,  lsx));
+decl_mct_fn(BF(dav1d_prep_8tap_sharp_smooth,   lsx));
 
 decl_mct_fn(BF(dav1d_prep_8tap_regular,        lasx));
 decl_mct_fn(BF(dav1d_prep_8tap_regular_smooth, lasx));
@@ -83,6 +98,10 @@ static ALWAYS_INLINE void mc_dsp_init_loongarch(Dav1dMCDSPContext *const c) {
     c->warp8x8 = BF(dav1d_warp_affine_8x8, lsx);
     c->warp8x8t = BF(dav1d_warp_affine_8x8t, lsx);
     c->w_mask[2] = BF(dav1d_w_mask_420, lsx);
+    c->blend = BF(dav1d_blend, lsx);
+    c->blend_v = BF(dav1d_blend_v, lsx);
+    c->blend_h = BF(dav1d_blend_h, lsx);
+    c->emu_edge = BF(dav1d_emu_edge, lsx);
 
     init_mc_fn(FILTER_2D_8TAP_REGULAR,         8tap_regular,        lsx);
     init_mc_fn(FILTER_2D_8TAP_REGULAR_SMOOTH,  8tap_regular_smooth, lsx);
@@ -94,6 +113,16 @@ static ALWAYS_INLINE void mc_dsp_init_loongarch(Dav1dMCDSPContext *const c) {
     init_mc_fn(FILTER_2D_8TAP_SHARP_SMOOTH,    8tap_sharp_smooth,   lsx);
     init_mc_fn(FILTER_2D_8TAP_SHARP,           8tap_sharp,          lsx);
 
+    init_mct_fn(FILTER_2D_8TAP_REGULAR,        8tap_regular,        lsx);
+    init_mct_fn(FILTER_2D_8TAP_REGULAR_SMOOTH, 8tap_regular_smooth, lsx);
+    init_mct_fn(FILTER_2D_8TAP_REGULAR_SHARP,  8tap_regular_sharp,  lsx);
+    init_mct_fn(FILTER_2D_8TAP_SMOOTH_REGULAR, 8tap_smooth_regular, lsx);
+    init_mct_fn(FILTER_2D_8TAP_SMOOTH,         8tap_smooth,         lsx);
+    init_mct_fn(FILTER_2D_8TAP_SMOOTH_SHARP,   8tap_smooth_sharp,   lsx);
+    init_mct_fn(FILTER_2D_8TAP_SHARP_REGULAR,  8tap_sharp_regular,  lsx);
+    init_mct_fn(FILTER_2D_8TAP_SHARP_SMOOTH,   8tap_sharp_smooth,   lsx);
+    init_mct_fn(FILTER_2D_8TAP_SHARP,          8tap_sharp,          lsx);
+
     if (!(flags & DAV1D_LOONGARCH_CPU_FLAG_LASX)) return;
 
     c->avg = BF(dav1d_avg, lasx);
@@ -102,6 +131,7 @@ static ALWAYS_INLINE void mc_dsp_init_loongarch(Dav1dMCDSPContext *const c) {
     c->warp8x8 = BF(dav1d_warp_affine_8x8, lasx);
     c->warp8x8t = BF(dav1d_warp_affine_8x8t, lasx);
     c->w_mask[2] = BF(dav1d_w_mask_420, lasx);
+    c->blend_h = BF(dav1d_blend_h, lasx);
 
     init_mct_fn(FILTER_2D_8TAP_REGULAR,        8tap_regular,        lasx);
     init_mct_fn(FILTER_2D_8TAP_REGULAR_SMOOTH, 8tap_regular_smooth, lasx);
diff --git a/src/loongarch/msac.S b/src/loongarch/msac.S
index 5bf1825..f36c4e0 100644
--- a/src/loongarch/msac.S
+++ b/src/loongarch/msac.S
@@ -31,28 +31,29 @@ const min_prob
   .short 60, 56, 52, 48, 44, 40, 36, 32, 28, 24, 20, 16, 12, 8, 4, 0
 endconst
 
+const ph_0xff00
+.rept 8
+  .short 0xff00
+.endr
+endconst
+
 .macro decode_symbol_adapt w
     addi.d          sp,      sp,     -48
-    addi.d          a4,      a0,      24
-    vldrepl.h       vr0,     a4,      0    //rng
+    vldrepl.h       vr0,     a0,      24   //rng
     fst.s           f0,      sp,      0    //val==0
     vld             vr1,     a1,      0    //cdf
 .if \w == 16
-    li.w            t4,      16
-    vldx            vr11,    a1,      t4
+    vld             vr11,    a1,      16
 .endif
-    addi.d          a6,      a0,      16
-    vldrepl.d       vr2,     a6,      0    //dif
-    addi.d          t0,      a0,      32
-    ld.w            t1,      t0,      0    //allow_update_cdf
+    vldrepl.d       vr2,     a0,      16   //dif
+    ld.w            t1,      a0,      32   //allow_update_cdf
     la.local        t2,      min_prob
-    addi.d          t2,      t2,      32
-    addi.w          t3,      a2,      1
-    slli.w          t3,      t3,      1
+    addi.d          t2,      t2,      30
+    slli.w          t3,      a2,      1
     sub.d           t2,      t2,      t3
     vld             vr3,     t2,      0    //min_prob
 .if \w == 16
-    vldx            vr13,    t2,      t4
+    vld             vr13,    t2,      16
 .endif
     vsrli.h         vr4,     vr0,     8    //r = s->rng >> 8
     vslli.h         vr4,     vr4,     8    //r << 8
@@ -68,17 +69,15 @@ endconst
     vmuh.hu         vr15,    vr4,     vr15
     vadd.h          vr15,    vr15,    vr13
 .endif
-    addi.d          t8,      sp,      4
+    addi.d          t8,      sp,      2
     vst             vr5,     t8,      0    //store v
 .if \w == 16
-    vstx            vr15,    t8,      t4
+    vst             vr15,    t8,      16
 .endif
     vreplvei.h      vr20,    vr2,     3    //c
-    vssub.hu        vr6,     vr5,     vr20 //c >=v
-    vseqi.h         vr6,     vr6,     0
+    vsle.hu         vr6,     vr5,     vr20
 .if \w == 16
-    vssub.hu        vr16,    vr15,    vr20 //c >=v
-    vseqi.h         vr16,    vr16,    0
+    vsle.hu         vr16,    vr15,    vr20
     vpickev.b       vr21,    vr16,    vr6
 .endif
 .if \w <= 8
@@ -92,10 +91,14 @@ endconst
     alsl.d          t1,      a2,      a1,   1
     ld.h            t2,      t1,      0    //count
     srli.w          t3,      t2,      4    //count >> 4
+.if \w == 16
+    addi.w          t3,      t3,      5    //rate
+.else
     addi.w          t3,      t3,      4
     li.w            t5,      2
     sltu            t5,      t5,      a2
     add.w           t3,      t3,      t5   //rate
+.endif
     sltui           t5,      t2,      32
     add.w           t2,      t2,      t5   //count + (count < 32)
     vreplgr2vr.h    vr9,     t3
@@ -118,7 +121,7 @@ endconst
 .if \w == 16
     vsra.h          vr15,    vr15,    vr9
     vadd.h          vr18,    vr18,    vr15
-    vstx            vr18,    a1,      t4
+    vst             vr18,    a1,      16
 .endif
     st.h            t2,      t1,      0
 
@@ -127,8 +130,7 @@ endconst
     ctz.w           a7,      t3            // ret
     alsl.d          t3,      a7,      t8,      1
     ld.hu           t4,      t3,      0    // v
-    addi.d          t3,      t3,      -2
-    ld.hu           t5,      t3,      0    // u
+    ld.hu           t5,      t3,      -2   // u
     sub.w           t5,      t5,      t4   // rng
     slli.d          t4,      t4,      48
     vpickve2gr.d    t6,      vr2,     0
@@ -136,11 +138,10 @@ endconst
     clz.w           t4,      t5            // d
     xori            t4,      t4,      16   // d
     sll.d           t6,      t6,      t4
-    addi.d          a5,      a0,      28   // cnt
-    ld.w            t0,      a5,      0
+    ld.w            t0,      a0,      28   //cnt
     sll.w           t5,      t5,      t4
     sub.w           t7,      t0,      t4   // cnt-d
-    st.w            t5,      a4,      0    // store rng
+    st.w            t5,      a0,      24   // store rng
     bgeu            t0,      t4,      9f
 
     // refill
@@ -186,8 +187,8 @@ endconst
 4:
     or              t6,      t6,      t3   // dif |= next_bits
 9:
-    st.w            t7,      a5,      0    // store cnt
-    st.d            t6,      a6,      0    // store dif
+    st.w            t7,      a0,      28   // store cnt
+    st.d            t6,      a0,      16   // store dif
     move            a0,      a7
     addi.d          sp,      sp,      48
 .endm
@@ -281,6 +282,82 @@ function msac_decode_bool_lsx
     move            a0,      t8
 endfunc
 
+function msac_decode_bool_equi_lsx
+    ld.w            t0,      a0,      24   // rng
+    ld.d            t1,      a0,      16   // dif
+    ld.w            a5,      a0,      28   // cnt
+    srli.w          t2,      t0,      8    // r >> 8
+    slli.w          t2,      t2,      7
+    addi.w          t2,      t2,      4    // v
+
+    slli.d          t3,      t2,      48   // vw
+    sltu            t4,      t1,      t3
+    move            t8,      t4            // ret
+    xori            t4,      t4,      1
+    maskeqz         t6,      t3,      t4   // if (ret) vw
+    sub.d           t6,      t1,      t6   // dif
+    slli.w          t5,      t2,      1
+    sub.w           t5,      t0,      t5   // r - 2v
+    maskeqz         t7,      t5,      t4   // if (ret) r - 2v
+    add.w           t5,      t2,      t7   // v(rng)
+
+    // renorm
+    clz.w           t4,      t5            // d
+    xori            t4,      t4,      16   // d
+    sll.d           t6,      t6,      t4
+    sll.w           t5,      t5,      t4
+    sub.w           t7,      a5,      t4   // cnt-d
+    st.w            t5,      a0,      24   // store rng
+    bgeu            a5,      t4,      9f
+
+    // refill
+    ld.d            t0,      a0,      0    // buf_pos
+    ld.d            t1,      a0,      8    // buf_end
+    addi.d          t2,      t0,      8
+    bltu            t1,      t2,      2f
+
+    ld.d            t3,      t0,      0    // next_bits
+    addi.w          t1,      t7,      -48  // shift_bits = cnt + 16 (- 64)
+    nor             t3,      t3,      t3
+    sub.w           t2,      zero,    t1
+    revb.d          t3,      t3            // next_bits = bswap(next_bits)
+    srli.w          t2,      t2,      3    // num_bytes_read
+    srl.d           t3,      t3,      t1   // next_bits >>= (shift_bits & 63)
+    b               3f
+1:
+    addi.w          t3,      t7,      -48
+    srl.d           t3,      t3,      t3   // pad with ones
+    b               4f
+2:
+    bgeu            t0,      t1,      1b
+    ld.d            t3,      t1,      -8   // next_bits
+    sub.w           t2,      t2,      t1
+    sub.w           t1,      t1,      t0   // num_bytes_left
+    slli.w          t2,      t2,      3
+    srl.d           t3,      t3,      t2
+    addi.w          t2,      t7,      -48
+    nor             t3,      t3,      t3
+    sub.w           t4,      zero,    t2
+    revb.d          t3,      t3
+    srli.w          t4,      t4,      3
+    srl.d           t3,      t3,      t2
+    sltu            t2,      t1,      t4
+    maskeqz         t1,      t1,      t2
+    masknez         t2,      t4,      t2
+    or              t2,      t2,      t1   // num_bytes_read
+3:
+    slli.w          t1,      t2,      3
+    add.d           t0,      t0,      t2
+    add.w           t7,      t7,      t1   // cnt += num_bits_read
+    st.d            t0,      a0,      0
+4:
+    or              t6,      t6,      t3   // dif |= next_bits
+9:
+    st.w            t7,      a0,      28   // store cnt
+    st.d            t6,      a0,      16   // store dif
+    move            a0,      t8
+endfunc
+
 function msac_decode_bool_adapt_lsx
     ld.hu           a3,      a1,      0    // cdf[0] /f
     ld.w            t0,      a0,      24   // rng
@@ -374,3 +451,162 @@ function msac_decode_bool_adapt_lsx
     st.d            t6,      a0,      16   // store dif
     move            a0,      t8
 endfunc
+
+.macro HI_TOK allow_update_cdf
+.\allow_update_cdf\()_hi_tok_lsx_start:
+.if \allow_update_cdf == 1
+    ld.hu        a4,    a1,    0x06 // cdf[3]
+.endif
+    vor.v        vr1,   vr0,   vr0
+    vsrli.h      vr1,   vr1,   0x06 // cdf[val] >> EC_PROB_SHIFT
+    vstelm.h     vr2,   sp,    0, 0 // -0x1a
+    vand.v       vr2,   vr2,   vr4  // (8 x rng) & 0xff00
+    vslli.h      vr1,   vr1,   0x07
+    vmuh.hu      vr1,   vr1,   vr2
+    vadd.h       vr1,   vr1,   vr5 // v += EC_MIN_PROB/* 4 */ * ((unsigned)n_symbols/* 3 */ - val);
+    vst          vr1,   sp,    0x02 // -0x18
+    vssub.hu     vr1,   vr1,   vr3 // v - c
+    vseqi.h      vr1,   vr1,   0
+.if \allow_update_cdf == 1
+    addi.d       t4,    a4,    0x50
+    srli.d       t4,    t4,    0x04
+    sltui        t7,    a4,    32
+    add.w        a4,    a4,    t7
+
+    vreplgr2vr.h vr7,   t4
+    vavgr.hu     vr9,   vr8,   vr1
+    vsub.h       vr9,   vr9,   vr0
+    vsub.h       vr0,   vr0,   vr1
+    vsra.h       vr9,   vr9,   vr7
+    vadd.h       vr0,   vr0,   vr9
+    vstelm.d     vr0,   a1,    0,  0
+    st.h         a4,    a1,    0x06
+.endif
+    vmsknz.b     vr7,   vr1
+    movfr2gr.s   t4,    f7
+    ctz.w        t4,    t4 // loop_times * 2
+    addi.d       t7,    t4,    2
+    ldx.hu       t6,    sp,    t4  // u
+    ldx.hu       t5,    sp,    t7  // v
+    addi.w       t3,    t3,    0x05
+    addi.w       t4,    t4,   -0x05 // if t4 == 3, continue
+    sub.w        t6,    t6,    t5   // u - v , rng for ctx_norm
+    slli.d       t5,    t5,    0x30 //  (ec_win)v << (EC_WIN_SIZE - 16)
+    sub.d        t1,    t1,    t5   //  s->dif - ((ec_win)v << (EC_WIN_SIZE - 16))
+    // Init ctx_norm  param
+    clz.w        t7,    t6
+    xori         t7,    t7,    0x1f
+    xori         t7,    t7,    0x0f //  d = 15 ^ (31 ^ clz(rng));
+    sll.d        t1,    t1,    t7   //  dif << d
+    sll.d        t6,    t6,    t7   //  rng << d
+    // update vr2 8 x rng
+    vreplgr2vr.h vr2,   t6
+    vreplvei.h   vr2,   vr2,   0
+    st.w         t6,    a0,    0x18 // store rng
+    move         t0,    t2
+    sub.w        t2,    t2,    t7   // cnt - d
+    bgeu         t0,    t7,    .\allow_update_cdf\()_hi_tok_lsx_ctx_norm_end     // if ((unsigned)cnt < (unsigned)d)  goto ctx_norm_end
+    // Step into ctx_fill
+    ld.d         t5,    a0,    0x00 // buf_pos
+    ld.d         t6,    a0,    0x08 // end_pos
+    addi.d       t7,    t5,    0x08 // buf_pos + 8
+    sub.d        t7,    t7,    t6   // (buf_pos + 8) - end_pos
+    blt          zero,  t7,    .\allow_update_cdf\()_hi_tok_lsx_ctx_refill_eob
+    // (end_pos - buf_pos) >= 8
+    ld.d         t6,    t5,    0x00 // load buf_pos[0]~buf_pos[7]
+    addi.w       t7,    t2,   -0x30 // cnt - 0x30
+    nor          t6,    t6,    t6   // not buf data
+    revb.d       t6,    t6          // Byte reversal
+    srl.d        t6,    t6,    t7   // Replace left shift with right shift
+    sub.w        t7,    zero,  t7   // neg
+    srli.w       t7,    t7,    0x03 // Loop times
+    or           t1,    t1,    t6   // dif |= (ec_win)(*buf_pos++ ^ 0xff) << c
+    b            .\allow_update_cdf\()_hi_tok_lsx_ctx_refill_end
+.\allow_update_cdf\()_hi_tok_lsx_ctx_refill_eob:
+    bge          t5,    t6,    .\allow_update_cdf\()_hi_tok_lsx_ctx_refill_one
+    // end_pos - buf_pos < 8 && buf_pos < end_pos
+    ld.d         t0,    t6,   -0x08
+    slli.d       t7,    t7,    0x03
+    srl.d        t6,    t0,    t7   // Retrieve the buf data and remove the excess data
+    addi.w       t7,    t2,   -0x30 // cnt - 0x30
+    nor          t6,    t6,    t6   // not
+    revb.d       t6,    t6          // Byte reversal
+    srl.d        t6,    t6,    t7   // Replace left shift with right shift
+    sub.w        t7,    zero,  t7   // neg
+    or           t1,    t1,    t6   // dif |= (ec_win)(*buf_pos++ ^ 0xff) << c
+    ld.d         t6,    a0,    0x08 // end_pos
+    srli.w       t7,    t7,    0x03 // Loop times
+    sub.d        t6,    t6,    t5   // end_pos - buf_pos
+    slt          t0,    t6,    t7
+    maskeqz      a3,    t6,    t0   // min(loop_times, end_pos - buf_pos)
+    masknez      t0,    t7,    t0
+    or           t7,    a3,    t0
+    b            .\allow_update_cdf\()_hi_tok_lsx_ctx_refill_end
+.\allow_update_cdf\()_hi_tok_lsx_ctx_refill_one:
+    // buf_pos >= end_pos
+    addi.w       t7,    t2,   -0x10
+    andi         t7,    t7,    0xf
+    nor          t0,    zero,  zero
+    srl.d        t0,    t0,    t7
+    or           t1,    t1,    t0 // dif |= ~(~(ec_win)0xff << c);
+    b            .\allow_update_cdf\()_hi_tok_lsx_ctx_norm_end
+.\allow_update_cdf\()_hi_tok_lsx_ctx_refill_end:
+    add.d        t5,    t5,    t7        // buf_pos + Loop_times
+    st.d         t5,    a0,    0x00      // Store buf_pos
+    alsl.w       t2,    t7,    t2,  0x03 // update cnt
+.\allow_update_cdf\()_hi_tok_lsx_ctx_norm_end:
+    srli.d       t7,    t1,    0x30
+    vreplgr2vr.h vr3,   t7        // broadcast the high 16 bits of dif
+    add.w        t3,    t4,    t3 // update control parameter
+    beqz         t3,    .\allow_update_cdf\()_hi_tok_lsx_end // control loop for at most 4 times.
+    blt          zero,  t4,    .\allow_update_cdf\()_hi_tok_lsx_start // tok_br == 3
+.\allow_update_cdf\()_hi_tok_lsx_end:
+    addi.d       t3,    t3,    0x1e
+    st.d         t1,    a0,    0x10 // store dif
+    st.w         t2,    a0,    0x1c // store cnt
+    srli.w       a0,    t3,    0x01 // tok
+    addi.d       sp,    sp,    0x1a
+.endm
+
+/**
+ * @param unsigned dav1d_msac_decode_hi_tok_c(MsacContext *const s, uint16_t *const cdf)
+ * * Reg Alloction
+ * * vr0: cdf;
+ * * vr1: temp;
+ * * vr2: rng;
+ * * vr3: dif;
+ * * vr4: const 0xff00ff00...ff00ff00;
+ * * vr5: const 0x0004080c;
+ * * vr6: const 0;
+ * * t0: allow_update_cdf, tmp;
+ * * t1: dif;
+ * * t2: cnt;
+ * * t3: 0xffffffe8, outermost control parameter;
+ * * t4: loop time
+ * * t5: v, buf_pos, temp;
+ * * t6: u, rng, end_pos, buf, temp;
+ * * t7: temp;
+ */
+function msac_decode_hi_tok_lsx
+    fld.d     f0,    a1,   0    // Load cdf[0]~cdf[3]
+    vldrepl.h vr2,   a0,   0x18 //  8 x rng, assert(rng <= 65535U), only the lower 16 bits are valid
+    vldrepl.h vr3,   a0,   0x16 // broadcast the high 16 bits of dif, c = s->dif >> (EC_WIN_SIZE - 16)
+    ld.w      t0,    a0,   0x20 // allow_update_cdf
+    la.local  t7,    ph_0xff00
+    vld       vr4,   t7,   0x00 // 0xff00ff00...ff00ff00
+    la.local  t7,    min_prob
+    vld       vr5,   t7,   12 * 2 // 0x0004080c
+    vxor.v    vr6,   vr6,  vr6    // const 0
+    ld.d      t1,    a0,   0x10   // dif
+    ld.w      t2,    a0,   0x1c   // cnt
+    orn       t3,    t3,   t3
+    srli.d    t3,    t3,   32
+    addi.d    t3,    t3,  -0x17 // 0xffffffe8
+    vseq.h    vr8,   vr8,  vr8
+    addi.d    sp,    sp,  -0x1a // alloc stack
+    beqz      t0,    .hi_tok_lsx_no_update_cdf
+    HI_TOK 1
+    jirl      zero,  ra,   0x0
+.hi_tok_lsx_no_update_cdf:
+    HI_TOK 0
+endfunc
diff --git a/src/loongarch/msac.h b/src/loongarch/msac.h
index fdcff83..146557c 100644
--- a/src/loongarch/msac.h
+++ b/src/loongarch/msac.h
@@ -36,11 +36,15 @@ unsigned dav1d_msac_decode_symbol_adapt16_lsx(MsacContext *s, uint16_t *cdf,
                                                size_t n_symbols);
 unsigned dav1d_msac_decode_bool_adapt_lsx(MsacContext *s, uint16_t *cdf);
 unsigned dav1d_msac_decode_bool_lsx(MsacContext *s, unsigned f);
+unsigned dav1d_msac_decode_bool_equi_lsx(MsacContext *s);
+unsigned dav1d_msac_decode_hi_tok_lsx(MsacContext *s, uint16_t *cdf);
 
 #define dav1d_msac_decode_symbol_adapt4  dav1d_msac_decode_symbol_adapt4_lsx
 #define dav1d_msac_decode_symbol_adapt8  dav1d_msac_decode_symbol_adapt8_lsx
 #define dav1d_msac_decode_symbol_adapt16 dav1d_msac_decode_symbol_adapt16_lsx
 #define dav1d_msac_decode_bool_adapt     dav1d_msac_decode_bool_adapt_lsx
 #define dav1d_msac_decode_bool           dav1d_msac_decode_bool_lsx
+#define dav1d_msac_decode_bool_equi      dav1d_msac_decode_bool_equi_lsx
+#define dav1d_msac_decode_hi_tok         dav1d_msac_decode_hi_tok_lsx
 
 #endif /* DAV1D_SRC_LOONGARCH_MSAC_H */
diff --git a/src/loongarch/refmvs.S b/src/loongarch/refmvs.S
index 63a83d3..2682d1b 100644
--- a/src/loongarch/refmvs.S
+++ b/src/loongarch/refmvs.S
@@ -150,3 +150,559 @@ function splat_mv_lsx
 
 .splat_end:
 endfunc
+
+const la_div_mult
+.short    0, 16384, 8192, 5461, 4096, 3276, 2730, 2340
+.short 2048,  1820, 1638, 1489, 1365, 1260, 1170, 1092
+.short 1024,   963,  910,  862,  819,  780,  744,  712
+.short  682,   655,  630,  606,  585,  564,  546,  528
+endconst
+
+/*
+ *  temp reg: a6 a7
+ */
+.macro LOAD_SET_LOOP is_odd
+    slli.d          a6,      t6,     2
+    add.d           a6,      a6,     t6  // col_w * 5
+0:
+    addi.d          a7,      zero,   0   // x
+.if \is_odd
+    stx.w           t7,      t3,     a7
+    addi.d          a7,      a7,     5
+    bge             a7,      a6,     2f
+.endif
+
+1:
+    stx.w           t7,      t3,     a7
+    addi.d          a7,      a7,     5
+    stx.w           t7,      t3,     a7
+    addi.d          a7,      a7,     5
+    blt             a7,      a6,     1b
+2:
+    add.d           t3,      t3,     t2
+    addi.d          t5,      t5,     1
+    blt             t5,      a5,     0b
+.endm
+
+/*
+ * static void load_tmvs_c(const refmvs_frame *const rf, int tile_row_idx,
+ *                         const int col_start8, const int col_end8,
+ *                         const int row_start8, int row_end8)
+ */
+function load_tmvs_lsx
+    addi.d         sp,      sp,       -80
+    st.d           s0,      sp,       0
+    st.d           s1,      sp,       8
+    st.d           s2,      sp,       16
+    st.d           s3,      sp,       24
+    st.d           s4,      sp,       32
+    st.d           s5,      sp,       40
+    st.d           s6,      sp,       48
+    st.d           s7,      sp,       56
+    st.d           s8,      sp,       64
+
+    vld           vr16,     a0,       16
+    vld           vr0,      a0,       52    // rf->mfmv_ref
+    ld.w          s8,       a0,       152   // [0] - rf->n_mfmvs
+    vld           vr17,     a0,       168   // [0] - rp_ref| [1]- rp_proj
+    ld.d          t1,       a0,       184   // stride
+    ld.w          t0,       a0,       200
+    addi.w        t0,       t0,       -1
+    bnez          t0,       1f
+    addi.w        a1,       zero,     0
+1:
+    addi.d        t0,       a3,       8
+    vinsgr2vr.w   vr1,      t0,       0
+    vinsgr2vr.w   vr1,      a5,       1
+    vmin.w        vr1,      vr1,      vr16  // [0] col_end8i [1] row_end8
+    addi.d        t0,       a2,       -8
+    bge           t0,       zero,     2f
+    addi.w        t0,       zero,     0     // t0 col_start8i
+2:
+    vpickve2gr.d  t4,       vr17,     1     // rf->rp_proj
+    slli.d        t2,       t1,       2
+    add.d         t2,       t2,       t1    // stride * 5
+    slli.d        a1,       a1,       4     // tile_row_idx * 16
+    andi          t3,       a4,       0xf
+    add.d         t3,       t3,       a1    // tile_row_idx * 16 + row_start8 & 15
+    mul.w         t3,       t3,       t2
+    mul.w         t8,       a1,       t2
+    vpickve2gr.w  a5,       vr1,      1
+    addi.d        t5,       a4,       0
+    sub.d         t6,       a3,       a2     // col_end8 - col_start8
+    li.w          t7,       0x80008000
+    slli.d        a7,       a2,       2
+    add.d         t3,       t3,       a2
+    add.d         t3,       t3,       a7
+    add.d         t3,       t3,       t4     // rp_proj
+    andi          a6,       t6,       1
+    bnez          a6,       3f
+    LOAD_SET_LOOP 0
+    b             4f
+3:
+    LOAD_SET_LOOP 1
+4:
+    addi.d        a6,       zero,     0      // n
+    bge           a6,       s8,       .end_load
+    add.d         t3,       t8,       t4     // rp_proj
+    mul.w         t6,       a4,       t2
+    addi.d        s7,       zero,     40
+    vpickve2gr.w  t1,       vr1,      0      // col_end8i
+    vbsrl.v       vr2,      vr0,      4      // rf->mfmv_ref2cur
+    addi.d        t5,       a0,       64     // rf->mfmv_ref2ref
+    la.local      t8,       la_div_mult
+    vld           vr6,      t8,       0
+    vld           vr7,      t8,       16
+    vld           vr8,      t8,       32
+    vld           vr9,      t8,       48
+    li.w          t8,       0x3fff
+    vreplgr2vr.h  vr21,     t8
+    vxor.v        vr18,     vr18,     vr18   // zero
+    vsub.h        vr20,     vr18,     vr21
+    vpickev.b     vr12,     vr7,      vr6
+    vpickod.b     vr13,     vr7,      vr6
+    vpickev.b     vr14,     vr9,      vr8
+    vpickod.b     vr15,     vr9,      vr8
+    vpickve2gr.d  s6,       vr17,     0       // rf->rp_ref
+5:
+    vld           vr10,     t5,       0
+    vld           vr11,     t5,       16
+    vpickev.h     vr10,     vr11,     vr10
+    vpickev.b     vr10,     vr11,     vr10    // [1...7]
+
+    vbsrl.v       vr0,      vr0,      1
+    vpickve2gr.wu t8,       vr2,      0       // ref2cur
+    vbsrl.v       vr2,      vr2,      4
+    srli.d        t4,       t8,       24
+    xori          t4,       t4,       0x80
+    beqz          t4,       8f
+
+    vreplgr2vr.h  vr23,     t8
+    vshuf.b       vr6,      vr14,     vr12,    vr10
+    vshuf.b       vr7,      vr15,     vr13,    vr10
+    vilvl.b       vr8,      vr7,      vr6
+    vmulwev.w.h   vr6,      vr8,      vr23
+    vmulwod.w.h   vr7,      vr8,      vr23
+
+    vpickve2gr.b  s0,       vr0,      0       // ref
+    slli.d        t8,       s0,       3
+    ldx.d         s1,       s6,       t8      // rf->rp_ref[ref]
+    addi.d        s0,       s0,       -4      // ref_sign
+    vreplgr2vr.h  vr19,     s0
+    add.d         s1,       s1,       t6      // &rf->rp_ref[ref][row_start8 * stride]
+    addi.d        s2,       a4,       0       // y
+    vilvl.w       vr8,      vr7,      vr6
+    vilvh.w       vr9,      vr7,      vr6
+6:                                            // for (int y = row_start8;
+    andi          s3,       s2,       0xff8
+
+    addi.d        s4,       s3,       8
+    blt           a4,       s3,       0f
+    addi.d        s3,       a4,       0        // y_proj_start
+0:
+    blt           s4,       a5,       0f
+    addi.d        s4,       a5,       0        // y_proj_end
+0:
+    addi.d        s5,       t0,       0        // x
+7:                                             // for (int x = col_start8i;
+    slli.d        a7,       s5,       2
+    add.d         a7,       a7,       s5
+    add.d         a7,       s1,       a7      // rb
+    vld           vr3,      a7,       0       // [rb]
+    vpickve2gr.b  t4,       vr3,      4       // b_ref
+    beqz          t4,       .end_x
+    vreplve.b     vr11,     vr10,     t4
+    vpickve2gr.b  t7,       vr11,     4       // ref2ref
+    beqz          t7,       .end_x
+    vsllwil.w.h   vr4,      vr3,      0
+    vreplgr2vr.w  vr6,      t4
+    vshuf.w       vr6,      vr9,      vr8      // frac
+    vmul.w        vr5,      vr6,      vr4
+    vsrai.w       vr4,      vr5,      31
+    vadd.w        vr4,      vr4,      vr5
+    vssrarni.h.w  vr4,      vr4,      14
+    vclip.h       vr4,      vr4,      vr20,    vr21  // offset
+    vxor.v        vr5,      vr4,      vr19    // offset.x ^ ref_sign
+    vori.b        vr5,      vr5,      0x1     // offset.x ^ ref_sign
+    vabsd.h       vr4,      vr4,      vr18
+    vsrli.h       vr4,      vr4,      6       // abs(offset.x) >> 6
+    vsigncov.h    vr4,      vr5,      vr4     // apply_sign
+    vpickve2gr.h  s0,       vr4,      0
+    add.d         s0,       s2,       s0      // pos_y
+    blt           s0,       s3,       .n_posy
+    bge           s0,       s4,       .n_posy
+    andi          s0,       s0,       0xf
+    mul.w         s0,       s0,       t2      // pos
+    vpickve2gr.h  t7,       vr4,      1
+    add.d         t7,       t7,       s5      // pos_x
+    add.d         s0,       t3,       s0      // rp_proj + pos
+
+.loop_posx:
+    andi          t4,       s5,       0xff8 // x_sb_align
+
+    blt           t7,       a2,       .n_posx
+    addi.d        t8,       t4,       -8
+    blt           t7,       t8,       .n_posx
+
+    bge           t7,       a3,       .n_posx
+    addi.d        t4,       t4,       16
+    bge           t7,       t4,       .n_posx
+
+    slli.d        t4,       t7,       2
+    add.d         t4,       t4,       t7      // pos_x * 5
+    add.d         t4,       s0,       t4      // rp_proj[pos + pos_x]
+    vstelm.w      vr3,      t4,       0,   0
+    vstelm.b      vr11,     t4,       4,   4
+
+.n_posx:
+    addi.d        s5,       s5,       1       // x + 1
+    bge           s5,       t1,       .ret_posx
+    addi.d        a7,       a7,       5       // rb + 1
+    vld           vr4,      a7,       0       // [rb]
+    vseq.b        vr5,      vr4,      vr3
+
+    vpickve2gr.d  t8,       vr5,      0
+    cto.d         t8,       t8
+    blt           t8,       s7,       7b
+
+    addi.d        t7,       t7,       1       // pos_x + 1
+
+    /*  Core computing loop expansion(sencond)  */
+    andi          t4,       s5,       0xff8 // x_sb_align
+
+    blt           t7,       a2,       .n_posx
+    addi.d        t8,       t4,       -8
+    blt           t7,       t8,       .n_posx
+
+    bge           t7,       a3,       .n_posx
+    addi.d        t4,       t4,       16
+    bge           t7,       t4,       .n_posx
+
+    slli.d        t4,       t7,       2
+    add.d         t4,       t4,       t7      // pos_x * 5
+    add.d         t4,       s0,       t4      // rp_proj[pos + pos_x]
+    vstelm.w      vr3,      t4,       0,   0
+    vstelm.b      vr11,     t4,       4,   4
+
+    addi.d        s5,       s5,       1       // x + 1
+    bge           s5,       t1,       .ret_posx
+    addi.d        a7,       a7,       5       // rb + 1
+    vld           vr4,      a7,       0       // [rb]
+    vseq.b        vr5,      vr4,      vr3
+
+    vpickve2gr.d  t8,       vr5,      0
+    cto.d         t8,       t8
+    blt           t8,       s7,       7b
+
+    addi.d        t7,       t7,       1       // pos_x + 1
+
+    /*  Core computing loop expansion(third)  */
+    andi          t4,       s5,       0xff8 // x_sb_align
+
+    blt           t7,       a2,       .n_posx
+    addi.d        t8,       t4,       -8
+    blt           t7,       t8,       .n_posx
+
+    bge           t7,       a3,       .n_posx
+    addi.d        t4,       t4,       16
+    bge           t7,       t4,       .n_posx
+
+    slli.d        t4,       t7,       2
+    add.d         t4,       t4,       t7      // pos_x * 5
+    add.d         t4,       s0,       t4      // rp_proj[pos + pos_x]
+    vstelm.w      vr3,      t4,       0,   0
+    vstelm.b      vr11,     t4,       4,   4
+
+    addi.d        s5,       s5,       1       // x + 1
+    bge           s5,       t1,       .ret_posx
+    addi.d        a7,       a7,       5       // rb + 1
+    vld           vr4,      a7,       0       // [rb]
+    vseq.b        vr5,      vr4,      vr3
+
+    vpickve2gr.d  t8,       vr5,      0
+    cto.d         t8,       t8
+    blt           t8,       s7,       7b
+
+    addi.d        t7,       t7,       1       // pos_x + 1
+
+    b             .loop_posx
+
+.n_posy:
+    addi.d        s5,       s5,       1       // x + 1
+    bge           s5,       t1,       .ret_posx
+    addi.d        a7,       a7,       5       // rb + 1
+    vld           vr4,      a7,       0       // [rb]
+    vseq.b        vr5,      vr4,      vr3
+
+    vpickve2gr.d  t8,       vr5,      0
+    cto.d         t8,       t8
+    blt           t8,       s7,       7b
+
+    addi.d        s5,       s5,       1       // x + 1
+    bge           s5,       t1,       .ret_posx
+    addi.d        a7,       a7,       5       // rb + 1
+    vld           vr4,      a7,       0       // [rb]
+    vseq.b        vr5,      vr4,      vr3
+
+    vpickve2gr.d  t8,       vr5,      0
+    cto.d         t8,       t8
+    blt           t8,       s7,       7b
+
+    b             .n_posy
+
+.end_x:
+    addi.d        s5,       s5,       1       // x + 1
+    blt           s5,       t1,       7b
+
+.ret_posx:
+    add.d         s1,       s1,       t2      // r + stride
+    addi.d        s2,       s2,       1       // y + 1
+    blt           s2,       a5,       6b
+8:
+    addi.d        a6,       a6,       1       // n + 1
+    addi.d        t5,       t5,       28      // mfmv_ref2ref(offset) + 28
+    blt           a6,       s8,       5b
+
+.end_load:
+    ld.d           s0,      sp,       0
+    ld.d           s1,      sp,       8
+    ld.d           s2,      sp,       16
+    ld.d           s3,      sp,       24
+    ld.d           s4,      sp,       32
+    ld.d           s5,      sp,       40
+    ld.d           s6,      sp,       48
+    ld.d           s7,      sp,       56
+    ld.d           s8,      sp,       64
+    addi.d         sp,      sp,       80
+endfunc
+
+const mv_tbls
+    .byte           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255
+    .byte           0, 1, 2, 3, 8, 0, 1, 2, 3, 8, 0, 1, 2, 3, 8, 0
+    .byte           4, 5, 6, 7, 9, 4, 5, 6, 7, 9, 4, 5, 6, 7, 9, 4
+    .byte           4, 5, 6, 7, 9, 4, 5, 6, 7, 9, 4, 5, 6, 7, 9, 4
+endconst
+
+const mask_mult
+    .byte           1, 0, 2, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0
+endconst
+
+const mask_mv0
+    .byte           1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16
+endconst
+
+const mask_mv1
+    .byte           4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19
+endconst
+
+// void dav1d_save_tmvs_lsx(refmvs_temporal_block *rp, ptrdiff_t stride,
+//                          refmvs_block **rr, const uint8_t *ref_sign,
+//                          int col_end8, int row_end8,
+//                          int col_start8, int row_start8)
+function save_tmvs_lsx
+    addi.d      sp,         sp,        -0x28
+    st.d        s0,         sp,         0x00
+    st.d        s1,         sp,         0x08
+    st.d        s2,         sp,         0x10
+    st.d        s3,         sp,         0x18
+    st.d        s4,         sp,         0x20
+    move        t0,         ra
+
+    vxor.v      vr10,       vr10,       vr10
+    vld         vr11,       a3,         0       // Load ref_sign[0] ~ Load ref_sign[7]
+    la.local    t2,         .save_tevs_tbl
+    la.local    s1,         mask_mult
+    la.local    t7,         mv_tbls
+    vld         vr9,        s1,         0       // Load mask_mult
+    vslli.d     vr11,       vr11,       8       // 0, ref_sign[0], ... ,ref_sign[6]
+    la.local    s3,         mask_mv0
+    vld         vr8,        s3,         0       // Load mask_mv0
+    la.local    s4,         mask_mv1
+    vld         vr7,        s4,         0       // Load mask_mv1
+    li.d        s0,         5
+    li.d        t8,         12 * 2
+    mul.d       a1,         a1,         s0     // stride *= 5
+    sub.d       a5,         a5,         a7      // h = row_end8 - row_start8
+    slli.d      a7,         a7,         1       // row_start8 <<= 1
+1:
+    li.d        s0,         5
+    andi        t3,         a7,         30      // (y & 15) * 2
+    slli.d      s4,         t3,         3
+    ldx.d       t3,         a2,         s4      // b = rr[(y & 15) * 2]
+    addi.d      t3,         t3,         12      // &b[... + 1]
+    mul.d       s4,         a4,         t8
+    add.d       t4,         s4,         t3      // end_cand_b = &b[col_end8*2 + 1]
+    mul.d       s3,         a6,         t8
+    add.d       t3,         s3,         t3      // cand_b = &b[x*2 + 1]
+    mul.d       s4,         a6,         s0
+    add.d       a3,         s4,         a0      // &rp[x]
+2:
+    /* First cand_b */
+    ld.b        t5,         t3,         10      // cand_b->bs
+    vld         vr0,        t3,         0       // cand_b->mv and ref
+    alsl.d      t5,         t5,         t2,     2  // bt2 index
+    ld.h        s3,         t3,         8       // cand_b->ref
+    ld.h        t6,         t5,         0       // bt2
+    move        s0,         t2
+    alsl.d      t3,         t6,         t3,     1   // Next cand_b += bt2 * 2
+    vor.v       vr2,        vr0,        vr0
+    vinsgr2vr.h vr1,        s3,         0
+    move        t1 ,        t3
+    bge         t3,         t4,        3f
+
+    /* Next cand_b */
+    ld.b        s0,         t3,         10      // cand_b->bs
+    vld         vr4,        t3,         0       // cand_b->mv and ref
+    alsl.d      s0,         s0,         t2,     2 // bt2 index
+    ld.h        s4,         t3,         8       // cand_b->ref
+    ld.h        t6,         s0,         0       // bt2
+    alsl.d      t3,         t6,         t3,     1   // Next cand_b += bt2*2
+    vpackev.d   vr2,        vr4,        vr0     // a0.mv[0] a0.mv[1] a1.mv[0], a1.mv[1]
+    vinsgr2vr.h vr1,        s4,         1   // a0.ref[0] a0.ref[1], a1.ref[0], a1.ref[1]
+3:
+    vabsd.h     vr2,        vr2,        vr10    // abs(mv[].xy)
+    vsle.b      vr16,       vr10,       vr1
+    vand.v      vr1,        vr16,       vr1
+    vshuf.b     vr1,        vr11,       vr11,   vr1     // ref_sign[ref]
+    vsrli.h     vr2,        vr2,        12      // abs(mv[].xy) >> 12
+    vilvl.b     vr1,        vr1,        vr1
+    vmulwev.h.bu    vr1,    vr1,        vr9    // ef_sign[ref] * {1, 2}
+
+    vseqi.w     vr2,        vr2,        0       // abs(mv[].xy) <= 4096
+    vpickev.h   vr2,        vr2,        vr2     // abs() condition to 16 bit
+
+    vand.v      vr1,        vr2,        vr1     // h[0-3] contains conditions for mv[0-1]
+    vhaddw.wu.hu    vr1,    vr1,        vr1     // Combine condition for [1] and [0]
+    vpickve2gr.wu   s1,     vr1,        0       // Extract case for first block
+    vpickve2gr.wu   s2,     vr1,        1
+
+    ld.hu           t5,     t5,         2       // Fetch jump table entry
+    ld.hu           s0,     s0,         2
+    alsl.d          s3,     s1,         t7,    4   // Load permutation table base on case
+    vld             vr1,    s3,         0
+    alsl.d          s4,     s2,         t7,    4
+    vld             vr5,    s4,         0
+    sub.d           t5,     t2,         t5     // Find jump table target
+    sub.d           s0,     t2,         s0
+
+    vshuf.b         vr0,    vr0,        vr0,    vr1 // Permute cand_b to output refmvs_temporal_block
+    vshuf.b         vr4,    vr4,        vr4,    vr5
+    vsle.b          vr16,   vr10,       vr1
+    vand.v          vr0,    vr16,       vr0
+
+    vsle.b          vr17,   vr10,       vr5
+    vand.v          vr4,    vr17,       vr4
+    // v1 follows on v0, with another 3 full repetitions of the pattern.
+    vshuf.b         vr1,    vr0,        vr0,    vr8 // 1, 2, 3, ... , 15, 16
+    vshuf.b         vr5,    vr4,        vr4,    vr8 // 1, 2, 3, ... , 15, 16
+    // v2 ends with 3 complete repetitions of the pattern.
+    vshuf.b         vr2,    vr1,        vr0,    vr7
+    vshuf.b         vr6,    vr5,        vr4,    vr7    // 4, 5, 6, 7, ... , 12, 13, 14, 15, 16, 17, 18, 19
+
+    jirl            ra,     t5,         0
+    bge             t1 ,    t4,         4f      // if (cand_b >= end)
+    vor.v           vr0,    vr4,        vr4
+    vor.v           vr1,    vr5,        vr5
+    vor.v           vr2,    vr6,        vr6
+    jirl            ra,     s0,         0
+    blt             t3,     t4,         2b      // if (cand_b < end)
+
+4:
+    addi.d          a5,     a5,         -1      // h--
+    addi.d          a7,     a7,         2       // y += 2
+    add.d           a0,     a0,         a1      // rp += stride
+    blt             zero,   a5,         1b
+
+    ld.d        s0,         sp,         0x00
+    ld.d        s1,         sp,         0x08
+    ld.d        s2,         sp,         0x10
+    ld.d        s3,         sp,         0x18
+    ld.d        s4,         sp,         0x20
+    addi.d      sp,         sp,         0x28
+
+    move            ra,     t0
+    jirl            zero,   ra,         0x00
+
+10:
+    addi.d          s1,     a3,         4
+    vstelm.w        vr0,    a3,         0,      0   // .mv
+    vstelm.b        vr0,    s1,         0,      4   // .ref
+    addi.d          a3,     a3,         5
+    jirl            zero,   ra,         0x00
+20:
+    addi.d          s1,     a3,         8
+    vstelm.d        vr0,    a3,         0,      0   // .mv
+    vstelm.h        vr0,    s1,         0,      4   // .ref
+    addi.d          a3,     a3,         2 * 5
+    jirl            zero,   ra,         0x00
+40:
+    vst             vr0,    a3,         0
+    vstelm.w        vr1,    a3,         0x10,   0
+    addi.d          a3,     a3,         4 * 5
+    jirl            zero,   ra,         0x00
+
+80:
+    vst             vr0,    a3,         0
+    vst             vr1,    a3,         0x10           // This writes 6 full entries plus 2 extra bytes
+    vst             vr2,    a3,         5 * 8 - 16     // Write the last few, overlapping with the first write.
+    addi.d          a3,     a3,         8 * 5
+    jirl            zero,   ra,         0x00
+160:
+    addi.d          s1,     a3,         6 * 5
+    addi.d          s2,     a3,         12 * 5
+    vst             vr0,    a3,         0
+    vst             vr1,    a3,         0x10          // This writes 6 full entries plus 2 extra bytes
+    vst             vr0,    a3,         6 * 5
+    vst             vr1,    a3,         6 * 5 + 16    // Write another 6 full entries, slightly overlapping with the first set
+    vstelm.d        vr0,    s2,         0,      0     // Write 8 bytes (one full entry) after the first 12
+    vst             vr2,    a3,         5 * 16 - 16   // Write the last 3 entries
+    addi.d          a3,     a3,         16 * 5
+    jirl            zero,   ra,         0x00
+
+.save_tevs_tbl:
+        .hword 16 * 12   // bt2 * 12, 12 is sizeof(refmvs_block)
+        .hword .save_tevs_tbl - 160b
+        .hword 16 * 12
+        .hword .save_tevs_tbl - 160b
+        .hword 8 * 12
+        .hword .save_tevs_tbl -  80b
+        .hword 8 * 12
+        .hword .save_tevs_tbl -  80b
+        .hword 8 * 12
+        .hword .save_tevs_tbl -  80b
+        .hword 8 * 12
+        .hword .save_tevs_tbl -  80b
+        .hword 4 * 12
+        .hword .save_tevs_tbl -  40b
+        .hword 4 * 12
+        .hword .save_tevs_tbl -  40b
+        .hword 4 * 12
+        .hword .save_tevs_tbl -  40b
+        .hword 4 * 12
+        .hword .save_tevs_tbl -  40b
+        .hword 2 * 12
+        .hword .save_tevs_tbl -  20b
+        .hword 2 * 12
+        .hword .save_tevs_tbl -  20b
+        .hword 2 * 12
+        .hword .save_tevs_tbl -  20b
+        .hword 2 * 12
+        .hword .save_tevs_tbl -  20b
+        .hword 2 * 12
+        .hword .save_tevs_tbl -  20b
+        .hword 1 * 12
+        .hword .save_tevs_tbl -  10b
+        .hword 1 * 12
+        .hword .save_tevs_tbl -  10b
+        .hword 1 * 12
+        .hword .save_tevs_tbl -  10b
+        .hword 1 * 12
+        .hword .save_tevs_tbl -  10b
+        .hword 1 * 12
+        .hword .save_tevs_tbl -  10b
+        .hword 1 * 12
+        .hword .save_tevs_tbl -  10b
+        .hword 1 * 12
+        .hword .save_tevs_tbl -  10b
+endfunc
+
diff --git a/src/loongarch/refmvs.h b/src/loongarch/refmvs.h
index 60ff435..f944f30 100644
--- a/src/loongarch/refmvs.h
+++ b/src/loongarch/refmvs.h
@@ -32,6 +32,8 @@
 #include "src/refmvs.h"
 
 decl_splat_mv_fn(dav1d_splat_mv_lsx);
+decl_load_tmvs_fn(dav1d_load_tmvs_lsx);
+decl_save_tmvs_fn(dav1d_save_tmvs_lsx);
 
 static ALWAYS_INLINE void refmvs_dsp_init_loongarch(Dav1dRefmvsDSPContext *const c) {
     const unsigned flags = dav1d_get_cpu_flags();
@@ -39,6 +41,8 @@ static ALWAYS_INLINE void refmvs_dsp_init_loongarch(Dav1dRefmvsDSPContext *const
     if (!(flags & DAV1D_LOONGARCH_CPU_FLAG_LSX)) return;
 
     c->splat_mv = dav1d_splat_mv_lsx;
+    c->load_tmvs = dav1d_load_tmvs_lsx;
+    c->save_tmvs = dav1d_save_tmvs_lsx;
 }
 
 #endif /* DAV1D_SRC_LOONGARCH_REFMVS_H */
diff --git a/src/mc_tmpl.c b/src/mc_tmpl.c
index 469fc5f..d6246d7 100644
--- a/src/mc_tmpl.c
+++ b/src/mc_tmpl.c
@@ -907,6 +907,8 @@ static void resize_c(pixel *dst, const ptrdiff_t dst_stride,
 #include "src/arm/mc.h"
 #elif ARCH_LOONGARCH64
 #include "src/loongarch/mc.h"
+#elif ARCH_RISCV
+#include "src/riscv/mc.h"
 #elif ARCH_X86
 #include "src/x86/mc.h"
 #endif
@@ -950,6 +952,8 @@ COLD void bitfn(dav1d_mc_dsp_init)(Dav1dMCDSPContext *const c) {
     mc_dsp_init_arm(c);
 #elif ARCH_LOONGARCH64
     mc_dsp_init_loongarch(c);
+#elif ARCH_RISCV
+    mc_dsp_init_riscv(c);
 #elif ARCH_X86
     mc_dsp_init_x86(c);
 #endif
diff --git a/src/mem.c b/src/mem.c
index 7e6eb4c..9f0e394 100644
--- a/src/mem.c
+++ b/src/mem.c
@@ -109,16 +109,7 @@ void *dav1d_malloc(const enum AllocationType type, const size_t sz) {
 void *dav1d_alloc_aligned(const enum AllocationType type,
                           const size_t sz, const size_t align)
 {
-    assert(!(align & (align - 1)));
-    void *ptr;
-#ifdef _WIN32
-    ptr = _aligned_malloc(sz + align, align);
-#elif defined(HAVE_POSIX_MEMALIGN)
-    if (posix_memalign(&ptr, align, sz + align)) return NULL;
-#else
-    ptr = memalign(align, sz + align);
-#endif
-
+    void *const ptr = dav1d_alloc_aligned_internal(align, sz + align);
     return track_alloc(type, ptr, sz, align);
 }
 
@@ -140,12 +131,7 @@ void dav1d_free(void *ptr) {
 
 void dav1d_free_aligned(void *ptr) {
     if (ptr) {
-        ptr = track_free(ptr);
-#ifdef _WIN32
-        _aligned_free(ptr);
-#else
-        free(ptr);
-#endif
+        dav1d_free_aligned_internal(track_free(ptr));
     }
 }
 
diff --git a/src/mem.h b/src/mem.h
index 0a8c18d..c8c45d3 100644
--- a/src/mem.h
+++ b/src/mem.h
@@ -32,7 +32,7 @@
 
 #include <stdlib.h>
 
-#if defined(_WIN32) || !defined(HAVE_POSIX_MEMALIGN)
+#if defined(_WIN32) || HAVE_MEMALIGN
 #include <malloc.h>
 #endif
 
@@ -79,39 +79,33 @@ typedef struct Dav1dMemPool {
 #endif
 } Dav1dMemPool;
 
-
-#if TRACK_HEAP_ALLOCATIONS
-void *dav1d_malloc(enum AllocationType type, size_t sz);
-void *dav1d_realloc(enum AllocationType type, void *ptr, size_t sz);
-void *dav1d_alloc_aligned(enum AllocationType type, size_t sz, size_t align);
-void dav1d_free(void *ptr);
-void dav1d_free_aligned(void *ptr);
-void dav1d_log_alloc_stats(Dav1dContext *c);
-#else
-#define dav1d_mem_pool_init(type, pool) dav1d_mem_pool_init(pool)
-#define dav1d_malloc(type, sz) malloc(sz)
-#define dav1d_realloc(type, ptr, sz) realloc(ptr, sz)
-#define dav1d_free(ptr) free(ptr)
+// TODO: Move this to a common location?
+#define ROUND_UP(x,a) (((x)+((a)-1)) & ~((a)-1))
 
 /*
  * Allocate align-byte aligned memory. The return value can be released
  * by calling the dav1d_free_aligned() function.
  */
-static inline void *dav1d_alloc_aligned(const size_t sz, const size_t align) {
+static inline void *dav1d_alloc_aligned_internal(const size_t sz, const size_t align) {
     assert(!(align & (align - 1)));
 #ifdef _WIN32
     return _aligned_malloc(sz, align);
-#elif defined(HAVE_POSIX_MEMALIGN)
+#elif HAVE_POSIX_MEMALIGN
     void *ptr;
     if (posix_memalign(&ptr, align, sz)) return NULL;
     return ptr;
-#else
+#elif HAVE_MEMALIGN
     return memalign(align, sz);
+#elif HAVE_ALIGNED_ALLOC
+    // The C11 standard specifies that the size parameter
+    // must be an integral multiple of alignment.
+    return aligned_alloc(align, ROUND_UP(sz, align));
+#else
+#error No aligned allocation functions are available
 #endif
 }
-#define dav1d_alloc_aligned(type, sz, align) dav1d_alloc_aligned(sz, align)
 
-static inline void dav1d_free_aligned(void *ptr) {
+static inline void dav1d_free_aligned_internal(void *ptr) {
 #ifdef _WIN32
     _aligned_free(ptr);
 #else
@@ -119,6 +113,20 @@ static inline void dav1d_free_aligned(void *ptr) {
 #endif
 }
 
+#if TRACK_HEAP_ALLOCATIONS
+void *dav1d_malloc(enum AllocationType type, size_t sz);
+void *dav1d_realloc(enum AllocationType type, void *ptr, size_t sz);
+void *dav1d_alloc_aligned(enum AllocationType type, size_t sz, size_t align);
+void dav1d_free(void *ptr);
+void dav1d_free_aligned(void *ptr);
+void dav1d_log_alloc_stats(Dav1dContext *c);
+#else
+#define dav1d_mem_pool_init(type, pool) dav1d_mem_pool_init(pool)
+#define dav1d_malloc(type, sz) malloc(sz)
+#define dav1d_realloc(type, ptr, sz) realloc(ptr, sz)
+#define dav1d_alloc_aligned(type, sz, align) dav1d_alloc_aligned_internal(sz, align)
+#define dav1d_free(ptr) free(ptr)
+#define dav1d_free_aligned(ptr) dav1d_free_aligned_internal(ptr)
 #endif /* TRACK_HEAP_ALLOCATIONS */
 
 void dav1d_mem_pool_push(Dav1dMemPool *pool, Dav1dMemPoolBuffer *buf);
diff --git a/src/meson.build b/src/meson.build
index c754668..29fd5d6 100644
--- a/src/meson.build
+++ b/src/meson.build
@@ -30,6 +30,7 @@
 libdav1d_sources = files(
     'cdf.c',
     'cpu.c',
+    'ctx.c',
     'data.c',
     'decode.c',
     'dequant_tables.c',
@@ -119,6 +120,7 @@ if is_asm_enabled
                     'arm/64/loopfilter16.S',
                     'arm/64/looprestoration16.S',
                     'arm/64/mc16.S',
+                    'arm/64/mc16_sve.S',
                 )
             endif
         elif host_machine.cpu_family().startswith('arm')
@@ -174,7 +176,6 @@ if is_asm_enabled
             'x86/itx_avx512.asm',
             'x86/cdef_avx2.asm',
             'x86/itx_avx2.asm',
-            'x86/looprestoration_avx2.asm',
             'x86/cdef_sse.asm',
             'x86/itx_sse.asm',
         )
@@ -190,6 +191,7 @@ if is_asm_enabled
                 'x86/filmgrain_avx2.asm',
                 'x86/ipred_avx2.asm',
                 'x86/loopfilter_avx2.asm',
+                'x86/looprestoration_avx2.asm',
                 'x86/mc_avx2.asm',
                 'x86/filmgrain_sse.asm',
                 'x86/ipred_sse.asm',
@@ -237,6 +239,8 @@ if is_asm_enabled
         )}
 
         libdav1d_sources_asm = files(
+            'loongarch/cdef.S',
+            'loongarch/ipred.S',
             'loongarch/mc.S',
             'loongarch/loopfilter.S',
             'loongarch/looprestoration.S',
@@ -256,6 +260,7 @@ if is_asm_enabled
         )}
         arch_flags += {'pwr9': ['-mcpu=power9', '-DDAV1D_PWR9']}
         libdav1d_arch_tmpl_sources += {'pwr9': files(
+            'ppc/itx_tmpl.c',
             'ppc/loopfilter_tmpl.c',
         )}
     elif host_machine.cpu_family().startswith('riscv')
@@ -264,9 +269,26 @@ if is_asm_enabled
         )
         if host_machine.cpu_family() == 'riscv64'
             libdav1d_sources += files(
+                'riscv/64/cdef.S',
                 'riscv/64/cpu.S',
                 'riscv/64/itx.S',
+                'riscv/64/pal.S',
+                'riscv/64/mc.S',
             )
+
+            if dav1d_bitdepths.contains('8')
+                libdav1d_sources += files(
+                    'riscv/64/cdef.S',
+                    'riscv/64/ipred.S',
+                )
+            endif
+
+            if dav1d_bitdepths.contains('16')
+                libdav1d_sources += files(
+                    'riscv/64/cdef16.S',
+                    'riscv/64/ipred16.S',
+                )
+            endif
         endif
     endif
 endif
@@ -370,7 +392,7 @@ libdav1d = library('dav1d',
 )
 
 dav1d_dep = declare_dependency(link_with: libdav1d,
-    include_directories : include_directories('../include/dav1d')
+    include_directories : include_directories('../include')
 )
 
 #
diff --git a/src/pal.c b/src/pal.c
index f50c7aa..8e2cdec 100644
--- a/src/pal.c
+++ b/src/pal.c
@@ -61,8 +61,10 @@ static void pal_idx_finish_c(uint8_t *dst, const uint8_t *src,
 }
 
 #if HAVE_ASM
-#if ARCH_X86
-#include "src/x86/pal.h"
+#if ARCH_RISCV
+#include "riscv/pal.h"
+#elif ARCH_X86
+#include "x86/pal.h"
 #endif
 #endif
 
@@ -70,7 +72,9 @@ COLD void dav1d_pal_dsp_init(Dav1dPalDSPContext *const c) {
     c->pal_idx_finish = pal_idx_finish_c;
 
 #if HAVE_ASM
-#if ARCH_X86
+#if ARCH_RISCV
+    pal_dsp_init_riscv(c);
+#elif ARCH_X86
     pal_dsp_init_x86(c);
 #endif
 #endif
diff --git a/src/picture.c b/src/picture.c
index 94365bc..290bd09 100644
--- a/src/picture.c
+++ b/src/picture.c
@@ -201,16 +201,6 @@ int dav1d_thread_picture_alloc(Dav1dContext *const c, Dav1dFrameContext *const f
                                   (void **) &p->progress);
     if (res) return res;
 
-    dav1d_picture_copy_props(&p->p, c->content_light, c->content_light_ref,
-                             c->mastering_display, c->mastering_display_ref,
-                             c->itut_t35, c->itut_t35_ref, c->n_itut_t35,
-                             &f->tile[0].data.m);
-
-    // Must be removed from the context after being attached to the frame
-    dav1d_ref_dec(&c->itut_t35_ref);
-    c->itut_t35 = NULL;
-    c->n_itut_t35 = 0;
-
     // Don't clear these flags from c->frame_flags if the frame is not going to be output.
     // This way they will be added to the next visible frame too.
     const int flags_mask = ((f->frame_hdr->show_frame || c->output_invisible_frames) &&
@@ -221,6 +211,22 @@ int dav1d_thread_picture_alloc(Dav1dContext *const c, Dav1dFrameContext *const f
 
     p->visible = f->frame_hdr->show_frame;
     p->showable = f->frame_hdr->showable_frame;
+
+    if (p->visible) {
+        // Only add HDR10+ and T35 metadata when show frame flag is enabled
+        dav1d_picture_copy_props(&p->p, c->content_light, c->content_light_ref,
+                                 c->mastering_display, c->mastering_display_ref,
+                                 c->itut_t35, c->itut_t35_ref, c->n_itut_t35,
+                                 &f->tile[0].data.m);
+
+        // Must be removed from the context after being attached to the frame
+        dav1d_ref_dec(&c->itut_t35_ref);
+        c->itut_t35 = NULL;
+        c->n_itut_t35 = 0;
+    } else {
+        dav1d_data_props_copy(&p->p.m, &f->tile[0].data.m);
+    }
+
     if (c->n_fc > 1) {
         atomic_init(&p->progress[0], 0);
         atomic_init(&p->progress[1], 0);
diff --git a/src/ppc/cpu.c b/src/ppc/cpu.c
index 5328763..f58e8fb 100644
--- a/src/ppc/cpu.c
+++ b/src/ppc/cpu.c
@@ -29,25 +29,26 @@
 
 #include "common/attributes.h"
 
+#include "src/cpu.h"
 #include "src/ppc/cpu.h"
 
-#if (defined(HAVE_GETAUXVAL) || defined(HAVE_ELF_AUX_INFO)) && ARCH_PPC64LE
+#define HAVE_AUX ((HAVE_GETAUXVAL || HAVE_ELF_AUX_INFO) && ARCH_PPC64LE)
+#if HAVE_AUX
 #include <sys/auxv.h>
-#define HAVE_AUX
 #endif
 
 COLD unsigned dav1d_get_cpu_flags_ppc(void) {
-    unsigned flags = 0;
-#if defined(HAVE_GETAUXVAL) && ARCH_PPC64LE
+    unsigned flags = dav1d_get_default_cpu_flags();
+#if HAVE_GETAUXVAL && ARCH_PPC64LE
     unsigned long hw_cap = getauxval(AT_HWCAP);
     unsigned long hw_cap2 = getauxval(AT_HWCAP2);
-#elif defined(HAVE_ELF_AUX_INFO) && ARCH_PPC64LE
+#elif HAVE_ELF_AUX_INFO && ARCH_PPC64LE
     unsigned long hw_cap = 0;
     unsigned long hw_cap2 = 0;
     elf_aux_info(AT_HWCAP, &hw_cap, sizeof(hw_cap));
     elf_aux_info(AT_HWCAP2, &hw_cap2, sizeof(hw_cap2));
 #endif
-#ifdef HAVE_AUX
+#if HAVE_AUX
     flags |= (hw_cap & PPC_FEATURE_HAS_VSX) ? DAV1D_PPC_CPU_FLAG_VSX : 0;
     flags |= (hw_cap2 & PPC_FEATURE2_ARCH_3_00) ? DAV1D_PPC_CPU_FLAG_PWR9 : 0;
 #endif
diff --git a/src/ppc/itx.h b/src/ppc/itx.h
new file mode 100644
index 0000000..6bddf7a
--- /dev/null
+++ b/src/ppc/itx.h
@@ -0,0 +1,65 @@
+/*
+ * Copyright © 2018, VideoLAN and dav1d authors
+ * Copyright © 2023, Luca Barbato
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "src/cpu.h"
+#include "src/itx.h"
+
+decl_itx17_fns( 4,  4, pwr9);
+decl_itx16_fns( 4,  8, pwr9);
+decl_itx16_fns( 4, 16, pwr9);
+decl_itx16_fns( 8,  4, pwr9);
+decl_itx16_fns( 8,  8, pwr9);
+decl_itx16_fns( 8, 16, pwr9);
+decl_itx2_fns ( 8, 32, pwr9);
+decl_itx16_fns(16,  4, pwr9);
+decl_itx16_fns(16,  8, pwr9);
+decl_itx12_fns(16, 16, pwr9);
+decl_itx2_fns (16, 32, pwr9);
+decl_itx2_fns (32,  8, pwr9);
+decl_itx2_fns (32, 16, pwr9);
+decl_itx2_fns (32, 32, pwr9);
+
+decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_16x64, pwr9));
+decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_32x64, pwr9));
+decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_64x16, pwr9));
+decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_64x32, pwr9));
+decl_itx_fn(BF(dav1d_inv_txfm_add_dct_dct_64x64, pwr9));
+
+static ALWAYS_INLINE void itx_dsp_init_ppc(Dav1dInvTxfmDSPContext *const c, const int bpc) {
+    const unsigned flags = dav1d_get_cpu_flags();
+
+    if (!(flags & DAV1D_PPC_CPU_FLAG_PWR9)) return;
+
+#if BITDEPTH == 8
+    assign_itx17_fn( ,  4,  4, pwr9);
+    assign_itx16_fn(R,  4,  8, pwr9);
+    assign_itx16_fn(R,  8,  4, pwr9);
+    assign_itx16_fn(,   8,  8, pwr9);
+    assign_itx16_fn(R,  4, 16, pwr9);
+    assign_itx16_fn(R,  16, 4, pwr9);
+#endif
+}
diff --git a/src/ppc/itx_tmpl.c b/src/ppc/itx_tmpl.c
new file mode 100644
index 0000000..8180655
--- /dev/null
+++ b/src/ppc/itx_tmpl.c
@@ -0,0 +1,2006 @@
+/*
+ * Copyright © 2024, VideoLAN and dav1d authors
+ * Copyright © 2024, Luca Barbato
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "src/ppc/dav1d_types.h"
+#include "src/ppc/itx.h"
+#include "src/ppc/utils.h"
+
+#if BITDEPTH == 8
+
+#define LOAD_4(src, stride, a, b, c, d) \
+{  \
+    uint8_t *s = src; \
+    a = vec_xl(0, s); \
+    s += stride; \
+    b = vec_xl(0, s); \
+    s += stride; \
+    c = vec_xl(0, s); \
+    s += stride; \
+    d = vec_xl(0, s); \
+}
+
+#define LOAD_DECLARE_2_I16(src, a, b) \
+    i16x8 a = vec_xl(0, src); \
+    i16x8 b = vec_xl(0, src + 8);
+
+#define UNPACK_DECLARE_4_I16_I32(sa, sb, a, b, c, d) \
+    i32x4 a = i16h_to_i32(sa); \
+    i32x4 b = i16l_to_i32(sa); \
+    i32x4 c = i16h_to_i32(sb); \
+    i32x4 d = i16l_to_i32(sb);
+
+#define LOAD_COEFF_4(coeff) \
+    LOAD_DECLARE_2_I16(coeff, c01, c23) \
+    UNPACK_DECLARE_4_I16_I32(c01, c23, c0, c1, c2, c3)
+
+#define LOAD_SCALE_COEFF_4x8(coeff, scale) \
+    LOAD_DECLARE_2_I16(coeff, c04, c15) \
+    LOAD_DECLARE_2_I16(coeff+16, c26, c37) \
+    i16x8 c01 = (i16x8)vec_mergeh((i64x2)c04, (i64x2)c15); \
+    i16x8 c23 = (i16x8)vec_mergeh((i64x2)c26, (i64x2)c37); \
+    i16x8 c45 = (i16x8)vec_mergel((i64x2)c04, (i64x2)c15); \
+    i16x8 c67 = (i16x8)vec_mergel((i64x2)c26, (i64x2)c37); \
+    c01 = vec_mradds(c01, scale, vec_splat_s16(0)); \
+    c23 = vec_mradds(c23, scale, vec_splat_s16(0)); \
+    UNPACK_DECLARE_4_I16_I32(c01, c23, c0, c1, c2, c3) \
+    c45 = vec_mradds(c45, scale, vec_splat_s16(0)); \
+    c67 = vec_mradds(c67, scale, vec_splat_s16(0)); \
+    UNPACK_DECLARE_4_I16_I32(c45, c67, c4, c5, c6, c7)
+
+#define LOAD_SCALE_COEFF_8x4(coeff, scale) \
+    LOAD_DECLARE_2_I16(coeff, c01, c23) \
+    LOAD_DECLARE_2_I16(coeff+16, c45, c67) \
+    c01 = vec_mradds(c01, scale, vec_splat_s16(0)); \
+    c23 = vec_mradds(c23, scale, vec_splat_s16(0)); \
+    UNPACK_DECLARE_4_I16_I32(c01, c23, c0, c1, c2, c3) \
+    c45 = vec_mradds(c45, scale, vec_splat_s16(0)); \
+    c67 = vec_mradds(c67, scale, vec_splat_s16(0)); \
+    UNPACK_DECLARE_4_I16_I32(c45, c67, c4, c5, c6, c7)
+
+#define LOAD_COEFF_8x8(coeff) \
+    LOAD_DECLARE_2_I16(coeff, c0, c1) \
+    LOAD_DECLARE_2_I16(coeff+16, c2, c3) \
+    LOAD_DECLARE_2_I16(coeff+32, c4, c5) \
+    LOAD_DECLARE_2_I16(coeff+48, c6, c7) \
+    UNPACK_DECLARE_4_I16_I32(c0, c1, c0h, c0l, c1h, c1l) \
+    UNPACK_DECLARE_4_I16_I32(c2, c3, c2h, c2l, c3h, c3l) \
+    UNPACK_DECLARE_4_I16_I32(c4, c5, c4h, c4l, c5h, c5l) \
+    UNPACK_DECLARE_4_I16_I32(c6, c7, c6h, c6l, c7h, c7l) \
+
+#define LOAD_COEFF_4x16(coeff) \
+    LOAD_DECLARE_2_I16(coeff,    a0b0, c0d0) \
+    LOAD_DECLARE_2_I16(coeff+16, a1b1, c1d1) \
+    LOAD_DECLARE_2_I16(coeff+32, a2b2, c2d2) \
+    LOAD_DECLARE_2_I16(coeff+48, a3b3, c3d3) \
+    UNPACK_DECLARE_4_I16_I32(a0b0, c0d0, cA0, cB0, cC0, cD0) \
+    UNPACK_DECLARE_4_I16_I32(a1b1, c1d1, cA1, cB1, cC1, cD1) \
+    UNPACK_DECLARE_4_I16_I32(a2b2, c2d2, cA2, cB2, cC2, cD2) \
+    UNPACK_DECLARE_4_I16_I32(a3b3, c3d3, cA3, cB3, cC3, cD3)
+
+#define LOAD_DECLARE_4(src, stride, a, b, c, d) \
+    u8x16 a, b, c, d; \
+    LOAD_4(src, stride, a, b, c, d)
+
+#define STORE_LEN(l, dst, stride, a, b, c, d) \
+{ \
+    uint8_t *dst2 = dst; \
+    vec_xst_len(a, dst2, l); \
+    dst2 += stride; \
+    vec_xst_len(b, dst2, l); \
+    dst2 += stride; \
+    vec_xst_len(c, dst2, l); \
+    dst2 += stride; \
+    vec_xst_len(d, dst2, l); \
+}
+
+#define STORE_4(dst, stride, a, b, c, d) \
+    STORE_LEN(4, dst, stride, a, b, c, d)
+
+#define STORE_8(dst, stride, ab, cd, ef, gh) \
+    STORE_LEN(8, dst, stride, ab, cd, ef, gh)
+
+#define STORE_16(dst, stride, l0, l1, l2, l3) \
+{ \
+    uint8_t *dst##2 = dst; \
+    vec_xst(l0, 0, dst##2); \
+    dst##2 += stride; \
+    vec_xst(l1, 0, dst##2); \
+    dst##2 += stride; \
+    vec_xst(l2, 0, dst##2); \
+    dst##2 += stride; \
+    vec_xst(l3, 0, dst##2); \
+}
+
+#define APPLY_COEFF_4(a, b, c, d, c01, c23) \
+{ \
+    u8x16 ab = (u8x16)vec_mergeh((u32x4)a, (u32x4)b); \
+    u8x16 cd = (u8x16)vec_mergeh((u32x4)c, (u32x4)d); \
+ \
+    c01 = vec_adds(c01, vec_splat_s16(8)); \
+    c23 = vec_adds(c23, vec_splat_s16(8)); \
+    c01 = vec_sra(c01, vec_splat_u16(4)); \
+    c23 = vec_sra(c23, vec_splat_u16(4)); \
+ \
+    i16x8 abs = u8h_to_i16(ab); \
+    i16x8 cds = u8h_to_i16(cd); \
+ \
+    abs = vec_adds(abs, c01); \
+    cds = vec_adds(cds, c23); \
+ \
+    a = vec_packsu(abs, abs); \
+    c = vec_packsu(cds, cds); \
+ \
+    b = (u8x16)vec_mergeo((u32x4)a, (u32x4)a); \
+    d = (u8x16)vec_mergeo((u32x4)c, (u32x4)c); \
+}
+
+#define APPLY_COEFF_8x4(ab, cd, c01, c23) \
+{ \
+    i16x8 abs = u8h_to_i16(ab); \
+    i16x8 cds = u8h_to_i16(cd); \
+    c01 = vec_adds(c01, vec_splat_s16(8)); \
+    c23 = vec_adds(c23, vec_splat_s16(8)); \
+    c01 = vec_sra(c01, vec_splat_u16(4)); \
+    c23 = vec_sra(c23, vec_splat_u16(4)); \
+ \
+    abs = vec_adds(abs, c01); \
+    cds = vec_adds(cds, c23); \
+ \
+    ab = vec_packsu(abs, abs); \
+    cd = vec_packsu(cds, cds); \
+}
+
+#define APPLY_COEFF_16x4(a, b, c, d, \
+                         c00c01, c02c03, c04c05, c06c07, \
+                         c08c09, c10c11, c12c13, c14c15) \
+{ \
+    i16x8 ah = u8h_to_i16(a); \
+    i16x8 al = u8l_to_i16(a); \
+    i16x8 bh = u8h_to_i16(b); \
+    i16x8 bl = u8l_to_i16(b); \
+    i16x8 ch = u8h_to_i16(c); \
+    i16x8 cl = u8l_to_i16(c); \
+    i16x8 dh = u8h_to_i16(d); \
+    i16x8 dl = u8l_to_i16(d); \
+    SCALE_ROUND_4(c00c01, c02c03, c04c05, c06c07, vec_splat_s16(8), vec_splat_u16(4)) \
+    SCALE_ROUND_4(c08c09, c10c11, c12c13, c14c15, vec_splat_s16(8), vec_splat_u16(4)) \
+ \
+    ah = vec_adds(ah, c00c01); \
+    al = vec_adds(al, c02c03); \
+    bh = vec_adds(bh, c04c05); \
+    bl = vec_adds(bl, c06c07); \
+    ch = vec_adds(ch, c08c09); \
+    cl = vec_adds(cl, c10c11); \
+    dh = vec_adds(dh, c12c13); \
+    dl = vec_adds(dl, c14c15); \
+ \
+    a = vec_packsu(ah, al); \
+    b = vec_packsu(bh, bl); \
+    c = vec_packsu(ch, cl); \
+    d = vec_packsu(dh, dl); \
+}
+
+#define IDCT_4_INNER(c0, c1, c2, c3) \
+{ \
+    i32x4 o0 = vec_add(c0, c2); \
+    i32x4 o1 = vec_sub(c0, c2); \
+ \
+    i32x4 v2896 = vec_splats(2896); \
+    i32x4 v1567 = vec_splats(1567); \
+    i32x4 v3784 = vec_splats(3784); \
+    i32x4 v2048 = vec_splats(2048); \
+ \
+    o0 = vec_mul(o0, v2896); \
+    o1 = vec_mul(o1, v2896); \
+ \
+    i32x4 o2a = vec_mul(c1, v1567); \
+    i32x4 o2b = vec_mul(c3, v3784); \
+    i32x4 o3a = vec_mul(c1, v3784); \
+    i32x4 o3b = vec_mul(c3, v1567); \
+ \
+    i32x4 o2 = vec_sub(o2a, o2b); \
+    i32x4 o3 = vec_add(o3a, o3b); \
+ \
+    u32x4 v12 = vec_splat_u32(12); \
+ \
+    o0 = vec_add(o0, v2048); \
+    o1 = vec_add(o1, v2048); \
+    o2 = vec_add(o2, v2048); \
+    o3 = vec_add(o3, v2048); \
+ \
+    o0 = vec_sra(o0, v12); \
+    o1 = vec_sra(o1, v12); \
+    o2 = vec_sra(o2, v12); \
+    o3 = vec_sra(o3, v12); \
+ \
+    c0 = vec_add(o0, o3); \
+    c1 = vec_add(o1, o2); \
+    c2 = vec_sub(o1, o2); \
+    c3 = vec_sub(o0, o3); \
+ \
+}
+
+#define dct4_for_dct8(c0, c1, c2, c3, c03, c12) \
+    IDCT_4_INNER(c0, c1, c2, c3) \
+    c03 = vec_packs(c0, c3); \
+    c12 = vec_packs(c1, c2); \
+
+#define dct_4_in(c0, c1, c2, c3, c01, c23) \
+{ \
+    IDCT_4_INNER(c0, c1, c2, c3) \
+    c01 = vec_packs(c0, c1); \
+    c23 = vec_packs(c2, c3); \
+    c0 = i16h_to_i32(c01); \
+    c1 = i16l_to_i32(c01); \
+    c2 = i16h_to_i32(c23); \
+    c3 = i16l_to_i32(c23); \
+}
+
+#define dct_4_out(c0, c1, c2, c3, c01, c23) \
+    IDCT_4_INNER(c0, c1, c2, c3) \
+    c01 = vec_packs(c0, c1); \
+    c23 = vec_packs(c2, c3); \
+
+
+#define IDENTITY_4(c01, c23) \
+{ \
+    i16x8 v1697 = vec_splats((int16_t)(1697*8)); \
+    i16x8 o01 = vec_mradds(c01, v1697, vec_splat_s16(0)); \
+    i16x8 o23 = vec_mradds(c23, v1697, vec_splat_s16(0)); \
+    c01 = vec_adds(c01, o01); \
+    c23 = vec_adds(c23, o23); \
+}
+
+#define identity_4_in(c0, c1, c2, c3, c01, c23) \
+{ \
+    IDENTITY_4(c01, c23) \
+    c0 = i16h_to_i32(c01); \
+    c1 = i16l_to_i32(c01); \
+    c2 = i16h_to_i32(c23); \
+    c3 = i16l_to_i32(c23); \
+}
+
+#define identity_4_out(c0, c1, c2, c3, c01, c23) \
+{ \
+    c01 = vec_packs(c0, c1); \
+    c23 = vec_packs(c2, c3); \
+    IDENTITY_4(c01, c23) \
+}
+
+#define ADST_INNER_4(c0, c1, c2, c3, oc0, oc1, oc2, oc3) \
+{ \
+    i32x4 v1321 = vec_splats(1321); \
+    i32x4 v3803 = vec_splats(3803); \
+    i32x4 v2482 = vec_splats(2482); \
+    i32x4 v3344 = vec_splats(3344); \
+    i32x4 v2048 = vec_splats(2048); \
+    i32x4 i0_v1321 = vec_mul(c0, v1321); \
+    i32x4 i0_v2482 = vec_mul(c0, v2482); \
+    i32x4 i0_v3803 = vec_mul(c0, v3803); \
+    i32x4 i1 = vec_mul(c1, v3344); \
+    i32x4 i2_v1321 = vec_mul(c2, v1321); \
+    i32x4 i2_v2482 = vec_mul(c2, v2482); \
+    i32x4 i2_v3803 = vec_mul(c2, v3803); \
+    i32x4 i3_v1321 = vec_mul(c3, v1321); \
+    i32x4 i3_v2482 = vec_mul(c3, v2482); \
+    i32x4 i3_v3803 = vec_mul(c3, v3803); \
+ \
+    i32x4 n1 = vec_sub(i1, v2048); \
+    i1 = vec_add(i1, v2048); \
+ \
+ \
+    i32x4 o0 = vec_add(i0_v1321, i2_v3803); \
+    i32x4 o1 = vec_sub(i0_v2482, i2_v1321); \
+    i32x4 o2 = vec_sub(c0, c2); \
+    i32x4 o3 = vec_add(i0_v3803, i2_v2482); \
+ \
+    o0 = vec_add(o0, i3_v2482); \
+    o1 = vec_sub(o1, i3_v3803); \
+    o2 = vec_add(o2, c3); \
+    o3 = vec_sub(o3, i3_v1321); \
+ \
+    o0 = vec_add(o0, i1); \
+    o1 = vec_add(o1, i1); \
+    o2 = vec_mul(o2, v3344); \
+    o3 = vec_sub(o3, n1); \
+ \
+    o2 = vec_add(o2, v2048); \
+ \
+    oc0 = vec_sra(o0, vec_splat_u32(12)); \
+    oc1 = vec_sra(o1, vec_splat_u32(12)); \
+    oc2 = vec_sra(o2, vec_splat_u32(12)); \
+    oc3 = vec_sra(o3, vec_splat_u32(12)); \
+}
+
+#define adst_4_in(c0, c1, c2, c3, c01, c23) \
+{ \
+    ADST_INNER_4(c0, c1, c2, c3, c0, c1, c2, c3) \
+}
+
+#define flipadst_4_in(c0, c1, c2, c3, c01, c23) \
+{ \
+    ADST_INNER_4(c0, c1, c2, c3, c3, c2, c1, c0) \
+}
+
+#define adst_4_out(c0, c1, c2, c3, c01, c23) \
+{ \
+    ADST_INNER_4(c0, c1, c2, c3, c0, c1, c2, c3) \
+    c01 = vec_packs(c0, c1); \
+    c23 = vec_packs(c2, c3); \
+}
+
+#define flipadst_4_out(c0, c1, c2, c3, c01, c23) \
+{ \
+    ADST_INNER_4(c0, c1, c2, c3, c3, c2, c1, c0) \
+    c01 = vec_packs(c0, c1); \
+    c23 = vec_packs(c2, c3); \
+}
+
+static void dc_only_4xN(uint8_t *dst, const ptrdiff_t stride, int16_t *const coeff, int n, int is_rect2, int shift)
+{
+    int dc = coeff[0];
+    const int rnd = (1 << shift) >> 1;
+    if (is_rect2)
+        dc = (dc * 181 + 128) >> 8;
+    dc = (dc * 181 + 128) >> 8;
+    dc = (dc + rnd) >> shift;
+    dc = (dc * 181 + 128 + 2048) >> 12;
+
+    i16x8 vdc = vec_splats((int16_t)dc);
+    coeff[0] = 0;
+    for (int i = 0; i < n; i++, dst += 4 * stride) {
+        LOAD_DECLARE_4(dst, stride, a, b, c, d)
+
+        i16x8 as = u8h_to_i16(a);
+        i16x8 bs = u8h_to_i16(b);
+        i16x8 cs = u8h_to_i16(c);
+        i16x8 ds = u8h_to_i16(d);
+
+        as = vec_adds(as, vdc);
+        bs = vec_adds(bs, vdc);
+        cs = vec_adds(cs, vdc);
+        ds = vec_adds(ds, vdc);
+
+        a = vec_packsu(as, as);
+        b = vec_packsu(bs, bs);
+        c = vec_packsu(cs, cs);
+        d = vec_packsu(ds, ds);
+
+        STORE_4(dst, stride, a, b, c, d)
+    }
+}
+
+static void dc_only_8xN(uint8_t *dst, const ptrdiff_t stride, int16_t *const coeff, int n, int is_rect2, int shift)
+{
+    int dc = coeff[0];
+    const int rnd = (1 << shift) >> 1;
+    if (is_rect2)
+        dc = (dc * 181 + 128) >> 8;
+    dc = (dc * 181 + 128) >> 8;
+    dc = (dc + rnd) >> shift;
+    dc = (dc * 181 + 128 + 2048) >> 12;
+
+    i16x8 vdc = vec_splats((int16_t)dc);
+    coeff[0] = 0;
+
+    for (int i = 0; i < n; i++, dst += 4 * stride) {
+        LOAD_DECLARE_4(dst, stride, a, b, c, d)
+
+        i16x8 as = u8h_to_i16(a);
+        i16x8 bs = u8h_to_i16(b);
+        i16x8 cs = u8h_to_i16(c);
+        i16x8 ds = u8h_to_i16(d);
+
+        as = vec_adds(as, vdc);
+        bs = vec_adds(bs, vdc);
+        cs = vec_adds(cs, vdc);
+        ds = vec_adds(ds, vdc);
+
+        a = vec_packsu(as, as);
+        b = vec_packsu(bs, bs);
+        c = vec_packsu(cs, cs);
+        d = vec_packsu(ds, ds);
+
+        STORE_8(dst, stride, a, b, c, d)
+    }
+}
+
+static void dc_only_16xN(uint8_t *dst, const ptrdiff_t stride, int16_t *const coeff, int n, int is_rect2, int shift)
+{
+    int dc = coeff[0];
+    const int rnd = (1 << shift) >> 1;
+    if (is_rect2)
+        dc = (dc * 181 + 128) >> 8;
+    dc = (dc * 181 + 128) >> 8;
+    dc = (dc + rnd) >> shift;
+    dc = (dc * 181 + 128 + 2048) >> 12;
+
+    i16x8 vdc = vec_splats((int16_t)dc);
+    coeff[0] = 0;
+
+    for (int i = 0; i < n; i++, dst += 4 * stride) {
+        LOAD_DECLARE_4(dst, stride, a, b, c, d)
+
+        i16x8 ah = u8h_to_i16(a);
+        i16x8 bh = u8h_to_i16(b);
+        i16x8 ch = u8h_to_i16(c);
+        i16x8 dh = u8h_to_i16(d);
+        i16x8 al = u8l_to_i16(a);
+        i16x8 bl = u8l_to_i16(b);
+        i16x8 cl = u8l_to_i16(c);
+        i16x8 dl = u8l_to_i16(d);
+
+        ah = vec_adds(ah, vdc);
+        bh = vec_adds(bh, vdc);
+        ch = vec_adds(ch, vdc);
+        dh = vec_adds(dh, vdc);
+        al = vec_adds(al, vdc);
+        bl = vec_adds(bl, vdc);
+        cl = vec_adds(cl, vdc);
+        dl = vec_adds(dl, vdc);
+
+        a = vec_packsu(ah, al);
+        b = vec_packsu(bh, bl);
+        c = vec_packsu(ch, cl);
+        d = vec_packsu(dh, dl);
+
+        STORE_16(dst, stride, a, b, c, d)
+    }
+}
+
+void dav1d_inv_txfm_add_dct_dct_4x4_8bpc_pwr9(uint8_t *dst, const ptrdiff_t stride,
+                                              int16_t *const coeff, const int eob)
+{
+    assert(eob >= 0);
+
+    if (eob < 1) {
+        return dc_only_4xN(dst, stride, coeff, 1, 0, 0);
+    }
+
+    LOAD_COEFF_4(coeff)
+
+    dct_4_in(c0, c1, c2, c3, c01, c23)
+
+    TRANSPOSE4_I32(c0, c1, c2, c3)
+
+    memset(coeff, 0, sizeof(*coeff) * 4 * 4);
+
+    dct_4_out(c0, c1, c2, c3, c01, c23)
+
+    LOAD_DECLARE_4(dst, stride, a, b, c, d)
+
+    APPLY_COEFF_4(a, b, c, d, c01, c23)
+
+    STORE_4(dst, stride, a, b, c, d)
+}
+
+void dav1d_inv_txfm_add_wht_wht_4x4_8bpc_pwr9(pixel *dst, const ptrdiff_t stride,
+                                              coef *const coeff, const int eob)
+{
+    LOAD_COEFF_4(coeff)
+
+    u32x4 v2 = vec_splat_u32(2);
+
+    c0 = vec_sra(c0, v2);
+    c1 = vec_sra(c1, v2);
+    c2 = vec_sra(c2, v2);
+    c3 = vec_sra(c3, v2);
+
+    i32x4 t0 = vec_add(c0, c1);
+    i32x4 t2 = vec_sub(c2, c3);
+    i32x4 t4 = vec_sra(vec_sub(t0, t2), vec_splat_u32(1));
+    i32x4 t3 = vec_sub(t4, c3);
+    i32x4 t1 = vec_sub(t4, c1);
+    c0 = vec_sub(t0, t3);
+    c1 = t3;
+    c2 = t1;
+    c3 = vec_add(t2, t1);
+
+    memset(coeff, 0, sizeof(*coeff) * 4 * 4);
+
+    TRANSPOSE4_I32(c0, c1, c2, c3)
+
+    t0 = vec_add(c0, c1);
+    t2 = vec_sub(c2, c3);
+    t4 = vec_sra(vec_sub(t0, t2), vec_splat_u32(1));
+    t3 = vec_sub(t4, c3);
+    t1 = vec_sub(t4, c1);
+    c0 = vec_sub(t0, t3);
+    c1 = t3;
+    c2 = t1;
+    c3 = vec_add(t2, t1);
+
+    c01 = vec_packs(c0, c1);
+    c23 = vec_packs(c2, c3);
+
+    LOAD_DECLARE_4(dst, stride, a, b, c, d)
+
+    u8x16 ab = (u8x16)vec_mergeh((u32x4)a, (u32x4)b);
+    u8x16 cd = (u8x16)vec_mergeh((u32x4)c, (u32x4)d);
+
+    i16x8 abs = u8h_to_i16(ab);
+    i16x8 cds = u8h_to_i16(cd);
+
+    abs = vec_adds(abs, c01);
+    cds = vec_adds(cds, c23);
+
+    a = vec_packsu(abs, abs);
+    c = vec_packsu(cds, cds);
+
+    b = (u8x16)vec_mergeo((u32x4)a, (u32x4)a);
+    d = (u8x16)vec_mergeo((u32x4)c, (u32x4)c);
+
+    STORE_4(dst, stride, a, b, c, d)
+}
+
+#define inv_txfm_fn4x4(type1, type2) \
+void dav1d_inv_txfm_add_##type1##_##type2##_4x4_8bpc_pwr9(uint8_t *dst, const ptrdiff_t stride, \
+                                                          int16_t *const coeff, const int eob) \
+{ \
+    LOAD_COEFF_4(coeff) \
+    type1##_4_in(c0, c1, c2, c3, c01, c23) \
+    memset(coeff, 0, sizeof(*coeff) * 4 * 4); \
+    TRANSPOSE4_I32(c0, c1, c2, c3) \
+    type2##_4_out(c0, c1, c2, c3, c01, c23) \
+    LOAD_DECLARE_4(dst, stride, a, b, c, d) \
+    APPLY_COEFF_4(a, b, c, d, c01, c23) \
+    STORE_4(dst, stride, a, b, c, d) \
+}
+
+inv_txfm_fn4x4(adst,     dct     )
+inv_txfm_fn4x4(dct,      adst    )
+inv_txfm_fn4x4(dct,      flipadst)
+inv_txfm_fn4x4(flipadst, dct     )
+inv_txfm_fn4x4(adst,     flipadst)
+inv_txfm_fn4x4(flipadst, adst    )
+inv_txfm_fn4x4(identity, dct     )
+inv_txfm_fn4x4(dct,      identity)
+inv_txfm_fn4x4(identity, flipadst)
+inv_txfm_fn4x4(flipadst, identity)
+inv_txfm_fn4x4(identity, adst   )
+inv_txfm_fn4x4(adst,     identity)
+inv_txfm_fn4x4(identity, identity)
+inv_txfm_fn4x4(adst,     adst    )
+inv_txfm_fn4x4(flipadst, flipadst)
+
+
+#define IDCT_8_INNER(c0, c1, c2, c3, c4, c5, c6, c7, c03, c12, c74, c65) \
+    dct4_for_dct8(c0, c2, c4, c6, c03, c12) \
+ \
+    i32x4 v799 = vec_splats(799); \
+    i32x4 v4017 = vec_splats(4017); \
+    i32x4 v3406 = vec_splats(3406); \
+    i32x4 v2276 = vec_splats(2276); \
+    i32x4 v2048 = vec_splats(2048); \
+    u32x4 v12 = vec_splat_u32(12); \
+ \
+    i32x4 c1v799 = vec_mul(c1, v799); \
+    i32x4 c7v4017 = vec_mul(c7, v4017); \
+    i32x4 c5v3406 = vec_mul(c5, v3406); \
+    i32x4 c3v2276 = vec_mul(c3, v2276); \
+    i32x4 c5v2276 = vec_mul(c5, v2276); \
+    i32x4 c3v3406 = vec_mul(c3, v3406); \
+    i32x4 c1v4017 = vec_mul(c1, v4017); \
+    i32x4 c7v799 = vec_mul(c7, v799); \
+ \
+    i32x4 t4a = vec_subs(c1v799, c7v4017); \
+    i32x4 t5a = vec_subs(c5v3406, c3v2276); \
+    i32x4 t6a = vec_adds(c5v2276, c3v3406); \
+    i32x4 t7a = vec_adds(c1v4017, c7v799); \
+ \
+    t4a = vec_adds(t4a, v2048); \
+    t5a = vec_adds(t5a, v2048); \
+    t6a = vec_adds(t6a, v2048); \
+    t7a = vec_adds(t7a, v2048); \
+ \
+    t4a = vec_sra(t4a, v12); \
+    t7a = vec_sra(t7a, v12); \
+    t5a = vec_sra(t5a, v12); \
+    t6a = vec_sra(t6a, v12); \
+ \
+    i16x8 t7at4a = vec_packs(t7a, t4a); \
+    i16x8 t6at5a = vec_packs(t6a, t5a); \
+ \
+    i16x8 t7t4 = vec_adds(t7at4a, t6at5a); \
+    t6at5a = vec_subs(t7at4a, t6at5a); \
+ \
+    t6a = i16h_to_i32(t6at5a); \
+    t5a = i16l_to_i32(t6at5a); \
+ \
+    i32x4 t6 = vec_add(t6a, t5a); \
+    i32x4 t5 = vec_sub(t6a, t5a); \
+ \
+    t6 = vec_mul(t6, vec_splats(181)); \
+    t5 = vec_mul(t5, vec_splats(181)); \
+    t6 = vec_add(t6, vec_splats(128)); \
+    t5 = vec_add(t5, vec_splats(128)); \
+ \
+    t6 = vec_sra(t6, vec_splat_u32(8)); \
+    t5 = vec_sra(t5, vec_splat_u32(8)); \
+ \
+    i16x8 t6t5 = vec_packs(t6, t5); \
+ \
+    c74 = vec_subs(c03, t7t4); \
+    c65 = vec_subs(c12, t6t5); \
+    c03 = vec_adds(c03, t7t4); \
+    c12 = vec_adds(c12, t6t5); \
+
+#define UNPACK_4_I16_I32(t0, t1, t2, t3) \
+    t0 = i16h_to_i32(t0##t1); \
+    t1 = i16l_to_i32(t0##t1); \
+    t2 = i16h_to_i32(t2##t3); \
+    t3 = i16l_to_i32(t2##t3);
+
+#define UNPACK_PAIR_I16_I32(hi, lo, v) \
+    hi = i16h_to_i32(v); \
+    lo = i16l_to_i32(v); \
+
+
+#define dct_8_in(c0, c1, c2, c3, c4, c5, c6, c7, ...) \
+{ \
+    i16x8 c0##c3, c1##c2, c7##c4, c6##c5; \
+    IDCT_8_INNER(c0, c1, c2, c3, c4, c5, c6, c7, c0##c3, c1##c2, c7##c4, c6##c5) \
+    UNPACK_4_I16_I32(c0, c3, c1, c2) \
+    UNPACK_4_I16_I32(c7, c4, c6, c5) \
+}
+
+#define dct_8_out(c0, c1, c2, c3, c4, c5, c6, c7, c01, c23, c45, c67) \
+{ \
+    i16x8 c03, c12, c74, c65; \
+    IDCT_8_INNER(c0, c1, c2, c3, c4, c5, c6, c7, c03, c12, c74, c65) \
+    c01 = (i16x8)vec_mergeh((u64x2)c03, (u64x2)c12); \
+    c23 = (i16x8)vec_mergel((u64x2)c12, (u64x2)c03); \
+    c45 = (i16x8)vec_mergel((u64x2)c74, (u64x2)c65); \
+    c67 = (i16x8)vec_mergeh((u64x2)c65, (u64x2)c74); \
+}
+
+#define dct_8x2_in(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                   c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                   c0, c1, c2, c3, c4, c5, c6, c7) \
+{ \
+    dct_8_in(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h,) \
+    dct_8_in(c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l,) \
+}
+
+#define dct_8x2_out(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                    c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                    c0, c1, c2, c3, c4, c5, c6, c7) \
+{ \
+    i16x8 c03h, c12h, c74h, c65h; \
+    i16x8 c03l, c12l, c74l, c65l; \
+    { \
+        IDCT_8_INNER(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, c03h, c12h, c74h, c65h) \
+    } \
+    { \
+        IDCT_8_INNER(c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, c03l, c12l, c74l, c65l) \
+    } \
+    c0 = (i16x8)vec_mergeh((u64x2)c03h, (u64x2)c03l); \
+    c3 = (i16x8)vec_mergel((u64x2)c03h, (u64x2)c03l); \
+    c1 = (i16x8)vec_mergeh((u64x2)c12h, (u64x2)c12l); \
+    c2 = (i16x8)vec_mergel((u64x2)c12h, (u64x2)c12l); \
+    c7 = (i16x8)vec_mergeh((u64x2)c74h, (u64x2)c74l); \
+    c4 = (i16x8)vec_mergel((u64x2)c74h, (u64x2)c74l); \
+    c6 = (i16x8)vec_mergeh((u64x2)c65h, (u64x2)c65l); \
+    c5 = (i16x8)vec_mergel((u64x2)c65h, (u64x2)c65l); \
+}
+
+#define IDENTITY_8(c01, c23, c45, c67) \
+{ \
+    c01 = vec_adds(c01, c01); \
+    c23 = vec_adds(c23, c23); \
+    c45 = vec_adds(c45, c45); \
+    c67 = vec_adds(c67, c67); \
+}
+
+#define identity_8_in(c0, c1, c2, c3, c4, c5, c6, c7, c01, c23, c45, c67) \
+{ \
+    IDENTITY_8(c01, c23, c45, c67) \
+    UNPACK_PAIR_I16_I32(c0, c1, c01) \
+    UNPACK_PAIR_I16_I32(c2, c3, c23) \
+    UNPACK_PAIR_I16_I32(c4, c5, c45) \
+    UNPACK_PAIR_I16_I32(c6, c7, c67) \
+}
+
+#define identity_8_out(c0, c1, c2, c3, c4, c5, c6, c7, c01, c23, c45, c67) \
+    c01 = vec_packs(c0, c1); \
+    c23 = vec_packs(c2, c3); \
+    c45 = vec_packs(c4, c5); \
+    c67 = vec_packs(c6, c7); \
+    IDENTITY_8(c01, c23, c45, c67)
+
+#define identity_8x2_in(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                        c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                        c0, c1, c2, c3, c4, c5, c6, c7) \
+{ \
+    IDENTITY_8(c0, c1, c2, c3) \
+    IDENTITY_8(c4, c5, c6, c7) \
+    UNPACK_PAIR_I16_I32(c0h, c0l, c0) \
+    UNPACK_PAIR_I16_I32(c1h, c1l, c1) \
+    UNPACK_PAIR_I16_I32(c2h, c2l, c2) \
+    UNPACK_PAIR_I16_I32(c3h, c3l, c3) \
+    UNPACK_PAIR_I16_I32(c4h, c4l, c4) \
+    UNPACK_PAIR_I16_I32(c5h, c5l, c5) \
+    UNPACK_PAIR_I16_I32(c6h, c6l, c6) \
+    UNPACK_PAIR_I16_I32(c7h, c7l, c7) \
+}
+
+#define PACK_4(c0, c1, c2, c3, \
+               c0h, c1h, c2h, c3h, \
+               c0l, c1l, c2l, c3l) \
+{ \
+    c0 = vec_packs(c0h, c0l); \
+    c1 = vec_packs(c1h, c1l); \
+    c2 = vec_packs(c2h, c2l); \
+    c3 = vec_packs(c3h, c3l); \
+}
+
+#define DECLARE_PACK_4(c0, c1, c2, c3, \
+                       c0h, c1h, c2h, c3h, \
+                       c0l, c1l, c2l, c3l) \
+    i16x8 c0, c1, c2, c3; \
+    PACK_4(c0, c1, c2, c3, c0h, c1h, c2h, c3h, c0l, c1l, c2l, c3l);
+
+#define PACK_8(c0, c1, c2, c3, c4, c5, c6, c7, \
+               c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+               c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l) \
+{ \
+    c0 = vec_packs(c0h, c0l); \
+    c1 = vec_packs(c1h, c1l); \
+    c2 = vec_packs(c2h, c2l); \
+    c3 = vec_packs(c3h, c3l); \
+    c4 = vec_packs(c4h, c4l); \
+    c5 = vec_packs(c5h, c5l); \
+    c6 = vec_packs(c6h, c6l); \
+    c7 = vec_packs(c7h, c7l); \
+}
+
+#define identity_8x2_out(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                         c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                         c0, c1, c2, c3, c4, c5, c6, c7) \
+{ \
+    PACK_8(c0, c1, c2, c3, c4, c5, c6, c7, \
+           c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+           c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l) \
+    IDENTITY_8(c0, c1, c2, c3) \
+    IDENTITY_8(c4, c5, c6, c7) \
+}
+
+#define DECLARE_SPLAT_I32(val) \
+    i32x4 v##val = vec_splats(val);
+
+#define DECLARE_MUL_PAIR_I32(ca, cb, va, vb) \
+    i32x4 ca##va = vec_mul(ca, va); \
+    i32x4 cb##vb = vec_mul(cb, vb); \
+    i32x4 ca##vb = vec_mul(ca, vb); \
+    i32x4 cb##va = vec_mul(cb, va);
+
+#define ADD_SUB_PAIR(r0, r1, ca, cb, va, vb) \
+    r0 = vec_adds(ca##va, cb##vb); \
+    r1 = vec_subs(ca##vb, cb##va);
+
+#define DECLARE_ADD_SUB_PAIR(r0, r1, ca, cb, va, vb) \
+    i32x4 r0, r1; \
+    ADD_SUB_PAIR(r0, r1, ca, cb, va, vb)
+
+#define SCALE_ROUND_4(a, b, c, d, rnd, shift) \
+    a = vec_adds(a, rnd); \
+    b = vec_adds(b, rnd); \
+    c = vec_adds(c, rnd); \
+    d = vec_adds(d, rnd); \
+    a = vec_sra(a, shift); \
+    b = vec_sra(b, shift); \
+    c = vec_sra(c, shift); \
+    d = vec_sra(d, shift);
+
+#define ADST_INNER_8(c0, c1, c2, c3, c4, c5, c6, c7, \
+                     o0, o1, o2, o3, o4, o5, o6, o7) \
+{ \
+    DECLARE_SPLAT_I32(4076) \
+    DECLARE_SPLAT_I32(401) \
+ \
+    DECLARE_SPLAT_I32(3612) \
+    DECLARE_SPLAT_I32(1931) \
+ \
+    DECLARE_SPLAT_I32(2598) \
+    DECLARE_SPLAT_I32(3166) \
+ \
+    DECLARE_SPLAT_I32(1189) \
+    DECLARE_SPLAT_I32(3920) \
+ \
+    DECLARE_SPLAT_I32(3784) \
+    DECLARE_SPLAT_I32(1567) \
+ \
+    DECLARE_SPLAT_I32(2048) \
+    u32x4 v12 = vec_splat_u32(12); \
+ \
+    DECLARE_MUL_PAIR_I32(c7, c0, v4076, v401) \
+    DECLARE_MUL_PAIR_I32(c5, c2, v3612, v1931) \
+    DECLARE_MUL_PAIR_I32(c3, c4, v2598, v3166) \
+    DECLARE_MUL_PAIR_I32(c1, c6, v1189, v3920) \
+ \
+    DECLARE_ADD_SUB_PAIR(t0a, t1a, c7, c0, v4076, v401) \
+    DECLARE_ADD_SUB_PAIR(t2a, t3a, c5, c2, v3612, v1931) \
+    DECLARE_ADD_SUB_PAIR(t4a, t5a, c3, c4, v2598, v3166) \
+    DECLARE_ADD_SUB_PAIR(t6a, t7a, c1, c6, v1189, v3920) \
+ \
+    SCALE_ROUND_4(t0a, t1a, t2a, t3a, v2048, v12) \
+    SCALE_ROUND_4(t4a, t5a, t6a, t7a, v2048, v12) \
+ \
+    i32x4 t0 = vec_add(t0a, t4a); \
+    i32x4 t1 = vec_add(t1a, t5a); \
+    i32x4 t2 = vec_add(t2a, t6a); \
+    i32x4 t3 = vec_add(t3a, t7a); \
+    i32x4 t4 = vec_sub(t0a, t4a); \
+    i32x4 t5 = vec_sub(t1a, t5a); \
+    i32x4 t6 = vec_sub(t2a, t6a); \
+    i32x4 t7 = vec_sub(t3a, t7a); \
+ \
+    i16x8 t0t1 = vec_packs(t0, t1); \
+    i16x8 t2t3 = vec_packs(t2, t3); \
+    i16x8 t4t5 = vec_packs(t4, t5); \
+    i16x8 t6t7 = vec_packs(t6, t7); \
+ \
+    UNPACK_4_I16_I32(t4, t5, t6, t7) \
+    UNPACK_4_I16_I32(t0, t1, t2, t3) \
+ \
+    DECLARE_MUL_PAIR_I32(t4, t5, v3784, v1567) \
+    DECLARE_MUL_PAIR_I32(t7, t6, v3784, v1567) \
+ \
+    ADD_SUB_PAIR(t4a, t5a, t4, t5, v3784, v1567) \
+    ADD_SUB_PAIR(t7a, t6a, t7, t6, v1567, v3784) \
+ \
+    SCALE_ROUND_4(t4a, t5a, t6a, t7a, v2048, v12) \
+  \
+    o0 = vec_add(t0, t2); \
+    o1 = vec_add(t4a, t6a); \
+    o7 = vec_add(t1, t3); \
+    o6 = vec_add(t5a, t7a); \
+    t2 = vec_sub(t0, t2); \
+    t3 = vec_sub(t1, t3); \
+    t6 = vec_sub(t4a, t6a); \
+    t7 = vec_sub(t5a, t7a); \
+ \
+    i16x8 o7##o1 = vec_packs(o7, o1); \
+    i16x8 o0##o6 = vec_packs(o0, o6); \
+    t2t3 = vec_packs(t2, t3); \
+    t6t7 = vec_packs(t6, t7); \
+ \
+    UNPACK_4_I16_I32(t2, t3, t6, t7) \
+    UNPACK_4_I16_I32(o7, o1, o0, o6) \
+ \
+    o7 = -o7; \
+    o1 = -o1; \
+ \
+    o3 = vec_add(t2, t3); \
+    o4 = vec_sub(t2, t3); \
+    o5 = vec_sub(t6, t7); \
+    o2 = vec_add(t6, t7); \
+ \
+    i32x4 v181 = vec_splats(181); \
+    i32x4 v128 = vec_splats(128); \
+    u32x4 v8 = vec_splat_u32(8); \
+ \
+    o2 = vec_mul(o2, v181); \
+    o3 = vec_mul(o3, v181); \
+    o4 = vec_mul(o4, v181); \
+    o5 = vec_mul(o5, v181); \
+ \
+    SCALE_ROUND_4(o2, o3, o4, o5, v128, v8) \
+ \
+    o3 = -o3; \
+    o5 = -o5; \
+}
+
+#define adst_8_in(c0, c1, c2, c3, c4, c5, c6, c7, c01, c23, c45, c67) \
+{\
+    ADST_INNER_8(c0, c1, c2, c3, c4, c5, c6, c7, \
+                 c0, c1, c2, c3, c4, c5, c6, c7) \
+    c01 = vec_packs(c0, c1); \
+    c23 = vec_packs(c2, c3); \
+    c45 = vec_packs(c4, c5); \
+    c67 = vec_packs(c6, c7); \
+    UNPACK_PAIR_I16_I32(c0, c1, c01) \
+    UNPACK_PAIR_I16_I32(c2, c3, c23) \
+    UNPACK_PAIR_I16_I32(c4, c5, c45) \
+    UNPACK_PAIR_I16_I32(c6, c7, c67) \
+}
+
+#define adst_8_out(c0, c1, c2, c3, c4, c5, c6, c7, c01, c23, c45, c67) \
+{\
+    ADST_INNER_8(c0, c1, c2, c3, c4, c5, c6, c7, \
+                 c0, c1, c2, c3, c4, c5, c6, c7) \
+    c01 = vec_packs(c0, c1); \
+    c23 = vec_packs(c2, c3); \
+    c45 = vec_packs(c4, c5); \
+    c67 = vec_packs(c6, c7); \
+}
+
+#define adst_8x2_in(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                    c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                    c0, c1, c2, c3, c4, c5, c6, c7) \
+{ \
+    ADST_INNER_8(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                 c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h) \
+    ADST_INNER_8(c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                 c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l) \
+}
+
+#define adst_8x2_out(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                    c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                    c0, c1, c2, c3, c4, c5, c6, c7) \
+{ \
+    ADST_INNER_8(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                 c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h) \
+    ADST_INNER_8(c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                 c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l) \
+    PACK_8(c0, c1, c2, c3, c4, c5, c6, c7, \
+           c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+           c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l) \
+}
+
+#define flipadst_8_in(c0, c1, c2, c3, c4, c5, c6, c7, c01, c23, c45, c67) \
+{\
+    ADST_INNER_8(c0, c1, c2, c3, c4, c5, c6, c7, \
+                 c7, c6, c5, c4, c3, c2, c1, c0) \
+    c01 = vec_packs(c0, c1); \
+    c23 = vec_packs(c2, c3); \
+    c45 = vec_packs(c4, c5); \
+    c67 = vec_packs(c6, c7); \
+    UNPACK_PAIR_I16_I32(c0, c1, c01) \
+    UNPACK_PAIR_I16_I32(c2, c3, c23) \
+    UNPACK_PAIR_I16_I32(c4, c5, c45) \
+    UNPACK_PAIR_I16_I32(c6, c7, c67) \
+}
+
+#define flipadst_8_out(c0, c1, c2, c3, c4, c5, c6, c7, c01, c23, c45, c67) \
+{\
+    ADST_INNER_8(c0, c1, c2, c3, c4, c5, c6, c7, \
+                 c7, c6, c5, c4, c3, c2, c1, c0) \
+    c01 = vec_packs(c0, c1); \
+    c23 = vec_packs(c2, c3); \
+    c45 = vec_packs(c4, c5); \
+    c67 = vec_packs(c6, c7); \
+}
+
+#define flipadst_8x2_in(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                        c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                        c0, c1, c2, c3, c4, c5, c6, c7) \
+{ \
+    ADST_INNER_8(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                 c7h, c6h, c5h, c4h, c3h, c2h, c1h, c0h) \
+    ADST_INNER_8(c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                 c7l, c6l, c5l, c4l, c3l, c2l, c1l, c0l) \
+}
+
+#define flipadst_8x2_out(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                         c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                         c0, c1, c2, c3, c4, c5, c6, c7) \
+{ \
+    ADST_INNER_8(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                 c7h, c6h, c5h, c4h, c3h, c2h, c1h, c0h) \
+    ADST_INNER_8(c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                 c7l, c6l, c5l, c4l, c3l, c2l, c1l, c0l) \
+    PACK_8(c0, c1, c2, c3, c4, c5, c6, c7, \
+           c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+           c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l) \
+}
+
+void dav1d_inv_txfm_add_dct_dct_4x8_8bpc_pwr9(uint8_t *dst, const ptrdiff_t stride,
+                                              int16_t *const coeff, const int eob)
+{
+    i16x8 v = vec_splats((int16_t)(2896*8));
+
+    if (eob < 1) {
+        return dc_only_4xN(dst, stride, coeff, 2, 1, 0);
+    }
+
+    LOAD_SCALE_COEFF_4x8(coeff, v)
+
+    dct_4_in(c0, c1, c2, c3, c01, c23)
+    dct_4_in(c4, c5, c6, c7, c45, c67)
+
+
+    memset(coeff, 0, sizeof(*coeff) * 4 * 8);
+
+    TRANSPOSE4_I32(c0, c1, c2, c3);
+    TRANSPOSE4_I32(c4, c5, c6, c7);
+
+    dct_8_out(c0, c1, c2, c3, c4, c5, c6, c7, c01, c23, c45, c67)
+
+    LOAD_DECLARE_4(dst, stride, a, b, cc, d)
+    LOAD_DECLARE_4(dst + 4 * stride, stride, e, f, g, hh)
+
+    APPLY_COEFF_4(a, b, cc, d, c01, c23)
+    APPLY_COEFF_4(e, f, g, hh, c45, c67)
+
+    STORE_4(dst, stride, a, b, cc, d)
+    STORE_4(dst + 4 * stride, stride, e, f, g, hh)
+}
+
+
+#define inv_txfm_fn4x8(type1, type2) \
+void dav1d_inv_txfm_add_##type1##_##type2##_4x8_8bpc_pwr9(uint8_t *dst, const ptrdiff_t stride, \
+                                                          int16_t *const coeff, const int eob) \
+{ \
+    i16x8 v = vec_splats((int16_t)(2896*8)); \
+    LOAD_SCALE_COEFF_4x8(coeff, v) \
+    type1##_4_in(c0, c1, c2, c3, c01, c23) \
+    type1##_4_in(c4, c5, c6, c7, c45, c67) \
+    memset(coeff, 0, sizeof(*coeff) * 4 * 8); \
+    TRANSPOSE4_I32(c0, c1, c2, c3); \
+    TRANSPOSE4_I32(c4, c5, c6, c7); \
+    type2##_8_out(c0, c1, c2, c3, c4, c5, c6, c7, c01, c23, c45, c67) \
+    LOAD_DECLARE_4(dst, stride, a, b, c, d) \
+    LOAD_DECLARE_4(dst + 4 * stride, stride, e, f, g, h) \
+    APPLY_COEFF_4(a, b, c, d, c01, c23) \
+    APPLY_COEFF_4(e, f, g, h, c45, c67) \
+    STORE_4(dst, stride, a, b, c, d) \
+    STORE_4(dst + 4 * stride, stride, e, f, g, h) \
+}
+
+inv_txfm_fn4x8(adst,     dct     )
+inv_txfm_fn4x8(dct,      adst    )
+inv_txfm_fn4x8(dct,      flipadst)
+inv_txfm_fn4x8(flipadst, dct     )
+inv_txfm_fn4x8(adst,     flipadst)
+inv_txfm_fn4x8(flipadst, adst    )
+inv_txfm_fn4x8(identity, dct     )
+inv_txfm_fn4x8(dct,      identity)
+inv_txfm_fn4x8(identity, flipadst)
+inv_txfm_fn4x8(flipadst, identity)
+inv_txfm_fn4x8(identity, adst   )
+inv_txfm_fn4x8(adst,     identity)
+inv_txfm_fn4x8(identity, identity)
+inv_txfm_fn4x8(adst,     adst    )
+inv_txfm_fn4x8(flipadst, flipadst)
+
+
+void dav1d_inv_txfm_add_dct_dct_8x4_8bpc_pwr9(uint8_t *dst, const ptrdiff_t stride,
+                                              int16_t *const coeff, const int eob)
+{
+    i16x8 v = vec_splats((int16_t)(2896*8));
+
+    if (eob < 1) {
+        return dc_only_8xN(dst, stride, coeff, 1, 1, 0);
+    }
+
+    LOAD_SCALE_COEFF_8x4(coeff, v)
+
+    dct_8_in(c0, c1, c2, c3, c4, c5, c6, c7, c01, c23, c45, c67)
+
+    memset(coeff, 0, sizeof(*coeff) * 8 * 4);
+
+    TRANSPOSE4_I32(c0, c1, c2, c3)
+    TRANSPOSE4_I32(c4, c5, c6, c7)
+
+    dct_4_out(c0, c1, c2, c3, c01, c23)
+    dct_4_out(c4, c5, c6, c7, c45, c67)
+
+    LOAD_DECLARE_4(dst, stride, ae, bf, cg, dh)
+
+    i16x8 c04 = (i16x8)vec_mergeh((u64x2)c01, (u64x2)c45);
+    i16x8 c15 = (i16x8)vec_mergel((u64x2)c01, (u64x2)c45);
+    i16x8 c26 = (i16x8)vec_mergeh((u64x2)c23, (u64x2)c67);
+    i16x8 c37 = (i16x8)vec_mergel((u64x2)c23, (u64x2)c67);
+
+    APPLY_COEFF_8x4(ae, bf, c04, c15)
+    APPLY_COEFF_8x4(cg, dh, c26, c37)
+
+    STORE_8(dst, stride, ae, bf, cg, dh)
+}
+
+
+#define inv_txfm_fn8x4(type1, type2) \
+void dav1d_inv_txfm_add_##type1##_##type2##_8x4_8bpc_pwr9(uint8_t *dst, const ptrdiff_t stride, \
+                                                          int16_t *const coeff, const int eob) \
+{ \
+    i16x8 v = vec_splats((int16_t)(2896*8)); \
+    LOAD_SCALE_COEFF_8x4(coeff, v) \
+    type1##_8_in(c0, c1, c2, c3, c4, c5, c6, c7, c01, c23, c45, c67) \
+    memset(coeff, 0, sizeof(*coeff) * 8 * 4); \
+    TRANSPOSE4_I32(c0, c1, c2, c3) \
+    TRANSPOSE4_I32(c4, c5, c6, c7) \
+    type2##_4_out(c0, c1, c2, c3, c01, c23) \
+    type2##_4_out(c4, c5, c6, c7, c45, c67) \
+    LOAD_DECLARE_4(dst, stride, ae, bf, cg, dh) \
+    i16x8 c04 = (i16x8)vec_mergeh((u64x2)c01, (u64x2)c45); \
+    i16x8 c15 = (i16x8)vec_mergel((u64x2)c01, (u64x2)c45); \
+    i16x8 c26 = (i16x8)vec_mergeh((u64x2)c23, (u64x2)c67); \
+    i16x8 c37 = (i16x8)vec_mergel((u64x2)c23, (u64x2)c67); \
+    APPLY_COEFF_8x4(ae, bf, c04, c15) \
+    APPLY_COEFF_8x4(cg, dh, c26, c37) \
+    STORE_8(dst, stride, ae, bf, cg, dh) \
+}
+inv_txfm_fn8x4(adst,     dct     )
+inv_txfm_fn8x4(dct,      adst    )
+inv_txfm_fn8x4(dct,      flipadst)
+inv_txfm_fn8x4(flipadst, dct     )
+inv_txfm_fn8x4(adst,     flipadst)
+inv_txfm_fn8x4(flipadst, adst    )
+inv_txfm_fn8x4(identity, dct     )
+inv_txfm_fn8x4(dct,      identity)
+inv_txfm_fn8x4(identity, flipadst)
+inv_txfm_fn8x4(flipadst, identity)
+inv_txfm_fn8x4(identity, adst   )
+inv_txfm_fn8x4(adst,     identity)
+inv_txfm_fn8x4(identity, identity)
+inv_txfm_fn8x4(adst,     adst    )
+inv_txfm_fn8x4(flipadst, flipadst)
+
+void dav1d_inv_txfm_add_dct_dct_8x8_8bpc_pwr9(uint8_t *dst, const ptrdiff_t stride,
+                                              int16_t *const coeff, const int eob)
+{
+    if (eob < 1) {
+        return dc_only_8xN(dst, stride, coeff, 2, 0, 1);
+    }
+
+    LOAD_COEFF_8x8(coeff)
+
+    dct_8x2_in(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h,
+               c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l,
+               c0, c1, c2, c3, c4, c5, c6, c7)
+
+    memset(coeff, 0, sizeof(*coeff) * 8 * 8);
+
+    SCALE_ROUND_4(c0h, c1h, c2h, c3h, vec_splat_s32(1), vec_splat_u32(1))
+    SCALE_ROUND_4(c4h, c5h, c6h, c7h, vec_splat_s32(1), vec_splat_u32(1))
+    SCALE_ROUND_4(c0l, c1l, c2l, c3l, vec_splat_s32(1), vec_splat_u32(1))
+    SCALE_ROUND_4(c4l, c5l, c6l, c7l, vec_splat_s32(1), vec_splat_u32(1))
+
+    TRANSPOSE8_I32(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h,
+                   c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l)
+
+    dct_8x2_out(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h,
+                c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l,
+                c0, c1, c2, c3, c4, c5, c6, c7)
+
+    LOAD_DECLARE_4(dst, stride, a, b, cc, d)
+    LOAD_DECLARE_4(dst + 4 * stride, stride, e, f, g, hh)
+
+    APPLY_COEFF_8x4(a, b, c0, c1)
+    APPLY_COEFF_8x4(cc, d, c2, c3)
+    APPLY_COEFF_8x4(e, f, c4, c5)
+    APPLY_COEFF_8x4(g, hh, c6, c7)
+
+    STORE_8(dst, stride, a, b, cc, d)
+    STORE_8(dst + 4 * stride, stride, e, f, g, hh)
+}
+
+#define inv_txfm_fn8x8(type1, type2) \
+void dav1d_inv_txfm_add_##type1##_##type2##_8x8_8bpc_pwr9(uint8_t *dst, const ptrdiff_t stride, \
+                                                          int16_t *const coeff, const int eob) \
+{ \
+    LOAD_COEFF_8x8(coeff) \
+    type1##_8x2_in(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                   c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                   c0, c1, c2, c3, c4, c5, c6, c7) \
+    SCALE_ROUND_4(c0h, c1h, c2h, c3h, vec_splat_s32(1), vec_splat_u32(1)) \
+    SCALE_ROUND_4(c4h, c5h, c6h, c7h, vec_splat_s32(1), vec_splat_u32(1)) \
+    SCALE_ROUND_4(c0l, c1l, c2l, c3l, vec_splat_s32(1), vec_splat_u32(1)) \
+    SCALE_ROUND_4(c4l, c5l, c6l, c7l, vec_splat_s32(1), vec_splat_u32(1)) \
+    memset(coeff, 0, sizeof(*coeff) * 8 * 8); \
+    TRANSPOSE8_I32(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                   c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l) \
+    type2##_8x2_out(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                    c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                    c0, c1, c2, c3, c4, c5, c6, c7) \
+    LOAD_DECLARE_4(dst, stride, a, b, c, d) \
+    LOAD_DECLARE_4(dst + 4 * stride, stride, e, f, g, h) \
+    APPLY_COEFF_8x4(a, b, c0, c1) \
+    APPLY_COEFF_8x4(c, d, c2, c3) \
+    APPLY_COEFF_8x4(e, f, c4, c5) \
+    APPLY_COEFF_8x4(g, h, c6, c7) \
+    STORE_8(dst, stride, a, b, c, d) \
+    STORE_8(dst + 4 * stride, stride, e, f, g, h) \
+}
+inv_txfm_fn8x8(adst,     dct     )
+inv_txfm_fn8x8(dct,      adst    )
+inv_txfm_fn8x8(dct,      flipadst)
+inv_txfm_fn8x8(flipadst, dct     )
+inv_txfm_fn8x8(adst,     flipadst)
+inv_txfm_fn8x8(flipadst, adst    )
+inv_txfm_fn8x8(dct,      identity)
+inv_txfm_fn8x8(flipadst, identity)
+inv_txfm_fn8x8(adst,     identity)
+inv_txfm_fn8x8(adst,     adst    )
+inv_txfm_fn8x8(flipadst, flipadst)
+
+// identity + scale is a no op
+#define inv_txfm_fn8x8_identity(type2) \
+void dav1d_inv_txfm_add_identity_##type2##_8x8_8bpc_pwr9(uint8_t *dst, const ptrdiff_t stride, \
+                                                         int16_t *const coeff, const int eob) \
+{ \
+    LOAD_COEFF_8x8(coeff) \
+    memset(coeff, 0, sizeof(*coeff) * 8 * 8); \
+    TRANSPOSE8_I32(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                   c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l) \
+    type2##_8x2_out(c0h, c1h, c2h, c3h, c4h, c5h, c6h, c7h, \
+                    c0l, c1l, c2l, c3l, c4l, c5l, c6l, c7l, \
+                    c0, c1, c2, c3, c4, c5, c6, c7) \
+    LOAD_DECLARE_4(dst, stride, a, b, c, d) \
+    LOAD_DECLARE_4(dst + 4 * stride, stride, e, f, g, h) \
+    APPLY_COEFF_8x4(a, b, c0, c1) \
+    APPLY_COEFF_8x4(c, d, c2, c3) \
+    APPLY_COEFF_8x4(e, f, c4, c5) \
+    APPLY_COEFF_8x4(g, h, c6, c7) \
+    STORE_8(dst, stride, a, b, c, d) \
+    STORE_8(dst + 4 * stride, stride, e, f, g, h) \
+}
+inv_txfm_fn8x8_identity(dct     )
+inv_txfm_fn8x8_identity(flipadst)
+inv_txfm_fn8x8_identity(adst    )
+inv_txfm_fn8x8_identity(identity)
+
+#define CLIP16_I32_8(a, b, c, d, e, f, g, h, \
+                     ab, cd, ef, gh) \
+{ \
+    ab = vec_packs(a, b); \
+    cd = vec_packs(c, d); \
+    ef = vec_packs(e, f); \
+    gh = vec_packs(g, h); \
+    UNPACK_PAIR_I16_I32(a, b, ab) \
+    UNPACK_PAIR_I16_I32(c, d, cd) \
+    UNPACK_PAIR_I16_I32(e, f, ef) \
+    UNPACK_PAIR_I16_I32(g, h, gh) \
+}
+
+#define MUL_4_INPLACE(a, b, c, d, v) \
+    a = vec_mul(a, v); \
+    b = vec_mul(b, v); \
+    c = vec_mul(c, v); \
+    d = vec_mul(d, v); \
+
+#define IDENTITY_16_V(v) \
+{ \
+    i16x8 v_ = vec_adds(v, v); \
+    v = vec_mradds(v, v1697_16, v_); \
+}
+
+#define IDENTITY_16_INNER(c00c01, c02c03, c04c05, c06c07, \
+                          c08c09, c10c11, c12c13, c14c15) \
+{ \
+    i16x8 v1697_16 = vec_splats((int16_t)(1697*16)); \
+    IDENTITY_16_V(c00c01) \
+    IDENTITY_16_V(c02c03) \
+    IDENTITY_16_V(c04c05) \
+    IDENTITY_16_V(c06c07) \
+    IDENTITY_16_V(c08c09) \
+    IDENTITY_16_V(c10c11) \
+    IDENTITY_16_V(c12c13) \
+    IDENTITY_16_V(c14c15) \
+}
+
+#define IDENTITY_16_4_I32(a, b, c, d) \
+{ \
+    i32x4 a2 = vec_add(a, a); \
+    i32x4 b2 = vec_add(b, b); \
+    i32x4 c2 = vec_add(c, c); \
+    i32x4 d2 = vec_add(d, d); \
+    MUL_4_INPLACE(a, b, c, d, v1697) \
+    SCALE_ROUND_4(a, b, c, d, v1024, vec_splat_u32(11)); \
+    a = vec_add(a2, a); \
+    b = vec_add(b2, b); \
+    c = vec_add(c2, c); \
+    d = vec_add(d2, d); \
+}
+
+
+#define identity_16_in(c00, c01, c02, c03, c04, c05, c06, c07, \
+                       c08, c09, c10, c11, c12, c13, c14, c15, \
+                       c00c01, c02c03, c04c05, c06c07, \
+                       c08c09, c10c11, c12c13, c14c15) \
+{ \
+    DECLARE_SPLAT_I32(1697) \
+    DECLARE_SPLAT_I32(1024) \
+    IDENTITY_16_4_I32(c00, c01, c02, c03) \
+    IDENTITY_16_4_I32(c04, c05, c06, c07) \
+    IDENTITY_16_4_I32(c08, c09, c10, c11) \
+    IDENTITY_16_4_I32(c12, c13, c14, c15) \
+}
+
+#define identity_16_out(c00, c01, c02, c03, c04, c05, c06, c07, \
+                        c08, c09, c10, c11, c12, c13, c14, c15, \
+                        c00c01, c02c03, c04c05, c06c07, \
+                        c08c09, c10c11, c12c13, c14c15) \
+{ \
+    PACK_8(c00c01, c02c03, c04c05, c06c07, c08c09, c10c11, c12c13, c14c15, \
+           c00, c02, c04, c06, c08, c10, c12, c14, \
+           c01, c03, c05, c07, c09, c11, c13, c15)  \
+    IDENTITY_16_INNER(c00c01, c02c03, c04c05, c06c07, \
+                      c08c09, c10c11, c12c13, c14c15) \
+}
+
+#define IDCT_16_INNER(c00, c01, c02, c03, c04, c05, c06, c07, \
+                      c08, c09, c10, c11, c12, c13, c14, c15, \
+                      c00c03, c01c02, c07c04, c06c05, \
+                      c08c11, c09c10, c14c13, c15c12) \
+    IDCT_8_INNER(c00, c02, c04, c06, c08, c10, c12, c14, \
+                 c00c03, c01c02, c07c04, c06c05) \
+    DECLARE_SPLAT_I32(128) \
+    DECLARE_SPLAT_I32(181) \
+    DECLARE_SPLAT_I32(401) \
+    DECLARE_SPLAT_I32(4076) \
+    DECLARE_SPLAT_I32(3166) \
+    DECLARE_SPLAT_I32(2598) \
+    DECLARE_SPLAT_I32(1931) \
+    DECLARE_SPLAT_I32(3612) \
+    DECLARE_SPLAT_I32(3920) \
+    DECLARE_SPLAT_I32(1189) \
+    DECLARE_SPLAT_I32(1567) \
+    DECLARE_SPLAT_I32(3784) \
+\
+    DECLARE_MUL_PAIR_I32(c01, c15,  v401, v4076) \
+    DECLARE_MUL_PAIR_I32(c09, c07, v3166, v2598) \
+    DECLARE_MUL_PAIR_I32(c05, c11, v1931, v3612) \
+    DECLARE_MUL_PAIR_I32(c13, c03, v3920, v1189) \
+\
+    DECLARE_ADD_SUB_PAIR(t15a, t08a, c01, c15, v4076,  v401) \
+    DECLARE_ADD_SUB_PAIR(t14a, t09a, c09, c07, v2598, v3166) \
+    DECLARE_ADD_SUB_PAIR(t13a, t10a, c05, c11, v3612, v1931) \
+    DECLARE_ADD_SUB_PAIR(t12a, t11a, c13, c03, v1189, v3920) \
+\
+    SCALE_ROUND_4(t15a, t08a, t14a, t09a, v2048, v12) \
+    SCALE_ROUND_4(t13a, t10a, t12a, t11a, v2048, v12) \
+\
+    CLIP16_I32_8(t15a, t08a, t14a, t09a, \
+                 t13a, t10a, t12a, t11a, \
+                 c08c11, c09c10, c14c13, c15c12) \
+    DECLARE_ADD_SUB_PAIR(t08, t09, t08a, t09a,,) \
+    DECLARE_ADD_SUB_PAIR(t11, t10, t11a, t10a,,) \
+    DECLARE_ADD_SUB_PAIR(t12, t13, t12a, t13a,,) \
+    DECLARE_ADD_SUB_PAIR(t15, t14, t15a, t14a,,) \
+\
+    CLIP16_I32_8(t08, t09, t11, t10, \
+                 t12, t13, t15, t14, \
+                 c08c11, c09c10, c14c13, c15c12) \
+\
+    DECLARE_MUL_PAIR_I32(t14, t09, v1567, v3784) \
+    DECLARE_MUL_PAIR_I32(t13, t10, v1567, v3784) \
+    \
+    ADD_SUB_PAIR(t14a, t09a, t14, t09, v3784, v1567) \
+    ADD_SUB_PAIR(t10a, t13a, t13, t10, v3784, v1567) \
+    t10a = -t10a; \
+\
+    SCALE_ROUND_4(t14a, t09a, t13a, t10a, v2048, v12) \
+\
+    ADD_SUB_PAIR(t08a, t11a, t08, t11,,) \
+    ADD_SUB_PAIR(t09, t10, t09a, t10a,,) \
+    ADD_SUB_PAIR(t15a, t12a, t15, t12,,) \
+    ADD_SUB_PAIR(t14, t13, t14a, t13a,,) \
+\
+    CLIP16_I32_8(t08a, t11a, t09, t10, \
+                 t15a, t12a, t14, t13, \
+                 c08c11, c09c10, c14c13, c15c12) \
+    ADD_SUB_PAIR(t13a, t10a, t13, t10,,); \
+    ADD_SUB_PAIR(t12, t11, t12a, t11a,,); \
+\
+    MUL_4_INPLACE(t13a, t10a, t12, t11, v181); \
+    SCALE_ROUND_4(t13a, t10a, t12, t11, v128, vec_splat_u32(8)) \
+\
+    DECLARE_PACK_4(t15at12, t14t13a, t08at11, t09t10a, \
+                   t15a, t14, t08a, t09, \
+                   t12, t13a, t11,  t10a) \
+\
+    c15c12 = vec_subs(c00c03, t15at12); \
+    c14c13 = vec_subs(c01c02, t14t13a); \
+    c08c11 = vec_subs(c07c04, t08at11); \
+    c09c10 = vec_subs(c06c05, t09t10a); \
+    c00c03 = vec_adds(c00c03, t15at12); \
+    c01c02 = vec_adds(c01c02, t14t13a); \
+    c07c04 = vec_adds(c07c04, t08at11); \
+    c06c05 = vec_adds(c06c05, t09t10a); \
+
+#define dct_16_out(c00, c01, c02, c03, c04, c05, c06, c07, \
+                   c08, c09, c10, c11, c12, c13, c14, c15, \
+                   c00c01, c02c03, c04c05, c06c07, c08c09, c10c11, c12c13, c14c15) \
+\
+    i16x8 c00c03, c01c02, c07c04, c06c05, c08c11, c09c10, c14c13, c15c12; \
+    IDCT_16_INNER(c00, c01, c02, c03, c04, c05, c06, c07, c08, c09, c10, c11, c12, c13, c14, c15, \
+                  c00c03, c01c02, c07c04, c06c05, c08c11, c09c10, c14c13, c15c12) \
+    c00c01 = (i16x8)vec_mergeh((u64x2)c00c03, (u64x2)c01c02); \
+    c02c03 = (i16x8)vec_mergel((u64x2)c01c02, (u64x2)c00c03); \
+    c04c05 = (i16x8)vec_mergel((u64x2)c07c04, (u64x2)c06c05); \
+    c06c07 = (i16x8)vec_mergeh((u64x2)c06c05, (u64x2)c07c04); \
+    c08c09 = (i16x8)vec_mergeh((u64x2)c08c11, (u64x2)c09c10); \
+    c10c11 = (i16x8)vec_mergel((u64x2)c09c10, (u64x2)c08c11); \
+    c12c13 = (i16x8)vec_mergel((u64x2)c15c12, (u64x2)c14c13); \
+    c14c15 = (i16x8)vec_mergeh((u64x2)c14c13, (u64x2)c15c12); \
+
+#define dct_16_in(c00, c01, c02, c03, c04, c05, c06, c07, \
+                  c08, c09, c10, c11, c12, c13, c14, c15, \
+                  c00c03, c01c02, c07c04, c06c05, c08c11, c09c10, c14c13, c15c12) \
+\
+    IDCT_16_INNER(c00, c01, c02, c03, c04, c05, c06, c07, c08, c09, c10, c11, c12, c13, c14, c15, \
+                  c00c03, c01c02, c07c04, c06c05, c08c11, c09c10, c14c13, c15c12) \
+    UNPACK_PAIR_I16_I32(c00, c03, c00c03) \
+    UNPACK_PAIR_I16_I32(c01, c02, c01c02) \
+    UNPACK_PAIR_I16_I32(c07, c04, c07c04) \
+    UNPACK_PAIR_I16_I32(c06, c05, c06c05) \
+    UNPACK_PAIR_I16_I32(c08, c11, c08c11) \
+    UNPACK_PAIR_I16_I32(c09, c10, c09c10) \
+    UNPACK_PAIR_I16_I32(c14, c13, c14c13) \
+    UNPACK_PAIR_I16_I32(c15, c12, c15c12) \
+
+
+#define dct_4x4_in(cA0, cA1, cA2, cA3, cB0, cB1, cB2, cB3, \
+                   cC0, cC1, cC2, cC3, cD0, cD1, cD2, cD3, \
+                   a0b0, c0d0, a1b1, c1d1, a2b2, c2d2, a3b3, c3d3) \
+    dct_4_in(cA0, cA1, cA2, cA3, a0b0, c0d0) \
+    dct_4_in(cB0, cB1, cB2, cB3, a1b1, c1d1) \
+    dct_4_in(cC0, cC1, cC2, cC3, a2b2, c2d2) \
+    dct_4_in(cD0, cD1, cD2, cD3, a3b3, c3d3)
+
+
+#define PACK_4x4(c00, c01, c02, c03, \
+                 c04, c05, c06, c07, \
+                 c08, c09, c10, c11, \
+                 c12, c13, c14, c15, \
+                 c00c01, c02c03, c04c05, c06c07, \
+                 c08c09, c10c11, c12c13, c14c15) \
+{ \
+    c00c01 = vec_packs(c00, c04); c02c03 = vec_packs(c08, c12); \
+    c04c05 = vec_packs(c01, c05); c06c07 = vec_packs(c09, c13); \
+    c08c09 = vec_packs(c02, c06); c10c11 = vec_packs(c10, c14); \
+    c12c13 = vec_packs(c03, c07); c14c15 = vec_packs(c11, c15); \
+}
+
+
+
+#define dct_4x4_out(c00, c01, c02, c03, \
+                    c04, c05, c06, c07, \
+                    c08, c09, c10, c11, \
+                    c12, c13, c14, c15, \
+                    c00c01, c02c03, c04c05, c06c07, \
+                    c08c09, c10c11, c12c13, c14c15) \
+{ \
+    IDCT_4_INNER(c00, c01, c02, c03) \
+    IDCT_4_INNER(c04, c05, c06, c07) \
+    IDCT_4_INNER(c08, c09, c10, c11) \
+    IDCT_4_INNER(c12, c13, c14, c15) \
+\
+    PACK_4x4(c00, c01, c02, c03, \
+             c04, c05, c06, c07, \
+             c08, c09, c10, c11, \
+             c12, c13, c14, c15, \
+             c00c01, c02c03, c04c05, c06c07, \
+             c08c09, c10c11, c12c13, c14c15) \
+}
+
+#define IDENTITY_4_I32(a, b, c, d) \
+{ \
+    DECLARE_SPLAT_I32(5793) \
+    DECLARE_SPLAT_I32(2048) \
+    MUL_4_INPLACE(a, b, c, d, v5793) \
+    SCALE_ROUND_4(a, b, c, d, v2048, vec_splat_u32(12)) \
+}
+
+#define identity_4x4_in(cA0, cA1, cA2, cA3, cB0, cB1, cB2, cB3, \
+                       cC0, cC1, cC2, cC3, cD0, cD1, cD2, cD3, \
+                       a0b0, c0d0, a1b1, c1d1, a2b2, c2d2, a3b3, c3d3) \
+{ \
+    IDENTITY_4_I32(cA0, cA1, cA2, cA3) \
+    IDENTITY_4_I32(cB0, cB1, cB2, cB3) \
+    IDENTITY_4_I32(cC0, cC1, cC2, cC3) \
+    IDENTITY_4_I32(cD0, cD1, cD2, cD3) \
+}
+
+#define identity_4x4_out(c00, c01, c02, c03, \
+                         c04, c05, c06, c07, \
+                         c08, c09, c10, c11, \
+                         c12, c13, c14, c15, \
+                         c00c01, c02c03, c04c05, c06c07, \
+                         c08c09, c10c11, c12c13, c14c15) \
+{ \
+    PACK_4x4(c00, c01, c02, c03, \
+             c04, c05, c06, c07, \
+             c08, c09, c10, c11, \
+             c12, c13, c14, c15, \
+             c00c01, c02c03, c04c05, c06c07, \
+             c08c09, c10c11, c12c13, c14c15) \
+    IDENTITY_4(c00c01, c02c03) \
+    IDENTITY_4(c04c05, c06c07) \
+    IDENTITY_4(c08c09, c10c11) \
+    IDENTITY_4(c12c13, c14c15) \
+}
+
+#define adst_4x4_in(cA0, cA1, cA2, cA3, cB0, cB1, cB2, cB3, \
+                    cC0, cC1, cC2, cC3, cD0, cD1, cD2, cD3, \
+                    a0b0, c0d0, a1b1, c1d1, a2b2, c2d2, a3b3, c3d3) \
+    adst_4_in(cA0, cA1, cA2, cA3, a0b0, c0d0) \
+    adst_4_in(cB0, cB1, cB2, cB3, a1b1, c1d1) \
+    adst_4_in(cC0, cC1, cC2, cC3, a2b2, c2d2) \
+    adst_4_in(cD0, cD1, cD2, cD3, a3b3, c3d3)
+
+#define adst_4x4_out(c00, c01, c02, c03, \
+                     c04, c05, c06, c07, \
+                     c08, c09, c10, c11, \
+                     c12, c13, c14, c15, \
+                     c00c01, c02c03, c04c05, c06c07, \
+                     c08c09, c10c11, c12c13, c14c15) \
+{ \
+    ADST_INNER_4(c00, c01, c02, c03, c00, c01, c02, c03) \
+    ADST_INNER_4(c04, c05, c06, c07, c04, c05, c06, c07) \
+    ADST_INNER_4(c08, c09, c10, c11, c08, c09, c10, c11) \
+    ADST_INNER_4(c12, c13, c14, c15, c12, c13, c14, c15) \
+\
+    PACK_4x4(c00, c01, c02, c03, \
+             c04, c05, c06, c07, \
+             c08, c09, c10, c11, \
+             c12, c13, c14, c15, \
+             c00c01, c02c03, c04c05, c06c07, \
+             c08c09, c10c11, c12c13, c14c15) \
+}
+
+#define flipadst_4x4_in(cA0, cA1, cA2, cA3, cB0, cB1, cB2, cB3, \
+                        cC0, cC1, cC2, cC3, cD0, cD1, cD2, cD3, \
+                        a0b0, c0d0, a1b1, c1d1, a2b2, c2d2, a3b3, c3d3) \
+    flipadst_4_in(cA0, cA1, cA2, cA3, a0b0, c0d0) \
+    flipadst_4_in(cB0, cB1, cB2, cB3, a1b1, c1d1) \
+    flipadst_4_in(cC0, cC1, cC2, cC3, a2b2, c2d2) \
+    flipadst_4_in(cD0, cD1, cD2, cD3, a3b3, c3d3)
+
+#define flipadst_4x4_out(c00, c01, c02, c03, \
+                         c04, c05, c06, c07, \
+                         c08, c09, c10, c11, \
+                         c12, c13, c14, c15, \
+                         c00c01, c02c03, c04c05, c06c07, \
+                         c08c09, c10c11, c12c13, c14c15) \
+{ \
+    ADST_INNER_4(c00, c01, c02, c03, c03, c02, c01, c00) \
+    ADST_INNER_4(c04, c05, c06, c07, c07, c06, c05, c04) \
+    ADST_INNER_4(c08, c09, c10, c11, c11, c10, c09, c08) \
+    ADST_INNER_4(c12, c13, c14, c15, c15, c14, c13, c12) \
+\
+    PACK_4x4(c00, c01, c02, c03, \
+             c04, c05, c06, c07, \
+             c08, c09, c10, c11, \
+             c12, c13, c14, c15, \
+             c00c01, c02c03, c04c05, c06c07, \
+             c08c09, c10c11, c12c13, c14c15) \
+}
+
+#define ADST_INNER_16(c00, c01, c02, c03, c04, c05, c06, c07, \
+                      c08, c09, c10, c11, c12, c13, c14, c15, \
+                      o00, o01, o02, o03, o04, o05, o06, o07, \
+                      o08, o09, o10, o11, o12, o13, o14, o15, \
+                      c00c01, c02c03, c04c05, c06c07) \
+    DECLARE_SPLAT_I32(2048); \
+    u32x4 v12 = vec_splat_u32(12); \
+    DECLARE_SPLAT_I32(4091) \
+    DECLARE_SPLAT_I32(201) \
+    DECLARE_SPLAT_I32(3973) \
+    DECLARE_SPLAT_I32(995) \
+    DECLARE_SPLAT_I32(3703) \
+    DECLARE_SPLAT_I32(1751) \
+    DECLARE_SPLAT_I32(3290) \
+    DECLARE_SPLAT_I32(2440) \
+    DECLARE_SPLAT_I32(2751) \
+    DECLARE_SPLAT_I32(3035) \
+    DECLARE_SPLAT_I32(2106) \
+    DECLARE_SPLAT_I32(3513) \
+    DECLARE_SPLAT_I32(1380) \
+    DECLARE_SPLAT_I32(3857) \
+    DECLARE_SPLAT_I32(601) \
+    DECLARE_SPLAT_I32(4052) \
+\
+    DECLARE_MUL_PAIR_I32(c15, c00, v4091, v201) \
+    DECLARE_MUL_PAIR_I32(c13, c02, v3973, v995) \
+    DECLARE_MUL_PAIR_I32(c11, c04, v3703, v1751) \
+    DECLARE_MUL_PAIR_I32(c09, c06, v3290, v2440) \
+    DECLARE_MUL_PAIR_I32(c07, c08, v2751, v3035) \
+    DECLARE_MUL_PAIR_I32(c05, c10, v2106, v3513) \
+    DECLARE_MUL_PAIR_I32(c03, c12, v1380, v3857) \
+    DECLARE_MUL_PAIR_I32(c01, c14,  v601, v4052) \
+\
+    DECLARE_ADD_SUB_PAIR(t00, t01, c15, c00, v4091, v201);\
+    DECLARE_ADD_SUB_PAIR(t02, t03, c13, c02, v3973, v995) \
+    DECLARE_ADD_SUB_PAIR(t04, t05, c11, c04, v3703, v1751) \
+    DECLARE_ADD_SUB_PAIR(t06, t07, c09, c06, v3290, v2440) \
+    DECLARE_ADD_SUB_PAIR(t08, t09, c07, c08, v2751, v3035) \
+    DECLARE_ADD_SUB_PAIR(t10, t11, c05, c10, v2106, v3513) \
+    DECLARE_ADD_SUB_PAIR(t12, t13, c03, c12, v1380, v3857) \
+    DECLARE_ADD_SUB_PAIR(t14, t15, c01, c14,  v601, v4052) \
+\
+    SCALE_ROUND_4(t00, t01, t02, t03, v2048, v12) \
+    SCALE_ROUND_4(t04, t05, t06, t07, v2048, v12) \
+    SCALE_ROUND_4(t08, t09, t10, t11, v2048, v12) \
+    SCALE_ROUND_4(t12, t13, t14, t15, v2048, v12) \
+\
+    DECLARE_ADD_SUB_PAIR(t00a, t08a, t00, t08,,) \
+    DECLARE_ADD_SUB_PAIR(t01a, t09a, t01, t09,,) \
+    DECLARE_ADD_SUB_PAIR(t02a, t10a, t02, t10,,) \
+    DECLARE_ADD_SUB_PAIR(t03a, t11a, t03, t11,,) \
+    DECLARE_ADD_SUB_PAIR(t04a, t12a, t04, t12,,) \
+    DECLARE_ADD_SUB_PAIR(t05a, t13a, t05, t13,,) \
+    DECLARE_ADD_SUB_PAIR(t06a, t14a, t06, t14,,) \
+    DECLARE_ADD_SUB_PAIR(t07a, t15a, t07, t15,,) \
+\
+    CLIP16_I32_8(t00a, t08a, t01a, t09a, t02a, t10a, t03a, t11a, \
+                 c00c01, c02c03, c04c05, c06c07); \
+    CLIP16_I32_8(t04a, t12a, t05a, t13a, t06a, t14a, t07a, t15a, \
+                 c00c01, c02c03, c04c05, c06c07); \
+\
+    DECLARE_SPLAT_I32(4017) \
+    DECLARE_SPLAT_I32(799) \
+    DECLARE_SPLAT_I32(2276) \
+    DECLARE_SPLAT_I32(3406) \
+\
+    DECLARE_MUL_PAIR_I32(t08a, t09a, v4017,  v799); \
+    DECLARE_MUL_PAIR_I32(t10a, t11a, v2276, v3406); \
+    DECLARE_MUL_PAIR_I32(t13a, t12a,  v799, v4017); \
+    DECLARE_MUL_PAIR_I32(t15a, t14a, v3406, v2276); \
+\
+    ADD_SUB_PAIR(t08, t09, t08a, t09a, v4017,  v799); \
+    ADD_SUB_PAIR(t10, t11, t10a, t11a, v2276, v3406); \
+    ADD_SUB_PAIR(t13, t12, t13a, t12a,  v799, v4017); \
+    ADD_SUB_PAIR(t15, t14, t15a, t14a, v3406, v2276); \
+\
+    SCALE_ROUND_4(t08, t09, t10, t11, v2048, v12) \
+    SCALE_ROUND_4(t13, t12, t15, t14, v2048, v12) \
+\
+    ADD_SUB_PAIR(t00, t04, t00a, t04a,,); \
+    ADD_SUB_PAIR(t01, t05, t01a, t05a,,); \
+    ADD_SUB_PAIR(t02, t06, t02a, t06a,,); \
+    ADD_SUB_PAIR(t03, t07, t03a, t07a,,); \
+    ADD_SUB_PAIR(t08a, t12a, t08, t12,,); \
+    ADD_SUB_PAIR(t09a, t13a, t09, t13,,); \
+    ADD_SUB_PAIR(t10a, t14a, t10, t14,,); \
+    ADD_SUB_PAIR(t11a, t15a, t11, t15,,); \
+\
+    CLIP16_I32_8(t00, t04, t01, t05, t02, t06, t03, t07, \
+                 c00c01, c02c03, c04c05, c06c07) \
+    CLIP16_I32_8(t08a, t12a, t09a, t13a, t10a, t14a, t11a, t15a, \
+                 c00c01, c02c03, c04c05, c06c07) \
+\
+    DECLARE_SPLAT_I32(3784) \
+    DECLARE_SPLAT_I32(1567) \
+\
+    DECLARE_MUL_PAIR_I32(t04, t05, v3784, v1567) \
+    DECLARE_MUL_PAIR_I32(t07, t06, v1567, v3784) \
+    DECLARE_MUL_PAIR_I32(t12a, t13a, v3784, v1567) \
+    DECLARE_MUL_PAIR_I32(t15a, t14a, v1567, v3784) \
+\
+    ADD_SUB_PAIR(t04a, t05a, t04, t05, v3784, v1567) \
+    ADD_SUB_PAIR(t07a, t06a, t07, t06, v1567, v3784) \
+    ADD_SUB_PAIR(t12, t13, t12a, t13a, v3784, v1567) \
+    ADD_SUB_PAIR(t15, t14, t15a, t14a, v1567, v3784) \
+\
+    SCALE_ROUND_4(t04a, t05a, t07a, t06a, v2048, v12) \
+    SCALE_ROUND_4(t12, t13, t15, t14, v2048, v12) \
+\
+    ADD_SUB_PAIR(o00, t02a, t00,  t02,,) \
+    ADD_SUB_PAIR(o15, t03a, t01,  t03,,) \
+    ADD_SUB_PAIR(o03, t06,  t04a, t06a,,) \
+    ADD_SUB_PAIR(o12, t07,  t05a, t07a,,) \
+    ADD_SUB_PAIR(o01, t10,  t08a, t10a,,) \
+    ADD_SUB_PAIR(o14, t11,  t09a, t11a,,) \
+    ADD_SUB_PAIR(o02, t14a, t12,  t14,,) \
+    ADD_SUB_PAIR(o13, t15a, t13,  t15,,) \
+\
+    CLIP16_I32_8(o00, t02a, o15, t03a, o03, t06, o12, t07, \
+                 c00c01, c02c03, c04c05, c06c07) \
+    CLIP16_I32_8(o01, t10, o14, t11, o02, t14a, o13, t15a, \
+                 c00c01, c02c03, c04c05, c06c07) \
+\
+    DECLARE_SPLAT_I32(181) \
+    DECLARE_SPLAT_I32(128) \
+    u32x4 v8 = vec_splat_u32(8); \
+\
+    ADD_SUB_PAIR(o07, o08, t02a, t03a,,) \
+    ADD_SUB_PAIR(o04, o11, t06,  t07,,) \
+    ADD_SUB_PAIR(o06, o09, t10,  t11,,) \
+    ADD_SUB_PAIR(o05, o10, t14a, t15a,,) \
+\
+    MUL_4_INPLACE(o07, o08, o04, o11, v181) \
+    MUL_4_INPLACE(o06, o09, o05, o10, v181) \
+\
+    SCALE_ROUND_4(o07, o08, o04, o11, v128, v8) \
+    SCALE_ROUND_4(o06, o09, o05, o10, v128, v8) \
+\
+    o01 = -o01; \
+    o03 = -o03; \
+    o05 = -o05; \
+    o07 = -o07; \
+    o09 = -o09; \
+    o11 = -o11; \
+    o13 = -o13; \
+    o15 = -o15; \
+
+#define adst_16_in(c00, c01, c02, c03, c04, c05, c06, c07, \
+                   c08, c09, c10, c11, c12, c13, c14, c15, \
+                   c00c01, c02c03, c04c05, c06c07, c08c09, c10c11, c12c13, c14c15) \
+{ \
+    ADST_INNER_16(c00, c01, c02, c03, c04, c05, c06, c07, c08, c09, c10, c11, c12, c13, c14, c15, \
+                  c00, c01, c02, c03, c04, c05, c06, c07, c08, c09, c10, c11, c12, c13, c14, c15, \
+                  c00c01, c02c03, c04c05, c06c07) \
+}
+
+#define adst_16_out(c00, c01, c02, c03, c04, c05, c06, c07, \
+                    c08, c09, c10, c11, c12, c13, c14, c15, \
+                    c00c01, c02c03, c04c05, c06c07, c08c09, c10c11, c12c13, c14c15) \
+{ \
+    ADST_INNER_16(c00, c01, c02, c03, c04, c05, c06, c07, c08, c09, c10, c11, c12, c13, c14, c15, \
+                  c00, c01, c02, c03, c04, c05, c06, c07, c08, c09, c10, c11, c12, c13, c14, c15, \
+                  c00c01, c02c03, c04c05, c06c07) \
+    PACK_8(c00c01, c02c03, c04c05, c06c07, c08c09, c10c11, c12c13, c14c15, \
+           c00, c02, c04, c06, c08, c10, c12, c14, \
+           c01, c03, c05, c07, c09, c11, c13, c15) \
+}
+
+#define flipadst_16_in(c00, c01, c02, c03, c04, c05, c06, c07, \
+                       c08, c09, c10, c11, c12, c13, c14, c15, \
+                       c00c01, c02c03, c04c05, c06c07, c08c09, c10c11, c12c13, c14c15) \
+{ \
+    ADST_INNER_16(c00, c01, c02, c03, c04, c05, c06, c07, c08, c09, c10, c11, c12, c13, c14, c15, \
+                  c15, c14, c13, c12, c11, c10, c09, c08, c07, c06, c05, c04, c03, c02, c01, c00, \
+                  c00c01, c02c03, c04c05, c06c07) \
+}
+
+#define flipadst_16_out(c00, c01, c02, c03, c04, c05, c06, c07, \
+                        c08, c09, c10, c11, c12, c13, c14, c15, \
+                        c00c01, c02c03, c04c05, c06c07, c08c09, c10c11, c12c13, c14c15) \
+{ \
+    ADST_INNER_16(c00, c01, c02, c03, c04, c05, c06, c07, c08, c09, c10, c11, c12, c13, c14, c15, \
+                  c15, c14, c13, c12, c11, c10, c09, c08, c07, c06, c05, c04, c03, c02, c01, c00, \
+                  c00c01, c02c03, c04c05, c06c07) \
+    PACK_8(c00c01, c02c03, c04c05, c06c07, c08c09, c10c11, c12c13, c14c15, \
+           c00, c02, c04, c06, c08, c10, c12, c14, \
+           c01, c03, c05, c07, c09, c11, c13, c15) \
+}
+
+
+void dav1d_inv_txfm_add_dct_dct_4x16_8bpc_pwr9(uint8_t *dst, const ptrdiff_t stride,
+                                               int16_t *const coeff, const int eob
+                                               HIGHBD_DECL_SUFFIX)
+{
+    if (eob < 1) {
+        return dc_only_4xN(dst, stride, coeff, 4, 0, 1);
+    }
+
+    LOAD_COEFF_4x16(coeff)
+
+    dct_4x4_in(cA0, cA1, cA2, cA3, cB0, cB1, cB2, cB3,
+               cC0, cC1, cC2, cC3, cD0, cD1, cD2, cD3,
+               a0b0, c0d0, a1b1, c1d1, a2b2, c2d2, a3b3, c3d3)
+
+    memset(coeff, 0, sizeof(*coeff) * 4 * 16);
+
+    SCALE_ROUND_4(cA0, cB0, cC0, cD0, vec_splat_s32(1), vec_splat_u32(1))
+    SCALE_ROUND_4(cA1, cB1, cC1, cD1, vec_splat_s32(1), vec_splat_u32(1))
+    SCALE_ROUND_4(cA2, cB2, cC2, cD2, vec_splat_s32(1), vec_splat_u32(1))
+    SCALE_ROUND_4(cA3, cB3, cC3, cD3, vec_splat_s32(1), vec_splat_u32(1))
+    TRANSPOSE4x16_I32(cA0, cA1, cA2, cA3, cB0, cB1, cB2, cB3,
+                      cC0, cC1, cC2, cC3, cD0, cD1, cD2, cD3)
+
+    dct_16_out(cA0, cA1, cA2, cA3, cB0, cB1, cB2, cB3,
+               cC0, cC1, cC2, cC3, cD0, cD1, cD2, cD3,
+               a0b0, c0d0, a1b1, c1d1, a2b2, c2d2, a3b3, c3d3)
+
+    LOAD_DECLARE_4(dst, stride, l00, l01, l02, l03)
+    LOAD_DECLARE_4(dst + 4 * stride, stride, l04, l05, l06, l07)
+    LOAD_DECLARE_4(dst + 8 * stride, stride, l08, l09, l10, l11)
+    LOAD_DECLARE_4(dst + 12 * stride, stride, l12, l13, l14, l15)
+
+    APPLY_COEFF_4(l00, l01, l02, l03, a0b0, c0d0);
+    APPLY_COEFF_4(l04, l05, l06, l07, a1b1, c1d1);
+    APPLY_COEFF_4(l08, l09, l10, l11, a2b2, c2d2);
+    APPLY_COEFF_4(l12, l13, l14, l15, a3b3, c3d3);
+
+    STORE_4(dst, stride,               l00, l01, l02, l03);
+    STORE_4(dst + 4 * stride, stride,  l04, l05, l06, l07);
+    STORE_4(dst + 8 * stride, stride,  l08, l09, l10, l11);
+    STORE_4(dst + 12 * stride, stride, l12, l13, l14, l15);
+}
+
+#define inv_txfm_fn4x16(type1, type2) \
+void dav1d_inv_txfm_add_##type1##_##type2##_4x16_8bpc_pwr9(uint8_t *dst, const ptrdiff_t stride, \
+                                                          int16_t *const coeff, const int eob) \
+{ \
+    LOAD_COEFF_4x16(coeff) \
+    type1##_4x4_in(cA0, cA1, cA2, cA3, cB0, cB1, cB2, cB3, \
+                   cC0, cC1, cC2, cC3, cD0, cD1, cD2, cD3, \
+                   a0b0, c0d0, a1b1, c1d1, a2b2, c2d2, a3b3, c3d3) \
+    memset(coeff, 0, sizeof(*coeff) * 4 * 16); \
+    SCALE_ROUND_4(cA0, cB0, cC0, cD0, vec_splat_s32(1), vec_splat_u32(1)) \
+    SCALE_ROUND_4(cA1, cB1, cC1, cD1, vec_splat_s32(1), vec_splat_u32(1)) \
+    SCALE_ROUND_4(cA2, cB2, cC2, cD2, vec_splat_s32(1), vec_splat_u32(1)) \
+    SCALE_ROUND_4(cA3, cB3, cC3, cD3, vec_splat_s32(1), vec_splat_u32(1)) \
+    TRANSPOSE4x16_I32(cA0, cA1, cA2, cA3, cB0, cB1, cB2, cB3, \
+                      cC0, cC1, cC2, cC3, cD0, cD1, cD2, cD3) \
+    type2##_16_out(cA0, cA1, cA2, cA3, cB0, cB1, cB2, cB3, \
+                   cC0, cC1, cC2, cC3, cD0, cD1, cD2, cD3, \
+                   a0b0, c0d0, a1b1, c1d1, a2b2, c2d2, a3b3, c3d3) \
+    LOAD_DECLARE_4(dst, stride, l00, l01, l02, l03) \
+    LOAD_DECLARE_4(dst + 4 * stride, stride, l04, l05, l06, l07) \
+    LOAD_DECLARE_4(dst + 8 * stride, stride, l08, l09, l10, l11) \
+    LOAD_DECLARE_4(dst + 12 * stride, stride, l12, l13, l14, l15) \
+    APPLY_COEFF_4(l00, l01, l02, l03, a0b0, c0d0); \
+    APPLY_COEFF_4(l04, l05, l06, l07, a1b1, c1d1); \
+    APPLY_COEFF_4(l08, l09, l10, l11, a2b2, c2d2); \
+    APPLY_COEFF_4(l12, l13, l14, l15, a3b3, c3d3); \
+    STORE_4(dst, stride,               l00, l01, l02, l03); \
+    STORE_4(dst + 4 * stride, stride,  l04, l05, l06, l07); \
+    STORE_4(dst + 8 * stride, stride,  l08, l09, l10, l11); \
+    STORE_4(dst + 12 * stride, stride, l12, l13, l14, l15); \
+}
+inv_txfm_fn4x16(adst,     dct     )
+inv_txfm_fn4x16(dct,      adst    )
+inv_txfm_fn4x16(dct,      flipadst)
+inv_txfm_fn4x16(flipadst, dct     )
+inv_txfm_fn4x16(adst,     flipadst)
+inv_txfm_fn4x16(flipadst, adst    )
+inv_txfm_fn4x16(identity, dct     )
+inv_txfm_fn4x16(dct,      identity)
+inv_txfm_fn4x16(identity, flipadst)
+inv_txfm_fn4x16(flipadst, identity)
+inv_txfm_fn4x16(identity, adst   )
+inv_txfm_fn4x16(adst,     identity)
+inv_txfm_fn4x16(identity, identity)
+inv_txfm_fn4x16(adst,     adst    )
+inv_txfm_fn4x16(flipadst, flipadst)
+
+void dav1d_inv_txfm_add_dct_dct_16x4_8bpc_pwr9(uint8_t *dst, const ptrdiff_t stride,
+                                               int16_t *const coeff, const int eob)
+{
+
+    if (eob < 1) {
+        return dc_only_16xN(dst, stride, coeff, 1, 0, 1);
+    }
+
+    LOAD_DECLARE_2_I16(coeff, c00c01, c02c03) \
+    LOAD_DECLARE_2_I16(coeff+16, c04c05, c06c07) \
+    LOAD_DECLARE_2_I16(coeff+32, c08c09, c10c11) \
+    LOAD_DECLARE_2_I16(coeff+48, c12c13, c14c15) \
+    UNPACK_DECLARE_4_I16_I32(c00c01, c02c03, c00, c01, c02, c03)
+    UNPACK_DECLARE_4_I16_I32(c04c05, c06c07, c04, c05, c06, c07)
+    UNPACK_DECLARE_4_I16_I32(c08c09, c10c11, c08, c09, c10, c11)
+    UNPACK_DECLARE_4_I16_I32(c12c13, c14c15, c12, c13, c14, c15)
+
+    dct_16_in(c00, c01, c02, c03, c04, c05, c06, c07,
+              c08, c09, c10, c11, c12, c13, c14, c15,
+              c00c01, c02c03, c04c05, c06c07, c08c09, c10c11, c12c13, c14c15)
+    memset(coeff, 0, sizeof(*coeff) * 16 * 4);
+    SCALE_ROUND_4(c00, c01, c02, c03, vec_splat_s32(1), vec_splat_u32(1))
+    SCALE_ROUND_4(c04, c05, c06, c07, vec_splat_s32(1), vec_splat_u32(1))
+    SCALE_ROUND_4(c08, c09, c10, c11, vec_splat_s32(1), vec_splat_u32(1))
+    SCALE_ROUND_4(c12, c13, c14, c15, vec_splat_s32(1), vec_splat_u32(1))
+
+    TRANSPOSE4_I32(c00, c01, c02, c03);
+    TRANSPOSE4_I32(c04, c05, c06, c07);
+    TRANSPOSE4_I32(c08, c09, c10, c11);
+    TRANSPOSE4_I32(c12, c13, c14, c15);
+
+    dct_4x4_out(c00, c01, c02, c03,
+                c04, c05, c06, c07,
+                c08, c09, c10, c11,
+                c12, c13, c14, c15,
+                c00c01, c02c03, c04c05, c06c07,
+                c08c09, c10c11, c12c13, c14c15)
+
+    LOAD_DECLARE_4(dst, stride, l0, l1, l2, l3)
+
+    APPLY_COEFF_16x4(l0, l1, l2, l3,
+                     c00c01, c02c03, c04c05, c06c07,
+                     c08c09, c10c11, c12c13, c14c15)
+
+    STORE_16(dst, stride, l0, l1, l2, l3)
+}
+
+#define inv_txfm_fn16x4(type1, type2) \
+void dav1d_inv_txfm_add_##type1##_##type2##_16x4_8bpc_pwr9(uint8_t *dst, const ptrdiff_t stride, \
+                                                          int16_t *const coeff, const int eob) \
+{ \
+    LOAD_DECLARE_2_I16(coeff, c00c01, c02c03) \
+    LOAD_DECLARE_2_I16(coeff+16, c04c05, c06c07) \
+    LOAD_DECLARE_2_I16(coeff+32, c08c09, c10c11) \
+    LOAD_DECLARE_2_I16(coeff+48, c12c13, c14c15) \
+    UNPACK_DECLARE_4_I16_I32(c00c01, c02c03, c00, c01, c02, c03) \
+    UNPACK_DECLARE_4_I16_I32(c04c05, c06c07, c04, c05, c06, c07) \
+    UNPACK_DECLARE_4_I16_I32(c08c09, c10c11, c08, c09, c10, c11) \
+    UNPACK_DECLARE_4_I16_I32(c12c13, c14c15, c12, c13, c14, c15) \
+    type1##_16_in(c00, c01, c02, c03, c04, c05, c06, c07, \
+                  c08, c09, c10, c11, c12, c13, c14, c15, \
+                  c00c01, c02c03, c04c05, c06c07, c08c09, c10c11, c12c13, c14c15) \
+    memset(coeff, 0, sizeof(*coeff) * 16 * 4); \
+    SCALE_ROUND_4(c00, c01, c02, c03, vec_splat_s32(1), vec_splat_u32(1)) \
+    SCALE_ROUND_4(c04, c05, c06, c07, vec_splat_s32(1), vec_splat_u32(1)) \
+    SCALE_ROUND_4(c08, c09, c10, c11, vec_splat_s32(1), vec_splat_u32(1)) \
+    SCALE_ROUND_4(c12, c13, c14, c15, vec_splat_s32(1), vec_splat_u32(1)) \
+    TRANSPOSE4_I32(c00, c01, c02, c03); \
+    TRANSPOSE4_I32(c04, c05, c06, c07); \
+    TRANSPOSE4_I32(c08, c09, c10, c11); \
+    TRANSPOSE4_I32(c12, c13, c14, c15); \
+    type2##_4x4_out(c00, c01, c02, c03, \
+                    c04, c05, c06, c07, \
+                    c08, c09, c10, c11, \
+                    c12, c13, c14, c15, \
+                    c00c01, c02c03, c04c05, c06c07, \
+                    c08c09, c10c11, c12c13, c14c15); \
+    LOAD_DECLARE_4(dst, stride, l0, l1, l2, l3) \
+    APPLY_COEFF_16x4(l0, l1, l2, l3, \
+                     c00c01, c02c03, c04c05, c06c07, \
+                     c08c09, c10c11, c12c13, c14c15) \
+    STORE_16(dst, stride, l0, l1, l2, l3) \
+}
+
+inv_txfm_fn16x4(adst,     dct     )
+inv_txfm_fn16x4(dct,      adst    )
+inv_txfm_fn16x4(dct,      flipadst)
+inv_txfm_fn16x4(flipadst, dct     )
+inv_txfm_fn16x4(adst,     flipadst)
+inv_txfm_fn16x4(flipadst, adst    )
+inv_txfm_fn16x4(dct,      identity)
+inv_txfm_fn16x4(flipadst, identity)
+inv_txfm_fn16x4(adst,     identity)
+inv_txfm_fn16x4(identity, identity)
+inv_txfm_fn16x4(adst,     adst    )
+inv_txfm_fn16x4(flipadst, flipadst)
+
+#define inv_txfm_fn16x4_identity(type2) \
+void dav1d_inv_txfm_add_identity_##type2##_16x4_8bpc_pwr9(uint8_t *dst, const ptrdiff_t stride, \
+                                                          int16_t *const coeff, const int eob) \
+{ \
+    LOAD_DECLARE_2_I16(coeff, c00c01, c02c03) \
+    LOAD_DECLARE_2_I16(coeff+16, c04c05, c06c07) \
+    LOAD_DECLARE_2_I16(coeff+32, c08c09, c10c11) \
+    LOAD_DECLARE_2_I16(coeff+48, c12c13, c14c15) \
+    UNPACK_DECLARE_4_I16_I32(c00c01, c02c03, c00, c01, c02, c03) \
+    UNPACK_DECLARE_4_I16_I32(c04c05, c06c07, c04, c05, c06, c07) \
+    UNPACK_DECLARE_4_I16_I32(c08c09, c10c11, c08, c09, c10, c11) \
+    UNPACK_DECLARE_4_I16_I32(c12c13, c14c15, c12, c13, c14, c15) \
+    identity_16_in(c00, c01, c02, c03, c04, c05, c06, c07, \
+                  c08, c09, c10, c11, c12, c13, c14, c15, \
+                  c00c01, c02c03, c04c05, c06c07, c08c09, c10c11, c12c13, c14c15) \
+    memset(coeff, 0, sizeof(*coeff) * 16 * 4); \
+    SCALE_ROUND_4(c00, c01, c02, c03, vec_splat_s32(1), vec_splat_u32(1)) \
+    SCALE_ROUND_4(c04, c05, c06, c07, vec_splat_s32(1), vec_splat_u32(1)) \
+    SCALE_ROUND_4(c08, c09, c10, c11, vec_splat_s32(1), vec_splat_u32(1)) \
+    SCALE_ROUND_4(c12, c13, c14, c15, vec_splat_s32(1), vec_splat_u32(1)) \
+    CLIP16_I32_8(c00, c01, c02, c03, c04, c05, c06, c07, c00c01, c02c03, c04c05, c06c07) \
+    CLIP16_I32_8(c08, c09, c10, c11, c12, c13, c14, c15, c08c09, c10c11, c12c13, c14c15) \
+    TRANSPOSE4_I32(c00, c01, c02, c03); \
+    TRANSPOSE4_I32(c04, c05, c06, c07); \
+    TRANSPOSE4_I32(c08, c09, c10, c11); \
+    TRANSPOSE4_I32(c12, c13, c14, c15); \
+    type2##_4x4_out(c00, c01, c02, c03, \
+                    c04, c05, c06, c07, \
+                    c08, c09, c10, c11, \
+                    c12, c13, c14, c15, \
+                    c00c01, c02c03, c04c05, c06c07, \
+                    c08c09, c10c11, c12c13, c14c15); \
+    LOAD_DECLARE_4(dst, stride, l0, l1, l2, l3) \
+    APPLY_COEFF_16x4(l0, l1, l2, l3, \
+                     c00c01, c02c03, c04c05, c06c07, \
+                     c08c09, c10c11, c12c13, c14c15) \
+    STORE_16(dst, stride, l0, l1, l2, l3) \
+}
+
+inv_txfm_fn16x4_identity(dct)
+inv_txfm_fn16x4_identity(adst)
+inv_txfm_fn16x4_identity(flipadst)
+
+#endif // BITDEPTH
diff --git a/src/ppc/loopfilter_tmpl.c b/src/ppc/loopfilter_tmpl.c
index 4e658a7..107192f 100644
--- a/src/ppc/loopfilter_tmpl.c
+++ b/src/ppc/loopfilter_tmpl.c
@@ -342,8 +342,7 @@ static inline void store_h_8(u8x16 outa, u8x16 outb, uint8_t *dst, int stridea)
 
 static inline void
 loop_filter_h_4_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
-                const ptrdiff_t stridea, b32x4 apply
-                HIGHBD_DECL_SUFFIX)
+                const ptrdiff_t stridea, b32x4 apply)
 {
     dst -= 2;
     uint8_t *dst2 = dst;
@@ -428,8 +427,7 @@ loop_filter_h_4_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
 
 static inline void
 loop_filter_h_6_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
-                    const ptrdiff_t stridea, b32x4 apply, b32x4 m6
-                    HIGHBD_DECL_SUFFIX)
+                    const ptrdiff_t stridea, b32x4 apply, b32x4 m6)
 {
     uint8_t *dst2 = dst - 2;
     dst -= 3;
@@ -572,8 +570,7 @@ loop_filter_h_6_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
 
 static inline void
 loop_filter_h_8_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
-                    const ptrdiff_t stridea, b32x4 apply, b32x4 m8
-                    HIGHBD_DECL_SUFFIX)
+                    const ptrdiff_t stridea, b32x4 apply, b32x4 m8)
 {
     uint8_t *dst2 = dst - 3;
     dst -= 4;
@@ -718,8 +715,7 @@ loop_filter_h_8_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
 
 static inline void
 loop_filter_h_16_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
-                    const ptrdiff_t stridea, b32x4 apply, b32x4 m8, b32x4 m16
-                    HIGHBD_DECL_SUFFIX)
+                    const ptrdiff_t stridea, b32x4 apply, b32x4 m8, b32x4 m16)
 {
     uint8_t *dst2 = dst -6 ;
     dst -= 7;
@@ -960,8 +956,7 @@ loop_filter_h_16_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
 
 static inline void
 loop_filter_v_4_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
-                    const ptrdiff_t strideb, b32x4 apply
-                    HIGHBD_DECL_SUFFIX)
+                    const ptrdiff_t strideb, b32x4 apply)
 {
     uint8_t *p1d = dst + strideb * -2;
     uint8_t *p0d = dst + strideb * -1;
@@ -1007,8 +1002,7 @@ loop_filter_v_4_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
 
 static inline void
 loop_filter_v_6_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
-                    const ptrdiff_t strideb, b32x4 apply, b32x4 m6
-                    HIGHBD_DECL_SUFFIX)
+                    const ptrdiff_t strideb, b32x4 apply, b32x4 m6)
 {
     uint8_t *p2d = dst + strideb * -3;
     uint8_t *p1d = dst + strideb * -2;
@@ -1114,9 +1108,7 @@ loop_filter_v_6_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
 
 static inline void
 loop_filter_v_8_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
-                    const ptrdiff_t strideb, b32x4 apply, b32x4 m8
-                    HIGHBD_DECL_SUFFIX)
-
+                    const ptrdiff_t strideb, b32x4 apply, b32x4 m8)
 {
 
     uint8_t *p3d = dst + strideb * -4;
@@ -1216,9 +1208,7 @@ loop_filter_v_8_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
 
 static inline void
 loop_filter_v_16_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
-                    const ptrdiff_t strideb, b32x4 apply, b32x4 m8, b32x4 m16
-                    HIGHBD_DECL_SUFFIX)
-
+                    const ptrdiff_t strideb, b32x4 apply, b32x4 m8, b32x4 m16)
 {
 
     uint8_t *p6d = dst + strideb * -7;
@@ -1373,8 +1363,7 @@ loop_filter_v_16_all(uint8_t *dst, u8x16 E, u8x16 I, u8x16 H,
 void LPF(h_sb_y)(pixel *dst, const ptrdiff_t stride,
                  const uint32_t *const vmask,
                  const uint8_t (*l)[4], ptrdiff_t b4_stride,
-                 const Av1FilterLUT *lut, const int h
-                 HIGHBD_DECL_SUFFIX)
+                 const Av1FilterLUT *lut, const int h)
 {
     unsigned vm = vmask[0] | vmask[1] | vmask[2];
 
@@ -1449,11 +1438,11 @@ void LPF(h_sb_y)(pixel *dst, const ptrdiff_t stride,
         apply = vec_and(m4, apply);
 
         if (vec_any_ne(wd16, zero)) {
-            loop_filter_h_16_all(dst, E, I, H, PXSTRIDE(stride), apply, m8, m16 HIGHBD_TAIL_SUFFIX);
+            loop_filter_h_16_all(dst, E, I, H, PXSTRIDE(stride), apply, m8, m16);
         } else if (vec_any_ne(wd8, zero)) {
-            loop_filter_h_8_all(dst, E, I, H, PXSTRIDE(stride), apply, m8 HIGHBD_TAIL_SUFFIX);
+            loop_filter_h_8_all(dst, E, I, H, PXSTRIDE(stride), apply, m8);
         } else { // wd4 == 0 already tested
-            loop_filter_h_4_all(dst, E, I, H, PXSTRIDE(stride), apply HIGHBD_TAIL_SUFFIX);
+            loop_filter_h_4_all(dst, E, I, H, PXSTRIDE(stride), apply);
         }
     }
 }
@@ -1461,8 +1450,7 @@ void LPF(h_sb_y)(pixel *dst, const ptrdiff_t stride,
 void LPF(v_sb_y)(pixel *dst, const ptrdiff_t stride,
                  const uint32_t *const vmask,
                  const uint8_t (*l)[4], ptrdiff_t b4_stride,
-                 const Av1FilterLUT *lut, const int w
-                 HIGHBD_DECL_SUFFIX)
+                 const Av1FilterLUT *lut, const int w)
 {
     unsigned vm = vmask[0] | vmask[1] | vmask[2];
 
@@ -1530,11 +1518,11 @@ void LPF(v_sb_y)(pixel *dst, const ptrdiff_t stride,
         apply = vec_and(apply, m4);
 
         if (vec_any_ne(wd16, zero)) {
-            loop_filter_v_16_all(dst, E, I, H, PXSTRIDE(stride), apply, m8, m16 HIGHBD_TAIL_SUFFIX);
+            loop_filter_v_16_all(dst, E, I, H, PXSTRIDE(stride), apply, m8, m16);
         } else if (vec_any_ne(wd8, zero)) {
-            loop_filter_v_8_all(dst, E, I, H, PXSTRIDE(stride), apply, m8 HIGHBD_TAIL_SUFFIX);
+            loop_filter_v_8_all(dst, E, I, H, PXSTRIDE(stride), apply, m8);
         } else {
-            loop_filter_v_4_all(dst, E, I, H, PXSTRIDE(stride), apply HIGHBD_TAIL_SUFFIX);
+            loop_filter_v_4_all(dst, E, I, H, PXSTRIDE(stride), apply);
         }
 
     }
@@ -1543,8 +1531,7 @@ void LPF(v_sb_y)(pixel *dst, const ptrdiff_t stride,
 void LPF(h_sb_uv)(pixel *dst, const ptrdiff_t stride,
                   const uint32_t *const vmask,
                   const uint8_t (*l)[4], ptrdiff_t b4_stride,
-                  const Av1FilterLUT *lut, const int h
-                  HIGHBD_DECL_SUFFIX)
+                  const Av1FilterLUT *lut, const int h)
 {
     unsigned vm = vmask[0] | vmask[1];
     u32x4 vm0 = vec_splats(vm);
@@ -1614,10 +1601,10 @@ void LPF(h_sb_uv)(pixel *dst, const ptrdiff_t stride,
         apply = vec_and(m4, apply);
 
         if (vec_any_ne(wd6, zero)) {
-            loop_filter_h_6_all(dst, E, I, H, PXSTRIDE(stride), apply, m6 HIGHBD_TAIL_SUFFIX);
+            loop_filter_h_6_all(dst, E, I, H, PXSTRIDE(stride), apply, m6);
             // loop_filter_h_8
         } else { // wd4 == 0 already tested
-            loop_filter_h_4_all(dst, E, I, H, PXSTRIDE(stride), apply HIGHBD_TAIL_SUFFIX);
+            loop_filter_h_4_all(dst, E, I, H, PXSTRIDE(stride), apply);
 
             // loop_filter_h_4
         }
@@ -1628,8 +1615,7 @@ void LPF(h_sb_uv)(pixel *dst, const ptrdiff_t stride,
 void LPF(v_sb_uv)(pixel *dst, const ptrdiff_t stride,
                   const uint32_t *const vmask,
                   const uint8_t (*l)[4], ptrdiff_t b4_stride,
-                  const Av1FilterLUT *lut, const int w
-                  HIGHBD_DECL_SUFFIX)
+                  const Av1FilterLUT *lut, const int w)
 {
     unsigned vm = vmask[0] | vmask[1];
 
@@ -1694,9 +1680,9 @@ void LPF(v_sb_uv)(pixel *dst, const ptrdiff_t stride,
         apply = vec_and(apply, m4);
 
         if (vec_any_ne(wd6, zero)) {
-            loop_filter_v_6_all(dst, E, I, H, PXSTRIDE(stride), apply, m6 HIGHBD_TAIL_SUFFIX);
+            loop_filter_v_6_all(dst, E, I, H, PXSTRIDE(stride), apply, m6);
         } else {
-            loop_filter_v_4_all(dst, E, I, H, PXSTRIDE(stride), apply HIGHBD_TAIL_SUFFIX);
+            loop_filter_v_4_all(dst, E, I, H, PXSTRIDE(stride), apply);
         }
     }
 }
diff --git a/src/ppc/looprestoration.h b/src/ppc/looprestoration.h
index 3fe1631..614234a 100644
--- a/src/ppc/looprestoration.h
+++ b/src/ppc/looprestoration.h
@@ -35,7 +35,7 @@ void dav1d_wiener_filter_vsx(uint8_t *p, const ptrdiff_t stride,
                              const uint8_t *lpf,
                              const int w, const int h,
                              const LooprestorationParams *const params,
-                             const enum LrEdgeFlags edges HIGHBD_DECL_SUFFIX);
+                             const enum LrEdgeFlags edges);
 
 static ALWAYS_INLINE void loop_restoration_dsp_init_ppc(Dav1dLoopRestorationDSPContext *const c, const int bpc) {
     const unsigned flags = dav1d_get_cpu_flags();
diff --git a/src/ppc/looprestoration_tmpl.c b/src/ppc/looprestoration_tmpl.c
index c0c64e1..76c1d07 100644
--- a/src/ppc/looprestoration_tmpl.c
+++ b/src/ppc/looprestoration_tmpl.c
@@ -305,7 +305,7 @@ void dav1d_wiener_filter_vsx(uint8_t *p, const ptrdiff_t stride,
                              const uint8_t *lpf,
                              const int w, const int h,
                              const LooprestorationParams *const params,
-                             const enum LrEdgeFlags edges HIGHBD_DECL_SUFFIX)
+                             const enum LrEdgeFlags edges)
 {
     const int16_t (*const filter)[8] = params->filter;
 
diff --git a/src/ppc/utils.h b/src/ppc/utils.h
new file mode 100644
index 0000000..3b0a544
--- /dev/null
+++ b/src/ppc/utils.h
@@ -0,0 +1,105 @@
+/*
+ * Copyright © 2024, VideoLAN and dav1d authors
+ * Copyright © 2024, Luca Barbato
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef DAV1D_SRC_PPC_UTILS_H
+#define DAV1D_SRC_PPC_UTILS_H
+
+#include "src/ppc/dav1d_types.h"
+
+#define assert_eq(a, b) \
+    if ((a) != (b)) \
+        printf("%d: %d vs %d\n", __LINE__, a, b); \
+    assert((a) == (b));
+
+#define MERGE_I32(a, b, h, l) \
+{ \
+    h = vec_mergeh(a, b); \
+    l = vec_mergel(a, b); \
+}
+
+#define DECLARE_MERGE_I32(a, b, h, l) \
+    i32x4 h, l; \
+    MERGE_I32(a, b, h, l)
+
+
+// Transpose a 4x4 matrix of i32x4 vectors
+#define TRANSPOSE4_I32(c0, c1, c2, c3) \
+{ \
+    DECLARE_MERGE_I32(c0, c2, m02h, m02l) \
+    DECLARE_MERGE_I32(c1, c3, m13h, m13l) \
+\
+    MERGE_I32(m02h, m13h, c0, c1) \
+    MERGE_I32(m02l, m13l, c2, c3) \
+}
+
+// Transpose a 8x8 matrix of i32x4 vectors
+#define TRANSPOSE8_I32(c0, c1, c2, c3, c4, c5, c6, c7, \
+                       c8, c9, cA, cB, cC, cD, cE, cF) \
+{ \
+    DECLARE_MERGE_I32(c0, c2, m02h, m02l) \
+    DECLARE_MERGE_I32(c1, c3, m13h, m13l) \
+    DECLARE_MERGE_I32(c4, c6, m46h, m46l) \
+    DECLARE_MERGE_I32(c5, c7, m57h, m57l) \
+    DECLARE_MERGE_I32(c8, cA, m8Ah, m8Al) \
+    DECLARE_MERGE_I32(c9, cB, m9Bh, m9Bl) \
+    DECLARE_MERGE_I32(cC, cE, mCEh, mCEl) \
+    DECLARE_MERGE_I32(cD, cF, mDFh, mDFl) \
+\
+    MERGE_I32(m02h, m13h, c0, c1) \
+    MERGE_I32(m02l, m13l, c2, c3) \
+    MERGE_I32(m46h, m57h, c8, c9) \
+    MERGE_I32(m46l, m57l, cA, cB) \
+    MERGE_I32(m8Ah, m9Bh, c4, c5) \
+    MERGE_I32(m8Al, m9Bl, c6, c7) \
+    MERGE_I32(mCEh, mDFh, cC, cD) \
+    MERGE_I32(mCEl, mDFl, cE, cF) \
+}
+
+// Transpose a 4x16 matrix of i32x4 vectors
+#define TRANSPOSE4x16_I32(c0, c1, c2, c3, c4, c5, c6, c7, \
+                          c8, c9, cA, cB, cC, cD, cE, cF) \
+{ \
+    DECLARE_MERGE_I32(c0, c2, m02h, m02l) \
+    DECLARE_MERGE_I32(c1, c3, m13h, m13l) \
+    DECLARE_MERGE_I32(c4, c6, m46h, m46l) \
+    DECLARE_MERGE_I32(c5, c7, m57h, m57l) \
+    DECLARE_MERGE_I32(c8, cA, m8Ah, m8Al) \
+    DECLARE_MERGE_I32(c9, cB, m9Bh, m9Bl) \
+    DECLARE_MERGE_I32(cC, cE, mCEh, mCEl) \
+    DECLARE_MERGE_I32(cD, cF, mDFh, mDFl) \
+\
+    MERGE_I32(m02h, m13h, c0, c1) \
+    MERGE_I32(m02l, m13l, c2, c3) \
+    MERGE_I32(m46h, m57h, c4, c5) \
+    MERGE_I32(m46l, m57l, c6, c7) \
+    MERGE_I32(m8Ah, m9Bh, c8, c9) \
+    MERGE_I32(m8Al, m9Bl, cA, cB) \
+    MERGE_I32(mCEh, mDFh, cC, cD) \
+    MERGE_I32(mCEl, mDFl, cE, cF) \
+}
+
+#endif // DAV1D_SRC_PPC_UTILS_H
diff --git a/src/recon_tmpl.c b/src/recon_tmpl.c
index 0afd06c..426fa40 100644
--- a/src/recon_tmpl.c
+++ b/src/recon_tmpl.c
@@ -402,7 +402,8 @@ static int decode_coefs(Dav1dTaskContext *const t,
 
     // find end-of-block (eob)
     int eob_bin;
-    const int tx2dszctx = imin(t_dim->lw, TX_32X32) + imin(t_dim->lh, TX_32X32);
+    const int slw = imin(t_dim->lw, TX_32X32), slh = imin(t_dim->lh, TX_32X32);
+    const int tx2dszctx = slw + slh;
     const enum TxClass tx_class = dav1d_tx_type_class[*txtp];
     const int is_1d = tx_class != TX_CLASS_2D;
     switch (tx2dszctx) {
@@ -449,10 +450,9 @@ static int decode_coefs(Dav1dTaskContext *const t,
     if (eob) {
         uint16_t (*const lo_cdf)[4] = ts->cdf.coef.base_tok[t_dim->ctx][chroma];
         uint8_t *const levels = t->scratch.levels; // bits 0-5: tok, 6-7: lo_tok
-        const int sw = imin(t_dim->w, 8), sh = imin(t_dim->h, 8);
 
         /* eob */
-        unsigned ctx = 1 + (eob > sw * sh * 2) + (eob > sw * sh * 4);
+        unsigned ctx = 1 + (eob > 2 << tx2dszctx) + (eob > 4 << tx2dszctx);
         int eob_tok = dav1d_msac_decode_symbol_adapt4(&ts->msac, eob_cdf[ctx], 2);
         int tok = eob_tok + 1;
         int level_tok = tok * 0x41;
@@ -460,6 +460,7 @@ static int decode_coefs(Dav1dTaskContext *const t,
 
 #define DECODE_COEFS_CLASS(tx_class) \
         unsigned x, y; \
+        uint8_t *level; \
         if (tx_class == TX_CLASS_2D) \
             rc = scan[eob], x = rc >> shift, y = rc & mask; \
         else if (tx_class == TX_CLASS_H) \
@@ -480,7 +481,11 @@ static int decode_coefs(Dav1dTaskContext *const t,
                        ts->msac.rng); \
         } \
         cf[rc] = tok << 11; \
-        levels[x * stride + y] = (uint8_t) level_tok; \
+        if (TX_CLASS_2D) \
+            level = levels + rc; \
+        else \
+            level = levels + x * stride + y; \
+        *level = (uint8_t) level_tok; \
         for (int i = eob - 1; i > 0; i--) { /* ac */ \
             unsigned rc_i; \
             if (tx_class == TX_CLASS_2D) \
@@ -490,7 +495,10 @@ static int decode_coefs(Dav1dTaskContext *const t,
             else /* tx_class == TX_CLASS_V */ \
                 x = i & mask, y = i >> shift, rc_i = (x << shift2) | y; \
             assert(x < 32 && y < 32); \
-            uint8_t *const level = levels + x * stride + y; \
+            if (TX_CLASS_2D) \
+                level = levels + rc; \
+            else \
+                level = levels + x * stride + y; \
             ctx = get_lo_ctx(level, tx_class, &mag, lo_ctx_offsets, x, y, stride); \
             if (tx_class == TX_CLASS_2D) \
                 y |= x; \
@@ -547,26 +555,26 @@ static int decode_coefs(Dav1dTaskContext *const t,
             const uint8_t (*const lo_ctx_offsets)[5] =
                 dav1d_lo_ctx_offsets[nonsquare_tx + (tx & nonsquare_tx)];
             scan = dav1d_scans[tx];
-            const ptrdiff_t stride = 4 * sh;
-            const unsigned shift = t_dim->lh < 4 ? t_dim->lh + 2 : 5, shift2 = 0;
-            const unsigned mask = 4 * sh - 1;
-            memset(levels, 0, stride * (4 * sw + 2));
+            const ptrdiff_t stride = 4 << slh;
+            const unsigned shift = slh + 2, shift2 = 0;
+            const unsigned mask = (4 << slh) - 1;
+            memset(levels, 0, stride * ((4 << slw) + 2));
             DECODE_COEFS_CLASS(TX_CLASS_2D);
         }
         case TX_CLASS_H: {
             const uint8_t (*const lo_ctx_offsets)[5] = NULL;
             const ptrdiff_t stride = 16;
-            const unsigned shift = t_dim->lh + 2, shift2 = 0;
-            const unsigned mask = 4 * sh - 1;
-            memset(levels, 0, stride * (4 * sh + 2));
+            const unsigned shift = slh + 2, shift2 = 0;
+            const unsigned mask = (4 << slh) - 1;
+            memset(levels, 0, stride * ((4 << slh) + 2));
             DECODE_COEFS_CLASS(TX_CLASS_H);
         }
         case TX_CLASS_V: {
             const uint8_t (*const lo_ctx_offsets)[5] = NULL;
             const ptrdiff_t stride = 16;
-            const unsigned shift = t_dim->lw + 2, shift2 = t_dim->lh + 2;
-            const unsigned mask = 4 * sw - 1;
-            memset(levels, 0, stride * (4 * sw + 2));
+            const unsigned shift = slw + 2, shift2 = slh + 2;
+            const unsigned mask = (4 << slw) - 1;
+            memset(levels, 0, stride * ((4 << slw) + 2));
             DECODE_COEFS_CLASS(TX_CLASS_V);
         }
 #undef DECODE_COEFS_CLASS
@@ -785,21 +793,15 @@ static void read_coef_tree(Dav1dTaskContext *const t,
             if (DEBUG_BLOCK_INFO)
                 printf("Post-y-cf-blk[tx=%d,txtp=%d,eob=%d]: r=%d\n",
                        ytx, txtp, eob, ts->msac.rng);
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-            rep_macro(type, t->dir lcoef, off, mul * cf_ctx)
-#define default_memset(dir, diridx, off, sz) \
-            memset(&t->dir lcoef[off], cf_ctx, sz)
-            case_set_upto16_with_default(imin(txh, f->bh - t->by), l., 1, by4);
-            case_set_upto16_with_default(imin(txw, f->bw - t->bx), a->, 0, bx4);
-#undef default_memset
-#undef set_ctx
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
+            dav1d_memset_likely_pow2(&t->a->lcoef[bx4], cf_ctx, imin(txw, f->bw - t->bx));
+            dav1d_memset_likely_pow2(&t->l.lcoef[by4], cf_ctx, imin(txh, f->bh - t->by));
+#define set_ctx(rep_macro) \
             for (int y = 0; y < txh; y++) { \
-                rep_macro(type, txtp_map, 0, mul * txtp); \
+                rep_macro(txtp_map, 0, txtp); \
                 txtp_map += 32; \
             }
             uint8_t *txtp_map = &t->scratch.txtp_map[by4 * 32 + bx4];
-            case_set_upto16(txw,,,);
+            case_set_upto16(t_dim->lw);
 #undef set_ctx
             if (t->frame_thread.pass == 1)
                 *ts->frame_thread[1].cbi++ = eob * (1 << 5) + txtp;
@@ -838,18 +840,16 @@ void bytefn(dav1d_read_coef_blocks)(Dav1dTaskContext *const t,
                            (bh4 > ss_ver || t->by & 1);
 
     if (b->skip) {
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-        rep_macro(type, t->dir lcoef, off, mul * 0x40)
-        case_set(bh4, l., 1, by4);
-        case_set(bw4, a->, 0, bx4);
-#undef set_ctx
+        BlockContext *const a = t->a;
+        dav1d_memset_pow2[b_dim[2]](&a->lcoef[bx4], 0x40);
+        dav1d_memset_pow2[b_dim[3]](&t->l.lcoef[by4], 0x40);
         if (has_chroma) {
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-            rep_macro(type, t->dir ccoef[0], off, mul * 0x40); \
-            rep_macro(type, t->dir ccoef[1], off, mul * 0x40)
-            case_set(cbh4, l., 1, cby4);
-            case_set(cbw4, a->, 0, cbx4);
-#undef set_ctx
+            dav1d_memset_pow2_fn memset_cw = dav1d_memset_pow2[ulog2(cbw4)];
+            dav1d_memset_pow2_fn memset_ch = dav1d_memset_pow2[ulog2(cbh4)];
+            memset_cw(&a->ccoef[0][cbx4], 0x40);
+            memset_cw(&a->ccoef[1][cbx4], 0x40);
+            memset_ch(&t->l.ccoef[0][cby4], 0x40);
+            memset_ch(&t->l.ccoef[1][cby4], 0x40);
         }
         return;
     }
@@ -890,16 +890,8 @@ void bytefn(dav1d_read_coef_blocks)(Dav1dTaskContext *const t,
                                    b->tx, txtp, eob, ts->msac.rng);
                         *ts->frame_thread[1].cbi++ = eob * (1 << 5) + txtp;
                         ts->frame_thread[1].cf += imin(t_dim->w, 8) * imin(t_dim->h, 8) * 16;
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-                        rep_macro(type, t->dir lcoef, off, mul * cf_ctx)
-#define default_memset(dir, diridx, off, sz) \
-                        memset(&t->dir lcoef[off], cf_ctx, sz)
-                        case_set_upto16_with_default(imin(t_dim->h, f->bh - t->by),
-                                                     l., 1, by4 + y);
-                        case_set_upto16_with_default(imin(t_dim->w, f->bw - t->bx),
-                                                     a->, 0, bx4 + x);
-#undef default_memset
-#undef set_ctx
+                        dav1d_memset_likely_pow2(&t->a->lcoef[bx4 + x], cf_ctx, imin(t_dim->w, f->bw - t->bx));
+                        dav1d_memset_likely_pow2(&t->l.lcoef[by4 + y], cf_ctx, imin(t_dim->h, f->bh - t->by));
                     }
                 }
                 t->bx -= x;
@@ -933,18 +925,10 @@ void bytefn(dav1d_read_coef_blocks)(Dav1dTaskContext *const t,
                                    pl, b->uvtx, txtp, eob, ts->msac.rng);
                         *ts->frame_thread[1].cbi++ = eob * (1 << 5) + txtp;
                         ts->frame_thread[1].cf += uv_t_dim->w * uv_t_dim->h * 16;
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-                        rep_macro(type, t->dir ccoef[pl], off, mul * cf_ctx)
-#define default_memset(dir, diridx, off, sz) \
-                        memset(&t->dir ccoef[pl][off], cf_ctx, sz)
-                        case_set_upto16_with_default( \
-                                 imin(uv_t_dim->h, (f->bh - t->by + ss_ver) >> ss_ver),
-                                 l., 1, cby4 + y);
-                        case_set_upto16_with_default( \
-                                 imin(uv_t_dim->w, (f->bw - t->bx + ss_hor) >> ss_hor),
-                                 a->, 0, cbx4 + x);
-#undef default_memset
-#undef set_ctx
+                        int ctw = imin(uv_t_dim->w, (f->bw - t->bx + ss_hor) >> ss_hor);
+                        int cth = imin(uv_t_dim->h, (f->bh - t->by + ss_ver) >> ss_ver);
+                        dav1d_memset_likely_pow2(&t->a->ccoef[pl][cbx4 + x], cf_ctx, ctw);
+                        dav1d_memset_likely_pow2(&t->l.ccoef[pl][cby4 + y], cf_ctx, cth);
                     }
                     t->bx -= x << ss_hor;
                 }
@@ -1329,16 +1313,8 @@ void bytefn(dav1d_recon_b_intra)(Dav1dTaskContext *const t, const enum BlockSize
                             if (DEBUG_BLOCK_INFO)
                                 printf("Post-y-cf-blk[tx=%d,txtp=%d,eob=%d]: r=%d\n",
                                        b->tx, txtp, eob, ts->msac.rng);
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-                            rep_macro(type, t->dir lcoef, off, mul * cf_ctx)
-#define default_memset(dir, diridx, off, sz) \
-                            memset(&t->dir lcoef[off], cf_ctx, sz)
-                            case_set_upto16_with_default(imin(t_dim->h, f->bh - t->by), \
-                                                         l., 1, by4 + y);
-                            case_set_upto16_with_default(imin(t_dim->w, f->bw - t->bx), \
-                                                         a->, 0, bx4 + x);
-#undef default_memset
-#undef set_ctx
+                            dav1d_memset_likely_pow2(&t->a->lcoef[bx4 + x], cf_ctx, imin(t_dim->w, f->bw - t->bx));
+                            dav1d_memset_likely_pow2(&t->l.lcoef[by4 + y], cf_ctx, imin(t_dim->h, f->bh - t->by));
                         }
                         if (eob >= 0) {
                             if (DEBUG_BLOCK_INFO && DEBUG_B_PIXELS)
@@ -1353,11 +1329,8 @@ void bytefn(dav1d_recon_b_intra)(Dav1dTaskContext *const t, const enum BlockSize
                                          t_dim->w * 4, t_dim->h * 4, "recon");
                         }
                     } else if (!t->frame_thread.pass) {
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-                        rep_macro(type, t->dir lcoef, off, mul * 0x40)
-                        case_set_upto16(t_dim->h, l., 1, by4 + y);
-                        case_set_upto16(t_dim->w, a->, 0, bx4 + x);
-#undef set_ctx
+                        dav1d_memset_pow2[t_dim->lw](&t->a->lcoef[bx4 + x], 0x40);
+                        dav1d_memset_pow2[t_dim->lh](&t->l.lcoef[by4 + y], 0x40);
                     }
                     dst += 4 * t_dim->w;
                 }
@@ -1554,18 +1527,10 @@ void bytefn(dav1d_recon_b_intra)(Dav1dTaskContext *const t, const enum BlockSize
                                     printf("Post-uv-cf-blk[pl=%d,tx=%d,"
                                            "txtp=%d,eob=%d]: r=%d [x=%d,cbx4=%d]\n",
                                            pl, b->uvtx, txtp, eob, ts->msac.rng, x, cbx4);
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-                                rep_macro(type, t->dir ccoef[pl], off, mul * cf_ctx)
-#define default_memset(dir, diridx, off, sz) \
-                                memset(&t->dir ccoef[pl][off], cf_ctx, sz)
-                                case_set_upto16_with_default( \
-                                         imin(uv_t_dim->h, (f->bh - t->by + ss_ver) >> ss_ver),
-                                         l., 1, cby4 + y);
-                                case_set_upto16_with_default( \
-                                         imin(uv_t_dim->w, (f->bw - t->bx + ss_hor) >> ss_hor),
-                                         a->, 0, cbx4 + x);
-#undef default_memset
-#undef set_ctx
+                                int ctw = imin(uv_t_dim->w, (f->bw - t->bx + ss_hor) >> ss_hor);
+                                int cth = imin(uv_t_dim->h, (f->bh - t->by + ss_ver) >> ss_ver);
+                                dav1d_memset_likely_pow2(&t->a->ccoef[pl][cbx4 + x], cf_ctx, ctw);
+                                dav1d_memset_likely_pow2(&t->l.ccoef[pl][cby4 + y], cf_ctx, cth);
                             }
                             if (eob >= 0) {
                                 if (DEBUG_BLOCK_INFO && DEBUG_B_PIXELS)
@@ -1579,11 +1544,8 @@ void bytefn(dav1d_recon_b_intra)(Dav1dTaskContext *const t, const enum BlockSize
                                              uv_t_dim->h * 4, "recon");
                             }
                         } else if (!t->frame_thread.pass) {
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-                            rep_macro(type, t->dir ccoef[pl], off, mul * 0x40)
-                            case_set_upto16(uv_t_dim->h, l., 1, cby4 + y);
-                            case_set_upto16(uv_t_dim->w, a->, 0, cbx4 + x);
-#undef set_ctx
+                            dav1d_memset_pow2[uv_t_dim->lw](&t->a->ccoef[pl][cbx4 + x], 0x40);
+                            dav1d_memset_pow2[uv_t_dim->lh](&t->l.ccoef[pl][cby4 + y], 0x40);
                         }
                         dst += uv_t_dim->w * 4;
                     }
@@ -1921,18 +1883,16 @@ int bytefn(dav1d_recon_b_inter)(Dav1dTaskContext *const t, const enum BlockSize
 
     if (b->skip) {
         // reset coef contexts
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-        rep_macro(type, t->dir lcoef, off, mul * 0x40)
-        case_set(bh4, l., 1, by4);
-        case_set(bw4, a->, 0, bx4);
-#undef set_ctx
+        BlockContext *const a = t->a;
+        dav1d_memset_pow2[b_dim[2]](&a->lcoef[bx4], 0x40);
+        dav1d_memset_pow2[b_dim[3]](&t->l.lcoef[by4], 0x40);
         if (has_chroma) {
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-            rep_macro(type, t->dir ccoef[0], off, mul * 0x40); \
-            rep_macro(type, t->dir ccoef[1], off, mul * 0x40)
-            case_set(cbh4, l., 1, cby4);
-            case_set(cbw4, a->, 0, cbx4);
-#undef set_ctx
+            dav1d_memset_pow2_fn memset_cw = dav1d_memset_pow2[ulog2(cbw4)];
+            dav1d_memset_pow2_fn memset_ch = dav1d_memset_pow2[ulog2(cbh4)];
+            memset_cw(&a->ccoef[0][cbx4], 0x40);
+            memset_cw(&a->ccoef[1][cbx4], 0x40);
+            memset_ch(&t->l.ccoef[0][cby4], 0x40);
+            memset_ch(&t->l.ccoef[1][cby4], 0x40);
         }
         return 0;
     }
@@ -1998,18 +1958,10 @@ int bytefn(dav1d_recon_b_inter)(Dav1dTaskContext *const t, const enum BlockSize
                                 printf("Post-uv-cf-blk[pl=%d,tx=%d,"
                                        "txtp=%d,eob=%d]: r=%d\n",
                                        pl, b->uvtx, txtp, eob, ts->msac.rng);
-#define set_ctx(type, dir, diridx, off, mul, rep_macro) \
-                            rep_macro(type, t->dir ccoef[pl], off, mul * cf_ctx)
-#define default_memset(dir, diridx, off, sz) \
-                            memset(&t->dir ccoef[pl][off], cf_ctx, sz)
-                            case_set_upto16_with_default( \
-                                     imin(uvtx->h, (f->bh - t->by + ss_ver) >> ss_ver),
-                                     l., 1, cby4 + y);
-                            case_set_upto16_with_default( \
-                                     imin(uvtx->w, (f->bw - t->bx + ss_hor) >> ss_hor),
-                                     a->, 0, cbx4 + x);
-#undef default_memset
-#undef set_ctx
+                            int ctw = imin(uvtx->w, (f->bw - t->bx + ss_hor) >> ss_hor);
+                            int cth = imin(uvtx->h, (f->bh - t->by + ss_ver) >> ss_ver);
+                            dav1d_memset_likely_pow2(&t->a->ccoef[pl][cbx4 + x], cf_ctx, ctw);
+                            dav1d_memset_likely_pow2(&t->l.ccoef[pl][cby4 + y], cf_ctx, cth);
                         }
                         if (eob >= 0) {
                             if (DEBUG_BLOCK_INFO && DEBUG_B_PIXELS)
diff --git a/src/refmvs.c b/src/refmvs.c
index 1da024b..40cc4ef 100644
--- a/src/refmvs.c
+++ b/src/refmvs.c
@@ -657,19 +657,19 @@ void dav1d_refmvs_tile_sbrow_init(refmvs_tile *const rt, const refmvs_frame *con
 {
     if (rf->n_tile_threads == 1) tile_row_idx = 0;
     rt->rp_proj = &rf->rp_proj[16 * rf->rp_stride * tile_row_idx];
-    const int uses_2pass = rf->n_tile_threads > 1 && rf->n_frame_threads > 1;
-    const ptrdiff_t pass_off = (uses_2pass && pass == 2) ?
-        35 * rf->r_stride * rf->n_tile_rows : 0;
-    refmvs_block *r = &rf->r[35 * rf->r_stride * tile_row_idx + pass_off];
+    const ptrdiff_t r_stride = rf->rp_stride * 2;
+    const ptrdiff_t pass_off = (rf->n_frame_threads > 1 && pass == 2) ?
+        35 * 2 * rf->n_blocks : 0;
+    refmvs_block *r = &rf->r[35 * r_stride * tile_row_idx + pass_off];
     const int sbsz = rf->sbsz;
     const int off = (sbsz * sby) & 16;
-    for (int i = 0; i < sbsz; i++, r += rf->r_stride)
+    for (int i = 0; i < sbsz; i++, r += r_stride)
         rt->r[off + 5 + i] = r;
     rt->r[off + 0] = r;
-    r += rf->r_stride;
+    r += r_stride;
     rt->r[off + 1] = NULL;
     rt->r[off + 2] = r;
-    r += rf->r_stride;
+    r += r_stride;
     rt->r[off + 3] = NULL;
     rt->r[off + 4] = r;
     if (sby & 1) {
@@ -805,37 +805,37 @@ int dav1d_refmvs_init_frame(refmvs_frame *const rf,
                             /*const*/ refmvs_temporal_block *const rp_ref[7],
                             const int n_tile_threads, const int n_frame_threads)
 {
+    const int rp_stride = ((frm_hdr->width[0] + 127) & ~127) >> 3;
+    const int n_tile_rows = n_tile_threads > 1 ? frm_hdr->tiling.rows : 1;
+    const int n_blocks = rp_stride * n_tile_rows;
+
     rf->sbsz = 16 << seq_hdr->sb128;
     rf->frm_hdr = frm_hdr;
     rf->iw8 = (frm_hdr->width[0] + 7) >> 3;
     rf->ih8 = (frm_hdr->height + 7) >> 3;
     rf->iw4 = rf->iw8 << 1;
     rf->ih4 = rf->ih8 << 1;
+    rf->rp = rp;
+    rf->rp_stride = rp_stride;
+    rf->n_tile_threads = n_tile_threads;
+    rf->n_frame_threads = n_frame_threads;
 
-    const ptrdiff_t r_stride = ((frm_hdr->width[0] + 127) & ~127) >> 2;
-    const int n_tile_rows = n_tile_threads > 1 ? frm_hdr->tiling.rows : 1;
-    if (r_stride != rf->r_stride || n_tile_rows != rf->n_tile_rows) {
-        if (rf->r) dav1d_freep_aligned(&rf->r);
-        const int uses_2pass = n_tile_threads > 1 && n_frame_threads > 1;
-        /* sizeof(refmvs_block) == 12 but it's accessed using 16-byte loads in asm,
-         * so add 4 bytes of padding to avoid buffer overreads. */
-        rf->r = dav1d_alloc_aligned(ALLOC_REFMVS, sizeof(*rf->r) * 35 * r_stride * n_tile_rows * (1 + uses_2pass) + 4, 64);
-        if (!rf->r) return DAV1D_ERR(ENOMEM);
-        rf->r_stride = r_stride;
-    }
+    if (n_blocks != rf->n_blocks) {
+        const size_t r_sz = sizeof(*rf->r) * 35 * 2 * n_blocks * (1 + (n_frame_threads > 1));
+        const size_t rp_proj_sz = sizeof(*rf->rp_proj) * 16 * n_blocks;
+        /* Note that sizeof(*rf->r) == 12, but it's accessed using 16-byte unaligned
+         * loads in save_tmvs() asm which can overread 4 bytes into rp_proj. */
+        dav1d_free_aligned(rf->r);
+        rf->r = dav1d_alloc_aligned(ALLOC_REFMVS, r_sz + rp_proj_sz, 64);
+        if (!rf->r) {
+            rf->n_blocks = 0;
+            return DAV1D_ERR(ENOMEM);
+        }
 
-    const ptrdiff_t rp_stride = r_stride >> 1;
-    if (rp_stride != rf->rp_stride || n_tile_rows != rf->n_tile_rows) {
-        if (rf->rp_proj) dav1d_freep_aligned(&rf->rp_proj);
-        rf->rp_proj = dav1d_alloc_aligned(ALLOC_REFMVS, sizeof(*rf->rp_proj) * 16 * rp_stride * n_tile_rows, 64);
-        if (!rf->rp_proj) return DAV1D_ERR(ENOMEM);
-        rf->rp_stride = rp_stride;
+        rf->rp_proj = (refmvs_temporal_block*)((uintptr_t)rf->r + r_sz);
+        rf->n_blocks = n_blocks;
     }
-    rf->n_tile_rows = n_tile_rows;
-    rf->n_tile_threads = n_tile_threads;
-    rf->n_frame_threads = n_frame_threads;
-    rf->rp = rp;
-    rf->rp_ref = rp_ref;
+
     const unsigned poc = frm_hdr->frame_offset;
     for (int i = 0; i < 7; i++) {
         const int poc_diff = get_poc_diff(seq_hdr->order_hint_n_bits,
@@ -848,6 +848,7 @@ int dav1d_refmvs_init_frame(refmvs_frame *const rf,
 
     // temporal MV setup
     rf->n_mfmvs = 0;
+    rf->rp_ref = rp_ref;
     if (frm_hdr->use_ref_frame_mvs && seq_hdr->order_hint_n_bits) {
         int total = 2;
         if (rp_ref[0] && ref_ref_poc[0][6] != ref_poc[3] /* alt-of-last != gold */) {
@@ -896,18 +897,6 @@ int dav1d_refmvs_init_frame(refmvs_frame *const rf,
     return 0;
 }
 
-void dav1d_refmvs_init(refmvs_frame *const rf) {
-    rf->r = NULL;
-    rf->r_stride = 0;
-    rf->rp_proj = NULL;
-    rf->rp_stride = 0;
-}
-
-void dav1d_refmvs_clear(refmvs_frame *const rf) {
-    if (rf->r) dav1d_freep_aligned(&rf->r);
-    if (rf->rp_proj) dav1d_freep_aligned(&rf->rp_proj);
-}
-
 static void splat_mv_c(refmvs_block **rr, const refmvs_block *const rmv,
                        const int bx4, const int bw4, int bh4)
 {
diff --git a/src/refmvs.h b/src/refmvs.h
index d63874d..2c42984 100644
--- a/src/refmvs.h
+++ b/src/refmvs.h
@@ -43,22 +43,26 @@ PACKED(typedef struct refmvs_temporal_block {
     mv mv;
     int8_t ref;
 }) refmvs_temporal_block;
+CHECK_SIZE(refmvs_temporal_block, 5);
 
-typedef union refmvs_refpair {
+PACKED(typedef union refmvs_refpair {
     int8_t ref[2]; // [0] = 0: intra=1, [1] = -1: comp=0
     uint16_t pair;
-} refmvs_refpair;
+}) ALIGN(refmvs_refpair, 2);
+CHECK_SIZE(refmvs_refpair, 2);
 
 typedef union refmvs_mvpair {
     mv mv[2];
     uint64_t n;
 } refmvs_mvpair;
+CHECK_SIZE(refmvs_mvpair, 8);
 
 PACKED(typedef struct refmvs_block {
     refmvs_mvpair mv;
     refmvs_refpair ref;
     uint8_t bs, mf; // 1 = globalmv+affine, 2 = newmv
 }) ALIGN(refmvs_block, 4);
+CHECK_SIZE(refmvs_block, 12);
 
 typedef struct refmvs_frame {
     const Dav1dFrameHeader *frm_hdr;
@@ -72,14 +76,14 @@ typedef struct refmvs_frame {
     int mfmv_ref2ref[3][7];
     int n_mfmvs;
 
+    int n_blocks;
     refmvs_temporal_block *rp;
     /*const*/ refmvs_temporal_block *const *rp_ref;
     refmvs_temporal_block *rp_proj;
     ptrdiff_t rp_stride;
 
     refmvs_block *r; // 35 x r_stride memory
-    ptrdiff_t r_stride;
-    int n_tile_rows, n_tile_threads, n_frame_threads;
+    int n_tile_threads, n_frame_threads;
 } refmvs_frame;
 
 typedef struct refmvs_tile {
@@ -121,10 +125,6 @@ typedef struct Dav1dRefmvsDSPContext {
     splat_mv_fn splat_mv;
 } Dav1dRefmvsDSPContext;
 
-// call once per frame thread
-void dav1d_refmvs_init(refmvs_frame *rf);
-void dav1d_refmvs_clear(refmvs_frame *rf);
-
 // call once per frame
 int dav1d_refmvs_init_frame(refmvs_frame *rf,
                             const Dav1dSequenceHeader *seq_hdr,
diff --git a/src/riscv/64/cdef.S b/src/riscv/64/cdef.S
new file mode 100644
index 0000000..cb550af
--- /dev/null
+++ b/src/riscv/64/cdef.S
@@ -0,0 +1,703 @@
+/******************************************************************************
+ * Copyright © 2018, VideoLAN and dav1d authors
+ * Copyright © 2024, Bogdan Gligorijevic
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *****************************************************************************/
+
+#include "src/riscv/asm.S"
+
+.macro constrain_vectors vec1, vec2, vec_sub, strength, shift, vec_tmp1, vec_tmp2
+    vmslt.vx v0, \vec_tmp1, zero
+    vneg.v \vec_tmp1, \vec_tmp1, v0.t
+    vmmv.m v1, v0
+
+    vmslt.vx v0, \vec_tmp2, zero
+    vneg.v \vec_tmp2, \vec_tmp2, v0.t
+
+    vsra.vx \vec1, \vec_tmp1, \shift
+    vsra.vx \vec2, \vec_tmp2, \shift
+
+    vrsub.vx \vec1, \vec1, \strength
+    vrsub.vx \vec2, \vec2, \strength
+
+    vmax.vx \vec1, \vec1, zero
+    vmax.vx \vec2, \vec2, zero
+
+    vmin.vv \vec_tmp1, \vec1, \vec_tmp1
+    vmin.vv \vec_tmp2, \vec2, \vec_tmp2
+
+    vneg.v \vec_tmp2, \vec_tmp2, v0.t
+
+    vmmv.m v0, v1
+    vneg.v \vec_tmp1, \vec_tmp1, v0.t
+.endm
+
+.macro padding_fn w, h
+    li t5, -32768 # INT16_MIN
+
+    andi t4, a7, 4
+    li t2, -2 # y_start
+
+.if \w == 4
+    vsetivli zero, \w + 4, e16, m1, ta, ma
+.else
+    vsetivli zero, \w + 4, e16, m2, ta, ma
+.endif
+    vmv.v.x v0, t5
+    bnez t4, L(top_done_\w\()x\h)
+
+    slli t5, a1, 1
+    addi t5, t5, 2
+    slli t5, t5, 1
+    sub t5, a0, t5
+
+    sh1add t4, a1, t5
+    vse16.v v0, (t5)
+    vse16.v v0, (t4)
+    li t2, 0
+
+L(top_done_\w\()x\h):
+    andi t4, a7, 8
+    li t3, 2 + \h # y_end
+    bnez t4, L(bottom_done_\w\()x\h)
+
+    li t5, \h
+    mul t5, a1, t5
+    addi t5, t5, -2
+    sh1add t5, t5, a0
+
+    sh1add t4, a1, t5
+    vse16.v v0, (t5)
+    vse16.v v0, (t4)
+    addi t3, t3, -2
+
+L(bottom_done_\w\()x\h):
+    andi t4, a7, 1
+    li t0, -2 # x_start
+
+.if \w == 4
+    vsetivli zero, 2, e16, m1, ta, ma
+.else
+    vsetivli zero, 2, e16, m2, ta, ma
+.endif
+
+    bnez t4, L(left_done_\w\()x\h)
+
+    mul t5, a1, t2
+    addi t5, t5, -2
+    sh1add t5, t5, a0
+
+    sub t0, t3, t2
+
+3:
+    vse16.v v0, (t5)
+    sh1add t5, a1, t5
+    addi t0, t0, -1
+    bnez t0, 3b
+
+L(left_done_\w\()x\h):
+
+    andi t4, a7, 2
+    li t1, 2 + \w # x_end
+    bnez t4, L(right_done_\w\()x\h)
+
+    mul t5, t2, a1
+    addi t5, t5, \w
+    sh1add t5, t5, a0
+
+    sub t1, t3, t2
+
+4:
+    vse16.v v0, (t5)
+    sh1add t5, a1, t5
+    addi t1, t1, -1
+    bnez t1, 4b
+
+    li t1, \w
+
+L(right_done_\w\()x\h):
+
+    beqz t2, L(top_skip_\w\()x\h)
+
+    mul t5, a1, t2
+    add t5, t0, t5
+    sh1add a0, t5, a0 # tmp += y_start * tmp_stride + x_start
+    add a5, a5, t0
+
+    sub t5, t1, t0 # x_end - x_start
+    slli t6, t0, 1
+.if \w == 4
+    vsetvli zero, t5, e16, m1, ta, ma
+.else
+    vsetvli zero, t5, e16, m2, ta, ma
+.endif
+
+5:
+    vle8.v v0, (a5)
+    addi t2, t2, 1
+    vzext.vf2 v2, v0
+    add a5, a3, a5
+    vse16.v v2, (a0)
+    sh1add a0, a1, a0
+    bnez t2, 5b
+
+    sub a0, a0, t6 # tmp -= x_start
+
+L(top_skip_\w\()x\h):
+
+    li a5, \h
+    beqz t0, L(left_skip_\w\()x\h)
+
+    sh1add a0, t0, a0 # tmp += x_start
+
+7:
+.if \w == 4
+    vsetivli zero, 2, e16, m1, ta, ma
+.else
+    vsetivli zero, 2, e16, m2, ta, ma
+.endif
+
+    vle8.v v0, (a4)
+    addi a5, a5, -1
+    vzext.vf2 v2, v0
+    addi a4, a4, 2
+    vse16.v v2, (a0)
+    sh1add a0, a1, a0
+    bnez a5, 7b
+
+    li a5, \h
+    mul t5, a1, a5
+    add t5, t5, t0
+    slli t5, t5, 1
+    sub a0, a0, t5 # tmp -= h * tmp_stride + x_start
+
+L(left_skip_\w\()x\h):
+
+8:
+.if \w == 4
+    vsetvli zero, t1, e16, m1, ta, ma
+.else
+    vsetvli zero, t1, e16, m2, ta, ma
+.endif
+
+    vle8.v v0, (a2)
+    vzext.vf2 v2, v0
+    vse16.v v2, (a0)
+    add a2, a3, a2
+    sh1add a0, a1, a0
+    addi a5, a5, -1
+    bnez a5, 8b
+
+
+    li a5, \h
+    sh1add a0, t0, a0 # tmp += x_start
+    add a6, a6, t0 # bottom += x_start
+    beq a5, t3, L(bottom_skip_\w\()x\h)
+
+    sub t5, t1, t0
+.if \w == 4
+    vsetvli zero, t5, e16, m1, ta, ma
+.else
+    vsetvli zero, t5, e16, m2, ta, ma
+.endif
+
+9:
+    vle8.v v0, (a6)
+    add a6, a3, a6
+    vzext.vf2 v2, v0
+    addi a5, a5, 1
+    vse16.v v2, (a0)
+    sh1add a0, a1, a0
+    bne a5, t3, 9b
+
+L(bottom_skip_\w\()x\h):
+    li t6, \h
+    mul t6, a3, t6
+    sub a2, a2, t6 # src -= h * src_stride
+    mul t5, a1, t3
+    add t5, t5, t0
+    slli t5, t5, 1
+    sub a0, a0, t5 # tmp -= y_end * tmp_stride + x_start
+.endm
+
+
+.macro cdef_fn w, h
+function cdef_filter_block_\w\()x\h\()_8bpc_rvv, export=1, ext="v,zba,zbb"
+    csrw vxrm, zero
+
+    addi sp, sp, -32 - 144*2
+    sd a5, 24(sp) # pri_strength
+    sd a6, 16(sp) # sec_strength
+    sd a7, 8(sp) # dir
+
+
+    ld a7, 8 + 32 + 144*2(sp) # edges
+    mv a6, a4 # bottom
+    mv a5, a3 # top
+    mv a4, a2 # left
+    mv a3, a1 # dst_stride
+    mv a2, a0 # dst
+    li a1, 12 # tmp_stride
+    addi a0, sp, 32 + 2*(2*12+2)
+    padding_fn \w, \h
+
+    ld a4, 32 + 2*144(sp) # damping
+    ld a5, 24(sp) # pri_strength
+    ld a6, 16(sp) # sec_strength
+    ld a7, 8(sp) # dir
+
+    beqz a5, cdef_filter_sec_only_\w\()x\h
+
+    bnez a6, cdef_filter_pri_sec_\w\()x\h
+
+    andi t0, a5, 1
+    li t1, 4
+    sub t4, t1, t0
+
+    li t1, 63
+    clz t2, a5
+    sub t1, t1, t2
+    sub t1, a4, t1
+
+    li t0, \h
+
+    la t2, dav1d_cdef_directions
+    addi t3, a7, 2
+    sh1add t2, t3, t2
+
+    blt zero, t1, 1f
+    mv t1, zero
+1:
+    vsetivli zero, \w, e16, m1, ta, mu
+
+    lb t3, 0(t2)
+
+    vle8.v v0, (a2)
+    vzext.vf2 v2, v0
+
+    sh1add t6, t3, a0
+    slli t3, t3, 1
+    sub t3, a0, t3
+
+    vle16.v v4, (t6)
+    vle16.v v6, (t3)
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    constrain_vectors v4, v6, v12, a5, t1, v8, v16
+
+    vmul.vx v28, v16, t4
+    vmacc.vx v28, t4, v8
+
+    lb t3, 1(t2)
+
+    andi t5, t4, 3
+    ori t5, t5, 2
+
+    sh1add t6, t3, a0
+    slli t3, t3, 1
+    sub t3, a0, t3
+
+    vsetvli zero, zero, e16, m1, ta, mu
+
+    vle16.v v4, (t6)
+    vle16.v v6, (t3)
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    constrain_vectors v4, v6, v12, a5, t1, v8, v16
+
+    vmacc.vx v28, t5, v16
+    vmacc.vx v28, t5, v8
+
+    vmslt.vx v0, v28, zero
+    vadd.vi v28, v28, -1, v0.t
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vnclip.wi v24, v28, 4
+
+    vadd.vv v28, v2, v24
+
+    vsetvli zero, zero, e8, mf2, ta, ma
+
+    vnclipu.wi v24, v28, 0
+
+    vse8.v v24, (a2)
+
+    addi t0, t0, -1
+    add a2, a2, a3
+    sh1add a0, a1, a0
+
+    bnez t0, 1b
+
+    addi sp, sp, 32 + 144*2
+    ret
+
+cdef_filter_sec_only_\w\()x\h:
+    li t1, 63
+    clz t2, a6
+    sub t1, t1, t2
+    sub t1, a4, t1
+
+    li t0, \h
+
+    la t2, dav1d_cdef_directions
+    addi t3, a7, 4
+    sh1add t3, t3, t2
+    sh1add t2, a7, t2
+
+2:
+    vsetivli zero, \w, e16, m1, ta, mu
+
+    lb t4, 0(t3)
+    lb t5, 0(t2)
+
+    vle8.v v0, (a2)
+    vzext.vf2 v2, v0
+
+    sh1add t6, t4, a0
+    slli t4, t4, 1
+    sub t4, a0, t4
+
+    vle16.v v4, (t6)
+    vle16.v v6, (t4)
+
+    sh1add t4, t5, a0
+    slli t5, t5, 1
+    sub t5, a0, t5
+
+    vle16.v v8, (t4)
+    vle16.v v10, (t5)
+
+    vwsub.vv v12, v4, v2
+    vwsub.vv v14, v6, v2
+    vwsub.vv v16, v8, v2
+    vwsub.vv v18, v10, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    li t4, 2
+    constrain_vectors v4, v6, v12, a6, t1, v12, v14
+    constrain_vectors v8, v10, v14, a6, t1, v16, v18
+
+    vmul.vx v28, v18, t4
+    vmacc.vx v28, t4, v16
+    vmacc.vx v28, t4, v14
+    vmacc.vx v28, t4, v12
+
+
+    lb t4, 1(t3)
+    lb t5, 1(t2)
+
+    sh1add t6, t4, a0
+    slli t4, t4, 1
+    sub t4, a0, t4
+
+    vsetvli zero, zero, e16, m1, ta, mu
+
+    vle16.v v4, (t6)
+    vle16.v v6, (t4)
+
+    sh1add t4, t5, a0
+    slli t5, t5, 1
+    sub t5, a0, t5
+
+    vle16.v v8, (t4)
+    vle16.v v10, (t5)
+
+    vwsub.vv v12, v4, v2
+    vwsub.vv v14, v6, v2
+    vwsub.vv v16, v8, v2
+    vwsub.vv v18, v10, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    constrain_vectors v4, v6, v12, a6, t1, v12, v14
+    constrain_vectors v8, v10, v14, a6, t1, v16, v18
+
+    vadd.vv v4, v28, v12
+    vadd.vv v28, v4, v14
+    vadd.vv v4, v28, v16
+    vadd.vv v28, v4, v18
+
+    vmslt.vx v0, v28, zero
+    vadd.vi v28, v28, -1, v0.t
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vnclip.wi v24, v28, 4
+
+    vadd.vv v28, v2, v24
+
+    vsetvli zero, zero, e8, mf2, ta, ma
+
+    vnclipu.wi v24, v28, 0
+
+    vse8.v v24, (a2)
+
+    addi t0, t0, -1
+    add a2, a2, a3
+    sh1add a0, a1, a0
+
+    bnez t0, 2b
+
+    addi sp, sp, 32 + 144*2
+    ret
+cdef_filter_pri_sec_\w\()x\h:
+
+    li t1, 63
+    clz t2, a5
+    clz t3, a6
+    sub t2, t1, t2
+    sub t3, t1, t3
+    sub t1, a4, t2
+    sub t2, a4, t3
+
+    li t0, \h
+
+    la t3, dav1d_cdef_directions
+
+    blt zero, t1, 3f
+    mv t1, zero
+3:
+    vsetivli zero, \w, e16, m1, ta, ma
+
+    li t4, 4
+    andi t6, a5, 1
+    addi t5, a7, 2
+    sub t4, t4, t6
+
+    sh1add t5, t5, t3
+
+    vle8.v v0, (a2)
+
+    lb t6, 0(t5)
+
+    vzext.vf2 v2, v0
+
+    sh1add a4, t6, a0
+    slli t6, t6, 1
+    sub t6, a0, t6
+
+    vle16.v v4, (a4)
+    vle16.v v6, (t6)
+
+    vminu.vv v20, v4, v2
+    vmax.vv v24, v4, v2
+    vminu.vv v20, v6, v20
+    vmax.vv v24, v6, v24
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    constrain_vectors v4, v6, v12, a5, t1, v8, v16
+
+    vmul.vx v28, v16, t4
+    vmacc.vx v28, t4, v8
+
+    lb t6, 1(t5)
+
+    andi t4, t4, 3
+    ori t4, t4, 2
+
+
+    sh1add a4, t6, a0
+    slli t6, t6, 1
+    sub t6, a0, t6
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vle16.v v4, (a4)
+    vle16.v v6, (t6)
+
+    vminu.vv v20, v4, v20
+    vmax.vv v24, v4, v24
+    vminu.vv v20, v6, v20
+    vmax.vv v24, v6, v24
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    constrain_vectors v4, v6, v12, a5, t1, v8, v16
+
+    addi t5, a7, 4
+    vmacc.vx v28, t4, v16
+    vmacc.vx v28, t4, v8
+
+    sh1add t5, t5, t3
+
+    lb t6, 0(t5)
+
+    sh1add a4, t6, a0
+    slli t6, t6, 1
+    sub t6, a0, t6
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vle16.v v4, (a4)
+    vle16.v v6, (t6)
+
+    vminu.vv v20, v4, v20
+    vmax.vv v24, v4, v24
+    vminu.vv v20, v6, v20
+    vmax.vv v24, v6, v24
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    li t6, 2
+    constrain_vectors v4, v6, v12, a6, t2, v8, v16
+
+    vmacc.vx v28, t6, v16
+    vmacc.vx v28, t6, v8
+
+    lb t6, 1(t5)
+
+    sh1add a4, t6, a0
+    slli t6, t6, 1
+    sub t6, a0, t6
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vle16.v v4, (a4)
+    vle16.v v6, (t6)
+
+    vminu.vv v20, v4, v20
+    vmax.vv v24, v4, v24
+    vminu.vv v20, v6, v20
+    vmax.vv v24, v6, v24
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    constrain_vectors v4, v6, v12, a6, t2, v8, v16
+
+    sh1add t5, a7, t3
+
+    vadd.vv v4, v28, v8
+    vadd.vv v28, v4, v16
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    lb t6, 0(t5)
+
+    sh1add a4, t6, a0
+    slli t6, t6, 1
+    sub t6, a0, t6
+
+    vle16.v v4, (a4)
+    vle16.v v6, (t6)
+
+    vminu.vv v20, v4, v20
+    vmax.vv v24, v4, v24
+    vminu.vv v20, v6, v20
+    vmax.vv v24, v6, v24
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    li t6, 2
+    constrain_vectors v4, v6, v12, a6, t2, v8, v16
+
+    vmacc.vx v28, t6, v16
+    vmacc.vx v28, t6, v8
+
+    lb t6, 1(t5)
+
+    sh1add a4, t6, a0
+    slli t6, t6, 1
+    sub t6, a0, t6
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vle16.v v4, (a4)
+    vle16.v v6, (t6)
+
+    vminu.vv v20, v4, v20
+    vmax.vv v24, v4, v24
+    vminu.vv v20, v6, v20
+    vmax.vv v24, v6, v24
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    constrain_vectors v4, v6, v12, a6, t2, v8, v16
+
+    vadd.vv v4, v28, v8
+    vadd.vv v28, v4, v16
+
+    vmslt.vx v0, v28, zero
+    vadd.vi v28, v28, -1, v0.t
+
+    vsetvli zero, zero, e16, m1, ta, mu
+
+    vnclip.wi v16, v28, 4
+
+    vadd.vv v28, v2, v16
+
+    vmslt.vv v0, v20, v28
+    vmerge.vvm v4, v20, v28, v0
+
+    vmslt.vv v0, v4, v24
+    vmerge.vvm v28, v24, v4, v0
+
+    vsetvli zero, zero, e8, mf2, ta, ma
+
+    vnclipu.wi v24, v28, 0
+
+    vse8.v v24, (a2)
+
+    addi t0, t0, -1
+    add a2, a2, a3
+    sh1add a0, a1, a0
+
+    bnez t0, 3b
+
+    addi sp, sp, 32 + 144*2
+    ret
+endfunc
+.endm
+
+cdef_fn 4, 4
+cdef_fn 4, 8
+cdef_fn 8, 8
diff --git a/src/riscv/64/cdef16.S b/src/riscv/64/cdef16.S
new file mode 100644
index 0000000..3ac770d
--- /dev/null
+++ b/src/riscv/64/cdef16.S
@@ -0,0 +1,689 @@
+/******************************************************************************
+ * Copyright © 2018, VideoLAN and dav1d authors
+ * Copyright © 2024, Bogdan Gligorijevic
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *****************************************************************************/
+
+#include "src/riscv/asm.S"
+
+.macro constrain_vectors vec1, vec2, vec_sub, strength, shift, vec_tmp1, vec_tmp2
+    vmslt.vx v0, \vec_tmp1, zero
+    vneg.v \vec_tmp1, \vec_tmp1, v0.t
+    vmmv.m v1, v0
+
+    vmslt.vx v0, \vec_tmp2, zero
+    vneg.v \vec_tmp2, \vec_tmp2, v0.t
+
+    vsra.vx \vec1, \vec_tmp1, \shift
+    vsra.vx \vec2, \vec_tmp2, \shift
+
+    vrsub.vx \vec1, \vec1, \strength
+    vrsub.vx \vec2, \vec2, \strength
+
+    vmax.vx \vec1, \vec1, zero
+    vmax.vx \vec2, \vec2, zero
+
+    vmin.vv \vec_tmp1, \vec1, \vec_tmp1
+    vmin.vv \vec_tmp2, \vec2, \vec_tmp2
+
+    vneg.v \vec_tmp2, \vec_tmp2, v0.t
+
+    vmmv.m v0, v1
+    vneg.v \vec_tmp1, \vec_tmp1, v0.t
+.endm
+
+.macro padding_fn w, h
+    li t5, -32768 # INT16_MIN
+
+    andi t4, a7, 4
+    li t2, -2 # y_start
+
+.if \w == 4
+    vsetivli zero, \w + 4, e16, m1, ta, ma
+.else
+    vsetivli zero, \w + 4, e16, m2, ta, ma
+.endif
+    vmv.v.x v0, t5
+    bnez t4, L(top_done_\w\()x\h)
+
+    slli t5, a1, 1
+    addi t5, t5, 2
+    slli t5, t5, 1
+    sub t5, a0, t5
+
+    sh1add t4, a1, t5
+    vse16.v v0, (t5)
+    vse16.v v0, (t4)
+    li t2, 0
+
+L(top_done_\w\()x\h):
+    andi t4, a7, 8
+    li t3, 2 + \h # y_end
+    bnez t4, L(bottom_done_\w\()x\h)
+
+    li t5, \h
+    mul t5, a1, t5
+    addi t5, t5, -2
+    sh1add t5, t5, a0
+
+    sh1add t4, a1, t5
+    vse16.v v0, (t5)
+    vse16.v v0, (t4)
+    addi t3, t3, -2
+
+L(bottom_done_\w\()x\h):
+    andi t4, a7, 1
+    li t0, -2 # x_start
+
+.if \w == 4
+    vsetivli zero, 2, e16, m1, ta, ma
+.else
+    vsetivli zero, 2, e16, m2, ta, ma
+.endif
+
+    bnez t4, L(left_done_\w\()x\h)
+
+    mul t5, a1, t2
+    addi t5, t5, -2
+    sh1add t5, t5, a0
+
+    sub t0, t3, t2
+
+3:
+    vse16.v v0, (t5)
+    sh1add t5, a1, t5
+    addi t0, t0, -1
+    bnez t0, 3b
+
+L(left_done_\w\()x\h):
+
+    andi t4, a7, 2
+    li t1, 2 + \w # x_end
+    bnez t4, L(right_done_\w\()x\h)
+
+    mul t5, t2, a1
+    addi t5, t5, \w
+    sh1add t5, t5, a0
+
+    sub t1, t3, t2
+
+4:
+    vse16.v v0, (t5)
+    sh1add t5, a1, t5
+    addi t1, t1, -1
+    bnez t1, 4b
+
+    li t1, \w
+
+L(right_done_\w\()x\h):
+
+    beqz t2, L(top_skip_\w\()x\h)
+
+    mul t5, a1, t2
+    add t5, t0, t5
+    sh1add a0, t5, a0 # tmp += y_start * tmp_stride + x_start
+    sh1add a5, t0, a5 # top += x_start
+
+    sub t5, t1, t0
+    slli t6, t0, 1
+.if \w == 4
+    vsetvli zero, t5, e16, m1, ta, ma
+.else
+    vsetvli zero, t5, e16, m2, ta, ma
+.endif
+
+5:
+    vle16.v v2, (a5)
+    addi t2, t2, 1
+    add a5, a3, a5
+    vse16.v v2, (a0)
+    sh1add a0, a1, a0
+    bnez t2, 5b
+
+    sub a0, a0, t6 # tmp -= x_start
+
+L(top_skip_\w\()x\h):
+
+    li a5, \h
+    beqz t0, L(left_skip_\w\()x\h)
+
+    sh1add a0, t0, a0 # tmp += x_start
+
+7:
+.if \w == 4
+    vsetivli zero, 2, e16, m1, ta, ma
+.else
+    vsetivli zero, 2, e16, m2, ta, ma
+.endif
+
+    vle16.v v2, (a4)
+    addi a5, a5, -1
+    addi a4, a4, 4
+    vse16.v v2, (a0)
+    sh1add a0, a1, a0
+    bnez a5, 7b
+
+    li a5, \h
+    mul t5, a1, a5
+    add t5, t5, t0
+    slli t5, t5, 1
+    sub a0, a0, t5 # tmp -= h * tmp_stride + x_start
+
+L(left_skip_\w\()x\h):
+
+8:
+.if \w == 4
+    vsetvli zero, t1, e16, m1, ta, ma
+.else
+    vsetvli zero, t1, e16, m2, ta, ma
+.endif
+
+    vle16.v v2, (a2)
+    add a2, a3, a2
+    vse16.v v2, (a0)
+    sh1add a0, a1, a0
+    addi a5, a5, -1
+    bnez a5, 8b
+
+
+    li a5, \h
+    sh1add a0, t0, a0 # tmp += x_start
+    sh1add a6, t0, a6 # bottom += x_start
+    beq a5, t3, L(bottom_skip_\w\()x\h)
+
+    sub t5, t1, t0
+.if \w == 4
+    vsetvli zero, t5, e16, m1, ta, ma
+.else
+    vsetvli zero, t5, e16, m2, ta, ma
+.endif
+
+9:
+    vle16.v v2, (a6)
+    add a6, a3, a6
+    addi a5, a5, 1
+    vse16.v v2, (a0)
+    sh1add a0, a1, a0
+    bne a5, t3, 9b
+
+L(bottom_skip_\w\()x\h):
+    li t6, \h
+    mul t6, a3, t6
+    sub a2, a2, t6 # src -= h * PXSTRIDE(src_stride)
+    mul t5, a1, t3
+    add t5, t5, t0
+    slli t5, t5, 1
+    sub a0, a0, t5 # tmp -= y_end * tmp_stride + x_start
+.endm
+
+.macro cdef_fn w, h
+function cdef_filter_block_\w\()x\h\()_16bpc_rvv, export=1, ext="v,zba,zbb"
+    csrw vxrm, zero
+
+    addi sp, sp, -32 - 144*2
+    sd a5, 24(sp) # pri_strength
+    sd a6, 16(sp) # sec_strength
+    sd a7, 8(sp) # dir
+
+    ld a7, 8 + 32 + 144*2(sp) # edges
+    mv a6, a4 # bottom
+    mv a5, a3 # top
+    mv a4, a2 # left
+    mv a3, a1 # dst_stride
+    mv a2, a0 # dst
+    li a1, 12 # tmp_stride
+    addi a0, sp, 32 + 2*(2*12+2)
+
+    padding_fn \w, \h
+
+    ld a4, 32 + 2*144(sp) # damping
+    ld a5, 24(sp) # pri_strength
+    ld a6, 16(sp) # sec_strength
+    ld a7, 8(sp) # dir
+
+    beqz a5, cdef_filter_sec_only_\w\()x\h
+
+    bnez a6, cdef_filter_pri_sec_\w\()x\h
+
+    li t1, 64-8
+    ld t4, 32 + 2*144 + 16(sp) # bitdepth_max
+    clz t4, t4
+    sub t4, t1, t4
+    sra t4, a5, t4
+    andi t0, t4, 1
+    li t1, 4
+    sub t4, t1, t0
+
+    li t1, 63
+    clz t2, a5
+    sub t1, t1, t2
+    sub t1, a4, t1
+
+    li t0, \h
+
+    la t2, dav1d_cdef_directions
+    addi t3, a7, 2
+    sh1add t2, t3, t2
+
+    vsetivli zero, \w, e16, m1, ta, ma
+    blt zero, t1, 1f
+    mv t1, zero
+1:
+    lb t3, 0(t2)
+
+    vle16.v v2, (a2)
+
+    sh1add t6, t3, a0
+    slli t3, t3, 1
+    sub t3, a0, t3
+
+    vle16.v v4, (t6)
+    vle16.v v6, (t3)
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    constrain_vectors v4, v6, v2, a5, t1, v8, v16
+
+    vmul.vx v28, v16, t4
+    vmacc.vx v28, t4, v8
+
+    lb t3, 1(t2)
+
+    andi t5, t4, 3
+    ori t5, t5, 2
+
+    sh1add t6, t3, a0
+    slli t3, t3, 1
+    sub t3, a0, t3
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vle16.v v4, (t6)
+    vle16.v v6, (t3)
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    constrain_vectors v4, v6, v2, a5, t1, v8, v16
+
+    vmacc.vx v28, t5, v16
+    vmacc.vx v28, t5, v8
+
+    vmslt.vx v0, v28, zero
+    vadd.vi v28, v28, -1, v0.t
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vnclip.wi v24, v28, 4
+
+    vadd.vv v28, v2, v24
+
+    vse16.v v28, (a2)
+
+    add a2, a2, a3
+    sh1add a0, a1, a0
+
+    addi t0, t0, -1
+    bnez t0, 1b
+
+    addi sp, sp, 32 + 144*2
+    ret
+
+cdef_filter_sec_only_\w\()x\h:
+    li t1, 63
+    clz t2, a6
+    sub t1, t1, t2
+    sub t1, a4, t1
+
+    li t0, \h
+
+    la t2, dav1d_cdef_directions
+    addi t3, a7, 4
+    sh1add t3, t3, t2
+    sh1add t2, a7, t2
+
+    vsetivli zero, \w, e16, m1, ta, ma
+2:
+
+    lb t4, 0(t3)
+    lb t5, 0(t2)
+
+    vle16.v v2, (a2)
+
+    sh1add t6, t4, a0
+    slli t4, t4, 1
+    sub t4, a0, t4
+
+    vle16.v v4, (t6)
+    vle16.v v6, (t4)
+
+    sh1add t4, t5, a0
+    slli t5, t5, 1
+    sub t5, a0, t5
+
+    vle16.v v8, (t4)
+    vle16.v v10, (t5)
+
+    vwsub.vv v12, v4, v2
+    vwsub.vv v14, v6, v2
+    vwsub.vv v16, v8, v2
+    vwsub.vv v18, v10, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    li t4, 2
+    constrain_vectors v4, v6, v2, a6, t1, v12, v14
+    constrain_vectors v8, v10, v2, a6, t1, v16, v18
+
+    vmul.vx v28, v18, t4
+    vmacc.vx v28, t4, v16
+    vmacc.vx v28, t4, v14
+    vmacc.vx v28, t4, v12
+
+    lb t4, 1(t3)
+    lb t5, 1(t2)
+
+    sh1add t6, t4, a0
+    slli t4, t4, 1
+    sub t4, a0, t4
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vle16.v v4, (t6)
+    vle16.v v6, (t4)
+
+    sh1add t4, t5, a0
+    slli t5, t5, 1
+    sub t5, a0, t5
+
+    vle16.v v8, (t4)
+    vle16.v v10, (t5)
+
+    vwsub.vv v12, v4, v2
+    vwsub.vv v14, v6, v2
+    vwsub.vv v16, v8, v2
+    vwsub.vv v18, v10, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    constrain_vectors v4, v6, v2, a6, t1, v12, v14
+    constrain_vectors v8, v10, v2, a6, t1, v16, v18
+
+    vadd.vv v4, v28, v12
+    vadd.vv v28, v4, v14
+    vadd.vv v4, v28, v16
+    vadd.vv v28, v4, v18
+
+    vmslt.vx v0, v28, zero
+    vadd.vi v28, v28, -1, v0.t
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vnclip.wi v24, v28, 4
+
+    vadd.vv v28, v2, v24
+
+    vse16.v v28, (a2)
+
+    add a2, a2, a3
+    sh1add a0, a1, a0
+
+    addi t0, t0, -1
+    bnez t0, 2b
+
+    addi sp, sp, 32 + 144*2
+    ret
+cdef_filter_pri_sec_\w\()x\h:
+
+    li t1, 63
+    clz t2, a5
+    clz t3, a6
+    sub t2, t1, t2
+    sub t3, t1, t3
+    sub t1, a4, t2
+    sub t2, a4, t3
+
+    li t0, \h
+
+    la t3, dav1d_cdef_directions
+
+    vsetivli zero, \w, e16, m1, ta, ma
+    blt zero, t1, 3f
+    mv t1, zero
+3:
+    li t5, 64-8
+    ld t4, 32 + 2*144 + 16(sp) # bitdepth_max
+    clz t4, t4
+    sub t4, t5, t4
+    sra t4, a5, t4
+    li t6, 4
+    andi t5, t4, 1
+    sub t4, t6, t5
+
+    addi t5, a7, 2
+
+    sh1add t5, t5, t3
+
+    vle16.v v2, (a2)
+
+    lb t6, 0(t5)
+
+    sh1add a4, t6, a0
+    slli t6, t6, 1
+    sub t6, a0, t6
+
+    vle16.v v4, (a4)
+    vle16.v v6, (t6)
+
+    vminu.vv v20, v4, v2
+    vmax.vv v24, v4, v2
+    vminu.vv v20, v6, v20
+    vmax.vv v24, v6, v24
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    constrain_vectors v4, v6, v2, a5, t1, v8, v16
+
+    vmul.vx v28, v16, t4
+    vmacc.vx v28, t4, v8
+
+    andi t4, t4, 3
+    ori t4, t4, 2
+
+    lb t6, 1(t5)
+
+    sh1add a4, t6, a0
+    slli t6, t6, 1
+    sub t6, a0, t6
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vle16.v v4, (a4)
+    vle16.v v6, (t6)
+
+    vminu.vv v20, v4, v20
+    vmax.vv v24, v4, v24
+    vminu.vv v20, v6, v20
+    vmax.vv v24, v6, v24
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    constrain_vectors v4, v6, v2, a5, t1, v8, v16
+
+    addi t5, a7, 4
+    vmacc.vx v28, t4, v16
+    vmacc.vx v28, t4, v8
+
+    sh1add t5, t5, t3
+
+    lb t6, 0(t5)
+
+    sh1add a4, t6, a0
+    slli t6, t6, 1
+    sub t6, a0, t6
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vle16.v v4, (a4)
+    vle16.v v6, (t6)
+
+    vminu.vv v20, v4, v20
+    vmax.vv v24, v4, v24
+    vminu.vv v20, v6, v20
+    vmax.vv v24, v6, v24
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    li t6, 2
+    constrain_vectors v4, v6, v2, a6, t2, v8, v16
+
+    vmacc.vx v28, t6, v16
+    vmacc.vx v28, t6, v8
+
+    lb t6, 1(t5)
+
+    sh1add a4, t6, a0
+    slli t6, t6, 1
+    sub t6, a0, t6
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vle16.v v4, (a4)
+    vle16.v v6, (t6)
+
+    vminu.vv v20, v4, v20
+    vmax.vv v24, v4, v24
+    vminu.vv v20, v6, v20
+    vmax.vv v24, v6, v24
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    constrain_vectors v4, v6, v2, a6, t2, v8, v16
+
+    sh1add t5, a7, t3
+
+    vadd.vv v4, v28, v8
+    vadd.vv v28, v4, v16
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    lb t6, 0(t5)
+
+    sh1add a4, t6, a0
+    slli t6, t6, 1
+    sub t6, a0, t6
+
+    vle16.v v4, (a4)
+    vle16.v v6, (t6)
+
+    vminu.vv v20, v4, v20
+    vmax.vv v24, v4, v24
+    vminu.vv v20, v6, v20
+    vmax.vv v24, v6, v24
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    li t6, 2
+    constrain_vectors v4, v6, v2, a6, t2, v8, v16
+
+    vmacc.vx v28, t6, v16
+    vmacc.vx v28, t6, v8
+
+    lb t6, 1(t5)
+
+    sh1add a4, t6, a0
+    slli t6, t6, 1
+    sub t6, a0, t6
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vle16.v v4, (a4)
+    vle16.v v6, (t6)
+
+    vminu.vv v20, v4, v20
+    vmax.vv v24, v4, v24
+    vminu.vv v20, v6, v20
+    vmax.vv v24, v6, v24
+
+    vwsub.vv v8, v4, v2
+    vwsub.vv v16, v6, v2
+
+    vsetvli zero, zero, e32, m2, ta, mu
+
+    constrain_vectors v4, v6, v2, a6, t2, v8, v16
+
+    vadd.vv v4, v28, v8
+    vadd.vv v28, v4, v16
+
+    vmslt.vx v0, v28, zero
+    vadd.vi v28, v28, -1, v0.t
+
+    vsetvli zero, zero, e16, m1, ta, ma
+
+    vnclip.wi v16, v28, 4
+
+    vadd.vv v28, v2, v16
+
+    vmslt.vv v0, v20, v28
+    vmerge.vvm v4, v20, v28, v0
+
+    vmslt.vv v0, v4, v24
+    vmerge.vvm v28, v24, v4, v0
+
+    vse16.v v28, (a2)
+
+    add a2, a2, a3
+    sh1add a0, a1, a0
+
+    addi t0, t0, -1
+    bnez t0, 3b
+
+    addi sp, sp, 32 + 144*2
+    ret
+endfunc
+.endm
+
+cdef_fn 4, 4
+cdef_fn 4, 8
+cdef_fn 8, 8
diff --git a/src/riscv/64/cpu.S b/src/riscv/64/cpu.S
index b0e76f6..a81c6d8 100644
--- a/src/riscv/64/cpu.S
+++ b/src/riscv/64/cpu.S
@@ -42,3 +42,8 @@ function has_compliant_rvv, export=1, ext=v
   sgtz a0, a0
   ret
 endfunc
+
+function get_vlenb, export=1
+  csrr a0, vlenb
+  ret
+endfunc
diff --git a/src/riscv/64/ipred.S b/src/riscv/64/ipred.S
new file mode 100644
index 0000000..0519fd0
--- /dev/null
+++ b/src/riscv/64/ipred.S
@@ -0,0 +1,461 @@
+/******************************************************************************
+ * Copyright © 2018, VideoLAN and dav1d authors
+ * Copyright © 2024, Bogdan Gligorijevic
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *****************************************************************************/
+
+#include "src/riscv/asm.S"
+
+function dc_gen_8bpc_rvv, export=1, ext="v,zbb"
+    .variant_cc dav1d_dc_gen_8bpc_rvv
+    add t1, a1, a2
+    srli t5, t1, 1
+    mv t1, a1
+    addi t2, a0, 1
+    vsetvli zero, t1, e16, m4, ta, ma
+    vmv.v.x v0, zero
+1:
+    vsetvli t3, t1, e8, m2, tu, ma
+    vle8.v v4, (t2)
+    vwaddu.wv v0, v0, v4
+
+    sub t1, t1, t3
+    add t2, t2, t3
+    bnez t1, 1b
+
+    mv t1, a2
+    mv t2, a0
+    vsetvli zero, t1, e16, m4, ta, ma
+    vmv.v.x v8, zero
+2:
+    vsetvli t3, t1, e8, m2, tu, ma
+    sub t2, t2, t3
+    vle8.v v4, (t2)
+    vwaddu.wv v8, v8, v4
+    sub t1, t1, t3
+
+    bnez t1, 2b
+
+    vsetvli zero, zero, e32, m8, ta, ma
+    vmv.s.x v16, t5
+    vmv.s.x v12, zero
+    vsetvli zero, a1, e16, m4, ta, ma
+    vwredsum.vs v24, v0, v16
+    vsetvli zero, a2, e16, m4, ta, ma
+    vwredsum.vs v16, v8, v12
+    vsetvli zero, zero, e32, m8, ta, ma
+    vmv.x.s t5, v24
+    vmv.x.s t1, v16
+    add t5, t5, t1
+
+    add t1, a1, a2
+    ctz t1, t1
+
+    srl a0, t5, t1
+
+
+    beq a1, a2, 5f
+    slli t1, a1, 1
+    sltu t2, t1, a2
+    slli t3, a2, 1
+    sltu t1, t3, a1
+    or t1, t1, t2
+    bnez t1, 3f
+
+    li t1, 0x5556
+    j 4f
+3:
+    li t1, 0x3334
+4:
+    mul a0, a0, t1
+    srli a0, a0, 16
+5:
+    jr t0
+endfunc
+
+function dc_gen_top_8bpc_rvv, export=1, ext="v,zbb"
+    .variant_cc dav1d_dc_gen_top_8bpc_rvv
+    mv t1, a1
+    srli t5, a1, 1
+    addi a0, a0, 1
+    vsetvli zero, t1, e16, m4, ta, ma
+    vmv.v.x v0, zero
+1:
+    vsetvli t3, t1, e8, m2, tu, ma
+    vle8.v v4, (a0)
+    vwaddu.wv v0, v0, v4
+    sub t1, t1, t3
+
+    add a0, a0, t3
+    bnez t1, 1b
+    j dc_gen_sum_up_8bpc_rvv
+endfunc
+
+function dc_gen_left_8bpc_rvv, export=1, ext="v,zbb"
+    .variant_cc dav1d_dc_gen_left_8bpc_rvv
+    mv t1, a1
+    srli t5, a1, 1
+    vsetvli t2, t1, e16, m4, ta, ma
+    vmv.v.x v0, zero
+
+1:
+    vsetvli t3, t1, e8, m2, tu, ma
+    sub a0, a0, t3
+    vle8.v v4, (a0)
+    vwaddu.wv v0, v0, v4
+    sub t1, t1, t3
+    bnez t1, 1b
+
+    j dc_gen_sum_up_8bpc_rvv
+endfunc
+
+function dc_gen_sum_up_8bpc_rvv, export=1, ext="v,zbb"
+    .variant_cc dav1d_dc_gen_sum_up_8bpc_rvv
+    vsetvli zero, a1, e32, m8, ta, ma
+    vmv.s.x v4, t5
+    vsetvli zero, zero, e16, m4, ta, ma
+    vwredsum.vs v8, v0, v4
+    vsetvli zero, zero, e32, m8, ta, ma
+    vmv.x.s t5, v8
+
+    ctz t1, a1
+
+    srl a0, t5, t1
+    jr t0
+endfunc
+
+function cfl_pred_8bpc_rvv, export=1, ext="v,zba"
+    csrw vxrm, zero
+1:
+    li t2, 0
+    mv t3, a2
+2:
+    vsetvli t0, t3, e16, m2, ta, ma
+    add t4, a0, t2
+    vle16.v v0, (a5)
+    sh1add a5, t0, a5
+
+    vwmul.vx v4, v0, a6
+    vsetvli zero, zero, e32, m4, ta, mu
+    vneg.v v8, v4
+    vmslt.vx v0, v4, x0
+    vmax.vv v12, v8, v4
+    vssra.vi v16, v12, 6
+    vneg.v v16, v16, v0.t
+    vadd.vx v20, v16, a4
+    vmax.vx v0, v20, zero
+    vsetvli zero, zero, e16, m2, ta, ma
+    vnclipu.wi v4, v0, 0
+    vsetvli zero, zero, e8, m1, ta, ma
+    vnclipu.wi v0, v4, 0
+    vse8.v v0, (t4)
+    add t2, t0, t2
+    sub t3, t3, t0
+    bnez t3, 2b
+    addi a3, a3, -1
+    add a0, a0, a1
+
+    bnez a3, 1b
+    ret
+endfunc
+
+function ipred_cfl_8bpc_rvv, export=1, ext=v
+    mv t6, a0 # dst
+    mv a0, a2 # topleft
+    mv t4, a1 # stride
+    mv a1, a3 # width
+    mv a2, a4 # height
+    jal t0, dc_gen_8bpc_rvv
+    mv a2, a3 # width
+    mv a3, a4 # height
+    mv a4, a0 # dc_get_top
+    mv a0, t6 # dst
+    mv a1, t4 # stride
+    j cfl_pred_8bpc_rvv
+endfunc
+
+function ipred_cfl_128_8bpc_rvv, export=1, ext="v,zba"
+    # dc = 128, then just rearrange registers
+    mv a2, a3
+    mv a3, a4
+    li a4, 128
+
+    j cfl_pred_8bpc_rvv
+endfunc
+
+function ipred_cfl_top_8bpc_rvv, export=1, ext=v
+    mv t6, a0 # dst
+    mv a0, a2 # topleft
+    mv t4, a1 # stride
+    mv a1, a3 # width
+    jal t0, dc_gen_top_8bpc_rvv
+    mv a3, a4 # height
+    mv a4, a0 # dc_get_top
+    mv a0, t6 # dst
+    mv a2, a1 # width
+    mv a1, t4 # stride
+    j cfl_pred_8bpc_rvv
+endfunc
+
+function ipred_cfl_left_8bpc_rvv, export=1, ext="v,zba"
+    mv t6, a0 # dst
+    mv a0, a2 # topleft
+    mv t4, a1 # stride
+    mv a1, a4 # height
+    mv a2, a3 # width
+    jal t0, dc_gen_left_8bpc_rvv
+    mv a3, a4 # height
+    mv a4, a0 # dc_get_left
+    mv a1, t4 # stride
+    mv a0, t6 # dst
+    j cfl_pred_8bpc_rvv
+endfunc
+
+function ipred_paeth_8bpc_rvv, export=1, ext="v,zba"
+    csrw vxrm, zero
+    li t0, 0
+    mv t3, a2
+    lbu t1, (a2)
+    addi a6, a2, -1
+    addi a2, a2, 1
+1:
+    lbu t2, (a6)
+    mv t3, a3
+2:
+    sub t5, a3, t3
+    add t5, a2, t5
+    vsetvli t6, t3, e8, m1, ta, ma
+    vle8.v v2, (t5)
+    vwaddu.vx v4, v2, t2
+    vsetvli zero, zero, e16, m2, ta, ma
+    vwsub.vx v8, v4, t1
+
+    vsetvli zero, zero, e32, m4, ta, mu
+    vzext.vf4 v24, v2
+    vsub.vx v12, v8, t1
+    vmslt.vx v0, v12, zero
+    vneg.v v12, v12, v0.t
+    vsub.vx v16, v8, t2
+    vmslt.vx v0, v16, zero
+    vneg.v v16, v16, v0.t
+    vsub.vv v20, v8, v24
+    vmslt.vx v0, v20, zero
+    vneg.v v20, v20, v0.t
+
+    sub t5, a3, t3
+    vmsleu.vv v4, v16, v20
+    vmsleu.vv v5, v16, v12
+    vmsgtu.vv v0, v20, v12
+    vmand.mm v6, v4, v5
+
+    vsetvli zero, zero, e8, m1, ta, ma
+    vmerge.vxm v8, v2, t1, v0
+    vmmv.m v0, v6
+    add t5, a0, t5
+    sub t3, t3, t6
+    vmerge.vxm v4, v8, t2, v0
+
+    vse8.v v4, (t5)
+
+    bnez t3, 2b
+
+    addi a4, a4, -1
+    addi a6, a6, -1
+    add a0, a0, a1
+    bnez a4, 1b
+    ret
+endfunc
+
+function ipred_smooth_8bpc_rvv, export=1, ext="v,zba"
+    csrw vxrm, zero
+    la t0, dav1d_sm_weights
+    add t1, t0, a3
+    add t2, a2, a3
+    add t0, t0, a4
+    lbu t2, (t2)
+    sub t3, a2, a4
+    addi a6, a2, -1
+    addi a2, a2, 1
+    lbu t3, (t3)
+1:
+    mv t6, a3
+
+    lbu a7, (a6)
+    lbu t4, (t0)
+2:
+    li a5, 256
+    vsetvli t5, t6, e8, m1, ta, ma
+    vle8.v v2, (t1)
+    add t1, t1, t5
+    vle8.v v4, (a2)
+    add a2, a2, t5
+    sub a5, a5, t4
+
+    vwmulu.vx v8, v4, t4
+    vsetvli zero, zero, e16, m2, ta, ma
+    mul a5, a5, t3
+
+    vadd.vx v4, v8, a5
+    vsetvli zero, zero, e8, m1, ta, ma
+    vwmulu.vx v8, v2, a7
+
+    vneg.v v12, v2
+    vwmaccu.vx v8, t2, v12
+    vsetvli zero, zero, e16, m2, ta, ma
+    vwaddu.vv v12, v4, v8
+
+    sub a5, a3, t6
+    sub t6, t6, t5
+    add a5, a5, a0
+    vnclipu.wi v2, v12, 9
+    vsetvli zero, zero, e8, m1, ta, ma
+    vnclipu.wi v0, v2, 0
+    vse8.v v0, (a5)
+
+    bnez t6, 2b
+
+    sub t1, t1, a3
+    add a0, a0, a1
+    sub a2, a2, a3
+    addi a4, a4, -1
+    addi t0, t0, 1
+    addi a6, a6, -1
+    bnez a4, 1b
+
+    ret
+endfunc
+
+function ipred_smooth_v_8bpc_rvv, export=1, ext="v,zba"
+    csrw vxrm, zero
+    la t0, dav1d_sm_weights
+    add t2, a2, a3
+    add t0, t0, a4
+    sub t3, a2, a4
+    addi a2, a2, 1
+    lbu t3, (t3)
+1:
+    mv t6, a3
+
+    lbu t4, (t0)
+2:
+    li a5, 256
+    vsetvli t5, t6, e8, m1, ta, ma
+    vle8.v v4, (a2)
+    add a2, a2, t5
+    sub a5, a5, t4
+
+    vwmulu.vx v8, v4, t4
+    vsetvli zero, zero, e16, m2, ta, ma
+    mul a5, a5, t3
+    vwaddu.vx v4, v8, a5
+
+    sub a5, a3, t6
+    sub t6, t6, t5
+    add a5, a5, a0
+    vsetvli zero, zero, e16, m2, ta, ma
+    vnclipu.wi v2, v4, 8
+    vsetvli zero, zero, e8, m1, ta, ma
+    vnclipu.wi v0, v2, 0
+    vse8.v v0, (a5)
+
+    bnez t6, 2b
+
+    add a0, a0, a1
+    sub a2, a2, a3
+    addi a4, a4, -1
+    addi t0, t0, 1
+    bnez a4, 1b
+
+    ret
+endfunc
+
+function ipred_smooth_h_8bpc_rvv, export=1, ext="v,zba"
+    csrw vxrm, zero
+    la t0, dav1d_sm_weights
+    add t1, t0, a3
+    add t2, a2, a3
+    lbu t2, (t2)
+    addi a6, a2, -1
+1:
+    mv t6, a3
+
+    lbu a7, (a6)
+2:
+    vsetvli t5, t6, e8, m1, ta, ma
+    vle8.v v2, (t1)
+    add t1, t1, t5
+
+    vwmulu.vx v8, v2, a7
+
+    vneg.v v12, v2
+    vwmaccu.vx v8, t2, v12
+
+    sub a5, a3, t6
+    sub t6, t6, t5
+    add a5, a5, a0
+    vsetvli zero, zero, e8, m1, ta, ma
+    vnclipu.wi v0, v8, 8
+    vse8.v v0, (a5)
+
+    bnez t6, 2b
+
+    sub t1, t1, a3
+    add a0, a0, a1
+    addi a4, a4, -1
+    addi a6, a6, -1
+    bnez a4, 1b
+
+    ret
+endfunc
+
+function pal_pred_8bpc_rvv, export=1, ext="v,zba"
+    csrw vxrm, zero
+    vsetivli t5, 8, e8, m1, ta, ma
+    vle8.v v30, (a2)
+    li t0, 2
+    srli t1, a4, 1
+1:
+    mv t4, a4
+2:
+    vsetvli t5, t1, e8, m1, ta, ma
+    vle8.v v0, (a3)
+    add a3, a3, t5
+    vsrl.vi v2, v0, 4
+    sub t6, a4, t4
+    vand.vi v1, v0, 7
+    add t6, a0, t6
+    vrgather.vv v3, v30, v1
+    addi t2, t6, 1
+    vrgather.vv v4, v30, v2
+    slli t5, t5, 1
+    vsse8.v v3, (t6), t0
+    sub t4, t4, t5
+    vsse8.v v4, (t2), t0
+
+    bnez t4, 2b
+    addi a5, a5, -1
+    add a0, a0, a1
+    bnez a5, 1b
+    ret
+endfunc
diff --git a/src/riscv/64/ipred16.S b/src/riscv/64/ipred16.S
new file mode 100644
index 0000000..cac304c
--- /dev/null
+++ b/src/riscv/64/ipred16.S
@@ -0,0 +1,471 @@
+/******************************************************************************
+ * Copyright © 2018, VideoLAN and dav1d authors
+ * Copyright © 2024, Bogdan Gligorijevic
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *****************************************************************************/
+
+#include "src/riscv/asm.S"
+
+function dc_gen_16bpc_rvv, export=1, ext="v,zba,zbb"
+    .variant_cc dav1d_dc_gen_8bpc_rvv
+    add t1, a1, a2
+    srli t5, t1, 1
+    mv t1, a1
+    addi t2, a0, 2
+    vsetvli zero, t1, e32, m8, ta, ma
+    vmv.v.x v0, zero
+1:
+    vsetvli t3, t1, e16, m4, tu, ma
+    vle16.v v8, (t2)
+    vwaddu.wv v0, v0, v8
+    sub t1, t1, t3
+
+    sh1add t2, t3, t2
+    bnez t1, 1b
+
+    mv t1, a2
+    mv t2, a0
+    vsetvli zero, t1, e32, m8, ta, ma
+    vmv.v.x v16, zero
+2:
+    vsetvli t3, t1, e16, m4, tu, ma
+    sub t1, t1, t3
+    sll t3, t3, 1
+    sub t2, t2, t3
+    vle16.v v8, (t2)
+    vwaddu.wv v16, v16, v8
+
+    bnez t1, 2b
+
+    vsetvli zero, a1, e32, m8, ta, ma
+    vmv.s.x v24, t5
+    vmv.s.x v25, zero
+    vredsum.vs v8, v0, v24
+    vsetvli zero, a2, e32, m8, ta, ma
+    vredsum.vs v0, v16, v25
+    vmv.x.s t5, v8
+    vmv.x.s t1, v0
+    add t5, t5, t1
+
+    add t1, a1, a2
+    ctz t1, t1
+
+    srl a0, t5, t1
+
+    beq a1, a2, 5f
+    slli t1, a1, 1
+    sltu t2, t1, a2
+    slli t3, a2, 1
+    sltu t1, t3, a1
+    or t1, t1, t2
+    bnez t1, 3f
+
+    li t1, 0xAAAB
+    j 4f
+3:
+    li t1, 0x6667
+4:
+    mul a0, a0, t1
+    li t1, 17
+    srl a0, a0, t1
+5:
+    jr t0
+endfunc
+
+function dc_gen_top_16bpc_rvv, export=1, ext="v,zba,zbb"
+    .variant_cc dav1d_dc_gen_top_16bpc_rvv
+    mv t1, a1
+    srli t5, a1, 1
+    addi a0, a0, 2
+    vsetvli zero, t1, e32, m2, ta, ma
+    vmv.v.x v0, zero
+1:
+    vsetvli t3, t1, e16, m1, tu, ma
+    vle16.v v4, (a0)
+    vwaddu.wv v0, v0, v4
+
+    sh1add a0, t3, a0
+    sub t1, t1, t3
+    bnez t1, 1b
+
+    j dc_gen_sum_up_16bpc_rvv
+endfunc
+
+function dc_gen_left_16bpc_rvv, export=1, ext="v,zba,zbb"
+    .variant_cc dav1d_dc_gen_left_16bpc_rvv
+    mv t1, a1
+    srli t5, a1, 1
+    vsetvli zero, t1, e32, m2, ta, ma
+    vmv.v.x v0, zero
+1:
+    vsetvli t3, t1, e16, m1, tu, ma
+    sub t1, t1, t3
+    slli t3, t3, 1
+    sub a0, a0, t3
+    vle16.v v4, (a0)
+    vwaddu.wv v0, v0, v4
+
+    bnez t1, 1b
+
+    j dc_gen_sum_up_16bpc_rvv
+endfunc
+
+function dc_gen_sum_up_16bpc_rvv, export=1, ext="v,zba,zbb"
+    .variant_cc dav1d_dc_gen_sum_up_16bpc_rvv
+
+    vsetvli zero, a1, e32, m2, ta, ma
+    vmv.s.x v4, t5
+    vredsum.vs v8, v0, v4
+    vmv.x.s t5, v8
+
+    ctz t1, a1
+
+    srl a0, t5, t1
+    jr t0
+endfunc
+
+function cfl_pred_16bpc_rvv, export=1, ext="v,zba"
+    csrw vxrm, zero
+1:
+    li t2, 0
+    mv t3, a2
+2:
+    vsetvli t0, t3, e16, m2, ta, ma
+    sh1add t4, t2, a0
+    vle16.v v0, (a5)
+    sh1add a5, t0, a5
+
+    vwmul.vx v4, v0, a6
+    vsetvli zero, zero, e32, m4, ta, mu
+    vneg.v v8, v4
+    vmslt.vx v0, v4, x0
+    vmax.vv v12, v8, v4
+    vssra.vi v16, v12, 6
+    vneg.v v16, v16, v0.t
+    vadd.vx v20, v16, a4
+    vmax.vx v0, v20, zero
+    vmin.vx v0, v0, a7
+    vsetvli zero, zero, e16, m2, ta, ma
+    vnclipu.wi v4, v0, 0
+    vse16.v v4, (t4)
+    add t2, t0, t2
+    sub t3, t3, t0
+    bnez t3, 2b
+    addi a3, a3, -1
+    add a0, a0, a1
+
+    bnez a3, 1b
+    ret
+endfunc
+
+function ipred_cfl_16bpc_rvv, export=1, ext=v
+    mv t6, a0 # dst
+    mv a0, a2 # topleft
+    mv t4, a1 # stride
+    mv a1, a3 # width
+    mv a2, a4 # height
+    jal t0, dc_gen_16bpc_rvv
+    mv a2, a3 # width
+    mv a3, a4 # height
+    mv a4, a0 # dc_get_top
+    mv a0, t6 # dst
+    mv a1, t4 # stride
+    j cfl_pred_16bpc_rvv
+endfunc
+
+function ipred_cfl_128_16bpc_rvv, export=1, ext="v,zba"
+    # dc = (bitdepth_max + 1) >> 1, then just rearrange registers
+    mv a2, a3
+    mv a3, a4
+    addi a4, a7, 1
+    srli a4, a4, 1
+
+    j cfl_pred_16bpc_rvv
+endfunc
+
+function ipred_cfl_top_16bpc_rvv, export=1, ext=v
+    mv t6, a0 # dst
+    mv a0, a2 # topleft
+    mv t4, a1 # stride
+    mv a1, a3 # width
+    jal t0, dc_gen_top_16bpc_rvv
+    mv a3, a4 # height
+    mv a4, a0 # dc_get_top
+    mv a0, t6 # dst
+    mv a2, a1 # width
+    mv a1, t4 # stride
+    j cfl_pred_16bpc_rvv
+endfunc
+
+function ipred_cfl_left_16bpc_rvv, export=1, ext=v
+    mv t6, a0 # dst
+    mv a0, a2 # topleft
+    mv t4, a1 # stride
+    mv a1, a4 # height
+    mv a2, a3 # width
+    jal t0, dc_gen_left_16bpc_rvv
+    mv a3, a4 # height
+    mv a4, a0 # dc_get_top
+    mv a1, t4 # stride
+    mv a0, t6 # dst
+    j cfl_pred_16bpc_rvv
+endfunc
+
+function ipred_paeth_16bpc_rvv, export=1, ext="v,zba"
+    csrw vxrm, zero
+    li t0, 0
+    mv t3, a2
+    lhu t1, (a2)
+    addi a6, a2, -2
+    addi a2, a2, 2
+1:
+    lhu t2, (a6)
+    mv t3, a3
+2:
+    sub t5, a3, t3
+    sh1add t5, t5, a2
+    vsetvli t6, t3, e16, m2, ta, ma
+    vle16.v v2, (t5)
+    vwaddu.vx v4, v2, t2
+
+    vsetvli zero, zero, e32, m4, ta, mu
+    vsub.vx v8, v4, t1
+    vzext.vf2 v24, v2
+    vsub.vx v12, v8, t1
+    vmslt.vx v0, v12, zero
+    vneg.v v12, v12, v0.t
+    vsub.vx v16, v8, t2
+    vmslt.vx v0, v16, zero
+    vneg.v v16, v16, v0.t
+    vsub.vv v20, v8, v24
+    vmslt.vx v0, v20, zero
+    vneg.v v20, v20, v0.t
+
+    sub t5, a3, t3
+    vmsleu.vv v4, v16, v20
+    vmsleu.vv v5, v16, v12
+    vmsgtu.vv v0, v20, v12
+    vmand.mm v6, v4, v5
+
+    vsetvli zero, zero, e16, m2, ta, ma
+    vmerge.vxm v8, v2, t1, v0
+    vmmv.m v0, v6
+    sh1add t5, t5, a0
+    sub t3, t3, t6
+    vmerge.vxm v4, v8, t2, v0
+
+    vse16.v v4, (t5)
+
+    bnez t3, 2b
+
+    addi a4, a4, -1
+    addi a6, a6, -2
+    add a0, a0, a1
+    bnez a4, 1b
+    ret
+endfunc
+
+function ipred_smooth_16bpc_rvv, export=1, ext="v,zba"
+    csrw vxrm, zero
+    la t0, dav1d_sm_weights
+    add t1, t0, a3
+    sh1add t2, a3, a2
+    slli t3, a4, 1
+    add t0, t0, a4
+    lhu t2, (t2)
+    sub t3, a2, t3
+    addi a6, a2, -2
+    addi a2, a2, 2
+    lhu t3, (t3)
+1:
+    mv t6, a3
+
+    lhu a7, (a6)
+    lbu t4, (t0)
+2:
+    li a5, 256
+    vsetvli t5, t6, e16, m2, ta, ma
+    vle8.v v2, (t1)
+    add t1, t1, t5
+    vle16.v v4, (a2)
+    sh1add a2, t5, a2
+    sub a5, a5, t4
+
+    vwmul.vx v8, v4, t4
+    mul a5, a5, t3
+
+    vsetvli zero, zero, e32, m4, ta, ma
+    vadd.vx v4, v8, a5
+
+    li a5, 256
+    vzext.vf4 v12, v2
+    vmul.vx v8, v12, a7
+
+    vrsub.vx v12, v12, a5
+    vmacc.vx v8, t2, v12
+    vadd.vv v12, v4, v8
+    vsetvli zero, zero, e32, m4, ta, ma
+
+    sub a5, a3, t6
+    sub t6, t6, t5
+    sh1add a5, a5, a0
+    vsetvli zero, zero, e16, m2, ta, ma
+    vnclipu.wi v2, v12, 9
+    vse16.v v2, (a5)
+
+    bnez t6, 2b
+
+    sub t1, t1, a3
+    slli t6, a3, 1
+    add a0, a0, a1
+    sub a2, a2, t6
+    addi a4, a4, -1
+    addi t0, t0, 1
+    addi a6, a6, -2
+    bnez a4, 1b
+
+    ret
+endfunc
+
+function ipred_smooth_v_16bpc_rvv, export=1, ext="v,zba"
+    csrw vxrm, zero
+    la t0, dav1d_sm_weights
+    slli t3, a4, 1
+    add t0, t0, a4
+    sub t3, a2, t3
+    addi a2, a2, 2
+    lhu t3, (t3)
+1:
+    mv t6, a3
+
+    lbu t4, (t0)
+2:
+    li a5, 256
+    vsetvli t5, t6, e16, m2, ta, ma
+    vle16.v v4, (a2)
+    sh1add a2, t5, a2
+    sub a5, a5, t4
+
+    vwmul.vx v8, v4, t4
+    mul a5, a5, t3
+
+    vsetvli zero, zero, e32, m4, ta, ma
+    vadd.vx v4, v8, a5
+    vsetvli zero, zero, e32, m4, ta, ma
+
+    sub a5, a3, t6
+    sub t6, t6, t5
+    sh1add a5, a5, a0
+    vsetvli zero, zero, e16, m2, ta, ma
+    vnclipu.wi v2, v4, 8
+    vse16.v v2, (a5)
+
+    bnez t6, 2b
+
+    slli t6, a3, 1
+    add a0, a0, a1
+    sub a2, a2, t6
+    addi a4, a4, -1
+    addi t0, t0, 1
+    bnez a4, 1b
+
+    ret
+endfunc
+
+function ipred_smooth_h_16bpc_rvv, export=1, ext="v,zba"
+    csrw vxrm, zero
+    la t0, dav1d_sm_weights
+    add t1, t0, a3
+    sh1add t2, a3, a2
+    lhu t2, (t2)
+    addi a6, a2, -2
+1:
+    mv t6, a3
+
+    lhu a7, (a6)
+2:
+    vsetvli t5, t6, e16, m2, ta, ma
+    vle8.v v2, (t1)
+    add t1, t1, t5
+
+    li a5, 256
+    vsetvli zero, zero, e32, m4, ta, ma
+    vzext.vf4 v12, v2
+    vmul.vx v8, v12, a7
+
+    vrsub.vx v12, v12, a5
+    vmacc.vx v8, t2, v12
+
+    sub a5, a3, t6
+    sub t6, t6, t5
+    sh1add a5, a5, a0
+    vsetvli zero, zero, e16, m2, ta, ma
+    vnclipu.wi v2, v8, 8
+    vse16.v v2, (a5)
+
+    bnez t6, 2b
+
+    sub t1, t1, a3
+    add a0, a0, a1
+    addi a4, a4, -1
+    addi a6, a6, -2
+    bnez a4, 1b
+
+    ret
+endfunc
+
+function pal_pred_16bpc_rvv, export=1, ext="v,zba"
+    csrw vxrm, zero
+    vsetivli t5, 8, e16, m1, ta, ma
+    vle16.v v30, (a2)
+    li t0, 4
+    srli t1, a4, 1
+    li t2, 1
+1:
+    mv t4, a4
+2:
+    vsetvli t5, t1, e8, mf2, ta, ma
+    vle8.v v0, (a3)
+    add a3, a3, t5
+    vand.vi v1, v0, 7
+    sub t6, a4, t4
+    vsrl.vi v2, v0, 4
+    vwmul.vx v4, v1, t2
+    vwmul.vx v6, v2, t2
+    vsetvli zero, zero, e16, m1, ta, ma
+    sh1add t6, t6, a0
+    vrgather.vv v8, v30, v4
+    addi t3, t6, 2
+    vrgather.vv v10, v30, v6
+    slli t5, t5, 1
+    vsse16.v v8, (t6), t0
+    vsse16.v v10, (t3), t0
+
+    sub t4, t4, t5
+    bnez t4, 2b
+    add a0, a0, a1
+    addi a5, a5, -1
+    bnez a5, 1b
+    ret
+endfunc
diff --git a/src/riscv/64/itx.S b/src/riscv/64/itx.S
index dfec548..e7e9915 100644
--- a/src/riscv/64/itx.S
+++ b/src/riscv/64/itx.S
@@ -145,17 +145,10 @@ endfunc
   vwmacc.vx v20, t2, \o3
   vwmacc.vx v22, t3, \o3
 
-  li t1, 2048
-
-  vwadd.wx v16, v16, t1
-  vwadd.wx v18, v18, t1
-  vwadd.wx v20, v20, t1
-  vwadd.wx v22, v22, t1
-
-  vnsra.wi v16, v16, 12
-  vnsra.wi v18, v18, 12
-  vnsra.wi v20, v20, 12
-  vnsra.wi v22, v22, 12
+  vnclip.wi v16, v16, 12
+  vnclip.wi v18, v18, 12
+  vnclip.wi v20, v20, 12
+  vnclip.wi v22, v22, 12
 
   vsadd.vv \o0, v16, v20
   vsadd.vv \o1, v18, v22
@@ -192,19 +185,12 @@ endfunc
   vadd.vv v18, v18, v22
   vsub.vv v22, v24, v22
 
-  li t1, 2048
-
-  vadd.vx v16, v16, t1
-  vadd.vx v18, v18, t1
-  vadd.vx v20, v20, t1
-  vadd.vx v22, v22, t1
-
   vsetvli zero, zero, e16, \lm, ta, ma
 
-  vnsra.wi \o0, v16, 12
-  vnsra.wi \o1, v18, 12
-  vnsra.wi \o2, v20, 12
-  vnsra.wi \o3, v22, 12
+  vnclip.wi \o0, v16, 12
+  vnclip.wi \o1, v18, 12
+  vnclip.wi \o2, v20, 12
+  vnclip.wi \o3, v22, 12
 .endm
 
 function inv_dct_e16_x4_rvv, export=1, ext=v
@@ -491,17 +477,10 @@ endfunc
   vwmacc.vx v20, t3, \o3
   vwmacc.vx v18, t4, \o3
 
-  li t1, 2048
-
-  vwadd.wx v16, v16, t1
-  vwadd.wx v18, v18, t1
-  vwadd.wx v20, v20, t1
-  vwadd.wx v22, v22, t1
-
-  vnsra.wi v16, v16, 12
-  vnsra.wi v18, v18, 12
-  vnsra.wi v20, v20, 12
-  vnsra.wi v22, v22, 12
+  vnclip.wi v16, v16, 12
+  vnclip.wi v18, v18, 12
+  vnclip.wi v20, v20, 12
+  vnclip.wi v22, v22, 12
 
   vssub.vv \o7, v22, v20
   vsadd.vv v22, v22, v20
@@ -516,11 +495,8 @@ endfunc
   neg t2, t2
   vwmacc.vx v18, t2, \o1
 
-  vwadd.wx v18, v18, t1
-  vwadd.wx v20, v20, t1
-
-  vnsra.wi v18, v18, 12
-  vnsra.wi v20, v20, 12
+  vnclip.wi v18, v18, 12
+  vnclip.wi v20, v20, 12
 
   vssub.vv \o7, \o0, v22
   vsadd.vv \o0, \o0, v22
@@ -559,7 +535,6 @@ endfunc
   vwmacc.vx v24, t6, v4
   vwmacc.vx v26, t5, v4
 
-  li t1, 2048
   li t2, 1189
   li t3, 3920
   li t4, 1567
@@ -572,23 +547,14 @@ endfunc
   vwmacc.vx v28, t3, v6
   vwmacc.vx v30, t2, v6
 
-  vwadd.wx v16, v16, t1
-  vwadd.wx v18, v18, t1
-  vwadd.wx v20, v20, t1
-  vwadd.wx v22, v22, t1
-  vwadd.wx v24, v24, t1
-  vwadd.wx v26, v26, t1
-  vwadd.wx v28, v28, t1
-  vwadd.wx v30, v30, t1
-
-  vnsra.wi v16, v16, 12
-  vnsra.wi v18, v18, 12
-  vnsra.wi v20, v20, 12
-  vnsra.wi v22, v22, 12
-  vnsra.wi v24, v24, 12
-  vnsra.wi v26, v26, 12
-  vnsra.wi v28, v28, 12
-  vnsra.wi v30, v30, 12
+  vnclip.wi v16, v16, 12
+  vnclip.wi v18, v18, 12
+  vnclip.wi v20, v20, 12
+  vnclip.wi v22, v22, 12
+  vnclip.wi v24, v24, 12
+  vnclip.wi v26, v26, 12
+  vnclip.wi v28, v28, 12
+  vnclip.wi v30, v30, 12
 
   vssub.vv  v4, v16, v24
   vsadd.vv v16, v16, v24
@@ -615,15 +581,10 @@ endfunc
   vwmacc.vx v20, t4, v6
   vwmacc.vx v18, t5, v5
 
-  vwadd.wx v16, v16, t1
-  vwadd.wx v18, v18, t1
-  vwadd.wx v20, v20, t1
-  vwadd.wx v22, v22, t1
-
-  vnsra.wi v16, v16, 12
-  vnsra.wi v18, v18, 12
-  vnsra.wi v20, v20, 12
-  vnsra.wi v22, v22, 12
+  vnclip.wi v16, v16, 12
+  vnclip.wi v18, v18, 12
+  vnclip.wi v20, v20, 12
+  vnclip.wi v22, v22, 12
 
   vsadd.vv \o1, v16, v20
   vsadd.vv \o6, v18, v22
@@ -640,15 +601,10 @@ endfunc
   vwmacc.vx v20, t6, v3
   vwmacc.vx v24, t6, v17
 
-  vwadd.wx v18, v18, t1
-  vwadd.wx v20, v20, t1
-  vwadd.wx v22, v22, t1
-  vwadd.wx v24, v24, t1
-
-  vnsra.wi \o3, v18, 12
-  vnsra.wi \o4, v20, 12
-  vnsra.wi \o2, v22, 12
-  vnsra.wi \o5, v24, 12
+  vnclip.wi \o3, v18, 12
+  vnclip.wi \o4, v20, 12
+  vnclip.wi \o2, v22, 12
+  vnclip.wi \o5, v24, 12
 
   vmv.v.x v16, zero
   vssub.vv \o1, v16, \o1
@@ -972,28 +928,18 @@ function inv_dct_e16_x16_rvv, export=1, ext=v
   vwmacc.vx v24, t3, v3
   vwmacc.vx v22, t4, v3
 
-  li t1, 2048
   li t2, 2896
   li t3, 1567
   li t4, 3784
 
-  vwadd.wx v16, v16, t1
-  vwadd.wx v18, v18, t1
-  vwadd.wx v20, v20, t1
-  vwadd.wx v22, v22, t1
-  vwadd.wx v24, v24, t1
-  vwadd.wx v26, v26, t1
-  vwadd.wx v28, v28, t1
-  vwadd.wx v30, v30, t1
-
-  vnsra.wi v16, v16, 12
-  vnsra.wi v18, v18, 12
-  vnsra.wi v20, v20, 12
-  vnsra.wi v22, v22, 12
-  vnsra.wi v24, v24, 12
-  vnsra.wi v26, v26, 12
-  vnsra.wi v28, v28, 12
-  vnsra.wi v30, v30, 12
+  vnclip.wi v16, v16, 12
+  vnclip.wi v18, v18, 12
+  vnclip.wi v20, v20, 12
+  vnclip.wi v22, v22, 12
+  vnclip.wi v24, v24, 12
+  vnclip.wi v26, v26, 12
+  vnclip.wi v28, v28, 12
+  vnclip.wi v30, v30, 12
 
   vssub.vv  v3, v16, v18
   vsadd.vv v16, v16, v18
@@ -1015,15 +961,10 @@ function inv_dct_e16_x16_rvv, export=1, ext=v
   vwmacc.vx v20, t3, v5
   vwmacc.vx v26, t4, v5
 
-  vwadd.wx v18, v18, t1
-  vwadd.wx v20, v20, t1
-  vwadd.wx v26, v26, t1
-  vwadd.wx v28, v28, t1
-
-  vnsra.wi v18, v18, 12
-  vnsra.wi v20, v20, 12
-  vnsra.wi v26, v26, 12
-  vnsra.wi v28, v28, 12
+  vnclip.wi v18, v18, 12
+  vnclip.wi v20, v20, 12
+  vnclip.wi v26, v26, 12
+  vnclip.wi v28, v28, 12
 
   vssub.vv  v5, v18, v20
   vsadd.vv v18, v18, v20
@@ -1045,15 +986,10 @@ function inv_dct_e16_x16_rvv, export=1, ext=v
   vwmacc.vx v20, t2, v5
   vwmacc.vx v22, t2, v7
 
-  vwadd.wx v20, v20, t1
-  vwadd.wx v22, v22, t1
-  vwadd.wx v24, v24, t1
-  vwadd.wx v26, v26, t1
-
-  vnsra.wi v20, v20, 12
-  vnsra.wi v22, v22, 12
-  vnsra.wi v24, v24, 12
-  vnsra.wi v26, v26, 12
+  vnclip.wi v20, v20, 12
+  vnclip.wi v22, v22, 12
+  vnclip.wi v24, v24, 12
+  vnclip.wi v26, v26, 12
 
   vssub.vv v15,  v0, v30
   vsadd.vv  v0,  v0, v30
@@ -1112,25 +1048,14 @@ endfunc
   vwmacc.vx v28, t4, v6
   vwmacc.vx v30, t3, v6
 
-  li t1, 2048
-
-  vwadd.wx v16, v16, t1
-  vwadd.wx v18, v18, t1
-  vwadd.wx v20, v20, t1
-  vwadd.wx v22, v22, t1
-  vwadd.wx v24, v24, t1
-  vwadd.wx v26, v26, t1
-  vwadd.wx v28, v28, t1
-  vwadd.wx v30, v30, t1
-
-  vnsra.wi  v0, v16, 12
-  vnsra.wi v18, v18, 12
-  vnsra.wi  v2, v20, 12
-  vnsra.wi v22, v22, 12
-  vnsra.wi  v4, v24, 12
-  vnsra.wi v26, v26, 12
-  vnsra.wi  v6, v28, 12
-  vnsra.wi v30, v30, 12
+  vnclip.wi  v0, v16, 12
+  vnclip.wi v18, v18, 12
+  vnclip.wi  v2, v20, 12
+  vnclip.wi v22, v22, 12
+  vnclip.wi  v4, v24, 12
+  vnclip.wi v26, v26, 12
+  vnclip.wi  v6, v28, 12
+  vnclip.wi v30, v30, 12
 
   li t1, 2751
   li t2, 3035
@@ -1149,17 +1074,10 @@ endfunc
   vwmacc.vx v24, t4, v10
   vwmacc.vx v28, t3, v10
 
-  li t1, 2048
-
-  vwadd.wx v16, v16, t1
-  vwadd.wx v20, v20, t1
-  vwadd.wx v24, v24, t1
-  vwadd.wx v28, v28, t1
-
-  vnsra.wi v16, v16, 12
-  vnsra.wi  v9, v20, 12
-  vnsra.wi v24, v24, 12
-  vnsra.wi v11, v28, 12
+  vnclip.wi v16, v16, 12
+  vnclip.wi  v9, v20, 12
+  vnclip.wi v24, v24, 12
+  vnclip.wi v11, v28, 12
 
   vssub.vv  v8,  v0, v16
   vsadd.vv  v0,  v0, v16
@@ -1183,17 +1101,10 @@ endfunc
   vwmacc.vx v24, t4, v14
   vwmacc.vx v28, t3, v14
 
-  li t1, 2048
-
-  vwadd.wx v16, v16, t1
-  vwadd.wx v20, v20, t1
-  vwadd.wx v24, v24, t1
-  vwadd.wx v28, v28, t1
-
-  vnsra.wi v16, v16, 12
-  vnsra.wi v13, v20, 12
-  vnsra.wi v24, v24, 12
-  vnsra.wi v15, v28, 12
+  vnclip.wi v16, v16, 12
+  vnclip.wi v13, v20, 12
+  vnclip.wi v24, v24, 12
+  vnclip.wi v15, v28, 12
 
   vssub.vv v12,  v4, v16
   vsadd.vv v16,  v4, v16
@@ -1244,28 +1155,18 @@ endfunc
   vwmacc.vx v24, t1, v12
   vwmacc.vx v28, t3, v14
 
-  li t1, 2048
   li t2, 2896
   li t3, 1567
   li t4, 3784
 
-  vwadd.wx v16, v16, t1
-  vwadd.wx v18, v18, t1
-  vwadd.wx v20, v20, t1
-  vwadd.wx v22, v22, t1
-  vwadd.wx v24, v24, t1
-  vwadd.wx v26, v26, t1
-  vwadd.wx v28, v28, t1
-  vwadd.wx v30, v30, t1
-
-  vnsra.wi v16, v16, 12
-  vnsra.wi v18, v18, 12
-  vnsra.wi v20, v20, 12
-  vnsra.wi v22, v22, 12
-  vnsra.wi v24, v24, 12
-  vnsra.wi v26, v26, 12
-  vnsra.wi v28, v28, 12
-  vnsra.wi v30, v30, 12
+  vnclip.wi v16, v16, 12
+  vnclip.wi v18, v18, 12
+  vnclip.wi v20, v20, 12
+  vnclip.wi v22, v22, 12
+  vnclip.wi v24, v24, 12
+  vnclip.wi v26, v26, 12
+  vnclip.wi v28, v28, 12
+  vnclip.wi v30, v30, 12
 
   vsadd.vv  v8, v16, v24
   vsadd.vv  v9, v18, v26
@@ -1295,23 +1196,14 @@ endfunc
   vwmacc.vx v18, t4,  v5
   vwmacc.vx v26, t4, v13
 
-  vwadd.wx v16, v16, t1
-  vwadd.wx v18, v18, t1
-  vwadd.wx v20, v20, t1
-  vwadd.wx v22, v22, t1
-  vwadd.wx v24, v24, t1
-  vwadd.wx v26, v26, t1
-  vwadd.wx v28, v28, t1
-  vwadd.wx v30, v30, t1
-
-  vnsra.wi v16, v16, 12
-  vnsra.wi v18, v18, 12
-  vnsra.wi v20, v20, 12
-  vnsra.wi v22, v22, 12
-  vnsra.wi v24, v24, 12
-  vnsra.wi v26, v26, 12
-  vnsra.wi v28, v28, 12
-  vnsra.wi v30, v30, 12
+  vnclip.wi v16, v16, 12
+  vnclip.wi v18, v18, 12
+  vnclip.wi v20, v20, 12
+  vnclip.wi v22, v22, 12
+  vnclip.wi v24, v24, 12
+  vnclip.wi v26, v26, 12
+  vnclip.wi v28, v28, 12
+  vnclip.wi v30, v30, 12
 
 .ifc \o0, v0
   vsadd.vv \o14, v9, v11
@@ -1365,23 +1257,14 @@ endfunc
   vwmacc.vx v16, t2, v9
   vwmacc.vx v18, t3, v9
 
-  vwadd.wx v16, v16, t1
-  vwadd.wx v18, v18, t1
-  vwadd.wx v20, v20, t1
-  vwadd.wx v22, v22, t1
-  vwadd.wx v24, v24, t1
-  vwadd.wx v26, v26, t1
-  vwadd.wx v28, v28, t1
-  vwadd.wx v30, v30, t1
-
-  vnsra.wi  \o7, v16, 12
-  vnsra.wi  \o8, v18, 12
-  vnsra.wi  \o4, v20, 12
-  vnsra.wi \o11, v22, 12
-  vnsra.wi  \o6, v24, 12
-  vnsra.wi  \o9, v26, 12
-  vnsra.wi  \o5, v28, 12
-  vnsra.wi \o10, v30, 12
+  vnclip.wi  \o7, v16, 12
+  vnclip.wi  \o8, v18, 12
+  vnclip.wi  \o4, v20, 12
+  vnclip.wi \o11, v22, 12
+  vnclip.wi  \o6, v24, 12
+  vnclip.wi  \o9, v26, 12
+  vnclip.wi  \o5, v28, 12
+  vnclip.wi \o10, v30, 12
 
   vmv.v.x v16, zero
   vssub.vv  \o1, v16,  \o1
@@ -1552,6 +1435,9 @@ endfunc
 
 .macro def_fn_16x16 txfm1, txfm2, eob_half
 function inv_txfm_add_\txfm1\()_\txfm2\()_16x16_8bpc_rvv, export=1, ext=v
+.ifc \txfm1\()_\txfm2, dct_dct
+ beqz a3, 1f
+.endif
 .ifc \txfm1, identity
   la a6, inv_txfm_horz_identity_16x8_rvv
 .else
@@ -1561,6 +1447,75 @@ function inv_txfm_add_\txfm1\()_\txfm2\()_16x16_8bpc_rvv, export=1, ext=v
   la a5, inv_\txfm2\()_e16_x16_rvv
   li a7, \eob_half
   j inv_txfm_add_16x16_rvv
+.ifc \txfm1\()_\txfm2, dct_dct
+1:
+  csrw vxrm, zero
+  vsetivli zero, 16, e16, m2, ta, ma
+  lh t2, (a2)
+  li t3, 2896*8
+  li t4, 1<<14
+  li t5, 0xFFFF
+  li t6, -0x10000
+
+  sh x0, (a2)
+
+  mul t2, t2, t3
+  add t2, t2, t4
+  srai t2, t2, 15
+  ble t2, t5, 3f
+  mv t2, t5
+3:
+  ble t6, t2, 4f
+  mv t2, t6
+4:
+  addi t2, t2, 2
+  srai t2, t2, 2
+  mul t2, t2, t3
+  add t2, t2, t4
+  srai t2, t2, 15
+  ble t2, t5, 5f
+  mv t2, t5
+5:
+  ble t6, t2, 6f
+  mv t2, t6
+6:
+  addi t2, t2, 8
+  srai t2, t2, 4
+  vmv.v.x v24, t2
+
+  vsetvli zero, zero, e8, m1, ta, ma
+  add t2, a1, a1
+  li t3, 16
+2:
+  add t0, a0, a1
+  vle8.v v16, (a0)
+  vle8.v v17, (t0)
+
+  vwaddu.wv v0, v24, v16
+  vwaddu.wv v2, v24, v17
+
+  addi t3, t3, -2 # loop counter
+
+
+  vsetvli zero, zero, e16, m2, ta, ma
+.irp i, 0, 2
+  vmax.vx v\i, v\i, zero
+.endr
+
+  vsetvli zero, zero, e8, m1, ta, ma
+
+  vnclipu.wi  v16, v0, 0
+  vnclipu.wi  v17, v2, 0
+
+  add t0, a0, a1
+  vse8.v v16, (a0)
+  add a0, a0, t2
+  vse8.v v17, (t0)
+
+  bnez t3, 2b
+
+  ret
+.endif
 endfunc
 .endm
 
diff --git a/src/riscv/64/mc.S b/src/riscv/64/mc.S
new file mode 100644
index 0000000..536185e
--- /dev/null
+++ b/src/riscv/64/mc.S
@@ -0,0 +1,532 @@
+/******************************************************************************
+ * Copyright © 2018, VideoLAN and dav1d authors
+ * Copyright © 2024, Nathan Egge, Niklas Haas, Bogdan Gligorijevic
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *****************************************************************************/
+
+#include "src/riscv/asm.S"
+
+function blend_vl256_8bpc_rvv, export=1, ext=zbb
+  ctz t0, a3
+  addi t0, t0, 0xc3
+  j L(blend_epilog)
+endfunc
+
+function blend_8bpc_rvv, export=1, ext="v,zbb"
+  ctz t0, a3
+  addi t0, t0, 0xc4
+L(blend_epilog):
+  csrw vxrm, zero
+  andi t0, t0, 0xc7
+  vsetvl zero, a3, t0
+  li t1, 64
+1:
+  addi a4, a4, -2
+  vle8.v v4, (a2)
+  add a2, a2, a3
+  vle8.v v6, (a2)
+  add a2, a2, a3
+  vle8.v v8, (a5)
+  add a5, a5, a3
+  vle8.v v10, (a5)
+  add a5, a5, a3
+  vle8.v v0, (a0)
+  add t0, a0, a1
+  vle8.v v2, (t0)
+  vwmulu.vv v16, v4, v8
+  vwmulu.vv v20, v6, v10
+  vrsub.vx v8, v8, t1
+  vrsub.vx v10, v10, t1
+  vwmaccu.vv v16, v0, v8
+  vwmaccu.vv v20, v2, v10
+  vnclipu.wi v0, v16, 6
+  vnclipu.wi v2, v20, 6
+  vse8.v v0, (a0)
+  vse8.v v2, (t0)
+  add a0, t0, a1
+  bnez a4, 1b
+  ret
+endfunc
+
+function blend_h_vl256_8bpc_rvv, export=1, ext=zbb
+  srai t0, a3, 2
+  li t2, 64
+  ctz t0, t0
+  addi t0, t0, 0xc5
+  j L(blend_h_epilog)
+endfunc
+
+function blend_h_8bpc_rvv, export=1, ext="v,zbb"
+  li t2, 64
+  bgt a3, t2, 128f
+  ctz t0, a3
+  addi t0, t0, 0xc4
+L(blend_h_epilog):
+  csrw vxrm, zero
+  andi t0, t0, 0xc7
+  vsetvl zero, a3, t0
+  la t1, dav1d_obmc_masks
+  srai t0, a4, 2
+  add t1, t1, a4
+  sub a4, a4, t0
+0:
+  mv t5, ra
+1:
+  addi a4, a4, -2
+  lbu t3, (t1)
+  addi t1, t1, 1
+  lbu t4, (t1)
+  addi t1, t1, 1
+  vle8.v v8, (a2)
+  add a2, a2, a3
+  vle8.v v12, (a2)
+  add a2, a2, a3
+  vle8.v v0, (a0)
+  add t0, a0, a1
+  vle8.v v4, (t0)
+  vwmulu.vx v16, v8, t3
+  vwmulu.vx v24, v12, t4
+  sub t3, t2, t3
+  sub t4, t2, t4
+  vwmaccu.vx v16, t3, v0
+  vwmaccu.vx v24, t4, v4
+  vnclipu.wi v0, v16, 6
+  vnclipu.wi v4, v24, 6
+  vse8.v v0, (a0)
+  vse8.v v4, (t0)
+  add a0, t0, a1
+  bgtz a4, 1b
+  jr t5
+128:
+  csrw vxrm, zero
+  vsetvli zero, t2, e8, m4, ta, ma
+  la t1, dav1d_obmc_masks
+  srai t0, a4, 2
+  add t1, t1, a4
+  sub a4, a4, t0
+  mv a5, a0
+  mv a6, a2
+  mv a7, a4
+  jal t5, 1b
+  add t1, t1, a4
+  add a0, a5, t2
+  add a2, a6, t2
+  mv a4, a7
+  sub t1, t1, a4
+  j 0b
+endfunc
+
+function blend_v_vl256_8bpc_rvv, export=1, ext=zbb
+  srai t0, a3, 2
+  ctz t0, t0
+  addi t0, t0, 0xc5
+  j L(blend_v_epilog)
+endfunc
+
+function blend_v_8bpc_rvv, export=1, ext="v,zbb"
+  ctz t0, a3
+  addi t0, t0, 0xc4
+L(blend_v_epilog):
+  andi t0, t0, 0xc7
+  vsetvl zero, a3, t0
+  csrw vxrm, zero
+  la t1, dav1d_obmc_masks
+  add t1, t1, a3
+  vle8.v v8, (t1)
+  li t0, 64
+  vrsub.vx v10, v8, t0
+1:
+  addi a4, a4, -2
+  vle8.v v4, (a2)
+  add a2, a2, a3
+  vle8.v v6, (a2)
+  add a2, a2, a3
+  vle8.v v0, (a0)
+  add t0, a0, a1
+  vle8.v v2, (t0)
+  vwmulu.vv v12, v4, v8
+  vwmulu.vv v16, v6, v8
+  vwmaccu.vv v12, v0, v10
+  vwmaccu.vv v16, v2, v10
+  vnclipu.wi v0, v12, 6
+  vnclipu.wi v2, v16, 6
+  vse8.v v0, (a0)
+  vse8.v v2, (t0)
+  add a0, t0, a1
+  bnez a4, 1b
+  ret
+endfunc
+
+.macro avg va, vb, vm
+    vadd.vv \va, \va, \vb
+.endm
+
+.macro w_avg va, vb, vm
+    vwmul.vx v24, \va, a6
+    vwmacc.vx v24, a7, \vb
+    vnclip.wi \va, v24, 8
+.endm
+
+.macro mask va, vb, vm
+    vwmul.vv v24, \va, \vm
+    vrsub.vx \vm, \vm, a7
+    vwmacc.vv v24, \vb, \vm
+    vnclip.wi \va, v24, 10
+.endm
+
+.macro bidir_fn type, shift
+function \type\()_8bpc_rvv, export=1, ext="v,zba,zbb"
+.ifc \type, w_avg
+    li a7, 16
+    sub a7, a7, a6
+.endif
+.ifc \type, mask
+    li a7, 64
+.endif
+    li t0, 4
+    csrw vxrm, zero
+    beq t0, a4, 4f
+    csrr t0, vlenb
+    ctz t1, a4
+    ctz t0, t0
+    li t2, 1
+    sub t0, t1, t0
+    li t4, -3
+    bgt t0, t2, 2f
+    max t0, t0, t4
+    andi t1, t0, 0x7
+    addi t0, t1, 1 # may overflow into E16 bit
+    ori t0, t0, MA | TA | E16
+    ori t1, t1, MA | TA | E8
+1:
+    addi a5, a5, -4
+.rept 2
+    vsetvl zero, a4, t0
+    sh1add t3, a4, a2
+    vle16.v v0, (a2)
+    sh1add a2, a4, t3
+    vle16.v v4, (t3)
+    sh1add t3, a4, a3
+    vle16.v v8, (a3)
+    sh1add a3, a4, t3
+    vle16.v v12, (t3)
+.ifc \type, mask
+    add t3, a4, a6
+    vle8.v v24, (a6)
+    add a6, a4, t3
+    vle8.v v26, (t3)
+    vzext.vf2 v16, v24
+    vzext.vf2 v20, v26
+.endif
+    \type v0, v8, v16
+    \type v4, v12, v20
+    vmax.vx v8, v0, zero
+    vmax.vx v12, v4, zero
+    vsetvl zero, zero, t1
+    vnclipu.wi v0, v8,  \shift
+    vnclipu.wi v2, v12, \shift
+    add t3, a1, a0
+    vse8.v v0, (a0)
+    add a0, a1, t3
+    vse8.v v2, (t3)
+.endr
+    bnez a5, 1b
+    ret
+2:
+    mv t0, a0
+    neg t4, a4
+    add a0, a1, a0
+    addi a5, a5, -1
+20:
+    vsetvli t2, a4, e16, m4, ta, ma
+    sh1add t4, t2, t4
+    sh1add t3, t2, a2
+    vle16.v v0, (a2)
+    sh1add a2, t2, t3
+    vle16.v v4, (t3)
+    sh1add t3, t2, a3
+    vle16.v v8, (a3)
+    sh1add a3, t2, t3
+    vle16.v v12, (t3)
+.ifc \type, mask
+    add t3, t2, a6
+    vle8.v v24, (a6)
+    add a6, t2, t3
+    vle8.v v26, (t3)
+    vzext.vf2 v16, v24
+    vzext.vf2 v20, v26
+.endif
+    \type v0, v8, v16
+    \type v4, v12, v20
+    vmax.vx v8, v0, zero
+    vmax.vx v12, v4, zero
+    vsetvli zero, zero, e8, m2, ta, ma
+    vnclipu.wi v0, v8,  \shift
+    vnclipu.wi v2, v12, \shift
+    add t3, t2, t0
+    vse8.v v0, (t0)
+    add t0, t2, t3
+    vse8.v v2, (t3)
+    bnez t4, 20b
+    bnez a5, 2b
+    ret
+4:
+    slli t0, a5, 2
+    vsetvli t1, t0, e16, m4, ta, ma
+    vle16.v v0, (a2)
+    sh1add a2, t1, a2
+    vle16.v v4, (a3)
+    sh1add a3, t1, a3
+.ifc \type, mask
+    vle8.v v16, (a6)
+    add a6, t1, a6
+    vzext.vf2 v8, v16
+.endif
+    \type v0, v4, v8
+    vmax.vx v8, v0, zero
+    vsetvli zero, zero, e8, m2, ta, ma
+    vnclipu.wi v0, v8, \shift
+    vsetvli t1, a5, e32, m2, ta, ma
+    vsse32.v v0, (a0), a1
+    ctz t0, t1
+    sub a5, a5, t1
+    sll t0, a1, t0
+    add a0, t0, a0
+    bnez a5, 4b
+    ret
+endfunc
+.endm
+
+bidir_fn avg,   5
+bidir_fn w_avg, 0
+bidir_fn mask,  0
+
+function warp_8x8_8bpc_rvv, export=1, ext="v"
+    csrw vxrm, zero
+
+    vsetivli zero, 8, e16, m1, ta, ma
+    addi sp, sp, -2*15*8
+    mv t5, sp
+    li t0, 3
+    mul t0, a3, t0
+    sub a2, a2, t0
+    addi a2, a2, -3
+
+    li t0, 64
+    addi a3, a3, -8
+    li t1, 15
+    la t2, dav1d_mc_warp_filter
+
+    lh t6, (a4)
+    lh t4, 2(a4)
+    vid.v v30
+    vwmul.vx v28, v30, t6
+1:
+    addi t1, t1, -1
+
+
+    vsetvli zero, zero, e32, m2, ta, ma
+    vadd.vx v4, v28, a5
+    add a5, a5, t4
+    vssra.vi v2, v4, 10
+    vadd.vx v2, v2, t0
+    vsll.vi v24, v2, 3
+    vsetvli zero, zero, e8, mf2, ta, ma
+
+    vluxseg8ei32.v v2, (t2), v24
+
+    vsetvli zero, zero, e16, m1, ta, ma
+.irp i, 2, 3, 4, 5, 6, 7, 8, 9
+    vle8.v v10, (a2)
+    addi a2, a2, 1
+
+    vsext.vf2 v14, v\i
+    vzext.vf2 v16, v10
+
+.if \i == 2
+    vwmulsu.vv v12, v14, v16
+.else
+    vwmaccsu.vv v12, v14, v16
+.endif
+.endr
+    vnclip.wi v10, v12, 3
+
+    add a2, a2, a3
+    vse16.v v10, (t5)
+    addi t5, t5, 16
+
+    bnez t1, 1b
+
+    mv t5, sp
+    li t1, 8
+
+    lh t6, 4(a4)
+    lh t4, 6(a4)
+    vwmul.vx v28, v30, t6
+2:
+    addi t1, t1, -1
+
+    vsetvli zero, zero, e32, m2, ta, ma
+    vadd.vx v4, v28, a6
+
+    add a6, a6, t4
+    vssra.vi v2, v4, 10
+    vadd.vx v2, v2, t0
+    vsll.vi v24, v2, 3
+    vsetvli zero, zero, e8, mf2, ta, ma
+
+    vluxseg8ei32.v v2, (t2), v24
+    vsetvli zero, zero, e16, m1, ta, ma
+
+.irp i, 2, 3, 4, 5, 6, 7, 8, 9
+    vle16.v v10, (t5)
+    addi t5, t5, 16
+
+    vsext.vf2 v14, v\i
+
+.if \i == 2
+    vwmul.vv v12, v14, v10
+.else
+    vwmacc.vv v12, v14, v10
+.endif
+.endr
+    addi t5, t5, -16*7
+    vnclip.wi v10, v12, 11
+
+    vmax.vx v10, v10, zero
+    vsetvli zero, zero, e8, mf2, ta, ma
+
+    vnclipu.wi v12, v10, 0
+
+    vse8.v v12, (a0)
+    add a0, a0, a1
+
+    bnez t1, 2b
+
+    addi sp, sp, 2*15*8
+
+    ret
+endfunc
+
+function warp_8x8t_8bpc_rvv, export=1, ext="v,zba"
+    csrw vxrm, zero
+
+    vsetivli zero, 8, e16, m1, ta, ma
+    addi sp, sp, -2*15*8
+    mv t5, sp
+    li t0, 3
+    mul t0, a3, t0
+    sub a2, a2, t0
+    addi a2, a2, -3
+
+    li t0, 64
+    addi a3, a3, -8
+    li t1, 15
+    la t2, dav1d_mc_warp_filter
+
+    lh t6, (a4)
+    lh t4, 2(a4)
+    vid.v v30
+    vwmul.vx v28, v30, t6
+1:
+    addi t1, t1, -1
+
+
+    vsetvli zero, zero, e32, m2, ta, ma
+    vadd.vx v4, v28, a5
+    add a5, a5, t4
+    vssra.vi v2, v4, 10
+    vadd.vx v2, v2, t0
+    vsll.vi v24, v2, 3
+    vsetvli zero, zero, e8, mf2, ta, ma
+
+    vluxseg8ei32.v v2, (t2), v24
+
+    vsetvli zero, zero, e16, m1, ta, ma
+.irp i, 2, 3, 4, 5, 6, 7, 8, 9
+    vle8.v v10, (a2)
+    addi a2, a2, 1
+
+    vsext.vf2 v14, v\i
+    vzext.vf2 v16, v10
+
+.if \i == 2
+    vwmulsu.vv v12, v14, v16
+.else
+    vwmaccsu.vv v12, v14, v16
+.endif
+.endr
+    vnclip.wi v10, v12, 3
+
+    add a2, a2, a3
+    vse16.v v10, (t5)
+    addi t5, t5, 16
+
+    bnez t1, 1b
+
+    mv t5, sp
+    li t1, 8
+
+    lh t6, 4(a4)
+    lh t4, 6(a4)
+    vwmul.vx v28, v30, t6
+2:
+    addi t1, t1, -1
+
+    vsetvli zero, zero, e32, m2, ta, ma
+    vadd.vx v4, v28, a6
+    add a6, a6, t4
+    vssra.vi v2, v4, 10
+    vadd.vx v2, v2, t0
+    vsll.vi v24, v2, 3
+    vsetvli zero, zero, e8, mf2, ta, ma
+
+    vluxseg8ei32.v v2, (t2), v24
+    vsetvli zero, zero, e16, m1, ta, ma
+
+.irp i, 2, 3, 4, 5, 6, 7, 8, 9
+    vle16.v v10, (t5)
+    addi t5, t5, 16
+
+    vsext.vf2 v14, v\i
+
+.if \i == 2
+    vwmul.vv v12, v14, v10
+.else
+    vwmacc.vv v12, v14, v10
+.endif
+
+.endr
+    addi t5, t5, -16*7
+    vnclip.wi v10, v12, 7
+
+    vse16.v v10, (a0)
+    sh1add a0, a1, a0
+
+    bnez t1, 2b
+
+    addi sp, sp, 2*15*8
+
+    ret
+endfunc
diff --git a/src/riscv/64/pal.S b/src/riscv/64/pal.S
new file mode 100644
index 0000000..b8fb6e3
--- /dev/null
+++ b/src/riscv/64/pal.S
@@ -0,0 +1,95 @@
+/******************************************************************************
+ * Copyright © 2018, VideoLAN and dav1d authors
+ * Copyright © 2024, Bogdan Gligorijevic
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *****************************************************************************/
+
+#include "src/riscv/asm.S"
+
+function pal_idx_finish_rvv, export=1, ext="v,zba,zbb"
+    csrw vxrm, zero
+    srl t0, a2, 1
+    sub a2, a2, a4
+    srl t1, a4, 1
+    mv t2, a5
+
+    csrr t6, vlenb
+    li t4, -3
+    ctz a6, t0
+    ctz t6, t6
+    li a7, 16
+    sub a6, a6, t6
+    li t6, 1<<4+1
+
+    // a6 is never > 3 for VLEN >=128
+    // that would've required stripmining with a6 set to 3
+    max a6, a6, t4
+    li t5, 2
+    andi a6, a6, 7
+    addi t4, a1, 1
+    ori a6, a6, 0xc0
+
+1:
+    sub t3, t0, t1
+    vsetvl zero, t1, a6
+    vlse8.v v0, (a1), t5
+    sh1add a1, t1, a1
+    vlse8.v v8, (t4), t5
+    sh1add t4, t1, t4
+    vmacc.vx v0, a7, v8
+    vse8.v v0, (a0)
+    add a0, a0, t1
+    ble t3, zero, 4f
+
+    lbu a4, -1(a1)
+    mul a4, a4, t6
+    vsetvl zero, t3, a6
+    vmv.v.x v0, a4
+    vse8.v v0, (a0)
+    add a0, a0, t3
+4:
+    addi t2, t2, -1
+    add a1, a1, a2
+    add t4, t4, a2
+    bnez t2, 1b
+
+    sub t1, a3, a5
+
+    sub t2, a0, t0
+    ble t1, zero, 7f
+
+    vsetvl zero, t0, a6
+    vle8.v v0, (t2)
+    add t2, a0, t0
+5:
+    addi t1, t1, -2
+    vse8.v v0, (a0)
+    vse8.v v0, (t2)
+    sh1add a0, t0, a0
+    sh1add t2, t0, t2
+
+    bnez t1, 5b
+7:
+    ret
+endfunc
diff --git a/src/riscv/asm.S b/src/riscv/asm.S
index eed4d67..c2e5a2f 100644
--- a/src/riscv/asm.S
+++ b/src/riscv/asm.S
@@ -51,6 +51,13 @@
 #define EXTERN PRIVATE_PREFIX
 #endif
 
+.macro arch ext:req, more:vararg
+    .option arch, +\ext
+    .ifnb \more
+        arch \more
+    .endif
+.endm
+
 .macro function name, export=0, ext=
     .macro endfunc
 #ifdef __ELF__
@@ -62,7 +69,7 @@
         .text
         .option push
     .ifnb \ext
-        .option arch, +\ext
+        arch \ext
     .endif
     .if \export
         .global EXTERN\name
@@ -125,4 +132,18 @@ EXTERN\name:
 
 #define L(x) .L ## x
 
+#define MA      (1 << 7)
+#define TA      (1 << 6)
+#define E8      (0 << 3)
+#define E16     (1 << 3)
+#define E32     (2 << 3)
+#define E64     (3 << 3)
+#define M1      0
+#define M2      1
+#define M4      2
+#define M8      3
+#define MF2     7
+#define MF4     6
+#define MF8     5
+
 #endif /* DAV1D_SRC_RISCV_ASM_S */
diff --git a/src/riscv/cdef.h b/src/riscv/cdef.h
new file mode 100644
index 0000000..2cf467e
--- /dev/null
+++ b/src/riscv/cdef.h
@@ -0,0 +1,31 @@
+#include "src/cpu.h"
+#include "src/cdef.h"
+
+extern void BF(dav1d_cdef_filter_block_4x4, rvv)(pixel *dst, const ptrdiff_t dst_stride,
+                                    const pixel (*left)[2],
+                                    const pixel *const top, const pixel *const bottom,
+                                    const int pri_strength, const int sec_strength, const int dir,
+                                    const int damping, const enum CdefEdgeFlags edges HIGHBD_DECL_SUFFIX);
+
+extern void BF(dav1d_cdef_filter_block_4x8, rvv)(pixel *dst, const ptrdiff_t dst_stride,
+                                    const pixel (*left)[2],
+                                    const pixel *const top, const pixel *const bottom,
+                                    const int pri_strength, const int sec_strength, const int dir,
+                                    const int damping, const enum CdefEdgeFlags edges HIGHBD_DECL_SUFFIX);
+
+extern void BF(dav1d_cdef_filter_block_8x8, rvv)(pixel *dst, const ptrdiff_t dst_stride,
+                                    const pixel (*left)[2],
+                                    const pixel *const top, const pixel *const bottom,
+                                    const int pri_strength, const int sec_strength, const int dir,
+                                    const int damping, const enum CdefEdgeFlags edges HIGHBD_DECL_SUFFIX);
+
+
+static ALWAYS_INLINE void cdef_dsp_init_riscv(Dav1dCdefDSPContext *const c) {
+    const unsigned flags = dav1d_get_cpu_flags();
+    if (!(flags & DAV1D_RISCV_CPU_FLAG_V)) return;
+
+    // c->dir = BF(dav1d_cdef_dir, rvv);
+    c->fb[0] = BF(dav1d_cdef_filter_block_8x8, rvv);
+    c->fb[1] = BF(dav1d_cdef_filter_block_4x8, rvv);
+    c->fb[2] = BF(dav1d_cdef_filter_block_4x4, rvv);
+}
diff --git a/src/riscv/cpu.c b/src/riscv/cpu.c
index 30e1354..345ff07 100644
--- a/src/riscv/cpu.c
+++ b/src/riscv/cpu.c
@@ -29,9 +29,10 @@
 
 #include "common/attributes.h"
 
+#include "src/cpu.h"
 #include "src/riscv/cpu.h"
 
-#if defined(HAVE_GETAUXVAL)
+#if HAVE_GETAUXVAL
 #include <sys/auxv.h>
 
 #define HWCAP_RVV (1 << ('v' - 'a'))
@@ -41,8 +42,8 @@
 int dav1d_has_compliant_rvv(void);
 
 COLD unsigned dav1d_get_cpu_flags_riscv(void) {
-    unsigned flags = 0;
-#if defined(HAVE_GETAUXVAL)
+    unsigned flags = dav1d_get_default_cpu_flags();
+#if HAVE_GETAUXVAL
     unsigned long hw_cap = getauxval(AT_HWCAP);
     flags |= (hw_cap & HWCAP_RVV) && dav1d_has_compliant_rvv() ? DAV1D_RISCV_CPU_FLAG_V : 0;
 #endif
diff --git a/src/riscv/cpu.h b/src/riscv/cpu.h
index 8ab7f53..f020f6a 100644
--- a/src/riscv/cpu.h
+++ b/src/riscv/cpu.h
@@ -34,4 +34,8 @@ enum CpuFlags {
 
 unsigned dav1d_get_cpu_flags_riscv(void);
 
+int dav1d_get_vlenb(void);
+
+#define dav1d_get_vlen() (dav1d_get_vlenb()*8)
+
 #endif /* DAV1D_SRC_RISCV_CPU_H */
diff --git a/src/riscv/ipred.h b/src/riscv/ipred.h
new file mode 100644
index 0000000..aac8dc9
--- /dev/null
+++ b/src/riscv/ipred.h
@@ -0,0 +1,74 @@
+/*
+ * Copyright © 2018, VideoLAN and dav1d authors
+ * Copyright © 2024, Bogdan Gligorijevic
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "src/cpu.h"
+#include "src/ipred.h"
+
+decl_cfl_pred_fn(BF(dav1d_ipred_cfl, rvv));
+decl_cfl_pred_fn(BF(dav1d_ipred_cfl_128, rvv));
+decl_cfl_pred_fn(BF(dav1d_ipred_cfl_top, rvv));
+decl_cfl_pred_fn(BF(dav1d_ipred_cfl_left, rvv));
+
+decl_angular_ipred_fn(BF(dav1d_ipred_paeth, rvv));
+decl_angular_ipred_fn(BF(dav1d_ipred_smooth, rvv));
+decl_angular_ipred_fn(BF(dav1d_ipred_smooth_v, rvv));
+decl_angular_ipred_fn(BF(dav1d_ipred_smooth_h, rvv));
+
+
+decl_pal_pred_fn(BF(dav1d_pal_pred, rvv));
+
+static ALWAYS_INLINE void intra_pred_dsp_init_riscv(Dav1dIntraPredDSPContext *const c) {
+  const unsigned flags = dav1d_get_cpu_flags();
+
+  if (!(flags & DAV1D_RISCV_CPU_FLAG_V)) return;
+
+#if BITDEPTH == 8
+    c->cfl_pred[DC_PRED     ] = dav1d_ipred_cfl_8bpc_rvv;
+    c->cfl_pred[DC_128_PRED ] = dav1d_ipred_cfl_128_8bpc_rvv;
+    c->cfl_pred[TOP_DC_PRED ] = dav1d_ipred_cfl_top_8bpc_rvv;
+    c->cfl_pred[LEFT_DC_PRED] = dav1d_ipred_cfl_left_8bpc_rvv;
+
+    c->intra_pred[PAETH_PRED   ] = dav1d_ipred_paeth_8bpc_rvv;
+    c->intra_pred[SMOOTH_PRED  ] = dav1d_ipred_smooth_8bpc_rvv;
+    c->intra_pred[SMOOTH_V_PRED] = dav1d_ipred_smooth_v_8bpc_rvv;
+    c->intra_pred[SMOOTH_H_PRED] = dav1d_ipred_smooth_h_8bpc_rvv;
+
+    c->pal_pred = dav1d_pal_pred_8bpc_rvv;
+#elif BITDEPTH == 16
+    c->cfl_pred[DC_PRED     ] = dav1d_ipred_cfl_16bpc_rvv;
+    c->cfl_pred[DC_128_PRED ] = dav1d_ipred_cfl_128_16bpc_rvv;
+    c->cfl_pred[TOP_DC_PRED ] = dav1d_ipred_cfl_top_16bpc_rvv;
+    c->cfl_pred[LEFT_DC_PRED] = dav1d_ipred_cfl_left_16bpc_rvv;
+
+    c->intra_pred[PAETH_PRED   ] = dav1d_ipred_paeth_16bpc_rvv;
+    c->intra_pred[SMOOTH_PRED  ] = dav1d_ipred_smooth_16bpc_rvv;
+    c->intra_pred[SMOOTH_V_PRED] = dav1d_ipred_smooth_v_16bpc_rvv;
+    c->intra_pred[SMOOTH_H_PRED] = dav1d_ipred_smooth_h_16bpc_rvv;
+
+    c->pal_pred = dav1d_pal_pred_16bpc_rvv;
+#endif
+}
diff --git a/src/riscv/mc.h b/src/riscv/mc.h
new file mode 100644
index 0000000..9784f49
--- /dev/null
+++ b/src/riscv/mc.h
@@ -0,0 +1,69 @@
+/*
+ * Copyright © 2024, VideoLAN and dav1d authors
+ * Copyright © 2024, Nathan Egge
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "src/cpu.h"
+#include "src/mc.h"
+
+decl_blend_fn(BF(dav1d_blend, rvv));
+decl_blend_dir_fn(BF(dav1d_blend_h, rvv));
+decl_blend_dir_fn(BF(dav1d_blend_v, rvv));
+
+decl_blend_fn(BF(dav1d_blend_vl256, rvv));
+decl_blend_dir_fn(BF(dav1d_blend_h_vl256, rvv));
+decl_blend_dir_fn(BF(dav1d_blend_v_vl256, rvv));
+
+decl_avg_fn(BF(dav1d_avg, rvv));
+decl_w_avg_fn(BF(dav1d_w_avg, rvv));
+decl_mask_fn(BF(dav1d_mask, rvv));
+
+decl_warp8x8_fn(BF(dav1d_warp_8x8, rvv));
+decl_warp8x8t_fn(BF(dav1d_warp_8x8t, rvv));
+
+static ALWAYS_INLINE void mc_dsp_init_riscv(Dav1dMCDSPContext *const c) {
+  const unsigned flags = dav1d_get_cpu_flags();
+
+  if (!(flags & DAV1D_RISCV_CPU_FLAG_V)) return;
+
+#if BITDEPTH == 8
+  c->blend = BF(dav1d_blend, rvv);
+  c->blend_h = BF(dav1d_blend_h, rvv);
+  c->blend_v = BF(dav1d_blend_v, rvv);
+
+  if (dav1d_get_vlen() >= 256) {
+    c->blend = BF(dav1d_blend_vl256, rvv);
+    c->blend_h = BF(dav1d_blend_h_vl256, rvv);
+    c->blend_v = BF(dav1d_blend_v_vl256, rvv);
+  }
+
+  c->avg     = BF(dav1d_avg, rvv);
+  c->w_avg   = BF(dav1d_w_avg, rvv);
+  c->mask    = BF(dav1d_mask, rvv);
+
+  c->warp8x8 = BF(dav1d_warp_8x8, rvv);
+  c->warp8x8t = BF(dav1d_warp_8x8t, rvv);
+#endif
+}
diff --git a/include/dav1d/version.h.in b/src/riscv/pal.h
similarity index 68%
rename from include/dav1d/version.h.in
rename to src/riscv/pal.h
index 4fa420d..f272d70 100644
--- a/include/dav1d/version.h.in
+++ b/src/riscv/pal.h
@@ -1,5 +1,6 @@
 /*
- * Copyright © 2019, VideoLAN and dav1d authors
+ * Copyright © 2023, VideoLAN and dav1d authors
+ * Copyright © 2024, Bogdan Gligorijevic
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -24,27 +25,15 @@
  * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  */
 
-#ifndef DAV1D_VERSION_H
-#define DAV1D_VERSION_H
+#include "src/cpu.h"
+#include "src/pal.h"
 
-#ifdef __cplusplus
-extern "C" {
-#endif
+decl_pal_idx_finish_fn(dav1d_pal_idx_finish_rvv);
 
-#define DAV1D_API_VERSION_MAJOR @DAV1D_API_VERSION_MAJOR@
-#define DAV1D_API_VERSION_MINOR @DAV1D_API_VERSION_MINOR@
-#define DAV1D_API_VERSION_PATCH @DAV1D_API_VERSION_PATCH@
+static ALWAYS_INLINE void pal_dsp_init_riscv(Dav1dPalDSPContext *const c) {
+    const unsigned flags = dav1d_get_cpu_flags();
 
-/**
- * Extract version components from the value returned by
- * dav1d_version_int()
- */
-#define DAV1D_API_MAJOR(v) (((v) >> 16) & 0xFF)
-#define DAV1D_API_MINOR(v) (((v) >>  8) & 0xFF)
-#define DAV1D_API_PATCH(v) (((v) >>  0) & 0xFF)
-
-#ifdef __cplusplus
-} /* extern "C" */
-#endif
+  if (!(flags & DAV1D_RISCV_CPU_FLAG_V)) return;
 
-#endif /* DAV1D_VERSION_H */
+    c->pal_idx_finish = dav1d_pal_idx_finish_rvv;
+}
diff --git a/src/scan.c b/src/scan.c
index 5261ccd..6f9dc03 100644
--- a/src/scan.c
+++ b/src/scan.c
@@ -28,7 +28,10 @@
 #include "config.h"
 
 #include "common/attributes.h"
+#include "common/intops.h"
+
 #include "src/scan.h"
+#include "src/thread.h"
 
 static const uint16_t ALIGN(scan_4x4[], 32) = {
      0,  4,  1,  2,
@@ -297,3 +300,76 @@ const uint16_t *const dav1d_scans[N_RECT_TX_SIZES] = {
     [RTX_16X64] = scan_16x32,
     [RTX_64X16] = scan_32x16,
 };
+
+static uint8_t last_nonzero_col_from_eob_4x4[16];
+static uint8_t last_nonzero_col_from_eob_8x8[64];
+static uint8_t last_nonzero_col_from_eob_16x16[256];
+static uint8_t last_nonzero_col_from_eob_32x32[1024];
+static uint8_t last_nonzero_col_from_eob_4x8[32];
+static uint8_t last_nonzero_col_from_eob_8x4[32];
+static uint8_t last_nonzero_col_from_eob_8x16[128];
+static uint8_t last_nonzero_col_from_eob_16x8[128];
+static uint8_t last_nonzero_col_from_eob_16x32[512];
+static uint8_t last_nonzero_col_from_eob_32x16[512];
+static uint8_t last_nonzero_col_from_eob_4x16[64];
+static uint8_t last_nonzero_col_from_eob_16x4[64];
+static uint8_t last_nonzero_col_from_eob_8x32[256];
+static uint8_t last_nonzero_col_from_eob_32x8[256];
+
+static COLD void init_tbl(uint8_t *const last_nonzero_col_from_eob,
+                          const uint16_t *const scan, const int w, const int h)
+{
+    int max_col = 0;
+    for (int y = 0, n = 0; y < h; y++) {
+        for (int x = 0; x < w; x++, n++) {
+            const int rc = scan[n];
+            const int rcx = rc & (h - 1);
+            max_col = imax(max_col, rcx);
+            last_nonzero_col_from_eob[n] = max_col;
+        }
+    }
+}
+
+static COLD void init_internal(void) {
+    init_tbl(last_nonzero_col_from_eob_4x4,   scan_4x4,    4,  4);
+    init_tbl(last_nonzero_col_from_eob_8x8,   scan_8x8,    8,  8);
+    init_tbl(last_nonzero_col_from_eob_16x16, scan_16x16, 16, 16);
+    init_tbl(last_nonzero_col_from_eob_32x32, scan_32x32, 32, 32);
+    init_tbl(last_nonzero_col_from_eob_4x8,   scan_4x8,    4,  8);
+    init_tbl(last_nonzero_col_from_eob_8x4,   scan_8x4,    8,  4);
+    init_tbl(last_nonzero_col_from_eob_8x16,  scan_8x16,   8, 16);
+    init_tbl(last_nonzero_col_from_eob_16x8,  scan_16x8,  16,  8);
+    init_tbl(last_nonzero_col_from_eob_16x32, scan_16x32, 16, 32);
+    init_tbl(last_nonzero_col_from_eob_32x16, scan_32x16, 32, 16);
+    init_tbl(last_nonzero_col_from_eob_4x16,  scan_4x16,   4, 16);
+    init_tbl(last_nonzero_col_from_eob_16x4,  scan_16x4,  16,  4);
+    init_tbl(last_nonzero_col_from_eob_8x32,  scan_8x32,   8, 32);
+    init_tbl(last_nonzero_col_from_eob_32x8,  scan_32x8,  32,  8);
+}
+
+COLD void dav1d_init_last_nonzero_col_from_eob_tables(void) {
+    static pthread_once_t initted = PTHREAD_ONCE_INIT;
+    pthread_once(&initted, init_internal);
+}
+
+const uint8_t *const dav1d_last_nonzero_col_from_eob[N_RECT_TX_SIZES] = {
+    [ TX_4X4  ] = last_nonzero_col_from_eob_4x4,
+    [ TX_8X8  ] = last_nonzero_col_from_eob_8x8,
+    [ TX_16X16] = last_nonzero_col_from_eob_16x16,
+    [ TX_32X32] = last_nonzero_col_from_eob_32x32,
+    [ TX_64X64] = last_nonzero_col_from_eob_32x32,
+    [RTX_4X8  ] = last_nonzero_col_from_eob_4x8,
+    [RTX_8X4  ] = last_nonzero_col_from_eob_8x4,
+    [RTX_8X16 ] = last_nonzero_col_from_eob_8x16,
+    [RTX_16X8 ] = last_nonzero_col_from_eob_16x8,
+    [RTX_16X32] = last_nonzero_col_from_eob_16x32,
+    [RTX_32X16] = last_nonzero_col_from_eob_32x16,
+    [RTX_32X64] = last_nonzero_col_from_eob_32x32,
+    [RTX_64X32] = last_nonzero_col_from_eob_32x32,
+    [RTX_4X16 ] = last_nonzero_col_from_eob_4x16,
+    [RTX_16X4 ] = last_nonzero_col_from_eob_16x4,
+    [RTX_8X32 ] = last_nonzero_col_from_eob_8x32,
+    [RTX_32X8 ] = last_nonzero_col_from_eob_32x8,
+    [RTX_16X64] = last_nonzero_col_from_eob_16x32,
+    [RTX_64X16] = last_nonzero_col_from_eob_32x16,
+};
diff --git a/src/scan.h b/src/scan.h
index 09df988..2bd0b5b 100644
--- a/src/scan.h
+++ b/src/scan.h
@@ -33,5 +33,8 @@
 #include "src/levels.h"
 
 EXTERN const uint16_t *const dav1d_scans[N_RECT_TX_SIZES];
+EXTERN const uint8_t *const dav1d_last_nonzero_col_from_eob[N_RECT_TX_SIZES];
+
+void dav1d_init_last_nonzero_col_from_eob_tables(void);
 
 #endif /* DAV1D_SRC_SCAN_H */
diff --git a/src/thread.h b/src/thread.h
index c44de73..459aace 100644
--- a/src/thread.h
+++ b/src/thread.h
@@ -132,6 +132,14 @@ static inline int pthread_cond_broadcast(pthread_cond_t *const cond) {
 #else
 
 #include <pthread.h>
+#if defined(__FreeBSD__)
+ /* ALIGN from <sys/param.h> conflicts with ALIGN from "common/attributes.h" */
+#define _SYS_PARAM_H_
+#include <sys/types.h>
+#endif
+#if HAVE_PTHREAD_NP_H
+#include <pthread_np.h>
+#endif
 
 #define dav1d_init_thread() do {} while (0)
 
@@ -145,29 +153,28 @@ static inline void dav1d_set_thread_name(const char *const name) {
     prctl(PR_SET_NAME, name);
 }
 
-#elif defined(__APPLE__)
+#elif HAVE_PTHREAD_SETNAME_NP && defined(__APPLE__)
 
 static inline void dav1d_set_thread_name(const char *const name) {
     pthread_setname_np(name);
 }
 
-#elif defined(__DragonFly__) || defined(__FreeBSD__) || defined(__OpenBSD__)
+#elif HAVE_PTHREAD_SETNAME_NP && defined(__NetBSD__)
 
-#if defined(__FreeBSD__)
- /* ALIGN from <sys/param.h> conflicts with ALIGN from "common/attributes.h" */
-#define _SYS_PARAM_H_
-#include <sys/types.h>
-#endif
-#include <pthread_np.h>
+static inline void dav1d_set_thread_name(const char *const name) {
+    pthread_setname_np(pthread_self(), "%s", (void*)name);
+}
+
+#elif HAVE_PTHREAD_SETNAME_NP
 
 static inline void dav1d_set_thread_name(const char *const name) {
-    pthread_set_name_np(pthread_self(), name);
+    pthread_setname_np(pthread_self(), name);
 }
 
-#elif defined(__NetBSD__)
+#elif HAVE_PTHREAD_SET_NAME_NP
 
 static inline void dav1d_set_thread_name(const char *const name) {
-    pthread_setname_np(pthread_self(), "%s", (void*)name);
+    pthread_set_name_np(pthread_self(), name);
 }
 
 #elif defined(__HAIKU__)
diff --git a/src/x86/cpu.c b/src/x86/cpu.c
index f570fd7..80f91e1 100644
--- a/src/x86/cpu.c
+++ b/src/x86/cpu.c
@@ -32,6 +32,7 @@
 
 #include "common/attributes.h"
 
+#include "src/cpu.h"
 #include "src/x86/cpu.h"
 
 typedef struct {
@@ -52,7 +53,7 @@ COLD unsigned dav1d_get_cpu_flags_x86(void) {
         };
     } cpu;
     dav1d_cpu_cpuid(&cpu.r, 0, 0);
-    unsigned flags = 0;
+    unsigned flags = dav1d_get_default_cpu_flags();
 
     if (cpu.max_leaf >= 1) {
         CpuidRegisters r;
diff --git a/src/x86/itx.h b/src/x86/itx.h
index 23d7a73..a8a490f 100644
--- a/src/x86/itx.h
+++ b/src/x86/itx.h
@@ -107,7 +107,9 @@ decl_itx_fns(ssse3);
 decl_itx_fn(dav1d_inv_txfm_add_wht_wht_4x4_16bpc_avx2);
 decl_itx_fn(BF(dav1d_inv_txfm_add_wht_wht_4x4, sse2));
 
-static ALWAYS_INLINE void itx_dsp_init_x86(Dav1dInvTxfmDSPContext *const c, const int bpc) {
+static ALWAYS_INLINE void itx_dsp_init_x86(Dav1dInvTxfmDSPContext *const c,
+                                           const int bpc, int *const all_simd)
+{
 #define assign_itx_bpc_fn(pfx, w, h, type, type_enum, bpc, ext) \
     c->itxfm_add[pfx##TX_##w##X##h][type_enum] = \
         BF_BPC(dav1d_inv_txfm_add_##type##_##w##x##h, bpc, ext)
@@ -167,6 +169,7 @@ static ALWAYS_INLINE void itx_dsp_init_x86(Dav1dInvTxfmDSPContext *const c, cons
     assign_itx1_fn (R, 64, 16, ssse3);
     assign_itx1_fn (R, 64, 32, ssse3);
     assign_itx1_fn ( , 64, 64, ssse3);
+    *all_simd = 1;
 #endif
 
     if (!(flags & DAV1D_X86_CPU_FLAG_SSE41)) return;
@@ -192,6 +195,7 @@ static ALWAYS_INLINE void itx_dsp_init_x86(Dav1dInvTxfmDSPContext *const c, cons
         assign_itx1_fn (R, 64, 16, sse4);
         assign_itx1_fn (R, 64, 32, sse4);
         assign_itx1_fn (,  64, 64, sse4);
+        *all_simd = 1;
     }
 #endif
 
diff --git a/src/x86/looprestoration16_avx2.asm b/src/x86/looprestoration16_avx2.asm
index 4cf8b90..4b18f77 100644
--- a/src/x86/looprestoration16_avx2.asm
+++ b/src/x86/looprestoration16_avx2.asm
@@ -52,18 +52,18 @@ pb_m2_m1:      times 2 db  -2, -1
 pb_2_3:        times 2 db   2,  3
 pb_6_7:        times 2 db   6,  7
 pw_1023:       times 2 dw 1023
+pw_164_24:     dw 164, 24
+pw_455_24:     dw 455, 24
 pd_8:          dd 8
 pd_25:         dd 25
 pd_4096:       dd 4096
 pd_34816:      dd 34816
 pd_m262128:    dd -262128
-pd_0xf00800a4: dd 0xf00800a4
-pd_0xf00801c7: dd 0xf00801c7
+pf_256:        dd 256.0
 
 %define pw_256 sgr_lshuf5
 
 cextern pb_0to63
-cextern sgr_x_by_x_avx2
 
 SECTION .text
 
@@ -651,29 +651,31 @@ ALIGN function_align
     jl .v_loop
     ret
 
-cglobal sgr_filter_5x5_16bpc, 4, 14, 15, 400*24+16, dst, stride, left, lpf, \
+cglobal sgr_filter_5x5_16bpc, 4, 14, 16, 400*24+16, dst, stride, left, lpf, \
                                                     w, h, edge, params
+%define base r13-pb_m10_m9
     movifnidn       wd, wm
     mov        paramsq, r6mp
-    lea            r13, [sgr_x_by_x_avx2+256*4]
+    lea            r13, [pb_m10_m9]
     movifnidn       hd, hm
     mov          edged, r7m
-    add             wd, wd
     vpbroadcastw    m7, [paramsq+8] ; w0
+    add             wd, wd
+    vpbroadcastd    m8, [base+pd_8]
     add           lpfq, wq
-    vpbroadcastd    m8, [pd_8]
+    vpbroadcastd    m9, [base+pd_25]
     add           dstq, wq
-    vpbroadcastd    m9, [pd_25]
+    mova          xm10, [base+sgr_lshuf5]
     lea             t3, [rsp+wq*2+400*12+16]
-    vpbroadcastd   m10, [paramsq+0] ; s0
+    vpbroadcastd   m11, [paramsq+0] ; s0
     lea             t4, [rsp+wq+400*20+16]
-    vpbroadcastd   m11, [pd_0xf00800a4]
+    vpbroadcastd   m12, [base+pw_164_24]
     lea             t1, [rsp+wq+20]
-    mova          xm12, [sgr_lshuf5]
+    vbroadcastss   m13, [base+pf_256]
     neg             wq
-    vpbroadcastd   m13, [pd_34816]  ; (1 << 11) + (1 << 15)
+    vpbroadcastd   m14, [base+pd_34816] ; (1 << 11) + (1 << 15)
     pxor            m6, m6
-    vpbroadcastd   m14, [pw_1023]
+    vpbroadcastd   m15, [base+pw_1023]
     psllw           m7, 4
     test         edgeb, 4 ; LR_HAVE_TOP
     jz .no_top
@@ -772,7 +774,7 @@ cglobal sgr_filter_5x5_16bpc, 4, 14, 15, 400*24+16, dst, stride, left, lpf, \
     jmp .h_main
 .h_extend_left:
     mova           xm4, [lpfq+wq]
-    pshufb         xm4, xm12
+    pshufb         xm4, xm10
     vinserti128     m4, [lpfq+wq+10], 1
     jmp .h_main
 .h_top:
@@ -853,7 +855,7 @@ ALIGN function_align
     jmp .hv_main
 .hv_extend_left:
     mova           xm4, [lpfq+wq]
-    pshufb         xm4, xm12
+    pshufb         xm4, xm10
     vinserti128     m4, [lpfq+wq+10], 1
     jmp .hv_main
 .hv_bottom:
@@ -924,21 +926,33 @@ ALIGN function_align
     pmaxud          m5, m3
     psubd           m4, m2             ; p
     psubd           m5, m3
-    pmulld          m4, m10            ; p * s
-    pmulld          m5, m10
-    pmaddwd         m0, m11            ; b * 164
-    pmaddwd         m1, m11
-    paddusw         m4, m11
-    paddusw         m5, m11
-    psrad           m3, m4, 20         ; min(z, 255) - 256
-    vpgatherdd      m2, [r13+m3*4], m4 ; x
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r13+m4*4], m5
+    pmulld          m4, m11            ; p * s
+    pmulld          m5, m11
+    pmaddwd         m0, m12            ; b * 164
+    pmaddwd         m1, m12
+    paddw           m4, m12
+    paddw           m5, m12
+    psrld           m4, 20             ; z + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4             ; 1 / (z + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m13, m4
+    pcmpgtd         m5, m13, m5
+    mulps           m2, m13            ; 256 / (z + 1)
+    mulps           m3, m13
+    psrld           m4, 24             ; z < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4             ; x
+    pminsw          m3, m5
     pmulld          m0, m2
     pmulld          m1, m3
     packssdw        m2, m3
-    paddd           m0, m13            ; x * b * 164 + (1 << 11) + (1 << 15)
-    paddd           m1, m13
+    paddd           m0, m14            ; x * b * 164 + (1 << 11) + (1 << 15)
+    paddd           m1, m14
     mova    [t4+r10+4], m2
     psrld           m0, 12             ; b
     psrld           m1, 12
@@ -993,21 +1007,33 @@ ALIGN function_align
     pmaxud          m5, m3
     psubd           m4, m2             ; p
     psubd           m5, m3
-    pmulld          m4, m10            ; p * s
-    pmulld          m5, m10
-    pmaddwd         m0, m11            ; b * 164
-    pmaddwd         m1, m11
-    paddusw         m4, m11
-    paddusw         m5, m11
-    psrad           m3, m4, 20         ; min(z, 255) - 256
-    vpgatherdd      m2, [r13+m3*4], m4 ; x
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r13+m4*4], m5
+    pmulld          m4, m11            ; p * s
+    pmulld          m5, m11
+    pmaddwd         m0, m12            ; b * 164
+    pmaddwd         m1, m12
+    paddw           m4, m12
+    paddw           m5, m12
+    psrld           m4, 20             ; z + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4             ; 1 / (z + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m13, m4
+    pcmpgtd         m5, m13, m5
+    mulps           m2, m13            ; 256 / (z + 1)
+    mulps           m3, m13
+    psrld           m4, 24             ; z < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4             ; x
+    pminsw          m3, m5
     pmulld          m0, m2
     pmulld          m1, m3
     packssdw        m2, m3
-    paddd           m0, m13            ; x * b * 164 + (1 << 11) + (1 << 15)
-    paddd           m1, m13
+    paddd           m0, m14            ; x * b * 164 + (1 << 11) + (1 << 15)
+    paddd           m1, m14
     mova    [t4+r10+4], m2
     psrld           m0, 12             ; b
     psrld           m1, 12
@@ -1090,7 +1116,7 @@ ALIGN function_align
     pmulhrsw        m1, m7
     paddw           m0, m1
     pmaxsw          m0, m6
-    pminsw          m0, m14
+    pminsw          m0, m15
     mova    [dstq+r10], m0
     add            r10, 32
     jl .n0_loop
@@ -1120,35 +1146,36 @@ ALIGN function_align
     pmulhrsw        m1, m7
     paddw           m0, m1
     pmaxsw          m0, m6
-    pminsw          m0, m14
+    pminsw          m0, m15
     mova    [dstq+r10], m0
     add            r10, 32
     jl .n1_loop
     add           dstq, strideq
     ret
 
-cglobal sgr_filter_3x3_16bpc, 4, 14, 14, 400*42+8, dst, stride, left, lpf, \
+cglobal sgr_filter_3x3_16bpc, 4, 14, 15, 400*42+8, dst, stride, left, lpf, \
                                                    w, h, edge, params
     movifnidn       wd, wm
     mov        paramsq, r6mp
-    lea            r13, [sgr_x_by_x_avx2+256*4]
+    lea            r13, [pb_m10_m9]
     add             wd, wd
     movifnidn       hd, hm
     mov          edged, r7m
-    add           lpfq, wq
     vpbroadcastw    m7, [paramsq+10] ; w1
+    add           lpfq, wq
+    vpbroadcastd    m8, [base+pd_8]
     add           dstq, wq
     vpbroadcastd    m9, [paramsq+ 4] ; s1
     lea             t3, [rsp+wq*2+400*12+8]
-    vpbroadcastd    m8, [pd_8]
+    mova          xm10, [base+sgr_lshuf3]
     lea             t4, [rsp+wq+400*32+8]
-    vpbroadcastd   m10, [pd_0xf00801c7]
+    vpbroadcastd   m11, [base+pw_455_24]
     lea             t1, [rsp+wq+12]
-    vpbroadcastd   m11, [pd_34816]
+    vbroadcastss   m12, [base+pf_256]
     neg             wq
-    mova          xm12, [sgr_lshuf3]
+    vpbroadcastd   m13, [base+pd_34816]
     pxor            m6, m6
-    vpbroadcastd   m13, [pw_1023]
+    vpbroadcastd   m14, [base+pw_1023]
     psllw           m7, 4
     test         edgeb, 4 ; LR_HAVE_TOP
     jz .no_top
@@ -1247,7 +1274,7 @@ cglobal sgr_filter_3x3_16bpc, 4, 14, 14, 400*42+8, dst, stride, left, lpf, \
     jmp .h_main
 .h_extend_left:
     mova           xm4, [lpfq+wq]
-    pshufb         xm4, xm12
+    pshufb         xm4, xm10
     vinserti128     m4, [lpfq+wq+12], 1
     jmp .h_main
 .h_top:
@@ -1297,7 +1324,7 @@ ALIGN function_align
     jmp .hv0_main
 .hv0_extend_left:
     mova           xm4, [lpfq+wq]
-    pshufb         xm4, xm12
+    pshufb         xm4, xm10
     vinserti128     m4, [lpfq+wq+12], 1
     jmp .hv0_main
 .hv0_bottom:
@@ -1362,19 +1389,31 @@ ALIGN function_align
     psubd           m5, m3
     pmulld          m4, m9             ; p * s
     pmulld          m5, m9
-    pmaddwd         m0, m10            ; b * 455
-    pmaddwd         m1, m10
-    paddusw         m4, m10
-    paddusw         m5, m10
-    psrad           m3, m4, 20         ; min(z, 255) - 256
-    vpgatherdd      m2, [r13+m3*4], m4 ; x
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r13+m4*4], m5
+    pmaddwd         m0, m11            ; b * 455
+    pmaddwd         m1, m11
+    paddw           m4, m11
+    paddw           m5, m11
+    psrld           m4, 20             ; z + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4             ; 1 / (z + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m12, m4
+    pcmpgtd         m5, m12, m5
+    mulps           m2, m12            ; 256 / (z + 1)
+    mulps           m3, m12
+    psrld           m4, 24             ; z < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4             ; x
+    pminsw          m3, m5
     pmulld          m0, m2
     pmulld          m1, m3
     packssdw        m2, m3
-    paddd           m0, m11            ; x * b * 455 + (1 << 11) + (1 << 15)
-    paddd           m1, m11
+    paddd           m0, m13            ; x * b * 455 + (1 << 11) + (1 << 15)
+    paddd           m1, m13
     psrld           m0, 12
     psrld           m1, 12
     mova         [t4+r10*1+400*0+ 4], m2
@@ -1398,7 +1437,7 @@ ALIGN function_align
     jmp .hv1_main
 .hv1_extend_left:
     mova           xm4, [lpfq+wq]
-    pshufb         xm4, xm12
+    pshufb         xm4, xm10
     vinserti128     m4, [lpfq+wq+12], 1
     jmp .hv1_main
 .hv1_bottom:
@@ -1457,19 +1496,31 @@ ALIGN function_align
     psubd           m5, m3
     pmulld          m4, m9             ; p * s
     pmulld          m5, m9
-    pmaddwd         m0, m10            ; b * 455
-    pmaddwd         m1, m10
-    paddusw         m4, m10
-    paddusw         m5, m10
-    psrad           m3, m4, 20         ; min(z, 255) - 256
-    vpgatherdd      m2, [r13+m3*4], m4 ; x
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r13+m4*4], m5
+    pmaddwd         m0, m11            ; b * 455
+    pmaddwd         m1, m11
+    paddw           m4, m11
+    paddw           m5, m11
+    psrld           m4, 20             ; z + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4             ; 1 / (z + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m12, m4
+    pcmpgtd         m5, m12, m5
+    mulps           m2, m12            ; 256 / (z + 1)
+    mulps           m3, m12
+    psrld           m4, 24             ; z < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4             ; x
+    pminsw          m3, m5
     pmulld          m0, m2
     pmulld          m1, m3
     packssdw        m2, m3
-    paddd           m0, m11            ; x * b * 455 + (1 << 11) + (1 << 15)
-    paddd           m1, m11
+    paddd           m0, m13            ; x * b * 455 + (1 << 11) + (1 << 15)
+    paddd           m1, m13
     psrld           m0, 12
     psrld           m1, 12
     mova         [t4+r10*1+400*2 +4], m2
@@ -1520,19 +1571,31 @@ ALIGN function_align
     psubd           m5, m3
     pmulld          m4, m9             ; p * s
     pmulld          m5, m9
-    pmaddwd         m0, m10            ; b * 455
-    pmaddwd         m1, m10
-    paddusw         m4, m10
-    paddusw         m5, m10
-    psrad           m3, m4, 20         ; min(z, 255) - 256
-    vpgatherdd      m2, [r13+m3*4], m4 ; x
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r13+m4*4], m5
+    pmaddwd         m0, m11            ; b * 455
+    pmaddwd         m1, m11
+    paddw           m4, m11
+    paddw           m5, m11
+    psrld           m4, 20             ; z + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4             ; 1 / (z + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m12, m4
+    pcmpgtd         m5, m12, m5
+    mulps           m2, m12            ; 256 / (z + 1)
+    mulps           m3, m12
+    psrld           m4, 24             ; z < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4             ; x
+    pminsw          m3, m5
     pmulld          m0, m2
     pmulld          m1, m3
     packssdw        m2, m3
-    paddd           m0, m11            ; x * b * 455 + (1 << 11) + (1 << 15)
-    paddd           m1, m11
+    paddd           m0, m13            ; x * b * 455 + (1 << 11) + (1 << 15)
+    paddd           m1, m13
     psrld           m0, 12
     psrld           m1, 12
     mova         [t4+r10*1+400*0+ 4], m2
@@ -1577,19 +1640,31 @@ ALIGN function_align
     psubd           m5, m3
     pmulld          m4, m9             ; p * s
     pmulld          m5, m9
-    pmaddwd         m0, m10            ; b * 455
-    pmaddwd         m1, m10
-    paddusw         m4, m10
-    paddusw         m5, m10
-    psrad           m3, m4, 20         ; min(z, 255) - 256
-    vpgatherdd      m2, [r13+m3*4], m4 ; x
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r13+m4*4], m5
+    pmaddwd         m0, m11            ; b * 455
+    pmaddwd         m1, m11
+    paddw           m4, m11
+    paddw           m5, m11
+    psrld           m4, 20             ; z + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4             ; 1 / (z + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m12, m4
+    pcmpgtd         m5, m12, m5
+    mulps           m2, m12            ; 256 / (z + 1)
+    mulps           m3, m12
+    psrld           m4, 24             ; z < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4             ; x
+    pminsw          m3, m5
     pmulld          m0, m2
     pmulld          m1, m3
     packssdw        m2, m3
-    paddd           m0, m11            ; x * b * 455 + (1 << 11) + (1 << 15)
-    paddd           m1, m11
+    paddd           m0, m13            ; x * b * 455 + (1 << 11) + (1 << 15)
+    paddd           m1, m13
     psrld           m0, 12
     psrld           m1, 12
     mova         [t4+r10*1+400*2+ 4], m2
@@ -1683,7 +1758,7 @@ ALIGN function_align
     pmulhrsw        m1, m7
     paddw           m0, m1
     pmaxsw          m0, m6
-    pminsw          m0, m13
+    pminsw          m0, m14
     mova    [dstq+r10], m0
     add            r10, 32
     jl .n0_loop
@@ -1737,7 +1812,7 @@ ALIGN function_align
     pmulhrsw        m1, m7
     paddw           m0, m1
     pmaxsw          m0, m6
-    pminsw          m0, m13
+    pminsw          m0, m14
     mova    [dstq+r10], m0
     add            r10, 32
     jl .n1_loop
@@ -1748,7 +1823,7 @@ cglobal sgr_filter_mix_16bpc, 4, 14, 16, 400*66+8, dst, stride, left, lpf, \
                                                    w, h, edge, params
     movifnidn       wd, wm
     mov        paramsq, r6mp
-    lea            r13, [sgr_x_by_x_avx2+256*4]
+    lea            r13, [pb_m10_m9]
     add             wd, wd
     movifnidn       hd, hm
     mov          edged, r7m
@@ -1759,13 +1834,13 @@ cglobal sgr_filter_mix_16bpc, 4, 14, 16, 400*66+8, dst, stride, left, lpf, \
     lea             t3, [rsp+wq*2+400*24+8]
     vpbroadcastd   m14, [paramsq+4] ; s1
     lea             t4, [rsp+wq+400*52+8]
-    vpbroadcastd    m9, [pd_8]
+    vpbroadcastd    m9, [base+pd_8]
     lea             t1, [rsp+wq+12]
-    vpbroadcastd   m10, [pd_34816]
+    vpbroadcastd   m10, [base+pd_34816]
     neg             wq
-    vpbroadcastd   m11, [pd_4096]
+    vbroadcastss   m11, [base+pf_256]
     pxor            m7, m7
-    vpbroadcastd   m12, [pd_0xf00801c7]
+    vpbroadcastd   m12, [base+pw_455_24]
     psllw          m15, 2
     test         edgeb, 4 ; LR_HAVE_TOP
     jz .no_top
@@ -1867,7 +1942,7 @@ cglobal sgr_filter_mix_16bpc, 4, 14, 16, 400*66+8, dst, stride, left, lpf, \
     jmp .h_main
 .h_extend_left:
     mova           xm4, [lpfq+wq]
-    pshufb         xm4, [sgr_lshuf5]
+    pshufb         xm4, [base+sgr_lshuf5]
     vinserti128     m4, [lpfq+wq+10], 1
     jmp .h_main
 .h_top:
@@ -1930,7 +2005,7 @@ ALIGN function_align
     jmp .hv0_main
 .hv0_extend_left:
     mova           xm4, [lpfq+wq]
-    pshufb         xm4, [sgr_lshuf5]
+    pshufb         xm4, [base+sgr_lshuf5]
     vinserti128     m4, [lpfq+wq+10], 1
     jmp .hv0_main
 .hv0_bottom:
@@ -2016,12 +2091,24 @@ ALIGN function_align
     pmulld          m5, m14
     pmaddwd         m0, m12            ; b3 * 455
     pmaddwd         m1, m12
-    paddusw         m4, m12
-    paddusw         m5, m12
-    psrad           m3, m4, 20         ; min(z3, 255) - 256
-    vpgatherdd      m2, [r13+m3*4], m4 ; x3
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r13+m4*4], m5
+    paddw           m4, m12
+    paddw           m5, m12
+    psrld           m4, 20             ; z3 + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4             ; 1 / (z3 + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m11, m4
+    pcmpgtd         m5, m11, m5
+    mulps           m2, m11            ; 256 / (z3 + 1)
+    mulps           m3, m11
+    psrld           m4, 24             ; z3 < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4             ; x3
+    pminsw          m3, m5
     pmulld          m0, m2
     pmulld          m1, m3
     packssdw        m2, m3
@@ -2050,7 +2137,7 @@ ALIGN function_align
     jmp .hv1_main
 .hv1_extend_left:
     mova           xm4, [lpfq+wq]
-    pshufb         xm4, [sgr_lshuf5]
+    pshufb         xm4, [base+sgr_lshuf5]
     vinserti128     m4, [lpfq+wq+10], 1
     jmp .hv1_main
 .hv1_bottom:
@@ -2121,12 +2208,24 @@ ALIGN function_align
     pmulld          m3, m14
     pmaddwd         m0, m12            ; b3 * 455
     pmaddwd         m1, m12
-    paddusw         m2, m12
-    paddusw         m3, m12
-    psrad           m7, m2, 20         ; min(z3, 255) - 256
-    vpgatherdd      m6, [r13+m7*4], m2 ; x3
-    psrad           m2, m3, 20
-    vpgatherdd      m7, [r13+m2*4], m3
+    paddw           m2, m12
+    paddw           m3, m12
+    psrld           m2, 20             ; z + 1
+    psrld           m3, 20
+    cvtdq2ps        m2, m2
+    cvtdq2ps        m3, m3
+    rcpps           m6, m2             ; 1 / (z + 1)
+    rcpps           m7, m3
+    pcmpgtd         m2, m11, m2
+    pcmpgtd         m3, m11, m3
+    mulps           m6, m11            ; 256 / (z + 1)
+    mulps           m7, m11
+    psrld           m2, 24             ; z < 255 ? 255 : 0
+    psrld           m3, 24
+    cvtps2dq        m6, m6
+    cvtps2dq        m7, m7
+    pminsw          m6, m2             ; x
+    pminsw          m7, m3
     pmulld          m0, m6
     packssdw        m6, m7
     pmulld          m7, m1
@@ -2148,7 +2247,8 @@ ALIGN function_align
     vextracti128 [t3+r10*2+400*8+40], m0, 1
     mova         [t3+r10*2+400*8+24], xm7
     vextracti128 [t3+r10*2+400*8+56], m7, 1
-    vpbroadcastd    m4, [pd_25]
+    vpbroadcastd    m4, [base+pd_25]
+    vpbroadcastd    m6, [base+pw_164_24]
     pxor            m7, m7
     paddd           m2, m9
     paddd           m3, m9
@@ -2166,19 +2266,30 @@ ALIGN function_align
     punpckhwd       m1, m7
     pmaxud          m2, m4
     psubd           m2, m4             ; p5
-    vpbroadcastd    m4, [pd_0xf00800a4]
     pmaxud          m3, m5
     psubd           m3, m5
     pmulld          m2, m13            ; p5 * s0
     pmulld          m3, m13
-    pmaddwd         m0, m4             ; b5 * 164
-    pmaddwd         m1, m4
-    paddusw         m2, m4
-    paddusw         m3, m4
-    psrad           m5, m2, 20         ; min(z5, 255) - 256
-    vpgatherdd      m4, [r13+m5*4], m2 ; x5
-    psrad           m2, m3, 20
-    vpgatherdd      m5, [r13+m2*4], m3
+    pmaddwd         m0, m6             ; b5 * 164
+    pmaddwd         m1, m6
+    paddw           m2, m6
+    paddw           m3, m6
+    psrld           m2, 20             ; z5 + 1
+    psrld           m3, 20
+    cvtdq2ps        m2, m2
+    cvtdq2ps        m3, m3
+    rcpps           m4, m2             ; 1 / (z5 + 1)
+    rcpps           m5, m3
+    pcmpgtd         m2, m11, m2
+    pcmpgtd         m3, m11, m3
+    mulps           m4, m11            ; 256 / (z5 + 1)
+    mulps           m5, m11
+    psrld           m2, 24             ; z5 < 255 ? 255 : 0
+    psrld           m3, 24
+    cvtps2dq        m4, m4
+    cvtps2dq        m5, m5
+    pminsw          m4, m2             ; x5
+    pminsw          m5, m3
     pmulld          m0, m4
     pmulld          m1, m5
     packssdw        m4, m5
@@ -2236,12 +2347,24 @@ ALIGN function_align
     pmulld          m5, m14
     pmaddwd         m0, m12            ; b3 * 455
     pmaddwd         m1, m12
-    paddusw         m4, m12
-    paddusw         m5, m12
-    psrad           m3, m4, 20         ; min(z3, 255) - 256
-    vpgatherdd      m2, [r13+m3*4], m4 ; x3
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r13+m4*4], m5
+    paddw           m4, m12
+    paddw           m5, m12
+    psrld           m4, 20             ; z + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4             ; 1 / (z + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m11, m4
+    pcmpgtd         m5, m11, m5
+    mulps           m2, m11            ; 256 / (z + 1)
+    mulps           m3, m11
+    psrld           m4, 24             ; z < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4             ; x
+    pminsw          m3, m5
     pmulld          m0, m2
     pmulld          m1, m3
     packssdw        m2, m3
@@ -2305,12 +2428,24 @@ ALIGN function_align
     pmulld          m5, m14
     pmaddwd         m0, m12            ; b3 * 455
     pmaddwd         m1, m12
-    paddusw         m4, m12
-    paddusw         m5, m12
-    psrad           m3, m4, 20         ; min(z3, 255) - 256
-    vpgatherdd      m2, [r13+m3*4], m4 ; x3
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r13+m4*4], m5
+    paddw           m4, m12
+    paddw           m5, m12
+    psrld           m4, 20             ; z + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4             ; 1 / (z + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m11, m4
+    pcmpgtd         m5, m11, m5
+    mulps           m2, m11            ; 256 / (z + 1)
+    mulps           m3, m11
+    psrld           m4, 24             ; z < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4             ; x
+    pminsw          m3, m5
     pmulld          m0, m2
     pmulld          m1, m3
     packssdw        m2, m3
@@ -2331,11 +2466,12 @@ ALIGN function_align
     mova [t2+r10+400*0], m4
     mova [t2+r10+400*2], m5
     mova [t2+r10+400*4], m6
-    vpbroadcastd    m4, [pd_25]
     mova         [t3+r10*2+400*8+ 8], xm0
     vextracti128 [t3+r10*2+400*8+40], m0, 1
     mova         [t3+r10*2+400*8+24], xm8
     vextracti128 [t3+r10*2+400*8+56], m8, 1
+    vpbroadcastd    m4, [base+pd_25]
+    vpbroadcastd    m6, [base+pw_164_24]
     paddd           m2, m9
     paddd           m3, m9
     psrld           m2, 4              ; (a5 + 8) >> 4
@@ -2352,19 +2488,30 @@ ALIGN function_align
     punpckhwd       m1, m7
     pmaxud          m2, m4
     psubd           m2, m4             ; p5
-    vpbroadcastd    m4, [pd_0xf00800a4]
     pmaxud          m3, m5
     psubd           m3, m5
     pmulld          m2, m13            ; p5 * s0
     pmulld          m3, m13
-    pmaddwd         m0, m4             ; b5 * 164
-    pmaddwd         m1, m4
-    paddusw         m2, m4
-    paddusw         m3, m4
-    psrad           m5, m2, 20         ; min(z5, 255) - 256
-    vpgatherdd      m4, [r13+m5*4], m2 ; x5
-    psrad           m2, m3, 20
-    vpgatherdd      m5, [r13+m2*4], m3
+    pmaddwd         m0, m6             ; b5 * 164
+    pmaddwd         m1, m6
+    paddw           m2, m6
+    paddw           m3, m6
+    psrld           m2, 20             ; z5 + 1
+    psrld           m3, 20
+    cvtdq2ps        m2, m2
+    cvtdq2ps        m3, m3
+    rcpps           m4, m2             ; 1 / (z5 + 1)
+    rcpps           m5, m3
+    pcmpgtd         m2, m11, m2
+    pcmpgtd         m3, m11, m3
+    mulps           m4, m11            ; 256 / (z5 + 1)
+    mulps           m5, m11
+    psrld           m2, 24             ; z5 < 255 ? 255 : 0
+    psrld           m3, 24
+    cvtps2dq        m4, m4
+    cvtps2dq        m5, m5
+    pminsw          m4, m2             ; x5
+    pminsw          m5, m3
     pmulld          m0, m4
     pmulld          m1, m5
     packssdw        m4, m5
@@ -2432,6 +2579,7 @@ ALIGN function_align
 ALIGN function_align
 .n0: ; neighbor + output (even rows)
     mov            r10, wq
+    vpbroadcastd    m6, [base+pd_4096]
 .n0_loop:
     movu           xm2, [t4+r10*1+2]
     paddw          xm0, xm2, [t4+r10*1+0]
@@ -2479,7 +2627,7 @@ ALIGN function_align
     pslld           m1, 7
     pblendw         m0, m1, 0xaa
     pmaddwd         m0, m15
-    paddd           m4, m11
+    paddd           m4, m6
     paddd           m0, m4
     psrad           m0, 7
     vextracti128   xm1, m0, 1
@@ -2493,6 +2641,7 @@ ALIGN function_align
 ALIGN function_align
 .n1: ; neighbor + output (odd rows)
     mov            r10, wq
+    vpbroadcastd    m6, [base+pd_4096]
 .n1_loop:
     mova           xm3, [t4+r10*1+400*4+0]
     paddw          xm3, [t4+r10*1+400*4+4]
@@ -2525,7 +2674,7 @@ ALIGN function_align
     pslld           m1, 7
     pblendw         m0, m1, 0xaa
     pmaddwd         m0, m15
-    paddd           m4, m11
+    paddd           m4, m6
     paddd           m0, m4
     psrad           m0, 7
     vextracti128   xm1, m0, 1
diff --git a/src/x86/looprestoration_avx2.asm b/src/x86/looprestoration_avx2.asm
index 7787997..59ebf07 100644
--- a/src/x86/looprestoration_avx2.asm
+++ b/src/x86/looprestoration_avx2.asm
@@ -38,43 +38,21 @@ wiener_shufC:  db  6,  5,  7,  6,  8,  7,  9,  8, 10,  9, 11, 10, 12, 11, 13, 12
 sgr_l_shuf:    db  0,  0,  0,  0,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11
 sgr_r_ext:     times 16 db 1
                times 16 db 9
+sgr_shuf:      db  1, -1,  2, -1,  3, -1,  4, -1,  5, -1,  6, -1,  7, -1,  8, -1
+               db  9, -1, 10, -1, 11, -1, 12, -1
 
-; dword version of dav1d_sgr_x_by_x[] for use with gathers, wastes a bit of
-; cache but eliminates some shifts in the inner sgr loop which is overall a win
-const sgr_x_by_x_avx2
-              dd 255,128, 85, 64, 51, 43, 37, 32, 28, 26, 23, 21, 20, 18, 17, 16
-              dd  15, 14, 13, 13, 12, 12, 11, 11, 10, 10,  9,  9,  9,  9,  8,  8
-              dd   8,  8,  7,  7,  7,  7,  7,  6,  6,  6,  6,  6,  6,  6,  5,  5
-              dd   5,  5,  5,  5,  5,  5,  5,  5,  4,  4,  4,  4,  4,  4,  4,  4
-              dd   4,  4,  4,  4,  4,  4,  4,  4,  4,  3,  3,  3,  3,  3,  3,  3
-              dd   3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3
-              dd   3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2
-              dd   2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2
-              dd   2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2
-              dd   2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2
-              dd   2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1
-              dd   1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1
-              dd   1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1
-              dd   1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1
-              dd   1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1
-              dd   1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0
-
-               times 4 db -1 ; needed for 16-bit sgr
 pb_m5:         times 4 db -5
 pb_3:          times 4 db 3
 pw_5_6:        dw 5, 6
-
-sgr_shuf:      db  1, -1,  2, -1,  3, -1,  4, -1,  5, -1,  6, -1,  7, -1,  8, -1
-               db  9, -1, 10, -1, 11, -1, 12, -1
-
+pw_164_24:     dw 164, 24
+pw_455_24:     dw 455, 24
 pw_256:        times 2 dw 256
 pw_2056:       times 2 dw 2056
 pw_m16380:     times 2 dw -16380
 pd_25:         dd 25
 pd_34816:      dd 34816
 pd_m4096:      dd -4096
-pd_0xf00801c7: dd 0xf00801c7
-pd_0xf00800a4: dd 0xf00800a4
+pf_256:        dd 256.0
 
 cextern pb_0to63
 
@@ -720,30 +698,28 @@ ALIGN function_align
     jl .v_loop
     ret
 
-cglobal sgr_filter_5x5_8bpc, 4, 13, 16, 400*24+16, dst, stride, left, lpf, \
+cglobal sgr_filter_5x5_8bpc, 4, 12, 16, 400*24+16, dst, stride, left, lpf, \
                                                    w, h, edge, params
-%define base r12-sgr_x_by_x_avx2-256*4
-    lea            r12, [sgr_x_by_x_avx2+256*4]
     mov        paramsq, r6mp
     mov             wd, wm
     movifnidn       hd, hm
+    vbroadcasti128  m8, [sgr_shuf+0]
     mov          edged, r7m
-    vbroadcasti128  m8, [base+sgr_shuf+0]
-    vbroadcasti128  m9, [base+sgr_shuf+8]
+    vbroadcasti128  m9, [sgr_shuf+8]
     add           lpfq, wq
-    vbroadcasti128 m10, [base+sgr_shuf+2]
+    vbroadcasti128 m10, [sgr_shuf+2]
     add           dstq, wq
-    vbroadcasti128 m11, [base+sgr_shuf+6]
+    vbroadcasti128 m11, [sgr_shuf+6]
     lea             t3, [rsp+wq*4+16+400*12]
-    vpbroadcastd   m12, [paramsq+0] ; s0
-    pxor            m6, m6
     vpbroadcastw    m7, [paramsq+8] ; w0
+    pxor            m6, m6
+    vpbroadcastd   m12, [paramsq+0] ; s0
     lea             t1, [rsp+wq*2+20]
-    vpbroadcastd   m13, [base+pd_0xf00800a4]
+    vpbroadcastd   m13, [pw_164_24]
     neg             wq
-    vpbroadcastd   m14, [base+pd_34816]  ; (1 << 11) + (1 << 15)
+    vbroadcastss   m14, [pf_256]
     psllw           m7, 4
-    vpbroadcastd   m15, [base+pd_m4096]
+    vpbroadcastd   m15, [pd_m4096]
     test         edgeb, 4 ; LR_HAVE_TOP
     jz .no_top
     call .h_top
@@ -841,7 +817,7 @@ cglobal sgr_filter_5x5_8bpc, 4, 13, 16, 400*24+16, dst, stride, left, lpf, \
     jmp .h_main
 .h_extend_left:
     mova           xm5, [lpfq+wq]
-    pshufb         xm5, [base+sgr_l_shuf]
+    pshufb         xm5, [sgr_l_shuf]
     jmp .h_main
 .h_top:
     lea            r10, [wq-2]
@@ -920,7 +896,7 @@ ALIGN function_align
     jmp .hv_main
 .hv_extend_left:
     mova           xm5, [lpfq+wq]
-    pshufb         xm5, [base+sgr_l_shuf]
+    pshufb         xm5, [sgr_l_shuf]
     jmp .hv_main
 .hv_bottom:
     lea            r10, [wq-2]
@@ -985,16 +961,29 @@ ALIGN function_align
     pmulld          m5, m12
     pmaddwd         m0, m13              ; b * 164
     pmaddwd         m1, m13
-    paddusw         m4, m13
-    paddusw         m5, m13
-    psrad           m3, m4, 20           ; min(z, 255) - 256
-    vpgatherdd      m2, [r12+m3*4], m4   ; x
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r12+m4*4], m5
+    paddw           m4, m13
+    paddw           m5, m13
+    psrld           m4, 20               ; z + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4               ; 1 / (z + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m14, m4
+    pcmpgtd         m5, m14, m5
+    mulps           m2, m14              ; 256 / (z + 1)
+    mulps           m3, m14
+    psrld           m4, 24               ; z < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4               ; x
+    pminsw          m3, m5
+    vpbroadcastd    m4, [pd_34816]
     pmulld          m0, m2
     pmulld          m1, m3
-    paddd           m0, m14              ; x * b * 164 + (1 << 11) + (1 << 15)
-    paddd           m1, m14
+    paddd           m0, m4               ; x * b * 164 + (1 << 11) + (1 << 15)
+    paddd           m1, m4
     pand            m0, m15
     pand            m1, m15
     por             m0, m2               ; a | (b << 12)
@@ -1045,16 +1034,29 @@ ALIGN function_align
     pmulld          m5, m12
     pmaddwd         m0, m13              ; b * 164
     pmaddwd         m1, m13
-    paddusw         m4, m13
-    paddusw         m5, m13
-    psrad           m3, m4, 20           ; min(z, 255) - 256
-    vpgatherdd      m2, [r12+m3*4], m4   ; x
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r12+m4*4], m5
+    paddw           m4, m13
+    paddw           m5, m13
+    psrld           m4, 20               ; z + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4               ; 1 / (z + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m14, m4
+    pcmpgtd         m5, m14, m5
+    mulps           m2, m14              ; 256 / (z + 1)
+    mulps           m3, m14
+    psrld           m4, 24               ; z < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4               ; x
+    pminsw          m3, m5
+    vpbroadcastd    m4, [pd_34816]
     pmulld          m0, m2
     pmulld          m1, m3
-    paddd           m0, m14              ; x * b * 164 + (1 << 11) + (1 << 15)
-    paddd           m1, m14
+    paddd           m0, m4               ; x * b * 164 + (1 << 11) + (1 << 15)
+    paddd           m1, m4
     pand            m0, m15
     pand            m1, m15
     por             m0, m2               ; a | (b << 12)
@@ -1167,29 +1169,28 @@ ALIGN function_align
     add           dstq, strideq
     ret
 
-cglobal sgr_filter_3x3_8bpc, 4, 15, 15, -400*28-16, dst, stride, left, lpf, \
+cglobal sgr_filter_3x3_8bpc, 4, 14, 16, -400*28-16, dst, stride, left, lpf, \
                                                     w, h, edge, params
-%define base r14-sgr_x_by_x_avx2-256*4
     mov        paramsq, r6mp
     mov             wd, wm
     movifnidn       hd, hm
+    vbroadcasti128  m8, [sgr_shuf+2]
     mov          edged, r7m
-    lea            r14, [sgr_x_by_x_avx2+256*4]
-    vbroadcasti128  m8, [base+sgr_shuf+2]
+    vbroadcasti128  m9, [sgr_shuf+4]
     add           lpfq, wq
-    vbroadcasti128  m9, [base+sgr_shuf+4]
+    vbroadcasti128 m10, [sgr_shuf+6]
     add           dstq, wq
-    vbroadcasti128 m10, [base+sgr_shuf+6]
+    vpbroadcastw    m7, [paramsq+10] ; w1
     lea             t3, [rsp+wq*4+16+400*12]
     vpbroadcastd   m11, [paramsq+ 4] ; s1
     pxor            m6, m6
-    vpbroadcastw    m7, [paramsq+10] ; w1
+    vpbroadcastd   m12, [pw_455_24]
     lea             t1, [rsp+wq*2+20]
-    vpbroadcastd   m12, [base+pd_0xf00801c7]
+    vbroadcastss   m13, [pf_256]
     neg             wq
-    vpbroadcastd   m13, [base+pd_34816] ; (1 << 11) + (1 << 15)
+    vpbroadcastd   m14, [pd_34816] ; (1 << 11) + (1 << 15)
     psllw           m7, 4
-    vpbroadcastd   m14, [base+pd_m4096]
+    vpbroadcastd   m15, [pd_m4096]
     test         edgeb, 4 ; LR_HAVE_TOP
     jz .no_top
     call .h_top
@@ -1262,7 +1263,7 @@ cglobal sgr_filter_3x3_8bpc, 4, 15, 15, -400*28-16, dst, stride, left, lpf, \
     jmp .h_main
 .h_extend_left:
     mova           xm5, [lpfq+wq]
-    pshufb         xm5, [base+sgr_l_shuf]
+    pshufb         xm5, [sgr_l_shuf]
     jmp .h_main
 .h_top:
     lea            r10, [wq-2]
@@ -1310,7 +1311,7 @@ ALIGN function_align
     jmp .hv_main
 .hv_extend_left:
     mova           xm5, [lpfq+wq]
-    pshufb         xm5, [base+sgr_l_shuf]
+    pshufb         xm5, [sgr_l_shuf]
     jmp .hv_main
 .hv_bottom:
     lea            r10, [wq-2]
@@ -1363,18 +1364,30 @@ ALIGN function_align
     pmulld          m5, m11
     pmaddwd         m0, m12              ; b * 455
     pmaddwd         m1, m12
-    paddusw         m4, m12
-    paddusw         m5, m12
-    psrad           m3, m4, 20           ; min(z, 255) - 256
-    vpgatherdd      m2, [r14+m3*4], m4
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r14+m4*4], m5
+    paddw           m4, m12
+    paddw           m5, m12
+    psrld           m4, 20               ; z + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4               ; 1 / (z + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m13, m4
+    pcmpgtd         m5, m13, m5
+    mulps           m2, m13              ; 256 / (z + 1)
+    mulps           m3, m13
+    psrld           m4, 24               ; z < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4               ; x
+    pminsw          m3, m5
     pmulld          m0, m2
     pmulld          m1, m3
-    paddd           m0, m13              ; x * b * 455 + (1 << 11) + (1 << 15)
-    paddd           m1, m13
-    pand            m0, m14
-    pand            m1, m14
+    paddd           m0, m14              ; x * b * 455 + (1 << 11) + (1 << 15)
+    paddd           m1, m14
+    pand            m0, m15
+    pand            m1, m15
     por             m0, m2               ; a | (b << 12)
     por             m1, m3
     mova         [t3+r10*4+ 8], xm0
@@ -1413,18 +1426,30 @@ ALIGN function_align
     pmulld          m5, m11
     pmaddwd         m0, m12              ; b * 455
     pmaddwd         m1, m12
-    paddusw         m4, m12
-    paddusw         m5, m12
-    psrad           m3, m4, 20           ; min(z, 255) - 256
-    vpgatherdd      m2, [r14+m3*4], m4
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r14+m4*4], m5
+    paddw           m4, m12
+    paddw           m5, m12
+    psrld           m4, 20               ; z + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4               ; 1 / (z + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m13, m4
+    pcmpgtd         m5, m13, m5
+    mulps           m2, m13              ; 256 / (z + 1)
+    mulps           m3, m13
+    psrld           m4, 24               ; z < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4               ; x
+    pminsw          m3, m5
     pmulld          m0, m2
     pmulld          m1, m3
-    paddd           m0, m13              ; x * b * 455 + (1 << 11) + (1 << 15)
-    paddd           m1, m13
-    pand            m0, m14
-    pand            m1, m14
+    paddd           m0, m14              ; x * b * 455 + (1 << 11) + (1 << 15)
+    paddd           m1, m14
+    pand            m0, m15
+    pand            m1, m15
     por             m0, m2               ; a | (b << 12)
     por             m1, m3
     mova         [t3+r10*4+ 8], xm0
@@ -1485,16 +1510,16 @@ ALIGN function_align
     paddd           m5, m5
     psubd           m5, m4
     mova [t5+r10*4+32], m5
-    pandn           m4, m14, m0
+    pandn           m4, m15, m0
     psrld           m0, 12
     paddd           m3, m5
-    pandn           m5, m14, m2
+    pandn           m5, m15, m2
     psrld           m2, 12
     paddd           m4, m5                ; a
-    pandn           m5, m14, m1
+    pandn           m5, m15, m1
     psrld           m1, 12
     paddd           m0, m2                ; b + (1 << 8)
-    pandn           m2, m14, m3
+    pandn           m2, m15, m3
     psrld           m3, 12
     paddd           m5, m2
     pmovzxbd        m2, [dstq+r10+0]
@@ -1522,19 +1547,17 @@ ALIGN function_align
     add           dstq, strideq
     ret
 
-cglobal sgr_filter_mix_8bpc, 4, 13, 16, 400*56+8, dst, stride, left, lpf, \
+cglobal sgr_filter_mix_8bpc, 4, 12, 16, 400*56+8, dst, stride, left, lpf, \
                                                   w, h, edge, params
-%define base r12-sgr_x_by_x_avx2-256*4
-    lea            r12, [sgr_x_by_x_avx2+256*4]
     mov        paramsq, r6mp
     mov             wd, wm
     movifnidn       hd, hm
     mov          edged, r7m
-    vbroadcasti128  m9, [base+sgr_shuf+0]
-    vbroadcasti128 m10, [base+sgr_shuf+8]
+    vbroadcasti128  m9, [sgr_shuf+0]
+    vbroadcasti128 m10, [sgr_shuf+8]
     add           lpfq, wq
-    vbroadcasti128 m11, [base+sgr_shuf+2]
-    vbroadcasti128 m12, [base+sgr_shuf+6]
+    vbroadcasti128 m11, [sgr_shuf+2]
+    vbroadcasti128 m12, [sgr_shuf+6]
     add           dstq, wq
     vpbroadcastd   m15, [paramsq+8] ; w0 w1
     lea             t3, [rsp+wq*4+400*24+8]
@@ -1643,7 +1666,7 @@ cglobal sgr_filter_mix_8bpc, 4, 13, 16, 400*56+8, dst, stride, left, lpf, \
     jmp .h_main
 .h_extend_left:
     mova           xm5, [lpfq+wq]
-    pshufb         xm5, [base+sgr_l_shuf]
+    pshufb         xm5, [sgr_l_shuf]
     jmp .h_main
 .h_top:
     lea            r10, [wq-2]
@@ -1704,7 +1727,7 @@ ALIGN function_align
     jmp .hv0_main
 .hv0_extend_left:
     mova           xm5, [lpfq+wq]
-    pshufb         xm5, [base+sgr_l_shuf]
+    pshufb         xm5, [sgr_l_shuf]
     jmp .hv0_main
 .hv0_bottom:
     lea            r10, [wq-2]
@@ -1765,7 +1788,9 @@ ALIGN function_align
     mova [t2+r10*2+400* 6], m0
     mova [t2+r10*2+400* 8], m4
     mova [t2+r10*2+400*10], m5
+    vpbroadcastd    m8, [pw_455_24]
     punpcklwd       m0, m1, m7           ; b3
+    vbroadcastss    m6, [pf_256]
     punpckhwd       m1, m7
     pslld           m4, m2, 3
     pslld           m5, m3, 3
@@ -1774,26 +1799,37 @@ ALIGN function_align
     paddd           m5, m3
     pmaddwd         m3, m1, m1
     psubd           m4, m2               ; p3
-    vpbroadcastd    m2, [base+pd_0xf00801c7]
     psubd           m5, m3
     pmulld          m4, m14              ; p3 * s1
     pmulld          m5, m14
-    pmaddwd         m0, m2               ; b3 * 455
-    pmaddwd         m1, m2
-    paddusw         m4, m2
-    paddusw         m5, m2
-    psrad           m3, m4, 20           ; min(z3, 255) - 256
-    vpgatherdd      m2, [r12+m3*4], m4
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r12+m4*4], m5
-    vpbroadcastd    m4, [base+pd_34816]
+    pmaddwd         m0, m8               ; b3 * 455
+    pmaddwd         m1, m8
+    paddw           m4, m8
+    paddw           m5, m8
+    vpbroadcastd    m8, [pd_34816]
+    psrld           m4, 20               ; z3 + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4               ; 1 / (z3 + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m6, m4
+    pcmpgtd         m5, m6, m5
+    mulps           m2, m6               ; 256 / (z3 + 1)
+    mulps           m3, m6
+    vpbroadcastd    m6, [pd_m4096]
+    psrld           m4, 24               ; z3 < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4               ; x3
+    pminsw          m3, m5
     pmulld          m0, m2
-    vpbroadcastd    m5, [base+pd_m4096]
     pmulld          m1, m3
-    paddd           m0, m4               ; x3 * b3 * 455 + (1 << 11) + (1 << 15)
-    paddd           m1, m4
-    pand            m0, m5
-    pand            m1, m5
+    paddd           m0, m8               ; x3 * b3 * 455 + (1 << 11) + (1 << 15)
+    paddd           m1, m8
+    pand            m0, m6
+    pand            m1, m6
     por             m0, m2               ; a3 | (b3 << 12)
     por             m1, m3
     mova         [t3+r10*4+400*4+ 8], xm0
@@ -1815,7 +1851,7 @@ ALIGN function_align
     jmp .hv1_main
 .hv1_extend_left:
     mova           xm5, [lpfq+wq]
-    pshufb         xm5, [base+sgr_l_shuf]
+    pshufb         xm5, [sgr_l_shuf]
     jmp .hv1_main
 .hv1_bottom:
     lea            r10, [wq-2]
@@ -1859,6 +1895,7 @@ ALIGN function_align
     paddd           m3, m5, [t2+r10*2+400*10]
     mova [t2+r10*2+400* 8], m4
     mova [t2+r10*2+400*10], m5
+    vpbroadcastd    m9, [pw_455_24]
     paddd           m4, m0 ; sumsq5
     paddd           m5, m6
     punpcklwd       m0, m1, m7           ; b3
@@ -1870,26 +1907,38 @@ ALIGN function_align
     paddd           m7, m3
     pmaddwd         m3, m1, m1
     psubd           m6, m2               ; p3
-    vpbroadcastd    m2, [base+pd_0xf00801c7]
     psubd           m7, m3
     pmulld          m6, m14              ; p3 * s1
     pmulld          m7, m14
-    pmaddwd         m0, m2               ; b3 * 455
-    pmaddwd         m1, m2
-    paddusw         m6, m2
-    paddusw         m7, m2
-    psrad           m3, m6, 20           ; min(z3, 255) - 256
-    vpgatherdd      m2, [r12+m3*4], m6
-    psrad           m6, m7, 20
-    vpgatherdd      m3, [r12+m6*4], m7
-    vpbroadcastd    m6, [base+pd_34816]  ; x3
+    pmaddwd         m0, m9               ; b3 * 455
+    pmaddwd         m1, m9
+    paddw           m6, m9
+    paddw           m7, m9
+    vbroadcastss    m9, [pf_256]
+    psrld           m6, 20               ; z3 + 1
+    psrld           m7, 20
+    cvtdq2ps        m6, m6
+    cvtdq2ps        m7, m7
+    rcpps           m2, m6               ; 1 / (z3 + 1)
+    rcpps           m3, m7
+    pcmpgtd         m6, m9, m6
+    pcmpgtd         m7, m9, m7
+    mulps           m2, m9               ; 256 / (z3 + 1)
+    mulps           m3, m9
+    vpbroadcastd    m9, [pd_34816]
+    psrld           m6, 24               ; z3 < 255 ? 255 : 0
+    psrld           m7, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m6               ; x3
+    vpbroadcastd    m6, [pd_m4096]
+    pminsw          m3, m7
     pmulld          m0, m2
-    vpbroadcastd    m7, [base+pd_m4096]
     pmulld          m1, m3
-    paddd           m0, m6               ; x3 * b3 * 455 + (1 << 11) + (1 << 15)
-    paddd           m1, m6
-    pand            m0, m7
-    pand            m7, m1
+    paddd           m0, m9               ; x3 * b3 * 455 + (1 << 11) + (1 << 15)
+    paddd           m1, m9
+    pand            m0, m6
+    pand            m7, m6, m1
     por             m0, m2               ; a3 | (b3 << 12)
     por             m7, m3
     paddw           m1, m8, [t2+r10*2+400*0]
@@ -1905,8 +1954,9 @@ ALIGN function_align
     vextracti128 [t3+r10*4+400*8+40], m0, 1
     mova         [t3+r10*4+400*8+24], xm7
     vextracti128 [t3+r10*4+400*8+56], m7, 1
-    vpbroadcastd    m4, [base+pd_25]
+    vpbroadcastd    m4, [pd_25]
     pxor            m7, m7
+    vpbroadcastd    m8, [pw_164_24]
     punpcklwd       m0, m1, m7           ; b5
     punpckhwd       m1, m7
     pmulld          m2, m4               ; a5 * 25
@@ -1914,23 +1964,35 @@ ALIGN function_align
     pmaddwd         m4, m0, m0           ; b5 * b5
     pmaddwd         m5, m1, m1
     psubd           m2, m4               ; p5
-    vpbroadcastd    m4, [base+pd_0xf00800a4]
     psubd           m3, m5
     pmulld          m2, m13              ; p5 * s0
     pmulld          m3, m13
-    pmaddwd         m0, m4               ; b5 * 164
-    pmaddwd         m1, m4
-    paddusw         m2, m4
-    paddusw         m3, m4
-    psrad           m5, m2, 20           ; min(z5, 255) - 256
-    vpgatherdd      m4, [r12+m5*4], m2   ; x5
-    psrad           m2, m3, 20
-    vpgatherdd      m5, [r12+m2*4], m3
+    pmaddwd         m0, m8               ; b5 * 164
+    pmaddwd         m1, m8
+    paddw           m2, m8
+    paddw           m3, m8
+    vbroadcastss    m8, [pf_256]
+    psrld           m2, 20               ; z5 + 1
+    psrld           m3, 20
+    cvtdq2ps        m2, m2
+    cvtdq2ps        m3, m3
+    rcpps           m4, m2               ; 1 / (z5 + 1)
+    rcpps           m5, m3
+    pcmpgtd         m2, m8, m2
+    pcmpgtd         m3, m8, m3
+    mulps           m4, m8               ; 256 / (z5 + 1)
+    mulps           m5, m8
+    psrld           m2, 24               ; z5 < 255 ? 255 : 0
+    psrld           m3, 24
+    cvtps2dq        m4, m4
+    cvtps2dq        m5, m5
+    pminsw          m4, m2               ; x5
+    pminsw          m5, m3
     pmulld          m0, m4
     pmulld          m1, m5
-    paddd           m0, m6               ; x5 * b5 * 164 + (1 << 11) + (1 << 15)
-    paddd           m1, m6
-    vpbroadcastd    m6, [base+pd_m4096]
+    paddd           m0, m9               ; x5 * b5 * 164 + (1 << 11) + (1 << 15)
+    paddd           m1, m9
+    vbroadcasti128  m9, [sgr_shuf]
     pand            m0, m6
     pand            m1, m6
     por             m0, m4               ; a5 | (b5 << 12)
@@ -1947,8 +2009,7 @@ ALIGN function_align
     ret
 .v0: ; vertical boxsums + ab3 (even rows)
     lea            r10, [wq-2]
-    vpbroadcastd    m6, [base+pd_34816]
-    vpbroadcastd    m8, [base+pd_m4096]
+    vpbroadcastd    m6, [pd_34816]
 .v0_loop:
     mova            m0, [t1+r10*2+400* 6]
     mova            m4, [t1+r10*2+400* 8]
@@ -1962,6 +2023,7 @@ ALIGN function_align
     mova [t2+r10*2+400* 6], m0
     mova [t2+r10*2+400* 8], m4
     mova [t2+r10*2+400*10], m5
+    vpbroadcastd    m8, [pw_455_24]
     punpcklwd       m0, m1, m7           ; b3
     punpckhwd       m1, m7
     pslld           m4, m2, 3
@@ -1971,18 +2033,31 @@ ALIGN function_align
     paddd           m5, m3
     pmaddwd         m3, m1, m1
     psubd           m4, m2               ; p3
-    vpbroadcastd    m2, [base+pd_0xf00801c7]
     psubd           m5, m3
     pmulld          m4, m14              ; p3 * s1
     pmulld          m5, m14
-    pmaddwd         m0, m2               ; b3 * 455
-    pmaddwd         m1, m2
-    paddusw         m4, m2
-    paddusw         m5, m2
-    psrad           m3, m4, 20           ; min(z3, 255) - 256
-    vpgatherdd      m2, [r12+m3*4], m4   ; x3
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r12+m4*4], m5
+    pmaddwd         m0, m8               ; b3 * 455
+    pmaddwd         m1, m8
+    paddw           m4, m8
+    paddw           m5, m8
+    vbroadcastss    m8, [pf_256]
+    psrld           m4, 20               ; z3 + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4               ; 1 / (z3 + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m8, m4
+    pcmpgtd         m5, m8, m5
+    mulps           m2, m8               ; 256 / (z3 + 1)
+    mulps           m3, m8
+    vpbroadcastd    m8, [pd_m4096]
+    psrld           m4, 24               ; z3 < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4               ; x3
+    pminsw          m3, m5
     pmulld          m0, m2
     pmulld          m1, m3
     paddd           m0, m6               ; x3 * b3 * 455 + (1 << 11) + (1 << 15)
@@ -2022,6 +2097,7 @@ ALIGN function_align
     mova [t2+r10*2+400* 6], m4
     mova [t2+r10*2+400* 8], m5
     mova [t2+r10*2+400*10], m6
+    vpbroadcastd    m8, [pw_455_24]
     punpcklwd       m0, m1, m7           ; b3
     punpckhwd       m1, m7
     pslld           m4, m2, 3
@@ -2031,21 +2107,33 @@ ALIGN function_align
     paddd           m5, m3
     pmaddwd         m3, m1, m1
     psubd           m4, m2               ; p3
-    vpbroadcastd    m2, [base+pd_0xf00801c7]
     psubd           m5, m3
     pmulld          m4, m14              ; p3 * s1
     pmulld          m5, m14
-    pmaddwd         m0, m2               ; b3 * 455
-    pmaddwd         m1, m2
-    paddusw         m4, m2
-    paddusw         m5, m2
-    psrad           m3, m4, 20           ; min(z3, 255) - 256
-    vpgatherdd      m2, [r12+m3*4], m4   ; x3
-    psrad           m4, m5, 20
-    vpgatherdd      m3, [r12+m4*4], m5
-    vpbroadcastd    m4, [base+pd_34816]
+    pmaddwd         m0, m8               ; b3 * 455
+    pmaddwd         m1, m8
+    paddw           m4, m8
+    paddw           m5, m8
+    vbroadcastss    m8, [pf_256]
+    psrld           m4, 20               ; z3 + 1
+    psrld           m5, 20
+    cvtdq2ps        m4, m4
+    cvtdq2ps        m5, m5
+    rcpps           m2, m4               ; 1 / (z3 + 1)
+    rcpps           m3, m5
+    pcmpgtd         m4, m8, m4
+    pcmpgtd         m5, m8, m5
+    mulps           m2, m8               ; 256 / (z3 + 1)
+    mulps           m3, m8
+    vpbroadcastd    m8, [pd_m4096]
+    psrld           m4, 24               ; z3 < 255 ? 255 : 0
+    psrld           m5, 24
+    cvtps2dq        m2, m2
+    cvtps2dq        m3, m3
+    pminsw          m2, m4               ; x3
+    vpbroadcastd    m4, [pd_34816]
+    pminsw          m3, m5
     pmulld          m0, m2
-    vpbroadcastd    m8, [base+pd_m4096]
     pmulld          m1, m3
     paddd           m0, m4               ; x3 * b3 * 455 + (1 << 11) + (1 << 15)
     paddd           m1, m4
@@ -2065,36 +2153,49 @@ ALIGN function_align
     mova [t2+r10*2+400*0], m4
     mova [t2+r10*2+400*2], m5
     mova [t2+r10*2+400*4], m6
-    vpbroadcastd    m4, [base+pd_25]
+    vpbroadcastd    m4, [pd_25]
     mova         [t3+r10*4+400*8+ 8], xm0
     vextracti128 [t3+r10*4+400*8+40], m0, 1
     mova         [t3+r10*4+400*8+24], xm8
     vextracti128 [t3+r10*4+400*8+56], m8, 1
+    vpbroadcastd    m8, [pw_164_24]
     punpcklwd       m0, m1, m7           ; b5
+    vbroadcastss    m6, [pf_256]
     punpckhwd       m1, m7
     pmulld          m2, m4               ; a5 * 25
     pmulld          m3, m4
     pmaddwd         m4, m0, m0           ; b5 * b5
     pmaddwd         m5, m1, m1
     psubd           m2, m4               ; p5
-    vpbroadcastd    m4, [base+pd_0xf00800a4]
     psubd           m3, m5
     pmulld          m2, m13              ; p5 * s0
     pmulld          m3, m13
-    pmaddwd         m0, m4               ; b5 * 164
-    pmaddwd         m1, m4
-    paddusw         m2, m4
-    paddusw         m3, m4
-    psrad           m5, m2, 20           ; min(z5, 255) - 256
-    vpgatherdd      m4, [r12+m5*4], m2   ; x5
-    psrad           m2, m3, 20
-    vpgatherdd      m5, [r12+m2*4], m3
+    pmaddwd         m0, m8               ; b5 * 164
+    pmaddwd         m1, m8
+    paddw           m2, m8
+    paddw           m3, m8
+    vpbroadcastd    m8, [pd_34816]
+    psrld           m2, 20               ; z5 + 1
+    psrld           m3, 20
+    cvtdq2ps        m2, m2
+    cvtdq2ps        m3, m3
+    rcpps           m4, m2               ; 1 / (z5 + 1)
+    rcpps           m5, m3
+    pcmpgtd         m2, m6, m2
+    pcmpgtd         m3, m6, m3
+    mulps           m4, m6               ; 256 / (z5 + 1)
+    mulps           m5, m6
+    vpbroadcastd    m6, [pd_m4096]
+    psrld           m2, 24               ; z5 < 255 ? 255 : 0
+    psrld           m3, 24
+    cvtps2dq        m4, m4
+    cvtps2dq        m5, m5
+    pminsw          m4, m2               ; x5
+    pminsw          m5, m3
     pmulld          m0, m4
-    vpbroadcastd    m6, [base+pd_34816]
     pmulld          m1, m5
-    paddd           m0, m6               ; x5 * b5 * 164 + (1 << 11) + (1 << 15)
-    paddd           m1, m6
-    vpbroadcastd    m6, [base+pd_m4096]
+    paddd           m0, m8               ; x5 * b5 * 164 + (1 << 11) + (1 << 15)
+    paddd           m1, m8
     pand            m0, m6
     pand            m1, m6
     por             m0, m4               ; a5 | (b5 << 12)
diff --git a/src/x86/mc.h b/src/x86/mc.h
index b142361..d1b7ae6 100644
--- a/src/x86/mc.h
+++ b/src/x86/mc.h
@@ -29,7 +29,6 @@
 #include "src/mc.h"
 
 #define decl_fn(type, name) \
-    decl_##type##_fn(BF(name, sse2)); \
     decl_##type##_fn(BF(name, ssse3)); \
     decl_##type##_fn(BF(name, avx2)); \
     decl_##type##_fn(BF(name, avx512icl));
@@ -108,25 +107,6 @@ decl_fn(resize, dav1d_resize);
 static ALWAYS_INLINE void mc_dsp_init_x86(Dav1dMCDSPContext *const c) {
     const unsigned flags = dav1d_get_cpu_flags();
 
-    if(!(flags & DAV1D_X86_CPU_FLAG_SSE2))
-        return;
-
-#if BITDEPTH == 8
-    init_mct_fn(FILTER_2D_BILINEAR,            bilin,               sse2);
-    init_mct_fn(FILTER_2D_8TAP_REGULAR,        8tap_regular,        sse2);
-    init_mct_fn(FILTER_2D_8TAP_REGULAR_SMOOTH, 8tap_regular_smooth, sse2);
-    init_mct_fn(FILTER_2D_8TAP_REGULAR_SHARP,  8tap_regular_sharp,  sse2);
-    init_mct_fn(FILTER_2D_8TAP_SMOOTH_REGULAR, 8tap_smooth_regular, sse2);
-    init_mct_fn(FILTER_2D_8TAP_SMOOTH,         8tap_smooth,         sse2);
-    init_mct_fn(FILTER_2D_8TAP_SMOOTH_SHARP,   8tap_smooth_sharp,   sse2);
-    init_mct_fn(FILTER_2D_8TAP_SHARP_REGULAR,  8tap_sharp_regular,  sse2);
-    init_mct_fn(FILTER_2D_8TAP_SHARP_SMOOTH,   8tap_sharp_smooth,   sse2);
-    init_mct_fn(FILTER_2D_8TAP_SHARP,          8tap_sharp,          sse2);
-
-    c->warp8x8  = BF(dav1d_warp_affine_8x8, sse2);
-    c->warp8x8t = BF(dav1d_warp_affine_8x8t, sse2);
-#endif
-
     if(!(flags & DAV1D_X86_CPU_FLAG_SSSE3))
         return;
 
diff --git a/src/x86/mc16_avx512.asm b/src/x86/mc16_avx512.asm
index 27715c1..3e352c5 100644
--- a/src/x86/mc16_avx512.asm
+++ b/src/x86/mc16_avx512.asm
@@ -1501,19 +1501,6 @@ cglobal %1_%2_16bpc
 %endif
 %endmacro
 
-%macro MC_8TAP_FN 4 ; prefix, type, type_h, type_v
-cglobal %1_8tap_%2_16bpc
-    mov                 t0d, FILTER_%3
-%ifidn %3, %4
-    mov                 t1d, t0d
-%else
-    mov                 t1d, FILTER_%4
-%endif
-%ifnidn %2, regular ; skip the jump in the last filter
-    jmp mangle(private_prefix %+ _%1_8tap_16bpc %+ SUFFIX)
-%endif
-%endmacro
-
 %if WIN64
 DECLARE_REG_TMP 4, 5
 %define buf rsp+stack_offset+8 ; shadow space
diff --git a/src/x86/mc16_sse.asm b/src/x86/mc16_sse.asm
index b0c4259..319dd45 100644
--- a/src/x86/mc16_sse.asm
+++ b/src/x86/mc16_sse.asm
@@ -67,6 +67,8 @@ pw_m512:          times 8 dw -512
 pd_63:            times 4 dd 63
 pd_64:            times 4 dd 64
 pd_512:           times 4 dd 512
+pd_2560:          times 2 dd 2560
+pd_8704:          times 2 dd 8704
 pd_m524256:       times 4 dd -524256 ; -8192 << 6 + 32
 pd_0x3ff:         times 4 dd 0x3ff
 pd_0x4000:        times 4 dd 0x4000
@@ -1158,7 +1160,7 @@ cglobal prep_bilin_16bpc, 4, 7, 0, tmp, src, stride, w, h, mxy, stride3
 %assign FILTER_SMOOTH  (1*15 << 16) | 4*15
 %assign FILTER_SHARP   (2*15 << 16) | 3*15
 
-%macro FN 4 ; prefix, type, type_h, type_v
+%macro FN 4-5 ; prefix, type, type_h, type_v, jmp_to
 cglobal %1_%2_16bpc
     mov                 t0d, FILTER_%3
 %ifidn %3, %4
@@ -1166,8 +1168,8 @@ cglobal %1_%2_16bpc
 %else
     mov                 t1d, FILTER_%4
 %endif
-%ifnidn %2, regular ; skip the jump in the last filter
-    jmp mangle(private_prefix %+ _%1_16bpc %+ SUFFIX)
+%if %0 == 5 ; skip the jump in the last filter
+    jmp mangle(private_prefix %+ _%5 %+ SUFFIX)
 %endif
 %endmacro
 
@@ -1180,40 +1182,25 @@ DECLARE_REG_TMP 7, 8, 8
 %endif
 
 %define PUT_8TAP_FN FN put_8tap,
-PUT_8TAP_FN sharp,          SHARP,   SHARP
-PUT_8TAP_FN sharp_smooth,   SHARP,   SMOOTH
-PUT_8TAP_FN smooth_sharp,   SMOOTH,  SHARP
-PUT_8TAP_FN smooth,         SMOOTH,  SMOOTH
-PUT_8TAP_FN sharp_regular,  SHARP,   REGULAR
-PUT_8TAP_FN regular_sharp,  REGULAR, SHARP
-PUT_8TAP_FN smooth_regular, SMOOTH,  REGULAR
-PUT_8TAP_FN regular_smooth, REGULAR, SMOOTH
+PUT_8TAP_FN smooth,         SMOOTH,  SMOOTH,  put_6tap_16bpc
+PUT_8TAP_FN smooth_regular, SMOOTH,  REGULAR, put_6tap_16bpc
+PUT_8TAP_FN regular_smooth, REGULAR, SMOOTH,  put_6tap_16bpc
 PUT_8TAP_FN regular,        REGULAR, REGULAR
 
+cglobal put_6tap_16bpc, 0, 9, 0, dst, ds, src, ss, w, h, mx, my
+    %define            base  t2-put_ssse3
 %if ARCH_X86_32
-cglobal put_8tap_16bpc, 0, 7, 8, dst, ds, src, ss, w, h, mx, my
-%define mxb r0b
-%define mxd r0
-%define mxq r0
-%define myb r1b
-%define myd r1
-%define myq r1
-%define  m8 [esp+16*0]
-%define  m9 [esp+16*1]
-%define m10 [esp+16*2]
-%define m11 [esp+16*3]
-%define m12 [esp+16*4]
-%define m13 [esp+16*5]
-%define m14 [esp+16*6]
-%define m15 [esp+16*7]
-%else
-cglobal put_8tap_16bpc, 4, 9, 0, dst, ds, src, ss, w, h, mx, my
-%endif
-%define base t2-put_ssse3
+    %define             mxb  r0b
+    %define             mxd  r0
+    %define             mxq  r0
+    %define             myb  r1b
+    %define             myd  r1
+    %define             myq  r1
+%endif
     imul                mxd, mxm, 0x010101
-    add                 mxd, t0d ; 8tap_h, mx, 4tap_h
+    add                 mxd, t0d ; 6tap_h, mx, 4tap_h
     imul                myd, mym, 0x010101
-    add                 myd, t1d ; 8tap_v, my, 4tap_v
+    add                 myd, t1d ; 6tap_v, my, 4tap_v
     LEA                  t2, put_ssse3
     movifnidn            wd, wm
     movifnidn          srcq, srcmp
@@ -1223,6 +1210,7 @@ cglobal put_8tap_16bpc, 4, 9, 0, dst, ds, src, ss, w, h, mx, my
     jnz .h
     test                myd, 0xf00
     jnz .v
+.put:
     tzcnt                wd, wd
     movzx                wd, word [base+put_ssse3_table+wq*2]
     movifnidn          dstq, dstmp
@@ -1233,24 +1221,6 @@ cglobal put_8tap_16bpc, 4, 9, 0, dst, ds, src, ss, w, h, mx, my
     pop                  r7
 %endif
     jmp                  wq
-.h:
-    test                myd, 0xf00
-    jnz .hv
-    mov                 myd, r8m
-    movd                 m5, r8m
-    shr                 myd, 11
-    movddup              m4, [base+put_8tap_h_rnd+myq*8]
-    movifnidn           dsq, dsmp
-    pshufb               m5, [base+pw_256]
-    cmp                  wd, 4
-    jg .h_w8
-    movzx               mxd, mxb
-    lea                srcq, [srcq-2]
-    movq                 m3, [base+subpel_filters+mxq*8]
-    movifnidn          dstq, dstmp
-    punpcklbw            m3, m3
-    psraw                m3, 8 ; sign-extend
-    je .h_w4
 .h_w2:
     mova                 m2, [base+spel_h_shuf2]
     pshufd               m3, m3, q2121
@@ -1277,89 +1247,111 @@ cglobal put_8tap_16bpc, 4, 9, 0, dst, ds, src, ss, w, h, mx, my
     jg .h_w2_loop
     RET
 .h_w4:
-    WIN64_SPILL_XMM       8
-    mova                 m6, [base+spel_h_shufA]
-    mova                 m7, [base+spel_h_shufB]
+    movzx               mxd, mxb
+    lea                srcq, [srcq-2]
+    movq                 m3, [base+subpel_filters+mxq*8]
+    movifnidn          dstq, dstmp
+    punpcklbw            m3, m3
+    psraw                m3, 8 ; sign-extend
+    jl .h_w2
+    WIN64_SPILL_XMM       9
+    mova                 m7, [base+spel_h_shufA]
+%if ARCH_X86_32
+    %define              m8  [base+spel_h_shufB]
+%else
+    mova                 m8, [base+spel_h_shufB]
+%endif
     pshufd               m2, m3, q1111
     pshufd               m3, m3, q2222
 .h_w4_loop:
-    movu                 m1, [srcq]
-    add                srcq, ssq
-    pshufb               m0, m1, m6 ; 0 1 1 2 2 3 3 4
-    pshufb               m1, m7     ; 2 3 3 4 4 5 5 6
-    pmaddwd              m0, m2
+    movu                 m0, [srcq+ssq*0]
+    movu                 m1, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    pshufb               m6, m0, m7 ; 0 1 1 2 2 3 3 4
+    pmaddwd              m6, m2
+    pshufb               m0, m8     ; 2 3 3 4 4 5 5 6
+    pmaddwd              m0, m3
+    paddd                m0, m6
+    pshufb               m6, m1, m7
+    pmaddwd              m6, m2
+    pshufb               m1, m8
     pmaddwd              m1, m3
     paddd                m0, m4
-    paddd                m0, m1
+    paddd                m6, m4
+    paddd                m1, m6
     psrad                m0, 6
-    packssdw             m0, m0
+    psrad                m1, 6
+    packssdw             m0, m1
     pxor                 m1, m1
     pminsw               m0, m5
     pmaxsw               m0, m1
-    movq             [dstq], m0
-    add                dstq, dsq
-    dec                  hd
+    movq       [dstq+dsq*0], m0
+    movhps     [dstq+dsq*1], m0
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
     jg .h_w4_loop
     RET
-.h_w8:
-    WIN64_SPILL_XMM      12
+.h:
+    RESET_STACK_STATE
+    test                myd, 0xf00
+    jnz .hv
+    mov                 myd, r8m
+    movd                 m5, r8m
+    shr                 myd, 11
+    movddup              m4, [base+put_8tap_h_rnd+myq*8]
+    movifnidn           dsq, dsmp
+    pshufb               m5, [base+pw_256]
+    sub                  wd, 4
+    jle .h_w4
+    WIN64_SPILL_XMM      11
     shr                 mxd, 16
-    movq                 m3, [base+subpel_filters+mxq*8]
+    movq                 m2, [base+subpel_filters+1+mxq*8]
     movifnidn          dstq, dstmp
     mova                 m6, [base+spel_h_shufA]
     mova                 m7, [base+spel_h_shufB]
-%if UNIX64
-    mov                  wd, wd
-%endif
     lea                srcq, [srcq+wq*2]
-    punpcklbw            m3, m3
+    punpcklbw            m2, m2
     lea                dstq, [dstq+wq*2]
-    psraw                m3, 8
+    psraw                m2, 8
     neg                  wq
 %if ARCH_X86_32
-    ALLOC_STACK       -16*4
-    pshufd               m0, m3, q0000
-    pshufd               m1, m3, q1111
-    pshufd               m2, m3, q2222
-    pshufd               m3, m3, q3333
+    ALLOC_STACK       -16*3
+    %define              m8  [rsp+16*0]
+    %define              m9  [rsp+16*1]
+    %define             m10  [rsp+16*2]
+    pshufd               m0, m2, q0000
+    pshufd               m1, m2, q1111
+    pshufd               m2, m2, q2222
     mova                 m8, m0
     mova                 m9, m1
     mova                m10, m2
-    mova                m11, m3
 %else
-    pshufd               m8, m3, q0000
-    pshufd               m9, m3, q1111
-    pshufd              m10, m3, q2222
-    pshufd              m11, m3, q3333
+    pshufd               m8, m2, q0000
+    pshufd               m9, m2, q1111
+    pshufd              m10, m2, q2222
 %endif
 .h_w8_loop0:
     mov                  r6, wq
 .h_w8_loop:
-    movu                 m0, [srcq+r6*2- 6]
-    movu                 m1, [srcq+r6*2+ 2]
-    pshufb               m2, m0, m6   ; 0 1 1 2 2 3 3 4
-    pshufb               m0, m7       ; 2 3 3 4 4 5 5 6
-    pmaddwd              m2, m8       ; abcd0
-    pmaddwd              m0, m9       ; abcd1
-    pshufb               m3, m1, m6   ; 4 5 5 6 6 7 7 8
-    pshufb               m1, m7       ; 6 7 7 8 8 9 9 a
-    paddd                m2, m4
-    paddd                m0, m2
+    movu                 m3, [srcq+r6*2-4]
+    movu                 m2, [srcq+r6*2+8]
+    pshufb               m0, m3, m6   ; 01 12 23 34
+    pmaddwd              m0, m8       ; abcd0
+    pshufb               m3, m7       ; 23 34 45 56
+    pmaddwd              m1, m9, m3   ; abcd1
+    paddd                m0, m1
+    pshufb               m1, m2, m6   ; 67 78 89 9a
+    shufpd               m3, m1, 0x01 ; 45 56 67 78
+    pmaddwd              m1, m9       ; efgh1
+    pshufb               m2, m7       ; 89 9a ab bc
+    pmaddwd              m2, m10      ; efgh2
+    paddd                m1, m2
     pmaddwd              m2, m10, m3  ; abcd2
     pmaddwd              m3, m8       ; efgh0
+    paddd                m0, m4
+    paddd                m1, m4
     paddd                m0, m2
-    pmaddwd              m2, m11, m1  ; abcd3
-    pmaddwd              m1, m9       ; efgh1
-    paddd                m0, m2
-    movu                 m2, [srcq+r6*2+10]
-    paddd                m3, m4
-    paddd                m1, m3
-    pshufb               m3, m2, m6   ; 8 9 9 a a b b c
-    pshufb               m2, m7       ; a b b c c d d e
-    pmaddwd              m3, m10      ; efgh2
-    pmaddwd              m2, m11      ; efgh3
     paddd                m1, m3
-    paddd                m1, m2
     psrad                m0, 6
     psrad                m1, 6
     packssdw             m0, m1
@@ -1379,78 +1371,71 @@ cglobal put_8tap_16bpc, 4, 9, 0, dst, ds, src, ss, w, h, mx, my
     shr                 myd, 16
     cmp                  hd, 6
     cmovb               myd, mxd
-    movq                 m3, [base+subpel_filters+myq*8]
-    WIN64_SPILL_XMM      15
-    movd                 m7, r8m
+    movq                 m2, [base+subpel_filters+1+myq*8]
+    WIN64_SPILL_XMM      11, 16
+    movd                 m5, r8m
     movifnidn          dstq, dstmp
     movifnidn           dsq, dsmp
-    punpcklbw            m3, m3
-    pshufb               m7, [base+pw_256]
-    psraw                m3, 8 ; sign-extend
+    punpcklbw            m2, m2
+    pshufb               m5, [base+pw_256]
+    psraw                m2, 8 ; sign-extend
 %if ARCH_X86_32
-    ALLOC_STACK       -16*7
-    pshufd               m0, m3, q0000
-    pshufd               m1, m3, q1111
-    pshufd               m2, m3, q2222
-    pshufd               m3, m3, q3333
+    ALLOC_STACK       -16*4
+    pshufd               m0, m2, q0000
+    mov                  r6, ssq
+    pshufd               m1, m2, q1111
+    neg                  r6
+    pshufd               m2, m2, q2222
     mova                 m8, m0
     mova                 m9, m1
     mova                m10, m2
-    mova                m11, m3
-%else
-    pshufd               m8, m3, q0000
-    pshufd               m9, m3, q1111
-    pshufd              m10, m3, q2222
-    pshufd              m11, m3, q3333
-%endif
-    lea                  r6, [ssq*3]
-    sub                srcq, r6
     cmp                  wd, 2
     jne .v_w4
+%else
+    mov                  r6, ssq
+    pshufd               m8, m2, q0000
+    neg                  r6
+    cmp                  wd, 4
+    jg .v_w8
+    pshufd               m9, m2, q1111
+    pshufd              m10, m2, q2222
+    je .v_w4
+%endif
 .v_w2:
-    movd                 m1, [srcq+ssq*0]
+    movd                 m1, [srcq+r6 *2]
+    movd                 m3, [srcq+r6 *1]
+    movd                 m2, [srcq+ssq*0]
     movd                 m4, [srcq+ssq*1]
-    movd                 m2, [srcq+ssq*2]
-    add                srcq, r6
-    movd                 m5, [srcq+ssq*0]
-    movd                 m3, [srcq+ssq*1]
-    movd                 m6, [srcq+ssq*2]
-    add                srcq, r6
+    lea                srcq, [srcq+ssq*2]
     movd                 m0, [srcq+ssq*0]
-    punpckldq            m1, m4      ; 0 1
-    punpckldq            m4, m2      ; 1 2
-    punpckldq            m2, m5      ; 2 3
-    punpckldq            m5, m3      ; 3 4
-    punpckldq            m3, m6      ; 4 5
-    punpckldq            m6, m0      ; 5 6
-    punpcklwd            m1, m4      ; 01 12
-    punpcklwd            m2, m5      ; 23 34
-    punpcklwd            m3, m6      ; 45 56
+    punpckldq            m1, m3      ; 0 1
+    punpckldq            m3, m2      ; 1 2
+    punpckldq            m2, m4      ; 2 3
+    punpckldq            m4, m0      ; 3 4
+    punpcklwd            m1, m3      ; 01 12
+    punpcklwd            m2, m4      ; 23 34
     pxor                 m6, m6
 .v_w2_loop:
-    movd                 m4, [srcq+ssq*1]
+    movd                 m3, [srcq+ssq*1]
     lea                srcq, [srcq+ssq*2]
-    pmaddwd              m5, m8, m1  ; a0 b0
+    pmaddwd              m4, m8, m1  ; a0 b0
     mova                 m1, m2
     pmaddwd              m2, m9      ; a1 b1
-    paddd                m5, m2
-    mova                 m2, m3
-    pmaddwd              m3, m10     ; a2 b2
-    paddd                m5, m3
-    punpckldq            m3, m0, m4  ; 6 7
+    paddd                m4, m2
+    punpckldq            m2, m0, m3  ; 4 5
     movd                 m0, [srcq+ssq*0]
-    punpckldq            m4, m0      ; 7 8
-    punpcklwd            m3, m4      ; 67 78
-    pmaddwd              m4, m11, m3 ; a3 b3
-    paddd                m5, m4
-    psrad                m5, 5
-    packssdw             m5, m5
-    pmaxsw               m5, m6
-    pavgw                m5, m6
-    pminsw               m5, m7
-    movd       [dstq+dsq*0], m5
-    pshuflw              m5, m5, q3232
-    movd       [dstq+dsq*1], m5
+    punpckldq            m3, m0      ; 5 6
+    punpcklwd            m2, m3      ; 67 78
+    pmaddwd              m3, m10, m2 ; a2 b2
+    paddd                m4, m3
+    psrad                m4, 5
+    packssdw             m4, m4
+    pmaxsw               m4, m6
+    pavgw                m4, m6
+    pminsw               m4, m5
+    movd       [dstq+dsq*0], m4
+    pshuflw              m4, m4, q3232
+    movd       [dstq+dsq*1], m4
     lea                dstq, [dstq+dsq*2]
     sub                  hd, 2
     jg .v_w2_loop
@@ -1458,563 +1443,1991 @@ cglobal put_8tap_16bpc, 4, 9, 0, dst, ds, src, ss, w, h, mx, my
 .v_w4:
 %if ARCH_X86_32
     shl                  wd, 14
-%if STACK_ALIGNMENT < 16
-    mov          [esp+4*29], srcq
-    mov          [esp+4*30], dstq
-%else
-    mov               srcmp, srcq
-%endif
+    lea                srcq, [srcq+r6*2]
     lea                  wd, [wq+hq-(1<<16)]
-%else
-    shl                  wd, 6
-    mov                  r7, srcq
-    mov                  r8, dstq
-    lea                  wd, [wq+hq-(1<<8)]
+%if STACK_ALIGNMENT < 16
+    %define           dstmp  [esp+16*3]
 %endif
 .v_w4_loop0:
+    mov               dstmp, dstq
     movq                 m1, [srcq+ssq*0]
     movq                 m2, [srcq+ssq*1]
-    movq                 m3, [srcq+ssq*2]
-    add                srcq, r6
-    movq                 m4, [srcq+ssq*0]
-    movq                 m5, [srcq+ssq*1]
-    movq                 m6, [srcq+ssq*2]
-    add                srcq, r6
-    movq                 m0, [srcq+ssq*0]
+    lea                  r6, [srcq+ssq*2]
+    movq                 m3, [r6  +ssq*0]
+    movq                 m4, [r6  +ssq*1]
+    lea                  r6, [r6  +ssq*2]
+%else
+    movq                 m1, [srcq+r6 *2]
+    movq                 m2, [srcq+r6 *1]
+    lea                  r6, [srcq+ssq*2]
+    movq                 m3, [srcq+ssq*0]
+    movq                 m4, [srcq+ssq*1]
+%endif
+    movq                 m0, [r6  +ssq*0]
     punpcklwd            m1, m2      ; 01
     punpcklwd            m2, m3      ; 12
     punpcklwd            m3, m4      ; 23
-    punpcklwd            m4, m5      ; 34
-    punpcklwd            m5, m6      ; 45
-    punpcklwd            m6, m0      ; 56
-%if ARCH_X86_32
-    jmp .v_w4_loop_start
+    punpcklwd            m4, m0      ; 34
 .v_w4_loop:
-    mova                 m1, m12
-    mova                 m2, m13
-    mova                 m3, m14
-.v_w4_loop_start:
-    pmaddwd              m1, m8      ; a0
-    pmaddwd              m2, m8      ; b0
-    mova                m12, m3
-    mova                m13, m4
+    pmaddwd              m6, m8, m1  ; a0
+    pmaddwd              m7, m8, m2  ; b0
+    mova                 m1, m3
     pmaddwd              m3, m9      ; a1
+    mova                 m2, m4
     pmaddwd              m4, m9      ; b1
-    paddd                m1, m3
-    paddd                m2, m4
-    mova                m14, m5
-    mova                 m4, m6
-    pmaddwd              m5, m10     ; a2
-    pmaddwd              m6, m10     ; b2
-    paddd                m1, m5
-    paddd                m2, m6
-    movq                 m6, [srcq+ssq*1]
-    lea                srcq, [srcq+ssq*2]
-    punpcklwd            m5, m0, m6  ; 67
-    movq                 m0, [srcq+ssq*0]
-    pmaddwd              m3, m11, m5 ; a3
-    punpcklwd            m6, m0      ; 78
-    paddd                m1, m3
-    pmaddwd              m3, m11, m6 ; b3
-    paddd                m2, m3
-    psrad                m1, 5
-    psrad                m2, 5
-    packssdw             m1, m2
-    pxor                 m2, m2
-    pmaxsw               m1, m2
-    pavgw                m1, m2
-    pminsw               m1, m7
-    movq       [dstq+dsq*0], m1
-    movhps     [dstq+dsq*1], m1
+    paddd                m6, m3
+    movq                 m3, [r6+ssq*0]
+    paddd                m7, m4
+    movq                 m4, [r6+ssq*1]
+    lea                  r6, [r6+ssq*2]
+    movq                 m0, [r6+ssq*0]
+    punpcklwd            m3, m4      ; 45
+    punpcklwd            m4, m0      ; 56
+    pmaddwd              m0, m10, m3 ; a2
+    paddd                m6, m0
+    pmaddwd              m0, m10, m4 ; b2
+    paddd                m7, m0
+    psrad                m6, 5
+    psrad                m7, 5
+    packssdw             m6, m7
+    pxor                 m7, m7
+    pmaxsw               m6, m7
+    pavgw                m6, m7
+    pminsw               m6, m5
+    movq       [dstq+dsq*0], m6
+    movhps     [dstq+dsq*1], m6
     lea                dstq, [dstq+dsq*2]
     sub                  hd, 2
     jg .v_w4_loop
-%if STACK_ALIGNMENT < 16
-    mov                srcq, [esp+4*29]
-    mov                dstq, [esp+4*30]
-    movzx                hd, ww
-    add                srcq, 8
-    add                dstq, 8
-    mov          [esp+4*29], srcq
-    mov          [esp+4*30], dstq
-%else
-    mov                srcq, srcmp
+%if ARCH_X86_32
     mov                dstq, dstmp
-    movzx                hd, ww
     add                srcq, 8
+    movzx                hd, ww
     add                dstq, 8
-    mov               srcmp, srcq
-    mov               dstmp, dstq
-%endif
     sub                  wd, 1<<16
+    jg .v_w4_loop0
+    RET
 %else
-.v_w4_loop:
-    pmaddwd             m12, m8, m1  ; a0
-    pmaddwd             m13, m8, m2  ; b0
-    mova                 m1, m3
-    mova                 m2, m4
-    pmaddwd              m3, m9      ; a1
-    pmaddwd              m4, m9      ; b1
-    paddd               m12, m3
-    paddd               m13, m4
-    mova                 m3, m5
-    mova                 m4, m6
-    pmaddwd              m5, m10     ; a2
-    pmaddwd              m6, m10     ; b2
-    paddd               m12, m5
-    paddd               m13, m6
-    movq                 m6, [srcq+ssq*1]
-    lea                srcq, [srcq+ssq*2]
-    punpcklwd            m5, m0, m6  ; 67
-    movq                 m0, [srcq+ssq*0]
-    pmaddwd             m14, m11, m5 ; a3
-    punpcklwd            m6, m0      ; 78
-    paddd               m12, m14
-    pmaddwd             m14, m11, m6 ; b3
-    paddd               m13, m14
-    psrad               m12, 5
-    psrad               m13, 5
-    packssdw            m12, m13
-    pxor                m13, m13
-    pmaxsw              m12, m13
-    pavgw               m12, m13
-    pminsw              m12, m7
-    movq       [dstq+dsq*0], m12
-    movhps     [dstq+dsq*1], m12
-    lea                dstq, [dstq+dsq*2]
+    RET
+.v_w8:
+    mova                r6m, m8
+    shl                  wd, 5
+    pshufd               m6, m2, q1111
+    lea                  wd, [wq+hq-(1<<8)]
+    pshufd               m7, m2, q2222
+    WIN64_PUSH_XMM       16
+.v_w8_loop0:
+    movu                 m9, [srcq+ r6*2]
+    movu                m11, [srcq+ r6*1]
+    lea                  r7, [srcq+ssq*2]
+    movu                m13, [srcq+ssq*0]
+    movu                m15, [srcq+ssq*1]
+    mov                  r8, dstq
+    movu                 m4, [r7  +ssq*0]
+    punpcklwd            m8, m9, m11  ; 01
+    punpckhwd            m9, m11
+    punpcklwd           m10, m11, m13 ; 12
+    punpckhwd           m11, m13
+    punpcklwd           m12, m13, m15 ; 23
+    punpckhwd           m13, m15
+    punpcklwd           m14, m15, m4  ; 34
+    punpckhwd           m15, m4
+.v_w8_loop:
+    mova                 m3, r6m
+    pmaddwd              m0, m8, m3   ; a0
+    pmaddwd              m2, m9, m3   ; a0'
+    pmaddwd              m1, m10, m3  ; b0
+    pmaddwd              m3, m11      ; b0'
+    mova                 m8, m12
+    pmaddwd             m12, m6       ; a1
+    mova                 m9, m13
+    pmaddwd             m13, m6       ; a1'
+    mova                m10, m14
+    pmaddwd             m14, m6       ; b1
+    mova                m11, m15
+    pmaddwd             m15, m6       ; b1'
+    paddd                m0, m12
+    paddd                m2, m13
+    movu                m13, [r7+ssq*0]
+    paddd                m1, m14
+    paddd                m3, m15
+    movu                m15, [r7+ssq*1]
+    lea                  r7, [r7+ssq*2]
+    movu                 m4, [r7+ssq*0]
+    punpcklwd           m12, m13, m15 ; 45
+    punpckhwd           m13, m15
+    punpcklwd           m14, m15, m4  ; 56
+    punpckhwd           m15, m4
+    pmaddwd              m4, m7, m12  ; a2
+    paddd                m0, m4
+    pmaddwd              m4, m7, m13  ; a2'
+    paddd                m2, m4
+    pmaddwd              m4, m7, m14  ; b2
+    paddd                m1, m4
+    pmaddwd              m4, m7, m15  ; b2'
+    paddd                m3, m4
+    REPX       {psrad x, 5}, m0, m2, m1, m3
+    packssdw             m0, m2
+    packssdw             m1, m3
+    pxor                 m2, m2
+    pmaxsw               m0, m2
+    pmaxsw               m1, m2
+    pavgw                m0, m2
+    pavgw                m1, m2
+    pminsw               m0, m5
+    pminsw               m1, m5
+    mova         [r8+dsq*0], m0
+    mova         [r8+dsq*1], m1
+    lea                  r8, [r8+dsq*2]
     sub                  hd, 2
-    jg .v_w4_loop
-    add                  r7, 8
-    add                  r8, 8
+    jg .v_w8_loop
+    add                srcq, 16
+    add                dstq, 16
     movzx                hd, wb
-    mov                srcq, r7
-    mov                dstq, r8
     sub                  wd, 1<<8
-%endif
-    jg .v_w4_loop0
+    jg .v_w8_loop0
     RET
+%endif
 .hv:
-    RESET_STACK_STATE
+    cmp                  wd, 4
+    jg .hv_w8
+    WIN64_SPILL_XMM      12, 16
 %if ARCH_X86_32
-    movd                 m4, r8m
-    mova                 m6, [base+pd_512]
-    pshufb               m4, [base+pw_256]
+    movd                 m3, r8m
+    pshufb               m3, [base+pw_256]
 %else
-%if WIN64
-    ALLOC_STACK        16*6, 16
-%endif
-    movd                m15, r8m
-    pshufb              m15, [base+pw_256]
+    movd                m11, r8m
+    pshufb              m11, [base+pw_256]
 %endif
-    cmp                  wd, 4
-    jg .hv_w8
     movzx               mxd, mxb
-    je .hv_w4
     movq                 m0, [base+subpel_filters+mxq*8]
     movzx               mxd, myb
     shr                 myd, 16
     cmp                  hd, 6
     cmovb               myd, mxd
-    movq                 m3, [base+subpel_filters+myq*8]
-%if ARCH_X86_32
-    mov                dstq, dstmp
-    mov                 dsq, dsmp
-    mova                 m5, [base+spel_h_shuf2]
-    ALLOC_STACK       -16*8
-%else
-    mova                 m6, [base+pd_512]
-    mova                 m9, [base+spel_h_shuf2]
-%endif
+    movq                 m2, [base+subpel_filters+1+myq*8]
+    movddup              m7, [base+pd_8704]
+    sub                srcq, 2
     pshuflw              m0, m0, q2121
-    pxor                 m7, m7
-    punpcklbw            m7, m0
-    punpcklbw            m3, m3
-    psraw                m3, 8 ; sign-extend
+    pxor                 m6, m6
+    punpcklbw            m6, m0
+    punpcklbw            m2, m2
+    psraw                m2, 8 ; sign-extend
     test          dword r8m, 0x800
     jz .hv_w2_10bpc
-    psraw                m7, 2
-    psllw                m3, 2
+    movddup              m7, [base+pd_2560]
+    psraw                m6, 2
+    psllw                m2, 2
 .hv_w2_10bpc:
-    lea                  r6, [ssq*3]
-    sub                srcq, 2
-    sub                srcq, r6
 %if ARCH_X86_32
-    pshufd               m0, m3, q0000
-    pshufd               m1, m3, q1111
-    pshufd               m2, m3, q2222
-    pshufd               m3, m3, q3333
-    mova                 m9, m5
-    mova                m11, m0
-    mova                m12, m1
-    mova                m13, m2
-    mova                m14, m3
-    mova                m15, m4
+%assign regs_used 2
+    ALLOC_STACK       -16*7
+%assign regs_used 7
+    mov                dstq, r0mp
+    mov                 dsq, r1mp
+    %define             m11  [esp+16*4]
+    pshufd               m0, m2, q0000
+    pshufd               m1, m2, q1111
+    pshufd               m2, m2, q2222
+    mova                 m8, m0
+    mova                 m9, m1
+    mova                m10, m2
+    mova                m11, m3
+    neg                 ssq
+    movu                 m3, [srcq+ssq*2]
+    movu                 m4, [srcq+ssq*1]
+    neg                 ssq
 %else
-    pshufd              m11, m3, q0000
-    pshufd              m12, m3, q1111
-    pshufd              m13, m3, q2222
-    pshufd              m14, m3, q3333
+    pshufd               m8, m2, q0000
+    mov                  r6, ssq
+    pshufd               m9, m2, q1111
+    neg                  r6
+    pshufd              m10, m2, q2222
+    movu                 m3, [srcq+r6 *2]
+    movu                 m4, [srcq+r6 *1]
 %endif
+    movu                 m1, [srcq+ssq*0]
+    movu                 m0, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
     movu                 m2, [srcq+ssq*0]
-    movu                 m3, [srcq+ssq*1]
-    movu                 m1, [srcq+ssq*2]
-    add                srcq, r6
-    movu                 m4, [srcq+ssq*0]
-%if ARCH_X86_32
-    REPX    {pshufb  x, m5}, m2, m3, m1, m4
-%else
-    REPX    {pshufb  x, m9}, m2, m3, m1, m4
-%endif
-    REPX    {pmaddwd x, m7}, m2, m3, m1, m4
-    phaddd               m2, m3        ; 0 1
-    phaddd               m1, m4        ; 2 3
-    movu                 m3, [srcq+ssq*1]
-    movu                 m4, [srcq+ssq*2]
-    add                srcq, r6
-    movu                 m0, [srcq+ssq*0]
-%if ARCH_X86_32
-    REPX    {pshufb  x, m5}, m3, m4, m0
-%else
-    REPX    {pshufb  x, m9}, m3, m4, m0
-%endif
-    REPX    {pmaddwd x, m7}, m3, m4, m0
-    phaddd               m3, m4        ; 4 5
-    phaddd               m0, m0        ; 6 6
-    REPX    {paddd   x, m6}, m2, m1, m3, m0
-    REPX    {psrad   x, 10}, m2, m1, m3, m0
-    packssdw             m2, m1        ; 0 1 2 3
-    packssdw             m3, m0        ; 4 5 6 _
-    palignr              m4, m3, m2, 4 ; 1 2 3 4
-    pshufd               m5, m3, q0321 ; 5 6 _ _
+    cmp                  wd, 4
+    je .hv_w4
+    mova                 m5, [base+spel_h_shuf2]
+    REPX    {pshufb  x, m5}, m3, m4, m0, m1, m2
+    REPX    {pmaddwd x, m6}, m3, m0, m4, m1, m2
+    phaddd               m3, m0        ; 0 3
+    phaddd               m4, m1        ; 1 2
+    phaddd               m0, m2        ; 3 4
+    REPX    {paddd   x, m7}, m3, m4, m0
+    REPX    {psrad   x, 10}, m3, m4, m0
+    packssdw             m3, m4        ; 0 3 1 2
+    packssdw             m4, m0        ; 1 2 3 4
+    pshufd               m2, m3, q1320 ; 0 1 2 3
     punpcklwd            m1, m2, m4    ; 01 12
     punpckhwd            m2, m4        ; 23 34
-    punpcklwd            m3, m5        ; 45 56
 .hv_w2_loop:
-    movu                 m4, [srcq+ssq*1]
+    movu                 m3, [srcq+ssq*1]
     lea                srcq, [srcq+ssq*2]
-    movu                 m5, [srcq+ssq*0]
-    pshufb               m4, m9
-    pshufb               m5, m9
-    pmaddwd              m4, m7
-    pmaddwd              m5, m7
-    phaddd               m4, m5
-    pmaddwd              m5, m11, m1   ; a0 b0
+    movu                 m4, [srcq+ssq*0]
+    pshufb               m3, m5
+    pshufb               m4, m5
+    pmaddwd              m3, m6
+    pmaddwd              m4, m6
+    phaddd               m3, m4
+    pmaddwd              m4, m8, m1    ; a0 b0
     mova                 m1, m2
-    pmaddwd              m2, m12       ; a1 b1
-    paddd                m5, m2
-    mova                 m2, m3
-    pmaddwd              m3, m13       ; a2 b2
+    pmaddwd              m2, m9        ; a1 b1
+    paddd                m4, m2
+    paddd                m3, m7
+    psrad                m3, 10        ; 5 6
+    packssdw             m0, m3
+    pshufd               m2, m0, q2103
+    punpckhwd            m2, m0        ; 45 56
+    mova                 m0, m3
+    pmaddwd              m3, m10, m2   ; a2 b2
+    paddd                m4, m3
+    psrad                m4, 10
+    packssdw             m4, m4
+    pxor                 m3, m3
+    pminsw               m4, m11
+    pmaxsw               m4, m3
+    movd       [dstq+dsq*0], m4
+    pshuflw              m4, m4, q1032
+    movd       [dstq+dsq*1], m4
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
+    jg .hv_w2_loop
+    RET
+.hv_w4:
+%if ARCH_X86_32
+    %define             m12  [esp+16*5]
+    %define             m13  [esp+16*6]
+    %define             m14  [base+spel_h_shufA]
+    %define             m15  [base+spel_h_shufB]
+    pshufd               m5, m6, q0000
+    pshufd               m6, m6, q1111
+    mova                m12, m5
+    mova                m13, m6
+%else
+    WIN64_PUSH_XMM       16
+    mova                m14, [base+spel_h_shufA]
+    mova                m15, [base+spel_h_shufB]
+    pshufd              m12, m6, q0000
+    pshufd              m13, m6, q1111
+%endif
+%macro HV_H_W4_6TAP 3-4 m15 ; dst, src, tmp, shufB
+    pshufb               %3, %2, m14
+    pmaddwd              %3, m12
+    pshufb               %2, %4
+    pmaddwd              %2, m13
+    paddd                %3, m7
+    paddd                %1, %2, %3
+%endmacro
+    HV_H_W4_6TAP         m3, m3, m5
+    HV_H_W4_6TAP         m4, m4, m5
+    HV_H_W4_6TAP         m5, m1, m5
+    HV_H_W4_6TAP         m0, m0, m1
+    HV_H_W4_6TAP         m2, m2, m1
+    REPX      {psrad x, 10}, m3, m5, m4, m0, m2
+    packssdw             m3, m5      ; 0 2
+    packssdw             m4, m0      ; 1 3
+    packssdw             m5, m2      ; 2 4
+    punpcklwd            m1, m3, m4  ; 01
+    punpckhwd            m3, m4      ; 23
+    punpcklwd            m2, m4, m5  ; 12
+    punpckhwd            m4, m5      ; 34
+.hv_w4_loop:
+    movu                 m0, [srcq+ssq*1]
+    pmaddwd              m5, m8, m1  ; a0
+    lea                srcq, [srcq+ssq*2]
+    pmaddwd              m6, m8, m2  ; b0
+    mova                 m1, m3
+    pmaddwd              m3, m9      ; a1
+    mova                 m2, m4
+    pmaddwd              m4, m9      ; b1
     paddd                m5, m3
-    paddd                m4, m6
-    psrad                m4, 10        ; 7 8
-    packssdw             m0, m4
-    pshufd               m3, m0, q2103
-    punpckhwd            m3, m0        ; 67 78
-    mova                 m0, m4
-    pmaddwd              m4, m14, m3   ; a3 b3
-    paddd                m5, m6
-    paddd                m5, m4
+    movu                 m3, [srcq+ssq*0]
+    paddd                m6, m4
+    HV_H_W4_6TAP         m0, m0, m4
+    HV_H_W4_6TAP         m3, m3, m4
+    psrad                m4, m2, 16
+    psrad                m0, 10
+    psrad                m3, 10
+    packssdw             m4, m0      ; 4 5
+    packssdw             m0, m3      ; 5 6
+    punpcklwd            m3, m4, m0  ; 45
+    punpckhwd            m4, m0      ; 56
+    pmaddwd              m0, m10, m3 ; a2
+    paddd                m5, m0
+    pmaddwd              m0, m10, m4 ; b2
+    paddd                m6, m0
     psrad                m5, 10
-    packssdw             m5, m5
-    pxor                 m4, m4
-    pminsw               m5, m15
-    pmaxsw               m5, m4
-    movd       [dstq+dsq*0], m5
-    pshuflw              m5, m5, q3232
-    movd       [dstq+dsq*1], m5
+    psrad                m6, 10
+    packssdw             m5, m6
+    pxor                 m6, m6
+    pminsw               m5, m11
+    pmaxsw               m5, m6
+    movq       [dstq+dsq*0], m5
+    movhps     [dstq+dsq*1], m5
     lea                dstq, [dstq+dsq*2]
     sub                  hd, 2
-    jg .hv_w2_loop
+    jg .hv_w4_loop
     RET
 .hv_w8:
+    RESET_STACK_STATE
     shr                 mxd, 16
-.hv_w4:
-    movq                 m2, [base+subpel_filters+mxq*8]
+    movq                 m2, [base+subpel_filters+1+mxq*8]
     movzx               mxd, myb
     shr                 myd, 16
     cmp                  hd, 6
     cmovb               myd, mxd
-    movq                 m3, [base+subpel_filters+myq*8]
-%if ARCH_X86_32
-    RESET_STACK_STATE
-    mov                dstq, dstmp
-    mov                 dsq, dsmp
-    mova                 m0, [base+spel_h_shufA]
-    mova                 m1, [base+spel_h_shufB]
-    ALLOC_STACK      -16*15
-    mova                 m8, m0
-    mova                 m9, m1
-    mova                m14, m6
-%else
-    mova                 m8, [base+spel_h_shufA]
-    mova                 m9, [base+spel_h_shufB]
-%endif
+    movq                 m1, [base+subpel_filters+1+myq*8]
+    movd                 m3, r8m
+    movddup              m4, [base+pd_8704]
+    pshufb               m3, [base+pw_256]
     pxor                 m0, m0
     punpcklbw            m0, m2
-    punpcklbw            m3, m3
-    psraw                m3, 8
+    punpcklbw            m1, m1
+    sub                srcq, 4
+    psraw                m1, 8 ; sign-extend
     test          dword r8m, 0x800
-    jz .hv_w4_10bpc
+    jz .hv_w8_10bpc
+    movddup              m4, [base+pd_2560]
     psraw                m0, 2
-    psllw                m3, 2
-.hv_w4_10bpc:
-    lea                  r6, [ssq*3]
-    sub                srcq, 6
-    sub                srcq, r6
+    psllw                m1, 2
+.hv_w8_10bpc:
+%if ARCH_X86_32
+%assign regs_used 2
+    ALLOC_STACK       -16*9
+%assign regs_used 7
+    mov                dstq, r0mp
+    mov                 dsq, r1mp
+    mova         [rsp+16*7], m4
+%else
+    ALLOC_STACK        16*7, 16
+%endif
+    mova         [rsp+16*6], m3
+    pshufd               m2, m0, q0000
+    mova         [rsp+16*0], m2
+    pshufd               m2, m0, q1111
+    mova         [rsp+16*1], m2
+    pshufd               m0, m0, q2222
+    mova         [rsp+16*2], m0
+    pshufd               m2, m1, q0000
+    mova         [rsp+16*3], m2
+    pshufd               m2, m1, q1111
+    mova         [rsp+16*4], m2
+    pshufd               m1, m1, q2222
+    mova         [rsp+16*5], m1
+    mov                  r6, ssq
+    neg                  r6
 %if ARCH_X86_32
-    %define tmp esp+16*8
     shl                  wd, 14
+    lea                 r4d, [wq+hq-(1<<16)]
 %if STACK_ALIGNMENT < 16
-    mov          [esp+4*61], srcq
-    mov          [esp+4*62], dstq
-%else
+    %define           srcmp  [esp+16*8+4*0]
+    %define           dstmp  [esp+16*8+4*1]
+%endif
+%macro HV_H_6TAP 3-6 [rsp+16*0], [rsp+16*1], [rsp+16*2] ; dst, src[1-2], mul[1-3]
+    punpcklwd            %1, %2, %3   ; 01 12 23 34
+    punpckhwd            %2, %3       ; 45 56 67 78
+    pmaddwd              %3, %4, %1   ; a0
+    shufpd               %1, %2, 0x01 ; 23 34 45 56
+    pmaddwd              %2, %6       ; a2
+    pmaddwd              %1, %5       ; a1
+    paddd                %2, %3
+    paddd                %1, %2
+%endmacro
+.hv_w8_loop0:
     mov               srcmp, srcq
-%endif
-    mova         [tmp+16*5], m4
-    lea                  wd, [wq+hq-(1<<16)]
-    pshufd               m1, m0, q0000
-    pshufd               m2, m0, q1111
-    pshufd               m5, m0, q2222
-    pshufd               m0, m0, q3333
-    mova                m10, m1
-    mova                m11, m2
-    mova                m12, m5
-    mova                m13, m0
-%else
-%if WIN64
-    %define tmp rsp
-%else
-    %define tmp rsp-104 ; red zone
-%endif
-    shl                  wd, 6
-    mov                  r7, srcq
-    mov                  r8, dstq
-    lea                  wd, [wq+hq-(1<<8)]
-    pshufd              m10, m0, q0000
-    pshufd              m11, m0, q1111
-    pshufd              m12, m0, q2222
-    pshufd              m13, m0, q3333
-    mova         [tmp+16*5], m15
-%endif
-    pshufd               m0, m3, q0000
-    pshufd               m1, m3, q1111
-    pshufd               m2, m3, q2222
-    pshufd               m3, m3, q3333
-    mova         [tmp+16*1], m0
-    mova         [tmp+16*2], m1
-    mova         [tmp+16*3], m2
-    mova         [tmp+16*4], m3
-%macro PUT_8TAP_HV_H 4-5 m14 ; dst/src+0, src+8, tmp, shift, [pd_512]
-    pshufb              m%3, m%1, m8 ; 0 1 1 2 2 3 3 4
-    pshufb              m%1, m9      ; 2 3 3 4 4 5 5 6
-    pmaddwd             m%3, m10
-    pmaddwd             m%1, m11
-    paddd               m%3, %5
-    paddd               m%1, m%3
-    pshufb              m%3, m%2, m8 ; 4 5 5 6 6 7 7 8
-    pshufb              m%2, m9      ; 6 7 7 8 8 9 9 a
-    pmaddwd             m%3, m12
-    pmaddwd             m%2, m13
-    paddd               m%1, m%3
-    paddd               m%1, m%2
-    psrad               m%1, %4
-%endmacro
-.hv_w4_loop0:
-%if ARCH_X86_64
-    mova                m14, [pd_512]
-%endif
-    movu                 m4, [srcq+ssq*0+0]
-    movu                 m1, [srcq+ssq*0+8]
+    mov               dstmp, dstq
+    movu                 m5, [srcq+r6*2+0]
+    movu                 m6, [srcq+r6*2+2]
+    mova                 m7, [rsp+16*0]
+    mova                 m1, [rsp+16*1]
+    mova                 m0, [rsp+16*2]
+    HV_H_6TAP            m2, m5, m6, m7, m1, m0
+    movu                 m5, [srcq+r6*1+0]
+    movu                 m6, [srcq+r6*1+2]
+    HV_H_6TAP            m3, m5, m6, m7, m1, m0
+    movu                 m5, [srcq+ssq*0+0]
+    movu                 m6, [srcq+ssq*0+2]
+    HV_H_6TAP            m4, m5, m6, m7, m1, m0
     movu                 m5, [srcq+ssq*1+0]
-    movu                 m2, [srcq+ssq*1+8]
-    movu                 m6, [srcq+ssq*2+0]
-    movu                 m3, [srcq+ssq*2+8]
-    add                srcq, r6
-    PUT_8TAP_HV_H         4, 1, 0, 10
-    PUT_8TAP_HV_H         5, 2, 0, 10
-    PUT_8TAP_HV_H         6, 3, 0, 10
-    movu                 m7, [srcq+ssq*0+0]
-    movu                 m2, [srcq+ssq*0+8]
-    movu                 m1, [srcq+ssq*1+0]
-    movu                 m3, [srcq+ssq*1+8]
-    PUT_8TAP_HV_H         7, 2, 0, 10
-    PUT_8TAP_HV_H         1, 3, 0, 10
-    movu                 m2, [srcq+ssq*2+0]
-    movu                 m3, [srcq+ssq*2+8]
-    add                srcq, r6
-    PUT_8TAP_HV_H         2, 3, 0, 10
-    packssdw             m4, m7      ; 0 3
-    packssdw             m5, m1      ; 1 4
-    movu                 m0, [srcq+ssq*0+0]
-    movu                 m1, [srcq+ssq*0+8]
-    PUT_8TAP_HV_H         0, 1, 3, 10
-    packssdw             m6, m2      ; 2 5
-    packssdw             m7, m0      ; 3 6
-    punpcklwd            m1, m4, m5  ; 01
-    punpckhwd            m4, m5      ; 34
-    punpcklwd            m2, m5, m6  ; 12
-    punpckhwd            m5, m6      ; 45
-    punpcklwd            m3, m6, m7  ; 23
-    punpckhwd            m6, m7      ; 56
-%if ARCH_X86_32
-    jmp .hv_w4_loop_start
-.hv_w4_loop:
-    mova                 m1, [tmp+16*6]
-    mova                 m2, m15
-.hv_w4_loop_start:
-    mova                 m7, [tmp+16*1]
-    pmaddwd              m1, m7      ; a0
-    pmaddwd              m2, m7      ; b0
-    mova                 m7, [tmp+16*2]
-    mova         [tmp+16*6], m3
-    pmaddwd              m3, m7      ; a1
-    mova                m15, m4
-    pmaddwd              m4, m7      ; b1
-    mova                 m7, [tmp+16*3]
-    paddd                m1, m3
-    paddd                m2, m4
-    mova                 m3, m5
-    pmaddwd              m5, m7      ; a2
-    mova                 m4, m6
-    pmaddwd              m6, m7      ; b2
-    paddd                m1, m5
-    paddd                m2, m6
-    movu                 m7, [srcq+ssq*1+0]
-    movu                 m5, [srcq+ssq*1+8]
+    movu                 m6, [srcq+ssq*1+2]
     lea                srcq, [srcq+ssq*2]
-    PUT_8TAP_HV_H         7, 5, 6, 10
-    packssdw             m0, m7      ; 6 7
-    mova         [tmp+16*0], m0
-    movu                 m0, [srcq+ssq*0+0]
-    movu                 m5, [srcq+ssq*0+8]
-    PUT_8TAP_HV_H         0, 5, 6, 10
-    mova                 m6, [tmp+16*0]
-    packssdw             m7, m0      ; 7 8
-    punpcklwd            m5, m6, m7  ; 67
-    punpckhwd            m6, m7      ; 78
-    pmaddwd              m7, m5, [tmp+16*4]
-    paddd                m1, m7      ; a3
-    pmaddwd              m7, m6, [tmp+16*4]
-    paddd                m2, m7      ; b3
-    psrad                m1, 9
-    psrad                m2, 9
-    packssdw             m1, m2
-    pxor                 m7, m7
-    pmaxsw               m1, m7
-    pavgw                m7, m1
-    pminsw               m7, [tmp+16*5]
-    movq       [dstq+dsq*0], m7
-    movhps     [dstq+dsq*1], m7
+    HV_H_6TAP            m0, m5, m6, m7, m1
+    movu                 m5, [srcq+ssq*0+0]
+    movu                 m6, [srcq+ssq*0+2]
+    HV_H_6TAP            m1, m5, m6, m7
+    mova                 m5, [rsp+16*7]
+    REPX      {paddd x, m5}, m2, m3, m4, m0, m1
+    REPX      {psrad x, 10}, m2, m4, m3, m0, m1
+    packssdw             m2, m4     ; 0 2
+    packssdw             m3, m0     ; 1 3
+    packssdw             m4, m1     ; 2 4
+    punpcklwd            m0, m2, m3 ; 01
+    punpckhwd            m2, m3     ; 23
+    punpcklwd            m1, m3, m4 ; 12
+    punpckhwd            m3, m4     ; 34
+.hv_w8_loop:
+    mova                 m5, [rsp+16*3]
+    mova                 m6, [rsp+16*4]
+    pmaddwd              m4, m0, m5 ; a0
+    pmaddwd              m5, m1     ; b0
+    mova                 m0, m2
+    pmaddwd              m2, m6     ; a1
+    mova                 m1, m3
+    pmaddwd              m3, m6     ; b1
+    paddd                m4, m2
+    movu                 m2, [srcq+ssq*1+0]
+    paddd                m5, m3
+    movu                 m3, [srcq+ssq*1+2]
+    lea                srcq, [srcq+ssq*2]
+    HV_H_6TAP            m6, m2, m3
+    movu                 m2, [srcq+ssq*0+0]
+    movu                 m3, [srcq+ssq*0+2]
+    HV_H_6TAP            m7, m2, m3
+    mova                 m2, [rsp+16*7]
+    psrad                m3, m1, 16
+    paddd                m6, m2
+    paddd                m7, m2
+    psrad                m6, 10
+    psrad                m7, 10
+    packssdw             m3, m6     ; 4 5
+    packssdw             m6, m7     ; 5 6
+    mova                 m7, [rsp+16*5]
+    punpcklwd            m2, m3, m6 ; 45
+    punpckhwd            m3, m6     ; 56
+    pmaddwd              m6, m2, m7 ; a2
+    pmaddwd              m7, m3     ; b2
+    paddd                m4, m6
+    paddd                m5, m7
+    psrad                m4, 10
+    psrad                m5, 10
+    packssdw             m4, m5
+    pxor                 m5, m5
+    pminsw               m4, [rsp+16*6]
+    pmaxsw               m4, m5
+    movq       [dstq+dsq*0], m4
+    movhps     [dstq+dsq*1], m4
     lea                dstq, [dstq+dsq*2]
     sub                  hd, 2
-    jg .hv_w4_loop
-%if STACK_ALIGNMENT < 16
-    mov                srcq, [esp+4*61]
-    mov                dstq, [esp+4*62]
-    add                srcq, 8
-    add                dstq, 8
-    mov          [esp+4*61], srcq
-    mov          [esp+4*62], dstq
-%else
+    jg .hv_w8_loop
     mov                srcq, srcmp
     mov                dstq, dstmp
+    movzx                hd, r4w
     add                srcq, 8
     add                dstq, 8
-    mov               srcmp, srcq
-    mov               dstmp, dstq
-%endif
-    movzx                hd, ww
-    sub                  wd, 1<<16
+    sub                 r4d, 1<<16
 %else
-.hv_w4_loop:
-    mova                m15, [tmp+16*1]
-    pmaddwd             m14, m15, m1 ; a0
-    pmaddwd             m15, m2      ; b0
-    mova                 m7, [tmp+16*2]
-    mova                 m1, m3
-    pmaddwd              m3, m7      ; a1
-    mova                 m2, m4
-    pmaddwd              m4, m7      ; b1
-    mova                 m7, [tmp+16*3]
-    paddd               m14, m3
-    paddd               m15, m4
-    mova                 m3, m5
-    pmaddwd              m5, m7      ; a2
-    mova                 m4, m6
-    pmaddwd              m6, m7      ; b2
-    paddd               m14, m5
-    paddd               m15, m6
-    movu                 m7, [srcq+ssq*1+0]
-    movu                 m5, [srcq+ssq*1+8]
-    lea                srcq, [srcq+ssq*2]
-    PUT_8TAP_HV_H         7, 5, 6, 10, [pd_512]
-    packssdw             m0, m7      ; 6 7
-    mova         [tmp+16*0], m0
-    movu                 m0, [srcq+ssq*0+0]
-    movu                 m5, [srcq+ssq*0+8]
-    PUT_8TAP_HV_H         0, 5, 6, 10, [pd_512]
-    mova                 m6, [tmp+16*0]
-    packssdw             m7, m0      ; 7 8
-    punpcklwd            m5, m6, m7  ; 67
-    punpckhwd            m6, m7      ; 78
-    pmaddwd              m7, m5, [tmp+16*4]
-    paddd               m14, m7      ; a3
-    pmaddwd              m7, m6, [tmp+16*4]
-    paddd               m15, m7      ; b3
-    psrad               m14, 9
-    psrad               m15, 9
-    packssdw            m14, m15
-    pxor                 m7, m7
-    pmaxsw              m14, m7
-    pavgw                m7, m14
-    pminsw               m7, [tmp+16*5]
-    movq       [dstq+dsq*0], m7
-    movhps     [dstq+dsq*1], m7
-    lea                dstq, [dstq+dsq*2]
+    shl                  wd, 5
+    lea                 r8d, [wq+hq-256]
+%macro HV_H_6TAP 5-9 [spel_h_shufA], [rsp+16*0], [rsp+16*1], [rsp+16*2] ; dst, src[1-3], shift, shuf, mul[1-3]
+%ifid %6
+    REPX     {pshufb x, %6}, %2, %3, %4
+%else
+    mova                 %1, %6
+    pshufb               %2, %1       ; 01 12 23 34
+    pshufb               %3, %1       ; 45 56 67 78
+    pshufb               %4, %1       ; 89 9a ab bc
+%endif
+    pmaddwd              %1, %7, %2
+    shufpd               %2, %3, 0x01 ; 23 34 45 56
+    pmaddwd              %2, %8
+    paddd                %1, %2
+    pmaddwd              %2, %9, %3
+    paddd                %1, %2
+    pmaddwd              %2, %7, %3
+    shufpd               %3, %4, 0x01 ; 67 78 89 9a
+    pmaddwd              %4, %9
+    pmaddwd              %3, %8
+    paddd                %1, m4
+    paddd                %2, m4
+    paddd                %3, %4
+    paddd                %2, %3
+    psrad                %1, %5
+    psrad                %2, %5
+    packssdw             %1, %2
+%endmacro
+.hv_w8_loop0:
+    mova                 m5, [spel_h_shufA]
+    movu                 m0, [srcq+r6*2+ 0]
+    mova                 m6, [rsp+16*0]
+    movu                 m1, [srcq+r6*2+ 8]
+    mova                 m7, [rsp+16*1]
+    movu                 m2, [srcq+r6*2+16]
+    mova                 m8, [rsp+16*2]
+    HV_H_6TAP            m9, m0, m1, m2, 10, m5, m6, m7, m8
+    movu                 m0, [srcq+r6*1+ 0]
+    movu                 m1, [srcq+r6*1+ 8]
+    movu                 m2, [srcq+r6*1+16]
+    lea                  r4, [srcq+ssq*2]
+    HV_H_6TAP           m11, m0, m1, m2, 10, m5, m6, m7, m8
+    movu                 m0, [srcq+ssq*0+ 0]
+    movu                 m1, [srcq+ssq*0+ 8]
+    movu                 m2, [srcq+ssq*0+16]
+    mov                  r7, dstq
+    HV_H_6TAP           m13, m0, m1, m2, 10, m5, m6, m7, m8
+    movu                 m0, [srcq+ssq*1+ 0]
+    movu                 m1, [srcq+ssq*1+ 8]
+    movu                 m2, [srcq+ssq*1+16]
+    HV_H_6TAP           m15, m0, m1, m2, 10, m5, m6, m7, m8
+    movu                 m0, [r4+ssq*0+ 0]
+    movu                 m1, [r4+ssq*0+ 8]
+    movu                 m2, [r4+ssq*0+16]
+    HV_H_6TAP            m5, m0, m1, m2, 10, m5, m6, m7, m8
+    punpcklwd            m8, m9, m11  ; 01
+    punpckhwd            m9, m11
+    punpcklwd           m10, m11, m13 ; 12
+    punpckhwd           m11, m13
+    punpcklwd           m12, m13, m15 ; 23
+    punpckhwd           m13, m15
+    punpcklwd           m14, m15, m5  ; 34
+    punpckhwd           m15, m5
+.hv_w8_loop:
+    mova                 m3, [rsp+16*3]
+    mova                 m7, [rsp+16*4]
+    pmaddwd              m0, m8, m3   ; a0
+    mova                 m8, m12
+    pmaddwd              m2, m9, m3   ; a0'
+    mova                 m9, m13
+    pmaddwd              m1, m10, m3  ; b0
+    mova                m10, m14
+    pmaddwd              m3, m11      ; b0'
+    mova                m11, m15
+    REPX    {pmaddwd x, m7}, m12, m13, m14, m15
+    movu                 m6, [r4+ssq*1+ 0]
+    paddd                m0, m12
+    movu                 m7, [r4+ssq*1+ 8]
+    paddd                m2, m13
+    movu                m12, [r4+ssq*1+16]
+    paddd                m1, m14
+    lea                  r4, [r4+ssq*2]
+    paddd                m3, m15
+    HV_H_6TAP           m15, m6, m7, m12, 10
+    movu                 m6, [r4+ssq*0+ 0]
+    movu                 m7, [r4+ssq*0+ 8]
+    movu                m14, [r4+ssq*0+16]
+    punpcklwd           m12, m5, m15 ; 45
+    punpckhwd           m13, m5, m15
+    HV_H_6TAP            m5, m6, m7, m14, 10
+    mova                 m7, [rsp+16*5]
+    punpcklwd           m14, m15, m5  ; 56
+    punpckhwd           m15, m5
+    pmaddwd              m6, m12, m7  ; a2
+    paddd                m0, m6
+    pmaddwd              m6, m13, m7  ; a2'
+    paddd                m2, m6
+    pmaddwd              m6, m14, m7  ; b2
+    pmaddwd              m7, m15      ; b2'
+    paddd                m1, m6
+    mova                 m6, [rsp+16*6]
+    paddd                m3, m7
+    REPX      {psrad x, 10}, m0, m2, m1, m3
+    packssdw             m0, m2
+    packssdw             m1, m3
+    pxor                 m2, m2
+    pminsw               m0, m6
+    pminsw               m1, m6
+    pmaxsw               m0, m2
+    pmaxsw               m1, m2
+    mova         [r7+dsq*0], m0
+    mova         [r7+dsq*1], m1
+    lea                  r7, [r7+dsq*2]
     sub                  hd, 2
-    jg .hv_w4_loop
-    add                  r7, 8
-    add                  r8, 8
-    movzx                hd, wb
-    mov                srcq, r7
-    mov                dstq, r8
-    sub                  wd, 1<<8
+    jg .hv_w8_loop
+    add                srcq, 16
+    add                dstq, 16
+    movzx                hd, r8b
+    sub                 r8d, 1<<8
 %endif
-    jg .hv_w4_loop0
+    jg .hv_w8_loop0
     RET
-%undef tmp
 
+PUT_8TAP_FN smooth_sharp,   SMOOTH,  SHARP,   put_8tap_16bpc
+PUT_8TAP_FN sharp_smooth,   SHARP,   SMOOTH,  put_8tap_16bpc
+PUT_8TAP_FN regular_sharp,  REGULAR, SHARP,   put_8tap_16bpc
+PUT_8TAP_FN sharp_regular,  SHARP,   REGULAR, put_8tap_16bpc
+PUT_8TAP_FN sharp,          SHARP,   SHARP
+
+cglobal put_8tap_16bpc, 0, 9, 0, dst, ds, src, ss, w, h, mx, my
 %if ARCH_X86_32
-DECLARE_REG_TMP 2, 1, 6, 4
-%elif WIN64
-DECLARE_REG_TMP 6, 4, 7, 4
-%else
-DECLARE_REG_TMP 6, 7, 7, 8
+    %define             mxb  r0b
+    %define             mxd  r0
+    %define             mxq  r0
+    %define             myb  r1b
+    %define             myd  r1
+    %define             myq  r1
+    %define              m8  [esp+16*0]
+    %define              m9  [esp+16*1]
+    %define             m10  [esp+16*2]
+    %define             m11  [esp+16*3]
+    %define             m12  [esp+16*4]
+    %define             m13  [esp+16*5]
+    %define             m14  [esp+16*6]
+    %define             m15  [esp+16*7]
 %endif
+    imul                mxd, mxm, 0x010101
+    add                 mxd, t0d ; 8tap_h, mx, 4tap_h
+    imul                myd, mym, 0x010101
+    add                 myd, t1d ; 8tap_v, my, 4tap_v
+    LEA                  t2, put_ssse3
+    movifnidn            wd, wm
+    movifnidn          srcq, srcmp
+    movifnidn           ssq, ssmp
+    movifnidn            hd, hm
+    test                mxd, 0xf00
+    jnz .h
+    test                myd, 0xf00
+    jz mangle(private_prefix %+ _put_6tap_16bpc_ssse3).put
+.v:
+    movzx               mxd, myb
+    shr                 myd, 16
+    cmp                  hd, 6
+    cmovb               myd, mxd
+    movq                 m3, [base+subpel_filters+myq*8]
+    WIN64_SPILL_XMM      15
+    movd                 m7, r8m
+    movifnidn          dstq, dstmp
+    movifnidn           dsq, dsmp
+    punpcklbw            m3, m3
+    pshufb               m7, [base+pw_256]
+    psraw                m3, 8 ; sign-extend
+%if ARCH_X86_32
+    ALLOC_STACK       -16*7
+    pshufd               m0, m3, q0000
+    pshufd               m1, m3, q1111
+    pshufd               m2, m3, q2222
+    pshufd               m3, m3, q3333
+    mova                 m8, m0
+    mova                 m9, m1
+    mova                m10, m2
+    mova                m11, m3
+%else
+    pshufd               m8, m3, q0000
+    pshufd               m9, m3, q1111
+    pshufd              m10, m3, q2222
+    pshufd              m11, m3, q3333
+%endif
+    lea                  r6, [ssq*3]
+    sub                srcq, r6
+    cmp                  wd, 2
+    jne .v_w4
+.v_w2:
+    movd                 m1, [srcq+ssq*0]
+    movd                 m4, [srcq+ssq*1]
+    movd                 m2, [srcq+ssq*2]
+    add                srcq, r6
+    movd                 m5, [srcq+ssq*0]
+    movd                 m3, [srcq+ssq*1]
+    movd                 m6, [srcq+ssq*2]
+    add                srcq, r6
+    movd                 m0, [srcq+ssq*0]
+    punpckldq            m1, m4      ; 0 1
+    punpckldq            m4, m2      ; 1 2
+    punpckldq            m2, m5      ; 2 3
+    punpckldq            m5, m3      ; 3 4
+    punpckldq            m3, m6      ; 4 5
+    punpckldq            m6, m0      ; 5 6
+    punpcklwd            m1, m4      ; 01 12
+    punpcklwd            m2, m5      ; 23 34
+    punpcklwd            m3, m6      ; 45 56
+    pxor                 m6, m6
+.v_w2_loop:
+    movd                 m4, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    pmaddwd              m5, m8, m1  ; a0 b0
+    mova                 m1, m2
+    pmaddwd              m2, m9      ; a1 b1
+    paddd                m5, m2
+    mova                 m2, m3
+    pmaddwd              m3, m10     ; a2 b2
+    paddd                m5, m3
+    punpckldq            m3, m0, m4  ; 6 7
+    movd                 m0, [srcq+ssq*0]
+    punpckldq            m4, m0      ; 7 8
+    punpcklwd            m3, m4      ; 67 78
+    pmaddwd              m4, m11, m3 ; a3 b3
+    paddd                m5, m4
+    psrad                m5, 5
+    packssdw             m5, m5
+    pmaxsw               m5, m6
+    pavgw                m5, m6
+    pminsw               m5, m7
+    movd       [dstq+dsq*0], m5
+    pshuflw              m5, m5, q3232
+    movd       [dstq+dsq*1], m5
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
+    jg .v_w2_loop
+    RET
+.v_w4:
+%if ARCH_X86_32
+    shl                  wd, 14
+%if STACK_ALIGNMENT < 16
+    mov          [esp+4*29], srcq
+    mov          [esp+4*30], dstq
+%else
+    mov               srcmp, srcq
+%endif
+    lea                  wd, [wq+hq-(1<<16)]
+%else
+    shl                  wd, 6
+    mov                  r7, srcq
+    mov                  r8, dstq
+    lea                  wd, [wq+hq-(1<<8)]
+%endif
+.v_w4_loop0:
+    movq                 m1, [srcq+ssq*0]
+    movq                 m2, [srcq+ssq*1]
+    movq                 m3, [srcq+ssq*2]
+    add                srcq, r6
+    movq                 m4, [srcq+ssq*0]
+    movq                 m5, [srcq+ssq*1]
+    movq                 m6, [srcq+ssq*2]
+    add                srcq, r6
+    movq                 m0, [srcq+ssq*0]
+    punpcklwd            m1, m2      ; 01
+    punpcklwd            m2, m3      ; 12
+    punpcklwd            m3, m4      ; 23
+    punpcklwd            m4, m5      ; 34
+    punpcklwd            m5, m6      ; 45
+    punpcklwd            m6, m0      ; 56
+%if ARCH_X86_32
+    jmp .v_w4_loop_start
+.v_w4_loop:
+    mova                 m1, m12
+    mova                 m2, m13
+    mova                 m3, m14
+.v_w4_loop_start:
+    pmaddwd              m1, m8      ; a0
+    pmaddwd              m2, m8      ; b0
+    mova                m12, m3
+    mova                m13, m4
+    pmaddwd              m3, m9      ; a1
+    pmaddwd              m4, m9      ; b1
+    paddd                m1, m3
+    paddd                m2, m4
+    mova                m14, m5
+    mova                 m4, m6
+    pmaddwd              m5, m10     ; a2
+    pmaddwd              m6, m10     ; b2
+    paddd                m1, m5
+    paddd                m2, m6
+    movq                 m6, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    punpcklwd            m5, m0, m6  ; 67
+    movq                 m0, [srcq+ssq*0]
+    pmaddwd              m3, m11, m5 ; a3
+    punpcklwd            m6, m0      ; 78
+    paddd                m1, m3
+    pmaddwd              m3, m11, m6 ; b3
+    paddd                m2, m3
+    psrad                m1, 5
+    psrad                m2, 5
+    packssdw             m1, m2
+    pxor                 m2, m2
+    pmaxsw               m1, m2
+    pavgw                m1, m2
+    pminsw               m1, m7
+    movq       [dstq+dsq*0], m1
+    movhps     [dstq+dsq*1], m1
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
+    jg .v_w4_loop
+%if STACK_ALIGNMENT < 16
+    mov                srcq, [esp+4*29]
+    mov                dstq, [esp+4*30]
+    movzx                hd, ww
+    add                srcq, 8
+    add                dstq, 8
+    mov          [esp+4*29], srcq
+    mov          [esp+4*30], dstq
+%else
+    mov                srcq, srcmp
+    mov                dstq, dstmp
+    movzx                hd, ww
+    add                srcq, 8
+    add                dstq, 8
+    mov               srcmp, srcq
+    mov               dstmp, dstq
+%endif
+    sub                  wd, 1<<16
+%else
+.v_w4_loop:
+    pmaddwd             m12, m8, m1  ; a0
+    pmaddwd             m13, m8, m2  ; b0
+    mova                 m1, m3
+    mova                 m2, m4
+    pmaddwd              m3, m9      ; a1
+    pmaddwd              m4, m9      ; b1
+    paddd               m12, m3
+    paddd               m13, m4
+    mova                 m3, m5
+    mova                 m4, m6
+    pmaddwd              m5, m10     ; a2
+    pmaddwd              m6, m10     ; b2
+    paddd               m12, m5
+    paddd               m13, m6
+    movq                 m6, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    punpcklwd            m5, m0, m6  ; 67
+    movq                 m0, [srcq+ssq*0]
+    pmaddwd             m14, m11, m5 ; a3
+    punpcklwd            m6, m0      ; 78
+    paddd               m12, m14
+    pmaddwd             m14, m11, m6 ; b3
+    paddd               m13, m14
+    psrad               m12, 5
+    psrad               m13, 5
+    packssdw            m12, m13
+    pxor                m13, m13
+    pmaxsw              m12, m13
+    pavgw               m12, m13
+    pminsw              m12, m7
+    movq       [dstq+dsq*0], m12
+    movhps     [dstq+dsq*1], m12
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
+    jg .v_w4_loop
+    add                  r7, 8
+    add                  r8, 8
+    movzx                hd, wb
+    mov                srcq, r7
+    mov                dstq, r8
+    sub                  wd, 1<<8
+%endif
+    jg .v_w4_loop0
+    RET
+.h:
+    RESET_STACK_STATE
+    test                myd, 0xf00
+    jnz .hv
+    mov                 myd, r8m
+    movd                 m5, r8m
+    shr                 myd, 11
+    movddup              m4, [base+put_8tap_h_rnd+myq*8]
+    movifnidn           dsq, dsmp
+    pshufb               m5, [base+pw_256]
+    cmp                  wd, 4
+    jle mangle(private_prefix %+ _put_6tap_16bpc_ssse3).h_w4
+    WIN64_SPILL_XMM      12
+    shr                 mxd, 16
+    movq                 m3, [base+subpel_filters+mxq*8]
+    movifnidn          dstq, dstmp
+    mova                 m6, [base+spel_h_shufA]
+    mova                 m7, [base+spel_h_shufB]
+%if UNIX64
+    mov                  wd, wd
+%endif
+    lea                srcq, [srcq+wq*2]
+    punpcklbw            m3, m3
+    lea                dstq, [dstq+wq*2]
+    psraw                m3, 8
+    neg                  wq
+%if ARCH_X86_32
+    ALLOC_STACK       -16*4
+    pshufd               m0, m3, q0000
+    pshufd               m1, m3, q1111
+    pshufd               m2, m3, q2222
+    pshufd               m3, m3, q3333
+    mova                 m8, m0
+    mova                 m9, m1
+    mova                m10, m2
+    mova                m11, m3
+%else
+    pshufd               m8, m3, q0000
+    pshufd               m9, m3, q1111
+    pshufd              m10, m3, q2222
+    pshufd              m11, m3, q3333
+%endif
+.h_w8_loop0:
+    mov                  r6, wq
+.h_w8_loop:
+    movu                 m0, [srcq+r6*2- 6]
+    movu                 m1, [srcq+r6*2+ 2]
+    pshufb               m2, m0, m6   ; 0 1 1 2 2 3 3 4
+    pshufb               m0, m7       ; 2 3 3 4 4 5 5 6
+    pmaddwd              m2, m8       ; abcd0
+    pmaddwd              m0, m9       ; abcd1
+    pshufb               m3, m1, m6   ; 4 5 5 6 6 7 7 8
+    pshufb               m1, m7       ; 6 7 7 8 8 9 9 a
+    paddd                m2, m4
+    paddd                m0, m2
+    pmaddwd              m2, m10, m3  ; abcd2
+    pmaddwd              m3, m8       ; efgh0
+    paddd                m0, m2
+    pmaddwd              m2, m11, m1  ; abcd3
+    pmaddwd              m1, m9       ; efgh1
+    paddd                m0, m2
+    movu                 m2, [srcq+r6*2+10]
+    paddd                m3, m4
+    paddd                m1, m3
+    pshufb               m3, m2, m6   ; 8 9 9 a a b b c
+    pshufb               m2, m7       ; a b b c c d d e
+    pmaddwd              m3, m10      ; efgh2
+    pmaddwd              m2, m11      ; efgh3
+    paddd                m1, m3
+    paddd                m1, m2
+    psrad                m0, 6
+    psrad                m1, 6
+    packssdw             m0, m1
+    pxor                 m1, m1
+    pminsw               m0, m5
+    pmaxsw               m0, m1
+    mova        [dstq+r6*2], m0
+    add                  r6, 8
+    jl .h_w8_loop
+    add                srcq, ssq
+    add                dstq, dsq
+    dec                  hd
+    jg .h_w8_loop0
+    RET
+.hv:
+    RESET_STACK_STATE
+%if ARCH_X86_32
+    movd                 m4, r8m
+    pshufb               m4, [base+pw_256]
+%else
+%if WIN64
+    ALLOC_STACK        16*6, 16
+%endif
+    movd                m15, r8m
+    pshufb              m15, [base+pw_256]
+%endif
+    cmp                  wd, 4
+    jg .hv_w8
+    movzx               mxd, mxb
+    je .hv_w4
+    movq                 m0, [base+subpel_filters+mxq*8]
+    movzx               mxd, myb
+    shr                 myd, 16
+    cmp                  hd, 6
+    cmovb               myd, mxd
+    movq                 m3, [base+subpel_filters+myq*8]
+    movddup              m6, [base+pd_8704]
+    pshuflw              m0, m0, q2121
+    pxor                 m7, m7
+    punpcklbw            m7, m0
+    punpcklbw            m3, m3
+    psraw                m3, 8 ; sign-extend
+    test          dword r8m, 0x800
+    jz .hv_w2_10bpc
+    movddup              m6, [base+pd_2560]
+    psraw                m7, 2
+    psllw                m3, 2
+.hv_w2_10bpc:
+%if ARCH_X86_32
+    mov                dstq, dstmp
+    mov                 dsq, dsmp
+    mova                 m5, [base+spel_h_shuf2]
+    ALLOC_STACK       -16*8
+    pshufd               m0, m3, q0000
+    pshufd               m1, m3, q1111
+    pshufd               m2, m3, q2222
+    pshufd               m3, m3, q3333
+    mova                 m9, m5
+    mova                m11, m0
+    mova                m12, m1
+    mova                m13, m2
+    mova                m14, m3
+    mova                m15, m4
+%else
+    mova                 m9, [base+spel_h_shuf2]
+    pshufd              m11, m3, q0000
+    pshufd              m12, m3, q1111
+    pshufd              m13, m3, q2222
+    pshufd              m14, m3, q3333
+%endif
+    lea                  r6, [ssq*3]
+    sub                srcq, 2
+    sub                srcq, r6
+    movu                 m2, [srcq+ssq*0]
+    movu                 m3, [srcq+ssq*1]
+    movu                 m1, [srcq+ssq*2]
+    add                srcq, r6
+    movu                 m4, [srcq+ssq*0]
+%if ARCH_X86_32
+    REPX    {pshufb  x, m5}, m2, m3, m1, m4
+%else
+    REPX    {pshufb  x, m9}, m2, m3, m1, m4
+%endif
+    REPX    {pmaddwd x, m7}, m2, m3, m1, m4
+    phaddd               m2, m3        ; 0 1
+    phaddd               m1, m4        ; 2 3
+    movu                 m3, [srcq+ssq*1]
+    movu                 m4, [srcq+ssq*2]
+    add                srcq, r6
+    movu                 m0, [srcq+ssq*0]
+%if ARCH_X86_32
+    REPX    {pshufb  x, m5}, m3, m4, m0
+%else
+    REPX    {pshufb  x, m9}, m3, m4, m0
+%endif
+    REPX    {pmaddwd x, m7}, m3, m4, m0
+    phaddd               m3, m4        ; 4 5
+    phaddd               m0, m0        ; 6 6
+    REPX    {paddd   x, m6}, m2, m1, m3, m0
+    REPX    {psrad   x, 10}, m2, m1, m3, m0
+    packssdw             m2, m1        ; 0 1 2 3
+    packssdw             m3, m0        ; 4 5 6 _
+    palignr              m4, m3, m2, 4 ; 1 2 3 4
+    pshufd               m5, m3, q0321 ; 5 6 _ _
+    punpcklwd            m1, m2, m4    ; 01 12
+    punpckhwd            m2, m4        ; 23 34
+    punpcklwd            m3, m5        ; 45 56
+.hv_w2_loop:
+    movu                 m4, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    movu                 m5, [srcq+ssq*0]
+    pshufb               m4, m9
+    pshufb               m5, m9
+    pmaddwd              m4, m7
+    pmaddwd              m5, m7
+    phaddd               m4, m5
+    pmaddwd              m5, m11, m1   ; a0 b0
+    mova                 m1, m2
+    pmaddwd              m2, m12       ; a1 b1
+    paddd                m5, m2
+    mova                 m2, m3
+    pmaddwd              m3, m13       ; a2 b2
+    paddd                m5, m3
+    paddd                m4, m6
+    psrad                m4, 10        ; 7 8
+    packssdw             m0, m4
+    pshufd               m3, m0, q2103
+    punpckhwd            m3, m0        ; 67 78
+    mova                 m0, m4
+    pmaddwd              m4, m14, m3   ; a3 b3
+    paddd                m5, m4
+    psrad                m5, 10
+    packssdw             m5, m5
+    pxor                 m4, m4
+    pminsw               m5, m15
+    pmaxsw               m5, m4
+    movd       [dstq+dsq*0], m5
+    pshuflw              m5, m5, q3232
+    movd       [dstq+dsq*1], m5
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
+    jg .hv_w2_loop
+    RET
+.hv_w8:
+    shr                 mxd, 16
+.hv_w4:
+    movq                 m2, [base+subpel_filters+mxq*8]
+    movzx               mxd, myb
+    shr                 myd, 16
+    cmp                  hd, 6
+    cmovb               myd, mxd
+    movq                 m3, [base+subpel_filters+myq*8]
+%if ARCH_X86_32
+    RESET_STACK_STATE
+    mov                dstq, dstmp
+    mov                 dsq, dsmp
+    mova                 m0, [base+spel_h_shufA]
+    mova                 m1, [base+spel_h_shufB]
+    mova                 m6, [base+pd_512]
+    ALLOC_STACK      -16*15
+    mova                 m8, m0
+    mova                 m9, m1
+    mova                m14, m6
+%else
+    mova                 m8, [base+spel_h_shufA]
+    mova                 m9, [base+spel_h_shufB]
+%endif
+    pxor                 m0, m0
+    punpcklbw            m0, m2
+    punpcklbw            m3, m3
+    psraw                m3, 8
+    test          dword r8m, 0x800
+    jz .hv_w4_10bpc
+    psraw                m0, 2
+    psllw                m3, 2
+.hv_w4_10bpc:
+    lea                  r6, [ssq*3]
+    sub                srcq, 6
+    sub                srcq, r6
+%if ARCH_X86_32
+    %define tmp esp+16*8
+    shl                  wd, 14
+%if STACK_ALIGNMENT < 16
+    mov          [esp+4*61], srcq
+    mov          [esp+4*62], dstq
+%else
+    mov               srcmp, srcq
+%endif
+    mova         [tmp+16*5], m4
+    lea                  wd, [wq+hq-(1<<16)]
+    pshufd               m1, m0, q0000
+    pshufd               m2, m0, q1111
+    pshufd               m5, m0, q2222
+    pshufd               m0, m0, q3333
+    mova                m10, m1
+    mova                m11, m2
+    mova                m12, m5
+    mova                m13, m0
+%else
+%if WIN64
+    %define tmp rsp
+%else
+    %define tmp rsp-104 ; red zone
+%endif
+    shl                  wd, 6
+    mov                  r7, srcq
+    mov                  r8, dstq
+    lea                  wd, [wq+hq-(1<<8)]
+    pshufd              m10, m0, q0000
+    pshufd              m11, m0, q1111
+    pshufd              m12, m0, q2222
+    pshufd              m13, m0, q3333
+    mova         [tmp+16*5], m15
+%endif
+    pshufd               m0, m3, q0000
+    pshufd               m1, m3, q1111
+    pshufd               m2, m3, q2222
+    pshufd               m3, m3, q3333
+    mova         [tmp+16*1], m0
+    mova         [tmp+16*2], m1
+    mova         [tmp+16*3], m2
+    mova         [tmp+16*4], m3
+%macro PUT_8TAP_HV_H 4-5 m14 ; dst/src+0, src+8, tmp, shift, [pd_512]
+    pshufb              m%3, m%1, m8 ; 0 1 1 2 2 3 3 4
+    pshufb              m%1, m9      ; 2 3 3 4 4 5 5 6
+    pmaddwd             m%3, m10
+    pmaddwd             m%1, m11
+    paddd               m%3, %5
+    paddd               m%1, m%3
+    pshufb              m%3, m%2, m8 ; 4 5 5 6 6 7 7 8
+    pshufb              m%2, m9      ; 6 7 7 8 8 9 9 a
+    pmaddwd             m%3, m12
+    pmaddwd             m%2, m13
+    paddd               m%1, m%3
+    paddd               m%1, m%2
+    psrad               m%1, %4
+%endmacro
+.hv_w4_loop0:
+%if ARCH_X86_64
+    mova                m14, [pd_512]
+%endif
+    movu                 m4, [srcq+ssq*0+0]
+    movu                 m1, [srcq+ssq*0+8]
+    movu                 m5, [srcq+ssq*1+0]
+    movu                 m2, [srcq+ssq*1+8]
+    movu                 m6, [srcq+ssq*2+0]
+    movu                 m3, [srcq+ssq*2+8]
+    add                srcq, r6
+    PUT_8TAP_HV_H         4, 1, 0, 10
+    PUT_8TAP_HV_H         5, 2, 0, 10
+    PUT_8TAP_HV_H         6, 3, 0, 10
+    movu                 m7, [srcq+ssq*0+0]
+    movu                 m2, [srcq+ssq*0+8]
+    movu                 m1, [srcq+ssq*1+0]
+    movu                 m3, [srcq+ssq*1+8]
+    PUT_8TAP_HV_H         7, 2, 0, 10
+    PUT_8TAP_HV_H         1, 3, 0, 10
+    movu                 m2, [srcq+ssq*2+0]
+    movu                 m3, [srcq+ssq*2+8]
+    add                srcq, r6
+    PUT_8TAP_HV_H         2, 3, 0, 10
+    packssdw             m4, m7      ; 0 3
+    packssdw             m5, m1      ; 1 4
+    movu                 m0, [srcq+ssq*0+0]
+    movu                 m1, [srcq+ssq*0+8]
+    PUT_8TAP_HV_H         0, 1, 3, 10
+    packssdw             m6, m2      ; 2 5
+    packssdw             m7, m0      ; 3 6
+    punpcklwd            m1, m4, m5  ; 01
+    punpckhwd            m4, m5      ; 34
+    punpcklwd            m2, m5, m6  ; 12
+    punpckhwd            m5, m6      ; 45
+    punpcklwd            m3, m6, m7  ; 23
+    punpckhwd            m6, m7      ; 56
+%if ARCH_X86_32
+    jmp .hv_w4_loop_start
+.hv_w4_loop:
+    mova                 m1, [tmp+16*6]
+    mova                 m2, m15
+.hv_w4_loop_start:
+    mova                 m7, [tmp+16*1]
+    pmaddwd              m1, m7      ; a0
+    pmaddwd              m2, m7      ; b0
+    mova                 m7, [tmp+16*2]
+    mova         [tmp+16*6], m3
+    pmaddwd              m3, m7      ; a1
+    mova                m15, m4
+    pmaddwd              m4, m7      ; b1
+    mova                 m7, [tmp+16*3]
+    paddd                m1, m3
+    paddd                m2, m4
+    mova                 m3, m5
+    pmaddwd              m5, m7      ; a2
+    mova                 m4, m6
+    pmaddwd              m6, m7      ; b2
+    paddd                m1, m5
+    paddd                m2, m6
+    movu                 m7, [srcq+ssq*1+0]
+    movu                 m5, [srcq+ssq*1+8]
+    lea                srcq, [srcq+ssq*2]
+    PUT_8TAP_HV_H         7, 5, 6, 10
+    packssdw             m0, m7      ; 6 7
+    mova         [tmp+16*0], m0
+    movu                 m0, [srcq+ssq*0+0]
+    movu                 m5, [srcq+ssq*0+8]
+    PUT_8TAP_HV_H         0, 5, 6, 10
+    mova                 m6, [tmp+16*0]
+    packssdw             m7, m0      ; 7 8
+    punpcklwd            m5, m6, m7  ; 67
+    punpckhwd            m6, m7      ; 78
+    pmaddwd              m7, m5, [tmp+16*4]
+    paddd                m1, m7      ; a3
+    pmaddwd              m7, m6, [tmp+16*4]
+    paddd                m2, m7      ; b3
+    psrad                m1, 9
+    psrad                m2, 9
+    packssdw             m1, m2
+    pxor                 m7, m7
+    pmaxsw               m1, m7
+    pavgw                m7, m1
+    pminsw               m7, [tmp+16*5]
+    movq       [dstq+dsq*0], m7
+    movhps     [dstq+dsq*1], m7
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
+    jg .hv_w4_loop
+%if STACK_ALIGNMENT < 16
+    mov                srcq, [esp+4*61]
+    mov                dstq, [esp+4*62]
+    add                srcq, 8
+    add                dstq, 8
+    mov          [esp+4*61], srcq
+    mov          [esp+4*62], dstq
+%else
+    mov                srcq, srcmp
+    mov                dstq, dstmp
+    add                srcq, 8
+    add                dstq, 8
+    mov               srcmp, srcq
+    mov               dstmp, dstq
+%endif
+    movzx                hd, ww
+    sub                  wd, 1<<16
+%else
+.hv_w4_loop:
+    mova                m15, [tmp+16*1]
+    pmaddwd             m14, m15, m1 ; a0
+    pmaddwd             m15, m2      ; b0
+    mova                 m7, [tmp+16*2]
+    mova                 m1, m3
+    pmaddwd              m3, m7      ; a1
+    mova                 m2, m4
+    pmaddwd              m4, m7      ; b1
+    mova                 m7, [tmp+16*3]
+    paddd               m14, m3
+    paddd               m15, m4
+    mova                 m3, m5
+    pmaddwd              m5, m7      ; a2
+    mova                 m4, m6
+    pmaddwd              m6, m7      ; b2
+    paddd               m14, m5
+    paddd               m15, m6
+    movu                 m7, [srcq+ssq*1+0]
+    movu                 m5, [srcq+ssq*1+8]
+    lea                srcq, [srcq+ssq*2]
+    PUT_8TAP_HV_H         7, 5, 6, 10, [pd_512]
+    packssdw             m0, m7      ; 6 7
+    mova         [tmp+16*0], m0
+    movu                 m0, [srcq+ssq*0+0]
+    movu                 m5, [srcq+ssq*0+8]
+    PUT_8TAP_HV_H         0, 5, 6, 10, [pd_512]
+    mova                 m6, [tmp+16*0]
+    packssdw             m7, m0      ; 7 8
+    punpcklwd            m5, m6, m7  ; 67
+    punpckhwd            m6, m7      ; 78
+    pmaddwd              m7, m5, [tmp+16*4]
+    paddd               m14, m7      ; a3
+    pmaddwd              m7, m6, [tmp+16*4]
+    paddd               m15, m7      ; b3
+    psrad               m14, 9
+    psrad               m15, 9
+    packssdw            m14, m15
+    pxor                 m7, m7
+    pmaxsw              m14, m7
+    pavgw                m7, m14
+    pminsw               m7, [tmp+16*5]
+    movq       [dstq+dsq*0], m7
+    movhps     [dstq+dsq*1], m7
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
+    jg .hv_w4_loop
+    add                  r7, 8
+    add                  r8, 8
+    movzx                hd, wb
+    mov                srcq, r7
+    mov                dstq, r8
+    sub                  wd, 1<<8
+%endif
+    jg .hv_w4_loop0
+    RET
+%undef tmp
+
+%if ARCH_X86_32
+DECLARE_REG_TMP 2, 1, 6, 4
+%elif WIN64
+DECLARE_REG_TMP 6, 4, 7, 4
+%else
+DECLARE_REG_TMP 6, 7, 7, 8
+%endif
+
+%define PREP_8TAP_FN FN prep_8tap,
+PREP_8TAP_FN smooth,         SMOOTH,  SMOOTH,  prep_6tap_16bpc
+PREP_8TAP_FN smooth_regular, SMOOTH,  REGULAR, prep_6tap_16bpc
+PREP_8TAP_FN regular_smooth, REGULAR, SMOOTH,  prep_6tap_16bpc
+PREP_8TAP_FN regular,        REGULAR, REGULAR
+
+cglobal prep_6tap_16bpc, 0, 8, 0, tmp, src, ss, w, h, mx, my
+    %define            base  t2-prep_ssse3
+%if ARCH_X86_32
+    %define             mxb  r0b
+    %define             mxd  r0
+    %define             mxq  r0
+    %define             myb  r2b
+    %define             myd  r2
+    %define             myq  r2
+%endif
+    imul                mxd, mxm, 0x010101
+    add                 mxd, t0d ; 6tap_h, mx, 4tap_h
+    imul                myd, mym, 0x010101
+    add                 myd, t1d ; 6tap_v, my, 4tap_v
+    LEA                  t2, prep_ssse3
+    movifnidn            wd, wm
+    movifnidn            hd, hm
+    movifnidn          srcq, srcmp
+    test                mxd, 0xf00
+    jnz .h
+    test                myd, 0xf00
+    jnz .v
+.prep:
+    tzcnt                wd, wd
+    mov                 myd, r7m ; bitdepth_max
+    movzx                wd, word [base+prep_ssse3_table+wq*2]
+    mova                 m5, [base+pw_8192]
+    shr                 myd, 11
+    add                  wq, t2
+    movddup              m4, [base+prep_mul+myq*8]
+    movifnidn           ssq, ssmp
+    movifnidn          tmpq, tmpmp
+    lea                  r6, [ssq*3]
+%if WIN64
+    pop                  r7
+%endif
+    jmp                  wq
+.h:
+    RESET_STACK_STATE
+    test                myd, 0xf00
+    jnz .hv
+    movifnidn           ssq, r2mp
+    movddup              m5, [base+prep_8tap_1d_rnd]
+    cmp                  wd, 4
+    je mangle(private_prefix %+ _prep_8tap_16bpc_ssse3).h_w4
+    WIN64_SPILL_XMM      10
+    shr                 mxd, 16
+    movq                 m2, [base+subpel_filters+1+mxq*8]
+    movifnidn          tmpq, r0mp
+    mova                 m4, [base+spel_h_shufA]
+    add                  wd, wd
+    mova                 m6, [base+spel_h_shufB]
+    add                srcq, wq
+    punpcklbw            m2, m2
+    add                tmpq, wq
+    psraw                m2, 8
+    neg                  wq
+    test          dword r7m, 0x800
+    jnz .h_w8_12bpc
+    psllw                m2, 2
+.h_w8_12bpc:
+    pshufd               m7, m2, q0000
+%if ARCH_X86_32
+    ALLOC_STACK       -16*2
+    %define              m8  [rsp+16*0]
+    %define              m9  [rsp+16*1]
+    pshufd               m0, m2, q1111
+    pshufd               m1, m2, q2222
+    mova                 m8, m0
+    mova                 m9, m1
+%else
+    pshufd               m8, m2, q1111
+    pshufd               m9, m2, q2222
+%endif
+.h_w8_loop0:
+    mov                  r6, wq
+.h_w8_loop:
+    movu                 m3, [srcq+r6-4]
+    movu                 m2, [srcq+r6+8]
+    pshufb               m0, m3, m4  ; 01 12 23 34
+    pmaddwd              m0, m7      ; abcd0
+    pshufb               m3, m6      ; 23 34 45 56
+    pmaddwd              m1, m8, m3  ; abcd1
+    paddd                m0, m1
+    pshufb               m1, m2, m4  ; 67 78 89 9a
+    shufpd               m3, m1, 0x01; 45 56 67 78
+    pmaddwd              m1, m8      ; efgh1
+    pshufb               m2, m6      ; 89 9a ab bc
+    pmaddwd              m2, m9      ; efgh2
+    paddd                m1, m2
+    pmaddwd              m2, m9 , m3 ; abcd2
+    pmaddwd              m3, m7      ; efgh0
+    paddd                m0, m5
+    paddd                m1, m5
+    paddd                m0, m2
+    paddd                m1, m3
+    psrad                m0, 4
+    psrad                m1, 4
+    packssdw             m0, m1
+    mova          [tmpq+r6], m0
+    add                  r6, 16
+    jl .h_w8_loop
+    add                srcq, ssq
+    sub                tmpq, wq
+    dec                  hd
+    jg .h_w8_loop0
+    RET
+.v:
+    movzx               mxd, myb
+    shr                 myd, 16
+    cmp                  hd, 6
+    cmovb               myd, mxd
+    movddup              m5, [base+prep_8tap_1d_rnd]
+    movq                 m2, [base+subpel_filters+1+myq*8]
+    WIN64_SPILL_XMM      11, 16
+    movifnidn           ssq, r2mp
+    movifnidn          tmpq, r0mp
+    punpcklbw            m2, m2
+    sub                srcq, ssq
+    psraw                m2, 8 ; sign-extend
+    test          dword r7m, 0x800
+    jnz .v_12bpc
+    psllw                m2, 2
+.v_12bpc:
+    sub                srcq, ssq
+%if ARCH_X86_32
+    ALLOC_STACK       -16*4
+    pshufd               m0, m2, q0000
+    mov                 r6d, wd
+    pshufd               m1, m2, q1111
+    shl                 r6d, 14
+    pshufd               m2, m2, q2222
+    lea                 r6d, [r6+hq-(1<<16)]
+    mova                 m8, m0
+    mova                 m9, m1
+    mova                m10, m2
+%if STACK_ALIGNMENT < 16
+    %define           srcmp  [esp+16*3+4*0]
+    %define           tmpmp  [esp+16*3+4*1]
+%endif
+.v_w4_loop0:
+    mov               srcmp, srcq
+    mov               tmpmp, tmpq
+%else
+    pshufd               m8, m2, q0000
+    and                  wd, -8
+    jnz .v_w8
+    pshufd               m9, m2, q1111
+    pshufd              m10, m2, q2222
+%endif
+    movq                 m1, [srcq+ssq*0]
+    movq                 m2, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    movq                 m3, [srcq+ssq*0]
+    movq                 m4, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    movq                 m0, [srcq+ssq*0]
+    punpcklwd            m1, m2      ; 01
+    punpcklwd            m2, m3      ; 12
+    punpcklwd            m3, m4      ; 23
+    punpcklwd            m4, m0      ; 34
+.v_w4_loop:
+    pmaddwd              m6, m8, m1  ; a0
+    pmaddwd              m7, m8, m2  ; b0
+    mova                 m1, m3
+    pmaddwd              m3, m9      ; a1
+    mova                 m2, m4
+    pmaddwd              m4, m9      ; b1
+    paddd                m6, m3
+    movq                 m3, [srcq+ssq*0]
+    paddd                m7, m4
+    movq                 m4, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    movq                 m0, [srcq+ssq*0]
+    punpcklwd            m3, m4      ; 45
+    punpcklwd            m4, m0      ; 56
+    pmaddwd              m0, m10, m3 ; a2
+    paddd                m6, m5
+    paddd                m6, m0
+    pmaddwd              m0, m10, m4 ; b2
+    paddd                m7, m5
+    paddd                m7, m0
+    psrad                m6, 4
+    psrad                m7, 4
+    packssdw             m6, m7
+%if ARCH_X86_32
+    movq        [tmpq+wq*0], m6
+    movhps      [tmpq+wq*2], m6
+    lea                tmpq, [tmpq+wq*4]
+    sub                  hd, 2
+    jg .v_w4_loop
+    mov                srcq, srcmp
+    mov                tmpq, tmpmp
+    movzx                hd, r6w
+    add                srcq, 8
+    add                tmpq, 8
+    sub                 r6d, 1<<16
+    jg .v_w4_loop0
+    RET
+%else
+    mova             [tmpq], m6
+    add                tmpq, 16
+    sub                  hd, 2
+    jg .v_w4_loop
+    RET
+.v_w8:
+    mova                r6m, m8
+    lea                 r6d, [wq*4-(1<<5)]
+    pshufd               m6, m2, q1111
+    lea                 r6d, [hq+r6*8]
+    pshufd               m7, m2, q2222
+    WIN64_PUSH_XMM       16
+.v_w8_loop0:
+    movu                 m9, [srcq+ssq*0]
+    lea                  r5, [srcq+ssq*2]
+    movu                m11, [srcq+ssq*1]
+    mov                  r7, tmpq
+    movu                m13, [r5+ssq*0]
+    movu                m15, [r5+ssq*1]
+    lea                  r5, [r5+ssq*2]
+    movu                 m4, [r5+ssq*0]
+    punpcklwd            m8, m9, m11  ; 01
+    punpckhwd            m9, m11
+    punpcklwd           m10, m11, m13 ; 12
+    punpckhwd           m11, m13
+    punpcklwd           m12, m13, m15 ; 23
+    punpckhwd           m13, m15
+    punpcklwd           m14, m15, m4  ; 34
+    punpckhwd           m15, m4
+.v_w8_loop:
+    mova                 m3, r6m
+    pmaddwd              m0, m8, m3   ; a0
+    pmaddwd              m2, m9, m3   ; a0'
+    pmaddwd              m1, m10, m3  ; b0
+    pmaddwd              m3, m11      ; b0'
+    mova                 m8, m12
+    pmaddwd             m12, m6       ; a1
+    mova                 m9, m13
+    pmaddwd             m13, m6       ; a1'
+    mova                m10, m14
+    pmaddwd             m14, m6       ; b1
+    mova                m11, m15
+    pmaddwd             m15, m6       ; b1'
+    paddd                m0, m12
+    paddd                m2, m13
+    movu                m13, [r5+ssq*0]
+    paddd                m1, m14
+    paddd                m3, m15
+    movu                m15, [r5+ssq*1]
+    lea                  r5, [r5+ssq*2]
+    movu                 m4, [r5+ssq*0]
+    REPX      {paddd x, m5}, m0, m2, m1, m3
+    punpcklwd           m12, m13, m15 ; 45
+    punpckhwd           m13, m15
+    punpcklwd           m14, m15, m4  ; 56
+    punpckhwd           m15, m4
+    pmaddwd              m4, m7, m12  ; a2
+    paddd                m0, m4
+    pmaddwd              m4, m7, m13  ; a2'
+    paddd                m2, m4
+    pmaddwd              m4, m7, m14  ; b2
+    paddd                m1, m4
+    pmaddwd              m4, m7, m15  ; b2'
+    paddd                m3, m4
+    REPX       {psrad x, 4}, m0, m2, m1, m3
+    packssdw             m0, m2
+    packssdw             m1, m3
+    mova          [r7+wq*0], m0
+    mova          [r7+wq*2], m1
+    lea                  r7, [r7+wq*4]
+    sub                  hd, 2
+    jg .v_w8_loop
+    add                srcq, 16
+    add                tmpq, 16
+    movzx                hd, r6b
+    sub                 r6d, 1<<8
+    jg .v_w8_loop0
+    RET
+%endif
+.hv:
+    and                  wd, -8
+    jnz .hv_w8
+    movzx               mxd, mxb
+    movq                 m0, [base+subpel_filters+mxq*8]
+    movzx               mxd, myb
+    shr                 myd, 16
+    cmp                  hd, 6
+    cmovb               myd, mxd
+    movq                 m2, [base+subpel_filters+1+myq*8]
+    WIN64_SPILL_XMM      15
+    movifnidn           ssq, r2mp
+    movifnidn          tmpq, r0mp
+    mova                 m7, [base+prep_8tap_2d_rnd]
+    sub                srcq, 2
+    pshuflw              m0, m0, q2121
+    pxor                 m6, m6
+    punpcklbw            m6, m0
+    punpcklbw            m2, m2
+    psraw                m6, 4
+    psraw                m2, 8
+    test          dword r7m, 0x800
+    jz .hv_w4_10bpc
+    psraw                m6, 2
+.hv_w4_10bpc:
+%if ARCH_X86_32
+%assign regs_used 4
+    ALLOC_STACK       -16*7
+%assign regs_used 7
+    %define             m10  [esp+16*3]
+    %define             m12  [esp+16*5]
+    %define             m13  [esp+16*6]
+    %define             m14  [base+spel_h_shufA]
+    %define             m11  [base+spel_h_shufB]
+    pshufd               m0, m2, q0000
+    pshufd               m1, m2, q1111
+    pshufd               m2, m2, q2222
+    pshufd               m5, m6, q0000
+    pshufd               m6, m6, q1111
+    mova                 m8, m0
+    mova                 m9, m1
+    mova                m10, m2
+    mova                m12, m5
+    mova                m13, m6
+    neg                 ssq
+    movu                 m3, [srcq+ssq*2]
+    movu                 m4, [srcq+ssq*1]
+    neg                 ssq
+%else
+    mov                  r6, ssq
+    pshufd               m8, m2, q0000
+    neg                  r6
+    pshufd               m9, m2, q1111
+    movu                 m3, [srcq+r6 *2]
+    pshufd              m10, m2, q2222
+    movu                 m4, [srcq+r6 *1]
+    pshufd              m12, m6, q0000
+    mova                m14, [base+spel_h_shufA]
+    pshufd              m13, m6, q1111
+    mova                m11, [base+spel_h_shufB]
+%endif
+    movu                 m1, [srcq+ssq*0]
+    movu                 m0, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    movu                 m2, [srcq+ssq*0]
+    HV_H_W4_6TAP         m3, m3, m5, m11
+    HV_H_W4_6TAP         m4, m4, m5, m11
+    HV_H_W4_6TAP         m5, m1, m5, m11
+    HV_H_W4_6TAP         m0, m0, m1, m11
+    HV_H_W4_6TAP         m2, m2, m1, m11
+    REPX       {psrad x, 6}, m3, m5, m4, m0, m2
+    packssdw             m3, m5      ; 0 2
+    packssdw             m4, m0      ; 1 3
+    packssdw             m5, m2      ; 2 4
+    punpcklwd            m1, m3, m4  ; 01
+    punpckhwd            m3, m4      ; 23
+    punpcklwd            m2, m4, m5  ; 12
+    punpckhwd            m4, m5      ; 34
+.hv_w4_loop:
+    movu                 m0, [srcq+ssq*1]
+    pmaddwd              m5, m8, m1  ; a0
+    lea                srcq, [srcq+ssq*2]
+    pmaddwd              m6, m8, m2  ; b0
+    mova                 m1, m3
+    pmaddwd              m3, m9      ; a1
+    mova                 m2, m4
+    pmaddwd              m4, m9      ; b1
+    paddd                m5, m3
+    movu                 m3, [srcq+ssq*0]
+    paddd                m6, m4
+    HV_H_W4_6TAP         m0, m0, m4, m11
+    HV_H_W4_6TAP         m3, m3, m4, m11
+    psrad                m4, m2, 16
+    psrad                m0, 6
+    psrad                m3, 6
+    packssdw             m4, m0      ; 4 5
+    packssdw             m0, m3      ; 5 6
+    punpcklwd            m3, m4, m0  ; 45
+    punpckhwd            m4, m0      ; 56
+    pmaddwd              m0, m10, m3 ; a2
+    paddd                m5, m7
+    paddd                m5, m0
+    pmaddwd              m0, m10, m4 ; b2
+    paddd                m6, m7
+    paddd                m6, m0
+    psrad                m5, 6
+    psrad                m6, 6
+    packssdw             m5, m6
+    mova             [tmpq], m5
+    add                tmpq, 16
+    sub                  hd, 2
+    jg .hv_w4_loop
+    RET
+.hv_w8:
+    RESET_STACK_STATE
+    shr                 mxd, 16
+    movq                 m2, [base+subpel_filters+1+mxq*8]
+    movzx               mxd, myb
+    shr                 myd, 16
+    cmp                  hd, 6
+    cmovb               myd, mxd
+    movq                 m1, [base+subpel_filters+1+myq*8]
+    movifnidn           ssq, r2mp
+    mova                 m4, [base+prep_8tap_2d_rnd]
+    pxor                 m0, m0
+    punpcklbw            m0, m2
+    punpcklbw            m1, m1
+    sub                srcq, 4
+    psraw                m0, 4
+    psraw                m1, 8
+    test          dword r7m, 0x800
+    jz .hv_w8_10bpc
+    psraw                m0, 2
+.hv_w8_10bpc:
+%if ARCH_X86_32
+%assign regs_used 1
+    ALLOC_STACK       -16*9
+%assign regs_used 7
+    mov                tmpq, r0mp
+    mova         [rsp+16*7], m4
+%else
+%if WIN64
+    PUSH                 r8
+%assign regs_used 9
+%endif
+    ALLOC_STACK        16*6, 16
+%endif
+    pshufd               m2, m0, q0000
+    mova         [rsp+16*0], m2
+    pshufd               m2, m0, q1111
+    mova         [rsp+16*1], m2
+    pshufd               m0, m0, q2222
+    mova         [rsp+16*2], m0
+    pshufd               m2, m1, q0000
+    mova         [rsp+16*3], m2
+    pshufd               m2, m1, q1111
+    mova         [rsp+16*4], m2
+    pshufd               m1, m1, q2222
+    mova         [rsp+16*5], m1
+    mov                  r6, ssq
+    neg                  r6
+%if ARCH_X86_32
+    mov                 r5d, wd
+    shl                 r5d, 14
+    lea                 r5d, [r5+hq-(1<<16)]
+%if STACK_ALIGNMENT < 16
+    %define           srcmp  [esp+16*8+4*0]
+    %define           tmpmp  [esp+16*8+4*1]
+%endif
+.hv_w8_loop0:
+    mov               srcmp, srcq
+    mov               tmpmp, tmpq
+    movu                 m5, [srcq+r6*2+0]
+    movu                 m6, [srcq+r6*2+2]
+    mova                 m7, [rsp+16*0]
+    mova                 m1, [rsp+16*1]
+    mova                 m0, [rsp+16*2]
+    HV_H_6TAP            m2, m5, m6, m7, m1, m0
+    movu                 m5, [srcq+r6*1+0]
+    movu                 m6, [srcq+r6*1+2]
+    HV_H_6TAP            m3, m5, m6, m7, m1, m0
+    movu                 m5, [srcq+ssq*0+0]
+    movu                 m6, [srcq+ssq*0+2]
+    HV_H_6TAP            m4, m5, m6, m7, m1, m0
+    movu                 m5, [srcq+ssq*1+0]
+    movu                 m6, [srcq+ssq*1+2]
+    lea                srcq, [srcq+ssq*2]
+    HV_H_6TAP            m0, m5, m6, m7, m1
+    movu                 m5, [srcq+ssq*0+0]
+    movu                 m6, [srcq+ssq*0+2]
+    HV_H_6TAP            m1, m5, m6, m7
+    mova                 m5, [rsp+16*7]
+    REPX      {paddd x, m5}, m2, m3, m4, m0, m1
+    REPX      {psrad x, 6 }, m2, m4, m3, m0, m1
+    packssdw             m2, m4     ; 0 2
+    packssdw             m3, m0     ; 1 3
+    packssdw             m4, m1     ; 2 4
+    punpcklwd            m0, m2, m3 ; 01
+    punpckhwd            m2, m3     ; 23
+    punpcklwd            m1, m3, m4 ; 12
+    punpckhwd            m3, m4     ; 34
+.hv_w8_loop:
+    mova                 m5, [rsp+16*3]
+    mova                 m6, [rsp+16*4]
+    pmaddwd              m4, m0, m5 ; a0
+    pmaddwd              m5, m1     ; b0
+    mova                 m0, m2
+    pmaddwd              m2, m6     ; a1
+    mova                 m1, m3
+    pmaddwd              m3, m6     ; b1
+    paddd                m4, m2
+    movu                 m2, [srcq+ssq*1+0]
+    paddd                m5, m3
+    movu                 m3, [srcq+ssq*1+2]
+    lea                srcq, [srcq+ssq*2]
+    HV_H_6TAP            m6, m2, m3
+    movu                 m2, [srcq+ssq*0+0]
+    movu                 m3, [srcq+ssq*0+2]
+    HV_H_6TAP            m7, m2, m3
+    mova                 m2, [rsp+16*7]
+    psrad                m3, m1, 16
+    REPX      {paddd x, m2}, m6, m7, m4, m5
+    psrad                m6, 6
+    psrad                m7, 6
+    packssdw             m3, m6     ; 4 5
+    packssdw             m6, m7     ; 5 6
+    mova                 m7, [rsp+16*5]
+    punpcklwd            m2, m3, m6 ; 45
+    punpckhwd            m3, m6     ; 56
+    pmaddwd              m6, m2, m7 ; a2
+    pmaddwd              m7, m3     ; b2
+    paddd                m4, m6
+    paddd                m5, m7
+    psrad                m4, 6
+    psrad                m5, 6
+    packssdw             m4, m5
+    movq        [tmpq+wq*0], m4
+    movhps      [tmpq+wq*2], m4
+    lea                tmpq, [tmpq+wq*4]
+    sub                  hd, 2
+    jg .hv_w8_loop
+    mov                srcq, srcmp
+    mov                tmpq, tmpmp
+    movzx                hd, r5w
+    add                srcq, 8
+    add                tmpq, 8
+    sub                 r5d, 1<<16
+%else
+    lea                 r8d, [wq*4-(1<<5)]
+    lea                 r8d, [hq+r8*8]
+.hv_w8_loop0:
+    mova                 m5, [spel_h_shufA]
+    movu                 m0, [srcq+r6*2+ 0]
+    mova                 m6, [rsp+16*0]
+    movu                 m1, [srcq+r6*2+ 8]
+    mova                 m7, [rsp+16*1]
+    movu                 m2, [srcq+r6*2+16]
+    mova                 m8, [rsp+16*2]
+    HV_H_6TAP            m9, m0, m1, m2, 6, m5, m6, m7, m8
+    movu                 m0, [srcq+r6*1+ 0]
+    movu                 m1, [srcq+r6*1+ 8]
+    movu                 m2, [srcq+r6*1+16]
+    lea                  r5, [srcq+ssq*2]
+    HV_H_6TAP           m11, m0, m1, m2, 6, m5, m6, m7, m8
+    movu                 m0, [srcq+ssq*0+ 0]
+    movu                 m1, [srcq+ssq*0+ 8]
+    movu                 m2, [srcq+ssq*0+16]
+    mov                  r7, tmpq
+    HV_H_6TAP           m13, m0, m1, m2, 6, m5, m6, m7, m8
+    movu                 m0, [srcq+ssq*1+ 0]
+    movu                 m1, [srcq+ssq*1+ 8]
+    movu                 m2, [srcq+ssq*1+16]
+    HV_H_6TAP           m15, m0, m1, m2, 6, m5, m6, m7, m8
+    movu                 m0, [r5+ssq*0+ 0]
+    movu                 m1, [r5+ssq*0+ 8]
+    movu                 m2, [r5+ssq*0+16]
+    HV_H_6TAP            m5, m0, m1, m2, 6, m5, m6, m7, m8
+    punpcklwd            m8, m9, m11  ; 01
+    punpckhwd            m9, m11
+    punpcklwd           m10, m11, m13 ; 12
+    punpckhwd           m11, m13
+    punpcklwd           m12, m13, m15 ; 23
+    punpckhwd           m13, m15
+    punpcklwd           m14, m15, m5  ; 34
+    punpckhwd           m15, m5
+.hv_w8_loop:
+    mova                 m3, [rsp+16*3]
+    mova                 m7, [rsp+16*4]
+    pmaddwd              m0, m8, m3   ; a0
+    mova                 m8, m12
+    pmaddwd              m2, m9, m3   ; a0'
+    mova                 m9, m13
+    pmaddwd              m1, m10, m3  ; b0
+    mova                m10, m14
+    pmaddwd              m3, m11      ; b0'
+    mova                m11, m15
+    REPX    {pmaddwd x, m7}, m12, m13, m14, m15
+    movu                 m6, [r5+ssq*1+ 0]
+    paddd                m0, m12
+    movu                 m7, [r5+ssq*1+ 8]
+    paddd                m2, m13
+    movu                m12, [r5+ssq*1+16]
+    paddd                m1, m14
+    lea                  r5, [r5+ssq*2]
+    paddd                m3, m15
+    HV_H_6TAP           m15, m6, m7, m12, 6
+    movu                 m6, [r5+ssq*0+ 0]
+    movu                 m7, [r5+ssq*0+ 8]
+    movu                m14, [r5+ssq*0+16]
+    punpcklwd           m12, m5, m15 ; 45
+    punpckhwd           m13, m5, m15
+    HV_H_6TAP            m5, m6, m7, m14, 6
+    mova                 m7, [rsp+16*5]
+    REPX      {paddd x, m4}, m0, m2, m1, m3
+    punpcklwd           m14, m15, m5  ; 56
+    punpckhwd           m15, m5
+    pmaddwd              m6, m12, m7  ; a2
+    paddd                m0, m6
+    pmaddwd              m6, m13, m7  ; a2'
+    paddd                m2, m6
+    pmaddwd              m6, m14, m7  ; b2
+    pmaddwd              m7, m15      ; b2'
+    paddd                m1, m6
+    paddd                m3, m7
+    REPX       {psrad x, 6}, m0, m2, m1, m3
+    packssdw             m0, m2
+    packssdw             m1, m3
+    mova          [r7+wq*0], m0
+    mova          [r7+wq*2], m1
+    lea                  r7, [r7+wq*4]
+    sub                  hd, 2
+    jg .hv_w8_loop
+    add                srcq, 16
+    add                tmpq, 16
+    movzx                hd, r8b
+    sub                 r8d, 1<<8
+%endif
+    jg .hv_w8_loop0
+    RET
 
-%define PREP_8TAP_FN FN prep_8tap,
+PREP_8TAP_FN smooth_sharp,   SMOOTH,  SHARP,   prep_8tap_16bpc
+PREP_8TAP_FN sharp_smooth,   SHARP,   SMOOTH,  prep_8tap_16bpc
+PREP_8TAP_FN regular_sharp,  REGULAR, SHARP,   prep_8tap_16bpc
+PREP_8TAP_FN sharp_regular,  SHARP,   REGULAR, prep_8tap_16bpc
 PREP_8TAP_FN sharp,          SHARP,   SHARP
-PREP_8TAP_FN sharp_smooth,   SHARP,   SMOOTH
-PREP_8TAP_FN smooth_sharp,   SMOOTH,  SHARP
-PREP_8TAP_FN smooth,         SMOOTH,  SMOOTH
-PREP_8TAP_FN sharp_regular,  SHARP,   REGULAR
-PREP_8TAP_FN regular_sharp,  REGULAR, SHARP
-PREP_8TAP_FN smooth_regular, SMOOTH,  REGULAR
-PREP_8TAP_FN regular_smooth, REGULAR, SMOOTH
-PREP_8TAP_FN regular,        REGULAR, REGULAR
 
+cglobal prep_8tap_16bpc, 0, 8, 0, tmp, src, ss, w, h, mx, my
 %if ARCH_X86_32
-cglobal prep_8tap_16bpc, 0, 7, 8, tmp, src, ss, w, h, mx, my
-%define mxb r0b
-%define mxd r0
-%define mxq r0
-%define myb r2b
-%define myd r2
-%define myq r2
-%else
-cglobal prep_8tap_16bpc, 4, 8, 0, tmp, src, ss, w, h, mx, my
+    %define             mxb  r0b
+    %define             mxd  r0
+    %define             mxq  r0
+    %define             myb  r2b
+    %define             myd  r2
+    %define             myq  r2
+    %define              m8  [esp+16*0]
+    %define              m9  [esp+16*1]
+    %define             m10  [esp+16*2]
+    %define             m11  [esp+16*3]
+    %define             m12  [esp+16*4]
+    %define             m13  [esp+16*5]
+    %define             m14  [esp+16*6]
+    %define             m15  [esp+16*7]
 %endif
-%define base t2-prep_ssse3
     imul                mxd, mxm, 0x010101
     add                 mxd, t0d ; 8tap_h, mx, 4tap_h
     imul                myd, mym, 0x010101
@@ -2026,138 +3439,7 @@ cglobal prep_8tap_16bpc, 4, 8, 0, tmp, src, ss, w, h, mx, my
     jnz .h
     movifnidn            hd, hm
     test                myd, 0xf00
-    jnz .v
-    tzcnt                wd, wd
-    mov                 myd, r7m ; bitdepth_max
-    movzx                wd, word [base+prep_ssse3_table+wq*2]
-    mova                 m5, [base+pw_8192]
-    shr                 myd, 11
-    add                  wq, t2
-    movddup              m4, [base+prep_mul+myq*8]
-    movifnidn           ssq, ssmp
-    movifnidn          tmpq, tmpmp
-    lea                  r6, [ssq*3]
-%if WIN64
-    pop                  r7
-%endif
-    jmp                  wq
-.h:
-    test                myd, 0xf00
-    jnz .hv
-    movifnidn           ssq, r2mp
-    movifnidn            hd, r4m
-    movddup              m5, [base+prep_8tap_1d_rnd]
-    cmp                  wd, 4
-    jne .h_w8
-    movzx               mxd, mxb
-    movq                 m0, [base+subpel_filters+mxq*8]
-    mova                 m3, [base+spel_h_shufA]
-    mova                 m4, [base+spel_h_shufB]
-    movifnidn          tmpq, tmpmp
-    sub                srcq, 2
-    WIN64_SPILL_XMM       8
-    punpcklbw            m0, m0
-    psraw                m0, 8
-    test          dword r7m, 0x800
-    jnz .h_w4_12bpc
-    psllw                m0, 2
-.h_w4_12bpc:
-    pshufd               m6, m0, q1111
-    pshufd               m7, m0, q2222
-.h_w4_loop:
-    movu                 m1, [srcq+ssq*0]
-    movu                 m2, [srcq+ssq*1]
-    lea                srcq, [srcq+ssq*2]
-    pshufb               m0, m1, m3 ; 0 1 1 2 2 3 3 4
-    pshufb               m1, m4     ; 2 3 3 4 4 5 5 6
-    pmaddwd              m0, m6
-    pmaddwd              m1, m7
-    paddd                m0, m5
-    paddd                m0, m1
-    pshufb               m1, m2, m3
-    pshufb               m2, m4
-    pmaddwd              m1, m6
-    pmaddwd              m2, m7
-    paddd                m1, m5
-    paddd                m1, m2
-    psrad                m0, 4
-    psrad                m1, 4
-    packssdw             m0, m1
-    mova             [tmpq], m0
-    add                tmpq, 16
-    sub                  hd, 2
-    jg .h_w4_loop
-    RET
-.h_w8:
-    WIN64_SPILL_XMM      11
-    shr                 mxd, 16
-    movq                 m2, [base+subpel_filters+mxq*8]
-    mova                 m4, [base+spel_h_shufA]
-    mova                 m6, [base+spel_h_shufB]
-    movifnidn          tmpq, r0mp
-    add                  wd, wd
-    punpcklbw            m2, m2
-    add                srcq, wq
-    psraw                m2, 8
-    add                tmpq, wq
-    neg                  wq
-    test          dword r7m, 0x800
-    jnz .h_w8_12bpc
-    psllw                m2, 2
-.h_w8_12bpc:
-    pshufd               m7, m2, q0000
-%if ARCH_X86_32
-    ALLOC_STACK       -16*3
-    pshufd               m0, m2, q1111
-    pshufd               m1, m2, q2222
-    pshufd               m2, m2, q3333
-    mova                 m8, m0
-    mova                 m9, m1
-    mova                m10, m2
-%else
-    pshufd               m8, m2, q1111
-    pshufd               m9, m2, q2222
-    pshufd              m10, m2, q3333
-%endif
-.h_w8_loop0:
-    mov                  r6, wq
-.h_w8_loop:
-    movu                 m0, [srcq+r6- 6]
-    movu                 m1, [srcq+r6+ 2]
-    pshufb               m2, m0, m4  ; 0 1 1 2 2 3 3 4
-    pshufb               m0, m6      ; 2 3 3 4 4 5 5 6
-    pmaddwd              m2, m7      ; abcd0
-    pmaddwd              m0, m8      ; abcd1
-    pshufb               m3, m1, m4  ; 4 5 5 6 6 7 7 8
-    pshufb               m1, m6      ; 6 7 7 8 8 9 9 a
-    paddd                m2, m5
-    paddd                m0, m2
-    pmaddwd              m2, m9, m3  ; abcd2
-    pmaddwd              m3, m7      ; efgh0
-    paddd                m0, m2
-    pmaddwd              m2, m10, m1 ; abcd3
-    pmaddwd              m1, m8      ; efgh1
-    paddd                m0, m2
-    movu                 m2, [srcq+r6+10]
-    paddd                m3, m5
-    paddd                m1, m3
-    pshufb               m3, m2, m4  ; a b b c c d d e
-    pshufb               m2, m6      ; 8 9 9 a a b b c
-    pmaddwd              m3, m9      ; efgh2
-    pmaddwd              m2, m10     ; efgh3
-    paddd                m1, m3
-    paddd                m1, m2
-    psrad                m0, 4
-    psrad                m1, 4
-    packssdw             m0, m1
-    mova          [tmpq+r6], m0
-    add                  r6, 16
-    jl .h_w8_loop
-    add                srcq, ssq
-    sub                tmpq, wq
-    dec                  hd
-    jg .h_w8_loop0
-    RET
+    jz mangle(private_prefix %+ _prep_6tap_16bpc_ssse3).prep
 .v:
     movzx               mxd, myb
     shr                 myd, 16
@@ -2315,6 +3597,125 @@ cglobal prep_8tap_16bpc, 4, 8, 0, tmp, src, ss, w, h, mx, my
     sub                  wd, 1<<8
     jg .v_loop0
     RET
+.h:
+    RESET_STACK_STATE
+    test                myd, 0xf00
+    jnz .hv
+    movifnidn           ssq, r2mp
+    movifnidn            hd, r4m
+    movddup              m5, [base+prep_8tap_1d_rnd]
+    cmp                  wd, 4
+    jne .h_w8
+.h_w4:
+    movzx               mxd, mxb
+    movq                 m0, [base+subpel_filters+mxq*8]
+    mova                 m3, [base+spel_h_shufA]
+    mova                 m4, [base+spel_h_shufB]
+    movifnidn          tmpq, tmpmp
+    sub                srcq, 2
+    WIN64_SPILL_XMM       8
+    punpcklbw            m0, m0
+    psraw                m0, 8
+    test          dword r7m, 0x800
+    jnz .h_w4_12bpc
+    psllw                m0, 2
+.h_w4_12bpc:
+    pshufd               m6, m0, q1111
+    pshufd               m7, m0, q2222
+.h_w4_loop:
+    movu                 m1, [srcq+ssq*0]
+    movu                 m2, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    pshufb               m0, m1, m3 ; 0 1 1 2 2 3 3 4
+    pshufb               m1, m4     ; 2 3 3 4 4 5 5 6
+    pmaddwd              m0, m6
+    pmaddwd              m1, m7
+    paddd                m0, m5
+    paddd                m0, m1
+    pshufb               m1, m2, m3
+    pshufb               m2, m4
+    pmaddwd              m1, m6
+    pmaddwd              m2, m7
+    paddd                m1, m5
+    paddd                m1, m2
+    psrad                m0, 4
+    psrad                m1, 4
+    packssdw             m0, m1
+    mova             [tmpq], m0
+    add                tmpq, 16
+    sub                  hd, 2
+    jg .h_w4_loop
+    RET
+.h_w8:
+    WIN64_SPILL_XMM      11
+    shr                 mxd, 16
+    movq                 m2, [base+subpel_filters+mxq*8]
+    mova                 m4, [base+spel_h_shufA]
+    mova                 m6, [base+spel_h_shufB]
+    movifnidn          tmpq, r0mp
+    add                  wd, wd
+    punpcklbw            m2, m2
+    add                srcq, wq
+    psraw                m2, 8
+    add                tmpq, wq
+    neg                  wq
+    test          dword r7m, 0x800
+    jnz .h_w8_12bpc
+    psllw                m2, 2
+.h_w8_12bpc:
+    pshufd               m7, m2, q0000
+%if ARCH_X86_32
+    ALLOC_STACK       -16*3
+    pshufd               m0, m2, q1111
+    pshufd               m1, m2, q2222
+    pshufd               m2, m2, q3333
+    mova                 m8, m0
+    mova                 m9, m1
+    mova                m10, m2
+%else
+    pshufd               m8, m2, q1111
+    pshufd               m9, m2, q2222
+    pshufd              m10, m2, q3333
+%endif
+.h_w8_loop0:
+    mov                  r6, wq
+.h_w8_loop:
+    movu                 m0, [srcq+r6- 6]
+    movu                 m1, [srcq+r6+ 2]
+    pshufb               m2, m0, m4  ; 0 1 1 2 2 3 3 4
+    pshufb               m0, m6      ; 2 3 3 4 4 5 5 6
+    pmaddwd              m2, m7      ; abcd0
+    pmaddwd              m0, m8      ; abcd1
+    pshufb               m3, m1, m4  ; 4 5 5 6 6 7 7 8
+    pshufb               m1, m6      ; 6 7 7 8 8 9 9 a
+    paddd                m2, m5
+    paddd                m0, m2
+    pmaddwd              m2, m9, m3  ; abcd2
+    pmaddwd              m3, m7      ; efgh0
+    paddd                m0, m2
+    pmaddwd              m2, m10, m1 ; abcd3
+    pmaddwd              m1, m8      ; efgh1
+    paddd                m0, m2
+    movu                 m2, [srcq+r6+10]
+    paddd                m3, m5
+    paddd                m1, m3
+    pshufb               m3, m2, m4  ; a b b c c d d e
+    pshufb               m2, m6      ; 8 9 9 a a b b c
+    pmaddwd              m3, m9      ; efgh2
+    pmaddwd              m2, m10     ; efgh3
+    paddd                m1, m3
+    paddd                m1, m2
+    psrad                m0, 4
+    psrad                m1, 4
+    packssdw             m0, m1
+    mova          [tmpq+r6], m0
+    add                  r6, 16
+    jl .h_w8_loop
+    add                srcq, ssq
+    sub                tmpq, wq
+    dec                  hd
+    jg .h_w8_loop0
+    RET
 .hv:
     RESET_STACK_STATE
     movzx               t3d, mxb
@@ -6427,16 +7828,18 @@ DECLARE_REG_TMP 6, 8
 %else
 DECLARE_REG_TMP 1, 2
 %endif
+
+%define PUT_8TAP_SCALED_FN FN put_8tap_scaled,
 BILIN_SCALED_FN put
-FN put_8tap_scaled, sharp,          SHARP,   SHARP
-FN put_8tap_scaled, sharp_smooth,   SHARP,   SMOOTH
-FN put_8tap_scaled, smooth_sharp,   SMOOTH,  SHARP
-FN put_8tap_scaled, smooth,         SMOOTH,  SMOOTH
-FN put_8tap_scaled, sharp_regular,  SHARP,   REGULAR
-FN put_8tap_scaled, regular_sharp,  REGULAR, SHARP
-FN put_8tap_scaled, smooth_regular, SMOOTH,  REGULAR
-FN put_8tap_scaled, regular_smooth, REGULAR, SMOOTH
-FN put_8tap_scaled, regular,        REGULAR, REGULAR
+PUT_8TAP_SCALED_FN sharp,          SHARP,   SHARP,   put_8tap_scaled_16bpc
+PUT_8TAP_SCALED_FN sharp_smooth,   SHARP,   SMOOTH,  put_8tap_scaled_16bpc
+PUT_8TAP_SCALED_FN smooth_sharp,   SMOOTH,  SHARP,   put_8tap_scaled_16bpc
+PUT_8TAP_SCALED_FN smooth,         SMOOTH,  SMOOTH,  put_8tap_scaled_16bpc
+PUT_8TAP_SCALED_FN sharp_regular,  SHARP,   REGULAR, put_8tap_scaled_16bpc
+PUT_8TAP_SCALED_FN regular_sharp,  REGULAR, SHARP,   put_8tap_scaled_16bpc
+PUT_8TAP_SCALED_FN smooth_regular, SMOOTH,  REGULAR, put_8tap_scaled_16bpc
+PUT_8TAP_SCALED_FN regular_smooth, REGULAR, SMOOTH,  put_8tap_scaled_16bpc
+PUT_8TAP_SCALED_FN regular,        REGULAR, REGULAR
 MC_8TAP_SCALED put
 
 %if WIN64
@@ -6446,16 +7849,18 @@ DECLARE_REG_TMP 6, 7
 %else
 DECLARE_REG_TMP 1, 2
 %endif
+
+%define PREP_8TAP_SCALED_FN FN prep_8tap_scaled,
 BILIN_SCALED_FN prep
-FN prep_8tap_scaled, sharp,          SHARP,   SHARP
-FN prep_8tap_scaled, sharp_smooth,   SHARP,   SMOOTH
-FN prep_8tap_scaled, smooth_sharp,   SMOOTH,  SHARP
-FN prep_8tap_scaled, smooth,         SMOOTH,  SMOOTH
-FN prep_8tap_scaled, sharp_regular,  SHARP,   REGULAR
-FN prep_8tap_scaled, regular_sharp,  REGULAR, SHARP
-FN prep_8tap_scaled, smooth_regular, SMOOTH,  REGULAR
-FN prep_8tap_scaled, regular_smooth, REGULAR, SMOOTH
-FN prep_8tap_scaled, regular,        REGULAR, REGULAR
+PREP_8TAP_SCALED_FN sharp,          SHARP,   SHARP,   prep_8tap_scaled_16bpc
+PREP_8TAP_SCALED_FN sharp_smooth,   SHARP,   SMOOTH,  prep_8tap_scaled_16bpc
+PREP_8TAP_SCALED_FN smooth_sharp,   SMOOTH,  SHARP,   prep_8tap_scaled_16bpc
+PREP_8TAP_SCALED_FN smooth,         SMOOTH,  SMOOTH,  prep_8tap_scaled_16bpc
+PREP_8TAP_SCALED_FN sharp_regular,  SHARP,   REGULAR, prep_8tap_scaled_16bpc
+PREP_8TAP_SCALED_FN regular_sharp,  REGULAR, SHARP,   prep_8tap_scaled_16bpc
+PREP_8TAP_SCALED_FN smooth_regular, SMOOTH,  REGULAR, prep_8tap_scaled_16bpc
+PREP_8TAP_SCALED_FN regular_smooth, REGULAR, SMOOTH,  prep_8tap_scaled_16bpc
+PREP_8TAP_SCALED_FN regular,        REGULAR, REGULAR
 MC_8TAP_SCALED prep
 
 %if ARCH_X86_64
diff --git a/src/x86/mc_sse.asm b/src/x86/mc_sse.asm
index a447a80..deeb6db 100644
--- a/src/x86/mc_sse.asm
+++ b/src/x86/mc_sse.asm
@@ -55,10 +55,12 @@ subpel_h_shuf4: db 0,  1,  2,  3,  1,  2,  3,  4,  8,  9, 10, 11,  9, 10, 11, 12
 subpel_h_shufA: db 0,  1,  2,  3,  1,  2,  3,  4,  2,  3,  4,  5,  3,  4,  5,  6
 subpel_h_shufB: db 4,  5,  6,  7,  5,  6,  7,  8,  6,  7,  8,  9,  7,  8,  9, 10
 subpel_h_shufC: db 8,  9, 10, 11,  9, 10, 11, 12, 10, 11, 12, 13, 11, 12, 13, 14
+subpel_h_shufD: db 0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8
+subpel_h_shufE: db 2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8,  8,  9,  9, 10
+subpel_h_shufF: db 4,  5,  5,  6,  6,  7,  7,  8,  8,  9,  9, 10, 10, 11, 11, 12
 subpel_s_shuf2: db 0,  1,  2,  3,  0,  1,  2,  3,  8,  9, 10, 11,  8,  9, 10, 11
 subpel_s_shuf8: db 0,  1,  8,  9,  2,  3, 10, 11,  4,  5, 12, 13,  6,  7, 14, 15
 bilin_h_shuf4:  db 0,  1,  1,  2,  2,  3,  3,  4,  8,  9,  9, 10, 10, 11, 11, 12
-bilin_h_shuf8:  db 0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,  8
 unpckw:         db 0,  1,  4,  5,  8,  9, 12, 13,  2,  3,  6,  7, 10, 11, 14, 15
 rescale_mul:    dd 0,  1,  2,  3
 resize_shuf:    db 0,  0,  0,  0,  0,  1,  2,  3,  4,  5,  6,  7,  7,  7,  7,  7
@@ -238,7 +240,6 @@ BIDIR_JMP_TABLE blend_h, ssse3, 2, 4, 8, 16, 16, 16, 16
     %endrep
 %endmacro
 
-%xdefine prep_sse2 mangle(private_prefix %+ _prep_bilin_8bpc_sse2.prep)
 %xdefine put_ssse3 mangle(private_prefix %+ _put_bilin_8bpc_ssse3.put)
 %xdefine prep_ssse3 mangle(private_prefix %+ _prep_bilin_8bpc_ssse3.prep)
 
@@ -277,9 +278,6 @@ BASE_JMP_TABLE prep, ssse3,    4, 8, 16, 32, 64, 128
     %endif
 %endmacro
 
-HV_JMP_TABLE prep,  8tap,  sse2, 1,    4, 8, 16, 32, 64, 128
-HV_JMP_TABLE prep, bilin,  sse2, 7,    4, 8, 16, 32, 64, 128
-HV_JMP_TABLE put,   8tap, ssse3, 3, 2, 4, 8, 16, 32, 64, 128
 HV_JMP_TABLE prep,  8tap, ssse3, 1,    4, 8, 16, 32, 64, 128
 HV_JMP_TABLE put,  bilin, ssse3, 7, 2, 4, 8, 16, 32, 64, 128
 HV_JMP_TABLE prep, bilin, ssse3, 7,    4, 8, 16, 32, 64, 128
@@ -442,7 +440,7 @@ cglobal put_bilin_8bpc, 1, 8, 0, dst, ds, src, ss, w, h, mxy
     ; (16 * src[x] + (mx * (src[x + 1] - src[x])) + 8) >> 4
     ; = ((16 - mx) * src[x] + mx * src[x + 1] + 8) >> 4
     imul               mxyd, 0x00ff00ff
-    mova                 m4, [base+bilin_h_shuf8]
+    mova                 m4, [base+subpel_h_shufD]
     mova                 m0, [base+bilin_h_shuf4]
     add                mxyd, 0x00100010
     movd                 m5, mxyd
@@ -900,56 +898,6 @@ cglobal put_bilin_8bpc, 1, 8, 0, dst, ds, src, ss, w, h, mxy
 %endif
     RET
 
-%macro PSHUFB_BILIN_H8 2 ; dst, src
- %if cpuflag(ssse3)
-    pshufb               %1, %2
- %else
-    psrldq               %2, %1, 1
-    punpcklbw            %1, %2
- %endif
-%endmacro
-
-%macro PSHUFB_BILIN_H4 3 ; dst, src, tmp
- %if cpuflag(ssse3)
-    pshufb               %1, %2
- %else
-    psrldq               %2, %1, 1
-    punpckhbw            %3, %1, %2
-    punpcklbw            %1, %2
-    punpcklqdq           %1, %3
- %endif
-%endmacro
-
-%macro PMADDUBSW 5 ; dst/src1, src2, zero, tmp, reset_zero
- %if cpuflag(ssse3)
-    pmaddubsw            %1, %2
- %else
-  %if %5 == 1
-    pxor                 %3, %3
-  %endif
-    punpckhbw            %4, %1, %3
-    punpcklbw            %1, %1, %3
-    pmaddwd              %4, %2
-    pmaddwd              %1, %2
-    packssdw             %1, %4
- %endif
-%endmacro
-
-%macro PMULHRSW 5 ; dst, src, tmp, rndval, shift
- %if cpuflag(ssse3)
-    pmulhrsw             %1, %2
- %else
-    punpckhwd            %3, %1, %4
-    punpcklwd            %1, %4
-    pmaddwd              %3, %2
-    pmaddwd              %1, %2
-    psrad                %3, %5
-    psrad                %1, %5
-    packssdw             %1, %3
- %endif
-%endmacro
-
-%macro PREP_BILIN 0
 %if ARCH_X86_32
     %define base r6-prep%+SUFFIX
 %else
@@ -958,7 +906,7 @@ cglobal put_bilin_8bpc, 1, 8, 0, dst, ds, src, ss, w, h, mxy
 
 cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     movifnidn          mxyd, r5m ; mx
-    LEA                  r6, prep%+SUFFIX
+    LEA                  r6, prep_ssse3
     tzcnt                wd, wm
     movifnidn            hd, hm
     test               mxyd, mxyd
@@ -967,10 +915,6 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     test               mxyd, mxyd
     jnz .v
 .prep:
-%if notcpuflag(ssse3)
-    add                  r6, prep_ssse3 - prep_sse2
-    jmp prep_ssse3
-%else
     movzx                wd, word [r6+wq*2+table_offset(prep,)]
     pxor                 m4, m4
     add                  wq, r6
@@ -1070,34 +1014,22 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     dec                  hd
     jg .prep_w32_vloop
     RET
-%endif
 .h:
     ; 16 * src[x] + (mx * (src[x + 1] - src[x]))
     ; = (16 - mx) * src[x] + mx * src[x + 1]
-%if cpuflag(ssse3)
     imul               mxyd, 0x00ff00ff
-    mova                 m4, [base+bilin_h_shuf8]
+    mova                 m4, [base+subpel_h_shufD]
     add                mxyd, 0x00100010
-%else
-    imul               mxyd, 0xffff
-    add                mxyd, 16
-%endif
     movd                 m5, mxyd
     mov                mxyd, r6m ; my
     pshufd               m5, m5, q0000
     test               mxyd, mxyd
     jnz .hv
     movzx                wd, word [r6+wq*2+table_offset(prep, _bilin_h)]
-%if notcpuflag(ssse3)
-    WIN64_SPILL_XMM 8
-    pxor                 m6, m6
-%endif
     add                  wq, r6
     jmp                  wq
 .h_w4:
-%if cpuflag(ssse3)
     mova                 m4, [base+bilin_h_shuf4]
-%endif
     lea            stride3q, [strideq*3]
 .h_w4_loop:
     movq                 m0, [srcq+strideq*0]
@@ -1105,10 +1037,10 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     movq                 m1, [srcq+strideq*2]
     movhps               m1, [srcq+stride3q ]
     lea                srcq, [srcq+strideq*4]
-    PSHUFB_BILIN_H4      m0, m4, m2
-    PMADDUBSW            m0, m5, m6, m2, 0
-    PSHUFB_BILIN_H4      m1, m4, m2
-    PMADDUBSW            m1, m5, m6, m2, 0
+    pshufb               m0, m4
+    pshufb               m1, m4
+    pmaddubsw            m0, m5
+    pmaddubsw            m1, m5
     mova          [tmpq+0 ], m0
     mova          [tmpq+16], m1
     add                tmpq, 32
@@ -1123,14 +1055,8 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     movu                 m2, [srcq+strideq*2]
     movu                 m3, [srcq+stride3q ]
     lea                srcq, [srcq+strideq*4]
-    PSHUFB_BILIN_H8      m0, m4
-    PSHUFB_BILIN_H8      m1, m4
-    PSHUFB_BILIN_H8      m2, m4
-    PSHUFB_BILIN_H8      m3, m4
-    PMADDUBSW            m0, m5, m6, m7, 0
-    PMADDUBSW            m1, m5, m6, m7, 0
-    PMADDUBSW            m2, m5, m6, m7, 0
-    PMADDUBSW            m3, m5, m6, m7, 0
+    REPX  {pshufb    x, m4}, m0, m1, m2, m3
+    REPX  {pmaddubsw x, m5}, m0, m1, m2, m3
     mova        [tmpq+16*0], m0
     mova        [tmpq+16*1], m1
     mova        [tmpq+16*2], m2
@@ -1145,14 +1071,8 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     movu                 m2, [srcq+strideq*1+8*0]
     movu                 m3, [srcq+strideq*1+8*1]
     lea                srcq, [srcq+strideq*2]
-    PSHUFB_BILIN_H8      m0, m4
-    PSHUFB_BILIN_H8      m1, m4
-    PSHUFB_BILIN_H8      m2, m4
-    PSHUFB_BILIN_H8      m3, m4
-    PMADDUBSW            m0, m5, m6, m7, 0
-    PMADDUBSW            m1, m5, m6, m7, 0
-    PMADDUBSW            m2, m5, m6, m7, 0
-    PMADDUBSW            m3, m5, m6, m7, 0
+    REPX  {pshufb    x, m4}, m0, m1, m2, m3
+    REPX  {pmaddubsw x, m5}, m0, m1, m2, m3
     mova        [tmpq+16*0], m0
     mova        [tmpq+16*1], m1
     mova        [tmpq+16*2], m2
@@ -1178,14 +1098,8 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     movu                 m1, [srcq+r6+8*1]
     movu                 m2, [srcq+r6+8*2]
     movu                 m3, [srcq+r6+8*3]
-    PSHUFB_BILIN_H8      m0, m4
-    PSHUFB_BILIN_H8      m1, m4
-    PSHUFB_BILIN_H8      m2, m4
-    PSHUFB_BILIN_H8      m3, m4
-    PMADDUBSW            m0, m5, m6, m7, 0
-    PMADDUBSW            m1, m5, m6, m7, 0
-    PMADDUBSW            m2, m5, m6, m7, 0
-    PMADDUBSW            m3, m5, m6, m7, 0
+    REPX  {pshufb    x, m4}, m0, m1, m2, m3
+    REPX  {pmaddubsw x, m5}, m0, m1, m2, m3
     mova        [tmpq+16*0], m0
     mova        [tmpq+16*1], m1
     mova        [tmpq+16*2], m2
@@ -1198,18 +1112,9 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     jg .h_w32_vloop
     RET
 .v:
-%if notcpuflag(ssse3)
-    WIN64_SPILL_XMM 8
-%endif
     movzx                wd, word [r6+wq*2+table_offset(prep, _bilin_v)]
-%if cpuflag(ssse3)
     imul               mxyd, 0x00ff00ff
     add                mxyd, 0x00100010
-%else
-    imul               mxyd, 0xffff
-    pxor                 m6, m6
-    add                mxyd, 16
-%endif
     add                  wq, r6
     lea            stride3q, [strideq*3]
     movd                 m5, mxyd
@@ -1225,13 +1130,13 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     punpckldq            m0, m1
     punpckldq            m1, m2
     punpcklbw            m0, m1 ; 01 12
-    PMADDUBSW            m0, m5, m6, m7, 0
+    pmaddubsw            m0, m5
     mova        [tmpq+16*0], m0
     movd                 m0, [srcq+strideq*0]
     punpckldq            m2, m3
     punpckldq            m3, m0
     punpcklbw            m2, m3 ; 23 34
-    PMADDUBSW            m2, m5, m6, m7, 0
+    pmaddubsw            m2, m5
     mova        [tmpq+16*1], m2
     add                tmpq, 16*2
     sub                  hd, 4
@@ -1246,15 +1151,15 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     lea                srcq, [srcq+strideq*4]
     punpcklbw            m0, m1 ; 01
     punpcklbw            m1, m2 ; 12
-    PMADDUBSW            m0, m5, m6, m7, 0
-    PMADDUBSW            m1, m5, m6, m7, 0
+    pmaddubsw            m0, m5
+    pmaddubsw            m1, m5
     mova        [tmpq+16*0], m0
     movq                 m0, [srcq+strideq*0]
     punpcklbw            m2, m3 ; 23
     punpcklbw            m3, m0 ; 34
-    PMADDUBSW            m2, m5, m6, m7, 0
+    pmaddubsw            m2, m5
     mova        [tmpq+16*1], m1
-    PMADDUBSW            m3, m5, m6, m7, 0
+    pmaddubsw            m3, m5
     mova        [tmpq+16*2], m2
     mova        [tmpq+16*3], m3
     add                tmpq, 16*4
@@ -1270,27 +1175,27 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     lea                srcq, [srcq+strideq*4]
     punpcklbw            m4, m0, m1
     punpckhbw            m0, m1
-    PMADDUBSW            m4, m5, m6, m7, 0
-    PMADDUBSW            m0, m5, m6, m7, 0
+    pmaddubsw            m4, m5
+    pmaddubsw            m0, m5
     mova        [tmpq+16*0], m4
     punpcklbw            m4, m1, m2
     punpckhbw            m1, m2
-    PMADDUBSW            m4, m5, m6, m7, 0
+    pmaddubsw            m4, m5
     mova        [tmpq+16*1], m0
     movu                 m0, [srcq+strideq*0]
-    PMADDUBSW            m1, m5, m6, m7, 0
+    pmaddubsw            m1, m5
     mova        [tmpq+16*2], m4
     punpcklbw            m4, m2, m3
     punpckhbw            m2, m3
-    PMADDUBSW            m4, m5, m6, m7, 0
+    pmaddubsw            m4, m5
     mova        [tmpq+16*3], m1
-    PMADDUBSW            m2, m5, m6, m7, 0
+    pmaddubsw            m2, m5
     mova        [tmpq+16*4], m4
     punpcklbw            m4, m3, m0
     punpckhbw            m3, m0
-    PMADDUBSW            m4, m5, m6, m7, 0
+    pmaddubsw            m4, m5
     mova        [tmpq+16*5], m2
-    PMADDUBSW            m3, m5, m6, m7, 0
+    pmaddubsw            m3, m5
     mova        [tmpq+16*6], m4
     mova        [tmpq+16*7], m3
     add                tmpq, 16*8
@@ -1325,29 +1230,29 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     lea                srcq, [srcq+strideq*2]
     punpcklbw            m4, m0, m2
     punpckhbw            m0, m2
-    PMADDUBSW            m4, m5, m6, m7, 0
-    PMADDUBSW            m0, m5, m6, m7, 0
+    pmaddubsw            m4, m5
+    pmaddubsw            m0, m5
     mova        [tmpq+16*0], m4
     mova        [tmpq+16*1], m0
     movu                 m0, [srcq+strideq*0+16*0]
     punpcklbw            m4, m1, m3
     punpckhbw            m1, m3
-    PMADDUBSW            m4, m5, m6, m7, 0
-    PMADDUBSW            m1, m5, m6, m7, 0
+    pmaddubsw            m4, m5
+    pmaddubsw            m1, m5
     mova        [tmpq+16*2], m4
     mova        [tmpq+16*3], m1
     movu                 m1, [srcq+strideq*0+16*1]
     add                tmpq, r6
     punpcklbw            m4, m2, m0
     punpckhbw            m2, m0
-    PMADDUBSW            m4, m5, m6, m7, 0
-    PMADDUBSW            m2, m5, m6, m7, 0
+    pmaddubsw            m4, m5
+    pmaddubsw            m2, m5
     mova        [tmpq+16*0], m4
     mova        [tmpq+16*1], m2
     punpcklbw            m4, m3, m1
     punpckhbw            m3, m1
-    PMADDUBSW            m4, m5, m6, m7, 0
-    PMADDUBSW            m3, m5, m6, m7, 0
+    pmaddubsw            m4, m5
+    pmaddubsw            m3, m5
     mova        [tmpq+16*2], m4
     mova        [tmpq+16*3], m3
     add                tmpq, r6
@@ -1374,51 +1279,36 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     ; (16 * src[x] + (my * (src[x + src_stride] - src[x])) + 8) >> 4
     ; = src[x] + (((my * (src[x + src_stride] - src[x])) + 8) >> 4)
     movzx                wd, word [r6+wq*2+table_offset(prep, _bilin_hv)]
-%if cpuflag(ssse3)
     imul               mxyd, 0x08000800
     WIN64_SPILL_XMM 8
-%else
-    or                 mxyd, 1<<16
-    WIN64_SPILL_XMM 9
- %if ARCH_X86_64
-    mova                 m8, [base+pw_8]
- %else
-  %define                m8  [base+pw_8]
- %endif
-    pxor                 m7, m7
-%endif
     movd                 m6, mxyd
     add                  wq, r6
     pshufd               m6, m6, q0000
     jmp                  wq
 .hv_w4:
-%if cpuflag(ssse3)
     mova                 m4, [base+bilin_h_shuf4]
     movddup              m0, [srcq+strideq*0]
-%else
-    movhps               m0, [srcq+strideq*0]
-%endif
     lea                  r3, [strideq*3]
-    PSHUFB_BILIN_H4      m0, m4, m3
-    PMADDUBSW            m0, m5, m7, m4, 0 ; _ 0
+    pshufb               m0, m4
+    pmaddubsw            m0, m5            ; _ 0
 .hv_w4_loop:
     movq                 m1, [srcq+strideq*1]
     movhps               m1, [srcq+strideq*2]
     movq                 m2, [srcq+r3       ]
     lea                srcq, [srcq+strideq*4]
     movhps               m2, [srcq+strideq*0]
-    PSHUFB_BILIN_H4      m1, m4, m3
-    PSHUFB_BILIN_H4      m2, m4, m3
-    PMADDUBSW            m1, m5, m7, m4, 0 ; 1 2
-    PMADDUBSW            m2, m5, m7, m4, 0 ; 3 4
+    pshufb               m1, m4
+    pshufb               m2, m4
+    pmaddubsw            m1, m5            ; 1 2
+    pmaddubsw            m2, m5            ; 3 4
     shufpd               m0, m1, 0x01      ; 0 1
     shufpd               m3, m1, m2, 0x01  ; 2 3
     psubw                m1, m0
-    PMULHRSW             m1, m6, m4, m8, 4
+    pmulhrsw             m1, m6
     paddw                m1, m0
     mova                 m0, m2
     psubw                m2, m3
-    PMULHRSW             m2, m6, m4, m8, 4
+    pmulhrsw             m2, m6
     paddw                m2, m3
     mova        [tmpq+16*0], m1
     mova        [tmpq+16*1], m2
@@ -1428,22 +1318,22 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     RET
 .hv_w8:
     movu                 m0, [srcq+strideq*0]
-    PSHUFB_BILIN_H8      m0, m4
-    PMADDUBSW            m0, m5, m7, m4, 0 ; 0
+    pshufb               m0, m4
+    pmaddubsw            m0, m5 ; 0
 .hv_w8_loop:
     movu                 m1, [srcq+strideq*1]
     lea                srcq, [srcq+strideq*2]
     movu                 m2, [srcq+strideq*0]
-    PSHUFB_BILIN_H8      m1, m4
-    PSHUFB_BILIN_H8      m2, m4
-    PMADDUBSW            m1, m5, m7, m4, 0 ; 1
-    PMADDUBSW            m2, m5, m7, m4, 0 ; 2
+    pshufb               m1, m4
+    pshufb               m2, m4
+    pmaddubsw            m1, m5 ; 1
+    pmaddubsw            m2, m5 ; 2
     psubw                m3, m1, m0
-    PMULHRSW             m3, m6, m4, m8, 4
+    pmulhrsw             m3, m6
     paddw                m3, m0
     mova                 m0, m2
     psubw                m2, m1
-    PMULHRSW             m2, m6, m4, m8, 4
+    pmulhrsw             m2, m6
     paddw                m2, m1
     mova        [tmpq+16*0], m3
     mova        [tmpq+16*1], m2
@@ -1467,9 +1357,7 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     xor                 r3d, r3d
     mov                 r5d, 32
 .hv_w16_start:
-%if ARCH_X86_64 || cpuflag(ssse3)
     mov                  r6, srcq
-%endif
 %if ARCH_X86_64
  %if WIN64
     PUSH                 r7
@@ -1479,39 +1367,39 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
 .hv_w16_hloop:
     movu                 m0, [srcq+strideq*0+8*0]
     movu                 m1, [srcq+strideq*0+8*1]
-    PSHUFB_BILIN_H8      m0, m4
-    PSHUFB_BILIN_H8      m1, m4
-    PMADDUBSW            m0, m5, m7, m4, 0 ; 0a
-    PMADDUBSW            m1, m5, m7, m4, 0 ; 0b
+    pshufb               m0, m4
+    pshufb               m1, m4
+    pmaddubsw            m0, m5 ; 0a
+    pmaddubsw            m1, m5 ; 0b
 .hv_w16_vloop:
     movu                 m2, [srcq+strideq*1+8*0]
-    PSHUFB_BILIN_H8      m2, m4
-    PMADDUBSW            m2, m5, m7, m4, 0 ; 1a
+    pshufb               m2, m4
+    pmaddubsw            m2, m5 ; 1a
     psubw                m3, m2, m0
-    PMULHRSW             m3, m6, m4, m8, 4
+    pmulhrsw             m3, m6
     paddw                m3, m0
     mova        [tmpq+16*0], m3
     movu                 m3, [srcq+strideq*1+8*1]
     lea                srcq, [srcq+strideq*2]
-    PSHUFB_BILIN_H8      m3, m4
-    PMADDUBSW            m3, m5, m7, m4, 0 ; 1b
+    pshufb               m3, m4
+    pmaddubsw            m3, m5 ; 1b
     psubw                m0, m3, m1
-    PMULHRSW             m0, m6, m4, m8, 4
+    pmulhrsw             m0, m6
     paddw                m0, m1
     mova        [tmpq+16*1], m0
     add                tmpq, r5
     movu                 m0, [srcq+strideq*0+8*0]
-    PSHUFB_BILIN_H8      m0, m4
-    PMADDUBSW            m0, m5, m7, m4, 0 ; 2a
+    pshufb               m0, m4
+    pmaddubsw            m0, m5 ; 2a
     psubw                m1, m0, m2
-    PMULHRSW             m1, m6, m4, m8, 4
+    pmulhrsw             m1, m6
     paddw                m1, m2
     mova        [tmpq+16*0], m1
     movu                 m1, [srcq+strideq*0+8*1]
-    PSHUFB_BILIN_H8      m1, m4
-    PMADDUBSW            m1, m5, m7, m4, 0 ; 2b
+    pshufb               m1, m4
+    pmaddubsw            m1, m5 ; 2b
     psubw                m2, m1, m3
-    PMULHRSW             m2, m6, m4, m8, 4
+    pmulhrsw             m2, m6
     paddw                m2, m3
     mova        [tmpq+16*1], m2
     add                tmpq, r5
@@ -1523,19 +1411,12 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     add                  r7, 2*16
     mov                srcq, r6
     mov                tmpq, r7
-%elif cpuflag(ssse3)
+%else
     mov                tmpq, tmpm
     add                  r6, 16
     add                tmpq, 2*16
     mov                srcq, r6
     mov                tmpm, tmpq
-%else
-    mov                srcq, srcm
-    mov                tmpq, tmpm
-    add                srcq, 16
-    add                tmpq, 2*16
-    mov                srcm, srcq
-    mov                tmpm, tmpq
 %endif
     sub                 r3d, 1<<8
     jg .hv_w16_hloop
@@ -1543,14 +1424,13 @@ cglobal prep_bilin_8bpc, 3, 7, 0, tmp, src, stride, w, h, mxy, stride3
     POP                  r7
 %endif
     RET
-%endmacro
 
 ; int8_t subpel_filters[5][15][8]
 %assign FILTER_REGULAR (0*15 << 16) | 3*15
 %assign FILTER_SMOOTH  (1*15 << 16) | 4*15
 %assign FILTER_SHARP   (2*15 << 16) | 3*15
 
-%macro FN 4 ; prefix, type, type_h, type_v
+%macro FN 4-5 ; prefix, type, type_h, type_v, jmp_to
 cglobal %1_%2_8bpc
     mov                 t0d, FILTER_%3
 %ifidn %3, %4
@@ -1558,8 +1438,8 @@ cglobal %1_%2_8bpc
 %else
     mov                 t1d, FILTER_%4
 %endif
-%ifnidn %2, regular ; skip the jump in the last filter
-    jmp mangle(private_prefix %+ _%1_8bpc %+ SUFFIX)
+%if %0 == 5 ; skip the jump in the last filter
+    jmp mangle(private_prefix %+ _%5 %+ SUFFIX)
 %endif
 %endmacro
 
@@ -1571,16 +1451,6 @@ DECLARE_REG_TMP 4, 5
 DECLARE_REG_TMP 7, 8
 %endif
 
-FN put_8tap, sharp,          SHARP,   SHARP
-FN put_8tap, sharp_smooth,   SHARP,   SMOOTH
-FN put_8tap, smooth_sharp,   SMOOTH,  SHARP
-FN put_8tap, smooth,         SMOOTH,  SMOOTH
-FN put_8tap, sharp_regular,  SHARP,   REGULAR
-FN put_8tap, regular_sharp,  REGULAR, SHARP
-FN put_8tap, smooth_regular, SMOOTH,  REGULAR
-FN put_8tap, regular_smooth, REGULAR, SMOOTH
-FN put_8tap, regular,        REGULAR, REGULAR
-
 %if ARCH_X86_32
  %define base_reg r1
  %define base base_reg-put_ssse3
@@ -1589,7 +1459,13 @@ FN put_8tap, regular,        REGULAR, REGULAR
  %define base 0
 %endif
 
-cglobal put_8tap_8bpc, 1, 9, 0, dst, ds, src, ss, w, h, mx, my, ss3
+%define PUT_8TAP_FN FN put_8tap,
+PUT_8TAP_FN smooth,         SMOOTH,  SMOOTH,  put_6tap_8bpc
+PUT_8TAP_FN smooth_regular, SMOOTH,  REGULAR, put_6tap_8bpc
+PUT_8TAP_FN regular_smooth, REGULAR, SMOOTH,  put_6tap_8bpc
+PUT_8TAP_FN regular,        REGULAR, REGULAR
+
+cglobal put_6tap_8bpc, 1, 9, 0, dst, ds, src, ss, w, h, mx, my, ns
     imul                mxd, mxm, 0x010101
     add                 mxd, t0d ; 8tap_h, mx, 4tap_h
 %if ARCH_X86_64
@@ -1611,12 +1487,12 @@ cglobal put_8tap_8bpc, 1, 9, 0, dst, ds, src, ss, w, h, mx, my, ss3
     test                myd, 0xf00
 %endif
     jnz .v
+.put:
     tzcnt                wd, wd
     movzx                wd, word [base_reg+wq*2+table_offset(put,)]
+    movifnidn           ssq, ssmp
     add                  wq, base_reg
-; put_bilin mangling jump
     movifnidn           dsq, dsmp
-    movifnidn           ssq, ssmp
 %if WIN64
     pop                  r8
 %endif
@@ -1630,118 +1506,56 @@ cglobal put_8tap_8bpc, 1, 9, 0, dst, ds, src, ss, w, h, mx, my, ss3
 %endif
     jnz .hv
     movifnidn           ssq, ssmp
-    WIN64_SPILL_XMM      12
+    mova                 m5, [base+pw_34] ; 2 + (8 << 2)
     cmp                  wd, 4
-    jl .h_w2
-    je .h_w4
-    tzcnt                wd, wd
+    jle mangle(private_prefix %+ _put_8tap_8bpc %+ SUFFIX).h_w4
+    WIN64_SPILL_XMM      11
 %if ARCH_X86_64
-    mova                m10, [base+subpel_h_shufA]
-    mova                m11, [base+subpel_h_shufB]
-    mova                 m9, [base+subpel_h_shufC]
+    mova                 m8, [base+subpel_h_shufD]
+    mova                 m9, [base+subpel_h_shufE]
+    mova                m10, [base+subpel_h_shufF]
 %endif
     shr                 mxd, 16
-    sub                srcq, 3
-    movzx                wd, word [base_reg+wq*2+table_offset(put, _8tap_h)]
-    movq                 m6, [base_reg+mxq*8+subpel_filters-put_ssse3]
-    mova                 m7, [base+pw_34] ; 2 + (8 << 2)
-    pshufd               m5, m6, q0000
-    pshufd               m6, m6, q1111
-    add                  wq, base_reg
-    jmp                  wq
-.h_w2:
+    sub                srcq, 2
+    movq                 m7, [base_reg-put_ssse3+subpel_filters+1+mxq*8]
+    punpcklwd            m7, m7
+    pshufd               m4, m7, q0000
+    pshufd               m6, m7, q1111
+    pshufd               m7, m7, q2222
+    sub                  wd, 16
+    jge .h_w16
+%macro PUT_6TAP_H 3 ; dst/src, tmp[1-2]
 %if ARCH_X86_32
-    and                 mxd, 0x7f
-%else
-    movzx               mxd, mxb
-%endif
-    dec                srcq
-    mova                 m4, [base+subpel_h_shuf4]
-    movd                 m3, [base_reg+mxq*8+subpel_filters-put_ssse3+2]
-    mova                 m5, [base+pw_34] ; 2 + (8 << 2)
-    pshufd               m3, m3, q0000
-    movifnidn           dsq, dsmp
-.h_w2_loop:
-    movq                 m0, [srcq+ssq*0]
-    movhps               m0, [srcq+ssq*1]
-    lea                srcq, [srcq+ssq*2]
-    pshufb               m0, m4
-    pmaddubsw            m0, m3
-    phaddw               m0, m0
-    paddw                m0, m5 ; pw34
-    psraw                m0, 6
-    packuswb             m0, m0
-    movd                r6d, m0
-    mov        [dstq+dsq*0], r6w
-    shr                 r6d, 16
-    mov        [dstq+dsq*1], r6w
-    lea                dstq, [dstq+dsq*2]
-    sub                  hd, 2
-    jg .h_w2_loop
-    RET
-.h_w4:
+    pshufb               %2, %1, [base+subpel_h_shufD]
+    pshufb               %3, %1, [base+subpel_h_shufE]
+    pshufb               %1, [base+subpel_h_shufF]
+%else
+    pshufb               %2, %1, m8
+    pshufb               %3, %1, m9
+    pshufb               %1, m10
+%endif
+    pmaddubsw            %2, m4
+    pmaddubsw            %3, m6
+    pmaddubsw            %1, m7
+    paddw                %2, m5
+    paddw                %2, %3
+    paddw                %1, %2
+    psraw                %1, 6
+%endmacro
 %if ARCH_X86_32
-    and                 mxd, 0x7f
-%else
-    movzx               mxd, mxb
+    mov                  r4, dsm
 %endif
-    dec                srcq
-    movd                 m3, [base_reg+mxq*8+subpel_filters-put_ssse3+2]
-    mova                 m6, [base+subpel_h_shufA]
-    mova                 m5, [base+pw_34] ; 2 + (8 << 2)
-    pshufd               m3, m3, q0000
-    movifnidn           dsq, dsmp
-.h_w4_loop:
-    movq                 m0, [srcq+ssq*0] ; 1
-    movq                 m1, [srcq+ssq*1] ; 2
-    lea                srcq, [srcq+ssq*2]
-    pshufb               m0, m6 ; subpel_h_shufA
-    pshufb               m1, m6 ; subpel_h_shufA
-    pmaddubsw            m0, m3 ; subpel_filters
-    pmaddubsw            m1, m3 ; subpel_filters
-    phaddw               m0, m1
-    paddw                m0, m5 ; pw34
-    psraw                m0, 6
-    packuswb             m0, m0
-    movd       [dstq+dsq*0], m0
-    psrlq                m0, 32
-    movd       [dstq+dsq*1], m0
-    lea                dstq, [dstq+dsq*2]
-    sub                  hd, 2
-    jg .h_w4_loop
-    RET
-%macro PUT_8TAP_H 4 ; dst/src, tmp[1-3]
- %if ARCH_X86_32
-    pshufb              %2, %1, [base+subpel_h_shufB]
-    pshufb              %3, %1, [base+subpel_h_shufC]
-    pshufb              %1,     [base+subpel_h_shufA]
- %else
-    pshufb              %2, %1, m11; subpel_h_shufB
-    pshufb              %3, %1, m9 ; subpel_h_shufC
-    pshufb              %1, m10    ; subpel_h_shufA
- %endif
-    pmaddubsw           %4, %2, m5 ; subpel +0 B0
-    pmaddubsw           %2, m6     ; subpel +4 B4
-    pmaddubsw           %3, m6     ; C4
-    pmaddubsw           %1, m5     ; A0
-    paddw               %3, %4     ; C4+B0
-    paddw               %1, %2     ; A0+B4
-    phaddw              %1, %3
-    paddw               %1, m7     ; pw34
-    psraw               %1, 6
-%endmacro
 .h_w8:
     movu                 m0, [srcq+ssq*0]
     movu                 m1, [srcq+ssq*1]
     lea                srcq, [srcq+ssq*2]
-    PUT_8TAP_H           m0, m2, m3, m4
-    PUT_8TAP_H           m1, m2, m3, m4
+    PUT_6TAP_H           m0, m2, m3
+    PUT_6TAP_H           m1, m2, m3
     packuswb             m0, m1
 %if ARCH_X86_32
-    movq             [dstq], m0
-    add                dstq, dsm
-    movhps           [dstq], m0
-    add                dstq, dsm
+    movq        [dstq+r4*0], m0
+    movhps      [dstq+r4*1], m0
+    lea                dstq, [dstq+r4*2]
 %else
     movq       [dstq+dsq*0], m0
     movhps     [dstq+dsq*1], m0
@@ -1750,27 +1564,17 @@ cglobal put_8tap_8bpc, 1, 9, 0, dst, ds, src, ss, w, h, mx, my, ss3
     sub                  hd, 2
     jg .h_w8
     RET
-.h_w128:
-    mov                  r4, -16*7
-    jmp .h_w16_start
-.h_w64:
-    mov                  r4, -16*3
-    jmp .h_w16_start
-.h_w32:
-    mov                  r4, -16*1
-    jmp .h_w16_start
 .h_w16:
-    xor                 r4d, r4d
-.h_w16_start:
-    sub                srcq, r4
-    sub                dstq, r4
+    add                srcq, wq
+    add                dstq, wq
+    neg                  wq
 .h_w16_loop_v:
-    mov                  r6, r4
+    mov                  r6, wq
 .h_w16_loop_h:
     movu                 m0, [srcq+r6+8*0]
     movu                 m1, [srcq+r6+8*1]
-    PUT_8TAP_H           m0, m2, m3, m4
-    PUT_8TAP_H           m1, m2, m3, m4
+    PUT_6TAP_H           m0, m2, m3
+    PUT_6TAP_H           m1, m2, m3
     packuswb             m0, m1
     mova          [dstq+r6], m0
     add                  r6, 16
@@ -1782,106 +1586,75 @@ cglobal put_8tap_8bpc, 1, 9, 0, dst, ds, src, ss, w, h, mx, my, ss3
     RET
 .v:
 %if ARCH_X86_32
+    %define             dsq  r4
+    %define              m8  [base+pw_512]
     movzx               mxd, ssb
     shr                 ssd, 16
     cmp                  hd, 6
     cmovs               ssd, mxd
-    movq                 m0, [base_reg+ssq*8+subpel_filters-put_ssse3]
+    movq                 m7, [base_reg-put_ssse3+subpel_filters+1+ssq*8]
+    mov                 ssq, ssm
+    punpcklwd            m7, m7
+    pshufd               m5, m7, q0000
+    mov                  r6, ssq
+    pshufd               m6, m7, q1111
+    neg                  r6
+    pshufd               m7, m7, q2222
+    cmp                  wd, 4
+    jge .v_w4
 %else
-    WIN64_SPILL_XMM      16
+    WIN64_SPILL_XMM       9, 12
     movzx               mxd, myb
     shr                 myd, 16
     cmp                  hd, 6
     cmovs               myd, mxd
-    movq                 m0, [base_reg+myq*8+subpel_filters-put_ssse3]
+    movq                 m7, [base_reg-put_ssse3+subpel_filters+1+myq*8]
+    mova                 m8, [base+pw_512]
+    punpcklwd            m7, m7
+    pshufd               m5, m7, q0000
+    mov                 nsq, ssq
+    pshufd               m6, m7, q1111
+    neg                 nsq
+    pshufd               m7, m7, q2222
+    cmp                  wd, 4
+    je .v_w4
+    jg .v_w8
 %endif
-    tzcnt               r6d, wd
-    movzx               r6d, word [base_reg+r6*2+table_offset(put, _8tap_v)]
-    punpcklwd            m0, m0
-    mova                 m7, [base+pw_512]
-    add                  r6, base_reg
+.v_w2:
 %if ARCH_X86_32
- %define            subpel0  [rsp+mmsize*0]
- %define            subpel1  [rsp+mmsize*1]
- %define            subpel2  [rsp+mmsize*2]
- %define            subpel3  [rsp+mmsize*3]
-%assign regs_used 2 ; use r1 (ds) as tmp for stack alignment if needed
-    ALLOC_STACK       -16*4
-%assign regs_used 7
-    pshufd               m1, m0, q0000
-    mova            subpel0, m1
-    pshufd               m1, m0, q1111
-    mova            subpel1, m1
-    pshufd               m1, m0, q2222
-    mova            subpel2, m1
-    pshufd               m1, m0, q3333
-    mova            subpel3, m1
-    mov                 ssq, [rstk+stack_offset+gprsize*4]
-    lea                 ssq, [ssq*3]
-    sub                srcq, ssq
-    mov                 ssq, [rstk+stack_offset+gprsize*4]
-    mov                 dsq, [rstk+stack_offset+gprsize*2]
+    mov                 dsq, dsm
+    movd                 m1, [srcq+r6 *2]
+    movd                 m3, [srcq+r6 *1]
 %else
- %define            subpel0  m8
- %define            subpel1  m9
- %define            subpel2  m10
- %define            subpel3  m11
-    lea                ss3q, [ssq*3]
-    pshufd               m8, m0, q0000
-    sub                srcq, ss3q
-    pshufd               m9, m0, q1111
-    pshufd              m10, m0, q2222
-    pshufd              m11, m0, q3333
+    movd                 m1, [srcq+nsq*2]
+    movd                 m3, [srcq+nsq*1]
 %endif
-    jmp                  r6
-.v_w2:
-    movd                 m1, [srcq+ssq*0]
-    movd                 m0, [srcq+ssq*1]
-%if ARCH_X86_32
-    lea                srcq, [srcq+ssq*2]
     movd                 m2, [srcq+ssq*0]
-    movd                 m5, [srcq+ssq*1]
-    lea                srcq, [srcq+ssq*2]
-    movd                 m3, [srcq+ssq*0]
     movd                 m4, [srcq+ssq*1]
     lea                srcq, [srcq+ssq*2]
-%else
-    movd                 m2, [srcq+ssq*2]
-    add                srcq, ss3q
-    movd                 m5, [srcq+ssq*0]
-    movd                 m3, [srcq+ssq*1]
-    movd                 m4, [srcq+ssq*2]
-    add                srcq, ss3q
-%endif
-    punpcklwd            m1, m0           ; 0 1
-    punpcklwd            m0, m2           ; 1 2
-    punpcklbw            m1, m0           ; 01 12
     movd                 m0, [srcq+ssq*0]
-    punpcklwd            m2, m5           ; 2 3
-    punpcklwd            m5, m3           ; 3 4
-    punpcklwd            m3, m4           ; 4 5
-    punpcklwd            m4, m0           ; 5 6
-    punpcklbw            m2, m5           ; 23 34
-    punpcklbw            m3, m4           ; 45 56
+    punpcklwd            m1, m3     ; 0 1
+    punpcklwd            m3, m2     ; 1 2
+    punpcklwd            m2, m4     ; 2 3
+    punpcklwd            m4, m0     ; 3 4
+    punpcklbw            m1, m3     ; 01 12
+    punpcklbw            m2, m4     ; 23 34
 .v_w2_loop:
-    movd                 m4, [srcq+ssq*1]
+    movd                 m3, [srcq+ssq*1]
     lea                srcq, [srcq+ssq*2]
-    pmaddubsw            m5, m1, subpel0     ; a0 b0
+    pmaddubsw            m4, m1, m5 ; a0 b0
     mova                 m1, m2
-    pmaddubsw            m2, subpel1         ; a1 b1
-    paddw                m5, m2
-    mova                 m2, m3
-    pmaddubsw            m3, subpel2         ; a2 b2
-    paddw                m5, m3
-    punpcklwd            m3, m0, m4          ; 6 7
+    pmaddubsw            m2, m6     ; a1 b1
+    paddw                m4, m2
+    punpcklwd            m2, m0, m3 ; 4 5
     movd                 m0, [srcq+ssq*0]
-    punpcklwd            m4, m0              ; 7 8
-    punpcklbw            m3, m4              ; 67 78
-    pmaddubsw            m4, m3, subpel3     ; a3 b3
-    paddw                m5, m4
-    pmulhrsw             m5, m7
-    packuswb             m5, m5
-    movd                r6d, m5
+    punpcklwd            m3, m0     ; 5 6
+    punpcklbw            m2, m3     ; 67 78
+    pmaddubsw            m3, m2, m7 ; a2 b2
+    paddw                m4, m3
+    pmulhrsw             m4, m8
+    packuswb             m4, m4
+    movd                r6d, m4
     mov        [dstq+dsq*0], r6w
     shr                 r6d, 16
     mov        [dstq+dsq*1], r6w
@@ -1891,78 +1664,57 @@ cglobal put_8tap_8bpc, 1, 9, 0, dst, ds, src, ss, w, h, mx, my, ss3
     RET
 .v_w4:
 %if ARCH_X86_32
-.v_w8:
-.v_w16:
-.v_w32:
-.v_w64:
-.v_w128:
     shl                  wd, 14
-%if STACK_ALIGNMENT < 16
- %define               dstm [rsp+mmsize*4+gprsize]
-    mov                dstm, dstq
-%endif
+    lea                srcq, [srcq+r6*2]
     lea                 r6d, [hq+wq-(1<<16)]
-    mov                  r4, srcq
+    mov                srcm, srcq
+    mov                 dsq, dsm
 .v_w4_loop0:
-%endif
     movd                 m1, [srcq+ssq*0]
-    movd                 m0, [srcq+ssq*1]
-%if ARCH_X86_32
+    movd                 m3, [srcq+ssq*1]
     lea                srcq, [srcq+ssq*2]
+%else
+    movd                 m1, [srcq+nsq*2]
+    movd                 m3, [srcq+nsq*1]
+%endif
     movd                 m2, [srcq+ssq*0]
-    movd                 m5, [srcq+ssq*1]
-    lea                srcq, [srcq+ssq*2]
-    movd                 m3, [srcq+ssq*0]
     movd                 m4, [srcq+ssq*1]
     lea                srcq, [srcq+ssq*2]
-%else
-    movd                 m2, [srcq+ssq*2]
-    add                srcq, ss3q
-    movd                 m5, [srcq+ssq*0]
-    movd                 m3, [srcq+ssq*1]
-    movd                 m4, [srcq+ssq*2]
-    add                srcq, ss3q
-%endif
-    punpckldq            m1, m0           ; 0 1
-    punpckldq            m0, m2           ; 1 2
-    punpcklbw            m1, m0           ; 01 12
     movd                 m0, [srcq+ssq*0]
-    punpckldq            m2, m5           ; 2 3
-    punpckldq            m5, m3           ; 3 4
-    punpckldq            m3, m4           ; 4 5
-    punpckldq            m4, m0           ; 5 6
-    punpcklbw            m2, m5           ; 23 34
-    punpcklbw            m3, m4           ; 45 56
+    punpckldq            m1, m3     ; 0 1
+    punpckldq            m3, m2     ; 1 2
+    punpckldq            m2, m4     ; 2 3
+    punpckldq            m4, m0     ; 3 4
+    punpcklbw            m1, m3     ; 01 12
+    punpcklbw            m2, m4     ; 23 34
 .v_w4_loop:
-    movd                 m4, [srcq+ssq*1]
+    movd                 m3, [srcq+ssq*1]
     lea                srcq, [srcq+ssq*2]
-    pmaddubsw            m5, m1, subpel0  ; a0 b0
+    pmaddubsw            m4, m1, m5 ; a0 b0
     mova                 m1, m2
-    pmaddubsw            m2, subpel1      ; a1 b1
-    paddw                m5, m2
-    mova                 m2, m3
-    pmaddubsw            m3, subpel2      ; a2 b2
-    paddw                m5, m3
-    punpckldq            m3, m0, m4       ; 6 7 _ _
+    pmaddubsw            m2, m6     ; a1 b1
+    paddw                m4, m2
+    punpckldq            m2, m0, m3 ; 4 5
     movd                 m0, [srcq+ssq*0]
-    punpckldq            m4, m0           ; 7 8 _ _
-    punpcklbw            m3, m4           ; 67 78
-    pmaddubsw            m4, m3, subpel3  ; a3 b3
-    paddw                m5, m4
-    pmulhrsw             m5, m7
-    packuswb             m5, m5
-    movd       [dstq+dsq*0], m5
-    psrlq                m5, 32
-    movd       [dstq+dsq*1], m5
+    punpckldq            m3, m0     ; 5 6
+    punpcklbw            m2, m3     ; 67 78
+    pmaddubsw            m3, m2, m7 ; a2 b2
+    paddw                m4, m3
+    pmulhrsw             m4, m8
+    packuswb             m4, m4
+    movd       [dstq+dsq*0], m4
+    psrlq                m4, 32
+    movd       [dstq+dsq*1], m4
     lea                dstq, [dstq+dsq*2]
     sub                  hd, 2
     jg .v_w4_loop
 %if ARCH_X86_32
+    mov                srcq, srcm
     mov                dstq, dstm
-    add                  r4, 4
     movzx                hd, r6w
+    add                srcq, 4
     add                dstq, 4
-    mov                srcq, r4
+    mov                srcm, srcq
     mov                dstm, dstq
     sub                 r6d, 1<<16
     jg .v_w4_loop0
@@ -1970,78 +1722,54 @@ cglobal put_8tap_8bpc, 1, 9, 0, dst, ds, src, ss, w, h, mx, my, ss3
     RET
 %if ARCH_X86_64
 .v_w8:
-.v_w16:
-.v_w32:
-.v_w64:
-.v_w128:
-    lea                 r6d, [wq*8-64]
-    mov                  r4, srcq
-    mov                  r7, dstq
-    lea                 r6d, [hq+r6*4]
+    WIN64_PUSH_XMM       12
+    shl                  wd, 5
+    lea                 r6d, [hq+wq-256]
 .v_w8_loop0:
-    movq                 m1, [srcq+ssq*0]
-    movq                 m2, [srcq+ssq*1]
-    movq                 m3, [srcq+ssq*2]
-    add                srcq, ss3q
-    movq                 m4, [srcq+ssq*0]
-    movq                 m5, [srcq+ssq*1]
-    movq                 m6, [srcq+ssq*2]
-    add                srcq, ss3q
-    movq                 m0, [srcq+ssq*0]
-    punpcklbw            m1, m2 ; 01
-    punpcklbw            m2, m3 ; 12
-    punpcklbw            m3, m4 ; 23
-    punpcklbw            m4, m5 ; 34
-    punpcklbw            m5, m6 ; 45
-    punpcklbw            m6, m0 ; 56
+    movq                 m1, [srcq+nsq*2]
+    movq                 m2, [srcq+nsq*1]
+    lea                  r4, [srcq+ssq*2]
+    movq                 m3, [srcq+ssq*0]
+    movq                 m4, [srcq+ssq*1]
+    mov                  r7, dstq
+    movq                 m0, [r4  +ssq*0]
+    punpcklbw            m1, m2     ; 01
+    punpcklbw            m2, m3     ; 12
+    punpcklbw            m3, m4     ; 23
+    punpcklbw            m4, m0     ; 34
 .v_w8_loop:
-    movq                m13, [srcq+ssq*1]
-    lea                srcq, [srcq+ssq*2]
-    pmaddubsw           m14, m1, subpel0 ; a0
+    pmaddubsw           m10, m1, m5 ; a0
     mova                 m1, m3
-    pmaddubsw           m15, m2, subpel0 ; b0
+    pmaddubsw           m11, m2, m5 ; b0
     mova                 m2, m4
-    pmaddubsw            m3, subpel1 ; a1
-    mova                m12, m0
-    pmaddubsw            m4, subpel1 ; b1
-    movq                 m0, [srcq+ssq*0]
-    paddw               m14, m3
-    paddw               m15, m4
-    mova                 m3, m5
-    pmaddubsw            m5, subpel2 ; a2
-    mova                 m4, m6
-    pmaddubsw            m6, subpel2 ; b2
-    punpcklbw           m12, m13     ; 67
-    punpcklbw           m13, m0      ; 78
-    paddw               m14, m5
-    mova                 m5, m12
-    pmaddubsw           m12, subpel3 ; a3
-    paddw               m15, m6
-    mova                 m6, m13
-    pmaddubsw           m13, subpel3 ; b3
-    paddw               m14, m12
-    paddw               m15, m13
-    pmulhrsw            m14, m7
-    pmulhrsw            m15, m7
-    packuswb            m14, m15
-    movq       [dstq+dsq*0], m14
-    movhps     [dstq+dsq*1], m14
-    lea                dstq, [dstq+dsq*2]
+    pmaddubsw            m3, m6     ; a1
+    pmaddubsw            m4, m6     ; b1
+    paddw               m10, m3
+    paddw               m11, m4
+    movq                 m4, [r4+ssq*1]
+    lea                  r4, [r4+ssq*2]
+    punpcklbw            m3, m0, m4 ; 67
+    movq                 m0, [r4+ssq*0]
+    punpcklbw            m4, m0     ; 78
+    pmaddubsw            m9, m3, m7 ; a2
+    paddw               m10, m9
+    pmaddubsw            m9, m4, m7 ; b2
+    paddw               m11, m9
+    pmulhrsw            m10, m8
+    pmulhrsw            m11, m8
+    packuswb            m10, m11
+    movq         [r7+dsq*0], m10
+    movhps       [r7+dsq*1], m10
+    lea                  r7, [r7+dsq*2]
     sub                  hd, 2
     jg .v_w8_loop
-    add                  r4, 8
-    add                  r7, 8
+    add                srcq, 8
+    add                dstq, 8
     movzx                hd, r6b
-    mov                srcq, r4
-    mov                dstq, r7
     sub                 r6d, 1<<8
     jg .v_w8_loop0
     RET
 %endif ;ARCH_X86_64
-%undef subpel0
-%undef subpel1
-%undef subpel2
-%undef subpel3
 .hv:
     RESET_STACK_STATE
     cmp                  wd, 4
@@ -2052,134 +1780,992 @@ cglobal put_8tap_8bpc, 1, 9, 0, dst, ds, src, ss, w, h, mx, my, ss3
     movzx               mxd, mxb
 %endif
     dec                srcq
-    movd                 m1, [base_reg+mxq*8+subpel_filters-put_ssse3+2]
+    movd                 m1, [base_reg-put_ssse3+subpel_filters+2+mxq*8]
 %if ARCH_X86_32
     movzx               mxd, ssb
     shr                 ssd, 16
     cmp                  hd, 6
     cmovs               ssd, mxd
-    movq                 m0, [base_reg+ssq*8+subpel_filters-put_ssse3]
+    movq                 m0, [base_reg-put_ssse3+subpel_filters+1+ssq*8]
     mov                 ssq, ssmp
-    lea                  r6, [ssq*3]
-    sub                srcq, r6
- %define           base_reg  r6
-    mov                  r6, r1; use as new base
- %assign regs_used 2
-    ALLOC_STACK  -mmsize*14
- %assign regs_used 7
-    mov                 dsq, [rstk+stack_offset+gprsize*2]
- %define           subpelv0  [rsp+mmsize*0]
- %define           subpelv1  [rsp+mmsize*1]
- %define           subpelv2  [rsp+mmsize*2]
- %define           subpelv3  [rsp+mmsize*3]
+    ALLOC_STACK   -mmsize*4
+    %define              m8  [rsp+mmsize*0]
+    %define              m9  [rsp+mmsize*1]
+    %define             m10  [rsp+mmsize*2]
     punpcklbw            m0, m0
+    sub                srcq, ssq
     psraw                m0, 8 ; sign-extend
-    pshufd               m6, m0, q0000
-    mova           subpelv0, m6
-    pshufd               m6, m0, q1111
-    mova           subpelv1, m6
-    pshufd               m6, m0, q2222
-    mova           subpelv2, m6
-    pshufd               m6, m0, q3333
-    mova           subpelv3, m6
+    sub                srcq, ssq
+    pshufd               m2, m0, q0000
+    mova                 m8, m2
+    pshufd               m2, m0, q1111
+    mova                 m9, m2
+    pshufd               m2, m0, q2222
+    mova                m10, m2
 %else
     movzx               mxd, myb
     shr                 myd, 16
     cmp                  hd, 6
     cmovs               myd, mxd
-    movq                 m0, [base_reg+myq*8+subpel_filters-put_ssse3]
-    ALLOC_STACK   mmsize*14, 14
-    lea                ss3q, [ssq*3]
-    sub                srcq, ss3q
- %define           subpelv0  m10
- %define           subpelv1  m11
- %define           subpelv2  m12
- %define           subpelv3  m13
+    movq                 m0, [base_reg-put_ssse3+subpel_filters+1+myq*8]
+    WIN64_SPILL_XMM      11, 14
+    mov                 nsq, ssq
     punpcklbw            m0, m0
+    neg                 nsq
     psraw                m0, 8 ; sign-extend
-    mova                 m8, [base+pw_8192]
-    mova                 m9, [base+pd_512]
-    pshufd              m10, m0, q0000
-    pshufd              m11, m0, q1111
-    pshufd              m12, m0, q2222
-    pshufd              m13, m0, q3333
+    pshufd               m8, m0, q0000
+    pshufd               m9, m0, q1111
+    pshufd              m10, m0, q2222
 %endif
-    pshufd               m7, m1, q0000
     cmp                  wd, 4
     je .hv_w4
 .hv_w2:
-    mova                 m6, [base+subpel_h_shuf4]
-    movq                 m2, [srcq+ssq*0]     ; 0
-    movhps               m2, [srcq+ssq*1]     ; 0 _ 1
+    mova                 m5, [base+subpel_h_shuf4]
+    mova                 m6, [base+pw_34]
+    pshufd               m7, m1, q0000
 %if ARCH_X86_32
- %define           w8192reg  [base+pw_8192]
- %define            d512reg  [base+pd_512]
-    lea                srcq, [srcq+ssq*2]
-    movq                 m0, [srcq+ssq*0]     ; 2
-    movhps               m0, [srcq+ssq*1]     ; 2 _ 3
+    movq                 m2, [srcq+ssq*0]
+    movhps               m2, [srcq+ssq*1]
     lea                srcq, [srcq+ssq*2]
+    mov                 dsq, [rstk+stack_offset+gprsize*2]
 %else
- %define           w8192reg  m8
- %define            d512reg  m9
-    movq                 m0, [srcq+ssq*2]     ; 2
-    add                srcq, ss3q
-    movhps               m0, [srcq+ssq*0]     ; 2 _ 3
+    movq                 m2, [srcq+nsq*2]
+    movhps               m2, [srcq+nsq*1] ; 0 1
 %endif
-    pshufb               m2, m6 ; 0 ~ 1 ~
-    pshufb               m0, m6 ; 2 ~ 3 ~
-    pmaddubsw            m2, m7 ; subpel_filters
-    pmaddubsw            m0, m7 ; subpel_filters
-    phaddw               m2, m0 ; 0 1 2 3
-    pmulhrsw             m2, w8192reg
-%if ARCH_X86_32
-    movq                 m3, [srcq+ssq*0]     ; 4
-    movhps               m3, [srcq+ssq*1]     ; 4 _ 5
+    movq                 m1, [srcq+ssq*0]
+    movhps               m1, [srcq+ssq*1] ; 2 3
     lea                srcq, [srcq+ssq*2]
-%else
-    movq                 m3, [srcq+ssq*1]     ; 4
-    movhps               m3, [srcq+ssq*2]     ; 4 _ 5
-    add                srcq, ss3q
-%endif
-    movq                 m0, [srcq+ssq*0]     ; 6
-    pshufb               m3, m6 ; 4 ~ 5 ~
-    pshufb               m0, m6 ; 6 ~
-    pmaddubsw            m3, m7 ; subpel_filters
-    pmaddubsw            m0, m7 ; subpel_filters
-    phaddw               m3, m0 ; 4 5 6 _
-    pmulhrsw             m3, w8192reg
-    palignr              m4, m3, m2, 4; V        1 2 3 4
-    punpcklwd            m1, m2, m4   ; V 01 12    0 1 1 2
-    punpckhwd            m2, m4       ; V 23 34    2 3 3 4
-    pshufd               m0, m3, q2121; V          5 6 5 6
-    punpcklwd            m3, m0       ; V 45 56    4 5 5 6
+    movq                 m0, [srcq+ssq*0] ; 4
+    REPX  {pshufb    x, m5}, m2, m1, m0
+    REPX  {pmaddubsw x, m7}, m2, m1, m0
+    phaddw               m2, m1
+    phaddw               m0, m0
+    paddw                m2, m6
+    paddw                m0, m6
+    psraw                m2, 2            ; 0 1 2 3
+    psraw                m0, 2
+    palignr              m0, m2, 4        ; 1 2 3 4
+    punpcklwd            m1, m2, m0       ; 01 12
+    punpckhwd            m2, m0           ; 23 34
 .hv_w2_loop:
-    movq                 m4, [srcq+ssq*1] ; V 7
-    lea                srcq, [srcq+ssq*2] ; V
-    movhps               m4, [srcq+ssq*0] ; V 7 8
-    pshufb               m4, m6
-    pmaddubsw            m4, m7
-    pmaddwd              m5, m1, subpelv0; V a0 b0
-    mova                 m1, m2       ; V
-    pmaddwd              m2, subpelv1 ; V a1 b1
-    paddd                m5, m2       ; V
-    mova                 m2, m3       ; V
-    pmaddwd              m3, subpelv2 ; a2 b2
-    phaddw               m4, m4
-    pmulhrsw             m4, w8192reg
-    paddd                m5, m3       ; V
-    palignr              m3, m4, m0, 12
-    mova                 m0, m4
-    punpcklwd            m3, m0           ; V 67 78
-    pmaddwd              m4, m3, subpelv3 ; V a3 b3
-    paddd                m5, d512reg
-    paddd                m5, m4
-    psrad                m5, 10
-    packssdw             m5, m5
-    packuswb             m5, m5
-    movd                r4d, m5
-    mov        [dstq+dsq*0], r4w
-    shr                 r4d, 16
-    mov        [dstq+dsq*1], r4w
+    movq                 m3, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    movhps               m3, [srcq+ssq*0] ; 5 6
+    pshufb               m3, m5
+    pmaddubsw            m3, m7
+    pmaddwd              m4, m8, m1       ; a0 b0
+    mova                 m1, m2
+    pmaddwd              m2, m9           ; a1 b1
+    phaddw               m3, m3
+    paddw                m3, m6
+    psraw                m3, 2
+    paddd                m4, m2
+    palignr              m2, m3, m0, 12   ; 4 5
+    mova                 m0, m3
+    punpcklwd            m2, m3           ; 45 56
+    pmaddwd              m3, m10, m2      ; a2 b2
+    paddd                m4, m3
+    psrad                m4, 10
+    packssdw             m4, m5
+    packuswb             m4, m4
+    movd                r6d, m4
+    mov        [dstq+dsq*0], r6w
+    shr                 r6d, 16
+    mov        [dstq+dsq*1], r6w
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
+    jg .hv_w2_loop
+    RET
+.hv_w4:
+%if ARCH_X86_32
+    movq                 m3, [srcq+ssq*0]
+    movq                 m4, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    mov                 dsq, [rstk+stack_offset+gprsize*2]
+    %define             m11  [base+pw_34]
+    %define             m12  [base+subpel_h_shufA]
+    %define             m13  [rsp+mmsize*3]
+    pshufd               m1, m1, q0000
+    mova                m13, m1
+%else
+    WIN64_PUSH_XMM       14
+    movq                 m3, [srcq+nsq*2]
+    movq                 m4, [srcq+nsq*1]
+    pshufd              m13, m1, q0000
+    mova                m12, [base+subpel_h_shufA]
+    mova                m11, [base+pw_34]
+%endif
+    movq                 m0, [srcq+ssq*0]
+    movq                 m1, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    movq                 m2, [srcq+ssq*0]
+%if ARCH_X86_32
+    mova                 m5, m12
+    mova                 m6, m13
+    REPX {pshufb    x, m5 }, m3, m4, m0, m1, m2
+    mova                 m5, m11
+    REPX {pmaddubsw x, m6 }, m3, m4, m0, m1, m2
+%else
+    REPX {pshufb    x, m12}, m3, m4, m0, m1, m2
+    REPX {pmaddubsw x, m13}, m3, m4, m0, m1, m2
+%endif
+    phaddw               m3, m0      ; 0 2
+    phaddw               m4, m1      ; 1 3
+    phaddw               m0, m2      ; 2 4
+%if ARCH_X86_32
+    REPX     {paddw x, m5 }, m3, m4, m0
+%else
+    REPX     {paddw x, m11}, m3, m4, m0
+%endif
+    REPX     {psraw x, 2  }, m3, m4, m0
+    punpcklwd            m1, m3, m4  ; 01
+    punpckhwd            m3, m4      ; 23
+    punpcklwd            m2, m4, m0  ; 12
+    punpckhwd            m4, m0      ; 34
+.hv_w4_loop:
+    movq                 m7, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    movq                 m6, [srcq+ssq*0]
+    pshufb               m7, m12
+    pshufb               m6, m12
+    pmaddubsw            m7, m13
+    pmaddubsw            m6, m13
+    pmaddwd              m5, m8, m1  ; a0
+    mova                 m1, m3
+    phaddw               m7, m6      ; 5 6
+    pmaddwd              m6, m8, m2  ; b0
+    mova                 m2, m4
+    pmaddwd              m3, m9      ; a1
+    pmaddwd              m4, m9      ; b1
+    paddw                m7, m11
+    psraw                m7, 2
+    paddd                m5, m3
+    paddd                m6, m4
+    shufpd               m4, m0, m7, 0x01 ; 4 5
+    mova                 m0, m7
+    punpcklwd            m3, m4, m7  ; 45
+    punpckhwd            m4, m7      ; 56
+    pmaddwd              m7, m10, m3 ; a2
+    paddd                m5, m7
+    pmaddwd              m7, m10, m4 ; b2
+    paddd                m6, m7
+    psrad                m5, 10
+    psrad                m6, 10
+    packssdw             m5, m6
+    packuswb             m5, m5
+    movd       [dstq+dsq*0], m5
+    psrlq                m5, 32
+    movd       [dstq+dsq*1], m5
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
+    jg .hv_w4_loop
+    RET
+.hv_w8:
+    RESET_STACK_STATE
+    shr                 mxd, 16
+    sub                srcq, 2
+%if ARCH_X86_32
+    movq                 m0, [base_reg-put_ssse3+subpel_filters+1+mxq*8]
+    movzx               mxd, ssb
+    shr                 ssd, 16
+    cmp                  hd, 6
+    cmovs               ssd, mxd
+    movq                 m1, [base_reg-put_ssse3+subpel_filters+1+ssq*8]
+    shl                  wd, 13
+    mov                 ssq, ssm
+    lea                 r6d, [hq+wq-(1<<16)]
+%assign regs_used 5
+    ALLOC_STACK  -mmsize*16
+%assign regs_used 7
+    mov                 dsq, [rstk+stack_offset+gprsize*2]
+    sub                srcq, ssq
+    sub                srcq, ssq
+%if STACK_ALIGNMENT < 16
+    %define            srcm  [esp+mmsize*15+gprsize*0]
+    %define            dstm  [esp+mmsize*15+gprsize*1]
+    mov                dstm, dstq
+%endif
+    mov                srcm, srcq
+%else
+    ALLOC_STACK        16*6, 16
+    movq                 m0, [base_reg-put_ssse3+subpel_filters+1+mxq*8]
+    movzx               mxd, myb
+    shr                 myd, 16
+    cmp                  hd, 6
+    cmovs               myd, mxd
+    movq                 m1, [base_reg-put_ssse3+subpel_filters+1+myq*8]
+    mov                 nsq, ssq
+    shl                  wd, 13
+    neg                 nsq
+    lea                 r6d, [hq+wq-(1<<16)]
+%endif
+    mova                 m7, [base+pw_34]
+    punpcklwd            m0, m0
+    punpcklbw            m1, m1
+    psraw                m1, 8 ; sign-extend
+    pshufd               m2, m0, q0000
+    mova         [rsp+16*0], m2
+    pshufd               m2, m0, q1111
+    mova         [rsp+16*1], m2
+    pshufd               m0, m0, q2222
+    mova         [rsp+16*2], m0
+    pshufd               m2, m1, q0000
+    mova         [rsp+16*3], m2
+    pshufd               m2, m1, q1111
+    mova         [rsp+16*4], m2
+    pshufd               m1, m1, q2222
+    mova         [rsp+16*5], m1
+%macro HV_H_6TAP 3-8 [base+subpel_h_shufD], [base+subpel_h_shufF], \
+                     [rsp+16*0], [rsp+16*1], [rsp+16*2] ; src/dst, tmp[1-2], shuf[1-2], mul[1-3]
+    pshufb               %2, %1, %4
+    pshufb               %1, %5
+    pmaddubsw            %3, %2, %6
+    shufps               %2, %1, q2121
+    pmaddubsw            %1, %8
+    pmaddubsw            %2, %7
+    paddw                %3, m7
+    paddw                %1, %3
+    paddw                %1, %2
+    psraw                %1, 2
+%endmacro
+.hv_w8_loop0:
+    mova                 m2, [base+subpel_h_shufD]
+    mova                 m3, [base+subpel_h_shufF]
+    mova                 m4, [rsp+16*0]
+%if ARCH_X86_32
+    movu                 m0, [srcq+ssq*0]
+    movu                 m1, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    HV_H_6TAP            m0, m5, m6, m2, m3, m4
+    HV_H_6TAP            m1, m5, m6, m2, m3, m4
+    movu                 m5, [srcq+ssq*0]
+    punpcklwd            m6, m0, m1   ; 01
+    punpckhwd            m0, m1
+    mova        [rsp+16* 6], m6
+    mova        [rsp+16* 7], m0
+    HV_H_6TAP            m5, m0, m6, m2, m3, m4
+    movu                 m0, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    punpcklwd            m6, m1, m5   ; 12
+    punpckhwd            m1, m5
+    mova        [rsp+16* 8], m6
+    mova        [rsp+16* 9], m1
+    HV_H_6TAP            m0, m1, m6, m2, m3, m4
+    movu                 m1, [srcq+ssq*0]
+    punpcklwd            m6, m5, m0   ; 23
+    punpckhwd            m5, m0
+    mova        [rsp+16*10], m6
+    mova        [rsp+16*11], m5
+    HV_H_6TAP            m1, m5, m6, m2, m3, m4
+    mova        [rsp+16*14], m1
+    punpcklwd            m6, m0, m1   ; 34
+    punpckhwd            m0, m1
+    mova        [rsp+16*12], m6
+    mova        [rsp+16*13], m0
+.hv_w8_loop:
+    mova                 m3, [rsp+16* 3]
+    pmaddwd              m0, m3, [rsp+16* 6] ; a0
+    pmaddwd              m2, m3, [rsp+16* 7] ; a0'
+    pmaddwd              m1, m3, [rsp+16* 8] ; b0
+    pmaddwd              m3, [rsp+16* 9]     ; b0'
+    mova                 m6, [rsp+16* 4]
+    mova                 m4, [rsp+16*10]
+    mova                 m5, [rsp+16*11]
+    mova        [rsp+16* 6], m4
+    pmaddwd              m4, m6       ; a1
+    mova        [rsp+16* 7], m5
+    pmaddwd              m5, m6       ; a1'
+    paddd                m0, m4
+    mova                 m4, [rsp+16*12]
+    paddd                m2, m5
+    mova                 m5, [rsp+16*13]
+    mova        [rsp+16* 8], m4
+    pmaddwd              m4, m6       ; b1
+    mova        [rsp+16* 9], m5
+    pmaddwd              m5, m6       ; b1'
+    movu                 m6, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    paddd                m1, m4
+    paddd                m3, m5
+    HV_H_6TAP            m6, m4, m5
+    mova                 m5, [rsp+16*14]
+    punpcklwd            m4, m5, m6   ; 45
+    punpckhwd            m5, m6
+    mova        [rsp+16*10], m4
+    mova        [rsp+16*11], m5
+    pmaddwd              m4, [rsp+16*5] ; a2
+    pmaddwd              m5, [rsp+16*5] ; a2'
+    paddd                m0, m4
+    movu                 m4, [srcq+ssq*0]
+    paddd                m2, m5
+    psrad                m0, 10
+    psrad                m2, 10
+    packssdw             m0, m2
+    HV_H_6TAP            m4, m2, m5
+    mova                 m2, [rsp+16*5]
+    punpcklwd            m5, m6, m4   ; 56
+    mova        [rsp+16*14], m4
+    punpckhwd            m6, m4
+    mova        [rsp+16*12], m5
+    pmaddwd              m5, m2       ; b2
+    mova        [rsp+16*13], m6
+    pmaddwd              m6, m2       ; b2'
+    paddd                m1, m5
+    paddd                m3, m6
+    psrad                m1, 10
+    psrad                m3, 10
+    packssdw             m1, m3
+    packuswb             m0, m1
+    movq       [dstq+dsq*0], m0
+    movhps     [dstq+dsq*1], m0
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
+    jg .hv_w8_loop
+    mov                srcq, srcm
+    mov                dstq, dstm
+    movzx                hd, r6w
+    add                srcq, 8
+    add                dstq, 8
+    mov                srcm, srcq
+    mov                dstm, dstq
+%else
+    movu                 m9, [srcq+nsq*2]
+    movu                m11, [srcq+nsq*1]
+    lea                  r4, [srcq+ssq*2]
+    movu                m13, [srcq+ssq*0]
+    movu                m15, [srcq+ssq*1]
+    mov                  r7, dstq
+    movu                 m6, [r4  +ssq*0]
+    mova                 m5, [rsp+16*1]
+    mova                 m8, [rsp+16*2]
+    HV_H_6TAP            m9, m0, m1, m2, m3, m4, m5, m8
+    HV_H_6TAP           m11, m0, m1, m2, m3, m4, m5, m8
+    HV_H_6TAP           m13, m0, m1, m2, m3, m4, m5, m8
+    HV_H_6TAP           m15, m0, m1, m2, m3, m4, m5, m8
+    HV_H_6TAP            m6, m0, m1, m2, m3, m4, m5, m8
+    punpcklwd            m8, m9, m11  ; 01
+    punpckhwd            m9, m11
+    punpcklwd           m10, m11, m13 ; 12
+    punpckhwd           m11, m13
+    punpcklwd           m12, m13, m15 ; 23
+    punpckhwd           m13, m15
+    punpcklwd           m14, m15, m6  ; 34
+    punpckhwd           m15, m6
+.hv_w8_loop:
+    mova                 m3, [rsp+16*3]
+    mova                 m4, [rsp+16*4]
+    pmaddwd              m0, m8, m3  ; a0
+    mova                 m8, m12
+    pmaddwd              m2, m9, m3  ; a0'
+    mova                 m9, m13
+    pmaddwd              m1, m10, m3 ; b0
+    mova                m10, m14
+    pmaddwd              m3, m11     ; b0'
+    mova                m11, m15
+    REPX    {pmaddwd x, m4}, m12, m13, m14, m15
+    paddd                m0, m12
+    paddd                m2, m13
+    paddd                m1, m14
+    paddd                m3, m15
+    movu                m15, [r4+ssq*1]
+    lea                  r4, [r4+ssq*2]
+    HV_H_6TAP           m15, m4, m5
+    punpcklwd           m12, m6, m15
+    punpckhwd           m13, m6, m15
+    movu                 m6, [r4+ssq*0]
+    HV_H_6TAP            m6, m4, m5
+    mova                 m4, [rsp+16*5]
+    punpcklwd           m14, m15, m6
+    punpckhwd           m15, m6
+    pmaddwd              m5, m12, m4  ; a2
+    paddd                m0, m5
+    pmaddwd              m5, m13, m4  ; a2'
+    paddd                m2, m5
+    pmaddwd              m5, m14, m4  ; b2
+    paddd                m1, m5
+    pmaddwd              m4, m15      ; b2'
+    paddd                m3, m4
+    REPX      {psrad x, 10}, m0, m2, m1, m3
+    packssdw             m0, m2
+    packssdw             m1, m3
+    packuswb             m0, m1
+    movq         [r7+dsq*0], m0
+    movhps       [r7+dsq*1], m0
+    lea                  r7, [r7+dsq*2]
+    sub                  hd, 2
+    jg .hv_w8_loop
+    add                srcq, 8
+    add                dstq, 8
+    movzx                hd, r6b
+%endif
+    sub                 r6d, 1<<16
+    jg .hv_w8_loop0
+    RET
+
+PUT_8TAP_FN smooth_sharp,   SMOOTH,  SHARP,   put_8tap_8bpc
+PUT_8TAP_FN sharp_smooth,   SHARP,   SMOOTH,  put_8tap_8bpc
+PUT_8TAP_FN regular_sharp,  REGULAR, SHARP,   put_8tap_8bpc
+PUT_8TAP_FN sharp_regular,  SHARP,   REGULAR, put_8tap_8bpc
+PUT_8TAP_FN sharp,          SHARP,   SHARP
+
+cglobal put_8tap_8bpc, 1, 9, 0, dst, ds, src, ss, w, h, mx, my, ss3
+    imul                mxd, mxm, 0x010101
+    add                 mxd, t0d ; 8tap_h, mx, 4tap_h
+%if ARCH_X86_64
+    imul                myd, mym, 0x010101
+    add                 myd, t1d ; 8tap_v, my, 4tap_v
+%else
+    imul                ssd, mym, 0x010101
+    add                 ssd, t1d ; 8tap_v, my, 4tap_v
+    mov                srcq, srcm
+%endif
+    mov                  wd, wm
+    movifnidn            hd, hm
+    LEA            base_reg, put_ssse3
+    test                mxd, 0xf00
+    jnz .h
+%if ARCH_X86_32
+    test                ssd, 0xf00
+%else
+    test                myd, 0xf00
+%endif
+    jnz .v
+    tzcnt                wd, wd
+    movzx                wd, word [base_reg+wq*2+table_offset(put,)]
+    movifnidn           ssq, ssmp
+    add                  wq, base_reg
+    movifnidn           dsq, dsmp
+%if WIN64
+    pop                  r8
+%endif
+    lea                  r6, [ssq*3]
+    jmp                  wq
+.h_w2:
+    mova                 m3, [base+subpel_h_shuf4]
+    movifnidn           dsq, dsmp
+.h_w2_loop:
+    movq                 m0, [srcq+ssq*0]
+    movhps               m0, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    pshufb               m0, m3
+    pmaddubsw            m0, m4
+    phaddw               m0, m0
+    paddw                m0, m5 ; pw34
+    psraw                m0, 6
+    packuswb             m0, m0
+    movd                r6d, m0
+    mov        [dstq+dsq*0], r6w
+    shr                 r6d, 16
+    mov        [dstq+dsq*1], r6w
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
+    jg .h_w2_loop
+    RET
+.h_w4:
+%if ARCH_X86_32
+    and                 mxd, 0x7f
+%else
+    movzx               mxd, mxb
+%endif
+    movd                 m4, [base_reg+mxq*8+subpel_filters-put_ssse3+2]
+    dec                srcq
+    pshufd               m4, m4, q0000
+    cmp                  wd, 4
+    jl .h_w2
+    mova                 m3, [base+subpel_h_shufA]
+    movifnidn           dsq, dsmp
+.h_w4_loop:
+    movq                 m0, [srcq+ssq*0] ; 1
+    movq                 m1, [srcq+ssq*1] ; 2
+    lea                srcq, [srcq+ssq*2]
+    pshufb               m0, m3 ; subpel_h_shufA
+    pshufb               m1, m3 ; subpel_h_shufA
+    pmaddubsw            m0, m4 ; subpel_filters
+    pmaddubsw            m1, m4 ; subpel_filters
+    phaddw               m0, m1
+    paddw                m0, m5 ; pw34
+    psraw                m0, 6
+    packuswb             m0, m0
+    movd       [dstq+dsq*0], m0
+    psrlq                m0, 32
+    movd       [dstq+dsq*1], m0
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
+    jg .h_w4_loop
+    RET
+.h:
+%if ARCH_X86_32
+    test                ssd, 0xf00
+%else
+    test                myd, 0xf00
+%endif
+    jnz .hv
+    movifnidn           ssq, ssmp
+    mova                 m5, [base+pw_34] ; 2 + (8 << 2)
+    cmp                  wd, 4
+    jle .h_w4
+    WIN64_SPILL_XMM      12
+%if ARCH_X86_64
+    mova                m10, [base+subpel_h_shufA]
+    mova                m11, [base+subpel_h_shufB]
+    mova                 m9, [base+subpel_h_shufC]
+%endif
+    shr                 mxd, 16
+    sub                srcq, 3
+    movq                 m7, [base_reg+mxq*8+subpel_filters-put_ssse3]
+    pshufd               m6, m7, q0000
+    pshufd               m7, m7, q1111
+    sub                  wd, 16
+    jge .h_w16
+%macro PUT_8TAP_H 4 ; dst/src, tmp[1-3]
+ %if ARCH_X86_32
+    pshufb              %2, %1, [base+subpel_h_shufB]
+    pshufb              %3, %1, [base+subpel_h_shufC]
+    pshufb              %1,     [base+subpel_h_shufA]
+ %else
+    pshufb              %2, %1, m11; subpel_h_shufB
+    pshufb              %3, %1, m9 ; subpel_h_shufC
+    pshufb              %1, m10    ; subpel_h_shufA
+ %endif
+    pmaddubsw           %4, %2, m6 ; subpel +0 B0
+    pmaddubsw           %2, m7     ; subpel +4 B4
+    pmaddubsw           %3, m7     ; C4
+    pmaddubsw           %1, m6     ; A0
+    paddw               %3, %4     ; C4+B0
+    paddw               %1, %2     ; A0+B4
+    phaddw              %1, %3
+    paddw               %1, m5     ; pw34
+    psraw               %1, 6
+%endmacro
+.h_w8:
+    movu                 m0, [srcq+ssq*0]
+    movu                 m1, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    PUT_8TAP_H           m0, m2, m3, m4
+    PUT_8TAP_H           m1, m2, m3, m4
+    packuswb             m0, m1
+%if ARCH_X86_32
+    movq             [dstq], m0
+    add                dstq, dsm
+    movhps           [dstq], m0
+    add                dstq, dsm
+%else
+    movq       [dstq+dsq*0], m0
+    movhps     [dstq+dsq*1], m0
+    lea                dstq, [dstq+dsq*2]
+%endif
+    sub                  hd, 2
+    jg .h_w8
+    RET
+.h_w16:
+    add                srcq, wq
+    add                dstq, wq
+    neg                  wq
+.h_w16_loop_v:
+    mov                  r6, wq
+.h_w16_loop_h:
+    movu                 m0, [srcq+r6+8*0]
+    movu                 m1, [srcq+r6+8*1]
+    PUT_8TAP_H           m0, m2, m3, m4
+    PUT_8TAP_H           m1, m2, m3, m4
+    packuswb             m0, m1
+    mova          [dstq+r6], m0
+    add                  r6, 16
+    jle .h_w16_loop_h
+    add                srcq, ssq
+    add                dstq, dsmp
+    dec                  hd
+    jg .h_w16_loop_v
+    RET
+.v:
+%if ARCH_X86_32
+    movzx               mxd, ssb
+    shr                 ssd, 16
+    cmp                  hd, 6
+    cmovs               ssd, mxd
+    movq                 m0, [base_reg+ssq*8+subpel_filters-put_ssse3]
+%else
+    WIN64_SPILL_XMM      16
+    movzx               mxd, myb
+    shr                 myd, 16
+    cmp                  hd, 6
+    cmovs               myd, mxd
+    movq                 m0, [base_reg+myq*8+subpel_filters-put_ssse3]
+%endif
+    punpcklwd            m0, m0
+    mova                 m7, [base+pw_512]
+%if ARCH_X86_32
+ %define            subpel0  [rsp+mmsize*0]
+ %define            subpel1  [rsp+mmsize*1]
+ %define            subpel2  [rsp+mmsize*2]
+ %define            subpel3  [rsp+mmsize*3]
+%assign regs_used 2 ; use r1 (ds) as tmp for stack alignment if needed
+    ALLOC_STACK       -16*4
+%assign regs_used 7
+    pshufd               m1, m0, q0000
+    mova            subpel0, m1
+    pshufd               m1, m0, q1111
+    mova            subpel1, m1
+    pshufd               m1, m0, q2222
+    mova            subpel2, m1
+    pshufd               m1, m0, q3333
+    mova            subpel3, m1
+    mov                 ssq, [rstk+stack_offset+gprsize*4]
+    lea                 ssq, [ssq*3]
+    sub                srcq, ssq
+    mov                 ssq, [rstk+stack_offset+gprsize*4]
+    mov                 dsq, [rstk+stack_offset+gprsize*2]
+    cmp                  wd, 2
+    jne .v_w4
+%else
+ %define            subpel0  m8
+ %define            subpel1  m9
+ %define            subpel2  m10
+ %define            subpel3  m11
+    lea                ss3q, [ssq*3]
+    pshufd               m8, m0, q0000
+    sub                srcq, ss3q
+    pshufd               m9, m0, q1111
+    pshufd              m10, m0, q2222
+    pshufd              m11, m0, q3333
+    cmp                  wd, 4
+    je .v_w4
+    jg .v_w8
+%endif
+.v_w2:
+    movd                 m1, [srcq+ssq*0]
+    movd                 m0, [srcq+ssq*1]
+%if ARCH_X86_32
+    lea                srcq, [srcq+ssq*2]
+    movd                 m2, [srcq+ssq*0]
+    movd                 m5, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    movd                 m3, [srcq+ssq*0]
+    movd                 m4, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+%else
+    movd                 m2, [srcq+ssq*2]
+    add                srcq, ss3q
+    movd                 m5, [srcq+ssq*0]
+    movd                 m3, [srcq+ssq*1]
+    movd                 m4, [srcq+ssq*2]
+    add                srcq, ss3q
+%endif
+    punpcklwd            m1, m0           ; 0 1
+    punpcklwd            m0, m2           ; 1 2
+    punpcklbw            m1, m0           ; 01 12
+    movd                 m0, [srcq+ssq*0]
+    punpcklwd            m2, m5           ; 2 3
+    punpcklwd            m5, m3           ; 3 4
+    punpcklwd            m3, m4           ; 4 5
+    punpcklwd            m4, m0           ; 5 6
+    punpcklbw            m2, m5           ; 23 34
+    punpcklbw            m3, m4           ; 45 56
+.v_w2_loop:
+    movd                 m4, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    pmaddubsw            m5, m1, subpel0     ; a0 b0
+    mova                 m1, m2
+    pmaddubsw            m2, subpel1         ; a1 b1
+    paddw                m5, m2
+    mova                 m2, m3
+    pmaddubsw            m3, subpel2         ; a2 b2
+    paddw                m5, m3
+    punpcklwd            m3, m0, m4          ; 6 7
+    movd                 m0, [srcq+ssq*0]
+    punpcklwd            m4, m0              ; 7 8
+    punpcklbw            m3, m4              ; 67 78
+    pmaddubsw            m4, m3, subpel3     ; a3 b3
+    paddw                m5, m4
+    pmulhrsw             m5, m7
+    packuswb             m5, m5
+    movd                r6d, m5
+    mov        [dstq+dsq*0], r6w
+    shr                 r6d, 16
+    mov        [dstq+dsq*1], r6w
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
+    jg .v_w2_loop
+    RET
+.v_w4:
+%if ARCH_X86_32
+    shl                  wd, 14
+%if STACK_ALIGNMENT < 16
+ %define               dstm [rsp+mmsize*4+gprsize]
+    mov                dstm, dstq
+%endif
+    lea                 r6d, [hq+wq-(1<<16)]
+    mov                  r4, srcq
+.v_w4_loop0:
+%endif
+    movd                 m1, [srcq+ssq*0]
+    movd                 m0, [srcq+ssq*1]
+%if ARCH_X86_32
+    lea                srcq, [srcq+ssq*2]
+    movd                 m2, [srcq+ssq*0]
+    movd                 m5, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    movd                 m3, [srcq+ssq*0]
+    movd                 m4, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+%else
+    movd                 m2, [srcq+ssq*2]
+    add                srcq, ss3q
+    movd                 m5, [srcq+ssq*0]
+    movd                 m3, [srcq+ssq*1]
+    movd                 m4, [srcq+ssq*2]
+    add                srcq, ss3q
+%endif
+    punpckldq            m1, m0           ; 0 1
+    punpckldq            m0, m2           ; 1 2
+    punpcklbw            m1, m0           ; 01 12
+    movd                 m0, [srcq+ssq*0]
+    punpckldq            m2, m5           ; 2 3
+    punpckldq            m5, m3           ; 3 4
+    punpckldq            m3, m4           ; 4 5
+    punpckldq            m4, m0           ; 5 6
+    punpcklbw            m2, m5           ; 23 34
+    punpcklbw            m3, m4           ; 45 56
+.v_w4_loop:
+    movd                 m4, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    pmaddubsw            m5, m1, subpel0  ; a0 b0
+    mova                 m1, m2
+    pmaddubsw            m2, subpel1      ; a1 b1
+    paddw                m5, m2
+    mova                 m2, m3
+    pmaddubsw            m3, subpel2      ; a2 b2
+    paddw                m5, m3
+    punpckldq            m3, m0, m4       ; 6 7 _ _
+    movd                 m0, [srcq+ssq*0]
+    punpckldq            m4, m0           ; 7 8 _ _
+    punpcklbw            m3, m4           ; 67 78
+    pmaddubsw            m4, m3, subpel3  ; a3 b3
+    paddw                m5, m4
+    pmulhrsw             m5, m7
+    packuswb             m5, m5
+    movd       [dstq+dsq*0], m5
+    psrlq                m5, 32
+    movd       [dstq+dsq*1], m5
+    lea                dstq, [dstq+dsq*2]
+    sub                  hd, 2
+    jg .v_w4_loop
+%if ARCH_X86_32
+    mov                dstq, dstm
+    add                  r4, 4
+    movzx                hd, r6w
+    add                dstq, 4
+    mov                srcq, r4
+    mov                dstm, dstq
+    sub                 r6d, 1<<16
+    jg .v_w4_loop0
+%endif
+    RET
+%if ARCH_X86_64
+.v_w8:
+    shl                  wd, 5
+    lea                 r6d, [hq+wq-256]
+.v_w8_loop0:
+    movq                 m1, [srcq+ssq*0]
+    movq                 m2, [srcq+ssq*1]
+    lea                  r4, [srcq+ss3q]
+    movq                 m3, [srcq+ssq*2]
+    movq                 m4, [r4  +ssq*0]
+    mov                  r7, dstq
+    movq                 m5, [r4  +ssq*1]
+    movq                 m6, [r4  +ssq*2]
+    add                  r4, ss3q
+    movq                 m0, [r4  +ssq*0]
+    punpcklbw            m1, m2 ; 01
+    punpcklbw            m2, m3 ; 12
+    punpcklbw            m3, m4 ; 23
+    punpcklbw            m4, m5 ; 34
+    punpcklbw            m5, m6 ; 45
+    punpcklbw            m6, m0 ; 56
+.v_w8_loop:
+    movq                m13, [r4+ssq*1]
+    lea                  r4, [r4+ssq*2]
+    pmaddubsw           m14, m1, subpel0 ; a0
+    mova                 m1, m3
+    pmaddubsw           m15, m2, subpel0 ; b0
+    mova                 m2, m4
+    pmaddubsw            m3, subpel1 ; a1
+    mova                m12, m0
+    pmaddubsw            m4, subpel1 ; b1
+    movq                 m0, [r4+ssq*0]
+    paddw               m14, m3
+    paddw               m15, m4
+    mova                 m3, m5
+    pmaddubsw            m5, subpel2 ; a2
+    mova                 m4, m6
+    pmaddubsw            m6, subpel2 ; b2
+    punpcklbw           m12, m13     ; 67
+    punpcklbw           m13, m0      ; 78
+    paddw               m14, m5
+    mova                 m5, m12
+    pmaddubsw           m12, subpel3 ; a3
+    paddw               m15, m6
+    mova                 m6, m13
+    pmaddubsw           m13, subpel3 ; b3
+    paddw               m14, m12
+    paddw               m15, m13
+    pmulhrsw            m14, m7
+    pmulhrsw            m15, m7
+    packuswb            m14, m15
+    movq         [r7+dsq*0], m14
+    movhps       [r7+dsq*1], m14
+    lea                  r7, [r7+dsq*2]
+    sub                  hd, 2
+    jg .v_w8_loop
+    add                srcq, 8
+    add                dstq, 8
+    movzx                hd, r6b
+    sub                 r6d, 1<<8
+    jg .v_w8_loop0
+    RET
+%endif ;ARCH_X86_64
+%undef subpel0
+%undef subpel1
+%undef subpel2
+%undef subpel3
+.hv:
+    RESET_STACK_STATE
+    cmp                  wd, 4
+    jg .hv_w8
+%if ARCH_X86_32
+    and                 mxd, 0x7f
+%else
+    movzx               mxd, mxb
+%endif
+    dec                srcq
+    movd                 m1, [base_reg+mxq*8+subpel_filters-put_ssse3+2]
+%if ARCH_X86_32
+    movzx               mxd, ssb
+    shr                 ssd, 16
+    cmp                  hd, 6
+    cmovs               ssd, mxd
+    movq                 m0, [base_reg+ssq*8+subpel_filters-put_ssse3]
+    mov                 ssq, ssmp
+    lea                  r6, [ssq*3]
+    sub                srcq, r6
+ %define           base_reg  r6
+    mov                  r6, r1; use as new base
+ %assign regs_used 2
+    ALLOC_STACK  -mmsize*14
+ %assign regs_used 7
+    mov                 dsq, [rstk+stack_offset+gprsize*2]
+ %define           subpelv0  [rsp+mmsize*0]
+ %define           subpelv1  [rsp+mmsize*1]
+ %define           subpelv2  [rsp+mmsize*2]
+ %define           subpelv3  [rsp+mmsize*3]
+    punpcklbw            m0, m0
+    psraw                m0, 8 ; sign-extend
+    pshufd               m6, m0, q0000
+    mova           subpelv0, m6
+    pshufd               m6, m0, q1111
+    mova           subpelv1, m6
+    pshufd               m6, m0, q2222
+    mova           subpelv2, m6
+    pshufd               m6, m0, q3333
+    mova           subpelv3, m6
+%else
+    movzx               mxd, myb
+    shr                 myd, 16
+    cmp                  hd, 6
+    cmovs               myd, mxd
+    movq                 m0, [base_reg+myq*8+subpel_filters-put_ssse3]
+    ALLOC_STACK   mmsize*14, 14
+    lea                ss3q, [ssq*3]
+    sub                srcq, ss3q
+ %define           subpelv0  m10
+ %define           subpelv1  m11
+ %define           subpelv2  m12
+ %define           subpelv3  m13
+    punpcklbw            m0, m0
+    psraw                m0, 8 ; sign-extend
+    mova                 m8, [base+pw_8192]
+    mova                 m9, [base+pd_512]
+    pshufd              m10, m0, q0000
+    pshufd              m11, m0, q1111
+    pshufd              m12, m0, q2222
+    pshufd              m13, m0, q3333
+%endif
+    pshufd               m7, m1, q0000
+    cmp                  wd, 4
+    je .hv_w4
+.hv_w2:
+    mova                 m6, [base+subpel_h_shuf4]
+    movq                 m2, [srcq+ssq*0]     ; 0
+    movhps               m2, [srcq+ssq*1]     ; 0 _ 1
+%if ARCH_X86_32
+ %define           w8192reg  [base+pw_8192]
+ %define            d512reg  [base+pd_512]
+    lea                srcq, [srcq+ssq*2]
+    movq                 m0, [srcq+ssq*0]     ; 2
+    movhps               m0, [srcq+ssq*1]     ; 2 _ 3
+    lea                srcq, [srcq+ssq*2]
+%else
+ %define           w8192reg  m8
+ %define            d512reg  m9
+    movq                 m0, [srcq+ssq*2]     ; 2
+    add                srcq, ss3q
+    movhps               m0, [srcq+ssq*0]     ; 2 _ 3
+%endif
+    pshufb               m2, m6 ; 0 ~ 1 ~
+    pshufb               m0, m6 ; 2 ~ 3 ~
+    pmaddubsw            m2, m7 ; subpel_filters
+    pmaddubsw            m0, m7 ; subpel_filters
+    phaddw               m2, m0 ; 0 1 2 3
+    pmulhrsw             m2, w8192reg
+%if ARCH_X86_32
+    movq                 m3, [srcq+ssq*0]     ; 4
+    movhps               m3, [srcq+ssq*1]     ; 4 _ 5
+    lea                srcq, [srcq+ssq*2]
+%else
+    movq                 m3, [srcq+ssq*1]     ; 4
+    movhps               m3, [srcq+ssq*2]     ; 4 _ 5
+    add                srcq, ss3q
+%endif
+    movq                 m0, [srcq+ssq*0]     ; 6
+    pshufb               m3, m6 ; 4 ~ 5 ~
+    pshufb               m0, m6 ; 6 ~
+    pmaddubsw            m3, m7 ; subpel_filters
+    pmaddubsw            m0, m7 ; subpel_filters
+    phaddw               m3, m0 ; 4 5 6 _
+    pmulhrsw             m3, w8192reg
+    palignr              m4, m3, m2, 4; V        1 2 3 4
+    punpcklwd            m1, m2, m4   ; V 01 12    0 1 1 2
+    punpckhwd            m2, m4       ; V 23 34    2 3 3 4
+    pshufd               m0, m3, q2121; V          5 6 5 6
+    punpcklwd            m3, m0       ; V 45 56    4 5 5 6
+.hv_w2_loop:
+    movq                 m4, [srcq+ssq*1] ; V 7
+    lea                srcq, [srcq+ssq*2] ; V
+    movhps               m4, [srcq+ssq*0] ; V 7 8
+    pshufb               m4, m6
+    pmaddubsw            m4, m7
+    pmaddwd              m5, m1, subpelv0; V a0 b0
+    mova                 m1, m2       ; V
+    pmaddwd              m2, subpelv1 ; V a1 b1
+    paddd                m5, m2       ; V
+    mova                 m2, m3       ; V
+    pmaddwd              m3, subpelv2 ; a2 b2
+    phaddw               m4, m4
+    pmulhrsw             m4, w8192reg
+    paddd                m5, m3       ; V
+    palignr              m3, m4, m0, 12
+    mova                 m0, m4
+    punpcklwd            m3, m0           ; V 67 78
+    pmaddwd              m4, m3, subpelv3 ; V a3 b3
+    paddd                m5, d512reg
+    paddd                m5, m4
+    psrad                m5, 10
+    packssdw             m5, m5
+    packuswb             m5, m5
+    movd                r4d, m5
+    mov        [dstq+dsq*0], r4w
+    shr                 r4d, 16
+    mov        [dstq+dsq*1], r4w
     lea                dstq, [dstq+dsq*2]
     sub                  hd, 2
     jg .hv_w2_loop
@@ -2625,465 +3211,651 @@ cglobal put_8tap_8bpc, 1, 9, 0, dst, ds, src, ss, w, h, mx, my, ss3
     jg .hv_w8_loop0
     RET
 
-%macro PSHUFB_SUBPEL_H_4 5 ; dst/src1, src2/mask, tmp1, tmp2, reset_mask
- %if cpuflag(ssse3)
-    pshufb               %1, %2
- %else
-  %if %5 == 1
-    pcmpeqd              %2, %2
-    psrlq                %2, 32
-  %endif
-    psrldq               %3, %1, 1
-    pshufd               %3, %3, q2301
-    pand                 %1, %2
-    pandn                %4, %2, %3
-    por                  %1, %4
- %endif
-%endmacro
-
-%macro PSHUFB_SUBPEL_H_4a 6 ; dst, src1, src2/mask, tmp1, tmp2, reset_mask
- %ifnidn %1, %2
-    mova                 %1, %2
- %endif
-    PSHUFB_SUBPEL_H_4    %1, %3, %4, %5, %6
-%endmacro
-
-%macro PSHUFB_SUBPEL_H_4b 6 ; dst, src1, src2/mask, tmp1, tmp2, reset_mask
- %if notcpuflag(ssse3)
-    psrlq                %1, %2, 16
- %elifnidn %1, %2
-    mova                 %1, %2
- %endif
-    PSHUFB_SUBPEL_H_4    %1, %3, %4, %5, %6
-%endmacro
-
-%macro PALIGNR 4-5 ; dst, src1, src2, shift[, tmp]
- %if cpuflag(ssse3)
-    palignr              %1, %2, %3, %4
- %else
-  %if %0 == 4
-   %assign %%i regnumof%+%1 + 1
-   %define %%tmp m %+ %%i
-  %else
-   %define %%tmp %5
-  %endif
-    psrldq               %1, %3, %4
-    pslldq            %%tmp, %2, 16-%4
-    por                  %1, %%tmp
- %endif
-%endmacro
-
-%macro PHADDW 4 ; dst, src, pw_1/tmp, load_pw_1
- %if cpuflag(ssse3)
-    phaddw               %1, %2
- %elifnidn %1, %2
-   %if %4 == 1
-    mova                 %3, [base+pw_1]
-   %endif
-    pmaddwd              %1, %3
-    pmaddwd              %2, %3
-    packssdw             %1, %2
- %else
-   %if %4 == 1
-    pmaddwd              %1, [base+pw_1]
-   %else
-    pmaddwd              %1, %3
-   %endif
-    packssdw             %1, %1
- %endif
-%endmacro
-
-%macro PMULHRSW_POW2 4 ; dst, src1, src2, shift
- %if cpuflag(ssse3)
-    pmulhrsw             %1, %2, %3
- %else
-    paddw                %1, %2, %3
-    psraw                %1, %4
- %endif
-%endmacro
-
-%macro PMULHRSW_8192 3 ; dst, src1, src2
-    PMULHRSW_POW2        %1, %2, %3, 2
-%endmacro
+%if ARCH_X86_32
+ DECLARE_REG_TMP 1, 2
+%elif WIN64
+ DECLARE_REG_TMP 6, 4
+%else
+ DECLARE_REG_TMP 6, 7
+%endif
 
-%macro PREP_8TAP_H_LOAD4 5 ; dst, src_memloc, tmp[1-2]
-   movd                  %1, [%2+0]
-   movd                  %3, [%2+1]
-   movd                  %4, [%2+2]
-   movd                  %5, [%2+3]
-   punpckldq             %1, %3
-   punpckldq             %4, %5
-   punpcklqdq            %1, %4
-%endmacro
+%if ARCH_X86_32
+ %define base_reg r2
+ %define base base_reg-prep_ssse3
+%else
+ %define base_reg r7
+ %define base 0
+%endif
 
-%macro PREP_8TAP_H_LOAD 2 ; dst0, src_memloc
- %if cpuflag(ssse3)
-    movu                m%1, [%2]
-    pshufb               m2, m%1, m11 ; subpel_h_shufB
-    pshufb               m3, m%1, m9  ; subpel_h_shufC
-    pshufb              m%1, m10      ; subpel_h_shufA
- %else
-  %if ARCH_X86_64
-    SWAP                m12, m5
-    SWAP                m13, m6
-    SWAP                m14, m7
-   %define %%mx0 m%+%%i
-   %define %%mx1 m%+%%j
-   %assign %%i 0
-   %rep 12
-    movd              %%mx0, [%2+%%i]
-    %assign %%i %%i+1
-   %endrep
-   %assign %%i 0
-   %rep 6
-    %assign %%j %%i+1
-    punpckldq         %%mx0, %%mx1
-    %assign %%i %%i+2
-   %endrep
-   %assign %%i 0
-   %rep 3
-    %assign %%j %%i+2
-    punpcklqdq        %%mx0, %%mx1
-    %assign %%i %%i+4
-   %endrep
-    SWAP                m%1, m0
-    SWAP                 m2, m4
-    SWAP                 m3, m8
-    SWAP                 m5, m12
-    SWAP                 m6, m13
-    SWAP                 m7, m14
-  %else
-    PREP_8TAP_H_LOAD4    m0, %2+0, m1, m4, m7
-    PREP_8TAP_H_LOAD4    m2, %2+4, m1, m4, m7
-    PREP_8TAP_H_LOAD4    m3, %2+8, m1, m4, m7
-    SWAP                m%1, m0
-  %endif
- %endif
-%endmacro
+%define PREP_8TAP_FN FN prep_8tap,
+PREP_8TAP_FN smooth,         SMOOTH,  SMOOTH,  prep_6tap_8bpc
+PREP_8TAP_FN smooth_regular, SMOOTH,  REGULAR, prep_6tap_8bpc
+PREP_8TAP_FN regular_smooth, REGULAR, SMOOTH,  prep_6tap_8bpc
+PREP_8TAP_FN regular,        REGULAR, REGULAR
 
-%macro PREP_8TAP_H 2 ; dst, src_memloc
-    PREP_8TAP_H_LOAD     %1, %2
- %if ARCH_X86_64 && notcpuflag(ssse3)
-    SWAP                 m8, m1
-    SWAP                 m9, m7
- %endif
- %xdefine mX m%+%1
- %assign %%i regnumof%+mX
- %define mX m%+%%i
-    mova                 m4, m2
-    PMADDUBSW            m4, m5, m1, m7, 1  ; subpel +0 B0
-    PMADDUBSW            m2, m6, m1, m7, 0  ; subpel +4 B4
-    PMADDUBSW            m3, m6, m1, m7, 0  ; subpel +4 C4
-    PMADDUBSW            mX, m5, m1, m7, 0  ; subpel +0 A0
- %undef mX
- %if ARCH_X86_64 && notcpuflag(ssse3)
-    SWAP                 m1, m8
-    SWAP                 m7, m9
- %endif
-    paddw                m3, m4
-    paddw               m%1, m2
-    PHADDW              m%1, m3, m15, ARCH_X86_32
- %if ARCH_X86_64 || cpuflag(ssse3)
-    PMULHRSW_8192       m%1, m%1, m7
- %else
-    PMULHRSW_8192       m%1, m%1, [base+pw_2]
- %endif
+cglobal prep_6tap_8bpc, 1, 9, 0, tmp, src, ss, w, h, mx, my, ns
+    imul                mxd, mxm, 0x010101
+    add                 mxd, t0d ; 8tap_h, mx, 4tap_h
+    imul                myd, mym, 0x010101
+    add                 myd, t1d ; 8tap_v, my, 4tap_v
+    mov                  wd, wm
+    movifnidn          srcd, srcm
+    movifnidn            hd, hm
+    LEA            base_reg, prep_ssse3
+    test                mxd, 0xf00
+    jnz .h
+    test                myd, 0xf00
+    jnz .v
+.prep:
+    tzcnt                wd, wd
+    movzx                wd, word [base_reg-prep_ssse3+prep_ssse3_table+wq*2]
+    pxor                 m4, m4
+    add                  wq, base_reg
+    movifnidn           ssq, ssmp
+    lea                  r6, [ssq*3]
+%if WIN64
+    pop                  r8
+    pop                  r7
+%endif
+    jmp                  wq
+.h:
+    test                myd, 0xf00
+    jnz .hv
+    test                myd, 0xf00
+    jnz .hv
+%if ARCH_X86_32
+ %define ssq r6
+    mov                 ssq, ssmp
+%endif
+    cmp                  wd, 4
+    jle mangle(private_prefix %+ _prep_8tap_8bpc %+ SUFFIX).h_w4
+    WIN64_SPILL_XMM      11
+    mova                 m5, [base+pw_8192]
+%if ARCH_X86_64
+    mova                 m8, [base+subpel_h_shufD]
+    mova                 m9, [base+subpel_h_shufE]
+    mova                m10, [base+subpel_h_shufF]
+%endif
+    shr                 mxd, 16
+    sub                srcq, 2
+    movq                 m7, [base_reg-prep_ssse3+subpel_filters+1+mxq*8]
+    punpcklwd            m7, m7
+    pshufd               m4, m7, q0000
+    pshufd               m6, m7, q1111
+    pshufd               m7, m7, q2222
+    sub                  wd, 16
+    jge .h_w16
+%macro PREP_6TAP_H 3 ; dst/src, tmp[1-2]
+%if ARCH_X86_32
+    pshufb               %2, %1, [base+subpel_h_shufD]
+    pshufb               %3, %1, [base+subpel_h_shufE]
+    pshufb               %1, [base+subpel_h_shufF]
+%else
+    pshufb               %2, %1, m8
+    pshufb               %3, %1, m9
+    pshufb               %1, m10
+%endif
+    pmaddubsw            %2, m4
+    pmaddubsw            %3, m6
+    pmaddubsw            %1, m7
+    paddw                %2, %3
+    paddw                %1, %2
+    pmulhrsw             %1, m5
 %endmacro
-
-%macro PREP_8TAP_HV 4 ; dst, src_memloc, tmp[1-2]
- %if cpuflag(ssse3)
-    movu                 %1, [%2]
-    pshufb               m2, %1, shufB
-    pshufb               m3, %1, shufC
-    pshufb               %1, shufA
- %else
-    PREP_8TAP_H_LOAD4    %1, %2+0, m1, %3, %4
-    PREP_8TAP_H_LOAD4    m2, %2+4, m1, %3, %4
-    PREP_8TAP_H_LOAD4    m3, %2+8, m1, %3, %4
- %endif
+.h_w8:
+    movu                 m0, [srcq+ssq*0]
+    movu                 m1, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    PREP_6TAP_H          m0, m2, m3
+    PREP_6TAP_H          m1, m2, m3
+    mova        [tmpq+16*0], m0
+    mova        [tmpq+16*1], m1
+    add                tmpq, 32
+    sub                  hd, 2
+    jg .h_w8
+    RET
+.h_w16:
+    add                srcq, wq
+    neg                  wq
+.h_w16_loop_v:
+    mov                  r5, wq
+.h_w16_loop_h:
+    movu                 m0, [srcq+r5+8*0]
+    movu                 m1, [srcq+r5+8*1]
+    PREP_6TAP_H          m0, m2, m3
+    PREP_6TAP_H          m1, m2, m3
+    mova        [tmpq+16*0], m0
+    mova        [tmpq+16*1], m1
+    add                tmpq, 32
+    add                  r5, 16
+    jle .h_w16_loop_h
+    add                srcq, ssq
+    dec                  hd
+    jg .h_w16_loop_v
+    RET
+.v:
+%if ARCH_X86_32
+    mov                 mxd, myd
+    and                 mxd, 0x7f
+%else
+    WIN64_SPILL_XMM       9, 12
+    movzx               mxd, myb
+%endif
+    shr                 myd, 16
+    cmp                  hd, 6
+    cmovs               myd, mxd
+    movq                 m7, [base_reg-prep_ssse3+subpel_filters+1+myq*8]
+    punpcklwd            m7, m7
+    pshufd               m5, m7, q0000
+    pshufd               m6, m7, q1111
+    pshufd               m7, m7, q2222
+%if ARCH_X86_32
+    %define              m8  [base+pw_8192]
+    mov                 ssq, ssm
+    sub                srcq, ssq
+    sub                srcq, ssq
+%else
+    mova                 m8, [base+pw_8192]
+    mov                 nsq, ssq
+    neg                 nsq
+    cmp                  wd, 4
+    jg .v_w8
+%endif
+.v_w4:
+%if ARCH_X86_32
+    lea                 r5d, [wq-4]
+    shl                 r5d, 14
+    add                 r5d, hd
+    mov                srcm, srcq
+.v_w4_loop0:
+    movd                 m1, [srcq+ssq*0]
+    movd                 m3, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+%else
+    movd                 m1, [srcq+nsq*2]
+    movd                 m3, [srcq+nsq*1]
+%endif
+    movd                 m2, [srcq+ssq*0]
+    movd                 m4, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    movd                 m0, [srcq+ssq*0]
+    punpckldq            m1, m3     ; 0 1
+    punpckldq            m3, m2     ; 1 2
+    punpckldq            m2, m4     ; 2 3
+    punpckldq            m4, m0     ; 3 4
+    punpcklbw            m1, m3     ; 01 12
+    punpcklbw            m2, m4     ; 23 34
+.v_w4_loop:
+    movd                 m3, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    pmaddubsw            m4, m1, m5 ; a0 b0
     mova                 m1, m2
-    PMADDUBSW            m1, subpelh0, %3, %4, 1 ; subpel +0 C0
-    PMADDUBSW            m3, subpelh1, %3, %4, 0 ; subpel +4 B4
-    PMADDUBSW            m2, subpelh1, %3, %4, 0 ; C4
-    PMADDUBSW            %1, subpelh0, %3, %4, 0 ; A0
-    paddw                m1, m3           ; C0+B4
-    paddw                %1, m2           ; A0+C4
-    PHADDW               %1, m1, %3, 1
-%endmacro
-
-%macro PREP_8TAP 0
+    pmaddubsw            m2, m6     ; a1 b1
+    paddw                m4, m2
+    punpckldq            m2, m0, m3 ; 4 5
+    movd                 m0, [srcq+ssq*0]
+    punpckldq            m3, m0     ; 5 6
+    punpcklbw            m2, m3     ; 67 78
+    pmaddubsw            m3, m2, m7 ; a2 b2
+    paddw                m4, m3
+    pmulhrsw             m4, m8
+%if ARCH_X86_32
+    movq        [tmpq+wq*0], m4
+    movhps      [tmpq+wq*2], m4
+    lea                tmpq, [tmpq+wq*4]
+    sub                  hd, 2
+    jg .v_w4_loop
+    mov                srcq, srcm
+    mov                tmpq, tmpm
+    movzx                hd, r5w
+    add                srcq, 4
+    add                tmpq, 8
+    mov                srcm, srcq
+    mov                tmpm, tmpq
+    sub                 r5d, 1<<16
+    jg .v_w4_loop0
+%else
+    mova             [tmpq], m4
+    add                tmpq, 16
+    sub                  hd, 2
+    jg .v_w4_loop
+%endif
+    RET
+%if ARCH_X86_64
+.v_w8:
+    WIN64_PUSH_XMM       12
+    lea                 r6d, [wq*4-32]
+    lea                 r6d, [r6*8+hq]
+.v_w8_loop0:
+    movq                 m1, [srcq+nsq*2]
+    movq                 m2, [srcq+nsq*1]
+    lea                  r5, [srcq+ssq*2]
+    movq                 m3, [srcq+ssq*0]
+    movq                 m4, [srcq+ssq*1]
+    mov                  r8, tmpq
+    movq                 m0, [r5  +ssq*0]
+    punpcklbw            m1, m2     ; 01
+    punpcklbw            m2, m3     ; 12
+    punpcklbw            m3, m4     ; 23
+    punpcklbw            m4, m0     ; 34
+.v_w8_loop:
+    pmaddubsw           m10, m1, m5 ; a0
+    mova                 m1, m3
+    pmaddubsw           m11, m2, m5 ; b0
+    mova                 m2, m4
+    pmaddubsw            m3, m6     ; a1
+    pmaddubsw            m4, m6     ; b1
+    paddw               m10, m3
+    paddw               m11, m4
+    movq                 m4, [r5+ssq*1]
+    lea                  r5, [r5+ssq*2]
+    punpcklbw            m3, m0, m4 ; 67
+    movq                 m0, [r5+ssq*0]
+    punpcklbw            m4, m0     ; 78
+    pmaddubsw            m9, m3, m7 ; a2
+    paddw               m10, m9
+    pmaddubsw            m9, m4, m7 ; b2
+    paddw               m11, m9
+    pmulhrsw            m10, m8
+    pmulhrsw            m11, m8
+    mova          [r8+wq*0], m10
+    mova          [r8+wq*2], m11
+    lea                  r8, [r8+wq*4]
+    sub                  hd, 2
+    jg .v_w8_loop
+    add                srcq, 8
+    add                tmpq, 16
+    movzx                hd, r6b
+    sub                 r6d, 1<<8
+    jg .v_w8_loop0
+    RET
+%endif ;ARCH_X86_64
+.hv:
+    RESET_STACK_STATE
+    cmp                  wd, 4
+    jg .hv_w8
 %if ARCH_X86_32
- DECLARE_REG_TMP 1, 2
-%elif WIN64
- DECLARE_REG_TMP 6, 4
+    and                 mxd, 0x7f
 %else
- DECLARE_REG_TMP 6, 7
+    movzx               mxd, mxb
 %endif
-
-FN prep_8tap, sharp,          SHARP,   SHARP
-FN prep_8tap, sharp_smooth,   SHARP,   SMOOTH
-FN prep_8tap, smooth_sharp,   SMOOTH,  SHARP
-FN prep_8tap, smooth,         SMOOTH,  SMOOTH
-FN prep_8tap, sharp_regular,  SHARP,   REGULAR
-FN prep_8tap, regular_sharp,  REGULAR, SHARP
-FN prep_8tap, smooth_regular, SMOOTH,  REGULAR
-FN prep_8tap, regular_smooth, REGULAR, SMOOTH
-FN prep_8tap, regular,        REGULAR, REGULAR
-
+    dec                srcq
+    movd                 m1, [base_reg-prep_ssse3+subpel_filters+2+mxq*8]
 %if ARCH_X86_32
- %define base_reg r2
- %define base base_reg-prep%+SUFFIX
+    mov                 mxd, myd
+    and                 mxd, 0x7f
 %else
- %define base_reg r7
- %define base 0
+    movzx               mxd, myb
 %endif
-cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
-    imul                mxd, mxm, 0x010101
-    add                 mxd, t0d ; 8tap_h, mx, 4tap_h
-    imul                myd, mym, 0x010101
-    add                 myd, t1d ; 8tap_v, my, 4tap_v
-    mov                  wd, wm
-    movifnidn          srcd, srcm
-    movifnidn            hd, hm
-    test                mxd, 0xf00
-    jnz .h
-    test                myd, 0xf00
-    jnz .v
-    LEA            base_reg, prep_ssse3
-    tzcnt                wd, wd
-    movzx                wd, word [base_reg-prep_ssse3+prep_ssse3_table+wq*2]
-    pxor                 m4, m4
-    add                  wq, base_reg
-    movifnidn       strided, stridem
-    lea                  r6, [strideq*3]
-%if WIN64
-    pop                  r8
-    pop                  r7
+    shr                 myd, 16
+    cmp                  hd, 6
+    cmovs               myd, mxd
+    movq                 m0, [base_reg-prep_ssse3+subpel_filters+1+myq*8]
+%if ARCH_X86_32
+    mov                 ssq, ssmp
+%define regs_used 6
+    ALLOC_STACK   -mmsize*4
+%define regs_used 7
+    %define              m8  [rsp+mmsize*0]
+    %define              m9  [rsp+mmsize*1]
+    %define             m10  [rsp+mmsize*2]
+    punpcklbw            m0, m0
+    sub                srcq, ssq
+    psraw                m0, 8 ; sign-extend
+    sub                srcq, ssq
+    pshufd               m2, m0, q0000
+    mova                 m8, m2
+    pshufd               m2, m0, q1111
+    mova                 m9, m2
+    pshufd               m2, m0, q2222
+    mova                m10, m2
+    movq                 m3, [srcq+ssq*0]
+    movq                 m4, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    %define             m11  [base+pw_8192]
+    %define             m12  [base+subpel_h_shufA]
+    %define             m13  [rsp+mmsize*3]
+    %define             m14  [base+pd_32]
+    pshufd               m1, m1, q0000
+    mova                m13, m1
+%else
+    WIN64_SPILL_XMM      15
+    mov                 nsq, ssq
+    punpcklbw            m0, m0
+    neg                 nsq
+    psraw                m0, 8 ; sign-extend
+    pshufd               m8, m0, q0000
+    pshufd               m9, m0, q1111
+    pshufd              m10, m0, q2222
+    movq                 m3, [srcq+nsq*2]
+    movq                 m4, [srcq+nsq*1]
+    pshufd              m13, m1, q0000
+    mova                m12, [base+subpel_h_shufA]
+    mova                m11, [base+pw_8192]
+    mova                m14, [base+pd_32]
 %endif
-    jmp                  wq
-.h:
-    LEA            base_reg, prep%+SUFFIX
-    test                myd, 0xf00
-    jnz .hv
-%if cpuflag(ssse3)
-    WIN64_SPILL_XMM      12
+    movq                 m0, [srcq+ssq*0]
+    movq                 m1, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    movq                 m2, [srcq+ssq*0]
+%if ARCH_X86_32
+    mova                 m5, m12
+    mova                 m6, m13
+    REPX {pshufb    x, m5 }, m3, m4, m0, m1, m2
+    mova                 m5, m11
+    REPX {pmaddubsw x, m6 }, m3, m4, m0, m1, m2
 %else
-    WIN64_SPILL_XMM      16
+    REPX {pshufb    x, m12}, m3, m4, m0, m1, m2
+    REPX {pmaddubsw x, m13}, m3, m4, m0, m1, m2
 %endif
+    phaddw               m3, m0      ; 0 2
+    phaddw               m4, m1      ; 1 3
+    phaddw               m0, m2      ; 2 4
 %if ARCH_X86_32
- %define strideq r6
-    mov             strideq, stridem
-%endif
-    cmp                  wd, 4
-    je .h_w4
-    tzcnt                wd, wd
-%if cpuflag(ssse3)
- %if ARCH_X86_64
-    mova                m10, [base+subpel_h_shufA]
-    mova                m11, [base+subpel_h_shufB]
-    mova                 m9, [base+subpel_h_shufC]
- %else
-  %define m10 [base+subpel_h_shufA]
-  %define m11 [base+subpel_h_shufB]
-  %define m9  [base+subpel_h_shufC]
- %endif
-%endif
-    shr                 mxd, 16
-    sub                srcq, 3
-    movzx                wd, word [base_reg+wq*2+table_offset(prep, _8tap_h)]
-    movq                 m6, [base_reg+mxq*8+subpel_filters-prep%+SUFFIX]
-%if cpuflag(ssse3)
-    mova                 m7, [base+pw_8192]
-    pshufd               m5, m6, q0000
-    pshufd               m6, m6, q1111
+    REPX  {pmulhrsw x, m5 }, m3, m4, m0
 %else
-    punpcklbw            m6, m6
-    psraw                m6, 8
- %if ARCH_X86_64
-    mova                 m7, [pw_2]
-    mova                m15, [pw_1]
- %else
-  %define m15 m4
- %endif
-    pshufd               m5, m6, q1010
-    punpckhqdq           m6, m6
+    REPX  {pmulhrsw x, m11}, m3, m4, m0
 %endif
-    add                  wq, base_reg
-    jmp                  wq
-.h_w4:
+    punpcklwd            m1, m3, m4  ; 01
+    punpckhwd            m3, m4      ; 23
+    punpcklwd            m2, m4, m0  ; 12
+    punpckhwd            m4, m0      ; 34
+.hv_w4_loop:
+    movq                 m7, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    movq                 m6, [srcq+ssq*0]
+    pshufb               m7, m12
+    pshufb               m6, m12
+    pmaddubsw            m7, m13
+    pmaddubsw            m6, m13
+    pmaddwd              m5, m8, m1  ; a0
+    mova                 m1, m3
+    phaddw               m7, m6      ; 5 6
+    pmaddwd              m6, m8, m2  ; b0
+    mova                 m2, m4
+    pmaddwd              m3, m9      ; a1
+    pmaddwd              m4, m9      ; b1
+    pmulhrsw             m7, m11
+    paddd                m5, m14
+    paddd                m6, m14
+    paddd                m5, m3
+    paddd                m6, m4
+    shufpd               m4, m0, m7, 0x01 ; 4 5
+    mova                 m0, m7
+    punpcklwd            m3, m4, m7  ; 45
+    punpckhwd            m4, m7      ; 56
+    pmaddwd              m7, m10, m3 ; a2
+    paddd                m5, m7
+    pmaddwd              m7, m10, m4 ; b2
+    paddd                m6, m7
+    psrad                m5, 6
+    psrad                m6, 6
+    packssdw             m5, m6
+    mova             [tmpq], m5
+    add                tmpq, 16
+    sub                  hd, 2
+    jg .hv_w4_loop
+    RET
+.hv_w8:
+    RESET_STACK_STATE
+    shr                 mxd, 16
+    sub                srcq, 2
+    movq                 m0, [base_reg-prep_ssse3+subpel_filters+1+mxq*8]
 %if ARCH_X86_32
+    mov                 mxd, myd
     and                 mxd, 0x7f
 %else
-    movzx               mxd, mxb
+    movzx               mxd, myb
 %endif
-    dec                srcq
-    movd                 m4, [base_reg+mxq*8+subpel_filters-prep%+SUFFIX+2]
-%if cpuflag(ssse3)
-    mova                 m6, [base+pw_8192]
-    mova                 m5, [base+subpel_h_shufA]
-    pshufd               m4, m4, q0000
-%else
-    mova                 m6, [base+pw_2]
- %if ARCH_X86_64
-    mova                m14, [pw_1]
- %else
-  %define m14 m7
- %endif
-    punpcklbw            m4, m4
-    psraw                m4, 8
-    punpcklqdq           m4, m4
+    shr                 myd, 16
+    cmp                  hd, 6
+    cmovs               myd, mxd
+    movq                 m1, [base_reg-prep_ssse3+subpel_filters+1+myq*8]
+%if ARCH_X86_32
+    mov                 ssq, ssm
+%assign regs_used 6
+    ALLOC_STACK  -mmsize*16
+%assign regs_used 7
+    sub                srcq, ssq
+    sub                srcq, ssq
+%if STACK_ALIGNMENT < 16
+    %define            srcm  [esp+mmsize*15+gprsize*0]
+    %define            tmpm  [esp+mmsize*15+gprsize*1]
+    mov                tmpm, tmpq
 %endif
-%if ARCH_X86_64
-    lea            stride3q, [strideq*3]
+    mov                srcm, srcq
+%else
+    ALLOC_STACK        16*6, 16
+    mov                 nsq, ssq
+    neg                 nsq
 %endif
-.h_w4_loop:
-%if cpuflag(ssse3)
-    movq                 m0, [srcq+strideq*0] ; 0
-    movq                 m1, [srcq+strideq*1] ; 1
- %if ARCH_X86_32
-    lea                srcq, [srcq+strideq*2]
-    movq                 m2, [srcq+strideq*0] ; 2
-    movq                 m3, [srcq+strideq*1] ; 3
-    lea                srcq, [srcq+strideq*2]
- %else
-    movq                 m2, [srcq+strideq*2] ; 2
-    movq                 m3, [srcq+stride3q ] ; 3
-    lea                srcq, [srcq+strideq*4]
- %endif
-    pshufb               m0, m5
-    pshufb               m1, m5
-    pshufb               m2, m5
-    pshufb               m3, m5
-%elif ARCH_X86_64
-    movd                 m0, [srcq+strideq*0+0]
-    movd                m12, [srcq+strideq*0+1]
-    movd                 m1, [srcq+strideq*1+0]
-    movd                 m5, [srcq+strideq*1+1]
-    movd                 m2, [srcq+strideq*2+0]
-    movd                m13, [srcq+strideq*2+1]
-    movd                 m3, [srcq+stride3q +0]
-    movd                 m7, [srcq+stride3q +1]
-    punpckldq            m0, m12
-    punpckldq            m1, m5
-    punpckldq            m2, m13
-    punpckldq            m3, m7
-    movd                m12, [srcq+strideq*0+2]
-    movd                 m8, [srcq+strideq*0+3]
-    movd                 m5, [srcq+strideq*1+2]
-    movd                 m9, [srcq+strideq*1+3]
-    movd                m13, [srcq+strideq*2+2]
-    movd                m10, [srcq+strideq*2+3]
-    movd                 m7, [srcq+stride3q +2]
-    movd                m11, [srcq+stride3q +3]
-    lea                srcq, [srcq+strideq*4]
-    punpckldq           m12, m8
-    punpckldq            m5, m9
-    punpckldq           m13, m10
-    punpckldq            m7, m11
-    punpcklqdq           m0, m12 ; 0
-    punpcklqdq           m1, m5  ; 1
-    punpcklqdq           m2, m13 ; 2
-    punpcklqdq           m3, m7  ; 3
-%else
-    movd                 m0, [srcq+strideq*0+0]
-    movd                 m1, [srcq+strideq*0+1]
-    movd                 m2, [srcq+strideq*0+2]
-    movd                 m3, [srcq+strideq*0+3]
-    punpckldq            m0, m1
-    punpckldq            m2, m3
-    punpcklqdq           m0, m2 ; 0
-    movd                 m1, [srcq+strideq*1+0]
-    movd                 m2, [srcq+strideq*1+1]
-    movd                 m3, [srcq+strideq*1+2]
-    movd                 m7, [srcq+strideq*1+3]
-    lea                srcq, [srcq+strideq*2]
-    punpckldq            m1, m2
-    punpckldq            m3, m7
-    punpcklqdq           m1, m3 ; 1
-    movd                 m2, [srcq+strideq*0+0]
-    movd                 m3, [srcq+strideq*0+1]
-    movd                 m7, [srcq+strideq*0+2]
-    movd                 m5, [srcq+strideq*0+3]
-    punpckldq            m2, m3
-    punpckldq            m7, m5
-    punpcklqdq           m2, m7 ; 2
-    movd                 m3, [srcq+strideq*1+0]
-    movd                 m7, [srcq+strideq*1+1]
-    punpckldq            m3, m7
-    movd                 m7, [srcq+strideq*1+2]
-    movd                 m5, [srcq+strideq*1+3]
-    lea                srcq, [srcq+strideq*2]
-    punpckldq            m7, m5
-    punpcklqdq           m3, m7 ; 3
-%endif
-    PMADDUBSW            m0, m4, m5, m7, 1 ; subpel_filters + 2
-    PMADDUBSW            m1, m4, m5, m7, 0
-    PMADDUBSW            m2, m4, m5, m7, 0
-    PMADDUBSW            m3, m4, m5, m7, 0
-    PHADDW               m0, m1, m14, ARCH_X86_32
-    PHADDW               m2, m3, m14, 0
-    PMULHRSW_8192        m0, m0, m6
-    PMULHRSW_8192        m2, m2, m6
-    mova        [tmpq+16*0], m0
-    mova        [tmpq+16*1], m2
-    add                tmpq, 32
-    sub                  hd, 4
-    jg .h_w4_loop
-    RET
-.h_w8:
-%if cpuflag(ssse3)
-    PREP_8TAP_H           0, srcq+strideq*0
-    PREP_8TAP_H           1, srcq+strideq*1
-    mova        [tmpq+16*0], m0
-    mova        [tmpq+16*1], m1
-    lea                srcq, [srcq+strideq*2]
-    add                tmpq, 32
+    mova                 m7, [base+pw_8192]
+    lea                 r5d, [wq-8]
+    punpcklwd            m0, m0
+    shl                 r5d, 13
+    punpcklbw            m1, m1
+    add                 r5d, hd
+    psraw                m1, 8 ; sign-extend
+    pshufd               m2, m0, q0000
+    mova         [rsp+16*0], m2
+    pshufd               m2, m0, q1111
+    mova         [rsp+16*1], m2
+    pshufd               m0, m0, q2222
+    mova         [rsp+16*2], m0
+    pshufd               m2, m1, q0000
+    mova         [rsp+16*3], m2
+    pshufd               m2, m1, q1111
+    mova         [rsp+16*4], m2
+    pshufd               m1, m1, q2222
+    mova         [rsp+16*5], m1
+%macro PREP_HV_H_6TAP 3-8 [base+subpel_h_shufD], [base+subpel_h_shufF], \
+                          [rsp+16*0], [rsp+16*1], [rsp+16*2] ; src/dst, tmp[1-2], shuf[1-2], mul[1-3]
+    pshufb               %2, %1, %4
+    pshufb               %1, %5
+    pmaddubsw            %3, %2, %6
+    shufps               %2, %1, q2121
+    pmaddubsw            %1, %8
+    pmaddubsw            %2, %7
+    paddw                %1, %3
+    paddw                %1, %2
+    pmulhrsw             %1, m7
+%endmacro
+.hv_w8_loop0:
+    mova                 m2, [base+subpel_h_shufD]
+    mova                 m3, [base+subpel_h_shufF]
+    mova                 m4, [rsp+16*0]
+%if ARCH_X86_32
+    movu                 m0, [srcq+ssq*0]
+    movu                 m1, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    PREP_HV_H_6TAP       m0, m5, m6, m2, m3, m4
+    PREP_HV_H_6TAP       m1, m5, m6, m2, m3, m4
+    movu                 m5, [srcq+ssq*0]
+    punpcklwd            m6, m0, m1   ; 01
+    punpckhwd            m0, m1
+    mova        [rsp+16* 6], m6
+    mova        [rsp+16* 7], m0
+    PREP_HV_H_6TAP       m5, m0, m6, m2, m3, m4
+    movu                 m0, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    punpcklwd            m6, m1, m5   ; 12
+    punpckhwd            m1, m5
+    mova        [rsp+16* 8], m6
+    mova        [rsp+16* 9], m1
+    PREP_HV_H_6TAP       m0, m1, m6, m2, m3, m4
+    movu                 m1, [srcq+ssq*0]
+    punpcklwd            m6, m5, m0   ; 23
+    punpckhwd            m5, m0
+    mova        [rsp+16*10], m6
+    mova        [rsp+16*11], m5
+    PREP_HV_H_6TAP       m1, m5, m6, m2, m3, m4
+    mova        [rsp+16*14], m1
+    punpcklwd            m6, m0, m1   ; 34
+    punpckhwd            m0, m1
+    mova        [rsp+16*12], m6
+    mova        [rsp+16*13], m0
+.hv_w8_loop:
+    mova                 m3, [rsp+16* 3]
+    pmaddwd              m0, m3, [rsp+16* 6] ; a0
+    pmaddwd              m2, m3, [rsp+16* 7] ; a0'
+    pmaddwd              m1, m3, [rsp+16* 8] ; b0
+    pmaddwd              m3, [rsp+16* 9]     ; b0'
+    mova                 m6, [rsp+16* 4]
+    mova                 m4, [rsp+16*10]
+    mova                 m5, [rsp+16*11]
+    mova        [rsp+16* 6], m4
+    pmaddwd              m4, m6       ; a1
+    mova        [rsp+16* 7], m5
+    pmaddwd              m5, m6       ; a1'
+    paddd                m0, m4
+    mova                 m4, [rsp+16*12]
+    paddd                m2, m5
+    mova                 m5, [rsp+16*13]
+    mova        [rsp+16* 8], m4
+    pmaddwd              m4, m6       ; b1
+    mova        [rsp+16* 9], m5
+    pmaddwd              m5, m6       ; b1'
+    movu                 m6, [srcq+ssq*1]
+    lea                srcq, [srcq+ssq*2]
+    paddd                m1, m4
+    paddd                m3, m5
+    PREP_HV_H_6TAP       m6, m4, m5
+    mova                 m4, [base+pd_32]
+    mova                 m5, [rsp+16*14]
+    REPX      {paddd x, m4}, m0, m2, m1, m3
+    punpcklwd            m4, m5, m6   ; 45
+    punpckhwd            m5, m6
+    mova        [rsp+16*10], m4
+    mova        [rsp+16*11], m5
+    pmaddwd              m4, [rsp+16*5] ; a2
+    pmaddwd              m5, [rsp+16*5] ; a2'
+    paddd                m0, m4
+    movu                 m4, [srcq+ssq*0]
+    paddd                m2, m5
+    psrad                m0, 6
+    psrad                m2, 6
+    packssdw             m0, m2
+    PREP_HV_H_6TAP       m4, m2, m5
+    mova                 m2, [rsp+16*5]
+    punpcklwd            m5, m6, m4   ; 56
+    mova        [rsp+16*14], m4
+    punpckhwd            m6, m4
+    mova        [rsp+16*12], m5
+    pmaddwd              m5, m2       ; b2
+    mova        [rsp+16*13], m6
+    pmaddwd              m6, m2       ; b2'
+    paddd                m1, m5
+    paddd                m3, m6
+    psrad                m1, 6
+    psrad                m3, 6
+    packssdw             m1, m3
+    mova        [tmpq+wq*0], m0
+    mova        [tmpq+wq*2], m1
+    lea                tmpq, [tmpq+wq*4]
     sub                  hd, 2
-%else
-    PREP_8TAP_H           0, srcq
-    mova             [tmpq], m0
-    add                srcq, strideq
+    jg .hv_w8_loop
+    mov                srcq, srcm
+    mov                tmpq, tmpm
+    movzx                hd, r5w
+    add                srcq, 8
     add                tmpq, 16
-    dec                  hd
-%endif
-    jg .h_w8
-    RET
-.h_w16:
-    mov                  r3, -16*1
-    jmp .h_start
-.h_w32:
-    mov                  r3, -16*2
-    jmp .h_start
-.h_w64:
-    mov                  r3, -16*4
-    jmp .h_start
-.h_w128:
-    mov                  r3, -16*8
-.h_start:
-    sub                srcq, r3
-    mov                  r5, r3
-.h_loop:
-%if cpuflag(ssse3)
-    PREP_8TAP_H           0, srcq+r3+8*0
-    PREP_8TAP_H           1, srcq+r3+8*1
-    mova        [tmpq+16*0], m0
-    mova        [tmpq+16*1], m1
-    add                tmpq, 32
-    add                  r3, 16
+    mov                srcm, srcq
+    mov                tmpm, tmpq
 %else
-    PREP_8TAP_H           0, srcq+r3
-    mova             [tmpq], m0
+    movu                 m9, [srcq+nsq*2]
+    movu                m11, [srcq+nsq*1]
+    lea                  r6, [srcq+ssq*2]
+    movu                m13, [srcq+ssq*0]
+    movu                m15, [srcq+ssq*1]
+    mov                  r8, tmpq
+    movu                 m6, [r6  +ssq*0]
+    mova                 m5, [rsp+16*1]
+    mova                 m8, [rsp+16*2]
+    PREP_HV_H_6TAP       m9, m0, m1, m2, m3, m4, m5, m8
+    PREP_HV_H_6TAP      m11, m0, m1, m2, m3, m4, m5, m8
+    PREP_HV_H_6TAP      m13, m0, m1, m2, m3, m4, m5, m8
+    PREP_HV_H_6TAP      m15, m0, m1, m2, m3, m4, m5, m8
+    PREP_HV_H_6TAP       m6, m0, m1, m2, m3, m4, m5, m8
+    punpcklwd            m8, m9, m11  ; 01
+    punpckhwd            m9, m11
+    punpcklwd           m10, m11, m13 ; 12
+    punpckhwd           m11, m13
+    punpcklwd           m12, m13, m15 ; 23
+    punpckhwd           m13, m15
+    punpcklwd           m14, m15, m6  ; 34
+    punpckhwd           m15, m6
+.hv_w8_loop:
+    mova                 m3, [rsp+16*3]
+    mova                 m4, [rsp+16*4]
+    mova                 m5, [base+pd_32]
+    pmaddwd              m0, m8, m3  ; a0
+    mova                 m8, m12
+    pmaddwd              m2, m9, m3  ; a0'
+    mova                 m9, m13
+    pmaddwd              m1, m10, m3 ; b0
+    mova                m10, m14
+    pmaddwd              m3, m11     ; b0'
+    mova                m11, m15
+    REPX    {pmaddwd x, m4}, m12, m13, m14, m15
+    REPX    {paddd   x, m5}, m0, m2, m1, m3
+    paddd                m0, m12
+    paddd                m2, m13
+    paddd                m1, m14
+    paddd                m3, m15
+    movu                m15, [r6+ssq*1]
+    lea                  r6, [r6+ssq*2]
+    PREP_HV_H_6TAP      m15, m4, m5
+    punpcklwd           m12, m6, m15
+    punpckhwd           m13, m6, m15
+    movu                 m6, [r6+ssq*0]
+    PREP_HV_H_6TAP       m6, m4, m5
+    mova                 m4, [rsp+16*5]
+    punpcklwd           m14, m15, m6
+    punpckhwd           m15, m6
+    pmaddwd              m5, m12, m4  ; a2
+    paddd                m0, m5
+    pmaddwd              m5, m13, m4  ; a2'
+    paddd                m2, m5
+    pmaddwd              m5, m14, m4  ; b2
+    paddd                m1, m5
+    pmaddwd              m4, m15      ; b2'
+    paddd                m3, m4
+    REPX       {psrad x, 6}, m0, m2, m1, m3
+    packssdw             m0, m2
+    packssdw             m1, m3
+    mova          [r8+wq*0], m0
+    mova          [r8+wq*2], m1
+    lea                  r8, [r8+wq*4]
+    sub                  hd, 2
+    jg .hv_w8_loop
+    add                srcq, 8
     add                tmpq, 16
-    add                  r3, 8
+    movzx                hd, r5b
 %endif
-    jl .h_loop
-    add                srcq, strideq
-    mov                  r3, r5
-    dec                  hd
-    jg .h_loop
+    sub                 r5d, 1<<16
+    jg .hv_w8_loop0
     RET
+
+PREP_8TAP_FN smooth_sharp,   SMOOTH,  SHARP,   prep_8tap_8bpc
+PREP_8TAP_FN sharp_smooth,   SHARP,   SMOOTH,  prep_8tap_8bpc
+PREP_8TAP_FN regular_sharp,  REGULAR, SHARP,   prep_8tap_8bpc
+PREP_8TAP_FN sharp_regular,  SHARP,   REGULAR, prep_8tap_8bpc
+PREP_8TAP_FN sharp,          SHARP,   SHARP
+
+cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
+    imul                mxd, mxm, 0x010101
+    add                 mxd, t0d ; 8tap_h, mx, 4tap_h
+    imul                myd, mym, 0x010101
+    add                 myd, t1d ; 8tap_v, my, 4tap_v
+    mov                  wd, wm
+    movifnidn          srcd, srcm
+    movifnidn            hd, hm
+    LEA            base_reg, prep_ssse3
+    test                mxd, 0xf00
+    jnz .h
+    test                myd, 0xf00
+    jz mangle(private_prefix %+ _prep_6tap_8bpc_ssse3).prep
 .v:
-    LEA            base_reg, prep%+SUFFIX
 %if ARCH_X86_32
     mov                 mxd, myd
     and                 mxd, 0x7f
@@ -3094,26 +3866,17 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     shr                 myd, 16
     cmp                  hd, 6
     cmovs               myd, mxd
-    movq                 m0, [base_reg+myq*8+subpel_filters-prep%+SUFFIX]
-%if cpuflag(ssse3)
+    movq                 m0, [base_reg+myq*8+subpel_filters-prep_ssse3]
     mova                 m2, [base+pw_512]
-    mova                 m7, [base+pw_8192]
-    punpcklwd            m0, m0
-%else
-    punpcklbw            m0, m0
-    psraw                m0, 8
-%endif
+    mova                 m7, [base+pw_8192]
+    punpcklwd            m0, m0
 %if ARCH_X86_32
  %define            subpel0  [rsp+mmsize*0]
  %define            subpel1  [rsp+mmsize*1]
  %define            subpel2  [rsp+mmsize*2]
  %define            subpel3  [rsp+mmsize*3]
 %assign regs_used 6 ; use r5 (mx) as tmp for stack alignment if needed
- %if cpuflag(ssse3)
     ALLOC_STACK   -mmsize*4
- %else
-    ALLOC_STACK   -mmsize*5
- %endif
 %assign regs_used 7
     mov             strideq, [rstk+stack_offset+gprsize*3]
     pshufd               m1, m0, q0000
@@ -3141,12 +3904,6 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     jns .v_w8
 %endif
 .v_w4:
-%if notcpuflag(ssse3)
-    pxor                 m6, m6
- %if ARCH_X86_64
-    mova                 m7, [base+pw_2]
- %endif
-%endif
 %if ARCH_X86_32
  %if STACK_ALIGNMENT < mmsize
   %define srcm [esp+stack_size+gprsize*1]
@@ -3188,25 +3945,13 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     punpcklbw            m2, m4 ; 23 34
     punpcklbw            m3, m5 ; 45 56
 .v_w4_loop:
-%if ARCH_X86_32 && notcpuflag(ssse3)
-    mova                 m7, subpel0
- %define subpel0 m7
-%endif
     mova                 m5, m1
-    PMADDUBSW            m5, subpel0, m6, m4, 0  ; a0 b0
-%if ARCH_X86_32 && notcpuflag(ssse3)
-    mova                 m7, subpel1
- %define subpel1 m7
-%endif
+    pmaddubsw            m5, subpel0      ; a0 b0
     mova                 m1, m2
-    PMADDUBSW            m2, subpel1, m6, m4, 0  ; a1 b1
+    pmaddubsw            m2, subpel1      ; a1 b1
     paddw                m5, m2
-%if ARCH_X86_32 && notcpuflag(ssse3)
-    mova                 m7, subpel2
- %define subpel2 m7
-%endif
     mova                 m2, m3
-    PMADDUBSW            m3, subpel2, m6, m4, 0  ; a2 b2
+    pmaddubsw            m3, subpel2      ; a2 b2
     movd                 m4, [srcq+strideq*1]
     lea                srcq, [srcq+strideq*2]
     paddw                m5, m3
@@ -3214,27 +3959,10 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     movd                 m0, [srcq+strideq*0]
     punpckldq            m4, m0           ; 7 8 _ _
     punpcklbw            m3, m4           ; 67 78
-%if notcpuflag(ssse3)
- %if ARCH_X86_64
-    SWAP                m12, m0
- %else
-    mova     [esp+mmsize*4], m0
-    mova                 m7, subpel3
-  %define subpel3 m7
- %endif
-%endif
     mova                 m4, m3
-    PMADDUBSW            m4, subpel3, m6, m0, 0  ; a3 b3
+    pmaddubsw            m4, subpel3      ; a3 b3
     paddw                m5, m4
-%if ARCH_X86_64 || cpuflag(ssse3)
- %if notcpuflag(ssse3)
-    SWAP                 m0, m12
- %endif
-    PMULHRSW_8192        m5, m5, m7
-%else
-    mova                 m0, [esp+mmsize*4]
-    PMULHRSW_8192        m5, m5, [base+pw_2]
-%endif
+    pmulhrsw             m5, m7
     movq        [tmpq+wq*0], m5
     movhps      [tmpq+wq*2], m5
     lea                tmpq, [tmpq+wq*4]
@@ -3277,7 +4005,6 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
 .v_w8_loop:
     movq                m13, [srcq+strideq*1]
     lea                srcq, [srcq+strideq*2]
-%if cpuflag(ssse3)
     pmaddubsw           m14, m1, subpel0 ; a0
     pmaddubsw           m15, m2, subpel0 ; b0
     mova                 m1, m3
@@ -3303,35 +4030,6 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     paddw               m15, m13
     pmulhrsw            m14, m7
     pmulhrsw            m15, m7
-%else
-    mova                m14, m1
-    PMADDUBSW           m14, subpel0, m7, m12, 1 ; a0
-    mova                m15, m2
-    PMADDUBSW           m15, subpel0, m7, m12, 0 ; b0
-    mova                 m1, m3
-    PMADDUBSW            m3, subpel1, m7, m12, 0 ; a1
-    mova                 m2, m4
-    PMADDUBSW            m4, subpel1, m7, m12, 0 ; b1
-    paddw               m14, m3
-    mova                 m3, m5
-    PMADDUBSW            m5, subpel2, m7, m12, 0 ; a2
-    paddw               m15, m4
-    mova                 m4, m6
-    PMADDUBSW            m6, subpel2, m7, m12, 0 ; b2
-    paddw               m15, m6
-    punpcklbw           m12, m0, m13 ; 67
-    movq                 m0, [srcq+strideq*0]
-    punpcklbw           m13, m0      ; 78
-    paddw               m14, m5
-    mova                 m5, m12
-    PMADDUBSW           m12, subpel3, m7, m6, 0  ; a3
-    paddw               m14, m12
-    mova                 m6, m13
-    PMADDUBSW           m13, subpel3, m7, m12, 0 ; b3
-    paddw               m15, m13
-    PMULHRSW_8192       m14, m14, [base+pw_2]
-    PMULHRSW_8192       m15, m15, [base+pw_2]
-%endif
     movu        [tmpq+wq*0], m14
     movu        [tmpq+wq*2], m15
     lea                tmpq, [tmpq+wq*4]
@@ -3350,19 +4048,132 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
 %undef subpel1
 %undef subpel2
 %undef subpel3
+.h_w4:
+    WIN64_SPILL_XMM       7
+%if ARCH_X86_32
+    and                 mxd, 0x7f
+%else
+    movzx               mxd, mxb
+%endif
+    dec                srcq
+    movd                 m4, [base_reg+mxq*8+subpel_filters-prep_ssse3+2]
+    mova                 m5, [base+subpel_h_shufA]
+    mova                 m6, [base+pw_8192]
+    movifnidn            r2, stridemp
+    pshufd               m4, m4, q0000
+    lea                  r3, [r2*3]
+.h_w4_loop:
+    movq                 m0, [srcq+r2*0]
+    movq                 m1, [srcq+r2*1]
+    movq                 m2, [srcq+r2*2]
+    movq                 m3, [srcq+r3  ]
+    lea                srcq, [srcq+r2*4]
+    REPX  {pshufb    x, m5}, m0, m1, m2, m3
+    REPX  {pmaddubsw x, m4}, m0, m1, m2, m3
+    phaddw               m0, m1
+    phaddw               m2, m3
+    pmulhrsw             m0, m6
+    pmulhrsw             m2, m6
+    mova        [tmpq+16*0], m0
+    mova        [tmpq+16*1], m2
+    add                tmpq, 32
+    sub                  hd, 4
+    jg .h_w4_loop
+    RET
+.h:
+    test                myd, 0xf00
+    jnz .hv
+    cmp                  wd, 4
+    je .h_w4
+    WIN64_SPILL_XMM      12
+%if ARCH_X86_32
+ %define strideq r6
+    mov             strideq, stridem
+%endif
+    tzcnt                wd, wd
+%if ARCH_X86_64
+    mova                m10, [base+subpel_h_shufA]
+    mova                m11, [base+subpel_h_shufB]
+    mova                 m9, [base+subpel_h_shufC]
+%else
+    %define             m10  [base+subpel_h_shufA]
+    %define             m11  [base+subpel_h_shufB]
+    %define              m9  [base+subpel_h_shufC]
+%endif
+    shr                 mxd, 16
+    sub                srcq, 3
+    movzx                wd, word [base_reg+wq*2+table_offset(prep, _8tap_h)]
+    movq                 m6, [base_reg+mxq*8+subpel_filters-prep_ssse3]
+    mova                 m7, [base+pw_8192]
+    pshufd               m5, m6, q0000
+    pshufd               m6, m6, q1111
+    add                  wq, base_reg
+    jmp                  wq
+%macro PREP_8TAP_H 2 ; dst, src_memloc
+    movu                m%1, [%2]
+    pshufb               m2, m%1, m11 ; subpel_h_shufB
+    pshufb               m3, m%1, m9  ; subpel_h_shufC
+    pshufb              m%1, m10      ; subpel_h_shufA
+    mova                 m4, m2
+    pmaddubsw            m4, m5       ; subpel +0 B0
+    pmaddubsw            m2, m6       ; subpel +4 B4
+    pmaddubsw            m3, m6       ; subpel +4 C4
+    pmaddubsw           m%1, m5       ; subpel +0 A0
+    paddw                m3, m4
+    paddw               m%1, m2
+    phaddw              m%1, m3
+    pmulhrsw            m%1, m7
+%endmacro
+.h_w8:
+    PREP_8TAP_H           0, srcq+strideq*0
+    PREP_8TAP_H           1, srcq+strideq*1
+    mova        [tmpq+16*0], m0
+    mova        [tmpq+16*1], m1
+    lea                srcq, [srcq+strideq*2]
+    add                tmpq, 32
+    sub                  hd, 2
+    jg .h_w8
+    RET
+.h_w16:
+    mov                  r3, -16*1
+    jmp .h_start
+.h_w32:
+    mov                  r3, -16*2
+    jmp .h_start
+.h_w64:
+    mov                  r3, -16*4
+    jmp .h_start
+.h_w128:
+    mov                  r3, -16*8
+.h_start:
+    sub                srcq, r3
+    mov                  r5, r3
+.h_loop:
+    PREP_8TAP_H           0, srcq+r3+8*0
+    PREP_8TAP_H           1, srcq+r3+8*1
+    mova        [tmpq+16*0], m0
+    mova        [tmpq+16*1], m1
+    add                tmpq, 32
+    add                  r3, 16
+    jl .h_loop
+    add                srcq, strideq
+    mov                  r3, r5
+    dec                  hd
+    jg .h_loop
+    RET
 .hv:
     RESET_STACK_STATE
     cmp                  wd, 4
     jg .hv_w8
     and                 mxd, 0x7f
-    movd                 m1, [base_reg+mxq*8+subpel_filters-prep%+SUFFIX+2]
+    movd                 m1, [base_reg+mxq*8+subpel_filters-prep_ssse3+2]
 %if ARCH_X86_32
     mov                 mxd, myd
     shr                 myd, 16
     and                 mxd, 0x7f
     cmp                  hd, 6
     cmovs               myd, mxd
-    movq                 m0, [base_reg+myq*8+subpel_filters-prep%+SUFFIX]
+    movq                 m0, [base_reg+myq*8+subpel_filters-prep_ssse3]
     mov             strideq, stridem
  %assign regs_used 6
     ALLOC_STACK  -mmsize*14
@@ -3388,12 +4199,8 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     shr                 myd, 16
     cmp                  hd, 6
     cmovs               myd, mxd
-    movq                 m0, [base_reg+myq*8+subpel_filters-prep%+SUFFIX]
- %if cpuflag(ssse3)
+    movq                 m0, [base_reg+myq*8+subpel_filters-prep_ssse3]
     ALLOC_STACK   mmsize*14, 14
- %else
-    ALLOC_STACK   mmsize*14, 16
- %endif
     lea            stride3q, [strideq*3]
     sub                srcq, stride3q
     dec                srcq
@@ -3403,11 +4210,7 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
  %define           subpelv3  m13
     punpcklbw            m0, m0
     psraw                m0, 8
- %if cpuflag(ssse3)
     mova                 m8, [base+pw_8192]
- %else
-    mova                 m8, [base+pw_2]
- %endif
     mova                 m9, [base+pd_32]
     pshufd              m10, m0, q0000
     pshufd              m11, m0, q1111
@@ -3415,10 +4218,6 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     pshufd              m13, m0, q3333
 %endif
     pshufd               m7, m1, q0000
-%if notcpuflag(ssse3)
-    punpcklbw            m7, m7
-    psraw                m7, 8
-%endif
 %define hv4_line_0_0 4
 %define hv4_line_0_1 5
 %define hv4_line_0_2 6
@@ -3430,26 +4229,14 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
 %define hv4_line_1_2 12
 %define hv4_line_1_3 13
 %if ARCH_X86_32
- %if cpuflag(ssse3)
-  %define          w8192reg  [base+pw_8192]
- %else
-  %define          w8192reg  [base+pw_2]
- %endif
- %define             d32reg  [base+pd_32]
+    %define        w8192reg  [base+pw_8192]
+    %define          d32reg  [base+pd_32]
 %else
- %define           w8192reg  m8
- %define             d32reg  m9
+    %define        w8192reg  m8
+    %define          d32reg  m9
 %endif
     ; lower shuffle 0 1 2 3 4
-%if cpuflag(ssse3)
     mova                 m6, [base+subpel_h_shuf4]
-%else
- %if ARCH_X86_64
-    mova                m15, [pw_1]
- %else
-  %define               m15 m1
- %endif
-%endif
     movq                 m5, [srcq+strideq*0]   ; 0 _ _ _
     movhps               m5, [srcq+strideq*1]   ; 0 _ 1 _
 %if ARCH_X86_32
@@ -3462,34 +4249,23 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     movhps               m4, [srcq+stride3q ]   ; 2 _ 3 _
     lea                srcq, [srcq+strideq*4]
 %endif
-    PSHUFB_SUBPEL_H_4a   m2, m5, m6, m1, m3, 1    ;H subpel_h_shuf4 0~1~
-    PSHUFB_SUBPEL_H_4a   m0, m4, m6, m1, m3, 0    ;H subpel_h_shuf4 2~3~
-    PMADDUBSW            m2, m7, m1, m3, 1        ;H subpel_filters
-    PMADDUBSW            m0, m7, m1, m3, 0        ;H subpel_filters
-    PHADDW               m2, m0, m15, ARCH_X86_32 ;H 0 1 2 3
-    PMULHRSW_8192        m2, m2, w8192reg
+    pshufb               m2, m5, m6             ;H subpel_h_shuf4 0~1~
+    pshufb               m0, m4, m6             ;H subpel_h_shuf4 2~3~
+    pmaddubsw            m2, m7                 ;H subpel_filters
+    pmaddubsw            m0, m7                 ;H subpel_filters
+    phaddw               m2, m0
+    pmulhrsw             m2, w8192reg
     SAVELINE_W4          m2, 2, 0
     ; upper shuffle 2 3 4 5 6
-%if cpuflag(ssse3)
     mova                 m6, [base+subpel_h_shuf4+16]
-%endif
-    PSHUFB_SUBPEL_H_4b   m2, m5, m6, m1, m3, 0    ;H subpel_h_shuf4 0~1~
-    PSHUFB_SUBPEL_H_4b   m0, m4, m6, m1, m3, 0    ;H subpel_h_shuf4 2~3~
-    PMADDUBSW            m2, m7, m1, m3, 1        ;H subpel_filters
-    PMADDUBSW            m0, m7, m1, m3, 0        ;H subpel_filters
-    PHADDW               m2, m0, m15, ARCH_X86_32 ;H 0 1 2 3
-    PMULHRSW_8192        m2, m2, w8192reg
-%if notcpuflag(ssse3)
- %if ARCH_X86_64
-    SWAP                m14, m2
- %else
-    mova     [esp+mmsize*4], m2
- %endif
-%endif
+    pshufb               m2, m5, m6             ;H subpel_h_shuf4 0~1~
+    pshufb               m0, m4, m6             ;H subpel_h_shuf4 2~3~
+    pmaddubsw            m2, m7                 ;H subpel_filters
+    pmaddubsw            m0, m7                 ;H subpel_filters
+    phaddw               m2, m0                 ;H 0 1 2 3
+    pmulhrsw             m2, w8192reg
     ; lower shuffle
-%if cpuflag(ssse3)
     mova                 m6, [base+subpel_h_shuf4]
-%endif
     movq                 m5, [srcq+strideq*0]   ; 4 _ _ _
     movhps               m5, [srcq+strideq*1]   ; 4 _ 5 _
 %if ARCH_X86_32
@@ -3500,32 +4276,23 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     movq                 m4, [srcq+strideq*2]   ; 6 _ _ _
     add                srcq, stride3q
 %endif
-    PSHUFB_SUBPEL_H_4a   m3, m5, m6, m1, m2, 0    ;H subpel_h_shuf4 4~5~
-    PSHUFB_SUBPEL_H_4a   m0, m4, m6, m1, m2, 0    ;H subpel_h_shuf4 6~6~
-    PMADDUBSW            m3, m7, m1, m2, 1        ;H subpel_filters
-    PMADDUBSW            m0, m7, m1, m2, 0        ;H subpel_filters
-    PHADDW               m3, m0, m15, ARCH_X86_32 ;H 4 5 6 7
-    PMULHRSW_8192        m3, m3, w8192reg
+    pshufb               m3, m5, m6             ;H subpel_h_shuf4 4~5~
+    pshufb               m0, m4, m6             ;H subpel_h_shuf4 6~6~
+    pmaddubsw            m3, m7                 ;H subpel_filters
+    pmaddubsw            m0, m7                 ;H subpel_filters
+    phaddw               m3, m0                 ;H 4 5 6 7
+    pmulhrsw             m3, w8192reg
     SAVELINE_W4          m3, 3, 0
     ; upper shuffle
-%if cpuflag(ssse3)
     mova                 m6, [base+subpel_h_shuf4+16]
-%endif
-    PSHUFB_SUBPEL_H_4b   m3, m5, m6, m1, m2, 0    ;H subpel_h_shuf4 4~5~
-    PSHUFB_SUBPEL_H_4b   m0, m4, m6, m1, m2, 0    ;H subpel_h_shuf4 6~6~
-    PMADDUBSW            m3, m7, m1, m2, 1        ;H subpel_filters
-    PMADDUBSW            m0, m7, m1, m2, 0        ;H subpel_filters
-    PHADDW               m3, m0, m15, ARCH_X86_32 ;H 4 5 6 7
-    PMULHRSW_8192        m3, m3, w8192reg
-%if notcpuflag(ssse3)
- %if ARCH_X86_64
-    SWAP                 m2, m14
- %else
-    mova                 m2, [esp+mmsize*4]
- %endif
-%endif
+    pshufb               m3, m5, m6             ;H subpel_h_shuf4 4~5~
+    pshufb               m0, m4, m6             ;H subpel_h_shuf4 6~6~
+    pmaddubsw            m3, m7                 ;H subpel_filters
+    pmaddubsw            m0, m7                 ;H subpel_filters
+    phaddw               m3, m0                 ;H 4 5 6 7
+    pmulhrsw             m3, w8192reg
     ;process high
-    PALIGNR              m4, m3, m2, 4;V 1 2 3 4
+    palignr              m4, m3, m2, 4;V 1 2 3 4
     punpcklwd            m1, m2, m4  ; V 01 12
     punpckhwd            m2, m4      ; V 23 34
     pshufd               m0, m3, q2121;V 5 6 5 6
@@ -3537,7 +4304,7 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     ;process low
     RESTORELINE_W4       m2, 2, 0
     RESTORELINE_W4       m3, 3, 0
-    PALIGNR              m4, m3, m2, 4;V 1 2 3 4
+    palignr              m4, m3, m2, 4;V 1 2 3 4
     punpcklwd            m1, m2, m4  ; V 01 12
     punpckhwd            m2, m4      ; V 23 34
     pshufd               m0, m3, q2121;V 5 6 5 6
@@ -3551,34 +4318,17 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     mova                 m2, m3
     pmaddwd              m3, subpelv2; V a2 b2
     paddd                m5, m3
-%if notcpuflag(ssse3)
- %if ARCH_X86_64
-    SWAP                m14, m5
- %else
-    mova     [esp+mmsize*4], m5
-  %define m15 m3
- %endif
-%endif
-%if cpuflag(ssse3)
     mova                 m6, [base+subpel_h_shuf4]
-%endif
     movq                 m4, [srcq+strideq*0] ; 7
     movhps               m4, [srcq+strideq*1] ; 7 _ 8 _
-    PSHUFB_SUBPEL_H_4a   m4, m4, m6, m3, m5, 0    ; H subpel_h_shuf4 7~8~
-    PMADDUBSW            m4, m7, m3, m5, 1        ; H subpel_filters
-    PHADDW               m4, m4, m15, ARCH_X86_32 ; H                7878
-    PMULHRSW_8192        m4, m4, w8192reg
-    PALIGNR              m3, m4, m0, 12, m5       ;                  6787
+    pshufb               m4, m6               ; H subpel_h_shuf4 7~8~
+    pmaddubsw            m4, m7               ; H subpel_filters
+    phaddw               m4, m4               ; H                7878
+    pmulhrsw             m4, w8192reg
+    palignr              m3, m4, m0, 12       ;                  6787
     mova                 m0, m4
     punpcklwd            m3, m4      ; 67 78
     pmaddwd              m4, m3, subpelv3; a3 b3
-%if notcpuflag(ssse3)
- %if ARCH_X86_64
-    SWAP                 m5, m14
- %else
-    mova                 m5, [esp+mmsize*4]
- %endif
-%endif
     paddd                m5, d32reg ; pd_32
     paddd                m5, m4
     psrad                m5, 6
@@ -3599,33 +4349,17 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     mova                 m2, m3
     pmaddwd              m3, subpelv2; V a2 b2
     paddd                m5, m3
-%if notcpuflag(ssse3)
- %if ARCH_X86_64
-    SWAP                m14, m5
- %else
-    mova         [esp+0xA0], m5
- %endif
-%endif
-%if cpuflag(ssse3)
     mova                 m6, [base+subpel_h_shuf4+16]
-%endif
     movq                 m4, [srcq+strideq*0] ; 7
     movhps               m4, [srcq+strideq*1] ; 7 _ 8 _
-    PSHUFB_SUBPEL_H_4b   m4, m4, m6, m3, m5, 0    ; H subpel_h_shuf4 7~8~
-    PMADDUBSW            m4, m7, m3, m5, 1        ; H subpel_filters
-    PHADDW               m4, m4, m15, ARCH_X86_32 ; H                7878
-    PMULHRSW_8192        m4, m4, w8192reg
-    PALIGNR              m3, m4, m0, 12, m5       ;                  6787
+    pshufb               m4, m6               ; H subpel_h_shuf4 7~8~
+    pmaddubsw            m4, m7               ; H subpel_filters
+    phaddw               m4, m4               ; H                7878
+    pmulhrsw             m4, w8192reg
+    palignr              m3, m4, m0, 12       ;                  6787
     mova                 m0, m4
     punpcklwd            m3, m4      ; 67 78
     pmaddwd              m4, m3, subpelv3; a3 b3
-%if notcpuflag(ssse3)
- %if ARCH_X86_64
-    SWAP                 m5, m14
- %else
-    mova                 m5, [esp+0xA0]
- %endif
-%endif
     paddd                m5, d32reg ; pd_32
     paddd                m5, m4
     psrad                m4, m5, 6
@@ -3667,13 +4401,13 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
  %define           subpelv3  [rsp+mmsize*10]
  %define             accuv0  [rsp+mmsize*11]
  %define             accuv1  [rsp+mmsize*12]
-    movq                 m1, [base_reg+mxq*8+subpel_filters-prep%+SUFFIX]
+    movq                 m1, [base_reg+mxq*8+subpel_filters-prep_ssse3]
     mov                 mxd, myd
     shr                 myd, 16
     and                 mxd, 0x7f
     cmp                  hd, 6
     cmovs               myd, mxd
-    movq                 m5, [base_reg+myq*8+subpel_filters-prep%+SUFFIX]
+    movq                 m5, [base_reg+myq*8+subpel_filters-prep_ssse3]
     mov             strideq, stridem
  %assign regs_used 6
     ALLOC_STACK  -mmsize*14
@@ -3685,15 +4419,8 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     mov                tmpm, tmpq
     mov             stridem, strideq
  %endif
- %if cpuflag(ssse3)
     pshufd               m0, m1, q0000
     pshufd               m1, m1, q1111
- %else
-    punpcklbw            m1, m1
-    psraw                m1, 8
-    pshufd               m0, m1, q1010
-    punpckhqdq           m1, m1
- %endif
     punpcklbw            m5, m5
     psraw                m5, 8
     pshufd               m2, m5, q0000
@@ -3719,22 +4446,14 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
  %define           subpelv3  m15
  %define             accuv0  m8
  %define             accuv1  m9
-    movq                 m0, [base_reg+mxq*8+subpel_filters-prep%+SUFFIX]
+    movq                 m0, [base_reg+mxq*8+subpel_filters-prep_ssse3]
     movzx               mxd, myb
     shr                 myd, 16
     cmp                  hd, 6
     cmovs               myd, mxd
-    movq                 m1, [base_reg+myq*8+subpel_filters-prep%+SUFFIX]
- %if cpuflag(ssse3)
+    movq                 m1, [base_reg+myq*8+subpel_filters-prep_ssse3]
     pshufd         subpelh0, m0, q0000
     pshufd         subpelh1, m0, q1111
- %else
-    punpcklbw            m0, m0
-    psraw                m0, 8
-    pshufd         subpelh0, m0, q1010
-    pshufd         subpelh1, m0, q3232
-    mova                 m7, [base+pw_2]
- %endif
     punpcklbw            m1, m1
     psraw                m1, 8
     pshufd         subpelv0, m1, q0000
@@ -3751,79 +4470,68 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     shl                 r5d, 14
     add                 r5d, hd
 .hv_w8_loop0:
-%if cpuflag(ssse3)
- %if ARCH_X86_64
+%if ARCH_X86_64
     mova                 m7, [base+subpel_h_shufA]
     mova                 m8, [base+subpel_h_shufB]
     mova                 m9, [base+subpel_h_shufC]
-  %define shufA m7
-  %define shufB m8
-  %define shufC m9
- %else
-  %define shufA [base+subpel_h_shufA]
-  %define shufB [base+subpel_h_shufB]
-  %define shufC [base+subpel_h_shufC]
- %endif
+    %define           shufA  m7
+    %define           shufB  m8
+    %define           shufC  m9
+%else
+    %define           shufA  [base+subpel_h_shufA]
+    %define           shufB  [base+subpel_h_shufB]
+    %define           shufC  [base+subpel_h_shufC]
 %endif
-    PREP_8TAP_HV         m4, srcq+strideq*0, m7, m0
-    PREP_8TAP_HV         m5, srcq+strideq*1, m7, m0
+%macro PREP_8TAP_HV 2 ; dst, src_memloc, tmp[1-2]
+    movu                 %1, [%2]
+    pshufb               m2, %1, shufB
+    pshufb               m3, %1, shufC
+    pshufb               %1, shufA
+    mova                 m1, m2
+    pmaddubsw            m1, subpelh0 ; subpel +0 C0
+    pmaddubsw            m3, subpelh1 ; subpel +4 B4
+    pmaddubsw            m2, subpelh1 ; C4
+    pmaddubsw            %1, subpelh0 ; A0
+    paddw                m1, m3       ; C0+B4
+    paddw                %1, m2       ; A0+C4
+    phaddw               %1, m1
+%endmacro
+    PREP_8TAP_HV         m4, srcq+strideq*0
+    PREP_8TAP_HV         m5, srcq+strideq*1
 %if ARCH_X86_64
-    PREP_8TAP_HV         m6, srcq+strideq*2, m7, m0
+    PREP_8TAP_HV         m6, srcq+strideq*2
     add                srcq, stride3q
-    PREP_8TAP_HV         m0, srcq+strideq*0, m7, m9
+    PREP_8TAP_HV         m0, srcq+strideq*0
 %else
     lea                srcq, [srcq+strideq*2]
- %if notcpuflag(ssse3)
-    mova              [esp], m4
- %endif
-    PREP_8TAP_HV         m6, srcq+strideq*0, m7, m4
-    PREP_8TAP_HV         m0, srcq+strideq*1, m7, m4
+    PREP_8TAP_HV         m6, srcq+strideq*0
+    PREP_8TAP_HV         m0, srcq+strideq*1
     lea                srcq, [srcq+strideq*2]
 %endif
-%if cpuflag(ssse3)
     mova                 m7, [base+pw_8192]
-%else
-    mova                 m7, [base+pw_2]
- %if ARCH_X86_32
-    mova                 m4, [esp]
- %endif
-%endif
-    PMULHRSW_8192        m4, m4, m7
-    PMULHRSW_8192        m5, m5, m7
-    PMULHRSW_8192        m6, m6, m7
-    PMULHRSW_8192        m0, m0, m7
+    REPX   {pmulhrsw x, m7}, m4, m5, m6, m0
     punpcklwd            m1, m4, m5 ; 01
     punpcklwd            m2, m5, m6 ; 12
     punpcklwd            m3, m6, m0 ; 23
     SAVELINE_W8           1, m1
     SAVELINE_W8           2, m2
     SAVELINE_W8           3, m3
-%if cpuflag(ssse3)
     mova                 m7, [base+subpel_h_shufA]
-%endif
 %if ARCH_X86_64
-    PREP_8TAP_HV         m4, srcq+strideq*1, m8, m9
-    PREP_8TAP_HV         m5, srcq+strideq*2, m8, m9
+    PREP_8TAP_HV         m4, srcq+strideq*1
+    PREP_8TAP_HV         m5, srcq+strideq*2
     add                srcq, stride3q
-    PREP_8TAP_HV         m6, srcq+strideq*0, m8, m9
+    PREP_8TAP_HV         m6, srcq+strideq*0
 %else
- %if notcpuflag(ssse3)
-    mova         [esp+0x30], m0
- %endif
-    PREP_8TAP_HV         m4, srcq+strideq*0, m7, m0
-    PREP_8TAP_HV         m5, srcq+strideq*1, m7, m0
+    PREP_8TAP_HV         m4, srcq+strideq*0
+    PREP_8TAP_HV         m5, srcq+strideq*1
     lea                srcq, [srcq+strideq*2]
-    PREP_8TAP_HV         m6, srcq+strideq*0, m7, m0
-%endif
-%if cpuflag(ssse3)
-    mova                 m7, [base+pw_8192]
-%elif ARCH_X86_32
-    mova                 m0, [esp+0x30]
-    mova                 m7, [base+pw_2]
+    PREP_8TAP_HV         m6, srcq+strideq*0
 %endif
-    PMULHRSW_8192        m1, m4, m7
-    PMULHRSW_8192        m2, m5, m7
-    PMULHRSW_8192        m3, m6, m7
+    mova                 m3, [base+pw_8192]
+    pmulhrsw             m1, m3, m4
+    pmulhrsw             m2, m3, m5
+    pmulhrsw             m3, m6
     punpcklwd            m4, m0, m1 ; 34
     punpcklwd            m5, m1, m2 ; 45
     punpcklwd            m6, m2, m3 ; 56
@@ -3866,25 +4574,19 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     mova                 m7, [base+pd_32]
     paddd            accuv0, m7
     paddd            accuv1, m7
- %if cpuflag(ssse3)
     mova                 m7, [base+subpel_h_shufB]
     mova                 m6, [base+subpel_h_shufC]
     mova                 m5, [base+subpel_h_shufA]
-  %define shufA m5
-  %define shufB m7
-  %define shufC m6
- %endif
+    %define           shufA  m5
+    %define           shufB  m7
+    %define           shufC  m6
 %endif
-    PREP_8TAP_HV         m0, srcq+strideq*1, m5, m6
+    PREP_8TAP_HV         m0, srcq+strideq*1
     lea                srcq, [srcq+strideq*2]
-    PREP_8TAP_HV         m4, srcq+strideq*0, m5, m6
-%if cpuflag(ssse3)
+    PREP_8TAP_HV         m4, srcq+strideq*0
     mova                 m5, [base+pw_8192]
-%else
-    mova                 m5, [base+pw_2]
-%endif
-    PMULHRSW_8192        m0, m0, m5
-    PMULHRSW_8192        m4, m4, m5
+    pmulhrsw             m0, m5
+    pmulhrsw             m4, m5
     RESTORELINE_W8        6, m6
     punpcklwd            m5, m6, m0 ; 67
     punpcklwd            m6, m0, m4 ; 78
@@ -3925,7 +4627,6 @@ cglobal prep_8tap_8bpc, 1, 9, 0, tmp, src, stride, w, h, mx, my, stride3
     sub                 r5d, 1<<16
     jg .hv_w8_loop0
     RET
-%endmacro
 
 %macro movifprep 2
  %if isprep
@@ -7262,16 +7963,17 @@ DECLARE_REG_TMP 6, 8
 %else
 DECLARE_REG_TMP 1, 2
 %endif
+%define PUT_8TAP_SCALED_FN FN put_8tap_scaled,
 BILIN_SCALED_FN put
-FN put_8tap_scaled, sharp,          SHARP,   SHARP
-FN put_8tap_scaled, sharp_smooth,   SHARP,   SMOOTH
-FN put_8tap_scaled, smooth_sharp,   SMOOTH,  SHARP
-FN put_8tap_scaled, smooth,         SMOOTH,  SMOOTH
-FN put_8tap_scaled, sharp_regular,  SHARP,   REGULAR
-FN put_8tap_scaled, regular_sharp,  REGULAR, SHARP
-FN put_8tap_scaled, smooth_regular, SMOOTH,  REGULAR
-FN put_8tap_scaled, regular_smooth, REGULAR, SMOOTH
-FN put_8tap_scaled, regular,        REGULAR, REGULAR
+PUT_8TAP_SCALED_FN sharp,          SHARP,   SHARP,   put_8tap_scaled_8bpc
+PUT_8TAP_SCALED_FN sharp_smooth,   SHARP,   SMOOTH,  put_8tap_scaled_8bpc
+PUT_8TAP_SCALED_FN smooth_sharp,   SMOOTH,  SHARP,   put_8tap_scaled_8bpc
+PUT_8TAP_SCALED_FN smooth,         SMOOTH,  SMOOTH,  put_8tap_scaled_8bpc
+PUT_8TAP_SCALED_FN sharp_regular,  SHARP,   REGULAR, put_8tap_scaled_8bpc
+PUT_8TAP_SCALED_FN regular_sharp,  REGULAR, SHARP,   put_8tap_scaled_8bpc
+PUT_8TAP_SCALED_FN smooth_regular, SMOOTH,  REGULAR, put_8tap_scaled_8bpc
+PUT_8TAP_SCALED_FN regular_smooth, REGULAR, SMOOTH,  put_8tap_scaled_8bpc
+PUT_8TAP_SCALED_FN regular,        REGULAR, REGULAR
 MC_8TAP_SCALED put
 
 %if WIN64
@@ -7281,16 +7983,17 @@ DECLARE_REG_TMP 6, 7
 %else
 DECLARE_REG_TMP 1, 2
 %endif
+%define PREP_8TAP_SCALED_FN FN prep_8tap_scaled,
 BILIN_SCALED_FN prep
-FN prep_8tap_scaled, sharp,          SHARP,   SHARP
-FN prep_8tap_scaled, sharp_smooth,   SHARP,   SMOOTH
-FN prep_8tap_scaled, smooth_sharp,   SMOOTH,  SHARP
-FN prep_8tap_scaled, smooth,         SMOOTH,  SMOOTH
-FN prep_8tap_scaled, sharp_regular,  SHARP,   REGULAR
-FN prep_8tap_scaled, regular_sharp,  REGULAR, SHARP
-FN prep_8tap_scaled, smooth_regular, SMOOTH,  REGULAR
-FN prep_8tap_scaled, regular_smooth, REGULAR, SMOOTH
-FN prep_8tap_scaled, regular,        REGULAR, REGULAR
+PREP_8TAP_SCALED_FN sharp,          SHARP,   SHARP,   prep_8tap_scaled_8bpc
+PREP_8TAP_SCALED_FN sharp_smooth,   SHARP,   SMOOTH,  prep_8tap_scaled_8bpc
+PREP_8TAP_SCALED_FN smooth_sharp,   SMOOTH,  SHARP,   prep_8tap_scaled_8bpc
+PREP_8TAP_SCALED_FN smooth,         SMOOTH,  SMOOTH,  prep_8tap_scaled_8bpc
+PREP_8TAP_SCALED_FN sharp_regular,  SHARP,   REGULAR, prep_8tap_scaled_8bpc
+PREP_8TAP_SCALED_FN regular_sharp,  REGULAR, SHARP,   prep_8tap_scaled_8bpc
+PREP_8TAP_SCALED_FN smooth_regular, SMOOTH,  REGULAR, prep_8tap_scaled_8bpc
+PREP_8TAP_SCALED_FN regular_smooth, REGULAR, SMOOTH,  prep_8tap_scaled_8bpc
+PREP_8TAP_SCALED_FN regular,        REGULAR, REGULAR
 MC_8TAP_SCALED prep
 
 %if ARCH_X86_32
@@ -7369,7 +8072,7 @@ MC_8TAP_SCALED prep
   %define m15 m7
   %define m11 m7
  %endif
- %if notcpuflag(ssse3) || ARCH_X86_32
+ %if ARCH_X86_32
     pxor                m11, m11
  %endif
     lea               tmp1d, [myq+deltaq*4]
@@ -7446,7 +8149,7 @@ MC_8TAP_SCALED prep
  %endif
 %endif
 
-%macro WARP_AFFINE_8X8T 0
+%macro WARP_AFFINE_8X8 0
 %if ARCH_X86_64
 cglobal warp_affine_8x8t_8bpc, 6, 14, 16, 0x90, tmp, ts
 %else
@@ -7468,7 +8171,6 @@ cglobal warp_affine_8x8t_8bpc, 0, 7, 16, -0x130-copy_args, tmp, ts
     mova                m14, [esp+0xE0]
     mova                m15, [esp+0xF0]
 %endif
-%if cpuflag(ssse3)
     psrad               m12, 13
     psrad               m13, 13
     psrad               m14, 13
@@ -7478,22 +8180,6 @@ cglobal warp_affine_8x8t_8bpc, 0, 7, 16, -0x130-copy_args, tmp, ts
     mova                m13, [PIC_sym(pw_8192)]
     pmulhrsw            m12, m13 ; (x + (1 << 6)) >> 7
     pmulhrsw            m14, m13
-%else
- %if ARCH_X86_32
-  %define m10 m0
- %endif
-    mova                m10, [PIC_sym(pd_16384)]
-    paddd               m12, m10
-    paddd               m13, m10
-    paddd               m14, m10
-    paddd               m15, m10
-    psrad               m12, 15
-    psrad               m13, 15
-    psrad               m14, 15
-    psrad               m15, 15
-    packssdw            m12, m13
-    packssdw            m14, m15
-%endif
     mova       [tmpq+tsq*0], m12
     mova       [tmpq+tsq*2], m14
     dec            counterd
@@ -7506,9 +8192,7 @@ cglobal warp_affine_8x8t_8bpc, 0, 7, 16, -0x130-copy_args, tmp, ts
     call mangle(private_prefix %+ _warp_affine_8x8_8bpc_%+cpuname).main2
     lea                tmpq, [tmpq+tsq*4]
     jmp .loop
-%endmacro
 
-%macro WARP_AFFINE_8X8 0
 %if ARCH_X86_64
 cglobal warp_affine_8x8_8bpc, 6, 14, 16, 0x90, \
                               dst, ds, src, ss, abcd, mx, tmp2, alpha, beta, \
@@ -7557,11 +8241,7 @@ cglobal warp_affine_8x8_8bpc, 0, 7, 16, -0x130-copy_args, \
     lea                dstq, [dstq+dsq*2]
 .start:
 %if notcpuflag(sse4)
- %if cpuflag(ssse3)
   %define roundval pw_8192
- %else
-  %define roundval pd_262144
- %endif
  %if ARCH_X86_64
     mova                m10, [PIC_sym(roundval)]
  %else
@@ -7584,18 +8264,10 @@ cglobal warp_affine_8x8_8bpc, 0, 7, 16, -0x130-copy_args, \
     packusdw            m12, m13
     pavgw               m12, m11 ; (x + (1 << 10)) >> 11
 %else
- %if cpuflag(ssse3)
     psrad               m12, 17
     psrad               m13, 17
     packssdw            m12, m13
     pmulhrsw            m12, m10
- %else
-    paddd               m12, m10
-    paddd               m13, m10
-    psrad               m12, 19
-    psrad               m13, 19
-    packssdw            m12, m13
- %endif
 %endif
 %if ARCH_X86_32
  %define m14 m6
@@ -7609,18 +8281,10 @@ cglobal warp_affine_8x8_8bpc, 0, 7, 16, -0x130-copy_args, \
     packusdw            m14, m15
     pavgw               m14, m11 ; (x + (1 << 10)) >> 11
 %else
- %if cpuflag(ssse3)
     psrad               m14, 17
     psrad               m15, 17
     packssdw            m14, m15
     pmulhrsw            m14, m10
- %else
-    paddd               m14, m10
-    paddd               m15, m10
-    psrad               m14, 19
-    psrad               m15, 19
-    packssdw            m14, m15
- %endif
 %endif
     packuswb            m12, m14
     movq       [dstq+dsq*0], m12
@@ -7670,17 +8334,12 @@ ALIGN function_align
     lea             filterq, [PIC_sym(mc_warp_filter2)]
 %if ARCH_X86_64
     mov                 myd, r6m
- %if cpuflag(ssse3)
     pxor                m11, m11
- %endif
 %endif
     call .h
     psrld                m2, m0, 16
     psrld                m3, m1, 16
 %if ARCH_X86_32
- %if notcpuflag(ssse3)
-    mova [esp+gprsize+0x00], m2
- %endif
     mova [esp+gprsize+0x10], m3
 %endif
     call .h
@@ -7694,9 +8353,6 @@ ALIGN function_align
 %if ARCH_X86_64
  %define blendmask [rsp+gprsize+0x80]
 %else
- %if notcpuflag(ssse3)
-    mova                 m2, [esp+gprsize+0x00]
- %endif
     mova                 m3, [esp+gprsize+0x10]
  %define blendmask [esp+gprsize+0x120]
  %define m10 m7
@@ -7720,9 +8376,6 @@ ALIGN function_align
     mova [rsp+gprsize+0x30], m5
     call .h
 %if ARCH_X86_32
- %if notcpuflag(ssse3)
-    mova                 m2, [esp+gprsize+0x00]
- %endif
     mova                 m3, [esp+gprsize+0x10]
  %define m10 m5
 %endif
@@ -7882,7 +8535,6 @@ ALIGN function_align
     lea               tmp2d, [mxq+alphaq*1]
     shr                 mxd, 10
     shr               tmp1d, 10
-%if cpuflag(ssse3)
     movq                m14, [filterq+mxq  *8]  ; 2 X
     movq                 m9, [filterq+tmp1q*8]  ; 6 X
     lea               tmp1d, [tmp2q+alphaq*4]
@@ -7901,95 +8553,6 @@ ALIGN function_align
     pmaddubsw           m10, m9
     phaddw               m0, m15
     phaddw               m1, m10
-%else
- %if ARCH_X86_32
-  %define m11 m2
- %endif
-    pcmpeqw              m0, m0
-    psrlw               m14, m0, 8
-    psrlw               m15, m10, 8     ; 01 03 05 07  09 11 13 15
-    pand                m14, m10        ; 00 02 04 06  08 10 12 14
-    packuswb            m14, m15        ; 00 02 04 06  08 10 12 14  01 03 05 07  09 11 13 15
-    psrldq               m9, m0, 4
-    pshufd               m0, m14, q0220
-    pand                 m0, m9
-    psrldq              m14, 1          ; 02 04 06 08  10 12 14 01  03 05 07 09  11 13 15 __
-    pslldq              m15, m14, 12
-    por                  m0, m15    ; shufA
-    psrlw               m15, m0, 8
-    psraw               m11, m1, 8
-    psllw                m0, 8
-    psllw                m1, 8
-    psrlw                m0, 8
-    psraw                m1, 8
-    pmullw              m15, m11
-    pmullw               m0, m1
-    paddw                m0, m15    ; pmaddubsw m0, m1
-    pshufd              m15, m14, q0220
-    pand                m15, m9
-    psrldq              m14, 1          ; 04 06 08 10  12 14 01 03  05 07 09 11  13 15 __ __
-    pslldq               m1, m14, 12
-    por                 m15, m1     ; shufC
-    pshufd               m1, m14, q0220
-    pand                 m1, m9
-    psrldq              m14, 1          ; 06 08 10 12  14 01 03 05  07 09 11 13  15 __ __ __
-    pslldq              m11, m14, 12
-    por                  m1, m11    ; shufB
-    pshufd              m10, m14, q0220
-    pand                m10, m9
-    psrldq              m14, 1          ; 08 10 12 14  01 03 05 07  09 11 13 15  __ __ __ __
-    pslldq              m14, m14, 12
-    por                 m10, m14    ; shufD
-    psrlw                m9, m1, 8
-    psraw               m11, m8, 8
-    psllw                m1, 8
-    psllw                m8, 8
-    psrlw                m1, 8
-    psraw                m8, 8
-    pmullw               m9, m11
-    pmullw               m1, m8
-    paddw                m1, m9     ; pmaddubsw m1, m8
-    movq                m14, [filterq+mxq  *8]  ; 2 X
-    movq                 m9, [filterq+tmp1q*8]  ; 6 X
-    lea               tmp1d, [tmp2q+alphaq*4]
-    lea                 mxd, [tmp2q+betaq]  ; mx += beta
-    shr               tmp2d, 10
-    shr               tmp1d, 10
-    movhps              m14, [filterq+tmp2q*8]  ; 2 3
-    movhps               m9, [filterq+tmp1q*8]  ; 6 7
-    psrlw                m8, m15, 8
-    psraw               m11, m14, 8
-    psllw               m15, 8
-    psllw               m14, 8
-    psrlw               m15, 8
-    psraw               m14, 8
-    pmullw               m8, m11
-    pmullw              m15, m14
-    paddw               m15, m8     ; pmaddubsw m15, m14
-    psrlw                m8, m10, 8
-    psraw               m11, m9, 8
-    psllw               m10, 8
-    psllw                m9, 8
-    psrlw               m10, 8
-    psraw                m9, 8
-    pmullw               m8, m11
-    pmullw              m10, m9
-    paddw               m10, m8     ; pmaddubsw m10, m9
-    pslld                m8, m0, 16
-    pslld                m9, m1, 16
-    pslld               m14, m15, 16
-    pslld               m11, m10, 16
-    paddw                m0, m8
-    paddw                m1, m9
-    paddw               m15, m14
-    paddw               m10, m11
-    psrad                m0, 16
-    psrad                m1, 16
-    psrad               m15, 16
-    psrad               m10, 16
-    packssdw             m0, m15    ; phaddw m0, m15
-    packssdw             m1, m10    ; phaddw m1, m10
-%endif
     mova                m14, [PIC_sym(pw_8192)]
     mova                 m9, [PIC_sym(pd_32768)]
     pmaddwd              m0, m14 ; 17-bit intermediate, upshifted by 13
@@ -9575,17 +10138,7 @@ cglobal resize_8bpc, 0, 6, 8, 3 * 16, dst, dst_stride, src, src_stride, \
     RET
 
 INIT_XMM ssse3
-PREP_BILIN
-PREP_8TAP
 WARP_AFFINE_8X8
-WARP_AFFINE_8X8T
 
 INIT_XMM sse4
 WARP_AFFINE_8X8
-WARP_AFFINE_8X8T
-
-INIT_XMM sse2
-PREP_BILIN
-PREP_8TAP
-WARP_AFFINE_8X8
-WARP_AFFINE_8X8T
diff --git a/src/x86/refmvs.asm b/src/x86/refmvs.asm
index d95861f..085c9b3 100644
--- a/src/x86/refmvs.asm
+++ b/src/x86/refmvs.asm
@@ -92,6 +92,31 @@ JMP_TABLE splat_mv_avx2,      1, 2, 4, 8, 16, 32
 
 JMP_TABLE splat_mv_sse2,      1, 2, 4, 8, 16, 32
 
+struc rf
+    .frm_hdr:         resq 1
+    .iw4:             resd 1
+    .ih4:             resd 1
+    .iw8:             resd 1
+    .ih8:             resd 1
+    .sbsz:            resd 1
+    .use_rf_mvs:      resd 1
+    .sign_bias:       resb 7
+    .mfmv_sign:       resb 7
+    .pocdiff:         resb 7
+    .mfmv_ref:        resb 3
+    .mfmv_ref2cur:    resd 3
+    .mfmv_ref2ref:    resd 3*7
+    .n_mfmvs:         resd 1
+    .n_blocks:        resd 1
+    .rp:              resq 1
+    .rp_ref:          resq 1
+    .rp_proj:         resq 1
+    .rp_stride:       resq 1
+    .r:               resq 1
+    .n_tile_threads:  resd 1
+    .n_frame_threads: resd 1
+endstruc
+
 SECTION .text
 
 %macro movif32 2
@@ -341,16 +366,16 @@ cglobal load_tmvs, 6, 15, 4, -0x50, rf, tridx, xstart, xend, ystart, yend, \
                                     stride, rp_proj, roff, troff, \
                                     xendi, xstarti, iw8, ih8, dst
     xor           r14d, r14d
-    cmp dword [rfq+212], 1          ; n_tile_threads
-    mov           ih8d, [rfq+20]    ; rf->ih8
-    mov           iw8d, [rfq+16]    ; rf->iw8
+    cmp dword [rfq+rf.n_tile_threads], 1
+    mov           ih8d, [rfq+rf.ih8]
+    mov           iw8d, [rfq+rf.iw8]
     mov        xstartd, xstartd
     mov          xendd, xendd
     cmove       tridxd, r14d
     lea       xstartid, [xstartq-8]
     lea         xendid, [xendq+8]
-    mov        strideq, [rfq+184]
-    mov       rp_projq, [rfq+176]
+    mov        strideq, [rfq+rf.rp_stride]
+    mov       rp_projq, [rfq+rf.rp_proj]
     cmp           ih8d, yendd
     mov     [rsp+0x30], strideq
     cmovs        yendd, ih8d
@@ -397,7 +422,7 @@ cglobal load_tmvs, 6, 15, 4, -0x50, rf, tridx, xstart, xend, ystart, yend, \
     jg .init_xloop_start
  DEFINE_ARGS rf, _, xstart, xend, ystart, yend, n7, stride, \
              _, _, xendi, xstarti, stride5, _, n
-    mov           r13d, [rfq+152]   ; rf->n_mfmvs
+    mov           r13d, [rfq+rf.n_mfmvs]
     test          r13d, r13d
     jz .ret
     mov     [rsp+0x0c], r13d
@@ -418,14 +443,14 @@ cglobal load_tmvs, 6, 15, 4, -0x50, rf, tridx, xstart, xend, ystart, yend, \
  DEFINE_ARGS y, off, xstart, xend, ystart, rf, n7, refsign, \
              ref, rp_ref, xendi, xstarti, _, _, n
     mov            rfq, [rsp+0x48]
-    mov           refd, [rfq+56+nq*4]       ; ref2cur
+    mov           refd, [rfq+rf.mfmv_ref2cur+nq*4]
     cmp           refd, 0x80000000
     je .next_n
     mov     [rsp+0x40], refd
     mov           offq, [rsp+0x00]          ; ystart * stride * 5
-    movzx         refd, byte [rfq+53+nq]    ; rf->mfmv_ref[n]
+    movzx         refd, byte [rfq+rf.mfmv_ref+nq]
     lea       refsignq, [refq-4]
-    mov        rp_refq, [rfq+168]
+    mov        rp_refq, [rfq+rf.rp_ref]
     movq            m2, refsignq
     add           offq, [rp_refq+refq*8]    ; r = rp_ref[ref] + row_offset
     mov     [rsp+0x14], nd
@@ -452,8 +477,8 @@ cglobal load_tmvs, 6, 15, 4, -0x50, rf, tridx, xstart, xend, ystart, yend, \
     test          refd, refd
     jz .next_x_bad_ref
     mov            rfq, [rsp+0x48]
-    lea           r14d, [16+n7q+refq]
-    mov       ref2refd, [rfq+r14*4]         ; rf->mfmv_ref2ref[n][b_ref-1]
+    lea       ref2refd, [(rf.mfmv_ref2ref/4)+n7q+refq-1]
+    mov       ref2refd, [rfq+ref2refq*4]    ; rf->mfmv_ref2ref[n][b_ref-1]
     test      ref2refd, ref2refd
     jz .next_x_bad_ref
     lea          fracq, [mv_proj]
diff --git a/tests/checkasm/arm/checkasm_32.S b/tests/checkasm/arm/checkasm_32.S
index a186ef8..09b88aa 100644
--- a/tests/checkasm/arm/checkasm_32.S
+++ b/tests/checkasm/arm/checkasm_32.S
@@ -101,9 +101,10 @@ function checked_call_\variant, export=1
         mov             r12, r0
         mov             r0,  r2
         mov             r1,  r3
-        ldrd            r2,  r3,  [sp, #ARG_STACK_A + pushed]
+        ldr             r2,  [sp, #ARG_STACK_A + pushed]
+        ldr             r3,  [sp, #ARG_STACK_A + pushed + 4]
         @ Call the target function
-        blx             r12
+        v4blx           r12
 
         @ Load the number of stack parameters, stack canary and its reference
         ldr             r12, [sp, #ARG_STACK_A + pushed + 8 + 4*(MAX_ARGS-4)]
@@ -120,7 +121,8 @@ function checked_call_\variant, export=1
         movrel          r12, register_init
 .ifc \variant, vfp
 .macro check_reg_vfp, dreg, offset
-        ldrd            r2,  r3,  [r12, #8 * (\offset)]
+        ldr             r2,  [r12, #(8 * (\offset))]
+        ldr             r3,  [r12, #(8 * (\offset)) + 4]
         vmov            r0,  lr,  \dreg
         eor             r2,  r2,  r0
         eor             r3,  r3,  lr
@@ -148,7 +150,8 @@ function checked_call_\variant, export=1
         @ keep track of the checked GPR
         mov             r1,  #4
 .macro check_reg reg1, reg2=
-        ldrd            r2,  r3,  [r12], #8
+        ldr             r2,  [r12], #4
+        ldr             r3,  [r12], #4
         eors            r2,  r2,  \reg1
         bne             2f
         add             r1,  r1,  #1
@@ -198,4 +201,5 @@ function checked_call_\variant, export=1
 endfunc
 .endm
 
+clobbercheck novfp
 clobbercheck vfp
diff --git a/tests/checkasm/checkasm.c b/tests/checkasm/checkasm.c
index 2faa01f..c0b2b11 100644
--- a/tests/checkasm/checkasm.c
+++ b/tests/checkasm/checkasm.c
@@ -44,12 +44,16 @@
 #define ENABLE_VIRTUAL_TERMINAL_PROCESSING 0x04
 #endif
 #else
-#include <unistd.h>
 #include <time.h>
+#if HAVE_UNISTD_H
+#include <unistd.h>
+#endif
+#if HAVE_PTHREAD_SETAFFINITY_NP
 #include <pthread.h>
-#ifdef HAVE_PTHREAD_NP_H
+#if HAVE_PTHREAD_NP_H
 #include <pthread_np.h>
 #endif
+#endif
 #ifdef __APPLE__
 #include <mach/mach_time.h>
 #endif
@@ -125,8 +129,6 @@ static const struct {
 
 #if ARCH_AARCH64 && HAVE_SVE
 int checkasm_sve_length(void);
-#elif ARCH_RISCV
-int checkasm_get_vlenb(void);
 #endif
 
 typedef struct CheckasmFuncVersion {
@@ -732,7 +734,7 @@ int main(int argc, char *argv[]) {
             } else {
                 fprintf(stderr, "checkasm: running on cpu %lu\n", affinity);
             }
-#elif defined(HAVE_PTHREAD_SETAFFINITY_NP) && defined(CPU_SET)
+#elif HAVE_PTHREAD_SETAFFINITY_NP && defined(CPU_SET)
             cpu_set_t set;
             CPU_ZERO(&set);
             CPU_SET(affinity, &set);
@@ -832,6 +834,14 @@ int main(int argc, char *argv[]) {
             state.simd_warmup = checkasm_warmup_avx2;
         checkasm_simd_warmup();
 #endif
+#if ARCH_ARM
+        void checkasm_checked_call_vfp(void *func, int dummy, ...);
+        void checkasm_checked_call_novfp(void *func, int dummy, ...);
+        if (cpu_flags & DAV1D_ARM_CPU_FLAG_NEON)
+            checkasm_checked_call_ptr = checkasm_checked_call_vfp;
+        else
+            checkasm_checked_call_ptr = checkasm_checked_call_novfp;
+#endif
 #if ARCH_X86
         unsigned checkasm_init_x86(char *name);
         char name[48];
@@ -841,10 +851,8 @@ int main(int argc, char *argv[]) {
         fprintf(stderr, "checkasm: %s (%08X) using random seed %u\n", name, cpuid, state.seed);
 #elif ARCH_RISCV
         char buf[32] = "";
-        if (cpu_flags & DAV1D_RISCV_CPU_FLAG_V) {
-            const int vlen = 8*checkasm_get_vlenb();
-            snprintf(buf, sizeof(buf), "VLEN=%i bits, ", vlen);
-        }
+        if (cpu_flags & DAV1D_RISCV_CPU_FLAG_V)
+            snprintf(buf, sizeof(buf), "VLEN=%i bits, ", dav1d_get_vlen());
         fprintf(stderr, "checkasm: %susing random seed %u\n", buf, state.seed);
 #elif ARCH_AARCH64 && HAVE_SVE
         char buf[48] = "";
@@ -1126,3 +1134,7 @@ void checkasm_simd_warmup(void)
         state.simd_warmup();
 }
 #endif
+
+#if ARCH_ARM
+void (*checkasm_checked_call_ptr)(void *func, int dummy, ...);
+#endif
diff --git a/tests/checkasm/checkasm.h b/tests/checkasm/checkasm.h
index 29de9b6..65c4d53 100644
--- a/tests/checkasm/checkasm.h
+++ b/tests/checkasm/checkasm.h
@@ -68,6 +68,10 @@ typedef sigjmp_buf checkasm_context;
 #include "include/common/bitdepth.h"
 #include "include/common/intops.h"
 
+#if ARCH_ARM
+#include "src/arm/arm-arch.h"
+#endif
+
 int xor128_rand(void);
 #define rnd xor128_rand
 
@@ -254,7 +258,7 @@ void checkasm_simd_warmup(void);
  * handled orthogonally from integer parameters passed in GPR registers. */
 #define IGNORED_FP_ARGS 8
 #endif
-#ifdef HAVE_C11_GENERIC
+#if HAVE_C11_GENERIC
 #define clobber_type(arg) _Generic((void (*)(void*, arg))NULL,\
      void (*)(void*, int32_t ): clobber_mask |= 1 << mpos++,\
      void (*)(void*, uint32_t): clobber_mask |= 1 << mpos++,\
@@ -302,12 +306,12 @@ void checkasm_simd_warmup(void);
 /* Use a dummy argument, to offset the real parameters by 2, not only 1.
  * This makes sure that potential 8-byte-alignment of parameters is kept
  * the same even when the extra parameters have been removed. */
-void checkasm_checked_call_vfp(void *func, int dummy, ...);
+extern void (*checkasm_checked_call_ptr)(void *func, int dummy, ...);
 #define declare_new(ret, ...)\
     ret (*checked_call)(void *, int dummy, __VA_ARGS__,\
                         int, int, int, int, int, int, int, int,\
                         int, int, int, int, int, int, int) =\
-    (void *)checkasm_checked_call_vfp;
+    (void *)checkasm_checked_call_ptr;
 #define call_new(...)\
     (checkasm_set_signal_handler_state(1),\
      checked_call(func_new, 0, __VA_ARGS__, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0));\
@@ -330,6 +334,17 @@ void checkasm_stack_clobber(uint64_t clobber, ...);
                   7, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0));\
     checkasm_set_signal_handler_state(0)
 #elif ARCH_RISCV
+#define declare_new(ret, ...)\
+    ret (*checked_call)(void *, int, int, int, int, int, int, int,\
+                        __VA_ARGS__, int, int, int, int, int, int, int, int,\
+                        int, int, int, int, int, int, int) =\
+    (void *)checkasm_checked_call;
+#define call_new(...)\
+    (checkasm_set_signal_handler_state(1),\
+     checked_call(func_new, 0, 0, 0, 0, 0, 0, 0, __VA_ARGS__,\
+                  7, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0));\
+    checkasm_set_signal_handler_state(0)
+#elif ARCH_LOONGARCH
 #define declare_new(ret, ...)\
     ret (*checked_call)(void *, int, int, int, int, int, int, int,\
                         __VA_ARGS__, int, int, int, int, int, int, int, int,\
diff --git a/tests/checkasm/itx.c b/tests/checkasm/itx.c
index c7cc411..b0de65d 100644
--- a/tests/checkasm/itx.c
+++ b/tests/checkasm/itx.c
@@ -130,7 +130,8 @@ static void fwht4_1d(double *const out, const double *const in)
 
 static int copy_subcoefs(coef *coeff,
                          const enum RectTxfmSize tx, const enum TxfmType txtp,
-                         const int sw, const int sh, const int subsh)
+                         const int sw, const int sh, const int subsh,
+                         int *const max_eob)
 {
     /* copy the topleft coefficients such that the return value (being the
      * coefficient scantable index for the eob token) guarantees that only
@@ -160,6 +161,7 @@ static int copy_subcoefs(coef *coeff,
         } else if (!eob && (rcx > sub_low || rcy > sub_low))
             eob = n; /* lower boundary */
     }
+    *max_eob = n - 1;
 
     if (eob)
         eob += rnd() % (n - eob - 1);
@@ -182,7 +184,7 @@ static int copy_subcoefs(coef *coeff,
 
 static int ftx(coef *const buf, const enum RectTxfmSize tx,
                const enum TxfmType txtp, const int w, const int h,
-               const int subsh, const int bitdepth_max)
+               const int subsh, int *const max_eob, const int bitdepth_max)
 {
     double out[64 * 64], temp[64 * 64];
     const double scale = scaling_factors[ctz(w * h) - 4];
@@ -236,7 +238,7 @@ static int ftx(coef *const buf, const enum RectTxfmSize tx,
         for (int x = 0; x < sw; x++)
             buf[y * sw + x] = (coef) (out[y * w + x] + 0.5);
 
-    return copy_subcoefs(buf, tx, txtp, sw, sh, subsh);
+    return copy_subcoefs(buf, tx, txtp, sw, sh, subsh, max_eob);
 }
 
 static void check_itxfm_add(Dav1dInvTxfmDSPContext *const c,
@@ -272,7 +274,9 @@ static void check_itxfm_add(Dav1dInvTxfmDSPContext *const c,
                                bpc))
                 {
                     const int bitdepth_max = (1 << bpc) - 1;
-                    const int eob = ftx(coeff[0], tx, txtp, w, h, subsh, bitdepth_max);
+                    int max_eob;
+                    const int eob = ftx(coeff[0], tx, txtp, w, h, subsh, &max_eob,
+                                        bitdepth_max);
                     memcpy(coeff[1], coeff[0], sizeof(*coeff));
 
                     CLEAR_PIXEL_RECT(c_dst);
@@ -295,7 +299,7 @@ static void check_itxfm_add(Dav1dInvTxfmDSPContext *const c,
                         fail();
 
                     bench_new(alternate(c_dst, a_dst), a_dst_stride,
-                              alternate(coeff[0], coeff[1]), eob HIGHBD_TAIL_SUFFIX);
+                              alternate(coeff[0], coeff[1]), max_eob HIGHBD_TAIL_SUFFIX);
                 }
     }
     report("add_%dx%d", w, h);
diff --git a/tests/checkasm/loongarch/checkasm.S b/tests/checkasm/loongarch/checkasm.S
new file mode 100644
index 0000000..bf5a2aa
--- /dev/null
+++ b/tests/checkasm/loongarch/checkasm.S
@@ -0,0 +1,213 @@
+/******************************************************************************
+ * Copyright © 2018, VideoLAN and dav1d authors
+ * Copyright © 2024, Loongson Technology Corporation Limited
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ *    list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ *    this list of conditions and the following disclaimer in the documentation
+ *    and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *****************************************************************************/
+
+#define PRIVATE_PREFIX checkasm_
+
+#include "src/loongarch/loongson_asm.S"
+
+const register_init, align=4
+.quad 0x21f86d66c8ca00ce
+.quad 0x75b6ba21077c48ad
+.quad 0xed56bb2dcb3c7736
+.quad 0x8bda43d3fd1a7e06
+.quad 0xb64a9c9e5d318408
+.quad 0xdf9a54b303f1d3a3
+.quad 0x4a75479abd64e097
+.quad 0x249214109d5d1c88
+.quad 0x1a1b2550a612b48c
+.quad 0x79445c159ce79064
+.quad 0x2eed899d5a28ddcd
+.quad 0x86b2536fcd8cf636
+.quad 0xb0856806085e7943
+.quad 0x3f2bf84fc0fcca4e
+.quad 0xacbd382dcf5b8de2
+.quad 0xd229e1f5b281303f
+.quad 0x71aeaff20b095fd9
+.quad 0xab63e2e11fa38ed9
+endconst
+
+const error_message
+.asciz "failed to preserve register"
+endconst
+
+// max number of args used by any asm function.
+#define MAX_ARGS 15
+
+#define CLOBBER_STACK ((8*MAX_ARGS + 15) & ~15)
+
+// Fill dirty data at stack space
+function stack_clobber
+    move    t0,     sp
+    addi.d  t1,     zero,   CLOBBER_STACK
+1:
+    st.d    a0,     sp,     0x00
+    st.d    a1,     sp,    -0x08
+    addi.d  sp,     sp,    -0x10
+    addi.d  t1,     t1,    -0x10
+    blt     zero,   t1,     1b
+    move    sp,     t0
+endfunc
+
+#define ARG_STACK ((8*(MAX_ARGS - 8) + 15) & ~15)
+
+function checked_call
+    // Saved s0 - s8, fs0 - fs7
+    move    t4,     sp
+    addi.d  sp,     sp,     -136
+    st.d    s0,     sp,     0
+    st.d    s1,     sp,     8
+    st.d    s2,     sp,     16
+    st.d    s3,     sp,     24
+    st.d    s4,     sp,     32
+    st.d    s5,     sp,     40
+    st.d    s6,     sp,     48
+    st.d    s7,     sp,     56
+    st.d    s8,     sp,     64
+    fst.d   fs0,    sp,     72
+    fst.d   fs1,    sp,     80
+    fst.d   fs2,    sp,     88
+    fst.d   fs3,    sp,     96
+    fst.d   fs4,    sp,     104
+    fst.d   fs5,    sp,     112
+    fst.d   fs6,    sp,     120
+    fst.d   fs7,    sp,     128
+
+    la.local    t1,   register_init
+    ld.d    s0,     t1,     0
+    ld.d    s1,     t1,     8
+    ld.d    s2,     t1,     16
+    ld.d    s3,     t1,     24
+    ld.d    s4,     t1,     32
+    ld.d    s5,     t1,     40
+    ld.d    s6,     t1,     48
+    ld.d    s7,     t1,     56
+    ld.d    s8,     t1,     64
+    fld.d   fs0,    t1,     72
+    fld.d   fs1,    t1,     80
+    fld.d   fs2,    t1,     88
+    fld.d   fs3,    t1,     96
+    fld.d   fs4,    t1,     104
+    fld.d   fs5,    t1,     112
+    fld.d   fs6,    t1,     120
+    fld.d   fs7,    t1,     128
+
+    addi.d  sp,     sp,     -16
+    st.d    a1,     sp,     0 // ok
+    st.d    ra,     sp,     8 // Ret address
+
+    addi.d  sp,     sp,     -ARG_STACK
+
+    addi.d  t0,     zero,   8*8
+    xor     t1,     t1,     t1
+.rept MAX_ARGS - 8
+    // Skip the first 8 args, that are loaded into registers
+    ldx.d   t2,     t4,     t0
+    stx.d   t2,     sp,     t1
+    addi.d  t0,     t0,     8
+    addi.d  t1,     t1,     8
+.endr
+    move    t3,     a0  // Func
+    ld.d    a0,     t4,     0
+    ld.d    a1,     t4,     8
+    ld.d    a2,     t4,     16
+    ld.d    a3,     t4,     24
+    ld.d    a4,     t4,     32
+    ld.d    a5,     t4,     40
+    ld.d    a6,     t4,     48
+    ld.d    a7,     t4,     56
+
+    jirl    ra,     t3,     0
+
+    addi.d  sp,     sp,     ARG_STACK
+    ld.d    t2,     sp,     0 // ok
+    ld.d    ra,     sp,     8 // Ret address
+    addi.d  sp,     sp,     16
+
+    la.local    t1,   register_init
+    xor         t3,   t3,   t3
+
+.macro check_reg_gr reg1
+    ld.d    t0,     t1,      0
+    xor     t0,     $s\reg1, t0
+    or      t3,     t3,      t0
+    addi.d  t1,     t1,      8
+.endm
+    check_reg_gr 0
+    check_reg_gr 1
+    check_reg_gr 2
+    check_reg_gr 3
+    check_reg_gr 4
+    check_reg_gr 5
+    check_reg_gr 6
+    check_reg_gr 7
+    check_reg_gr 8
+
+.macro check_reg_fr reg1
+    ld.d        t0,     t1,     0
+    movfr2gr.d  t4,     $fs\reg1
+    xor         t0,     t0,     t4
+    or          t3,     t3,     t0
+    addi.d      t1,     t1,     8
+.endm
+    check_reg_fr 0
+    check_reg_fr 1
+    check_reg_fr 2
+    check_reg_fr 3
+    check_reg_fr 4
+    check_reg_fr 5
+    check_reg_fr 6
+    check_reg_fr 7
+
+    beqz    t3,     0f
+
+    st.d        zero,   t2,     0x00 // Set OK to 0
+    la.local    a0,     error_message
+    addi.d      sp,     sp,     -8
+    st.d        ra,     sp,     0
+    bl          puts
+    ld.d        ra,     sp,     0
+    addi.d      sp,     sp,     8
+0:
+    ld.d    s0,     sp,     0
+    ld.d    s1,     sp,     8
+    ld.d    s2,     sp,     16
+    ld.d    s3,     sp,     24
+    ld.d    s4,     sp,     32
+    ld.d    s5,     sp,     40
+    ld.d    s6,     sp,     48
+    ld.d    s7,     sp,     56
+    ld.d    s8,     sp,     64
+    fld.d   fs0,    sp,     72
+    fld.d   fs1,    sp,     80
+    fld.d   fs2,    sp,     88
+    fld.d   fs3,    sp,     96
+    fld.d   fs4,    sp,     104
+    fld.d   fs5,    sp,     112
+    fld.d   fs6,    sp,     120
+    fld.d   fs7,    sp,     128
+    addi.d  sp,     sp,     136
+endfunc
diff --git a/tests/checkasm/msac.c b/tests/checkasm/msac.c
index 2866bf5..3771c22 100644
--- a/tests/checkasm/msac.c
+++ b/tests/checkasm/msac.c
@@ -280,6 +280,8 @@ void checkasm_check_msac(void) {
         c.decode_symbol_adapt16 = dav1d_msac_decode_symbol_adapt16_lsx;
         c.decode_bool_adapt     = dav1d_msac_decode_bool_adapt_lsx;
         c.decode_bool           = dav1d_msac_decode_bool_lsx;
+        c.decode_bool_equi      = dav1d_msac_decode_bool_equi_lsx;
+        c.decode_hi_tok         = dav1d_msac_decode_hi_tok_lsx;
     }
 #elif ARCH_X86 && HAVE_ASM
     if (dav1d_get_cpu_flags() & DAV1D_X86_CPU_FLAG_SSE2) {
diff --git a/tests/checkasm/riscv/checkasm_64.S b/tests/checkasm/riscv/checkasm_64.S
index 8557eab..0d02e5f 100644
--- a/tests/checkasm/riscv/checkasm_64.S
+++ b/tests/checkasm/riscv/checkasm_64.S
@@ -83,11 +83,6 @@ endconst
 
 thread_local saved_regs, quads=29 # 5 + 12 + 12
 
-function get_vlenb, export=1
-  csrr a0, vlenb
-  ret
-endfunc
-
 function checked_call, export=1, ext=v
   /* Save the function ptr, RA, SP, unallocatable and callee-saved registers */
   la.tls.ie t0, saved_regs
diff --git a/tests/dav1d_argon.bash b/tests/dav1d_argon.bash
index ead3e6e..fd5bade 100755
--- a/tests/dav1d_argon.bash
+++ b/tests/dav1d_argon.bash
@@ -6,6 +6,9 @@ FILMGRAIN=1
 CPUMASK=-1
 THREADS=1
 JOBS=0
+WRAP=""
+FAIL_FAST=0
+
 
 usage() {
     NAME=$(basename "$0")
@@ -15,12 +18,14 @@ usage() {
         printf "Used to verify that dav1d can decode the Argon AV1 test vectors correctly.\n\n"
         printf " DIRECTORY one or more dirs in the argon folder to check against\n"
         printf "             (default: everything except large scale tiles and stress files)\n"
+        printf " -f        fail fast\n"
         printf " -d dav1d  path to dav1d executable (default: tools/dav1d)\n"
         printf " -a dir    path to argon dir (default: 'tests/argon' if found; '.' otherwise)\n"
         printf " -g \$num   enable filmgrain (default: 1)\n"
         printf " -c \$mask  use restricted cpumask (default: -1)\n"
         printf " -t \$num   number of threads per dav1d (default: 1)\n"
-        printf " -j \$num   number of parallel dav1d processes (default: 0)\n\n"
+        printf " -j \$num   number of parallel dav1d processes (default: 0)\n"
+        printf " -w tool   execute dav1d with a wrapper tool\n\n"
     } >&2
     exit 1
 }
@@ -32,6 +37,7 @@ error() {
 
 fail() {
     printf "\033[1K\rMismatch in %s\n" "$1"
+    [[ $FAIL_FAST = 1 ]] && exit 1
     (( failed++ ))
 }
 
@@ -79,8 +85,11 @@ if [ -d "$tests_dir/argon" ]; then
     ARGON_DIR="$tests_dir/argon"
 fi
 
-while getopts ":d:a:g:c:t:j:" opt; do
+while getopts ":d:a:g:c:t:j:w:f" opt; do
     case "$opt" in
+        f)
+            FAIL_FAST=1
+            ;;
         d)
             DAV1D="$OPTARG"
             ;;
@@ -99,6 +108,9 @@ while getopts ":d:a:g:c:t:j:" opt; do
         j)
             JOBS="$OPTARG"
             ;;
+        w)
+            WRAP="$OPTARG"
+            ;;
         \?)
             printf "Error! Invalid option: -%s\n" "$OPTARG" >&2
             usage
@@ -158,7 +170,7 @@ for i in "${!files[@]}"; do
     md5=${md5/ */}
 
     printf '\033[1K\r[%3d%% %*d/%d] Verifying %s' "$(((i+1)*100/num_files))" "${#num_files}" "$((i+1))" "$num_files" "${f#"$ARGON_DIR"/}"
-    cmd=("$DAV1D" -i "$f" --filmgrain "$FILMGRAIN" --verify "$md5" --cpumask "$CPUMASK" --threads "$THREADS" -q)
+    cmd=($WRAP "$DAV1D" -i "$f" --filmgrain "$FILMGRAIN" --verify "$md5" --cpumask "$CPUMASK" --threads "$THREADS" -q)
     if [ "$JOBS" -gt 1 ]; then
         "${cmd[@]}" 2>/dev/null &
         p=$!
diff --git a/tests/meson.build b/tests/meson.build
index 38a591b..619b80f 100644
--- a/tests/meson.build
+++ b/tests/meson.build
@@ -73,6 +73,8 @@ if is_asm_enabled
         checkasm_asm_sources += files('checkasm/riscv/checkasm_64.S')
     elif host_machine.cpu_family().startswith('x86')
         checkasm_asm_objs += nasm_gen.process(files('checkasm/x86/checkasm.asm'))
+    elif host_machine.cpu_family().startswith('loongarch')
+        checkasm_asm_objs += files('checkasm/loongarch/checkasm.S')
     endif
 
     if use_gaspp
diff --git a/tests/seek_stress.c b/tests/seek_stress.c
index a85ec86..7f75ea8 100644
--- a/tests/seek_stress.c
+++ b/tests/seek_stress.c
@@ -60,7 +60,7 @@ static unsigned get_seed(void) {
 static unsigned get_seed(void) {
 #ifdef __APPLE__
     return (unsigned) mach_absolute_time();
-#elif defined(HAVE_CLOCK_GETTIME)
+#elif HAVE_CLOCK_GETTIME
     struct timespec ts;
     clock_gettime(CLOCK_MONOTONIC, &ts);
     return (unsigned) (1000000000ULL * ts.tv_sec + ts.tv_nsec);
diff --git a/tools/compat/getopt.c b/tools/compat/getopt.c
index ac1fda4..ab375bd 100644
--- a/tools/compat/getopt.c
+++ b/tools/compat/getopt.c
@@ -55,7 +55,11 @@
 #include <getopt.h>
 #include <stdarg.h>
 #include <stdio.h>
+#ifdef _WIN32
 #include <windows.h>
+#else
+#include <err.h>
+#endif
 
 #define	REPLACE_GETOPT		/* use this getopt as the system getopt(3) */
 
@@ -80,12 +84,6 @@ char    *optarg;		/* argument associated with option */
 #define	BADARG		((*options == ':') ? (int)':' : (int)'?')
 #define	INORDER 	(int)1
 
-#ifndef __CYGWIN__
-#define __progname __argv[0]
-#else
-extern char __declspec(dllimport) *__progname;
-#endif
-
 #ifdef __CYGWIN__
 static char EMSG[] = "";
 #else
@@ -113,6 +111,13 @@ static const char noarg[] = "option doesn't take an argument -- %.*s";
 static const char illoptchar[] = "unknown option -- %c";
 static const char illoptstring[] = "unknown option -- %s";
 
+#ifdef _WIN32
+#ifndef __CYGWIN__
+#define __progname __argv[0]
+#else
+extern char __declspec(dllimport) *__progname;
+#endif
+
 static void
 _vwarnx(const char *fmt,va_list ap)
 {
@@ -130,6 +135,7 @@ warnx(const char *fmt,...)
   _vwarnx(fmt,ap);
   va_end(ap);
 }
+#endif
 
 /*
  * Compute the greatest common divisor of a and b.
diff --git a/tools/dav1d.c b/tools/dav1d.c
index 4d8d072..eb19a80 100644
--- a/tools/dav1d.c
+++ b/tools/dav1d.c
@@ -38,10 +38,10 @@
 #include <stdio.h>
 #include <string.h>
 #include <time.h>
-#ifdef HAVE_UNISTD_H
+#if HAVE_UNISTD_H
 # include <unistd.h>
 #endif
-#ifdef HAVE_IO_H
+#if HAVE_IO_H
 # include <io.h>
 #endif
 #ifdef _WIN32
@@ -68,7 +68,7 @@ static uint64_t get_time_nanos(void) {
     uint64_t seconds = t.QuadPart / frequency.QuadPart;
     uint64_t fractions = t.QuadPart % frequency.QuadPart;
     return 1000000000 * seconds + 1000000000 * fractions / frequency.QuadPart;
-#elif defined(HAVE_CLOCK_GETTIME)
+#elif HAVE_CLOCK_GETTIME
     struct timespec ts;
     clock_gettime(CLOCK_MONOTONIC, &ts);
     return 1000000000ULL * ts.tv_sec + ts.tv_nsec;
diff --git a/tools/dav1d_cli_parse.c b/tools/dav1d_cli_parse.c
index f425964..134be46 100644
--- a/tools/dav1d_cli_parse.c
+++ b/tools/dav1d_cli_parse.c
@@ -35,7 +35,7 @@
 #include <stdio.h>
 #include <stdlib.h>
 #include <string.h>
-#ifdef HAVE_UNISTD_H
+#if HAVE_UNISTD_H
 # include <unistd.h>
 #endif
 
diff --git a/tools/input/parse.h b/tools/input/parse.h
index f5805e8..f39f80f 100644
--- a/tools/input/parse.h
+++ b/tools/input/parse.h
@@ -89,6 +89,8 @@ static inline int parse_obu_header(const uint8_t *buf, int buf_size,
     buf_size--;
 
     if (extension_flag) {
+        if (!buf_size)
+            return -1;
         buf++;
         buf_size--;
         // ignore fields
diff --git a/tools/input/section5.c b/tools/input/section5.c
index db1b34c..99cb761 100644
--- a/tools/input/section5.c
+++ b/tools/input/section5.c
@@ -32,7 +32,9 @@
 #include <stdint.h>
 #include <stdlib.h>
 #include <string.h>
+#if HAVE_SYS_TYPES_H
 #include <sys/types.h>
+#endif
 
 #include "dav1d/headers.h"
 
diff --git a/tools/output/md5.c b/tools/output/md5.c
index 7d192c2..cfad4f0 100644
--- a/tools/output/md5.c
+++ b/tools/output/md5.c
@@ -31,7 +31,6 @@
 #include <stdio.h>
 #include <stdlib.h>
 #include <string.h>
-#include <sys/stat.h>
 
 #include "common/intops.h"
 
diff --git a/tools/output/y4m2.c b/tools/output/y4m2.c
index 8766f64..40411d1 100644
--- a/tools/output/y4m2.c
+++ b/tools/output/y4m2.c
@@ -32,7 +32,6 @@
 #include <stdio.h>
 #include <stdlib.h>
 #include <string.h>
-#include <sys/stat.h>
 
 #include "output/muxer.h"
 
diff --git a/tools/output/yuv.c b/tools/output/yuv.c
index 406f284..e0c0ec4 100644
--- a/tools/output/yuv.c
+++ b/tools/output/yuv.c
@@ -31,7 +31,6 @@
 #include <stdio.h>
 #include <stdlib.h>
 #include <string.h>
-#include <sys/stat.h>
 
 #include "output/muxer.h"
 
```

