```diff
diff --git a/Android.bp b/Android.bp
index 9fa270e52..620e44764 100644
--- a/Android.bp
+++ b/Android.bp
@@ -55,12 +55,14 @@ aom_av1_common_intrin_neon_dotprod = [
     "av1/common/arm/av1_convolve_scale_neon_dotprod.c",
     "av1/common/arm/compound_convolve_neon_dotprod.c",
     "av1/common/arm/convolve_neon_dotprod.c",
+    "av1/common/arm/resize_neon_dotprod.c",
 ]
 
 aom_av1_common_intrin_neon_i8mm = [
     "av1/common/arm/av1_convolve_scale_neon_i8mm.c",
     "av1/common/arm/compound_convolve_neon_i8mm.c",
     "av1/common/arm/convolve_neon_i8mm.c",
+    "av1/common/arm/resize_neon_i8mm.c",
     "av1/common/arm/warp_plane_neon_i8mm.c",
 ]
 
@@ -321,7 +323,6 @@ aom_av1_encoder_sources = [
     "av1/encoder/superres_scale.c",
     "av1/encoder/svc_layercontext.c",
     "av1/encoder/temporal_filter.c",
-    "av1/encoder/thirdpass.c",
     "av1/encoder/tokenize.c",
     "av1/encoder/tpl_model.c",
     "av1/encoder/tx_search.c",
@@ -360,7 +361,6 @@ aom_dsp_common_asm_sse2 = [
     "aom_dsp/x86/aom_high_subpixel_bilinear_sse2.asm",
     "aom_dsp/x86/highbd_intrapred_asm_sse2.asm",
     "aom_dsp/x86/intrapred_asm_sse2.asm",
-    "aom_dsp/x86/inv_wht_sse2.asm",
 ]
 
 aom_dsp_common_asm_ssse3 = [
diff --git a/CHANGELOG b/CHANGELOG
index 8ffa7d22e..f828efb7b 100644
--- a/CHANGELOG
+++ b/CHANGELOG
@@ -1,3 +1,46 @@
+2024-10-24 v3.11.0
+  This release includes perceptual quality improvements, binary size reduction
+  under certain configurations and many bug fixes. This release changes the
+  default encoder configuration for the AOM_USAGE_REALTIME mode. This release
+  is ABI compatible with the last release.
+
+  - Perceptual Quality Improvements
+    * Visual quality improvements for RTC screen content
+      * Higher quality on scene or slide changes
+      * Faster quality ramp-up for static content after scene change
+      * Quality improvements for screen content with active maps
+
+  - Speedup
+    * Added and improved Neon SIMD paths for dynamic frame scaling with ~1.5%
+      overall encoding speedup.
+
+  - Other Improvements
+    * Binary size reduction: 10% compared with last release, with
+      CONFIG_REALTIME_ONLY enabled, CONFIG_AV1_DECODER and
+      CONFIG_AV1_HIGHBITDEPTH disabled.
+    * Update default_extra_cfg for CONFIG_REALTIME_ONLY to provide proper RTC
+      defaults settings
+    * Change the default valuess of the following encoder config options in the
+      AOM_USAGE_REALTIME mode:
+      *  rc_overshoot_pct and rc_undershoot_pct are changed from 25 to 50
+      *  rc_buf_sz is changed from 6000 to 1000
+      *  rc_buf_initial_sz is changed from 4000 to 600
+      *  rc_buf_optimal_sz is changed from 5000 to 600
+
+  - Bug Fixes
+    * aomedia:363016123: rtc: Fix setting of intra-only frame for
+      set_ref_frame_config and add checks
+    * aomedia:42302583: rtc: Fix for artifacts for screen with active_maps
+    * b:365088425: rtc: Allow for lower-QP on scene/slide changes
+    * b:367285382: Fix to encoder quality max-out too early for screen
+    * b:362936830: rtc: Allow QP to decrease more aggressively for static
+      content
+    * b:361617762: Clamp the calculation of sb64_target_rate to INT_MAX
+    * chromium:362123224: rtc-svc: Reset ref_map_idx for references not used
+    * chromium:367892770: Fix to possible integer overflow in reset_rc
+    * webrtc:369633254: rtc-svc: Fix to reset ref_idx for svc
+    * Fix exit condition in rate correction update
+
 2024-08-27 v3.10.0
   This release includes new codec interfaces, compression efficiency and
   perceptual improvements, speedup and memory optimizations and many bug
diff --git a/CMakeLists.txt b/CMakeLists.txt
index b21b3f924..a95c312fe 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -58,9 +58,9 @@ endif()
 # passed to libtool.
 #
 # We set SO_FILE_VERSION = [c-a].a.r
-set(LT_CURRENT 13)
+set(LT_CURRENT 14)
 set(LT_REVISION 0)
-set(LT_AGE 10)
+set(LT_AGE 11)
 math(EXPR SO_VERSION "${LT_CURRENT} - ${LT_AGE}")
 set(SO_FILE_VERSION "${SO_VERSION}.${LT_AGE}.${LT_REVISION}")
 unset(LT_CURRENT)
@@ -505,13 +505,15 @@ if(CONFIG_AV1_ENCODER)
     add_executable(twopass_encoder "${AOM_ROOT}/examples/twopass_encoder.c"
                                    $<TARGET_OBJECTS:aom_common_app_util>
                                    $<TARGET_OBJECTS:aom_encoder_app_util>)
-    add_executable(noise_model "${AOM_ROOT}/examples/noise_model.c"
-                               $<TARGET_OBJECTS:aom_common_app_util>
-                               $<TARGET_OBJECTS:aom_encoder_app_util>)
-    add_executable(photon_noise_table
-                   "${AOM_ROOT}/examples/photon_noise_table.c"
-                   $<TARGET_OBJECTS:aom_common_app_util>
-                   $<TARGET_OBJECTS:aom_encoder_app_util>)
+    if(NOT BUILD_SHARED_LIBS AND NOT CONFIG_REALTIME_ONLY)
+      add_executable(noise_model "${AOM_ROOT}/examples/noise_model.c"
+                                 $<TARGET_OBJECTS:aom_common_app_util>
+                                 $<TARGET_OBJECTS:aom_encoder_app_util>)
+      add_executable(photon_noise_table
+                     "${AOM_ROOT}/examples/photon_noise_table.c"
+                     $<TARGET_OBJECTS:aom_common_app_util>
+                     $<TARGET_OBJECTS:aom_encoder_app_util>)
+    endif()
     add_executable(scalable_encoder "${AOM_ROOT}/examples/scalable_encoder.c"
                                     $<TARGET_OBJECTS:aom_common_app_util>
                                     $<TARGET_OBJECTS:aom_encoder_app_util>)
@@ -521,9 +523,12 @@ if(CONFIG_AV1_ENCODER)
     target_link_libraries(svc_encoder_rtc ${AOM_LIB_LINK_TYPE} aom_av1_rc)
 
     # Maintain a list of encoder example targets.
-    list(APPEND AOM_ENCODER_EXAMPLE_TARGETS aomenc lossless_encoder noise_model
-                photon_noise_table set_maps simple_encoder scalable_encoder
-                svc_encoder_rtc twopass_encoder)
+    list(APPEND AOM_ENCODER_EXAMPLE_TARGETS aomenc lossless_encoder set_maps
+                simple_encoder scalable_encoder svc_encoder_rtc twopass_encoder)
+    if(NOT BUILD_SHARED_LIBS AND NOT CONFIG_REALTIME_ONLY)
+      list(APPEND AOM_ENCODER_EXAMPLE_TARGETS noise_model photon_noise_table)
+    endif()
+
   endif()
 
   if(ENABLE_TOOLS)
diff --git a/METADATA b/METADATA
index a85faedf1..fe8b7f4f7 100644
--- a/METADATA
+++ b/METADATA
@@ -23,7 +23,7 @@ third_party {
   version: "v3.10.0"
   last_upgrade_date {
     year: 2024
-    month: 9
-    day: 17
+    month: 11
+    day: 19
   }
 }
diff --git a/README.android b/README.android
index 88a078a7c..fe5e9238b 100644
--- a/README.android
+++ b/README.android
@@ -4,9 +4,9 @@ Version: v3.10.0
 License: BSD
 License File: libaom/LICENSE
 
-Date: Tuesday September 17 2024
-Branch: v3.10.0
-Commit: c2fe6bf370f7c14fbaf12884b76244a3cfd7c5fc
+Date: Tuesday November 19 2024
+Branch: v3.11.0
+Commit: d6f30ae474dd6c358f26de0a0fc26a0d7340a84c
 
 Cherry-picks:
 None
diff --git a/README.md b/README.md
index 814ee898a..db7ad37b8 100644
--- a/README.md
+++ b/README.md
@@ -700,4 +700,5 @@ please email aomediacodec@jointdevelopment.kavi.com for help.
 ## Bug reports {#bug-reports}
 
 Bug reports can be filed in the Alliance for Open Media
-[issue tracker](https://aomedia.issues.chromium.org/).
+[issue tracker](https://aomedia.issues.chromium.org/). For security reports,
+select 'Security report' from the Template dropdown.
diff --git a/aom/aom_codec.h b/aom/aom_codec.h
index 1e589ca57..84454a281 100644
--- a/aom/aom_codec.h
+++ b/aom/aom_codec.h
@@ -223,10 +223,27 @@ typedef long aom_codec_caps_t;
  *  Certain codec features must be known at initialization time, to allow for
  *  proper memory allocation.
  *
- *  The available flags are specified by AOM_CODEC_USE_* defines.
+ *  The available flags are specified by AOM_CODEC_USE_* defines. The bits are
+ *  allocated as follows:
+ *      0x1 -     0x80: codec (common to decoder and encoder)
+ *    0x100 -   0x8000: decoder
+ *  0x10000 - 0x800000: encoder
  */
 typedef long aom_codec_flags_t;
 
+// Experimental feature policy
+//
+// New features may be marked as experimental. Experimental features are not
+// part of the stable API and may be modified or removed in a future release.
+// Experimental features are made available only if you pass the
+// AOM_CODEC_USE_EXPERIMENTAL flag to the codec init function.
+//
+// If you use experimental features, you must rebuild your code whenever you
+// update to a new libaom release, and you must be prepared to modify your code
+// when an experimental feature you use is modified or removed. If you are not
+// sure, DO NOT use experimental features.
+#define AOM_CODEC_USE_EXPERIMENTAL 0x1 /**< Enables experimental features */
+
 /*!\brief Time Stamp Type
  *
  * An integer, which when multiplied by the stream's time base, provides
diff --git a/aom/aom_encoder.h b/aom/aom_encoder.h
index f4c653b92..3b956bc91 100644
--- a/aom/aom_encoder.h
+++ b/aom/aom_encoder.h
@@ -78,6 +78,8 @@ extern "C" {
  */
 #define AOM_CODEC_USE_PSNR 0x10000         /**< Calculate PSNR on each frame */
 #define AOM_CODEC_USE_HIGHBITDEPTH 0x40000 /**< Use high bitdepth */
+// 0x80000 was used for the experimental feature AOM_CODEC_USE_PRESET during
+// libaom v3.11.0 development but was removed before the release.
 
 /*!\brief Generic fixed size buffer structure
  *
diff --git a/aom/aom_image.h b/aom/aom_image.h
index 1b790d50c..248b5b6b8 100644
--- a/aom/aom_image.h
+++ b/aom/aom_image.h
@@ -278,7 +278,9 @@ aom_image_t *aom_img_alloc(aom_image_t *img, aom_img_fmt_t fmt,
  *                         (2^27).
  * \param[in]    align     Alignment, in bytes, of each row in the image
  *                         (stride). Must not exceed 65536.
- * \param[in]    img_data  Storage to use for the image
+ * \param[in]    img_data  Storage to use for the image. The storage must
+ *                         outlive the returned image descriptor; it can be
+ *                         disposed of after calling aom_img_free().
  *
  * \return Returns a pointer to the initialized image descriptor. If the img
  *         parameter is non-null, the value of the img parameter will be
diff --git a/aom/aomcx.h b/aom/aomcx.h
index 2a9a090b4..2070e8083 100644
--- a/aom/aomcx.h
+++ b/aom/aomcx.h
@@ -1280,8 +1280,7 @@ enum aome_enc_control_id {
    */
   AV1E_SET_SVC_PARAMS = 132,
 
-  /*!\brief Codec control function to set reference frame config:
-   * the ref_idx and the refresh flags for each buffer slot.
+  /*!\brief Codec control function to set the reference frame config,
    * aom_svc_ref_frame_config_t* parameter
    */
   AV1E_SET_SVC_REF_FRAME_CONFIG = 133,
@@ -1721,12 +1720,25 @@ typedef struct aom_svc_params {
 
 /*!brief Parameters for setting ref frame config */
 typedef struct aom_svc_ref_frame_config {
-  // 7 references: The index 0 - 6 refers to the references:
+  // Three arrays need to be set: reference[], ref_id[], refresh[].
+  // reference[i]: is a boolean flag to indicate which of the 7 possible
+  // references are used for prediction. Values are 0 (not used as reference)
+  // or 1 (use as reference). The index 0 - 6 refers to the references:
   // last(0), last2(1), last3(2), golden(3), bwdref(4), altref2(5), altref(6).
+  // ref_idx[i]: maps a reference to one of the 8 buffers slots, values are
+  // 0 - 7. The ref_idx for a unused reference (reference[i] = 1, and not used
+  // for refresh, see below) can be set to the ref_idx of the first reference
+  // used (usually LAST).
+  // refresh[i] is a boolean flag to indicate if a buffer is updated/refreshed
+  // with the current encoded frame. Values are 0 (no refresh) or 1 (refresh).
+  // The refresh is done internally by looking at the ref_idx[j], for j = 0 - 6,
+  // so to refresh a buffer slot (i) a reference must be mapped to that slot
+  // (i = ref_idx[j]).
+  // Examples for usage (for RTC encoding) are in: examples/svc_encoder_rtc.c.
   int reference[7]; /**< Reference flag for each of the 7 references. */
-  /*! Buffer slot index for each of 7 references indexed above. */
+  /*! Buffer slot index (0..7) for each of 7 references indexed above. */
   int ref_idx[7];
-  int refresh[8]; /**< Refresh flag for each of the 8 slots. */
+  int refresh[8]; /**< Refresh flag for each of the 8 buffer slots. */
 } aom_svc_ref_frame_config_t;
 
 /*!brief Parameters for setting ref frame compound prediction */
diff --git a/aom/exports_com b/aom/exports_com
index 266e2943a..cd7fcc67d 100644
--- a/aom/exports_com
+++ b/aom/exports_com
@@ -17,8 +17,6 @@ text aom_img_alloc_with_border
 text aom_img_flip
 text aom_img_free
 text aom_img_get_metadata
-text aom_img_metadata_array_free
-text aom_img_metadata_array_alloc
 text aom_img_metadata_free
 text aom_img_metadata_alloc
 text aom_img_num_metadata
@@ -39,4 +37,3 @@ text aom_uleb_size_in_bytes
 text aom_wb_bytes_written
 text aom_wb_write_bit
 text aom_wb_write_literal
-text aom_wb_write_unsigned_literal
diff --git a/aom/exports_enc b/aom/exports_enc
index 1473d9d2b..eeb4060b0 100644
--- a/aom/exports_enc
+++ b/aom/exports_enc
@@ -6,12 +6,3 @@ text aom_codec_get_cx_data
 text aom_codec_get_global_headers
 text aom_codec_get_preview_frame
 text aom_codec_set_cx_data_buf
-text aom_film_grain_table_append
-text aom_film_grain_table_free
-text aom_film_grain_table_write
-text aom_flat_block_finder_init
-text aom_flat_block_finder_run
-text aom_noise_model_init
-text aom_noise_model_get_grain_parameters
-text aom_noise_model_save_latest
-text aom_noise_model_update
diff --git a/aom/exports_test b/aom/exports_test
deleted file mode 100644
index 452a532ce..000000000
--- a/aom/exports_test
+++ /dev/null
@@ -1,4 +0,0 @@
-text aom_copy_metadata_to_frame_buffer
-text aom_dsp_rtcd
-text aom_remove_metadata_from_frame_buffer
-text aom_scale_rtcd
diff --git a/aom_dsp/aom_dsp.cmake b/aom_dsp/aom_dsp.cmake
index e5f9698bf..317e2abbe 100644
--- a/aom_dsp/aom_dsp.cmake
+++ b/aom_dsp/aom_dsp.cmake
@@ -47,11 +47,13 @@ list(APPEND AOM_DSP_COMMON_SOURCES
             "${AOM_ROOT}/aom_dsp/x86/convolve_common_intrin.h")
 
 list(APPEND AOM_DSP_COMMON_ASM_SSE2
-            "${AOM_ROOT}/aom_dsp/x86/aom_high_subpixel_8t_sse2.asm"
-            "${AOM_ROOT}/aom_dsp/x86/aom_high_subpixel_bilinear_sse2.asm"
-            "${AOM_ROOT}/aom_dsp/x86/highbd_intrapred_asm_sse2.asm"
-            "${AOM_ROOT}/aom_dsp/x86/intrapred_asm_sse2.asm"
-            "${AOM_ROOT}/aom_dsp/x86/inv_wht_sse2.asm")
+            "${AOM_ROOT}/aom_dsp/x86/intrapred_asm_sse2.asm")
+if(CONFIG_AV1_HIGHBITDEPTH)
+  list(APPEND AOM_DSP_COMMON_ASM_SSE2
+              "${AOM_ROOT}/aom_dsp/x86/aom_high_subpixel_8t_sse2.asm"
+              "${AOM_ROOT}/aom_dsp/x86/aom_high_subpixel_bilinear_sse2.asm"
+              "${AOM_ROOT}/aom_dsp/x86/highbd_intrapred_asm_sse2.asm")
+endif()
 
 list(APPEND AOM_DSP_COMMON_INTRIN_SSE2
             "${AOM_ROOT}/aom_dsp/x86/aom_convolve_copy_sse2.c"
@@ -157,8 +159,6 @@ endif()
 if(CONFIG_AV1_ENCODER)
   list(APPEND AOM_DSP_ENCODER_SOURCES
               "${AOM_ROOT}/aom_dsp/avg.c"
-              "${AOM_ROOT}/aom_dsp/binary_codes_writer.c"
-              "${AOM_ROOT}/aom_dsp/binary_codes_writer.h"
               "${AOM_ROOT}/aom_dsp/bitwriter.c"
               "${AOM_ROOT}/aom_dsp/bitwriter.h"
               "${AOM_ROOT}/aom_dsp/blk_sse_sum.c"
@@ -167,12 +167,6 @@ if(CONFIG_AV1_ENCODER)
               "${AOM_ROOT}/aom_dsp/fft.c"
               "${AOM_ROOT}/aom_dsp/fft_common.h"
               "${AOM_ROOT}/aom_dsp/fwd_txfm.c"
-              "${AOM_ROOT}/aom_dsp/grain_table.c"
-              "${AOM_ROOT}/aom_dsp/grain_table.h"
-              "${AOM_ROOT}/aom_dsp/noise_model.c"
-              "${AOM_ROOT}/aom_dsp/noise_model.h"
-              "${AOM_ROOT}/aom_dsp/noise_util.c"
-              "${AOM_ROOT}/aom_dsp/noise_util.h"
               "${AOM_ROOT}/aom_dsp/psnr.c"
               "${AOM_ROOT}/aom_dsp/psnr.h"
               "${AOM_ROOT}/aom_dsp/quantize.c"
@@ -187,13 +181,22 @@ if(CONFIG_AV1_ENCODER)
               "${AOM_ROOT}/aom_dsp/variance.c"
               "${AOM_ROOT}/aom_dsp/variance.h")
 
-  # Flow estimation library
+  # Flow estimation library and grain/noise table/model.
   if(NOT CONFIG_REALTIME_ONLY)
-    list(APPEND AOM_DSP_ENCODER_SOURCES "${AOM_ROOT}/aom_dsp/pyramid.c"
+    list(APPEND AOM_DSP_ENCODER_SOURCES
+                "${AOM_ROOT}/aom_dsp/pyramid.c"
+                "${AOM_ROOT}/aom_dsp/binary_codes_writer.c"
+                "${AOM_ROOT}/aom_dsp/binary_codes_writer.h"
                 "${AOM_ROOT}/aom_dsp/flow_estimation/corner_detect.c"
                 "${AOM_ROOT}/aom_dsp/flow_estimation/corner_match.c"
                 "${AOM_ROOT}/aom_dsp/flow_estimation/disflow.c"
                 "${AOM_ROOT}/aom_dsp/flow_estimation/flow_estimation.c"
+                "${AOM_ROOT}/aom_dsp/grain_table.c"
+                "${AOM_ROOT}/aom_dsp/grain_table.h"
+                "${AOM_ROOT}/aom_dsp/noise_model.c"
+                "${AOM_ROOT}/aom_dsp/noise_model.h"
+                "${AOM_ROOT}/aom_dsp/noise_util.c"
+                "${AOM_ROOT}/aom_dsp/noise_util.h"
                 "${AOM_ROOT}/aom_dsp/flow_estimation/ransac.c")
 
     list(APPEND AOM_DSP_ENCODER_INTRIN_SSE4_1
diff --git a/aom_dsp/aom_dsp_rtcd_defs.pl b/aom_dsp/aom_dsp_rtcd_defs.pl
index a7f74eee3..d3f6b2ea8 100755
--- a/aom_dsp/aom_dsp_rtcd_defs.pl
+++ b/aom_dsp/aom_dsp_rtcd_defs.pl
@@ -63,7 +63,10 @@ foreach $w (@tx_dims) {
   push @tx_sizes, [$w, $w];
   foreach $h (@tx_dims) {
     push @tx_sizes, [$w, $h] if ($w >=4 && $h >=4 && ($w == 2*$h || $h == 2*$w));
-    push @tx_sizes, [$w, $h] if ($w >=4 && $h >=4 && ($w == 4*$h || $h == 4*$w));
+    if ((aom_config("CONFIG_REALTIME_ONLY") ne "yes") ||
+        (aom_config("CONFIG_AV1_DECODER") eq "yes")) {
+      push @tx_sizes, [$w, $h] if ($w >=4 && $h >=4 && ($w == 4*$h || $h == 4*$w));
+    }  # !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
   }
 }
 
@@ -87,181 +90,127 @@ foreach (@tx_sizes) {
 
 specialize qw/aom_dc_top_predictor_4x4 neon sse2/;
 specialize qw/aom_dc_top_predictor_4x8 neon sse2/;
-specialize qw/aom_dc_top_predictor_4x16 neon sse2/;
 specialize qw/aom_dc_top_predictor_8x4 neon sse2/;
 specialize qw/aom_dc_top_predictor_8x8 neon sse2/;
 specialize qw/aom_dc_top_predictor_8x16 neon sse2/;
-specialize qw/aom_dc_top_predictor_8x32 neon sse2/;
-specialize qw/aom_dc_top_predictor_16x4 neon sse2/;
 specialize qw/aom_dc_top_predictor_16x8 neon sse2/;
 specialize qw/aom_dc_top_predictor_16x16 neon sse2/;
 specialize qw/aom_dc_top_predictor_16x32 neon sse2/;
-specialize qw/aom_dc_top_predictor_16x64 neon sse2/;
-specialize qw/aom_dc_top_predictor_32x8 neon sse2/;
 specialize qw/aom_dc_top_predictor_32x16 neon sse2 avx2/;
 specialize qw/aom_dc_top_predictor_32x32 neon sse2 avx2/;
 specialize qw/aom_dc_top_predictor_32x64 neon sse2 avx2/;
-specialize qw/aom_dc_top_predictor_64x16 neon sse2 avx2/;
 specialize qw/aom_dc_top_predictor_64x32 neon sse2 avx2/;
 specialize qw/aom_dc_top_predictor_64x64 neon sse2 avx2/;
 
 specialize qw/aom_dc_left_predictor_4x4 neon sse2/;
 specialize qw/aom_dc_left_predictor_4x8 neon sse2/;
-specialize qw/aom_dc_left_predictor_4x16 neon sse2/;
 specialize qw/aom_dc_left_predictor_8x4 neon sse2/;
 specialize qw/aom_dc_left_predictor_8x8 neon sse2/;
 specialize qw/aom_dc_left_predictor_8x16 neon sse2/;
-specialize qw/aom_dc_left_predictor_8x32 neon sse2/;
-specialize qw/aom_dc_left_predictor_16x4 neon sse2/;
 specialize qw/aom_dc_left_predictor_16x8 neon sse2/;
 specialize qw/aom_dc_left_predictor_16x16 neon sse2/;
 specialize qw/aom_dc_left_predictor_16x32 neon sse2/;
-specialize qw/aom_dc_left_predictor_16x64 neon sse2/;
-specialize qw/aom_dc_left_predictor_32x8 neon sse2/;
 specialize qw/aom_dc_left_predictor_32x16 neon sse2 avx2/;
 specialize qw/aom_dc_left_predictor_32x32 neon sse2 avx2/;
 specialize qw/aom_dc_left_predictor_32x64 neon sse2 avx2/;
-specialize qw/aom_dc_left_predictor_64x16 neon sse2 avx2/;
 specialize qw/aom_dc_left_predictor_64x32 neon sse2 avx2/;
 specialize qw/aom_dc_left_predictor_64x64 neon sse2 avx2/;
 
 specialize qw/aom_dc_128_predictor_4x4 neon sse2/;
 specialize qw/aom_dc_128_predictor_4x8 neon sse2/;
-specialize qw/aom_dc_128_predictor_4x16 neon sse2/;
 specialize qw/aom_dc_128_predictor_8x4 neon sse2/;
 specialize qw/aom_dc_128_predictor_8x8 neon sse2/;
 specialize qw/aom_dc_128_predictor_8x16 neon sse2/;
-specialize qw/aom_dc_128_predictor_8x32 neon sse2/;
-specialize qw/aom_dc_128_predictor_16x4 neon sse2/;
 specialize qw/aom_dc_128_predictor_16x8 neon sse2/;
 specialize qw/aom_dc_128_predictor_16x16 neon sse2/;
 specialize qw/aom_dc_128_predictor_16x32 neon sse2/;
-specialize qw/aom_dc_128_predictor_16x64 neon sse2/;
-specialize qw/aom_dc_128_predictor_32x8 neon sse2/;
 specialize qw/aom_dc_128_predictor_32x16 neon sse2 avx2/;
 specialize qw/aom_dc_128_predictor_32x32 neon sse2 avx2/;
 specialize qw/aom_dc_128_predictor_32x64 neon sse2 avx2/;
-specialize qw/aom_dc_128_predictor_64x16 neon sse2 avx2/;
 specialize qw/aom_dc_128_predictor_64x32 neon sse2 avx2/;
 specialize qw/aom_dc_128_predictor_64x64 neon sse2 avx2/;
 
 specialize qw/aom_v_predictor_4x4 neon sse2/;
 specialize qw/aom_v_predictor_4x8 neon sse2/;
-specialize qw/aom_v_predictor_4x16 neon sse2/;
 specialize qw/aom_v_predictor_8x4 neon sse2/;
 specialize qw/aom_v_predictor_8x8 neon sse2/;
 specialize qw/aom_v_predictor_8x16 neon sse2/;
-specialize qw/aom_v_predictor_8x32 neon sse2/;
-specialize qw/aom_v_predictor_16x4 neon sse2/;
 specialize qw/aom_v_predictor_16x8 neon sse2/;
 specialize qw/aom_v_predictor_16x16 neon sse2/;
 specialize qw/aom_v_predictor_16x32 neon sse2/;
-specialize qw/aom_v_predictor_16x64 neon sse2/;
-specialize qw/aom_v_predictor_32x8 neon sse2/;
 specialize qw/aom_v_predictor_32x16 neon sse2 avx2/;
 specialize qw/aom_v_predictor_32x32 neon sse2 avx2/;
 specialize qw/aom_v_predictor_32x64 neon sse2 avx2/;
-specialize qw/aom_v_predictor_64x16 neon sse2 avx2/;
 specialize qw/aom_v_predictor_64x32 neon sse2 avx2/;
 specialize qw/aom_v_predictor_64x64 neon sse2 avx2/;
 
 specialize qw/aom_h_predictor_4x4 neon sse2/;
 specialize qw/aom_h_predictor_4x8 neon sse2/;
-specialize qw/aom_h_predictor_4x16 neon sse2/;
 specialize qw/aom_h_predictor_8x4 neon sse2/;
 specialize qw/aom_h_predictor_8x8 neon sse2/;
 specialize qw/aom_h_predictor_8x16 neon sse2/;
-specialize qw/aom_h_predictor_8x32 neon sse2/;
-specialize qw/aom_h_predictor_16x4 neon sse2/;
 specialize qw/aom_h_predictor_16x8 neon sse2/;
 specialize qw/aom_h_predictor_16x16 neon sse2/;
 specialize qw/aom_h_predictor_16x32 neon sse2/;
-specialize qw/aom_h_predictor_16x64 neon sse2/;
-specialize qw/aom_h_predictor_32x8 neon sse2/;
 specialize qw/aom_h_predictor_32x16 neon sse2/;
 specialize qw/aom_h_predictor_32x32 neon sse2 avx2/;
 specialize qw/aom_h_predictor_32x64 neon sse2/;
-specialize qw/aom_h_predictor_64x16 neon sse2/;
 specialize qw/aom_h_predictor_64x32 neon sse2/;
 specialize qw/aom_h_predictor_64x64 neon sse2/;
 
 specialize qw/aom_paeth_predictor_4x4 ssse3 neon/;
 specialize qw/aom_paeth_predictor_4x8 ssse3 neon/;
-specialize qw/aom_paeth_predictor_4x16 ssse3 neon/;
 specialize qw/aom_paeth_predictor_8x4 ssse3 neon/;
 specialize qw/aom_paeth_predictor_8x8 ssse3 neon/;
 specialize qw/aom_paeth_predictor_8x16 ssse3 neon/;
-specialize qw/aom_paeth_predictor_8x32 ssse3 neon/;
-specialize qw/aom_paeth_predictor_16x4 ssse3 neon/;
 specialize qw/aom_paeth_predictor_16x8 ssse3 avx2 neon/;
 specialize qw/aom_paeth_predictor_16x16 ssse3 avx2 neon/;
 specialize qw/aom_paeth_predictor_16x32 ssse3 avx2 neon/;
-specialize qw/aom_paeth_predictor_16x64 ssse3 avx2 neon/;
-specialize qw/aom_paeth_predictor_32x8 ssse3 neon/;
 specialize qw/aom_paeth_predictor_32x16 ssse3 avx2 neon/;
 specialize qw/aom_paeth_predictor_32x32 ssse3 avx2 neon/;
 specialize qw/aom_paeth_predictor_32x64 ssse3 avx2 neon/;
-specialize qw/aom_paeth_predictor_64x16 ssse3 avx2 neon/;
 specialize qw/aom_paeth_predictor_64x32 ssse3 avx2 neon/;
 specialize qw/aom_paeth_predictor_64x64 ssse3 avx2 neon/;
 
 specialize qw/aom_smooth_predictor_4x4 neon ssse3/;
 specialize qw/aom_smooth_predictor_4x8 neon ssse3/;
-specialize qw/aom_smooth_predictor_4x16 neon ssse3/;
 specialize qw/aom_smooth_predictor_8x4 neon ssse3/;
 specialize qw/aom_smooth_predictor_8x8 neon ssse3/;
 specialize qw/aom_smooth_predictor_8x16 neon ssse3/;
-specialize qw/aom_smooth_predictor_8x32 neon ssse3/;
-specialize qw/aom_smooth_predictor_16x4 neon ssse3/;
 specialize qw/aom_smooth_predictor_16x8 neon ssse3/;
 specialize qw/aom_smooth_predictor_16x16 neon ssse3/;
 specialize qw/aom_smooth_predictor_16x32 neon ssse3/;
-specialize qw/aom_smooth_predictor_16x64 neon ssse3/;
-specialize qw/aom_smooth_predictor_32x8 neon ssse3/;
 specialize qw/aom_smooth_predictor_32x16 neon ssse3/;
 specialize qw/aom_smooth_predictor_32x32 neon ssse3/;
 specialize qw/aom_smooth_predictor_32x64 neon ssse3/;
-specialize qw/aom_smooth_predictor_64x16 neon ssse3/;
 specialize qw/aom_smooth_predictor_64x32 neon ssse3/;
 specialize qw/aom_smooth_predictor_64x64 neon ssse3/;
 
 specialize qw/aom_smooth_v_predictor_4x4 neon ssse3/;
 specialize qw/aom_smooth_v_predictor_4x8 neon ssse3/;
-specialize qw/aom_smooth_v_predictor_4x16 neon ssse3/;
 specialize qw/aom_smooth_v_predictor_8x4 neon ssse3/;
 specialize qw/aom_smooth_v_predictor_8x8 neon ssse3/;
 specialize qw/aom_smooth_v_predictor_8x16 neon ssse3/;
-specialize qw/aom_smooth_v_predictor_8x32 neon ssse3/;
-specialize qw/aom_smooth_v_predictor_16x4 neon ssse3/;
 specialize qw/aom_smooth_v_predictor_16x8 neon ssse3/;
 specialize qw/aom_smooth_v_predictor_16x16 neon ssse3/;
 specialize qw/aom_smooth_v_predictor_16x32 neon ssse3/;
-specialize qw/aom_smooth_v_predictor_16x64 neon ssse3/;
-specialize qw/aom_smooth_v_predictor_32x8 neon ssse3/;
 specialize qw/aom_smooth_v_predictor_32x16 neon ssse3/;
 specialize qw/aom_smooth_v_predictor_32x32 neon ssse3/;
 specialize qw/aom_smooth_v_predictor_32x64 neon ssse3/;
-specialize qw/aom_smooth_v_predictor_64x16 neon ssse3/;
 specialize qw/aom_smooth_v_predictor_64x32 neon ssse3/;
 specialize qw/aom_smooth_v_predictor_64x64 neon ssse3/;
 
 specialize qw/aom_smooth_h_predictor_4x4 neon ssse3/;
 specialize qw/aom_smooth_h_predictor_4x8 neon ssse3/;
-specialize qw/aom_smooth_h_predictor_4x16 neon ssse3/;
 specialize qw/aom_smooth_h_predictor_8x4 neon ssse3/;
 specialize qw/aom_smooth_h_predictor_8x8 neon ssse3/;
 specialize qw/aom_smooth_h_predictor_8x16 neon ssse3/;
-specialize qw/aom_smooth_h_predictor_8x32 neon ssse3/;
-specialize qw/aom_smooth_h_predictor_16x4 neon ssse3/;
 specialize qw/aom_smooth_h_predictor_16x8 neon ssse3/;
 specialize qw/aom_smooth_h_predictor_16x16 neon ssse3/;
 specialize qw/aom_smooth_h_predictor_16x32 neon ssse3/;
-specialize qw/aom_smooth_h_predictor_16x64 neon ssse3/;
-specialize qw/aom_smooth_h_predictor_32x8 neon ssse3/;
 specialize qw/aom_smooth_h_predictor_32x16 neon ssse3/;
 specialize qw/aom_smooth_h_predictor_32x32 neon ssse3/;
 specialize qw/aom_smooth_h_predictor_32x64 neon ssse3/;
-specialize qw/aom_smooth_h_predictor_64x16 neon ssse3/;
 specialize qw/aom_smooth_h_predictor_64x32 neon ssse3/;
 specialize qw/aom_smooth_h_predictor_64x64 neon ssse3/;
 
@@ -269,41 +218,103 @@ specialize qw/aom_smooth_h_predictor_64x64 neon ssse3/;
 # by multiply and shift.
 specialize qw/aom_dc_predictor_4x4 neon sse2/;
 specialize qw/aom_dc_predictor_4x8 neon sse2/;
-specialize qw/aom_dc_predictor_4x16 neon sse2/;
 specialize qw/aom_dc_predictor_8x4 neon sse2/;
 specialize qw/aom_dc_predictor_8x8 neon sse2/;
 specialize qw/aom_dc_predictor_8x16 neon sse2/;
-specialize qw/aom_dc_predictor_8x32 neon sse2/;
-specialize qw/aom_dc_predictor_16x4 neon sse2/;
 specialize qw/aom_dc_predictor_16x8 neon sse2/;
 specialize qw/aom_dc_predictor_16x16 neon sse2/;
 specialize qw/aom_dc_predictor_16x32 neon sse2/;
-specialize qw/aom_dc_predictor_16x64 neon sse2/;
-specialize qw/aom_dc_predictor_32x8 neon sse2/;
 specialize qw/aom_dc_predictor_32x16 neon sse2 avx2/;
 specialize qw/aom_dc_predictor_32x32 neon sse2 avx2/;
 specialize qw/aom_dc_predictor_32x64 neon sse2 avx2/;
 specialize qw/aom_dc_predictor_64x64 neon sse2 avx2/;
 specialize qw/aom_dc_predictor_64x32 neon sse2 avx2/;
-specialize qw/aom_dc_predictor_64x16 neon sse2 avx2/;
+
+
+if ((aom_config("CONFIG_REALTIME_ONLY") ne "yes") || (aom_config("CONFIG_AV1_DECODER") eq "yes")) {
+  specialize qw/aom_dc_top_predictor_4x16 neon sse2/;
+  specialize qw/aom_dc_top_predictor_8x32 neon sse2/;
+  specialize qw/aom_dc_top_predictor_16x4 neon sse2/;
+  specialize qw/aom_dc_top_predictor_16x64 neon sse2/;
+  specialize qw/aom_dc_top_predictor_32x8 neon sse2/;
+  specialize qw/aom_dc_top_predictor_64x16 neon sse2 avx2/;
+
+  specialize qw/aom_dc_left_predictor_4x16 neon sse2/;
+  specialize qw/aom_dc_left_predictor_8x32 neon sse2/;
+  specialize qw/aom_dc_left_predictor_16x4 neon sse2/;
+  specialize qw/aom_dc_left_predictor_16x64 neon sse2/;
+  specialize qw/aom_dc_left_predictor_32x8 neon sse2/;
+  specialize qw/aom_dc_left_predictor_64x16 neon sse2 avx2/;
+
+  specialize qw/aom_dc_128_predictor_4x16 neon sse2/;
+  specialize qw/aom_dc_128_predictor_8x32 neon sse2/;
+  specialize qw/aom_dc_128_predictor_16x4 neon sse2/;
+  specialize qw/aom_dc_128_predictor_16x64 neon sse2/;
+  specialize qw/aom_dc_128_predictor_32x8 neon sse2/;
+  specialize qw/aom_dc_128_predictor_64x16 neon sse2 avx2/;
+
+  specialize qw/aom_v_predictor_4x16 neon sse2/;
+  specialize qw/aom_v_predictor_8x32 neon sse2/;
+  specialize qw/aom_v_predictor_16x4 neon sse2/;
+  specialize qw/aom_v_predictor_16x64 neon sse2/;
+  specialize qw/aom_v_predictor_32x8 neon sse2/;
+  specialize qw/aom_v_predictor_64x16 neon sse2 avx2/;
+
+  specialize qw/aom_h_predictor_4x16 neon sse2/;
+  specialize qw/aom_h_predictor_8x32 neon sse2/;
+  specialize qw/aom_h_predictor_16x4 neon sse2/;
+  specialize qw/aom_h_predictor_16x64 neon sse2/;
+  specialize qw/aom_h_predictor_32x8 neon sse2/;
+  specialize qw/aom_h_predictor_64x16 neon sse2/;
+
+  specialize qw/aom_paeth_predictor_4x16 ssse3 neon/;
+  specialize qw/aom_paeth_predictor_8x32 ssse3 neon/;
+  specialize qw/aom_paeth_predictor_16x4 ssse3 neon/;
+  specialize qw/aom_paeth_predictor_16x64 ssse3 avx2 neon/;
+  specialize qw/aom_paeth_predictor_32x8 ssse3 neon/;
+  specialize qw/aom_paeth_predictor_64x16 ssse3 avx2 neon/;
+
+  specialize qw/aom_smooth_predictor_4x16 neon ssse3/;
+  specialize qw/aom_smooth_predictor_8x32 neon ssse3/;
+  specialize qw/aom_smooth_predictor_16x4 neon ssse3/;
+  specialize qw/aom_smooth_predictor_16x64 neon ssse3/;
+  specialize qw/aom_smooth_predictor_32x8 neon ssse3/;
+  specialize qw/aom_smooth_predictor_64x16 neon ssse3/;
+
+  specialize qw/aom_smooth_v_predictor_4x16 neon ssse3/;
+  specialize qw/aom_smooth_v_predictor_8x32 neon ssse3/;
+  specialize qw/aom_smooth_v_predictor_16x4 neon ssse3/;
+  specialize qw/aom_smooth_v_predictor_16x64 neon ssse3/;
+  specialize qw/aom_smooth_v_predictor_32x8 neon ssse3/;
+  specialize qw/aom_smooth_v_predictor_64x16 neon ssse3/;
+
+  specialize qw/aom_smooth_h_predictor_4x16 neon ssse3/;
+  specialize qw/aom_smooth_h_predictor_8x32 neon ssse3/;
+  specialize qw/aom_smooth_h_predictor_16x4 neon ssse3/;
+  specialize qw/aom_smooth_h_predictor_16x64 neon ssse3/;
+  specialize qw/aom_smooth_h_predictor_32x8 neon ssse3/;
+  specialize qw/aom_smooth_h_predictor_64x16 neon ssse3/;
+
+  specialize qw/aom_dc_predictor_4x16 neon sse2/;
+  specialize qw/aom_dc_predictor_8x32 neon sse2/;
+  specialize qw/aom_dc_predictor_16x4 neon sse2/;
+  specialize qw/aom_dc_predictor_16x64 neon sse2/;
+  specialize qw/aom_dc_predictor_32x8 neon sse2/;
+  specialize qw/aom_dc_predictor_64x16 neon sse2 avx2/;
+}  # !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
+
 if (aom_config("CONFIG_AV1_HIGHBITDEPTH") eq "yes") {
   specialize qw/aom_highbd_v_predictor_4x4 sse2 neon/;
   specialize qw/aom_highbd_v_predictor_4x8 sse2 neon/;
-  specialize qw/aom_highbd_v_predictor_4x16 neon/;
   specialize qw/aom_highbd_v_predictor_8x4 sse2 neon/;
   specialize qw/aom_highbd_v_predictor_8x8 sse2 neon/;
   specialize qw/aom_highbd_v_predictor_8x16 sse2 neon/;
-  specialize qw/aom_highbd_v_predictor_8x32 neon/;
-  specialize qw/aom_highbd_v_predictor_16x4 neon/;
   specialize qw/aom_highbd_v_predictor_16x8 sse2 neon/;
   specialize qw/aom_highbd_v_predictor_16x16 sse2 neon/;
   specialize qw/aom_highbd_v_predictor_16x32 sse2 neon/;
-  specialize qw/aom_highbd_v_predictor_16x64 neon/;
-  specialize qw/aom_highbd_v_predictor_32x8 neon/;
   specialize qw/aom_highbd_v_predictor_32x16 sse2 neon/;
   specialize qw/aom_highbd_v_predictor_32x32 sse2 neon/;
   specialize qw/aom_highbd_v_predictor_32x64 neon/;
-  specialize qw/aom_highbd_v_predictor_64x16 neon/;
   specialize qw/aom_highbd_v_predictor_64x32 neon/;
   specialize qw/aom_highbd_v_predictor_64x64 neon/;
 
@@ -311,183 +322,202 @@ if (aom_config("CONFIG_AV1_HIGHBITDEPTH") eq "yes") {
   # by multiply and shift.
   specialize qw/aom_highbd_dc_predictor_4x4 sse2 neon/;
   specialize qw/aom_highbd_dc_predictor_4x8 sse2 neon/;
-  specialize qw/aom_highbd_dc_predictor_4x16 neon/;
   specialize qw/aom_highbd_dc_predictor_8x4 sse2 neon/;
   specialize qw/aom_highbd_dc_predictor_8x8 sse2 neon/;
   specialize qw/aom_highbd_dc_predictor_8x16 sse2 neon/;
-  specialize qw/aom_highbd_dc_predictor_8x32 neon/;
-  specialize qw/aom_highbd_dc_predictor_16x4 neon/;
   specialize qw/aom_highbd_dc_predictor_16x8 sse2 neon/;
   specialize qw/aom_highbd_dc_predictor_16x16 sse2 neon/;
   specialize qw/aom_highbd_dc_predictor_16x32 sse2 neon/;
-  specialize qw/aom_highbd_dc_predictor_16x64 neon/;
-  specialize qw/aom_highbd_dc_predictor_32x8 neon/;
   specialize qw/aom_highbd_dc_predictor_32x16 sse2 neon/;
   specialize qw/aom_highbd_dc_predictor_32x32 sse2 neon/;
   specialize qw/aom_highbd_dc_predictor_32x64 neon/;
-  specialize qw/aom_highbd_dc_predictor_64x16 neon/;
   specialize qw/aom_highbd_dc_predictor_64x32 neon/;
   specialize qw/aom_highbd_dc_predictor_64x64 neon/;
 
   specialize qw/aom_highbd_h_predictor_4x4 sse2 neon/;
   specialize qw/aom_highbd_h_predictor_4x8 sse2 neon/;
-  specialize qw/aom_highbd_h_predictor_4x16 neon/;
   specialize qw/aom_highbd_h_predictor_8x4 sse2 neon/;
   specialize qw/aom_highbd_h_predictor_8x8 sse2 neon/;
   specialize qw/aom_highbd_h_predictor_8x16 sse2 neon/;
-  specialize qw/aom_highbd_h_predictor_8x32 neon/;
-  specialize qw/aom_highbd_h_predictor_16x4 neon/;
   specialize qw/aom_highbd_h_predictor_16x8 sse2 neon/;
   specialize qw/aom_highbd_h_predictor_16x16 sse2 neon/;
   specialize qw/aom_highbd_h_predictor_16x32 sse2 neon/;
-  specialize qw/aom_highbd_h_predictor_16x64 neon/;
-  specialize qw/aom_highbd_h_predictor_32x8 neon/;
   specialize qw/aom_highbd_h_predictor_32x16 sse2 neon/;
   specialize qw/aom_highbd_h_predictor_32x32 sse2 neon/;
   specialize qw/aom_highbd_h_predictor_32x64 neon/;
-  specialize qw/aom_highbd_h_predictor_64x16 neon/;
   specialize qw/aom_highbd_h_predictor_64x32 neon/;
   specialize qw/aom_highbd_h_predictor_64x64 neon/;
 
   specialize qw/aom_highbd_dc_128_predictor_4x4 sse2 neon/;
   specialize qw/aom_highbd_dc_128_predictor_4x8 sse2 neon/;
-  specialize qw/aom_highbd_dc_128_predictor_4x16 neon/;
   specialize qw/aom_highbd_dc_128_predictor_8x4 sse2 neon/;
   specialize qw/aom_highbd_dc_128_predictor_8x8 sse2 neon/;
   specialize qw/aom_highbd_dc_128_predictor_8x16 sse2 neon/;
-  specialize qw/aom_highbd_dc_128_predictor_8x32 neon/;
-  specialize qw/aom_highbd_dc_128_predictor_16x4 neon/;
   specialize qw/aom_highbd_dc_128_predictor_16x8 sse2 neon/;
   specialize qw/aom_highbd_dc_128_predictor_16x16 sse2 neon/;
   specialize qw/aom_highbd_dc_128_predictor_16x32 sse2 neon/;
-  specialize qw/aom_highbd_dc_128_predictor_16x64 neon/;
-  specialize qw/aom_highbd_dc_128_predictor_32x8 neon/;
   specialize qw/aom_highbd_dc_128_predictor_32x16 sse2 neon/;
   specialize qw/aom_highbd_dc_128_predictor_32x32 sse2 neon/;
   specialize qw/aom_highbd_dc_128_predictor_32x64 neon/;
-  specialize qw/aom_highbd_dc_128_predictor_64x16 neon/;
   specialize qw/aom_highbd_dc_128_predictor_64x32 neon/;
   specialize qw/aom_highbd_dc_128_predictor_64x64 neon/;
 
   specialize qw/aom_highbd_dc_left_predictor_4x4 sse2 neon/;
   specialize qw/aom_highbd_dc_left_predictor_4x8 sse2 neon/;
-  specialize qw/aom_highbd_dc_left_predictor_4x16 neon/;
   specialize qw/aom_highbd_dc_left_predictor_8x4 sse2 neon/;
   specialize qw/aom_highbd_dc_left_predictor_8x8 sse2 neon/;
   specialize qw/aom_highbd_dc_left_predictor_8x16 sse2 neon/;
-  specialize qw/aom_highbd_dc_left_predictor_8x32 neon/;
-  specialize qw/aom_highbd_dc_left_predictor_16x4 neon/;
   specialize qw/aom_highbd_dc_left_predictor_16x8 sse2 neon/;
   specialize qw/aom_highbd_dc_left_predictor_16x16 sse2 neon/;
   specialize qw/aom_highbd_dc_left_predictor_16x32 sse2 neon/;
-  specialize qw/aom_highbd_dc_left_predictor_16x64 neon/;
-  specialize qw/aom_highbd_dc_left_predictor_32x8 neon/;
   specialize qw/aom_highbd_dc_left_predictor_32x16 sse2 neon/;
   specialize qw/aom_highbd_dc_left_predictor_32x32 sse2 neon/;
   specialize qw/aom_highbd_dc_left_predictor_32x64 neon/;
-  specialize qw/aom_highbd_dc_left_predictor_64x16 neon/;
   specialize qw/aom_highbd_dc_left_predictor_64x32 neon/;
   specialize qw/aom_highbd_dc_left_predictor_64x64 neon/;
 
   specialize qw/aom_highbd_dc_top_predictor_4x4 sse2 neon/;
   specialize qw/aom_highbd_dc_top_predictor_4x8 sse2 neon/;
-  specialize qw/aom_highbd_dc_top_predictor_4x16 neon/;
   specialize qw/aom_highbd_dc_top_predictor_8x4 sse2 neon/;
   specialize qw/aom_highbd_dc_top_predictor_8x8 sse2 neon/;
   specialize qw/aom_highbd_dc_top_predictor_8x16 sse2 neon/;
-  specialize qw/aom_highbd_dc_top_predictor_8x32 neon/;
-  specialize qw/aom_highbd_dc_top_predictor_16x4 neon/;
   specialize qw/aom_highbd_dc_top_predictor_16x8 sse2 neon/;
   specialize qw/aom_highbd_dc_top_predictor_16x16 sse2 neon/;
   specialize qw/aom_highbd_dc_top_predictor_16x32 sse2 neon/;
-  specialize qw/aom_highbd_dc_top_predictor_16x64 neon/;
-  specialize qw/aom_highbd_dc_top_predictor_32x8 neon/;
   specialize qw/aom_highbd_dc_top_predictor_32x16 sse2 neon/;
   specialize qw/aom_highbd_dc_top_predictor_32x32 sse2 neon/;
   specialize qw/aom_highbd_dc_top_predictor_32x64 neon/;
-  specialize qw/aom_highbd_dc_top_predictor_64x16 neon/;
   specialize qw/aom_highbd_dc_top_predictor_64x32 neon/;
   specialize qw/aom_highbd_dc_top_predictor_64x64 neon/;
 
   specialize qw/aom_highbd_paeth_predictor_4x4 neon/;
   specialize qw/aom_highbd_paeth_predictor_4x8 neon/;
-  specialize qw/aom_highbd_paeth_predictor_4x16 neon/;
   specialize qw/aom_highbd_paeth_predictor_8x4 neon/;
   specialize qw/aom_highbd_paeth_predictor_8x8 neon/;
   specialize qw/aom_highbd_paeth_predictor_8x16 neon/;
-  specialize qw/aom_highbd_paeth_predictor_8x32 neon/;
-  specialize qw/aom_highbd_paeth_predictor_16x4 neon/;
   specialize qw/aom_highbd_paeth_predictor_16x8 neon/;
   specialize qw/aom_highbd_paeth_predictor_16x16 neon/;
   specialize qw/aom_highbd_paeth_predictor_16x32 neon/;
-  specialize qw/aom_highbd_paeth_predictor_16x64 neon/;
-  specialize qw/aom_highbd_paeth_predictor_32x8 neon/;
   specialize qw/aom_highbd_paeth_predictor_32x16 neon/;
   specialize qw/aom_highbd_paeth_predictor_32x32 neon/;
   specialize qw/aom_highbd_paeth_predictor_32x64 neon/;
-  specialize qw/aom_highbd_paeth_predictor_64x16 neon/;
   specialize qw/aom_highbd_paeth_predictor_64x32 neon/;
   specialize qw/aom_highbd_paeth_predictor_64x64 neon/;
 
   specialize qw/aom_highbd_smooth_predictor_4x4 neon/;
   specialize qw/aom_highbd_smooth_predictor_4x8 neon/;
-  specialize qw/aom_highbd_smooth_predictor_4x16 neon/;
   specialize qw/aom_highbd_smooth_predictor_8x4 neon/;
   specialize qw/aom_highbd_smooth_predictor_8x8 neon/;
   specialize qw/aom_highbd_smooth_predictor_8x16 neon/;
-  specialize qw/aom_highbd_smooth_predictor_8x32 neon/;
-  specialize qw/aom_highbd_smooth_predictor_16x4 neon/;
   specialize qw/aom_highbd_smooth_predictor_16x8 neon/;
   specialize qw/aom_highbd_smooth_predictor_16x16 neon/;
   specialize qw/aom_highbd_smooth_predictor_16x32 neon/;
-  specialize qw/aom_highbd_smooth_predictor_16x64 neon/;
-  specialize qw/aom_highbd_smooth_predictor_32x8 neon/;
   specialize qw/aom_highbd_smooth_predictor_32x16 neon/;
   specialize qw/aom_highbd_smooth_predictor_32x32 neon/;
   specialize qw/aom_highbd_smooth_predictor_32x64 neon/;
-  specialize qw/aom_highbd_smooth_predictor_64x16 neon/;
   specialize qw/aom_highbd_smooth_predictor_64x32 neon/;
   specialize qw/aom_highbd_smooth_predictor_64x64 neon/;
 
   specialize qw/aom_highbd_smooth_v_predictor_4x4 neon/;
   specialize qw/aom_highbd_smooth_v_predictor_4x8 neon/;
-  specialize qw/aom_highbd_smooth_v_predictor_4x16 neon/;
   specialize qw/aom_highbd_smooth_v_predictor_8x4 neon/;
   specialize qw/aom_highbd_smooth_v_predictor_8x8 neon/;
   specialize qw/aom_highbd_smooth_v_predictor_8x16 neon/;
-  specialize qw/aom_highbd_smooth_v_predictor_8x32 neon/;
-  specialize qw/aom_highbd_smooth_v_predictor_16x4 neon/;
   specialize qw/aom_highbd_smooth_v_predictor_16x8 neon/;
   specialize qw/aom_highbd_smooth_v_predictor_16x16 neon/;
   specialize qw/aom_highbd_smooth_v_predictor_16x32 neon/;
-  specialize qw/aom_highbd_smooth_v_predictor_16x64 neon/;
-  specialize qw/aom_highbd_smooth_v_predictor_32x8 neon/;
   specialize qw/aom_highbd_smooth_v_predictor_32x16 neon/;
   specialize qw/aom_highbd_smooth_v_predictor_32x32 neon/;
   specialize qw/aom_highbd_smooth_v_predictor_32x64 neon/;
-  specialize qw/aom_highbd_smooth_v_predictor_64x16 neon/;
   specialize qw/aom_highbd_smooth_v_predictor_64x32 neon/;
   specialize qw/aom_highbd_smooth_v_predictor_64x64 neon/;
-
   specialize qw/aom_highbd_smooth_h_predictor_4x4 neon/;
   specialize qw/aom_highbd_smooth_h_predictor_4x8 neon/;
-  specialize qw/aom_highbd_smooth_h_predictor_4x16 neon/;
+
   specialize qw/aom_highbd_smooth_h_predictor_8x4 neon/;
   specialize qw/aom_highbd_smooth_h_predictor_8x8 neon/;
   specialize qw/aom_highbd_smooth_h_predictor_8x16 neon/;
-  specialize qw/aom_highbd_smooth_h_predictor_8x32 neon/;
-  specialize qw/aom_highbd_smooth_h_predictor_16x4 neon/;
   specialize qw/aom_highbd_smooth_h_predictor_16x8 neon/;
   specialize qw/aom_highbd_smooth_h_predictor_16x16 neon/;
   specialize qw/aom_highbd_smooth_h_predictor_16x32 neon/;
-  specialize qw/aom_highbd_smooth_h_predictor_16x64 neon/;
-  specialize qw/aom_highbd_smooth_h_predictor_32x8 neon/;
   specialize qw/aom_highbd_smooth_h_predictor_32x16 neon/;
   specialize qw/aom_highbd_smooth_h_predictor_32x32 neon/;
   specialize qw/aom_highbd_smooth_h_predictor_32x64 neon/;
-  specialize qw/aom_highbd_smooth_h_predictor_64x16 neon/;
   specialize qw/aom_highbd_smooth_h_predictor_64x32 neon/;
   specialize qw/aom_highbd_smooth_h_predictor_64x64 neon/;
+
+  if ((aom_config("CONFIG_REALTIME_ONLY") ne "yes") ||
+      (aom_config("CONFIG_AV1_DECODER") eq "yes")) {
+    specialize qw/aom_highbd_v_predictor_4x16 neon/;
+    specialize qw/aom_highbd_v_predictor_8x32 neon/;
+    specialize qw/aom_highbd_v_predictor_16x4 neon/;
+    specialize qw/aom_highbd_v_predictor_16x64 neon/;
+    specialize qw/aom_highbd_v_predictor_32x8 neon/;
+    specialize qw/aom_highbd_v_predictor_64x16 neon/;
+
+    specialize qw/aom_highbd_dc_predictor_4x16 neon/;
+    specialize qw/aom_highbd_dc_predictor_8x32 neon/;
+    specialize qw/aom_highbd_dc_predictor_16x4 neon/;
+    specialize qw/aom_highbd_dc_predictor_16x64 neon/;
+    specialize qw/aom_highbd_dc_predictor_32x8 neon/;
+    specialize qw/aom_highbd_dc_predictor_64x16 neon/;
+
+    specialize qw/aom_highbd_h_predictor_4x16 neon/;
+    specialize qw/aom_highbd_h_predictor_8x32 neon/;
+    specialize qw/aom_highbd_h_predictor_16x4 neon/;
+    specialize qw/aom_highbd_h_predictor_16x64 neon/;
+    specialize qw/aom_highbd_h_predictor_32x8 neon/;
+    specialize qw/aom_highbd_h_predictor_64x16 neon/;
+
+    specialize qw/aom_highbd_dc_128_predictor_4x16 neon/;
+    specialize qw/aom_highbd_dc_128_predictor_8x32 neon/;
+    specialize qw/aom_highbd_dc_128_predictor_16x4 neon/;
+    specialize qw/aom_highbd_dc_128_predictor_16x64 neon/;
+    specialize qw/aom_highbd_dc_128_predictor_32x8 neon/;
+    specialize qw/aom_highbd_dc_128_predictor_64x16 neon/;
+
+    specialize qw/aom_highbd_dc_left_predictor_4x16 neon/;
+    specialize qw/aom_highbd_dc_left_predictor_8x32 neon/;
+    specialize qw/aom_highbd_dc_left_predictor_16x4 neon/;
+    specialize qw/aom_highbd_dc_left_predictor_16x64 neon/;
+    specialize qw/aom_highbd_dc_left_predictor_32x8 neon/;
+    specialize qw/aom_highbd_dc_left_predictor_64x16 neon/;
+
+    specialize qw/aom_highbd_dc_top_predictor_4x16 neon/;
+    specialize qw/aom_highbd_dc_top_predictor_8x32 neon/;
+    specialize qw/aom_highbd_dc_top_predictor_16x4 neon/;
+    specialize qw/aom_highbd_dc_top_predictor_16x64 neon/;
+    specialize qw/aom_highbd_dc_top_predictor_32x8 neon/;
+    specialize qw/aom_highbd_dc_top_predictor_64x16 neon/;
+
+    specialize qw/aom_highbd_paeth_predictor_4x16 neon/;
+    specialize qw/aom_highbd_paeth_predictor_8x32 neon/;
+    specialize qw/aom_highbd_paeth_predictor_16x4 neon/;
+    specialize qw/aom_highbd_paeth_predictor_16x64 neon/;
+    specialize qw/aom_highbd_paeth_predictor_32x8 neon/;
+    specialize qw/aom_highbd_paeth_predictor_64x16 neon/;
+
+    specialize qw/aom_highbd_smooth_predictor_4x16 neon/;
+    specialize qw/aom_highbd_smooth_predictor_8x32 neon/;
+    specialize qw/aom_highbd_smooth_predictor_16x4 neon/;
+    specialize qw/aom_highbd_smooth_predictor_16x64 neon/;
+    specialize qw/aom_highbd_smooth_predictor_32x8 neon/;
+    specialize qw/aom_highbd_smooth_predictor_64x16 neon/;
+
+    specialize qw/aom_highbd_smooth_v_predictor_4x16 neon/;
+    specialize qw/aom_highbd_smooth_v_predictor_8x32 neon/;
+    specialize qw/aom_highbd_smooth_v_predictor_16x4 neon/;
+    specialize qw/aom_highbd_smooth_v_predictor_16x64 neon/;
+    specialize qw/aom_highbd_smooth_v_predictor_32x8 neon/;
+    specialize qw/aom_highbd_smooth_v_predictor_64x16 neon/;
+
+    specialize qw/aom_highbd_smooth_h_predictor_4x16 neon/;
+    specialize qw/aom_highbd_smooth_h_predictor_8x32 neon/;
+    specialize qw/aom_highbd_smooth_h_predictor_16x4 neon/;
+    specialize qw/aom_highbd_smooth_h_predictor_16x64 neon/;
+    specialize qw/aom_highbd_smooth_h_predictor_32x8 neon/;
+    specialize qw/aom_highbd_smooth_h_predictor_64x16 neon/;
+  }  # !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 }
 #
 # Sub Pixel Filters
@@ -797,8 +827,10 @@ if (aom_config("CONFIG_AV1_ENCODER") eq "yes") {
   add_proto qw/uint64_t aom_var_2d_u8/, "uint8_t *src, int src_stride, int width, int height";
   specialize qw/aom_var_2d_u8 sse2 avx2 neon neon_dotprod/;
 
-  add_proto qw/uint64_t aom_var_2d_u16/, "uint8_t *src, int src_stride, int width, int height";
-  specialize qw/aom_var_2d_u16 sse2 avx2 neon sve/;
+  if (aom_config("CONFIG_AV1_HIGHBITDEPTH") eq "yes") {
+    add_proto qw/uint64_t aom_var_2d_u16/, "uint8_t *src, int src_stride, int width, int height";
+    specialize qw/aom_var_2d_u16 sse2 avx2 neon sve/;
+  }
 
   #
   # Single block SAD / Single block Avg SAD
@@ -1283,11 +1315,6 @@ if (aom_config("CONFIG_AV1_ENCODER") eq "yes") {
   add_proto qw/void aom_ssim_parms_8x8/, "const uint8_t *s, int sp, const uint8_t *r, int rp, uint32_t *sum_s, uint32_t *sum_r, uint32_t *sum_sq_s, uint32_t *sum_sq_r, uint32_t *sum_sxr";
   specialize qw/aom_ssim_parms_8x8/, "$sse2_x86_64";
 
-  if (aom_config("CONFIG_INTERNAL_STATS") eq "yes") {
-    add_proto qw/void aom_ssim_parms_16x16/, "const uint8_t *s, int sp, const uint8_t *r, int rp, uint32_t *sum_s, uint32_t *sum_r, uint32_t *sum_sq_s, uint32_t *sum_sq_r, uint32_t *sum_sxr";
-    specialize qw/aom_ssim_parms_16x16/, "$sse2_x86_64";
-  }
-
   if (aom_config("CONFIG_AV1_HIGHBITDEPTH") eq "yes") {
     add_proto qw/void aom_highbd_ssim_parms_8x8/, "const uint16_t *s, int sp, const uint16_t *r, int rp, uint32_t *sum_s, uint32_t *sum_r, uint32_t *sum_sq_s, uint32_t *sum_sq_r, uint32_t *sum_sxr";
   }
@@ -1341,11 +1368,10 @@ if (aom_config("CONFIG_AV1_ENCODER") eq "yes") {
     }
   }
 
-  #
-  #
-  #
-  add_proto qw/unsigned int aom_get_mb_ss/, "const int16_t *";
-  specialize qw/aom_get_mb_ss sse2 neon/;
+  if (aom_config("CONFIG_REALTIME_ONLY") ne "yes") {
+    add_proto qw/unsigned int aom_get_mb_ss/, "const int16_t *";
+    specialize qw/aom_get_mb_ss sse2 neon/;
+  }
 
   #
   # Variance / Subpixel Variance / Subpixel Avg Variance
diff --git a/aom_dsp/arm/highbd_intrapred_neon.c b/aom_dsp/arm/highbd_intrapred_neon.c
index fe4f6fdd2..3df51fbfb 100644
--- a/aom_dsp/arm/highbd_intrapred_neon.c
+++ b/aom_dsp/arm/highbd_intrapred_neon.c
@@ -200,6 +200,7 @@ static inline int highbd_dc_predictor_rect(int bw, int bh, int sum, int shift1,
     highbd_dc_store_##w##xh(dst, stride, (h), vdup##q##_n_u16(dc0));    \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_DC_PREDICTOR_RECT(4, 8, , 2, HIGHBD_DC_MULTIPLIER_1X2)
 HIGHBD_DC_PREDICTOR_RECT(4, 16, , 2, HIGHBD_DC_MULTIPLIER_1X4)
 HIGHBD_DC_PREDICTOR_RECT(8, 4, q, 2, HIGHBD_DC_MULTIPLIER_1X2)
@@ -214,6 +215,16 @@ HIGHBD_DC_PREDICTOR_RECT(32, 16, q, 4, HIGHBD_DC_MULTIPLIER_1X2)
 HIGHBD_DC_PREDICTOR_RECT(32, 64, q, 5, HIGHBD_DC_MULTIPLIER_1X2)
 HIGHBD_DC_PREDICTOR_RECT(64, 16, q, 4, HIGHBD_DC_MULTIPLIER_1X4)
 HIGHBD_DC_PREDICTOR_RECT(64, 32, q, 5, HIGHBD_DC_MULTIPLIER_1X2)
+#else
+HIGHBD_DC_PREDICTOR_RECT(4, 8, , 2, HIGHBD_DC_MULTIPLIER_1X2)
+HIGHBD_DC_PREDICTOR_RECT(8, 4, q, 2, HIGHBD_DC_MULTIPLIER_1X2)
+HIGHBD_DC_PREDICTOR_RECT(8, 16, q, 3, HIGHBD_DC_MULTIPLIER_1X2)
+HIGHBD_DC_PREDICTOR_RECT(16, 8, q, 3, HIGHBD_DC_MULTIPLIER_1X2)
+HIGHBD_DC_PREDICTOR_RECT(16, 32, q, 4, HIGHBD_DC_MULTIPLIER_1X2)
+HIGHBD_DC_PREDICTOR_RECT(32, 16, q, 4, HIGHBD_DC_MULTIPLIER_1X2)
+HIGHBD_DC_PREDICTOR_RECT(32, 64, q, 5, HIGHBD_DC_MULTIPLIER_1X2)
+HIGHBD_DC_PREDICTOR_RECT(64, 32, q, 5, HIGHBD_DC_MULTIPLIER_1X2)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #undef HIGHBD_DC_PREDICTOR_RECT
 #undef HIGHBD_DC_MULTIPLIER_1X2
@@ -233,6 +244,7 @@ HIGHBD_DC_PREDICTOR_RECT(64, 32, q, 5, HIGHBD_DC_MULTIPLIER_1X2)
                             vdup##q##_n_u16(0x80 << (bd - 8))); \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_DC_PREDICTOR_128(4, 4, )
 HIGHBD_DC_PREDICTOR_128(4, 8, )
 HIGHBD_DC_PREDICTOR_128(4, 16, )
@@ -252,6 +264,21 @@ HIGHBD_DC_PREDICTOR_128(32, 64, q)
 HIGHBD_DC_PREDICTOR_128(64, 16, q)
 HIGHBD_DC_PREDICTOR_128(64, 32, q)
 HIGHBD_DC_PREDICTOR_128(64, 64, q)
+#else
+HIGHBD_DC_PREDICTOR_128(4, 4, )
+HIGHBD_DC_PREDICTOR_128(4, 8, )
+HIGHBD_DC_PREDICTOR_128(8, 4, q)
+HIGHBD_DC_PREDICTOR_128(8, 8, q)
+HIGHBD_DC_PREDICTOR_128(8, 16, q)
+HIGHBD_DC_PREDICTOR_128(16, 8, q)
+HIGHBD_DC_PREDICTOR_128(16, 16, q)
+HIGHBD_DC_PREDICTOR_128(16, 32, q)
+HIGHBD_DC_PREDICTOR_128(32, 16, q)
+HIGHBD_DC_PREDICTOR_128(32, 32, q)
+HIGHBD_DC_PREDICTOR_128(32, 64, q)
+HIGHBD_DC_PREDICTOR_128(64, 32, q)
+HIGHBD_DC_PREDICTOR_128(64, 64, q)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #undef HIGHBD_DC_PREDICTOR_128
 
@@ -294,6 +321,7 @@ static inline uint32x4_t highbd_dc_load_sum_64(const uint16_t *left) {
     highbd_dc_store_##w##xh(dst, stride, (h), vdup##q##_lane_u16(dc0, 0)); \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 DC_PREDICTOR_LEFT(4, 4, 2, )
 DC_PREDICTOR_LEFT(4, 8, 3, )
 DC_PREDICTOR_LEFT(4, 16, 4, )
@@ -313,6 +341,21 @@ DC_PREDICTOR_LEFT(32, 64, 6, q)
 DC_PREDICTOR_LEFT(64, 16, 4, q)
 DC_PREDICTOR_LEFT(64, 32, 5, q)
 DC_PREDICTOR_LEFT(64, 64, 6, q)
+#else
+DC_PREDICTOR_LEFT(4, 4, 2, )
+DC_PREDICTOR_LEFT(4, 8, 3, )
+DC_PREDICTOR_LEFT(8, 4, 2, q)
+DC_PREDICTOR_LEFT(8, 8, 3, q)
+DC_PREDICTOR_LEFT(8, 16, 4, q)
+DC_PREDICTOR_LEFT(16, 8, 3, q)
+DC_PREDICTOR_LEFT(16, 16, 4, q)
+DC_PREDICTOR_LEFT(16, 32, 5, q)
+DC_PREDICTOR_LEFT(32, 16, 4, q)
+DC_PREDICTOR_LEFT(32, 32, 5, q)
+DC_PREDICTOR_LEFT(32, 64, 6, q)
+DC_PREDICTOR_LEFT(64, 32, 5, q)
+DC_PREDICTOR_LEFT(64, 64, 6, q)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #undef DC_PREDICTOR_LEFT
 
@@ -330,6 +373,7 @@ DC_PREDICTOR_LEFT(64, 64, 6, q)
     highbd_dc_store_##w##xh(dst, stride, (h), vdup##q##_lane_u16(dc0, 0)); \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 DC_PREDICTOR_TOP(4, 4, 2, )
 DC_PREDICTOR_TOP(4, 8, 2, )
 DC_PREDICTOR_TOP(4, 16, 2, )
@@ -349,6 +393,21 @@ DC_PREDICTOR_TOP(32, 64, 5, q)
 DC_PREDICTOR_TOP(64, 16, 6, q)
 DC_PREDICTOR_TOP(64, 32, 6, q)
 DC_PREDICTOR_TOP(64, 64, 6, q)
+#else
+DC_PREDICTOR_TOP(4, 4, 2, )
+DC_PREDICTOR_TOP(4, 8, 2, )
+DC_PREDICTOR_TOP(8, 4, 3, q)
+DC_PREDICTOR_TOP(8, 8, 3, q)
+DC_PREDICTOR_TOP(8, 16, 3, q)
+DC_PREDICTOR_TOP(16, 8, 4, q)
+DC_PREDICTOR_TOP(16, 16, 4, q)
+DC_PREDICTOR_TOP(16, 32, 4, q)
+DC_PREDICTOR_TOP(32, 16, 5, q)
+DC_PREDICTOR_TOP(32, 32, 5, q)
+DC_PREDICTOR_TOP(32, 64, 5, q)
+DC_PREDICTOR_TOP(64, 32, 6, q)
+DC_PREDICTOR_TOP(64, 64, 6, q)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #undef DC_PREDICTOR_TOP
 
@@ -459,6 +518,7 @@ static inline void vertical64xh_neon(uint16_t *dst, ptrdiff_t stride,
   } while (y != 0);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_V_NXM(4, 4)
 HIGHBD_V_NXM(4, 8)
 HIGHBD_V_NXM(4, 16)
@@ -482,6 +542,25 @@ HIGHBD_V_NXM(32, 64)
 HIGHBD_V_NXM(64, 16)
 HIGHBD_V_NXM(64, 32)
 HIGHBD_V_NXM(64, 64)
+#else
+HIGHBD_V_NXM(4, 4)
+HIGHBD_V_NXM(4, 8)
+
+HIGHBD_V_NXM(8, 4)
+HIGHBD_V_NXM(8, 8)
+HIGHBD_V_NXM(8, 16)
+
+HIGHBD_V_NXM(16, 8)
+HIGHBD_V_NXM(16, 16)
+HIGHBD_V_NXM(16, 32)
+
+HIGHBD_V_NXM(32, 16)
+HIGHBD_V_NXM(32, 32)
+HIGHBD_V_NXM(32, 64)
+
+HIGHBD_V_NXM(64, 32)
+HIGHBD_V_NXM(64, 64)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // -----------------------------------------------------------------------------
 // H_PRED
@@ -585,6 +664,7 @@ void aom_highbd_h_predictor_8x8_neon(uint16_t *dst, ptrdiff_t stride,
   highbd_h_store_8x4(dst + 4 * stride, stride, vget_high_u16(l));
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_highbd_h_predictor_16x4_neon(uint16_t *dst, ptrdiff_t stride,
                                       const uint16_t *above,
                                       const uint16_t *left, int bd) {
@@ -592,6 +672,7 @@ void aom_highbd_h_predictor_16x4_neon(uint16_t *dst, ptrdiff_t stride,
   (void)bd;
   highbd_h_store_16x4(dst, stride, vld1_u16(left));
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_highbd_h_predictor_16x8_neon(uint16_t *dst, ptrdiff_t stride,
                                       const uint16_t *above,
@@ -603,6 +684,7 @@ void aom_highbd_h_predictor_16x8_neon(uint16_t *dst, ptrdiff_t stride,
   highbd_h_store_16x4(dst + 4 * stride, stride, vget_high_u16(l));
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_highbd_h_predictor_32x8_neon(uint16_t *dst, ptrdiff_t stride,
                                       const uint16_t *above,
                                       const uint16_t *left, int bd) {
@@ -612,6 +694,7 @@ void aom_highbd_h_predictor_32x8_neon(uint16_t *dst, ptrdiff_t stride,
   highbd_h_store_32x4(dst + 0 * stride, stride, vget_low_u16(l));
   highbd_h_store_32x4(dst + 4 * stride, stride, vget_high_u16(l));
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // For cases where height >= 16 we use pairs of loads to get LDP instructions.
 #define HIGHBD_H_WXH_LARGE(w, h)                                            \
@@ -632,6 +715,7 @@ void aom_highbd_h_predictor_32x8_neon(uint16_t *dst, ptrdiff_t stride,
     }                                                                       \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_H_WXH_LARGE(4, 16)
 HIGHBD_H_WXH_LARGE(8, 16)
 HIGHBD_H_WXH_LARGE(8, 32)
@@ -644,6 +728,16 @@ HIGHBD_H_WXH_LARGE(32, 64)
 HIGHBD_H_WXH_LARGE(64, 16)
 HIGHBD_H_WXH_LARGE(64, 32)
 HIGHBD_H_WXH_LARGE(64, 64)
+#else
+HIGHBD_H_WXH_LARGE(8, 16)
+HIGHBD_H_WXH_LARGE(16, 16)
+HIGHBD_H_WXH_LARGE(16, 32)
+HIGHBD_H_WXH_LARGE(32, 16)
+HIGHBD_H_WXH_LARGE(32, 32)
+HIGHBD_H_WXH_LARGE(32, 64)
+HIGHBD_H_WXH_LARGE(64, 32)
+HIGHBD_H_WXH_LARGE(64, 64)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #undef HIGHBD_H_WXH_LARGE
 
@@ -706,6 +800,7 @@ static inline void highbd_paeth_4or8_x_h_neon(uint16_t *dest, ptrdiff_t stride,
     highbd_paeth_4or8_x_h_neon(dst, stride, above, left, W, H); \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_PAETH_NXM(4, 4)
 HIGHBD_PAETH_NXM(4, 8)
 HIGHBD_PAETH_NXM(4, 16)
@@ -713,6 +808,13 @@ HIGHBD_PAETH_NXM(8, 4)
 HIGHBD_PAETH_NXM(8, 8)
 HIGHBD_PAETH_NXM(8, 16)
 HIGHBD_PAETH_NXM(8, 32)
+#else
+HIGHBD_PAETH_NXM(4, 4)
+HIGHBD_PAETH_NXM(4, 8)
+HIGHBD_PAETH_NXM(8, 4)
+HIGHBD_PAETH_NXM(8, 8)
+HIGHBD_PAETH_NXM(8, 16)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // Select the closest values and collect them.
 static inline uint16x8_t select_paeth(const uint16x8_t top,
@@ -798,6 +900,7 @@ static inline void highbd_paeth16_plus_x_h_neon(
     highbd_paeth16_plus_x_h_neon(dst, stride, above, left, W, H); \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_PAETH_NXM_WIDE(16, 4)
 HIGHBD_PAETH_NXM_WIDE(16, 8)
 HIGHBD_PAETH_NXM_WIDE(16, 16)
@@ -810,6 +913,16 @@ HIGHBD_PAETH_NXM_WIDE(32, 64)
 HIGHBD_PAETH_NXM_WIDE(64, 16)
 HIGHBD_PAETH_NXM_WIDE(64, 32)
 HIGHBD_PAETH_NXM_WIDE(64, 64)
+#else
+HIGHBD_PAETH_NXM_WIDE(16, 8)
+HIGHBD_PAETH_NXM_WIDE(16, 16)
+HIGHBD_PAETH_NXM_WIDE(16, 32)
+HIGHBD_PAETH_NXM_WIDE(32, 16)
+HIGHBD_PAETH_NXM_WIDE(32, 32)
+HIGHBD_PAETH_NXM_WIDE(32, 64)
+HIGHBD_PAETH_NXM_WIDE(64, 32)
+HIGHBD_PAETH_NXM_WIDE(64, 64)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // -----------------------------------------------------------------------------
 // SMOOTH
@@ -916,6 +1029,7 @@ static void highbd_smooth_8xh_neon(uint16_t *dst, ptrdiff_t stride,
     highbd_smooth_##W##xh_neon(dst, y_stride, above, left, H);  \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_SMOOTH_NXM(4, 4)
 HIGHBD_SMOOTH_NXM(4, 8)
 HIGHBD_SMOOTH_NXM(8, 4)
@@ -923,6 +1037,13 @@ HIGHBD_SMOOTH_NXM(8, 8)
 HIGHBD_SMOOTH_NXM(4, 16)
 HIGHBD_SMOOTH_NXM(8, 16)
 HIGHBD_SMOOTH_NXM(8, 32)
+#else
+HIGHBD_SMOOTH_NXM(4, 4)
+HIGHBD_SMOOTH_NXM(4, 8)
+HIGHBD_SMOOTH_NXM(8, 4)
+HIGHBD_SMOOTH_NXM(8, 8)
+HIGHBD_SMOOTH_NXM(8, 16)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #undef HIGHBD_SMOOTH_NXM
 
@@ -989,6 +1110,7 @@ HIGHBD_SMOOTH_PREDICTOR(64)
     highbd_smooth_##W##xh_neon(dst, y_stride, above, left, H);  \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_SMOOTH_NXM_WIDE(16, 4)
 HIGHBD_SMOOTH_NXM_WIDE(16, 8)
 HIGHBD_SMOOTH_NXM_WIDE(16, 16)
@@ -1001,6 +1123,16 @@ HIGHBD_SMOOTH_NXM_WIDE(32, 64)
 HIGHBD_SMOOTH_NXM_WIDE(64, 16)
 HIGHBD_SMOOTH_NXM_WIDE(64, 32)
 HIGHBD_SMOOTH_NXM_WIDE(64, 64)
+#else
+HIGHBD_SMOOTH_NXM_WIDE(16, 8)
+HIGHBD_SMOOTH_NXM_WIDE(16, 16)
+HIGHBD_SMOOTH_NXM_WIDE(16, 32)
+HIGHBD_SMOOTH_NXM_WIDE(32, 16)
+HIGHBD_SMOOTH_NXM_WIDE(32, 32)
+HIGHBD_SMOOTH_NXM_WIDE(32, 64)
+HIGHBD_SMOOTH_NXM_WIDE(64, 32)
+HIGHBD_SMOOTH_NXM_WIDE(64, 64)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #undef HIGHBD_SMOOTH_NXM_WIDE
 
@@ -1060,6 +1192,7 @@ static void highbd_smooth_v_8xh_neon(uint16_t *dst, const ptrdiff_t stride,
     highbd_smooth_v_##W##xh_neon(dst, y_stride, above, left, H); \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_SMOOTH_V_NXM(4, 4)
 HIGHBD_SMOOTH_V_NXM(4, 8)
 HIGHBD_SMOOTH_V_NXM(4, 16)
@@ -1067,6 +1200,13 @@ HIGHBD_SMOOTH_V_NXM(8, 4)
 HIGHBD_SMOOTH_V_NXM(8, 8)
 HIGHBD_SMOOTH_V_NXM(8, 16)
 HIGHBD_SMOOTH_V_NXM(8, 32)
+#else
+HIGHBD_SMOOTH_V_NXM(4, 4)
+HIGHBD_SMOOTH_V_NXM(4, 8)
+HIGHBD_SMOOTH_V_NXM(8, 4)
+HIGHBD_SMOOTH_V_NXM(8, 8)
+HIGHBD_SMOOTH_V_NXM(8, 16)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #undef HIGHBD_SMOOTH_V_NXM
 
@@ -1121,6 +1261,7 @@ HIGHBD_SMOOTH_V_PREDICTOR(64)
     highbd_smooth_v_##W##xh_neon(dst, y_stride, above, left, H); \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_SMOOTH_V_NXM_WIDE(16, 4)
 HIGHBD_SMOOTH_V_NXM_WIDE(16, 8)
 HIGHBD_SMOOTH_V_NXM_WIDE(16, 16)
@@ -1133,6 +1274,16 @@ HIGHBD_SMOOTH_V_NXM_WIDE(32, 64)
 HIGHBD_SMOOTH_V_NXM_WIDE(64, 16)
 HIGHBD_SMOOTH_V_NXM_WIDE(64, 32)
 HIGHBD_SMOOTH_V_NXM_WIDE(64, 64)
+#else
+HIGHBD_SMOOTH_V_NXM_WIDE(16, 8)
+HIGHBD_SMOOTH_V_NXM_WIDE(16, 16)
+HIGHBD_SMOOTH_V_NXM_WIDE(16, 32)
+HIGHBD_SMOOTH_V_NXM_WIDE(32, 16)
+HIGHBD_SMOOTH_V_NXM_WIDE(32, 32)
+HIGHBD_SMOOTH_V_NXM_WIDE(32, 64)
+HIGHBD_SMOOTH_V_NXM_WIDE(64, 32)
+HIGHBD_SMOOTH_V_NXM_WIDE(64, 64)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #undef HIGHBD_SMOOTH_V_NXM_WIDE
 
@@ -1190,6 +1341,7 @@ static inline void highbd_smooth_h_8xh_neon(uint16_t *dst, ptrdiff_t stride,
     highbd_smooth_h_##W##xh_neon(dst, y_stride, above, left, H); \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_SMOOTH_H_NXM(4, 4)
 HIGHBD_SMOOTH_H_NXM(4, 8)
 HIGHBD_SMOOTH_H_NXM(4, 16)
@@ -1197,6 +1349,13 @@ HIGHBD_SMOOTH_H_NXM(8, 4)
 HIGHBD_SMOOTH_H_NXM(8, 8)
 HIGHBD_SMOOTH_H_NXM(8, 16)
 HIGHBD_SMOOTH_H_NXM(8, 32)
+#else
+HIGHBD_SMOOTH_H_NXM(4, 4)
+HIGHBD_SMOOTH_H_NXM(4, 8)
+HIGHBD_SMOOTH_H_NXM(8, 4)
+HIGHBD_SMOOTH_H_NXM(8, 8)
+HIGHBD_SMOOTH_H_NXM(8, 16)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #undef HIGHBD_SMOOTH_H_NXM
 
@@ -1254,6 +1413,7 @@ HIGHBD_SMOOTH_H_PREDICTOR(64)
     highbd_smooth_h_##W##xh_neon(dst, y_stride, above, left, H); \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_SMOOTH_H_NXM_WIDE(16, 4)
 HIGHBD_SMOOTH_H_NXM_WIDE(16, 8)
 HIGHBD_SMOOTH_H_NXM_WIDE(16, 16)
@@ -1266,6 +1426,16 @@ HIGHBD_SMOOTH_H_NXM_WIDE(32, 64)
 HIGHBD_SMOOTH_H_NXM_WIDE(64, 16)
 HIGHBD_SMOOTH_H_NXM_WIDE(64, 32)
 HIGHBD_SMOOTH_H_NXM_WIDE(64, 64)
+#else
+HIGHBD_SMOOTH_H_NXM_WIDE(16, 8)
+HIGHBD_SMOOTH_H_NXM_WIDE(16, 16)
+HIGHBD_SMOOTH_H_NXM_WIDE(16, 32)
+HIGHBD_SMOOTH_H_NXM_WIDE(32, 16)
+HIGHBD_SMOOTH_H_NXM_WIDE(32, 32)
+HIGHBD_SMOOTH_H_NXM_WIDE(32, 64)
+HIGHBD_SMOOTH_H_NXM_WIDE(64, 32)
+HIGHBD_SMOOTH_H_NXM_WIDE(64, 64)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #undef HIGHBD_SMOOTH_H_NXM_WIDE
 
@@ -1900,6 +2070,7 @@ static AOM_FORCE_INLINE uint16x8_t highbd_dr_prediction_z2_step_x8(
     } while (++r < bh);                                                    \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_DR_PREDICTOR_Z2_WXH(4, 16)
 HIGHBD_DR_PREDICTOR_Z2_WXH(8, 16)
 HIGHBD_DR_PREDICTOR_Z2_WXH(8, 32)
@@ -1915,6 +2086,16 @@ HIGHBD_DR_PREDICTOR_Z2_WXH(32, 64)
 HIGHBD_DR_PREDICTOR_Z2_WXH(64, 16)
 HIGHBD_DR_PREDICTOR_Z2_WXH(64, 32)
 HIGHBD_DR_PREDICTOR_Z2_WXH(64, 64)
+#else
+HIGHBD_DR_PREDICTOR_Z2_WXH(8, 16)
+HIGHBD_DR_PREDICTOR_Z2_WXH(16, 8)
+HIGHBD_DR_PREDICTOR_Z2_WXH(16, 16)
+HIGHBD_DR_PREDICTOR_Z2_WXH(16, 32)
+HIGHBD_DR_PREDICTOR_Z2_WXH(32, 32)
+HIGHBD_DR_PREDICTOR_Z2_WXH(32, 64)
+HIGHBD_DR_PREDICTOR_Z2_WXH(64, 32)
+HIGHBD_DR_PREDICTOR_Z2_WXH(64, 64)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #undef HIGHBD_DR_PREDICTOR_Z2_WXH
 
@@ -2442,6 +2623,7 @@ static void highbd_dr_prediction_z2_8x8_neon(uint16_t *dst, ptrdiff_t stride,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static highbd_dr_prediction_z2_ptr dr_predictor_z2_arr_neon[7][7] = {
   { NULL, NULL, NULL, NULL, NULL, NULL, NULL },
   { NULL, NULL, NULL, NULL, NULL, NULL, NULL },
@@ -2460,6 +2642,24 @@ static highbd_dr_prediction_z2_ptr dr_predictor_z2_arr_neon[7][7] = {
   { NULL, NULL, NULL, NULL, &highbd_dr_prediction_z2_64x16_neon,
     &highbd_dr_prediction_z2_64x32_neon, &highbd_dr_prediction_z2_64x64_neon },
 };
+#else
+static highbd_dr_prediction_z2_ptr dr_predictor_z2_arr_neon[7][7] = {
+  { NULL, NULL, NULL, NULL, NULL, NULL, NULL },
+  { NULL, NULL, NULL, NULL, NULL, NULL, NULL },
+  { NULL, NULL, &highbd_dr_prediction_z2_4x4_neon,
+    &highbd_dr_prediction_z2_4x8_neon, NULL, NULL, NULL },
+  { NULL, NULL, &highbd_dr_prediction_z2_8x4_neon,
+    &highbd_dr_prediction_z2_8x8_neon, &highbd_dr_prediction_z2_8x16_neon, NULL,
+    NULL },
+  { NULL, NULL, NULL, &highbd_dr_prediction_z2_16x8_neon,
+    &highbd_dr_prediction_z2_16x16_neon, &highbd_dr_prediction_z2_16x32_neon,
+    NULL },
+  { NULL, NULL, NULL, NULL, NULL, &highbd_dr_prediction_z2_32x32_neon,
+    &highbd_dr_prediction_z2_32x64_neon },
+  { NULL, NULL, NULL, NULL, NULL, &highbd_dr_prediction_z2_64x32_neon,
+    &highbd_dr_prediction_z2_64x64_neon },
+};
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // Directional prediction, zone 2: 90 < angle < 180
 void av1_highbd_dr_prediction_z2_neon(uint16_t *dst, ptrdiff_t stride, int bw,
diff --git a/aom_dsp/arm/intrapred_neon.c b/aom_dsp/arm/intrapred_neon.c
index 29936c260..b1bdc832c 100644
--- a/aom_dsp/arm/intrapred_neon.c
+++ b/aom_dsp/arm/intrapred_neon.c
@@ -374,6 +374,7 @@ void aom_dc_predictor_8x4_neon(uint8_t *dst, ptrdiff_t stride,
   dc_store_8xh(dst, stride, 4, vdup_n_u8(dc));
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_predictor_4x16_neon(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
   uint8x8_t a = load_u8_4x1(above);
@@ -393,6 +394,7 @@ void aom_dc_predictor_16x4_neon(uint8_t *dst, ptrdiff_t stride,
   uint32_t dc = calculate_dc_from_sum(16, 4, sum, 2, DC_MULTIPLIER_1X4);
   dc_store_16xh(dst, stride, 4, vdupq_n_u8(dc));
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_predictor_8x16_neon(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
@@ -414,6 +416,7 @@ void aom_dc_predictor_16x8_neon(uint8_t *dst, ptrdiff_t stride,
   dc_store_16xh(dst, stride, 8, vdupq_n_u8(dc));
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_predictor_8x32_neon(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
   uint8x8_t a = vld1_u8(above);
@@ -433,6 +436,7 @@ void aom_dc_predictor_32x8_neon(uint8_t *dst, ptrdiff_t stride,
   uint32_t dc = calculate_dc_from_sum(32, 8, sum, 3, DC_MULTIPLIER_1X4);
   dc_store_32xh(dst, stride, 8, vdupq_n_u8(dc));
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_predictor_16x32_neon(uint8_t *dst, ptrdiff_t stride,
                                  const uint8_t *above, const uint8_t *left) {
@@ -454,6 +458,7 @@ void aom_dc_predictor_32x16_neon(uint8_t *dst, ptrdiff_t stride,
   dc_store_32xh(dst, stride, 16, vdupq_n_u8(dc));
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_predictor_16x64_neon(uint8_t *dst, ptrdiff_t stride,
                                  const uint8_t *above, const uint8_t *left) {
   uint16x8_t sum_above = dc_load_partial_sum_16(above);
@@ -473,6 +478,7 @@ void aom_dc_predictor_64x16_neon(uint8_t *dst, ptrdiff_t stride,
   uint32_t dc = calculate_dc_from_sum(64, 16, sum, 4, DC_MULTIPLIER_1X4);
   dc_store_64xh(dst, stride, 16, vdupq_n_u8(dc));
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_predictor_32x64_neon(uint8_t *dst, ptrdiff_t stride,
                                  const uint8_t *above, const uint8_t *left) {
@@ -506,20 +512,22 @@ void aom_dc_predictor_64x32_neon(uint8_t *dst, ptrdiff_t stride,
     dc_store_##w##xh(dst, stride, (h), vdup##q##_n_u8(0x80));                \
   }
 
-DC_PREDICTOR_128(4, 8, )
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 DC_PREDICTOR_128(4, 16, )
-DC_PREDICTOR_128(8, 4, )
-DC_PREDICTOR_128(8, 16, )
 DC_PREDICTOR_128(8, 32, )
 DC_PREDICTOR_128(16, 4, q)
-DC_PREDICTOR_128(16, 8, q)
-DC_PREDICTOR_128(16, 32, q)
 DC_PREDICTOR_128(16, 64, q)
 DC_PREDICTOR_128(32, 8, q)
+DC_PREDICTOR_128(64, 16, q)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
+DC_PREDICTOR_128(4, 8, )
+DC_PREDICTOR_128(8, 4, )
+DC_PREDICTOR_128(8, 16, )
+DC_PREDICTOR_128(16, 8, q)
+DC_PREDICTOR_128(16, 32, q)
 DC_PREDICTOR_128(32, 16, q)
 DC_PREDICTOR_128(32, 64, q)
 DC_PREDICTOR_128(64, 32, q)
-DC_PREDICTOR_128(64, 16, q)
 
 #undef DC_PREDICTOR_128
 
@@ -541,12 +549,14 @@ DC_PREDICTOR_LEFT(16, 32, 5, q)
 DC_PREDICTOR_LEFT(32, 16, 4, q)
 DC_PREDICTOR_LEFT(32, 64, 6, q)
 DC_PREDICTOR_LEFT(64, 32, 5, q)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 DC_PREDICTOR_LEFT(4, 16, 4, )
 DC_PREDICTOR_LEFT(16, 4, 2, q)
 DC_PREDICTOR_LEFT(8, 32, 5, )
 DC_PREDICTOR_LEFT(32, 8, 3, q)
 DC_PREDICTOR_LEFT(16, 64, 6, q)
 DC_PREDICTOR_LEFT(64, 16, 4, q)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #undef DC_PREDICTOR_LEFT
 
@@ -560,19 +570,21 @@ DC_PREDICTOR_LEFT(64, 16, 4, q)
     dc_store_##w##xh(dst, stride, (h), vdup##q##_lane_u8(dc0, 0));           \
   }
 
-DC_PREDICTOR_TOP(4, 8, 2, )
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
+DC_PREDICTOR_TOP(8, 32, 3, )
 DC_PREDICTOR_TOP(4, 16, 2, )
+DC_PREDICTOR_TOP(16, 4, 4, q)
+DC_PREDICTOR_TOP(16, 64, 4, q)
+DC_PREDICTOR_TOP(32, 8, 5, q)
+DC_PREDICTOR_TOP(64, 16, 6, q)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
+DC_PREDICTOR_TOP(4, 8, 2, )
 DC_PREDICTOR_TOP(8, 4, 3, )
 DC_PREDICTOR_TOP(8, 16, 3, )
-DC_PREDICTOR_TOP(8, 32, 3, )
-DC_PREDICTOR_TOP(16, 4, 4, q)
 DC_PREDICTOR_TOP(16, 8, 4, q)
 DC_PREDICTOR_TOP(16, 32, 4, q)
-DC_PREDICTOR_TOP(16, 64, 4, q)
-DC_PREDICTOR_TOP(32, 8, 5, q)
 DC_PREDICTOR_TOP(32, 16, 5, q)
 DC_PREDICTOR_TOP(32, 64, 5, q)
-DC_PREDICTOR_TOP(64, 16, 6, q)
 DC_PREDICTOR_TOP(64, 32, 6, q)
 
 #undef DC_PREDICTOR_TOP
@@ -653,11 +665,13 @@ void aom_v_predictor_4x8_neon(uint8_t *dst, ptrdiff_t stride,
   v_store_4xh(dst, stride, 8, load_u8_4x1(above));
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_v_predictor_4x16_neon(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
   (void)left;
   v_store_4xh(dst, stride, 16, load_u8_4x1(above));
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_v_predictor_8x4_neon(uint8_t *dst, ptrdiff_t stride,
                               const uint8_t *above, const uint8_t *left) {
@@ -671,6 +685,7 @@ void aom_v_predictor_8x16_neon(uint8_t *dst, ptrdiff_t stride,
   v_store_8xh(dst, stride, 16, vld1_u8(above));
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_v_predictor_8x32_neon(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
   (void)left;
@@ -682,6 +697,7 @@ void aom_v_predictor_16x4_neon(uint8_t *dst, ptrdiff_t stride,
   (void)left;
   v_store_16xh(dst, stride, 4, vld1q_u8(above));
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_v_predictor_16x8_neon(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
@@ -695,6 +711,7 @@ void aom_v_predictor_16x32_neon(uint8_t *dst, ptrdiff_t stride,
   v_store_16xh(dst, stride, 32, vld1q_u8(above));
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_v_predictor_16x64_neon(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
   (void)left;
@@ -708,6 +725,7 @@ void aom_v_predictor_32x8_neon(uint8_t *dst, ptrdiff_t stride,
   (void)left;
   v_store_32xh(dst, stride, 8, d0, d1);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_v_predictor_32x16_neon(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
@@ -725,6 +743,7 @@ void aom_v_predictor_32x64_neon(uint8_t *dst, ptrdiff_t stride,
   v_store_32xh(dst, stride, 64, d0, d1);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_v_predictor_64x16_neon(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
   const uint8x16_t d0 = vld1q_u8(above);
@@ -734,6 +753,7 @@ void aom_v_predictor_64x16_neon(uint8_t *dst, ptrdiff_t stride,
   (void)left;
   v_store_64xh(dst, stride, 16, d0, d1, d2, d3);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_v_predictor_64x32_neon(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
@@ -901,6 +921,7 @@ void aom_h_predictor_4x8_neon(uint8_t *dst, ptrdiff_t stride,
   h_store_4x8(dst, stride, d0);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_h_predictor_4x16_neon(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
   const uint8x16_t d0 = vld1q_u8(left);
@@ -908,6 +929,7 @@ void aom_h_predictor_4x16_neon(uint8_t *dst, ptrdiff_t stride,
   h_store_4x8(dst + 0 * stride, stride, vget_low_u8(d0));
   h_store_4x8(dst + 8 * stride, stride, vget_high_u8(d0));
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_h_predictor_8x4_neon(uint8_t *dst, ptrdiff_t stride,
                               const uint8_t *above, const uint8_t *left) {
@@ -927,6 +949,7 @@ void aom_h_predictor_8x16_neon(uint8_t *dst, ptrdiff_t stride,
   h_store_8x8(dst + 8 * stride, stride, vget_high_u8(d0));
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_h_predictor_8x32_neon(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
   const uint8x16_t d0 = vld1q_u8(left);
@@ -947,6 +970,7 @@ void aom_h_predictor_16x4_neon(uint8_t *dst, ptrdiff_t stride,
   vst1q_u8(dst + 2 * stride, vdupq_lane_u8(d0, 2));
   vst1q_u8(dst + 3 * stride, vdupq_lane_u8(d0, 3));
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_h_predictor_16x8_neon(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
@@ -966,6 +990,7 @@ void aom_h_predictor_16x32_neon(uint8_t *dst, ptrdiff_t stride,
   h_store_16x8(dst + 24 * stride, stride, vget_high_u8(d1));
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_h_predictor_16x64_neon(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
   const uint8x16_t d0 = vld1q_u8(left);
@@ -989,6 +1014,7 @@ void aom_h_predictor_32x8_neon(uint8_t *dst, ptrdiff_t stride,
   (void)above;
   h_store_32x8(dst, stride, d0);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_h_predictor_32x16_neon(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
@@ -1015,6 +1041,7 @@ void aom_h_predictor_32x64_neon(uint8_t *dst, ptrdiff_t stride,
   h_store_32x8(dst + 56 * stride, stride, vget_high_u8(d3));
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_h_predictor_64x16_neon(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
   const uint8x16_t d0 = vld1q_u8(left);
@@ -1022,6 +1049,7 @@ void aom_h_predictor_64x16_neon(uint8_t *dst, ptrdiff_t stride,
   h_store_64x8(dst + 0 * stride, stride, vget_low_u8(d0));
   h_store_64x8(dst + 8 * stride, stride, vget_high_u8(d0));
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_h_predictor_64x32_neon(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
@@ -1050,7 +1078,7 @@ void aom_h_predictor_64x64_neon(uint8_t *dst, ptrdiff_t stride,
 /* ---------------------P R E D I C T I O N   Z 1--------------------------- */
 
 // Low bit depth functions
-static DECLARE_ALIGNED(32, uint8_t, BaseMask[33][32]) = {
+static DECLARE_ALIGNED(32, const uint8_t, BaseMask[33][32]) = {
   { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
   { 0xff, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
@@ -1491,7 +1519,7 @@ void av1_dr_prediction_z1_neon(uint8_t *dst, ptrdiff_t stride, int bw, int bh,
 // TODO(aomedia:349428506): enable this for armv7 after SIGBUS is fixed.
 #if AOM_ARCH_AARCH64
 #if !AOM_ARCH_AARCH64
-static DECLARE_ALIGNED(16, uint8_t, LoadMaskz2[4][16]) = {
+static DECLARE_ALIGNED(16, const uint8_t, LoadMaskz2[4][16]) = {
   { 0xff, 0xff, 0xff, 0xff, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
   { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0, 0, 0, 0, 0, 0, 0, 0 },
   { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0,
@@ -2043,7 +2071,7 @@ void av1_dr_prediction_z2_neon(uint8_t *dst, ptrdiff_t stride, int bw, int bh,
 #endif  // AOM_ARCH_AARCH64
 
 /* ---------------------P R E D I C T I O N   Z 3--------------------------- */
-
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static AOM_FORCE_INLINE void z3_transpose_arrays_u8_16x4(const uint8x16_t *x,
                                                          uint8x16x2_t *d) {
   uint8x16x2_t w0 = vzipq_u8(x[0], x[1]);
@@ -2054,6 +2082,7 @@ static AOM_FORCE_INLINE void z3_transpose_arrays_u8_16x4(const uint8x16_t *x,
   d[1] = aom_reinterpretq_u8_u16_x2(vzipq_u16(vreinterpretq_u16_u8(w0.val[1]),
                                               vreinterpretq_u16_u8(w1.val[1])));
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 static AOM_FORCE_INLINE void z3_transpose_arrays_u8_4x4(const uint8x8_t *x,
                                                         uint8x8x2_t *d) {
@@ -2176,6 +2205,7 @@ static void dr_prediction_z3_16x8_neon(uint8_t *dst, ptrdiff_t stride,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static void dr_prediction_z3_4x16_neon(uint8_t *dst, ptrdiff_t stride,
                                        const uint8_t *left, int upsample_left,
                                        int dy) {
@@ -2236,6 +2266,7 @@ static void dr_prediction_z3_32x8_neon(uint8_t *dst, ptrdiff_t stride,
     vst1q_u8(dst + i * stride + 16, d[i + 8]);
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 static void dr_prediction_z3_16x16_neon(uint8_t *dst, ptrdiff_t stride,
                                         const uint8_t *left, int upsample_left,
@@ -2326,6 +2357,7 @@ static void dr_prediction_z3_64x32_neon(uint8_t *dst, ptrdiff_t stride,
   z3_transpose_arrays_u8_16nx16n(dstT, 32, dst, stride, 64, 32);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static void dr_prediction_z3_16x64_neon(uint8_t *dst, ptrdiff_t stride,
                                         const uint8_t *left, int upsample_left,
                                         int dy) {
@@ -2350,12 +2382,29 @@ static void dr_prediction_z3_64x16_neon(uint8_t *dst, ptrdiff_t stride,
     }
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 typedef void (*dr_prediction_z3_fn)(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *left, int upsample_left,
                                     int dy);
 
-static dr_prediction_z3_fn dr_prediction_z3_arr[7][7] = {
+#if CONFIG_REALTIME_ONLY && !CONFIG_AV1_DECODER
+static const dr_prediction_z3_fn dr_prediction_z3_arr[7][7] = {
+  { NULL, NULL, NULL, NULL, NULL, NULL, NULL },
+  { NULL, NULL, NULL, NULL, NULL, NULL, NULL },
+  { NULL, NULL, dr_prediction_z3_4x4_neon, dr_prediction_z3_4x8_neon, NULL,
+    NULL, NULL },
+  { NULL, NULL, dr_prediction_z3_8x4_neon, dr_prediction_z3_8x8_neon,
+    dr_prediction_z3_8x16_neon, NULL, NULL },
+  { NULL, NULL, NULL, dr_prediction_z3_16x8_neon, dr_prediction_z3_16x16_neon,
+    dr_prediction_z3_16x32_neon, NULL },
+  { NULL, NULL, NULL, NULL, dr_prediction_z3_32x16_neon,
+    dr_prediction_z3_32x32_neon, dr_prediction_z3_32x64_neon },
+  { NULL, NULL, NULL, NULL, NULL, dr_prediction_z3_64x32_neon,
+    dr_prediction_z3_64x64_neon },
+};
+#else
+static const dr_prediction_z3_fn dr_prediction_z3_arr[7][7] = {
   { NULL, NULL, NULL, NULL, NULL, NULL, NULL },
   { NULL, NULL, NULL, NULL, NULL, NULL, NULL },
   { NULL, NULL, dr_prediction_z3_4x4_neon, dr_prediction_z3_4x8_neon,
@@ -2370,6 +2419,7 @@ static dr_prediction_z3_fn dr_prediction_z3_arr[7][7] = {
   { NULL, NULL, NULL, NULL, dr_prediction_z3_64x16_neon,
     dr_prediction_z3_64x32_neon, dr_prediction_z3_64x64_neon },
 };
+#endif  // CONFIG_REALTIME_ONLY && !CONFIG_AV1_DECODER
 
 void av1_dr_prediction_z3_neon(uint8_t *dst, ptrdiff_t stride, int bw, int bh,
                                const uint8_t *above, const uint8_t *left,
@@ -2486,9 +2536,11 @@ SMOOTH_NXM(4, 4)
 SMOOTH_NXM(4, 8)
 SMOOTH_NXM(8, 4)
 SMOOTH_NXM(8, 8)
-SMOOTH_NXM(4, 16)
 SMOOTH_NXM(8, 16)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
+SMOOTH_NXM(4, 16)
 SMOOTH_NXM(8, 32)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #undef SMOOTH_NXM
 
@@ -2607,16 +2659,18 @@ SMOOTH_PREDICTOR(64)
     smooth_##W##xh_neon(dst, y_stride, above, left, H);                        \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 SMOOTH_NXM_WIDE(16, 4)
+SMOOTH_NXM_WIDE(16, 64)
+SMOOTH_NXM_WIDE(32, 8)
+SMOOTH_NXM_WIDE(64, 16)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 SMOOTH_NXM_WIDE(16, 8)
 SMOOTH_NXM_WIDE(16, 16)
 SMOOTH_NXM_WIDE(16, 32)
-SMOOTH_NXM_WIDE(16, 64)
-SMOOTH_NXM_WIDE(32, 8)
 SMOOTH_NXM_WIDE(32, 16)
 SMOOTH_NXM_WIDE(32, 32)
 SMOOTH_NXM_WIDE(32, 64)
-SMOOTH_NXM_WIDE(64, 16)
 SMOOTH_NXM_WIDE(64, 32)
 SMOOTH_NXM_WIDE(64, 64)
 
@@ -2675,13 +2729,15 @@ SMOOTH_V_PREDICTOR(8)
     smooth_v_##W##xh_neon(dst, y_stride, above, left, H);     \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
+SMOOTH_V_NXM(4, 16)
+SMOOTH_V_NXM(8, 32)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 SMOOTH_V_NXM(4, 4)
 SMOOTH_V_NXM(4, 8)
-SMOOTH_V_NXM(4, 16)
 SMOOTH_V_NXM(8, 4)
 SMOOTH_V_NXM(8, 8)
 SMOOTH_V_NXM(8, 16)
-SMOOTH_V_NXM(8, 32)
 
 #undef SMOOTH_V_NXM
 
@@ -2764,16 +2820,18 @@ SMOOTH_V_PREDICTOR(64)
     smooth_v_##W##xh_neon(dst, y_stride, above, left, H);     \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 SMOOTH_V_NXM_WIDE(16, 4)
+SMOOTH_V_NXM_WIDE(32, 8)
+SMOOTH_V_NXM_WIDE(64, 16)
+SMOOTH_V_NXM_WIDE(16, 64)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 SMOOTH_V_NXM_WIDE(16, 8)
 SMOOTH_V_NXM_WIDE(16, 16)
 SMOOTH_V_NXM_WIDE(16, 32)
-SMOOTH_V_NXM_WIDE(16, 64)
-SMOOTH_V_NXM_WIDE(32, 8)
 SMOOTH_V_NXM_WIDE(32, 16)
 SMOOTH_V_NXM_WIDE(32, 32)
 SMOOTH_V_NXM_WIDE(32, 64)
-SMOOTH_V_NXM_WIDE(64, 16)
 SMOOTH_V_NXM_WIDE(64, 32)
 SMOOTH_V_NXM_WIDE(64, 64)
 
@@ -2825,13 +2883,15 @@ SMOOTH_H_PREDICTOR(8)
     smooth_h_##W##xh_neon(dst, y_stride, above, left, H);     \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
+SMOOTH_H_NXM(4, 16)
+SMOOTH_H_NXM(8, 32)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 SMOOTH_H_NXM(4, 4)
 SMOOTH_H_NXM(4, 8)
-SMOOTH_H_NXM(4, 16)
 SMOOTH_H_NXM(8, 4)
 SMOOTH_H_NXM(8, 8)
 SMOOTH_H_NXM(8, 16)
-SMOOTH_H_NXM(8, 32)
 
 #undef SMOOTH_H_NXM
 
@@ -2923,16 +2983,18 @@ SMOOTH_H_PREDICTOR(64)
     smooth_h_##W##xh_neon(dst, y_stride, above, left, H);     \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 SMOOTH_H_NXM_WIDE(16, 4)
+SMOOTH_H_NXM_WIDE(16, 64)
+SMOOTH_H_NXM_WIDE(32, 8)
+SMOOTH_H_NXM_WIDE(64, 16)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 SMOOTH_H_NXM_WIDE(16, 8)
 SMOOTH_H_NXM_WIDE(16, 16)
 SMOOTH_H_NXM_WIDE(16, 32)
-SMOOTH_H_NXM_WIDE(16, 64)
-SMOOTH_H_NXM_WIDE(32, 8)
 SMOOTH_H_NXM_WIDE(32, 16)
 SMOOTH_H_NXM_WIDE(32, 32)
 SMOOTH_H_NXM_WIDE(32, 64)
-SMOOTH_H_NXM_WIDE(64, 16)
 SMOOTH_H_NXM_WIDE(64, 32)
 SMOOTH_H_NXM_WIDE(64, 64)
 
@@ -3000,15 +3062,16 @@ static inline void paeth_4or8_x_h_neon(uint8_t *dest, ptrdiff_t stride,
     paeth_4or8_x_h_neon(dst, stride, above, left, W, H);                    \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
+PAETH_NXM(4, 16)
+PAETH_NXM(8, 32)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 PAETH_NXM(4, 4)
 PAETH_NXM(4, 8)
 PAETH_NXM(8, 4)
 PAETH_NXM(8, 8)
 PAETH_NXM(8, 16)
 
-PAETH_NXM(4, 16)
-PAETH_NXM(8, 32)
-
 // Calculate X distance <= TopLeft distance and pack the resulting mask into
 // uint8x8_t.
 static inline uint8x16_t x_le_top_left(const uint8x16_t x_dist,
@@ -3141,6 +3204,12 @@ static inline void paeth16_plus_x_h_neon(uint8_t *dest, ptrdiff_t stride,
     paeth16_plus_x_h_neon(dst, stride, above, left, W, H);                  \
   }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
+PAETH_NXM_WIDE(16, 4)
+PAETH_NXM_WIDE(16, 64)
+PAETH_NXM_WIDE(32, 8)
+PAETH_NXM_WIDE(64, 16)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 PAETH_NXM_WIDE(16, 8)
 PAETH_NXM_WIDE(16, 16)
 PAETH_NXM_WIDE(16, 32)
@@ -3149,8 +3218,3 @@ PAETH_NXM_WIDE(32, 32)
 PAETH_NXM_WIDE(32, 64)
 PAETH_NXM_WIDE(64, 32)
 PAETH_NXM_WIDE(64, 64)
-
-PAETH_NXM_WIDE(16, 4)
-PAETH_NXM_WIDE(16, 64)
-PAETH_NXM_WIDE(32, 8)
-PAETH_NXM_WIDE(64, 16)
diff --git a/aom_dsp/arm/mem_neon.h b/aom_dsp/arm/mem_neon.h
index c4a1ed27d..494dde14a 100644
--- a/aom_dsp/arm/mem_neon.h
+++ b/aom_dsp/arm/mem_neon.h
@@ -163,6 +163,23 @@ static inline void load_u8_8x7(const uint8_t *s, ptrdiff_t p,
   *s6 = vld1_u8(s);
 }
 
+static inline void load_u8_8x6(const uint8_t *s, ptrdiff_t p,
+                               uint8x8_t *const s0, uint8x8_t *const s1,
+                               uint8x8_t *const s2, uint8x8_t *const s3,
+                               uint8x8_t *const s4, uint8x8_t *const s5) {
+  *s0 = vld1_u8(s);
+  s += p;
+  *s1 = vld1_u8(s);
+  s += p;
+  *s2 = vld1_u8(s);
+  s += p;
+  *s3 = vld1_u8(s);
+  s += p;
+  *s4 = vld1_u8(s);
+  s += p;
+  *s5 = vld1_u8(s);
+}
+
 static inline void load_u8_8x4(const uint8_t *s, const ptrdiff_t p,
                                uint8x8_t *const s0, uint8x8_t *const s1,
                                uint8x8_t *const s2, uint8x8_t *const s3) {
@@ -1214,6 +1231,20 @@ static inline uint16x8_t load_unaligned_u16_4x2(const uint16_t *buf,
   return vreinterpretq_u16_u64(a_u64);
 }
 
+static inline int16x8_t load_unaligned_s16_4x2(const int16_t *buf,
+                                               uint32_t stride) {
+  int64_t a;
+  int64x2_t a_s64;
+  memcpy(&a, buf, 8);
+  buf += stride;
+  a_s64 = vdupq_n_s64(0);
+  a_s64 = vsetq_lane_s64(a, a_s64, 0);
+  memcpy(&a, buf, 8);
+  buf += stride;
+  a_s64 = vsetq_lane_s64(a, a_s64, 1);
+  return vreinterpretq_s16_s64(a_s64);
+}
+
 static inline void load_unaligned_u16_4x4(const uint16_t *buf, uint32_t stride,
                                           uint16x8_t *tu0, uint16x8_t *tu1) {
   *tu0 = load_unaligned_u16_4x2(buf, stride);
@@ -1353,6 +1384,17 @@ static inline void store_u8x2_strided_x2(uint8_t *dst, uint32_t dst_stride,
   store_u8_2x1_lane(dst, src, 1);
 }
 
+static inline void store_u8x2_strided_x4(uint8_t *dst, uint32_t dst_stride,
+                                         uint8x8_t src) {
+  store_u8_2x1_lane(dst, src, 0);
+  dst += dst_stride;
+  store_u8_2x1_lane(dst, src, 1);
+  dst += dst_stride;
+  store_u8_2x1_lane(dst, src, 2);
+  dst += dst_stride;
+  store_u8_2x1_lane(dst, src, 3);
+}
+
 // Store two blocks of 32-bits from a single vector.
 static inline void store_u8x4_strided_x2(uint8_t *dst, ptrdiff_t stride,
                                          uint8x8_t src) {
diff --git a/aom_dsp/arm/sum_squares_neon.c b/aom_dsp/arm/sum_squares_neon.c
index a02faf8f7..0c79fb6c2 100644
--- a/aom_dsp/arm/sum_squares_neon.c
+++ b/aom_dsp/arm/sum_squares_neon.c
@@ -14,6 +14,7 @@
 
 #include "aom_dsp/arm/mem_neon.h"
 #include "aom_dsp/arm/sum_neon.h"
+#include "config/aom_config.h"
 #include "config/aom_dsp_rtcd.h"
 
 static inline uint64_t aom_sum_squares_2d_i16_4x4_neon(const int16_t *src,
@@ -474,6 +475,7 @@ uint64_t aom_var_2d_u8_neon(uint8_t *src, int src_stride, int width,
   return aom_var_2d_u8_c(src, src_stride, width, height);
 }
 
+#if CONFIG_AV1_HIGHBITDEPTH
 static inline uint64_t aom_var_2d_u16_4xh_neon(uint8_t *src, int src_stride,
                                                int width, int height) {
   uint16_t *src_u16 = CONVERT_TO_SHORTPTR(src);
@@ -572,3 +574,4 @@ uint64_t aom_var_2d_u16_neon(uint8_t *src, int src_stride, int width,
   }
   return aom_var_2d_u16_c(src, src_stride, width, height);
 }
+#endif  // CONFIG_AV1_HIGHBITDEPTH
diff --git a/aom_dsp/arm/sum_squares_sve.c b/aom_dsp/arm/sum_squares_sve.c
index a9734f633..450fb029b 100644
--- a/aom_dsp/arm/sum_squares_sve.c
+++ b/aom_dsp/arm/sum_squares_sve.c
@@ -13,6 +13,7 @@
 
 #include "aom_dsp/arm/aom_neon_sve_bridge.h"
 #include "aom_dsp/arm/mem_neon.h"
+#include "config/aom_config.h"
 #include "config/aom_dsp_rtcd.h"
 
 static inline uint64_t aom_sum_squares_2d_i16_4xh_sve(const int16_t *src,
@@ -237,6 +238,7 @@ uint64_t aom_sum_sse_2d_i16_sve(const int16_t *src, int stride, int width,
   return sse;
 }
 
+#if CONFIG_AV1_HIGHBITDEPTH
 static inline uint64_t aom_var_2d_u16_4xh_sve(uint8_t *src, int src_stride,
                                               int width, int height) {
   uint16_t *src_u16 = CONVERT_TO_SHORTPTR(src);
@@ -400,3 +402,4 @@ uint64_t aom_var_2d_u16_sve(uint8_t *src, int src_stride, int width,
   }
   return aom_var_2d_u16_neon(src, src_stride, width, height);
 }
+#endif  // CONFIG_AV1_HIGHBITDEPTH
diff --git a/aom_dsp/arm/transpose_neon.h b/aom_dsp/arm/transpose_neon.h
index 91cda0f46..88df0d634 100644
--- a/aom_dsp/arm/transpose_neon.h
+++ b/aom_dsp/arm/transpose_neon.h
@@ -1330,4 +1330,36 @@ static inline void transpose_arrays_s16_8x4(const int16x8_t *const in,
   out[7] = vget_high_s16(vreinterpretq_s16_u32(c1.val[1]));
 }
 
+static inline void transpose_arrays_s64_4x4(const int64x2_t *in,
+                                            int64x2_t *out) {
+  // Perform a 4x4 matrix transpose going from:
+  // in[0] = 00 01
+  // in[1] = 02 03
+  // in[2] = 10 11
+  // in[3] = 12 13
+  // in[4] = 20 21
+  // in[5] = 22 23
+  // in[6] = 30 31
+  // in[7] = 32 33
+  //
+  // to:
+  // out[0] = 00 10
+  // out[1] = 20 30
+  // out[2] = 01 11
+  // out[3] = 21 31
+  // out[4] = 02 12
+  // out[5] = 22 32
+  // out[6] = 03 13
+  // out[7] = 23 33
+
+  out[0] = aom_vtrn1q_s64(in[0], in[2]);
+  out[1] = aom_vtrn1q_s64(in[4], in[6]);
+  out[2] = aom_vtrn2q_s64(in[0], in[2]);
+  out[3] = aom_vtrn2q_s64(in[4], in[6]);
+  out[4] = aom_vtrn1q_s64(in[1], in[3]);
+  out[5] = aom_vtrn1q_s64(in[5], in[7]);
+  out[6] = aom_vtrn2q_s64(in[1], in[3]);
+  out[7] = aom_vtrn2q_s64(in[5], in[7]);
+}
+
 #endif  // AOM_AOM_DSP_ARM_TRANSPOSE_NEON_H_
diff --git a/aom_dsp/arm/variance_neon.c b/aom_dsp/arm/variance_neon.c
index 74524add0..fb46857a7 100644
--- a/aom_dsp/arm/variance_neon.c
+++ b/aom_dsp/arm/variance_neon.c
@@ -445,6 +445,7 @@ uint64_t aom_mse_wxh_16bit_neon(uint8_t *dst, int dstride, uint16_t *src,
   return horizontal_add_u64x2(mse_wxh_16bit(dst, dstride, src, sstride, w, h));
 }
 
+#if !CONFIG_REALTIME_ONLY
 uint32_t aom_get_mb_ss_neon(const int16_t *a) {
   int32x4_t sse[2] = { vdupq_n_s32(0), vdupq_n_s32(0) };
 
@@ -457,6 +458,7 @@ uint32_t aom_get_mb_ss_neon(const int16_t *a) {
 
   return horizontal_add_s32x4(vaddq_s32(sse[0], sse[1]));
 }
+#endif  // !CONFIG_REALTIME_ONLY
 
 uint64_t aom_mse_16xh_16bit_neon(uint8_t *dst, int dstride, uint16_t *src,
                                  int w, int h) {
diff --git a/aom_dsp/bitreader_buffer.c b/aom_dsp/bitreader_buffer.c
index 1d8a7d31c..addd8cd94 100644
--- a/aom_dsp/bitreader_buffer.c
+++ b/aom_dsp/bitreader_buffer.c
@@ -42,6 +42,7 @@ int aom_rb_read_literal(struct aom_read_bit_buffer *rb, int bits) {
   return value;
 }
 
+#if CONFIG_AV1_DECODER
 uint32_t aom_rb_read_unsigned_literal(struct aom_read_bit_buffer *rb,
                                       int bits) {
   assert(bits <= 32);
@@ -57,6 +58,7 @@ int aom_rb_read_inv_signed_literal(struct aom_read_bit_buffer *rb, int bits) {
   const unsigned value = (unsigned)aom_rb_read_literal(rb, bits + 1) << nbits;
   return ((int)value) >> nbits;
 }
+#endif  // CONFIG_AV1_DECODER
 
 uint32_t aom_rb_read_uvlc(struct aom_read_bit_buffer *rb) {
   int leading_zeros = 0;
@@ -68,6 +70,7 @@ uint32_t aom_rb_read_uvlc(struct aom_read_bit_buffer *rb) {
   return base + value;
 }
 
+#if CONFIG_AV1_DECODER
 static uint16_t aom_rb_read_primitive_quniform(struct aom_read_bit_buffer *rb,
                                                uint16_t n) {
   if (n <= 1) return 0;
@@ -114,3 +117,4 @@ int16_t aom_rb_read_signed_primitive_refsubexpfin(
   const uint16_t scaled_n = (n << 1) - 1;
   return aom_rb_read_primitive_refsubexpfin(rb, scaled_n, k, ref) - n + 1;
 }
+#endif  // CONFIG_AV1_DECODER
diff --git a/aom_dsp/bitreader_buffer.h b/aom_dsp/bitreader_buffer.h
index f9f42b295..7666a4188 100644
--- a/aom_dsp/bitreader_buffer.h
+++ b/aom_dsp/bitreader_buffer.h
@@ -15,6 +15,7 @@
 #include <limits.h>
 
 #include "aom/aom_integer.h"
+#include "config/aom_config.h"
 
 #ifdef __cplusplus
 extern "C" {
@@ -37,14 +38,16 @@ int aom_rb_read_bit(struct aom_read_bit_buffer *rb);
 
 int aom_rb_read_literal(struct aom_read_bit_buffer *rb, int bits);
 
+uint32_t aom_rb_read_uvlc(struct aom_read_bit_buffer *rb);
+
+#if CONFIG_AV1_DECODER
 uint32_t aom_rb_read_unsigned_literal(struct aom_read_bit_buffer *rb, int bits);
 
 int aom_rb_read_inv_signed_literal(struct aom_read_bit_buffer *rb, int bits);
 
-uint32_t aom_rb_read_uvlc(struct aom_read_bit_buffer *rb);
-
 int16_t aom_rb_read_signed_primitive_refsubexpfin(
     struct aom_read_bit_buffer *rb, uint16_t n, uint16_t k, int16_t ref);
+#endif  // CONFIG_AV1_DECODER
 
 #ifdef __cplusplus
 }  // extern "C"
diff --git a/aom_dsp/bitwriter_buffer.c b/aom_dsp/bitwriter_buffer.c
index e6e03ccfe..4df2d15fb 100644
--- a/aom_dsp/bitwriter_buffer.c
+++ b/aom_dsp/bitwriter_buffer.c
@@ -41,7 +41,7 @@ void aom_wb_write_bit(struct aom_write_bit_buffer *wb, int bit) {
   wb->bit_offset = off + 1;
 }
 
-void aom_wb_overwrite_bit(struct aom_write_bit_buffer *wb, int bit) {
+static void overwrite_bit(struct aom_write_bit_buffer *wb, int bit) {
   // Do not zero bytes but overwrite exisiting values
   const int off = (int)wb->bit_offset;
   const int p = off / CHAR_BIT;
@@ -67,8 +67,7 @@ void aom_wb_write_unsigned_literal(struct aom_write_bit_buffer *wb,
 void aom_wb_overwrite_literal(struct aom_write_bit_buffer *wb, int data,
                               int bits) {
   int bit;
-  for (bit = bits - 1; bit >= 0; bit--)
-    aom_wb_overwrite_bit(wb, (data >> bit) & 1);
+  for (bit = bits - 1; bit >= 0; bit--) overwrite_bit(wb, (data >> bit) & 1);
 }
 
 void aom_wb_write_inv_signed_literal(struct aom_write_bit_buffer *wb, int data,
diff --git a/aom_dsp/bitwriter_buffer.h b/aom_dsp/bitwriter_buffer.h
index 1d61e8611..31d87c1c8 100644
--- a/aom_dsp/bitwriter_buffer.h
+++ b/aom_dsp/bitwriter_buffer.h
@@ -29,8 +29,6 @@ uint32_t aom_wb_bytes_written(const struct aom_write_bit_buffer *wb);
 
 void aom_wb_write_bit(struct aom_write_bit_buffer *wb, int bit);
 
-void aom_wb_overwrite_bit(struct aom_write_bit_buffer *wb, int bit);
-
 void aom_wb_write_literal(struct aom_write_bit_buffer *wb, int data, int bits);
 
 void aom_wb_write_unsigned_literal(struct aom_write_bit_buffer *wb,
diff --git a/aom_dsp/fft.c b/aom_dsp/fft.c
index 9787dc0d7..5996e7fb9 100644
--- a/aom_dsp/fft.c
+++ b/aom_dsp/fft.c
@@ -184,15 +184,15 @@ void aom_ifft_2d_gen(const float *input, float *temp, float *output, int n,
   transpose(temp, output, n);
 }
 
-GEN_IFFT_2(void, float, float, float, *, store_float)
-GEN_IFFT_4(void, float, float, float, *, store_float, (float), add_float,
+GEN_IFFT_2(static void, float, float, float, *, store_float)
+GEN_IFFT_4(static void, float, float, float, *, store_float, (float), add_float,
            sub_float)
-GEN_IFFT_8(void, float, float, float, *, store_float, (float), add_float,
+GEN_IFFT_8(static void, float, float, float, *, store_float, (float), add_float,
            sub_float, mul_float)
-GEN_IFFT_16(void, float, float, float, *, store_float, (float), add_float,
-            sub_float, mul_float)
-GEN_IFFT_32(void, float, float, float, *, store_float, (float), add_float,
-            sub_float, mul_float)
+GEN_IFFT_16(static void, float, float, float, *, store_float, (float),
+            add_float, sub_float, mul_float)
+GEN_IFFT_32(static void, float, float, float, *, store_float, (float),
+            add_float, sub_float, mul_float)
 
 void aom_ifft2x2_float_c(const float *input, float *temp, float *output) {
   aom_ifft_2d_gen(input, temp, output, 2, aom_fft1d_2_float, aom_fft1d_2_float,
diff --git a/aom_dsp/fft_common.h b/aom_dsp/fft_common.h
index 3f7e03cf8..43677308a 100644
--- a/aom_dsp/fft_common.h
+++ b/aom_dsp/fft_common.h
@@ -52,11 +52,6 @@ void aom_fft1d_4_float(const float *input, float *output, int stride);
 void aom_fft1d_8_float(const float *input, float *output, int stride);
 void aom_fft1d_16_float(const float *input, float *output, int stride);
 void aom_fft1d_32_float(const float *input, float *output, int stride);
-void aom_ifft1d_2_float(const float *input, float *output, int stride);
-void aom_ifft1d_4_float(const float *input, float *output, int stride);
-void aom_ifft1d_8_float(const float *input, float *output, int stride);
-void aom_ifft1d_16_float(const float *input, float *output, int stride);
-void aom_ifft1d_32_float(const float *input, float *output, int stride);
 
 /**\!brief Function pointer for transposing a matrix of floats.
  *
diff --git a/aom_dsp/intrapred.c b/aom_dsp/intrapred.c
index ad7a25cf0..6affefec3 100644
--- a/aom_dsp/intrapred.c
+++ b/aom_dsp/intrapred.c
@@ -297,6 +297,7 @@ void aom_dc_predictor_8x4_c(uint8_t *dst, ptrdiff_t stride,
   dc_predictor_rect(dst, stride, 8, 4, above, left, 2, DC_MULTIPLIER_1X2);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_predictor_4x16_c(uint8_t *dst, ptrdiff_t stride,
                              const uint8_t *above, const uint8_t *left) {
   dc_predictor_rect(dst, stride, 4, 16, above, left, 2, DC_MULTIPLIER_1X4);
@@ -306,6 +307,7 @@ void aom_dc_predictor_16x4_c(uint8_t *dst, ptrdiff_t stride,
                              const uint8_t *above, const uint8_t *left) {
   dc_predictor_rect(dst, stride, 16, 4, above, left, 2, DC_MULTIPLIER_1X4);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_predictor_8x16_c(uint8_t *dst, ptrdiff_t stride,
                              const uint8_t *above, const uint8_t *left) {
@@ -317,6 +319,7 @@ void aom_dc_predictor_16x8_c(uint8_t *dst, ptrdiff_t stride,
   dc_predictor_rect(dst, stride, 16, 8, above, left, 3, DC_MULTIPLIER_1X2);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_predictor_8x32_c(uint8_t *dst, ptrdiff_t stride,
                              const uint8_t *above, const uint8_t *left) {
   dc_predictor_rect(dst, stride, 8, 32, above, left, 3, DC_MULTIPLIER_1X4);
@@ -326,6 +329,7 @@ void aom_dc_predictor_32x8_c(uint8_t *dst, ptrdiff_t stride,
                              const uint8_t *above, const uint8_t *left) {
   dc_predictor_rect(dst, stride, 32, 8, above, left, 3, DC_MULTIPLIER_1X4);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_predictor_16x32_c(uint8_t *dst, ptrdiff_t stride,
                               const uint8_t *above, const uint8_t *left) {
@@ -337,6 +341,7 @@ void aom_dc_predictor_32x16_c(uint8_t *dst, ptrdiff_t stride,
   dc_predictor_rect(dst, stride, 32, 16, above, left, 4, DC_MULTIPLIER_1X2);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_predictor_16x64_c(uint8_t *dst, ptrdiff_t stride,
                               const uint8_t *above, const uint8_t *left) {
   dc_predictor_rect(dst, stride, 16, 64, above, left, 4, DC_MULTIPLIER_1X4);
@@ -346,6 +351,7 @@ void aom_dc_predictor_64x16_c(uint8_t *dst, ptrdiff_t stride,
                               const uint8_t *above, const uint8_t *left) {
   dc_predictor_rect(dst, stride, 64, 16, above, left, 4, DC_MULTIPLIER_1X4);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_predictor_32x64_c(uint8_t *dst, ptrdiff_t stride,
                               const uint8_t *above, const uint8_t *left) {
@@ -620,6 +626,7 @@ void aom_highbd_dc_predictor_8x4_c(uint16_t *dst, ptrdiff_t stride,
                            HIGHBD_DC_MULTIPLIER_1X2);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_highbd_dc_predictor_4x16_c(uint16_t *dst, ptrdiff_t stride,
                                     const uint16_t *above, const uint16_t *left,
                                     int bd) {
@@ -633,6 +640,7 @@ void aom_highbd_dc_predictor_16x4_c(uint16_t *dst, ptrdiff_t stride,
   highbd_dc_predictor_rect(dst, stride, 16, 4, above, left, bd, 2,
                            HIGHBD_DC_MULTIPLIER_1X4);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_highbd_dc_predictor_8x16_c(uint16_t *dst, ptrdiff_t stride,
                                     const uint16_t *above, const uint16_t *left,
@@ -648,6 +656,7 @@ void aom_highbd_dc_predictor_16x8_c(uint16_t *dst, ptrdiff_t stride,
                            HIGHBD_DC_MULTIPLIER_1X2);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_highbd_dc_predictor_8x32_c(uint16_t *dst, ptrdiff_t stride,
                                     const uint16_t *above, const uint16_t *left,
                                     int bd) {
@@ -661,6 +670,7 @@ void aom_highbd_dc_predictor_32x8_c(uint16_t *dst, ptrdiff_t stride,
   highbd_dc_predictor_rect(dst, stride, 32, 8, above, left, bd, 3,
                            HIGHBD_DC_MULTIPLIER_1X4);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_highbd_dc_predictor_16x32_c(uint16_t *dst, ptrdiff_t stride,
                                      const uint16_t *above,
@@ -676,6 +686,7 @@ void aom_highbd_dc_predictor_32x16_c(uint16_t *dst, ptrdiff_t stride,
                            HIGHBD_DC_MULTIPLIER_1X2);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_highbd_dc_predictor_16x64_c(uint16_t *dst, ptrdiff_t stride,
                                      const uint16_t *above,
                                      const uint16_t *left, int bd) {
@@ -689,6 +700,7 @@ void aom_highbd_dc_predictor_64x16_c(uint16_t *dst, ptrdiff_t stride,
   highbd_dc_predictor_rect(dst, stride, 64, 16, above, left, bd, 4,
                            HIGHBD_DC_MULTIPLIER_1X4);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_highbd_dc_predictor_32x64_c(uint16_t *dst, ptrdiff_t stride,
                                      const uint16_t *above,
@@ -730,6 +742,25 @@ void aom_highbd_dc_predictor_64x32_c(uint16_t *dst, ptrdiff_t stride,
 #endif  // CONFIG_AV1_HIGHBITDEPTH
 
 /* clang-format off */
+#if CONFIG_REALTIME_ONLY && !CONFIG_AV1_DECODER
+#define intra_pred_rectangular(type) \
+  intra_pred_sized(type, 4, 8) \
+  intra_pred_sized(type, 8, 4) \
+  intra_pred_sized(type, 8, 16) \
+  intra_pred_sized(type, 16, 8) \
+  intra_pred_sized(type, 16, 32) \
+  intra_pred_sized(type, 32, 16) \
+  intra_pred_sized(type, 32, 64) \
+  intra_pred_sized(type, 64, 32) \
+  intra_pred_highbd_sized(type, 4, 8) \
+  intra_pred_highbd_sized(type, 8, 4) \
+  intra_pred_highbd_sized(type, 8, 16) \
+  intra_pred_highbd_sized(type, 16, 8) \
+  intra_pred_highbd_sized(type, 16, 32) \
+  intra_pred_highbd_sized(type, 32, 16) \
+  intra_pred_highbd_sized(type, 32, 64) \
+  intra_pred_highbd_sized(type, 64, 32)
+#else
 #define intra_pred_rectangular(type) \
   intra_pred_sized(type, 4, 8) \
   intra_pred_sized(type, 8, 4) \
@@ -759,6 +790,7 @@ void aom_highbd_dc_predictor_64x32_c(uint16_t *dst, ptrdiff_t stride,
   intra_pred_highbd_sized(type, 32, 8) \
   intra_pred_highbd_sized(type, 16, 64) \
   intra_pred_highbd_sized(type, 64, 16)
+#endif // CONFIG_REALTIME_ONLY && !CONFIG_AV1_DECODER
 
 #define intra_pred_above_4x4(type) \
   intra_pred_sized(type, 8, 8) \
diff --git a/aom_dsp/ssim.c b/aom_dsp/ssim.c
index c6edbcb9c..3dc0ae725 100644
--- a/aom_dsp/ssim.c
+++ b/aom_dsp/ssim.c
@@ -17,24 +17,6 @@
 #include "aom_dsp/ssim.h"
 #include "aom_ports/mem.h"
 
-#if CONFIG_INTERNAL_STATS
-void aom_ssim_parms_16x16_c(const uint8_t *s, int sp, const uint8_t *r, int rp,
-                            uint32_t *sum_s, uint32_t *sum_r,
-                            uint32_t *sum_sq_s, uint32_t *sum_sq_r,
-                            uint32_t *sum_sxr) {
-  int i, j;
-  for (i = 0; i < 16; i++, s += sp, r += rp) {
-    for (j = 0; j < 16; j++) {
-      *sum_s += s[j];
-      *sum_r += r[j];
-      *sum_sq_s += s[j] * s[j];
-      *sum_sq_r += r[j] * r[j];
-      *sum_sxr += s[j] * r[j];
-    }
-  }
-}
-#endif  // CONFIG_INTERNAL_STATS
-
 void aom_ssim_parms_8x8_c(const uint8_t *s, int sp, const uint8_t *r, int rp,
                           uint32_t *sum_s, uint32_t *sum_r, uint32_t *sum_sq_s,
                           uint32_t *sum_sq_r, uint32_t *sum_sxr) {
diff --git a/aom_dsp/sum_squares.c b/aom_dsp/sum_squares.c
index f0afccc91..2cb67ee0d 100644
--- a/aom_dsp/sum_squares.c
+++ b/aom_dsp/sum_squares.c
@@ -11,6 +11,7 @@
 
 #include <assert.h>
 
+#include "config/aom_config.h"
 #include "config/aom_dsp_rtcd.h"
 
 uint64_t aom_sum_squares_2d_i16_c(const int16_t *src, int src_stride, int width,
@@ -55,6 +56,7 @@ uint64_t aom_var_2d_u8_c(uint8_t *src, int src_stride, int width, int height) {
   return (ss - s * s / (width * height));
 }
 
+#if CONFIG_AV1_HIGHBITDEPTH
 uint64_t aom_var_2d_u16_c(uint8_t *src, int src_stride, int width, int height) {
   uint16_t *srcp = CONVERT_TO_SHORTPTR(src);
   int r, c;
@@ -71,6 +73,7 @@ uint64_t aom_var_2d_u16_c(uint8_t *src, int src_stride, int width, int height) {
 
   return (ss - s * s / (width * height));
 }
+#endif  // CONFIG_AV1_HIGHBITDEPTH
 
 uint64_t aom_sum_sse_2d_i16_c(const int16_t *src, int src_stride, int width,
                               int height, int *sum) {
diff --git a/aom_dsp/variance.c b/aom_dsp/variance.c
index e1cc07af6..6481cf7f3 100644
--- a/aom_dsp/variance.c
+++ b/aom_dsp/variance.c
@@ -24,6 +24,7 @@
 #include "av1/common/filter.h"
 #include "av1/common/reconinter.h"
 
+#if !CONFIG_REALTIME_ONLY
 uint32_t aom_get_mb_ss_c(const int16_t *a) {
   unsigned int i, sum = 0;
 
@@ -33,6 +34,7 @@ uint32_t aom_get_mb_ss_c(const int16_t *a) {
 
   return sum;
 }
+#endif  // !CONFIG_REALTIME_ONLY
 
 static void variance(const uint8_t *a, int a_stride, const uint8_t *b,
                      int b_stride, int w, int h, uint32_t *sse, int *sum) {
diff --git a/aom_dsp/x86/highbd_sad_sse2.asm b/aom_dsp/x86/highbd_sad_sse2.asm
index 7aa44ea82..c61c134e5 100644
--- a/aom_dsp/x86/highbd_sad_sse2.asm
+++ b/aom_dsp/x86/highbd_sad_sse2.asm
@@ -180,13 +180,15 @@ cglobal highbd_sad_skip_%1x%2, 4, %3, %5, src, src_stride, ref, ref_stride, \
 INIT_XMM sse2
 HIGH_SAD64XN 64 ; highbd_sad64x64_sse2
 HIGH_SAD64XN 32 ; highbd_sad64x32_sse2
-HIGH_SAD64XN 16 ; highbd_sad_64x16_sse2
 HIGH_SAD64XN 64, 1 ; highbd_sad64x64_avg_sse2
 HIGH_SAD64XN 32, 1 ; highbd_sad64x32_avg_sse2
-HIGH_SAD64XN 16, 1 ; highbd_sad_64x16_avg_sse2
 HIGH_SAD64XN 64, 2 ; highbd_sad_skip_64x64_sse2
 HIGH_SAD64XN 32, 2 ; highbd_sad_skip_64x32_sse2
+%if CONFIG_REALTIME_ONLY==0
+HIGH_SAD64XN 16 ; highbd_sad_64x16_sse2
+HIGH_SAD64XN 16, 1 ; highbd_sad_64x16_avg_sse2
 HIGH_SAD64XN 16, 2 ; highbd_sad_skip_64x16_sse2
+%endif
 
 ; unsigned int aom_highbd_sad32x{16,32,64}_sse2(uint8_t *src, int src_stride,
 ;                                    uint8_t *ref, int ref_stride);
@@ -259,15 +261,17 @@ INIT_XMM sse2
 HIGH_SAD32XN 64 ; highbd_sad32x64_sse2
 HIGH_SAD32XN 32 ; highbd_sad32x32_sse2
 HIGH_SAD32XN 16 ; highbd_sad32x16_sse2
-HIGH_SAD32XN  8 ; highbd_sad_32x8_sse2
 HIGH_SAD32XN 64, 1 ; highbd_sad32x64_avg_sse2
 HIGH_SAD32XN 32, 1 ; highbd_sad32x32_avg_sse2
 HIGH_SAD32XN 16, 1 ; highbd_sad32x16_avg_sse2
-HIGH_SAD32XN  8, 1 ; highbd_sad_32x8_avg_sse2
 HIGH_SAD32XN 64, 2 ; highbd_sad_skip_32x64_sse2
 HIGH_SAD32XN 32, 2 ; highbd_sad_skip_32x32_sse2
 HIGH_SAD32XN 16, 2 ; highbd_sad_skip_32x16_sse2
+%if CONFIG_REALTIME_ONLY==0
+HIGH_SAD32XN  8 ; highbd_sad_32x8_sse2
+HIGH_SAD32XN  8, 1 ; highbd_sad_32x8_avg_sse2
 HIGH_SAD32XN  8, 2 ; highbd_sad_skip_32x8_sse2
+%endif
 
 ; unsigned int aom_highbd_sad16x{8,16,32}_sse2(uint8_t *src, int src_stride,
 ;                                    uint8_t *ref, int ref_stride);
@@ -337,22 +341,24 @@ HIGH_SAD32XN  8, 2 ; highbd_sad_skip_32x8_sse2
 %endmacro
 
 INIT_XMM sse2
-HIGH_SAD16XN 64 ; highbd_sad_16x64_sse2
 HIGH_SAD16XN 32 ; highbd_sad16x32_sse2
 HIGH_SAD16XN 16 ; highbd_sad16x16_sse2
 HIGH_SAD16XN  8 ; highbd_sad16x8_sse2
-HIGH_SAD16XN  4 ; highbd_sad_16x4_sse2
-HIGH_SAD16XN 64, 1 ; highbd_sad_16x64_avg_sse2
 HIGH_SAD16XN 32, 1 ; highbd_sad16x32_avg_sse2
 HIGH_SAD16XN 16, 1 ; highbd_sad16x16_avg_sse2
 HIGH_SAD16XN  8, 1 ; highbd_sad16x8_avg_sse2
-HIGH_SAD16XN  4, 1 ; highbd_sad_16x4_avg_sse2
-HIGH_SAD16XN 64, 2 ; highbd_sad_skip_16x64_sse2
 HIGH_SAD16XN 32, 2 ; highbd_sad_skip_16x32_sse2
 HIGH_SAD16XN 16, 2 ; highbd_sad_skip_16x16_sse2
 HIGH_SAD16XN  8, 2 ; highbd_sad_skip_16x8_sse2
+%if CONFIG_REALTIME_ONLY==0
+HIGH_SAD16XN 64 ; highbd_sad_16x64_sse2
+HIGH_SAD16XN  4 ; highbd_sad_16x4_sse2
+HIGH_SAD16XN 64, 1 ; highbd_sad_16x64_avg_sse2
+HIGH_SAD16XN  4, 1 ; highbd_sad_16x4_avg_sse2
+HIGH_SAD16XN 64, 2 ; highbd_sad_skip_16x64_sse2
 ; Current code fails there are only 2 rows
 ; HIGH_SAD16XN  4, 2 ; highbd_sad_skip_16x4_sse2
+%endif
 
 ; unsigned int aom_highbd_sad8x{4,8,16}_sse2(uint8_t *src, int src_stride,
 ;                                    uint8_t *ref, int ref_stride);
@@ -430,19 +436,21 @@ HIGH_SAD16XN  8, 2 ; highbd_sad_skip_16x8_sse2
 %endmacro
 
 INIT_XMM sse2
-HIGH_SAD8XN 32 ; highbd_sad_8x32_sse2
 HIGH_SAD8XN 16 ; highbd_sad8x16_sse2
 HIGH_SAD8XN  8 ; highbd_sad8x8_sse2
 HIGH_SAD8XN  4 ; highbd_sad8x4_sse2
-HIGH_SAD8XN 32, 1 ; highbd_sad_8x32_avg_sse2
 HIGH_SAD8XN 16, 1 ; highbd_sad8x16_avg_sse2
 HIGH_SAD8XN  8, 1 ; highbd_sad8x8_avg_sse2
 HIGH_SAD8XN  4, 1 ; highbd_sad8x4_avg_sse2
-HIGH_SAD8XN 32, 2 ; highbd_sad_skip_8x32_sse2
 HIGH_SAD8XN 16, 2 ; highbd_sad_skip_8x16_sse2
 HIGH_SAD8XN  8, 2 ; highbd_sad_skip_8x8_sse2
 ; Current code fails there are only 2 rows
 ; HIGH_SAD8XN  4, 2 ; highbd_sad8x4_avg_sse2
+%if CONFIG_REALTIME_ONLY==0
+HIGH_SAD8XN 32 ; highbd_sad_8x32_sse2
+HIGH_SAD8XN 32, 1 ; highbd_sad_8x32_avg_sse2
+HIGH_SAD8XN 32, 2 ; highbd_sad_skip_8x32_sse2
+%endif
 
 ; unsigned int aom_highbd_sad4x{4,8,16}_sse2(uint8_t *src, int src_stride,
 ;                                    uint8_t *ref, int ref_stride);
@@ -512,13 +520,15 @@ HIGH_SAD8XN  8, 2 ; highbd_sad_skip_8x8_sse2
 %endmacro
 
 INIT_XMM sse2
-HIGH_SAD4XN 16 ; highbd_sad4x16_sse2
 HIGH_SAD4XN  8 ; highbd_sad4x8_sse2
 HIGH_SAD4XN  4 ; highbd_sad4x4_sse2
-HIGH_SAD4XN 16, 1 ; highbd_sad4x16_avg_sse2
 HIGH_SAD4XN  8, 1 ; highbd_sad4x8_avg_sse2
 HIGH_SAD4XN  4, 1 ; highbd_sad4x4_avg_sse2
-HIGH_SAD4XN 16, 2 ; highbd_sad_skip_4x16_sse2
 HIGH_SAD4XN  8, 2 ; highbd_sad_skip_4x8_sse2
 ; Current code fails there are only 2 rows
 ; HIGH_SAD4XN  4, 2 ; highbd_sad_skip_4x4_sse2
+%if CONFIG_REALTIME_ONLY==0
+HIGH_SAD4XN 16 ; highbd_sad4x16_sse2
+HIGH_SAD4XN 16, 1 ; highbd_sad4x16_avg_sse2
+HIGH_SAD4XN 16, 2 ; highbd_sad_skip_4x16_sse2
+%endif
diff --git a/aom_dsp/x86/intrapred_avx2.c b/aom_dsp/x86/intrapred_avx2.c
index 7a41618d1..f6c54224b 100644
--- a/aom_dsp/x86/intrapred_avx2.c
+++ b/aom_dsp/x86/intrapred_avx2.c
@@ -135,6 +135,7 @@ static DECLARE_ALIGNED(32, uint16_t, HighbdBaseMask[17][16]) = {
     0xffff, 0xffff, 0xffff, 0xffff, 0xffff, 0xffff, 0xffff }
 };
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static inline void highbd_transpose16x4_8x8_sse2(__m128i *x, __m128i *d) {
   __m128i r0, r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, r11, r12, r13, r14, r15;
 
@@ -199,6 +200,7 @@ static inline void highbd_transpose4x16_avx2(__m256i *x, __m256i *d) {
   d[2] = _mm256_unpacklo_epi64(ww0, ww1);  // 02 12 22 32 42 52 62 72
   d[3] = _mm256_unpackhi_epi64(ww0, ww1);  // 03 13 23 33 43 53 63 73
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 static inline void highbd_transpose8x16_16x8_avx2(__m256i *x, __m256i *d) {
   __m256i w0, w1, w2, w3, ww0, ww1;
@@ -471,6 +473,7 @@ void aom_dc_predictor_64x32_avx2(uint8_t *dst, ptrdiff_t stride,
   row_store_64xh(&row, 32, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_predictor_64x16_avx2(uint8_t *dst, ptrdiff_t stride,
                                  const uint8_t *above, const uint8_t *left) {
   const __m256i sum_above = dc_sum_64(above);
@@ -482,6 +485,7 @@ void aom_dc_predictor_64x16_avx2(uint8_t *dst, ptrdiff_t stride,
   const __m256i row = _mm256_set1_epi8((int8_t)sum);
   row_store_64xh(&row, 16, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_top_predictor_32x16_avx2(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
@@ -539,6 +543,7 @@ void aom_dc_top_predictor_64x32_avx2(uint8_t *dst, ptrdiff_t stride,
   row_store_64xh(&row, 32, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_top_predictor_64x16_avx2(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
                                      const uint8_t *left) {
@@ -552,6 +557,7 @@ void aom_dc_top_predictor_64x16_avx2(uint8_t *dst, ptrdiff_t stride,
   __m256i row = _mm256_shuffle_epi8(sum, zero);
   row_store_64xh(&row, 16, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_left_predictor_32x16_avx2(uint8_t *dst, ptrdiff_t stride,
                                       const uint8_t *above,
@@ -610,6 +616,7 @@ void aom_dc_left_predictor_64x32_avx2(uint8_t *dst, ptrdiff_t stride,
   row_store_64xh(&row, 32, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_left_predictor_64x16_avx2(uint8_t *dst, ptrdiff_t stride,
                                       const uint8_t *above,
                                       const uint8_t *left) {
@@ -624,6 +631,7 @@ void aom_dc_left_predictor_64x16_avx2(uint8_t *dst, ptrdiff_t stride,
   const __m256i row = _mm256_inserti128_si256(_mm256_castsi128_si256(r), r, 1);
   row_store_64xh(&row, 16, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_128_predictor_32x16_avx2(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
@@ -661,6 +669,7 @@ void aom_dc_128_predictor_64x32_avx2(uint8_t *dst, ptrdiff_t stride,
   row_store_64xh(&row, 32, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_128_predictor_64x16_avx2(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
                                      const uint8_t *left) {
@@ -669,6 +678,7 @@ void aom_dc_128_predictor_64x16_avx2(uint8_t *dst, ptrdiff_t stride,
   const __m256i row = _mm256_set1_epi8((int8_t)0x80);
   row_store_64xh(&row, 16, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_v_predictor_32x16_avx2(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
@@ -700,6 +710,7 @@ void aom_v_predictor_64x32_avx2(uint8_t *dst, ptrdiff_t stride,
   row_store_32x2xh(&row0, &row1, 32, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_v_predictor_64x16_avx2(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
   const __m256i row0 = _mm256_loadu_si256((const __m256i *)above);
@@ -707,6 +718,7 @@ void aom_v_predictor_64x16_avx2(uint8_t *dst, ptrdiff_t stride,
   (void)left;
   row_store_32x2xh(&row0, &row1, 16, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // -----------------------------------------------------------------------------
 // PAETH_PRED
@@ -826,6 +838,7 @@ void aom_paeth_predictor_16x32_avx2(uint8_t *dst, ptrdiff_t stride,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_paeth_predictor_16x64_avx2(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *above, const uint8_t *left) {
   const __m256i tl16 = _mm256_set1_epi16((int16_t)above[-1]);
@@ -845,6 +858,7 @@ void aom_paeth_predictor_16x64_avx2(uint8_t *dst, ptrdiff_t stride,
     }
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // Return 32 8-bit pixels in one row (__m256i)
 static inline __m256i paeth_32x1_pred(const __m256i *left, const __m256i *top0,
@@ -1012,6 +1026,7 @@ void aom_paeth_predictor_64x64_avx2(uint8_t *dst, ptrdiff_t stride,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_paeth_predictor_64x16_avx2(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *above, const uint8_t *left) {
   const __m256i t0 = get_top_vector(above);
@@ -1041,6 +1056,7 @@ void aom_paeth_predictor_64x16_avx2(uint8_t *dst, ptrdiff_t stride,
     rep = _mm256_add_epi16(rep, one);
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #if CONFIG_AV1_HIGHBITDEPTH
 
@@ -3027,6 +3043,7 @@ static void highbd_dr_prediction_z3_16x8_avx2(uint16_t *dst, ptrdiff_t stride,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static void highbd_dr_prediction_z3_4x16_avx2(uint16_t *dst, ptrdiff_t stride,
                                               const uint16_t *left,
                                               int upsample_left, int dy,
@@ -3139,6 +3156,7 @@ static void highbd_dr_prediction_z3_32x8_avx2(uint16_t *dst, ptrdiff_t stride,
     _mm_storeu_si128((__m128i *)(dst + i * stride + 24), d[i + 24]);
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 static void highbd_dr_prediction_z3_16x16_avx2(uint16_t *dst, ptrdiff_t stride,
                                                const uint16_t *left,
@@ -3282,6 +3300,7 @@ static void highbd_dr_prediction_z3_64x32_avx2(uint16_t *dst, ptrdiff_t stride,
   return;
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static void highbd_dr_prediction_z3_16x64_avx2(uint16_t *dst, ptrdiff_t stride,
                                                const uint16_t *left,
                                                int upsample_left, int dy,
@@ -3315,6 +3334,7 @@ static void highbd_dr_prediction_z3_64x16_avx2(uint16_t *dst, ptrdiff_t stride,
     }
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void av1_highbd_dr_prediction_z3_avx2(uint16_t *dst, ptrdiff_t stride, int bw,
                                       int bh, const uint16_t *above,
@@ -3371,6 +3391,7 @@ void av1_highbd_dr_prediction_z3_avx2(uint16_t *dst, ptrdiff_t stride, int bw,
         }
       } else {
         switch (bw) {
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
           case 4:
             highbd_dr_prediction_z3_4x16_avx2(dst, stride, left, upsample_left,
                                               dy, bd);
@@ -3383,6 +3404,7 @@ void av1_highbd_dr_prediction_z3_avx2(uint16_t *dst, ptrdiff_t stride, int bw,
             highbd_dr_prediction_z3_16x64_avx2(dst, stride, left, upsample_left,
                                                dy, bd);
             break;
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
         }
       }
     } else {
@@ -3407,6 +3429,7 @@ void av1_highbd_dr_prediction_z3_avx2(uint16_t *dst, ptrdiff_t stride, int bw,
         }
       } else {
         switch (bh) {
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
           case 4:
             highbd_dr_prediction_z3_16x4_avx2(dst, stride, left, upsample_left,
                                               dy, bd);
@@ -3419,6 +3442,7 @@ void av1_highbd_dr_prediction_z3_avx2(uint16_t *dst, ptrdiff_t stride, int bw,
             highbd_dr_prediction_z3_64x16_avx2(dst, stride, left, upsample_left,
                                                dy, bd);
             break;
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
         }
       }
     }
@@ -4428,6 +4452,7 @@ static void dr_prediction_z3_16x8_avx2(uint8_t *dst, ptrdiff_t stride,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static void dr_prediction_z3_4x16_avx2(uint8_t *dst, ptrdiff_t stride,
                                        const uint8_t *left, int upsample_left,
                                        int dy) {
@@ -4506,6 +4531,7 @@ static void dr_prediction_z3_32x8_avx2(uint8_t *dst, ptrdiff_t stride,
     _mm_storeu_si128((__m128i *)(dst + i * stride + 16), d[i + 8]);
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 static void dr_prediction_z3_16x16_avx2(uint8_t *dst, ptrdiff_t stride,
                                         const uint8_t *left, int upsample_left,
@@ -4597,6 +4623,7 @@ static void dr_prediction_z3_64x32_avx2(uint8_t *dst, ptrdiff_t stride,
   return;
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static void dr_prediction_z3_16x64_avx2(uint8_t *dst, ptrdiff_t stride,
                                         const uint8_t *left, int upsample_left,
                                         int dy) {
@@ -4618,6 +4645,7 @@ static void dr_prediction_z3_64x16_avx2(uint8_t *dst, ptrdiff_t stride,
     }
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void av1_dr_prediction_z3_avx2(uint8_t *dst, ptrdiff_t stride, int bw, int bh,
                                const uint8_t *above, const uint8_t *left,
@@ -4664,6 +4692,7 @@ void av1_dr_prediction_z3_avx2(uint8_t *dst, ptrdiff_t stride, int bw, int bh,
         }
       } else {
         switch (bw) {
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
           case 4:
             dr_prediction_z3_4x16_avx2(dst, stride, left, upsample_left, dy);
             break;
@@ -4673,6 +4702,7 @@ void av1_dr_prediction_z3_avx2(uint8_t *dst, ptrdiff_t stride, int bw, int bh,
           case 16:
             dr_prediction_z3_16x64_avx2(dst, stride, left, upsample_left, dy);
             break;
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
         }
       }
     } else {
@@ -4693,6 +4723,7 @@ void av1_dr_prediction_z3_avx2(uint8_t *dst, ptrdiff_t stride, int bw, int bh,
         }
       } else {
         switch (bh) {
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
           case 4:
             dr_prediction_z3_16x4_avx2(dst, stride, left, upsample_left, dy);
             break;
@@ -4702,6 +4733,7 @@ void av1_dr_prediction_z3_avx2(uint8_t *dst, ptrdiff_t stride, int bw, int bh,
           case 16:
             dr_prediction_z3_64x16_avx2(dst, stride, left, upsample_left, dy);
             break;
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
         }
       }
     }
diff --git a/aom_dsp/x86/intrapred_sse2.c b/aom_dsp/x86/intrapred_sse2.c
index 16ef4717a..595f45ac7 100644
--- a/aom_dsp/x86/intrapred_sse2.c
+++ b/aom_dsp/x86/intrapred_sse2.c
@@ -121,6 +121,7 @@ void aom_dc_predictor_4x8_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_4xh(pred, 8, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_predictor_4x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
   const __m128i sum_left = dc_sum_16_sse2(left);
@@ -135,6 +136,7 @@ void aom_dc_predictor_4x16_sse2(uint8_t *dst, ptrdiff_t stride,
   const uint32_t pred = (uint32_t)_mm_cvtsi128_si32(row);
   dc_store_4xh(pred, 16, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_predictor_8x4_sse2(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
@@ -163,6 +165,7 @@ void aom_dc_predictor_8x16_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_8xh(&row, 16, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_predictor_8x32_sse2(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
   const __m128i sum_left = dc_sum_32_sse2(left);
@@ -188,6 +191,7 @@ void aom_dc_predictor_16x4_sse2(uint8_t *dst, ptrdiff_t stride,
   const __m128i row = _mm_set1_epi8((int8_t)sum);
   dc_store_16xh(&row, 4, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_predictor_16x8_sse2(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
@@ -215,6 +219,7 @@ void aom_dc_predictor_16x32_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_16xh(&row, 32, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_predictor_16x64_sse2(uint8_t *dst, ptrdiff_t stride,
                                  const uint8_t *above, const uint8_t *left) {
   const __m128i sum_left = dc_sum_64(left);
@@ -240,6 +245,7 @@ void aom_dc_predictor_32x8_sse2(uint8_t *dst, ptrdiff_t stride,
   const __m128i row = _mm_set1_epi8((int8_t)sum);
   dc_store_32xh(&row, 8, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_predictor_32x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                  const uint8_t *above, const uint8_t *left) {
@@ -293,6 +299,7 @@ void aom_dc_predictor_64x32_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_64xh(&row, 32, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_predictor_64x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                  const uint8_t *above, const uint8_t *left) {
   __m128i sum_above = dc_sum_64(above);
@@ -305,6 +312,7 @@ void aom_dc_predictor_64x16_sse2(uint8_t *dst, ptrdiff_t stride,
   const __m128i row = _mm_set1_epi8((int8_t)sum);
   dc_store_64xh(&row, 16, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // -----------------------------------------------------------------------------
 // DC_TOP
@@ -323,6 +331,7 @@ void aom_dc_top_predictor_4x8_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_4xh(pred, 8, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_top_predictor_4x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *above, const uint8_t *left) {
   (void)left;
@@ -336,6 +345,7 @@ void aom_dc_top_predictor_4x16_sse2(uint8_t *dst, ptrdiff_t stride,
   const uint32_t pred = (uint32_t)_mm_cvtsi128_si32(sum_above);
   dc_store_4xh(pred, 16, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_top_predictor_8x4_sse2(uint8_t *dst, ptrdiff_t stride,
                                    const uint8_t *above, const uint8_t *left) {
@@ -361,6 +371,7 @@ void aom_dc_top_predictor_8x16_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_8xh(&row, 16, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_top_predictor_8x32_sse2(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *above, const uint8_t *left) {
   (void)left;
@@ -385,6 +396,7 @@ void aom_dc_top_predictor_16x4_sse2(uint8_t *dst, ptrdiff_t stride,
   const __m128i row = _mm_unpacklo_epi64(sum_above, sum_above);
   dc_store_16xh(&row, 4, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_top_predictor_16x8_sse2(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *above, const uint8_t *left) {
@@ -413,6 +425,7 @@ void aom_dc_top_predictor_16x32_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_16xh(&row, 32, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_top_predictor_16x64_sse2(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
                                      const uint8_t *left) {
@@ -439,6 +452,7 @@ void aom_dc_top_predictor_32x8_sse2(uint8_t *dst, ptrdiff_t stride,
   const __m128i row = _mm_unpacklo_epi64(sum_above, sum_above);
   dc_store_32xh(&row, 8, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_top_predictor_32x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
@@ -496,6 +510,7 @@ void aom_dc_top_predictor_64x32_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_64xh(&row, 32, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_top_predictor_64x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
                                      const uint8_t *left) {
@@ -509,6 +524,7 @@ void aom_dc_top_predictor_64x16_sse2(uint8_t *dst, ptrdiff_t stride,
   const __m128i row = _mm_unpacklo_epi64(sum_above, sum_above);
   dc_store_64xh(&row, 16, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // -----------------------------------------------------------------------------
 // DC_LEFT
@@ -527,6 +543,7 @@ void aom_dc_left_predictor_4x8_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_4xh(pred, 8, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_left_predictor_4x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
                                      const uint8_t *left) {
@@ -541,6 +558,7 @@ void aom_dc_left_predictor_4x16_sse2(uint8_t *dst, ptrdiff_t stride,
   const uint32_t pred = (uint32_t)_mm_cvtsi128_si32(sum_left);
   dc_store_4xh(pred, 16, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_left_predictor_8x4_sse2(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *above, const uint8_t *left) {
@@ -567,6 +585,7 @@ void aom_dc_left_predictor_8x16_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_8xh(&row, 16, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_left_predictor_8x32_sse2(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
                                      const uint8_t *left) {
@@ -593,6 +612,7 @@ void aom_dc_left_predictor_16x4_sse2(uint8_t *dst, ptrdiff_t stride,
   const __m128i row = _mm_unpacklo_epi64(sum_left, sum_left);
   dc_store_16xh(&row, 4, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_left_predictor_16x8_sse2(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
@@ -622,6 +642,7 @@ void aom_dc_left_predictor_16x32_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_16xh(&row, 32, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_left_predictor_16x64_sse2(uint8_t *dst, ptrdiff_t stride,
                                       const uint8_t *above,
                                       const uint8_t *left) {
@@ -649,6 +670,7 @@ void aom_dc_left_predictor_32x8_sse2(uint8_t *dst, ptrdiff_t stride,
   const __m128i row = _mm_unpacklo_epi64(sum_left, sum_left);
   dc_store_32xh(&row, 8, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_left_predictor_32x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                       const uint8_t *above,
@@ -706,6 +728,7 @@ void aom_dc_left_predictor_64x32_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_64xh(&row, 32, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_left_predictor_64x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                       const uint8_t *above,
                                       const uint8_t *left) {
@@ -719,6 +742,7 @@ void aom_dc_left_predictor_64x16_sse2(uint8_t *dst, ptrdiff_t stride,
   const __m128i row = _mm_unpacklo_epi64(sum_left, sum_left);
   dc_store_64xh(&row, 16, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // -----------------------------------------------------------------------------
 // DC_128
@@ -731,6 +755,7 @@ void aom_dc_128_predictor_4x8_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_4xh(pred, 8, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_128_predictor_4x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *above, const uint8_t *left) {
   (void)above;
@@ -738,6 +763,7 @@ void aom_dc_128_predictor_4x16_sse2(uint8_t *dst, ptrdiff_t stride,
   const uint32_t pred = 0x80808080;
   dc_store_4xh(pred, 16, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_128_predictor_8x4_sse2(uint8_t *dst, ptrdiff_t stride,
                                    const uint8_t *above, const uint8_t *left) {
@@ -755,6 +781,7 @@ void aom_dc_128_predictor_8x16_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_8xh(&row, 16, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_128_predictor_8x32_sse2(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *above, const uint8_t *left) {
   (void)above;
@@ -770,6 +797,7 @@ void aom_dc_128_predictor_16x4_sse2(uint8_t *dst, ptrdiff_t stride,
   const __m128i row = _mm_set1_epi8((int8_t)128);
   dc_store_16xh(&row, 4, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_128_predictor_16x8_sse2(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *above, const uint8_t *left) {
@@ -788,6 +816,7 @@ void aom_dc_128_predictor_16x32_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_16xh(&row, 32, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_128_predictor_16x64_sse2(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
                                      const uint8_t *left) {
@@ -804,6 +833,7 @@ void aom_dc_128_predictor_32x8_sse2(uint8_t *dst, ptrdiff_t stride,
   const __m128i row = _mm_set1_epi8((int8_t)128);
   dc_store_32xh(&row, 8, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_dc_128_predictor_32x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
@@ -841,6 +871,7 @@ void aom_dc_128_predictor_64x32_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_64xh(&row, 32, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_dc_128_predictor_64x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
                                      const uint8_t *left) {
@@ -849,6 +880,7 @@ void aom_dc_128_predictor_64x16_sse2(uint8_t *dst, ptrdiff_t stride,
   const __m128i row = _mm_set1_epi8((int8_t)128);
   dc_store_64xh(&row, 16, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // -----------------------------------------------------------------------------
 // V_PRED
@@ -860,12 +892,14 @@ void aom_v_predictor_4x8_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_4xh(pred, 8, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_v_predictor_4x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
   const uint32_t pred = *(uint32_t *)above;
   (void)left;
   dc_store_4xh(pred, 16, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_v_predictor_8x4_sse2(uint8_t *dst, ptrdiff_t stride,
                               const uint8_t *above, const uint8_t *left) {
@@ -881,6 +915,7 @@ void aom_v_predictor_8x16_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_8xh(&row, 16, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_v_predictor_8x32_sse2(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
   const __m128i row = _mm_loadl_epi64((__m128i const *)above);
@@ -894,6 +929,7 @@ void aom_v_predictor_16x4_sse2(uint8_t *dst, ptrdiff_t stride,
   (void)left;
   dc_store_16xh(&row, 4, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_v_predictor_16x8_sse2(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
@@ -909,12 +945,14 @@ void aom_v_predictor_16x32_sse2(uint8_t *dst, ptrdiff_t stride,
   dc_store_16xh(&row, 32, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_v_predictor_16x64_sse2(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
   const __m128i row = _mm_load_si128((__m128i const *)above);
   (void)left;
   dc_store_16xh(&row, 64, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 static inline void v_predictor_32xh(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *above, int height) {
@@ -927,11 +965,13 @@ static inline void v_predictor_32xh(uint8_t *dst, ptrdiff_t stride,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_v_predictor_32x8_sse2(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
   (void)left;
   v_predictor_32xh(dst, stride, above, 8);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_v_predictor_32x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
@@ -972,11 +1012,13 @@ void aom_v_predictor_64x32_sse2(uint8_t *dst, ptrdiff_t stride,
   v_predictor_64xh(dst, stride, above, 32);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_v_predictor_64x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
   (void)left;
   v_predictor_64xh(dst, stride, above, 16);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // -----------------------------------------------------------------------------
 // H_PRED
@@ -1012,6 +1054,7 @@ void aom_h_predictor_4x8_sse2(uint8_t *dst, ptrdiff_t stride,
   *(int *)dst = _mm_cvtsi128_si32(row3);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_h_predictor_4x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
   (void)above;
@@ -1072,6 +1115,7 @@ void aom_h_predictor_4x16_sse2(uint8_t *dst, ptrdiff_t stride,
   dst += stride;
   *(int *)dst = _mm_cvtsi128_si32(row3);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_h_predictor_8x4_sse2(uint8_t *dst, ptrdiff_t stride,
                               const uint8_t *above, const uint8_t *left) {
@@ -1162,10 +1206,12 @@ void aom_h_predictor_8x16_sse2(uint8_t *dst, ptrdiff_t stride,
   h_predictor_8x16xc(dst, stride, above, left, 1);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_h_predictor_8x32_sse2(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
   h_predictor_8x16xc(dst, stride, above, left, 2);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 static inline void h_pred_store_16xh(const __m128i *row, int h, uint8_t *dst,
                                      ptrdiff_t stride) {
@@ -1218,6 +1264,7 @@ static inline void h_prediction_16x8_2(const __m128i *left, uint8_t *dst,
   h_pred_store_16xh(row, 4, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_h_predictor_16x4_sse2(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
   (void)above;
@@ -1225,6 +1272,7 @@ void aom_h_predictor_16x4_sse2(uint8_t *dst, ptrdiff_t stride,
   const __m128i left_col_8p = _mm_unpacklo_epi8(left_col, left_col);
   h_prediction_16x8_1(&left_col_8p, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_h_predictor_16x8_sse2(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
@@ -1264,11 +1312,13 @@ void aom_h_predictor_16x32_sse2(uint8_t *dst, ptrdiff_t stride,
   h_predictor_16xh(dst, stride, left, 2);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_h_predictor_16x64_sse2(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
   (void)above;
   h_predictor_16xh(dst, stride, left, 4);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 static inline void h_pred_store_32xh(const __m128i *row, int h, uint8_t *dst,
                                      ptrdiff_t stride) {
@@ -1298,6 +1348,7 @@ static inline void h_prediction_32x8_2(const __m128i *left, uint8_t *dst,
   h_pred_store_32xh(row, 4, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_h_predictor_32x8_sse2(uint8_t *dst, ptrdiff_t stride,
                                const uint8_t *above, const uint8_t *left) {
   __m128i left_col, left_col_8p;
@@ -1310,6 +1361,7 @@ void aom_h_predictor_32x8_sse2(uint8_t *dst, ptrdiff_t stride,
   dst += stride << 2;
   h_prediction_32x8_2(&left_col_8p, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_h_predictor_32x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
@@ -1404,8 +1456,10 @@ void aom_h_predictor_64x32_sse2(uint8_t *dst, ptrdiff_t stride,
   h_predictor_64xh(dst, stride, left, 32);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_h_predictor_64x16_sse2(uint8_t *dst, ptrdiff_t stride,
                                 const uint8_t *above, const uint8_t *left) {
   (void)above;
   h_predictor_64xh(dst, stride, left, 16);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
diff --git a/aom_dsp/x86/intrapred_sse4.c b/aom_dsp/x86/intrapred_sse4.c
index ebe772447..34408d080 100644
--- a/aom_dsp/x86/intrapred_sse4.c
+++ b/aom_dsp/x86/intrapred_sse4.c
@@ -1021,6 +1021,7 @@ static void dr_prediction_z3_16x8_sse4_1(uint8_t *dst, ptrdiff_t stride,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static void dr_prediction_z3_4x16_sse4_1(uint8_t *dst, ptrdiff_t stride,
                                          const uint8_t *left, int upsample_left,
                                          int dy) {
@@ -1100,6 +1101,7 @@ static void dr_prediction_z3_32x8_sse4_1(uint8_t *dst, ptrdiff_t stride,
     _mm_storeu_si128((__m128i *)(dst + i * stride + 16), d[i + 8]);
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 static void dr_prediction_z3_16x16_sse4_1(uint8_t *dst, ptrdiff_t stride,
                                           const uint8_t *left,
@@ -1190,6 +1192,7 @@ static void dr_prediction_z3_64x32_sse4_1(uint8_t *dst, ptrdiff_t stride,
   return;
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static void dr_prediction_z3_16x64_sse4_1(uint8_t *dst, ptrdiff_t stride,
                                           const uint8_t *left,
                                           int upsample_left, int dy) {
@@ -1211,6 +1214,7 @@ static void dr_prediction_z3_64x16_sse4_1(uint8_t *dst, ptrdiff_t stride,
     }
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void av1_dr_prediction_z3_sse4_1(uint8_t *dst, ptrdiff_t stride, int bw, int bh,
                                  const uint8_t *above, const uint8_t *left,
@@ -1259,6 +1263,7 @@ void av1_dr_prediction_z3_sse4_1(uint8_t *dst, ptrdiff_t stride, int bw, int bh,
         }
       } else {
         switch (bw) {
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
           case 4:
             dr_prediction_z3_4x16_sse4_1(dst, stride, left, upsample_left, dy);
             break;
@@ -1269,6 +1274,7 @@ void av1_dr_prediction_z3_sse4_1(uint8_t *dst, ptrdiff_t stride, int bw, int bh,
             dr_prediction_z3_16x64_sse4_1(dst, stride, left, upsample_left, dy);
             break;
           default: assert(0 && "Invalid block size");
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
         }
       }
     } else {
@@ -1290,6 +1296,7 @@ void av1_dr_prediction_z3_sse4_1(uint8_t *dst, ptrdiff_t stride, int bw, int bh,
         }
       } else {
         switch (bh) {
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
           case 4:
             dr_prediction_z3_16x4_sse4_1(dst, stride, left, upsample_left, dy);
             break;
@@ -1300,6 +1307,7 @@ void av1_dr_prediction_z3_sse4_1(uint8_t *dst, ptrdiff_t stride, int bw, int bh,
             dr_prediction_z3_64x16_sse4_1(dst, stride, left, upsample_left, dy);
             break;
           default: assert(0 && "Invalid block size");
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
         }
       }
     }
diff --git a/aom_dsp/x86/intrapred_ssse3.c b/aom_dsp/x86/intrapred_ssse3.c
index 20a149ab2..302da29f0 100644
--- a/aom_dsp/x86/intrapred_ssse3.c
+++ b/aom_dsp/x86/intrapred_ssse3.c
@@ -83,6 +83,7 @@ void aom_paeth_predictor_4x8_ssse3(uint8_t *dst, ptrdiff_t stride,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_paeth_predictor_4x16_ssse3(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *above, const uint8_t *left) {
   __m128i l = _mm_load_si128((const __m128i *)left);
@@ -102,6 +103,7 @@ void aom_paeth_predictor_4x16_ssse3(uint8_t *dst, ptrdiff_t stride,
     rep = _mm_add_epi16(rep, one);
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_paeth_predictor_8x4_ssse3(uint8_t *dst, ptrdiff_t stride,
                                    const uint8_t *above, const uint8_t *left) {
@@ -166,6 +168,7 @@ void aom_paeth_predictor_8x16_ssse3(uint8_t *dst, ptrdiff_t stride,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_paeth_predictor_8x32_ssse3(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *above, const uint8_t *left) {
   const __m128i t = _mm_loadl_epi64((const __m128i *)above);
@@ -187,6 +190,7 @@ void aom_paeth_predictor_8x32_ssse3(uint8_t *dst, ptrdiff_t stride,
     }
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // Return 16 8-bit pixels in one row
 static inline __m128i paeth_16x1_pred(const __m128i *left, const __m128i *top0,
@@ -197,6 +201,7 @@ static inline __m128i paeth_16x1_pred(const __m128i *left, const __m128i *top0,
   return _mm_packus_epi16(p0, p1);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_paeth_predictor_16x4_ssse3(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *above, const uint8_t *left) {
   __m128i l = _mm_cvtsi32_si128(((const int *)left)[0]);
@@ -217,6 +222,7 @@ void aom_paeth_predictor_16x4_ssse3(uint8_t *dst, ptrdiff_t stride,
     rep = _mm_add_epi16(rep, one);
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_paeth_predictor_16x8_ssse3(uint8_t *dst, ptrdiff_t stride,
                                     const uint8_t *above, const uint8_t *left) {
@@ -298,6 +304,7 @@ void aom_paeth_predictor_16x32_ssse3(uint8_t *dst, ptrdiff_t stride,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_paeth_predictor_16x64_ssse3(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
                                      const uint8_t *left) {
@@ -348,6 +355,7 @@ void aom_paeth_predictor_32x8_ssse3(uint8_t *dst, ptrdiff_t stride,
     rep = _mm_add_epi16(rep, one);
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_paeth_predictor_32x16_ssse3(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
@@ -538,6 +546,7 @@ void aom_paeth_predictor_64x64_ssse3(uint8_t *dst, ptrdiff_t stride,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_paeth_predictor_64x16_ssse3(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
                                      const uint8_t *left) {
@@ -577,6 +586,7 @@ void aom_paeth_predictor_64x16_ssse3(uint8_t *dst, ptrdiff_t stride,
     rep = _mm_add_epi16(rep, one);
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // -----------------------------------------------------------------------------
 // SMOOTH_PRED
@@ -686,6 +696,7 @@ void aom_smooth_predictor_4x8_ssse3(uint8_t *dst, ptrdiff_t stride,
   smooth_pred_4xh(pixels, wh, ww, 8, dst, stride, 0);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_smooth_predictor_4x16_ssse3(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
                                      const uint8_t *left) {
@@ -699,6 +710,7 @@ void aom_smooth_predictor_4x16_ssse3(uint8_t *dst, ptrdiff_t stride,
   dst += stride << 3;
   smooth_pred_4xh(pixels, &wh[2], ww, 8, dst, stride, 1);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // pixels[0]: above and below_pred interleave vector, first half
 // pixels[1]: above and below_pred interleave vector, second half
@@ -865,6 +877,7 @@ void aom_smooth_predictor_8x16_ssse3(uint8_t *dst, ptrdiff_t stride,
   smooth_pred_8xh(pixels, &wh[2], ww, 8, dst, stride, 1);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_smooth_predictor_8x32_ssse3(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
                                      const uint8_t *left) {
@@ -882,6 +895,7 @@ void aom_smooth_predictor_8x32_ssse3(uint8_t *dst, ptrdiff_t stride,
   dst += stride << 3;
   smooth_pred_8xh(&pixels[4], &wh[6], ww, 8, dst, stride, 1);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // TODO(slavarnway): Visual Studio only supports restrict when /std:c11
 // (available in 2019+) or greater is specified; __restrict can be used in that
@@ -998,11 +1012,13 @@ static void smooth_predictor_wxh(uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_smooth_predictor_16x4_ssse3(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
                                      const uint8_t *left) {
   smooth_predictor_wxh(dst, stride, above, left, 16, 4);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_smooth_predictor_16x8_ssse3(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *above,
@@ -1022,6 +1038,7 @@ void aom_smooth_predictor_16x32_ssse3(uint8_t *dst, ptrdiff_t stride,
   smooth_predictor_wxh(dst, stride, above, left, 16, 32);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_smooth_predictor_16x64_ssse3(uint8_t *dst, ptrdiff_t stride,
                                       const uint8_t *above,
                                       const uint8_t *left) {
@@ -1033,6 +1050,7 @@ void aom_smooth_predictor_32x8_ssse3(uint8_t *dst, ptrdiff_t stride,
                                      const uint8_t *left) {
   smooth_predictor_wxh(dst, stride, above, left, 32, 8);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_smooth_predictor_32x16_ssse3(uint8_t *dst, ptrdiff_t stride,
                                       const uint8_t *above,
@@ -1052,11 +1070,13 @@ void aom_smooth_predictor_32x64_ssse3(uint8_t *dst, ptrdiff_t stride,
   smooth_predictor_wxh(dst, stride, above, left, 32, 64);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_smooth_predictor_64x16_ssse3(uint8_t *dst, ptrdiff_t stride,
                                       const uint8_t *above,
                                       const uint8_t *left) {
   smooth_predictor_wxh(dst, stride, above, left, 64, 16);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_smooth_predictor_64x32_ssse3(uint8_t *dst, ptrdiff_t stride,
                                       const uint8_t *above,
@@ -1198,6 +1218,7 @@ void aom_smooth_v_predictor_4x8_ssse3(
   write_smooth_vertical4xh(&pixels, weights, 8, dst, stride);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_smooth_v_predictor_4x16_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
     const uint8_t *LIBAOM_RESTRICT top_row,
@@ -1212,6 +1233,7 @@ void aom_smooth_v_predictor_4x16_ssse3(
   dst += stride << 3;
   write_smooth_vertical4xh(&pixels, &weights[2], 8, dst, stride);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_smooth_v_predictor_8x4_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
@@ -1311,6 +1333,7 @@ void aom_smooth_v_predictor_8x16_ssse3(
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_smooth_v_predictor_8x32_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
     const uint8_t *LIBAOM_RESTRICT top_row,
@@ -1419,6 +1442,7 @@ void aom_smooth_v_predictor_16x4_ssse3(
                                  scaled_bottom_left_y, scaled_bottom_left_y,
                                  round);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_smooth_v_predictor_16x8_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
@@ -1561,6 +1585,7 @@ void aom_smooth_v_predictor_16x32_ssse3(
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_smooth_v_predictor_16x64_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
     const uint8_t *LIBAOM_RESTRICT top_row,
@@ -1639,6 +1664,7 @@ void aom_smooth_v_predictor_32x8_ssse3(
     dst += stride;
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_smooth_v_predictor_32x16_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
@@ -1832,6 +1858,7 @@ void aom_smooth_v_predictor_32x64_ssse3(
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_smooth_v_predictor_64x16_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
     const uint8_t *LIBAOM_RESTRICT top_row,
@@ -1901,6 +1928,7 @@ void aom_smooth_v_predictor_64x16_ssse3(
     dst += stride;
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_smooth_v_predictor_64x32_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
@@ -2178,6 +2206,7 @@ void aom_smooth_h_predictor_4x8_ssse3(
                                &round);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_smooth_h_predictor_4x16_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
     const uint8_t *LIBAOM_RESTRICT top_row,
@@ -2259,6 +2288,7 @@ void aom_smooth_h_predictor_4x16_ssse3(
   write_smooth_horizontal_sum4(dst, &left_y, &weights, &scaled_top_right,
                                &round);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // For SMOOTH_H, |pixels| is the repeated left value for the row. For SMOOTH_V,
 // |pixels| is a segment of the top row or the whole top row, and |weights| is
@@ -2343,6 +2373,7 @@ void aom_smooth_h_predictor_8x16_ssse3(
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_smooth_h_predictor_8x32_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
     const uint8_t *LIBAOM_RESTRICT top_row,
@@ -2424,6 +2455,7 @@ void aom_smooth_h_predictor_16x4_ssse3(
   write_smooth_directional_sum16(dst, left_y, left_y, weights1, weights2,
                                  scaled_top_right1, scaled_top_right2, round);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_smooth_h_predictor_16x8_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
@@ -2535,6 +2567,7 @@ void aom_smooth_h_predictor_16x32_ssse3(
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_smooth_h_predictor_16x64_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
     const uint8_t *LIBAOM_RESTRICT top_row,
@@ -2600,6 +2633,7 @@ void aom_smooth_h_predictor_32x8_ssse3(
     dst += stride;
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_smooth_h_predictor_32x16_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
@@ -2757,6 +2791,7 @@ void aom_smooth_h_predictor_32x64_ssse3(
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void aom_smooth_h_predictor_64x16_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
     const uint8_t *LIBAOM_RESTRICT top_row,
@@ -2829,6 +2864,7 @@ void aom_smooth_h_predictor_64x16_ssse3(
     dst += stride;
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void aom_smooth_h_predictor_64x32_ssse3(
     uint8_t *LIBAOM_RESTRICT dst, ptrdiff_t stride,
diff --git a/aom_dsp/x86/inv_wht_sse2.asm b/aom_dsp/x86/inv_wht_sse2.asm
deleted file mode 100644
index 7ad3fbf3f..000000000
--- a/aom_dsp/x86/inv_wht_sse2.asm
+++ /dev/null
@@ -1,107 +0,0 @@
-;
-; Copyright (c) 2016, Alliance for Open Media. All rights reserved.
-;
-; This source code is subject to the terms of the BSD 2 Clause License and
-; the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
-; was not distributed with this source code in the LICENSE file, you can
-; obtain it at www.aomedia.org/license/software. If the Alliance for Open
-; Media Patent License 1.0 was not distributed with this source code in the
-; PATENTS file, you can obtain it at www.aomedia.org/license/patent.
-;
-
-;
-
-%include "third_party/x86inc/x86inc.asm"
-
-SECTION .text
-
-%macro REORDER_INPUTS 0
-  ; a c d b  to  a b c d
-  SWAP 1, 3, 2
-%endmacro
-
-%macro TRANSFORM_COLS 0
-  ; input:
-  ; m0 a
-  ; m1 b
-  ; m2 c
-  ; m3 d
-  paddw           m0,        m2
-  psubw           m3,        m1
-
-  ; wide subtract
-  punpcklwd       m4,        m0
-  punpcklwd       m5,        m3
-  psrad           m4,        16
-  psrad           m5,        16
-  psubd           m4,        m5
-  psrad           m4,        1
-  packssdw        m4,        m4             ; e
-
-  psubw           m5,        m4,        m1  ; b
-  psubw           m4,        m2             ; c
-  psubw           m0,        m5
-  paddw           m3,        m4
-                                ; m0 a
-  SWAP            1,         5  ; m1 b
-  SWAP            2,         4  ; m2 c
-                                ; m3 d
-%endmacro
-
-%macro TRANSPOSE_4X4 0
-  punpcklwd       m0,        m2
-  punpcklwd       m1,        m3
-  mova            m2,        m0
-  punpcklwd       m0,        m1
-  punpckhwd       m2,        m1
-  pshufd          m1,        m0, 0x0e
-  pshufd          m3,        m2, 0x0e
-%endmacro
-
-; transpose a 4x4 int16 matrix in xmm0 and xmm1 to the bottom half of xmm0-xmm3
-%macro TRANSPOSE_4X4_WIDE 0
-  mova            m3, m0
-  punpcklwd       m0, m1
-  punpckhwd       m3, m1
-  mova            m2, m0
-  punpcklwd       m0, m3
-  punpckhwd       m2, m3
-  pshufd          m1, m0, 0x0e
-  pshufd          m3, m2, 0x0e
-%endmacro
-
-%macro ADD_STORE_4P_2X 5  ; src1, src2, tmp1, tmp2, zero
-  movd            m%3,       [outputq]
-  movd            m%4,       [outputq + strideq]
-  punpcklbw       m%3,       m%5
-  punpcklbw       m%4,       m%5
-  paddw           m%1,       m%3
-  paddw           m%2,       m%4
-  packuswb        m%1,       m%5
-  packuswb        m%2,       m%5
-  movd            [outputq], m%1
-  movd            [outputq + strideq], m%2
-%endmacro
-
-INIT_XMM sse2
-cglobal iwht4x4_16_add, 3, 3, 7, input, output, stride
-  mova            m0,        [inputq +  0]
-  packssdw        m0,        [inputq + 16]
-  mova            m1,        [inputq + 32]
-  packssdw        m1,        [inputq + 48]
-  psraw           m0,        2
-  psraw           m1,        2
-
-  TRANSPOSE_4X4_WIDE
-  REORDER_INPUTS
-  TRANSFORM_COLS
-  TRANSPOSE_4X4
-  REORDER_INPUTS
-  TRANSFORM_COLS
-
-  pxor            m4, m4
-  ADD_STORE_4P_2X  0, 1, 5, 6, 4
-  lea             outputq, [outputq + 2 * strideq]
-  ADD_STORE_4P_2X  2, 3, 5, 6, 4
-
-  RET
diff --git a/aom_dsp/x86/sad_sse2.asm b/aom_dsp/x86/sad_sse2.asm
index 0209add0e..424a95434 100644
--- a/aom_dsp/x86/sad_sse2.asm
+++ b/aom_dsp/x86/sad_sse2.asm
@@ -190,15 +190,17 @@ INIT_XMM sse2
 SAD64XN 128     ; sad64x128_sse2
 SAD64XN  64     ; sad64x64_sse2
 SAD64XN  32     ; sad64x32_sse2
-SAD64XN  16     ; sad64x16_sse2
 SAD64XN 128, 1  ; sad64x128_avg_sse2
 SAD64XN  64, 1  ; sad64x64_avg_sse2
 SAD64XN  32, 1  ; sad64x32_avg_sse2
-SAD64XN  16, 1  ; sad64x16_avg_sse2
 SAD64XN 128, 2  ; sad64x128_skip_sse2
 SAD64XN  64, 2  ; sad64x64_skip_sse2
 SAD64XN  32, 2  ; sad64x32_skip_sse2
+%if CONFIG_REALTIME_ONLY==0
+SAD64XN  16     ; sad64x16_sse2
+SAD64XN  16, 1  ; sad64x16_avg_sse2
 SAD64XN  16, 2  ; sad64x16_skip_sse2
+%endif
 
 ; unsigned int aom_sad32x32_sse2(uint8_t *src, int src_stride,
 ;                                uint8_t *ref, int ref_stride);
@@ -248,15 +250,17 @@ INIT_XMM sse2
 SAD32XN 64    ; sad32x64_sse2
 SAD32XN 32    ; sad32x32_sse2
 SAD32XN 16    ; sad32x16_sse2
-SAD32XN  8    ; sad_32x8_sse2
 SAD32XN 64, 1 ; sad32x64_avg_sse2
 SAD32XN 32, 1 ; sad32x32_avg_sse2
 SAD32XN 16, 1 ; sad32x16_avg_sse2
-SAD32XN  8, 1 ; sad_32x8_avg_sse2
 SAD32XN 64, 2 ; sad32x64_skip_sse2
 SAD32XN 32, 2 ; sad32x32_skip_sse2
 SAD32XN 16, 2 ; sad32x16_skip_sse2
+%if CONFIG_REALTIME_ONLY==0
+SAD32XN  8    ; sad_32x8_sse2
+SAD32XN  8, 1 ; sad_32x8_avg_sse2
 SAD32XN  8, 2 ; sad_32x8_skip_sse2
+%endif
 
 ; unsigned int aom_sad16x{8,16}_sse2(uint8_t *src, int src_stride,
 ;                                    uint8_t *ref, int ref_stride);
@@ -304,20 +308,22 @@ SAD32XN  8, 2 ; sad_32x8_skip_sse2
 %endmacro
 
 INIT_XMM sse2
-SAD16XN 64    ; sad_16x64_sse2
 SAD16XN 32    ; sad16x32_sse2
 SAD16XN 16    ; sad16x16_sse2
 SAD16XN  8    ; sad16x8_sse2
-SAD16XN  4    ; sad_16x4_sse2
-SAD16XN 64, 1 ; sad_16x64_avg_sse2
 SAD16XN 32, 1 ; sad16x32_avg_sse2
 SAD16XN 16, 1 ; sad16x16_avg_sse2
 SAD16XN  8, 1 ; sad16x8_avg_sse2
-SAD16XN  4, 1 ; sad_16x4_avg_sse2
-SAD16XN 64, 2 ; sad_16x64_skip_sse2
 SAD16XN 32, 2 ; sad16x32_skip_sse2
 SAD16XN 16, 2 ; sad16x16_skip_sse2
 SAD16XN  8, 2 ; sad16x8_skip_sse2
+%if CONFIG_REALTIME_ONLY==0
+SAD16XN 64    ; sad_16x64_sse2
+SAD16XN  4    ; sad_16x4_sse2
+SAD16XN 64, 1 ; sad_16x64_avg_sse2
+SAD16XN  4, 1 ; sad_16x4_avg_sse2
+SAD16XN 64, 2 ; sad_16x64_skip_sse2
+%endif
 
 ; unsigned int aom_sad8x{8,16}_sse2(uint8_t *src, int src_stride,
 ;                                   uint8_t *ref, int ref_stride);
@@ -363,17 +369,19 @@ SAD16XN  8, 2 ; sad16x8_skip_sse2
 %endmacro
 
 INIT_XMM sse2
-SAD8XN 32    ; sad_8x32_sse2
 SAD8XN 16    ; sad8x16_sse2
 SAD8XN  8    ; sad8x8_sse2
 SAD8XN  4    ; sad8x4_sse2
-SAD8XN 32, 1 ; sad_8x32_avg_sse2
 SAD8XN 16, 1 ; sad8x16_avg_sse2
 SAD8XN  8, 1 ; sad8x8_avg_sse2
 SAD8XN  4, 1 ; sad8x4_avg_sse2
-SAD8XN 32, 2 ; sad_8x32_skip_sse2
 SAD8XN 16, 2 ; sad8x16_skip_sse2
 SAD8XN  8, 2 ; sad8x8_skip_sse2
+%if CONFIG_REALTIME_ONLY==0
+SAD8XN 32    ; sad_8x32_sse2
+SAD8XN 32, 1 ; sad_8x32_avg_sse2
+SAD8XN 32, 2 ; sad_8x32_skip_sse2
+%endif
 
 ; unsigned int aom_sad4x{4, 8}_sse2(uint8_t *src, int src_stride,
 ;                                   uint8_t *ref, int ref_stride);
@@ -422,11 +430,13 @@ SAD8XN  8, 2 ; sad8x8_skip_sse2
 %endmacro
 
 INIT_XMM sse2
+SAD4XN  8 ; sad4x8_sse2
+SAD4XN  4 ; sad4x4_sse2
+SAD4XN  8, 1 ; sad4x8_avg_sse2
+SAD4XN  4, 1 ; sad4x4_avg_sse2
+SAD4XN  8, 2 ; sad4x8_skip_sse2
+%if CONFIG_REALTIME_ONLY==0
 SAD4XN 16 ; sad_4x16_sse2
-SAD4XN  8 ; sad4x8_sse
-SAD4XN  4 ; sad4x4_sse
 SAD4XN 16, 1 ; sad_4x16_avg_sse2
-SAD4XN  8, 1 ; sad4x8_avg_sse
-SAD4XN  4, 1 ; sad4x4_avg_sse
 SAD4XN 16, 2 ; sad_4x16_skip_sse2
-SAD4XN  8, 2 ; sad4x8_skip_sse
+%endif
diff --git a/aom_dsp/x86/ssim_sse2_x86_64.asm b/aom_dsp/x86/ssim_sse2_x86_64.asm
index f95e83435..51dc311c7 100644
--- a/aom_dsp/x86/ssim_sse2_x86_64.asm
+++ b/aom_dsp/x86/ssim_sse2_x86_64.asm
@@ -50,97 +50,7 @@
 
 SECTION .text
 
-;void ssim_parms_sse2(
-;    unsigned char *s,
-;    int sp,
-;    unsigned char *r,
-;    int rp
-;    uint32_t *sum_s,
-;    uint32_t *sum_r,
-;    uint32_t *sum_sq_s,
-;    uint32_t *sum_sq_r,
-;    uint32_t *sum_sxr);
-;
-; TODO: Use parm passing through structure, probably don't need the pxors
-; ( calling app will initialize to 0 ) could easily fit everything in sse2
-; without too much hastle, and can probably do better estimates with psadw
-; or pavgb At this point this is just meant to be first pass for calculating
-; all the parms needed for 16x16 ssim so we can play with dssim as distortion
-; in mode selection code.
-globalsym(aom_ssim_parms_16x16_sse2)
-sym(aom_ssim_parms_16x16_sse2):
-    push        rbp
-    mov         rbp, rsp
-    SHADOW_ARGS_TO_STACK 9
-    SAVE_XMM 15
-    push        rsi
-    push        rdi
-    ; end prolog
-
-    mov             rsi,        arg(0) ;s
-    mov             rcx,        arg(1) ;sp
-    mov             rdi,        arg(2) ;r
-    mov             rax,        arg(3) ;rp
-
-    pxor            xmm0, xmm0
-    pxor            xmm15,xmm15  ;sum_s
-    pxor            xmm14,xmm14  ;sum_r
-    pxor            xmm13,xmm13  ;sum_sq_s
-    pxor            xmm12,xmm12  ;sum_sq_r
-    pxor            xmm11,xmm11  ;sum_sxr
-
-    mov             rdx, 16      ;row counter
-.NextRow:
-
-    ;grab source and reference pixels
-    movdqu          xmm5, [rsi]
-    movdqu          xmm6, [rdi]
-    movdqa          xmm3, xmm5
-    movdqa          xmm4, xmm6
-    punpckhbw       xmm3, xmm0 ; high_s
-    punpckhbw       xmm4, xmm0 ; high_r
-
-    TABULATE_SSIM
-
-    movdqa          xmm3, xmm5
-    movdqa          xmm4, xmm6
-    punpcklbw       xmm3, xmm0 ; low_s
-    punpcklbw       xmm4, xmm0 ; low_r
-
-    TABULATE_SSIM
-
-    add             rsi, rcx   ; next s row
-    add             rdi, rax   ; next r row
-
-    dec             rdx        ; counter
-    jnz .NextRow
-
-    SUM_ACROSS_W    xmm15
-    SUM_ACROSS_W    xmm14
-    SUM_ACROSS_Q    xmm13
-    SUM_ACROSS_Q    xmm12
-    SUM_ACROSS_Q    xmm11
-
-    mov             rdi,arg(4)
-    movd            [rdi], xmm15;
-    mov             rdi,arg(5)
-    movd            [rdi], xmm14;
-    mov             rdi,arg(6)
-    movd            [rdi], xmm13;
-    mov             rdi,arg(7)
-    movd            [rdi], xmm12;
-    mov             rdi,arg(8)
-    movd            [rdi], xmm11;
-
-    ; begin epilog
-    pop         rdi
-    pop         rsi
-    RESTORE_XMM
-    UNSHADOW_ARGS
-    pop         rbp
-    ret
-
-;void ssim_parms_sse2(
+;void aom_ssim_parms_8x8_sse2(
 ;    unsigned char *s,
 ;    int sp,
 ;    unsigned char *r,
diff --git a/aom_dsp/x86/sum_squares_avx2.c b/aom_dsp/x86/sum_squares_avx2.c
index e2f5327ef..6774374e6 100644
--- a/aom_dsp/x86/sum_squares_avx2.c
+++ b/aom_dsp/x86/sum_squares_avx2.c
@@ -15,6 +15,7 @@
 #include "aom_dsp/x86/synonyms.h"
 #include "aom_dsp/x86/synonyms_avx2.h"
 #include "aom_dsp/x86/sum_squares_sse2.h"
+#include "config/aom_config.h"
 #include "config/aom_dsp_rtcd.h"
 
 static uint64_t aom_sum_squares_2d_i16_nxn_avx2(const int16_t *src, int stride,
@@ -256,6 +257,7 @@ uint64_t aom_var_2d_u8_avx2(uint8_t *src, int src_stride, int width,
   return (ss - s * s / (width * height));
 }
 
+#if CONFIG_AV1_HIGHBITDEPTH
 uint64_t aom_var_2d_u16_avx2(uint8_t *src, int src_stride, int width,
                              int height) {
   uint16_t *srcp1 = CONVERT_TO_SHORTPTR(src), *srcp;
@@ -324,3 +326,4 @@ uint64_t aom_var_2d_u16_avx2(uint8_t *src, int src_stride, int width,
   }
   return (ss - s * s / (width * height));
 }
+#endif  // CONFIG_AV1_HIGHBITDEPTH
diff --git a/aom_dsp/x86/sum_squares_sse2.c b/aom_dsp/x86/sum_squares_sse2.c
index b12c10d8e..1061c15b3 100644
--- a/aom_dsp/x86/sum_squares_sse2.c
+++ b/aom_dsp/x86/sum_squares_sse2.c
@@ -408,6 +408,7 @@ uint64_t aom_var_2d_u8_sse2(uint8_t *src, int src_stride, int width,
   return (ss - s * s / (width * height));
 }
 
+#if CONFIG_AV1_HIGHBITDEPTH
 uint64_t aom_var_2d_u16_sse2(uint8_t *src, int src_stride, int width,
                              int height) {
   uint16_t *srcp1 = CONVERT_TO_SHORTPTR(src), *srcp;
@@ -476,3 +477,4 @@ uint64_t aom_var_2d_u16_sse2(uint8_t *src, int src_stride, int width,
   }
   return (ss - s * s / (width * height));
 }
+#endif  // CONFIG_AV1_HIGHBITDEPTH
diff --git a/aom_dsp/x86/variance_sse2.c b/aom_dsp/x86/variance_sse2.c
index 9d6a238a2..8731e953c 100644
--- a/aom_dsp/x86/variance_sse2.c
+++ b/aom_dsp/x86/variance_sse2.c
@@ -20,6 +20,7 @@
 #include "aom_dsp/x86/synonyms.h"
 #include "aom_ports/mem.h"
 
+#if !CONFIG_REALTIME_ONLY
 unsigned int aom_get_mb_ss_sse2(const int16_t *src) {
   __m128i vsum = _mm_setzero_si128();
   int i;
@@ -34,6 +35,7 @@ unsigned int aom_get_mb_ss_sse2(const int16_t *src) {
   vsum = _mm_add_epi32(vsum, _mm_srli_si128(vsum, 4));
   return (unsigned int)_mm_cvtsi128_si32(vsum);
 }
+#endif  // !CONFIG_REALTIME_ONLY
 
 static inline __m128i load4x2_sse2(const uint8_t *const p, const int stride) {
   const __m128i p0 = _mm_cvtsi32_si128(loadu_int32(p + 0 * stride));
diff --git a/aom_ports/aarch64_cpudetect.c b/aom_ports/aarch64_cpudetect.c
index cfab17b77..3b7c2d7c7 100644
--- a/aom_ports/aarch64_cpudetect.c
+++ b/aom_ports/aarch64_cpudetect.c
@@ -85,7 +85,35 @@ static int arm_get_cpu_caps(void) {
   }
 #endif  // defined(PF_ARM_V82_DP_INSTRUCTIONS_AVAILABLE)
 #endif  // HAVE_NEON_DOTPROD
-  // No I8MM or SVE feature detection available on Windows at time of writing.
+#if HAVE_NEON_I8MM
+// Support for PF_ARM_SVE_I8MM_INSTRUCTIONS_AVAILABLE was added in Windows SDK
+// 26100.
+#if defined(PF_ARM_SVE_I8MM_INSTRUCTIONS_AVAILABLE)
+  // There's no PF_* flag that indicates whether plain I8MM is available
+  // or not. But if SVE_I8MM is available, that also implies that
+  // regular I8MM is available.
+  if (IsProcessorFeaturePresent(PF_ARM_SVE_I8MM_INSTRUCTIONS_AVAILABLE)) {
+    flags |= HAS_NEON_I8MM;
+  }
+#endif  // defined(PF_ARM_SVE_I8MM_INSTRUCTIONS_AVAILABLE)
+#endif  // HAVE_NEON_I8MM
+#if HAVE_SVE
+// Support for PF_ARM_SVE_INSTRUCTIONS_AVAILABLE was added in Windows SDK 26100.
+#if defined(PF_ARM_SVE_INSTRUCTIONS_AVAILABLE)
+  if (IsProcessorFeaturePresent(PF_ARM_SVE_INSTRUCTIONS_AVAILABLE)) {
+    flags |= HAS_SVE;
+  }
+#endif  // defined(PF_ARM_SVE_INSTRUCTIONS_AVAILABLE)
+#endif  // HAVE_SVE
+#if HAVE_SVE2
+// Support for PF_ARM_SVE2_INSTRUCTIONS_AVAILABLE was added in Windows SDK
+// 26100.
+#if defined(PF_ARM_SVE2_INSTRUCTIONS_AVAILABLE)
+  if (IsProcessorFeaturePresent(PF_ARM_SVE2_INSTRUCTIONS_AVAILABLE)) {
+    flags |= HAS_SVE2;
+  }
+#endif  // defined(PF_ARM_SVE2_INSTRUCTIONS_AVAILABLE)
+#endif  // HAVE_SVE2
   return flags;
 }
 
diff --git a/av1/av1.cmake b/av1/av1.cmake
index 047f3672c..1c697b525 100644
--- a/av1/av1.cmake
+++ b/av1/av1.cmake
@@ -232,8 +232,6 @@ list(APPEND AOM_AV1_ENCODER_SOURCES
             "${AOM_ROOT}/av1/encoder/svc_layercontext.h"
             "${AOM_ROOT}/av1/encoder/temporal_filter.c"
             "${AOM_ROOT}/av1/encoder/temporal_filter.h"
-            "${AOM_ROOT}/av1/encoder/thirdpass.c"
-            "${AOM_ROOT}/av1/encoder/thirdpass.h"
             "${AOM_ROOT}/av1/encoder/tokenize.c"
             "${AOM_ROOT}/av1/encoder/tokenize.h"
             "${AOM_ROOT}/av1/encoder/tpl_model.c"
@@ -260,6 +258,11 @@ list(APPEND AOM_AV1_ENCODER_SOURCES
             "${AOM_ROOT}/av1/encoder/dwt.c"
             "${AOM_ROOT}/av1/encoder/dwt.h")
 
+if(CONFIG_REALTIME_ONLY)
+  list(REMOVE_ITEM AOM_AV1_ENCODER_SOURCES
+                   "${AOM_ROOT}/av1/encoder/grain_test_vectors.h")
+endif()
+
 list(APPEND AOM_AV1_COMMON_INTRIN_SSE2
             "${AOM_ROOT}/av1/common/x86/av1_txfm_sse2.h"
             "${AOM_ROOT}/av1/common/x86/cfl_sse2.c"
@@ -420,12 +423,14 @@ list(APPEND AOM_AV1_COMMON_INTRIN_NEON
 list(APPEND AOM_AV1_COMMON_INTRIN_NEON_DOTPROD
             "${AOM_ROOT}/av1/common/arm/av1_convolve_scale_neon_dotprod.c"
             "${AOM_ROOT}/av1/common/arm/compound_convolve_neon_dotprod.c"
-            "${AOM_ROOT}/av1/common/arm/convolve_neon_dotprod.c")
+            "${AOM_ROOT}/av1/common/arm/convolve_neon_dotprod.c"
+            "${AOM_ROOT}/av1/common/arm/resize_neon_dotprod.c")
 
 list(APPEND AOM_AV1_COMMON_INTRIN_NEON_I8MM
             "${AOM_ROOT}/av1/common/arm/av1_convolve_scale_neon_i8mm.c"
             "${AOM_ROOT}/av1/common/arm/compound_convolve_neon_i8mm.c"
             "${AOM_ROOT}/av1/common/arm/convolve_neon_i8mm.c"
+            "${AOM_ROOT}/av1/common/arm/resize_neon_i8mm.c"
             "${AOM_ROOT}/av1/common/arm/warp_plane_neon_i8mm.c")
 
 list(APPEND AOM_AV1_COMMON_INTRIN_SVE
@@ -440,6 +445,11 @@ list(APPEND AOM_AV1_ENCODER_INTRIN_SSE4_2
 
 list(APPEND AOM_AV1_COMMON_INTRIN_VSX "${AOM_ROOT}/av1/common/ppc/cfl_ppc.c")
 
+if(CONFIG_THREE_PASS)
+  list(APPEND AOM_AV1_ENCODER_SOURCES "${AOM_ROOT}/av1/encoder/thirdpass.c"
+              "${AOM_ROOT}/av1/encoder/thirdpass.h")
+endif()
+
 if(CONFIG_TUNE_VMAF)
   list(APPEND AOM_AV1_ENCODER_SOURCES "${AOM_ROOT}/av1/encoder/tune_vmaf.c"
               "${AOM_ROOT}/av1/encoder/tune_vmaf.h")
@@ -549,6 +559,54 @@ if(CONFIG_INTERNAL_STATS)
 endif()
 
 if(CONFIG_REALTIME_ONLY)
+  if(NOT CONFIG_AV1_DECODER)
+    list(REMOVE_ITEM AOM_AV1_COMMON_SOURCES "${AOM_ROOT}/av1/common/cfl.c"
+                     "${AOM_ROOT}/av1/common/cfl.h"
+                     "${AOM_ROOT}/av1/common/restoration.c"
+                     "${AOM_ROOT}/av1/common/restoration.h"
+                     "${AOM_ROOT}/av1/common/warped_motion.c"
+                     "${AOM_ROOT}/av1/common/warped_motion.h")
+
+    list(REMOVE_ITEM AOM_AV1_COMMON_INTRIN_SSE2
+                     "${AOM_ROOT}/av1/common/x86/cfl_sse2.c"
+                     "${AOM_ROOT}/av1/common/x86/warp_plane_sse2.c"
+                     "${AOM_ROOT}/av1/common/x86/wiener_convolve_sse2.c")
+
+    list(REMOVE_ITEM AOM_AV1_COMMON_INTRIN_SSE4_1
+                     "${AOM_ROOT}/av1/common/x86/highbd_warp_plane_sse4.c"
+                     "${AOM_ROOT}/av1/common/x86/selfguided_sse4.c"
+                     "${AOM_ROOT}/av1/common/x86/warp_plane_sse4.c")
+
+    list(
+      REMOVE_ITEM AOM_AV1_COMMON_INTRIN_SSSE3
+                  "${AOM_ROOT}/av1/common/x86/cfl_ssse3.c"
+                  "${AOM_ROOT}/av1/common/x86/highbd_wiener_convolve_ssse3.c")
+
+    list(REMOVE_ITEM AOM_AV1_COMMON_INTRIN_AVX2
+                     "${AOM_ROOT}/av1/common/x86/cfl_avx2.c"
+                     "${AOM_ROOT}/av1/common/x86/highbd_warp_affine_avx2.c"
+                     "${AOM_ROOT}/av1/common/x86/highbd_wiener_convolve_avx2.c"
+                     "${AOM_ROOT}/av1/common/x86/selfguided_avx2.c"
+                     "${AOM_ROOT}/av1/common/x86/warp_plane_avx2.c"
+                     "${AOM_ROOT}/av1/common/x86/wiener_convolve_avx2.c")
+
+    list(REMOVE_ITEM AOM_AV1_COMMON_INTRIN_NEON
+                     "${AOM_ROOT}/av1/common/arm/cfl_neon.c"
+                     "${AOM_ROOT}/av1/common/arm/highbd_warp_plane_neon.c"
+                     "${AOM_ROOT}/av1/common/arm/highbd_wiener_convolve_neon.c"
+                     "${AOM_ROOT}/av1/common/arm/selfguided_neon.c"
+                     "${AOM_ROOT}/av1/common/arm/warp_plane_neon.c"
+                     "${AOM_ROOT}/av1/common/arm/warp_plane_neon.h"
+                     "${AOM_ROOT}/av1/common/arm/wiener_convolve_neon.c")
+
+    list(REMOVE_ITEM AOM_AV1_COMMON_INTRIN_NEON_I8MM
+                     "${AOM_ROOT}/av1/common/arm/warp_plane_neon_i8mm.c")
+
+    list(REMOVE_ITEM AOM_AV1_COMMON_INTRIN_SVE
+                     "${AOM_ROOT}/av1/common/arm/highbd_warp_plane_sve.c"
+                     "${AOM_ROOT}/av1/common/arm/warp_plane_sve.c")
+  endif()
+
   list(REMOVE_ITEM AOM_AV1_ENCODER_INTRIN_SSE2
                    "${AOM_ROOT}/av1/encoder/x86/highbd_temporal_filter_sse2.c"
                    "${AOM_ROOT}/av1/encoder/x86/temporal_filter_sse2.c")
@@ -572,6 +630,9 @@ if(CONFIG_REALTIME_ONLY)
   list(REMOVE_ITEM AOM_AV1_ENCODER_INTRIN_NEON_DOTPROD
                    "${AOM_ROOT}/av1/encoder/arm/temporal_filter_neon_dotprod.c")
 
+  list(REMOVE_ITEM AOM_AV1_ENCODER_INTRIN_SVE
+                   "${AOM_ROOT}/av1/encoder/arm/pickrst_sve.c")
+
   list(REMOVE_ITEM AOM_AV1_ENCODER_SOURCES
                    "${AOM_ROOT}/av1/encoder/cnn.c"
                    "${AOM_ROOT}/av1/encoder/cnn.h"
diff --git a/av1/av1_cx_iface.c b/av1/av1_cx_iface.c
index a13ba518a..2de2cb4b0 100644
--- a/av1/av1_cx_iface.c
+++ b/av1/av1_cx_iface.c
@@ -48,6 +48,7 @@
 #include "common/args_helper.h"
 
 struct av1_extracfg {
+  unsigned int usage;  // Same as g_usage in aom_codec_enc_cfg_t
   int cpu_used;
   unsigned int enable_auto_alt_ref;
   unsigned int enable_auto_bwd_ref;
@@ -213,322 +214,314 @@ struct av1_extracfg {
   int sb_qp_sweep;
 };
 
-#if CONFIG_REALTIME_ONLY
-// Settings changed for realtime only build:
-// cpu_used: 7
-// enable_tpl_model: 0
-// enable_restoration: 0
-// enable_obmc: 0
-// deltaq_mode: NO_DELTA_Q
-// enable_global_motion usage: 0
-// enable_warped_motion at sequence level: 0
-// allow_warped_motion at frame level: 0
-// coeff_cost_upd_freq: COST_UPD_OFF
-// mode_cost_upd_freq: COST_UPD_OFF
-// mv_cost_upd_freq: COST_UPD_OFF
-// dv_cost_upd_freq: COST_UPD_OFF
-static const struct av1_extracfg default_extra_cfg = {
-  7,              // cpu_used
-  1,              // enable_auto_alt_ref
-  0,              // enable_auto_bwd_ref
-  0,              // noise_sensitivity
-  0,              // sharpness
-  0,              // static_thresh
-  1,              // row_mt
-  0,              // fp_mt
-  0,              // tile_columns
-  0,              // tile_rows
-  0,              // auto_tiles
-  0,              // enable_tpl_model
-  1,              // enable_keyframe_filtering
-  7,              // arnr_max_frames
-  5,              // arnr_strength
-  0,              // min_gf_interval; 0 -> default decision
-  0,              // max_gf_interval; 0 -> default decision
-  0,              // gf_min_pyr_height
-  5,              // gf_max_pyr_height
-  AOM_TUNE_PSNR,  // tuning
-  "/usr/local/share/model/vmaf_v0.6.1.json",  // VMAF model path
-  ".",                                        // partition info path
-  0,                                          // enable rate guide deltaq
-  "./rate_map.txt",                           // rate distribution input
-  AOM_DIST_METRIC_PSNR,                       // dist_metric
-  10,                                         // cq_level
-  0,                                          // rc_max_intra_bitrate_pct
-  0,                                          // rc_max_inter_bitrate_pct
-  0,                                          // gf_cbr_boost_pct
-  0,                                          // lossless
-  1,                                          // enable_cdef
-  0,                                          // enable_restoration
-  0,                                          // force_video_mode
-  0,                                          // enable_obmc
-  3,                                          // disable_trellis_quant
-  0,                                          // enable_qm
-  DEFAULT_QM_Y,                               // qm_y
-  DEFAULT_QM_U,                               // qm_u
-  DEFAULT_QM_V,                               // qm_v
-  DEFAULT_QM_FIRST,                           // qm_min
-  DEFAULT_QM_LAST,                            // qm_max
-  1,                                          // max number of tile groups
-  0,                                          // mtu_size
-  AOM_TIMING_UNSPECIFIED,       // No picture timing signaling in bitstream
-  0,                            // frame_parallel_decoding_mode
-  1,                            // enable dual filter
-  0,                            // enable delta quant in chroma planes
-  NO_AQ,                        // aq_mode
-  NO_DELTA_Q,                   // deltaq_mode
-  100,                          // deltaq_strength
-  0,                            // delta lf mode
-  0,                            // frame_periodic_boost
-  AOM_CONTENT_DEFAULT,          // content
-  AOM_CICP_CP_UNSPECIFIED,      // CICP color primaries
-  AOM_CICP_TC_UNSPECIFIED,      // CICP transfer characteristics
-  AOM_CICP_MC_UNSPECIFIED,      // CICP matrix coefficients
-  AOM_CSP_UNKNOWN,              // chroma sample position
-  0,                            // color range
-  0,                            // render width
-  0,                            // render height
-  AOM_SUPERBLOCK_SIZE_DYNAMIC,  // superblock_size
-  1,                            // this depends on large_scale_tile.
-  0,                            // error_resilient_mode off by default.
-  0,                            // s_frame_mode off by default.
-  0,                            // film_grain_test_vector
-  NULL,                         // film_grain_table_filename
-  0,                            // motion_vector_unit_test
+static const struct av1_extracfg default_extra_cfg[] = {
+#if !CONFIG_REALTIME_ONLY
+  {
+      AOM_USAGE_GOOD_QUALITY,  // usage
+      0,                       // cpu_used
+      1,                       // enable_auto_alt_ref
+      0,                       // enable_auto_bwd_ref
+      0,                       // noise_sensitivity
+      0,                       // sharpness
+      0,                       // static_thresh
+      1,                       // row_mt
+      0,                       // fp_mt
+      0,                       // tile_columns
+      0,                       // tile_rows
+      0,                       // auto_tiles
+      1,                       // enable_tpl_model
+      1,                       // enable_keyframe_filtering
+      7,                       // arnr_max_frames
+      5,                       // arnr_strength
+      0,                       // min_gf_interval; 0 -> default decision
+      0,                       // max_gf_interval; 0 -> default decision
+      0,                       // gf_min_pyr_height
+      5,                       // gf_max_pyr_height
+      AOM_TUNE_PSNR,           // tuning
+      "/usr/local/share/model/vmaf_v0.6.1.json",  // VMAF model path
+      ".",                                        // partition info path
+      0,                                          // enable rate guide deltaq
+      "./rate_map.txt",                           // rate distribution input
+      AOM_DIST_METRIC_PSNR,                       // dist_metric
+      10,                                         // cq_level
+      0,                                          // rc_max_intra_bitrate_pct
+      0,                                          // rc_max_inter_bitrate_pct
+      0,                                          // gf_cbr_boost_pct
+      0,                                          // lossless
+      1,                                          // enable_cdef
+      1,                                          // enable_restoration
+      0,                                          // force_video_mode
+      1,                                          // enable_obmc
+      3,                                          // disable_trellis_quant
+      0,                                          // enable_qm
+      DEFAULT_QM_Y,                               // qm_y
+      DEFAULT_QM_U,                               // qm_u
+      DEFAULT_QM_V,                               // qm_v
+      DEFAULT_QM_FIRST,                           // qm_min
+      DEFAULT_QM_LAST,                            // qm_max
+      1,                                          // max number of tile groups
+      0,                                          // mtu_size
+      AOM_TIMING_UNSPECIFIED,       // No picture timing signaling in bitstream
+      0,                            // frame_parallel_decoding_mode
+      1,                            // enable dual filter
+      0,                            // enable delta quant in chroma planes
+      NO_AQ,                        // aq_mode
+      DELTA_Q_OBJECTIVE,            // deltaq_mode
+      100,                          // deltaq_strength
+      0,                            // delta lf mode
+      0,                            // frame_periodic_boost
+      AOM_CONTENT_DEFAULT,          // content
+      AOM_CICP_CP_UNSPECIFIED,      // CICP color primaries
+      AOM_CICP_TC_UNSPECIFIED,      // CICP transfer characteristics
+      AOM_CICP_MC_UNSPECIFIED,      // CICP matrix coefficients
+      AOM_CSP_UNKNOWN,              // chroma sample position
+      0,                            // color range
+      0,                            // render width
+      0,                            // render height
+      AOM_SUPERBLOCK_SIZE_DYNAMIC,  // superblock_size
+      1,                            // this depends on large_scale_tile.
+      0,                            // error_resilient_mode off by default.
+      0,                            // s_frame_mode off by default.
+      0,                            // film_grain_test_vector
+      NULL,                         // film_grain_table_filename
+      0,                            // motion_vector_unit_test
 #if CONFIG_FPMT_TEST
-  0,  // fpmt_unit_test
+      0,  // fpmt_unit_test
 #endif
-  1,    // CDF update mode
-  1,    // enable rectangular partitions
-  1,    // enable ab shape partitions
-  1,    // enable 1:4 and 4:1 partitions
-  4,    // min_partition_size
-  128,  // max_partition_size
-  1,    // enable intra edge filter
-  1,    // frame order hint
-  1,    // enable 64-pt transform usage
-  1,    // enable flip and identity transform
-  1,    // enable rectangular transform usage
-  1,    // dist-wtd compound
-  7,    // max_reference_frames
-  0,    // enable_reduced_reference_set
-  1,    // enable_ref_frame_mvs sequence level
-  1,    // allow ref_frame_mvs frame level
-  1,    // enable masked compound at sequence level
-  1,    // enable one sided compound at sequence level
-  1,    // enable interintra compound at sequence level
-  1,    // enable smooth interintra mode
-  1,    // enable difference-weighted compound
-  1,    // enable interinter wedge compound
-  1,    // enable interintra wedge compound
-  0,    // enable_global_motion usage
-  0,    // enable_warped_motion at sequence level
-  0,    // allow_warped_motion at frame level
-  1,    // enable filter intra at sequence level
-  1,    // enable smooth intra modes usage for sequence
-  1,    // enable Paeth intra mode usage for sequence
-  1,    // enable CFL uv intra mode usage for sequence
-  1,    // enable directional intra mode usage for sequence
-  1,    // enable D45 to D203 intra mode usage for sequence
-  1,    // superres
-  1,    // enable overlay
-  1,    // enable palette
-  1,    // enable intrabc
-  1,    // enable angle delta
+      1,    // CDF update mode
+      1,    // enable rectangular partitions
+      1,    // enable ab shape partitions
+      1,    // enable 1:4 and 4:1 partitions
+      4,    // min_partition_size
+      128,  // max_partition_size
+      1,    // enable intra edge filter
+      1,    // frame order hint
+      1,    // enable 64-pt transform usage
+      1,    // enable flip and identity transform
+      1,    // enable rectangular transform usage
+      1,    // dist-wtd compound
+      7,    // max_reference_frames
+      0,    // enable_reduced_reference_set
+      1,    // enable_ref_frame_mvs sequence level
+      1,    // allow ref_frame_mvs frame level
+      1,    // enable masked compound at sequence level
+      1,    // enable one sided compound at sequence level
+      1,    // enable interintra compound at sequence level
+      1,    // enable smooth interintra mode
+      1,    // enable difference-weighted compound
+      1,    // enable interinter wedge compound
+      1,    // enable interintra wedge compound
+      1,    // enable_global_motion usage
+      1,    // enable_warped_motion at sequence level
+      1,    // allow_warped_motion at frame level
+      1,    // enable filter intra at sequence level
+      1,    // enable smooth intra modes usage for sequence
+      1,    // enable Paeth intra mode usage for sequence
+      1,    // enable CFL uv intra mode usage for sequence
+      1,    // enable directional intra mode usage for sequence
+      1,    // enable D45 to D203 intra mode usage for sequence
+      1,    // superres
+      1,    // enable overlay
+      1,    // enable palette
+      1,    // enable intrabc
+      1,    // enable angle delta
 #if CONFIG_DENOISE
-  0,   // noise_level
-  32,  // noise_block_size
-  1,   // enable_dnl_denoising
+      0,   // noise_level
+      32,  // noise_block_size
+      1,   // enable_dnl_denoising
 #endif
-  0,  // chroma_subsampling_x
-  0,  // chroma_subsampling_y
-  0,  // reduced_tx_type_set
-  0,  // use_intra_dct_only
-  0,  // use_inter_dct_only
-  0,  // use_intra_default_tx_only
-  1,  // enable_tx_size_search
-  0,  // quant_b_adapt
-  0,  // vbr_corpus_complexity_lap
+      0,  // chroma_subsampling_x
+      0,  // chroma_subsampling_y
+      0,  // reduced_tx_type_set
+      0,  // use_intra_dct_only
+      0,  // use_inter_dct_only
+      0,  // use_intra_default_tx_only
+      1,  // enable_tx_size_search
+      0,  // quant_b_adapt
+      0,  // vbr_corpus_complexity_lap
+      {
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+      },               // target_seq_level_idx
+      0,               // tier_mask
+      0,               // min_cr
+      COST_UPD_SB,     // coeff_cost_upd_freq
+      COST_UPD_SB,     // mode_cost_upd_freq
+      COST_UPD_SB,     // mv_cost_upd_freq
+      COST_UPD_SB,     // dv_cost_upd_freq
+      0,               // ext_tile_debug
+      0,               // sb_multipass_unit_test
+      -1,              // passes
+      -1,              // fwd_kf_dist
+      LOOPFILTER_ALL,  // loopfilter_control
+      0,               // skip_postproc_filtering
+      NULL,            // two_pass_output
+      NULL,            // second_pass_log
+      0,               // auto_intra_tools_off
+      0,               // strict_level_conformance
+      -1,              // kf_max_pyr_height
+      0,               // sb_qp_sweep
+  },
+#endif  // !CONFIG_REALTIME_ONLY
   {
-      SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
-      SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
-      SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
-      SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
-      SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
-      SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
-      SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
-  },               // target_seq_level_idx
-  0,               // tier_mask
-  0,               // min_cr
-  COST_UPD_OFF,    // coeff_cost_upd_freq
-  COST_UPD_OFF,    // mode_cost_upd_freq
-  COST_UPD_OFF,    // mv_cost_upd_freq
-  COST_UPD_OFF,    // dv_cost_upd_freq
-  0,               // ext_tile_debug
-  0,               // sb_multipass_unit_test
-  -1,              // passes
-  -1,              // fwd_kf_dist
-  LOOPFILTER_ALL,  // loopfilter_control
-  0,               // skip_postproc_filtering
-  NULL,            // two_pass_output
-  NULL,            // second_pass_log
-  0,               // auto_intra_tools_off
-  0,               // strict_level_conformance
-  -1,              // kf_max_pyr_height
-  0,               // sb_qp_sweep
-};
-#else
-static const struct av1_extracfg default_extra_cfg = {
-  0,              // cpu_used
-  1,              // enable_auto_alt_ref
-  0,              // enable_auto_bwd_ref
-  0,              // noise_sensitivity
-  0,              // sharpness
-  0,              // static_thresh
-  1,              // row_mt
-  0,              // fp_mt
-  0,              // tile_columns
-  0,              // tile_rows
-  0,              // auto_tiles
-  1,              // enable_tpl_model
-  1,              // enable_keyframe_filtering
-  7,              // arnr_max_frames
-  5,              // arnr_strength
-  0,              // min_gf_interval; 0 -> default decision
-  0,              // max_gf_interval; 0 -> default decision
-  0,              // gf_min_pyr_height
-  5,              // gf_max_pyr_height
-  AOM_TUNE_PSNR,  // tuning
-  "/usr/local/share/model/vmaf_v0.6.1.json",  // VMAF model path
-  ".",                                        // partition info path
-  0,                                          // enable rate guide deltaq
-  "./rate_map.txt",                           // rate distribution input
-  AOM_DIST_METRIC_PSNR,                       // dist_metric
-  10,                                         // cq_level
-  0,                                          // rc_max_intra_bitrate_pct
-  0,                                          // rc_max_inter_bitrate_pct
-  0,                                          // gf_cbr_boost_pct
-  0,                                          // lossless
-  1,                                          // enable_cdef
-  1,                                          // enable_restoration
-  0,                                          // force_video_mode
-  1,                                          // enable_obmc
-  3,                                          // disable_trellis_quant
-  0,                                          // enable_qm
-  DEFAULT_QM_Y,                               // qm_y
-  DEFAULT_QM_U,                               // qm_u
-  DEFAULT_QM_V,                               // qm_v
-  DEFAULT_QM_FIRST,                           // qm_min
-  DEFAULT_QM_LAST,                            // qm_max
-  1,                                          // max number of tile groups
-  0,                                          // mtu_size
-  AOM_TIMING_UNSPECIFIED,       // No picture timing signaling in bitstream
-  0,                            // frame_parallel_decoding_mode
-  1,                            // enable dual filter
-  0,                            // enable delta quant in chroma planes
-  NO_AQ,                        // aq_mode
-  DELTA_Q_OBJECTIVE,            // deltaq_mode
-  100,                          // deltaq_strength
-  0,                            // delta lf mode
-  0,                            // frame_periodic_boost
-  AOM_CONTENT_DEFAULT,          // content
-  AOM_CICP_CP_UNSPECIFIED,      // CICP color primaries
-  AOM_CICP_TC_UNSPECIFIED,      // CICP transfer characteristics
-  AOM_CICP_MC_UNSPECIFIED,      // CICP matrix coefficients
-  AOM_CSP_UNKNOWN,              // chroma sample position
-  0,                            // color range
-  0,                            // render width
-  0,                            // render height
-  AOM_SUPERBLOCK_SIZE_DYNAMIC,  // superblock_size
-  1,                            // this depends on large_scale_tile.
-  0,                            // error_resilient_mode off by default.
-  0,                            // s_frame_mode off by default.
-  0,                            // film_grain_test_vector
-  NULL,                         // film_grain_table_filename
-  0,                            // motion_vector_unit_test
+      AOM_USAGE_REALTIME,  // usage
+      10,                  // cpu_used
+      1,                   // enable_auto_alt_ref
+      0,                   // enable_auto_bwd_ref
+      0,                   // noise_sensitivity
+      0,                   // sharpness
+      0,                   // static_thresh
+      1,                   // row_mt
+      0,                   // fp_mt
+      0,                   // tile_columns
+      0,                   // tile_rows
+      0,                   // auto_tiles
+      0,                   // enable_tpl_model
+      0,                   // enable_keyframe_filtering
+      7,                   // arnr_max_frames
+      5,                   // arnr_strength
+      0,                   // min_gf_interval; 0 -> default decision
+      0,                   // max_gf_interval; 0 -> default decision
+      0,                   // gf_min_pyr_height
+      5,                   // gf_max_pyr_height
+      AOM_TUNE_PSNR,       // tuning
+      "/usr/local/share/model/vmaf_v0.6.1.json",  // VMAF model path
+      ".",                                        // partition info path
+      0,                                          // enable rate guide deltaq
+      "./rate_map.txt",                           // rate distribution input
+      AOM_DIST_METRIC_PSNR,                       // dist_metric
+      10,                                         // cq_level
+      300,                                        // rc_max_intra_bitrate_pct
+      0,                                          // rc_max_inter_bitrate_pct
+      0,                                          // gf_cbr_boost_pct
+      0,                                          // lossless
+      1,                                          // enable_cdef
+      0,                                          // enable_restoration
+      0,                                          // force_video_mode
+      0,                                          // enable_obmc
+      3,                                          // disable_trellis_quant
+      0,                                          // enable_qm
+      DEFAULT_QM_Y,                               // qm_y
+      DEFAULT_QM_U,                               // qm_u
+      DEFAULT_QM_V,                               // qm_v
+      DEFAULT_QM_FIRST,                           // qm_min
+      DEFAULT_QM_LAST,                            // qm_max
+      1,                                          // max number of tile groups
+      0,                                          // mtu_size
+      AOM_TIMING_UNSPECIFIED,       // No picture timing signaling in bitstream
+      0,                            // frame_parallel_decoding_mode
+      0,                            // enable dual filter
+      0,                            // enable delta quant in chroma planes
+      CYCLIC_REFRESH_AQ,            // aq_mode
+      NO_DELTA_Q,                   // deltaq_mode
+      100,                          // deltaq_strength
+      0,                            // delta lf mode
+      0,                            // frame_periodic_boost
+      AOM_CONTENT_DEFAULT,          // content
+      AOM_CICP_CP_UNSPECIFIED,      // CICP color primaries
+      AOM_CICP_TC_UNSPECIFIED,      // CICP transfer characteristics
+      AOM_CICP_MC_UNSPECIFIED,      // CICP matrix coefficients
+      AOM_CSP_UNKNOWN,              // chroma sample position
+      0,                            // color range
+      0,                            // render width
+      0,                            // render height
+      AOM_SUPERBLOCK_SIZE_DYNAMIC,  // superblock_size
+      1,                            // this depends on large_scale_tile.
+      0,                            // error_resilient_mode off by default.
+      0,                            // s_frame_mode off by default.
+      0,                            // film_grain_test_vector
+      NULL,                         // film_grain_table_filename
+      0,                            // motion_vector_unit_test
 #if CONFIG_FPMT_TEST
-  0,                            // fpmt_unit_test
+      0,  // fpmt_unit_test
 #endif
-  1,                            // CDF update mode
-  1,                            // enable rectangular partitions
-  1,                            // enable ab shape partitions
-  1,                            // enable 1:4 and 4:1 partitions
-  4,                            // min_partition_size
-  128,                          // max_partition_size
-  1,                            // enable intra edge filter
-  1,                            // frame order hint
-  1,                            // enable 64-pt transform usage
-  1,                            // enable flip and identity transform
-  1,                            // enable rectangular transform usage
-  1,                            // dist-wtd compound
-  7,                            // max_reference_frames
-  0,                            // enable_reduced_reference_set
-  1,                            // enable_ref_frame_mvs sequence level
-  1,                            // allow ref_frame_mvs frame level
-  1,                            // enable masked compound at sequence level
-  1,                            // enable one sided compound at sequence level
-  1,                            // enable interintra compound at sequence level
-  1,                            // enable smooth interintra mode
-  1,                            // enable difference-weighted compound
-  1,                            // enable interinter wedge compound
-  1,                            // enable interintra wedge compound
-  1,                            // enable_global_motion usage
-  1,                            // enable_warped_motion at sequence level
-  1,                            // allow_warped_motion at frame level
-  1,                            // enable filter intra at sequence level
-  1,                            // enable smooth intra modes usage for sequence
-  1,                            // enable Paeth intra mode usage for sequence
-  1,                            // enable CFL uv intra mode usage for sequence
-  1,   // enable directional intra mode usage for sequence
-  1,   // enable D45 to D203 intra mode usage for sequence
-  1,   // superres
-  1,   // enable overlay
-  1,   // enable palette
-  1,   // enable intrabc
-  1,   // enable angle delta
+      1,    // CDF update mode
+      0,    // enable rectangular partitions
+      0,    // enable ab shape partitions
+      0,    // enable 1:4 and 4:1 partitions
+      4,    // min_partition_size
+      128,  // max_partition_size
+      0,    // enable intra edge filter
+      0,    // frame order hint
+      0,    // enable 64-pt transform usage
+      1,    // enable flip and identity transform
+      1,    // enable rectangular transform usage
+      0,    // dist-wtd compound
+      3,    // max_reference_frames
+      0,    // enable_reduced_reference_set
+      0,    // enable_ref_frame_mvs sequence level
+      0,    // allow ref_frame_mvs frame level
+      0,    // enable masked compound at sequence level
+      0,    // enable one sided compound at sequence level
+      0,    // enable interintra compound at sequence level
+      0,    // enable smooth interintra mode
+      0,    // enable difference-weighted compound
+      0,    // enable interinter wedge compound
+      0,    // enable interintra wedge compound
+      0,    // enable_global_motion usage
+      0,    // enable_warped_motion at sequence level
+      0,    // allow_warped_motion at frame level
+      0,    // enable filter intra at sequence level
+      0,    // enable smooth intra modes usage for sequence
+      0,    // enable Paeth intra mode usage for sequence
+      0,    // enable CFL uv intra mode usage for sequence
+      1,    // enable directional intra mode usage for sequence
+      1,    // enable D45 to D203 intra mode usage for sequence
+      0,    // superres
+      0,    // enable overlay
+      1,    // enable palette
+      0,    // enable intrabc
+      0,    // enable angle delta
 #if CONFIG_DENOISE
-  0,   // noise_level
-  32,  // noise_block_size
-  1,   // enable_dnl_denoising
+      0,   // noise_level
+      32,  // noise_block_size
+      1,   // enable_dnl_denoising
 #endif
-  0,   // chroma_subsampling_x
-  0,   // chroma_subsampling_y
-  0,   // reduced_tx_type_set
-  0,   // use_intra_dct_only
-  0,   // use_inter_dct_only
-  0,   // use_intra_default_tx_only
-  1,   // enable_tx_size_search
-  0,   // quant_b_adapt
-  0,   // vbr_corpus_complexity_lap
-  {
-      SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
-      SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
-      SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
-      SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
-      SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
-      SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
-      SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
-  },               // target_seq_level_idx
-  0,               // tier_mask
-  0,               // min_cr
-  COST_UPD_SB,     // coeff_cost_upd_freq
-  COST_UPD_SB,     // mode_cost_upd_freq
-  COST_UPD_SB,     // mv_cost_upd_freq
-  COST_UPD_SB,     // dv_cost_upd_freq
-  0,               // ext_tile_debug
-  0,               // sb_multipass_unit_test
-  -1,              // passes
-  -1,              // fwd_kf_dist
-  LOOPFILTER_ALL,  // loopfilter_control
-  0,               // skip_postproc_filtering
-  NULL,            // two_pass_output
-  NULL,            // second_pass_log
-  0,               // auto_intra_tools_off
-  0,               // strict_level_conformance
-  -1,              // kf_max_pyr_height
-  0,               // sb_qp_sweep
+      0,  // chroma_subsampling_x
+      0,  // chroma_subsampling_y
+      0,  // reduced_tx_type_set
+      0,  // use_intra_dct_only
+      0,  // use_inter_dct_only
+      1,  // use_intra_default_tx_only
+      1,  // enable_tx_size_search
+      0,  // quant_b_adapt
+      0,  // vbr_corpus_complexity_lap
+      {
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+          SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX, SEQ_LEVEL_MAX,
+      },               // target_seq_level_idx
+      0,               // tier_mask
+      0,               // min_cr
+      COST_UPD_OFF,    // coeff_cost_upd_freq
+      COST_UPD_OFF,    // mode_cost_upd_freq
+      COST_UPD_OFF,    // mv_cost_upd_freq
+      COST_UPD_OFF,    // dv_cost_upd_freq
+      0,               // ext_tile_debug
+      0,               // sb_multipass_unit_test
+      -1,              // passes
+      -1,              // fwd_kf_dist
+      LOOPFILTER_ALL,  // loopfilter_control
+      0,               // skip_postproc_filtering
+      NULL,            // two_pass_output
+      NULL,            // second_pass_log
+      0,               // auto_intra_tools_off
+      0,               // strict_level_conformance
+      -1,              // kf_max_pyr_height
+      0,               // sb_qp_sweep
+  },
 };
-#endif
 
 struct aom_codec_alg_priv {
   aom_codec_priv_t base;
@@ -1531,7 +1524,7 @@ static void set_encoder_config(AV1EncoderConfig *oxcf,
 
 AV1EncoderConfig av1_get_encoder_config(const aom_codec_enc_cfg_t *cfg) {
   AV1EncoderConfig oxcf;
-  struct av1_extracfg extra_cfg = default_extra_cfg;
+  struct av1_extracfg extra_cfg = default_extra_cfg[0];
   set_encoder_config(&oxcf, cfg, &extra_cfg);
   return oxcf;
 }
@@ -2190,6 +2183,11 @@ static aom_codec_err_t ctrl_set_enable_cfl_intra(aom_codec_alg_priv_t *ctx,
                                                  va_list args) {
   struct av1_extracfg extra_cfg = ctx->extra_cfg;
   extra_cfg.enable_cfl_intra = CAST(AV1E_SET_ENABLE_CFL_INTRA, args);
+#if CONFIG_REALTIME_ONLY
+  if (extra_cfg.enable_cfl_intra) {
+    ERROR("cfl can't be turned on in realtime only build.");
+  }
+#endif
   return update_extra_cfg(ctx, &extra_cfg);
 }
 
@@ -2356,7 +2354,7 @@ static aom_codec_err_t ctrl_set_vmaf_model_path(aom_codec_alg_priv_t *ctx,
   struct av1_extracfg extra_cfg = ctx->extra_cfg;
   const char *str = CAST(AV1E_SET_VMAF_MODEL_PATH, args);
   const aom_codec_err_t ret = allocate_and_set_string(
-      str, default_extra_cfg.vmaf_model_path, &extra_cfg.vmaf_model_path,
+      str, default_extra_cfg[0].vmaf_model_path, &extra_cfg.vmaf_model_path,
       ctx->ppi->error.detail);
   if (ret != AOM_CODEC_OK) return ret;
   return update_extra_cfg(ctx, &extra_cfg);
@@ -2367,7 +2365,7 @@ static aom_codec_err_t ctrl_set_partition_info_path(aom_codec_alg_priv_t *ctx,
   struct av1_extracfg extra_cfg = ctx->extra_cfg;
   const char *str = CAST(AV1E_SET_PARTITION_INFO_PATH, args);
   const aom_codec_err_t ret = allocate_and_set_string(
-      str, default_extra_cfg.partition_info_path,
+      str, default_extra_cfg[0].partition_info_path,
       &extra_cfg.partition_info_path, ctx->ppi->error.detail);
   if (ret != AOM_CODEC_OK) return ret;
   return update_extra_cfg(ctx, &extra_cfg);
@@ -2386,7 +2384,7 @@ static aom_codec_err_t ctrl_set_rate_distribution_info(
   struct av1_extracfg extra_cfg = ctx->extra_cfg;
   const char *str = CAST(AV1E_SET_RATE_DISTRIBUTION_INFO, args);
   const aom_codec_err_t ret = allocate_and_set_string(
-      str, default_extra_cfg.rate_distribution_info,
+      str, default_extra_cfg[0].rate_distribution_info,
       &extra_cfg.rate_distribution_info, ctx->ppi->error.detail);
   if (ret != AOM_CODEC_OK) return ret;
   return update_extra_cfg(ctx, &extra_cfg);
@@ -2408,8 +2406,11 @@ static aom_codec_err_t ctrl_set_film_grain_table(aom_codec_alg_priv_t *ctx,
     // this parameter allows NULL as its value
     extra_cfg.film_grain_table_filename = str;
   } else {
+#if CONFIG_REALTIME_ONLY
+    ERROR("film_grain removed from realtime only build.");
+#endif
     const aom_codec_err_t ret = allocate_and_set_string(
-        str, default_extra_cfg.film_grain_table_filename,
+        str, default_extra_cfg[0].film_grain_table_filename,
         &extra_cfg.film_grain_table_filename, ctx->ppi->error.detail);
     if (ret != AOM_CODEC_OK) return ret;
   }
@@ -2838,7 +2839,7 @@ static aom_codec_err_t encoder_init(aom_codec_ctx_t *ctx) {
     priv->cfg = *ctx->config.enc;
     ctx->config.enc = &priv->cfg;
 
-    priv->extra_cfg = default_extra_cfg;
+    priv->extra_cfg = default_extra_cfg[0];
     // Special handling:
     // By default, if omitted, --enable-cdef = 1.
     // Here we set its default value to 0 when --allintra is turned on.
@@ -2952,18 +2953,18 @@ static void check_and_free_string(const char *default_str, const char **ptr) {
 
 static void destroy_extra_config(struct av1_extracfg *extra_cfg) {
 #if CONFIG_TUNE_VMAF
-  check_and_free_string(default_extra_cfg.vmaf_model_path,
+  check_and_free_string(default_extra_cfg[0].vmaf_model_path,
                         &extra_cfg->vmaf_model_path);
 #endif
-  check_and_free_string(default_extra_cfg.two_pass_output,
+  check_and_free_string(default_extra_cfg[0].two_pass_output,
                         &extra_cfg->two_pass_output);
-  check_and_free_string(default_extra_cfg.two_pass_output,
+  check_and_free_string(default_extra_cfg[0].two_pass_output,
                         &extra_cfg->second_pass_log);
-  check_and_free_string(default_extra_cfg.partition_info_path,
+  check_and_free_string(default_extra_cfg[0].partition_info_path,
                         &extra_cfg->partition_info_path);
-  check_and_free_string(default_extra_cfg.rate_distribution_info,
+  check_and_free_string(default_extra_cfg[0].rate_distribution_info,
                         &extra_cfg->rate_distribution_info);
-  check_and_free_string(default_extra_cfg.film_grain_table_filename,
+  check_and_free_string(default_extra_cfg[0].film_grain_table_filename,
                         &extra_cfg->film_grain_table_filename);
 }
 
@@ -3097,9 +3098,10 @@ static aom_codec_err_t encoder_encode(aom_codec_alg_priv_t *ctx,
         }
       }
       for (int i = 0; i < ppi->num_fp_contexts - 1; i++) {
-        if (ppi->parallel_frames_data[i].cx_data == NULL) {
-          ppi->parallel_frames_data[i].cx_data_sz = uncompressed_frame_sz;
-          ppi->parallel_frames_data[i].frame_display_order_hint = -1;
+        if (ppi->parallel_frames_data[i].cx_data == NULL ||
+            ppi->parallel_frames_data[i].cx_data_sz < data_sz) {
+          ppi->parallel_frames_data[i].cx_data_sz = data_sz;
+          free(ppi->parallel_frames_data[i].cx_data);
           ppi->parallel_frames_data[i].frame_size = 0;
           ppi->parallel_frames_data[i].cx_data =
               (unsigned char *)malloc(ppi->parallel_frames_data[i].cx_data_sz);
@@ -3435,6 +3437,10 @@ static aom_codec_err_t encoder_encode(aom_codec_alg_priv_t *ctx,
 
       if (!cpi_data.frame_size) continue;
       assert(cpi_data.cx_data != NULL && cpi_data.cx_data_sz != 0);
+      if (cpi_data.frame_size > cpi_data.cx_data_sz) {
+        aom_internal_error(&ppi->error, AOM_CODEC_ERROR,
+                           "cpi_data.cx_data buffer overflow");
+      }
       const int write_temporal_delimiter =
           !cpi->common.spatial_layer_id && !ctx->pending_cx_data_sz;
 
@@ -3445,37 +3451,48 @@ static aom_codec_err_t encoder_encode(aom_codec_alg_priv_t *ctx,
             aom_uleb_size_in_bytes(obu_payload_size);
 
         const size_t move_offset = obu_header_size + length_field_size;
+        assert(ctx->cx_data_sz == cpi_data.cx_data_sz);
+        if (move_offset > ctx->cx_data_sz - cpi_data.frame_size) {
+          aom_internal_error(&ppi->error, AOM_CODEC_ERROR,
+                             "ctx->cx_data buffer full");
+        }
         memmove(ctx->cx_data + move_offset, ctx->cx_data, cpi_data.frame_size);
         obu_header_size = av1_write_obu_header(
             &ppi->level_params, &cpi->frame_header_count,
             OBU_TEMPORAL_DELIMITER,
             ppi->seq_params.has_nonzero_operating_point_idc, 0, ctx->cx_data);
+        if (obu_header_size != 1) {
+          aom_internal_error(&ppi->error, AOM_CODEC_ERROR, NULL);
+        }
 
         // OBUs are preceded/succeeded by an unsigned leb128 coded integer.
-        if (av1_write_uleb_obu_size(obu_header_size, obu_payload_size,
-                                    ctx->cx_data) != AOM_CODEC_OK) {
+        if (av1_write_uleb_obu_size(obu_payload_size,
+                                    ctx->cx_data + obu_header_size,
+                                    length_field_size) != AOM_CODEC_OK) {
           aom_internal_error(&ppi->error, AOM_CODEC_ERROR, NULL);
         }
 
-        cpi_data.frame_size +=
-            obu_header_size + obu_payload_size + length_field_size;
+        cpi_data.frame_size += move_offset;
       }
 
       if (ctx->oxcf.save_as_annexb) {
-        size_t curr_frame_size = cpi_data.frame_size;
-        if (av1_convert_sect5obus_to_annexb(cpi_data.cx_data,
-                                            &curr_frame_size) != AOM_CODEC_OK) {
+        if (av1_convert_sect5obus_to_annexb(
+                cpi_data.cx_data, cpi_data.cx_data_sz, &cpi_data.frame_size) !=
+            AOM_CODEC_OK) {
           aom_internal_error(&ppi->error, AOM_CODEC_ERROR, NULL);
         }
-        cpi_data.frame_size = curr_frame_size;
 
         // B_PRIME (add frame size)
         const size_t length_field_size =
             aom_uleb_size_in_bytes(cpi_data.frame_size);
+        if (length_field_size > cpi_data.cx_data_sz - cpi_data.frame_size) {
+          aom_internal_error(&ppi->error, AOM_CODEC_ERROR,
+                             "cpi_data.cx_data buffer full");
+        }
         memmove(cpi_data.cx_data + length_field_size, cpi_data.cx_data,
                 cpi_data.frame_size);
-        if (av1_write_uleb_obu_size(0, (uint32_t)cpi_data.frame_size,
-                                    cpi_data.cx_data) != AOM_CODEC_OK) {
+        if (av1_write_uleb_obu_size(cpi_data.frame_size, cpi_data.cx_data,
+                                    length_field_size) != AOM_CODEC_OK) {
           aom_internal_error(&ppi->error, AOM_CODEC_ERROR, NULL);
         }
         cpi_data.frame_size += length_field_size;
@@ -3502,8 +3519,16 @@ static aom_codec_err_t encoder_encode(aom_codec_alg_priv_t *ctx,
         //  B_PRIME (add TU size)
         size_t tu_size = ctx->pending_cx_data_sz;
         const size_t length_field_size = aom_uleb_size_in_bytes(tu_size);
+        if (tu_size > ctx->cx_data_sz) {
+          aom_internal_error(&ppi->error, AOM_CODEC_ERROR,
+                             "ctx->cx_data buffer overflow");
+        }
+        if (length_field_size > ctx->cx_data_sz - tu_size) {
+          aom_internal_error(&ppi->error, AOM_CODEC_ERROR,
+                             "ctx->cx_data buffer full");
+        }
         memmove(ctx->cx_data + length_field_size, ctx->cx_data, tu_size);
-        if (av1_write_uleb_obu_size(0, (uint32_t)tu_size, ctx->cx_data) !=
+        if (av1_write_uleb_obu_size(tu_size, ctx->cx_data, length_field_size) !=
             AOM_CODEC_OK) {
           aom_internal_error(&ppi->error, AOM_CODEC_ERROR, NULL);
         }
@@ -3526,9 +3551,13 @@ static aom_codec_err_t encoder_encode(aom_codec_alg_priv_t *ctx,
         // the delayed random access point flag.
         pkt.data.frame.flags |= AOM_FRAME_IS_DELAYED_RANDOM_ACCESS_POINT;
       }
-      pkt.data.frame.duration = (uint32_t)ticks_to_timebase_units(
+      const int64_t duration64 = ticks_to_timebase_units(
           cpi_data.timestamp_ratio,
           cpi_data.ts_frame_end - cpi_data.ts_frame_start);
+      if (duration64 > UINT32_MAX) {
+        aom_internal_error(&ppi->error, AOM_CODEC_ERROR, NULL);
+      }
+      pkt.data.frame.duration = (uint32_t)duration64;
 
       aom_codec_pkt_list_add(&ctx->pkt_list.head, &pkt);
 
@@ -3839,11 +3868,18 @@ static aom_codec_err_t ctrl_set_svc_ref_frame_config(aom_codec_alg_priv_t *ctx,
       va_arg(args, aom_svc_ref_frame_config_t *);
   cpi->ppi->rtc_ref.set_ref_frame_config = 1;
   for (unsigned int i = 0; i < INTER_REFS_PER_FRAME; ++i) {
+    if (data->reference[i] != 0 && data->reference[i] != 1)
+      return AOM_CODEC_INVALID_PARAM;
+    if (data->ref_idx[i] > 7 || data->ref_idx[i] < 0)
+      return AOM_CODEC_INVALID_PARAM;
     cpi->ppi->rtc_ref.reference[i] = data->reference[i];
     cpi->ppi->rtc_ref.ref_idx[i] = data->ref_idx[i];
   }
-  for (unsigned int i = 0; i < REF_FRAMES; ++i)
+  for (unsigned int i = 0; i < REF_FRAMES; ++i) {
+    if (data->refresh[i] != 0 && data->refresh[i] != 1)
+      return AOM_CODEC_INVALID_PARAM;
     cpi->ppi->rtc_ref.refresh[i] = data->refresh[i];
+  }
   cpi->svc.use_flexible_mode = 1;
   cpi->svc.ksvc_fixed_mode = 0;
   return AOM_CODEC_OK;
@@ -4041,14 +4077,15 @@ static aom_codec_err_t encoder_set_option(aom_codec_alg_priv_t *ctx,
 #if CONFIG_TUNE_VMAF
   else if (arg_match_helper(&arg, &g_av1_codec_arg_defs.vmaf_model_path, argv,
                             err_string)) {
-    err = allocate_and_set_string(value, default_extra_cfg.vmaf_model_path,
+    err = allocate_and_set_string(value, default_extra_cfg[0].vmaf_model_path,
                                   &extra_cfg.vmaf_model_path, err_string);
   }
 #endif
   else if (arg_match_helper(&arg, &g_av1_codec_arg_defs.partition_info_path,
                             argv, err_string)) {
-    err = allocate_and_set_string(value, default_extra_cfg.partition_info_path,
-                                  &extra_cfg.partition_info_path, err_string);
+    err =
+        allocate_and_set_string(value, default_extra_cfg[0].partition_info_path,
+                                &extra_cfg.partition_info_path, err_string);
   } else if (arg_match_helper(&arg,
                               &g_av1_codec_arg_defs.enable_rate_guide_deltaq,
                               argv, err_string)) {
@@ -4057,9 +4094,9 @@ static aom_codec_err_t encoder_set_option(aom_codec_alg_priv_t *ctx,
   } else if (arg_match_helper(&arg,
                               &g_av1_codec_arg_defs.rate_distribution_info,
                               argv, err_string)) {
-    err =
-        allocate_and_set_string(value, default_extra_cfg.rate_distribution_info,
-                                &extra_cfg.rate_distribution_info, err_string);
+    err = allocate_and_set_string(
+        value, default_extra_cfg[0].rate_distribution_info,
+        &extra_cfg.rate_distribution_info, err_string);
   } else if (arg_match_helper(&arg, &g_av1_codec_arg_defs.dist_metric, argv,
                               err_string)) {
     extra_cfg.dist_metric = arg_parse_enum_helper(&arg, err_string);
@@ -4177,7 +4214,7 @@ static aom_codec_err_t encoder_set_option(aom_codec_alg_priv_t *ctx,
       extra_cfg.film_grain_table_filename = value;
     } else {
       err = allocate_and_set_string(
-          value, default_extra_cfg.film_grain_table_filename,
+          value, default_extra_cfg[0].film_grain_table_filename,
           &extra_cfg.film_grain_table_filename, err_string);
     }
   } else if (arg_match_helper(&arg, &g_av1_codec_arg_defs.cdf_update_mode, argv,
@@ -4375,11 +4412,11 @@ static aom_codec_err_t encoder_set_option(aom_codec_alg_priv_t *ctx,
     extra_cfg.fwd_kf_dist = arg_parse_int_helper(&arg, err_string);
   } else if (arg_match_helper(&arg, &g_av1_codec_arg_defs.two_pass_output, argv,
                               err_string)) {
-    err = allocate_and_set_string(value, default_extra_cfg.two_pass_output,
+    err = allocate_and_set_string(value, default_extra_cfg[0].two_pass_output,
                                   &extra_cfg.two_pass_output, err_string);
   } else if (arg_match_helper(&arg, &g_av1_codec_arg_defs.second_pass_log, argv,
                               err_string)) {
-    err = allocate_and_set_string(value, default_extra_cfg.second_pass_log,
+    err = allocate_and_set_string(value, default_extra_cfg[0].second_pass_log,
                                   &extra_cfg.second_pass_log, err_string);
   } else if (arg_match_helper(&arg, &g_av1_codec_arg_defs.loopfilter_control,
                               argv, err_string)) {
@@ -4757,12 +4794,12 @@ static const aom_codec_enc_cfg_t encoder_usage_cfg[] = {
       256,          // rc_target_bitrate
       0,            // rc_min_quantizer
       63,           // rc_max_quantizer
-      25,           // rc_undershoot_pct
-      25,           // rc_overshoot_pct
+      50,           // rc_undershoot_pct
+      50,           // rc_overshoot_pct
 
-      6000,  // rc_buf_sz
-      4000,  // rc_buf_initial_sz
-      5000,  // rc_buf_optimal_sz
+      1000,  // rc_buf_sz
+      600,   // rc_buf_initial_sz
+      600,   // rc_buf_optimal_sz
 
       50,    // rc_2pass_vbr_bias_pct
       0,     // rc_2pass_vbr_minsection_pct
diff --git a/av1/common/alloccommon.c b/av1/common/alloccommon.c
index 2a28c301f..38b9e6464 100644
--- a/av1/common/alloccommon.c
+++ b/av1/common/alloccommon.c
@@ -294,6 +294,7 @@ void av1_alloc_cdef_buffers(AV1_COMMON *const cm,
                       cdef_info->allocated_mi_rows);
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 // Allocate buffers which are independent of restoration_unit_size
 void av1_alloc_restoration_buffers(AV1_COMMON *cm, bool is_sgr_enabled) {
   const int num_planes = av1_num_planes(cm);
@@ -364,6 +365,7 @@ void av1_free_restoration_buffers(AV1_COMMON *cm) {
 
   aom_free_frame_buffer(&cm->rst_frame);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 void av1_free_above_context_buffers(CommonContexts *above_contexts) {
   int i;
diff --git a/av1/common/alloccommon.h b/av1/common/alloccommon.h
index ccbf75789..7b74ee02e 100644
--- a/av1/common/alloccommon.h
+++ b/av1/common/alloccommon.h
@@ -50,8 +50,10 @@ void av1_alloc_cdef_buffers(struct AV1Common *const cm,
 void av1_free_cdef_buffers(struct AV1Common *const cm,
                            struct AV1CdefWorker **cdef_worker,
                            struct AV1CdefSyncData *cdef_sync);
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void av1_alloc_restoration_buffers(struct AV1Common *cm, bool is_sgr_enabled);
 void av1_free_restoration_buffers(struct AV1Common *cm);
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 int av1_alloc_state_buffers(struct AV1Common *cm, int width, int height);
 void av1_free_state_buffers(struct AV1Common *cm);
diff --git a/av1/common/arm/cdef_block_neon.c b/av1/common/arm/cdef_block_neon.c
index be6df922e..418314469 100644
--- a/av1/common/arm/cdef_block_neon.c
+++ b/av1/common/arm/cdef_block_neon.c
@@ -50,6 +50,7 @@ void cdef_copy_rect8_8bit_to_16bit_neon(uint16_t *dst, int dstride,
   } while (--height != 0);
 }
 
+#if CONFIG_AV1_HIGHBITDEPTH
 void cdef_copy_rect8_16bit_to_16bit_neon(uint16_t *dst, int dstride,
                                          const uint16_t *src, int sstride,
                                          int width, int height) {
@@ -73,6 +74,7 @@ void cdef_copy_rect8_16bit_to_16bit_neon(uint16_t *dst, int dstride,
     dst += dstride;
   } while (--height != 0);
 }
+#endif  // CONFIG_AV1_HIGHBITDEPTH
 
 // partial A is a 16-bit vector of the form:
 // [x8 x7 x6 x5 x4 x3 x2 x1] and partial B has the form:
diff --git a/av1/common/arm/resize_neon.c b/av1/common/arm/resize_neon.c
index fea7d1db0..b6fb11057 100644
--- a/av1/common/arm/resize_neon.c
+++ b/av1/common/arm/resize_neon.c
@@ -9,410 +9,335 @@
  * Media Patent License 1.0 was not distributed with this source code in the
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
+
 #include <arm_neon.h>
 #include <assert.h>
 
 #include "aom_dsp/arm/mem_neon.h"
 #include "aom_dsp/arm/transpose_neon.h"
+#include "av1/common/arm/resize_neon.h"
 #include "av1/common/resize.h"
-#include "config/av1_rtcd.h"
 #include "config/aom_scale_rtcd.h"
-
-static inline uint8x8_t scale_filter_8(const uint8x8_t *const s,
-                                       const int16x8_t filter) {
-  const int16x4_t filter_lo = vget_low_s16(filter);
-  const int16x4_t filter_hi = vget_high_s16(filter);
-
-  int16x8_t ss0 = vreinterpretq_s16_u16(vmovl_u8(s[0]));
-  int16x8_t ss1 = vreinterpretq_s16_u16(vmovl_u8(s[1]));
-  int16x8_t ss2 = vreinterpretq_s16_u16(vmovl_u8(s[2]));
-  int16x8_t ss3 = vreinterpretq_s16_u16(vmovl_u8(s[3]));
-  int16x8_t ss4 = vreinterpretq_s16_u16(vmovl_u8(s[4]));
-  int16x8_t ss5 = vreinterpretq_s16_u16(vmovl_u8(s[5]));
-  int16x8_t ss6 = vreinterpretq_s16_u16(vmovl_u8(s[6]));
-  int16x8_t ss7 = vreinterpretq_s16_u16(vmovl_u8(s[7]));
-
-  int16x8_t sum = vmulq_lane_s16(ss0, filter_lo, 0);
-  sum = vmlaq_lane_s16(sum, ss1, filter_lo, 1);
-  sum = vmlaq_lane_s16(sum, ss2, filter_lo, 2);
-  sum = vmlaq_lane_s16(sum, ss5, filter_hi, 1);
-  sum = vmlaq_lane_s16(sum, ss6, filter_hi, 2);
-  sum = vmlaq_lane_s16(sum, ss7, filter_hi, 3);
-  sum = vqaddq_s16(sum, vmulq_lane_s16(ss3, filter_lo, 3));
-  sum = vqaddq_s16(sum, vmulq_lane_s16(ss4, filter_hi, 0));
-
-  return vqrshrun_n_s16(sum, FILTER_BITS);
-}
+#include "config/av1_rtcd.h"
 
 static inline void scale_plane_2_to_1_phase_0(const uint8_t *src,
                                               const int src_stride,
                                               uint8_t *dst,
-                                              const int dst_stride, const int w,
-                                              const int h) {
-  const int max_width = (w + 15) & ~15;
-  int y = h;
-
-  assert(w && h);
+                                              const int dst_stride, int w,
+                                              int h) {
+  assert(w > 0 && h > 0);
 
   do {
-    int x = max_width;
+    const uint8_t *s = src;
+    uint8_t *d = dst;
+    int width = w;
+
     do {
-      const uint8x16x2_t s = vld2q_u8(src);
-      vst1q_u8(dst, s.val[0]);
-      src += 32;
-      dst += 16;
-      x -= 16;
-    } while (x);
-    src += 2 * (src_stride - max_width);
-    dst += dst_stride - max_width;
-  } while (--y);
+      const uint8x16x2_t s0 = vld2q_u8(s);
+
+      vst1q_u8(d, s0.val[0]);
+
+      s += 32;
+      d += 16;
+      width -= 16;
+    } while (width > 0);
+
+    src += 2 * src_stride;
+    dst += dst_stride;
+  } while (--h != 0);
 }
 
 static inline void scale_plane_4_to_1_phase_0(const uint8_t *src,
                                               const int src_stride,
                                               uint8_t *dst,
-                                              const int dst_stride, const int w,
-                                              const int h) {
-  const int max_width = (w + 15) & ~15;
-  int y = h;
-
-  assert(w && h);
+                                              const int dst_stride, int w,
+                                              int h) {
+  assert(w > 0 && h > 0);
 
   do {
-    int x = max_width;
+    const uint8_t *s = src;
+    uint8_t *d = dst;
+    int width = w;
+
     do {
-      const uint8x16x4_t s = vld4q_u8(src);
-      vst1q_u8(dst, s.val[0]);
-      src += 64;
-      dst += 16;
-      x -= 16;
-    } while (x);
-    src += 4 * (src_stride - max_width);
-    dst += dst_stride - max_width;
-  } while (--y);
+      const uint8x16x4_t s0 = vld4q_u8(s);
+
+      vst1q_u8(d, s0.val[0]);
+
+      s += 64;
+      d += 16;
+      width -= 16;
+    } while (width > 0);
+
+    src += 4 * src_stride;
+    dst += dst_stride;
+  } while (--h != 0);
 }
 
-static inline void scale_plane_bilinear_kernel(
-    const uint8x16_t in0, const uint8x16_t in1, const uint8x16_t in2,
-    const uint8x16_t in3, const uint8x8_t coef0, const uint8x8_t coef1,
-    uint8_t *const dst) {
-  const uint16x8_t h0 = vmull_u8(vget_low_u8(in0), coef0);
-  const uint16x8_t h1 = vmull_u8(vget_high_u8(in0), coef0);
-  const uint16x8_t h2 = vmull_u8(vget_low_u8(in2), coef0);
-  const uint16x8_t h3 = vmull_u8(vget_high_u8(in2), coef0);
-  const uint16x8_t h4 = vmlal_u8(h0, vget_low_u8(in1), coef1);
-  const uint16x8_t h5 = vmlal_u8(h1, vget_high_u8(in1), coef1);
-  const uint16x8_t h6 = vmlal_u8(h2, vget_low_u8(in3), coef1);
-  const uint16x8_t h7 = vmlal_u8(h3, vget_high_u8(in3), coef1);
-
-  const uint8x8_t hor0 = vrshrn_n_u16(h4, 7);  // temp: 00 01 02 03 04 05 06 07
-  const uint8x8_t hor1 = vrshrn_n_u16(h5, 7);  // temp: 08 09 0A 0B 0C 0D 0E 0F
-  const uint8x8_t hor2 = vrshrn_n_u16(h6, 7);  // temp: 10 11 12 13 14 15 16 17
-  const uint8x8_t hor3 = vrshrn_n_u16(h7, 7);  // temp: 18 19 1A 1B 1C 1D 1E 1F
-  const uint16x8_t v0 = vmull_u8(hor0, coef0);
-  const uint16x8_t v1 = vmull_u8(hor1, coef0);
-  const uint16x8_t v2 = vmlal_u8(v0, hor2, coef1);
-  const uint16x8_t v3 = vmlal_u8(v1, hor3, coef1);
-  // dst: 0 1 2 3 4 5 6 7  8 9 A B C D E F
-  const uint8x16_t d = vcombine_u8(vrshrn_n_u16(v2, 7), vrshrn_n_u16(v3, 7));
-  vst1q_u8(dst, d);
+static inline uint8x16_t scale_plane_bilinear_kernel(
+    const uint8x16_t s0_even, const uint8x16_t s0_odd, const uint8x16_t s1_even,
+    const uint8x16_t s1_odd, const uint8x8_t filter0, const uint8x8_t filter1) {
+  // A shim of 1 << (FILTER_BITS - 1) enables us to use non-rounding
+  // shifts - which are generally faster than rounding shifts on modern CPUs.
+  uint16x8_t offset = vdupq_n_u16(1 << (FILTER_BITS - 1));
+
+  // Horizontal filtering
+  uint16x8_t h0_lo = vmlal_u8(offset, vget_low_u8(s0_even), filter0);
+  uint16x8_t h0_hi = vmlal_u8(offset, vget_high_u8(s0_even), filter0);
+  uint16x8_t h1_lo = vmlal_u8(offset, vget_low_u8(s1_even), filter0);
+  uint16x8_t h1_hi = vmlal_u8(offset, vget_high_u8(s1_even), filter0);
+
+  h0_lo = vmlal_u8(h0_lo, vget_low_u8(s0_odd), filter1);
+  h0_hi = vmlal_u8(h0_hi, vget_high_u8(s0_odd), filter1);
+  h1_lo = vmlal_u8(h1_lo, vget_low_u8(s1_odd), filter1);
+  h1_hi = vmlal_u8(h1_hi, vget_high_u8(s1_odd), filter1);
+
+  const uint8x8_t h0_lo_u8 = vshrn_n_u16(h0_lo, FILTER_BITS);
+  const uint8x8_t h0_hi_u8 = vshrn_n_u16(h0_hi, FILTER_BITS);
+  const uint8x8_t h1_lo_u8 = vshrn_n_u16(h1_lo, FILTER_BITS);
+  const uint8x8_t h1_hi_u8 = vshrn_n_u16(h1_hi, FILTER_BITS);
+
+  // Vertical filtering
+  uint16x8_t v_lo = vmlal_u8(offset, h0_lo_u8, filter0);
+  uint16x8_t v_hi = vmlal_u8(offset, h0_hi_u8, filter0);
+
+  v_lo = vmlal_u8(v_lo, h1_lo_u8, filter1);
+  v_hi = vmlal_u8(v_hi, h1_hi_u8, filter1);
+
+  return vcombine_u8(vshrn_n_u16(v_lo, FILTER_BITS),
+                     vshrn_n_u16(v_hi, FILTER_BITS));
 }
 
 static inline void scale_plane_2_to_1_bilinear(
-    const uint8_t *const src, const int src_stride, uint8_t *dst,
-    const int dst_stride, const int w, const int h, const int16_t c0,
-    const int16_t c1) {
-  const int max_width = (w + 15) & ~15;
-  const uint8_t *src0 = src;
-  const uint8_t *src1 = src + src_stride;
-  const uint8x8_t coef0 = vdup_n_u8(c0);
-  const uint8x8_t coef1 = vdup_n_u8(c1);
-  int y = h;
-
-  assert(w && h);
+    const uint8_t *src, const int src_stride, uint8_t *dst,
+    const int dst_stride, int w, int h, const int16_t f0, const int16_t f1) {
+  assert(w > 0 && h > 0);
+  const uint8x8_t filter0 = vdup_n_u8(f0);
+  const uint8x8_t filter1 = vdup_n_u8(f1);
 
   do {
-    int x = max_width;
+    const uint8_t *s = src;
+    uint8_t *d = dst;
+    int width = w;
+
     do {
-      // 000 002 004 006 008 00A 00C 00E  010 012 014 016 018 01A 01C 01E
-      // 001 003 005 007 009 00B 00D 00F  011 013 015 017 019 01B 01D 01F
-      const uint8x16x2_t s0 = vld2q_u8(src0);
-      // 100 102 104 106 108 10A 10C 10E  110 112 114 116 118 11A 11C 11E
-      // 101 103 105 107 109 10B 10D 10F  111 113 115 117 119 11B 11D 11F
-      const uint8x16x2_t s1 = vld2q_u8(src1);
-      scale_plane_bilinear_kernel(s0.val[0], s0.val[1], s1.val[0], s1.val[1],
-                                  coef0, coef1, dst);
-      src0 += 32;
-      src1 += 32;
-      dst += 16;
-      x -= 16;
-    } while (x);
-    src0 += 2 * (src_stride - max_width);
-    src1 += 2 * (src_stride - max_width);
-    dst += dst_stride - max_width;
-  } while (--y);
+      const uint8x16x2_t s0 = vld2q_u8(s + 0 * src_stride);
+      const uint8x16x2_t s1 = vld2q_u8(s + 1 * src_stride);
+
+      uint8x16_t d0 = scale_plane_bilinear_kernel(
+          s0.val[0], s0.val[1], s1.val[0], s1.val[1], filter0, filter1);
+
+      vst1q_u8(d, d0);
+
+      s += 32;
+      d += 16;
+      width -= 16;
+    } while (width > 0);
+
+    src += 2 * src_stride;
+    dst += dst_stride;
+  } while (--h != 0);
 }
 
 static inline void scale_plane_4_to_1_bilinear(
-    const uint8_t *const src, const int src_stride, uint8_t *dst,
-    const int dst_stride, const int w, const int h, const int16_t c0,
-    const int16_t c1) {
-  const int max_width = (w + 15) & ~15;
-  const uint8_t *src0 = src;
-  const uint8_t *src1 = src + src_stride;
-  const uint8x8_t coef0 = vdup_n_u8(c0);
-  const uint8x8_t coef1 = vdup_n_u8(c1);
-  int y = h;
-
-  assert(w && h);
+    const uint8_t *src, const int src_stride, uint8_t *dst,
+    const int dst_stride, int w, int h, const int16_t f0, const int16_t f1) {
+  assert(w > 0 && h > 0);
+  const uint8x8_t filter0 = vdup_n_u8(f0);
+  const uint8x8_t filter1 = vdup_n_u8(f1);
 
   do {
-    int x = max_width;
+    const uint8_t *s = src;
+    uint8_t *d = dst;
+    int width = w;
+
     do {
-      // (*) -- useless
-      // 000 004 008 00C 010 014 018 01C  020 024 028 02C 030 034 038 03C
-      // 001 005 009 00D 011 015 019 01D  021 025 029 02D 031 035 039 03D
-      // 002 006 00A 00E 012 016 01A 01E  022 026 02A 02E 032 036 03A 03E (*)
-      // 003 007 00B 00F 013 017 01B 01F  023 027 02B 02F 033 037 03B 03F (*)
-      const uint8x16x4_t s0 = vld4q_u8(src0);
-      // 100 104 108 10C 110 114 118 11C  120 124 128 12C 130 134 138 13C
-      // 101 105 109 10D 111 115 119 11D  121 125 129 12D 131 135 139 13D
-      // 102 106 10A 10E 112 116 11A 11E  122 126 12A 12E 132 136 13A 13E (*)
-      // 103 107 10B 10F 113 117 11B 11F  123 127 12B 12F 133 137 13B 13F (*)
-      const uint8x16x4_t s1 = vld4q_u8(src1);
-      scale_plane_bilinear_kernel(s0.val[0], s0.val[1], s1.val[0], s1.val[1],
-                                  coef0, coef1, dst);
-      src0 += 64;
-      src1 += 64;
-      dst += 16;
-      x -= 16;
-    } while (x);
-    src0 += 4 * (src_stride - max_width);
-    src1 += 4 * (src_stride - max_width);
-    dst += dst_stride - max_width;
-  } while (--y);
-}
+      const uint8x16x4_t s0 = vld4q_u8(s + 0 * src_stride);
+      const uint8x16x4_t s1 = vld4q_u8(s + 1 * src_stride);
 
-static void scale_plane_2_to_1_general(const uint8_t *src, const int src_stride,
-                                       uint8_t *dst, const int dst_stride,
-                                       const int w, const int h,
-                                       const int16_t *const coef,
-                                       uint8_t *const temp_buffer) {
-  const int width_hor = (w + 3) & ~3;
-  const int width_ver = (w + 7) & ~7;
-  const int height_hor = (2 * h + SUBPEL_TAPS - 2 + 7) & ~7;
-  const int height_ver = (h + 3) & ~3;
-  const int16x8_t filters = vld1q_s16(coef);
-  int x, y = height_hor;
-  uint8_t *t = temp_buffer;
-  uint8x8_t s[14], d[4];
+      uint8x16_t d0 = scale_plane_bilinear_kernel(
+          s0.val[0], s0.val[1], s1.val[0], s1.val[1], filter0, filter1);
 
-  assert(w && h);
+      vst1q_u8(d, d0);
 
-  src -= (SUBPEL_TAPS / 2 - 1) * src_stride + SUBPEL_TAPS / 2 + 1;
+      s += 64;
+      d += 16;
+      width -= 16;
+    } while (width > 0);
 
-  // horizontal 4x8
-  // Note: processing 4x8 is about 20% faster than processing row by row using
-  // vld4_u8().
-  do {
-    load_u8_8x8(src + 2, src_stride, &s[0], &s[1], &s[2], &s[3], &s[4], &s[5],
-                &s[6], &s[7]);
-    transpose_elems_inplace_u8_8x8(&s[0], &s[1], &s[2], &s[3], &s[4], &s[5],
-                                   &s[6], &s[7]);
-    x = width_hor;
+    src += 4 * src_stride;
+    dst += dst_stride;
+  } while (--h != 0);
+}
 
-    do {
-      src += 8;
-      load_u8_8x8(src, src_stride, &s[6], &s[7], &s[8], &s[9], &s[10], &s[11],
-                  &s[12], &s[13]);
-      transpose_elems_inplace_u8_8x8(&s[6], &s[7], &s[8], &s[9], &s[10], &s[11],
-                                     &s[12], &s[13]);
-
-      d[0] = scale_filter_8(&s[0], filters);  // 00 10 20 30 40 50 60 70
-      d[1] = scale_filter_8(&s[2], filters);  // 01 11 21 31 41 51 61 71
-      d[2] = scale_filter_8(&s[4], filters);  // 02 12 22 32 42 52 62 72
-      d[3] = scale_filter_8(&s[6], filters);  // 03 13 23 33 43 53 63 73
-      // 00 01 02 03 40 41 42 43
-      // 10 11 12 13 50 51 52 53
-      // 20 21 22 23 60 61 62 63
-      // 30 31 32 33 70 71 72 73
-      transpose_elems_inplace_u8_8x4(&d[0], &d[1], &d[2], &d[3]);
-      vst1_lane_u32((uint32_t *)(t + 0 * width_hor), vreinterpret_u32_u8(d[0]),
-                    0);
-      vst1_lane_u32((uint32_t *)(t + 1 * width_hor), vreinterpret_u32_u8(d[1]),
-                    0);
-      vst1_lane_u32((uint32_t *)(t + 2 * width_hor), vreinterpret_u32_u8(d[2]),
-                    0);
-      vst1_lane_u32((uint32_t *)(t + 3 * width_hor), vreinterpret_u32_u8(d[3]),
-                    0);
-      vst1_lane_u32((uint32_t *)(t + 4 * width_hor), vreinterpret_u32_u8(d[0]),
-                    1);
-      vst1_lane_u32((uint32_t *)(t + 5 * width_hor), vreinterpret_u32_u8(d[1]),
-                    1);
-      vst1_lane_u32((uint32_t *)(t + 6 * width_hor), vreinterpret_u32_u8(d[2]),
-                    1);
-      vst1_lane_u32((uint32_t *)(t + 7 * width_hor), vreinterpret_u32_u8(d[3]),
-                    1);
+static inline void scale_2_to_1_horiz_6tap(const uint8_t *src,
+                                           const int src_stride, int w, int h,
+                                           uint8_t *dst, const int dst_stride,
+                                           const int16x8_t filters) {
+  do {
+    uint8x8_t t0, t1, t2, t3, t4, t5, t6, t7;
+    load_u8_8x8(src, src_stride, &t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
 
-      s[0] = s[8];
-      s[1] = s[9];
-      s[2] = s[10];
-      s[3] = s[11];
-      s[4] = s[12];
-      s[5] = s[13];
+    transpose_elems_inplace_u8_8x8(&t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
 
-      t += 4;
-      x -= 4;
-    } while (x);
-    src += 8 * src_stride - 2 * width_hor;
-    t += 7 * width_hor;
-    y -= 8;
-  } while (y);
+    int16x8_t s0 = vreinterpretq_s16_u16(vmovl_u8(t0));
+    int16x8_t s1 = vreinterpretq_s16_u16(vmovl_u8(t1));
+    int16x8_t s2 = vreinterpretq_s16_u16(vmovl_u8(t2));
+    int16x8_t s3 = vreinterpretq_s16_u16(vmovl_u8(t3));
+    int16x8_t s4 = vreinterpretq_s16_u16(vmovl_u8(t4));
+    int16x8_t s5 = vreinterpretq_s16_u16(vmovl_u8(t5));
 
-  // vertical 8x4
-  x = width_ver;
-  t = temp_buffer;
-  do {
-    load_u8_8x8(t, width_hor, &s[0], &s[1], &s[2], &s[3], &s[4], &s[5], &s[6],
-                &s[7]);
-    t += 6 * width_hor;
-    y = height_ver;
+    const uint8_t *s = src + 6;
+    uint8_t *d = dst;
+    int width = w;
 
     do {
-      load_u8_8x8(t, width_hor, &s[6], &s[7], &s[8], &s[9], &s[10], &s[11],
-                  &s[12], &s[13]);
-      t += 8 * width_hor;
-
-      d[0] = scale_filter_8(&s[0], filters);  // 00 01 02 03 04 05 06 07
-      d[1] = scale_filter_8(&s[2], filters);  // 10 11 12 13 14 15 16 17
-      d[2] = scale_filter_8(&s[4], filters);  // 20 21 22 23 24 25 26 27
-      d[3] = scale_filter_8(&s[6], filters);  // 30 31 32 33 34 35 36 37
-      vst1_u8(dst + 0 * dst_stride, d[0]);
-      vst1_u8(dst + 1 * dst_stride, d[1]);
-      vst1_u8(dst + 2 * dst_stride, d[2]);
-      vst1_u8(dst + 3 * dst_stride, d[3]);
+      uint8x8_t t8, t9, t10, t11, t12, t13;
+      load_u8_8x8(s, src_stride, &t6, &t7, &t8, &t9, &t10, &t11, &t12, &t13);
+
+      transpose_elems_inplace_u8_8x8(&t6, &t7, &t8, &t9, &t10, &t11, &t12,
+                                     &t13);
+
+      int16x8_t s6 = vreinterpretq_s16_u16(vmovl_u8(t6));
+      int16x8_t s7 = vreinterpretq_s16_u16(vmovl_u8(t7));
+      int16x8_t s8 = vreinterpretq_s16_u16(vmovl_u8(t8));
+      int16x8_t s9 = vreinterpretq_s16_u16(vmovl_u8(t9));
+      int16x8_t s10 = vreinterpretq_s16_u16(vmovl_u8(t10));
+      int16x8_t s11 = vreinterpretq_s16_u16(vmovl_u8(t11));
+      int16x8_t s12 = vreinterpretq_s16_u16(vmovl_u8(t12));
+      int16x8_t s13 = vreinterpretq_s16_u16(vmovl_u8(t13));
+
+      uint8x8_t d0 = scale_filter6_8(s0, s1, s2, s3, s4, s5, filters);
+      uint8x8_t d1 = scale_filter6_8(s2, s3, s4, s5, s6, s7, filters);
+      uint8x8_t d2 = scale_filter6_8(s4, s5, s6, s7, s8, s9, filters);
+      uint8x8_t d3 = scale_filter6_8(s6, s7, s8, s9, s10, s11, filters);
+
+      transpose_elems_inplace_u8_8x4(&d0, &d1, &d2, &d3);
+
+      store_u8x4_strided_x2(d + 0 * dst_stride, 4 * dst_stride, d0);
+      store_u8x4_strided_x2(d + 1 * dst_stride, 4 * dst_stride, d1);
+      store_u8x4_strided_x2(d + 2 * dst_stride, 4 * dst_stride, d2);
+      store_u8x4_strided_x2(d + 3 * dst_stride, 4 * dst_stride, d3);
+
+      s0 = s8;
+      s1 = s9;
+      s2 = s10;
+      s3 = s11;
+      s4 = s12;
+      s5 = s13;
+
+      d += 4;
+      s += 8;
+      width -= 4;
+    } while (width > 0);
+
+    dst += 8 * dst_stride;
+    src += 8 * src_stride;
+    h -= 8;
+  } while (h > 0);
+}
 
-      s[0] = s[8];
-      s[1] = s[9];
-      s[2] = s[10];
-      s[3] = s[11];
-      s[4] = s[12];
-      s[5] = s[13];
+static inline void scale_plane_2_to_1_6tap(const uint8_t *src,
+                                           const int src_stride, uint8_t *dst,
+                                           const int dst_stride, const int w,
+                                           const int h,
+                                           const int16_t *const filter_ptr,
+                                           uint8_t *const im_block) {
+  assert(w > 0 && h > 0);
+  const int im_h = 2 * h + SUBPEL_TAPS - 3;
+  const int im_stride = (w + 3) & ~3;
 
-      dst += 4 * dst_stride;
-      y -= 4;
-    } while (y);
-    t -= width_hor * (2 * height_ver + 6);
-    t += 8;
-    dst -= height_ver * dst_stride;
-    dst += 8;
-    x -= 8;
-  } while (x);
-}
+  // All filter values are even, halve them to stay in 16-bit elements when
+  // applying filter.
+  const int16x8_t filters = vshrq_n_s16(vld1q_s16(filter_ptr), 1);
 
-static void scale_plane_4_to_1_general(const uint8_t *src, const int src_stride,
-                                       uint8_t *dst, const int dst_stride,
-                                       const int w, const int h,
-                                       const int16_t *const coef,
-                                       uint8_t *const temp_buffer) {
-  const int width_hor = (w + 1) & ~1;
-  const int width_ver = (w + 7) & ~7;
-  const int height_hor = (4 * h + SUBPEL_TAPS - 2 + 7) & ~7;
-  const int height_ver = (h + 1) & ~1;
-  const int16x8_t filters = vld1q_s16(coef);
-  int x, y = height_hor;
-  uint8_t *t = temp_buffer;
-  uint8x8_t s[12], d[2];
+  const ptrdiff_t horiz_offset = SUBPEL_TAPS / 2 - 2;
+  const ptrdiff_t vert_offset = (SUBPEL_TAPS / 2 - 2) * src_stride;
 
-  assert(w && h);
+  scale_2_to_1_horiz_6tap(src - horiz_offset - vert_offset, src_stride, w, im_h,
+                          im_block, im_stride, filters);
 
-  src -= (SUBPEL_TAPS / 2 - 1) * src_stride + SUBPEL_TAPS / 2 + 3;
+  scale_2_to_1_vert_6tap(im_block, im_stride, w, h, dst, dst_stride, filters);
+}
 
-  // horizontal 2x8
-  // Note: processing 2x8 is about 20% faster than processing row by row using
-  // vld4_u8().
+static inline void scale_4_to_1_horiz_6tap(const uint8_t *src,
+                                           const int src_stride, int w, int h,
+                                           uint8_t *dst, const int dst_stride,
+                                           const int16x8_t filters) {
   do {
-    load_u8_8x8(src + 4, src_stride, &s[0], &s[1], &s[2], &s[3], &s[4], &s[5],
-                &s[6], &s[7]);
-    transpose_elems_u8_4x8(s[0], s[1], s[2], s[3], s[4], s[5], s[6], s[7],
-                           &s[0], &s[1], &s[2], &s[3]);
-    x = width_hor;
+    uint8x8_t t0, t1, t2, t3, t4, t5, t6, t7;
+    load_u8_8x8(src, src_stride, &t0, &t1, &t2, &t3, &t4, &t5, &t6, &t7);
 
-    do {
-      uint8x8x2_t dd;
-      src += 8;
-      load_u8_8x8(src, src_stride, &s[4], &s[5], &s[6], &s[7], &s[8], &s[9],
-                  &s[10], &s[11]);
-      transpose_elems_inplace_u8_8x8(&s[4], &s[5], &s[6], &s[7], &s[8], &s[9],
-                                     &s[10], &s[11]);
-
-      d[0] = scale_filter_8(&s[0], filters);  // 00 10 20 30 40 50 60 70
-      d[1] = scale_filter_8(&s[4], filters);  // 01 11 21 31 41 51 61 71
-      // dd.val[0]: 00 01 20 21 40 41 60 61
-      // dd.val[1]: 10 11 30 31 50 51 70 71
-      dd = vtrn_u8(d[0], d[1]);
-      vst1_lane_u16((uint16_t *)(t + 0 * width_hor),
-                    vreinterpret_u16_u8(dd.val[0]), 0);
-      vst1_lane_u16((uint16_t *)(t + 1 * width_hor),
-                    vreinterpret_u16_u8(dd.val[1]), 0);
-      vst1_lane_u16((uint16_t *)(t + 2 * width_hor),
-                    vreinterpret_u16_u8(dd.val[0]), 1);
-      vst1_lane_u16((uint16_t *)(t + 3 * width_hor),
-                    vreinterpret_u16_u8(dd.val[1]), 1);
-      vst1_lane_u16((uint16_t *)(t + 4 * width_hor),
-                    vreinterpret_u16_u8(dd.val[0]), 2);
-      vst1_lane_u16((uint16_t *)(t + 5 * width_hor),
-                    vreinterpret_u16_u8(dd.val[1]), 2);
-      vst1_lane_u16((uint16_t *)(t + 6 * width_hor),
-                    vreinterpret_u16_u8(dd.val[0]), 3);
-      vst1_lane_u16((uint16_t *)(t + 7 * width_hor),
-                    vreinterpret_u16_u8(dd.val[1]), 3);
+    transpose_elems_u8_4x8(t0, t1, t2, t3, t4, t5, t6, t7, &t0, &t1, &t2, &t3);
 
-      s[0] = s[8];
-      s[1] = s[9];
-      s[2] = s[10];
-      s[3] = s[11];
+    int16x8_t s0 = vreinterpretq_s16_u16(vmovl_u8(t0));
+    int16x8_t s1 = vreinterpretq_s16_u16(vmovl_u8(t1));
+    int16x8_t s2 = vreinterpretq_s16_u16(vmovl_u8(t2));
+    int16x8_t s3 = vreinterpretq_s16_u16(vmovl_u8(t3));
 
-      t += 2;
-      x -= 2;
-    } while (x);
-    src += 8 * src_stride - 4 * width_hor;
-    t += 7 * width_hor;
-    y -= 8;
-  } while (y);
-
-  // vertical 8x2
-  x = width_ver;
-  t = temp_buffer;
-  do {
-    load_u8_8x4(t, width_hor, &s[0], &s[1], &s[2], &s[3]);
-    t += 4 * width_hor;
-    y = height_ver;
+    const uint8_t *s = src + 4;
+    uint8_t *d = dst;
+    int width = w;
 
     do {
-      load_u8_8x8(t, width_hor, &s[4], &s[5], &s[6], &s[7], &s[8], &s[9],
-                  &s[10], &s[11]);
-      t += 8 * width_hor;
-
-      d[0] = scale_filter_8(&s[0], filters);  // 00 01 02 03 04 05 06 07
-      d[1] = scale_filter_8(&s[4], filters);  // 10 11 12 13 14 15 16 17
-      vst1_u8(dst + 0 * dst_stride, d[0]);
-      vst1_u8(dst + 1 * dst_stride, d[1]);
-
-      s[0] = s[8];
-      s[1] = s[9];
-      s[2] = s[10];
-      s[3] = s[11];
+      uint8x8_t t8, t9, t10, t11;
+      load_u8_8x8(s, src_stride, &t4, &t5, &t6, &t7, &t8, &t9, &t10, &t11);
+
+      transpose_elems_inplace_u8_8x8(&t4, &t5, &t6, &t7, &t8, &t9, &t10, &t11);
+
+      int16x8_t s4 = vreinterpretq_s16_u16(vmovl_u8(t4));
+      int16x8_t s5 = vreinterpretq_s16_u16(vmovl_u8(t5));
+      int16x8_t s6 = vreinterpretq_s16_u16(vmovl_u8(t6));
+      int16x8_t s7 = vreinterpretq_s16_u16(vmovl_u8(t7));
+      int16x8_t s8 = vreinterpretq_s16_u16(vmovl_u8(t8));
+      int16x8_t s9 = vreinterpretq_s16_u16(vmovl_u8(t9));
+      int16x8_t s10 = vreinterpretq_s16_u16(vmovl_u8(t10));
+      int16x8_t s11 = vreinterpretq_s16_u16(vmovl_u8(t11));
+
+      uint8x8_t d0 = scale_filter6_8(s0, s1, s2, s3, s4, s5, filters);
+      uint8x8_t d1 = scale_filter6_8(s4, s5, s6, s7, s8, s9, filters);
+
+      uint8x8x2_t d01 = vtrn_u8(d0, d1);
+
+      store_u8x2_strided_x4(d + 0 * dst_stride, 2 * dst_stride, d01.val[0]);
+      store_u8x2_strided_x4(d + 1 * dst_stride, 2 * dst_stride, d01.val[1]);
+
+      s0 = s8;
+      s1 = s9;
+      s2 = s10;
+      s3 = s11;
+
+      d += 2;
+      s += 8;
+      width -= 2;
+    } while (width > 0);
+
+    dst += 8 * dst_stride;
+    src += 8 * src_stride;
+    h -= 8;
+  } while (h > 0);
+}
 
-      dst += 2 * dst_stride;
-      y -= 2;
-    } while (y);
-    t -= width_hor * (4 * height_ver + 4);
-    t += 8;
-    dst -= height_ver * dst_stride;
-    dst += 8;
-    x -= 8;
-  } while (x);
+static inline void scale_plane_4_to_1_6tap(const uint8_t *src,
+                                           const int src_stride, uint8_t *dst,
+                                           const int dst_stride, const int w,
+                                           const int h,
+                                           const int16_t *const filter_ptr,
+                                           uint8_t *const im_block) {
+  assert(w > 0 && h > 0);
+  const int im_h = 4 * h + SUBPEL_TAPS - 3;
+  const int im_stride = (w + 1) & ~1;
+  // All filter values are even, halve them to stay in 16-bit elements when
+  // applying filter.
+  const int16x8_t filters = vshrq_n_s16(vld1q_s16(filter_ptr), 1);
+
+  const ptrdiff_t horiz_offset = SUBPEL_TAPS / 2 - 2;
+  const ptrdiff_t vert_offset = (SUBPEL_TAPS / 2 - 2) * src_stride;
+
+  scale_4_to_1_horiz_6tap(src - horiz_offset - vert_offset, src_stride, w, im_h,
+                          im_block, im_stride, filters);
+
+  scale_4_to_1_vert_6tap(im_block, im_stride, w, h, dst, dst_stride, filters);
 }
 
 static inline uint8x8_t scale_filter_bilinear(const uint8x8_t *const s,
@@ -441,11 +366,10 @@ static inline uint8x8_t scale_filter_bilinear(const uint8x8_t *const s,
 // 5. The physical location of the last row of the 4 to 3 scaled frame is
 // decided by phase_scaler, and are always less than 1 pixel below the last row
 // of the original image.
-static void scale_plane_4_to_3_bilinear(const uint8_t *src,
-                                        const int src_stride, uint8_t *dst,
-                                        const int dst_stride, const int w,
-                                        const int h, const int phase_scaler,
-                                        uint8_t *const temp_buffer) {
+static inline void scale_plane_4_to_3_bilinear(
+    const uint8_t *src, const int src_stride, uint8_t *dst,
+    const int dst_stride, const int w, const int h, const int phase_scaler,
+    uint8_t *const temp_buffer) {
   static const int step_q4 = 16 * 4 / 3;
   const int width_hor = (w + 5) - ((w + 5) % 6);
   const int stride_hor = width_hor + 2;  // store 2 extra pixels
@@ -582,12 +506,39 @@ static void scale_plane_4_to_3_bilinear(const uint8_t *src,
   } while (x);
 }
 
-static void scale_plane_4_to_3_general(const uint8_t *src, const int src_stride,
-                                       uint8_t *dst, const int dst_stride,
-                                       const int w, const int h,
-                                       const InterpKernel *const coef,
-                                       const int phase_scaler,
-                                       uint8_t *const temp_buffer) {
+static inline uint8x8_t scale_filter_8(const uint8x8_t *const s,
+                                       const int16x8_t filter) {
+  const int16x4_t filter_lo = vget_low_s16(filter);
+  const int16x4_t filter_hi = vget_high_s16(filter);
+
+  int16x8_t ss0 = vreinterpretq_s16_u16(vmovl_u8(s[0]));
+  int16x8_t ss1 = vreinterpretq_s16_u16(vmovl_u8(s[1]));
+  int16x8_t ss2 = vreinterpretq_s16_u16(vmovl_u8(s[2]));
+  int16x8_t ss3 = vreinterpretq_s16_u16(vmovl_u8(s[3]));
+  int16x8_t ss4 = vreinterpretq_s16_u16(vmovl_u8(s[4]));
+  int16x8_t ss5 = vreinterpretq_s16_u16(vmovl_u8(s[5]));
+  int16x8_t ss6 = vreinterpretq_s16_u16(vmovl_u8(s[6]));
+  int16x8_t ss7 = vreinterpretq_s16_u16(vmovl_u8(s[7]));
+
+  int16x8_t sum = vmulq_lane_s16(ss0, filter_lo, 0);
+  sum = vmlaq_lane_s16(sum, ss1, filter_lo, 1);
+  sum = vmlaq_lane_s16(sum, ss2, filter_lo, 2);
+  sum = vmlaq_lane_s16(sum, ss5, filter_hi, 1);
+  sum = vmlaq_lane_s16(sum, ss6, filter_hi, 2);
+  sum = vmlaq_lane_s16(sum, ss7, filter_hi, 3);
+  sum = vqaddq_s16(sum, vmulq_lane_s16(ss3, filter_lo, 3));
+  sum = vqaddq_s16(sum, vmulq_lane_s16(ss4, filter_hi, 0));
+
+  return vqrshrun_n_s16(sum, FILTER_BITS);
+}
+
+static inline void scale_plane_4_to_3_8tap(const uint8_t *src,
+                                           const int src_stride, uint8_t *dst,
+                                           const int dst_stride, const int w,
+                                           const int h,
+                                           const InterpKernel *const coef,
+                                           const int phase_scaler,
+                                           uint8_t *const temp_buffer) {
   static const int step_q4 = 16 * 4 / 3;
   const int width_hor = (w + 5) - ((w + 5) % 6);
   const int stride_hor = width_hor + 2;  // store 2 extra pixels
@@ -606,7 +557,7 @@ static void scale_plane_4_to_3_general(const uint8_t *src, const int src_stride,
   uint8_t *t = temp_buffer;
   uint8x8_t s[15], d[8];
 
-  assert(w && h);
+  assert(w > 0 && h > 0);
 
   src -= (SUBPEL_TAPS / 2 - 1) * src_stride + SUBPEL_TAPS / 2;
   d[6] = vdup_n_u8(0);
@@ -743,6 +694,9 @@ void av1_resize_and_extend_frame_neon(const YV12_BUFFER_CONFIG *src,
                                       YV12_BUFFER_CONFIG *dst,
                                       const InterpFilter filter,
                                       const int phase, const int num_planes) {
+  assert(filter == BILINEAR || filter == EIGHTTAP_SMOOTH ||
+         filter == EIGHTTAP_REGULAR);
+
   bool has_normative_scaler =
       has_normative_scaler_neon(src->y_crop_width, src->y_crop_height,
                                 dst->y_crop_width, dst->y_crop_height);
@@ -794,9 +748,9 @@ void av1_resize_and_extend_frame_neon(const YV12_BUFFER_CONFIG *src,
         const InterpKernel *interp_kernel =
             (const InterpKernel *)av1_interp_filter_params_list[filter]
                 .filter_ptr;
-        scale_plane_2_to_1_general(src->buffers[i], src->strides[is_uv],
-                                   dst->buffers[i], dst->strides[is_uv], dst_w,
-                                   dst_h, interp_kernel[phase], temp_buffer);
+        scale_plane_2_to_1_6tap(src->buffers[i], src->strides[is_uv],
+                                dst->buffers[i], dst->strides[is_uv], dst_w,
+                                dst_h, interp_kernel[phase], temp_buffer);
         free(temp_buffer);
       }
     } else if (4 * dst_w == src_w && 4 * dst_h == src_h) {
@@ -822,9 +776,9 @@ void av1_resize_and_extend_frame_neon(const YV12_BUFFER_CONFIG *src,
         const InterpKernel *interp_kernel =
             (const InterpKernel *)av1_interp_filter_params_list[filter]
                 .filter_ptr;
-        scale_plane_4_to_1_general(src->buffers[i], src->strides[is_uv],
-                                   dst->buffers[i], dst->strides[is_uv], dst_w,
-                                   dst_h, interp_kernel[phase], temp_buffer);
+        scale_plane_4_to_1_6tap(src->buffers[i], src->strides[is_uv],
+                                dst->buffers[i], dst->strides[is_uv], dst_w,
+                                dst_h, interp_kernel[phase], temp_buffer);
         free(temp_buffer);
       }
     } else {
@@ -846,9 +800,9 @@ void av1_resize_and_extend_frame_neon(const YV12_BUFFER_CONFIG *src,
         const InterpKernel *interp_kernel =
             (const InterpKernel *)av1_interp_filter_params_list[filter]
                 .filter_ptr;
-        scale_plane_4_to_3_general(src->buffers[i], src->strides[is_uv],
-                                   dst->buffers[i], dst->strides[is_uv], dst_w,
-                                   dst_h, interp_kernel, phase, temp_buffer);
+        scale_plane_4_to_3_8tap(src->buffers[i], src->strides[is_uv],
+                                dst->buffers[i], dst->strides[is_uv], dst_w,
+                                dst_h, interp_kernel, phase, temp_buffer);
       }
       free(temp_buffer);
     }
diff --git a/av1/common/arm/resize_neon.h b/av1/common/arm/resize_neon.h
new file mode 100644
index 000000000..78e370988
--- /dev/null
+++ b/av1/common/arm/resize_neon.h
@@ -0,0 +1,140 @@
+/*
+ * Copyright (c) 2024, Alliance for Open Media. All rights reserved.
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#ifndef AOM_AV1_COMMON_ARM_RESIZE_NEON_H_
+#define AOM_AV1_COMMON_ARM_RESIZE_NEON_H_
+
+#include <arm_neon.h>
+
+#include "aom_dsp/aom_filter.h"
+#include "aom_dsp/arm/mem_neon.h"
+#include "aom_dsp/arm/transpose_neon.h"
+
+static inline uint8x8_t scale_filter6_8(const int16x8_t s0, const int16x8_t s1,
+                                        const int16x8_t s2, const int16x8_t s3,
+                                        const int16x8_t s4, const int16x8_t s5,
+                                        int16x8_t filter) {
+  const int16x4_t filter_lo = vget_low_s16(filter);
+  const int16x4_t filter_hi = vget_high_s16(filter);
+
+  // Filter values at indices 0 and 7 are 0.
+  int16x8_t sum = vmulq_lane_s16(s0, filter_lo, 1);
+  sum = vmlaq_lane_s16(sum, s1, filter_lo, 2);
+  sum = vmlaq_lane_s16(sum, s2, filter_lo, 3);
+  sum = vmlaq_lane_s16(sum, s3, filter_hi, 0);
+  sum = vmlaq_lane_s16(sum, s4, filter_hi, 1);
+  sum = vmlaq_lane_s16(sum, s5, filter_hi, 2);
+
+  // We halved the convolution filter values so -1 from the right shift.
+  return vqrshrun_n_s16(sum, FILTER_BITS - 1);
+}
+
+static inline void scale_2_to_1_vert_6tap(const uint8_t *src,
+                                          const int src_stride, int w, int h,
+                                          uint8_t *dst, const int dst_stride,
+                                          const int16x8_t filters) {
+  do {
+    uint8x8_t t0, t1, t2, t3;
+    load_u8_8x4(src, src_stride, &t0, &t1, &t2, &t3);
+
+    int16x8_t s0 = vreinterpretq_s16_u16(vmovl_u8(t0));
+    int16x8_t s1 = vreinterpretq_s16_u16(vmovl_u8(t1));
+    int16x8_t s2 = vreinterpretq_s16_u16(vmovl_u8(t2));
+    int16x8_t s3 = vreinterpretq_s16_u16(vmovl_u8(t3));
+
+    const uint8_t *s = src + 4 * src_stride;
+    uint8_t *d = dst;
+    int height = h;
+
+    do {
+      uint8x8_t t4, t5, t6, t7, t8, t9, t10, t11;
+      load_u8_8x8(s, src_stride, &t4, &t5, &t6, &t7, &t8, &t9, &t10, &t11);
+
+      int16x8_t s4 = vreinterpretq_s16_u16(vmovl_u8(t4));
+      int16x8_t s5 = vreinterpretq_s16_u16(vmovl_u8(t5));
+      int16x8_t s6 = vreinterpretq_s16_u16(vmovl_u8(t6));
+      int16x8_t s7 = vreinterpretq_s16_u16(vmovl_u8(t7));
+      int16x8_t s8 = vreinterpretq_s16_u16(vmovl_u8(t8));
+      int16x8_t s9 = vreinterpretq_s16_u16(vmovl_u8(t9));
+      int16x8_t s10 = vreinterpretq_s16_u16(vmovl_u8(t10));
+      int16x8_t s11 = vreinterpretq_s16_u16(vmovl_u8(t11));
+
+      uint8x8_t d0 = scale_filter6_8(s0, s1, s2, s3, s4, s5, filters);
+      uint8x8_t d1 = scale_filter6_8(s2, s3, s4, s5, s6, s7, filters);
+      uint8x8_t d2 = scale_filter6_8(s4, s5, s6, s7, s8, s9, filters);
+      uint8x8_t d3 = scale_filter6_8(s6, s7, s8, s9, s10, s11, filters);
+
+      store_u8_8x4(d, dst_stride, d0, d1, d2, d3);
+
+      s0 = s8;
+      s1 = s9;
+      s2 = s10;
+      s3 = s11;
+
+      d += 4 * dst_stride;
+      s += 8 * src_stride;
+      height -= 4;
+    } while (height > 0);
+
+    dst += 8;
+    src += 8;
+    w -= 8;
+  } while (w > 0);
+}
+
+static inline void scale_4_to_1_vert_6tap(const uint8_t *src,
+                                          const int src_stride, int w, int h,
+                                          uint8_t *dst, const int dst_stride,
+                                          const int16x8_t filters) {
+  do {
+    uint8x8_t t0 = vld1_u8(src + 0 * src_stride);
+    uint8x8_t t1 = vld1_u8(src + 1 * src_stride);
+
+    int16x8_t s0 = vreinterpretq_s16_u16(vmovl_u8(t0));
+    int16x8_t s1 = vreinterpretq_s16_u16(vmovl_u8(t1));
+
+    const uint8_t *s = src + 2 * src_stride;
+    uint8_t *d = dst;
+    int height = h;
+
+    do {
+      uint8x8_t t2, t3, t4, t5, t6, t7, t8, t9;
+      load_u8_8x8(s, src_stride, &t2, &t3, &t4, &t5, &t6, &t7, &t8, &t9);
+
+      int16x8_t s2 = vreinterpretq_s16_u16(vmovl_u8(t2));
+      int16x8_t s3 = vreinterpretq_s16_u16(vmovl_u8(t3));
+      int16x8_t s4 = vreinterpretq_s16_u16(vmovl_u8(t4));
+      int16x8_t s5 = vreinterpretq_s16_u16(vmovl_u8(t5));
+      int16x8_t s6 = vreinterpretq_s16_u16(vmovl_u8(t6));
+      int16x8_t s7 = vreinterpretq_s16_u16(vmovl_u8(t7));
+      int16x8_t s8 = vreinterpretq_s16_u16(vmovl_u8(t8));
+      int16x8_t s9 = vreinterpretq_s16_u16(vmovl_u8(t9));
+
+      uint8x8_t d0 = scale_filter6_8(s0, s1, s2, s3, s4, s5, filters);
+      uint8x8_t d1 = scale_filter6_8(s4, s5, s6, s7, s8, s9, filters);
+
+      store_u8_8x2(d, dst_stride, d0, d1);
+
+      s0 = s8;
+      s1 = s9;
+
+      s += 8 * src_stride;
+      d += 2 * dst_stride;
+      height -= 2;
+    } while (height > 0);
+
+    src += 8;
+    dst += 8;
+    w -= 8;
+  } while (w > 0);
+}
+
+#endif  // AOM_AV1_COMMON_ARM_RESIZE_NEON_H_
diff --git a/av1/common/arm/resize_neon_dotprod.c b/av1/common/arm/resize_neon_dotprod.c
new file mode 100644
index 000000000..3228633f1
--- /dev/null
+++ b/av1/common/arm/resize_neon_dotprod.c
@@ -0,0 +1,314 @@
+/*
+ * Copyright (c) 2024, Alliance for Open Media. All rights reserved.
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <arm_neon.h>
+#include <assert.h>
+
+#include "aom_dsp/arm/mem_neon.h"
+#include "aom_dsp/arm/transpose_neon.h"
+#include "av1/common/arm/resize_neon.h"
+#include "av1/common/resize.h"
+#include "config/aom_scale_rtcd.h"
+#include "config/av1_rtcd.h"
+
+// clang-format off
+DECLARE_ALIGNED(16, static const uint8_t, kScale2DotProdPermuteTbl[32]) = {
+  0, 1, 2, 3, 2, 3, 4, 5, 4, 5,  6,  7,  6,  7,  8,  9,
+  4, 5, 6, 7, 6, 7, 8, 9, 8, 9, 10, 11, 10, 11, 12, 13
+};
+DECLARE_ALIGNED(16, static const uint8_t, kScale4DotProdPermuteTbl[16]) = {
+  0, 1, 2, 3, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11
+};
+// clang-format on
+
+static inline uint8x8_t scale_2_to_1_filter8_8(const uint8x16_t s0,
+                                               const uint8x16_t s1,
+                                               const uint8x16x2_t permute_tbl,
+                                               const int8x8_t filter) {
+  // Transform sample range to [-128, 127] for 8-bit signed dot product.
+  int8x16_t s0_128 = vreinterpretq_s8_u8(vsubq_u8(s0, vdupq_n_u8(128)));
+  int8x16_t s1_128 = vreinterpretq_s8_u8(vsubq_u8(s1, vdupq_n_u8(128)));
+
+  // Permute samples ready for dot product.
+  int8x16_t perm_samples[4] = { vqtbl1q_s8(s0_128, permute_tbl.val[0]),
+                                vqtbl1q_s8(s0_128, permute_tbl.val[1]),
+                                vqtbl1q_s8(s1_128, permute_tbl.val[0]),
+                                vqtbl1q_s8(s1_128, permute_tbl.val[1]) };
+
+  // Dot product constant:
+  // The shim of 128 << FILTER_BITS is needed because we are subtracting 128
+  // from every source value. The additional right shift by one is needed
+  // because we halve the filter values.
+  const int32x4_t acc = vdupq_n_s32((128 << FILTER_BITS) >> 1);
+
+  // First 4 output values.
+  int32x4_t sum0123 = vdotq_lane_s32(acc, perm_samples[0], filter, 0);
+  sum0123 = vdotq_lane_s32(sum0123, perm_samples[1], filter, 1);
+  // Second 4 output values.
+  int32x4_t sum4567 = vdotq_lane_s32(acc, perm_samples[2], filter, 0);
+  sum4567 = vdotq_lane_s32(sum4567, perm_samples[3], filter, 1);
+
+  int16x8_t sum = vcombine_s16(vmovn_s32(sum0123), vmovn_s32(sum4567));
+
+  // We halved the filter values so -1 from right shift.
+  return vqrshrun_n_s16(sum, FILTER_BITS - 1);
+}
+
+static inline void scale_2_to_1_horiz_8tap(const uint8_t *src,
+                                           const int src_stride, int w, int h,
+                                           uint8_t *dst, const int dst_stride,
+                                           const int16x8_t filters) {
+  const int8x8_t filter = vmovn_s16(filters);
+  const uint8x16x2_t permute_tbl = vld1q_u8_x2(kScale2DotProdPermuteTbl);
+
+  do {
+    const uint8_t *s = src;
+    uint8_t *d = dst;
+    int width = w;
+    do {
+      uint8x16_t s0[2], s1[2], s2[2], s3[2], s4[2], s5[2], s6[2], s7[2];
+      load_u8_16x8(s, src_stride, &s0[0], &s1[0], &s2[0], &s3[0], &s4[0],
+                   &s5[0], &s6[0], &s7[0]);
+      load_u8_16x8(s + 8, src_stride, &s0[1], &s1[1], &s2[1], &s3[1], &s4[1],
+                   &s5[1], &s6[1], &s7[1]);
+
+      uint8x8_t d0 = scale_2_to_1_filter8_8(s0[0], s0[1], permute_tbl, filter);
+      uint8x8_t d1 = scale_2_to_1_filter8_8(s1[0], s1[1], permute_tbl, filter);
+      uint8x8_t d2 = scale_2_to_1_filter8_8(s2[0], s2[1], permute_tbl, filter);
+      uint8x8_t d3 = scale_2_to_1_filter8_8(s3[0], s3[1], permute_tbl, filter);
+
+      uint8x8_t d4 = scale_2_to_1_filter8_8(s4[0], s4[1], permute_tbl, filter);
+      uint8x8_t d5 = scale_2_to_1_filter8_8(s5[0], s5[1], permute_tbl, filter);
+      uint8x8_t d6 = scale_2_to_1_filter8_8(s6[0], s6[1], permute_tbl, filter);
+      uint8x8_t d7 = scale_2_to_1_filter8_8(s7[0], s7[1], permute_tbl, filter);
+
+      store_u8_8x8(d, dst_stride, d0, d1, d2, d3, d4, d5, d6, d7);
+
+      d += 8;
+      s += 16;
+      width -= 8;
+    } while (width > 0);
+
+    dst += 8 * dst_stride;
+    src += 8 * src_stride;
+    h -= 8;
+  } while (h > 0);
+}
+
+static inline void scale_plane_2_to_1_8tap(const uint8_t *src,
+                                           const int src_stride, uint8_t *dst,
+                                           const int dst_stride, const int w,
+                                           const int h,
+                                           const int16_t *const filter_ptr,
+                                           uint8_t *const im_block) {
+  assert(w > 0 && h > 0);
+
+  const int im_h = 2 * h + SUBPEL_TAPS - 3;
+  const int im_stride = (w + 7) & ~7;
+  // All filter values are even, halve them to fit in int8_t when applying
+  // horizontal filter and stay in 16-bit elements when applying vertical
+  // filter.
+  const int16x8_t filters = vshrq_n_s16(vld1q_s16(filter_ptr), 1);
+
+  const ptrdiff_t horiz_offset = SUBPEL_TAPS / 2 - 1;
+  const ptrdiff_t vert_offset = (SUBPEL_TAPS / 2 - 1) * src_stride;
+
+  scale_2_to_1_horiz_8tap(src - horiz_offset - vert_offset, src_stride, w, im_h,
+                          im_block, im_stride, filters);
+
+  // We can specialise the vertical filtering for 6-tap filters given that the
+  // EIGHTTAP_SMOOTH and EIGHTTAP_REGULAR filters are 0-padded.
+  scale_2_to_1_vert_6tap(im_block + im_stride, im_stride, w, h, dst, dst_stride,
+                         filters);
+}
+
+static inline uint8x8_t scale_4_to_1_filter8_8(
+    const uint8x16_t s0, const uint8x16_t s1, const uint8x16_t s2,
+    const uint8x16_t s3, const uint8x16_t permute_tbl, const int8x8_t filter) {
+  int8x16_t filters = vcombine_s8(filter, filter);
+
+  // Transform sample range to [-128, 127] for 8-bit signed dot product.
+  int8x16_t s0_128 = vreinterpretq_s8_u8(vsubq_u8(s0, vdupq_n_u8(128)));
+  int8x16_t s1_128 = vreinterpretq_s8_u8(vsubq_u8(s1, vdupq_n_u8(128)));
+  int8x16_t s2_128 = vreinterpretq_s8_u8(vsubq_u8(s2, vdupq_n_u8(128)));
+  int8x16_t s3_128 = vreinterpretq_s8_u8(vsubq_u8(s3, vdupq_n_u8(128)));
+
+  int8x16_t perm_samples[4] = { vqtbl1q_s8(s0_128, permute_tbl),
+                                vqtbl1q_s8(s1_128, permute_tbl),
+                                vqtbl1q_s8(s2_128, permute_tbl),
+                                vqtbl1q_s8(s3_128, permute_tbl) };
+
+  // Dot product constant:
+  // The shim of 128 << FILTER_BITS is needed because we are subtracting 128
+  // from every source value. The additional right shift by one is needed
+  // because we halved the filter values and will use a pairwise add.
+  const int32x4_t acc = vdupq_n_s32((128 << FILTER_BITS) >> 2);
+
+  int32x4_t sum0 = vdotq_s32(acc, perm_samples[0], filters);
+  int32x4_t sum1 = vdotq_s32(acc, perm_samples[1], filters);
+  int32x4_t sum2 = vdotq_s32(acc, perm_samples[2], filters);
+  int32x4_t sum3 = vdotq_s32(acc, perm_samples[3], filters);
+
+  int32x4_t sum01 = vpaddq_s32(sum0, sum1);
+  int32x4_t sum23 = vpaddq_s32(sum2, sum3);
+
+  int16x8_t sum = vcombine_s16(vmovn_s32(sum01), vmovn_s32(sum23));
+
+  // We halved the filter values so -1 from right shift.
+  return vqrshrun_n_s16(sum, FILTER_BITS - 1);
+}
+
+static inline void scale_4_to_1_horiz_8tap(const uint8_t *src,
+                                           const int src_stride, int w, int h,
+                                           uint8_t *dst, const int dst_stride,
+                                           const int16x8_t filters) {
+  const int8x8_t filter = vmovn_s16(filters);
+  const uint8x16_t permute_tbl = vld1q_u8(kScale4DotProdPermuteTbl);
+
+  do {
+    const uint8_t *s = src;
+    uint8_t *d = dst;
+    int width = w;
+
+    do {
+      uint8x16_t s0, s1, s2, s3, s4, s5, s6, s7;
+      load_u8_16x8(s, src_stride, &s0, &s1, &s2, &s3, &s4, &s5, &s6, &s7);
+
+      uint8x8_t d0 =
+          scale_4_to_1_filter8_8(s0, s1, s2, s3, permute_tbl, filter);
+      uint8x8_t d1 =
+          scale_4_to_1_filter8_8(s4, s5, s6, s7, permute_tbl, filter);
+
+      store_u8x2_strided_x4(d + 0 * dst_stride, dst_stride, d0);
+      store_u8x2_strided_x4(d + 4 * dst_stride, dst_stride, d1);
+
+      d += 2;
+      s += 8;
+      width -= 2;
+    } while (width > 0);
+
+    dst += 8 * dst_stride;
+    src += 8 * src_stride;
+    h -= 8;
+  } while (h > 0);
+}
+
+static inline void scale_plane_4_to_1_8tap(const uint8_t *src,
+                                           const int src_stride, uint8_t *dst,
+                                           const int dst_stride, const int w,
+                                           const int h,
+                                           const int16_t *const filter_ptr,
+                                           uint8_t *const im_block) {
+  assert(w > 0 && h > 0);
+  const int im_h = 4 * h + SUBPEL_TAPS - 2;
+  const int im_stride = (w + 1) & ~1;
+  // All filter values are even, halve them to fit in int8_t when applying
+  // horizontal filter and stay in 16-bit elements when applying vertical
+  // filter.
+  const int16x8_t filters = vshrq_n_s16(vld1q_s16(filter_ptr), 1);
+
+  const ptrdiff_t horiz_offset = SUBPEL_TAPS / 2 - 1;
+  const ptrdiff_t vert_offset = (SUBPEL_TAPS / 2 - 1) * src_stride;
+
+  scale_4_to_1_horiz_8tap(src - horiz_offset - vert_offset, src_stride, w, im_h,
+                          im_block, im_stride, filters);
+
+  // We can specialise the vertical filtering for 6-tap filters given that the
+  // EIGHTTAP_SMOOTH and EIGHTTAP_REGULAR filters are 0-padded.
+  scale_4_to_1_vert_6tap(im_block + im_stride, im_stride, w, h, dst, dst_stride,
+                         filters);
+}
+
+static inline bool has_normative_scaler_neon_dotprod(const int src_width,
+                                                     const int src_height,
+                                                     const int dst_width,
+                                                     const int dst_height) {
+  return (2 * dst_width == src_width && 2 * dst_height == src_height) ||
+         (4 * dst_width == src_width && 4 * dst_height == src_height);
+}
+
+void av1_resize_and_extend_frame_neon_dotprod(const YV12_BUFFER_CONFIG *src,
+                                              YV12_BUFFER_CONFIG *dst,
+                                              const InterpFilter filter,
+                                              const int phase,
+                                              const int num_planes) {
+  assert(filter == BILINEAR || filter == EIGHTTAP_SMOOTH ||
+         filter == EIGHTTAP_REGULAR);
+
+  bool has_normative_scaler =
+      has_normative_scaler_neon_dotprod(src->y_crop_width, src->y_crop_height,
+                                        dst->y_crop_width, dst->y_crop_height);
+
+  if (num_planes > 1) {
+    has_normative_scaler =
+        has_normative_scaler && has_normative_scaler_neon_dotprod(
+                                    src->uv_crop_width, src->uv_crop_height,
+                                    dst->uv_crop_width, dst->uv_crop_height);
+  }
+
+  if (!has_normative_scaler || filter == BILINEAR || phase == 0) {
+    av1_resize_and_extend_frame_neon(src, dst, filter, phase, num_planes);
+    return;
+  }
+
+  // We use AOMMIN(num_planes, MAX_MB_PLANE) instead of num_planes to quiet
+  // the static analysis warnings.
+  int malloc_failed = 0;
+  for (int i = 0; i < AOMMIN(num_planes, MAX_MB_PLANE); ++i) {
+    const int is_uv = i > 0;
+    const int src_w = src->crop_widths[is_uv];
+    const int src_h = src->crop_heights[is_uv];
+    const int dst_w = dst->crop_widths[is_uv];
+    const int dst_h = dst->crop_heights[is_uv];
+    const int dst_y_w = (dst->crop_widths[0] + 1) & ~1;
+    const int dst_y_h = (dst->crop_heights[0] + 1) & ~1;
+
+    if (2 * dst_w == src_w && 2 * dst_h == src_h) {
+      const int buffer_stride = (dst_y_w + 7) & ~7;
+      const int buffer_height = (2 * dst_y_h + SUBPEL_TAPS - 2 + 7) & ~7;
+      uint8_t *const temp_buffer =
+          (uint8_t *)malloc(buffer_stride * buffer_height);
+      if (!temp_buffer) {
+        malloc_failed = 1;
+        break;
+      }
+      const InterpKernel *interp_kernel =
+          (const InterpKernel *)av1_interp_filter_params_list[filter]
+              .filter_ptr;
+      scale_plane_2_to_1_8tap(src->buffers[i], src->strides[is_uv],
+                              dst->buffers[i], dst->strides[is_uv], dst_w,
+                              dst_h, interp_kernel[phase], temp_buffer);
+      free(temp_buffer);
+    } else if (4 * dst_w == src_w && 4 * dst_h == src_h) {
+      const int buffer_stride = (dst_y_w + 1) & ~1;
+      const int buffer_height = (4 * dst_y_h + SUBPEL_TAPS - 2 + 7) & ~7;
+      uint8_t *const temp_buffer =
+          (uint8_t *)malloc(buffer_stride * buffer_height);
+      if (!temp_buffer) {
+        malloc_failed = 1;
+        break;
+      }
+      const InterpKernel *interp_kernel =
+          (const InterpKernel *)av1_interp_filter_params_list[filter]
+              .filter_ptr;
+      scale_plane_4_to_1_8tap(src->buffers[i], src->strides[is_uv],
+                              dst->buffers[i], dst->strides[is_uv], dst_w,
+                              dst_h, interp_kernel[phase], temp_buffer);
+      free(temp_buffer);
+    }
+  }
+
+  if (malloc_failed) {
+    av1_resize_and_extend_frame_c(src, dst, filter, phase, num_planes);
+  } else {
+    aom_extend_frame_borders(dst, num_planes);
+  }
+}
diff --git a/av1/common/arm/resize_neon_i8mm.c b/av1/common/arm/resize_neon_i8mm.c
new file mode 100644
index 000000000..76e37a6a8
--- /dev/null
+++ b/av1/common/arm/resize_neon_i8mm.c
@@ -0,0 +1,286 @@
+/*
+ * Copyright (c) 2024, Alliance for Open Media. All rights reserved.
+ *
+ * This source code is subject to the terms of the BSD 2 Clause License and
+ * the Alliance for Open Media Patent License 1.0. If the BSD 2 Clause License
+ * was not distributed with this source code in the LICENSE file, you can
+ * obtain it at www.aomedia.org/license/software. If the Alliance for Open
+ * Media Patent License 1.0 was not distributed with this source code in the
+ * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
+ */
+
+#include <arm_neon.h>
+#include <assert.h>
+
+#include "aom_dsp/arm/mem_neon.h"
+#include "aom_dsp/arm/transpose_neon.h"
+#include "av1/common/arm/resize_neon.h"
+#include "av1/common/resize.h"
+#include "config/aom_scale_rtcd.h"
+#include "config/av1_rtcd.h"
+
+// clang-format off
+DECLARE_ALIGNED(16, static const uint8_t, kScalePermuteTbl[16]) = {
+  0, 1, 2, 3, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11
+};
+// clang-format on
+
+static inline uint8x8_t scale_2_to_1_filter8_8(const uint8x16_t s0,
+                                               const uint8x16_t s1,
+                                               const uint8x16_t permute_tbl,
+                                               const int8x16_t filters) {
+  // Permute samples ready for matrix multiply.
+  // { 0, 1, 2, 3, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11 }
+  uint8x16_t perm_samples[2] = { vqtbl1q_u8(s0, permute_tbl),
+                                 vqtbl1q_u8(s1, permute_tbl) };
+
+  // These instructions multiply a 2x8 matrix (samples) by an 8x2 matrix
+  // (filter), destructively accumulating into the destination register.
+  int32x4_t sum0123 = vusmmlaq_s32(vdupq_n_s32(0), perm_samples[0], filters);
+  int32x4_t sum4567 = vusmmlaq_s32(vdupq_n_s32(0), perm_samples[1], filters);
+
+  int16x8_t sum = vcombine_s16(vmovn_s32(sum0123), vmovn_s32(sum4567));
+
+  // We halved the filter values so -1 from right shift.
+  return vqrshrun_n_s16(sum, FILTER_BITS - 1);
+}
+
+static inline void scale_2_to_1_horiz_6tap(const uint8_t *src,
+                                           const int src_stride, int w, int h,
+                                           uint8_t *dst, const int dst_stride,
+                                           const int16x8_t filter) {
+  const int8x8_t filter_s8 = vmovn_s16(filter);
+  // Stagger the filter for use with the matrix multiply instructions.
+  // { f1, f2, f3, f4, f5, f6, 0, 0, 0, 0, f1, f2, f3, f4, f5, f6 }
+  const int8x16_t filters = vcombine_s8(vext_s8(filter_s8, filter_s8, 1),
+                                        vext_s8(filter_s8, filter_s8, 7));
+  const uint8x16_t permute_tbl = vld1q_u8(kScalePermuteTbl);
+
+  do {
+    const uint8_t *s = src;
+    uint8_t *d = dst;
+    int width = w;
+
+    do {
+      uint8x16_t s0[2], s1[2], s2[2], s3[2], s4[2], s5[2], s6[2], s7[2];
+      load_u8_16x8(s, src_stride, &s0[0], &s1[0], &s2[0], &s3[0], &s4[0],
+                   &s5[0], &s6[0], &s7[0]);
+      load_u8_16x8(s + 8, src_stride, &s0[1], &s1[1], &s2[1], &s3[1], &s4[1],
+                   &s5[1], &s6[1], &s7[1]);
+
+      uint8x8_t d0 = scale_2_to_1_filter8_8(s0[0], s0[1], permute_tbl, filters);
+      uint8x8_t d1 = scale_2_to_1_filter8_8(s1[0], s1[1], permute_tbl, filters);
+      uint8x8_t d2 = scale_2_to_1_filter8_8(s2[0], s2[1], permute_tbl, filters);
+      uint8x8_t d3 = scale_2_to_1_filter8_8(s3[0], s3[1], permute_tbl, filters);
+
+      uint8x8_t d4 = scale_2_to_1_filter8_8(s4[0], s4[1], permute_tbl, filters);
+      uint8x8_t d5 = scale_2_to_1_filter8_8(s5[0], s5[1], permute_tbl, filters);
+      uint8x8_t d6 = scale_2_to_1_filter8_8(s6[0], s6[1], permute_tbl, filters);
+      uint8x8_t d7 = scale_2_to_1_filter8_8(s7[0], s7[1], permute_tbl, filters);
+
+      store_u8_8x8(d, dst_stride, d0, d1, d2, d3, d4, d5, d6, d7);
+
+      d += 8;
+      s += 16;
+      width -= 8;
+    } while (width > 0);
+
+    dst += 8 * dst_stride;
+    src += 8 * src_stride;
+    h -= 8;
+  } while (h > 0);
+}
+
+static inline void scale_plane_2_to_1_6tap(const uint8_t *src,
+                                           const int src_stride, uint8_t *dst,
+                                           const int dst_stride, const int w,
+                                           const int h,
+                                           const int16_t *const filter_ptr,
+                                           uint8_t *const im_block) {
+  assert(w > 0 && h > 0);
+
+  const int im_h = 2 * h + SUBPEL_TAPS - 3;
+  const int im_stride = (w + 7) & ~7;
+  // All filter values are even, halve them to fit in int8_t when applying
+  // horizontal filter and stay in 16-bit elements when applying vertical
+  // filter.
+  const int16x8_t filters = vshrq_n_s16(vld1q_s16(filter_ptr), 1);
+
+  const ptrdiff_t horiz_offset = SUBPEL_TAPS / 2 - 2;
+  const ptrdiff_t vert_offset = (SUBPEL_TAPS / 2 - 2) * src_stride;
+
+  scale_2_to_1_horiz_6tap(src - horiz_offset - vert_offset, src_stride, w, im_h,
+                          im_block, im_stride, filters);
+
+  scale_2_to_1_vert_6tap(im_block, im_stride, w, h, dst, dst_stride, filters);
+}
+
+static inline uint8x8_t scale_4_to_1_filter8_8(
+    const uint8x16_t s0, const uint8x16_t s1, const uint8x16_t s2,
+    const uint8x16_t s3, const uint8x16_t permute_tbl, const int8x8_t filter) {
+  int8x16_t filters = vcombine_s8(filter, filter);
+
+  uint8x16_t perm_samples[4] = { vqtbl1q_u8(s0, permute_tbl),
+                                 vqtbl1q_u8(s1, permute_tbl),
+                                 vqtbl1q_u8(s2, permute_tbl),
+                                 vqtbl1q_u8(s3, permute_tbl) };
+
+  int32x4_t sum0 = vusdotq_s32(vdupq_n_s32(0), perm_samples[0], filters);
+  int32x4_t sum1 = vusdotq_s32(vdupq_n_s32(0), perm_samples[1], filters);
+  int32x4_t sum2 = vusdotq_s32(vdupq_n_s32(0), perm_samples[2], filters);
+  int32x4_t sum3 = vusdotq_s32(vdupq_n_s32(0), perm_samples[3], filters);
+
+  int32x4_t sum01 = vpaddq_s32(sum0, sum1);
+  int32x4_t sum23 = vpaddq_s32(sum2, sum3);
+
+  int16x8_t sum = vcombine_s16(vmovn_s32(sum01), vmovn_s32(sum23));
+
+  // We halved the filter values so -1 from right shift.
+  return vqrshrun_n_s16(sum, FILTER_BITS - 1);
+}
+
+static inline void scale_4_to_1_horiz_8tap(const uint8_t *src,
+                                           const int src_stride, int w, int h,
+                                           uint8_t *dst, const int dst_stride,
+                                           const int16x8_t filters) {
+  const int8x8_t filter = vmovn_s16(filters);
+  const uint8x16_t permute_tbl = vld1q_u8(kScalePermuteTbl);
+
+  do {
+    const uint8_t *s = src;
+    uint8_t *d = dst;
+    int width = w;
+
+    do {
+      uint8x16_t s0, s1, s2, s3, s4, s5, s6, s7;
+      load_u8_16x8(s, src_stride, &s0, &s1, &s2, &s3, &s4, &s5, &s6, &s7);
+
+      uint8x8_t d0 =
+          scale_4_to_1_filter8_8(s0, s1, s2, s3, permute_tbl, filter);
+      uint8x8_t d1 =
+          scale_4_to_1_filter8_8(s4, s5, s6, s7, permute_tbl, filter);
+
+      store_u8x2_strided_x4(d + 0 * dst_stride, dst_stride, d0);
+      store_u8x2_strided_x4(d + 4 * dst_stride, dst_stride, d1);
+
+      d += 2;
+      s += 8;
+      width -= 2;
+    } while (width > 0);
+
+    dst += 8 * dst_stride;
+    src += 8 * src_stride;
+    h -= 8;
+  } while (h > 0);
+}
+
+static inline void scale_plane_4_to_1_8tap(const uint8_t *src,
+                                           const int src_stride, uint8_t *dst,
+                                           const int dst_stride, const int w,
+                                           const int h,
+                                           const int16_t *const filter_ptr,
+                                           uint8_t *const im_block) {
+  assert(w > 0 && h > 0);
+  const int im_h = 4 * h + SUBPEL_TAPS - 3;
+  const int im_stride = (w + 1) & ~1;
+  // All filter values are even, halve them to fit in int8_t when applying
+  // horizontal filter and stay in 16-bit elements when applying vertical
+  // filter.
+  const int16x8_t filters = vshrq_n_s16(vld1q_s16(filter_ptr), 1);
+
+  const ptrdiff_t horiz_offset = SUBPEL_TAPS / 2 - 1;
+  const ptrdiff_t vert_offset = (SUBPEL_TAPS / 2 - 2) * src_stride;
+
+  scale_4_to_1_horiz_8tap(src - horiz_offset - vert_offset, src_stride, w, im_h,
+                          im_block, im_stride, filters);
+
+  // We can specialise the vertical filtering for 6-tap filters given that the
+  // EIGHTTAP_SMOOTH and EIGHTTAP_REGULAR filters are 0-padded.
+  scale_4_to_1_vert_6tap(im_block, im_stride, w, h, dst, dst_stride, filters);
+}
+
+static inline bool has_normative_scaler_neon_i8mm(const int src_width,
+                                                  const int src_height,
+                                                  const int dst_width,
+                                                  const int dst_height) {
+  return (2 * dst_width == src_width && 2 * dst_height == src_height) ||
+         (4 * dst_width == src_width && 4 * dst_height == src_height);
+}
+
+void av1_resize_and_extend_frame_neon_i8mm(const YV12_BUFFER_CONFIG *src,
+                                           YV12_BUFFER_CONFIG *dst,
+                                           const InterpFilter filter,
+                                           const int phase,
+                                           const int num_planes) {
+  assert(filter == BILINEAR || filter == EIGHTTAP_SMOOTH ||
+         filter == EIGHTTAP_REGULAR);
+
+  bool has_normative_scaler =
+      has_normative_scaler_neon_i8mm(src->y_crop_width, src->y_crop_height,
+                                     dst->y_crop_width, dst->y_crop_height);
+
+  if (num_planes > 1) {
+    has_normative_scaler =
+        has_normative_scaler &&
+        has_normative_scaler_neon_i8mm(src->uv_crop_width, src->uv_crop_height,
+                                       dst->uv_crop_width, dst->uv_crop_height);
+  }
+
+  if (!has_normative_scaler || filter == BILINEAR || phase == 0) {
+    av1_resize_and_extend_frame_neon(src, dst, filter, phase, num_planes);
+    return;
+  }
+
+  // We use AOMMIN(num_planes, MAX_MB_PLANE) instead of num_planes to quiet
+  // the static analysis warnings.
+  int malloc_failed = 0;
+  for (int i = 0; i < AOMMIN(num_planes, MAX_MB_PLANE); ++i) {
+    const int is_uv = i > 0;
+    const int src_w = src->crop_widths[is_uv];
+    const int src_h = src->crop_heights[is_uv];
+    const int dst_w = dst->crop_widths[is_uv];
+    const int dst_h = dst->crop_heights[is_uv];
+    const int dst_y_w = (dst->crop_widths[0] + 1) & ~1;
+    const int dst_y_h = (dst->crop_heights[0] + 1) & ~1;
+
+    if (2 * dst_w == src_w && 2 * dst_h == src_h) {
+      const int buffer_stride = (dst_y_w + 7) & ~7;
+      const int buffer_height = (2 * dst_y_h + SUBPEL_TAPS - 2 + 7) & ~7;
+      uint8_t *const temp_buffer =
+          (uint8_t *)malloc(buffer_stride * buffer_height);
+      if (!temp_buffer) {
+        malloc_failed = 1;
+        break;
+      }
+      const InterpKernel *interp_kernel =
+          (const InterpKernel *)av1_interp_filter_params_list[filter]
+              .filter_ptr;
+      scale_plane_2_to_1_6tap(src->buffers[i], src->strides[is_uv],
+                              dst->buffers[i], dst->strides[is_uv], dst_w,
+                              dst_h, interp_kernel[phase], temp_buffer);
+      free(temp_buffer);
+    } else if (4 * dst_w == src_w && 4 * dst_h == src_h) {
+      const int buffer_stride = (dst_y_w + 1) & ~1;
+      const int buffer_height = (4 * dst_y_h + SUBPEL_TAPS - 2 + 7) & ~7;
+      uint8_t *const temp_buffer =
+          (uint8_t *)malloc(buffer_stride * buffer_height);
+      if (!temp_buffer) {
+        malloc_failed = 1;
+        break;
+      }
+      const InterpKernel *interp_kernel =
+          (const InterpKernel *)av1_interp_filter_params_list[filter]
+              .filter_ptr;
+      scale_plane_4_to_1_8tap(src->buffers[i], src->strides[is_uv],
+                              dst->buffers[i], dst->strides[is_uv], dst_w,
+                              dst_h, interp_kernel[phase], temp_buffer);
+      free(temp_buffer);
+    }
+  }
+
+  if (malloc_failed) {
+    av1_resize_and_extend_frame_c(src, dst, filter, phase, num_planes);
+  } else {
+    aom_extend_frame_borders(dst, num_planes);
+  }
+}
diff --git a/av1/common/arm/warp_plane_neon.c b/av1/common/arm/warp_plane_neon.c
index 2604eaf27..3656beb39 100644
--- a/av1/common/arm/warp_plane_neon.c
+++ b/av1/common/arm/warp_plane_neon.c
@@ -75,13 +75,10 @@ static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f8(const uint8x16_t in,
   return vreinterpretq_s16_u16(res);
 }
 
-static AOM_FORCE_INLINE int16x8_t horizontal_filter_4x1_f1(const uint8x16_t in,
-                                                           int sx) {
+static AOM_FORCE_INLINE int16x8_t
+horizontal_filter_4x1_f1_beta0(const uint8x16_t in, int16x8_t f_s16) {
   const int32x4_t add_const = vdupq_n_s32(1 << (8 + FILTER_BITS - 1));
 
-  int16x8_t f_s16 =
-      vld1q_s16((int16_t *)(av1_warped_filter + (sx >> WARPEDDIFF_PREC_BITS)));
-
   int16x8_t in16_lo = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(in)));
   int16x8_t in16_hi = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(in)));
 
@@ -102,12 +99,16 @@ static AOM_FORCE_INLINE int16x8_t horizontal_filter_4x1_f1(const uint8x16_t in,
   return vreinterpretq_s16_u16(res);
 }
 
-static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f1(const uint8x16_t in,
+static AOM_FORCE_INLINE int16x8_t horizontal_filter_4x1_f1(const uint8x16_t in,
                                                            int sx) {
-  const int32x4_t add_const = vdupq_n_s32(1 << (8 + FILTER_BITS - 1));
-
   int16x8_t f_s16 =
       vld1q_s16((int16_t *)(av1_warped_filter + (sx >> WARPEDDIFF_PREC_BITS)));
+  return horizontal_filter_4x1_f1_beta0(in, f_s16);
+}
+
+static AOM_FORCE_INLINE int16x8_t
+horizontal_filter_8x1_f1_beta0(const uint8x16_t in, int16x8_t f_s16) {
+  const int32x4_t add_const = vdupq_n_s32(1 << (8 + FILTER_BITS - 1));
 
   int16x8_t in16_lo = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(in)));
   int16x8_t in16_hi = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(in)));
@@ -137,6 +138,13 @@ static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f1(const uint8x16_t in,
   return vreinterpretq_s16_u16(res);
 }
 
+static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f1(const uint8x16_t in,
+                                                           int sx) {
+  int16x8_t f_s16 =
+      vld1q_s16((int16_t *)(av1_warped_filter + (sx >> WARPEDDIFF_PREC_BITS)));
+  return horizontal_filter_8x1_f1_beta0(in, f_s16);
+}
+
 static AOM_FORCE_INLINE void vertical_filter_4x1_f1(const int16x8_t *src,
                                                     int32x4_t *res, int sy) {
   int16x4_t s0 = vget_low_s16(src[0]);
diff --git a/av1/common/arm/warp_plane_neon.h b/av1/common/arm/warp_plane_neon.h
index 7fceb3aaf..777ac4b95 100644
--- a/av1/common/arm/warp_plane_neon.h
+++ b/av1/common/arm/warp_plane_neon.h
@@ -36,6 +36,12 @@ static AOM_FORCE_INLINE int16x8_t horizontal_filter_4x1_f1(const uint8x16_t in,
 static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f1(const uint8x16_t in,
                                                            int sx);
 
+static AOM_FORCE_INLINE int16x8_t
+horizontal_filter_4x1_f1_beta0(const uint8x16_t in, int16x8_t f_s16);
+
+static AOM_FORCE_INLINE int16x8_t
+horizontal_filter_8x1_f1_beta0(const uint8x16_t in, int16x8_t f_s16);
+
 static AOM_FORCE_INLINE void vertical_filter_4x1_f1(const int16x8_t *src,
                                                     int32x4_t *res, int sy);
 
@@ -169,7 +175,9 @@ static AOM_FORCE_INLINE void warp_affine_horizontal(
   if (p_width == 4) {
     if (beta == 0) {
       if (alpha == 0) {
-        APPLY_HORIZONTAL_SHIFT(horizontal_filter_4x1_f1, sx4);
+        int16x8_t f_s16 = vld1q_s16(
+            (int16_t *)(av1_warped_filter + (sx4 >> WARPEDDIFF_PREC_BITS)));
+        APPLY_HORIZONTAL_SHIFT(horizontal_filter_4x1_f1_beta0, f_s16);
       } else {
         APPLY_HORIZONTAL_SHIFT(horizontal_filter_4x1_f4, sx4, alpha);
       }
@@ -185,7 +193,9 @@ static AOM_FORCE_INLINE void warp_affine_horizontal(
   } else {
     if (beta == 0) {
       if (alpha == 0) {
-        APPLY_HORIZONTAL_SHIFT(horizontal_filter_8x1_f1, sx4);
+        int16x8_t f_s16 = vld1q_s16(
+            (int16_t *)(av1_warped_filter + (sx4 >> WARPEDDIFF_PREC_BITS)));
+        APPLY_HORIZONTAL_SHIFT(horizontal_filter_8x1_f1_beta0, f_s16);
       } else {
         APPLY_HORIZONTAL_SHIFT(horizontal_filter_8x1_f8, sx4, alpha);
       }
diff --git a/av1/common/arm/warp_plane_neon_i8mm.c b/av1/common/arm/warp_plane_neon_i8mm.c
index 9ccc863eb..7ef762692 100644
--- a/av1/common/arm/warp_plane_neon_i8mm.c
+++ b/av1/common/arm/warp_plane_neon_i8mm.c
@@ -19,7 +19,10 @@ DECLARE_ALIGNED(16, static const uint8_t, usdot_permute_idx[48]) = {
 
 static AOM_FORCE_INLINE int16x8_t horizontal_filter_4x1_f4(const uint8x16_t in,
                                                            int sx, int alpha) {
-  const int32x4_t add_const = vdupq_n_s32(1 << (8 + FILTER_BITS - 1));
+  // Only put the constant in every other lane to avoid double-counting when
+  // performing the pairwise add later.
+  const int32x4_t add_const =
+      vreinterpretq_s32_u64(vdupq_n_u64(1 << (8 + FILTER_BITS - 1)));
 
   // Loading the 8 filter taps
   int16x8_t f[4];
@@ -33,21 +36,22 @@ static AOM_FORCE_INLINE int16x8_t horizontal_filter_4x1_f4(const uint8x16_t in,
   uint8x8_t in2 = vget_low_u8(vextq_u8(in, in, 2));
   uint8x8_t in3 = vget_low_u8(vextq_u8(in, in, 3));
 
-  int32x4_t m01 = vusdotq_s32(vdupq_n_s32(0), vcombine_u8(in0, in1), f01_u8);
-  int32x4_t m23 = vusdotq_s32(vdupq_n_s32(0), vcombine_u8(in2, in3), f23_u8);
-
-  int32x4_t tmp_res_low = vpaddq_s32(m01, m23);
+  int32x4_t m01 = vusdotq_s32(add_const, vcombine_u8(in0, in1), f01_u8);
+  int32x4_t m23 = vusdotq_s32(add_const, vcombine_u8(in2, in3), f23_u8);
 
-  tmp_res_low = vaddq_s32(tmp_res_low, add_const);
+  int32x4_t m0123 = vpaddq_s32(m01, m23);
 
   uint16x8_t res =
-      vcombine_u16(vqrshrun_n_s32(tmp_res_low, ROUND0_BITS), vdup_n_u16(0));
+      vcombine_u16(vqrshrun_n_s32(m0123, ROUND0_BITS), vdup_n_u16(0));
   return vreinterpretq_s16_u16(res);
 }
 
 static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f8(const uint8x16_t in,
                                                            int sx, int alpha) {
-  const int32x4_t add_const = vdupq_n_s32(1 << (8 + FILTER_BITS - 1));
+  // Only put the constant in every other lane to avoid double-counting when
+  // performing the pairwise add later.
+  const int32x4_t add_const =
+      vreinterpretq_s32_u64(vdupq_n_u64(1 << (8 + FILTER_BITS - 1)));
 
   // Loading the 8 filter taps
   int16x8_t f[8];
@@ -67,29 +71,23 @@ static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f8(const uint8x16_t in,
   uint8x8_t in6 = vget_low_u8(vextq_u8(in, in, 6));
   uint8x8_t in7 = vget_low_u8(vextq_u8(in, in, 7));
 
-  int32x4_t m01 = vusdotq_s32(vdupq_n_s32(0), vcombine_u8(in0, in1), f01_u8);
-  int32x4_t m23 = vusdotq_s32(vdupq_n_s32(0), vcombine_u8(in2, in3), f23_u8);
-  int32x4_t m45 = vusdotq_s32(vdupq_n_s32(0), vcombine_u8(in4, in5), f45_u8);
-  int32x4_t m67 = vusdotq_s32(vdupq_n_s32(0), vcombine_u8(in6, in7), f67_u8);
-
-  int32x4_t tmp_res_low = vpaddq_s32(m01, m23);
-  int32x4_t tmp_res_high = vpaddq_s32(m45, m67);
+  int32x4_t m01 = vusdotq_s32(add_const, vcombine_u8(in0, in1), f01_u8);
+  int32x4_t m23 = vusdotq_s32(add_const, vcombine_u8(in2, in3), f23_u8);
+  int32x4_t m45 = vusdotq_s32(add_const, vcombine_u8(in4, in5), f45_u8);
+  int32x4_t m67 = vusdotq_s32(add_const, vcombine_u8(in6, in7), f67_u8);
 
-  tmp_res_low = vaddq_s32(tmp_res_low, add_const);
-  tmp_res_high = vaddq_s32(tmp_res_high, add_const);
+  int32x4_t m0123 = vpaddq_s32(m01, m23);
+  int32x4_t m4567 = vpaddq_s32(m45, m67);
 
-  uint16x8_t res = vcombine_u16(vqrshrun_n_s32(tmp_res_low, ROUND0_BITS),
-                                vqrshrun_n_s32(tmp_res_high, ROUND0_BITS));
+  uint16x8_t res = vcombine_u16(vqrshrun_n_s32(m0123, ROUND0_BITS),
+                                vqrshrun_n_s32(m4567, ROUND0_BITS));
   return vreinterpretq_s16_u16(res);
 }
 
-static AOM_FORCE_INLINE int16x8_t horizontal_filter_4x1_f1(const uint8x16_t in,
-                                                           int sx) {
+static AOM_FORCE_INLINE int16x8_t
+horizontal_filter_4x1_f1_beta0(const uint8x16_t in, int16x8_t f_s16) {
   const int32x4_t add_const = vdupq_n_s32(1 << (8 + FILTER_BITS - 1));
 
-  int16x8_t f_s16 =
-      vld1q_s16((int16_t *)(av1_warped_filter + (sx >> WARPEDDIFF_PREC_BITS)));
-
   int8x16_t f_s8 = vcombine_s8(vmovn_s16(f_s16), vmovn_s16(f_s16));
 
   uint8x16_t perm0 = vld1q_u8(&usdot_permute_idx[0]);
@@ -101,24 +99,24 @@ static AOM_FORCE_INLINE int16x8_t horizontal_filter_4x1_f1(const uint8x16_t in,
   uint8x16_t in_0123 = vqtbl1q_u8(in, perm0);
   uint8x16_t in_4567 = vqtbl1q_u8(in, perm1);
 
-  int32x4_t m0123 = vusdotq_laneq_s32(vdupq_n_s32(0), in_0123, f_s8, 0);
+  int32x4_t m0123 = vusdotq_laneq_s32(add_const, in_0123, f_s8, 0);
   m0123 = vusdotq_laneq_s32(m0123, in_4567, f_s8, 1);
 
-  int32x4_t tmp_res_low = m0123;
-
-  tmp_res_low = vaddq_s32(tmp_res_low, add_const);
-
   uint16x8_t res =
-      vcombine_u16(vqrshrun_n_s32(tmp_res_low, ROUND0_BITS), vdup_n_u16(0));
+      vcombine_u16(vqrshrun_n_s32(m0123, ROUND0_BITS), vdup_n_u16(0));
   return vreinterpretq_s16_u16(res);
 }
 
-static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f1(const uint8x16_t in,
+static AOM_FORCE_INLINE int16x8_t horizontal_filter_4x1_f1(const uint8x16_t in,
                                                            int sx) {
-  const int32x4_t add_const = vdupq_n_s32(1 << (8 + FILTER_BITS - 1));
-
   int16x8_t f_s16 =
       vld1q_s16((int16_t *)(av1_warped_filter + (sx >> WARPEDDIFF_PREC_BITS)));
+  return horizontal_filter_4x1_f1_beta0(in, f_s16);
+}
+
+static AOM_FORCE_INLINE int16x8_t
+horizontal_filter_8x1_f1_beta0(const uint8x16_t in, int16x8_t f_s16) {
+  const int32x4_t add_const = vdupq_n_s32(1 << (8 + FILTER_BITS - 1));
 
   int8x16_t f_s8 = vcombine_s8(vmovn_s16(f_s16), vmovn_s16(f_s16));
 
@@ -134,23 +132,24 @@ static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f1(const uint8x16_t in,
   uint8x16_t in_4567 = vqtbl1q_u8(in, perm1);
   uint8x16_t in_89ab = vqtbl1q_u8(in, perm2);
 
-  int32x4_t m0123 = vusdotq_laneq_s32(vdupq_n_s32(0), in_0123, f_s8, 0);
+  int32x4_t m0123 = vusdotq_laneq_s32(add_const, in_0123, f_s8, 0);
   m0123 = vusdotq_laneq_s32(m0123, in_4567, f_s8, 1);
 
-  int32x4_t m4567 = vusdotq_laneq_s32(vdupq_n_s32(0), in_4567, f_s8, 0);
+  int32x4_t m4567 = vusdotq_laneq_s32(add_const, in_4567, f_s8, 0);
   m4567 = vusdotq_laneq_s32(m4567, in_89ab, f_s8, 1);
 
-  int32x4_t tmp_res_low = m0123;
-  int32x4_t tmp_res_high = m4567;
-
-  tmp_res_low = vaddq_s32(tmp_res_low, add_const);
-  tmp_res_high = vaddq_s32(tmp_res_high, add_const);
-
-  uint16x8_t res = vcombine_u16(vqrshrun_n_s32(tmp_res_low, ROUND0_BITS),
-                                vqrshrun_n_s32(tmp_res_high, ROUND0_BITS));
+  uint16x8_t res = vcombine_u16(vqrshrun_n_s32(m0123, ROUND0_BITS),
+                                vqrshrun_n_s32(m4567, ROUND0_BITS));
   return vreinterpretq_s16_u16(res);
 }
 
+static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f1(const uint8x16_t in,
+                                                           int sx) {
+  int16x8_t f_s16 =
+      vld1q_s16((int16_t *)(av1_warped_filter + (sx >> WARPEDDIFF_PREC_BITS)));
+  return horizontal_filter_8x1_f1_beta0(in, f_s16);
+}
+
 static AOM_FORCE_INLINE void vertical_filter_4x1_f1(const int16x8_t *src,
                                                     int32x4_t *res, int sy) {
   int16x4_t s0 = vget_low_s16(src[0]);
diff --git a/av1/common/arm/warp_plane_sve.c b/av1/common/arm/warp_plane_sve.c
index 9d5761b05..51b1bb75e 100644
--- a/av1/common/arm/warp_plane_sve.c
+++ b/av1/common/arm/warp_plane_sve.c
@@ -22,7 +22,10 @@ DECLARE_ALIGNED(16, static const uint8_t, usdot_permute_idx[48]) = {
 
 static AOM_FORCE_INLINE int16x8_t horizontal_filter_4x1_f4(const uint8x16_t in,
                                                            int sx, int alpha) {
-  const int32x4_t add_const = vdupq_n_s32(1 << (8 + FILTER_BITS - 1));
+  // Only put the constant in every other lane to avoid double-counting when
+  // performing the pairwise add later.
+  const int32x4_t add_const =
+      vreinterpretq_s32_u64(vdupq_n_u64(1 << (8 + FILTER_BITS - 1)));
 
   // Loading the 8 filter taps
   int16x8_t f[4];
@@ -36,21 +39,22 @@ static AOM_FORCE_INLINE int16x8_t horizontal_filter_4x1_f4(const uint8x16_t in,
   uint8x8_t in2 = vget_low_u8(vextq_u8(in, in, 2));
   uint8x8_t in3 = vget_low_u8(vextq_u8(in, in, 3));
 
-  int32x4_t m01 = vusdotq_s32(vdupq_n_s32(0), vcombine_u8(in0, in1), f01_u8);
-  int32x4_t m23 = vusdotq_s32(vdupq_n_s32(0), vcombine_u8(in2, in3), f23_u8);
-
-  int32x4_t tmp_res_low = vpaddq_s32(m01, m23);
+  int32x4_t m01 = vusdotq_s32(add_const, vcombine_u8(in0, in1), f01_u8);
+  int32x4_t m23 = vusdotq_s32(add_const, vcombine_u8(in2, in3), f23_u8);
 
-  tmp_res_low = vaddq_s32(tmp_res_low, add_const);
+  int32x4_t m0123 = vpaddq_s32(m01, m23);
 
   uint16x8_t res =
-      vcombine_u16(vqrshrun_n_s32(tmp_res_low, ROUND0_BITS), vdup_n_u16(0));
+      vcombine_u16(vqrshrun_n_s32(m0123, ROUND0_BITS), vdup_n_u16(0));
   return vreinterpretq_s16_u16(res);
 }
 
 static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f8(const uint8x16_t in,
                                                            int sx, int alpha) {
-  const int32x4_t add_const = vdupq_n_s32(1 << (8 + FILTER_BITS - 1));
+  // Only put the constant in every other lane to avoid double-counting when
+  // performing the pairwise add later.
+  const int32x4_t add_const =
+      vreinterpretq_s32_u64(vdupq_n_u64(1 << (8 + FILTER_BITS - 1)));
 
   // Loading the 8 filter taps
   int16x8_t f[8];
@@ -70,29 +74,23 @@ static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f8(const uint8x16_t in,
   uint8x8_t in6 = vget_low_u8(vextq_u8(in, in, 6));
   uint8x8_t in7 = vget_low_u8(vextq_u8(in, in, 7));
 
-  int32x4_t m01 = vusdotq_s32(vdupq_n_s32(0), vcombine_u8(in0, in1), f01_u8);
-  int32x4_t m23 = vusdotq_s32(vdupq_n_s32(0), vcombine_u8(in2, in3), f23_u8);
-  int32x4_t m45 = vusdotq_s32(vdupq_n_s32(0), vcombine_u8(in4, in5), f45_u8);
-  int32x4_t m67 = vusdotq_s32(vdupq_n_s32(0), vcombine_u8(in6, in7), f67_u8);
-
-  int32x4_t tmp_res_low = vpaddq_s32(m01, m23);
-  int32x4_t tmp_res_high = vpaddq_s32(m45, m67);
+  int32x4_t m01 = vusdotq_s32(add_const, vcombine_u8(in0, in1), f01_u8);
+  int32x4_t m23 = vusdotq_s32(add_const, vcombine_u8(in2, in3), f23_u8);
+  int32x4_t m45 = vusdotq_s32(add_const, vcombine_u8(in4, in5), f45_u8);
+  int32x4_t m67 = vusdotq_s32(add_const, vcombine_u8(in6, in7), f67_u8);
 
-  tmp_res_low = vaddq_s32(tmp_res_low, add_const);
-  tmp_res_high = vaddq_s32(tmp_res_high, add_const);
+  int32x4_t m0123 = vpaddq_s32(m01, m23);
+  int32x4_t m4567 = vpaddq_s32(m45, m67);
 
-  uint16x8_t res = vcombine_u16(vqrshrun_n_s32(tmp_res_low, ROUND0_BITS),
-                                vqrshrun_n_s32(tmp_res_high, ROUND0_BITS));
+  uint16x8_t res = vcombine_u16(vqrshrun_n_s32(m0123, ROUND0_BITS),
+                                vqrshrun_n_s32(m4567, ROUND0_BITS));
   return vreinterpretq_s16_u16(res);
 }
 
-static AOM_FORCE_INLINE int16x8_t horizontal_filter_4x1_f1(const uint8x16_t in,
-                                                           int sx) {
+static AOM_FORCE_INLINE int16x8_t
+horizontal_filter_4x1_f1_beta0(const uint8x16_t in, int16x8_t f_s16) {
   const int32x4_t add_const = vdupq_n_s32(1 << (8 + FILTER_BITS - 1));
 
-  int16x8_t f_s16 =
-      vld1q_s16((int16_t *)(av1_warped_filter + (sx >> WARPEDDIFF_PREC_BITS)));
-
   int8x16_t f_s8 = vcombine_s8(vmovn_s16(f_s16), vmovn_s16(f_s16));
 
   uint8x16_t perm0 = vld1q_u8(&usdot_permute_idx[0]);
@@ -104,24 +102,24 @@ static AOM_FORCE_INLINE int16x8_t horizontal_filter_4x1_f1(const uint8x16_t in,
   uint8x16_t in_0123 = vqtbl1q_u8(in, perm0);
   uint8x16_t in_4567 = vqtbl1q_u8(in, perm1);
 
-  int32x4_t m0123 = vusdotq_laneq_s32(vdupq_n_s32(0), in_0123, f_s8, 0);
+  int32x4_t m0123 = vusdotq_laneq_s32(add_const, in_0123, f_s8, 0);
   m0123 = vusdotq_laneq_s32(m0123, in_4567, f_s8, 1);
 
-  int32x4_t tmp_res_low = m0123;
-
-  tmp_res_low = vaddq_s32(tmp_res_low, add_const);
-
   uint16x8_t res =
-      vcombine_u16(vqrshrun_n_s32(tmp_res_low, ROUND0_BITS), vdup_n_u16(0));
+      vcombine_u16(vqrshrun_n_s32(m0123, ROUND0_BITS), vdup_n_u16(0));
   return vreinterpretq_s16_u16(res);
 }
 
-static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f1(const uint8x16_t in,
+static AOM_FORCE_INLINE int16x8_t horizontal_filter_4x1_f1(const uint8x16_t in,
                                                            int sx) {
-  const int32x4_t add_const = vdupq_n_s32(1 << (8 + FILTER_BITS - 1));
-
   int16x8_t f_s16 =
       vld1q_s16((int16_t *)(av1_warped_filter + (sx >> WARPEDDIFF_PREC_BITS)));
+  return horizontal_filter_4x1_f1_beta0(in, f_s16);
+}
+
+static AOM_FORCE_INLINE int16x8_t
+horizontal_filter_8x1_f1_beta0(const uint8x16_t in, int16x8_t f_s16) {
+  const int32x4_t add_const = vdupq_n_s32(1 << (8 + FILTER_BITS - 1));
 
   int8x16_t f_s8 = vcombine_s8(vmovn_s16(f_s16), vmovn_s16(f_s16));
 
@@ -137,23 +135,24 @@ static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f1(const uint8x16_t in,
   uint8x16_t in_4567 = vqtbl1q_u8(in, perm1);
   uint8x16_t in_89ab = vqtbl1q_u8(in, perm2);
 
-  int32x4_t m0123 = vusdotq_laneq_s32(vdupq_n_s32(0), in_0123, f_s8, 0);
+  int32x4_t m0123 = vusdotq_laneq_s32(add_const, in_0123, f_s8, 0);
   m0123 = vusdotq_laneq_s32(m0123, in_4567, f_s8, 1);
 
-  int32x4_t m4567 = vusdotq_laneq_s32(vdupq_n_s32(0), in_4567, f_s8, 0);
+  int32x4_t m4567 = vusdotq_laneq_s32(add_const, in_4567, f_s8, 0);
   m4567 = vusdotq_laneq_s32(m4567, in_89ab, f_s8, 1);
 
-  int32x4_t tmp_res_low = m0123;
-  int32x4_t tmp_res_high = m4567;
-
-  tmp_res_low = vaddq_s32(tmp_res_low, add_const);
-  tmp_res_high = vaddq_s32(tmp_res_high, add_const);
-
-  uint16x8_t res = vcombine_u16(vqrshrun_n_s32(tmp_res_low, ROUND0_BITS),
-                                vqrshrun_n_s32(tmp_res_high, ROUND0_BITS));
+  uint16x8_t res = vcombine_u16(vqrshrun_n_s32(m0123, ROUND0_BITS),
+                                vqrshrun_n_s32(m4567, ROUND0_BITS));
   return vreinterpretq_s16_u16(res);
 }
 
+static AOM_FORCE_INLINE int16x8_t horizontal_filter_8x1_f1(const uint8x16_t in,
+                                                           int sx) {
+  int16x8_t f_s16 =
+      vld1q_s16((int16_t *)(av1_warped_filter + (sx >> WARPEDDIFF_PREC_BITS)));
+  return horizontal_filter_8x1_f1_beta0(in, f_s16);
+}
+
 static AOM_FORCE_INLINE void vertical_filter_4x1_f1(const int16x8_t *src,
                                                     int32x4_t *res, int sy) {
   int16x4_t s0 = vget_low_s16(src[0]);
diff --git a/av1/common/av1_common_int.h b/av1/common/av1_common_int.h
index 113b3f186..99ca54133 100644
--- a/av1/common/av1_common_int.h
+++ b/av1/common/av1_common_int.h
@@ -1257,7 +1257,9 @@ static inline void ensure_mv_buffer(RefCntBuffer *buf, AV1_COMMON *cm) {
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void cfl_init(CFL_CTX *cfl, const SequenceHeader *seq_params);
+#endif
 
 static inline int av1_num_planes(const AV1_COMMON *cm) {
   return cm->seq_params->monochrome ? 1 : MAX_MB_PLANE;
@@ -1300,7 +1302,9 @@ static inline void av1_init_macroblockd(AV1_COMMON *cm, MACROBLOCKD *xd) {
   }
   xd->mi_stride = cm->mi_params.mi_stride;
   xd->error_info = cm->error;
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
   cfl_init(&xd->cfl, cm->seq_params);
+#endif
 }
 
 static inline void set_entropy_context(MACROBLOCKD *xd, int mi_row, int mi_col,
diff --git a/av1/common/av1_rtcd_defs.pl b/av1/common/av1_rtcd_defs.pl
index 60bf5ca43..4ed92a250 100644
--- a/av1/common/av1_rtcd_defs.pl
+++ b/av1/common/av1_rtcd_defs.pl
@@ -104,12 +104,17 @@ if(aom_config("CONFIG_AV1_HIGHBITDEPTH") eq "yes") {
   add_proto qw/void av1_highbd_convolve_horiz_rs/, "const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const int16_t *x_filters, int x0_qn, int x_step_qn, int bd";
   specialize qw/av1_highbd_convolve_horiz_rs sse4_1 neon/;
 
-  add_proto qw/void av1_highbd_wiener_convolve_add_src/, "const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, const WienerConvolveParams *conv_params, int bd";
-  specialize qw/av1_highbd_wiener_convolve_add_src ssse3 avx2 neon/;
+  if ((aom_config("CONFIG_REALTIME_ONLY") ne "yes") ||
+      (aom_config("CONFIG_AV1_DECODER") eq "yes")) {
+    add_proto qw/void av1_highbd_wiener_convolve_add_src/, "const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, const WienerConvolveParams *conv_params, int bd";
+    specialize qw/av1_highbd_wiener_convolve_add_src ssse3 avx2 neon/;
+  }
 }
 
-add_proto qw/void av1_wiener_convolve_add_src/,       "const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, const WienerConvolveParams *conv_params";
-specialize qw/av1_wiener_convolve_add_src sse2 avx2 neon/;
+if ((aom_config("CONFIG_REALTIME_ONLY") ne "yes") || (aom_config("CONFIG_AV1_DECODER") eq "yes")) {
+  add_proto qw/void av1_wiener_convolve_add_src/,       "const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, const WienerConvolveParams *conv_params";
+  specialize qw/av1_wiener_convolve_add_src sse2 avx2 neon/;
+}
 
 # directional intra predictor functions
 add_proto qw/void av1_dr_prediction_z1/, "uint8_t *dst, ptrdiff_t stride, int bw, int bh, const uint8_t *above, const uint8_t *left, int upsample_above, int dx, int dy";
@@ -258,7 +263,7 @@ specialize "av1_round_shift_array", qw/sse4_1 neon/;
 
 # Resize functions.
 add_proto qw/void av1_resize_and_extend_frame/, "const YV12_BUFFER_CONFIG *src, YV12_BUFFER_CONFIG *dst, const InterpFilter filter, const int phase, const int num_planes";
-specialize qw/av1_resize_and_extend_frame ssse3 neon/;
+specialize qw/av1_resize_and_extend_frame ssse3 neon neon_dotprod neon_i8mm/;
 
 #
 # Encoder functions below this point.
@@ -503,7 +508,9 @@ add_proto qw/void cdef_filter_16_2/, "void *dst16, int dstride, const uint16_t *
 add_proto qw/void cdef_filter_16_3/, "void *dst16, int dstride, const uint16_t *in, int pri_strength, int sec_strength, int dir, int pri_damping, int sec_damping, int coeff_shift, int block_width, int block_height";
 
 add_proto qw/void cdef_copy_rect8_8bit_to_16bit/, "uint16_t *dst, int dstride, const uint8_t *src, int sstride, int width, int height";
-add_proto qw/void cdef_copy_rect8_16bit_to_16bit/, "uint16_t *dst, int dstride, const uint16_t *src, int sstride, int width, int height";
+if (aom_config("CONFIG_AV1_HIGHBITDEPTH") eq "yes") {
+  add_proto qw/void cdef_copy_rect8_16bit_to_16bit/, "uint16_t *dst, int dstride, const uint16_t *src, int sstride, int width, int height";
+}
 
 # VS compiling for 32 bit targets does not support vector types in
 # structs as arguments, which makes the v256 type of the intrinsics
@@ -523,11 +530,15 @@ if ($opts{config} !~ /libs-x86-win32-vs.*/) {
   specialize qw/cdef_filter_16_3 sse4_1 avx2 neon/, "$ssse3_x86";
 
   specialize qw/cdef_copy_rect8_8bit_to_16bit sse4_1 avx2 neon/, "$ssse3_x86";
-  specialize qw/cdef_copy_rect8_16bit_to_16bit sse4_1 avx2 neon/, "$ssse3_x86";
+  if (aom_config("CONFIG_AV1_HIGHBITDEPTH") eq "yes") {
+    specialize qw/cdef_copy_rect8_16bit_to_16bit sse4_1 avx2 neon/, "$ssse3_x86";
+  }
 }
 
 # WARPED_MOTION / GLOBAL_MOTION functions
-if (aom_config("CONFIG_AV1_HIGHBITDEPTH") eq "yes") {
+if (aom_config("CONFIG_AV1_HIGHBITDEPTH") eq "yes" &&
+    ((aom_config("CONFIG_REALTIME_ONLY") ne "yes") ||
+      (aom_config("CONFIG_AV1_DECODER") eq "yes"))) {
   add_proto qw/void av1_highbd_warp_affine/, "const int32_t *mat, const uint16_t *ref, int width, int height, int stride, uint16_t *pred, int p_col, int p_row, int p_width, int p_height, int p_stride, int subsampling_x, int subsampling_y, int bd, ConvolveParams *conv_params, int16_t alpha, int16_t beta, int16_t gamma, int16_t delta";
   # TODO(aomedia:349450845): enable NEON for armv7 after SIGBUS is fixed.
   if (aom_config("AOM_ARCH_ARM") eq "yes" && aom_config("AOM_ARCH_AARCH64") eq "") {
@@ -543,17 +554,21 @@ specialize qw/av1_resize_vert_dir sse2 avx2/;
 add_proto qw/void av1_resize_horz_dir/, "const uint8_t *const input, int in_stride, uint8_t *intbuf, int height, int filtered_length, int width2";
 specialize qw/av1_resize_horz_dir sse2 avx2/;
 
-add_proto qw/void av1_warp_affine/, "const int32_t *mat, const uint8_t *ref, int width, int height, int stride, uint8_t *pred, int p_col, int p_row, int p_width, int p_height, int p_stride, int subsampling_x, int subsampling_y, ConvolveParams *conv_params, int16_t alpha, int16_t beta, int16_t gamma, int16_t delta";
-specialize qw/av1_warp_affine sse4_1 avx2 neon neon_i8mm sve/;
+if ((aom_config("CONFIG_REALTIME_ONLY") ne "yes") || (aom_config("CONFIG_AV1_DECODER") eq "yes")) {
+  add_proto qw/void av1_warp_affine/, "const int32_t *mat, const uint8_t *ref, int width, int height, int stride, uint8_t *pred, int p_col, int p_row, int p_width, int p_height, int p_stride, int subsampling_x, int subsampling_y, ConvolveParams *conv_params, int16_t alpha, int16_t beta, int16_t gamma, int16_t delta";
+  specialize qw/av1_warp_affine sse4_1 avx2 neon neon_i8mm sve/;
+}
 
 # LOOP_RESTORATION functions
-add_proto qw/int av1_apply_selfguided_restoration/, "const uint8_t *dat, int width, int height, int stride, int eps, const int *xqd, uint8_t *dst, int dst_stride, int32_t *tmpbuf, int bit_depth, int highbd";
-specialize qw/av1_apply_selfguided_restoration sse4_1 avx2 neon/;
-
-add_proto qw/int av1_selfguided_restoration/, "const uint8_t *dgd8, int width, int height,
-                                int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
-                                int sgr_params_idx, int bit_depth, int highbd";
-specialize qw/av1_selfguided_restoration sse4_1 avx2 neon/;
+if ((aom_config("CONFIG_REALTIME_ONLY") ne "yes") || (aom_config("CONFIG_AV1_DECODER") eq "yes")) {
+  add_proto qw/int av1_apply_selfguided_restoration/, "const uint8_t *dat, int width, int height, int stride, int eps, const int *xqd, uint8_t *dst, int dst_stride, int32_t *tmpbuf, int bit_depth, int highbd";
+  specialize qw/av1_apply_selfguided_restoration sse4_1 avx2 neon/;
+
+  add_proto qw/int av1_selfguided_restoration/, "const uint8_t *dgd8, int width, int height,
+                                  int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
+                                  int sgr_params_idx, int bit_depth, int highbd";
+  specialize qw/av1_selfguided_restoration sse4_1 avx2 neon/;
+}
 
 # CONVOLVE_ROUND/COMPOUND_ROUND functions
 
@@ -622,33 +637,35 @@ if (aom_config("CONFIG_AV1_HIGHBITDEPTH") eq "yes") {
 }
 
 # CFL
-add_proto qw/cfl_subtract_average_fn cfl_get_subtract_average_fn/, "TX_SIZE tx_size";
-specialize qw/cfl_get_subtract_average_fn sse2 avx2 neon vsx/;
+if ((aom_config("CONFIG_REALTIME_ONLY") ne "yes") ||
+      (aom_config("CONFIG_AV1_DECODER") eq "yes")) {
+  add_proto qw/cfl_subtract_average_fn cfl_get_subtract_average_fn/, "TX_SIZE tx_size";
+  specialize qw/cfl_get_subtract_average_fn sse2 avx2 neon vsx/;
 
-add_proto qw/cfl_subsample_lbd_fn cfl_get_luma_subsampling_420_lbd/, "TX_SIZE tx_size";
-specialize qw/cfl_get_luma_subsampling_420_lbd ssse3 avx2 neon/;
+  add_proto qw/cfl_subsample_lbd_fn cfl_get_luma_subsampling_420_lbd/, "TX_SIZE tx_size";
+  specialize qw/cfl_get_luma_subsampling_420_lbd ssse3 avx2 neon/;
 
-add_proto qw/cfl_subsample_lbd_fn cfl_get_luma_subsampling_422_lbd/, "TX_SIZE tx_size";
-specialize qw/cfl_get_luma_subsampling_422_lbd ssse3 avx2 neon/;
+  add_proto qw/cfl_subsample_lbd_fn cfl_get_luma_subsampling_422_lbd/, "TX_SIZE tx_size";
+  specialize qw/cfl_get_luma_subsampling_422_lbd ssse3 avx2 neon/;
 
-add_proto qw/cfl_subsample_lbd_fn cfl_get_luma_subsampling_444_lbd/, "TX_SIZE tx_size";
-specialize qw/cfl_get_luma_subsampling_444_lbd ssse3 avx2 neon/;
+  add_proto qw/cfl_subsample_lbd_fn cfl_get_luma_subsampling_444_lbd/, "TX_SIZE tx_size";
+  specialize qw/cfl_get_luma_subsampling_444_lbd ssse3 avx2 neon/;
 
-if (aom_config("CONFIG_AV1_HIGHBITDEPTH") eq "yes") {
-  add_proto qw/cfl_subsample_hbd_fn cfl_get_luma_subsampling_420_hbd/, "TX_SIZE tx_size";
-  specialize qw/cfl_get_luma_subsampling_420_hbd ssse3 avx2 neon/;
+  if (aom_config("CONFIG_AV1_HIGHBITDEPTH") eq "yes") {
+    add_proto qw/cfl_subsample_hbd_fn cfl_get_luma_subsampling_420_hbd/, "TX_SIZE tx_size";
+    specialize qw/cfl_get_luma_subsampling_420_hbd ssse3 avx2 neon/;
 
-  add_proto qw/cfl_subsample_hbd_fn cfl_get_luma_subsampling_422_hbd/, "TX_SIZE tx_size";
-  specialize qw/cfl_get_luma_subsampling_422_hbd ssse3 avx2 neon/;
+    add_proto qw/cfl_subsample_hbd_fn cfl_get_luma_subsampling_422_hbd/, "TX_SIZE tx_size";
+    specialize qw/cfl_get_luma_subsampling_422_hbd ssse3 avx2 neon/;
 
-  add_proto qw/cfl_subsample_hbd_fn cfl_get_luma_subsampling_444_hbd/, "TX_SIZE tx_size";
-  specialize qw/cfl_get_luma_subsampling_444_hbd ssse3 avx2 neon/;
+    add_proto qw/cfl_subsample_hbd_fn cfl_get_luma_subsampling_444_hbd/, "TX_SIZE tx_size";
+    specialize qw/cfl_get_luma_subsampling_444_hbd ssse3 avx2 neon/;
 
-  add_proto qw/cfl_predict_hbd_fn cfl_get_predict_hbd_fn/, "TX_SIZE tx_size";
-  specialize qw/cfl_get_predict_hbd_fn ssse3 avx2 neon/;
+    add_proto qw/cfl_predict_hbd_fn cfl_get_predict_hbd_fn/, "TX_SIZE tx_size";
+    specialize qw/cfl_get_predict_hbd_fn ssse3 avx2 neon/;
+  }
+  add_proto qw/cfl_predict_lbd_fn cfl_get_predict_lbd_fn/, "TX_SIZE tx_size";
+  specialize qw/cfl_get_predict_lbd_fn ssse3 avx2 neon/;
 }
 
-add_proto qw/cfl_predict_lbd_fn cfl_get_predict_lbd_fn/, "TX_SIZE tx_size";
-specialize qw/cfl_get_predict_lbd_fn ssse3 avx2 neon/;
-
 1;
diff --git a/av1/common/cdef.c b/av1/common/cdef.c
index 02de17a46..c39fca308 100644
--- a/av1/common/cdef.c
+++ b/av1/common/cdef.c
@@ -82,6 +82,7 @@ void cdef_copy_rect8_8bit_to_16bit_c(uint16_t *dst, int dstride,
   }
 }
 
+#if CONFIG_AV1_HIGHBITDEPTH
 void cdef_copy_rect8_16bit_to_16bit_c(uint16_t *dst, int dstride,
                                       const uint16_t *src, int sstride,
                                       int width, int height) {
@@ -91,6 +92,7 @@ void cdef_copy_rect8_16bit_to_16bit_c(uint16_t *dst, int dstride,
     }
   }
 }
+#endif  // CONFIG_AV1_HIGHBITDEPTH
 
 void av1_cdef_copy_sb8_16_lowbd(uint16_t *const dst, int dstride,
                                 const uint8_t *src, int src_voffset,
@@ -100,6 +102,7 @@ void av1_cdef_copy_sb8_16_lowbd(uint16_t *const dst, int dstride,
   cdef_copy_rect8_8bit_to_16bit(dst, dstride, base, sstride, hsize, vsize);
 }
 
+#if CONFIG_AV1_HIGHBITDEPTH
 void av1_cdef_copy_sb8_16_highbd(uint16_t *const dst, int dstride,
                                  const uint8_t *src, int src_voffset,
                                  int src_hoffset, int sstride, int vsize,
@@ -108,17 +111,22 @@ void av1_cdef_copy_sb8_16_highbd(uint16_t *const dst, int dstride,
       &CONVERT_TO_SHORTPTR(src)[src_voffset * (ptrdiff_t)sstride + src_hoffset];
   cdef_copy_rect8_16bit_to_16bit(dst, dstride, base, sstride, hsize, vsize);
 }
+#endif  // CONFIG_AV1_HIGHBITDEPTH
 
 void av1_cdef_copy_sb8_16(const AV1_COMMON *const cm, uint16_t *const dst,
                           int dstride, const uint8_t *src, int src_voffset,
                           int src_hoffset, int sstride, int vsize, int hsize) {
+#if CONFIG_AV1_HIGHBITDEPTH
   if (cm->seq_params->use_highbitdepth) {
     av1_cdef_copy_sb8_16_highbd(dst, dstride, src, src_voffset, src_hoffset,
                                 sstride, vsize, hsize);
-  } else {
-    av1_cdef_copy_sb8_16_lowbd(dst, dstride, src, src_voffset, src_hoffset,
-                               sstride, vsize, hsize);
+    return;
   }
+#else
+  (void)cm;
+#endif  // CONFIG_AV1_HIGHBITDEPTH
+  av1_cdef_copy_sb8_16_lowbd(dst, dstride, src, src_voffset, src_hoffset,
+                             sstride, vsize, hsize);
 }
 
 static inline void copy_rect(uint16_t *dst, int dstride, const uint16_t *src,
diff --git a/av1/common/cdef_block.c b/av1/common/cdef_block.c
index 318779bed..3822ba14c 100644
--- a/av1/common/cdef_block.c
+++ b/av1/common/cdef_block.c
@@ -22,7 +22,7 @@ beginning and end of the table. The cdef direction range is [0, 7] and the
 first index is offset +/-2. This removes the need to constrain the first
 index to the same range using e.g., & 7.
 */
-DECLARE_ALIGNED(16, const int, cdef_directions_padded[12][2]) = {
+DECLARE_ALIGNED(16, static const int, cdef_directions_padded[12][2]) = {
   /* Padding: cdef_directions[6] */
   { 1 * CDEF_BSTRIDE + 0, 2 * CDEF_BSTRIDE + 0 },
   /* Padding: cdef_directions[7] */
diff --git a/av1/common/cdef_block_simd.h b/av1/common/cdef_block_simd.h
index 56c5baa61..35a729d5b 100644
--- a/av1/common/cdef_block_simd.h
+++ b/av1/common/cdef_block_simd.h
@@ -12,6 +12,7 @@
 #ifndef AOM_AV1_COMMON_CDEF_BLOCK_SIMD_H_
 #define AOM_AV1_COMMON_CDEF_BLOCK_SIMD_H_
 
+#include "config/aom_config.h"
 #include "config/av1_rtcd.h"
 
 #include "av1/common/cdef_block.h"
@@ -824,6 +825,7 @@ void SIMD_FUNC(cdef_filter_16_3)(void *dest, int dstride, const uint16_t *in,
   }
 }
 
+#if CONFIG_AV1_HIGHBITDEPTH
 void SIMD_FUNC(cdef_copy_rect8_16bit_to_16bit)(uint16_t *dst, int dstride,
                                                const uint16_t *src, int sstride,
                                                int width, int height) {
@@ -838,6 +840,7 @@ void SIMD_FUNC(cdef_copy_rect8_16bit_to_16bit)(uint16_t *dst, int dstride,
     }
   }
 }
+#endif  // CONFIG_AV1_HIGHBITDEPTH
 
 #undef CDEF_INLINE
 
diff --git a/av1/common/convolve.c b/av1/common/convolve.c
index 46270cd1f..5db748791 100644
--- a/av1/common/convolve.c
+++ b/av1/common/convolve.c
@@ -655,17 +655,17 @@ void av1_convolve_2d_facade(const uint8_t *src, int src_stride, uint8_t *dst,
     assert(filter_params_x->taps == 2 && filter_params_y->taps == 2);
     assert(!scaled);
     if (subpel_x_qn && subpel_y_qn) {
-      av1_convolve_2d_sr_intrabc_c(src, src_stride, dst, dst_stride, w, h,
-                                   filter_params_x, filter_params_y,
-                                   subpel_x_qn, subpel_y_qn, conv_params);
+      av1_convolve_2d_sr_intrabc(src, src_stride, dst, dst_stride, w, h,
+                                 filter_params_x, filter_params_y, subpel_x_qn,
+                                 subpel_y_qn, conv_params);
       return;
     } else if (subpel_x_qn) {
-      av1_convolve_x_sr_intrabc_c(src, src_stride, dst, dst_stride, w, h,
-                                  filter_params_x, subpel_x_qn, conv_params);
+      av1_convolve_x_sr_intrabc(src, src_stride, dst, dst_stride, w, h,
+                                filter_params_x, subpel_x_qn, conv_params);
       return;
     } else if (subpel_y_qn) {
-      av1_convolve_y_sr_intrabc_c(src, src_stride, dst, dst_stride, w, h,
-                                  filter_params_y, subpel_y_qn);
+      av1_convolve_y_sr_intrabc(src, src_stride, dst, dst_stride, w, h,
+                                filter_params_y, subpel_y_qn);
       return;
     }
   }
@@ -1314,6 +1314,7 @@ void av1_highbd_convolve_2d_facade(const uint8_t *src8, int src_stride,
 // --((128 - 1) * 32 + 15) >> 4 + 8 = 263.
 #define WIENER_MAX_EXT_SIZE 263
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static inline int horz_scalar_product(const uint8_t *a, const int16_t *b) {
   int sum = 0;
   for (int k = 0; k < SUBPEL_TAPS; ++k) sum += a[k] * b[k];
@@ -1427,8 +1428,10 @@ void av1_wiener_convolve_add_src_c(const uint8_t *src, ptrdiff_t src_stride,
                             MAX_SB_SIZE, dst, dst_stride, filters_y, y0_q4,
                             y_step_q4, w, h, conv_params->round_1);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #if CONFIG_AV1_HIGHBITDEPTH
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static void highbd_convolve_add_src_horiz_hip(
     const uint8_t *src8, ptrdiff_t src_stride, uint16_t *dst,
     ptrdiff_t dst_stride, const InterpKernel *x_filters, int x0_q4,
@@ -1507,4 +1510,5 @@ void av1_highbd_wiener_convolve_add_src_c(
       temp + MAX_SB_SIZE * (SUBPEL_TAPS / 2 - 1), MAX_SB_SIZE, dst, dst_stride,
       filters_y, y0_q4, y_step_q4, w, h, conv_params->round_1, bd);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // CONFIG_AV1_HIGHBITDEPTH
diff --git a/av1/common/quant_common.c b/av1/common/quant_common.c
index 6a295844b..9be6106af 100644
--- a/av1/common/quant_common.c
+++ b/av1/common/quant_common.c
@@ -236,12 +236,6 @@ bool av1_use_qmatrix(const CommonQuantParams *quant_params,
   return quant_params->using_qmatrix && !xd->lossless[segment_id];
 }
 
-const qm_val_t *av1_iqmatrix(const CommonQuantParams *quant_params, int qmlevel,
-                             int plane, TX_SIZE tx_size) {
-  assert(quant_params->giqmatrix[qmlevel][plane][tx_size] != NULL ||
-         qmlevel == NUM_QM_LEVELS - 1);
-  return quant_params->giqmatrix[qmlevel][plane][tx_size];
-}
 const qm_val_t *av1_qmatrix(const CommonQuantParams *quant_params, int qmlevel,
                             int plane, TX_SIZE tx_size) {
   assert(quant_params->gqmatrix[qmlevel][plane][tx_size] != NULL ||
diff --git a/av1/common/quant_common.h b/av1/common/quant_common.h
index 1cd48c6c5..34754f88a 100644
--- a/av1/common/quant_common.h
+++ b/av1/common/quant_common.h
@@ -61,9 +61,6 @@ static inline int aom_get_qmlevel(int qindex, int first, int last) {
 // Initialize all global quant/dequant matrices.
 void av1_qm_init(struct CommonQuantParams *quant_params, int num_planes);
 
-// Get global dequant matrix.
-const qm_val_t *av1_iqmatrix(const struct CommonQuantParams *quant_params,
-                             int qmlevel, int plane, TX_SIZE tx_size);
 // Get global quant matrix.
 const qm_val_t *av1_qmatrix(const struct CommonQuantParams *quant_params,
                             int qmlevel, int plane, TX_SIZE tx_size);
diff --git a/av1/common/reconinter.c b/av1/common/reconinter.c
index ec43becc3..604ef9e70 100644
--- a/av1/common/reconinter.c
+++ b/av1/common/reconinter.c
@@ -68,6 +68,10 @@ void av1_init_warp_params(InterPredParams *inter_pred_params,
   if (allow_warp(mi, warp_types, &xd->global_motion[mi->ref_frame[ref]], 0,
                  inter_pred_params->scale_factors,
                  &inter_pred_params->warp_params)) {
+#if CONFIG_REALTIME_ONLY && !CONFIG_AV1_DECODER
+    aom_internal_error(xd->error_info, AOM_CODEC_UNSUP_FEATURE,
+                       "Warped motion is disabled in realtime only build.");
+#endif  // CONFIG_REALTIME_ONLY && !CONFIG_AV1_DECODER
     inter_pred_params->mode = WARP_PRED;
   }
 }
@@ -103,6 +107,7 @@ void av1_make_inter_predictor(const uint8_t *src, int src_stride, uint8_t *dst,
                     inter_pred_params->interp_filter_params);
 #endif
   }
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
   // TODO(jingning): av1_warp_plane() can be further cleaned up.
   else if (inter_pred_params->mode == WARP_PRED) {
     av1_warp_plane(
@@ -115,7 +120,9 @@ void av1_make_inter_predictor(const uint8_t *src, int src_stride, uint8_t *dst,
         inter_pred_params->block_width, inter_pred_params->block_height,
         dst_stride, inter_pred_params->subsampling_x,
         inter_pred_params->subsampling_y, &inter_pred_params->conv_params);
-  } else {
+  }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
+  else {
     assert(0 && "Unsupported inter_pred_params->mode");
   }
 }
@@ -832,10 +839,12 @@ int av1_skip_u4x4_pred_in_obmc(BLOCK_SIZE bsize,
   }
 }
 
-void av1_modify_neighbor_predictor_for_obmc(MB_MODE_INFO *mbmi) {
+#if CONFIG_AV1_DECODER
+static void modify_neighbor_predictor_for_obmc(MB_MODE_INFO *mbmi) {
   mbmi->ref_frame[1] = NONE_FRAME;
   mbmi->interinter_comp.type = COMPOUND_AVERAGE;
 }
+#endif  // CONFIG_AV1_DECODER
 
 struct obmc_inter_pred_ctxt {
   uint8_t **adjacent;
@@ -969,6 +978,7 @@ void av1_setup_obmc_dst_bufs(MACROBLOCKD *xd, uint8_t **dst_buf1,
   }
 }
 
+#if CONFIG_AV1_DECODER
 void av1_setup_build_prediction_by_above_pred(
     MACROBLOCKD *xd, int rel_mi_col, uint8_t above_mi_width,
     MB_MODE_INFO *above_mbmi, struct build_prediction_ctxt *ctxt,
@@ -976,7 +986,7 @@ void av1_setup_build_prediction_by_above_pred(
   const BLOCK_SIZE a_bsize = AOMMAX(BLOCK_8X8, above_mbmi->bsize);
   const int above_mi_col = xd->mi_col + rel_mi_col;
 
-  av1_modify_neighbor_predictor_for_obmc(above_mbmi);
+  modify_neighbor_predictor_for_obmc(above_mbmi);
 
   for (int j = 0; j < num_planes; ++j) {
     struct macroblockd_plane *const pd = &xd->plane[j];
@@ -1015,7 +1025,7 @@ void av1_setup_build_prediction_by_left_pred(MACROBLOCKD *xd, int rel_mi_row,
   const BLOCK_SIZE l_bsize = AOMMAX(BLOCK_8X8, left_mbmi->bsize);
   const int left_mi_row = xd->mi_row + rel_mi_row;
 
-  av1_modify_neighbor_predictor_for_obmc(left_mbmi);
+  modify_neighbor_predictor_for_obmc(left_mbmi);
 
   for (int j = 0; j < num_planes; ++j) {
     struct macroblockd_plane *const pd = &xd->plane[j];
@@ -1046,6 +1056,7 @@ void av1_setup_build_prediction_by_left_pred(MACROBLOCKD *xd, int rel_mi_row,
       ctxt->mb_to_far_edge +
       GET_MV_SUBPEL((xd->height - rel_mi_row - left_mi_height) * MI_SIZE);
 }
+#endif  // CONFIG_AV1_DECODER
 
 static inline void combine_interintra(
     INTERINTRA_MODE mode, int8_t use_wedge_interintra, int8_t wedge_index,
diff --git a/av1/common/reconinter.h b/av1/common/reconinter.h
index ed06ee0bb..0a33c9348 100644
--- a/av1/common/reconinter.h
+++ b/av1/common/reconinter.h
@@ -293,7 +293,6 @@ static inline void highbd_inter_predictor(
   }
 }
 
-void av1_modify_neighbor_predictor_for_obmc(MB_MODE_INFO *mbmi);
 int av1_skip_u4x4_pred_in_obmc(BLOCK_SIZE bsize,
                                const struct macroblockd_plane *pd, int dir);
 
diff --git a/av1/common/reconintra.c b/av1/common/reconintra.c
index 58a7ca804..33016a2c2 100644
--- a/av1/common/reconintra.c
+++ b/av1/common/reconintra.c
@@ -464,6 +464,17 @@ static intra_high_pred_fn dc_pred_high[2][2][TX_SIZES_ALL];
 static void init_intra_predictors_internal(void) {
   assert(NELEMENTS(mode_to_angle_map) == INTRA_MODES);
 
+#if CONFIG_REALTIME_ONLY && !CONFIG_AV1_DECODER
+#define INIT_RECTANGULAR(p, type)             \
+  p[TX_4X8] = aom_##type##_predictor_4x8;     \
+  p[TX_8X4] = aom_##type##_predictor_8x4;     \
+  p[TX_8X16] = aom_##type##_predictor_8x16;   \
+  p[TX_16X8] = aom_##type##_predictor_16x8;   \
+  p[TX_16X32] = aom_##type##_predictor_16x32; \
+  p[TX_32X16] = aom_##type##_predictor_32x16; \
+  p[TX_32X64] = aom_##type##_predictor_32x64; \
+  p[TX_64X32] = aom_##type##_predictor_64x32;
+#else
 #define INIT_RECTANGULAR(p, type)             \
   p[TX_4X8] = aom_##type##_predictor_4x8;     \
   p[TX_8X4] = aom_##type##_predictor_8x4;     \
@@ -479,6 +490,7 @@ static void init_intra_predictors_internal(void) {
   p[TX_32X8] = aom_##type##_predictor_32x8;   \
   p[TX_16X64] = aom_##type##_predictor_16x64; \
   p[TX_64X16] = aom_##type##_predictor_64x16;
+#endif  // CONFIG_REALTIME_ONLY && !CONFIG_AV1_DECODER
 
 #define INIT_NO_4X4(p, type)                  \
   p[TX_8X8] = aom_##type##_predictor_8x8;     \
@@ -1838,6 +1850,7 @@ void av1_predict_intra_block_facade(const AV1_COMMON *cm, MACROBLOCKD *xd,
   const int angle_delta = mbmi->angle_delta[plane != AOM_PLANE_Y] * ANGLE_STEP;
   const SequenceHeader *seq_params = cm->seq_params;
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
   if (plane != AOM_PLANE_Y && mbmi->uv_mode == UV_CFL_PRED) {
 #if CONFIG_DEBUG
     assert(is_cfl_allowed(xd));
@@ -1870,6 +1883,7 @@ void av1_predict_intra_block_facade(const AV1_COMMON *cm, MACROBLOCKD *xd,
     av1_cfl_predict_block(xd, dst, dst_stride, tx_size, plane);
     return;
   }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
   av1_predict_intra_block(
       xd, seq_params->sb_size, seq_params->enable_intra_edge_filter, pd->width,
       pd->height, tx_size, mode, angle_delta, use_palette, filter_intra_mode,
diff --git a/av1/common/resize.c b/av1/common/resize.c
index 839a7e3ea..dc640be85 100644
--- a/av1/common/resize.c
+++ b/av1/common/resize.c
@@ -1031,97 +1031,6 @@ static bool highbd_upscale_normative_rect(const uint8_t *const input,
 }
 #endif  // CONFIG_AV1_HIGHBITDEPTH
 
-void av1_resize_frame420(const uint8_t *y, int y_stride, const uint8_t *u,
-                         const uint8_t *v, int uv_stride, int height, int width,
-                         uint8_t *oy, int oy_stride, uint8_t *ou, uint8_t *ov,
-                         int ouv_stride, int oheight, int owidth) {
-  if (!av1_resize_plane(y, height, width, y_stride, oy, oheight, owidth,
-                        oy_stride))
-    abort();
-  if (!av1_resize_plane(u, height / 2, width / 2, uv_stride, ou, oheight / 2,
-                        owidth / 2, ouv_stride))
-    abort();
-  if (!av1_resize_plane(v, height / 2, width / 2, uv_stride, ov, oheight / 2,
-                        owidth / 2, ouv_stride))
-    abort();
-}
-
-bool av1_resize_frame422(const uint8_t *y, int y_stride, const uint8_t *u,
-                         const uint8_t *v, int uv_stride, int height, int width,
-                         uint8_t *oy, int oy_stride, uint8_t *ou, uint8_t *ov,
-                         int ouv_stride, int oheight, int owidth) {
-  if (!av1_resize_plane(y, height, width, y_stride, oy, oheight, owidth,
-                        oy_stride))
-    return false;
-  if (!av1_resize_plane(u, height, width / 2, uv_stride, ou, oheight,
-                        owidth / 2, ouv_stride))
-    return false;
-  if (!av1_resize_plane(v, height, width / 2, uv_stride, ov, oheight,
-                        owidth / 2, ouv_stride))
-    return false;
-  return true;
-}
-
-bool av1_resize_frame444(const uint8_t *y, int y_stride, const uint8_t *u,
-                         const uint8_t *v, int uv_stride, int height, int width,
-                         uint8_t *oy, int oy_stride, uint8_t *ou, uint8_t *ov,
-                         int ouv_stride, int oheight, int owidth) {
-  if (!av1_resize_plane(y, height, width, y_stride, oy, oheight, owidth,
-                        oy_stride))
-    return false;
-  if (!av1_resize_plane(u, height, width, uv_stride, ou, oheight, owidth,
-                        ouv_stride))
-    return false;
-  if (!av1_resize_plane(v, height, width, uv_stride, ov, oheight, owidth,
-                        ouv_stride))
-    return false;
-  return true;
-}
-
-#if CONFIG_AV1_HIGHBITDEPTH
-void av1_highbd_resize_frame420(const uint8_t *y, int y_stride,
-                                const uint8_t *u, const uint8_t *v,
-                                int uv_stride, int height, int width,
-                                uint8_t *oy, int oy_stride, uint8_t *ou,
-                                uint8_t *ov, int ouv_stride, int oheight,
-                                int owidth, int bd) {
-  av1_highbd_resize_plane(y, height, width, y_stride, oy, oheight, owidth,
-                          oy_stride, bd);
-  av1_highbd_resize_plane(u, height / 2, width / 2, uv_stride, ou, oheight / 2,
-                          owidth / 2, ouv_stride, bd);
-  av1_highbd_resize_plane(v, height / 2, width / 2, uv_stride, ov, oheight / 2,
-                          owidth / 2, ouv_stride, bd);
-}
-
-void av1_highbd_resize_frame422(const uint8_t *y, int y_stride,
-                                const uint8_t *u, const uint8_t *v,
-                                int uv_stride, int height, int width,
-                                uint8_t *oy, int oy_stride, uint8_t *ou,
-                                uint8_t *ov, int ouv_stride, int oheight,
-                                int owidth, int bd) {
-  av1_highbd_resize_plane(y, height, width, y_stride, oy, oheight, owidth,
-                          oy_stride, bd);
-  av1_highbd_resize_plane(u, height, width / 2, uv_stride, ou, oheight,
-                          owidth / 2, ouv_stride, bd);
-  av1_highbd_resize_plane(v, height, width / 2, uv_stride, ov, oheight,
-                          owidth / 2, ouv_stride, bd);
-}
-
-void av1_highbd_resize_frame444(const uint8_t *y, int y_stride,
-                                const uint8_t *u, const uint8_t *v,
-                                int uv_stride, int height, int width,
-                                uint8_t *oy, int oy_stride, uint8_t *ou,
-                                uint8_t *ov, int ouv_stride, int oheight,
-                                int owidth, int bd) {
-  av1_highbd_resize_plane(y, height, width, y_stride, oy, oheight, owidth,
-                          oy_stride, bd);
-  av1_highbd_resize_plane(u, height, width, uv_stride, ou, oheight, owidth,
-                          ouv_stride, bd);
-  av1_highbd_resize_plane(v, height, width, uv_stride, ov, oheight, owidth,
-                          ouv_stride, bd);
-}
-#endif  // CONFIG_AV1_HIGHBITDEPTH
-
 void av1_resize_and_extend_frame_c(const YV12_BUFFER_CONFIG *src,
                                    YV12_BUFFER_CONFIG *dst,
                                    const InterpFilter filter,
@@ -1277,9 +1186,9 @@ void av1_upscale_normative_rows(const AV1_COMMON *cm, const uint8_t *src,
   }
 }
 
-void av1_upscale_normative_and_extend_frame(const AV1_COMMON *cm,
-                                            const YV12_BUFFER_CONFIG *src,
-                                            YV12_BUFFER_CONFIG *dst) {
+static void upscale_normative_and_extend_frame(const AV1_COMMON *cm,
+                                               const YV12_BUFFER_CONFIG *src,
+                                               YV12_BUFFER_CONFIG *dst) {
   const int num_planes = av1_num_planes(cm);
   for (int i = 0; i < num_planes; ++i) {
     const int is_uv = (i > 0);
@@ -1492,7 +1401,7 @@ void av1_superres_upscale(AV1_COMMON *cm, BufferPool *const pool,
 
   // Scale up and back into frame_to_show.
   assert(frame_to_show->y_crop_width != cm->width);
-  av1_upscale_normative_and_extend_frame(cm, &copy_buffer, frame_to_show);
+  upscale_normative_and_extend_frame(cm, &copy_buffer, frame_to_show);
 
   // Free the copy buffer
   aom_free_frame_buffer(&copy_buffer);
diff --git a/av1/common/resize.h b/av1/common/resize.h
index 489ad81f5..51dbe63c4 100644
--- a/av1/common/resize.h
+++ b/av1/common/resize.h
@@ -27,49 +27,14 @@ static const int16_t av1_down2_symodd_half_filter[] = { 64, 35, 0, -3 };
 bool av1_resize_plane(const uint8_t *input, int height, int width,
                       int in_stride, uint8_t *output, int height2, int width2,
                       int out_stride);
-// TODO(aomedia:3228): In libaom 4.0.0, remove av1_resize_frame420 from
-// av1/exports_com and delete this function.
-void av1_resize_frame420(const uint8_t *y, int y_stride, const uint8_t *u,
-                         const uint8_t *v, int uv_stride, int height, int width,
-                         uint8_t *oy, int oy_stride, uint8_t *ou, uint8_t *ov,
-                         int ouv_stride, int oheight, int owidth);
-bool av1_resize_frame422(const uint8_t *y, int y_stride, const uint8_t *u,
-                         const uint8_t *v, int uv_stride, int height, int width,
-                         uint8_t *oy, int oy_stride, uint8_t *ou, uint8_t *ov,
-                         int ouv_stride, int oheight, int owidth);
-bool av1_resize_frame444(const uint8_t *y, int y_stride, const uint8_t *u,
-                         const uint8_t *v, int uv_stride, int height, int width,
-                         uint8_t *oy, int oy_stride, uint8_t *ou, uint8_t *ov,
-                         int ouv_stride, int oheight, int owidth);
 
 void av1_highbd_resize_plane(const uint8_t *input, int height, int width,
                              int in_stride, uint8_t *output, int height2,
                              int width2, int out_stride, int bd);
-void av1_highbd_resize_frame420(const uint8_t *y, int y_stride,
-                                const uint8_t *u, const uint8_t *v,
-                                int uv_stride, int height, int width,
-                                uint8_t *oy, int oy_stride, uint8_t *ou,
-                                uint8_t *ov, int ouv_stride, int oheight,
-                                int owidth, int bd);
-void av1_highbd_resize_frame422(const uint8_t *y, int y_stride,
-                                const uint8_t *u, const uint8_t *v,
-                                int uv_stride, int height, int width,
-                                uint8_t *oy, int oy_stride, uint8_t *ou,
-                                uint8_t *ov, int ouv_stride, int oheight,
-                                int owidth, int bd);
-void av1_highbd_resize_frame444(const uint8_t *y, int y_stride,
-                                const uint8_t *u, const uint8_t *v,
-                                int uv_stride, int height, int width,
-                                uint8_t *oy, int oy_stride, uint8_t *ou,
-                                uint8_t *ov, int ouv_stride, int oheight,
-                                int owidth, int bd);
 
 void av1_upscale_normative_rows(const AV1_COMMON *cm, const uint8_t *src,
                                 int src_stride, uint8_t *dst, int dst_stride,
                                 int plane, int rows);
-void av1_upscale_normative_and_extend_frame(const AV1_COMMON *cm,
-                                            const YV12_BUFFER_CONFIG *src,
-                                            YV12_BUFFER_CONFIG *dst);
 
 YV12_BUFFER_CONFIG *av1_realloc_and_scale_if_required(
     AV1_COMMON *cm, YV12_BUFFER_CONFIG *unscaled, YV12_BUFFER_CONFIG *scaled,
diff --git a/av1/common/thread_common.c b/av1/common/thread_common.c
index 7efed9918..0ff623899 100644
--- a/av1/common/thread_common.c
+++ b/av1/common/thread_common.c
@@ -42,6 +42,7 @@ static inline int get_sync_range(int width) {
     return 8;
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static inline int get_lr_sync_range(int width) {
 #if 0
   // nsync numbers are picked by testing. For example, for 4k
@@ -59,6 +60,7 @@ static inline int get_lr_sync_range(int width) {
   return 1;
 #endif
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // Allocate memory for lf row synchronization
 void av1_loop_filter_alloc(AV1LfSync *lf_sync, AV1_COMMON *cm, int rows,
@@ -519,6 +521,7 @@ void av1_loop_filter_frame_mt(YV12_BUFFER_CONFIG *frame, AV1_COMMON *cm,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 static inline void lr_sync_read(void *const lr_sync, int r, int c, int plane) {
 #if CONFIG_MULTITHREAD
   AV1LrSync *const loop_res_sync = (AV1LrSync *)lr_sync;
@@ -991,6 +994,7 @@ void av1_loop_restoration_filter_frame_mt(YV12_BUFFER_CONFIG *frame,
   foreach_rest_unit_in_planes_mt(loop_rest_ctxt, workers, num_workers, lr_sync,
                                  cm, do_extend_border);
 }
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 // Initializes cdef_sync parameters.
 static inline void reset_cdef_job_info(AV1CdefSync *const cdef_sync) {
diff --git a/av1/common/thread_common.h b/av1/common/thread_common.h
index 0c27e12f2..1f2557a78 100644
--- a/av1/common/thread_common.h
+++ b/av1/common/thread_common.h
@@ -164,10 +164,12 @@ void av1_cdef_copy_sb8_16_lowbd(uint16_t *const dst, int dstride,
                                 const uint8_t *src, int src_voffset,
                                 int src_hoffset, int sstride, int vsize,
                                 int hsize);
+#if CONFIG_AV1_HIGHBITDEPTH
 void av1_cdef_copy_sb8_16_highbd(uint16_t *const dst, int dstride,
                                  const uint8_t *src, int src_voffset,
                                  int src_hoffset, int sstride, int vsize,
                                  int hsize);
+#endif  // CONFIG_AV1_HIGHBITDEPTH
 void av1_alloc_cdef_sync(AV1_COMMON *const cm, AV1CdefSync *cdef_sync,
                          int num_workers);
 void av1_free_cdef_sync(AV1CdefSync *cdef_sync);
@@ -186,6 +188,7 @@ void av1_loop_filter_frame_mt(YV12_BUFFER_CONFIG *frame, struct AV1Common *cm,
                               AVxWorker *workers, int num_workers,
                               AV1LfSync *lf_sync, int lpf_opt_level);
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 void av1_loop_restoration_filter_frame_mt(YV12_BUFFER_CONFIG *frame,
                                           struct AV1Common *cm,
                                           int optimized_lr, AVxWorker *workers,
@@ -195,6 +198,8 @@ void av1_loop_restoration_dealloc(AV1LrSync *lr_sync);
 void av1_loop_restoration_alloc(AV1LrSync *lr_sync, AV1_COMMON *cm,
                                 int num_workers, int num_rows_lr,
                                 int num_planes, int width);
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
+
 int av1_get_intrabc_extra_top_right_sb_delay(const AV1_COMMON *cm);
 
 void av1_thread_loop_filter_rows(
diff --git a/av1/common/txb_common.c b/av1/common/txb_common.c
index a6c2c33ed..ed3572684 100644
--- a/av1/common/txb_common.c
+++ b/av1/common/txb_common.c
@@ -15,18 +15,18 @@
 // The ctx offset table when TX is TX_CLASS_2D.
 // TX col and row indices are clamped to 4
 
-const int8_t av1_nz_map_ctx_offset_4x4[16] = {
+static const int8_t av1_nz_map_ctx_offset_4x4[16] = {
   0, 1, 6, 6, 1, 6, 6, 21, 6, 6, 21, 21, 6, 21, 21, 21,
 };
 
-const int8_t av1_nz_map_ctx_offset_8x8[64] = {
+static const int8_t av1_nz_map_ctx_offset_8x8[64] = {
   0,  1,  6,  6,  21, 21, 21, 21, 1,  6,  6,  21, 21, 21, 21, 21,
   6,  6,  21, 21, 21, 21, 21, 21, 6,  21, 21, 21, 21, 21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
 };
 
-const int8_t av1_nz_map_ctx_offset_16x16[256] = {
+static const int8_t av1_nz_map_ctx_offset_16x16[256] = {
   0,  1,  6,  6,  21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 1,  6,  6,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 6,  6,  21, 21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 6,  21, 21, 21, 21, 21, 21, 21, 21,
@@ -43,7 +43,7 @@ const int8_t av1_nz_map_ctx_offset_16x16[256] = {
   21, 21, 21, 21, 21, 21, 21, 21, 21,
 };
 
-const int8_t av1_nz_map_ctx_offset_32x32[1024] = {
+static const int8_t av1_nz_map_ctx_offset_32x32[1024] = {
   0,  1,  6,  6,  21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 1,  6,  6,  21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
@@ -100,12 +100,12 @@ const int8_t av1_nz_map_ctx_offset_32x32[1024] = {
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
 };
 
-const int8_t av1_nz_map_ctx_offset_4x8[32] = {
+static const int8_t av1_nz_map_ctx_offset_4x8[32] = {
   0,  11, 6,  6,  21, 21, 21, 21, 11, 11, 6,  21, 21, 21, 21, 21,
   11, 11, 21, 21, 21, 21, 21, 21, 11, 11, 21, 21, 21, 21, 21, 21,
 };
 
-const int8_t av1_nz_map_ctx_offset_8x16[128] = {
+static const int8_t av1_nz_map_ctx_offset_8x16[128] = {
   0,  11, 6,  6,  21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 11, 11, 6,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 11, 11, 21, 21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 11, 11, 21, 21, 21, 21, 21, 21, 21,
@@ -115,7 +115,7 @@ const int8_t av1_nz_map_ctx_offset_8x16[128] = {
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
 };
 
-const int8_t av1_nz_map_ctx_offset_16x32[512] = {
+static const int8_t av1_nz_map_ctx_offset_16x32[512] = {
   0,  11, 6,  6,  21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 11, 11, 6,  21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
@@ -145,7 +145,7 @@ const int8_t av1_nz_map_ctx_offset_16x32[512] = {
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
 };
 
-const int8_t av1_nz_map_ctx_offset_32x16[512] = {
+static const int8_t av1_nz_map_ctx_offset_32x16[512] = {
   0,  16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,
   16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6,  6,  21, 21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 6,  21, 21, 21, 21, 21, 21, 21, 21,
@@ -175,7 +175,7 @@ const int8_t av1_nz_map_ctx_offset_32x16[512] = {
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
 };
 
-const int8_t av1_nz_map_ctx_offset_32x64[1024] = {
+static const int8_t av1_nz_map_ctx_offset_32x64[1024] = {
   0,  11, 6,  6,  21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 11, 11, 6,  21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
@@ -232,7 +232,7 @@ const int8_t av1_nz_map_ctx_offset_32x64[1024] = {
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
 };
 
-const int8_t av1_nz_map_ctx_offset_64x32[1024] = {
+static const int8_t av1_nz_map_ctx_offset_64x32[1024] = {
   0,  16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,
   16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,
   16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,
@@ -289,21 +289,21 @@ const int8_t av1_nz_map_ctx_offset_64x32[1024] = {
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
 };
 
-const int8_t av1_nz_map_ctx_offset_4x16[64] = {
+static const int8_t av1_nz_map_ctx_offset_4x16[64] = {
   0,  11, 6,  6,  21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
   11, 11, 6,  21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
   11, 11, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
   11, 11, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
 };
 
-const int8_t av1_nz_map_ctx_offset_16x4[64] = {
+static const int8_t av1_nz_map_ctx_offset_16x4[64] = {
   0,  16, 16, 16, 16, 16, 16, 16, 6,  6,  21, 21, 6,  21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
 };
 
-const int8_t av1_nz_map_ctx_offset_8x32[256] = {
+static const int8_t av1_nz_map_ctx_offset_8x32[256] = {
   0,  11, 6,  6,  21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 11, 11, 6,  21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
@@ -320,7 +320,7 @@ const int8_t av1_nz_map_ctx_offset_8x32[256] = {
   21, 21, 21, 21, 21, 21, 21, 21, 21,
 };
 
-const int8_t av1_nz_map_ctx_offset_32x8[256] = {
+static const int8_t av1_nz_map_ctx_offset_32x8[256] = {
   0,  16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6,  6,  21,
   21, 21, 21, 21, 21, 6,  21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
   21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,
diff --git a/av1/decoder/decodeframe.c b/av1/decoder/decodeframe.c
index 064a2aea4..7d8495361 100644
--- a/av1/decoder/decodeframe.c
+++ b/av1/decoder/decodeframe.c
@@ -173,7 +173,7 @@ static inline void read_coeffs_tx_intra_block(
     struct aom_usec_timer timer;
     aom_usec_timer_start(&timer);
 #endif
-    av1_read_coeffs_txb_facade(cm, dcb, r, plane, row, col, tx_size);
+    av1_read_coeffs_txb(cm, dcb, r, plane, row, col, tx_size);
 #if TXCOEFF_TIMER
     aom_usec_timer_mark(&timer);
     const int64_t elapsed_time = aom_usec_timer_elapsed(&timer);
@@ -1811,6 +1811,14 @@ static inline void setup_quantization(CommonQuantParams *quant_params,
   }
 }
 
+// Get global dequant matrix.
+static const qm_val_t *get_iqmatrix(const CommonQuantParams *quant_params,
+                                    int qmlevel, int plane, TX_SIZE tx_size) {
+  assert(quant_params->giqmatrix[qmlevel][plane][tx_size] != NULL ||
+         qmlevel == NUM_QM_LEVELS - 1);
+  return quant_params->giqmatrix[qmlevel][plane][tx_size];
+}
+
 // Build y/uv dequant values based on segmentation.
 static inline void setup_segmentation_dequant(AV1_COMMON *const cm,
                                               MACROBLOCKD *const xd) {
@@ -1839,19 +1847,19 @@ static inline void setup_segmentation_dequant(AV1_COMMON *const cm,
         use_qmatrix ? quant_params->qmatrix_level_y : NUM_QM_LEVELS - 1;
     for (int j = 0; j < TX_SIZES_ALL; ++j) {
       quant_params->y_iqmatrix[i][j] =
-          av1_iqmatrix(quant_params, qmlevel_y, AOM_PLANE_Y, j);
+          get_iqmatrix(quant_params, qmlevel_y, AOM_PLANE_Y, j);
     }
     const int qmlevel_u =
         use_qmatrix ? quant_params->qmatrix_level_u : NUM_QM_LEVELS - 1;
     for (int j = 0; j < TX_SIZES_ALL; ++j) {
       quant_params->u_iqmatrix[i][j] =
-          av1_iqmatrix(quant_params, qmlevel_u, AOM_PLANE_U, j);
+          get_iqmatrix(quant_params, qmlevel_u, AOM_PLANE_U, j);
     }
     const int qmlevel_v =
         use_qmatrix ? quant_params->qmatrix_level_v : NUM_QM_LEVELS - 1;
     for (int j = 0; j < TX_SIZES_ALL; ++j) {
       quant_params->v_iqmatrix[i][j] =
-          av1_iqmatrix(quant_params, qmlevel_v, AOM_PLANE_V, j);
+          get_iqmatrix(quant_params, qmlevel_v, AOM_PLANE_V, j);
     }
   }
 }
@@ -1861,12 +1869,18 @@ static InterpFilter read_frame_interp_filter(struct aom_read_bit_buffer *rb) {
                              : aom_rb_read_literal(rb, LOG_SWITCHABLE_FILTERS);
 }
 
+static void read_frame_size(struct aom_read_bit_buffer *rb, int num_bits_width,
+                            int num_bits_height, int *width, int *height) {
+  *width = aom_rb_read_literal(rb, num_bits_width) + 1;
+  *height = aom_rb_read_literal(rb, num_bits_height) + 1;
+}
+
 static inline void setup_render_size(AV1_COMMON *cm,
                                      struct aom_read_bit_buffer *rb) {
   cm->render_width = cm->superres_upscaled_width;
   cm->render_height = cm->superres_upscaled_height;
   if (aom_rb_read_bit(rb))
-    av1_read_frame_size(rb, 16, 16, &cm->render_width, &cm->render_height);
+    read_frame_size(rb, 16, 16, &cm->render_width, &cm->render_height);
 }
 
 // TODO(afergs): make "struct aom_read_bit_buffer *const rb"?
@@ -1970,7 +1984,7 @@ static inline void setup_frame_size(AV1_COMMON *cm,
   if (frame_size_override_flag) {
     int num_bits_width = seq_params->num_bits_width;
     int num_bits_height = seq_params->num_bits_height;
-    av1_read_frame_size(rb, num_bits_width, num_bits_height, &width, &height);
+    read_frame_size(rb, num_bits_width, num_bits_height, &width, &height);
     if (width > seq_params->max_frame_width ||
         height > seq_params->max_frame_height) {
       aom_internal_error(cm->error, AOM_CODEC_CORRUPT_FRAME,
@@ -2035,7 +2049,7 @@ static inline void setup_frame_size_with_refs(AV1_COMMON *cm,
     int num_bits_width = seq_params->num_bits_width;
     int num_bits_height = seq_params->num_bits_height;
 
-    av1_read_frame_size(rb, num_bits_width, num_bits_height, &width, &height);
+    read_frame_size(rb, num_bits_width, num_bits_height, &width, &height);
     setup_superres(cm, rb, &width, &height);
     resize_context_buffers(cm, width, height);
     setup_render_size(cm, rb);
@@ -2691,7 +2705,7 @@ static inline void set_decode_func_pointers(ThreadData *td,
 
   if (parse_decode_flag & 0x1) {
     td->read_coeffs_tx_intra_block_visit = read_coeffs_tx_intra_block;
-    td->read_coeffs_tx_inter_block_visit = av1_read_coeffs_txb_facade;
+    td->read_coeffs_tx_inter_block_visit = av1_read_coeffs_txb;
   }
   if (parse_decode_flag & 0x2) {
     td->predict_and_recon_intra_block_visit =
@@ -5131,12 +5145,6 @@ struct aom_read_bit_buffer *av1_init_read_bit_buffer(
   return rb;
 }
 
-void av1_read_frame_size(struct aom_read_bit_buffer *rb, int num_bits_width,
-                         int num_bits_height, int *width, int *height) {
-  *width = aom_rb_read_literal(rb, num_bits_width) + 1;
-  *height = aom_rb_read_literal(rb, num_bits_height) + 1;
-}
-
 BITSTREAM_PROFILE av1_read_profile(struct aom_read_bit_buffer *rb) {
   int profile = aom_rb_read_literal(rb, PROFILE_BITS);
   return (BITSTREAM_PROFILE)profile;
diff --git a/av1/decoder/decodeframe.h b/av1/decoder/decodeframe.h
index bb5c031b6..a3cac62c2 100644
--- a/av1/decoder/decodeframe.h
+++ b/av1/decoder/decodeframe.h
@@ -26,8 +26,6 @@ struct ThreadData;
 void av1_read_sequence_header(AV1_COMMON *cm, struct aom_read_bit_buffer *rb,
                               SequenceHeader *seq_params);
 
-void av1_read_frame_size(struct aom_read_bit_buffer *rb, int num_bits_width,
-                         int num_bits_height, int *width, int *height);
 BITSTREAM_PROFILE av1_read_profile(struct aom_read_bit_buffer *rb);
 
 // Returns 0 on success. Sets pbi->common.error.error_code and returns -1 on
diff --git a/av1/decoder/decodetxb.c b/av1/decoder/decodetxb.c
index eb3eb16cb..17d67cbf6 100644
--- a/av1/decoder/decodetxb.c
+++ b/av1/decoder/decodetxb.c
@@ -107,11 +107,11 @@ static inline void read_coeffs_reverse(aom_reader *r, TX_SIZE tx_size,
   }
 }
 
-uint8_t av1_read_coeffs_txb(const AV1_COMMON *const cm, DecoderCodingBlock *dcb,
-                            aom_reader *const r, const int blk_row,
-                            const int blk_col, const int plane,
-                            const TXB_CTX *const txb_ctx,
-                            const TX_SIZE tx_size) {
+static uint8_t read_coeffs_txb(const AV1_COMMON *const cm,
+                               DecoderCodingBlock *dcb, aom_reader *const r,
+                               const int blk_row, const int blk_col,
+                               const int plane, const TXB_CTX *const txb_ctx,
+                               const TX_SIZE tx_size) {
   MACROBLOCKD *const xd = &dcb->xd;
   FRAME_CONTEXT *const ec_ctx = xd->tile_ctx;
   const int32_t max_value = (1 << (7 + xd->bd)) - 1;
@@ -322,10 +322,9 @@ uint8_t av1_read_coeffs_txb(const AV1_COMMON *const cm, DecoderCodingBlock *dcb,
   return cul_level;
 }
 
-void av1_read_coeffs_txb_facade(const AV1_COMMON *const cm,
-                                DecoderCodingBlock *dcb, aom_reader *const r,
-                                const int plane, const int row, const int col,
-                                const TX_SIZE tx_size) {
+void av1_read_coeffs_txb(const AV1_COMMON *const cm, DecoderCodingBlock *dcb,
+                         aom_reader *const r, const int plane, const int row,
+                         const int col, const TX_SIZE tx_size) {
 #if TXCOEFF_TIMER
   struct aom_usec_timer timer;
   aom_usec_timer_start(&timer);
@@ -343,7 +342,7 @@ void av1_read_coeffs_txb_facade(const AV1_COMMON *const cm,
   get_txb_ctx(plane_bsize, tx_size, plane, pd->above_entropy_context + col,
               pd->left_entropy_context + row, &txb_ctx);
   const uint8_t cul_level =
-      av1_read_coeffs_txb(cm, dcb, r, row, col, plane, &txb_ctx, tx_size);
+      read_coeffs_txb(cm, dcb, r, row, col, plane, &txb_ctx, tx_size);
   av1_set_entropy_contexts(xd, pd, plane, plane_bsize, tx_size, cul_level, col,
                            row);
 
diff --git a/av1/decoder/decodetxb.h b/av1/decoder/decodetxb.h
index 5c70b21a9..3ad8e5fe2 100644
--- a/av1/decoder/decodetxb.h
+++ b/av1/decoder/decodetxb.h
@@ -19,16 +19,8 @@ struct AV1Common;
 struct DecoderCodingBlock;
 struct txb_ctx;
 
-uint8_t av1_read_coeffs_txb(const struct AV1Common *const cm,
-                            struct DecoderCodingBlock *dcb,
-                            struct aom_reader *const r, const int blk_row,
-                            const int blk_col, const int plane,
-                            const struct txb_ctx *const txb_ctx,
-                            const TX_SIZE tx_size);
-
-void av1_read_coeffs_txb_facade(const struct AV1Common *const cm,
-                                struct DecoderCodingBlock *dcb,
-                                struct aom_reader *const r, const int plane,
-                                const int row, const int col,
-                                const TX_SIZE tx_size);
+void av1_read_coeffs_txb(const struct AV1Common *const cm,
+                         struct DecoderCodingBlock *dcb,
+                         struct aom_reader *const r, const int plane,
+                         const int row, const int col, const TX_SIZE tx_size);
 #endif  // AOM_AV1_DECODER_DECODETXB_H_
diff --git a/av1/encoder/aq_variance.c b/av1/encoder/aq_variance.c
index 66e02c00e..2dc2cd0c9 100644
--- a/av1/encoder/aq_variance.c
+++ b/av1/encoder/aq_variance.c
@@ -20,7 +20,9 @@
 #include "av1/encoder/rd.h"
 #include "av1/encoder/segmentation.h"
 #include "av1/encoder/dwt.h"
+#include "config/aom_config.h"
 
+#if !CONFIG_REALTIME_ONLY
 static const double rate_ratio[MAX_SEGMENTS] = { 2.2, 1.7, 1.3, 1.0,
                                                  0.9, .8,  .7,  .6 };
 
@@ -32,11 +34,6 @@ static const double deltaq_rate_ratio[MAX_SEGMENTS] = { 2.5,  2.0, 1.5, 1.0,
 #define ENERGY_IN_BOUNDS(energy) \
   assert((energy) >= ENERGY_MIN && (energy) <= ENERGY_MAX)
 
-DECLARE_ALIGNED(16, static const uint8_t, av1_all_zeros[MAX_SB_SIZE]) = { 0 };
-
-DECLARE_ALIGNED(16, static const uint16_t,
-                av1_highbd_all_zeros[MAX_SB_SIZE]) = { 0 };
-
 static const int segment_id[ENERGY_SPAN] = { 0, 1, 1, 2, 3, 4 };
 
 #define SEGMENT_ID(i) segment_id[(i)-ENERGY_MIN]
@@ -92,52 +89,6 @@ void av1_vaq_frame_setup(AV1_COMP *cpi) {
   }
 }
 
-int av1_log_block_var(const AV1_COMP *cpi, MACROBLOCK *x, BLOCK_SIZE bs) {
-  // This functions returns a score for the blocks local variance as calculated
-  // by: sum of the log of the (4x4 variances) of each subblock to the current
-  // block (x,bs)
-  // * 32 / number of pixels in the block_size.
-  // This is used for segmentation because to avoid situations in which a large
-  // block with a gentle gradient gets marked high variance even though each
-  // subblock has a low variance.   This allows us to assign the same segment
-  // number for the same sorts of area regardless of how the partitioning goes.
-
-  MACROBLOCKD *xd = &x->e_mbd;
-  double var = 0;
-  unsigned int sse;
-  int i, j;
-
-  int right_overflow =
-      (xd->mb_to_right_edge < 0) ? ((-xd->mb_to_right_edge) >> 3) : 0;
-  int bottom_overflow =
-      (xd->mb_to_bottom_edge < 0) ? ((-xd->mb_to_bottom_edge) >> 3) : 0;
-
-  const int bw = MI_SIZE * mi_size_wide[bs] - right_overflow;
-  const int bh = MI_SIZE * mi_size_high[bs] - bottom_overflow;
-
-  for (i = 0; i < bh; i += 4) {
-    for (j = 0; j < bw; j += 4) {
-      if (is_cur_buf_hbd(xd)) {
-        var += log1p(cpi->ppi->fn_ptr[BLOCK_4X4].vf(
-                         x->plane[0].src.buf + i * x->plane[0].src.stride + j,
-                         x->plane[0].src.stride,
-                         CONVERT_TO_BYTEPTR(av1_highbd_all_zeros), 0, &sse) /
-                     16.0);
-      } else {
-        var += log1p(cpi->ppi->fn_ptr[BLOCK_4X4].vf(
-                         x->plane[0].src.buf + i * x->plane[0].src.stride + j,
-                         x->plane[0].src.stride, av1_all_zeros, 0, &sse) /
-                     16.0);
-      }
-    }
-  }
-  // Use average of 4x4 log variance. The range for 8 bit 0 - 9.704121561.
-  var /= (bw / 4 * bh / 4);
-  if (var > 7) var = 7;
-
-  return (int)(var);
-}
-
 int av1_log_block_avg(const AV1_COMP *cpi, MACROBLOCK *x, BLOCK_SIZE bs,
                       int mi_row, int mi_col) {
   // This functions returns the block average of luma block
@@ -218,3 +169,54 @@ int av1_compute_q_from_energy_level_deltaq_mode(const AV1_COMP *const cpi,
   }
   return base_qindex + qindex_delta;
 }
+#endif  // !CONFIG_REALTIME_ONLY
+
+int av1_log_block_var(const AV1_COMP *cpi, MACROBLOCK *x, BLOCK_SIZE bs) {
+  DECLARE_ALIGNED(16, static const uint16_t,
+                  av1_highbd_all_zeros[MAX_SB_SIZE]) = { 0 };
+  DECLARE_ALIGNED(16, static const uint8_t, av1_all_zeros[MAX_SB_SIZE]) = { 0 };
+
+  // This function returns a score for the blocks local variance as calculated
+  // by: sum of the log of the (4x4 variances) of each subblock to the current
+  // block (x,bs)
+  // * 32 / number of pixels in the block_size.
+  // This is used for segmentation because to avoid situations in which a large
+  // block with a gentle gradient gets marked high variance even though each
+  // subblock has a low variance.   This allows us to assign the same segment
+  // number for the same sorts of area regardless of how the partitioning goes.
+
+  MACROBLOCKD *xd = &x->e_mbd;
+  double var = 0;
+  unsigned int sse;
+  int i, j;
+
+  int right_overflow =
+      (xd->mb_to_right_edge < 0) ? ((-xd->mb_to_right_edge) >> 3) : 0;
+  int bottom_overflow =
+      (xd->mb_to_bottom_edge < 0) ? ((-xd->mb_to_bottom_edge) >> 3) : 0;
+
+  const int bw = MI_SIZE * mi_size_wide[bs] - right_overflow;
+  const int bh = MI_SIZE * mi_size_high[bs] - bottom_overflow;
+
+  for (i = 0; i < bh; i += 4) {
+    for (j = 0; j < bw; j += 4) {
+      if (is_cur_buf_hbd(xd)) {
+        var += log1p(cpi->ppi->fn_ptr[BLOCK_4X4].vf(
+                         x->plane[0].src.buf + i * x->plane[0].src.stride + j,
+                         x->plane[0].src.stride,
+                         CONVERT_TO_BYTEPTR(av1_highbd_all_zeros), 0, &sse) /
+                     16.0);
+      } else {
+        var += log1p(cpi->ppi->fn_ptr[BLOCK_4X4].vf(
+                         x->plane[0].src.buf + i * x->plane[0].src.stride + j,
+                         x->plane[0].src.stride, av1_all_zeros, 0, &sse) /
+                     16.0);
+      }
+    }
+  }
+  // Use average of 4x4 log variance. The range for 8 bit 0 - 9.704121561.
+  var /= (bw / 4 * bh / 4);
+  if (var > 7) var = 7;
+
+  return (int)(var);
+}
diff --git a/av1/encoder/aq_variance.h b/av1/encoder/aq_variance.h
index 8d07cd95b..2e9a73aec 100644
--- a/av1/encoder/aq_variance.h
+++ b/av1/encoder/aq_variance.h
@@ -13,20 +13,24 @@
 #define AOM_AV1_ENCODER_AQ_VARIANCE_H_
 
 #include "av1/encoder/encoder.h"
+#include "config/aom_config.h"
 
 #ifdef __cplusplus
 extern "C" {
 #endif
 
+#if !CONFIG_REALTIME_ONLY
 void av1_vaq_frame_setup(AV1_COMP *cpi);
 
-int av1_log_block_var(const AV1_COMP *cpi, MACROBLOCK *x, BLOCK_SIZE bs);
 int av1_log_block_avg(const AV1_COMP *cpi, MACROBLOCK *x, BLOCK_SIZE bs,
                       int mi_row, int mi_col);
 int av1_compute_q_from_energy_level_deltaq_mode(const AV1_COMP *const cpi,
                                                 int block_var_level);
 int av1_block_wavelet_energy_level(const AV1_COMP *cpi, MACROBLOCK *x,
                                    BLOCK_SIZE bs);
+#endif  // !CONFIG_REALTIME_ONLY
+
+int av1_log_block_var(const AV1_COMP *cpi, MACROBLOCK *x, BLOCK_SIZE bs);
 
 #ifdef __cplusplus
 }  // extern "C"
diff --git a/av1/encoder/arm/highbd_pickrst_neon.c b/av1/encoder/arm/highbd_pickrst_neon.c
index 3d69e200c..00f88bcf0 100644
--- a/av1/encoder/arm/highbd_pickrst_neon.c
+++ b/av1/encoder/arm/highbd_pickrst_neon.c
@@ -15,6 +15,7 @@
 
 #include "aom_dsp/arm/mem_neon.h"
 #include "aom_dsp/arm/sum_neon.h"
+#include "aom_dsp/arm/transpose_neon.h"
 #include "av1/encoder/arm/pickrst_neon.h"
 #include "av1/encoder/pickrst.h"
 
@@ -272,680 +273,1382 @@ void av1_calc_proj_params_high_bd_neon(const uint8_t *src8, int width,
   }
 }
 
-static inline int16x8_t tbl2q(int16x8_t a, int16x8_t b, uint8x16_t idx) {
+static inline void hadd_update_4_stats_neon(const int64_t *const src,
+                                            const int32x4_t *deltas,
+                                            int64_t *const dst) {
+  int64x2_t delta0_s64 = vpaddlq_s32(deltas[0]);
+  int64x2_t delta1_s64 = vpaddlq_s32(deltas[1]);
+  int64x2_t delta2_s64 = vpaddlq_s32(deltas[2]);
+  int64x2_t delta3_s64 = vpaddlq_s32(deltas[3]);
+
 #if AOM_ARCH_AARCH64
-  uint8x16x2_t table = { { vreinterpretq_u8_s16(a), vreinterpretq_u8_s16(b) } };
-  return vreinterpretq_s16_u8(vqtbl2q_u8(table, idx));
+  int64x2_t delta01 = vpaddq_s64(delta0_s64, delta1_s64);
+  int64x2_t delta23 = vpaddq_s64(delta2_s64, delta3_s64);
+
+  int64x2_t src0 = vld1q_s64(src);
+  int64x2_t src1 = vld1q_s64(src + 2);
+  vst1q_s64(dst, vaddq_s64(src0, delta01));
+  vst1q_s64(dst + 2, vaddq_s64(src1, delta23));
 #else
-  uint8x8x4_t table = { { vreinterpret_u8_s16(vget_low_s16(a)),
-                          vreinterpret_u8_s16(vget_high_s16(a)),
-                          vreinterpret_u8_s16(vget_low_s16(b)),
-                          vreinterpret_u8_s16(vget_high_s16(b)) } };
-  return vreinterpretq_s16_u8(vcombine_u8(vtbl4_u8(table, vget_low_u8(idx)),
-                                          vtbl4_u8(table, vget_high_u8(idx))));
+  dst[0] = src[0] + horizontal_add_s64x2(delta0_s64);
+  dst[1] = src[1] + horizontal_add_s64x2(delta1_s64);
+  dst[2] = src[2] + horizontal_add_s64x2(delta2_s64);
+  dst[3] = src[3] + horizontal_add_s64x2(delta3_s64);
 #endif
 }
 
-static inline int16x8_t tbl3q(int16x8_t a, int16x8_t b, int16x8_t c,
-                              uint8x16_t idx) {
+static inline void compute_stats_win5_highbd_neon(
+    const int16_t *const d, const int32_t d_stride, const int16_t *const s,
+    const int32_t s_stride, const int32_t width, const int32_t height,
+    int64_t *const M, int64_t *const H, aom_bit_depth_t bit_depth) {
+  const int32_t wiener_win = WIENER_WIN_CHROMA;
+  const int32_t wiener_win2 = wiener_win * wiener_win;
+  const int32_t w16 = width & ~15;
+  const int32_t h8 = height & ~7;
+  int16x8_t mask[2];
+  mask[0] = vld1q_s16(&(mask_16bit[16]) - width % 16);
+  mask[1] = vld1q_s16(&(mask_16bit[16]) - width % 16 + 8);
+  int32_t i, j, x, y;
+
+  const int32_t num_bit_left =
+      32 - 1 /* sign */ - 2 * bit_depth /* energy */ + 2 /* SIMD */;
+  const int32_t h_allowed =
+      (1 << num_bit_left) / (w16 + ((w16 != width) ? 16 : 0));
+
+  // Step 1: Calculate the top edge of the whole matrix, i.e., the top
+  // edge of each triangle and square on the top row.
+  j = 0;
+  do {
+    const int16_t *s_t = s;
+    const int16_t *d_t = d;
+    int32_t height_t = 0;
+    int64x2_t sum_m[WIENER_WIN_CHROMA] = { vdupq_n_s64(0) };
+    int64x2_t sum_h[WIENER_WIN_CHROMA] = { vdupq_n_s64(0) };
+    int16x8_t src[2], dgd[2];
+
+    do {
+      const int32_t h_t =
+          ((height - height_t) < h_allowed) ? (height - height_t) : h_allowed;
+      int32x4_t row_m[WIENER_WIN_CHROMA] = { vdupq_n_s32(0) };
+      int32x4_t row_h[WIENER_WIN_CHROMA] = { vdupq_n_s32(0) };
+
+      y = h_t;
+      do {
+        x = 0;
+        while (x < w16) {
+          src[0] = vld1q_s16(s_t + x + 0);
+          src[1] = vld1q_s16(s_t + x + 8);
+          dgd[0] = vld1q_s16(d_t + x + 0);
+          dgd[1] = vld1q_s16(d_t + x + 8);
+          stats_top_win5_neon(src, dgd, d_t + j + x, d_stride, row_m, row_h);
+          x += 16;
+        }
+
+        if (w16 != width) {
+          src[0] = vld1q_s16(s_t + w16 + 0);
+          src[1] = vld1q_s16(s_t + w16 + 8);
+          dgd[0] = vld1q_s16(d_t + w16 + 0);
+          dgd[1] = vld1q_s16(d_t + w16 + 8);
+          src[0] = vandq_s16(src[0], mask[0]);
+          src[1] = vandq_s16(src[1], mask[1]);
+          dgd[0] = vandq_s16(dgd[0], mask[0]);
+          dgd[1] = vandq_s16(dgd[1], mask[1]);
+          stats_top_win5_neon(src, dgd, d_t + j + w16, d_stride, row_m, row_h);
+        }
+
+        s_t += s_stride;
+        d_t += d_stride;
+      } while (--y);
+
+      sum_m[0] = vpadalq_s32(sum_m[0], row_m[0]);
+      sum_m[1] = vpadalq_s32(sum_m[1], row_m[1]);
+      sum_m[2] = vpadalq_s32(sum_m[2], row_m[2]);
+      sum_m[3] = vpadalq_s32(sum_m[3], row_m[3]);
+      sum_m[4] = vpadalq_s32(sum_m[4], row_m[4]);
+      sum_h[0] = vpadalq_s32(sum_h[0], row_h[0]);
+      sum_h[1] = vpadalq_s32(sum_h[1], row_h[1]);
+      sum_h[2] = vpadalq_s32(sum_h[2], row_h[2]);
+      sum_h[3] = vpadalq_s32(sum_h[3], row_h[3]);
+      sum_h[4] = vpadalq_s32(sum_h[4], row_h[4]);
+
+      height_t += h_t;
+    } while (height_t < height);
+
 #if AOM_ARCH_AARCH64
-  uint8x16x3_t table = { { vreinterpretq_u8_s16(a), vreinterpretq_u8_s16(b),
-                           vreinterpretq_u8_s16(c) } };
-  return vreinterpretq_s16_u8(vqtbl3q_u8(table, idx));
+    int64x2_t sum_m0 = vpaddq_s64(sum_m[0], sum_m[1]);
+    int64x2_t sum_m2 = vpaddq_s64(sum_m[2], sum_m[3]);
+    vst1q_s64(&M[wiener_win * j + 0], sum_m0);
+    vst1q_s64(&M[wiener_win * j + 2], sum_m2);
+    M[wiener_win * j + 4] = vaddvq_s64(sum_m[4]);
+
+    int64x2_t sum_h0 = vpaddq_s64(sum_h[0], sum_h[1]);
+    int64x2_t sum_h2 = vpaddq_s64(sum_h[2], sum_h[3]);
+    vst1q_s64(&H[wiener_win * j + 0], sum_h0);
+    vst1q_s64(&H[wiener_win * j + 2], sum_h2);
+    H[wiener_win * j + 4] = vaddvq_s64(sum_h[4]);
 #else
-  // This is a specific implementation working only for compute stats with
-  // wiener_win == 5.
-  uint8x8x3_t table_lo = { { vreinterpret_u8_s16(vget_low_s16(a)),
-                             vreinterpret_u8_s16(vget_high_s16(a)),
-                             vreinterpret_u8_s16(vget_low_s16(b)) } };
-  uint8x8x3_t table_hi = { { vreinterpret_u8_s16(vget_low_s16(b)),
-                             vreinterpret_u8_s16(vget_high_s16(b)),
-                             vreinterpret_u8_s16(vget_low_s16(c)) } };
-  return vreinterpretq_s16_u8(vcombine_u8(
-      vtbl3_u8(table_lo, vget_low_u8(idx)),
-      vtbl3_u8(table_hi, vsub_u8(vget_high_u8(idx), vdup_n_u8(16)))));
-#endif
-}
+    M[wiener_win * j + 0] = horizontal_add_s64x2(sum_m[0]);
+    M[wiener_win * j + 1] = horizontal_add_s64x2(sum_m[1]);
+    M[wiener_win * j + 2] = horizontal_add_s64x2(sum_m[2]);
+    M[wiener_win * j + 3] = horizontal_add_s64x2(sum_m[3]);
+    M[wiener_win * j + 4] = horizontal_add_s64x2(sum_m[4]);
+
+    H[wiener_win * j + 0] = horizontal_add_s64x2(sum_h[0]);
+    H[wiener_win * j + 1] = horizontal_add_s64x2(sum_h[1]);
+    H[wiener_win * j + 2] = horizontal_add_s64x2(sum_h[2]);
+    H[wiener_win * j + 3] = horizontal_add_s64x2(sum_h[3]);
+    H[wiener_win * j + 4] = horizontal_add_s64x2(sum_h[4]);
+#endif  // AOM_ARCH_AARCH64
+  } while (++j < wiener_win);
+
+  // Step 2: Calculate the left edge of each square on the top row.
+  j = 1;
+  do {
+    const int16_t *d_t = d;
+    int32_t height_t = 0;
+    int64x2_t sum_h[WIENER_WIN_CHROMA - 1] = { vdupq_n_s64(0) };
+    int16x8_t dgd[2];
 
-static inline int64_t div_shift_s64(int64_t x, int power) {
-  return (x < 0 ? x + (1ll << power) - 1 : x) >> power;
-}
+    do {
+      const int32_t h_t =
+          ((height - height_t) < h_allowed) ? (height - height_t) : h_allowed;
+      int32x4_t row_h[WIENER_WIN_CHROMA - 1] = { vdupq_n_s32(0) };
+
+      y = h_t;
+      do {
+        x = 0;
+        while (x < w16) {
+          dgd[0] = vld1q_s16(d_t + j + x + 0);
+          dgd[1] = vld1q_s16(d_t + j + x + 8);
+          stats_left_win5_neon(dgd, d_t + x, d_stride, row_h);
+          x += 16;
+        }
+
+        if (w16 != width) {
+          dgd[0] = vld1q_s16(d_t + j + x + 0);
+          dgd[1] = vld1q_s16(d_t + j + x + 8);
+          dgd[0] = vandq_s16(dgd[0], mask[0]);
+          dgd[1] = vandq_s16(dgd[1], mask[1]);
+          stats_left_win5_neon(dgd, d_t + x, d_stride, row_h);
+        }
 
-// The M matrix is accumulated in a bitdepth-dependent number of steps to
-// speed up the computation. This function computes the final M from the
-// accumulated (src_s64) and the residual parts (src_s32). It also transposes
-// the result as the output needs to be column-major.
-static inline void acc_transpose_M(int64_t *dst, const int64_t *src_s64,
-                                   const int32_t *src_s32, const int wiener_win,
-                                   int shift) {
-  for (int i = 0; i < wiener_win; ++i) {
-    for (int j = 0; j < wiener_win; ++j) {
-      int tr_idx = j * wiener_win + i;
-      *dst++ = div_shift_s64(src_s64[tr_idx] + src_s32[tr_idx], shift);
+        d_t += d_stride;
+      } while (--y);
+
+      sum_h[0] = vpadalq_s32(sum_h[0], row_h[0]);
+      sum_h[1] = vpadalq_s32(sum_h[1], row_h[1]);
+      sum_h[2] = vpadalq_s32(sum_h[2], row_h[2]);
+      sum_h[3] = vpadalq_s32(sum_h[3], row_h[3]);
+
+      height_t += h_t;
+    } while (height_t < height);
+
+#if AOM_ARCH_AARCH64
+    int64x2_t sum_h0 = vpaddq_s64(sum_h[0], sum_h[1]);
+    int64x2_t sum_h1 = vpaddq_s64(sum_h[2], sum_h[3]);
+    vst1_s64(&H[1 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h0));
+    vst1_s64(&H[2 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h0));
+    vst1_s64(&H[3 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h1));
+    vst1_s64(&H[4 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h1));
+#else
+    H[1 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[0]);
+    H[2 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[1]);
+    H[3 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[2]);
+    H[4 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[3]);
+#endif  // AOM_ARCH_AARCH64
+  } while (++j < wiener_win);
+
+  // Step 3: Derive the top edge of each triangle along the diagonal. No
+  // triangle in top row.
+  {
+    const int16_t *d_t = d;
+
+    if (height % 2) {
+      int32x4_t deltas[(WIENER_WIN + 1) * 2] = { vdupq_n_s32(0) };
+      int32x4_t deltas_tr[(WIENER_WIN + 1) * 2] = { vdupq_n_s32(0) };
+      int16x8_t ds[WIENER_WIN * 2];
+
+      load_s16_8x4(d_t, d_stride, &ds[0], &ds[2], &ds[4], &ds[6]);
+      load_s16_8x4(d_t + width, d_stride, &ds[1], &ds[3], &ds[5], &ds[7]);
+      d_t += 4 * d_stride;
+
+      step3_win5_oneline_neon(&d_t, d_stride, width, height, ds, deltas);
+      transpose_arrays_s32_8x8(deltas, deltas_tr);
+
+      update_5_stats_neon(H + 0 * wiener_win * wiener_win2 + 0 * wiener_win,
+                          deltas_tr[0], vgetq_lane_s32(deltas_tr[4], 0),
+                          H + 1 * wiener_win * wiener_win2 + 1 * wiener_win);
+
+      update_5_stats_neon(H + 1 * wiener_win * wiener_win2 + 1 * wiener_win,
+                          deltas_tr[1], vgetq_lane_s32(deltas_tr[5], 0),
+                          H + 2 * wiener_win * wiener_win2 + 2 * wiener_win);
+
+      update_5_stats_neon(H + 2 * wiener_win * wiener_win2 + 2 * wiener_win,
+                          deltas_tr[2], vgetq_lane_s32(deltas_tr[6], 0),
+                          H + 3 * wiener_win * wiener_win2 + 3 * wiener_win);
+
+      update_5_stats_neon(H + 3 * wiener_win * wiener_win2 + 3 * wiener_win,
+                          deltas_tr[3], vgetq_lane_s32(deltas_tr[7], 0),
+                          H + 4 * wiener_win * wiener_win2 + 4 * wiener_win);
+
+    } else {
+      int32x4_t deltas[WIENER_WIN_CHROMA * 2] = { vdupq_n_s32(0) };
+      int16x8_t ds[WIENER_WIN_CHROMA * 2];
+
+      ds[0] = load_unaligned_s16_4x2(d_t + 0 * d_stride, width);
+      ds[1] = load_unaligned_s16_4x2(d_t + 1 * d_stride, width);
+      ds[2] = load_unaligned_s16_4x2(d_t + 2 * d_stride, width);
+      ds[3] = load_unaligned_s16_4x2(d_t + 3 * d_stride, width);
+
+      step3_win5_neon(d_t + 4 * d_stride, d_stride, width, height, ds, deltas);
+
+      transpose_elems_inplace_s32_4x4(&deltas[0], &deltas[1], &deltas[2],
+                                      &deltas[3]);
+
+      update_5_stats_neon(H + 0 * wiener_win * wiener_win2 + 0 * wiener_win,
+                          deltas[0], vgetq_lane_s32(deltas[4], 0),
+                          H + 1 * wiener_win * wiener_win2 + 1 * wiener_win);
+
+      update_5_stats_neon(H + 1 * wiener_win * wiener_win2 + 1 * wiener_win,
+                          deltas[1], vgetq_lane_s32(deltas[4], 1),
+                          H + 2 * wiener_win * wiener_win2 + 2 * wiener_win);
+
+      update_5_stats_neon(H + 2 * wiener_win * wiener_win2 + 2 * wiener_win,
+                          deltas[2], vgetq_lane_s32(deltas[4], 2),
+                          H + 3 * wiener_win * wiener_win2 + 3 * wiener_win);
+
+      update_5_stats_neon(H + 3 * wiener_win * wiener_win2 + 3 * wiener_win,
+                          deltas[3], vgetq_lane_s32(deltas[4], 3),
+                          H + 4 * wiener_win * wiener_win2 + 4 * wiener_win);
     }
   }
-}
 
-// The resulting H is a column-major matrix accumulated from the transposed
-// (column-major) samples of the filter kernel (5x5 or 7x7) viewed as a single
-// vector. For the 7x7 filter case: H(49x49) = [49 x 1] x [1 x 49]. This
-// function transforms back to the originally expected format (double
-// transpose). The H matrix is accumulated in a bitdepth-dependent number of
-// steps to speed up the computation. This function computes the final H from
-// the accumulated (src_s64) and the residual parts (src_s32). The computed H is
-// only an upper triangle matrix, this function also fills the lower triangle of
-// the resulting matrix.
-static inline void update_H(int64_t *dst, const int64_t *src_s64,
-                            const int32_t *src_s32, const int wiener_win,
-                            int stride, int shift) {
-  // For a simplified theoretical 3x3 case where `wiener_win` is 3 and
-  // `wiener_win2` is 9, the M matrix is 3x3:
-  // 0, 3, 6
-  // 1, 4, 7
-  // 2, 5, 8
-  //
-  // This is viewed as a vector to compute H (9x9) by vector outer product:
-  // 0, 3, 6, 1, 4, 7, 2, 5, 8
-  //
-  // Double transpose and upper triangle remapping for 3x3 -> 9x9 case:
-  // 0,    3,    6,    1,    4,    7,    2,    5,    8,
-  // 3,   30,   33,   12,   31,   34,   21,   32,   35,
-  // 6,   33,   60,   15,   42,   61,   24,   51,   62,
-  // 1,   12,   15,   10,   13,   16,   11,   14,   17,
-  // 4,   31,   42,   13,   40,   43,   22,   41,   44,
-  // 7,   34,   61,   16,   43,   70,   25,   52,   71,
-  // 2,   21,   24,   11,   22,   25,   20,   23,   26,
-  // 5,   32,   51,   14,   41,   52,   23,   50,   53,
-  // 8,   35,   62,   17,   44,   71,   26,   53,   80,
-  const int wiener_win2 = wiener_win * wiener_win;
-
-  // Loop through the indices according to the remapping above, along the
-  // columns:
-  // 0, wiener_win, 2 * wiener_win, ..., 1, 1 + 2 * wiener_win, ...,
-  // wiener_win - 1, wiener_win - 1 + wiener_win, ...
-  // For the 3x3 case `j` will be: 0, 3, 6, 1, 4, 7, 2, 5, 8.
-  for (int i = 0; i < wiener_win; ++i) {
-    for (int j = i; j < wiener_win2; j += wiener_win) {
-      // These two inner loops are the same as the two outer loops, but running
-      // along rows instead of columns. For the 3x3 case `l` will be:
-      // 0, 3, 6, 1, 4, 7, 2, 5, 8.
-      for (int k = 0; k < wiener_win; ++k) {
-        for (int l = k; l < wiener_win2; l += wiener_win) {
-          // The nominal double transpose indexing would be:
-          // int idx = stride * j + l;
-          // However we need the upper-right triangle, it is easy with some
-          // min/max operations.
-          int tr_idx = stride * AOMMIN(j, l) + AOMMAX(j, l);
-
-          // Resulting matrix is filled by combining the 64-bit and the residual
-          // 32-bit matrices together with scaling.
-          *dst++ = div_shift_s64(src_s64[tr_idx] + src_s32[tr_idx], shift);
-        }
+  // Step 4: Derive the top and left edge of each square. No square in top and
+  // bottom row.
+
+  {
+    y = h8;
+
+    int16x4_t d_s[12];
+    int16x4_t d_e[12];
+    const int16_t *d_t = d;
+    int16x4_t zeros = vdup_n_s16(0);
+    load_s16_4x4(d_t, d_stride, &d_s[0], &d_s[1], &d_s[2], &d_s[3]);
+    load_s16_4x4(d_t + width, d_stride, &d_e[0], &d_e[1], &d_e[2], &d_e[3]);
+    int32x4_t deltas[6][18] = { { vdupq_n_s32(0) }, { vdupq_n_s32(0) } };
+
+    while (y >= 8) {
+      load_s16_4x8(d_t + 4 * d_stride, d_stride, &d_s[4], &d_s[5], &d_s[6],
+                   &d_s[7], &d_s[8], &d_s[9], &d_s[10], &d_s[11]);
+      load_s16_4x8(d_t + width + 4 * d_stride, d_stride, &d_e[4], &d_e[5],
+                   &d_e[6], &d_e[7], &d_e[8], &d_e[9], &d_e[10], &d_e[11]);
+
+      int16x8_t s_tr[8], e_tr[8];
+      transpose_elems_s16_4x8(d_s[0], d_s[1], d_s[2], d_s[3], d_s[4], d_s[5],
+                              d_s[6], d_s[7], &s_tr[0], &s_tr[1], &s_tr[2],
+                              &s_tr[3]);
+      transpose_elems_s16_4x8(d_s[8], d_s[9], d_s[10], d_s[11], zeros, zeros,
+                              zeros, zeros, &s_tr[4], &s_tr[5], &s_tr[6],
+                              &s_tr[7]);
+
+      transpose_elems_s16_4x8(d_e[0], d_e[1], d_e[2], d_e[3], d_e[4], d_e[5],
+                              d_e[6], d_e[7], &e_tr[0], &e_tr[1], &e_tr[2],
+                              &e_tr[3]);
+      transpose_elems_s16_4x8(d_e[8], d_e[9], d_e[10], d_e[11], zeros, zeros,
+                              zeros, zeros, &e_tr[4], &e_tr[5], &e_tr[6],
+                              &e_tr[7]);
+
+      int16x8_t start_col0[5], start_col1[5], start_col2[5], start_col3[5];
+      start_col0[0] = s_tr[0];
+      start_col0[1] = vextq_s16(s_tr[0], s_tr[4], 1);
+      start_col0[2] = vextq_s16(s_tr[0], s_tr[4], 2);
+      start_col0[3] = vextq_s16(s_tr[0], s_tr[4], 3);
+      start_col0[4] = vextq_s16(s_tr[0], s_tr[4], 4);
+
+      start_col1[0] = s_tr[1];
+      start_col1[1] = vextq_s16(s_tr[1], s_tr[5], 1);
+      start_col1[2] = vextq_s16(s_tr[1], s_tr[5], 2);
+      start_col1[3] = vextq_s16(s_tr[1], s_tr[5], 3);
+      start_col1[4] = vextq_s16(s_tr[1], s_tr[5], 4);
+
+      start_col2[0] = s_tr[2];
+      start_col2[1] = vextq_s16(s_tr[2], s_tr[6], 1);
+      start_col2[2] = vextq_s16(s_tr[2], s_tr[6], 2);
+      start_col2[3] = vextq_s16(s_tr[2], s_tr[6], 3);
+      start_col2[4] = vextq_s16(s_tr[2], s_tr[6], 4);
+
+      start_col3[0] = s_tr[3];
+      start_col3[1] = vextq_s16(s_tr[3], s_tr[7], 1);
+      start_col3[2] = vextq_s16(s_tr[3], s_tr[7], 2);
+      start_col3[3] = vextq_s16(s_tr[3], s_tr[7], 3);
+      start_col3[4] = vextq_s16(s_tr[3], s_tr[7], 4);
+
+      // i = 1, j = 2;
+      sub_deltas_step4(start_col0, start_col1, deltas[0]);
+
+      // i = 1, j = 3;
+      sub_deltas_step4(start_col0, start_col2, deltas[1]);
+
+      // i = 1, j = 4
+      sub_deltas_step4(start_col0, start_col3, deltas[2]);
+
+      // i = 2, j =3
+      sub_deltas_step4(start_col1, start_col2, deltas[3]);
+
+      // i = 2, j = 4
+      sub_deltas_step4(start_col1, start_col3, deltas[4]);
+
+      // i = 3, j = 4
+      sub_deltas_step4(start_col2, start_col3, deltas[5]);
+
+      int16x8_t end_col0[5], end_col1[5], end_col2[5], end_col3[5];
+      end_col0[0] = e_tr[0];
+      end_col0[1] = vextq_s16(e_tr[0], e_tr[4], 1);
+      end_col0[2] = vextq_s16(e_tr[0], e_tr[4], 2);
+      end_col0[3] = vextq_s16(e_tr[0], e_tr[4], 3);
+      end_col0[4] = vextq_s16(e_tr[0], e_tr[4], 4);
+
+      end_col1[0] = e_tr[1];
+      end_col1[1] = vextq_s16(e_tr[1], e_tr[5], 1);
+      end_col1[2] = vextq_s16(e_tr[1], e_tr[5], 2);
+      end_col1[3] = vextq_s16(e_tr[1], e_tr[5], 3);
+      end_col1[4] = vextq_s16(e_tr[1], e_tr[5], 4);
+
+      end_col2[0] = e_tr[2];
+      end_col2[1] = vextq_s16(e_tr[2], e_tr[6], 1);
+      end_col2[2] = vextq_s16(e_tr[2], e_tr[6], 2);
+      end_col2[3] = vextq_s16(e_tr[2], e_tr[6], 3);
+      end_col2[4] = vextq_s16(e_tr[2], e_tr[6], 4);
+
+      end_col3[0] = e_tr[3];
+      end_col3[1] = vextq_s16(e_tr[3], e_tr[7], 1);
+      end_col3[2] = vextq_s16(e_tr[3], e_tr[7], 2);
+      end_col3[3] = vextq_s16(e_tr[3], e_tr[7], 3);
+      end_col3[4] = vextq_s16(e_tr[3], e_tr[7], 4);
+
+      // i = 1, j = 2;
+      add_deltas_step4(end_col0, end_col1, deltas[0]);
+
+      // i = 1, j = 3;
+      add_deltas_step4(end_col0, end_col2, deltas[1]);
+
+      // i = 1, j = 4
+      add_deltas_step4(end_col0, end_col3, deltas[2]);
+
+      // i = 2, j =3
+      add_deltas_step4(end_col1, end_col2, deltas[3]);
+
+      // i = 2, j = 4
+      add_deltas_step4(end_col1, end_col3, deltas[4]);
+
+      // i = 3, j = 4
+      add_deltas_step4(end_col2, end_col3, deltas[5]);
+
+      d_s[0] = d_s[8];
+      d_s[1] = d_s[9];
+      d_s[2] = d_s[10];
+      d_s[3] = d_s[11];
+      d_e[0] = d_e[8];
+      d_e[1] = d_e[9];
+      d_e[2] = d_e[10];
+      d_e[3] = d_e[11];
+
+      d_t += 8 * d_stride;
+      y -= 8;
+    }
+
+    if (h8 != height) {
+      const int16x8_t mask_h = vld1q_s16(&mask_16bit[16] - (height % 8));
+
+      load_s16_4x8(d_t + 4 * d_stride, d_stride, &d_s[4], &d_s[5], &d_s[6],
+                   &d_s[7], &d_s[8], &d_s[9], &d_s[10], &d_s[11]);
+      load_s16_4x8(d_t + width + 4 * d_stride, d_stride, &d_e[4], &d_e[5],
+                   &d_e[6], &d_e[7], &d_e[8], &d_e[9], &d_e[10], &d_e[11]);
+      int16x8_t s_tr[8], e_tr[8];
+      transpose_elems_s16_4x8(d_s[0], d_s[1], d_s[2], d_s[3], d_s[4], d_s[5],
+                              d_s[6], d_s[7], &s_tr[0], &s_tr[1], &s_tr[2],
+                              &s_tr[3]);
+      transpose_elems_s16_4x8(d_s[8], d_s[9], d_s[10], d_s[11], zeros, zeros,
+                              zeros, zeros, &s_tr[4], &s_tr[5], &s_tr[6],
+                              &s_tr[7]);
+      transpose_elems_s16_4x8(d_e[0], d_e[1], d_e[2], d_e[3], d_e[4], d_e[5],
+                              d_e[6], d_e[7], &e_tr[0], &e_tr[1], &e_tr[2],
+                              &e_tr[3]);
+      transpose_elems_s16_4x8(d_e[8], d_e[9], d_e[10], d_e[11], zeros, zeros,
+                              zeros, zeros, &e_tr[4], &e_tr[5], &e_tr[6],
+                              &e_tr[7]);
+
+      int16x8_t start_col0[5], start_col1[5], start_col2[5], start_col3[5];
+      start_col0[0] = vandq_s16(s_tr[0], mask_h);
+      start_col0[1] = vandq_s16(vextq_s16(s_tr[0], s_tr[4], 1), mask_h);
+      start_col0[2] = vandq_s16(vextq_s16(s_tr[0], s_tr[4], 2), mask_h);
+      start_col0[3] = vandq_s16(vextq_s16(s_tr[0], s_tr[4], 3), mask_h);
+      start_col0[4] = vandq_s16(vextq_s16(s_tr[0], s_tr[4], 4), mask_h);
+
+      start_col1[0] = vandq_s16(s_tr[1], mask_h);
+      start_col1[1] = vandq_s16(vextq_s16(s_tr[1], s_tr[5], 1), mask_h);
+      start_col1[2] = vandq_s16(vextq_s16(s_tr[1], s_tr[5], 2), mask_h);
+      start_col1[3] = vandq_s16(vextq_s16(s_tr[1], s_tr[5], 3), mask_h);
+      start_col1[4] = vandq_s16(vextq_s16(s_tr[1], s_tr[5], 4), mask_h);
+
+      start_col2[0] = vandq_s16(s_tr[2], mask_h);
+      start_col2[1] = vandq_s16(vextq_s16(s_tr[2], s_tr[6], 1), mask_h);
+      start_col2[2] = vandq_s16(vextq_s16(s_tr[2], s_tr[6], 2), mask_h);
+      start_col2[3] = vandq_s16(vextq_s16(s_tr[2], s_tr[6], 3), mask_h);
+      start_col2[4] = vandq_s16(vextq_s16(s_tr[2], s_tr[6], 4), mask_h);
+
+      start_col3[0] = vandq_s16(s_tr[3], mask_h);
+      start_col3[1] = vandq_s16(vextq_s16(s_tr[3], s_tr[7], 1), mask_h);
+      start_col3[2] = vandq_s16(vextq_s16(s_tr[3], s_tr[7], 2), mask_h);
+      start_col3[3] = vandq_s16(vextq_s16(s_tr[3], s_tr[7], 3), mask_h);
+      start_col3[4] = vandq_s16(vextq_s16(s_tr[3], s_tr[7], 4), mask_h);
+
+      // i = 1, j = 2;
+      sub_deltas_step4(start_col0, start_col1, deltas[0]);
+
+      // i = 1, j = 3;
+      sub_deltas_step4(start_col0, start_col2, deltas[1]);
+
+      // i = 1, j = 4
+      sub_deltas_step4(start_col0, start_col3, deltas[2]);
+
+      // i = 2, j = 3
+      sub_deltas_step4(start_col1, start_col2, deltas[3]);
+
+      // i = 2, j = 4
+      sub_deltas_step4(start_col1, start_col3, deltas[4]);
+
+      // i = 3, j = 4
+      sub_deltas_step4(start_col2, start_col3, deltas[5]);
+
+      int16x8_t end_col0[5], end_col1[5], end_col2[5], end_col3[5];
+      end_col0[0] = vandq_s16(e_tr[0], mask_h);
+      end_col0[1] = vandq_s16(vextq_s16(e_tr[0], e_tr[4], 1), mask_h);
+      end_col0[2] = vandq_s16(vextq_s16(e_tr[0], e_tr[4], 2), mask_h);
+      end_col0[3] = vandq_s16(vextq_s16(e_tr[0], e_tr[4], 3), mask_h);
+      end_col0[4] = vandq_s16(vextq_s16(e_tr[0], e_tr[4], 4), mask_h);
+
+      end_col1[0] = vandq_s16(e_tr[1], mask_h);
+      end_col1[1] = vandq_s16(vextq_s16(e_tr[1], e_tr[5], 1), mask_h);
+      end_col1[2] = vandq_s16(vextq_s16(e_tr[1], e_tr[5], 2), mask_h);
+      end_col1[3] = vandq_s16(vextq_s16(e_tr[1], e_tr[5], 3), mask_h);
+      end_col1[4] = vandq_s16(vextq_s16(e_tr[1], e_tr[5], 4), mask_h);
+
+      end_col2[0] = vandq_s16(e_tr[2], mask_h);
+      end_col2[1] = vandq_s16(vextq_s16(e_tr[2], e_tr[6], 1), mask_h);
+      end_col2[2] = vandq_s16(vextq_s16(e_tr[2], e_tr[6], 2), mask_h);
+      end_col2[3] = vandq_s16(vextq_s16(e_tr[2], e_tr[6], 3), mask_h);
+      end_col2[4] = vandq_s16(vextq_s16(e_tr[2], e_tr[6], 4), mask_h);
+
+      end_col3[0] = vandq_s16(e_tr[3], mask_h);
+      end_col3[1] = vandq_s16(vextq_s16(e_tr[3], e_tr[7], 1), mask_h);
+      end_col3[2] = vandq_s16(vextq_s16(e_tr[3], e_tr[7], 2), mask_h);
+      end_col3[3] = vandq_s16(vextq_s16(e_tr[3], e_tr[7], 3), mask_h);
+      end_col3[4] = vandq_s16(vextq_s16(e_tr[3], e_tr[7], 4), mask_h);
+
+      // i = 1, j = 2;
+      add_deltas_step4(end_col0, end_col1, deltas[0]);
+
+      // i = 1, j = 3;
+      add_deltas_step4(end_col0, end_col2, deltas[1]);
+
+      // i = 1, j = 4
+      add_deltas_step4(end_col0, end_col3, deltas[2]);
+
+      // i = 2, j =3
+      add_deltas_step4(end_col1, end_col2, deltas[3]);
+
+      // i = 2, j = 4
+      add_deltas_step4(end_col1, end_col3, deltas[4]);
+
+      // i = 3, j = 4
+      add_deltas_step4(end_col2, end_col3, deltas[5]);
+    }
+
+    int32x4_t delta[6][2];
+    int32_t single_delta[6];
+
+    delta[0][0] = horizontal_add_4d_s32x4(&deltas[0][0]);
+    delta[1][0] = horizontal_add_4d_s32x4(&deltas[1][0]);
+    delta[2][0] = horizontal_add_4d_s32x4(&deltas[2][0]);
+    delta[3][0] = horizontal_add_4d_s32x4(&deltas[3][0]);
+    delta[4][0] = horizontal_add_4d_s32x4(&deltas[4][0]);
+    delta[5][0] = horizontal_add_4d_s32x4(&deltas[5][0]);
+
+    delta[0][1] = horizontal_add_4d_s32x4(&deltas[0][5]);
+    delta[1][1] = horizontal_add_4d_s32x4(&deltas[1][5]);
+    delta[2][1] = horizontal_add_4d_s32x4(&deltas[2][5]);
+    delta[3][1] = horizontal_add_4d_s32x4(&deltas[3][5]);
+    delta[4][1] = horizontal_add_4d_s32x4(&deltas[4][5]);
+    delta[5][1] = horizontal_add_4d_s32x4(&deltas[5][5]);
+
+    single_delta[0] = horizontal_add_s32x4(deltas[0][4]);
+    single_delta[1] = horizontal_add_s32x4(deltas[1][4]);
+    single_delta[2] = horizontal_add_s32x4(deltas[2][4]);
+    single_delta[3] = horizontal_add_s32x4(deltas[3][4]);
+    single_delta[4] = horizontal_add_s32x4(deltas[4][4]);
+    single_delta[5] = horizontal_add_s32x4(deltas[5][4]);
+
+    int idx = 0;
+    for (i = 1; i < wiener_win - 1; i++) {
+      for (j = i + 1; j < wiener_win; j++) {
+        update_4_stats_neon(
+            H + (i - 1) * wiener_win * wiener_win2 + (j - 1) * wiener_win,
+            delta[idx][0], H + i * wiener_win * wiener_win2 + j * wiener_win);
+        H[i * wiener_win * wiener_win2 + j * wiener_win + 4] =
+            H[(i - 1) * wiener_win * wiener_win2 + (j - 1) * wiener_win + 4] +
+            single_delta[idx];
+
+        H[(i * wiener_win + 1) * wiener_win2 + j * wiener_win] =
+            H[((i - 1) * wiener_win + 1) * wiener_win2 + (j - 1) * wiener_win] +
+            vgetq_lane_s32(delta[idx][1], 0);
+        H[(i * wiener_win + 2) * wiener_win2 + j * wiener_win] =
+            H[((i - 1) * wiener_win + 2) * wiener_win2 + (j - 1) * wiener_win] +
+            vgetq_lane_s32(delta[idx][1], 1);
+        H[(i * wiener_win + 3) * wiener_win2 + j * wiener_win] =
+            H[((i - 1) * wiener_win + 3) * wiener_win2 + (j - 1) * wiener_win] +
+            vgetq_lane_s32(delta[idx][1], 2);
+        H[(i * wiener_win + 4) * wiener_win2 + j * wiener_win] =
+            H[((i - 1) * wiener_win + 4) * wiener_win2 + (j - 1) * wiener_win] +
+            vgetq_lane_s32(delta[idx][1], 3);
+
+        idx++;
       }
     }
   }
-}
 
-// Load 7x7 matrix into 7 128-bit vectors from consecutive rows, the last load
-// address is offset to prevent out-of-bounds access.
-static inline void load_and_pack_s16_8x7(int16x8_t dst[7], const int16_t *src,
-                                         ptrdiff_t stride) {
-  dst[0] = vld1q_s16(src);
-  src += stride;
-  dst[1] = vld1q_s16(src);
-  src += stride;
-  dst[2] = vld1q_s16(src);
-  src += stride;
-  dst[3] = vld1q_s16(src);
-  src += stride;
-  dst[4] = vld1q_s16(src);
-  src += stride;
-  dst[5] = vld1q_s16(src);
-  src += stride;
-  dst[6] = vld1q_s16(src - 1);
-}
+  // Step 5: Derive other points of each square. No square in bottom row.
+  i = 0;
+  do {
+    const int16_t *const di = d + i;
 
-static inline void highbd_compute_stats_win7_neon(
-    const uint16_t *dgd, const uint16_t *src, int avg, int width, int height,
-    int dgd_stride, int src_stride, int64_t *M, int64_t *H,
-    aom_bit_depth_t bit_depth) {
-  // Matrix names are capitalized to help readability.
-  DECLARE_ALIGNED(64, int16_t, DGD_AVG0[WIENER_WIN2_ALIGN3]);
-  DECLARE_ALIGNED(64, int16_t, DGD_AVG1[WIENER_WIN2_ALIGN3]);
-  DECLARE_ALIGNED(64, int32_t, M_s32[WIENER_WIN2_ALIGN3]);
-  DECLARE_ALIGNED(64, int64_t, M_s64[WIENER_WIN2_ALIGN3]);
-  DECLARE_ALIGNED(64, int32_t, H_s32[WIENER_WIN2 * WIENER_WIN2_ALIGN2]);
-  DECLARE_ALIGNED(64, int64_t, H_s64[WIENER_WIN2 * WIENER_WIN2_ALIGN2]);
-
-  memset(M_s32, 0, sizeof(M_s32));
-  memset(M_s64, 0, sizeof(M_s64));
-  memset(H_s32, 0, sizeof(H_s32));
-  memset(H_s64, 0, sizeof(H_s64));
-
-  // Look-up tables to create 8x6 matrix with consecutive elements from two 7x7
-  // matrices.
-  // clang-format off
-  DECLARE_ALIGNED(16, static const uint8_t, shuffle_stats7_highbd[192]) = {
-    0,  1,  2,  3,  4,  5,  6,  7,  8,  9,  10, 11, 12, 13, 16, 17,
-    2,  3,  4,  5,  6,  7,  8,  9,  10, 11, 12, 13, 16, 17, 18, 19,
-    4,  5,  6,  7,  8,  9,  10, 11, 12, 13, 16, 17, 18, 19, 20, 21,
-    6,  7,  8,  9,  10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 22, 23,
-    8,  9,  10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
-    10, 11, 12, 13, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,
-    2,  3,  4,  5,  6,  7,  8,  9,  10, 11, 12, 13, 14, 15, 18, 19,
-    4,  5,  6,  7,  8,  9,  10, 11, 12, 13, 14, 15, 18, 19, 20, 21,
-    6,  7,  8,  9,  10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23,
-    8,  9,  10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25,
-    10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,
-    12, 13, 14, 15, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,
-  };
-  // clang-format on
-
-  const uint8x16_t lut0 = vld1q_u8(shuffle_stats7_highbd + 0);
-  const uint8x16_t lut1 = vld1q_u8(shuffle_stats7_highbd + 16);
-  const uint8x16_t lut2 = vld1q_u8(shuffle_stats7_highbd + 32);
-  const uint8x16_t lut3 = vld1q_u8(shuffle_stats7_highbd + 48);
-  const uint8x16_t lut4 = vld1q_u8(shuffle_stats7_highbd + 64);
-  const uint8x16_t lut5 = vld1q_u8(shuffle_stats7_highbd + 80);
-  const uint8x16_t lut6 = vld1q_u8(shuffle_stats7_highbd + 96);
-  const uint8x16_t lut7 = vld1q_u8(shuffle_stats7_highbd + 112);
-  const uint8x16_t lut8 = vld1q_u8(shuffle_stats7_highbd + 128);
-  const uint8x16_t lut9 = vld1q_u8(shuffle_stats7_highbd + 144);
-  const uint8x16_t lut10 = vld1q_u8(shuffle_stats7_highbd + 160);
-  const uint8x16_t lut11 = vld1q_u8(shuffle_stats7_highbd + 176);
-
-  // We can accumulate up to 32768/2048/128 8/10/12-bit multiplication results
-  // in a signed 32-bit integer. We are processing 2 pixels at a time, so the
-  // accumulator max can be as high as 16384/1024/64 for the compute stats.
-  const int acc_cnt_max = (1 << (31 - 2 * bit_depth)) >> 1;
-  int acc_cnt = acc_cnt_max;
-  const int src_next = src_stride - width;
-  const int dgd_next = dgd_stride - width;
-  const int16x8_t avg_s16 = vdupq_n_s16(avg);
+    j = i + 1;
+    do {
+      const int16_t *const dj = d + j;
+      int32x4_t deltas[WIENER_WIN_CHROMA - 1][WIENER_WIN_CHROMA - 1] = {
+        { vdupq_n_s32(0) }, { vdupq_n_s32(0) }
+      };
+      int16x8_t d_is[WIN_CHROMA], d_ie[WIN_CHROMA];
+      int16x8_t d_js[WIN_CHROMA], d_je[WIN_CHROMA];
+
+      x = 0;
+      while (x < w16) {
+        load_square_win5_neon(di + x, dj + x, d_stride, height, d_is, d_ie,
+                              d_js, d_je);
+        derive_square_win5_neon(d_is, d_ie, d_js, d_je, deltas);
+        x += 16;
+      }
 
-  do {
-    int j = width;
-    while (j >= 2) {
-      // Load two adjacent, overlapping 7x7 matrices: a 8x7 matrix with the
-      // middle 6x7 elements being shared.
-      int16x8_t dgd_rows[7];
-      load_and_pack_s16_8x7(dgd_rows, (const int16_t *)dgd, dgd_stride);
-
-      const int16_t *dgd_ptr = (const int16_t *)dgd + dgd_stride * 6;
-      dgd += 2;
-
-      dgd_rows[0] = vsubq_s16(dgd_rows[0], avg_s16);
-      dgd_rows[1] = vsubq_s16(dgd_rows[1], avg_s16);
-      dgd_rows[2] = vsubq_s16(dgd_rows[2], avg_s16);
-      dgd_rows[3] = vsubq_s16(dgd_rows[3], avg_s16);
-      dgd_rows[4] = vsubq_s16(dgd_rows[4], avg_s16);
-      dgd_rows[5] = vsubq_s16(dgd_rows[5], avg_s16);
-      dgd_rows[6] = vsubq_s16(dgd_rows[6], avg_s16);
-
-      // Re-arrange the combined 8x7 matrix to have the 2 whole 7x7 matrices (1
-      // for each of the 2 pixels) separated into distinct int16x8_t[6] arrays.
-      // These arrays contain 48 elements of the 49 (7x7). Compute `dgd - avg`
-      // for both buffers. Each DGD_AVG buffer contains 49 consecutive elements.
-      int16x8_t dgd_avg0[6];
-      int16x8_t dgd_avg1[6];
-
-      dgd_avg0[0] = tbl2q(dgd_rows[0], dgd_rows[1], lut0);
-      dgd_avg1[0] = tbl2q(dgd_rows[0], dgd_rows[1], lut6);
-      dgd_avg0[1] = tbl2q(dgd_rows[1], dgd_rows[2], lut1);
-      dgd_avg1[1] = tbl2q(dgd_rows[1], dgd_rows[2], lut7);
-      dgd_avg0[2] = tbl2q(dgd_rows[2], dgd_rows[3], lut2);
-      dgd_avg1[2] = tbl2q(dgd_rows[2], dgd_rows[3], lut8);
-      dgd_avg0[3] = tbl2q(dgd_rows[3], dgd_rows[4], lut3);
-      dgd_avg1[3] = tbl2q(dgd_rows[3], dgd_rows[4], lut9);
-      dgd_avg0[4] = tbl2q(dgd_rows[4], dgd_rows[5], lut4);
-      dgd_avg1[4] = tbl2q(dgd_rows[4], dgd_rows[5], lut10);
-      dgd_avg0[5] = tbl2q(dgd_rows[5], dgd_rows[6], lut5);
-      dgd_avg1[5] = tbl2q(dgd_rows[5], dgd_rows[6], lut11);
-
-      vst1q_s16(DGD_AVG0, dgd_avg0[0]);
-      vst1q_s16(DGD_AVG1, dgd_avg1[0]);
-      vst1q_s16(DGD_AVG0 + 8, dgd_avg0[1]);
-      vst1q_s16(DGD_AVG1 + 8, dgd_avg1[1]);
-      vst1q_s16(DGD_AVG0 + 16, dgd_avg0[2]);
-      vst1q_s16(DGD_AVG1 + 16, dgd_avg1[2]);
-      vst1q_s16(DGD_AVG0 + 24, dgd_avg0[3]);
-      vst1q_s16(DGD_AVG1 + 24, dgd_avg1[3]);
-      vst1q_s16(DGD_AVG0 + 32, dgd_avg0[4]);
-      vst1q_s16(DGD_AVG1 + 32, dgd_avg1[4]);
-      vst1q_s16(DGD_AVG0 + 40, dgd_avg0[5]);
-      vst1q_s16(DGD_AVG1 + 40, dgd_avg1[5]);
-
-      // The remaining last (49th) elements of `dgd - avg`.
-      DGD_AVG0[48] = dgd_ptr[6] - avg;
-      DGD_AVG1[48] = dgd_ptr[7] - avg;
-
-      // Accumulate into row-major variant of matrix M (cross-correlation) for 2
-      // output pixels at a time. M is of size 7 * 7. It needs to be filled such
-      // that multiplying one element from src with each element of a row of the
-      // wiener window will fill one column of M. However this is not very
-      // convenient in terms of memory access, as it means we do contiguous
-      // loads of dgd but strided stores to M. As a result, we use an
-      // intermediate matrix M_s32 which is instead filled such that one row of
-      // the wiener window gives one row of M_s32. Once fully computed, M_s32 is
-      // then transposed to return M.
-      int src_avg0 = *src++ - avg;
-      int src_avg1 = *src++ - avg;
-      int16x4_t src_avg0_s16 = vdup_n_s16(src_avg0);
-      int16x4_t src_avg1_s16 = vdup_n_s16(src_avg1);
-      update_M_2pixels(M_s32 + 0, src_avg0_s16, src_avg1_s16, dgd_avg0[0],
-                       dgd_avg1[0]);
-      update_M_2pixels(M_s32 + 8, src_avg0_s16, src_avg1_s16, dgd_avg0[1],
-                       dgd_avg1[1]);
-      update_M_2pixels(M_s32 + 16, src_avg0_s16, src_avg1_s16, dgd_avg0[2],
-                       dgd_avg1[2]);
-      update_M_2pixels(M_s32 + 24, src_avg0_s16, src_avg1_s16, dgd_avg0[3],
-                       dgd_avg1[3]);
-      update_M_2pixels(M_s32 + 32, src_avg0_s16, src_avg1_s16, dgd_avg0[4],
-                       dgd_avg1[4]);
-      update_M_2pixels(M_s32 + 40, src_avg0_s16, src_avg1_s16, dgd_avg0[5],
-                       dgd_avg1[5]);
-
-      // Last (49th) element of M_s32 can be computed as scalar more efficiently
-      // for 2 output pixels.
-      M_s32[48] += DGD_AVG0[48] * src_avg0 + DGD_AVG1[48] * src_avg1;
-
-      // Start accumulating into row-major version of matrix H
-      // (auto-covariance), it expects the DGD_AVG[01] matrices to also be
-      // row-major. H is of size 49 * 49. It is filled by multiplying every pair
-      // of elements of the wiener window together (vector outer product). Since
-      // it is a symmetric matrix, we only compute the upper-right triangle, and
-      // then copy it down to the lower-left later. The upper triangle is
-      // covered by 4x4 tiles. The original algorithm assumes the M matrix is
-      // column-major and the resulting H matrix is also expected to be
-      // column-major. It is not efficient to work with column-major matrices,
-      // so we accumulate into a row-major matrix H_s32. At the end of the
-      // algorithm a double transpose transformation will convert H_s32 back to
-      // the expected output layout.
-      update_H_7x7_2pixels(H_s32, DGD_AVG0, DGD_AVG1);
-
-      // The last element of the triangle of H_s32 matrix can be computed as a
-      // scalar more efficiently.
-      H_s32[48 * WIENER_WIN2_ALIGN2 + 48] +=
-          DGD_AVG0[48] * DGD_AVG0[48] + DGD_AVG1[48] * DGD_AVG1[48];
-
-      // Accumulate into 64-bit after a bit depth dependent number of iterations
-      // to prevent overflow.
-      if (--acc_cnt == 0) {
-        acc_cnt = acc_cnt_max;
-
-        accumulate_and_clear(M_s64, M_s32, WIENER_WIN2_ALIGN2);
-
-        // The widening accumulation is only needed for the upper triangle part
-        // of the matrix.
-        int64_t *lh = H_s64;
-        int32_t *lh32 = H_s32;
-        for (int k = 0; k < WIENER_WIN2; ++k) {
-          // The widening accumulation is only run for the relevant parts
-          // (upper-right triangle) in a row 4-element aligned.
-          int k4 = k / 4 * 4;
-          accumulate_and_clear(lh + k4, lh32 + k4, 48 - k4);
-
-          // Last element of the row is computed separately.
-          lh[48] += lh32[48];
-          lh32[48] = 0;
-
-          lh += WIENER_WIN2_ALIGN2;
-          lh32 += WIENER_WIN2_ALIGN2;
-        }
+      if (w16 != width) {
+        load_square_win5_neon(di + x, dj + x, d_stride, height, d_is, d_ie,
+                              d_js, d_je);
+        d_is[0] = vandq_s16(d_is[0], mask[0]);
+        d_is[1] = vandq_s16(d_is[1], mask[1]);
+        d_is[2] = vandq_s16(d_is[2], mask[0]);
+        d_is[3] = vandq_s16(d_is[3], mask[1]);
+        d_is[4] = vandq_s16(d_is[4], mask[0]);
+        d_is[5] = vandq_s16(d_is[5], mask[1]);
+        d_is[6] = vandq_s16(d_is[6], mask[0]);
+        d_is[7] = vandq_s16(d_is[7], mask[1]);
+        d_ie[0] = vandq_s16(d_ie[0], mask[0]);
+        d_ie[1] = vandq_s16(d_ie[1], mask[1]);
+        d_ie[2] = vandq_s16(d_ie[2], mask[0]);
+        d_ie[3] = vandq_s16(d_ie[3], mask[1]);
+        d_ie[4] = vandq_s16(d_ie[4], mask[0]);
+        d_ie[5] = vandq_s16(d_ie[5], mask[1]);
+        d_ie[6] = vandq_s16(d_ie[6], mask[0]);
+        d_ie[7] = vandq_s16(d_ie[7], mask[1]);
+        derive_square_win5_neon(d_is, d_ie, d_js, d_je, deltas);
       }
 
-      j -= 2;
+      hadd_update_4_stats_neon(
+          H + (i * wiener_win + 0) * wiener_win2 + j * wiener_win, deltas[0],
+          H + (i * wiener_win + 1) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_4_stats_neon(
+          H + (i * wiener_win + 1) * wiener_win2 + j * wiener_win, deltas[1],
+          H + (i * wiener_win + 2) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_4_stats_neon(
+          H + (i * wiener_win + 2) * wiener_win2 + j * wiener_win, deltas[2],
+          H + (i * wiener_win + 3) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_4_stats_neon(
+          H + (i * wiener_win + 3) * wiener_win2 + j * wiener_win, deltas[3],
+          H + (i * wiener_win + 4) * wiener_win2 + j * wiener_win + 1);
+    } while (++j < wiener_win);
+  } while (++i < wiener_win - 1);
+
+  // Step 6: Derive other points of each upper triangle along the diagonal.
+  i = 0;
+  do {
+    const int16_t *const di = d + i;
+    int32x4_t deltas[WIENER_WIN_CHROMA * 2 + 1] = { vdupq_n_s32(0) };
+    int16x8_t d_is[WIN_CHROMA], d_ie[WIN_CHROMA];
+
+    x = 0;
+    while (x < w16) {
+      load_triangle_win5_neon(di + x, d_stride, height, d_is, d_ie);
+      derive_triangle_win5_neon(d_is, d_ie, deltas);
+      x += 16;
     }
 
-    // Computations for odd pixel in the row.
-    if (width & 1) {
-      // Load two adjacent, overlapping 7x7 matrices: a 8x7 matrix with the
-      // middle 6x7 elements being shared.
-      int16x8_t dgd_rows[7];
-      load_and_pack_s16_8x7(dgd_rows, (const int16_t *)dgd, dgd_stride);
-
-      const int16_t *dgd_ptr = (const int16_t *)dgd + dgd_stride * 6;
-      ++dgd;
-
-      // Re-arrange the combined 8x7 matrix to have a whole 7x7 matrix tightly
-      // packed into a int16x8_t[6] array. This array contains 48 elements of
-      // the 49 (7x7). Compute `dgd - avg` for the whole buffer. The DGD_AVG
-      // buffer contains 49 consecutive elements.
-      int16x8_t dgd_avg0[6];
-
-      dgd_avg0[0] = vsubq_s16(tbl2q(dgd_rows[0], dgd_rows[1], lut0), avg_s16);
-      dgd_avg0[1] = vsubq_s16(tbl2q(dgd_rows[1], dgd_rows[2], lut1), avg_s16);
-      dgd_avg0[2] = vsubq_s16(tbl2q(dgd_rows[2], dgd_rows[3], lut2), avg_s16);
-      dgd_avg0[3] = vsubq_s16(tbl2q(dgd_rows[3], dgd_rows[4], lut3), avg_s16);
-      dgd_avg0[4] = vsubq_s16(tbl2q(dgd_rows[4], dgd_rows[5], lut4), avg_s16);
-      dgd_avg0[5] = vsubq_s16(tbl2q(dgd_rows[5], dgd_rows[6], lut5), avg_s16);
-
-      vst1q_s16(DGD_AVG0, dgd_avg0[0]);
-      vst1q_s16(DGD_AVG0 + 8, dgd_avg0[1]);
-      vst1q_s16(DGD_AVG0 + 16, dgd_avg0[2]);
-      vst1q_s16(DGD_AVG0 + 24, dgd_avg0[3]);
-      vst1q_s16(DGD_AVG0 + 32, dgd_avg0[4]);
-      vst1q_s16(DGD_AVG0 + 40, dgd_avg0[5]);
-
-      // The remaining last (49th) element of `dgd - avg`.
-      DGD_AVG0[48] = dgd_ptr[6] - avg;
-
-      // Accumulate into row-major order variant of matrix M (cross-correlation)
-      // for 1 output pixel at a time. M is of size 7 * 7. It needs to be filled
-      // such that multiplying one element from src with each element of a row
-      // of the wiener window will fill one column of M. However this is not
-      // very convenient in terms of memory access, as it means we do
-      // contiguous loads of dgd but strided stores to M. As a result, we use an
-      // intermediate matrix M_s32 which is instead filled such that one row of
-      // the wiener window gives one row of M_s32. Once fully computed, M_s32 is
-      // then transposed to return M.
-      int src_avg0 = *src++ - avg;
-      int16x4_t src_avg0_s16 = vdup_n_s16(src_avg0);
-      update_M_1pixel(M_s32 + 0, src_avg0_s16, dgd_avg0[0]);
-      update_M_1pixel(M_s32 + 8, src_avg0_s16, dgd_avg0[1]);
-      update_M_1pixel(M_s32 + 16, src_avg0_s16, dgd_avg0[2]);
-      update_M_1pixel(M_s32 + 24, src_avg0_s16, dgd_avg0[3]);
-      update_M_1pixel(M_s32 + 32, src_avg0_s16, dgd_avg0[4]);
-      update_M_1pixel(M_s32 + 40, src_avg0_s16, dgd_avg0[5]);
-
-      // Last (49th) element of M_s32 can be computed as scalar more efficiently
-      // for 1 output pixel.
-      M_s32[48] += DGD_AVG0[48] * src_avg0;
-
-      // Start accumulating into row-major order version of matrix H
-      // (auto-covariance), it expects the DGD_AVG0 matrix to also be row-major.
-      // H is of size 49 * 49. It is filled by multiplying every pair of
-      // elements of the wiener window together (vector outer product). Since it
-      // is a symmetric matrix, we only compute the upper-right triangle, and
-      // then copy it down to the lower-left later. The upper triangle is
-      // covered by 4x4 tiles. The original algorithm assumes the M matrix is
-      // column-major and the resulting H matrix is also expected to be
-      // column-major. It is not efficient to work column-major matrices, so we
-      // accumulate into a row-major matrix H_s32. At the end of the algorithm a
-      // double transpose transformation will convert H_s32 back to the expected
-      // output layout.
-      update_H_1pixel(H_s32, DGD_AVG0, WIENER_WIN2_ALIGN2, 48);
-
-      // The last element of the triangle of H_s32 matrix can be computed as
-      // scalar more efficiently.
-      H_s32[48 * WIENER_WIN2_ALIGN2 + 48] += DGD_AVG0[48] * DGD_AVG0[48];
+    if (w16 != width) {
+      load_triangle_win5_neon(di + x, d_stride, height, d_is, d_ie);
+      d_is[0] = vandq_s16(d_is[0], mask[0]);
+      d_is[1] = vandq_s16(d_is[1], mask[1]);
+      d_is[2] = vandq_s16(d_is[2], mask[0]);
+      d_is[3] = vandq_s16(d_is[3], mask[1]);
+      d_is[4] = vandq_s16(d_is[4], mask[0]);
+      d_is[5] = vandq_s16(d_is[5], mask[1]);
+      d_is[6] = vandq_s16(d_is[6], mask[0]);
+      d_is[7] = vandq_s16(d_is[7], mask[1]);
+      d_ie[0] = vandq_s16(d_ie[0], mask[0]);
+      d_ie[1] = vandq_s16(d_ie[1], mask[1]);
+      d_ie[2] = vandq_s16(d_ie[2], mask[0]);
+      d_ie[3] = vandq_s16(d_ie[3], mask[1]);
+      d_ie[4] = vandq_s16(d_ie[4], mask[0]);
+      d_ie[5] = vandq_s16(d_ie[5], mask[1]);
+      d_ie[6] = vandq_s16(d_ie[6], mask[0]);
+      d_ie[7] = vandq_s16(d_ie[7], mask[1]);
+      derive_triangle_win5_neon(d_is, d_ie, deltas);
     }
 
-    src += src_next;
-    dgd += dgd_next;
-  } while (--height != 0);
+    // Row 1: 4 points
+    hadd_update_4_stats_neon(
+        H + (i * wiener_win + 0) * wiener_win2 + i * wiener_win, deltas,
+        H + (i * wiener_win + 1) * wiener_win2 + i * wiener_win + 1);
 
-  int bit_depth_shift = bit_depth - AOM_BITS_8;
+    // Row 2: 3 points
+    int64x2_t delta4_s64 = vpaddlq_s32(deltas[4]);
+    int64x2_t delta5_s64 = vpaddlq_s32(deltas[5]);
 
-  acc_transpose_M(M, M_s64, M_s32, WIENER_WIN, bit_depth_shift);
+#if AOM_ARCH_AARCH64
+    int64x2_t deltas45 = vpaddq_s64(delta4_s64, delta5_s64);
+    int64x2_t src =
+        vld1q_s64(H + (i * wiener_win + 1) * wiener_win2 + i * wiener_win + 1);
+    int64x2_t dst = vaddq_s64(src, deltas45);
+    vst1q_s64(H + (i * wiener_win + 2) * wiener_win2 + i * wiener_win + 2, dst);
+#else
+    H[(i * wiener_win + 2) * wiener_win2 + i * wiener_win + 2 + 0] =
+        H[(i * wiener_win + 1) * wiener_win2 + i * wiener_win + 1 + 0] +
+        horizontal_add_s64x2(delta4_s64);
+    H[(i * wiener_win + 2) * wiener_win2 + i * wiener_win + 2 + 1] =
+        H[(i * wiener_win + 1) * wiener_win2 + i * wiener_win + 1 + 1] +
+        horizontal_add_s64x2(delta5_s64);
+#endif  // AOM_ARCH_AARCH64
+
+    H[(i * wiener_win + 2) * wiener_win2 + i * wiener_win + 4] =
+        H[(i * wiener_win + 1) * wiener_win2 + i * wiener_win + 3] +
+        horizontal_long_add_s32x4(deltas[6]);
+
+    // Row 3: 2 points
+    int64x2_t delta7_s64 = vpaddlq_s32(deltas[7]);
+    int64x2_t delta8_s64 = vpaddlq_s32(deltas[8]);
 
-  update_H(H, H_s64, H_s32, WIENER_WIN, WIENER_WIN2_ALIGN2, bit_depth_shift);
+#if AOM_ARCH_AARCH64
+    int64x2_t deltas78 = vpaddq_s64(delta7_s64, delta8_s64);
+    vst1q_s64(H + (i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3,
+              vaddq_s64(dst, deltas78));
+#else
+    H[(i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3 + 0] =
+        H[(i * wiener_win + 2) * wiener_win2 + i * wiener_win + 2 + 0] +
+        horizontal_add_s64x2(delta7_s64);
+    H[(i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3 + 1] =
+        H[(i * wiener_win + 2) * wiener_win2 + i * wiener_win + 2 + 1] +
+        horizontal_add_s64x2(delta8_s64);
+#endif  // AOM_ARCH_AARCH64
+
+    // Row 4: 1 point
+    H[(i * wiener_win + 4) * wiener_win2 + i * wiener_win + 4] =
+        H[(i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3] +
+        horizontal_long_add_s32x4(deltas[9]);
+  } while (++i < wiener_win);
 }
 
-// Load 5x5 matrix into 5 128-bit vectors from consecutive rows, the last load
-// address is offset to prevent out-of-bounds access.
-static inline void load_and_pack_s16_6x5(int16x8_t dst[5], const int16_t *src,
-                                         ptrdiff_t stride) {
-  dst[0] = vld1q_s16(src);
-  src += stride;
-  dst[1] = vld1q_s16(src);
-  src += stride;
-  dst[2] = vld1q_s16(src);
-  src += stride;
-  dst[3] = vld1q_s16(src);
-  src += stride;
-  dst[4] = vld1q_s16(src - 3);
+static inline void hadd_update_6_stats_neon(const int64_t *const src,
+                                            const int32x4_t *deltas,
+                                            int64_t *const dst) {
+  int64x2_t delta0_s64 = vpaddlq_s32(deltas[0]);
+  int64x2_t delta1_s64 = vpaddlq_s32(deltas[1]);
+  int64x2_t delta2_s64 = vpaddlq_s32(deltas[2]);
+  int64x2_t delta3_s64 = vpaddlq_s32(deltas[3]);
+  int64x2_t delta4_s64 = vpaddlq_s32(deltas[4]);
+  int64x2_t delta5_s64 = vpaddlq_s32(deltas[5]);
+
+#if AOM_ARCH_AARCH64
+  int64x2_t delta01 = vpaddq_s64(delta0_s64, delta1_s64);
+  int64x2_t delta23 = vpaddq_s64(delta2_s64, delta3_s64);
+  int64x2_t delta45 = vpaddq_s64(delta4_s64, delta5_s64);
+
+  int64x2_t src0 = vld1q_s64(src);
+  int64x2_t src1 = vld1q_s64(src + 2);
+  int64x2_t src2 = vld1q_s64(src + 4);
+
+  vst1q_s64(dst, vaddq_s64(src0, delta01));
+  vst1q_s64(dst + 2, vaddq_s64(src1, delta23));
+  vst1q_s64(dst + 4, vaddq_s64(src2, delta45));
+#else
+  dst[0] = src[0] + horizontal_add_s64x2(delta0_s64);
+  dst[1] = src[1] + horizontal_add_s64x2(delta1_s64);
+  dst[2] = src[2] + horizontal_add_s64x2(delta2_s64);
+  dst[3] = src[3] + horizontal_add_s64x2(delta3_s64);
+  dst[4] = src[4] + horizontal_add_s64x2(delta4_s64);
+  dst[5] = src[5] + horizontal_add_s64x2(delta5_s64);
+#endif
 }
 
-static void highbd_compute_stats_win5_neon(const uint16_t *dgd,
-                                           const uint16_t *src, int avg,
-                                           int width, int height,
-                                           int dgd_stride, int src_stride,
-                                           int64_t *M, int64_t *H,
-                                           aom_bit_depth_t bit_depth) {
-  // Matrix names are capitalized to help readability.
-  DECLARE_ALIGNED(64, int16_t, DGD_AVG0[WIENER_WIN2_REDUCED_ALIGN3]);
-  DECLARE_ALIGNED(64, int16_t, DGD_AVG1[WIENER_WIN2_REDUCED_ALIGN3]);
-  DECLARE_ALIGNED(64, int32_t, M_s32[WIENER_WIN2_REDUCED_ALIGN3]);
-  DECLARE_ALIGNED(64, int64_t, M_s64[WIENER_WIN2_REDUCED_ALIGN3]);
-  DECLARE_ALIGNED(64, int32_t,
-                  H_s32[WIENER_WIN2_REDUCED * WIENER_WIN2_REDUCED_ALIGN2]);
-  DECLARE_ALIGNED(64, int64_t,
-                  H_s64[WIENER_WIN2_REDUCED * WIENER_WIN2_REDUCED_ALIGN2]);
-
-  memset(M_s32, 0, sizeof(M_s32));
-  memset(M_s64, 0, sizeof(M_s64));
-  memset(H_s32, 0, sizeof(H_s32));
-  memset(H_s64, 0, sizeof(H_s64));
-
-  // Look-up tables to create 8x3 matrix with consecutive elements from 5x5
-  // matrix.
-  DECLARE_ALIGNED(16, static const uint8_t, shuffle_stats5_highbd[96]) = {
-    0, 1, 2,  3,  4,  5,  6,  7,  8,  9,  16, 17, 18, 19, 20, 21,
-    6, 7, 8,  9,  16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 32, 33,
-    2, 3, 4,  5,  6,  7,  8,  9,  22, 23, 24, 25, 26, 27, 28, 29,
-    2, 3, 4,  5,  6,  7,  8,  9,  10, 11, 18, 19, 20, 21, 22, 23,
-    8, 9, 10, 11, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 34, 35,
-    4, 5, 6,  7,  8,  9,  10, 11, 24, 25, 26, 27, 28, 29, 30, 31,
-  };
-
-  const uint8x16_t lut0 = vld1q_u8(shuffle_stats5_highbd + 0);
-  const uint8x16_t lut1 = vld1q_u8(shuffle_stats5_highbd + 16);
-  const uint8x16_t lut2 = vld1q_u8(shuffle_stats5_highbd + 32);
-  const uint8x16_t lut3 = vld1q_u8(shuffle_stats5_highbd + 48);
-  const uint8x16_t lut4 = vld1q_u8(shuffle_stats5_highbd + 64);
-  const uint8x16_t lut5 = vld1q_u8(shuffle_stats5_highbd + 80);
-
-  // We can accumulate up to 32768/2048/128 8/10/12-bit multiplication results
-  // in a signed 32-bit integer. We are processing 2 pixels at a time, so the
-  // accumulator max can be as high as 16384/1024/64 for the compute stats.
-  const int acc_cnt_max = (1 << (31 - 2 * bit_depth)) >> 1;
-  int acc_cnt = acc_cnt_max;
-  const int src_next = src_stride - width;
-  const int dgd_next = dgd_stride - width;
-  const int16x8_t avg_s16 = vdupq_n_s16(avg);
+static inline void compute_stats_win7_highbd_neon(
+    const int16_t *const d, const int32_t d_stride, const int16_t *const s,
+    const int32_t s_stride, const int32_t width, const int32_t height,
+    int64_t *const M, int64_t *const H, aom_bit_depth_t bit_depth) {
+  const int32_t wiener_win = WIENER_WIN;
+  const int32_t wiener_win2 = wiener_win * wiener_win;
+  const int32_t w16 = width & ~15;
+  const int32_t h8 = height & ~7;
+  int16x8_t mask[2];
+  mask[0] = vld1q_s16(&(mask_16bit[16]) - width % 16);
+  mask[1] = vld1q_s16(&(mask_16bit[16]) - width % 16 + 8);
+  int32_t i, j, x, y;
+
+  const int32_t num_bit_left =
+      32 - 1 /* sign */ - 2 * bit_depth /* energy */ + 2 /* SIMD */;
+  const int32_t h_allowed =
+      (1 << num_bit_left) / (w16 + ((w16 != width) ? 16 : 0));
+
+  // Step 1: Calculate the top edge of the whole matrix, i.e., the top
+  // edge of each triangle and square on the top row.
+  j = 0;
+  do {
+    const int16_t *s_t = s;
+    const int16_t *d_t = d;
+    int32_t height_t = 0;
+    int64x2_t sum_m[WIENER_WIN] = { vdupq_n_s64(0) };
+    int64x2_t sum_h[WIENER_WIN] = { vdupq_n_s64(0) };
+    int16x8_t src[2], dgd[2];
+
+    do {
+      const int32_t h_t =
+          ((height - height_t) < h_allowed) ? (height - height_t) : h_allowed;
+      int32x4_t row_m[WIENER_WIN * 2] = { vdupq_n_s32(0) };
+      int32x4_t row_h[WIENER_WIN * 2] = { vdupq_n_s32(0) };
+
+      y = h_t;
+      do {
+        x = 0;
+        while (x < w16) {
+          src[0] = vld1q_s16(s_t + x);
+          src[1] = vld1q_s16(s_t + x + 8);
+          dgd[0] = vld1q_s16(d_t + x);
+          dgd[1] = vld1q_s16(d_t + x + 8);
+          stats_top_win7_neon(src, dgd, d_t + j + x, d_stride, row_m, row_h);
+          x += 16;
+        }
+
+        if (w16 != width) {
+          src[0] = vld1q_s16(s_t + w16);
+          src[1] = vld1q_s16(s_t + w16 + 8);
+          dgd[0] = vld1q_s16(d_t + w16);
+          dgd[1] = vld1q_s16(d_t + w16 + 8);
+          src[0] = vandq_s16(src[0], mask[0]);
+          src[1] = vandq_s16(src[1], mask[1]);
+          dgd[0] = vandq_s16(dgd[0], mask[0]);
+          dgd[1] = vandq_s16(dgd[1], mask[1]);
+          stats_top_win7_neon(src, dgd, d_t + j + w16, d_stride, row_m, row_h);
+        }
 
+        s_t += s_stride;
+        d_t += d_stride;
+      } while (--y);
+
+      sum_m[0] = vpadalq_s32(sum_m[0], row_m[0]);
+      sum_m[1] = vpadalq_s32(sum_m[1], row_m[1]);
+      sum_m[2] = vpadalq_s32(sum_m[2], row_m[2]);
+      sum_m[3] = vpadalq_s32(sum_m[3], row_m[3]);
+      sum_m[4] = vpadalq_s32(sum_m[4], row_m[4]);
+      sum_m[5] = vpadalq_s32(sum_m[5], row_m[5]);
+      sum_m[6] = vpadalq_s32(sum_m[6], row_m[6]);
+
+      sum_h[0] = vpadalq_s32(sum_h[0], row_h[0]);
+      sum_h[1] = vpadalq_s32(sum_h[1], row_h[1]);
+      sum_h[2] = vpadalq_s32(sum_h[2], row_h[2]);
+      sum_h[3] = vpadalq_s32(sum_h[3], row_h[3]);
+      sum_h[4] = vpadalq_s32(sum_h[4], row_h[4]);
+      sum_h[5] = vpadalq_s32(sum_h[5], row_h[5]);
+      sum_h[6] = vpadalq_s32(sum_h[6], row_h[6]);
+
+      height_t += h_t;
+    } while (height_t < height);
+
+#if AOM_ARCH_AARCH64
+    vst1q_s64(M + wiener_win * j + 0, vpaddq_s64(sum_m[0], sum_m[1]));
+    vst1q_s64(M + wiener_win * j + 2, vpaddq_s64(sum_m[2], sum_m[3]));
+    vst1q_s64(M + wiener_win * j + 4, vpaddq_s64(sum_m[4], sum_m[5]));
+    M[wiener_win * j + 6] = vaddvq_s64(sum_m[6]);
+
+    vst1q_s64(H + wiener_win * j + 0, vpaddq_s64(sum_h[0], sum_h[1]));
+    vst1q_s64(H + wiener_win * j + 2, vpaddq_s64(sum_h[2], sum_h[3]));
+    vst1q_s64(H + wiener_win * j + 4, vpaddq_s64(sum_h[4], sum_h[5]));
+    H[wiener_win * j + 6] = vaddvq_s64(sum_h[6]);
+#else
+    M[wiener_win * j + 0] = horizontal_add_s64x2(sum_m[0]);
+    M[wiener_win * j + 1] = horizontal_add_s64x2(sum_m[1]);
+    M[wiener_win * j + 2] = horizontal_add_s64x2(sum_m[2]);
+    M[wiener_win * j + 3] = horizontal_add_s64x2(sum_m[3]);
+    M[wiener_win * j + 4] = horizontal_add_s64x2(sum_m[4]);
+    M[wiener_win * j + 5] = horizontal_add_s64x2(sum_m[5]);
+    M[wiener_win * j + 6] = horizontal_add_s64x2(sum_m[6]);
+
+    H[wiener_win * j + 0] = horizontal_add_s64x2(sum_h[0]);
+    H[wiener_win * j + 1] = horizontal_add_s64x2(sum_h[1]);
+    H[wiener_win * j + 2] = horizontal_add_s64x2(sum_h[2]);
+    H[wiener_win * j + 3] = horizontal_add_s64x2(sum_h[3]);
+    H[wiener_win * j + 4] = horizontal_add_s64x2(sum_h[4]);
+    H[wiener_win * j + 5] = horizontal_add_s64x2(sum_h[5]);
+    H[wiener_win * j + 6] = horizontal_add_s64x2(sum_h[6]);
+#endif  // AOM_ARCH_AARCH64
+  } while (++j < wiener_win);
+
+  // Step 2: Calculate the left edge of each square on the top row.
+  j = 1;
   do {
-    int j = width;
-    while (j >= 2) {
-      // Load two adjacent, overlapping 5x5 matrices: a 6x5 matrix with the
-      // middle 4x5 elements being shared.
-      int16x8_t dgd_rows[5];
-      load_and_pack_s16_6x5(dgd_rows, (const int16_t *)dgd, dgd_stride);
-
-      const int16_t *dgd_ptr = (const int16_t *)dgd + dgd_stride * 4;
-      dgd += 2;
-
-      dgd_rows[0] = vsubq_s16(dgd_rows[0], avg_s16);
-      dgd_rows[1] = vsubq_s16(dgd_rows[1], avg_s16);
-      dgd_rows[2] = vsubq_s16(dgd_rows[2], avg_s16);
-      dgd_rows[3] = vsubq_s16(dgd_rows[3], avg_s16);
-      dgd_rows[4] = vsubq_s16(dgd_rows[4], avg_s16);
-
-      // Re-arrange the combined 6x5 matrix to have the 2 whole 5x5 matrices (1
-      // for each of the 2 pixels) separated into distinct int16x8_t[3] arrays.
-      // These arrays contain 24 elements of the 25 (5x5). Compute `dgd - avg`
-      // for both buffers. Each DGD_AVG buffer contains 25 consecutive elements.
-      int16x8_t dgd_avg0[3];
-      int16x8_t dgd_avg1[3];
-
-      dgd_avg0[0] = tbl2q(dgd_rows[0], dgd_rows[1], lut0);
-      dgd_avg1[0] = tbl2q(dgd_rows[0], dgd_rows[1], lut3);
-      dgd_avg0[1] = tbl3q(dgd_rows[1], dgd_rows[2], dgd_rows[3], lut1);
-      dgd_avg1[1] = tbl3q(dgd_rows[1], dgd_rows[2], dgd_rows[3], lut4);
-      dgd_avg0[2] = tbl2q(dgd_rows[3], dgd_rows[4], lut2);
-      dgd_avg1[2] = tbl2q(dgd_rows[3], dgd_rows[4], lut5);
-
-      vst1q_s16(DGD_AVG0, dgd_avg0[0]);
-      vst1q_s16(DGD_AVG1, dgd_avg1[0]);
-      vst1q_s16(DGD_AVG0 + 8, dgd_avg0[1]);
-      vst1q_s16(DGD_AVG1 + 8, dgd_avg1[1]);
-      vst1q_s16(DGD_AVG0 + 16, dgd_avg0[2]);
-      vst1q_s16(DGD_AVG1 + 16, dgd_avg1[2]);
-
-      // The remaining last (25th) elements of `dgd - avg`.
-      DGD_AVG0[24] = dgd_ptr[4] - avg;
-      DGD_AVG1[24] = dgd_ptr[5] - avg;
-
-      // Accumulate into row-major variant of matrix M (cross-correlation) for 2
-      // output pixels at a time. M is of size 5 * 5. It needs to be filled such
-      // that multiplying one element from src with each element of a row of the
-      // wiener window will fill one column of M. However this is not very
-      // convenient in terms of memory access, as it means we do contiguous
-      // loads of dgd but strided stores to M. As a result, we use an
-      // intermediate matrix M_s32 which is instead filled such that one row of
-      // the wiener window gives one row of M_s32. Once fully computed, M_s32 is
-      // then transposed to return M.
-      int src_avg0 = *src++ - avg;
-      int src_avg1 = *src++ - avg;
-      int16x4_t src_avg0_s16 = vdup_n_s16(src_avg0);
-      int16x4_t src_avg1_s16 = vdup_n_s16(src_avg1);
-      update_M_2pixels(M_s32 + 0, src_avg0_s16, src_avg1_s16, dgd_avg0[0],
-                       dgd_avg1[0]);
-      update_M_2pixels(M_s32 + 8, src_avg0_s16, src_avg1_s16, dgd_avg0[1],
-                       dgd_avg1[1]);
-      update_M_2pixels(M_s32 + 16, src_avg0_s16, src_avg1_s16, dgd_avg0[2],
-                       dgd_avg1[2]);
-
-      // Last (25th) element of M_s32 can be computed as scalar more efficiently
-      // for 2 output pixels.
-      M_s32[24] += DGD_AVG0[24] * src_avg0 + DGD_AVG1[24] * src_avg1;
-
-      // Start accumulating into row-major version of matrix H
-      // (auto-covariance), it expects the DGD_AVG[01] matrices to also be
-      // row-major. H is of size 25 * 25. It is filled by multiplying every pair
-      // of elements of the wiener window together (vector outer product). Since
-      // it is a symmetric matrix, we only compute the upper-right triangle, and
-      // then copy it down to the lower-left later. The upper triangle is
-      // covered by 4x4 tiles. The original algorithm assumes the M matrix is
-      // column-major and the resulting H matrix is also expected to be
-      // column-major. It is not efficient to work with column-major matrices,
-      // so we accumulate into a row-major matrix H_s32. At the end of the
-      // algorithm a double transpose transformation will convert H_s32 back to
-      // the expected output layout.
-      update_H_5x5_2pixels(H_s32, DGD_AVG0, DGD_AVG1);
-
-      // The last element of the triangle of H_s32 matrix can be computed as a
-      // scalar more efficiently.
-      H_s32[24 * WIENER_WIN2_REDUCED_ALIGN2 + 24] +=
-          DGD_AVG0[24] * DGD_AVG0[24] + DGD_AVG1[24] * DGD_AVG1[24];
-
-      // Accumulate into 64-bit after a bit depth dependent number of iterations
-      // to prevent overflow.
-      if (--acc_cnt == 0) {
-        acc_cnt = acc_cnt_max;
-
-        accumulate_and_clear(M_s64, M_s32, WIENER_WIN2_REDUCED_ALIGN2);
-
-        // The widening accumulation is only needed for the upper triangle part
-        // of the matrix.
-        int64_t *lh = H_s64;
-        int32_t *lh32 = H_s32;
-        for (int k = 0; k < WIENER_WIN2_REDUCED; ++k) {
-          // The widening accumulation is only run for the relevant parts
-          // (upper-right triangle) in a row 4-element aligned.
-          int k4 = k / 4 * 4;
-          accumulate_and_clear(lh + k4, lh32 + k4, 24 - k4);
-
-          // Last element of the row is computed separately.
-          lh[24] += lh32[24];
-          lh32[24] = 0;
-
-          lh += WIENER_WIN2_REDUCED_ALIGN2;
-          lh32 += WIENER_WIN2_REDUCED_ALIGN2;
+    const int16_t *d_t = d;
+    int32_t height_t = 0;
+    int64x2_t sum_h[WIENER_WIN - 1] = { vdupq_n_s64(0) };
+    int16x8_t dgd[2];
+
+    do {
+      const int32_t h_t =
+          ((height - height_t) < h_allowed) ? (height - height_t) : h_allowed;
+      int32x4_t row_h[WIENER_WIN - 1] = { vdupq_n_s32(0) };
+
+      y = h_t;
+      do {
+        x = 0;
+        while (x < w16) {
+          dgd[0] = vld1q_s16(d_t + j + x + 0);
+          dgd[1] = vld1q_s16(d_t + j + x + 8);
+          stats_left_win7_neon(dgd, d_t + x, d_stride, row_h);
+          x += 16;
+        }
+
+        if (w16 != width) {
+          dgd[0] = vld1q_s16(d_t + j + x + 0);
+          dgd[1] = vld1q_s16(d_t + j + x + 8);
+          dgd[0] = vandq_s16(dgd[0], mask[0]);
+          dgd[1] = vandq_s16(dgd[1], mask[1]);
+          stats_left_win7_neon(dgd, d_t + x, d_stride, row_h);
         }
+
+        d_t += d_stride;
+      } while (--y);
+
+      sum_h[0] = vpadalq_s32(sum_h[0], row_h[0]);
+      sum_h[1] = vpadalq_s32(sum_h[1], row_h[1]);
+      sum_h[2] = vpadalq_s32(sum_h[2], row_h[2]);
+      sum_h[3] = vpadalq_s32(sum_h[3], row_h[3]);
+      sum_h[4] = vpadalq_s32(sum_h[4], row_h[4]);
+      sum_h[5] = vpadalq_s32(sum_h[5], row_h[5]);
+
+      height_t += h_t;
+    } while (height_t < height);
+
+#if AOM_ARCH_AARCH64
+    int64x2_t sum_h0 = vpaddq_s64(sum_h[0], sum_h[1]);
+    int64x2_t sum_h2 = vpaddq_s64(sum_h[2], sum_h[3]);
+    int64x2_t sum_h4 = vpaddq_s64(sum_h[4], sum_h[5]);
+    vst1_s64(&H[1 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h0));
+    vst1_s64(&H[2 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h0));
+    vst1_s64(&H[3 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h2));
+    vst1_s64(&H[4 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h2));
+    vst1_s64(&H[5 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h4));
+    vst1_s64(&H[6 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h4));
+#else
+    H[1 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[0]);
+    H[2 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[1]);
+    H[3 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[2]);
+    H[4 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[3]);
+    H[5 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[4]);
+    H[6 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[5]);
+#endif  // AOM_ARCH_AARCH64
+
+  } while (++j < wiener_win);
+
+  // Step 3: Derive the top edge of each triangle along the diagonal. No
+  // triangle in top row.
+  {
+    const int16_t *d_t = d;
+    // Pad to call transpose function.
+    int32x4_t deltas[(WIENER_WIN + 1) * 2] = { vdupq_n_s32(0) };
+    int32x4_t deltas_tr[(WIENER_WIN + 1) * 2] = { vdupq_n_s32(0) };
+    int16x8_t ds[WIENER_WIN * 2];
+
+    load_s16_8x6(d_t, d_stride, &ds[0], &ds[2], &ds[4], &ds[6], &ds[8],
+                 &ds[10]);
+    load_s16_8x6(d_t + width, d_stride, &ds[1], &ds[3], &ds[5], &ds[7], &ds[9],
+                 &ds[11]);
+
+    d_t += 6 * d_stride;
+
+    step3_win7_neon(d_t, d_stride, width, height, ds, deltas);
+    transpose_arrays_s32_8x8(deltas, deltas_tr);
+
+    update_8_stats_neon(H + 0 * wiener_win * wiener_win2 + 0 * wiener_win,
+                        deltas_tr[0], deltas_tr[4],
+                        H + 1 * wiener_win * wiener_win2 + 1 * wiener_win);
+    update_8_stats_neon(H + 1 * wiener_win * wiener_win2 + 1 * wiener_win,
+                        deltas_tr[1], deltas_tr[5],
+                        H + 2 * wiener_win * wiener_win2 + 2 * wiener_win);
+    update_8_stats_neon(H + 2 * wiener_win * wiener_win2 + 2 * wiener_win,
+                        deltas_tr[2], deltas_tr[6],
+                        H + 3 * wiener_win * wiener_win2 + 3 * wiener_win);
+    update_8_stats_neon(H + 3 * wiener_win * wiener_win2 + 3 * wiener_win,
+                        deltas_tr[3], deltas_tr[7],
+                        H + 4 * wiener_win * wiener_win2 + 4 * wiener_win);
+    update_8_stats_neon(H + 4 * wiener_win * wiener_win2 + 4 * wiener_win,
+                        deltas_tr[8], deltas_tr[12],
+                        H + 5 * wiener_win * wiener_win2 + 5 * wiener_win);
+    update_8_stats_neon(H + 5 * wiener_win * wiener_win2 + 5 * wiener_win,
+                        deltas_tr[9], deltas_tr[13],
+                        H + 6 * wiener_win * wiener_win2 + 6 * wiener_win);
+  }
+
+  // Step 4: Derive the top and left edge of each square. No square in top and
+  // bottom row.
+
+  i = 1;
+  do {
+    j = i + 1;
+    do {
+      const int16_t *di = d + i - 1;
+      const int16_t *dj = d + j - 1;
+      int32x4_t deltas[(2 * WIENER_WIN - 1) * 2] = { vdupq_n_s32(0) };
+      int16x8_t dd[WIENER_WIN * 2], ds[WIENER_WIN * 2];
+
+      dd[5] = vdupq_n_s16(0);  // Initialize to avoid warning.
+      const int16_t dd0_values[] = { di[0 * d_stride],
+                                     di[1 * d_stride],
+                                     di[2 * d_stride],
+                                     di[3 * d_stride],
+                                     di[4 * d_stride],
+                                     di[5 * d_stride],
+                                     0,
+                                     0 };
+      dd[0] = vld1q_s16(dd0_values);
+      const int16_t dd1_values[] = { di[0 * d_stride + width],
+                                     di[1 * d_stride + width],
+                                     di[2 * d_stride + width],
+                                     di[3 * d_stride + width],
+                                     di[4 * d_stride + width],
+                                     di[5 * d_stride + width],
+                                     0,
+                                     0 };
+      dd[1] = vld1q_s16(dd1_values);
+      const int16_t ds0_values[] = { dj[0 * d_stride],
+                                     dj[1 * d_stride],
+                                     dj[2 * d_stride],
+                                     dj[3 * d_stride],
+                                     dj[4 * d_stride],
+                                     dj[5 * d_stride],
+                                     0,
+                                     0 };
+      ds[0] = vld1q_s16(ds0_values);
+      int16_t ds1_values[] = { dj[0 * d_stride + width],
+                               dj[1 * d_stride + width],
+                               dj[2 * d_stride + width],
+                               dj[3 * d_stride + width],
+                               dj[4 * d_stride + width],
+                               dj[5 * d_stride + width],
+                               0,
+                               0 };
+      ds[1] = vld1q_s16(ds1_values);
+
+      y = 0;
+      while (y < h8) {
+        // 00s 10s 20s 30s 40s 50s 60s 70s  00e 10e 20e 30e 40e 50e 60e 70e
+        dd[0] = vsetq_lane_s16(di[6 * d_stride], dd[0], 6);
+        dd[0] = vsetq_lane_s16(di[7 * d_stride], dd[0], 7);
+        dd[1] = vsetq_lane_s16(di[6 * d_stride + width], dd[1], 6);
+        dd[1] = vsetq_lane_s16(di[7 * d_stride + width], dd[1], 7);
+
+        // 00s 10s 20s 30s 40s 50s 60s 70s  00e 10e 20e 30e 40e 50e 60e 70e
+        // 01s 11s 21s 31s 41s 51s 61s 71s  01e 11e 21e 31e 41e 51e 61e 71e
+        ds[0] = vsetq_lane_s16(dj[6 * d_stride], ds[0], 6);
+        ds[0] = vsetq_lane_s16(dj[7 * d_stride], ds[0], 7);
+        ds[1] = vsetq_lane_s16(dj[6 * d_stride + width], ds[1], 6);
+        ds[1] = vsetq_lane_s16(dj[7 * d_stride + width], ds[1], 7);
+
+        load_more_16_neon(di + 8 * d_stride, width, &dd[0], &dd[2]);
+        load_more_16_neon(dj + 8 * d_stride, width, &ds[0], &ds[2]);
+        load_more_16_neon(di + 9 * d_stride, width, &dd[2], &dd[4]);
+        load_more_16_neon(dj + 9 * d_stride, width, &ds[2], &ds[4]);
+        load_more_16_neon(di + 10 * d_stride, width, &dd[4], &dd[6]);
+        load_more_16_neon(dj + 10 * d_stride, width, &ds[4], &ds[6]);
+        load_more_16_neon(di + 11 * d_stride, width, &dd[6], &dd[8]);
+        load_more_16_neon(dj + 11 * d_stride, width, &ds[6], &ds[8]);
+        load_more_16_neon(di + 12 * d_stride, width, &dd[8], &dd[10]);
+        load_more_16_neon(dj + 12 * d_stride, width, &ds[8], &ds[10]);
+        load_more_16_neon(di + 13 * d_stride, width, &dd[10], &dd[12]);
+        load_more_16_neon(dj + 13 * d_stride, width, &ds[10], &ds[12]);
+
+        madd_neon(&deltas[0], dd[0], ds[0]);
+        madd_neon(&deltas[1], dd[1], ds[1]);
+        madd_neon(&deltas[2], dd[0], ds[2]);
+        madd_neon(&deltas[3], dd[1], ds[3]);
+        madd_neon(&deltas[4], dd[0], ds[4]);
+        madd_neon(&deltas[5], dd[1], ds[5]);
+        madd_neon(&deltas[6], dd[0], ds[6]);
+        madd_neon(&deltas[7], dd[1], ds[7]);
+        madd_neon(&deltas[8], dd[0], ds[8]);
+        madd_neon(&deltas[9], dd[1], ds[9]);
+        madd_neon(&deltas[10], dd[0], ds[10]);
+        madd_neon(&deltas[11], dd[1], ds[11]);
+        madd_neon(&deltas[12], dd[0], ds[12]);
+        madd_neon(&deltas[13], dd[1], ds[13]);
+        madd_neon(&deltas[14], dd[2], ds[0]);
+        madd_neon(&deltas[15], dd[3], ds[1]);
+        madd_neon(&deltas[16], dd[4], ds[0]);
+        madd_neon(&deltas[17], dd[5], ds[1]);
+        madd_neon(&deltas[18], dd[6], ds[0]);
+        madd_neon(&deltas[19], dd[7], ds[1]);
+        madd_neon(&deltas[20], dd[8], ds[0]);
+        madd_neon(&deltas[21], dd[9], ds[1]);
+        madd_neon(&deltas[22], dd[10], ds[0]);
+        madd_neon(&deltas[23], dd[11], ds[1]);
+        madd_neon(&deltas[24], dd[12], ds[0]);
+        madd_neon(&deltas[25], dd[13], ds[1]);
+
+        dd[0] = vextq_s16(dd[12], vdupq_n_s16(0), 2);
+        dd[1] = vextq_s16(dd[13], vdupq_n_s16(0), 2);
+        ds[0] = vextq_s16(ds[12], vdupq_n_s16(0), 2);
+        ds[1] = vextq_s16(ds[13], vdupq_n_s16(0), 2);
+
+        di += 8 * d_stride;
+        dj += 8 * d_stride;
+        y += 8;
       }
 
-      j -= 2;
+      deltas[0] = hadd_four_32_neon(deltas[0], deltas[2], deltas[4], deltas[6]);
+      deltas[1] = hadd_four_32_neon(deltas[1], deltas[3], deltas[5], deltas[7]);
+      deltas[2] =
+          hadd_four_32_neon(deltas[8], deltas[10], deltas[12], deltas[12]);
+      deltas[3] =
+          hadd_four_32_neon(deltas[9], deltas[11], deltas[13], deltas[13]);
+      deltas[4] =
+          hadd_four_32_neon(deltas[14], deltas[16], deltas[18], deltas[20]);
+      deltas[5] =
+          hadd_four_32_neon(deltas[15], deltas[17], deltas[19], deltas[21]);
+      deltas[6] =
+          hadd_four_32_neon(deltas[22], deltas[24], deltas[22], deltas[24]);
+      deltas[7] =
+          hadd_four_32_neon(deltas[23], deltas[25], deltas[23], deltas[25]);
+      deltas[0] = vsubq_s32(deltas[1], deltas[0]);
+      deltas[1] = vsubq_s32(deltas[3], deltas[2]);
+      deltas[2] = vsubq_s32(deltas[5], deltas[4]);
+      deltas[3] = vsubq_s32(deltas[7], deltas[6]);
+
+      if (h8 != height) {
+        const int16_t ds0_vals[] = {
+          dj[0 * d_stride], dj[0 * d_stride + width],
+          dj[1 * d_stride], dj[1 * d_stride + width],
+          dj[2 * d_stride], dj[2 * d_stride + width],
+          dj[3 * d_stride], dj[3 * d_stride + width]
+        };
+        ds[0] = vld1q_s16(ds0_vals);
+
+        ds[1] = vsetq_lane_s16(dj[4 * d_stride], ds[1], 0);
+        ds[1] = vsetq_lane_s16(dj[4 * d_stride + width], ds[1], 1);
+        ds[1] = vsetq_lane_s16(dj[5 * d_stride], ds[1], 2);
+        ds[1] = vsetq_lane_s16(dj[5 * d_stride + width], ds[1], 3);
+        const int16_t dd4_vals[] = {
+          -di[1 * d_stride], di[1 * d_stride + width],
+          -di[2 * d_stride], di[2 * d_stride + width],
+          -di[3 * d_stride], di[3 * d_stride + width],
+          -di[4 * d_stride], di[4 * d_stride + width]
+        };
+        dd[4] = vld1q_s16(dd4_vals);
+
+        dd[5] = vsetq_lane_s16(-di[5 * d_stride], dd[5], 0);
+        dd[5] = vsetq_lane_s16(di[5 * d_stride + width], dd[5], 1);
+        do {
+          dd[0] = vdupq_n_s16(-di[0 * d_stride]);
+          dd[2] = dd[3] = vdupq_n_s16(di[0 * d_stride + width]);
+          dd[0] = dd[1] = vzipq_s16(dd[0], dd[2]).val[0];
+
+          ds[4] = vdupq_n_s16(dj[0 * d_stride]);
+          ds[6] = ds[7] = vdupq_n_s16(dj[0 * d_stride + width]);
+          ds[4] = ds[5] = vzipq_s16(ds[4], ds[6]).val[0];
+
+          dd[5] = vsetq_lane_s16(-di[6 * d_stride], dd[5], 2);
+          dd[5] = vsetq_lane_s16(di[6 * d_stride + width], dd[5], 3);
+          ds[1] = vsetq_lane_s16(dj[6 * d_stride], ds[1], 4);
+          ds[1] = vsetq_lane_s16(dj[6 * d_stride + width], ds[1], 5);
+
+          madd_neon_pairwise(&deltas[0], dd[0], ds[0]);
+          madd_neon_pairwise(&deltas[1], dd[1], ds[1]);
+          madd_neon_pairwise(&deltas[2], dd[4], ds[4]);
+          madd_neon_pairwise(&deltas[3], dd[5], ds[5]);
+
+          int32_t tmp0 = vgetq_lane_s32(vreinterpretq_s32_s16(ds[0]), 0);
+          ds[0] = vextq_s16(ds[0], ds[1], 2);
+          ds[1] = vextq_s16(ds[1], ds[0], 2);
+          ds[1] = vreinterpretq_s16_s32(
+              vsetq_lane_s32(tmp0, vreinterpretq_s32_s16(ds[1]), 3));
+          int32_t tmp1 = vgetq_lane_s32(vreinterpretq_s32_s16(dd[4]), 0);
+          dd[4] = vextq_s16(dd[4], dd[5], 2);
+          dd[5] = vextq_s16(dd[5], dd[4], 2);
+          dd[5] = vreinterpretq_s16_s32(
+              vsetq_lane_s32(tmp1, vreinterpretq_s32_s16(dd[5]), 3));
+          di += d_stride;
+          dj += d_stride;
+        } while (++y < height);
+      }
+
+      // Writing one more element on the top edge of a square falls to
+      // the next square in the same row or the first element in the next
+      // row, which will just be overwritten later.
+      update_8_stats_neon(
+          H + (i - 1) * wiener_win * wiener_win2 + (j - 1) * wiener_win,
+          deltas[0], deltas[1],
+          H + i * wiener_win * wiener_win2 + j * wiener_win);
+
+      H[(i * wiener_win + 1) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 1) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s32(deltas[2], 0);
+      H[(i * wiener_win + 2) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 2) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s32(deltas[2], 1);
+      H[(i * wiener_win + 3) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 3) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s32(deltas[2], 2);
+      H[(i * wiener_win + 4) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 4) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s32(deltas[2], 3);
+      H[(i * wiener_win + 5) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 5) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s32(deltas[3], 0);
+      H[(i * wiener_win + 6) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 6) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s32(deltas[3], 1);
+    } while (++j < wiener_win);
+  } while (++i < wiener_win - 1);
+
+  // Step 5: Derive other points of each square. No square in bottom row.
+  i = 0;
+  do {
+    const int16_t *const di = d + i;
+
+    j = i + 1;
+    do {
+      const int16_t *const dj = d + j;
+      int32x4_t deltas[WIENER_WIN - 1][WIN_7] = { { vdupq_n_s32(0) },
+                                                  { vdupq_n_s32(0) } };
+      int16x8_t d_is[WIN_7];
+      int16x8_t d_ie[WIN_7];
+      int16x8_t d_js[WIN_7];
+      int16x8_t d_je[WIN_7];
+
+      x = 0;
+      while (x < w16) {
+        load_square_win7_neon(di + x, dj + x, d_stride, height, d_is, d_ie,
+                              d_js, d_je);
+        derive_square_win7_neon(d_is, d_ie, d_js, d_je, deltas);
+        x += 16;
+      }
+
+      if (w16 != width) {
+        load_square_win7_neon(di + x, dj + x, d_stride, height, d_is, d_ie,
+                              d_js, d_je);
+        d_is[0] = vandq_s16(d_is[0], mask[0]);
+        d_is[1] = vandq_s16(d_is[1], mask[1]);
+        d_is[2] = vandq_s16(d_is[2], mask[0]);
+        d_is[3] = vandq_s16(d_is[3], mask[1]);
+        d_is[4] = vandq_s16(d_is[4], mask[0]);
+        d_is[5] = vandq_s16(d_is[5], mask[1]);
+        d_is[6] = vandq_s16(d_is[6], mask[0]);
+        d_is[7] = vandq_s16(d_is[7], mask[1]);
+        d_is[8] = vandq_s16(d_is[8], mask[0]);
+        d_is[9] = vandq_s16(d_is[9], mask[1]);
+        d_is[10] = vandq_s16(d_is[10], mask[0]);
+        d_is[11] = vandq_s16(d_is[11], mask[1]);
+        d_ie[0] = vandq_s16(d_ie[0], mask[0]);
+        d_ie[1] = vandq_s16(d_ie[1], mask[1]);
+        d_ie[2] = vandq_s16(d_ie[2], mask[0]);
+        d_ie[3] = vandq_s16(d_ie[3], mask[1]);
+        d_ie[4] = vandq_s16(d_ie[4], mask[0]);
+        d_ie[5] = vandq_s16(d_ie[5], mask[1]);
+        d_ie[6] = vandq_s16(d_ie[6], mask[0]);
+        d_ie[7] = vandq_s16(d_ie[7], mask[1]);
+        d_ie[8] = vandq_s16(d_ie[8], mask[0]);
+        d_ie[9] = vandq_s16(d_ie[9], mask[1]);
+        d_ie[10] = vandq_s16(d_ie[10], mask[0]);
+        d_ie[11] = vandq_s16(d_ie[11], mask[1]);
+        derive_square_win7_neon(d_is, d_ie, d_js, d_je, deltas);
+      }
+
+      hadd_update_6_stats_neon(
+          H + (i * wiener_win + 0) * wiener_win2 + j * wiener_win, deltas[0],
+          H + (i * wiener_win + 1) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_neon(
+          H + (i * wiener_win + 1) * wiener_win2 + j * wiener_win, deltas[1],
+          H + (i * wiener_win + 2) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_neon(
+          H + (i * wiener_win + 2) * wiener_win2 + j * wiener_win, deltas[2],
+          H + (i * wiener_win + 3) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_neon(
+          H + (i * wiener_win + 3) * wiener_win2 + j * wiener_win, deltas[3],
+          H + (i * wiener_win + 4) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_neon(
+          H + (i * wiener_win + 4) * wiener_win2 + j * wiener_win, deltas[4],
+          H + (i * wiener_win + 5) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_neon(
+          H + (i * wiener_win + 5) * wiener_win2 + j * wiener_win, deltas[5],
+          H + (i * wiener_win + 6) * wiener_win2 + j * wiener_win + 1);
+    } while (++j < wiener_win);
+  } while (++i < wiener_win - 1);
+
+  // Step 6: Derive other points of each upper triangle along the diagonal.
+  i = 0;
+  do {
+    const int16_t *const di = d + i;
+    int32x4_t deltas[WIENER_WIN * (WIENER_WIN - 1)] = { vdupq_n_s32(0) };
+    int16x8_t d_is[WIN_7], d_ie[WIN_7];
+
+    x = 0;
+    while (x < w16) {
+      load_triangle_win7_neon(di + x, d_stride, height, d_is, d_ie);
+      derive_triangle_win7_neon(d_is, d_ie, deltas);
+      x += 16;
     }
 
-    // Computations for odd pixel in the row.
-    if (width & 1) {
-      // Load two adjacent, overlapping 5x5 matrices: a 6x5 matrix with the
-      // middle 4x5 elements being shared.
-      int16x8_t dgd_rows[5];
-      load_and_pack_s16_6x5(dgd_rows, (const int16_t *)dgd, dgd_stride);
-
-      const int16_t *dgd_ptr = (const int16_t *)dgd + dgd_stride * 4;
-      ++dgd;
-
-      // Re-arrange (and widen) the combined 6x5 matrix to have a whole 5x5
-      // matrix tightly packed into a int16x8_t[3] array. This array contains
-      // 24 elements of the 25 (5x5). Compute `dgd - avg` for the whole buffer.
-      // The DGD_AVG buffer contains 25 consecutive elements.
-      int16x8_t dgd_avg0[3];
-
-      dgd_avg0[0] = vsubq_s16(tbl2q(dgd_rows[0], dgd_rows[1], lut0), avg_s16);
-      dgd_avg0[1] = vsubq_s16(
-          tbl3q(dgd_rows[1], dgd_rows[2], dgd_rows[3], lut1), avg_s16);
-      dgd_avg0[2] = vsubq_s16(tbl2q(dgd_rows[3], dgd_rows[4], lut2), avg_s16);
-
-      vst1q_s16(DGD_AVG0, dgd_avg0[0]);
-      vst1q_s16(DGD_AVG0 + 8, dgd_avg0[1]);
-      vst1q_s16(DGD_AVG0 + 16, dgd_avg0[2]);
-
-      // The remaining last (25th) element of `dgd - avg`.
-      DGD_AVG0[24] = dgd_ptr[4] - avg;
-      DGD_AVG1[24] = dgd_ptr[5] - avg;
-
-      // Accumulate into row-major order variant of matrix M (cross-correlation)
-      // for 1 output pixel at a time. M is of size 5 * 5. It needs to be filled
-      // such that multiplying one element from src with each element of a row
-      // of the wiener window will fill one column of M. However this is not
-      // very convenient in terms of memory access, as it means we do
-      // contiguous loads of dgd but strided stores to M. As a result, we use an
-      // intermediate matrix M_s32 which is instead filled such that one row of
-      // the wiener window gives one row of M_s32. Once fully computed, M_s32 is
-      // then transposed to return M.
-      int src_avg0 = *src++ - avg;
-      int16x4_t src_avg0_s16 = vdup_n_s16(src_avg0);
-      update_M_1pixel(M_s32 + 0, src_avg0_s16, dgd_avg0[0]);
-      update_M_1pixel(M_s32 + 8, src_avg0_s16, dgd_avg0[1]);
-      update_M_1pixel(M_s32 + 16, src_avg0_s16, dgd_avg0[2]);
-
-      // Last (25th) element of M_s32 can be computed as scalar more efficiently
-      // for 1 output pixel.
-      M_s32[24] += DGD_AVG0[24] * src_avg0;
-
-      // Start accumulating into row-major order version of matrix H
-      // (auto-covariance), it expects the DGD_AVG0 matrix to also be row-major.
-      // H is of size 25 * 25. It is filled by multiplying every pair of
-      // elements of the wiener window together (vector outer product). Since it
-      // is a symmetric matrix, we only compute the upper-right triangle, and
-      // then copy it down to the lower-left later. The upper triangle is
-      // covered by 4x4 tiles. The original algorithm assumes the M matrix is
-      // column-major and the resulting H matrix is also expected to be
-      // column-major. It is not efficient to work with column-major matrices,
-      // so we accumulate into a row-major matrix H_s32. At the end of the
-      // algorithm a double transpose transformation will convert H_s32 back to
-      // the expected output layout.
-      update_H_1pixel(H_s32, DGD_AVG0, WIENER_WIN2_REDUCED_ALIGN2, 24);
-
-      // The last element of the triangle of H_s32 matrix can be computed as a
-      // scalar more efficiently.
-      H_s32[24 * WIENER_WIN2_REDUCED_ALIGN2 + 24] +=
-          DGD_AVG0[24] * DGD_AVG0[24];
+    if (w16 != width) {
+      load_triangle_win7_neon(di + x, d_stride, height, d_is, d_ie);
+      d_is[0] = vandq_s16(d_is[0], mask[0]);
+      d_is[1] = vandq_s16(d_is[1], mask[1]);
+      d_is[2] = vandq_s16(d_is[2], mask[0]);
+      d_is[3] = vandq_s16(d_is[3], mask[1]);
+      d_is[4] = vandq_s16(d_is[4], mask[0]);
+      d_is[5] = vandq_s16(d_is[5], mask[1]);
+      d_is[6] = vandq_s16(d_is[6], mask[0]);
+      d_is[7] = vandq_s16(d_is[7], mask[1]);
+      d_is[8] = vandq_s16(d_is[8], mask[0]);
+      d_is[9] = vandq_s16(d_is[9], mask[1]);
+      d_is[10] = vandq_s16(d_is[10], mask[0]);
+      d_is[11] = vandq_s16(d_is[11], mask[1]);
+      d_ie[0] = vandq_s16(d_ie[0], mask[0]);
+      d_ie[1] = vandq_s16(d_ie[1], mask[1]);
+      d_ie[2] = vandq_s16(d_ie[2], mask[0]);
+      d_ie[3] = vandq_s16(d_ie[3], mask[1]);
+      d_ie[4] = vandq_s16(d_ie[4], mask[0]);
+      d_ie[5] = vandq_s16(d_ie[5], mask[1]);
+      d_ie[6] = vandq_s16(d_ie[6], mask[0]);
+      d_ie[7] = vandq_s16(d_ie[7], mask[1]);
+      d_ie[8] = vandq_s16(d_ie[8], mask[0]);
+      d_ie[9] = vandq_s16(d_ie[9], mask[1]);
+      d_ie[10] = vandq_s16(d_ie[10], mask[0]);
+      d_ie[11] = vandq_s16(d_ie[11], mask[1]);
+      derive_triangle_win7_neon(d_is, d_ie, deltas);
     }
 
-    src += src_next;
-    dgd += dgd_next;
-  } while (--height != 0);
+    // Row 1: 6 points
+    hadd_update_6_stats_neon(
+        H + (i * wiener_win + 0) * wiener_win2 + i * wiener_win, deltas,
+        H + (i * wiener_win + 1) * wiener_win2 + i * wiener_win + 1);
+
+    // Row 2: 5 points
+    hadd_update_4_stats_neon(
+        H + (i * wiener_win + 1) * wiener_win2 + i * wiener_win + 1, deltas + 6,
+        H + (i * wiener_win + 2) * wiener_win2 + i * wiener_win + 2);
+    H[(i * wiener_win + 2) * wiener_win2 + i * wiener_win + 6] =
+        H[(i * wiener_win + 1) * wiener_win2 + i * wiener_win + 5] +
+        horizontal_long_add_s32x4(deltas[10]);
+
+    // Row 3: 4 points
+    hadd_update_4_stats_neon(
+        H + (i * wiener_win + 2) * wiener_win2 + i * wiener_win + 2,
+        deltas + 11,
+        H + (i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3);
+
+    // Row 4: 3 points
+#if AOM_ARCH_AARCH64
+    int64x2_t delta15_s64 = vpaddlq_s32(deltas[15]);
+    int64x2_t delta16_s64 = vpaddlq_s32(deltas[16]);
+    int64x2_t delta1516 = vpaddq_s64(delta15_s64, delta16_s64);
+
+    int64x2_t h0 =
+        vld1q_s64(H + (i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3);
+    vst1q_s64(H + (i * wiener_win + 4) * wiener_win2 + i * wiener_win + 4,
+              vaddq_s64(h0, delta1516));
+#else
+    H[(i * wiener_win + 4) * wiener_win2 + i * wiener_win + 4 + 0] =
+        H[(i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3 + 0] +
+        horizontal_long_add_s32x4(deltas[15]);
+    H[(i * wiener_win + 4) * wiener_win2 + i * wiener_win + 4 + 1] =
+        H[(i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3 + 1] +
+        horizontal_long_add_s32x4(deltas[16]);
+#endif  // AOM_ARCH_AARCH64
+
+    H[(i * wiener_win + 4) * wiener_win2 + i * wiener_win + 6] =
+        H[(i * wiener_win + 3) * wiener_win2 + i * wiener_win + 5] +
+        horizontal_long_add_s32x4(deltas[17]);
+
+    // Row 5: 2 points
+    int64x2_t delta18_s64 = vpaddlq_s32(deltas[18]);
+    int64x2_t delta19_s64 = vpaddlq_s32(deltas[19]);
+
+#if AOM_ARCH_AARCH64
+    int64x2_t delta1819 = vpaddq_s64(delta18_s64, delta19_s64);
 
-  int bit_depth_shift = bit_depth - AOM_BITS_8;
+    int64x2_t h1 =
+        vld1q_s64(H + (i * wiener_win + 4) * wiener_win2 + i * wiener_win + 4);
+    vst1q_s64(H + (i * wiener_win + 5) * wiener_win2 + i * wiener_win + 5,
+              vaddq_s64(h1, delta1819));
+#else
+    H[(i * wiener_win + 5) * wiener_win2 + i * wiener_win + 5] =
+        H[(i * wiener_win + 4) * wiener_win2 + i * wiener_win + 4] +
+        horizontal_add_s64x2(delta18_s64);
+    H[(i * wiener_win + 5) * wiener_win2 + i * wiener_win + 5 + 1] =
+        H[(i * wiener_win + 4) * wiener_win2 + i * wiener_win + 4 + 1] +
+        horizontal_add_s64x2(delta19_s64);
+#endif  // AOM_ARCH_AARCH64
+
+    // Row 6: 1 points
+    H[(i * wiener_win + 6) * wiener_win2 + i * wiener_win + 6] =
+        H[(i * wiener_win + 5) * wiener_win2 + i * wiener_win + 5] +
+        horizontal_long_add_s32x4(deltas[20]);
+  } while (++i < wiener_win);
+}
 
-  acc_transpose_M(M, M_s64, M_s32, WIENER_WIN_REDUCED, bit_depth_shift);
+static inline void sub_avg_block_highbd_neon(const uint16_t *src,
+                                             const int32_t src_stride,
+                                             const uint16_t avg,
+                                             const int32_t width,
+                                             const int32_t height, int16_t *dst,
+                                             const int32_t dst_stride) {
+  const uint16x8_t a = vdupq_n_u16(avg);
 
-  update_H(H, H_s64, H_s32, WIENER_WIN_REDUCED, WIENER_WIN2_REDUCED_ALIGN2,
-           bit_depth_shift);
+  int32_t i = height + 1;
+  do {
+    int32_t j = 0;
+    while (j < width) {
+      const uint16x8_t s = vld1q_u16(src + j);
+      const uint16x8_t d = vsubq_u16(s, a);
+      vst1q_s16(dst + j, vreinterpretq_s16_u16(d));
+      j += 8;
+    }
+
+    src += src_stride;
+    dst += dst_stride;
+  } while (--i);
 }
 
-static uint16_t highbd_find_average_neon(const uint16_t *src, int src_stride,
-                                         int width, int height) {
+static inline uint16_t highbd_find_average_neon(const uint16_t *src,
+                                                int src_stride, int width,
+                                                int height) {
   assert(width > 0);
   assert(height > 0);
 
   uint64x2_t sum_u64 = vdupq_n_u64(0);
   uint64_t sum = 0;
+  const uint16x8_t mask =
+      vreinterpretq_u16_s16(vld1q_s16(&mask_16bit[16] - (width % 8)));
 
   int h = height;
   do {
@@ -987,16 +1690,12 @@ static uint16_t highbd_find_average_neon(const uint16_t *src, int src_stride,
       w -= 8;
     }
 
-    if (w >= 4) {
-      uint16x8_t s0 = vcombine_u16(vld1_u16(row), vdup_n_u16(0));
-      sum_u32[0] = vpadalq_u16(sum_u32[0], s0);
-
-      row += 4;
-      w -= 4;
-    }
+    if (w) {
+      uint16x8_t s0 = vandq_u16(vld1q_u16(row), mask);
+      sum_u32[1] = vpadalq_u16(sum_u32[1], s0);
 
-    while (w-- > 0) {
-      sum += *row++;
+      row += 8;
+      w -= 8;
     }
 
     sum_u64 = vpadalq_u32(sum_u64, vaddq_u32(sum_u32[0], sum_u32[1]));
@@ -1007,45 +1706,86 @@ static uint16_t highbd_find_average_neon(const uint16_t *src, int src_stride,
   return (uint16_t)((horizontal_add_u64x2(sum_u64) + sum) / (height * width));
 }
 
-void av1_compute_stats_highbd_neon(int wiener_win, const uint8_t *dgd8,
+void av1_compute_stats_highbd_neon(int32_t wiener_win, const uint8_t *dgd8,
                                    const uint8_t *src8, int16_t *dgd_avg,
-                                   int16_t *src_avg, int h_start, int h_end,
-                                   int v_start, int v_end, int dgd_stride,
-                                   int src_stride, int64_t *M, int64_t *H,
+                                   int16_t *src_avg, int32_t h_start,
+                                   int32_t h_end, int32_t v_start,
+                                   int32_t v_end, int32_t dgd_stride,
+                                   int32_t src_stride, int64_t *M, int64_t *H,
                                    aom_bit_depth_t bit_depth) {
-  (void)dgd_avg;
-  (void)src_avg;
-  assert(wiener_win == WIENER_WIN || wiener_win == WIENER_WIN_REDUCED);
-
-  const int wiener_halfwin = wiener_win >> 1;
+  const int32_t wiener_win2 = wiener_win * wiener_win;
+  const int32_t wiener_halfwin = (wiener_win >> 1);
   const uint16_t *src = CONVERT_TO_SHORTPTR(src8);
   const uint16_t *dgd = CONVERT_TO_SHORTPTR(dgd8);
-  const int height = v_end - v_start;
-  const int width = h_end - h_start;
-
+  const int32_t width = h_end - h_start;
+  const int32_t height = v_end - v_start;
   const uint16_t *dgd_start = dgd + h_start + v_start * dgd_stride;
-  const uint16_t *src_start = src + h_start + v_start * src_stride;
+  const uint16_t avg =
+      highbd_find_average_neon(dgd_start, dgd_stride, width, height);
+  const int32_t d_stride = (width + 2 * wiener_halfwin + 15) & ~15;
+  const int32_t s_stride = (width + 15) & ~15;
+
+  sub_avg_block_highbd_neon(src + v_start * src_stride + h_start, src_stride,
+                            avg, width, height, src_avg, s_stride);
+  sub_avg_block_highbd_neon(
+      dgd + (v_start - wiener_halfwin) * dgd_stride + h_start - wiener_halfwin,
+      dgd_stride, avg, width + 2 * wiener_halfwin, height + 2 * wiener_halfwin,
+      dgd_avg, d_stride);
 
-  // The wiener window will slide along the dgd frame, centered on each pixel.
-  // For the top left pixel and all the pixels on the side of the frame this
-  // means half of the window will be outside of the frame. As such the actual
-  // buffer that we need to subtract the avg from will be 2 * wiener_halfwin
-  // wider and 2 * wiener_halfwin higher than the original dgd buffer.
-  const int vert_offset = v_start - wiener_halfwin;
-  const int horiz_offset = h_start - wiener_halfwin;
-  const uint16_t *dgd_win = dgd + horiz_offset + vert_offset * dgd_stride;
+  if (wiener_win == WIENER_WIN) {
+    compute_stats_win7_highbd_neon(dgd_avg, d_stride, src_avg, s_stride, width,
+                                   height, M, H, bit_depth);
+  } else if (wiener_win == WIENER_WIN_CHROMA) {
+    compute_stats_win5_highbd_neon(dgd_avg, d_stride, src_avg, s_stride, width,
+                                   height, M, H, bit_depth);
+  }
 
-  uint16_t avg = highbd_find_average_neon(dgd_start, dgd_stride, width, height);
+  // H is a symmetric matrix, so we only need to fill out the upper triangle.
+  // We can copy it down to the lower triangle outside the (i, j) loops.
+  if (bit_depth == AOM_BITS_8) {
+    diagonal_copy_stats_neon(wiener_win2, H);
+  } else if (bit_depth == AOM_BITS_10) {  // bit_depth == AOM_BITS_10
+    const int32_t k4 = wiener_win2 & ~3;
 
-  if (wiener_win == WIENER_WIN) {
-    highbd_compute_stats_win7_neon(dgd_win, src_start, avg, width, height,
-                                   dgd_stride, src_stride, M, H, bit_depth);
-  } else {
-    highbd_compute_stats_win5_neon(dgd_win, src_start, avg, width, height,
-                                   dgd_stride, src_stride, M, H, bit_depth);
+    int32_t k = 0;
+    do {
+      int64x2_t dst = div4_neon(vld1q_s64(M + k));
+      vst1q_s64(M + k, dst);
+      dst = div4_neon(vld1q_s64(M + k + 2));
+      vst1q_s64(M + k + 2, dst);
+      H[k * wiener_win2 + k] /= 4;
+      k += 4;
+    } while (k < k4);
+
+    H[k * wiener_win2 + k] /= 4;
+
+    for (; k < wiener_win2; ++k) {
+      M[k] /= 4;
+    }
+
+    div4_diagonal_copy_stats_neon(wiener_win2, H);
+  } else {  // bit_depth == AOM_BITS_12
+    const int32_t k4 = wiener_win2 & ~3;
+
+    int32_t k = 0;
+    do {
+      int64x2_t dst = div16_neon(vld1q_s64(M + k));
+      vst1q_s64(M + k, dst);
+      dst = div16_neon(vld1q_s64(M + k + 2));
+      vst1q_s64(M + k + 2, dst);
+      H[k * wiener_win2 + k] /= 16;
+      k += 4;
+    } while (k < k4);
+
+    H[k * wiener_win2 + k] /= 16;
+
+    for (; k < wiener_win2; ++k) {
+      M[k] /= 16;
+    }
+
+    div16_diagonal_copy_stats_neon(wiener_win2, H);
   }
 }
-
 int64_t av1_highbd_pixel_proj_error_neon(
     const uint8_t *src8, int width, int height, int src_stride,
     const uint8_t *dat8, int dat_stride, int32_t *flt0, int flt0_stride,
diff --git a/av1/encoder/arm/highbd_pickrst_sve.c b/av1/encoder/arm/highbd_pickrst_sve.c
index ef7089eca..2fcf03c92 100644
--- a/av1/encoder/arm/highbd_pickrst_sve.c
+++ b/av1/encoder/arm/highbd_pickrst_sve.c
@@ -11,21 +11,21 @@
 
 #include <arm_neon.h>
 #include <arm_sve.h>
-#include <string.h>
 
-#include "config/aom_config.h"
-#include "config/av1_rtcd.h"
+#include <assert.h>
+#include <stdint.h>
 
 #include "aom_dsp/arm/aom_neon_sve_bridge.h"
 #include "aom_dsp/arm/mem_neon.h"
 #include "aom_dsp/arm/sum_neon.h"
 #include "aom_dsp/arm/transpose_neon.h"
-#include "av1/common/restoration.h"
-#include "av1/encoder/pickrst.h"
+#include "av1/encoder/arm/pickrst_neon.h"
 #include "av1/encoder/arm/pickrst_sve.h"
+#include "av1/encoder/pickrst.h"
 
-static inline uint16_t find_average_sve(const uint16_t *src, int src_stride,
-                                        int width, int height) {
+static inline uint16_t highbd_find_average_sve(const uint16_t *src,
+                                               int src_stride, int width,
+                                               int height) {
   uint64x2_t avg_u64 = vdupq_n_u64(0);
   uint16x8_t ones = vdupq_n_u16(1);
 
@@ -51,9 +51,10 @@ static inline uint16_t find_average_sve(const uint16_t *src, int src_stride,
   return (uint16_t)(vaddvq_u64(avg_u64) / (width * height));
 }
 
-static inline void compute_sub_avg(const uint16_t *buf, int buf_stride,
-                                   int16_t avg, int16_t *buf_avg,
-                                   int buf_avg_stride, int width, int height) {
+static inline void sub_avg_block_highbd_sve(const uint16_t *buf, int buf_stride,
+                                            int16_t avg, int width, int height,
+                                            int16_t *buf_avg,
+                                            int buf_avg_stride) {
   uint16x8_t avg_u16 = vdupq_n_u16(avg);
 
   // Use a predicate to compute the last columns.
@@ -81,361 +82,86 @@ static inline void compute_sub_avg(const uint16_t *buf, int buf_stride,
   } while (--height > 0);
 }
 
-static inline void copy_upper_triangle(int64_t *H, int64_t *H_tmp,
-                                       const int wiener_win2,
-                                       const int divider) {
-  for (int i = 0; i < wiener_win2 - 2; i = i + 2) {
-    // Transpose the first 2x2 square. It needs a special case as the element
-    // of the bottom left is on the diagonal.
-    int64x2_t row0 = vld1q_s64(H_tmp + i * wiener_win2 + i + 1);
-    int64x2_t row1 = vld1q_s64(H_tmp + (i + 1) * wiener_win2 + i + 1);
-
-    int64x2_t tr_row = aom_vtrn2q_s64(row0, row1);
-
-    vst1_s64(H_tmp + (i + 1) * wiener_win2 + i, vget_low_s64(row0));
-    vst1q_s64(H_tmp + (i + 2) * wiener_win2 + i, tr_row);
-
-    // Transpose and store all the remaining 2x2 squares of the line.
-    for (int j = i + 3; j < wiener_win2; j = j + 2) {
-      row0 = vld1q_s64(H_tmp + i * wiener_win2 + j);
-      row1 = vld1q_s64(H_tmp + (i + 1) * wiener_win2 + j);
-
-      int64x2_t tr_row0 = aom_vtrn1q_s64(row0, row1);
-      int64x2_t tr_row1 = aom_vtrn2q_s64(row0, row1);
-
-      vst1q_s64(H_tmp + (j + 0) * wiener_win2 + i, tr_row0);
-      vst1q_s64(H_tmp + (j + 1) * wiener_win2 + i, tr_row1);
-    }
-  }
-  for (int i = 0; i < wiener_win2 * wiener_win2; i++) {
-    H[i] += H_tmp[i] / divider;
-  }
-}
-
-// Transpose the matrix that has just been computed and accumulate it in M.
-static inline void acc_transpose_M(int64_t *M, const int64_t *M_trn,
-                                   const int wiener_win, const int divider) {
-  for (int i = 0; i < wiener_win; ++i) {
-    for (int j = 0; j < wiener_win; ++j) {
-      int tr_idx = j * wiener_win + i;
-      *M++ += (int64_t)(M_trn[tr_idx] / divider);
-    }
-  }
-}
-
-// This function computes two matrices: the cross-correlation between the src
-// buffer and dgd buffer (M), and the auto-covariance of the dgd buffer (H).
-//
-// M is of size 7 * 7. It needs to be filled such that multiplying one element
-// from src with each element of a row of the wiener window will fill one
-// column of M. However this is not very convenient in terms of memory
-// accesses, as it means we do contiguous loads of dgd but strided stores to M.
-// As a result, we use an intermediate matrix M_trn which is instead filled
-// such that one row of the wiener window gives one row of M_trn. Once fully
-// computed, M_trn is then transposed to return M.
-//
-// H is of size 49 * 49. It is filled by multiplying every pair of elements of
-// the wiener window together. Since it is a symmetric matrix, we only compute
-// the upper triangle, and then copy it down to the lower one. Here we fill it
-// by taking each different pair of columns, and multiplying all the elements of
-// the first one with all the elements of the second one, with a special case
-// when multiplying a column by itself.
-static inline void highbd_compute_stats_win7_sve(
-    int16_t *dgd_avg, int dgd_avg_stride, int16_t *src_avg, int src_avg_stride,
-    int width, int height, int64_t *M, int64_t *H, int bit_depth_divider) {
-  const int wiener_win = 7;
-  const int wiener_win2 = wiener_win * wiener_win;
-
-  // Use a predicate to compute the last columns of the block for H.
-  svbool_t pattern = svwhilelt_b16_u32(0, width % 8 == 0 ? 8 : width % 8);
-
-  // Use intermediate matrices for H and M to perform the computation, they
-  // will be accumulated into the original H and M at the end.
-  int64_t M_trn[49];
-  memset(M_trn, 0, sizeof(M_trn));
-
-  int64_t H_tmp[49 * 49];
-  memset(H_tmp, 0, sizeof(H_tmp));
-
-  do {
-    // Cross-correlation (M).
-    for (int row = 0; row < wiener_win; row++) {
-      int j = 0;
-      while (j < width) {
-        int16x8_t dgd[7];
-        load_s16_8x7(dgd_avg + row * dgd_avg_stride + j, 1, &dgd[0], &dgd[1],
-                     &dgd[2], &dgd[3], &dgd[4], &dgd[5], &dgd[6]);
-        int16x8_t s = vld1q_s16(src_avg + j);
-
-        // Compute all the elements of one row of M.
-        compute_M_one_row_win7(s, dgd, M_trn, row);
-
-        j += 8;
-      }
-    }
-
-    // Auto-covariance (H).
-    int j = 0;
-    while (j < width - 8) {
-      for (int col0 = 0; col0 < wiener_win; col0++) {
-        int16x8_t dgd0[7];
-        load_s16_8x7(dgd_avg + j + col0, dgd_avg_stride, &dgd0[0], &dgd0[1],
-                     &dgd0[2], &dgd0[3], &dgd0[4], &dgd0[5], &dgd0[6]);
-
-        // Perform computation of the first column with itself (28 elements).
-        // For the first column this will fill the upper triangle of the 7x7
-        // matrix at the top left of the H matrix. For the next columns this
-        // will fill the upper triangle of the other 7x7 matrices around H's
-        // diagonal.
-        compute_H_one_col(dgd0, col0, H_tmp, wiener_win, wiener_win2);
-
-        // All computation next to the matrix diagonal has already been done.
-        for (int col1 = col0 + 1; col1 < wiener_win; col1++) {
-          // Load second column and scale based on downsampling factor.
-          int16x8_t dgd1[7];
-          load_s16_8x7(dgd_avg + j + col1, dgd_avg_stride, &dgd1[0], &dgd1[1],
-                       &dgd1[2], &dgd1[3], &dgd1[4], &dgd1[5], &dgd1[6]);
-
-          // Compute all elements from the combination of both columns (49
-          // elements).
-          compute_H_two_rows_win7(dgd0, dgd1, col0, col1, H_tmp);
-        }
-      }
-      j += 8;
-    }
-
-    // Process remaining columns using a predicate to discard excess elements.
-    for (int col0 = 0; col0 < wiener_win; col0++) {
-      // Load first column.
-      int16x8_t dgd0[7];
-      dgd0[0] = svget_neonq_s16(
-          svld1_s16(pattern, dgd_avg + 0 * dgd_avg_stride + j + col0));
-      dgd0[1] = svget_neonq_s16(
-          svld1_s16(pattern, dgd_avg + 1 * dgd_avg_stride + j + col0));
-      dgd0[2] = svget_neonq_s16(
-          svld1_s16(pattern, dgd_avg + 2 * dgd_avg_stride + j + col0));
-      dgd0[3] = svget_neonq_s16(
-          svld1_s16(pattern, dgd_avg + 3 * dgd_avg_stride + j + col0));
-      dgd0[4] = svget_neonq_s16(
-          svld1_s16(pattern, dgd_avg + 4 * dgd_avg_stride + j + col0));
-      dgd0[5] = svget_neonq_s16(
-          svld1_s16(pattern, dgd_avg + 5 * dgd_avg_stride + j + col0));
-      dgd0[6] = svget_neonq_s16(
-          svld1_s16(pattern, dgd_avg + 6 * dgd_avg_stride + j + col0));
-
-      // Perform computation of the first column with itself (28 elements).
-      // For the first column this will fill the upper triangle of the 7x7
-      // matrix at the top left of the H matrix. For the next columns this
-      // will fill the upper triangle of the other 7x7 matrices around H's
-      // diagonal.
-      compute_H_one_col(dgd0, col0, H_tmp, wiener_win, wiener_win2);
-
-      // All computation next to the matrix diagonal has already been done.
-      for (int col1 = col0 + 1; col1 < wiener_win; col1++) {
-        // Load second column and scale based on downsampling factor.
-        int16x8_t dgd1[7];
-        load_s16_8x7(dgd_avg + j + col1, dgd_avg_stride, &dgd1[0], &dgd1[1],
-                     &dgd1[2], &dgd1[3], &dgd1[4], &dgd1[5], &dgd1[6]);
-
-        // Compute all elements from the combination of both columns (49
-        // elements).
-        compute_H_two_rows_win7(dgd0, dgd1, col0, col1, H_tmp);
-      }
-    }
-    dgd_avg += dgd_avg_stride;
-    src_avg += src_avg_stride;
-  } while (--height != 0);
-
-  // Transpose M_trn.
-  acc_transpose_M(M, M_trn, 7, bit_depth_divider);
-
-  // Copy upper triangle of H in the lower one.
-  copy_upper_triangle(H, H_tmp, wiener_win2, bit_depth_divider);
-}
-
-// This function computes two matrices: the cross-correlation between the src
-// buffer and dgd buffer (M), and the auto-covariance of the dgd buffer (H).
-//
-// M is of size 5 * 5. It needs to be filled such that multiplying one element
-// from src with each element of a row of the wiener window will fill one
-// column of M. However this is not very convenient in terms of memory
-// accesses, as it means we do contiguous loads of dgd but strided stores to M.
-// As a result, we use an intermediate matrix M_trn which is instead filled
-// such that one row of the wiener window gives one row of M_trn. Once fully
-// computed, M_trn is then transposed to return M.
-//
-// H is of size 25 * 25. It is filled by multiplying every pair of elements of
-// the wiener window together. Since it is a symmetric matrix, we only compute
-// the upper triangle, and then copy it down to the lower one. Here we fill it
-// by taking each different pair of columns, and multiplying all the elements of
-// the first one with all the elements of the second one, with a special case
-// when multiplying a column by itself.
-static inline void highbd_compute_stats_win5_sve(
-    int16_t *dgd_avg, int dgd_avg_stride, int16_t *src_avg, int src_avg_stride,
-    int width, int height, int64_t *M, int64_t *H, int bit_depth_divider) {
-  const int wiener_win = 5;
-  const int wiener_win2 = wiener_win * wiener_win;
-
-  // Use a predicate to compute the last columns of the block for H.
-  svbool_t pattern = svwhilelt_b16_u32(0, width % 8 == 0 ? 8 : width % 8);
-
-  // Use intermediate matrices for H and M to perform the computation, they
-  // will be accumulated into the original H and M at the end.
-  int64_t M_trn[25];
-  memset(M_trn, 0, sizeof(M_trn));
-
-  int64_t H_tmp[25 * 25];
-  memset(H_tmp, 0, sizeof(H_tmp));
-
-  do {
-    // Cross-correlation (M).
-    for (int row = 0; row < wiener_win; row++) {
-      int j = 0;
-      while (j < width) {
-        int16x8_t dgd[5];
-        load_s16_8x5(dgd_avg + row * dgd_avg_stride + j, 1, &dgd[0], &dgd[1],
-                     &dgd[2], &dgd[3], &dgd[4]);
-        int16x8_t s = vld1q_s16(src_avg + j);
-
-        // Compute all the elements of one row of M.
-        compute_M_one_row_win5(s, dgd, M_trn, row);
-
-        j += 8;
-      }
-    }
-
-    // Auto-covariance (H).
-    int j = 0;
-    while (j < width - 8) {
-      for (int col0 = 0; col0 < wiener_win; col0++) {
-        // Load first column.
-        int16x8_t dgd0[5];
-        load_s16_8x5(dgd_avg + j + col0, dgd_avg_stride, &dgd0[0], &dgd0[1],
-                     &dgd0[2], &dgd0[3], &dgd0[4]);
-
-        // Perform computation of the first column with itself (15 elements).
-        // For the first column this will fill the upper triangle of the 5x5
-        // matrix at the top left of the H matrix. For the next columns this
-        // will fill the upper triangle of the other 5x5 matrices around H's
-        // diagonal.
-        compute_H_one_col(dgd0, col0, H_tmp, wiener_win, wiener_win2);
-
-        // All computation next to the matrix diagonal has already been done.
-        for (int col1 = col0 + 1; col1 < wiener_win; col1++) {
-          // Load second column and scale based on downsampling factor.
-          int16x8_t dgd1[5];
-          load_s16_8x5(dgd_avg + j + col1, dgd_avg_stride, &dgd1[0], &dgd1[1],
-                       &dgd1[2], &dgd1[3], &dgd1[4]);
-
-          // Compute all elements from the combination of both columns (25
-          // elements).
-          compute_H_two_rows_win5(dgd0, dgd1, col0, col1, H_tmp);
-        }
-      }
-      j += 8;
-    }
-
-    // Process remaining columns using a predicate to discard excess elements.
-    for (int col0 = 0; col0 < wiener_win; col0++) {
-      int16x8_t dgd0[5];
-      dgd0[0] = svget_neonq_s16(
-          svld1_s16(pattern, dgd_avg + 0 * dgd_avg_stride + j + col0));
-      dgd0[1] = svget_neonq_s16(
-          svld1_s16(pattern, dgd_avg + 1 * dgd_avg_stride + j + col0));
-      dgd0[2] = svget_neonq_s16(
-          svld1_s16(pattern, dgd_avg + 2 * dgd_avg_stride + j + col0));
-      dgd0[3] = svget_neonq_s16(
-          svld1_s16(pattern, dgd_avg + 3 * dgd_avg_stride + j + col0));
-      dgd0[4] = svget_neonq_s16(
-          svld1_s16(pattern, dgd_avg + 4 * dgd_avg_stride + j + col0));
-
-      // Perform computation of the first column with itself (15 elements).
-      // For the first column this will fill the upper triangle of the 5x5
-      // matrix at the top left of the H matrix. For the next columns this
-      // will fill the upper triangle of the other 5x5 matrices around H's
-      // diagonal.
-      compute_H_one_col(dgd0, col0, H_tmp, wiener_win, wiener_win2);
-
-      // All computation next to the matrix diagonal has already been done.
-      for (int col1 = col0 + 1; col1 < wiener_win; col1++) {
-        // Load second column and scale based on downsampling factor.
-        int16x8_t dgd1[5];
-        load_s16_8x5(dgd_avg + j + col1, dgd_avg_stride, &dgd1[0], &dgd1[1],
-                     &dgd1[2], &dgd1[3], &dgd1[4]);
-
-        // Compute all elements from the combination of both columns (25
-        // elements).
-        compute_H_two_rows_win5(dgd0, dgd1, col0, col1, H_tmp);
-      }
-    }
-    dgd_avg += dgd_avg_stride;
-    src_avg += src_avg_stride;
-  } while (--height != 0);
-
-  // Transpose M_trn.
-  acc_transpose_M(M, M_trn, 5, bit_depth_divider);
-
-  // Copy upper triangle of H in the lower one.
-  copy_upper_triangle(H, H_tmp, wiener_win2, bit_depth_divider);
-}
-
-void av1_compute_stats_highbd_sve(int wiener_win, const uint8_t *dgd8,
+void av1_compute_stats_highbd_sve(int32_t wiener_win, const uint8_t *dgd8,
                                   const uint8_t *src8, int16_t *dgd_avg,
-                                  int16_t *src_avg, int h_start, int h_end,
-                                  int v_start, int v_end, int dgd_stride,
-                                  int src_stride, int64_t *M, int64_t *H,
+                                  int16_t *src_avg, int32_t h_start,
+                                  int32_t h_end, int32_t v_start, int32_t v_end,
+                                  int32_t dgd_stride, int32_t src_stride,
+                                  int64_t *M, int64_t *H,
                                   aom_bit_depth_t bit_depth) {
-  assert(wiener_win == WIENER_WIN || wiener_win == WIENER_WIN_CHROMA);
-
+  const int32_t wiener_win2 = wiener_win * wiener_win;
+  const int32_t wiener_halfwin = (wiener_win >> 1);
   const uint16_t *src = CONVERT_TO_SHORTPTR(src8);
   const uint16_t *dgd = CONVERT_TO_SHORTPTR(dgd8);
-  const int wiener_win2 = wiener_win * wiener_win;
-  const int wiener_halfwin = wiener_win >> 1;
   const int32_t width = h_end - h_start;
   const int32_t height = v_end - v_start;
+  const int32_t d_stride = (width + 2 * wiener_halfwin + 15) & ~15;
+  const int32_t s_stride = (width + 15) & ~15;
+
+  const uint16_t *dgd_start = dgd + h_start + v_start * dgd_stride;
+  const uint16_t *src_start = src + h_start + v_start * src_stride;
+  const uint16_t avg =
+      highbd_find_average_sve(dgd_start, dgd_stride, width, height);
 
-  uint8_t bit_depth_divider = 1;
-  if (bit_depth == AOM_BITS_12)
-    bit_depth_divider = 16;
-  else if (bit_depth == AOM_BITS_10)
-    bit_depth_divider = 4;
+  sub_avg_block_highbd_sve(src_start, src_stride, avg, width, height, src_avg,
+                           s_stride);
+  sub_avg_block_highbd_sve(
+      dgd + (v_start - wiener_halfwin) * dgd_stride + h_start - wiener_halfwin,
+      dgd_stride, avg, width + 2 * wiener_halfwin, height + 2 * wiener_halfwin,
+      dgd_avg, d_stride);
 
-  const uint16_t *dgd_start = &dgd[v_start * dgd_stride + h_start];
-  memset(H, 0, sizeof(*H) * wiener_win2 * wiener_win2);
-  memset(M, 0, sizeof(*M) * wiener_win * wiener_win);
+  if (wiener_win == WIENER_WIN) {
+    compute_stats_win7_sve(dgd_avg, d_stride, src_avg, s_stride, width, height,
+                           M, H);
+  } else {
+    assert(wiener_win == WIENER_WIN_CHROMA);
+    compute_stats_win5_sve(dgd_avg, d_stride, src_avg, s_stride, width, height,
+                           M, H);
+  }
 
-  const uint16_t avg = find_average_sve(dgd_start, dgd_stride, width, height);
+  // H is a symmetric matrix, so we only need to fill out the upper triangle.
+  // We can copy it down to the lower triangle outside the (i, j) loops.
+  if (bit_depth == AOM_BITS_8) {
+    diagonal_copy_stats_neon(wiener_win2, H);
+  } else if (bit_depth == AOM_BITS_10) {  // bit_depth == EB_TEN_BIT
+    const int32_t k4 = wiener_win2 & ~3;
+
+    int32_t k = 0;
+    do {
+      int64x2_t dst = div4_neon(vld1q_s64(M + k));
+      vst1q_s64(M + k, dst);
+      dst = div4_neon(vld1q_s64(M + k + 2));
+      vst1q_s64(M + k + 2, dst);
+      H[k * wiener_win2 + k] /= 4;
+      k += 4;
+    } while (k < k4);
+
+    H[k * wiener_win2 + k] /= 4;
+
+    for (; k < wiener_win2; ++k) {
+      M[k] /= 4;
+    }
 
-  // dgd_avg and src_avg have been memset to zero before calling this function
-  // so round up the stride to the next multiple of 8 so that we don't have to
-  // worry about a tail loop when computing M.
-  const int dgd_avg_stride = ((width + 2 * wiener_halfwin) & ~7) + 8;
-  const int src_avg_stride = (width & ~7) + 8;
+    div4_diagonal_copy_stats_neon(wiener_win2, H);
+  } else {  // bit_depth == AOM_BITS_12
+    const int32_t k4 = wiener_win2 & ~3;
 
-  // Compute (dgd - avg) and store it in dgd_avg.
-  // The wiener window will slide along the dgd frame, centered on each pixel.
-  // For the top left pixel and all the pixels on the side of the frame this
-  // means half of the window will be outside of the frame. As such the actual
-  // buffer that we need to subtract the avg from will be 2 * wiener_halfwin
-  // wider and 2 * wiener_halfwin higher than the original dgd buffer.
-  const int vert_offset = v_start - wiener_halfwin;
-  const int horiz_offset = h_start - wiener_halfwin;
-  const uint16_t *dgd_win = dgd + horiz_offset + vert_offset * dgd_stride;
-  compute_sub_avg(dgd_win, dgd_stride, avg, dgd_avg, dgd_avg_stride,
-                  width + 2 * wiener_halfwin, height + 2 * wiener_halfwin);
+    int32_t k = 0;
+    do {
+      int64x2_t dst = div16_neon(vld1q_s64(M + k));
+      vst1q_s64(M + k, dst);
+      dst = div16_neon(vld1q_s64(M + k + 2));
+      vst1q_s64(M + k + 2, dst);
+      H[k * wiener_win2 + k] /= 16;
+      k += 4;
+    } while (k < k4);
 
-  // Compute (src - avg), downsample if necessary and store in src-avg.
-  const uint16_t *src_start = src + h_start + v_start * src_stride;
-  compute_sub_avg(src_start, src_stride, avg, src_avg, src_avg_stride, width,
-                  height);
+    H[k * wiener_win2 + k] /= 16;
 
-  if (wiener_win == WIENER_WIN) {
-    highbd_compute_stats_win7_sve(dgd_avg, dgd_avg_stride, src_avg,
-                                  src_avg_stride, width, height, M, H,
-                                  bit_depth_divider);
-  } else {
-    highbd_compute_stats_win5_sve(dgd_avg, dgd_avg_stride, src_avg,
-                                  src_avg_stride, width, height, M, H,
-                                  bit_depth_divider);
+    for (; k < wiener_win2; ++k) {
+      M[k] /= 16;
+    }
+
+    div16_diagonal_copy_stats_neon(wiener_win2, H);
   }
 }
diff --git a/av1/encoder/arm/pickrst_neon.c b/av1/encoder/arm/pickrst_neon.c
index 63551cc55..f83e693fb 100644
--- a/av1/encoder/arm/pickrst_neon.c
+++ b/av1/encoder/arm/pickrst_neon.c
@@ -14,7 +14,9 @@
 #include "config/aom_config.h"
 #include "config/av1_rtcd.h"
 
+#include "aom_dsp/arm/mem_neon.h"
 #include "aom_dsp/arm/sum_neon.h"
+#include "aom_dsp/arm/transpose_neon.h"
 #include "av1/common/restoration.h"
 #include "av1/encoder/arm/pickrst_neon.h"
 #include "av1/encoder/pickrst.h"
@@ -292,11 +294,10 @@ static inline void load_and_pack_u8_8x7(uint8x16_t dst[4], const uint8_t *src,
   dst[3] = vcombine_u8(vld1_u8(src - 1), vdup_n_u8(0));
 }
 
-static inline void compute_stats_win7_neon(const uint8_t *dgd,
-                                           const uint8_t *src, int width,
-                                           int height, int dgd_stride,
-                                           int src_stride, int avg, int64_t *M,
-                                           int64_t *H, int downsample_factor) {
+static inline void compute_stats_win7_downsampled_neon(
+    const uint8_t *dgd, const uint8_t *src, int width, int height,
+    int dgd_stride, int src_stride, int avg, int64_t *M, int64_t *H,
+    int downsample_factor) {
   // Matrix names are capitalized to help readability.
   DECLARE_ALIGNED(64, int16_t, DGD_AVG0[WIENER_WIN2_ALIGN3]);
   DECLARE_ALIGNED(64, int16_t, DGD_AVG1[WIENER_WIN2_ALIGN3]);
@@ -589,11 +590,10 @@ static inline void load_and_pack_u8_6x5(uint8x16_t dst[3], const uint8_t *src,
   dst[2] = vcombine_u8(vld1_u8(src - 3), vdup_n_u8(0));
 }
 
-static inline void compute_stats_win5_neon(const uint8_t *dgd,
-                                           const uint8_t *src, int width,
-                                           int height, int dgd_stride,
-                                           int src_stride, int avg, int64_t *M,
-                                           int64_t *H, int downsample_factor) {
+static inline void compute_stats_win5_downsampled_neon(
+    const uint8_t *dgd, const uint8_t *src, int width, int height,
+    int dgd_stride, int src_stride, int avg, int64_t *M, int64_t *H,
+    int downsample_factor) {
   // Matrix names are capitalized to help readability.
   DECLARE_ALIGNED(64, int16_t, DGD_AVG0[WIENER_WIN2_REDUCED_ALIGN3]);
   DECLARE_ALIGNED(64, int16_t, DGD_AVG1[WIENER_WIN2_REDUCED_ALIGN3]);
@@ -826,6 +826,1291 @@ static inline void compute_stats_win5_neon(const uint8_t *dgd,
            downsample_factor);
 }
 
+static inline void hadd_update_6_stats_neon(const int64_t *const src,
+                                            const int32x4_t *deltas,
+                                            int64_t *const dst) {
+  int32x4_t delta01 = horizontal_add_2d_s32(deltas[0], deltas[1]);
+  int32x4_t delta23 = horizontal_add_2d_s32(deltas[2], deltas[3]);
+  int32x4_t delta45 = horizontal_add_2d_s32(deltas[4], deltas[5]);
+
+  int64x2_t delta01_s64 = vpaddlq_s32(delta01);
+  int64x2_t delta23_s64 = vpaddlq_s32(delta23);
+  int64x2_t delta45_s64 = vpaddlq_s32(delta45);
+
+  int64x2_t src0 = vld1q_s64(src);
+  int64x2_t src1 = vld1q_s64(src + 2);
+  int64x2_t src2 = vld1q_s64(src + 4);
+
+  vst1q_s64(dst, vaddq_s64(src0, delta01_s64));
+  vst1q_s64(dst + 2, vaddq_s64(src1, delta23_s64));
+  vst1q_s64(dst + 4, vaddq_s64(src2, delta45_s64));
+}
+
+static inline void hadd_update_4_stats_neon(const int64_t *const src,
+                                            const int32x4_t *deltas,
+                                            int64_t *const dst) {
+  int32x4_t delta01 = horizontal_add_2d_s32(deltas[0], deltas[1]);
+  int32x4_t delta23 = horizontal_add_2d_s32(deltas[2], deltas[3]);
+  int64x2_t delta01_s64 = vpaddlq_s32(delta01);
+  int64x2_t delta23_s64 = vpaddlq_s32(delta23);
+
+  int64x2_t src0 = vld1q_s64(src);
+  int64x2_t src1 = vld1q_s64(src + 2);
+  vst1q_s64(dst, vaddq_s64(src0, delta01_s64));
+  vst1q_s64(dst + 2, vaddq_s64(src1, delta23_s64));
+}
+
+static inline void compute_stats_win5_neon(
+    const int16_t *const d, const int32_t d_stride, const int16_t *const s,
+    const int32_t s_stride, const int32_t width, const int32_t height,
+    int64_t *const M, int64_t *const H) {
+  const int32_t wiener_win = WIENER_WIN_CHROMA;
+  const int32_t wiener_win2 = wiener_win * wiener_win;
+  const int32_t w16 = width & ~15;
+  const int32_t h8 = height & ~7;
+  int16x8_t mask[2];
+  mask[0] = vld1q_s16(&(mask_16bit[16]) - width % 16);
+  mask[1] = vld1q_s16(&(mask_16bit[16]) - width % 16 + 8);
+  const int bit_depth = 8;
+  int32_t i, j, x, y;
+
+  const int32_t num_bit_left =
+      32 - 1 /* sign */ - 2 * bit_depth /* energy */ + 2 /* SIMD */;
+  const int32_t h_allowed =
+      (1 << num_bit_left) / (w16 + ((w16 != width) ? 16 : 0));
+
+  // Step 1: Calculate the top edge of the whole matrix, i.e., the top
+  // edge of each triangle and square on the top row.
+  j = 0;
+  do {
+    const int16_t *s_t = s;
+    const int16_t *d_t = d;
+    int32_t height_t = 0;
+    int64x2_t sum_m[WIENER_WIN_CHROMA] = { vdupq_n_s64(0) };
+    int64x2_t sum_h[WIENER_WIN_CHROMA] = { vdupq_n_s64(0) };
+    int16x8_t src[2], dgd[2];
+
+    do {
+      const int32_t h_t =
+          ((height - height_t) < h_allowed) ? (height - height_t) : h_allowed;
+      int32x4_t row_m[WIENER_WIN_CHROMA] = { vdupq_n_s32(0) };
+      int32x4_t row_h[WIENER_WIN_CHROMA] = { vdupq_n_s32(0) };
+
+      y = h_t;
+      do {
+        x = 0;
+        while (x < w16) {
+          src[0] = vld1q_s16(s_t + x + 0);
+          src[1] = vld1q_s16(s_t + x + 8);
+          dgd[0] = vld1q_s16(d_t + x + 0);
+          dgd[1] = vld1q_s16(d_t + x + 8);
+          stats_top_win5_neon(src, dgd, d_t + j + x, d_stride, row_m, row_h);
+          x += 16;
+        }
+
+        if (w16 != width) {
+          src[0] = vld1q_s16(s_t + w16 + 0);
+          src[1] = vld1q_s16(s_t + w16 + 8);
+          dgd[0] = vld1q_s16(d_t + w16 + 0);
+          dgd[1] = vld1q_s16(d_t + w16 + 8);
+          src[0] = vandq_s16(src[0], mask[0]);
+          src[1] = vandq_s16(src[1], mask[1]);
+          dgd[0] = vandq_s16(dgd[0], mask[0]);
+          dgd[1] = vandq_s16(dgd[1], mask[1]);
+          stats_top_win5_neon(src, dgd, d_t + j + w16, d_stride, row_m, row_h);
+        }
+
+        s_t += s_stride;
+        d_t += d_stride;
+      } while (--y);
+
+      sum_m[0] = vpadalq_s32(sum_m[0], row_m[0]);
+      sum_m[1] = vpadalq_s32(sum_m[1], row_m[1]);
+      sum_m[2] = vpadalq_s32(sum_m[2], row_m[2]);
+      sum_m[3] = vpadalq_s32(sum_m[3], row_m[3]);
+      sum_m[4] = vpadalq_s32(sum_m[4], row_m[4]);
+      sum_h[0] = vpadalq_s32(sum_h[0], row_h[0]);
+      sum_h[1] = vpadalq_s32(sum_h[1], row_h[1]);
+      sum_h[2] = vpadalq_s32(sum_h[2], row_h[2]);
+      sum_h[3] = vpadalq_s32(sum_h[3], row_h[3]);
+      sum_h[4] = vpadalq_s32(sum_h[4], row_h[4]);
+
+      height_t += h_t;
+    } while (height_t < height);
+
+#if AOM_ARCH_AARCH64
+    int64x2_t sum_m0 = vpaddq_s64(sum_m[0], sum_m[1]);
+    int64x2_t sum_m2 = vpaddq_s64(sum_m[2], sum_m[3]);
+    vst1q_s64(&M[wiener_win * j + 0], sum_m0);
+    vst1q_s64(&M[wiener_win * j + 2], sum_m2);
+    M[wiener_win * j + 4] = vaddvq_s64(sum_m[4]);
+
+    int64x2_t sum_h0 = vpaddq_s64(sum_h[0], sum_h[1]);
+    int64x2_t sum_h2 = vpaddq_s64(sum_h[2], sum_h[3]);
+    vst1q_s64(&H[wiener_win * j + 0], sum_h0);
+    vst1q_s64(&H[wiener_win * j + 2], sum_h2);
+    H[wiener_win * j + 4] = vaddvq_s64(sum_h[4]);
+#else
+    M[wiener_win * j + 0] = horizontal_add_s64x2(sum_m[0]);
+    M[wiener_win * j + 1] = horizontal_add_s64x2(sum_m[1]);
+    M[wiener_win * j + 2] = horizontal_add_s64x2(sum_m[2]);
+    M[wiener_win * j + 3] = horizontal_add_s64x2(sum_m[3]);
+    M[wiener_win * j + 4] = horizontal_add_s64x2(sum_m[4]);
+
+    H[wiener_win * j + 0] = horizontal_add_s64x2(sum_h[0]);
+    H[wiener_win * j + 1] = horizontal_add_s64x2(sum_h[1]);
+    H[wiener_win * j + 2] = horizontal_add_s64x2(sum_h[2]);
+    H[wiener_win * j + 3] = horizontal_add_s64x2(sum_h[3]);
+    H[wiener_win * j + 4] = horizontal_add_s64x2(sum_h[4]);
+#endif  // AOM_ARCH_AARCH64
+  } while (++j < wiener_win);
+
+  // Step 2: Calculate the left edge of each square on the top row.
+  j = 1;
+  do {
+    const int16_t *d_t = d;
+    int32_t height_t = 0;
+    int64x2_t sum_h[WIENER_WIN_CHROMA - 1] = { vdupq_n_s64(0) };
+    int16x8_t dgd[2];
+
+    do {
+      const int32_t h_t =
+          ((height - height_t) < h_allowed) ? (height - height_t) : h_allowed;
+      int32x4_t row_h[WIENER_WIN_CHROMA - 1] = { vdupq_n_s32(0) };
+
+      y = h_t;
+      do {
+        x = 0;
+        while (x < w16) {
+          dgd[0] = vld1q_s16(d_t + j + x + 0);
+          dgd[1] = vld1q_s16(d_t + j + x + 8);
+          stats_left_win5_neon(dgd, d_t + x, d_stride, row_h);
+          x += 16;
+        }
+
+        if (w16 != width) {
+          dgd[0] = vld1q_s16(d_t + j + x + 0);
+          dgd[1] = vld1q_s16(d_t + j + x + 8);
+          dgd[0] = vandq_s16(dgd[0], mask[0]);
+          dgd[1] = vandq_s16(dgd[1], mask[1]);
+          stats_left_win5_neon(dgd, d_t + x, d_stride, row_h);
+        }
+
+        d_t += d_stride;
+      } while (--y);
+
+      sum_h[0] = vpadalq_s32(sum_h[0], row_h[0]);
+      sum_h[1] = vpadalq_s32(sum_h[1], row_h[1]);
+      sum_h[2] = vpadalq_s32(sum_h[2], row_h[2]);
+      sum_h[3] = vpadalq_s32(sum_h[3], row_h[3]);
+
+      height_t += h_t;
+    } while (height_t < height);
+
+#if AOM_ARCH_AARCH64
+    int64x2_t sum_h0 = vpaddq_s64(sum_h[0], sum_h[1]);
+    int64x2_t sum_h1 = vpaddq_s64(sum_h[2], sum_h[3]);
+    vst1_s64(&H[1 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h0));
+    vst1_s64(&H[2 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h0));
+    vst1_s64(&H[3 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h1));
+    vst1_s64(&H[4 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h1));
+#else
+    H[1 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[0]);
+    H[2 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[1]);
+    H[3 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[2]);
+    H[4 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[3]);
+#endif  // AOM_ARCH_AARCH64
+  } while (++j < wiener_win);
+
+  // Step 3: Derive the top edge of each triangle along the diagonal. No
+  // triangle in top row.
+  {
+    const int16_t *d_t = d;
+
+    if (height % 2) {
+      int32x4_t deltas[(WIENER_WIN + 1) * 2] = { vdupq_n_s32(0) };
+      int32x4_t deltas_tr[(WIENER_WIN + 1) * 2] = { vdupq_n_s32(0) };
+      int16x8_t ds[WIENER_WIN * 2];
+
+      load_s16_8x4(d_t, d_stride, &ds[0], &ds[2], &ds[4], &ds[6]);
+      load_s16_8x4(d_t + width, d_stride, &ds[1], &ds[3], &ds[5], &ds[7]);
+      d_t += 4 * d_stride;
+
+      step3_win5_oneline_neon(&d_t, d_stride, width, height, ds, deltas);
+      transpose_arrays_s32_8x8(deltas, deltas_tr);
+
+      update_5_stats_neon(H + 0 * wiener_win * wiener_win2 + 0 * wiener_win,
+                          deltas_tr[0], vgetq_lane_s32(deltas_tr[4], 0),
+                          H + 1 * wiener_win * wiener_win2 + 1 * wiener_win);
+
+      update_5_stats_neon(H + 1 * wiener_win * wiener_win2 + 1 * wiener_win,
+                          deltas_tr[1], vgetq_lane_s32(deltas_tr[5], 0),
+                          H + 2 * wiener_win * wiener_win2 + 2 * wiener_win);
+
+      update_5_stats_neon(H + 2 * wiener_win * wiener_win2 + 2 * wiener_win,
+                          deltas_tr[2], vgetq_lane_s32(deltas_tr[6], 0),
+                          H + 3 * wiener_win * wiener_win2 + 3 * wiener_win);
+
+      update_5_stats_neon(H + 3 * wiener_win * wiener_win2 + 3 * wiener_win,
+                          deltas_tr[3], vgetq_lane_s32(deltas_tr[7], 0),
+                          H + 4 * wiener_win * wiener_win2 + 4 * wiener_win);
+
+    } else {
+      int32x4_t deltas[WIENER_WIN_CHROMA * 2] = { vdupq_n_s32(0) };
+      int16x8_t ds[WIENER_WIN_CHROMA * 2];
+
+      ds[0] = load_unaligned_s16_4x2(d_t + 0 * d_stride, width);
+      ds[1] = load_unaligned_s16_4x2(d_t + 1 * d_stride, width);
+      ds[2] = load_unaligned_s16_4x2(d_t + 2 * d_stride, width);
+      ds[3] = load_unaligned_s16_4x2(d_t + 3 * d_stride, width);
+
+      step3_win5_neon(d_t + 4 * d_stride, d_stride, width, height, ds, deltas);
+
+      transpose_elems_inplace_s32_4x4(&deltas[0], &deltas[1], &deltas[2],
+                                      &deltas[3]);
+
+      update_5_stats_neon(H + 0 * wiener_win * wiener_win2 + 0 * wiener_win,
+                          deltas[0], vgetq_lane_s32(deltas[4], 0),
+                          H + 1 * wiener_win * wiener_win2 + 1 * wiener_win);
+
+      update_5_stats_neon(H + 1 * wiener_win * wiener_win2 + 1 * wiener_win,
+                          deltas[1], vgetq_lane_s32(deltas[4], 1),
+                          H + 2 * wiener_win * wiener_win2 + 2 * wiener_win);
+
+      update_5_stats_neon(H + 2 * wiener_win * wiener_win2 + 2 * wiener_win,
+                          deltas[2], vgetq_lane_s32(deltas[4], 2),
+                          H + 3 * wiener_win * wiener_win2 + 3 * wiener_win);
+
+      update_5_stats_neon(H + 3 * wiener_win * wiener_win2 + 3 * wiener_win,
+                          deltas[3], vgetq_lane_s32(deltas[4], 3),
+                          H + 4 * wiener_win * wiener_win2 + 4 * wiener_win);
+    }
+  }
+
+  // Step 4: Derive the top and left edge of each square. No square in top and
+  // bottom row.
+
+  {
+    y = h8;
+
+    int16x4_t d_s[12];
+    int16x4_t d_e[12];
+    const int16_t *d_t = d;
+    int16x4_t zeros = vdup_n_s16(0);
+    load_s16_4x4(d_t, d_stride, &d_s[0], &d_s[1], &d_s[2], &d_s[3]);
+    load_s16_4x4(d_t + width, d_stride, &d_e[0], &d_e[1], &d_e[2], &d_e[3]);
+    int32x4_t deltas[6][18] = { { vdupq_n_s32(0) }, { vdupq_n_s32(0) } };
+
+    while (y >= 8) {
+      load_s16_4x8(d_t + 4 * d_stride, d_stride, &d_s[4], &d_s[5], &d_s[6],
+                   &d_s[7], &d_s[8], &d_s[9], &d_s[10], &d_s[11]);
+      load_s16_4x8(d_t + width + 4 * d_stride, d_stride, &d_e[4], &d_e[5],
+                   &d_e[6], &d_e[7], &d_e[8], &d_e[9], &d_e[10], &d_e[11]);
+
+      int16x8_t s_tr[8], e_tr[8];
+      transpose_elems_s16_4x8(d_s[0], d_s[1], d_s[2], d_s[3], d_s[4], d_s[5],
+                              d_s[6], d_s[7], &s_tr[0], &s_tr[1], &s_tr[2],
+                              &s_tr[3]);
+      transpose_elems_s16_4x8(d_s[8], d_s[9], d_s[10], d_s[11], zeros, zeros,
+                              zeros, zeros, &s_tr[4], &s_tr[5], &s_tr[6],
+                              &s_tr[7]);
+
+      transpose_elems_s16_4x8(d_e[0], d_e[1], d_e[2], d_e[3], d_e[4], d_e[5],
+                              d_e[6], d_e[7], &e_tr[0], &e_tr[1], &e_tr[2],
+                              &e_tr[3]);
+      transpose_elems_s16_4x8(d_e[8], d_e[9], d_e[10], d_e[11], zeros, zeros,
+                              zeros, zeros, &e_tr[4], &e_tr[5], &e_tr[6],
+                              &e_tr[7]);
+
+      int16x8_t start_col0[5], start_col1[5], start_col2[5], start_col3[5];
+      start_col0[0] = s_tr[0];
+      start_col0[1] = vextq_s16(s_tr[0], s_tr[4], 1);
+      start_col0[2] = vextq_s16(s_tr[0], s_tr[4], 2);
+      start_col0[3] = vextq_s16(s_tr[0], s_tr[4], 3);
+      start_col0[4] = vextq_s16(s_tr[0], s_tr[4], 4);
+
+      start_col1[0] = s_tr[1];
+      start_col1[1] = vextq_s16(s_tr[1], s_tr[5], 1);
+      start_col1[2] = vextq_s16(s_tr[1], s_tr[5], 2);
+      start_col1[3] = vextq_s16(s_tr[1], s_tr[5], 3);
+      start_col1[4] = vextq_s16(s_tr[1], s_tr[5], 4);
+
+      start_col2[0] = s_tr[2];
+      start_col2[1] = vextq_s16(s_tr[2], s_tr[6], 1);
+      start_col2[2] = vextq_s16(s_tr[2], s_tr[6], 2);
+      start_col2[3] = vextq_s16(s_tr[2], s_tr[6], 3);
+      start_col2[4] = vextq_s16(s_tr[2], s_tr[6], 4);
+
+      start_col3[0] = s_tr[3];
+      start_col3[1] = vextq_s16(s_tr[3], s_tr[7], 1);
+      start_col3[2] = vextq_s16(s_tr[3], s_tr[7], 2);
+      start_col3[3] = vextq_s16(s_tr[3], s_tr[7], 3);
+      start_col3[4] = vextq_s16(s_tr[3], s_tr[7], 4);
+
+      // i = 1, j = 2;
+      sub_deltas_step4(start_col0, start_col1, deltas[0]);
+
+      // i = 1, j = 3;
+      sub_deltas_step4(start_col0, start_col2, deltas[1]);
+
+      // i = 1, j = 4
+      sub_deltas_step4(start_col0, start_col3, deltas[2]);
+
+      // i = 2, j =3
+      sub_deltas_step4(start_col1, start_col2, deltas[3]);
+
+      // i = 2, j = 4
+      sub_deltas_step4(start_col1, start_col3, deltas[4]);
+
+      // i = 3, j = 4
+      sub_deltas_step4(start_col2, start_col3, deltas[5]);
+
+      int16x8_t end_col0[5], end_col1[5], end_col2[5], end_col3[5];
+      end_col0[0] = e_tr[0];
+      end_col0[1] = vextq_s16(e_tr[0], e_tr[4], 1);
+      end_col0[2] = vextq_s16(e_tr[0], e_tr[4], 2);
+      end_col0[3] = vextq_s16(e_tr[0], e_tr[4], 3);
+      end_col0[4] = vextq_s16(e_tr[0], e_tr[4], 4);
+
+      end_col1[0] = e_tr[1];
+      end_col1[1] = vextq_s16(e_tr[1], e_tr[5], 1);
+      end_col1[2] = vextq_s16(e_tr[1], e_tr[5], 2);
+      end_col1[3] = vextq_s16(e_tr[1], e_tr[5], 3);
+      end_col1[4] = vextq_s16(e_tr[1], e_tr[5], 4);
+
+      end_col2[0] = e_tr[2];
+      end_col2[1] = vextq_s16(e_tr[2], e_tr[6], 1);
+      end_col2[2] = vextq_s16(e_tr[2], e_tr[6], 2);
+      end_col2[3] = vextq_s16(e_tr[2], e_tr[6], 3);
+      end_col2[4] = vextq_s16(e_tr[2], e_tr[6], 4);
+
+      end_col3[0] = e_tr[3];
+      end_col3[1] = vextq_s16(e_tr[3], e_tr[7], 1);
+      end_col3[2] = vextq_s16(e_tr[3], e_tr[7], 2);
+      end_col3[3] = vextq_s16(e_tr[3], e_tr[7], 3);
+      end_col3[4] = vextq_s16(e_tr[3], e_tr[7], 4);
+
+      // i = 1, j = 2;
+      add_deltas_step4(end_col0, end_col1, deltas[0]);
+
+      // i = 1, j = 3;
+      add_deltas_step4(end_col0, end_col2, deltas[1]);
+
+      // i = 1, j = 4
+      add_deltas_step4(end_col0, end_col3, deltas[2]);
+
+      // i = 2, j =3
+      add_deltas_step4(end_col1, end_col2, deltas[3]);
+
+      // i = 2, j = 4
+      add_deltas_step4(end_col1, end_col3, deltas[4]);
+
+      // i = 3, j = 4
+      add_deltas_step4(end_col2, end_col3, deltas[5]);
+
+      d_s[0] = d_s[8];
+      d_s[1] = d_s[9];
+      d_s[2] = d_s[10];
+      d_s[3] = d_s[11];
+      d_e[0] = d_e[8];
+      d_e[1] = d_e[9];
+      d_e[2] = d_e[10];
+      d_e[3] = d_e[11];
+
+      d_t += 8 * d_stride;
+      y -= 8;
+    }
+
+    if (h8 != height) {
+      const int16x8_t mask_h = vld1q_s16(&mask_16bit[16] - (height % 8));
+
+      load_s16_4x8(d_t + 4 * d_stride, d_stride, &d_s[4], &d_s[5], &d_s[6],
+                   &d_s[7], &d_s[8], &d_s[9], &d_s[10], &d_s[11]);
+      load_s16_4x8(d_t + width + 4 * d_stride, d_stride, &d_e[4], &d_e[5],
+                   &d_e[6], &d_e[7], &d_e[8], &d_e[9], &d_e[10], &d_e[11]);
+      int16x8_t s_tr[8], e_tr[8];
+      transpose_elems_s16_4x8(d_s[0], d_s[1], d_s[2], d_s[3], d_s[4], d_s[5],
+                              d_s[6], d_s[7], &s_tr[0], &s_tr[1], &s_tr[2],
+                              &s_tr[3]);
+      transpose_elems_s16_4x8(d_s[8], d_s[9], d_s[10], d_s[11], zeros, zeros,
+                              zeros, zeros, &s_tr[4], &s_tr[5], &s_tr[6],
+                              &s_tr[7]);
+      transpose_elems_s16_4x8(d_e[0], d_e[1], d_e[2], d_e[3], d_e[4], d_e[5],
+                              d_e[6], d_e[7], &e_tr[0], &e_tr[1], &e_tr[2],
+                              &e_tr[3]);
+      transpose_elems_s16_4x8(d_e[8], d_e[9], d_e[10], d_e[11], zeros, zeros,
+                              zeros, zeros, &e_tr[4], &e_tr[5], &e_tr[6],
+                              &e_tr[7]);
+
+      int16x8_t start_col0[5], start_col1[5], start_col2[5], start_col3[5];
+      start_col0[0] = vandq_s16(s_tr[0], mask_h);
+      start_col0[1] = vandq_s16(vextq_s16(s_tr[0], s_tr[4], 1), mask_h);
+      start_col0[2] = vandq_s16(vextq_s16(s_tr[0], s_tr[4], 2), mask_h);
+      start_col0[3] = vandq_s16(vextq_s16(s_tr[0], s_tr[4], 3), mask_h);
+      start_col0[4] = vandq_s16(vextq_s16(s_tr[0], s_tr[4], 4), mask_h);
+
+      start_col1[0] = vandq_s16(s_tr[1], mask_h);
+      start_col1[1] = vandq_s16(vextq_s16(s_tr[1], s_tr[5], 1), mask_h);
+      start_col1[2] = vandq_s16(vextq_s16(s_tr[1], s_tr[5], 2), mask_h);
+      start_col1[3] = vandq_s16(vextq_s16(s_tr[1], s_tr[5], 3), mask_h);
+      start_col1[4] = vandq_s16(vextq_s16(s_tr[1], s_tr[5], 4), mask_h);
+
+      start_col2[0] = vandq_s16(s_tr[2], mask_h);
+      start_col2[1] = vandq_s16(vextq_s16(s_tr[2], s_tr[6], 1), mask_h);
+      start_col2[2] = vandq_s16(vextq_s16(s_tr[2], s_tr[6], 2), mask_h);
+      start_col2[3] = vandq_s16(vextq_s16(s_tr[2], s_tr[6], 3), mask_h);
+      start_col2[4] = vandq_s16(vextq_s16(s_tr[2], s_tr[6], 4), mask_h);
+
+      start_col3[0] = vandq_s16(s_tr[3], mask_h);
+      start_col3[1] = vandq_s16(vextq_s16(s_tr[3], s_tr[7], 1), mask_h);
+      start_col3[2] = vandq_s16(vextq_s16(s_tr[3], s_tr[7], 2), mask_h);
+      start_col3[3] = vandq_s16(vextq_s16(s_tr[3], s_tr[7], 3), mask_h);
+      start_col3[4] = vandq_s16(vextq_s16(s_tr[3], s_tr[7], 4), mask_h);
+
+      // i = 1, j = 2;
+      sub_deltas_step4(start_col0, start_col1, deltas[0]);
+
+      // i = 1, j = 3;
+      sub_deltas_step4(start_col0, start_col2, deltas[1]);
+
+      // i = 1, j = 4
+      sub_deltas_step4(start_col0, start_col3, deltas[2]);
+
+      // i = 2, j = 3
+      sub_deltas_step4(start_col1, start_col2, deltas[3]);
+
+      // i = 2, j = 4
+      sub_deltas_step4(start_col1, start_col3, deltas[4]);
+
+      // i = 3, j = 4
+      sub_deltas_step4(start_col2, start_col3, deltas[5]);
+
+      int16x8_t end_col0[5], end_col1[5], end_col2[5], end_col3[5];
+      end_col0[0] = vandq_s16(e_tr[0], mask_h);
+      end_col0[1] = vandq_s16(vextq_s16(e_tr[0], e_tr[4], 1), mask_h);
+      end_col0[2] = vandq_s16(vextq_s16(e_tr[0], e_tr[4], 2), mask_h);
+      end_col0[3] = vandq_s16(vextq_s16(e_tr[0], e_tr[4], 3), mask_h);
+      end_col0[4] = vandq_s16(vextq_s16(e_tr[0], e_tr[4], 4), mask_h);
+
+      end_col1[0] = vandq_s16(e_tr[1], mask_h);
+      end_col1[1] = vandq_s16(vextq_s16(e_tr[1], e_tr[5], 1), mask_h);
+      end_col1[2] = vandq_s16(vextq_s16(e_tr[1], e_tr[5], 2), mask_h);
+      end_col1[3] = vandq_s16(vextq_s16(e_tr[1], e_tr[5], 3), mask_h);
+      end_col1[4] = vandq_s16(vextq_s16(e_tr[1], e_tr[5], 4), mask_h);
+
+      end_col2[0] = vandq_s16(e_tr[2], mask_h);
+      end_col2[1] = vandq_s16(vextq_s16(e_tr[2], e_tr[6], 1), mask_h);
+      end_col2[2] = vandq_s16(vextq_s16(e_tr[2], e_tr[6], 2), mask_h);
+      end_col2[3] = vandq_s16(vextq_s16(e_tr[2], e_tr[6], 3), mask_h);
+      end_col2[4] = vandq_s16(vextq_s16(e_tr[2], e_tr[6], 4), mask_h);
+
+      end_col3[0] = vandq_s16(e_tr[3], mask_h);
+      end_col3[1] = vandq_s16(vextq_s16(e_tr[3], e_tr[7], 1), mask_h);
+      end_col3[2] = vandq_s16(vextq_s16(e_tr[3], e_tr[7], 2), mask_h);
+      end_col3[3] = vandq_s16(vextq_s16(e_tr[3], e_tr[7], 3), mask_h);
+      end_col3[4] = vandq_s16(vextq_s16(e_tr[3], e_tr[7], 4), mask_h);
+
+      // i = 1, j = 2;
+      add_deltas_step4(end_col0, end_col1, deltas[0]);
+
+      // i = 1, j = 3;
+      add_deltas_step4(end_col0, end_col2, deltas[1]);
+
+      // i = 1, j = 4
+      add_deltas_step4(end_col0, end_col3, deltas[2]);
+
+      // i = 2, j =3
+      add_deltas_step4(end_col1, end_col2, deltas[3]);
+
+      // i = 2, j = 4
+      add_deltas_step4(end_col1, end_col3, deltas[4]);
+
+      // i = 3, j = 4
+      add_deltas_step4(end_col2, end_col3, deltas[5]);
+    }
+
+    int32x4_t delta[6][2];
+    int32_t single_delta[6];
+
+    delta[0][0] = horizontal_add_4d_s32x4(&deltas[0][0]);
+    delta[1][0] = horizontal_add_4d_s32x4(&deltas[1][0]);
+    delta[2][0] = horizontal_add_4d_s32x4(&deltas[2][0]);
+    delta[3][0] = horizontal_add_4d_s32x4(&deltas[3][0]);
+    delta[4][0] = horizontal_add_4d_s32x4(&deltas[4][0]);
+    delta[5][0] = horizontal_add_4d_s32x4(&deltas[5][0]);
+
+    delta[0][1] = horizontal_add_4d_s32x4(&deltas[0][5]);
+    delta[1][1] = horizontal_add_4d_s32x4(&deltas[1][5]);
+    delta[2][1] = horizontal_add_4d_s32x4(&deltas[2][5]);
+    delta[3][1] = horizontal_add_4d_s32x4(&deltas[3][5]);
+    delta[4][1] = horizontal_add_4d_s32x4(&deltas[4][5]);
+    delta[5][1] = horizontal_add_4d_s32x4(&deltas[5][5]);
+
+    single_delta[0] = horizontal_add_s32x4(deltas[0][4]);
+    single_delta[1] = horizontal_add_s32x4(deltas[1][4]);
+    single_delta[2] = horizontal_add_s32x4(deltas[2][4]);
+    single_delta[3] = horizontal_add_s32x4(deltas[3][4]);
+    single_delta[4] = horizontal_add_s32x4(deltas[4][4]);
+    single_delta[5] = horizontal_add_s32x4(deltas[5][4]);
+
+    int idx = 0;
+    for (i = 1; i < wiener_win - 1; i++) {
+      for (j = i + 1; j < wiener_win; j++) {
+        update_4_stats_neon(
+            H + (i - 1) * wiener_win * wiener_win2 + (j - 1) * wiener_win,
+            delta[idx][0], H + i * wiener_win * wiener_win2 + j * wiener_win);
+        H[i * wiener_win * wiener_win2 + j * wiener_win + 4] =
+            H[(i - 1) * wiener_win * wiener_win2 + (j - 1) * wiener_win + 4] +
+            single_delta[idx];
+
+        H[(i * wiener_win + 1) * wiener_win2 + j * wiener_win] =
+            H[((i - 1) * wiener_win + 1) * wiener_win2 + (j - 1) * wiener_win] +
+            vgetq_lane_s32(delta[idx][1], 0);
+        H[(i * wiener_win + 2) * wiener_win2 + j * wiener_win] =
+            H[((i - 1) * wiener_win + 2) * wiener_win2 + (j - 1) * wiener_win] +
+            vgetq_lane_s32(delta[idx][1], 1);
+        H[(i * wiener_win + 3) * wiener_win2 + j * wiener_win] =
+            H[((i - 1) * wiener_win + 3) * wiener_win2 + (j - 1) * wiener_win] +
+            vgetq_lane_s32(delta[idx][1], 2);
+        H[(i * wiener_win + 4) * wiener_win2 + j * wiener_win] =
+            H[((i - 1) * wiener_win + 4) * wiener_win2 + (j - 1) * wiener_win] +
+            vgetq_lane_s32(delta[idx][1], 3);
+
+        idx++;
+      }
+    }
+  }
+
+  // Step 5: Derive other points of each square. No square in bottom row.
+  i = 0;
+  do {
+    const int16_t *const di = d + i;
+
+    j = i + 1;
+    do {
+      const int16_t *const dj = d + j;
+      int32x4_t deltas[WIENER_WIN_CHROMA - 1][WIENER_WIN_CHROMA - 1] = {
+        { vdupq_n_s32(0) }, { vdupq_n_s32(0) }
+      };
+      int16x8_t d_is[WIN_CHROMA], d_ie[WIN_CHROMA];
+      int16x8_t d_js[WIN_CHROMA], d_je[WIN_CHROMA];
+
+      x = 0;
+      while (x < w16) {
+        load_square_win5_neon(di + x, dj + x, d_stride, height, d_is, d_ie,
+                              d_js, d_je);
+        derive_square_win5_neon(d_is, d_ie, d_js, d_je, deltas);
+        x += 16;
+      }
+
+      if (w16 != width) {
+        load_square_win5_neon(di + x, dj + x, d_stride, height, d_is, d_ie,
+                              d_js, d_je);
+        d_is[0] = vandq_s16(d_is[0], mask[0]);
+        d_is[1] = vandq_s16(d_is[1], mask[1]);
+        d_is[2] = vandq_s16(d_is[2], mask[0]);
+        d_is[3] = vandq_s16(d_is[3], mask[1]);
+        d_is[4] = vandq_s16(d_is[4], mask[0]);
+        d_is[5] = vandq_s16(d_is[5], mask[1]);
+        d_is[6] = vandq_s16(d_is[6], mask[0]);
+        d_is[7] = vandq_s16(d_is[7], mask[1]);
+        d_ie[0] = vandq_s16(d_ie[0], mask[0]);
+        d_ie[1] = vandq_s16(d_ie[1], mask[1]);
+        d_ie[2] = vandq_s16(d_ie[2], mask[0]);
+        d_ie[3] = vandq_s16(d_ie[3], mask[1]);
+        d_ie[4] = vandq_s16(d_ie[4], mask[0]);
+        d_ie[5] = vandq_s16(d_ie[5], mask[1]);
+        d_ie[6] = vandq_s16(d_ie[6], mask[0]);
+        d_ie[7] = vandq_s16(d_ie[7], mask[1]);
+        derive_square_win5_neon(d_is, d_ie, d_js, d_je, deltas);
+      }
+
+      hadd_update_4_stats_neon(
+          H + (i * wiener_win + 0) * wiener_win2 + j * wiener_win, deltas[0],
+          H + (i * wiener_win + 1) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_4_stats_neon(
+          H + (i * wiener_win + 1) * wiener_win2 + j * wiener_win, deltas[1],
+          H + (i * wiener_win + 2) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_4_stats_neon(
+          H + (i * wiener_win + 2) * wiener_win2 + j * wiener_win, deltas[2],
+          H + (i * wiener_win + 3) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_4_stats_neon(
+          H + (i * wiener_win + 3) * wiener_win2 + j * wiener_win, deltas[3],
+          H + (i * wiener_win + 4) * wiener_win2 + j * wiener_win + 1);
+    } while (++j < wiener_win);
+  } while (++i < wiener_win - 1);
+
+  // Step 6: Derive other points of each upper triangle along the diagonal.
+  i = 0;
+  do {
+    const int16_t *const di = d + i;
+    int32x4_t deltas[WIENER_WIN_CHROMA * 2 + 1] = { vdupq_n_s32(0) };
+    int16x8_t d_is[WIN_CHROMA], d_ie[WIN_CHROMA];
+
+    x = 0;
+    while (x < w16) {
+      load_triangle_win5_neon(di + x, d_stride, height, d_is, d_ie);
+      derive_triangle_win5_neon(d_is, d_ie, deltas);
+      x += 16;
+    }
+
+    if (w16 != width) {
+      load_triangle_win5_neon(di + x, d_stride, height, d_is, d_ie);
+      d_is[0] = vandq_s16(d_is[0], mask[0]);
+      d_is[1] = vandq_s16(d_is[1], mask[1]);
+      d_is[2] = vandq_s16(d_is[2], mask[0]);
+      d_is[3] = vandq_s16(d_is[3], mask[1]);
+      d_is[4] = vandq_s16(d_is[4], mask[0]);
+      d_is[5] = vandq_s16(d_is[5], mask[1]);
+      d_is[6] = vandq_s16(d_is[6], mask[0]);
+      d_is[7] = vandq_s16(d_is[7], mask[1]);
+      d_ie[0] = vandq_s16(d_ie[0], mask[0]);
+      d_ie[1] = vandq_s16(d_ie[1], mask[1]);
+      d_ie[2] = vandq_s16(d_ie[2], mask[0]);
+      d_ie[3] = vandq_s16(d_ie[3], mask[1]);
+      d_ie[4] = vandq_s16(d_ie[4], mask[0]);
+      d_ie[5] = vandq_s16(d_ie[5], mask[1]);
+      d_ie[6] = vandq_s16(d_ie[6], mask[0]);
+      d_ie[7] = vandq_s16(d_ie[7], mask[1]);
+      derive_triangle_win5_neon(d_is, d_ie, deltas);
+    }
+
+    // Row 1: 4 points
+    hadd_update_4_stats_neon(
+        H + (i * wiener_win + 0) * wiener_win2 + i * wiener_win, deltas,
+        H + (i * wiener_win + 1) * wiener_win2 + i * wiener_win + 1);
+
+    // Row 2: 3 points
+    int32x4_t deltas45 = horizontal_add_2d_s32(deltas[4], deltas[5]);
+    int32x4_t deltas78 = horizontal_add_2d_s32(deltas[7], deltas[8]);
+
+    int64x2_t deltas45_s64 = vpaddlq_s32(deltas45);
+    int64x2_t deltas78_s64 = vpaddlq_s32(deltas78);
+
+    int64x2_t src =
+        vld1q_s64(H + (i * wiener_win + 1) * wiener_win2 + i * wiener_win + 1);
+    int64x2_t dst = vaddq_s64(src, deltas45_s64);
+    vst1q_s64(H + (i * wiener_win + 2) * wiener_win2 + i * wiener_win + 2, dst);
+
+    int32x4_t delta69 = horizontal_add_2d_s32(deltas[6], deltas[9]);
+    int64x2_t delta69_s64 = vpaddlq_s32(delta69);
+    H[(i * wiener_win + 2) * wiener_win2 + i * wiener_win + 4] =
+        H[(i * wiener_win + 1) * wiener_win2 + i * wiener_win + 3] +
+        vgetq_lane_s64(delta69_s64, 0);
+
+    // Row 3: 2 points
+    vst1q_s64(H + (i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3,
+              vaddq_s64(dst, deltas78_s64));
+
+    // Row 4: 1 point
+    H[(i * wiener_win + 4) * wiener_win2 + i * wiener_win + 4] =
+        H[(i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3] +
+        vgetq_lane_s64(delta69_s64, 1);
+  } while (++i < wiener_win);
+}
+
+static inline void compute_stats_win7_neon(
+    const int16_t *const d, const int32_t d_stride, const int16_t *const s,
+    const int32_t s_stride, const int32_t width, const int32_t height,
+    int64_t *const M, int64_t *const H) {
+  const int32_t wiener_win = WIENER_WIN;
+  const int32_t wiener_win2 = wiener_win * wiener_win;
+  const int32_t w16 = width & ~15;
+  const int32_t h8 = height & ~7;
+  int16x8_t mask[2];
+  mask[0] = vld1q_s16(&(mask_16bit[16]) - width % 16);
+  mask[1] = vld1q_s16(&(mask_16bit[16]) - width % 16 + 8);
+  const int bit_depth = 8;
+  int32_t i, j, x, y;
+
+  const int32_t num_bit_left =
+      32 - 1 /* sign */ - 2 * bit_depth /* energy */ + 2 /* SIMD */;
+  const int32_t h_allowed =
+      (1 << num_bit_left) / (w16 + ((w16 != width) ? 16 : 0));
+
+  // Step 1: Calculate the top edge of the whole matrix, i.e., the top
+  // edge of each triangle and square on the top row.
+  j = 0;
+  do {
+    const int16_t *s_t = s;
+    const int16_t *d_t = d;
+    int32_t height_t = 0;
+    int64x2_t sum_m[WIENER_WIN] = { vdupq_n_s64(0) };
+    int64x2_t sum_h[WIENER_WIN] = { vdupq_n_s64(0) };
+    int16x8_t src[2], dgd[2];
+
+    do {
+      const int32_t h_t =
+          ((height - height_t) < h_allowed) ? (height - height_t) : h_allowed;
+      int32x4_t row_m[WIENER_WIN * 2] = { vdupq_n_s32(0) };
+      int32x4_t row_h[WIENER_WIN * 2] = { vdupq_n_s32(0) };
+
+      y = h_t;
+      do {
+        x = 0;
+        while (x < w16) {
+          src[0] = vld1q_s16(s_t + x);
+          src[1] = vld1q_s16(s_t + x + 8);
+          dgd[0] = vld1q_s16(d_t + x);
+          dgd[1] = vld1q_s16(d_t + x + 8);
+          stats_top_win7_neon(src, dgd, d_t + j + x, d_stride, row_m, row_h);
+          x += 16;
+        }
+
+        if (w16 != width) {
+          src[0] = vld1q_s16(s_t + w16);
+          src[1] = vld1q_s16(s_t + w16 + 8);
+          dgd[0] = vld1q_s16(d_t + w16);
+          dgd[1] = vld1q_s16(d_t + w16 + 8);
+          src[0] = vandq_s16(src[0], mask[0]);
+          src[1] = vandq_s16(src[1], mask[1]);
+          dgd[0] = vandq_s16(dgd[0], mask[0]);
+          dgd[1] = vandq_s16(dgd[1], mask[1]);
+          stats_top_win7_neon(src, dgd, d_t + j + w16, d_stride, row_m, row_h);
+        }
+
+        s_t += s_stride;
+        d_t += d_stride;
+      } while (--y);
+
+      sum_m[0] = vpadalq_s32(sum_m[0], row_m[0]);
+      sum_m[1] = vpadalq_s32(sum_m[1], row_m[1]);
+      sum_m[2] = vpadalq_s32(sum_m[2], row_m[2]);
+      sum_m[3] = vpadalq_s32(sum_m[3], row_m[3]);
+      sum_m[4] = vpadalq_s32(sum_m[4], row_m[4]);
+      sum_m[5] = vpadalq_s32(sum_m[5], row_m[5]);
+      sum_m[6] = vpadalq_s32(sum_m[6], row_m[6]);
+
+      sum_h[0] = vpadalq_s32(sum_h[0], row_h[0]);
+      sum_h[1] = vpadalq_s32(sum_h[1], row_h[1]);
+      sum_h[2] = vpadalq_s32(sum_h[2], row_h[2]);
+      sum_h[3] = vpadalq_s32(sum_h[3], row_h[3]);
+      sum_h[4] = vpadalq_s32(sum_h[4], row_h[4]);
+      sum_h[5] = vpadalq_s32(sum_h[5], row_h[5]);
+      sum_h[6] = vpadalq_s32(sum_h[6], row_h[6]);
+
+      height_t += h_t;
+    } while (height_t < height);
+
+#if AOM_ARCH_AARCH64
+    vst1q_s64(M + wiener_win * j + 0, vpaddq_s64(sum_m[0], sum_m[1]));
+    vst1q_s64(M + wiener_win * j + 2, vpaddq_s64(sum_m[2], sum_m[3]));
+    vst1q_s64(M + wiener_win * j + 4, vpaddq_s64(sum_m[4], sum_m[5]));
+    M[wiener_win * j + 6] = vaddvq_s64(sum_m[6]);
+
+    vst1q_s64(H + wiener_win * j + 0, vpaddq_s64(sum_h[0], sum_h[1]));
+    vst1q_s64(H + wiener_win * j + 2, vpaddq_s64(sum_h[2], sum_h[3]));
+    vst1q_s64(H + wiener_win * j + 4, vpaddq_s64(sum_h[4], sum_h[5]));
+    H[wiener_win * j + 6] = vaddvq_s64(sum_h[6]);
+#else
+    M[wiener_win * j + 0] = horizontal_add_s64x2(sum_m[0]);
+    M[wiener_win * j + 1] = horizontal_add_s64x2(sum_m[1]);
+    M[wiener_win * j + 2] = horizontal_add_s64x2(sum_m[2]);
+    M[wiener_win * j + 3] = horizontal_add_s64x2(sum_m[3]);
+    M[wiener_win * j + 4] = horizontal_add_s64x2(sum_m[4]);
+    M[wiener_win * j + 5] = horizontal_add_s64x2(sum_m[5]);
+    M[wiener_win * j + 6] = horizontal_add_s64x2(sum_m[6]);
+
+    H[wiener_win * j + 0] = horizontal_add_s64x2(sum_h[0]);
+    H[wiener_win * j + 1] = horizontal_add_s64x2(sum_h[1]);
+    H[wiener_win * j + 2] = horizontal_add_s64x2(sum_h[2]);
+    H[wiener_win * j + 3] = horizontal_add_s64x2(sum_h[3]);
+    H[wiener_win * j + 4] = horizontal_add_s64x2(sum_h[4]);
+    H[wiener_win * j + 5] = horizontal_add_s64x2(sum_h[5]);
+    H[wiener_win * j + 6] = horizontal_add_s64x2(sum_h[6]);
+#endif  // AOM_ARCH_AARCH64
+  } while (++j < wiener_win);
+
+  // Step 2: Calculate the left edge of each square on the top row.
+  j = 1;
+  do {
+    const int16_t *d_t = d;
+    int32_t height_t = 0;
+    int64x2_t sum_h[WIENER_WIN - 1] = { vdupq_n_s64(0) };
+    int16x8_t dgd[2];
+
+    do {
+      const int32_t h_t =
+          ((height - height_t) < h_allowed) ? (height - height_t) : h_allowed;
+      int32x4_t row_h[WIENER_WIN - 1] = { vdupq_n_s32(0) };
+
+      y = h_t;
+      do {
+        x = 0;
+        while (x < w16) {
+          dgd[0] = vld1q_s16(d_t + j + x + 0);
+          dgd[1] = vld1q_s16(d_t + j + x + 8);
+          stats_left_win7_neon(dgd, d_t + x, d_stride, row_h);
+          x += 16;
+        }
+
+        if (w16 != width) {
+          dgd[0] = vld1q_s16(d_t + j + x + 0);
+          dgd[1] = vld1q_s16(d_t + j + x + 8);
+          dgd[0] = vandq_s16(dgd[0], mask[0]);
+          dgd[1] = vandq_s16(dgd[1], mask[1]);
+          stats_left_win7_neon(dgd, d_t + x, d_stride, row_h);
+        }
+
+        d_t += d_stride;
+      } while (--y);
+
+      sum_h[0] = vpadalq_s32(sum_h[0], row_h[0]);
+      sum_h[1] = vpadalq_s32(sum_h[1], row_h[1]);
+      sum_h[2] = vpadalq_s32(sum_h[2], row_h[2]);
+      sum_h[3] = vpadalq_s32(sum_h[3], row_h[3]);
+      sum_h[4] = vpadalq_s32(sum_h[4], row_h[4]);
+      sum_h[5] = vpadalq_s32(sum_h[5], row_h[5]);
+
+      height_t += h_t;
+    } while (height_t < height);
+
+#if AOM_ARCH_AARCH64
+    int64x2_t sum_h0 = vpaddq_s64(sum_h[0], sum_h[1]);
+    int64x2_t sum_h2 = vpaddq_s64(sum_h[2], sum_h[3]);
+    int64x2_t sum_h4 = vpaddq_s64(sum_h[4], sum_h[5]);
+    vst1_s64(&H[1 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h0));
+    vst1_s64(&H[2 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h0));
+    vst1_s64(&H[3 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h2));
+    vst1_s64(&H[4 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h2));
+    vst1_s64(&H[5 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h4));
+    vst1_s64(&H[6 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h4));
+#else
+    H[1 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[0]);
+    H[2 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[1]);
+    H[3 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[2]);
+    H[4 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[3]);
+    H[5 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[4]);
+    H[6 * wiener_win2 + j * wiener_win] = horizontal_add_s64x2(sum_h[5]);
+#endif  // AOM_ARCH_AARCH64
+  } while (++j < wiener_win);
+
+  // Step 3: Derive the top edge of each triangle along the diagonal. No
+  // triangle in top row.
+  {
+    const int16_t *d_t = d;
+    // Pad to call transpose function.
+    int32x4_t deltas[(WIENER_WIN + 1) * 2] = { vdupq_n_s32(0) };
+    int32x4_t deltas_tr[(WIENER_WIN + 1) * 2] = { vdupq_n_s32(0) };
+    int16x8_t ds[WIENER_WIN * 2];
+
+    load_s16_8x6(d_t, d_stride, &ds[0], &ds[2], &ds[4], &ds[6], &ds[8],
+                 &ds[10]);
+    load_s16_8x6(d_t + width, d_stride, &ds[1], &ds[3], &ds[5], &ds[7], &ds[9],
+                 &ds[11]);
+
+    d_t += 6 * d_stride;
+
+    step3_win7_neon(d_t, d_stride, width, height, ds, deltas);
+    transpose_arrays_s32_8x8(deltas, deltas_tr);
+
+    update_8_stats_neon(H + 0 * wiener_win * wiener_win2 + 0 * wiener_win,
+                        deltas_tr[0], deltas_tr[4],
+                        H + 1 * wiener_win * wiener_win2 + 1 * wiener_win);
+    update_8_stats_neon(H + 1 * wiener_win * wiener_win2 + 1 * wiener_win,
+                        deltas_tr[1], deltas_tr[5],
+                        H + 2 * wiener_win * wiener_win2 + 2 * wiener_win);
+    update_8_stats_neon(H + 2 * wiener_win * wiener_win2 + 2 * wiener_win,
+                        deltas_tr[2], deltas_tr[6],
+                        H + 3 * wiener_win * wiener_win2 + 3 * wiener_win);
+    update_8_stats_neon(H + 3 * wiener_win * wiener_win2 + 3 * wiener_win,
+                        deltas_tr[3], deltas_tr[7],
+                        H + 4 * wiener_win * wiener_win2 + 4 * wiener_win);
+    update_8_stats_neon(H + 4 * wiener_win * wiener_win2 + 4 * wiener_win,
+                        deltas_tr[8], deltas_tr[12],
+                        H + 5 * wiener_win * wiener_win2 + 5 * wiener_win);
+    update_8_stats_neon(H + 5 * wiener_win * wiener_win2 + 5 * wiener_win,
+                        deltas_tr[9], deltas_tr[13],
+                        H + 6 * wiener_win * wiener_win2 + 6 * wiener_win);
+  }
+
+  // Step 4: Derive the top and left edge of each square. No square in top and
+  // bottom row.
+
+  i = 1;
+  do {
+    j = i + 1;
+    do {
+      const int16_t *di = d + i - 1;
+      const int16_t *dj = d + j - 1;
+      int32x4_t deltas[(2 * WIENER_WIN - 1) * 2] = { vdupq_n_s32(0) };
+      int16x8_t dd[WIENER_WIN * 2], ds[WIENER_WIN * 2];
+
+      dd[5] = vdupq_n_s16(0);  // Initialize to avoid warning.
+      const int16_t dd0_values[] = { di[0 * d_stride],
+                                     di[1 * d_stride],
+                                     di[2 * d_stride],
+                                     di[3 * d_stride],
+                                     di[4 * d_stride],
+                                     di[5 * d_stride],
+                                     0,
+                                     0 };
+      dd[0] = vld1q_s16(dd0_values);
+      const int16_t dd1_values[] = { di[0 * d_stride + width],
+                                     di[1 * d_stride + width],
+                                     di[2 * d_stride + width],
+                                     di[3 * d_stride + width],
+                                     di[4 * d_stride + width],
+                                     di[5 * d_stride + width],
+                                     0,
+                                     0 };
+      dd[1] = vld1q_s16(dd1_values);
+      const int16_t ds0_values[] = { dj[0 * d_stride],
+                                     dj[1 * d_stride],
+                                     dj[2 * d_stride],
+                                     dj[3 * d_stride],
+                                     dj[4 * d_stride],
+                                     dj[5 * d_stride],
+                                     0,
+                                     0 };
+      ds[0] = vld1q_s16(ds0_values);
+      int16_t ds1_values[] = { dj[0 * d_stride + width],
+                               dj[1 * d_stride + width],
+                               dj[2 * d_stride + width],
+                               dj[3 * d_stride + width],
+                               dj[4 * d_stride + width],
+                               dj[5 * d_stride + width],
+                               0,
+                               0 };
+      ds[1] = vld1q_s16(ds1_values);
+
+      y = 0;
+      while (y < h8) {
+        // 00s 10s 20s 30s 40s 50s 60s 70s  00e 10e 20e 30e 40e 50e 60e 70e
+        dd[0] = vsetq_lane_s16(di[6 * d_stride], dd[0], 6);
+        dd[0] = vsetq_lane_s16(di[7 * d_stride], dd[0], 7);
+        dd[1] = vsetq_lane_s16(di[6 * d_stride + width], dd[1], 6);
+        dd[1] = vsetq_lane_s16(di[7 * d_stride + width], dd[1], 7);
+
+        // 00s 10s 20s 30s 40s 50s 60s 70s  00e 10e 20e 30e 40e 50e 60e 70e
+        // 01s 11s 21s 31s 41s 51s 61s 71s  01e 11e 21e 31e 41e 51e 61e 71e
+        ds[0] = vsetq_lane_s16(dj[6 * d_stride], ds[0], 6);
+        ds[0] = vsetq_lane_s16(dj[7 * d_stride], ds[0], 7);
+        ds[1] = vsetq_lane_s16(dj[6 * d_stride + width], ds[1], 6);
+        ds[1] = vsetq_lane_s16(dj[7 * d_stride + width], ds[1], 7);
+
+        load_more_16_neon(di + 8 * d_stride, width, &dd[0], &dd[2]);
+        load_more_16_neon(dj + 8 * d_stride, width, &ds[0], &ds[2]);
+        load_more_16_neon(di + 9 * d_stride, width, &dd[2], &dd[4]);
+        load_more_16_neon(dj + 9 * d_stride, width, &ds[2], &ds[4]);
+        load_more_16_neon(di + 10 * d_stride, width, &dd[4], &dd[6]);
+        load_more_16_neon(dj + 10 * d_stride, width, &ds[4], &ds[6]);
+        load_more_16_neon(di + 11 * d_stride, width, &dd[6], &dd[8]);
+        load_more_16_neon(dj + 11 * d_stride, width, &ds[6], &ds[8]);
+        load_more_16_neon(di + 12 * d_stride, width, &dd[8], &dd[10]);
+        load_more_16_neon(dj + 12 * d_stride, width, &ds[8], &ds[10]);
+        load_more_16_neon(di + 13 * d_stride, width, &dd[10], &dd[12]);
+        load_more_16_neon(dj + 13 * d_stride, width, &ds[10], &ds[12]);
+
+        madd_neon(&deltas[0], dd[0], ds[0]);
+        madd_neon(&deltas[1], dd[1], ds[1]);
+        madd_neon(&deltas[2], dd[0], ds[2]);
+        madd_neon(&deltas[3], dd[1], ds[3]);
+        madd_neon(&deltas[4], dd[0], ds[4]);
+        madd_neon(&deltas[5], dd[1], ds[5]);
+        madd_neon(&deltas[6], dd[0], ds[6]);
+        madd_neon(&deltas[7], dd[1], ds[7]);
+        madd_neon(&deltas[8], dd[0], ds[8]);
+        madd_neon(&deltas[9], dd[1], ds[9]);
+        madd_neon(&deltas[10], dd[0], ds[10]);
+        madd_neon(&deltas[11], dd[1], ds[11]);
+        madd_neon(&deltas[12], dd[0], ds[12]);
+        madd_neon(&deltas[13], dd[1], ds[13]);
+        madd_neon(&deltas[14], dd[2], ds[0]);
+        madd_neon(&deltas[15], dd[3], ds[1]);
+        madd_neon(&deltas[16], dd[4], ds[0]);
+        madd_neon(&deltas[17], dd[5], ds[1]);
+        madd_neon(&deltas[18], dd[6], ds[0]);
+        madd_neon(&deltas[19], dd[7], ds[1]);
+        madd_neon(&deltas[20], dd[8], ds[0]);
+        madd_neon(&deltas[21], dd[9], ds[1]);
+        madd_neon(&deltas[22], dd[10], ds[0]);
+        madd_neon(&deltas[23], dd[11], ds[1]);
+        madd_neon(&deltas[24], dd[12], ds[0]);
+        madd_neon(&deltas[25], dd[13], ds[1]);
+
+        dd[0] = vextq_s16(dd[12], vdupq_n_s16(0), 2);
+        dd[1] = vextq_s16(dd[13], vdupq_n_s16(0), 2);
+        ds[0] = vextq_s16(ds[12], vdupq_n_s16(0), 2);
+        ds[1] = vextq_s16(ds[13], vdupq_n_s16(0), 2);
+
+        di += 8 * d_stride;
+        dj += 8 * d_stride;
+        y += 8;
+      }
+
+      deltas[0] = hadd_four_32_neon(deltas[0], deltas[2], deltas[4], deltas[6]);
+      deltas[1] = hadd_four_32_neon(deltas[1], deltas[3], deltas[5], deltas[7]);
+      deltas[2] =
+          hadd_four_32_neon(deltas[8], deltas[10], deltas[12], deltas[12]);
+      deltas[3] =
+          hadd_four_32_neon(deltas[9], deltas[11], deltas[13], deltas[13]);
+      deltas[4] =
+          hadd_four_32_neon(deltas[14], deltas[16], deltas[18], deltas[20]);
+      deltas[5] =
+          hadd_four_32_neon(deltas[15], deltas[17], deltas[19], deltas[21]);
+      deltas[6] =
+          hadd_four_32_neon(deltas[22], deltas[24], deltas[22], deltas[24]);
+      deltas[7] =
+          hadd_four_32_neon(deltas[23], deltas[25], deltas[23], deltas[25]);
+      deltas[0] = vsubq_s32(deltas[1], deltas[0]);
+      deltas[1] = vsubq_s32(deltas[3], deltas[2]);
+      deltas[2] = vsubq_s32(deltas[5], deltas[4]);
+      deltas[3] = vsubq_s32(deltas[7], deltas[6]);
+
+      if (h8 != height) {
+        const int16_t ds0_vals[] = {
+          dj[0 * d_stride], dj[0 * d_stride + width],
+          dj[1 * d_stride], dj[1 * d_stride + width],
+          dj[2 * d_stride], dj[2 * d_stride + width],
+          dj[3 * d_stride], dj[3 * d_stride + width]
+        };
+        ds[0] = vld1q_s16(ds0_vals);
+
+        ds[1] = vsetq_lane_s16(dj[4 * d_stride], ds[1], 0);
+        ds[1] = vsetq_lane_s16(dj[4 * d_stride + width], ds[1], 1);
+        ds[1] = vsetq_lane_s16(dj[5 * d_stride], ds[1], 2);
+        ds[1] = vsetq_lane_s16(dj[5 * d_stride + width], ds[1], 3);
+        const int16_t dd4_vals[] = {
+          -di[1 * d_stride], di[1 * d_stride + width],
+          -di[2 * d_stride], di[2 * d_stride + width],
+          -di[3 * d_stride], di[3 * d_stride + width],
+          -di[4 * d_stride], di[4 * d_stride + width]
+        };
+        dd[4] = vld1q_s16(dd4_vals);
+
+        dd[5] = vsetq_lane_s16(-di[5 * d_stride], dd[5], 0);
+        dd[5] = vsetq_lane_s16(di[5 * d_stride + width], dd[5], 1);
+        do {
+          dd[0] = vdupq_n_s16(-di[0 * d_stride]);
+          dd[2] = dd[3] = vdupq_n_s16(di[0 * d_stride + width]);
+          dd[0] = dd[1] = vzipq_s16(dd[0], dd[2]).val[0];
+
+          ds[4] = vdupq_n_s16(dj[0 * d_stride]);
+          ds[6] = ds[7] = vdupq_n_s16(dj[0 * d_stride + width]);
+          ds[4] = ds[5] = vzipq_s16(ds[4], ds[6]).val[0];
+
+          dd[5] = vsetq_lane_s16(-di[6 * d_stride], dd[5], 2);
+          dd[5] = vsetq_lane_s16(di[6 * d_stride + width], dd[5], 3);
+          ds[1] = vsetq_lane_s16(dj[6 * d_stride], ds[1], 4);
+          ds[1] = vsetq_lane_s16(dj[6 * d_stride + width], ds[1], 5);
+
+          madd_neon_pairwise(&deltas[0], dd[0], ds[0]);
+          madd_neon_pairwise(&deltas[1], dd[1], ds[1]);
+          madd_neon_pairwise(&deltas[2], dd[4], ds[4]);
+          madd_neon_pairwise(&deltas[3], dd[5], ds[5]);
+
+          int32_t tmp0 = vgetq_lane_s32(vreinterpretq_s32_s16(ds[0]), 0);
+          ds[0] = vextq_s16(ds[0], ds[1], 2);
+          ds[1] = vextq_s16(ds[1], ds[0], 2);
+          ds[1] = vreinterpretq_s16_s32(
+              vsetq_lane_s32(tmp0, vreinterpretq_s32_s16(ds[1]), 3));
+          int32_t tmp1 = vgetq_lane_s32(vreinterpretq_s32_s16(dd[4]), 0);
+          dd[4] = vextq_s16(dd[4], dd[5], 2);
+          dd[5] = vextq_s16(dd[5], dd[4], 2);
+          dd[5] = vreinterpretq_s16_s32(
+              vsetq_lane_s32(tmp1, vreinterpretq_s32_s16(dd[5]), 3));
+          di += d_stride;
+          dj += d_stride;
+        } while (++y < height);
+      }
+
+      // Writing one more element on the top edge of a square falls to
+      // the next square in the same row or the first element in the next
+      // row, which will just be overwritten later.
+      update_8_stats_neon(
+          H + (i - 1) * wiener_win * wiener_win2 + (j - 1) * wiener_win,
+          deltas[0], deltas[1],
+          H + i * wiener_win * wiener_win2 + j * wiener_win);
+
+      H[(i * wiener_win + 1) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 1) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s32(deltas[2], 0);
+      H[(i * wiener_win + 2) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 2) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s32(deltas[2], 1);
+      H[(i * wiener_win + 3) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 3) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s32(deltas[2], 2);
+      H[(i * wiener_win + 4) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 4) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s32(deltas[2], 3);
+      H[(i * wiener_win + 5) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 5) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s32(deltas[3], 0);
+      H[(i * wiener_win + 6) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 6) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s32(deltas[3], 1);
+    } while (++j < wiener_win);
+  } while (++i < wiener_win - 1);
+
+  // Step 5: Derive other points of each square. No square in bottom row.
+  i = 0;
+  do {
+    const int16_t *const di = d + i;
+
+    j = i + 1;
+    do {
+      const int16_t *const dj = d + j;
+      int32x4_t deltas[WIENER_WIN - 1][WIN_7] = { { vdupq_n_s32(0) },
+                                                  { vdupq_n_s32(0) } };
+      int16x8_t d_is[WIN_7];
+      int16x8_t d_ie[WIN_7];
+      int16x8_t d_js[WIN_7];
+      int16x8_t d_je[WIN_7];
+
+      x = 0;
+      while (x < w16) {
+        load_square_win7_neon(di + x, dj + x, d_stride, height, d_is, d_ie,
+                              d_js, d_je);
+        derive_square_win7_neon(d_is, d_ie, d_js, d_je, deltas);
+        x += 16;
+      }
+
+      if (w16 != width) {
+        load_square_win7_neon(di + x, dj + x, d_stride, height, d_is, d_ie,
+                              d_js, d_je);
+        d_is[0] = vandq_s16(d_is[0], mask[0]);
+        d_is[1] = vandq_s16(d_is[1], mask[1]);
+        d_is[2] = vandq_s16(d_is[2], mask[0]);
+        d_is[3] = vandq_s16(d_is[3], mask[1]);
+        d_is[4] = vandq_s16(d_is[4], mask[0]);
+        d_is[5] = vandq_s16(d_is[5], mask[1]);
+        d_is[6] = vandq_s16(d_is[6], mask[0]);
+        d_is[7] = vandq_s16(d_is[7], mask[1]);
+        d_is[8] = vandq_s16(d_is[8], mask[0]);
+        d_is[9] = vandq_s16(d_is[9], mask[1]);
+        d_is[10] = vandq_s16(d_is[10], mask[0]);
+        d_is[11] = vandq_s16(d_is[11], mask[1]);
+        d_ie[0] = vandq_s16(d_ie[0], mask[0]);
+        d_ie[1] = vandq_s16(d_ie[1], mask[1]);
+        d_ie[2] = vandq_s16(d_ie[2], mask[0]);
+        d_ie[3] = vandq_s16(d_ie[3], mask[1]);
+        d_ie[4] = vandq_s16(d_ie[4], mask[0]);
+        d_ie[5] = vandq_s16(d_ie[5], mask[1]);
+        d_ie[6] = vandq_s16(d_ie[6], mask[0]);
+        d_ie[7] = vandq_s16(d_ie[7], mask[1]);
+        d_ie[8] = vandq_s16(d_ie[8], mask[0]);
+        d_ie[9] = vandq_s16(d_ie[9], mask[1]);
+        d_ie[10] = vandq_s16(d_ie[10], mask[0]);
+        d_ie[11] = vandq_s16(d_ie[11], mask[1]);
+        derive_square_win7_neon(d_is, d_ie, d_js, d_je, deltas);
+      }
+
+      hadd_update_6_stats_neon(
+          H + (i * wiener_win + 0) * wiener_win2 + j * wiener_win, deltas[0],
+          H + (i * wiener_win + 1) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_neon(
+          H + (i * wiener_win + 1) * wiener_win2 + j * wiener_win, deltas[1],
+          H + (i * wiener_win + 2) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_neon(
+          H + (i * wiener_win + 2) * wiener_win2 + j * wiener_win, deltas[2],
+          H + (i * wiener_win + 3) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_neon(
+          H + (i * wiener_win + 3) * wiener_win2 + j * wiener_win, deltas[3],
+          H + (i * wiener_win + 4) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_neon(
+          H + (i * wiener_win + 4) * wiener_win2 + j * wiener_win, deltas[4],
+          H + (i * wiener_win + 5) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_neon(
+          H + (i * wiener_win + 5) * wiener_win2 + j * wiener_win, deltas[5],
+          H + (i * wiener_win + 6) * wiener_win2 + j * wiener_win + 1);
+    } while (++j < wiener_win);
+  } while (++i < wiener_win - 1);
+
+  // Step 6: Derive other points of each upper triangle along the diagonal.
+  i = 0;
+  do {
+    const int16_t *const di = d + i;
+    int32x4_t deltas[WIENER_WIN * (WIENER_WIN - 1)] = { vdupq_n_s32(0) };
+    int16x8_t d_is[WIN_7], d_ie[WIN_7];
+
+    x = 0;
+    while (x < w16) {
+      load_triangle_win7_neon(di + x, d_stride, height, d_is, d_ie);
+      derive_triangle_win7_neon(d_is, d_ie, deltas);
+      x += 16;
+    }
+
+    if (w16 != width) {
+      load_triangle_win7_neon(di + x, d_stride, height, d_is, d_ie);
+      d_is[0] = vandq_s16(d_is[0], mask[0]);
+      d_is[1] = vandq_s16(d_is[1], mask[1]);
+      d_is[2] = vandq_s16(d_is[2], mask[0]);
+      d_is[3] = vandq_s16(d_is[3], mask[1]);
+      d_is[4] = vandq_s16(d_is[4], mask[0]);
+      d_is[5] = vandq_s16(d_is[5], mask[1]);
+      d_is[6] = vandq_s16(d_is[6], mask[0]);
+      d_is[7] = vandq_s16(d_is[7], mask[1]);
+      d_is[8] = vandq_s16(d_is[8], mask[0]);
+      d_is[9] = vandq_s16(d_is[9], mask[1]);
+      d_is[10] = vandq_s16(d_is[10], mask[0]);
+      d_is[11] = vandq_s16(d_is[11], mask[1]);
+      d_ie[0] = vandq_s16(d_ie[0], mask[0]);
+      d_ie[1] = vandq_s16(d_ie[1], mask[1]);
+      d_ie[2] = vandq_s16(d_ie[2], mask[0]);
+      d_ie[3] = vandq_s16(d_ie[3], mask[1]);
+      d_ie[4] = vandq_s16(d_ie[4], mask[0]);
+      d_ie[5] = vandq_s16(d_ie[5], mask[1]);
+      d_ie[6] = vandq_s16(d_ie[6], mask[0]);
+      d_ie[7] = vandq_s16(d_ie[7], mask[1]);
+      d_ie[8] = vandq_s16(d_ie[8], mask[0]);
+      d_ie[9] = vandq_s16(d_ie[9], mask[1]);
+      d_ie[10] = vandq_s16(d_ie[10], mask[0]);
+      d_ie[11] = vandq_s16(d_ie[11], mask[1]);
+      derive_triangle_win7_neon(d_is, d_ie, deltas);
+    }
+
+    // Row 1: 6 points
+    hadd_update_6_stats_neon(
+        H + (i * wiener_win + 0) * wiener_win2 + i * wiener_win, deltas,
+        H + (i * wiener_win + 1) * wiener_win2 + i * wiener_win + 1);
+
+    int32x4_t delta1710 = horizontal_add_2d_s32(deltas[17], deltas[10]);
+    int32x4_t delta1516 = horizontal_add_2d_s32(deltas[15], deltas[16]);
+
+    int64x2_t delta1710_s64 = vpaddlq_s32(delta1710);
+    int64x2_t delta1516_s64 = vpaddlq_s32(delta1516);
+
+    // Row 2: 5 points
+    hadd_update_4_stats_neon(
+        H + (i * wiener_win + 1) * wiener_win2 + i * wiener_win + 1, deltas + 6,
+        H + (i * wiener_win + 2) * wiener_win2 + i * wiener_win + 2);
+    H[(i * wiener_win + 2) * wiener_win2 + i * wiener_win + 6] =
+        H[(i * wiener_win + 1) * wiener_win2 + i * wiener_win + 5] +
+        vgetq_lane_s64(delta1710_s64, 1);
+
+    // Row 3: 4 points
+    hadd_update_4_stats_neon(
+        H + (i * wiener_win + 2) * wiener_win2 + i * wiener_win + 2,
+        deltas + 11,
+        H + (i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3);
+
+    // Row 4: 3 points
+    int64x2_t h0 =
+        vld1q_s64(H + (i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3);
+    vst1q_s64(H + (i * wiener_win + 4) * wiener_win2 + i * wiener_win + 4,
+              vaddq_s64(h0, delta1516_s64));
+    H[(i * wiener_win + 4) * wiener_win2 + i * wiener_win + 6] =
+        H[(i * wiener_win + 3) * wiener_win2 + i * wiener_win + 5] +
+        vgetq_lane_s64(delta1710_s64, 0);
+
+    int32x4_t delta1819 = horizontal_add_2d_s32(deltas[18], deltas[19]);
+    int64x2_t delta1819_s64 = vpaddlq_s32(delta1819);
+
+    // Row 5: 2 points
+    int64x2_t h1 =
+        vld1q_s64(H + (i * wiener_win + 4) * wiener_win2 + i * wiener_win + 4);
+    vst1q_s64(H + (i * wiener_win + 5) * wiener_win2 + i * wiener_win + 5,
+              vaddq_s64(h1, delta1819_s64));
+
+    // Row 6: 1 points
+    H[(i * wiener_win + 6) * wiener_win2 + i * wiener_win + 6] =
+        H[(i * wiener_win + 5) * wiener_win2 + i * wiener_win + 5] +
+        horizontal_long_add_s32x4(deltas[20]);
+  } while (++i < wiener_win);
+}
+
 static inline uint8_t find_average_neon(const uint8_t *src, int src_stride,
                                         int width, int height) {
   uint64_t sum = 0;
@@ -915,12 +2200,52 @@ static inline uint8_t find_average_neon(const uint8_t *src, int src_stride,
   return (uint8_t)(sum / (width * height));
 }
 
-void av1_compute_stats_neon(int wiener_win, const uint8_t *dgd,
-                            const uint8_t *src, int16_t *dgd_avg,
-                            int16_t *src_avg, int h_start, int h_end,
-                            int v_start, int v_end, int dgd_stride,
-                            int src_stride, int64_t *M, int64_t *H,
-                            int use_downsampled_wiener_stats) {
+static inline void compute_sub_avg(const uint8_t *buf, int buf_stride, int avg,
+                                   int16_t *buf_avg, int buf_avg_stride,
+                                   int width, int height,
+                                   int downsample_factor) {
+  uint8x8_t avg_u8 = vdup_n_u8(avg);
+
+  if (width > 8) {
+    int i = 0;
+    do {
+      int j = width;
+      const uint8_t *buf_ptr = buf;
+      int16_t *buf_avg_ptr = buf_avg;
+      do {
+        uint8x8_t d = vld1_u8(buf_ptr);
+        vst1q_s16(buf_avg_ptr, vreinterpretq_s16_u16(vsubl_u8(d, avg_u8)));
+
+        j -= 8;
+        buf_ptr += 8;
+        buf_avg_ptr += 8;
+      } while (j >= 8);
+      while (j > 0) {
+        *buf_avg_ptr = (int16_t)buf[width - j] - (int16_t)avg;
+        buf_avg_ptr++;
+        j--;
+      }
+      buf += buf_stride;
+      buf_avg += buf_avg_stride;
+      i += downsample_factor;
+    } while (i < height);
+  } else {
+    // For width < 8, don't use Neon.
+    for (int i = 0; i < height; i = i + downsample_factor) {
+      for (int j = 0; j < width; j++) {
+        buf_avg[j] = (int16_t)buf[j] - (int16_t)avg;
+      }
+      buf += buf_stride;
+      buf_avg += buf_avg_stride;
+    }
+  }
+}
+
+static inline void av1_compute_stats_downsampled_neon(
+    int wiener_win, const uint8_t *dgd, const uint8_t *src, int16_t *dgd_avg,
+    int16_t *src_avg, int h_start, int h_end, int v_start, int v_end,
+    int dgd_stride, int src_stride, int64_t *M, int64_t *H,
+    int use_downsampled_wiener_stats) {
   assert(wiener_win == WIENER_WIN || wiener_win == WIENER_WIN_CHROMA);
   assert(WIENER_STATS_DOWNSAMPLE_FACTOR == 4);
   (void)dgd_avg;
@@ -959,13 +2284,13 @@ void av1_compute_stats_neon(int wiener_win, const uint8_t *dgd,
   // Calculate the M and H matrices for the normal and downsampled cases.
   if (downsampled_height > 0) {
     if (wiener_win == WIENER_WIN) {
-      compute_stats_win7_neon(dgd_win, src_start, width, downsampled_height,
-                              dgd_stride, src_stride, avg, M, H,
-                              downsample_factor);
+      compute_stats_win7_downsampled_neon(
+          dgd_win, src_start, width, downsampled_height, dgd_stride, src_stride,
+          avg, M, H, downsample_factor);
     } else {
-      compute_stats_win5_neon(dgd_win, src_start, width, downsampled_height,
-                              dgd_stride, src_stride, avg, M, H,
-                              downsample_factor);
+      compute_stats_win5_downsampled_neon(
+          dgd_win, src_start, width, downsampled_height, dgd_stride, src_stride,
+          avg, M, H, downsample_factor);
     }
   }
 
@@ -973,19 +2298,62 @@ void av1_compute_stats_neon(int wiener_win, const uint8_t *dgd,
   if (downsample_remainder > 0) {
     int remainder_offset = height - downsample_remainder;
     if (wiener_win == WIENER_WIN) {
-      compute_stats_win7_neon(dgd_win + remainder_offset * dgd_stride,
-                              src_start + remainder_offset * src_stride, width,
-                              1, dgd_stride, src_stride, avg, M, H,
-                              downsample_remainder);
+      compute_stats_win7_downsampled_neon(
+          dgd_win + remainder_offset * dgd_stride,
+          src_start + remainder_offset * src_stride, width, 1, dgd_stride,
+          src_stride, avg, M, H, downsample_remainder);
     } else {
-      compute_stats_win5_neon(dgd_win + remainder_offset * dgd_stride,
-                              src_start + remainder_offset * src_stride, width,
-                              1, dgd_stride, src_stride, avg, M, H,
-                              downsample_remainder);
+      compute_stats_win5_downsampled_neon(
+          dgd_win + remainder_offset * dgd_stride,
+          src_start + remainder_offset * src_stride, width, 1, dgd_stride,
+          src_stride, avg, M, H, downsample_remainder);
     }
   }
 }
 
+void av1_compute_stats_neon(int32_t wiener_win, const uint8_t *dgd,
+                            const uint8_t *src, int16_t *dgd_avg,
+                            int16_t *src_avg, int32_t h_start, int32_t h_end,
+                            int32_t v_start, int32_t v_end, int32_t dgd_stride,
+                            int32_t src_stride, int64_t *M, int64_t *H,
+                            int use_downsampled_wiener_stats) {
+  assert(WIENER_STATS_DOWNSAMPLE_FACTOR == 4);
+  if (use_downsampled_wiener_stats) {
+    av1_compute_stats_downsampled_neon(
+        wiener_win, dgd, src, dgd_avg, src_avg, h_start, h_end, v_start, v_end,
+        dgd_stride, src_stride, M, H, use_downsampled_wiener_stats);
+    return;
+  }
+
+  const int32_t wiener_win2 = wiener_win * wiener_win;
+  const int32_t wiener_halfwin = (wiener_win >> 1);
+  const int32_t width = h_end - h_start;
+  const int32_t height = v_end - v_start;
+  const uint8_t *dgd_start = dgd + h_start + v_start * dgd_stride;
+  const uint8_t avg = find_average_neon(dgd_start, dgd_stride, width, height);
+  const int32_t d_stride = (width + 2 * wiener_halfwin + 15) & ~15;
+  const int32_t s_stride = (width + 15) & ~15;
+
+  compute_sub_avg(src + v_start * src_stride + h_start, src_stride, avg,
+                  src_avg, s_stride, width, height, 1);
+  compute_sub_avg(
+      dgd + (v_start - wiener_halfwin) * dgd_stride + h_start - wiener_halfwin,
+      dgd_stride, avg, dgd_avg, d_stride, width + 2 * wiener_halfwin,
+      height + 2 * wiener_halfwin, 1);
+
+  if (wiener_win == WIENER_WIN) {
+    compute_stats_win7_neon(dgd_avg, d_stride, src_avg, s_stride, width, height,
+                            M, H);
+  } else if (wiener_win == WIENER_WIN_CHROMA) {
+    compute_stats_win5_neon(dgd_avg, d_stride, src_avg, s_stride, width, height,
+                            M, H);
+  }
+
+  // H is a symmetric matrix, so we only need to fill out the upper triangle.
+  // We can copy it down to the lower triangle outside the (i, j) loops.
+  diagonal_copy_stats_neon(wiener_win2, H);
+}
+
 static inline void calc_proj_params_r0_r1_neon(
     const uint8_t *src8, int width, int height, int src_stride,
     const uint8_t *dat8, int dat_stride, int32_t *flt0, int flt0_stride,
diff --git a/av1/encoder/arm/pickrst_neon.h b/av1/encoder/arm/pickrst_neon.h
index 945593008..356c42dbc 100644
--- a/av1/encoder/arm/pickrst_neon.h
+++ b/av1/encoder/arm/pickrst_neon.h
@@ -16,6 +16,9 @@
 
 #include "av1/common/restoration.h"
 
+#define WIN_7 ((WIENER_WIN - 1) * 2)
+#define WIN_CHROMA ((WIENER_WIN_CHROMA - 1) * 2)
+
 // Aligned sizes for Wiener filters.
 #define WIENER_WIN2_ALIGN2 ALIGN_POWER_OF_TWO(WIENER_WIN2, 2)
 #define WIENER_WIN2_ALIGN3 ALIGN_POWER_OF_TWO(WIENER_WIN2, 3)
@@ -185,4 +188,1022 @@ static inline void accumulate_and_clear(int64_t *dst, int32_t *src,
   } while (length > 0);
 }
 
+// clang-format off
+// Constant pool to act as a mask to zero n top elements in an int16x8_t vector.
+// The index we load from depends on n.
+static const int16_t mask_16bit[32] = {
+  0xffff, 0xffff, 0xffff, 0xffff, 0xffff, 0xffff, 0xffff, 0xffff,
+  0xffff, 0xffff, 0xffff, 0xffff, 0xffff, 0xffff, 0xffff, 0xffff,
+       0,      0,      0,      0,      0,      0,      0,      0,
+       0,      0,      0,      0,      0,      0,      0,      0,
+};
+// clang-format on
+
+static inline void madd_neon_pairwise(int32x4_t *sum, const int16x8_t src,
+                                      const int16x8_t dgd) {
+  const int32x4_t sd =
+      horizontal_add_2d_s32(vmull_s16(vget_low_s16(src), vget_low_s16(dgd)),
+                            vmull_s16(vget_high_s16(src), vget_high_s16(dgd)));
+  *sum = vaddq_s32(*sum, sd);
+}
+
+static inline void madd_neon(int32x4_t *sum, const int16x8_t src,
+                             const int16x8_t dgd) {
+  *sum = vmlal_s16(*sum, vget_low_s16(src), vget_low_s16(dgd));
+  *sum = vmlal_s16(*sum, vget_high_s16(src), vget_high_s16(dgd));
+}
+
+static inline void msub_neon(int32x4_t *sum, const int16x8_t src,
+                             const int16x8_t dgd) {
+  *sum = vmlsl_s16(*sum, vget_low_s16(src), vget_low_s16(dgd));
+  *sum = vmlsl_s16(*sum, vget_high_s16(src), vget_high_s16(dgd));
+}
+
+static inline void compute_delta_step3(int32x4_t *sum0, int32x4_t *sum1,
+                                       const int16x8_t src0,
+                                       const int16x8_t src1,
+                                       const int16x8_t dgd0,
+                                       const int16x8_t dgd1) {
+  *sum0 = vmlsl_s16(*sum0, vget_low_s16(src0), vget_low_s16(dgd0));
+  *sum0 = vmlal_s16(*sum0, vget_low_s16(src1), vget_low_s16(dgd1));
+  *sum1 = vmlsl_s16(*sum1, vget_high_s16(src0), vget_high_s16(dgd0));
+  *sum1 = vmlal_s16(*sum1, vget_high_s16(src1), vget_high_s16(dgd1));
+}
+
+static inline int32x4_t hadd_four_32_neon(const int32x4_t src0,
+                                          const int32x4_t src1,
+                                          const int32x4_t src2,
+                                          const int32x4_t src3) {
+  int32x4_t src[4] = { src0, src1, src2, src3 };
+  return horizontal_add_4d_s32x4(src);
+}
+
+static inline void update_4_stats_neon(const int64_t *const src,
+                                       const int32x4_t delta,
+                                       int64_t *const dst) {
+  const int64x2_t s1 = vld1q_s64(src);
+  const int64x2_t s2 = vld1q_s64(src + 2);
+
+  const int64x2_t d1 = vaddw_s32(s1, vget_low_s32(delta));
+  const int64x2_t d2 = vaddw_s32(s2, vget_high_s32(delta));
+
+  vst1q_s64(dst, d1);
+  vst1q_s64(dst + 2, d2);
+}
+
+static inline void load_more_16_neon(const int16_t *const src,
+                                     const int32_t width,
+                                     const int16x8_t org[2], int16x8_t dst[2]) {
+  int16x8_t s0 = vld1q_dup_s16(src);
+  int16x8_t s1 = vld1q_dup_s16(src + width);
+  dst[0] = vextq_s16(org[0], s0, 1);
+  dst[1] = vextq_s16(org[1], s1, 1);
+}
+
+static inline void stats_top_win5_neon(const int16x8_t src[2],
+                                       const int16x8_t dgd[2],
+                                       const int16_t *const d,
+                                       const int32_t d_stride, int32x4_t *sum_m,
+                                       int32x4_t *sum_h) {
+  int16x8_t dgds[WIENER_WIN_CHROMA * 2];
+
+  load_s16_8x5(d + 0, d_stride, &dgds[0], &dgds[2], &dgds[4], &dgds[6],
+               &dgds[8]);
+  load_s16_8x5(d + 8, d_stride, &dgds[1], &dgds[3], &dgds[5], &dgds[7],
+               &dgds[9]);
+
+  madd_neon(&sum_m[0], src[0], dgds[0]);
+  madd_neon(&sum_m[0], src[1], dgds[1]);
+  madd_neon(&sum_m[1], src[0], dgds[2]);
+  madd_neon(&sum_m[1], src[1], dgds[3]);
+  madd_neon(&sum_m[2], src[0], dgds[4]);
+  madd_neon(&sum_m[2], src[1], dgds[5]);
+  madd_neon(&sum_m[3], src[0], dgds[6]);
+  madd_neon(&sum_m[3], src[1], dgds[7]);
+  madd_neon(&sum_m[4], src[0], dgds[8]);
+  madd_neon(&sum_m[4], src[1], dgds[9]);
+
+  madd_neon(&sum_h[0], dgd[0], dgds[0]);
+  madd_neon(&sum_h[0], dgd[1], dgds[1]);
+  madd_neon(&sum_h[1], dgd[0], dgds[2]);
+  madd_neon(&sum_h[1], dgd[1], dgds[3]);
+  madd_neon(&sum_h[2], dgd[0], dgds[4]);
+  madd_neon(&sum_h[2], dgd[1], dgds[5]);
+  madd_neon(&sum_h[3], dgd[0], dgds[6]);
+  madd_neon(&sum_h[3], dgd[1], dgds[7]);
+  madd_neon(&sum_h[4], dgd[0], dgds[8]);
+  madd_neon(&sum_h[4], dgd[1], dgds[9]);
+}
+
+static inline void stats_left_win5_neon(const int16x8_t src[2],
+                                        const int16_t *d,
+                                        const int32_t d_stride,
+                                        int32x4_t *sum) {
+  int16x8_t dgds[WIN_CHROMA];
+
+  load_s16_8x4(d + d_stride + 0, d_stride, &dgds[0], &dgds[2], &dgds[4],
+               &dgds[6]);
+  load_s16_8x4(d + d_stride + 8, d_stride, &dgds[1], &dgds[3], &dgds[5],
+               &dgds[7]);
+
+  madd_neon(&sum[0], src[0], dgds[0]);
+  madd_neon(&sum[0], src[1], dgds[1]);
+  madd_neon(&sum[1], src[0], dgds[2]);
+  madd_neon(&sum[1], src[1], dgds[3]);
+  madd_neon(&sum[2], src[0], dgds[4]);
+  madd_neon(&sum[2], src[1], dgds[5]);
+  madd_neon(&sum[3], src[0], dgds[6]);
+  madd_neon(&sum[3], src[1], dgds[7]);
+}
+
+static inline void derive_square_win5_neon(
+    const int16x8_t *d_is, const int16x8_t *d_ie, const int16x8_t *d_js,
+    const int16x8_t *d_je,
+    int32x4_t deltas[WIENER_WIN_CHROMA - 1][WIENER_WIN_CHROMA - 1]) {
+  msub_neon(&deltas[0][0], d_is[0], d_js[0]);
+  msub_neon(&deltas[0][0], d_is[1], d_js[1]);
+  msub_neon(&deltas[0][1], d_is[0], d_js[2]);
+  msub_neon(&deltas[0][1], d_is[1], d_js[3]);
+  msub_neon(&deltas[0][2], d_is[0], d_js[4]);
+  msub_neon(&deltas[0][2], d_is[1], d_js[5]);
+  msub_neon(&deltas[0][3], d_is[0], d_js[6]);
+  msub_neon(&deltas[0][3], d_is[1], d_js[7]);
+
+  msub_neon(&deltas[1][0], d_is[2], d_js[0]);
+  msub_neon(&deltas[1][0], d_is[3], d_js[1]);
+  msub_neon(&deltas[1][1], d_is[2], d_js[2]);
+  msub_neon(&deltas[1][1], d_is[3], d_js[3]);
+  msub_neon(&deltas[1][2], d_is[2], d_js[4]);
+  msub_neon(&deltas[1][2], d_is[3], d_js[5]);
+  msub_neon(&deltas[1][3], d_is[2], d_js[6]);
+  msub_neon(&deltas[1][3], d_is[3], d_js[7]);
+
+  msub_neon(&deltas[2][0], d_is[4], d_js[0]);
+  msub_neon(&deltas[2][0], d_is[5], d_js[1]);
+  msub_neon(&deltas[2][1], d_is[4], d_js[2]);
+  msub_neon(&deltas[2][1], d_is[5], d_js[3]);
+  msub_neon(&deltas[2][2], d_is[4], d_js[4]);
+  msub_neon(&deltas[2][2], d_is[5], d_js[5]);
+  msub_neon(&deltas[2][3], d_is[4], d_js[6]);
+  msub_neon(&deltas[2][3], d_is[5], d_js[7]);
+
+  msub_neon(&deltas[3][0], d_is[6], d_js[0]);
+  msub_neon(&deltas[3][0], d_is[7], d_js[1]);
+  msub_neon(&deltas[3][1], d_is[6], d_js[2]);
+  msub_neon(&deltas[3][1], d_is[7], d_js[3]);
+  msub_neon(&deltas[3][2], d_is[6], d_js[4]);
+  msub_neon(&deltas[3][2], d_is[7], d_js[5]);
+  msub_neon(&deltas[3][3], d_is[6], d_js[6]);
+  msub_neon(&deltas[3][3], d_is[7], d_js[7]);
+
+  madd_neon(&deltas[0][0], d_ie[0], d_je[0]);
+  madd_neon(&deltas[0][0], d_ie[1], d_je[1]);
+  madd_neon(&deltas[0][1], d_ie[0], d_je[2]);
+  madd_neon(&deltas[0][1], d_ie[1], d_je[3]);
+  madd_neon(&deltas[0][2], d_ie[0], d_je[4]);
+  madd_neon(&deltas[0][2], d_ie[1], d_je[5]);
+  madd_neon(&deltas[0][3], d_ie[0], d_je[6]);
+  madd_neon(&deltas[0][3], d_ie[1], d_je[7]);
+
+  madd_neon(&deltas[1][0], d_ie[2], d_je[0]);
+  madd_neon(&deltas[1][0], d_ie[3], d_je[1]);
+  madd_neon(&deltas[1][1], d_ie[2], d_je[2]);
+  madd_neon(&deltas[1][1], d_ie[3], d_je[3]);
+  madd_neon(&deltas[1][2], d_ie[2], d_je[4]);
+  madd_neon(&deltas[1][2], d_ie[3], d_je[5]);
+  madd_neon(&deltas[1][3], d_ie[2], d_je[6]);
+  madd_neon(&deltas[1][3], d_ie[3], d_je[7]);
+
+  madd_neon(&deltas[2][0], d_ie[4], d_je[0]);
+  madd_neon(&deltas[2][0], d_ie[5], d_je[1]);
+  madd_neon(&deltas[2][1], d_ie[4], d_je[2]);
+  madd_neon(&deltas[2][1], d_ie[5], d_je[3]);
+  madd_neon(&deltas[2][2], d_ie[4], d_je[4]);
+  madd_neon(&deltas[2][2], d_ie[5], d_je[5]);
+  madd_neon(&deltas[2][3], d_ie[4], d_je[6]);
+  madd_neon(&deltas[2][3], d_ie[5], d_je[7]);
+
+  madd_neon(&deltas[3][0], d_ie[6], d_je[0]);
+  madd_neon(&deltas[3][0], d_ie[7], d_je[1]);
+  madd_neon(&deltas[3][1], d_ie[6], d_je[2]);
+  madd_neon(&deltas[3][1], d_ie[7], d_je[3]);
+  madd_neon(&deltas[3][2], d_ie[6], d_je[4]);
+  madd_neon(&deltas[3][2], d_ie[7], d_je[5]);
+  madd_neon(&deltas[3][3], d_ie[6], d_je[6]);
+  madd_neon(&deltas[3][3], d_ie[7], d_je[7]);
+}
+
+static inline void load_square_win5_neon(const int16_t *const di,
+                                         const int16_t *const dj,
+                                         const int32_t d_stride,
+                                         const int32_t height, int16x8_t *d_is,
+                                         int16x8_t *d_ie, int16x8_t *d_js,
+                                         int16x8_t *d_je) {
+  load_s16_8x4(di + 0, d_stride, &d_is[0], &d_is[2], &d_is[4], &d_is[6]);
+  load_s16_8x4(di + 8, d_stride, &d_is[1], &d_is[3], &d_is[5], &d_is[7]);
+  load_s16_8x4(dj + 0, d_stride, &d_js[0], &d_js[2], &d_js[4], &d_js[6]);
+  load_s16_8x4(dj + 8, d_stride, &d_js[1], &d_js[3], &d_js[5], &d_js[7]);
+
+  load_s16_8x4(di + height * d_stride + 0, d_stride, &d_ie[0], &d_ie[2],
+               &d_ie[4], &d_ie[6]);
+  load_s16_8x4(di + height * d_stride + 8, d_stride, &d_ie[1], &d_ie[3],
+               &d_ie[5], &d_ie[7]);
+  load_s16_8x4(dj + height * d_stride + 0, d_stride, &d_je[0], &d_je[2],
+               &d_je[4], &d_je[6]);
+  load_s16_8x4(dj + height * d_stride + 8, d_stride, &d_je[1], &d_je[3],
+               &d_je[5], &d_je[7]);
+}
+
+static inline void update_5_stats_neon(const int64_t *const src,
+                                       const int32x4_t delta,
+                                       const int64_t delta4,
+                                       int64_t *const dst) {
+  update_4_stats_neon(src + 0, delta, dst + 0);
+  dst[4] = src[4] + delta4;
+}
+
+static inline void compute_delta_step3_two_lines(int32x4_t *sum,
+                                                 const int16x8_t src,
+                                                 const int16x8_t dgd) {
+  *sum = vmlsl_s16(*sum, vget_low_s16(src), vget_low_s16(dgd));
+  *sum = vmlal_s16(*sum, vget_high_s16(src), vget_high_s16(dgd));
+}
+
+static inline void step3_win5_neon(const int16_t *d, const int32_t d_stride,
+                                   const int32_t width, const int32_t height,
+                                   int16x8_t *ds, int32x4_t *deltas) {
+  int32_t y = height;
+  do {
+    ds[4] = load_unaligned_s16_4x2(d + 0 * d_stride, width);
+    ds[5] = load_unaligned_s16_4x2(d + 1 * d_stride, width);
+
+    compute_delta_step3_two_lines(&deltas[0], ds[0], ds[0]);
+    compute_delta_step3_two_lines(&deltas[1], ds[0], ds[1]);
+    compute_delta_step3_two_lines(&deltas[2], ds[0], ds[2]);
+    compute_delta_step3_two_lines(&deltas[3], ds[0], ds[3]);
+    compute_delta_step3_two_lines(&deltas[4], ds[0], ds[4]);
+    compute_delta_step3_two_lines(&deltas[0], ds[1], ds[1]);
+    compute_delta_step3_two_lines(&deltas[1], ds[1], ds[2]);
+    compute_delta_step3_two_lines(&deltas[2], ds[1], ds[3]);
+    compute_delta_step3_two_lines(&deltas[3], ds[1], ds[4]);
+    compute_delta_step3_two_lines(&deltas[4], ds[1], ds[5]);
+
+    ds[0] = ds[2];
+    ds[1] = ds[3];
+    ds[2] = ds[4];
+    ds[3] = ds[5];
+
+    d += 2 * d_stride;
+    y -= 2;
+  } while (y);
+}
+
+static inline void step3_win5_oneline_neon(const int16_t **const d,
+                                           const int32_t d_stride,
+                                           const int32_t width,
+                                           const int32_t height, int16x8_t *ds,
+                                           int32x4_t *deltas) {
+  int32_t y = height;
+  do {
+    ds[8] = vld1q_s16(*d);
+    ds[9] = vld1q_s16(*d + width);
+
+    compute_delta_step3(&deltas[0], &deltas[4], ds[0], ds[1], ds[0], ds[1]);
+    compute_delta_step3(&deltas[1], &deltas[5], ds[0], ds[1], ds[2], ds[3]);
+    compute_delta_step3(&deltas[2], &deltas[6], ds[0], ds[1], ds[4], ds[5]);
+    compute_delta_step3(&deltas[3], &deltas[7], ds[0], ds[1], ds[6], ds[7]);
+    compute_delta_step3(&deltas[8], &deltas[12], ds[0], ds[1], ds[8], ds[9]);
+
+    ds[0] = ds[2];
+    ds[1] = ds[3];
+    ds[2] = ds[4];
+    ds[3] = ds[5];
+    ds[4] = ds[6];
+    ds[5] = ds[7];
+    ds[6] = ds[8];
+    ds[7] = ds[9];
+
+    *d += d_stride;
+  } while (--y);
+}
+
+static inline void derive_triangle_win5_neon(const int16x8_t *d_is,
+                                             const int16x8_t *d_ie,
+                                             int32x4_t *deltas) {
+  msub_neon(&deltas[0], d_is[0], d_is[0]);
+  msub_neon(&deltas[0], d_is[1], d_is[1]);
+  msub_neon(&deltas[1], d_is[0], d_is[2]);
+  msub_neon(&deltas[1], d_is[1], d_is[3]);
+  msub_neon(&deltas[2], d_is[0], d_is[4]);
+  msub_neon(&deltas[2], d_is[1], d_is[5]);
+  msub_neon(&deltas[3], d_is[0], d_is[6]);
+  msub_neon(&deltas[3], d_is[1], d_is[7]);
+  msub_neon(&deltas[4], d_is[2], d_is[2]);
+  msub_neon(&deltas[4], d_is[3], d_is[3]);
+  msub_neon(&deltas[5], d_is[2], d_is[4]);
+  msub_neon(&deltas[5], d_is[3], d_is[5]);
+  msub_neon(&deltas[6], d_is[2], d_is[6]);
+  msub_neon(&deltas[6], d_is[3], d_is[7]);
+  msub_neon(&deltas[7], d_is[4], d_is[4]);
+  msub_neon(&deltas[7], d_is[5], d_is[5]);
+  msub_neon(&deltas[8], d_is[4], d_is[6]);
+  msub_neon(&deltas[8], d_is[5], d_is[7]);
+  msub_neon(&deltas[9], d_is[6], d_is[6]);
+  msub_neon(&deltas[9], d_is[7], d_is[7]);
+
+  madd_neon(&deltas[0], d_ie[0], d_ie[0]);
+  madd_neon(&deltas[0], d_ie[1], d_ie[1]);
+  madd_neon(&deltas[1], d_ie[0], d_ie[2]);
+  madd_neon(&deltas[1], d_ie[1], d_ie[3]);
+  madd_neon(&deltas[2], d_ie[0], d_ie[4]);
+  madd_neon(&deltas[2], d_ie[1], d_ie[5]);
+  madd_neon(&deltas[3], d_ie[0], d_ie[6]);
+  madd_neon(&deltas[3], d_ie[1], d_ie[7]);
+  madd_neon(&deltas[4], d_ie[2], d_ie[2]);
+  madd_neon(&deltas[4], d_ie[3], d_ie[3]);
+  madd_neon(&deltas[5], d_ie[2], d_ie[4]);
+  madd_neon(&deltas[5], d_ie[3], d_ie[5]);
+  madd_neon(&deltas[6], d_ie[2], d_ie[6]);
+  madd_neon(&deltas[6], d_ie[3], d_ie[7]);
+  madd_neon(&deltas[7], d_ie[4], d_ie[4]);
+  madd_neon(&deltas[7], d_ie[5], d_ie[5]);
+  madd_neon(&deltas[8], d_ie[4], d_ie[6]);
+  madd_neon(&deltas[8], d_ie[5], d_ie[7]);
+  madd_neon(&deltas[9], d_ie[6], d_ie[6]);
+  madd_neon(&deltas[9], d_ie[7], d_ie[7]);
+}
+
+static inline void load_triangle_win5_neon(const int16_t *const di,
+                                           const int32_t d_stride,
+                                           const int32_t height,
+                                           int16x8_t *d_is, int16x8_t *d_ie) {
+  load_s16_8x4(di + 0, d_stride, &d_is[0], &d_is[2], &d_is[4], &d_is[6]);
+  load_s16_8x4(di + 8, d_stride, &d_is[1], &d_is[3], &d_is[5], &d_is[7]);
+
+  load_s16_8x4(di + height * d_stride + 0, d_stride, &d_ie[0], &d_ie[2],
+               &d_ie[4], &d_ie[6]);
+  load_s16_8x4(di + height * d_stride + 8, d_stride, &d_ie[1], &d_ie[3],
+               &d_ie[5], &d_ie[7]);
+}
+
+static inline void sub_deltas_step4(int16x8_t *A, int16x8_t *B,
+                                    int32x4_t *deltas) {
+  deltas[0] = vmlsl_s16(deltas[0], vget_low_s16(A[0]), vget_low_s16(B[0]));
+  deltas[0] = vmlsl_s16(deltas[0], vget_high_s16(A[0]), vget_high_s16(B[0]));
+  deltas[1] = vmlsl_s16(deltas[1], vget_low_s16(A[0]), vget_low_s16(B[1]));
+  deltas[1] = vmlsl_s16(deltas[1], vget_high_s16(A[0]), vget_high_s16(B[1]));
+  deltas[2] = vmlsl_s16(deltas[2], vget_low_s16(A[0]), vget_low_s16(B[2]));
+  deltas[2] = vmlsl_s16(deltas[2], vget_high_s16(A[0]), vget_high_s16(B[2]));
+  deltas[3] = vmlsl_s16(deltas[3], vget_low_s16(A[0]), vget_low_s16(B[3]));
+  deltas[3] = vmlsl_s16(deltas[3], vget_high_s16(A[0]), vget_high_s16(B[3]));
+  deltas[4] = vmlsl_s16(deltas[4], vget_low_s16(A[0]), vget_low_s16(B[4]));
+  deltas[4] = vmlsl_s16(deltas[4], vget_high_s16(A[0]), vget_high_s16(B[4]));
+  deltas[5] = vmlsl_s16(deltas[5], vget_low_s16(A[1]), vget_low_s16(B[0]));
+  deltas[5] = vmlsl_s16(deltas[5], vget_high_s16(A[1]), vget_high_s16(B[0]));
+  deltas[6] = vmlsl_s16(deltas[6], vget_low_s16(A[2]), vget_low_s16(B[0]));
+  deltas[6] = vmlsl_s16(deltas[6], vget_high_s16(A[2]), vget_high_s16(B[0]));
+  deltas[7] = vmlsl_s16(deltas[7], vget_low_s16(A[3]), vget_low_s16(B[0]));
+  deltas[7] = vmlsl_s16(deltas[7], vget_high_s16(A[3]), vget_high_s16(B[0]));
+  deltas[8] = vmlsl_s16(deltas[8], vget_low_s16(A[4]), vget_low_s16(B[0]));
+  deltas[8] = vmlsl_s16(deltas[8], vget_high_s16(A[4]), vget_high_s16(B[0]));
+}
+
+static inline void add_deltas_step4(int16x8_t *A, int16x8_t *B,
+                                    int32x4_t *deltas) {
+  deltas[0] = vmlal_s16(deltas[0], vget_low_s16(A[0]), vget_low_s16(B[0]));
+  deltas[0] = vmlal_s16(deltas[0], vget_high_s16(A[0]), vget_high_s16(B[0]));
+  deltas[1] = vmlal_s16(deltas[1], vget_low_s16(A[0]), vget_low_s16(B[1]));
+  deltas[1] = vmlal_s16(deltas[1], vget_high_s16(A[0]), vget_high_s16(B[1]));
+  deltas[2] = vmlal_s16(deltas[2], vget_low_s16(A[0]), vget_low_s16(B[2]));
+  deltas[2] = vmlal_s16(deltas[2], vget_high_s16(A[0]), vget_high_s16(B[2]));
+  deltas[3] = vmlal_s16(deltas[3], vget_low_s16(A[0]), vget_low_s16(B[3]));
+  deltas[3] = vmlal_s16(deltas[3], vget_high_s16(A[0]), vget_high_s16(B[3]));
+  deltas[4] = vmlal_s16(deltas[4], vget_low_s16(A[0]), vget_low_s16(B[4]));
+  deltas[4] = vmlal_s16(deltas[4], vget_high_s16(A[0]), vget_high_s16(B[4]));
+  deltas[5] = vmlal_s16(deltas[5], vget_low_s16(A[1]), vget_low_s16(B[0]));
+  deltas[5] = vmlal_s16(deltas[5], vget_high_s16(A[1]), vget_high_s16(B[0]));
+  deltas[6] = vmlal_s16(deltas[6], vget_low_s16(A[2]), vget_low_s16(B[0]));
+  deltas[6] = vmlal_s16(deltas[6], vget_high_s16(A[2]), vget_high_s16(B[0]));
+  deltas[7] = vmlal_s16(deltas[7], vget_low_s16(A[3]), vget_low_s16(B[0]));
+  deltas[7] = vmlal_s16(deltas[7], vget_high_s16(A[3]), vget_high_s16(B[0]));
+  deltas[8] = vmlal_s16(deltas[8], vget_low_s16(A[4]), vget_low_s16(B[0]));
+  deltas[8] = vmlal_s16(deltas[8], vget_high_s16(A[4]), vget_high_s16(B[0]));
+}
+
+static inline void stats_top_win7_neon(const int16x8_t src[2],
+                                       const int16x8_t dgd[2],
+                                       const int16_t *const d,
+                                       const int32_t d_stride, int32x4_t *sum_m,
+                                       int32x4_t *sum_h) {
+  int16x8_t dgds[WIENER_WIN * 2];
+
+  load_s16_8x7(d + 0, d_stride, &dgds[0], &dgds[2], &dgds[4], &dgds[6],
+               &dgds[8], &dgds[10], &dgds[12]);
+  load_s16_8x7(d + 8, d_stride, &dgds[1], &dgds[3], &dgds[5], &dgds[7],
+               &dgds[9], &dgds[11], &dgds[13]);
+
+  madd_neon(&sum_m[0], src[0], dgds[0]);
+  madd_neon(&sum_m[0], src[1], dgds[1]);
+  madd_neon(&sum_m[1], src[0], dgds[2]);
+  madd_neon(&sum_m[1], src[1], dgds[3]);
+  madd_neon(&sum_m[2], src[0], dgds[4]);
+  madd_neon(&sum_m[2], src[1], dgds[5]);
+  madd_neon(&sum_m[3], src[0], dgds[6]);
+  madd_neon(&sum_m[3], src[1], dgds[7]);
+  madd_neon(&sum_m[4], src[0], dgds[8]);
+  madd_neon(&sum_m[4], src[1], dgds[9]);
+  madd_neon(&sum_m[5], src[0], dgds[10]);
+  madd_neon(&sum_m[5], src[1], dgds[11]);
+  madd_neon(&sum_m[6], src[0], dgds[12]);
+  madd_neon(&sum_m[6], src[1], dgds[13]);
+
+  madd_neon(&sum_h[0], dgd[0], dgds[0]);
+  madd_neon(&sum_h[0], dgd[1], dgds[1]);
+  madd_neon(&sum_h[1], dgd[0], dgds[2]);
+  madd_neon(&sum_h[1], dgd[1], dgds[3]);
+  madd_neon(&sum_h[2], dgd[0], dgds[4]);
+  madd_neon(&sum_h[2], dgd[1], dgds[5]);
+  madd_neon(&sum_h[3], dgd[0], dgds[6]);
+  madd_neon(&sum_h[3], dgd[1], dgds[7]);
+  madd_neon(&sum_h[4], dgd[0], dgds[8]);
+  madd_neon(&sum_h[4], dgd[1], dgds[9]);
+  madd_neon(&sum_h[5], dgd[0], dgds[10]);
+  madd_neon(&sum_h[5], dgd[1], dgds[11]);
+  madd_neon(&sum_h[6], dgd[0], dgds[12]);
+  madd_neon(&sum_h[6], dgd[1], dgds[13]);
+}
+
+static inline void derive_square_win7_neon(const int16x8_t *d_is,
+                                           const int16x8_t *d_ie,
+                                           const int16x8_t *d_js,
+                                           const int16x8_t *d_je,
+                                           int32x4_t deltas[][WIN_7]) {
+  msub_neon(&deltas[0][0], d_is[0], d_js[0]);
+  msub_neon(&deltas[0][0], d_is[1], d_js[1]);
+  msub_neon(&deltas[0][1], d_is[0], d_js[2]);
+  msub_neon(&deltas[0][1], d_is[1], d_js[3]);
+  msub_neon(&deltas[0][2], d_is[0], d_js[4]);
+  msub_neon(&deltas[0][2], d_is[1], d_js[5]);
+  msub_neon(&deltas[0][3], d_is[0], d_js[6]);
+  msub_neon(&deltas[0][3], d_is[1], d_js[7]);
+  msub_neon(&deltas[0][4], d_is[0], d_js[8]);
+  msub_neon(&deltas[0][4], d_is[1], d_js[9]);
+  msub_neon(&deltas[0][5], d_is[0], d_js[10]);
+  msub_neon(&deltas[0][5], d_is[1], d_js[11]);
+
+  msub_neon(&deltas[1][0], d_is[2], d_js[0]);
+  msub_neon(&deltas[1][0], d_is[3], d_js[1]);
+  msub_neon(&deltas[1][1], d_is[2], d_js[2]);
+  msub_neon(&deltas[1][1], d_is[3], d_js[3]);
+  msub_neon(&deltas[1][2], d_is[2], d_js[4]);
+  msub_neon(&deltas[1][2], d_is[3], d_js[5]);
+  msub_neon(&deltas[1][3], d_is[2], d_js[6]);
+  msub_neon(&deltas[1][3], d_is[3], d_js[7]);
+  msub_neon(&deltas[1][4], d_is[2], d_js[8]);
+  msub_neon(&deltas[1][4], d_is[3], d_js[9]);
+  msub_neon(&deltas[1][5], d_is[2], d_js[10]);
+  msub_neon(&deltas[1][5], d_is[3], d_js[11]);
+
+  msub_neon(&deltas[2][0], d_is[4], d_js[0]);
+  msub_neon(&deltas[2][0], d_is[5], d_js[1]);
+  msub_neon(&deltas[2][1], d_is[4], d_js[2]);
+  msub_neon(&deltas[2][1], d_is[5], d_js[3]);
+  msub_neon(&deltas[2][2], d_is[4], d_js[4]);
+  msub_neon(&deltas[2][2], d_is[5], d_js[5]);
+  msub_neon(&deltas[2][3], d_is[4], d_js[6]);
+  msub_neon(&deltas[2][3], d_is[5], d_js[7]);
+  msub_neon(&deltas[2][4], d_is[4], d_js[8]);
+  msub_neon(&deltas[2][4], d_is[5], d_js[9]);
+  msub_neon(&deltas[2][5], d_is[4], d_js[10]);
+  msub_neon(&deltas[2][5], d_is[5], d_js[11]);
+
+  msub_neon(&deltas[3][0], d_is[6], d_js[0]);
+  msub_neon(&deltas[3][0], d_is[7], d_js[1]);
+  msub_neon(&deltas[3][1], d_is[6], d_js[2]);
+  msub_neon(&deltas[3][1], d_is[7], d_js[3]);
+  msub_neon(&deltas[3][2], d_is[6], d_js[4]);
+  msub_neon(&deltas[3][2], d_is[7], d_js[5]);
+  msub_neon(&deltas[3][3], d_is[6], d_js[6]);
+  msub_neon(&deltas[3][3], d_is[7], d_js[7]);
+  msub_neon(&deltas[3][4], d_is[6], d_js[8]);
+  msub_neon(&deltas[3][4], d_is[7], d_js[9]);
+  msub_neon(&deltas[3][5], d_is[6], d_js[10]);
+  msub_neon(&deltas[3][5], d_is[7], d_js[11]);
+
+  msub_neon(&deltas[4][0], d_is[8], d_js[0]);
+  msub_neon(&deltas[4][0], d_is[9], d_js[1]);
+  msub_neon(&deltas[4][1], d_is[8], d_js[2]);
+  msub_neon(&deltas[4][1], d_is[9], d_js[3]);
+  msub_neon(&deltas[4][2], d_is[8], d_js[4]);
+  msub_neon(&deltas[4][2], d_is[9], d_js[5]);
+  msub_neon(&deltas[4][3], d_is[8], d_js[6]);
+  msub_neon(&deltas[4][3], d_is[9], d_js[7]);
+  msub_neon(&deltas[4][4], d_is[8], d_js[8]);
+  msub_neon(&deltas[4][4], d_is[9], d_js[9]);
+  msub_neon(&deltas[4][5], d_is[8], d_js[10]);
+  msub_neon(&deltas[4][5], d_is[9], d_js[11]);
+
+  msub_neon(&deltas[5][0], d_is[10], d_js[0]);
+  msub_neon(&deltas[5][0], d_is[11], d_js[1]);
+  msub_neon(&deltas[5][1], d_is[10], d_js[2]);
+  msub_neon(&deltas[5][1], d_is[11], d_js[3]);
+  msub_neon(&deltas[5][2], d_is[10], d_js[4]);
+  msub_neon(&deltas[5][2], d_is[11], d_js[5]);
+  msub_neon(&deltas[5][3], d_is[10], d_js[6]);
+  msub_neon(&deltas[5][3], d_is[11], d_js[7]);
+  msub_neon(&deltas[5][4], d_is[10], d_js[8]);
+  msub_neon(&deltas[5][4], d_is[11], d_js[9]);
+  msub_neon(&deltas[5][5], d_is[10], d_js[10]);
+  msub_neon(&deltas[5][5], d_is[11], d_js[11]);
+
+  madd_neon(&deltas[0][0], d_ie[0], d_je[0]);
+  madd_neon(&deltas[0][0], d_ie[1], d_je[1]);
+  madd_neon(&deltas[0][1], d_ie[0], d_je[2]);
+  madd_neon(&deltas[0][1], d_ie[1], d_je[3]);
+  madd_neon(&deltas[0][2], d_ie[0], d_je[4]);
+  madd_neon(&deltas[0][2], d_ie[1], d_je[5]);
+  madd_neon(&deltas[0][3], d_ie[0], d_je[6]);
+  madd_neon(&deltas[0][3], d_ie[1], d_je[7]);
+  madd_neon(&deltas[0][4], d_ie[0], d_je[8]);
+  madd_neon(&deltas[0][4], d_ie[1], d_je[9]);
+  madd_neon(&deltas[0][5], d_ie[0], d_je[10]);
+  madd_neon(&deltas[0][5], d_ie[1], d_je[11]);
+
+  madd_neon(&deltas[1][0], d_ie[2], d_je[0]);
+  madd_neon(&deltas[1][0], d_ie[3], d_je[1]);
+  madd_neon(&deltas[1][1], d_ie[2], d_je[2]);
+  madd_neon(&deltas[1][1], d_ie[3], d_je[3]);
+  madd_neon(&deltas[1][2], d_ie[2], d_je[4]);
+  madd_neon(&deltas[1][2], d_ie[3], d_je[5]);
+  madd_neon(&deltas[1][3], d_ie[2], d_je[6]);
+  madd_neon(&deltas[1][3], d_ie[3], d_je[7]);
+  madd_neon(&deltas[1][4], d_ie[2], d_je[8]);
+  madd_neon(&deltas[1][4], d_ie[3], d_je[9]);
+  madd_neon(&deltas[1][5], d_ie[2], d_je[10]);
+  madd_neon(&deltas[1][5], d_ie[3], d_je[11]);
+
+  madd_neon(&deltas[2][0], d_ie[4], d_je[0]);
+  madd_neon(&deltas[2][0], d_ie[5], d_je[1]);
+  madd_neon(&deltas[2][1], d_ie[4], d_je[2]);
+  madd_neon(&deltas[2][1], d_ie[5], d_je[3]);
+  madd_neon(&deltas[2][2], d_ie[4], d_je[4]);
+  madd_neon(&deltas[2][2], d_ie[5], d_je[5]);
+  madd_neon(&deltas[2][3], d_ie[4], d_je[6]);
+  madd_neon(&deltas[2][3], d_ie[5], d_je[7]);
+  madd_neon(&deltas[2][4], d_ie[4], d_je[8]);
+  madd_neon(&deltas[2][4], d_ie[5], d_je[9]);
+  madd_neon(&deltas[2][5], d_ie[4], d_je[10]);
+  madd_neon(&deltas[2][5], d_ie[5], d_je[11]);
+
+  madd_neon(&deltas[3][0], d_ie[6], d_je[0]);
+  madd_neon(&deltas[3][0], d_ie[7], d_je[1]);
+  madd_neon(&deltas[3][1], d_ie[6], d_je[2]);
+  madd_neon(&deltas[3][1], d_ie[7], d_je[3]);
+  madd_neon(&deltas[3][2], d_ie[6], d_je[4]);
+  madd_neon(&deltas[3][2], d_ie[7], d_je[5]);
+  madd_neon(&deltas[3][3], d_ie[6], d_je[6]);
+  madd_neon(&deltas[3][3], d_ie[7], d_je[7]);
+  madd_neon(&deltas[3][4], d_ie[6], d_je[8]);
+  madd_neon(&deltas[3][4], d_ie[7], d_je[9]);
+  madd_neon(&deltas[3][5], d_ie[6], d_je[10]);
+  madd_neon(&deltas[3][5], d_ie[7], d_je[11]);
+
+  madd_neon(&deltas[4][0], d_ie[8], d_je[0]);
+  madd_neon(&deltas[4][0], d_ie[9], d_je[1]);
+  madd_neon(&deltas[4][1], d_ie[8], d_je[2]);
+  madd_neon(&deltas[4][1], d_ie[9], d_je[3]);
+  madd_neon(&deltas[4][2], d_ie[8], d_je[4]);
+  madd_neon(&deltas[4][2], d_ie[9], d_je[5]);
+  madd_neon(&deltas[4][3], d_ie[8], d_je[6]);
+  madd_neon(&deltas[4][3], d_ie[9], d_je[7]);
+  madd_neon(&deltas[4][4], d_ie[8], d_je[8]);
+  madd_neon(&deltas[4][4], d_ie[9], d_je[9]);
+  madd_neon(&deltas[4][5], d_ie[8], d_je[10]);
+  madd_neon(&deltas[4][5], d_ie[9], d_je[11]);
+
+  madd_neon(&deltas[5][0], d_ie[10], d_je[0]);
+  madd_neon(&deltas[5][0], d_ie[11], d_je[1]);
+  madd_neon(&deltas[5][1], d_ie[10], d_je[2]);
+  madd_neon(&deltas[5][1], d_ie[11], d_je[3]);
+  madd_neon(&deltas[5][2], d_ie[10], d_je[4]);
+  madd_neon(&deltas[5][2], d_ie[11], d_je[5]);
+  madd_neon(&deltas[5][3], d_ie[10], d_je[6]);
+  madd_neon(&deltas[5][3], d_ie[11], d_je[7]);
+  madd_neon(&deltas[5][4], d_ie[10], d_je[8]);
+  madd_neon(&deltas[5][4], d_ie[11], d_je[9]);
+  madd_neon(&deltas[5][5], d_ie[10], d_je[10]);
+  madd_neon(&deltas[5][5], d_ie[11], d_je[11]);
+}
+
+static inline void update_8_stats_neon(const int64_t *const src,
+                                       const int32x4_t delta0,
+                                       const int32x4_t delta1,
+                                       int64_t *const dst) {
+  update_4_stats_neon(src + 0, delta0, dst + 0);
+  update_4_stats_neon(src + 4, delta1, dst + 4);
+}
+
+static inline void load_square_win7_neon(const int16_t *const di,
+                                         const int16_t *const dj,
+                                         const int32_t d_stride,
+                                         const int32_t height, int16x8_t *d_is,
+                                         int16x8_t *d_ie, int16x8_t *d_js,
+                                         int16x8_t *d_je) {
+  load_s16_8x6(di + 0, d_stride, &d_is[0], &d_is[2], &d_is[4], &d_is[6],
+               &d_is[8], &d_is[10]);
+  load_s16_8x6(di + 8, d_stride, &d_is[1], &d_is[3], &d_is[5], &d_is[7],
+               &d_is[9], &d_is[11]);
+  load_s16_8x6(dj + 0, d_stride, &d_js[0], &d_js[2], &d_js[4], &d_js[6],
+               &d_js[8], &d_js[10]);
+  load_s16_8x6(dj + 8, d_stride, &d_js[1], &d_js[3], &d_js[5], &d_js[7],
+               &d_js[9], &d_js[11]);
+
+  load_s16_8x6(di + height * d_stride + 0, d_stride, &d_ie[0], &d_ie[2],
+               &d_ie[4], &d_ie[6], &d_ie[8], &d_ie[10]);
+  load_s16_8x6(di + height * d_stride + 8, d_stride, &d_ie[1], &d_ie[3],
+               &d_ie[5], &d_ie[7], &d_ie[9], &d_ie[11]);
+  load_s16_8x6(dj + height * d_stride + 0, d_stride, &d_je[0], &d_je[2],
+               &d_je[4], &d_je[6], &d_je[8], &d_je[10]);
+  load_s16_8x6(dj + height * d_stride + 8, d_stride, &d_je[1], &d_je[3],
+               &d_je[5], &d_je[7], &d_je[9], &d_je[11]);
+}
+
+static inline void load_triangle_win7_neon(const int16_t *const di,
+                                           const int32_t d_stride,
+                                           const int32_t height,
+                                           int16x8_t *d_is, int16x8_t *d_ie) {
+  load_s16_8x6(di, d_stride, &d_is[0], &d_is[2], &d_is[4], &d_is[6], &d_is[8],
+               &d_is[10]);
+  load_s16_8x6(di + 8, d_stride, &d_is[1], &d_is[3], &d_is[5], &d_is[7],
+               &d_is[9], &d_is[11]);
+
+  load_s16_8x6(di + height * d_stride, d_stride, &d_ie[0], &d_ie[2], &d_ie[4],
+               &d_ie[6], &d_ie[8], &d_ie[10]);
+  load_s16_8x6(di + height * d_stride + 8, d_stride, &d_ie[1], &d_ie[3],
+               &d_ie[5], &d_ie[7], &d_ie[9], &d_ie[11]);
+}
+
+static inline void stats_left_win7_neon(const int16x8_t src[2],
+                                        const int16_t *d,
+                                        const int32_t d_stride,
+                                        int32x4_t *sum) {
+  int16x8_t dgds[WIN_7];
+
+  load_s16_8x6(d + d_stride + 0, d_stride, &dgds[0], &dgds[2], &dgds[4],
+               &dgds[6], &dgds[8], &dgds[10]);
+  load_s16_8x6(d + d_stride + 8, d_stride, &dgds[1], &dgds[3], &dgds[5],
+               &dgds[7], &dgds[9], &dgds[11]);
+
+  madd_neon(&sum[0], src[0], dgds[0]);
+  madd_neon(&sum[0], src[1], dgds[1]);
+  madd_neon(&sum[1], src[0], dgds[2]);
+  madd_neon(&sum[1], src[1], dgds[3]);
+  madd_neon(&sum[2], src[0], dgds[4]);
+  madd_neon(&sum[2], src[1], dgds[5]);
+  madd_neon(&sum[3], src[0], dgds[6]);
+  madd_neon(&sum[3], src[1], dgds[7]);
+  madd_neon(&sum[4], src[0], dgds[8]);
+  madd_neon(&sum[4], src[1], dgds[9]);
+  madd_neon(&sum[5], src[0], dgds[10]);
+  madd_neon(&sum[5], src[1], dgds[11]);
+}
+
+static inline void step3_win7_neon(const int16_t *d, const int32_t d_stride,
+                                   const int32_t width, const int32_t height,
+                                   int16x8_t *ds, int32x4_t *deltas) {
+  int32_t y = height;
+  do {
+    ds[12] = vld1q_s16(d);
+    ds[13] = vld1q_s16(d + width);
+
+    compute_delta_step3(&deltas[0], &deltas[4], ds[0], ds[1], ds[0], ds[1]);
+    compute_delta_step3(&deltas[1], &deltas[5], ds[0], ds[1], ds[2], ds[3]);
+    compute_delta_step3(&deltas[2], &deltas[6], ds[0], ds[1], ds[4], ds[5]);
+    compute_delta_step3(&deltas[3], &deltas[7], ds[0], ds[1], ds[6], ds[7]);
+    compute_delta_step3(&deltas[8], &deltas[12], ds[0], ds[1], ds[8], ds[9]);
+    compute_delta_step3(&deltas[9], &deltas[13], ds[0], ds[1], ds[10], ds[11]);
+    compute_delta_step3(&deltas[10], &deltas[14], ds[0], ds[1], ds[12], ds[13]);
+
+    ds[0] = ds[2];
+    ds[1] = ds[3];
+    ds[2] = ds[4];
+    ds[3] = ds[5];
+    ds[4] = ds[6];
+    ds[5] = ds[7];
+    ds[6] = ds[8];
+    ds[7] = ds[9];
+    ds[8] = ds[10];
+    ds[9] = ds[11];
+    ds[10] = ds[12];
+    ds[11] = ds[13];
+
+    d += d_stride;
+  } while (--y);
+}
+
+static inline void derive_triangle_win7_neon(const int16x8_t *d_is,
+                                             const int16x8_t *d_ie,
+                                             int32x4_t *deltas) {
+  msub_neon(&deltas[0], d_is[0], d_is[0]);
+  msub_neon(&deltas[0], d_is[1], d_is[1]);
+  msub_neon(&deltas[1], d_is[0], d_is[2]);
+  msub_neon(&deltas[1], d_is[1], d_is[3]);
+  msub_neon(&deltas[2], d_is[0], d_is[4]);
+  msub_neon(&deltas[2], d_is[1], d_is[5]);
+  msub_neon(&deltas[3], d_is[0], d_is[6]);
+  msub_neon(&deltas[3], d_is[1], d_is[7]);
+  msub_neon(&deltas[4], d_is[0], d_is[8]);
+  msub_neon(&deltas[4], d_is[1], d_is[9]);
+  msub_neon(&deltas[5], d_is[0], d_is[10]);
+  msub_neon(&deltas[5], d_is[1], d_is[11]);
+
+  msub_neon(&deltas[6], d_is[2], d_is[2]);
+  msub_neon(&deltas[6], d_is[3], d_is[3]);
+  msub_neon(&deltas[7], d_is[2], d_is[4]);
+  msub_neon(&deltas[7], d_is[3], d_is[5]);
+  msub_neon(&deltas[8], d_is[2], d_is[6]);
+  msub_neon(&deltas[8], d_is[3], d_is[7]);
+  msub_neon(&deltas[9], d_is[2], d_is[8]);
+  msub_neon(&deltas[9], d_is[3], d_is[9]);
+  msub_neon(&deltas[10], d_is[2], d_is[10]);
+  msub_neon(&deltas[10], d_is[3], d_is[11]);
+
+  msub_neon(&deltas[11], d_is[4], d_is[4]);
+  msub_neon(&deltas[11], d_is[5], d_is[5]);
+  msub_neon(&deltas[12], d_is[4], d_is[6]);
+  msub_neon(&deltas[12], d_is[5], d_is[7]);
+  msub_neon(&deltas[13], d_is[4], d_is[8]);
+  msub_neon(&deltas[13], d_is[5], d_is[9]);
+  msub_neon(&deltas[14], d_is[4], d_is[10]);
+  msub_neon(&deltas[14], d_is[5], d_is[11]);
+
+  msub_neon(&deltas[15], d_is[6], d_is[6]);
+  msub_neon(&deltas[15], d_is[7], d_is[7]);
+  msub_neon(&deltas[16], d_is[6], d_is[8]);
+  msub_neon(&deltas[16], d_is[7], d_is[9]);
+  msub_neon(&deltas[17], d_is[6], d_is[10]);
+  msub_neon(&deltas[17], d_is[7], d_is[11]);
+
+  msub_neon(&deltas[18], d_is[8], d_is[8]);
+  msub_neon(&deltas[18], d_is[9], d_is[9]);
+  msub_neon(&deltas[19], d_is[8], d_is[10]);
+  msub_neon(&deltas[19], d_is[9], d_is[11]);
+
+  msub_neon(&deltas[20], d_is[10], d_is[10]);
+  msub_neon(&deltas[20], d_is[11], d_is[11]);
+
+  madd_neon(&deltas[0], d_ie[0], d_ie[0]);
+  madd_neon(&deltas[0], d_ie[1], d_ie[1]);
+  madd_neon(&deltas[1], d_ie[0], d_ie[2]);
+  madd_neon(&deltas[1], d_ie[1], d_ie[3]);
+  madd_neon(&deltas[2], d_ie[0], d_ie[4]);
+  madd_neon(&deltas[2], d_ie[1], d_ie[5]);
+  madd_neon(&deltas[3], d_ie[0], d_ie[6]);
+  madd_neon(&deltas[3], d_ie[1], d_ie[7]);
+  madd_neon(&deltas[4], d_ie[0], d_ie[8]);
+  madd_neon(&deltas[4], d_ie[1], d_ie[9]);
+  madd_neon(&deltas[5], d_ie[0], d_ie[10]);
+  madd_neon(&deltas[5], d_ie[1], d_ie[11]);
+
+  madd_neon(&deltas[6], d_ie[2], d_ie[2]);
+  madd_neon(&deltas[6], d_ie[3], d_ie[3]);
+  madd_neon(&deltas[7], d_ie[2], d_ie[4]);
+  madd_neon(&deltas[7], d_ie[3], d_ie[5]);
+  madd_neon(&deltas[8], d_ie[2], d_ie[6]);
+  madd_neon(&deltas[8], d_ie[3], d_ie[7]);
+  madd_neon(&deltas[9], d_ie[2], d_ie[8]);
+  madd_neon(&deltas[9], d_ie[3], d_ie[9]);
+  madd_neon(&deltas[10], d_ie[2], d_ie[10]);
+  madd_neon(&deltas[10], d_ie[3], d_ie[11]);
+
+  madd_neon(&deltas[11], d_ie[4], d_ie[4]);
+  madd_neon(&deltas[11], d_ie[5], d_ie[5]);
+  madd_neon(&deltas[12], d_ie[4], d_ie[6]);
+  madd_neon(&deltas[12], d_ie[5], d_ie[7]);
+  madd_neon(&deltas[13], d_ie[4], d_ie[8]);
+  madd_neon(&deltas[13], d_ie[5], d_ie[9]);
+  madd_neon(&deltas[14], d_ie[4], d_ie[10]);
+  madd_neon(&deltas[14], d_ie[5], d_ie[11]);
+
+  madd_neon(&deltas[15], d_ie[6], d_ie[6]);
+  madd_neon(&deltas[15], d_ie[7], d_ie[7]);
+  madd_neon(&deltas[16], d_ie[6], d_ie[8]);
+  madd_neon(&deltas[16], d_ie[7], d_ie[9]);
+  madd_neon(&deltas[17], d_ie[6], d_ie[10]);
+  madd_neon(&deltas[17], d_ie[7], d_ie[11]);
+
+  madd_neon(&deltas[18], d_ie[8], d_ie[8]);
+  madd_neon(&deltas[18], d_ie[9], d_ie[9]);
+  madd_neon(&deltas[19], d_ie[8], d_ie[10]);
+  madd_neon(&deltas[19], d_ie[9], d_ie[11]);
+
+  madd_neon(&deltas[20], d_ie[10], d_ie[10]);
+  madd_neon(&deltas[20], d_ie[11], d_ie[11]);
+}
+
+static inline void diagonal_copy_stats_neon(const int32_t wiener_win2,
+                                            int64_t *const H) {
+  for (int32_t i = 0; i < wiener_win2 - 1; i += 4) {
+    int64x2_t in[8], out[8];
+
+    in[0] = vld1q_s64(H + (i + 0) * wiener_win2 + i + 1);
+    in[1] = vld1q_s64(H + (i + 0) * wiener_win2 + i + 3);
+    in[2] = vld1q_s64(H + (i + 1) * wiener_win2 + i + 1);
+    in[3] = vld1q_s64(H + (i + 1) * wiener_win2 + i + 3);
+    in[4] = vld1q_s64(H + (i + 2) * wiener_win2 + i + 1);
+    in[5] = vld1q_s64(H + (i + 2) * wiener_win2 + i + 3);
+    in[6] = vld1q_s64(H + (i + 3) * wiener_win2 + i + 1);
+    in[7] = vld1q_s64(H + (i + 3) * wiener_win2 + i + 3);
+
+    transpose_arrays_s64_4x4(in, out);
+
+    vst1_s64(H + (i + 1) * wiener_win2 + i, vget_low_s64(out[0]));
+    vst1q_s64(H + (i + 2) * wiener_win2 + i, out[2]);
+    vst1q_s64(H + (i + 3) * wiener_win2 + i, out[4]);
+    vst1q_s64(H + (i + 3) * wiener_win2 + i + 2, out[5]);
+    vst1q_s64(H + (i + 4) * wiener_win2 + i, out[6]);
+    vst1q_s64(H + (i + 4) * wiener_win2 + i + 2, out[7]);
+
+    for (int32_t j = i + 5; j < wiener_win2; j += 4) {
+      in[0] = vld1q_s64(H + (i + 0) * wiener_win2 + j);
+      in[1] = vld1q_s64(H + (i + 0) * wiener_win2 + j + 2);
+      in[2] = vld1q_s64(H + (i + 1) * wiener_win2 + j);
+      in[3] = vld1q_s64(H + (i + 1) * wiener_win2 + j + 2);
+      in[4] = vld1q_s64(H + (i + 2) * wiener_win2 + j);
+      in[5] = vld1q_s64(H + (i + 2) * wiener_win2 + j + 2);
+      in[6] = vld1q_s64(H + (i + 3) * wiener_win2 + j);
+      in[7] = vld1q_s64(H + (i + 3) * wiener_win2 + j + 2);
+
+      transpose_arrays_s64_4x4(in, out);
+
+      vst1q_s64(H + (j + 0) * wiener_win2 + i, out[0]);
+      vst1q_s64(H + (j + 0) * wiener_win2 + i + 2, out[1]);
+      vst1q_s64(H + (j + 1) * wiener_win2 + i, out[2]);
+      vst1q_s64(H + (j + 1) * wiener_win2 + i + 2, out[3]);
+      vst1q_s64(H + (j + 2) * wiener_win2 + i, out[4]);
+      vst1q_s64(H + (j + 2) * wiener_win2 + i + 2, out[5]);
+      vst1q_s64(H + (j + 3) * wiener_win2 + i, out[6]);
+      vst1q_s64(H + (j + 3) * wiener_win2 + i + 2, out[7]);
+    }
+  }
+}
+
+static inline int64x2_t div4_neon(const int64x2_t src) {
+#if AOM_ARCH_AARCH64
+  uint64x2_t sign = vcltzq_s64(src);
+  int64x2_t abs = vabsq_s64(src);
+  // divide by 4
+  abs = vshrq_n_s64(abs, 2);
+  // re-apply sign
+  return vbslq_s64(sign, vnegq_s64(abs), abs);
+#else
+  int64x2_t sign = vshrq_n_s64(src, 63);
+  int64x2_t abs = vsubq_s64(veorq_s64(src, sign), sign);
+  // divide by 4
+  abs = vshrq_n_s64(abs, 2);
+  // re-apply sign
+  return vsubq_s64(veorq_s64(abs, sign), sign);
+#endif  // AOM_ARCH_AARCH64
+}
+
+static inline void div4_4x4_neon(const int32_t wiener_win2, int64_t *const H,
+                                 int64x2_t out[8]) {
+  out[0] = vld1q_s64(H + 0 * wiener_win2 + 0);
+  out[1] = vld1q_s64(H + 0 * wiener_win2 + 2);
+  out[2] = vld1q_s64(H + 1 * wiener_win2 + 0);
+  out[3] = vld1q_s64(H + 1 * wiener_win2 + 2);
+  out[4] = vld1q_s64(H + 2 * wiener_win2 + 0);
+  out[5] = vld1q_s64(H + 2 * wiener_win2 + 2);
+  out[6] = vld1q_s64(H + 3 * wiener_win2 + 0);
+  out[7] = vld1q_s64(H + 3 * wiener_win2 + 2);
+
+  out[0] = div4_neon(out[0]);
+  out[1] = div4_neon(out[1]);
+  out[2] = div4_neon(out[2]);
+  out[3] = div4_neon(out[3]);
+  out[4] = div4_neon(out[4]);
+  out[5] = div4_neon(out[5]);
+  out[6] = div4_neon(out[6]);
+  out[7] = div4_neon(out[7]);
+
+  vst1q_s64(H + 0 * wiener_win2 + 0, out[0]);
+  vst1q_s64(H + 0 * wiener_win2 + 2, out[1]);
+  vst1q_s64(H + 1 * wiener_win2 + 0, out[2]);
+  vst1q_s64(H + 1 * wiener_win2 + 2, out[3]);
+  vst1q_s64(H + 2 * wiener_win2 + 0, out[4]);
+  vst1q_s64(H + 2 * wiener_win2 + 2, out[5]);
+  vst1q_s64(H + 3 * wiener_win2 + 0, out[6]);
+  vst1q_s64(H + 3 * wiener_win2 + 2, out[7]);
+}
+
+static inline int64x2_t div16_neon(const int64x2_t src) {
+#if AOM_ARCH_AARCH64
+  uint64x2_t sign = vcltzq_s64(src);
+  int64x2_t abs = vabsq_s64(src);
+  // divide by 16
+  abs = vshrq_n_s64(abs, 4);
+  // re-apply sign
+  return vbslq_s64(sign, vnegq_s64(abs), abs);
+#else
+  int64x2_t sign = vshrq_n_s64(src, 63);
+  int64x2_t abs = vsubq_s64(veorq_s64(src, sign), sign);
+  // divide by 16
+  abs = vshrq_n_s64(abs, 4);
+  // re-apply sign
+  return vsubq_s64(veorq_s64(abs, sign), sign);
+#endif  // AOM_ARCH_AARCH64
+}
+
+static inline void div16_4x4_neon(const int32_t wiener_win2, int64_t *const H,
+                                  int64x2_t out[8]) {
+  out[0] = vld1q_s64(H + 0 * wiener_win2 + 0);
+  out[1] = vld1q_s64(H + 0 * wiener_win2 + 2);
+  out[2] = vld1q_s64(H + 1 * wiener_win2 + 0);
+  out[3] = vld1q_s64(H + 1 * wiener_win2 + 2);
+  out[4] = vld1q_s64(H + 2 * wiener_win2 + 0);
+  out[5] = vld1q_s64(H + 2 * wiener_win2 + 2);
+  out[6] = vld1q_s64(H + 3 * wiener_win2 + 0);
+  out[7] = vld1q_s64(H + 3 * wiener_win2 + 2);
+
+  out[0] = div16_neon(out[0]);
+  out[1] = div16_neon(out[1]);
+  out[2] = div16_neon(out[2]);
+  out[3] = div16_neon(out[3]);
+  out[4] = div16_neon(out[4]);
+  out[5] = div16_neon(out[5]);
+  out[6] = div16_neon(out[6]);
+  out[7] = div16_neon(out[7]);
+
+  vst1q_s64(H + 0 * wiener_win2 + 0, out[0]);
+  vst1q_s64(H + 0 * wiener_win2 + 2, out[1]);
+  vst1q_s64(H + 1 * wiener_win2 + 0, out[2]);
+  vst1q_s64(H + 1 * wiener_win2 + 2, out[3]);
+  vst1q_s64(H + 2 * wiener_win2 + 0, out[4]);
+  vst1q_s64(H + 2 * wiener_win2 + 2, out[5]);
+  vst1q_s64(H + 3 * wiener_win2 + 0, out[6]);
+  vst1q_s64(H + 3 * wiener_win2 + 2, out[7]);
+}
+
+static inline void div4_diagonal_copy_stats_neon(const int32_t wiener_win2,
+                                                 int64_t *const H) {
+  for (int32_t i = 0; i < wiener_win2 - 1; i += 4) {
+    int64x2_t in[8], out[8];
+
+    div4_4x4_neon(wiener_win2, H + i * wiener_win2 + i + 1, in);
+    transpose_arrays_s64_4x4(in, out);
+
+    vst1_s64(H + (i + 1) * wiener_win2 + i + 0, vget_low_s64(out[0]));
+    vst1q_s64(H + (i + 2) * wiener_win2 + i + 0, out[2]);
+    vst1q_s64(H + (i + 3) * wiener_win2 + i + 0, out[4]);
+    vst1q_s64(H + (i + 3) * wiener_win2 + i + 2, out[5]);
+    vst1q_s64(H + (i + 4) * wiener_win2 + i + 0, out[6]);
+    vst1q_s64(H + (i + 4) * wiener_win2 + i + 2, out[7]);
+
+    for (int32_t j = i + 5; j < wiener_win2; j += 4) {
+      div4_4x4_neon(wiener_win2, H + i * wiener_win2 + j, in);
+      transpose_arrays_s64_4x4(in, out);
+
+      vst1q_s64(H + (j + 0) * wiener_win2 + i + 0, out[0]);
+      vst1q_s64(H + (j + 0) * wiener_win2 + i + 2, out[1]);
+      vst1q_s64(H + (j + 1) * wiener_win2 + i + 0, out[2]);
+      vst1q_s64(H + (j + 1) * wiener_win2 + i + 2, out[3]);
+      vst1q_s64(H + (j + 2) * wiener_win2 + i + 0, out[4]);
+      vst1q_s64(H + (j + 2) * wiener_win2 + i + 2, out[5]);
+      vst1q_s64(H + (j + 3) * wiener_win2 + i + 0, out[6]);
+      vst1q_s64(H + (j + 3) * wiener_win2 + i + 2, out[7]);
+    }
+  }
+}
+
+static inline void div16_diagonal_copy_stats_neon(const int32_t wiener_win2,
+                                                  int64_t *const H) {
+  for (int32_t i = 0; i < wiener_win2 - 1; i += 4) {
+    int64x2_t in[8], out[8];
+
+    div16_4x4_neon(wiener_win2, H + i * wiener_win2 + i + 1, in);
+    transpose_arrays_s64_4x4(in, out);
+
+    vst1_s64(H + (i + 1) * wiener_win2 + i + 0, vget_low_s64(out[0]));
+    vst1q_s64(H + (i + 2) * wiener_win2 + i + 0, out[2]);
+    vst1q_s64(H + (i + 3) * wiener_win2 + i + 0, out[4]);
+    vst1q_s64(H + (i + 3) * wiener_win2 + i + 2, out[5]);
+    vst1q_s64(H + (i + 4) * wiener_win2 + i + 0, out[6]);
+    vst1q_s64(H + (i + 4) * wiener_win2 + i + 2, out[7]);
+
+    for (int32_t j = i + 5; j < wiener_win2; j += 4) {
+      div16_4x4_neon(wiener_win2, H + i * wiener_win2 + j, in);
+      transpose_arrays_s64_4x4(in, out);
+
+      vst1q_s64(H + (j + 0) * wiener_win2 + i + 0, out[0]);
+      vst1q_s64(H + (j + 0) * wiener_win2 + i + 2, out[1]);
+      vst1q_s64(H + (j + 1) * wiener_win2 + i + 0, out[2]);
+      vst1q_s64(H + (j + 1) * wiener_win2 + i + 2, out[3]);
+      vst1q_s64(H + (j + 2) * wiener_win2 + i + 0, out[4]);
+      vst1q_s64(H + (j + 2) * wiener_win2 + i + 2, out[5]);
+      vst1q_s64(H + (j + 3) * wiener_win2 + i + 0, out[6]);
+      vst1q_s64(H + (j + 3) * wiener_win2 + i + 2, out[7]);
+    }
+  }
+}
+
 #endif  // AOM_AV1_ENCODER_ARM_PICKRST_NEON_H_
diff --git a/av1/encoder/arm/pickrst_sve.c b/av1/encoder/arm/pickrst_sve.c
index 50d4961bc..a66fde5af 100644
--- a/av1/encoder/arm/pickrst_sve.c
+++ b/av1/encoder/arm/pickrst_sve.c
@@ -142,10 +142,9 @@ static inline void acc_transpose_M(int64_t *M, const int64_t *M_trn,
 // by taking each different pair of columns, and multiplying all the elements of
 // the first one with all the elements of the second one, with a special case
 // when multiplying a column by itself.
-static inline void compute_stats_win7_sve(int16_t *dgd_avg, int dgd_avg_stride,
-                                          int16_t *src_avg, int src_avg_stride,
-                                          int width, int height, int64_t *M,
-                                          int64_t *H, int downsample_factor) {
+static inline void compute_stats_win7_downsampled_sve(
+    int16_t *dgd_avg, int dgd_avg_stride, int16_t *src_avg, int src_avg_stride,
+    int width, int height, int64_t *M, int64_t *H, int downsample_factor) {
   const int wiener_win = 7;
   const int wiener_win2 = wiener_win * wiener_win;
 
@@ -276,10 +275,9 @@ static inline void compute_stats_win7_sve(int16_t *dgd_avg, int dgd_avg_stride,
 // by taking each different pair of columns, and multiplying all the elements of
 // the first one with all the elements of the second one, with a special case
 // when multiplying a column by itself.
-static inline void compute_stats_win5_sve(int16_t *dgd_avg, int dgd_avg_stride,
-                                          int16_t *src_avg, int src_avg_stride,
-                                          int width, int height, int64_t *M,
-                                          int64_t *H, int downsample_factor) {
+static inline void compute_stats_win5_downsampled_sve(
+    int16_t *dgd_avg, int dgd_avg_stride, int16_t *src_avg, int src_avg_stride,
+    int width, int height, int64_t *M, int64_t *H, int downsample_factor) {
   const int wiener_win = 5;
   const int wiener_win2 = wiener_win * wiener_win;
 
@@ -389,12 +387,10 @@ static inline void compute_stats_win5_sve(int16_t *dgd_avg, int dgd_avg_stride,
   copy_upper_triangle(H, H_tmp, wiener_win2, downsample_factor);
 }
 
-void av1_compute_stats_sve(int wiener_win, const uint8_t *dgd,
-                           const uint8_t *src, int16_t *dgd_avg,
-                           int16_t *src_avg, int h_start, int h_end,
-                           int v_start, int v_end, int dgd_stride,
-                           int src_stride, int64_t *M, int64_t *H,
-                           int use_downsampled_wiener_stats) {
+static inline void av1_compute_stats_downsampled_sve(
+    int wiener_win, const uint8_t *dgd, const uint8_t *src, int16_t *dgd_avg,
+    int16_t *src_avg, int h_start, int h_end, int v_start, int v_end,
+    int dgd_stride, int src_stride, int64_t *M, int64_t *H) {
   assert(wiener_win == WIENER_WIN || wiener_win == WIENER_WIN_CHROMA);
 
   const int wiener_win2 = wiener_win * wiener_win;
@@ -406,8 +402,7 @@ void av1_compute_stats_sve(int wiener_win, const uint8_t *dgd,
   memset(M, 0, sizeof(*M) * wiener_win * wiener_win);
 
   const uint8_t avg = find_average_sve(dgd_start, dgd_stride, width, height);
-  const int downsample_factor =
-      use_downsampled_wiener_stats ? WIENER_STATS_DOWNSAMPLE_FACTOR : 1;
+  const int downsample_factor = WIENER_STATS_DOWNSAMPLE_FACTOR;
 
   // dgd_avg and src_avg have been memset to zero before calling this
   // function, so round up the stride to the next multiple of 8 so that we
@@ -427,7 +422,7 @@ void av1_compute_stats_sve(int wiener_win, const uint8_t *dgd,
   compute_sub_avg(dgd_win, dgd_stride, avg, dgd_avg, dgd_avg_stride,
                   width + 2 * wiener_halfwin, height + 2 * wiener_halfwin, 1);
 
-  // Compute (src - avg), downsample if necessary and store in src-avg.
+  // Compute (src - avg), downsample and store in src-avg.
   const uint8_t *src_start = src + h_start + v_start * src_stride;
   compute_sub_avg(src_start, src_stride * downsample_factor, avg, src_avg,
                   src_avg_stride, width, height, downsample_factor);
@@ -440,26 +435,89 @@ void av1_compute_stats_sve(int wiener_win, const uint8_t *dgd,
 
   if (downsample_height > 0) {
     if (wiener_win == WIENER_WIN) {
-      compute_stats_win7_sve(dgd_avg, dgd_avg_stride, src_avg, src_avg_stride,
-                             width, downsample_height, M, H, downsample_factor);
+      compute_stats_win7_downsampled_sve(
+          dgd_avg, dgd_avg_stride, src_avg, src_avg_stride, width,
+          downsample_height, M, H, downsample_factor);
     } else {
-      compute_stats_win5_sve(dgd_avg, dgd_avg_stride, src_avg, src_avg_stride,
-                             width, downsample_height, M, H, downsample_factor);
+      compute_stats_win5_downsampled_sve(
+          dgd_avg, dgd_avg_stride, src_avg, src_avg_stride, width,
+          downsample_height, M, H, downsample_factor);
     }
   }
 
   if (downsample_remainder > 0) {
     const int remainder_offset = height - downsample_remainder;
     if (wiener_win == WIENER_WIN) {
-      compute_stats_win7_sve(
+      compute_stats_win7_downsampled_sve(
           dgd_avg + remainder_offset * dgd_avg_stride, dgd_avg_stride,
           src_avg + downsample_height * src_avg_stride, src_avg_stride, width,
           1, M, H, downsample_remainder);
     } else {
-      compute_stats_win5_sve(
+      compute_stats_win5_downsampled_sve(
           dgd_avg + remainder_offset * dgd_avg_stride, dgd_avg_stride,
           src_avg + downsample_height * src_avg_stride, src_avg_stride, width,
           1, M, H, downsample_remainder);
     }
   }
 }
+
+void av1_compute_stats_sve(int wiener_win, const uint8_t *dgd,
+                           const uint8_t *src, int16_t *dgd_avg,
+                           int16_t *src_avg, int h_start, int h_end,
+                           int v_start, int v_end, int dgd_stride,
+                           int src_stride, int64_t *M, int64_t *H,
+                           int use_downsampled_wiener_stats) {
+  assert(wiener_win == WIENER_WIN || wiener_win == WIENER_WIN_CHROMA);
+
+  if (use_downsampled_wiener_stats) {
+    av1_compute_stats_downsampled_sve(wiener_win, dgd, src, dgd_avg, src_avg,
+                                      h_start, h_end, v_start, v_end,
+                                      dgd_stride, src_stride, M, H);
+    return;
+  }
+
+  const int wiener_win2 = wiener_win * wiener_win;
+  const int wiener_halfwin = wiener_win >> 1;
+  const int32_t width = h_end - h_start;
+  const int32_t height = v_end - v_start;
+  const uint8_t *dgd_start = &dgd[v_start * dgd_stride + h_start];
+  memset(H, 0, sizeof(*H) * wiener_win2 * wiener_win2);
+  memset(M, 0, sizeof(*M) * wiener_win * wiener_win);
+
+  const uint8_t avg = find_average_sve(dgd_start, dgd_stride, width, height);
+
+  // dgd_avg and src_avg have been memset to zero before calling this
+  // function, so round up the stride to the next multiple of 8 so that we
+  // don't have to worry about a tail loop when computing M.
+  const int dgd_avg_stride = ((width + 2 * wiener_halfwin) & ~7) + 8;
+  const int src_avg_stride = (width & ~7) + 8;
+
+  // Compute (dgd - avg) and store it in dgd_avg.
+  // The wiener window will slide along the dgd frame, centered on each pixel.
+  // For the top left pixel and all the pixels on the side of the frame this
+  // means half of the window will be outside of the frame. As such the actual
+  // buffer that we need to subtract the avg from will be 2 * wiener_halfwin
+  // wider and 2 * wiener_halfwin higher than the original dgd buffer.
+  const int vert_offset = v_start - wiener_halfwin;
+  const int horiz_offset = h_start - wiener_halfwin;
+  const uint8_t *dgd_win = dgd + horiz_offset + vert_offset * dgd_stride;
+  compute_sub_avg(dgd_win, dgd_stride, avg, dgd_avg, dgd_avg_stride,
+                  width + 2 * wiener_halfwin, height + 2 * wiener_halfwin, 1);
+
+  // Compute (src - avg), and store in src-avg.
+  const uint8_t *src_start = src + h_start + v_start * src_stride;
+  compute_sub_avg(src_start, src_stride, avg, src_avg, src_avg_stride, width,
+                  height, 1);
+
+  if (wiener_win == WIENER_WIN) {
+    compute_stats_win7_sve(dgd_avg, dgd_avg_stride, src_avg, src_avg_stride,
+                           width, height, M, H);
+  } else {
+    compute_stats_win5_sve(dgd_avg, dgd_avg_stride, src_avg, src_avg_stride,
+                           width, height, M, H);
+  }
+
+  // H is a symmetric matrix, so we only need to fill out the upper triangle.
+  // We can copy it down to the lower triangle outside the (i, j) loops.
+  diagonal_copy_stats_neon(wiener_win2, H);
+}
diff --git a/av1/encoder/arm/pickrst_sve.h b/av1/encoder/arm/pickrst_sve.h
index d5b533030..94c03753d 100644
--- a/av1/encoder/arm/pickrst_sve.h
+++ b/av1/encoder/arm/pickrst_sve.h
@@ -16,6 +16,7 @@
 #include <arm_sve.h>
 
 #include "aom_dsp/arm/aom_neon_sve_bridge.h"
+#include "av1/encoder/arm/pickrst_neon.h"
 
 // Swap each half of the dgd vectors so that we can accumulate the result of
 // the dot-products directly in the destination matrix.
@@ -148,4 +149,1749 @@ static inline void compute_H_two_rows_win7(int16x8_t *dgd0, int16x8_t *dgd1,
   }
 }
 
+static inline void stats_top_win5_sve(const int16x8_t src[2],
+                                      const int16x8_t dgd[2],
+                                      const int16_t *const d,
+                                      const int32_t d_stride, int64x2_t *sum_m,
+                                      int64x2_t *sum_h) {
+  int16x8_t dgds[WIENER_WIN_CHROMA * 2];
+
+  load_s16_8x5(d + 0, d_stride, &dgds[0], &dgds[2], &dgds[4], &dgds[6],
+               &dgds[8]);
+  load_s16_8x5(d + 8, d_stride, &dgds[1], &dgds[3], &dgds[5], &dgds[7],
+               &dgds[9]);
+
+  sum_m[0] = aom_sdotq_s16(sum_m[0], src[0], dgds[0]);
+  sum_m[0] = aom_sdotq_s16(sum_m[0], src[1], dgds[1]);
+  sum_m[1] = aom_sdotq_s16(sum_m[1], src[0], dgds[2]);
+  sum_m[1] = aom_sdotq_s16(sum_m[1], src[1], dgds[3]);
+  sum_m[2] = aom_sdotq_s16(sum_m[2], src[0], dgds[4]);
+  sum_m[2] = aom_sdotq_s16(sum_m[2], src[1], dgds[5]);
+  sum_m[3] = aom_sdotq_s16(sum_m[3], src[0], dgds[6]);
+  sum_m[3] = aom_sdotq_s16(sum_m[3], src[1], dgds[7]);
+  sum_m[4] = aom_sdotq_s16(sum_m[4], src[0], dgds[8]);
+  sum_m[4] = aom_sdotq_s16(sum_m[4], src[1], dgds[9]);
+
+  sum_h[0] = aom_sdotq_s16(sum_h[0], dgd[0], dgds[0]);
+  sum_h[0] = aom_sdotq_s16(sum_h[0], dgd[1], dgds[1]);
+  sum_h[1] = aom_sdotq_s16(sum_h[1], dgd[0], dgds[2]);
+  sum_h[1] = aom_sdotq_s16(sum_h[1], dgd[1], dgds[3]);
+  sum_h[2] = aom_sdotq_s16(sum_h[2], dgd[0], dgds[4]);
+  sum_h[2] = aom_sdotq_s16(sum_h[2], dgd[1], dgds[5]);
+  sum_h[3] = aom_sdotq_s16(sum_h[3], dgd[0], dgds[6]);
+  sum_h[3] = aom_sdotq_s16(sum_h[3], dgd[1], dgds[7]);
+  sum_h[4] = aom_sdotq_s16(sum_h[4], dgd[0], dgds[8]);
+  sum_h[4] = aom_sdotq_s16(sum_h[4], dgd[1], dgds[9]);
+}
+
+static inline void stats_left_win5_sve(const int16x8_t src[2], const int16_t *d,
+                                       const int32_t d_stride, int64x2_t *sum) {
+  int16x8_t dgds[WIN_CHROMA];
+
+  load_s16_8x4(d + d_stride + 0, d_stride, &dgds[0], &dgds[2], &dgds[4],
+               &dgds[6]);
+  load_s16_8x4(d + d_stride + 8, d_stride, &dgds[1], &dgds[3], &dgds[5],
+               &dgds[7]);
+
+  sum[0] = aom_sdotq_s16(sum[0], src[0], dgds[0]);
+  sum[0] = aom_sdotq_s16(sum[0], src[1], dgds[1]);
+  sum[1] = aom_sdotq_s16(sum[1], src[0], dgds[2]);
+  sum[1] = aom_sdotq_s16(sum[1], src[1], dgds[3]);
+  sum[2] = aom_sdotq_s16(sum[2], src[0], dgds[4]);
+  sum[2] = aom_sdotq_s16(sum[2], src[1], dgds[5]);
+  sum[3] = aom_sdotq_s16(sum[3], src[0], dgds[6]);
+  sum[3] = aom_sdotq_s16(sum[3], src[1], dgds[7]);
+}
+
+static inline void sub_deltas_step4_sve(int16x8_t *A, int16x8_t *B,
+                                        int64x2_t *deltas) {
+  deltas[0] = aom_sdotq_s16(deltas[0], vnegq_s16(A[0]), B[0]);
+  deltas[1] = aom_sdotq_s16(deltas[1], vnegq_s16(A[0]), B[1]);
+  deltas[2] = aom_sdotq_s16(deltas[2], vnegq_s16(A[0]), B[2]);
+  deltas[3] = aom_sdotq_s16(deltas[3], vnegq_s16(A[0]), B[3]);
+  deltas[4] = aom_sdotq_s16(deltas[4], vnegq_s16(A[0]), B[4]);
+  deltas[5] = aom_sdotq_s16(deltas[5], vnegq_s16(A[1]), B[0]);
+  deltas[6] = aom_sdotq_s16(deltas[6], vnegq_s16(A[2]), B[0]);
+  deltas[7] = aom_sdotq_s16(deltas[7], vnegq_s16(A[3]), B[0]);
+  deltas[8] = aom_sdotq_s16(deltas[8], vnegq_s16(A[4]), B[0]);
+}
+
+static inline void add_deltas_step4_sve(int16x8_t *A, int16x8_t *B,
+                                        int64x2_t *deltas) {
+  deltas[0] = aom_sdotq_s16(deltas[0], A[0], B[0]);
+  deltas[1] = aom_sdotq_s16(deltas[1], A[0], B[1]);
+  deltas[2] = aom_sdotq_s16(deltas[2], A[0], B[2]);
+  deltas[3] = aom_sdotq_s16(deltas[3], A[0], B[3]);
+  deltas[4] = aom_sdotq_s16(deltas[4], A[0], B[4]);
+  deltas[5] = aom_sdotq_s16(deltas[5], A[1], B[0]);
+  deltas[6] = aom_sdotq_s16(deltas[6], A[2], B[0]);
+  deltas[7] = aom_sdotq_s16(deltas[7], A[3], B[0]);
+  deltas[8] = aom_sdotq_s16(deltas[8], A[4], B[0]);
+}
+
+static inline void load_square_win5_sve(
+    const int16_t *const di, const int16_t *const dj, const int32_t d_stride,
+    const int32_t height, int16x8_t *d_is, int16x8_t *d_ie, int16x8_t *d_js,
+    int16x8_t *d_je, svbool_t p0, svbool_t p1) {
+  d_is[0] = svget_neonq_s16(svld1_s16(p0, di + 0 * d_stride + 0));
+  d_is[1] = svget_neonq_s16(svld1_s16(p1, di + 0 * d_stride + 8));
+  d_is[2] = svget_neonq_s16(svld1_s16(p0, di + 1 * d_stride + 0));
+  d_is[3] = svget_neonq_s16(svld1_s16(p1, di + 1 * d_stride + 8));
+  d_is[4] = svget_neonq_s16(svld1_s16(p0, di + 2 * d_stride + 0));
+  d_is[5] = svget_neonq_s16(svld1_s16(p1, di + 2 * d_stride + 8));
+  d_is[6] = svget_neonq_s16(svld1_s16(p0, di + 3 * d_stride + 0));
+  d_is[7] = svget_neonq_s16(svld1_s16(p1, di + 3 * d_stride + 8));
+
+  d_ie[0] = svget_neonq_s16(svld1_s16(p0, di + (height + 0) * d_stride + 0));
+  d_ie[1] = svget_neonq_s16(svld1_s16(p1, di + (height + 0) * d_stride + 8));
+  d_ie[2] = svget_neonq_s16(svld1_s16(p0, di + (height + 1) * d_stride + 0));
+  d_ie[3] = svget_neonq_s16(svld1_s16(p1, di + (height + 1) * d_stride + 8));
+  d_ie[4] = svget_neonq_s16(svld1_s16(p0, di + (height + 2) * d_stride + 0));
+  d_ie[5] = svget_neonq_s16(svld1_s16(p1, di + (height + 2) * d_stride + 8));
+  d_ie[6] = svget_neonq_s16(svld1_s16(p0, di + (height + 3) * d_stride + 0));
+  d_ie[7] = svget_neonq_s16(svld1_s16(p1, di + (height + 3) * d_stride + 8));
+
+  load_s16_8x4(dj + 0, d_stride, &d_js[0], &d_js[2], &d_js[4], &d_js[6]);
+  load_s16_8x4(dj + 8, d_stride, &d_js[1], &d_js[3], &d_js[5], &d_js[7]);
+  load_s16_8x4(dj + height * d_stride + 0, d_stride, &d_je[0], &d_je[2],
+               &d_je[4], &d_je[6]);
+  load_s16_8x4(dj + height * d_stride + 8, d_stride, &d_je[1], &d_je[3],
+               &d_je[5], &d_je[7]);
+}
+
+static inline void update_4_stats_sve(const int64_t *const src,
+                                      const int64x2_t *delta,
+                                      int64_t *const dst) {
+  const int64x2_t s1 = vld1q_s64(src);
+  const int64x2_t s2 = vld1q_s64(src + 2);
+
+  vst1q_s64(dst + 0, vaddq_s64(s1, delta[0]));
+  vst1q_s64(dst + 2, vaddq_s64(s2, delta[1]));
+}
+
+static inline void derive_square_win5_sve(
+    int16x8_t *d_is, const int16x8_t *d_ie, const int16x8_t *d_js,
+    const int16x8_t *d_je,
+    int64x2_t deltas[WIENER_WIN_CHROMA - 1][WIENER_WIN_CHROMA - 1]) {
+  d_is[0] = vnegq_s16(d_is[0]);
+  d_is[1] = vnegq_s16(d_is[1]);
+  d_is[2] = vnegq_s16(d_is[2]);
+  d_is[3] = vnegq_s16(d_is[3]);
+  d_is[4] = vnegq_s16(d_is[4]);
+  d_is[5] = vnegq_s16(d_is[5]);
+  d_is[6] = vnegq_s16(d_is[6]);
+  d_is[7] = vnegq_s16(d_is[7]);
+
+  deltas[0][0] = aom_sdotq_s16(deltas[0][0], d_is[0], d_js[0]);
+  deltas[0][0] = aom_sdotq_s16(deltas[0][0], d_is[1], d_js[1]);
+  deltas[0][1] = aom_sdotq_s16(deltas[0][1], d_is[0], d_js[2]);
+  deltas[0][1] = aom_sdotq_s16(deltas[0][1], d_is[1], d_js[3]);
+  deltas[0][2] = aom_sdotq_s16(deltas[0][2], d_is[0], d_js[4]);
+  deltas[0][2] = aom_sdotq_s16(deltas[0][2], d_is[1], d_js[5]);
+  deltas[0][3] = aom_sdotq_s16(deltas[0][3], d_is[0], d_js[6]);
+  deltas[0][3] = aom_sdotq_s16(deltas[0][3], d_is[1], d_js[7]);
+
+  deltas[1][0] = aom_sdotq_s16(deltas[1][0], d_is[2], d_js[0]);
+  deltas[1][0] = aom_sdotq_s16(deltas[1][0], d_is[3], d_js[1]);
+  deltas[1][1] = aom_sdotq_s16(deltas[1][1], d_is[2], d_js[2]);
+  deltas[1][1] = aom_sdotq_s16(deltas[1][1], d_is[3], d_js[3]);
+  deltas[1][2] = aom_sdotq_s16(deltas[1][2], d_is[2], d_js[4]);
+  deltas[1][2] = aom_sdotq_s16(deltas[1][2], d_is[3], d_js[5]);
+  deltas[1][3] = aom_sdotq_s16(deltas[1][3], d_is[2], d_js[6]);
+  deltas[1][3] = aom_sdotq_s16(deltas[1][3], d_is[3], d_js[7]);
+
+  deltas[2][0] = aom_sdotq_s16(deltas[2][0], d_is[4], d_js[0]);
+  deltas[2][0] = aom_sdotq_s16(deltas[2][0], d_is[5], d_js[1]);
+  deltas[2][1] = aom_sdotq_s16(deltas[2][1], d_is[4], d_js[2]);
+  deltas[2][1] = aom_sdotq_s16(deltas[2][1], d_is[5], d_js[3]);
+  deltas[2][2] = aom_sdotq_s16(deltas[2][2], d_is[4], d_js[4]);
+  deltas[2][2] = aom_sdotq_s16(deltas[2][2], d_is[5], d_js[5]);
+  deltas[2][3] = aom_sdotq_s16(deltas[2][3], d_is[4], d_js[6]);
+  deltas[2][3] = aom_sdotq_s16(deltas[2][3], d_is[5], d_js[7]);
+
+  deltas[3][0] = aom_sdotq_s16(deltas[3][0], d_is[6], d_js[0]);
+  deltas[3][0] = aom_sdotq_s16(deltas[3][0], d_is[7], d_js[1]);
+  deltas[3][1] = aom_sdotq_s16(deltas[3][1], d_is[6], d_js[2]);
+  deltas[3][1] = aom_sdotq_s16(deltas[3][1], d_is[7], d_js[3]);
+  deltas[3][2] = aom_sdotq_s16(deltas[3][2], d_is[6], d_js[4]);
+  deltas[3][2] = aom_sdotq_s16(deltas[3][2], d_is[7], d_js[5]);
+  deltas[3][3] = aom_sdotq_s16(deltas[3][3], d_is[6], d_js[6]);
+  deltas[3][3] = aom_sdotq_s16(deltas[3][3], d_is[7], d_js[7]);
+
+  deltas[0][0] = aom_sdotq_s16(deltas[0][0], d_ie[0], d_je[0]);
+  deltas[0][0] = aom_sdotq_s16(deltas[0][0], d_ie[1], d_je[1]);
+  deltas[0][1] = aom_sdotq_s16(deltas[0][1], d_ie[0], d_je[2]);
+  deltas[0][1] = aom_sdotq_s16(deltas[0][1], d_ie[1], d_je[3]);
+  deltas[0][2] = aom_sdotq_s16(deltas[0][2], d_ie[0], d_je[4]);
+  deltas[0][2] = aom_sdotq_s16(deltas[0][2], d_ie[1], d_je[5]);
+  deltas[0][3] = aom_sdotq_s16(deltas[0][3], d_ie[0], d_je[6]);
+  deltas[0][3] = aom_sdotq_s16(deltas[0][3], d_ie[1], d_je[7]);
+
+  deltas[1][0] = aom_sdotq_s16(deltas[1][0], d_ie[2], d_je[0]);
+  deltas[1][0] = aom_sdotq_s16(deltas[1][0], d_ie[3], d_je[1]);
+  deltas[1][1] = aom_sdotq_s16(deltas[1][1], d_ie[2], d_je[2]);
+  deltas[1][1] = aom_sdotq_s16(deltas[1][1], d_ie[3], d_je[3]);
+  deltas[1][2] = aom_sdotq_s16(deltas[1][2], d_ie[2], d_je[4]);
+  deltas[1][2] = aom_sdotq_s16(deltas[1][2], d_ie[3], d_je[5]);
+  deltas[1][3] = aom_sdotq_s16(deltas[1][3], d_ie[2], d_je[6]);
+  deltas[1][3] = aom_sdotq_s16(deltas[1][3], d_ie[3], d_je[7]);
+
+  deltas[2][0] = aom_sdotq_s16(deltas[2][0], d_ie[4], d_je[0]);
+  deltas[2][0] = aom_sdotq_s16(deltas[2][0], d_ie[5], d_je[1]);
+  deltas[2][1] = aom_sdotq_s16(deltas[2][1], d_ie[4], d_je[2]);
+  deltas[2][1] = aom_sdotq_s16(deltas[2][1], d_ie[5], d_je[3]);
+  deltas[2][2] = aom_sdotq_s16(deltas[2][2], d_ie[4], d_je[4]);
+  deltas[2][2] = aom_sdotq_s16(deltas[2][2], d_ie[5], d_je[5]);
+  deltas[2][3] = aom_sdotq_s16(deltas[2][3], d_ie[4], d_je[6]);
+  deltas[2][3] = aom_sdotq_s16(deltas[2][3], d_ie[5], d_je[7]);
+
+  deltas[3][0] = aom_sdotq_s16(deltas[3][0], d_ie[6], d_je[0]);
+  deltas[3][0] = aom_sdotq_s16(deltas[3][0], d_ie[7], d_je[1]);
+  deltas[3][1] = aom_sdotq_s16(deltas[3][1], d_ie[6], d_je[2]);
+  deltas[3][1] = aom_sdotq_s16(deltas[3][1], d_ie[7], d_je[3]);
+  deltas[3][2] = aom_sdotq_s16(deltas[3][2], d_ie[6], d_je[4]);
+  deltas[3][2] = aom_sdotq_s16(deltas[3][2], d_ie[7], d_je[5]);
+  deltas[3][3] = aom_sdotq_s16(deltas[3][3], d_ie[6], d_je[6]);
+  deltas[3][3] = aom_sdotq_s16(deltas[3][3], d_ie[7], d_je[7]);
+}
+
+static inline void hadd_update_4_stats_sve(const int64_t *const src,
+                                           const int64x2_t *deltas,
+                                           int64_t *const dst) {
+  int64x2_t src0 = vld1q_s64(src);
+  int64x2_t src1 = vld1q_s64(src + 2);
+  vst1q_s64(dst + 0, vaddq_s64(src0, vpaddq_s64(deltas[0], deltas[1])));
+  vst1q_s64(dst + 2, vaddq_s64(src1, vpaddq_s64(deltas[2], deltas[3])));
+}
+
+static inline void load_triangle_win5_sve(const int16_t *const di,
+                                          const int32_t d_stride,
+                                          const int32_t height, int16x8_t *d_is,
+                                          int16x8_t *d_ie, svbool_t p0,
+                                          svbool_t p1) {
+  d_is[0] = svget_neonq_s16(svld1_s16(p0, di + 0 * d_stride + 0));
+  d_is[1] = svget_neonq_s16(svld1_s16(p1, di + 0 * d_stride + 8));
+  d_is[2] = svget_neonq_s16(svld1_s16(p0, di + 1 * d_stride + 0));
+  d_is[3] = svget_neonq_s16(svld1_s16(p1, di + 1 * d_stride + 8));
+  d_is[4] = svget_neonq_s16(svld1_s16(p0, di + 2 * d_stride + 0));
+  d_is[5] = svget_neonq_s16(svld1_s16(p1, di + 2 * d_stride + 8));
+  d_is[6] = svget_neonq_s16(svld1_s16(p0, di + 3 * d_stride + 0));
+  d_is[7] = svget_neonq_s16(svld1_s16(p1, di + 3 * d_stride + 8));
+  d_ie[0] = svget_neonq_s16(svld1_s16(p0, di + (height + 0) * d_stride + 0));
+  d_ie[1] = svget_neonq_s16(svld1_s16(p1, di + (height + 0) * d_stride + 8));
+  d_ie[2] = svget_neonq_s16(svld1_s16(p0, di + (height + 1) * d_stride + 0));
+  d_ie[3] = svget_neonq_s16(svld1_s16(p1, di + (height + 1) * d_stride + 8));
+  d_ie[4] = svget_neonq_s16(svld1_s16(p0, di + (height + 2) * d_stride + 0));
+  d_ie[5] = svget_neonq_s16(svld1_s16(p1, di + (height + 2) * d_stride + 8));
+  d_ie[6] = svget_neonq_s16(svld1_s16(p0, di + (height + 3) * d_stride + 0));
+  d_ie[7] = svget_neonq_s16(svld1_s16(p1, di + (height + 3) * d_stride + 8));
+}
+
+static inline void derive_triangle_win5_sve(const int16x8_t *d_is,
+                                            const int16x8_t *d_ie,
+                                            int64x2_t *deltas) {
+  deltas[0] = aom_sdotq_s16(deltas[0], vnegq_s16(d_is[0]), d_is[0]);
+  deltas[0] = aom_sdotq_s16(deltas[0], vnegq_s16(d_is[1]), d_is[1]);
+  deltas[1] = aom_sdotq_s16(deltas[1], vnegq_s16(d_is[0]), d_is[2]);
+  deltas[1] = aom_sdotq_s16(deltas[1], vnegq_s16(d_is[1]), d_is[3]);
+  deltas[2] = aom_sdotq_s16(deltas[2], vnegq_s16(d_is[0]), d_is[4]);
+  deltas[2] = aom_sdotq_s16(deltas[2], vnegq_s16(d_is[1]), d_is[5]);
+  deltas[3] = aom_sdotq_s16(deltas[3], vnegq_s16(d_is[0]), d_is[6]);
+  deltas[3] = aom_sdotq_s16(deltas[3], vnegq_s16(d_is[1]), d_is[7]);
+  deltas[4] = aom_sdotq_s16(deltas[4], vnegq_s16(d_is[2]), d_is[2]);
+  deltas[4] = aom_sdotq_s16(deltas[4], vnegq_s16(d_is[3]), d_is[3]);
+  deltas[5] = aom_sdotq_s16(deltas[5], vnegq_s16(d_is[2]), d_is[4]);
+  deltas[5] = aom_sdotq_s16(deltas[5], vnegq_s16(d_is[3]), d_is[5]);
+  deltas[6] = aom_sdotq_s16(deltas[6], vnegq_s16(d_is[2]), d_is[6]);
+  deltas[6] = aom_sdotq_s16(deltas[6], vnegq_s16(d_is[3]), d_is[7]);
+  deltas[7] = aom_sdotq_s16(deltas[7], vnegq_s16(d_is[4]), d_is[4]);
+  deltas[7] = aom_sdotq_s16(deltas[7], vnegq_s16(d_is[5]), d_is[5]);
+  deltas[8] = aom_sdotq_s16(deltas[8], vnegq_s16(d_is[4]), d_is[6]);
+  deltas[8] = aom_sdotq_s16(deltas[8], vnegq_s16(d_is[5]), d_is[7]);
+  deltas[9] = aom_sdotq_s16(deltas[9], vnegq_s16(d_is[6]), d_is[6]);
+  deltas[9] = aom_sdotq_s16(deltas[9], vnegq_s16(d_is[7]), d_is[7]);
+
+  deltas[0] = aom_sdotq_s16(deltas[0], d_ie[0], d_ie[0]);
+  deltas[0] = aom_sdotq_s16(deltas[0], d_ie[1], d_ie[1]);
+  deltas[1] = aom_sdotq_s16(deltas[1], d_ie[0], d_ie[2]);
+  deltas[1] = aom_sdotq_s16(deltas[1], d_ie[1], d_ie[3]);
+  deltas[2] = aom_sdotq_s16(deltas[2], d_ie[0], d_ie[4]);
+  deltas[2] = aom_sdotq_s16(deltas[2], d_ie[1], d_ie[5]);
+  deltas[3] = aom_sdotq_s16(deltas[3], d_ie[0], d_ie[6]);
+  deltas[3] = aom_sdotq_s16(deltas[3], d_ie[1], d_ie[7]);
+  deltas[4] = aom_sdotq_s16(deltas[4], d_ie[2], d_ie[2]);
+  deltas[4] = aom_sdotq_s16(deltas[4], d_ie[3], d_ie[3]);
+  deltas[5] = aom_sdotq_s16(deltas[5], d_ie[2], d_ie[4]);
+  deltas[5] = aom_sdotq_s16(deltas[5], d_ie[3], d_ie[5]);
+  deltas[6] = aom_sdotq_s16(deltas[6], d_ie[2], d_ie[6]);
+  deltas[6] = aom_sdotq_s16(deltas[6], d_ie[3], d_ie[7]);
+  deltas[7] = aom_sdotq_s16(deltas[7], d_ie[4], d_ie[4]);
+  deltas[7] = aom_sdotq_s16(deltas[7], d_ie[5], d_ie[5]);
+  deltas[8] = aom_sdotq_s16(deltas[8], d_ie[4], d_ie[6]);
+  deltas[8] = aom_sdotq_s16(deltas[8], d_ie[5], d_ie[7]);
+  deltas[9] = aom_sdotq_s16(deltas[9], d_ie[6], d_ie[6]);
+  deltas[9] = aom_sdotq_s16(deltas[9], d_ie[7], d_ie[7]);
+}
+
+static inline void compute_stats_win5_sve(
+    const int16_t *const d, const int32_t d_stride, const int16_t *const s,
+    const int32_t s_stride, const int32_t width, const int32_t height,
+    int64_t *const M, int64_t *const H) {
+  const int32_t wiener_win = WIENER_WIN_CHROMA;
+  const int32_t wiener_win2 = wiener_win * wiener_win;
+  const int32_t h8 = height & ~7;
+  int32_t i, j, x, y;
+
+  // Use a predicate to compute the last columns.
+  svbool_t p0 = svwhilelt_b16_u32(0, width % 16 == 0 ? 16 : width % 16);
+  svbool_t p1 = svwhilelt_b16_u32(8, width % 16 == 0 ? 16 : width % 16);
+
+  // Step 1: Calculate the top edge of the whole matrix, i.e., the top
+  // edge of each triangle and square on the top row.
+  j = 0;
+  do {
+    const int16_t *s_t = s;
+    const int16_t *d_t = d;
+    int64x2_t sum_m[WIENER_WIN_CHROMA] = { vdupq_n_s64(0) };
+    int64x2_t sum_h[WIENER_WIN_CHROMA] = { vdupq_n_s64(0) };
+    int16x8_t src[2], dgd[2];
+
+    y = height;
+    do {
+      x = 0;
+      while (x < width - 16) {
+        src[0] = vld1q_s16(s_t + x + 0);
+        src[1] = vld1q_s16(s_t + x + 8);
+        dgd[0] = vld1q_s16(d_t + x + 0);
+        dgd[1] = vld1q_s16(d_t + x + 8);
+        stats_top_win5_sve(src, dgd, d_t + j + x, d_stride, sum_m, sum_h);
+        x += 16;
+      }
+
+      src[0] = svget_neonq_s16(svld1_s16(p0, s_t + x + 0));
+      src[1] = svget_neonq_s16(svld1_s16(p1, s_t + x + 8));
+      dgd[0] = svget_neonq_s16(svld1_s16(p0, d_t + x + 0));
+      dgd[1] = svget_neonq_s16(svld1_s16(p1, d_t + x + 8));
+
+      stats_top_win5_sve(src, dgd, d_t + j + x, d_stride, sum_m, sum_h);
+
+      s_t += s_stride;
+      d_t += d_stride;
+    } while (--y);
+
+    vst1q_s64(&M[wiener_win * j + 0], vpaddq_s64(sum_m[0], sum_m[1]));
+    vst1q_s64(&M[wiener_win * j + 2], vpaddq_s64(sum_m[2], sum_m[3]));
+    M[wiener_win * j + 4] = vaddvq_s64(sum_m[4]);
+
+    vst1q_s64(&H[wiener_win * j + 0], vpaddq_s64(sum_h[0], sum_h[1]));
+    vst1q_s64(&H[wiener_win * j + 2], vpaddq_s64(sum_h[2], sum_h[3]));
+    H[wiener_win * j + 4] = vaddvq_s64(sum_h[4]);
+  } while (++j < wiener_win);
+
+  // Step 2: Calculate the left edge of each square on the top row.
+  j = 1;
+  do {
+    const int16_t *d_t = d;
+    int64x2_t sum_h[WIENER_WIN_CHROMA - 1] = { vdupq_n_s64(0) };
+    int16x8_t dgd[2];
+
+    y = height;
+    do {
+      x = 0;
+      while (x < width - 16) {
+        dgd[0] = vld1q_s16(d_t + j + x + 0);
+        dgd[1] = vld1q_s16(d_t + j + x + 8);
+        stats_left_win5_sve(dgd, d_t + x, d_stride, sum_h);
+        x += 16;
+      }
+
+      dgd[0] = svget_neonq_s16(svld1_s16(p0, d_t + j + x + 0));
+      dgd[1] = svget_neonq_s16(svld1_s16(p1, d_t + j + x + 8));
+
+      stats_left_win5_sve(dgd, d_t + x, d_stride, sum_h);
+
+      d_t += d_stride;
+    } while (--y);
+
+    int64x2_t sum_h01 = vpaddq_s64(sum_h[0], sum_h[1]);
+    int64x2_t sum_h23 = vpaddq_s64(sum_h[2], sum_h[3]);
+    vst1_s64(&H[1 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h01));
+    vst1_s64(&H[2 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h01));
+    vst1_s64(&H[3 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h23));
+    vst1_s64(&H[4 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h23));
+
+  } while (++j < wiener_win);
+
+  // Step 3: Derive the top edge of each triangle along the diagonal. No
+  // triangle in top row.
+  {
+    const int16_t *d_t = d;
+
+    if (height % 2) {
+      int32x4_t deltas[(WIENER_WIN + 1) * 2] = { vdupq_n_s32(0) };
+      int32x4_t deltas_tr[(WIENER_WIN + 1) * 2] = { vdupq_n_s32(0) };
+      int16x8_t ds[WIENER_WIN * 2];
+
+      load_s16_8x4(d_t, d_stride, &ds[0], &ds[2], &ds[4], &ds[6]);
+      load_s16_8x4(d_t + width, d_stride, &ds[1], &ds[3], &ds[5], &ds[7]);
+      d_t += 4 * d_stride;
+
+      step3_win5_oneline_neon(&d_t, d_stride, width, height, ds, deltas);
+      transpose_arrays_s32_8x8(deltas, deltas_tr);
+
+      update_5_stats_neon(H + 0 * wiener_win * wiener_win2 + 0 * wiener_win,
+                          deltas_tr[0], vgetq_lane_s32(deltas_tr[4], 0),
+                          H + 1 * wiener_win * wiener_win2 + 1 * wiener_win);
+
+      update_5_stats_neon(H + 1 * wiener_win * wiener_win2 + 1 * wiener_win,
+                          deltas_tr[1], vgetq_lane_s32(deltas_tr[5], 0),
+                          H + 2 * wiener_win * wiener_win2 + 2 * wiener_win);
+
+      update_5_stats_neon(H + 2 * wiener_win * wiener_win2 + 2 * wiener_win,
+                          deltas_tr[2], vgetq_lane_s32(deltas_tr[6], 0),
+                          H + 3 * wiener_win * wiener_win2 + 3 * wiener_win);
+
+      update_5_stats_neon(H + 3 * wiener_win * wiener_win2 + 3 * wiener_win,
+                          deltas_tr[3], vgetq_lane_s32(deltas_tr[7], 0),
+                          H + 4 * wiener_win * wiener_win2 + 4 * wiener_win);
+
+    } else {
+      int32x4_t deltas[WIENER_WIN_CHROMA * 2] = { vdupq_n_s32(0) };
+      int16x8_t ds[WIENER_WIN_CHROMA * 2];
+
+      ds[0] = load_unaligned_s16_4x2(d_t + 0 * d_stride, width);
+      ds[1] = load_unaligned_s16_4x2(d_t + 1 * d_stride, width);
+      ds[2] = load_unaligned_s16_4x2(d_t + 2 * d_stride, width);
+      ds[3] = load_unaligned_s16_4x2(d_t + 3 * d_stride, width);
+
+      step3_win5_neon(d_t + 4 * d_stride, d_stride, width, height, ds, deltas);
+
+      transpose_elems_inplace_s32_4x4(&deltas[0], &deltas[1], &deltas[2],
+                                      &deltas[3]);
+
+      update_5_stats_neon(H + 0 * wiener_win * wiener_win2 + 0 * wiener_win,
+                          deltas[0], vgetq_lane_s32(deltas[4], 0),
+                          H + 1 * wiener_win * wiener_win2 + 1 * wiener_win);
+
+      update_5_stats_neon(H + 1 * wiener_win * wiener_win2 + 1 * wiener_win,
+                          deltas[1], vgetq_lane_s32(deltas[4], 1),
+                          H + 2 * wiener_win * wiener_win2 + 2 * wiener_win);
+
+      update_5_stats_neon(H + 2 * wiener_win * wiener_win2 + 2 * wiener_win,
+                          deltas[2], vgetq_lane_s32(deltas[4], 2),
+                          H + 3 * wiener_win * wiener_win2 + 3 * wiener_win);
+
+      update_5_stats_neon(H + 3 * wiener_win * wiener_win2 + 3 * wiener_win,
+                          deltas[3], vgetq_lane_s32(deltas[4], 3),
+                          H + 4 * wiener_win * wiener_win2 + 4 * wiener_win);
+    }
+  }
+
+  // Step 4: Derive the top and left edge of each square. No square in top and
+  // bottom row.
+  {
+    y = h8;
+
+    int16x4_t d_s[12];
+    int16x4_t d_e[12];
+    const int16_t *d_t = d;
+    int16x4_t zeros = vdup_n_s16(0);
+    load_s16_4x4(d_t, d_stride, &d_s[0], &d_s[1], &d_s[2], &d_s[3]);
+    load_s16_4x4(d_t + width, d_stride, &d_e[0], &d_e[1], &d_e[2], &d_e[3]);
+    int64x2_t deltas[6][18] = { { vdupq_n_s64(0) }, { vdupq_n_s64(0) } };
+
+    while (y >= 8) {
+      load_s16_4x8(d_t + 4 * d_stride, d_stride, &d_s[4], &d_s[5], &d_s[6],
+                   &d_s[7], &d_s[8], &d_s[9], &d_s[10], &d_s[11]);
+      load_s16_4x8(d_t + width + 4 * d_stride, d_stride, &d_e[4], &d_e[5],
+                   &d_e[6], &d_e[7], &d_e[8], &d_e[9], &d_e[10], &d_e[11]);
+
+      int16x8_t s_tr[8], e_tr[8];
+      transpose_elems_s16_4x8(d_s[0], d_s[1], d_s[2], d_s[3], d_s[4], d_s[5],
+                              d_s[6], d_s[7], &s_tr[0], &s_tr[1], &s_tr[2],
+                              &s_tr[3]);
+      transpose_elems_s16_4x8(d_s[8], d_s[9], d_s[10], d_s[11], zeros, zeros,
+                              zeros, zeros, &s_tr[4], &s_tr[5], &s_tr[6],
+                              &s_tr[7]);
+
+      transpose_elems_s16_4x8(d_e[0], d_e[1], d_e[2], d_e[3], d_e[4], d_e[5],
+                              d_e[6], d_e[7], &e_tr[0], &e_tr[1], &e_tr[2],
+                              &e_tr[3]);
+      transpose_elems_s16_4x8(d_e[8], d_e[9], d_e[10], d_e[11], zeros, zeros,
+                              zeros, zeros, &e_tr[4], &e_tr[5], &e_tr[6],
+                              &e_tr[7]);
+
+      int16x8_t start_col0[5], start_col1[5], start_col2[5], start_col3[5];
+      start_col0[0] = s_tr[0];
+      start_col0[1] = vextq_s16(s_tr[0], s_tr[4], 1);
+      start_col0[2] = vextq_s16(s_tr[0], s_tr[4], 2);
+      start_col0[3] = vextq_s16(s_tr[0], s_tr[4], 3);
+      start_col0[4] = vextq_s16(s_tr[0], s_tr[4], 4);
+
+      start_col1[0] = s_tr[1];
+      start_col1[1] = vextq_s16(s_tr[1], s_tr[5], 1);
+      start_col1[2] = vextq_s16(s_tr[1], s_tr[5], 2);
+      start_col1[3] = vextq_s16(s_tr[1], s_tr[5], 3);
+      start_col1[4] = vextq_s16(s_tr[1], s_tr[5], 4);
+
+      start_col2[0] = s_tr[2];
+      start_col2[1] = vextq_s16(s_tr[2], s_tr[6], 1);
+      start_col2[2] = vextq_s16(s_tr[2], s_tr[6], 2);
+      start_col2[3] = vextq_s16(s_tr[2], s_tr[6], 3);
+      start_col2[4] = vextq_s16(s_tr[2], s_tr[6], 4);
+
+      start_col3[0] = s_tr[3];
+      start_col3[1] = vextq_s16(s_tr[3], s_tr[7], 1);
+      start_col3[2] = vextq_s16(s_tr[3], s_tr[7], 2);
+      start_col3[3] = vextq_s16(s_tr[3], s_tr[7], 3);
+      start_col3[4] = vextq_s16(s_tr[3], s_tr[7], 4);
+
+      // i = 1, j = 2;
+      sub_deltas_step4_sve(start_col0, start_col1, deltas[0]);
+
+      // i = 1, j = 3;
+      sub_deltas_step4_sve(start_col0, start_col2, deltas[1]);
+
+      // i = 1, j = 4
+      sub_deltas_step4_sve(start_col0, start_col3, deltas[2]);
+
+      // i = 2, j =3
+      sub_deltas_step4_sve(start_col1, start_col2, deltas[3]);
+
+      // i = 2, j = 4
+      sub_deltas_step4_sve(start_col1, start_col3, deltas[4]);
+
+      // i = 3, j = 4
+      sub_deltas_step4_sve(start_col2, start_col3, deltas[5]);
+
+      int16x8_t end_col0[5], end_col1[5], end_col2[5], end_col3[5];
+      end_col0[0] = e_tr[0];
+      end_col0[1] = vextq_s16(e_tr[0], e_tr[4], 1);
+      end_col0[2] = vextq_s16(e_tr[0], e_tr[4], 2);
+      end_col0[3] = vextq_s16(e_tr[0], e_tr[4], 3);
+      end_col0[4] = vextq_s16(e_tr[0], e_tr[4], 4);
+
+      end_col1[0] = e_tr[1];
+      end_col1[1] = vextq_s16(e_tr[1], e_tr[5], 1);
+      end_col1[2] = vextq_s16(e_tr[1], e_tr[5], 2);
+      end_col1[3] = vextq_s16(e_tr[1], e_tr[5], 3);
+      end_col1[4] = vextq_s16(e_tr[1], e_tr[5], 4);
+
+      end_col2[0] = e_tr[2];
+      end_col2[1] = vextq_s16(e_tr[2], e_tr[6], 1);
+      end_col2[2] = vextq_s16(e_tr[2], e_tr[6], 2);
+      end_col2[3] = vextq_s16(e_tr[2], e_tr[6], 3);
+      end_col2[4] = vextq_s16(e_tr[2], e_tr[6], 4);
+
+      end_col3[0] = e_tr[3];
+      end_col3[1] = vextq_s16(e_tr[3], e_tr[7], 1);
+      end_col3[2] = vextq_s16(e_tr[3], e_tr[7], 2);
+      end_col3[3] = vextq_s16(e_tr[3], e_tr[7], 3);
+      end_col3[4] = vextq_s16(e_tr[3], e_tr[7], 4);
+
+      // i = 1, j = 2;
+      add_deltas_step4_sve(end_col0, end_col1, deltas[0]);
+
+      // i = 1, j = 3;
+      add_deltas_step4_sve(end_col0, end_col2, deltas[1]);
+
+      // i = 1, j = 4
+      add_deltas_step4_sve(end_col0, end_col3, deltas[2]);
+
+      // i = 2, j =3
+      add_deltas_step4_sve(end_col1, end_col2, deltas[3]);
+
+      // i = 2, j = 4
+      add_deltas_step4_sve(end_col1, end_col3, deltas[4]);
+
+      // i = 3, j = 4
+      add_deltas_step4_sve(end_col2, end_col3, deltas[5]);
+
+      d_s[0] = d_s[8];
+      d_s[1] = d_s[9];
+      d_s[2] = d_s[10];
+      d_s[3] = d_s[11];
+      d_e[0] = d_e[8];
+      d_e[1] = d_e[9];
+      d_e[2] = d_e[10];
+      d_e[3] = d_e[11];
+
+      d_t += 8 * d_stride;
+      y -= 8;
+    }
+
+    if (h8 != height) {
+      const int16x8_t mask_h = vld1q_s16(&mask_16bit[16] - (height % 8));
+
+      load_s16_4x8(d_t + 4 * d_stride, d_stride, &d_s[4], &d_s[5], &d_s[6],
+                   &d_s[7], &d_s[8], &d_s[9], &d_s[10], &d_s[11]);
+      load_s16_4x8(d_t + width + 4 * d_stride, d_stride, &d_e[4], &d_e[5],
+                   &d_e[6], &d_e[7], &d_e[8], &d_e[9], &d_e[10], &d_e[11]);
+      int16x8_t s_tr[8], e_tr[8];
+      transpose_elems_s16_4x8(d_s[0], d_s[1], d_s[2], d_s[3], d_s[4], d_s[5],
+                              d_s[6], d_s[7], &s_tr[0], &s_tr[1], &s_tr[2],
+                              &s_tr[3]);
+      transpose_elems_s16_4x8(d_s[8], d_s[9], d_s[10], d_s[11], zeros, zeros,
+                              zeros, zeros, &s_tr[4], &s_tr[5], &s_tr[6],
+                              &s_tr[7]);
+      transpose_elems_s16_4x8(d_e[0], d_e[1], d_e[2], d_e[3], d_e[4], d_e[5],
+                              d_e[6], d_e[7], &e_tr[0], &e_tr[1], &e_tr[2],
+                              &e_tr[3]);
+      transpose_elems_s16_4x8(d_e[8], d_e[9], d_e[10], d_e[11], zeros, zeros,
+                              zeros, zeros, &e_tr[4], &e_tr[5], &e_tr[6],
+                              &e_tr[7]);
+
+      int16x8_t start_col0[5], start_col1[5], start_col2[5], start_col3[5];
+      start_col0[0] = vandq_s16(s_tr[0], mask_h);
+      start_col0[1] = vandq_s16(vextq_s16(s_tr[0], s_tr[4], 1), mask_h);
+      start_col0[2] = vandq_s16(vextq_s16(s_tr[0], s_tr[4], 2), mask_h);
+      start_col0[3] = vandq_s16(vextq_s16(s_tr[0], s_tr[4], 3), mask_h);
+      start_col0[4] = vandq_s16(vextq_s16(s_tr[0], s_tr[4], 4), mask_h);
+
+      start_col1[0] = vandq_s16(s_tr[1], mask_h);
+      start_col1[1] = vandq_s16(vextq_s16(s_tr[1], s_tr[5], 1), mask_h);
+      start_col1[2] = vandq_s16(vextq_s16(s_tr[1], s_tr[5], 2), mask_h);
+      start_col1[3] = vandq_s16(vextq_s16(s_tr[1], s_tr[5], 3), mask_h);
+      start_col1[4] = vandq_s16(vextq_s16(s_tr[1], s_tr[5], 4), mask_h);
+
+      start_col2[0] = vandq_s16(s_tr[2], mask_h);
+      start_col2[1] = vandq_s16(vextq_s16(s_tr[2], s_tr[6], 1), mask_h);
+      start_col2[2] = vandq_s16(vextq_s16(s_tr[2], s_tr[6], 2), mask_h);
+      start_col2[3] = vandq_s16(vextq_s16(s_tr[2], s_tr[6], 3), mask_h);
+      start_col2[4] = vandq_s16(vextq_s16(s_tr[2], s_tr[6], 4), mask_h);
+
+      start_col3[0] = vandq_s16(s_tr[3], mask_h);
+      start_col3[1] = vandq_s16(vextq_s16(s_tr[3], s_tr[7], 1), mask_h);
+      start_col3[2] = vandq_s16(vextq_s16(s_tr[3], s_tr[7], 2), mask_h);
+      start_col3[3] = vandq_s16(vextq_s16(s_tr[3], s_tr[7], 3), mask_h);
+      start_col3[4] = vandq_s16(vextq_s16(s_tr[3], s_tr[7], 4), mask_h);
+
+      // i = 1, j = 2;
+      sub_deltas_step4_sve(start_col0, start_col1, deltas[0]);
+
+      // i = 1, j = 3;
+      sub_deltas_step4_sve(start_col0, start_col2, deltas[1]);
+
+      // i = 1, j = 4
+      sub_deltas_step4_sve(start_col0, start_col3, deltas[2]);
+
+      // i = 2, j = 3
+      sub_deltas_step4_sve(start_col1, start_col2, deltas[3]);
+
+      // i = 2, j = 4
+      sub_deltas_step4_sve(start_col1, start_col3, deltas[4]);
+
+      // i = 3, j = 4
+      sub_deltas_step4_sve(start_col2, start_col3, deltas[5]);
+
+      int16x8_t end_col0[5], end_col1[5], end_col2[5], end_col3[5];
+      end_col0[0] = vandq_s16(e_tr[0], mask_h);
+      end_col0[1] = vandq_s16(vextq_s16(e_tr[0], e_tr[4], 1), mask_h);
+      end_col0[2] = vandq_s16(vextq_s16(e_tr[0], e_tr[4], 2), mask_h);
+      end_col0[3] = vandq_s16(vextq_s16(e_tr[0], e_tr[4], 3), mask_h);
+      end_col0[4] = vandq_s16(vextq_s16(e_tr[0], e_tr[4], 4), mask_h);
+
+      end_col1[0] = vandq_s16(e_tr[1], mask_h);
+      end_col1[1] = vandq_s16(vextq_s16(e_tr[1], e_tr[5], 1), mask_h);
+      end_col1[2] = vandq_s16(vextq_s16(e_tr[1], e_tr[5], 2), mask_h);
+      end_col1[3] = vandq_s16(vextq_s16(e_tr[1], e_tr[5], 3), mask_h);
+      end_col1[4] = vandq_s16(vextq_s16(e_tr[1], e_tr[5], 4), mask_h);
+
+      end_col2[0] = vandq_s16(e_tr[2], mask_h);
+      end_col2[1] = vandq_s16(vextq_s16(e_tr[2], e_tr[6], 1), mask_h);
+      end_col2[2] = vandq_s16(vextq_s16(e_tr[2], e_tr[6], 2), mask_h);
+      end_col2[3] = vandq_s16(vextq_s16(e_tr[2], e_tr[6], 3), mask_h);
+      end_col2[4] = vandq_s16(vextq_s16(e_tr[2], e_tr[6], 4), mask_h);
+
+      end_col3[0] = vandq_s16(e_tr[3], mask_h);
+      end_col3[1] = vandq_s16(vextq_s16(e_tr[3], e_tr[7], 1), mask_h);
+      end_col3[2] = vandq_s16(vextq_s16(e_tr[3], e_tr[7], 2), mask_h);
+      end_col3[3] = vandq_s16(vextq_s16(e_tr[3], e_tr[7], 3), mask_h);
+      end_col3[4] = vandq_s16(vextq_s16(e_tr[3], e_tr[7], 4), mask_h);
+
+      // i = 1, j = 2;
+      add_deltas_step4_sve(end_col0, end_col1, deltas[0]);
+
+      // i = 1, j = 3;
+      add_deltas_step4_sve(end_col0, end_col2, deltas[1]);
+
+      // i = 1, j = 4
+      add_deltas_step4_sve(end_col0, end_col3, deltas[2]);
+
+      // i = 2, j =3
+      add_deltas_step4_sve(end_col1, end_col2, deltas[3]);
+
+      // i = 2, j = 4
+      add_deltas_step4_sve(end_col1, end_col3, deltas[4]);
+
+      // i = 3, j = 4
+      add_deltas_step4_sve(end_col2, end_col3, deltas[5]);
+    }
+
+    int64_t single_delta[6];
+
+    deltas[0][0] = vpaddq_s64(deltas[0][0], deltas[0][1]);
+    deltas[0][1] = vpaddq_s64(deltas[0][2], deltas[0][3]);
+    deltas[1][0] = vpaddq_s64(deltas[1][0], deltas[1][1]);
+    deltas[1][1] = vpaddq_s64(deltas[1][2], deltas[1][3]);
+    deltas[2][0] = vpaddq_s64(deltas[2][0], deltas[2][1]);
+    deltas[2][1] = vpaddq_s64(deltas[2][2], deltas[2][3]);
+    deltas[3][0] = vpaddq_s64(deltas[3][0], deltas[3][1]);
+    deltas[3][1] = vpaddq_s64(deltas[3][2], deltas[3][3]);
+    deltas[4][0] = vpaddq_s64(deltas[4][0], deltas[4][1]);
+    deltas[4][1] = vpaddq_s64(deltas[4][2], deltas[4][3]);
+    deltas[5][0] = vpaddq_s64(deltas[5][0], deltas[5][1]);
+    deltas[5][1] = vpaddq_s64(deltas[5][2], deltas[5][3]);
+
+    deltas[0][5] = vpaddq_s64(deltas[0][5], deltas[0][6]);
+    deltas[0][7] = vpaddq_s64(deltas[0][7], deltas[0][8]);
+    deltas[1][5] = vpaddq_s64(deltas[1][5], deltas[1][6]);
+    deltas[1][7] = vpaddq_s64(deltas[1][7], deltas[1][8]);
+    deltas[2][5] = vpaddq_s64(deltas[2][5], deltas[2][6]);
+    deltas[2][7] = vpaddq_s64(deltas[2][7], deltas[2][8]);
+    deltas[3][5] = vpaddq_s64(deltas[3][5], deltas[3][6]);
+    deltas[3][7] = vpaddq_s64(deltas[3][7], deltas[3][8]);
+    deltas[4][5] = vpaddq_s64(deltas[4][5], deltas[4][6]);
+    deltas[4][7] = vpaddq_s64(deltas[4][7], deltas[4][8]);
+    deltas[5][5] = vpaddq_s64(deltas[5][5], deltas[5][6]);
+    deltas[5][7] = vpaddq_s64(deltas[5][7], deltas[5][8]);
+
+    vst1q_s64(single_delta + 0, vpaddq_s64(deltas[0][4], deltas[1][4]));
+    vst1q_s64(single_delta + 2, vpaddq_s64(deltas[2][4], deltas[3][4]));
+    vst1q_s64(single_delta + 4, vpaddq_s64(deltas[4][4], deltas[5][4]));
+
+    int idx = 0;
+    for (i = 1; i < wiener_win - 1; i++) {
+      for (j = i + 1; j < wiener_win; j++) {
+        update_4_stats_sve(
+            H + (i - 1) * wiener_win * wiener_win2 + (j - 1) * wiener_win,
+            deltas[idx], H + i * wiener_win * wiener_win2 + j * wiener_win);
+        H[i * wiener_win * wiener_win2 + j * wiener_win + 4] =
+            H[(i - 1) * wiener_win * wiener_win2 + (j - 1) * wiener_win + 4] +
+            single_delta[idx];
+
+        H[(i * wiener_win + 1) * wiener_win2 + j * wiener_win] =
+            H[((i - 1) * wiener_win + 1) * wiener_win2 + (j - 1) * wiener_win] +
+            vgetq_lane_s64(deltas[idx][5], 0);
+        H[(i * wiener_win + 2) * wiener_win2 + j * wiener_win] =
+            H[((i - 1) * wiener_win + 2) * wiener_win2 + (j - 1) * wiener_win] +
+            vgetq_lane_s64(deltas[idx][5], 1);
+        H[(i * wiener_win + 3) * wiener_win2 + j * wiener_win] =
+            H[((i - 1) * wiener_win + 3) * wiener_win2 + (j - 1) * wiener_win] +
+            vgetq_lane_s64(deltas[idx][7], 0);
+        H[(i * wiener_win + 4) * wiener_win2 + j * wiener_win] =
+            H[((i - 1) * wiener_win + 4) * wiener_win2 + (j - 1) * wiener_win] +
+            vgetq_lane_s64(deltas[idx][7], 1);
+
+        idx++;
+      }
+    }
+  }
+
+  // Step 5: Derive other points of each square. No square in bottom row.
+  i = 0;
+  do {
+    const int16_t *const di = d + i;
+
+    j = i + 1;
+    do {
+      const int16_t *const dj = d + j;
+      int64x2_t deltas[WIENER_WIN_CHROMA - 1][WIENER_WIN_CHROMA - 1] = {
+        { vdupq_n_s64(0) }, { vdupq_n_s64(0) }
+      };
+      int16x8_t d_is[WIN_CHROMA], d_ie[WIN_CHROMA];
+      int16x8_t d_js[WIN_CHROMA], d_je[WIN_CHROMA];
+
+      x = 0;
+      while (x < width - 16) {
+        load_square_win5_neon(di + x, dj + x, d_stride, height, d_is, d_ie,
+                              d_js, d_je);
+        derive_square_win5_sve(d_is, d_ie, d_js, d_je, deltas);
+        x += 16;
+      }
+
+      load_square_win5_sve(di + x, dj + x, d_stride, height, d_is, d_ie, d_js,
+                           d_je, p0, p1);
+      derive_square_win5_sve(d_is, d_ie, d_js, d_je, deltas);
+
+      hadd_update_4_stats_sve(
+          H + (i * wiener_win + 0) * wiener_win2 + j * wiener_win, deltas[0],
+          H + (i * wiener_win + 1) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_4_stats_sve(
+          H + (i * wiener_win + 1) * wiener_win2 + j * wiener_win, deltas[1],
+          H + (i * wiener_win + 2) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_4_stats_sve(
+          H + (i * wiener_win + 2) * wiener_win2 + j * wiener_win, deltas[2],
+          H + (i * wiener_win + 3) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_4_stats_sve(
+          H + (i * wiener_win + 3) * wiener_win2 + j * wiener_win, deltas[3],
+          H + (i * wiener_win + 4) * wiener_win2 + j * wiener_win + 1);
+    } while (++j < wiener_win);
+  } while (++i < wiener_win - 1);
+
+  // Step 6: Derive other points of each upper triangle along the diagonal.
+  i = 0;
+  do {
+    const int16_t *const di = d + i;
+    int64x2_t deltas[WIENER_WIN_CHROMA * 2 + 1] = { vdupq_n_s64(0) };
+    int16x8_t d_is[WIN_CHROMA], d_ie[WIN_CHROMA];
+
+    x = 0;
+    while (x < width - 16) {
+      load_triangle_win5_neon(di + x, d_stride, height, d_is, d_ie);
+      derive_triangle_win5_sve(d_is, d_ie, deltas);
+      x += 16;
+    }
+
+    load_triangle_win5_sve(di + x, d_stride, height, d_is, d_ie, p0, p1);
+    derive_triangle_win5_sve(d_is, d_ie, deltas);
+
+    // Row 1: 4 points
+    hadd_update_4_stats_sve(
+        H + (i * wiener_win + 0) * wiener_win2 + i * wiener_win, deltas,
+        H + (i * wiener_win + 1) * wiener_win2 + i * wiener_win + 1);
+
+    // Row 2: 3 points
+    int64x2_t src0 =
+        vld1q_s64(H + (i * wiener_win + 1) * wiener_win2 + i * wiener_win + 1);
+    vst1q_s64(H + (i * wiener_win + 2) * wiener_win2 + i * wiener_win + 2,
+              vaddq_s64(src0, vpaddq_s64(deltas[4], deltas[5])));
+
+    int64x2_t deltas69 = vpaddq_s64(deltas[6], deltas[9]);
+
+    H[(i * wiener_win + 2) * wiener_win2 + i * wiener_win + 4] =
+        H[(i * wiener_win + 1) * wiener_win2 + i * wiener_win + 3] +
+        vgetq_lane_s64(deltas69, 0);
+
+    // Row 3: 2 points
+    int64x2_t src1 =
+        vld1q_s64(H + (i * wiener_win + 2) * wiener_win2 + i * wiener_win + 2);
+    vst1q_s64(H + (i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3,
+              vaddq_s64(src1, vpaddq_s64(deltas[7], deltas[8])));
+
+    // Row 4: 1 point
+    H[(i * wiener_win + 4) * wiener_win2 + i * wiener_win + 4] =
+        H[(i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3] +
+        vgetq_lane_s64(deltas69, 1);
+  } while (++i < wiener_win);
+}
+
+static inline void stats_top_win7_sve(const int16x8_t src[2],
+                                      const int16x8_t dgd[2],
+                                      const int16_t *const d,
+                                      const int32_t d_stride, int64x2_t *sum_m,
+                                      int64x2_t *sum_h) {
+  int16x8_t dgds[WIENER_WIN * 2];
+
+  load_s16_8x7(d + 0, d_stride, &dgds[0], &dgds[2], &dgds[4], &dgds[6],
+               &dgds[8], &dgds[10], &dgds[12]);
+  load_s16_8x7(d + 8, d_stride, &dgds[1], &dgds[3], &dgds[5], &dgds[7],
+               &dgds[9], &dgds[11], &dgds[13]);
+
+  sum_m[0] = aom_sdotq_s16(sum_m[0], src[0], dgds[0]);
+  sum_m[0] = aom_sdotq_s16(sum_m[0], src[1], dgds[1]);
+  sum_m[1] = aom_sdotq_s16(sum_m[1], src[0], dgds[2]);
+  sum_m[1] = aom_sdotq_s16(sum_m[1], src[1], dgds[3]);
+  sum_m[2] = aom_sdotq_s16(sum_m[2], src[0], dgds[4]);
+  sum_m[2] = aom_sdotq_s16(sum_m[2], src[1], dgds[5]);
+  sum_m[3] = aom_sdotq_s16(sum_m[3], src[0], dgds[6]);
+  sum_m[3] = aom_sdotq_s16(sum_m[3], src[1], dgds[7]);
+  sum_m[4] = aom_sdotq_s16(sum_m[4], src[0], dgds[8]);
+  sum_m[4] = aom_sdotq_s16(sum_m[4], src[1], dgds[9]);
+  sum_m[5] = aom_sdotq_s16(sum_m[5], src[0], dgds[10]);
+  sum_m[5] = aom_sdotq_s16(sum_m[5], src[1], dgds[11]);
+  sum_m[6] = aom_sdotq_s16(sum_m[6], src[0], dgds[12]);
+  sum_m[6] = aom_sdotq_s16(sum_m[6], src[1], dgds[13]);
+
+  sum_h[0] = aom_sdotq_s16(sum_h[0], dgd[0], dgds[0]);
+  sum_h[0] = aom_sdotq_s16(sum_h[0], dgd[1], dgds[1]);
+  sum_h[1] = aom_sdotq_s16(sum_h[1], dgd[0], dgds[2]);
+  sum_h[1] = aom_sdotq_s16(sum_h[1], dgd[1], dgds[3]);
+  sum_h[2] = aom_sdotq_s16(sum_h[2], dgd[0], dgds[4]);
+  sum_h[2] = aom_sdotq_s16(sum_h[2], dgd[1], dgds[5]);
+  sum_h[3] = aom_sdotq_s16(sum_h[3], dgd[0], dgds[6]);
+  sum_h[3] = aom_sdotq_s16(sum_h[3], dgd[1], dgds[7]);
+  sum_h[4] = aom_sdotq_s16(sum_h[4], dgd[0], dgds[8]);
+  sum_h[4] = aom_sdotq_s16(sum_h[4], dgd[1], dgds[9]);
+  sum_h[5] = aom_sdotq_s16(sum_h[5], dgd[0], dgds[10]);
+  sum_h[5] = aom_sdotq_s16(sum_h[5], dgd[1], dgds[11]);
+  sum_h[6] = aom_sdotq_s16(sum_h[6], dgd[0], dgds[12]);
+  sum_h[6] = aom_sdotq_s16(sum_h[6], dgd[1], dgds[13]);
+}
+
+static inline void stats_left_win7_sve(const int16x8_t src[2], const int16_t *d,
+                                       const int32_t d_stride, int64x2_t *sum) {
+  int16x8_t dgds[WIN_7];
+
+  load_s16_8x6(d + d_stride + 0, d_stride, &dgds[0], &dgds[2], &dgds[4],
+               &dgds[6], &dgds[8], &dgds[10]);
+  load_s16_8x6(d + d_stride + 8, d_stride, &dgds[1], &dgds[3], &dgds[5],
+               &dgds[7], &dgds[9], &dgds[11]);
+
+  sum[0] = aom_sdotq_s16(sum[0], src[0], dgds[0]);
+  sum[0] = aom_sdotq_s16(sum[0], src[1], dgds[1]);
+  sum[1] = aom_sdotq_s16(sum[1], src[0], dgds[2]);
+  sum[1] = aom_sdotq_s16(sum[1], src[1], dgds[3]);
+  sum[2] = aom_sdotq_s16(sum[2], src[0], dgds[4]);
+  sum[2] = aom_sdotq_s16(sum[2], src[1], dgds[5]);
+  sum[3] = aom_sdotq_s16(sum[3], src[0], dgds[6]);
+  sum[3] = aom_sdotq_s16(sum[3], src[1], dgds[7]);
+  sum[4] = aom_sdotq_s16(sum[4], src[0], dgds[8]);
+  sum[4] = aom_sdotq_s16(sum[4], src[1], dgds[9]);
+  sum[5] = aom_sdotq_s16(sum[5], src[0], dgds[10]);
+  sum[5] = aom_sdotq_s16(sum[5], src[1], dgds[11]);
+}
+
+static inline void load_square_win7_sve(
+    const int16_t *const di, const int16_t *const dj, const int32_t d_stride,
+    const int32_t height, int16x8_t *d_is, int16x8_t *d_ie, int16x8_t *d_js,
+    int16x8_t *d_je, svbool_t p0, svbool_t p1) {
+  d_is[0] = svget_neonq_s16(svld1_s16(p0, di + 0 * d_stride + 0));
+  d_is[1] = svget_neonq_s16(svld1_s16(p1, di + 0 * d_stride + 8));
+  d_is[2] = svget_neonq_s16(svld1_s16(p0, di + 1 * d_stride + 0));
+  d_is[3] = svget_neonq_s16(svld1_s16(p1, di + 1 * d_stride + 8));
+  d_is[4] = svget_neonq_s16(svld1_s16(p0, di + 2 * d_stride + 0));
+  d_is[5] = svget_neonq_s16(svld1_s16(p1, di + 2 * d_stride + 8));
+  d_is[6] = svget_neonq_s16(svld1_s16(p0, di + 3 * d_stride + 0));
+  d_is[7] = svget_neonq_s16(svld1_s16(p1, di + 3 * d_stride + 8));
+  d_is[8] = svget_neonq_s16(svld1_s16(p0, di + 4 * d_stride + 0));
+  d_is[9] = svget_neonq_s16(svld1_s16(p1, di + 4 * d_stride + 8));
+  d_is[10] = svget_neonq_s16(svld1_s16(p0, di + 5 * d_stride + 0));
+  d_is[11] = svget_neonq_s16(svld1_s16(p1, di + 5 * d_stride + 8));
+
+  d_ie[0] = svget_neonq_s16(svld1_s16(p0, di + (height + 0) * d_stride + 0));
+  d_ie[1] = svget_neonq_s16(svld1_s16(p1, di + (height + 0) * d_stride + 8));
+  d_ie[2] = svget_neonq_s16(svld1_s16(p0, di + (height + 1) * d_stride + 0));
+  d_ie[3] = svget_neonq_s16(svld1_s16(p1, di + (height + 1) * d_stride + 8));
+  d_ie[4] = svget_neonq_s16(svld1_s16(p0, di + (height + 2) * d_stride + 0));
+  d_ie[5] = svget_neonq_s16(svld1_s16(p1, di + (height + 2) * d_stride + 8));
+  d_ie[6] = svget_neonq_s16(svld1_s16(p0, di + (height + 3) * d_stride + 0));
+  d_ie[7] = svget_neonq_s16(svld1_s16(p1, di + (height + 3) * d_stride + 8));
+  d_ie[8] = svget_neonq_s16(svld1_s16(p0, di + (height + 4) * d_stride + 0));
+  d_ie[9] = svget_neonq_s16(svld1_s16(p1, di + (height + 4) * d_stride + 8));
+  d_ie[10] = svget_neonq_s16(svld1_s16(p0, di + (height + 5) * d_stride + 0));
+  d_ie[11] = svget_neonq_s16(svld1_s16(p1, di + (height + 5) * d_stride + 8));
+
+  load_s16_8x6(dj + 0, d_stride, &d_js[0], &d_js[2], &d_js[4], &d_js[6],
+               &d_js[8], &d_js[10]);
+  load_s16_8x6(dj + 8, d_stride, &d_js[1], &d_js[3], &d_js[5], &d_js[7],
+               &d_js[9], &d_js[11]);
+  load_s16_8x6(dj + height * d_stride + 0, d_stride, &d_je[0], &d_je[2],
+               &d_je[4], &d_je[6], &d_je[8], &d_je[10]);
+  load_s16_8x6(dj + height * d_stride + 8, d_stride, &d_je[1], &d_je[3],
+               &d_je[5], &d_je[7], &d_je[9], &d_je[11]);
+}
+
+static inline void derive_square_win7_sve(int16x8_t *d_is,
+                                          const int16x8_t *d_ie,
+                                          const int16x8_t *d_js,
+                                          const int16x8_t *d_je,
+                                          int64x2_t deltas[][WIN_7]) {
+  d_is[0] = vnegq_s16(d_is[0]);
+  d_is[1] = vnegq_s16(d_is[1]);
+  d_is[2] = vnegq_s16(d_is[2]);
+  d_is[3] = vnegq_s16(d_is[3]);
+  d_is[4] = vnegq_s16(d_is[4]);
+  d_is[5] = vnegq_s16(d_is[5]);
+  d_is[6] = vnegq_s16(d_is[6]);
+  d_is[7] = vnegq_s16(d_is[7]);
+  d_is[8] = vnegq_s16(d_is[8]);
+  d_is[9] = vnegq_s16(d_is[9]);
+  d_is[10] = vnegq_s16(d_is[10]);
+  d_is[11] = vnegq_s16(d_is[11]);
+
+  deltas[0][0] = aom_sdotq_s16(deltas[0][0], d_is[0], d_js[0]);
+  deltas[0][0] = aom_sdotq_s16(deltas[0][0], d_is[1], d_js[1]);
+  deltas[0][1] = aom_sdotq_s16(deltas[0][1], d_is[0], d_js[2]);
+  deltas[0][1] = aom_sdotq_s16(deltas[0][1], d_is[1], d_js[3]);
+  deltas[0][2] = aom_sdotq_s16(deltas[0][2], d_is[0], d_js[4]);
+  deltas[0][2] = aom_sdotq_s16(deltas[0][2], d_is[1], d_js[5]);
+  deltas[0][3] = aom_sdotq_s16(deltas[0][3], d_is[0], d_js[6]);
+  deltas[0][3] = aom_sdotq_s16(deltas[0][3], d_is[1], d_js[7]);
+  deltas[0][4] = aom_sdotq_s16(deltas[0][4], d_is[0], d_js[8]);
+  deltas[0][4] = aom_sdotq_s16(deltas[0][4], d_is[1], d_js[9]);
+  deltas[0][5] = aom_sdotq_s16(deltas[0][5], d_is[0], d_js[10]);
+  deltas[0][5] = aom_sdotq_s16(deltas[0][5], d_is[1], d_js[11]);
+
+  deltas[1][0] = aom_sdotq_s16(deltas[1][0], d_is[2], d_js[0]);
+  deltas[1][0] = aom_sdotq_s16(deltas[1][0], d_is[3], d_js[1]);
+  deltas[1][1] = aom_sdotq_s16(deltas[1][1], d_is[2], d_js[2]);
+  deltas[1][1] = aom_sdotq_s16(deltas[1][1], d_is[3], d_js[3]);
+  deltas[1][2] = aom_sdotq_s16(deltas[1][2], d_is[2], d_js[4]);
+  deltas[1][2] = aom_sdotq_s16(deltas[1][2], d_is[3], d_js[5]);
+  deltas[1][3] = aom_sdotq_s16(deltas[1][3], d_is[2], d_js[6]);
+  deltas[1][3] = aom_sdotq_s16(deltas[1][3], d_is[3], d_js[7]);
+  deltas[1][4] = aom_sdotq_s16(deltas[1][4], d_is[2], d_js[8]);
+  deltas[1][4] = aom_sdotq_s16(deltas[1][4], d_is[3], d_js[9]);
+  deltas[1][5] = aom_sdotq_s16(deltas[1][5], d_is[2], d_js[10]);
+  deltas[1][5] = aom_sdotq_s16(deltas[1][5], d_is[3], d_js[11]);
+
+  deltas[2][0] = aom_sdotq_s16(deltas[2][0], d_is[4], d_js[0]);
+  deltas[2][0] = aom_sdotq_s16(deltas[2][0], d_is[5], d_js[1]);
+  deltas[2][1] = aom_sdotq_s16(deltas[2][1], d_is[4], d_js[2]);
+  deltas[2][1] = aom_sdotq_s16(deltas[2][1], d_is[5], d_js[3]);
+  deltas[2][2] = aom_sdotq_s16(deltas[2][2], d_is[4], d_js[4]);
+  deltas[2][2] = aom_sdotq_s16(deltas[2][2], d_is[5], d_js[5]);
+  deltas[2][3] = aom_sdotq_s16(deltas[2][3], d_is[4], d_js[6]);
+  deltas[2][3] = aom_sdotq_s16(deltas[2][3], d_is[5], d_js[7]);
+  deltas[2][4] = aom_sdotq_s16(deltas[2][4], d_is[4], d_js[8]);
+  deltas[2][4] = aom_sdotq_s16(deltas[2][4], d_is[5], d_js[9]);
+  deltas[2][5] = aom_sdotq_s16(deltas[2][5], d_is[4], d_js[10]);
+  deltas[2][5] = aom_sdotq_s16(deltas[2][5], d_is[5], d_js[11]);
+
+  deltas[3][0] = aom_sdotq_s16(deltas[3][0], d_is[6], d_js[0]);
+  deltas[3][0] = aom_sdotq_s16(deltas[3][0], d_is[7], d_js[1]);
+  deltas[3][1] = aom_sdotq_s16(deltas[3][1], d_is[6], d_js[2]);
+  deltas[3][1] = aom_sdotq_s16(deltas[3][1], d_is[7], d_js[3]);
+  deltas[3][2] = aom_sdotq_s16(deltas[3][2], d_is[6], d_js[4]);
+  deltas[3][2] = aom_sdotq_s16(deltas[3][2], d_is[7], d_js[5]);
+  deltas[3][3] = aom_sdotq_s16(deltas[3][3], d_is[6], d_js[6]);
+  deltas[3][3] = aom_sdotq_s16(deltas[3][3], d_is[7], d_js[7]);
+  deltas[3][4] = aom_sdotq_s16(deltas[3][4], d_is[6], d_js[8]);
+  deltas[3][4] = aom_sdotq_s16(deltas[3][4], d_is[7], d_js[9]);
+  deltas[3][5] = aom_sdotq_s16(deltas[3][5], d_is[6], d_js[10]);
+  deltas[3][5] = aom_sdotq_s16(deltas[3][5], d_is[7], d_js[11]);
+
+  deltas[4][0] = aom_sdotq_s16(deltas[4][0], d_is[8], d_js[0]);
+  deltas[4][0] = aom_sdotq_s16(deltas[4][0], d_is[9], d_js[1]);
+  deltas[4][1] = aom_sdotq_s16(deltas[4][1], d_is[8], d_js[2]);
+  deltas[4][1] = aom_sdotq_s16(deltas[4][1], d_is[9], d_js[3]);
+  deltas[4][2] = aom_sdotq_s16(deltas[4][2], d_is[8], d_js[4]);
+  deltas[4][2] = aom_sdotq_s16(deltas[4][2], d_is[9], d_js[5]);
+  deltas[4][3] = aom_sdotq_s16(deltas[4][3], d_is[8], d_js[6]);
+  deltas[4][3] = aom_sdotq_s16(deltas[4][3], d_is[9], d_js[7]);
+  deltas[4][4] = aom_sdotq_s16(deltas[4][4], d_is[8], d_js[8]);
+  deltas[4][4] = aom_sdotq_s16(deltas[4][4], d_is[9], d_js[9]);
+  deltas[4][5] = aom_sdotq_s16(deltas[4][5], d_is[8], d_js[10]);
+  deltas[4][5] = aom_sdotq_s16(deltas[4][5], d_is[9], d_js[11]);
+
+  deltas[5][0] = aom_sdotq_s16(deltas[5][0], d_is[10], d_js[0]);
+  deltas[5][0] = aom_sdotq_s16(deltas[5][0], d_is[11], d_js[1]);
+  deltas[5][1] = aom_sdotq_s16(deltas[5][1], d_is[10], d_js[2]);
+  deltas[5][1] = aom_sdotq_s16(deltas[5][1], d_is[11], d_js[3]);
+  deltas[5][2] = aom_sdotq_s16(deltas[5][2], d_is[10], d_js[4]);
+  deltas[5][2] = aom_sdotq_s16(deltas[5][2], d_is[11], d_js[5]);
+  deltas[5][3] = aom_sdotq_s16(deltas[5][3], d_is[10], d_js[6]);
+  deltas[5][3] = aom_sdotq_s16(deltas[5][3], d_is[11], d_js[7]);
+  deltas[5][4] = aom_sdotq_s16(deltas[5][4], d_is[10], d_js[8]);
+  deltas[5][4] = aom_sdotq_s16(deltas[5][4], d_is[11], d_js[9]);
+  deltas[5][5] = aom_sdotq_s16(deltas[5][5], d_is[10], d_js[10]);
+  deltas[5][5] = aom_sdotq_s16(deltas[5][5], d_is[11], d_js[11]);
+
+  deltas[0][0] = aom_sdotq_s16(deltas[0][0], d_ie[0], d_je[0]);
+  deltas[0][0] = aom_sdotq_s16(deltas[0][0], d_ie[1], d_je[1]);
+  deltas[0][1] = aom_sdotq_s16(deltas[0][1], d_ie[0], d_je[2]);
+  deltas[0][1] = aom_sdotq_s16(deltas[0][1], d_ie[1], d_je[3]);
+  deltas[0][2] = aom_sdotq_s16(deltas[0][2], d_ie[0], d_je[4]);
+  deltas[0][2] = aom_sdotq_s16(deltas[0][2], d_ie[1], d_je[5]);
+  deltas[0][3] = aom_sdotq_s16(deltas[0][3], d_ie[0], d_je[6]);
+  deltas[0][3] = aom_sdotq_s16(deltas[0][3], d_ie[1], d_je[7]);
+  deltas[0][4] = aom_sdotq_s16(deltas[0][4], d_ie[0], d_je[8]);
+  deltas[0][4] = aom_sdotq_s16(deltas[0][4], d_ie[1], d_je[9]);
+  deltas[0][5] = aom_sdotq_s16(deltas[0][5], d_ie[0], d_je[10]);
+  deltas[0][5] = aom_sdotq_s16(deltas[0][5], d_ie[1], d_je[11]);
+
+  deltas[1][0] = aom_sdotq_s16(deltas[1][0], d_ie[2], d_je[0]);
+  deltas[1][0] = aom_sdotq_s16(deltas[1][0], d_ie[3], d_je[1]);
+  deltas[1][1] = aom_sdotq_s16(deltas[1][1], d_ie[2], d_je[2]);
+  deltas[1][1] = aom_sdotq_s16(deltas[1][1], d_ie[3], d_je[3]);
+  deltas[1][2] = aom_sdotq_s16(deltas[1][2], d_ie[2], d_je[4]);
+  deltas[1][2] = aom_sdotq_s16(deltas[1][2], d_ie[3], d_je[5]);
+  deltas[1][3] = aom_sdotq_s16(deltas[1][3], d_ie[2], d_je[6]);
+  deltas[1][3] = aom_sdotq_s16(deltas[1][3], d_ie[3], d_je[7]);
+  deltas[1][4] = aom_sdotq_s16(deltas[1][4], d_ie[2], d_je[8]);
+  deltas[1][4] = aom_sdotq_s16(deltas[1][4], d_ie[3], d_je[9]);
+  deltas[1][5] = aom_sdotq_s16(deltas[1][5], d_ie[2], d_je[10]);
+  deltas[1][5] = aom_sdotq_s16(deltas[1][5], d_ie[3], d_je[11]);
+
+  deltas[2][0] = aom_sdotq_s16(deltas[2][0], d_ie[4], d_je[0]);
+  deltas[2][0] = aom_sdotq_s16(deltas[2][0], d_ie[5], d_je[1]);
+  deltas[2][1] = aom_sdotq_s16(deltas[2][1], d_ie[4], d_je[2]);
+  deltas[2][1] = aom_sdotq_s16(deltas[2][1], d_ie[5], d_je[3]);
+  deltas[2][2] = aom_sdotq_s16(deltas[2][2], d_ie[4], d_je[4]);
+  deltas[2][2] = aom_sdotq_s16(deltas[2][2], d_ie[5], d_je[5]);
+  deltas[2][3] = aom_sdotq_s16(deltas[2][3], d_ie[4], d_je[6]);
+  deltas[2][3] = aom_sdotq_s16(deltas[2][3], d_ie[5], d_je[7]);
+  deltas[2][4] = aom_sdotq_s16(deltas[2][4], d_ie[4], d_je[8]);
+  deltas[2][4] = aom_sdotq_s16(deltas[2][4], d_ie[5], d_je[9]);
+  deltas[2][5] = aom_sdotq_s16(deltas[2][5], d_ie[4], d_je[10]);
+  deltas[2][5] = aom_sdotq_s16(deltas[2][5], d_ie[5], d_je[11]);
+
+  deltas[3][0] = aom_sdotq_s16(deltas[3][0], d_ie[6], d_je[0]);
+  deltas[3][0] = aom_sdotq_s16(deltas[3][0], d_ie[7], d_je[1]);
+  deltas[3][1] = aom_sdotq_s16(deltas[3][1], d_ie[6], d_je[2]);
+  deltas[3][1] = aom_sdotq_s16(deltas[3][1], d_ie[7], d_je[3]);
+  deltas[3][2] = aom_sdotq_s16(deltas[3][2], d_ie[6], d_je[4]);
+  deltas[3][2] = aom_sdotq_s16(deltas[3][2], d_ie[7], d_je[5]);
+  deltas[3][3] = aom_sdotq_s16(deltas[3][3], d_ie[6], d_je[6]);
+  deltas[3][3] = aom_sdotq_s16(deltas[3][3], d_ie[7], d_je[7]);
+  deltas[3][4] = aom_sdotq_s16(deltas[3][4], d_ie[6], d_je[8]);
+  deltas[3][4] = aom_sdotq_s16(deltas[3][4], d_ie[7], d_je[9]);
+  deltas[3][5] = aom_sdotq_s16(deltas[3][5], d_ie[6], d_je[10]);
+  deltas[3][5] = aom_sdotq_s16(deltas[3][5], d_ie[7], d_je[11]);
+
+  deltas[4][0] = aom_sdotq_s16(deltas[4][0], d_ie[8], d_je[0]);
+  deltas[4][0] = aom_sdotq_s16(deltas[4][0], d_ie[9], d_je[1]);
+  deltas[4][1] = aom_sdotq_s16(deltas[4][1], d_ie[8], d_je[2]);
+  deltas[4][1] = aom_sdotq_s16(deltas[4][1], d_ie[9], d_je[3]);
+  deltas[4][2] = aom_sdotq_s16(deltas[4][2], d_ie[8], d_je[4]);
+  deltas[4][2] = aom_sdotq_s16(deltas[4][2], d_ie[9], d_je[5]);
+  deltas[4][3] = aom_sdotq_s16(deltas[4][3], d_ie[8], d_je[6]);
+  deltas[4][3] = aom_sdotq_s16(deltas[4][3], d_ie[9], d_je[7]);
+  deltas[4][4] = aom_sdotq_s16(deltas[4][4], d_ie[8], d_je[8]);
+  deltas[4][4] = aom_sdotq_s16(deltas[4][4], d_ie[9], d_je[9]);
+  deltas[4][5] = aom_sdotq_s16(deltas[4][5], d_ie[8], d_je[10]);
+  deltas[4][5] = aom_sdotq_s16(deltas[4][5], d_ie[9], d_je[11]);
+
+  deltas[5][0] = aom_sdotq_s16(deltas[5][0], d_ie[10], d_je[0]);
+  deltas[5][0] = aom_sdotq_s16(deltas[5][0], d_ie[11], d_je[1]);
+  deltas[5][1] = aom_sdotq_s16(deltas[5][1], d_ie[10], d_je[2]);
+  deltas[5][1] = aom_sdotq_s16(deltas[5][1], d_ie[11], d_je[3]);
+  deltas[5][2] = aom_sdotq_s16(deltas[5][2], d_ie[10], d_je[4]);
+  deltas[5][2] = aom_sdotq_s16(deltas[5][2], d_ie[11], d_je[5]);
+  deltas[5][3] = aom_sdotq_s16(deltas[5][3], d_ie[10], d_je[6]);
+  deltas[5][3] = aom_sdotq_s16(deltas[5][3], d_ie[11], d_je[7]);
+  deltas[5][4] = aom_sdotq_s16(deltas[5][4], d_ie[10], d_je[8]);
+  deltas[5][4] = aom_sdotq_s16(deltas[5][4], d_ie[11], d_je[9]);
+  deltas[5][5] = aom_sdotq_s16(deltas[5][5], d_ie[10], d_je[10]);
+  deltas[5][5] = aom_sdotq_s16(deltas[5][5], d_ie[11], d_je[11]);
+}
+
+static inline void hadd_update_6_stats_sve(const int64_t *const src,
+                                           const int64x2_t *deltas,
+                                           int64_t *const dst) {
+  int64x2_t src0 = vld1q_s64(src + 0);
+  int64x2_t src1 = vld1q_s64(src + 2);
+  int64x2_t src2 = vld1q_s64(src + 4);
+
+  int64x2_t deltas01 = vpaddq_s64(deltas[0], deltas[1]);
+  int64x2_t deltas23 = vpaddq_s64(deltas[2], deltas[3]);
+  int64x2_t deltas45 = vpaddq_s64(deltas[4], deltas[5]);
+
+  vst1q_s64(dst + 0, vaddq_s64(src0, deltas01));
+  vst1q_s64(dst + 2, vaddq_s64(src1, deltas23));
+  vst1q_s64(dst + 4, vaddq_s64(src2, deltas45));
+}
+
+static inline void load_triangle_win7_sve(const int16_t *const di,
+                                          const int32_t d_stride,
+                                          const int32_t height, int16x8_t *d_is,
+                                          int16x8_t *d_ie, svbool_t p0,
+                                          svbool_t p1) {
+  d_is[0] = svget_neonq_s16(svld1_s16(p0, di + 0 * d_stride + 0));
+  d_is[1] = svget_neonq_s16(svld1_s16(p1, di + 0 * d_stride + 8));
+  d_is[2] = svget_neonq_s16(svld1_s16(p0, di + 1 * d_stride + 0));
+  d_is[3] = svget_neonq_s16(svld1_s16(p1, di + 1 * d_stride + 8));
+  d_is[4] = svget_neonq_s16(svld1_s16(p0, di + 2 * d_stride + 0));
+  d_is[5] = svget_neonq_s16(svld1_s16(p1, di + 2 * d_stride + 8));
+  d_is[6] = svget_neonq_s16(svld1_s16(p0, di + 3 * d_stride + 0));
+  d_is[7] = svget_neonq_s16(svld1_s16(p1, di + 3 * d_stride + 8));
+  d_is[8] = svget_neonq_s16(svld1_s16(p0, di + 4 * d_stride + 0));
+  d_is[9] = svget_neonq_s16(svld1_s16(p1, di + 4 * d_stride + 8));
+  d_is[10] = svget_neonq_s16(svld1_s16(p0, di + 5 * d_stride + 0));
+  d_is[11] = svget_neonq_s16(svld1_s16(p1, di + 5 * d_stride + 8));
+
+  d_ie[0] = svget_neonq_s16(svld1_s16(p0, di + (height + 0) * d_stride + 0));
+  d_ie[1] = svget_neonq_s16(svld1_s16(p1, di + (height + 0) * d_stride + 8));
+  d_ie[2] = svget_neonq_s16(svld1_s16(p0, di + (height + 1) * d_stride + 0));
+  d_ie[3] = svget_neonq_s16(svld1_s16(p1, di + (height + 1) * d_stride + 8));
+  d_ie[4] = svget_neonq_s16(svld1_s16(p0, di + (height + 2) * d_stride + 0));
+  d_ie[5] = svget_neonq_s16(svld1_s16(p1, di + (height + 2) * d_stride + 8));
+  d_ie[6] = svget_neonq_s16(svld1_s16(p0, di + (height + 3) * d_stride + 0));
+  d_ie[7] = svget_neonq_s16(svld1_s16(p1, di + (height + 3) * d_stride + 8));
+  d_ie[8] = svget_neonq_s16(svld1_s16(p0, di + (height + 4) * d_stride + 0));
+  d_ie[9] = svget_neonq_s16(svld1_s16(p1, di + (height + 4) * d_stride + 8));
+  d_ie[10] = svget_neonq_s16(svld1_s16(p0, di + (height + 5) * d_stride + 0));
+  d_ie[11] = svget_neonq_s16(svld1_s16(p1, di + (height + 5) * d_stride + 8));
+}
+
+static inline void derive_triangle_win7_sve(const int16x8_t *d_is,
+                                            const int16x8_t *d_ie,
+                                            int64x2_t *deltas) {
+  deltas[0] = aom_sdotq_s16(deltas[0], vnegq_s16(d_is[0]), d_is[0]);
+  deltas[0] = aom_sdotq_s16(deltas[0], vnegq_s16(d_is[1]), d_is[1]);
+  deltas[1] = aom_sdotq_s16(deltas[1], vnegq_s16(d_is[0]), d_is[2]);
+  deltas[1] = aom_sdotq_s16(deltas[1], vnegq_s16(d_is[1]), d_is[3]);
+  deltas[2] = aom_sdotq_s16(deltas[2], vnegq_s16(d_is[0]), d_is[4]);
+  deltas[2] = aom_sdotq_s16(deltas[2], vnegq_s16(d_is[1]), d_is[5]);
+  deltas[3] = aom_sdotq_s16(deltas[3], vnegq_s16(d_is[0]), d_is[6]);
+  deltas[3] = aom_sdotq_s16(deltas[3], vnegq_s16(d_is[1]), d_is[7]);
+  deltas[4] = aom_sdotq_s16(deltas[4], vnegq_s16(d_is[0]), d_is[8]);
+  deltas[4] = aom_sdotq_s16(deltas[4], vnegq_s16(d_is[1]), d_is[9]);
+  deltas[5] = aom_sdotq_s16(deltas[5], vnegq_s16(d_is[0]), d_is[10]);
+  deltas[5] = aom_sdotq_s16(deltas[5], vnegq_s16(d_is[1]), d_is[11]);
+
+  deltas[6] = aom_sdotq_s16(deltas[6], vnegq_s16(d_is[2]), d_is[2]);
+  deltas[6] = aom_sdotq_s16(deltas[6], vnegq_s16(d_is[3]), d_is[3]);
+  deltas[7] = aom_sdotq_s16(deltas[7], vnegq_s16(d_is[2]), d_is[4]);
+  deltas[7] = aom_sdotq_s16(deltas[7], vnegq_s16(d_is[3]), d_is[5]);
+  deltas[8] = aom_sdotq_s16(deltas[8], vnegq_s16(d_is[2]), d_is[6]);
+  deltas[8] = aom_sdotq_s16(deltas[8], vnegq_s16(d_is[3]), d_is[7]);
+  deltas[9] = aom_sdotq_s16(deltas[9], vnegq_s16(d_is[2]), d_is[8]);
+  deltas[9] = aom_sdotq_s16(deltas[9], vnegq_s16(d_is[3]), d_is[9]);
+  deltas[10] = aom_sdotq_s16(deltas[10], vnegq_s16(d_is[2]), d_is[10]);
+  deltas[10] = aom_sdotq_s16(deltas[10], vnegq_s16(d_is[3]), d_is[11]);
+
+  deltas[11] = aom_sdotq_s16(deltas[11], vnegq_s16(d_is[4]), d_is[4]);
+  deltas[11] = aom_sdotq_s16(deltas[11], vnegq_s16(d_is[5]), d_is[5]);
+  deltas[12] = aom_sdotq_s16(deltas[12], vnegq_s16(d_is[4]), d_is[6]);
+  deltas[12] = aom_sdotq_s16(deltas[12], vnegq_s16(d_is[5]), d_is[7]);
+  deltas[13] = aom_sdotq_s16(deltas[13], vnegq_s16(d_is[4]), d_is[8]);
+  deltas[13] = aom_sdotq_s16(deltas[13], vnegq_s16(d_is[5]), d_is[9]);
+  deltas[14] = aom_sdotq_s16(deltas[14], vnegq_s16(d_is[4]), d_is[10]);
+  deltas[14] = aom_sdotq_s16(deltas[14], vnegq_s16(d_is[5]), d_is[11]);
+
+  deltas[15] = aom_sdotq_s16(deltas[15], vnegq_s16(d_is[6]), d_is[6]);
+  deltas[15] = aom_sdotq_s16(deltas[15], vnegq_s16(d_is[7]), d_is[7]);
+  deltas[16] = aom_sdotq_s16(deltas[16], vnegq_s16(d_is[6]), d_is[8]);
+  deltas[16] = aom_sdotq_s16(deltas[16], vnegq_s16(d_is[7]), d_is[9]);
+  deltas[17] = aom_sdotq_s16(deltas[17], vnegq_s16(d_is[6]), d_is[10]);
+  deltas[17] = aom_sdotq_s16(deltas[17], vnegq_s16(d_is[7]), d_is[11]);
+
+  deltas[18] = aom_sdotq_s16(deltas[18], vnegq_s16(d_is[8]), d_is[8]);
+  deltas[18] = aom_sdotq_s16(deltas[18], vnegq_s16(d_is[9]), d_is[9]);
+  deltas[19] = aom_sdotq_s16(deltas[19], vnegq_s16(d_is[8]), d_is[10]);
+  deltas[19] = aom_sdotq_s16(deltas[19], vnegq_s16(d_is[9]), d_is[11]);
+
+  deltas[20] = aom_sdotq_s16(deltas[20], vnegq_s16(d_is[10]), d_is[10]);
+  deltas[20] = aom_sdotq_s16(deltas[20], vnegq_s16(d_is[11]), d_is[11]);
+
+  deltas[0] = aom_sdotq_s16(deltas[0], d_ie[0], d_ie[0]);
+  deltas[0] = aom_sdotq_s16(deltas[0], d_ie[1], d_ie[1]);
+  deltas[1] = aom_sdotq_s16(deltas[1], d_ie[0], d_ie[2]);
+  deltas[1] = aom_sdotq_s16(deltas[1], d_ie[1], d_ie[3]);
+  deltas[2] = aom_sdotq_s16(deltas[2], d_ie[0], d_ie[4]);
+  deltas[2] = aom_sdotq_s16(deltas[2], d_ie[1], d_ie[5]);
+  deltas[3] = aom_sdotq_s16(deltas[3], d_ie[0], d_ie[6]);
+  deltas[3] = aom_sdotq_s16(deltas[3], d_ie[1], d_ie[7]);
+  deltas[4] = aom_sdotq_s16(deltas[4], d_ie[0], d_ie[8]);
+  deltas[4] = aom_sdotq_s16(deltas[4], d_ie[1], d_ie[9]);
+  deltas[5] = aom_sdotq_s16(deltas[5], d_ie[0], d_ie[10]);
+  deltas[5] = aom_sdotq_s16(deltas[5], d_ie[1], d_ie[11]);
+
+  deltas[6] = aom_sdotq_s16(deltas[6], d_ie[2], d_ie[2]);
+  deltas[6] = aom_sdotq_s16(deltas[6], d_ie[3], d_ie[3]);
+  deltas[7] = aom_sdotq_s16(deltas[7], d_ie[2], d_ie[4]);
+  deltas[7] = aom_sdotq_s16(deltas[7], d_ie[3], d_ie[5]);
+  deltas[8] = aom_sdotq_s16(deltas[8], d_ie[2], d_ie[6]);
+  deltas[8] = aom_sdotq_s16(deltas[8], d_ie[3], d_ie[7]);
+  deltas[9] = aom_sdotq_s16(deltas[9], d_ie[2], d_ie[8]);
+  deltas[9] = aom_sdotq_s16(deltas[9], d_ie[3], d_ie[9]);
+  deltas[10] = aom_sdotq_s16(deltas[10], d_ie[2], d_ie[10]);
+  deltas[10] = aom_sdotq_s16(deltas[10], d_ie[3], d_ie[11]);
+
+  deltas[11] = aom_sdotq_s16(deltas[11], d_ie[4], d_ie[4]);
+  deltas[11] = aom_sdotq_s16(deltas[11], d_ie[5], d_ie[5]);
+  deltas[12] = aom_sdotq_s16(deltas[12], d_ie[4], d_ie[6]);
+  deltas[12] = aom_sdotq_s16(deltas[12], d_ie[5], d_ie[7]);
+  deltas[13] = aom_sdotq_s16(deltas[13], d_ie[4], d_ie[8]);
+  deltas[13] = aom_sdotq_s16(deltas[13], d_ie[5], d_ie[9]);
+  deltas[14] = aom_sdotq_s16(deltas[14], d_ie[4], d_ie[10]);
+  deltas[14] = aom_sdotq_s16(deltas[14], d_ie[5], d_ie[11]);
+
+  deltas[15] = aom_sdotq_s16(deltas[15], d_ie[6], d_ie[6]);
+  deltas[15] = aom_sdotq_s16(deltas[15], d_ie[7], d_ie[7]);
+  deltas[16] = aom_sdotq_s16(deltas[16], d_ie[6], d_ie[8]);
+  deltas[16] = aom_sdotq_s16(deltas[16], d_ie[7], d_ie[9]);
+  deltas[17] = aom_sdotq_s16(deltas[17], d_ie[6], d_ie[10]);
+  deltas[17] = aom_sdotq_s16(deltas[17], d_ie[7], d_ie[11]);
+
+  deltas[18] = aom_sdotq_s16(deltas[18], d_ie[8], d_ie[8]);
+  deltas[18] = aom_sdotq_s16(deltas[18], d_ie[9], d_ie[9]);
+  deltas[19] = aom_sdotq_s16(deltas[19], d_ie[8], d_ie[10]);
+  deltas[19] = aom_sdotq_s16(deltas[19], d_ie[9], d_ie[11]);
+
+  deltas[20] = aom_sdotq_s16(deltas[20], d_ie[10], d_ie[10]);
+  deltas[20] = aom_sdotq_s16(deltas[20], d_ie[11], d_ie[11]);
+}
+
+static inline void compute_stats_win7_sve(
+    const int16_t *const d, const int32_t d_stride, const int16_t *const s,
+    const int32_t s_stride, const int32_t width, const int32_t height,
+    int64_t *const M, int64_t *const H) {
+  const int32_t wiener_win = WIENER_WIN;
+  const int32_t wiener_win2 = wiener_win * wiener_win;
+  const int32_t h8 = height & ~7;
+  int32_t i, j, x, y;
+
+  // Use a predicate to compute the last columns.
+  svbool_t p0 = svwhilelt_b16_u32(0, width % 16 == 0 ? 16 : width % 16);
+  svbool_t p1 = svwhilelt_b16_u32(8, width % 16 == 0 ? 16 : width % 16);
+
+  // Step 1: Calculate the top edge of the whole matrix, i.e., the top
+  // edge of each triangle and square on the top row.
+  j = 0;
+  do {
+    const int16_t *s_t = s;
+    const int16_t *d_t = d;
+    int64x2_t sum_m[WIENER_WIN] = { vdupq_n_s64(0) };
+    int64x2_t sum_h[WIENER_WIN] = { vdupq_n_s64(0) };
+    int16x8_t src[2], dgd[2];
+
+    y = height;
+    do {
+      x = 0;
+      while (x < width - 16) {
+        src[0] = vld1q_s16(s_t + x + 0);
+        src[1] = vld1q_s16(s_t + x + 8);
+        dgd[0] = vld1q_s16(d_t + x + 0);
+        dgd[1] = vld1q_s16(d_t + x + 8);
+        stats_top_win7_sve(src, dgd, d_t + j + x, d_stride, sum_m, sum_h);
+        x += 16;
+      }
+
+      src[0] = svget_neonq_s16(svld1_s16(p0, s_t + x + 0));
+      src[1] = svget_neonq_s16(svld1_s16(p1, s_t + x + 8));
+      dgd[0] = svget_neonq_s16(svld1_s16(p0, d_t + x + 0));
+      dgd[1] = svget_neonq_s16(svld1_s16(p1, d_t + x + 8));
+      stats_top_win7_sve(src, dgd, d_t + j + x, d_stride, sum_m, sum_h);
+
+      s_t += s_stride;
+      d_t += d_stride;
+    } while (--y);
+
+    vst1q_s64(M + wiener_win * j + 0, vpaddq_s64(sum_m[0], sum_m[1]));
+    vst1q_s64(M + wiener_win * j + 2, vpaddq_s64(sum_m[2], sum_m[3]));
+    vst1q_s64(M + wiener_win * j + 4, vpaddq_s64(sum_m[4], sum_m[5]));
+    M[wiener_win * j + 6] = vaddvq_s64(sum_m[6]);
+
+    vst1q_s64(H + wiener_win * j + 0, vpaddq_s64(sum_h[0], sum_h[1]));
+    vst1q_s64(H + wiener_win * j + 2, vpaddq_s64(sum_h[2], sum_h[3]));
+    vst1q_s64(H + wiener_win * j + 4, vpaddq_s64(sum_h[4], sum_h[5]));
+    H[wiener_win * j + 6] = vaddvq_s64(sum_h[6]);
+  } while (++j < wiener_win);
+
+  // Step 2: Calculate the left edge of each square on the top row.
+  j = 1;
+  do {
+    const int16_t *d_t = d;
+    int64x2_t sum_h[WIENER_WIN - 1] = { vdupq_n_s64(0) };
+    int16x8_t dgd[2];
+
+    y = height;
+    do {
+      x = 0;
+      while (x < width - 16) {
+        dgd[0] = vld1q_s16(d_t + j + x + 0);
+        dgd[1] = vld1q_s16(d_t + j + x + 8);
+        stats_left_win7_sve(dgd, d_t + x, d_stride, sum_h);
+        x += 16;
+      }
+
+      dgd[0] = svget_neonq_s16(svld1_s16(p0, d_t + j + x + 0));
+      dgd[1] = svget_neonq_s16(svld1_s16(p1, d_t + j + x + 8));
+      stats_left_win7_sve(dgd, d_t + x, d_stride, sum_h);
+
+      d_t += d_stride;
+    } while (--y);
+
+    int64x2_t sum_h01 = vpaddq_s64(sum_h[0], sum_h[1]);
+    int64x2_t sum_h23 = vpaddq_s64(sum_h[2], sum_h[3]);
+    int64x2_t sum_h45 = vpaddq_s64(sum_h[4], sum_h[5]);
+    vst1_s64(&H[1 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h01));
+    vst1_s64(&H[2 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h01));
+    vst1_s64(&H[3 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h23));
+    vst1_s64(&H[4 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h23));
+    vst1_s64(&H[5 * wiener_win2 + j * wiener_win], vget_low_s64(sum_h45));
+    vst1_s64(&H[6 * wiener_win2 + j * wiener_win], vget_high_s64(sum_h45));
+  } while (++j < wiener_win);
+
+  // Step 3: Derive the top edge of each triangle along the diagonal. No
+  // triangle in top row.
+  {
+    const int16_t *d_t = d;
+    // Pad to call transpose function.
+    int32x4_t deltas[(WIENER_WIN + 1) * 2] = { vdupq_n_s32(0) };
+    int32x4_t deltas_tr[(WIENER_WIN + 1) * 2] = { vdupq_n_s32(0) };
+    int16x8_t ds[WIENER_WIN * 2];
+
+    load_s16_8x6(d_t, d_stride, &ds[0], &ds[2], &ds[4], &ds[6], &ds[8],
+                 &ds[10]);
+    load_s16_8x6(d_t + width, d_stride, &ds[1], &ds[3], &ds[5], &ds[7], &ds[9],
+                 &ds[11]);
+
+    d_t += 6 * d_stride;
+
+    step3_win7_neon(d_t, d_stride, width, height, ds, deltas);
+    transpose_arrays_s32_8x8(deltas, deltas_tr);
+
+    update_8_stats_neon(H + 0 * wiener_win * wiener_win2 + 0 * wiener_win,
+                        deltas_tr[0], deltas_tr[4],
+                        H + 1 * wiener_win * wiener_win2 + 1 * wiener_win);
+    update_8_stats_neon(H + 1 * wiener_win * wiener_win2 + 1 * wiener_win,
+                        deltas_tr[1], deltas_tr[5],
+                        H + 2 * wiener_win * wiener_win2 + 2 * wiener_win);
+    update_8_stats_neon(H + 2 * wiener_win * wiener_win2 + 2 * wiener_win,
+                        deltas_tr[2], deltas_tr[6],
+                        H + 3 * wiener_win * wiener_win2 + 3 * wiener_win);
+    update_8_stats_neon(H + 3 * wiener_win * wiener_win2 + 3 * wiener_win,
+                        deltas_tr[3], deltas_tr[7],
+                        H + 4 * wiener_win * wiener_win2 + 4 * wiener_win);
+    update_8_stats_neon(H + 4 * wiener_win * wiener_win2 + 4 * wiener_win,
+                        deltas_tr[8], deltas_tr[12],
+                        H + 5 * wiener_win * wiener_win2 + 5 * wiener_win);
+    update_8_stats_neon(H + 5 * wiener_win * wiener_win2 + 5 * wiener_win,
+                        deltas_tr[9], deltas_tr[13],
+                        H + 6 * wiener_win * wiener_win2 + 6 * wiener_win);
+  }
+
+  // Step 4: Derive the top and left edge of each square. No square in top and
+  // bottom row.
+
+  i = 1;
+  do {
+    j = i + 1;
+    do {
+      const int16_t *di = d + i - 1;
+      const int16_t *dj = d + j - 1;
+      int64x2_t deltas[(2 * WIENER_WIN - 1) * 2] = { vdupq_n_s64(0) };
+      int16x8_t dd[WIENER_WIN * 2], ds[WIENER_WIN * 2];
+
+      dd[5] = vdupq_n_s16(0);  // Initialize to avoid warning.
+      const int16_t dd0_values[] = { di[0 * d_stride],
+                                     di[1 * d_stride],
+                                     di[2 * d_stride],
+                                     di[3 * d_stride],
+                                     di[4 * d_stride],
+                                     di[5 * d_stride],
+                                     0,
+                                     0 };
+      dd[0] = vld1q_s16(dd0_values);
+      const int16_t dd1_values[] = { di[0 * d_stride + width],
+                                     di[1 * d_stride + width],
+                                     di[2 * d_stride + width],
+                                     di[3 * d_stride + width],
+                                     di[4 * d_stride + width],
+                                     di[5 * d_stride + width],
+                                     0,
+                                     0 };
+      dd[1] = vld1q_s16(dd1_values);
+      const int16_t ds0_values[] = { dj[0 * d_stride],
+                                     dj[1 * d_stride],
+                                     dj[2 * d_stride],
+                                     dj[3 * d_stride],
+                                     dj[4 * d_stride],
+                                     dj[5 * d_stride],
+                                     0,
+                                     0 };
+      ds[0] = vld1q_s16(ds0_values);
+      int16_t ds1_values[] = { dj[0 * d_stride + width],
+                               dj[1 * d_stride + width],
+                               dj[2 * d_stride + width],
+                               dj[3 * d_stride + width],
+                               dj[4 * d_stride + width],
+                               dj[5 * d_stride + width],
+                               0,
+                               0 };
+      ds[1] = vld1q_s16(ds1_values);
+
+      y = 0;
+      while (y < h8) {
+        // 00s 10s 20s 30s 40s 50s 60s 70s  00e 10e 20e 30e 40e 50e 60e 70e
+        dd[0] = vsetq_lane_s16(di[6 * d_stride], dd[0], 6);
+        dd[0] = vsetq_lane_s16(di[7 * d_stride], dd[0], 7);
+        dd[1] = vsetq_lane_s16(di[6 * d_stride + width], dd[1], 6);
+        dd[1] = vsetq_lane_s16(di[7 * d_stride + width], dd[1], 7);
+
+        // 00s 10s 20s 30s 40s 50s 60s 70s  00e 10e 20e 30e 40e 50e 60e 70e
+        // 01s 11s 21s 31s 41s 51s 61s 71s  01e 11e 21e 31e 41e 51e 61e 71e
+        ds[0] = vsetq_lane_s16(dj[6 * d_stride], ds[0], 6);
+        ds[0] = vsetq_lane_s16(dj[7 * d_stride], ds[0], 7);
+        ds[1] = vsetq_lane_s16(dj[6 * d_stride + width], ds[1], 6);
+        ds[1] = vsetq_lane_s16(dj[7 * d_stride + width], ds[1], 7);
+
+        load_more_16_neon(di + 8 * d_stride, width, &dd[0], &dd[2]);
+        load_more_16_neon(dj + 8 * d_stride, width, &ds[0], &ds[2]);
+        load_more_16_neon(di + 9 * d_stride, width, &dd[2], &dd[4]);
+        load_more_16_neon(dj + 9 * d_stride, width, &ds[2], &ds[4]);
+        load_more_16_neon(di + 10 * d_stride, width, &dd[4], &dd[6]);
+        load_more_16_neon(dj + 10 * d_stride, width, &ds[4], &ds[6]);
+        load_more_16_neon(di + 11 * d_stride, width, &dd[6], &dd[8]);
+        load_more_16_neon(dj + 11 * d_stride, width, &ds[6], &ds[8]);
+        load_more_16_neon(di + 12 * d_stride, width, &dd[8], &dd[10]);
+        load_more_16_neon(dj + 12 * d_stride, width, &ds[8], &ds[10]);
+        load_more_16_neon(di + 13 * d_stride, width, &dd[10], &dd[12]);
+        load_more_16_neon(dj + 13 * d_stride, width, &ds[10], &ds[12]);
+
+        deltas[0] = aom_sdotq_s16(deltas[0], dd[0], ds[0]);
+        deltas[1] = aom_sdotq_s16(deltas[1], dd[1], ds[1]);
+        deltas[2] = aom_sdotq_s16(deltas[2], dd[0], ds[2]);
+        deltas[3] = aom_sdotq_s16(deltas[3], dd[1], ds[3]);
+        deltas[4] = aom_sdotq_s16(deltas[4], dd[0], ds[4]);
+        deltas[5] = aom_sdotq_s16(deltas[5], dd[1], ds[5]);
+        deltas[6] = aom_sdotq_s16(deltas[6], dd[0], ds[6]);
+        deltas[7] = aom_sdotq_s16(deltas[7], dd[1], ds[7]);
+        deltas[8] = aom_sdotq_s16(deltas[8], dd[0], ds[8]);
+        deltas[9] = aom_sdotq_s16(deltas[9], dd[1], ds[9]);
+        deltas[10] = aom_sdotq_s16(deltas[10], dd[0], ds[10]);
+        deltas[11] = aom_sdotq_s16(deltas[11], dd[1], ds[11]);
+        deltas[12] = aom_sdotq_s16(deltas[12], dd[0], ds[12]);
+        deltas[13] = aom_sdotq_s16(deltas[13], dd[1], ds[13]);
+        deltas[14] = aom_sdotq_s16(deltas[14], dd[2], ds[0]);
+        deltas[15] = aom_sdotq_s16(deltas[15], dd[3], ds[1]);
+        deltas[16] = aom_sdotq_s16(deltas[16], dd[4], ds[0]);
+        deltas[17] = aom_sdotq_s16(deltas[17], dd[5], ds[1]);
+        deltas[18] = aom_sdotq_s16(deltas[18], dd[6], ds[0]);
+        deltas[19] = aom_sdotq_s16(deltas[19], dd[7], ds[1]);
+        deltas[20] = aom_sdotq_s16(deltas[20], dd[8], ds[0]);
+        deltas[21] = aom_sdotq_s16(deltas[21], dd[9], ds[1]);
+        deltas[22] = aom_sdotq_s16(deltas[22], dd[10], ds[0]);
+        deltas[23] = aom_sdotq_s16(deltas[23], dd[11], ds[1]);
+        deltas[24] = aom_sdotq_s16(deltas[24], dd[12], ds[0]);
+        deltas[25] = aom_sdotq_s16(deltas[25], dd[13], ds[1]);
+
+        dd[0] = vextq_s16(dd[12], vdupq_n_s16(0), 2);
+        dd[1] = vextq_s16(dd[13], vdupq_n_s16(0), 2);
+        ds[0] = vextq_s16(ds[12], vdupq_n_s16(0), 2);
+        ds[1] = vextq_s16(ds[13], vdupq_n_s16(0), 2);
+
+        di += 8 * d_stride;
+        dj += 8 * d_stride;
+        y += 8;
+      }
+
+      int64x2_t deltas02 = vpaddq_s64(deltas[0], deltas[2]);
+      int64x2_t deltas13 = vpaddq_s64(deltas[1], deltas[3]);
+      int64x2_t deltas46 = vpaddq_s64(deltas[4], deltas[6]);
+      int64x2_t deltas57 = vpaddq_s64(deltas[5], deltas[7]);
+      int64x2_t deltas810 = vpaddq_s64(deltas[8], deltas[10]);
+      int64x2_t deltas911 = vpaddq_s64(deltas[9], deltas[11]);
+      int64x2_t deltas1212 = vpaddq_s64(deltas[12], deltas[12]);
+      int64x2_t deltas1313 = vpaddq_s64(deltas[13], deltas[13]);
+      int64x2_t deltas1416 = vpaddq_s64(deltas[14], deltas[16]);
+      int64x2_t deltas1820 = vpaddq_s64(deltas[18], deltas[20]);
+      int64x2_t deltas1517 = vpaddq_s64(deltas[15], deltas[17]);
+      int64x2_t deltas1921 = vpaddq_s64(deltas[19], deltas[21]);
+      int64x2_t deltas2224 = vpaddq_s64(deltas[22], deltas[24]);
+      int64x2_t deltas2325 = vpaddq_s64(deltas[23], deltas[25]);
+      deltas02 = vsubq_s64(deltas13, deltas02);
+      deltas46 = vsubq_s64(deltas57, deltas46);
+      deltas810 = vsubq_s64(deltas911, deltas810);
+      deltas1212 = vsubq_s64(deltas1313, deltas1212);
+      deltas1416 = vsubq_s64(deltas1517, deltas1416);
+      deltas1820 = vsubq_s64(deltas1921, deltas1820);
+      deltas2224 = vsubq_s64(deltas2325, deltas2224);
+
+      if (h8 != height) {
+        const int16_t ds0_vals[] = {
+          dj[0 * d_stride], dj[0 * d_stride + width],
+          dj[1 * d_stride], dj[1 * d_stride + width],
+          dj[2 * d_stride], dj[2 * d_stride + width],
+          dj[3 * d_stride], dj[3 * d_stride + width]
+        };
+        ds[0] = vld1q_s16(ds0_vals);
+
+        ds[1] = vsetq_lane_s16(dj[4 * d_stride], ds[1], 0);
+        ds[1] = vsetq_lane_s16(dj[4 * d_stride + width], ds[1], 1);
+        ds[1] = vsetq_lane_s16(dj[5 * d_stride], ds[1], 2);
+        ds[1] = vsetq_lane_s16(dj[5 * d_stride + width], ds[1], 3);
+        const int16_t dd4_vals[] = {
+          -di[1 * d_stride], di[1 * d_stride + width],
+          -di[2 * d_stride], di[2 * d_stride + width],
+          -di[3 * d_stride], di[3 * d_stride + width],
+          -di[4 * d_stride], di[4 * d_stride + width]
+        };
+        dd[4] = vld1q_s16(dd4_vals);
+
+        dd[5] = vsetq_lane_s16(-di[5 * d_stride], dd[5], 0);
+        dd[5] = vsetq_lane_s16(di[5 * d_stride + width], dd[5], 1);
+        do {
+          dd[0] = vdupq_n_s16(-di[0 * d_stride]);
+          dd[2] = dd[3] = vdupq_n_s16(di[0 * d_stride + width]);
+          dd[0] = dd[1] = vzip1q_s16(dd[0], dd[2]);
+
+          ds[4] = vdupq_n_s16(dj[0 * d_stride]);
+          ds[6] = ds[7] = vdupq_n_s16(dj[0 * d_stride + width]);
+          ds[4] = ds[5] = vzip1q_s16(ds[4], ds[6]);
+
+          dd[5] = vsetq_lane_s16(-di[6 * d_stride], dd[5], 2);
+          dd[5] = vsetq_lane_s16(di[6 * d_stride + width], dd[5], 3);
+          ds[1] = vsetq_lane_s16(dj[6 * d_stride], ds[1], 4);
+          ds[1] = vsetq_lane_s16(dj[6 * d_stride + width], ds[1], 5);
+
+          const int32x4_t res0 =
+              vpaddq_s32(vmull_s16(vget_low_s16(dd[0]), vget_low_s16(ds[0])),
+                         vmull_s16(vget_high_s16(dd[0]), vget_high_s16(ds[0])));
+          deltas02 = vaddw_s32(deltas02, vget_low_s32(res0));
+          deltas46 = vaddw_s32(deltas46, vget_high_s32(res0));
+          const int32x4_t res1 =
+              vpaddq_s32(vmull_s16(vget_low_s16(dd[1]), vget_low_s16(ds[1])),
+                         vmull_s16(vget_high_s16(dd[1]), vget_high_s16(ds[1])));
+          deltas810 = vaddw_s32(deltas810, vget_low_s32(res1));
+          deltas1212 = vaddw_s32(deltas1212, vget_high_s32(res1));
+          const int32x4_t res2 =
+              vpaddq_s32(vmull_s16(vget_low_s16(dd[4]), vget_low_s16(ds[4])),
+                         vmull_s16(vget_high_s16(dd[4]), vget_high_s16(ds[4])));
+          deltas1416 = vaddw_s32(deltas1416, vget_low_s32(res2));
+          deltas1820 = vaddw_s32(deltas1820, vget_high_s32(res2));
+          const int32x4_t res3 =
+              vpaddq_s32(vmull_s16(vget_low_s16(dd[5]), vget_low_s16(ds[5])),
+                         vmull_s16(vget_high_s16(dd[5]), vget_high_s16(ds[5])));
+          deltas2224 = vaddw_s32(deltas2224, vget_low_s32(res3));
+
+          int32_t tmp0 = vgetq_lane_s32(vreinterpretq_s32_s16(ds[0]), 0);
+          ds[0] = vextq_s16(ds[0], ds[1], 2);
+          ds[1] = vextq_s16(ds[1], ds[0], 2);
+          ds[1] = vreinterpretq_s16_s32(
+              vsetq_lane_s32(tmp0, vreinterpretq_s32_s16(ds[1]), 3));
+          int32_t tmp1 = vgetq_lane_s32(vreinterpretq_s32_s16(dd[4]), 0);
+          dd[4] = vextq_s16(dd[4], dd[5], 2);
+          dd[5] = vextq_s16(dd[5], dd[4], 2);
+          dd[5] = vreinterpretq_s16_s32(
+              vsetq_lane_s32(tmp1, vreinterpretq_s32_s16(dd[5]), 3));
+          di += d_stride;
+          dj += d_stride;
+        } while (++y < height);
+      }
+
+      // Writing one more element on the top edge of a square falls to
+      // the next square in the same row or the first element in the next
+      // row, which will just be overwritten later.
+      int64x2_t s0 = vld1q_s64(H + (i - 1) * wiener_win * wiener_win2 +
+                               (j - 1) * wiener_win + 0);
+      int64x2_t s1 = vld1q_s64(H + (i - 1) * wiener_win * wiener_win2 +
+                               (j - 1) * wiener_win + 2);
+      int64x2_t s2 = vld1q_s64(H + (i - 1) * wiener_win * wiener_win2 +
+                               (j - 1) * wiener_win + 4);
+      int64x2_t s3 = vld1q_s64(H + (i - 1) * wiener_win * wiener_win2 +
+                               (j - 1) * wiener_win + 6);
+
+      vst1q_s64(H + i * wiener_win * wiener_win2 + j * wiener_win + 0,
+                vaddq_s64(s0, deltas02));
+      vst1q_s64(H + i * wiener_win * wiener_win2 + j * wiener_win + 2,
+                vaddq_s64(s1, deltas46));
+      vst1q_s64(H + i * wiener_win * wiener_win2 + j * wiener_win + 4,
+                vaddq_s64(s2, deltas810));
+      vst1q_s64(H + i * wiener_win * wiener_win2 + j * wiener_win + 6,
+                vaddq_s64(s3, deltas1212));
+
+      H[(i * wiener_win + 1) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 1) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s64(deltas1416, 0);
+      H[(i * wiener_win + 2) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 2) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s64(deltas1416, 1);
+      H[(i * wiener_win + 3) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 3) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s64(deltas1820, 0);
+      H[(i * wiener_win + 4) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 4) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s64(deltas1820, 1);
+      H[(i * wiener_win + 5) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 5) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s64(deltas2224, 0);
+      H[(i * wiener_win + 6) * wiener_win2 + j * wiener_win] =
+          H[((i - 1) * wiener_win + 6) * wiener_win2 + (j - 1) * wiener_win] +
+          vgetq_lane_s64(deltas2224, 1);
+    } while (++j < wiener_win);
+  } while (++i < wiener_win - 1);
+
+  // Step 5: Derive other points of each square. No square in bottom row.
+  i = 0;
+  do {
+    const int16_t *const di = d + i;
+
+    j = i + 1;
+    do {
+      const int16_t *const dj = d + j;
+      int64x2_t deltas[WIENER_WIN - 1][WIN_7] = { { vdupq_n_s64(0) },
+                                                  { vdupq_n_s64(0) } };
+      int16x8_t d_is[WIN_7];
+      int16x8_t d_ie[WIN_7];
+      int16x8_t d_js[WIN_7];
+      int16x8_t d_je[WIN_7];
+
+      x = 0;
+      while (x < width - 16) {
+        load_square_win7_neon(di + x, dj + x, d_stride, height, d_is, d_ie,
+                              d_js, d_je);
+        derive_square_win7_sve(d_is, d_ie, d_js, d_je, deltas);
+        x += 16;
+      }
+
+      load_square_win7_sve(di + x, dj + x, d_stride, height, d_is, d_ie, d_js,
+                           d_je, p0, p1);
+      derive_square_win7_sve(d_is, d_ie, d_js, d_je, deltas);
+
+      hadd_update_6_stats_sve(
+          H + (i * wiener_win + 0) * wiener_win2 + j * wiener_win, deltas[0],
+          H + (i * wiener_win + 1) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_sve(
+          H + (i * wiener_win + 1) * wiener_win2 + j * wiener_win, deltas[1],
+          H + (i * wiener_win + 2) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_sve(
+          H + (i * wiener_win + 2) * wiener_win2 + j * wiener_win, deltas[2],
+          H + (i * wiener_win + 3) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_sve(
+          H + (i * wiener_win + 3) * wiener_win2 + j * wiener_win, deltas[3],
+          H + (i * wiener_win + 4) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_sve(
+          H + (i * wiener_win + 4) * wiener_win2 + j * wiener_win, deltas[4],
+          H + (i * wiener_win + 5) * wiener_win2 + j * wiener_win + 1);
+      hadd_update_6_stats_sve(
+          H + (i * wiener_win + 5) * wiener_win2 + j * wiener_win, deltas[5],
+          H + (i * wiener_win + 6) * wiener_win2 + j * wiener_win + 1);
+    } while (++j < wiener_win);
+  } while (++i < wiener_win - 1);
+
+  // Step 6: Derive other points of each upper triangle along the diagonal.
+  i = 0;
+  do {
+    const int16_t *const di = d + i;
+    int64x2_t deltas[3 * WIENER_WIN] = { vdupq_n_s64(0) };
+    int16x8_t d_is[WIN_7], d_ie[WIN_7];
+
+    x = 0;
+    while (x < width - 16) {
+      load_triangle_win7_neon(di + x, d_stride, height, d_is, d_ie);
+      derive_triangle_win7_sve(d_is, d_ie, deltas);
+      x += 16;
+    }
+
+    load_triangle_win7_sve(di + x, d_stride, height, d_is, d_ie, p0, p1);
+    derive_triangle_win7_sve(d_is, d_ie, deltas);
+
+    // Row 1: 6 points
+    hadd_update_6_stats_sve(
+        H + (i * wiener_win + 0) * wiener_win2 + i * wiener_win, deltas,
+        H + (i * wiener_win + 1) * wiener_win2 + i * wiener_win + 1);
+
+    int64x2_t deltas1017 = vpaddq_s64(deltas[10], deltas[17]);
+
+    // Row 2: 5 points
+    hadd_update_4_stats_sve(
+        H + (i * wiener_win + 1) * wiener_win2 + i * wiener_win + 1, deltas + 6,
+        H + (i * wiener_win + 2) * wiener_win2 + i * wiener_win + 2);
+    H[(i * wiener_win + 2) * wiener_win2 + i * wiener_win + 6] =
+        H[(i * wiener_win + 1) * wiener_win2 + i * wiener_win + 5] +
+        vgetq_lane_s64(deltas1017, 0);
+
+    // Row 3: 4 points
+    hadd_update_4_stats_sve(
+        H + (i * wiener_win + 2) * wiener_win2 + i * wiener_win + 2,
+        deltas + 11,
+        H + (i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3);
+
+    // Row 4: 3 points
+    int64x2_t h0 =
+        vld1q_s64(H + (i * wiener_win + 3) * wiener_win2 + i * wiener_win + 3);
+    vst1q_s64(H + (i * wiener_win + 4) * wiener_win2 + i * wiener_win + 4,
+              vaddq_s64(h0, vpaddq_s64(deltas[15], deltas[16])));
+    H[(i * wiener_win + 4) * wiener_win2 + i * wiener_win + 6] =
+        H[(i * wiener_win + 3) * wiener_win2 + i * wiener_win + 5] +
+        vgetq_lane_s64(deltas1017, 1);
+
+    // Row 5: 2 points
+    int64x2_t h1 =
+        vld1q_s64(H + (i * wiener_win + 4) * wiener_win2 + i * wiener_win + 4);
+    vst1q_s64(H + (i * wiener_win + 5) * wiener_win2 + i * wiener_win + 5,
+              vaddq_s64(h1, vpaddq_s64(deltas[18], deltas[19])));
+
+    // Row 6: 1 points
+    H[(i * wiener_win + 6) * wiener_win2 + i * wiener_win + 6] =
+        H[(i * wiener_win + 5) * wiener_win2 + i * wiener_win + 5] +
+        vaddvq_s64(deltas[20]);
+  } while (++i < wiener_win);
+}
+
 #endif  // AOM_AV1_ENCODER_ARM_PICKRST_SVE_H_
diff --git a/av1/encoder/arm/temporal_filter_neon.c b/av1/encoder/arm/temporal_filter_neon.c
index 6229900bc..2baa010c5 100644
--- a/av1/encoder/arm/temporal_filter_neon.c
+++ b/av1/encoder/arm/temporal_filter_neon.c
@@ -86,7 +86,7 @@ static void apply_temporal_filter(
   assert(((block_width == 16) || (block_width == 32)) &&
          ((block_height == 16) || (block_height == 32)));
 
-  uint32_t acc_5x5_neon[BH][BW];
+  uint32_t diff_sse[BH][BW];
   const uint16x8x4_t vmask = vld1q_u16_x4(kSlidingWindowMask);
 
   // Traverse 4 columns at a time - first and last two columns need padding.
@@ -104,13 +104,26 @@ static void apply_temporal_filter(
     vsrc[0] = vsrc[2];
     vsrc[1] = vsrc[2];
 
+    uint32x4_t vsum[4] = { vdupq_n_u32(0), vdupq_n_u32(0), vdupq_n_u32(0),
+                           vdupq_n_u32(0) };
+
+    for (int i = 0; i < 4; i++) {
+      vsum[i] = vpadalq_u16(vsum[i], vandq_u16(vsrc[0], vmask.val[i]));
+      vsum[i] = vpadalq_u16(vsum[i], vandq_u16(vsrc[1], vmask.val[i]));
+      vsum[i] = vpadalq_u16(vsum[i], vandq_u16(vsrc[2], vmask.val[i]));
+      vsum[i] = vpadalq_u16(vsum[i], vandq_u16(vsrc[3], vmask.val[i]));
+      vsum[i] = vpadalq_u16(vsum[i], vandq_u16(vsrc[4], vmask.val[i]));
+    }
+
     for (unsigned int row = 0; row < block_height; row++) {
+      uint32x4_t sum_luma = vld1q_u32(luma_sse_sum + row * BW + col);
+      uint32x4_t sum_src = horizontal_add_4d_u32x4(vsum);
+
+      vst1q_u32(&diff_sse[row][col], vaddq_u32(sum_src, sum_luma));
+
       for (int i = 0; i < 4; i++) {
-        uint32x4_t vsum = vdupq_n_u32(0);
-        for (int j = 0; j < 5; j++) {
-          vsum = vpadalq_u16(vsum, vandq_u16(vsrc[j], vmask.val[i]));
-        }
-        acc_5x5_neon[row][col + i] = horizontal_add_u32x4(vsum);
+        uint32x4_t vsum_0 = vpaddlq_u16(vandq_u16(vsrc[0], vmask.val[i]));
+        vsum[i] = vsubq_u32(vsum[i], vsum_0);
       }
 
       // Push all rows in the sliding window up one.
@@ -126,6 +139,10 @@ static void apply_temporal_filter(
         // Pad the bottom 2 rows.
         vsrc[4] = vsrc[3];
       }
+
+      for (int i = 0; i < 4; i++) {
+        vsum[i] = vpadalq_u16(vsum[i], vandq_u16(vsrc[4], vmask.val[i]));
+      }
     }
   }
 
@@ -134,9 +151,8 @@ static void apply_temporal_filter(
     for (unsigned int i = 0, k = 0; i < block_height; i++) {
       for (unsigned int j = 0; j < block_width; j++, k++) {
         const int pixel_value = frame[i * stride + j];
-        const uint32_t diff_sse = acc_5x5_neon[i][j] + luma_sse_sum[i * BW + j];
 
-        const double window_error = diff_sse * inv_num_ref_pixels;
+        const double window_error = diff_sse[i][j] * inv_num_ref_pixels;
         const int subblock_idx =
             (i >= block_height / 2) * 2 + (j >= block_width / 2);
         const double block_error = (double)subblock_mses[subblock_idx];
@@ -155,9 +171,8 @@ static void apply_temporal_filter(
     for (unsigned int i = 0, k = 0; i < block_height; i++) {
       for (unsigned int j = 0; j < block_width; j++, k++) {
         const int pixel_value = frame[i * stride + j];
-        const uint32_t diff_sse = acc_5x5_neon[i][j] + luma_sse_sum[i * BW + j];
 
-        const double window_error = diff_sse * inv_num_ref_pixels;
+        const double window_error = diff_sse[i][j] * inv_num_ref_pixels;
         const int subblock_idx =
             (i >= block_height / 2) * 2 + (j >= block_width / 2);
         const double block_error = (double)subblock_mses[subblock_idx];
diff --git a/av1/encoder/arm/temporal_filter_neon_dotprod.c b/av1/encoder/arm/temporal_filter_neon_dotprod.c
index 63e5602a9..c8fd69900 100644
--- a/av1/encoder/arm/temporal_filter_neon_dotprod.c
+++ b/av1/encoder/arm/temporal_filter_neon_dotprod.c
@@ -74,7 +74,7 @@ static void apply_temporal_filter(
   assert(((block_width == 16) || (block_width == 32)) &&
          ((block_height == 16) || (block_height == 32)));
 
-  uint32_t acc_5x5_neon[BH][BW];
+  uint32_t diff_sse[BH][BW];
   const uint8x16x2_t vmask = vld1q_u8_x2(kSlidingWindowMask);
   const uint8x16_t pad_tbl0 = vld1q_u8(kLoadPad[0]);
   const uint8x16_t pad_tbl1 = vld1q_u8(kLoadPad[1]);
@@ -109,23 +109,32 @@ static void apply_temporal_filter(
     vsrc[1][0] = vsrc[2][0];
     vsrc[1][1] = vsrc[2][1];
 
+    uint32x4_t sum_01 = vdupq_n_u32(0);
+    uint32x4_t sum_23 = vdupq_n_u32(0);
+
+    sum_01 = vdotq_u32(sum_01, vsrc[0][0], vsrc[0][0]);
+    sum_01 = vdotq_u32(sum_01, vsrc[1][0], vsrc[1][0]);
+    sum_01 = vdotq_u32(sum_01, vsrc[2][0], vsrc[2][0]);
+    sum_01 = vdotq_u32(sum_01, vsrc[3][0], vsrc[3][0]);
+    sum_01 = vdotq_u32(sum_01, vsrc[4][0], vsrc[4][0]);
+
+    sum_23 = vdotq_u32(sum_23, vsrc[0][1], vsrc[0][1]);
+    sum_23 = vdotq_u32(sum_23, vsrc[1][1], vsrc[1][1]);
+    sum_23 = vdotq_u32(sum_23, vsrc[2][1], vsrc[2][1]);
+    sum_23 = vdotq_u32(sum_23, vsrc[3][1], vsrc[3][1]);
+    sum_23 = vdotq_u32(sum_23, vsrc[4][1], vsrc[4][1]);
+
     for (unsigned int row = 0; row < block_height; row++) {
-      uint32x4_t sum_01 = vdupq_n_u32(0);
-      uint32x4_t sum_23 = vdupq_n_u32(0);
+      uint32x4_t sum_luma = vld1q_u32(luma_sse_sum + row * BW + col);
+      uint32x4_t sum_0123 = vpaddq_u32(sum_01, sum_23);
 
-      sum_01 = vdotq_u32(sum_01, vsrc[0][0], vsrc[0][0]);
-      sum_01 = vdotq_u32(sum_01, vsrc[1][0], vsrc[1][0]);
-      sum_01 = vdotq_u32(sum_01, vsrc[2][0], vsrc[2][0]);
-      sum_01 = vdotq_u32(sum_01, vsrc[3][0], vsrc[3][0]);
-      sum_01 = vdotq_u32(sum_01, vsrc[4][0], vsrc[4][0]);
+      vst1q_u32(&diff_sse[row][col], vaddq_u32(sum_0123, sum_luma));
 
-      sum_23 = vdotq_u32(sum_23, vsrc[0][1], vsrc[0][1]);
-      sum_23 = vdotq_u32(sum_23, vsrc[1][1], vsrc[1][1]);
-      sum_23 = vdotq_u32(sum_23, vsrc[2][1], vsrc[2][1]);
-      sum_23 = vdotq_u32(sum_23, vsrc[3][1], vsrc[3][1]);
-      sum_23 = vdotq_u32(sum_23, vsrc[4][1], vsrc[4][1]);
+      uint32x4_t sub_01 = vdotq_u32(vdupq_n_u32(0), vsrc[0][0], vsrc[0][0]);
+      uint32x4_t sub_23 = vdotq_u32(vdupq_n_u32(0), vsrc[0][1], vsrc[0][1]);
 
-      vst1q_u32(&acc_5x5_neon[row][col], vpaddq_u32(sum_01, sum_23));
+      sum_01 = vsubq_u32(sum_01, sub_01);
+      sum_23 = vsubq_u32(sum_23, sub_23);
 
       // Push all rows in the sliding window up one.
       for (int i = 0; i < 4; i++) {
@@ -153,6 +162,9 @@ static void apply_temporal_filter(
         vsrc[4][0] = vsrc[3][0];
         vsrc[4][1] = vsrc[3][1];
       }
+
+      sum_01 = vdotq_u32(sum_01, vsrc[4][0], vsrc[4][0]);
+      sum_23 = vdotq_u32(sum_23, vsrc[4][1], vsrc[4][1]);
     }
   }
 
@@ -161,9 +173,8 @@ static void apply_temporal_filter(
     for (unsigned int i = 0, k = 0; i < block_height; i++) {
       for (unsigned int j = 0; j < block_width; j++, k++) {
         const int pixel_value = frame[i * stride + j];
-        const uint32_t diff_sse = acc_5x5_neon[i][j] + luma_sse_sum[i * BW + j];
 
-        const double window_error = diff_sse * inv_num_ref_pixels;
+        const double window_error = diff_sse[i][j] * inv_num_ref_pixels;
         const int subblock_idx =
             (i >= block_height / 2) * 2 + (j >= block_width / 2);
         const double block_error = (double)subblock_mses[subblock_idx];
@@ -182,9 +193,8 @@ static void apply_temporal_filter(
     for (unsigned int i = 0, k = 0; i < block_height; i++) {
       for (unsigned int j = 0; j < block_width; j++, k++) {
         const int pixel_value = frame[i * stride + j];
-        const uint32_t diff_sse = acc_5x5_neon[i][j] + luma_sse_sum[i * BW + j];
 
-        const double window_error = diff_sse * inv_num_ref_pixels;
+        const double window_error = diff_sse[i][j] * inv_num_ref_pixels;
         const int subblock_idx =
             (i >= block_height / 2) * 2 + (j >= block_width / 2);
         const double block_error = (double)subblock_mses[subblock_idx];
diff --git a/av1/encoder/av1_quantize.c b/av1/encoder/av1_quantize.c
index b39be5b19..d44795dca 100644
--- a/av1/encoder/av1_quantize.c
+++ b/av1/encoder/av1_quantize.c
@@ -708,8 +708,18 @@ void av1_init_quantizer(EncQuantDequantParams *const enc_quant_dequant_params,
   prev_deltaq_params->v_ac_delta_q = quant_params->v_ac_delta_q;
 }
 
-void av1_set_q_index(const EncQuantDequantParams *enc_quant_dequant_params,
-                     int qindex, MACROBLOCK *x) {
+/*!\brief Update quantize parameters in MACROBLOCK
+ *
+ * \param[in]  enc_quant_dequant_params This parameter cached the quantize and
+ *                                      dequantize parameters for all q
+ *                                      indices.
+ * \param[in]  qindex                   Quantize index used for the current
+ *                                      superblock.
+ * \param[out] x                        A superblock data structure for
+ *                                      encoder.
+ */
+static void set_q_index(const EncQuantDequantParams *enc_quant_dequant_params,
+                        int qindex, MACROBLOCK *x) {
   const QUANTS *const quants = &enc_quant_dequant_params->quants;
   const Dequants *const dequants = &enc_quant_dequant_params->dequants;
   x->qindex = qindex;
@@ -744,8 +754,15 @@ void av1_set_q_index(const EncQuantDequantParams *enc_quant_dequant_params,
   x->plane[2].dequant_QTX = dequants->v_dequant_QTX[qindex];
 }
 
-void av1_set_qmatrix(const CommonQuantParams *quant_params, int segment_id,
-                     MACROBLOCKD *xd) {
+/*!\brief Update quantize matrix in MACROBLOCKD based on segment id
+ *
+ * \param[in]  quant_params  Quantize parameters used by encoder and decoder
+ * \param[in]  segment_id    Segment id.
+ * \param[out] xd            A superblock data structure used by encoder and
+ * decoder.
+ */
+static void set_qmatrix(const CommonQuantParams *quant_params, int segment_id,
+                        MACROBLOCKD *xd) {
   const int use_qmatrix = av1_use_qmatrix(quant_params, xd, segment_id);
   const int qmlevel_y =
       use_qmatrix ? quant_params->qmatrix_level_y : NUM_QM_LEVELS - 1;
@@ -802,13 +819,13 @@ void av1_init_plane_quantizers(const AV1_COMP *cpi, MACROBLOCK *x,
 
   const int qindex_change = x->qindex != qindex;
   if (qindex_change || do_update) {
-    av1_set_q_index(&cpi->enc_quant_dequant_params, qindex, x);
+    set_q_index(&cpi->enc_quant_dequant_params, qindex, x);
   }
 
   MACROBLOCKD *const xd = &x->e_mbd;
   if ((segment_id != x->prev_segment_id) ||
       av1_use_qmatrix(quant_params, xd, segment_id)) {
-    av1_set_qmatrix(quant_params, segment_id, xd);
+    set_qmatrix(quant_params, segment_id, xd);
   }
 
   x->seg_skip_block = segfeature_active(&cm->seg, segment_id, SEG_LVL_SKIP);
diff --git a/av1/encoder/av1_quantize.h b/av1/encoder/av1_quantize.h
index 3dea441a3..35b0fa822 100644
--- a/av1/encoder/av1_quantize.h
+++ b/av1/encoder/av1_quantize.h
@@ -170,29 +170,6 @@ void av1_quantize_dc_facade(const tran_low_t *coeff_ptr, intptr_t n_coeffs,
                             tran_low_t *dqcoeff_ptr, uint16_t *eob_ptr,
                             const SCAN_ORDER *sc, const QUANT_PARAM *qparam);
 
-/*!\brief Update quantize parameters in MACROBLOCK
- *
- * \param[in]  enc_quant_dequant_params This parameter cached the quantize and
- *                                      dequantize parameters for all q
- *                                      indices.
- * \param[in]  qindex                   Quantize index used for the current
- *                                      superblock.
- * \param[out] x                        A superblock data structure for
- *                                      encoder.
- */
-void av1_set_q_index(const EncQuantDequantParams *enc_quant_dequant_params,
-                     int qindex, MACROBLOCK *x);
-
-/*!\brief Update quantize matrix in MACROBLOCKD based on segment id
- *
- * \param[in]  quant_params  Quantize parameters used by encoder and decoder
- * \param[in]  segment_id    Segment id.
- * \param[out] xd            A superblock data structure used by encoder and
- * decoder.
- */
-void av1_set_qmatrix(const CommonQuantParams *quant_params, int segment_id,
-                     MACROBLOCKD *xd);
-
 #if CONFIG_AV1_HIGHBITDEPTH
 void av1_highbd_quantize_fp_facade(const tran_low_t *coeff_ptr,
                                    intptr_t n_coeffs, const MACROBLOCK_PLANE *p,
diff --git a/av1/encoder/bitstream.c b/av1/encoder/bitstream.c
index 4b9f3f7d7..bff49cf5e 100644
--- a/av1/encoder/bitstream.c
+++ b/av1/encoder/bitstream.c
@@ -12,7 +12,9 @@
 #include <assert.h>
 #include <limits.h>
 #include <stdbool.h>
+#include <stdint.h>
 #include <stdio.h>
+#include <string.h>
 
 #include "aom/aom_encoder.h"
 #include "aom_dsp/aom_dsp_common.h"
@@ -3099,12 +3101,34 @@ static inline void write_uncompressed_header_obu(
         const int gld_ref = get_ref_frame_map_idx(cm, GOLDEN_FRAME);
         aom_wb_write_literal(wb, gld_ref, REF_FRAMES_LOG2);
       }
-
+      int first_ref_map_idx = INVALID_IDX;
+      if (cpi->ppi->rtc_ref.set_ref_frame_config) {
+        for (ref_frame = LAST_FRAME; ref_frame <= ALTREF_FRAME; ++ref_frame) {
+          if (cpi->ppi->rtc_ref.reference[ref_frame - 1] == 1) {
+            first_ref_map_idx = cpi->ppi->rtc_ref.ref_idx[ref_frame - 1];
+            break;
+          }
+        }
+      }
       for (ref_frame = LAST_FRAME; ref_frame <= ALTREF_FRAME; ++ref_frame) {
         assert(get_ref_frame_map_idx(cm, ref_frame) != INVALID_IDX);
-        if (!current_frame->frame_refs_short_signaling)
-          aom_wb_write_literal(wb, get_ref_frame_map_idx(cm, ref_frame),
-                               REF_FRAMES_LOG2);
+        if (!current_frame->frame_refs_short_signaling) {
+          if (cpi->ppi->rtc_ref.set_ref_frame_config &&
+              first_ref_map_idx != INVALID_IDX &&
+              cpi->svc.number_spatial_layers == 1 &&
+              !seq_params->order_hint_info.enable_order_hint) {
+            // For the usage of set_ref_frame_config:
+            // for any reference not used set their ref_map_idx
+            // to the first used reference.
+            const int map_idx = cpi->ppi->rtc_ref.reference[ref_frame - 1]
+                                    ? get_ref_frame_map_idx(cm, ref_frame)
+                                    : first_ref_map_idx;
+            aom_wb_write_literal(wb, map_idx, REF_FRAMES_LOG2);
+          } else {
+            aom_wb_write_literal(wb, get_ref_frame_map_idx(cm, ref_frame),
+                                 REF_FRAMES_LOG2);
+          }
+        }
         if (seq_params->frame_id_numbers_present_flag) {
           int i = get_ref_frame_map_idx(cm, ref_frame);
           int frame_id_len = seq_params->frame_id_length;
@@ -3362,7 +3386,6 @@ uint32_t av1_write_obu_header(AV1LevelParams *const level_params,
       (obu_type == OBU_FRAME || obu_type == OBU_FRAME_HEADER))
     ++(*frame_header_count);
 
-  struct aom_write_bit_buffer wb = { dst, 0 };
   uint32_t size = 0;
 
   // The AV1 spec Version 1.0.0 with Errata 1 has the following requirements on
@@ -3388,29 +3411,39 @@ uint32_t av1_write_obu_header(AV1LevelParams *const level_params,
         (obu_type == OBU_FRAME_HEADER || obu_type == OBU_TILE_GROUP ||
          obu_type == OBU_FRAME || obu_type == OBU_REDUNDANT_FRAME_HEADER);
   }
+  const int obu_has_size_field = 1;
 
-  aom_wb_write_literal(&wb, 0, 1);  // forbidden bit.
-  aom_wb_write_literal(&wb, (int)obu_type, 4);
-  aom_wb_write_literal(&wb, obu_extension_flag, 1);
-  aom_wb_write_literal(&wb, 1, 1);  // obu_has_size_field
-  aom_wb_write_literal(&wb, 0, 1);  // reserved
+  dst[0] = ((int)obu_type << 3) | (obu_extension_flag << 2) |
+           (obu_has_size_field << 1);
+  size++;
 
   if (obu_extension_flag) {
-    aom_wb_write_literal(&wb, obu_extension & 0xFF, 8);
+    dst[1] = obu_extension & 0xFF;
+    size++;
   }
 
-  size = aom_wb_bytes_written(&wb);
   return size;
 }
 
-int av1_write_uleb_obu_size(size_t obu_header_size, size_t obu_payload_size,
-                            uint8_t *dest) {
-  const size_t offset = obu_header_size;
+int av1_write_uleb_obu_size(size_t obu_payload_size, uint8_t *dest,
+                            size_t dest_size) {
+  size_t coded_obu_size = 0;
+
+  if (aom_uleb_encode(obu_payload_size, dest_size, dest, &coded_obu_size) !=
+      0) {
+    return AOM_CODEC_ERROR;
+  }
+  if (coded_obu_size != dest_size) {
+    return AOM_CODEC_ERROR;
+  }
+
+  return AOM_CODEC_OK;
+}
+
+int av1_write_uleb_obu_size_unsafe(size_t obu_payload_size, uint8_t *dest) {
   size_t coded_obu_size = 0;
-  const uint32_t obu_size = (uint32_t)obu_payload_size;
-  assert(obu_size == obu_payload_size);
 
-  if (aom_uleb_encode(obu_size, sizeof(obu_size), dest + offset,
+  if (aom_uleb_encode(obu_payload_size, sizeof(uint32_t), dest,
                       &coded_obu_size) != 0) {
     return AOM_CODEC_ERROR;
   }
@@ -3418,10 +3451,30 @@ int av1_write_uleb_obu_size(size_t obu_header_size, size_t obu_payload_size,
   return AOM_CODEC_OK;
 }
 
+// Returns 0 on failure.
 static size_t obu_memmove(size_t obu_header_size, size_t obu_payload_size,
-                          uint8_t *data) {
+                          uint8_t *data, size_t data_size) {
+  const size_t length_field_size = aom_uleb_size_in_bytes(obu_payload_size);
+  const size_t move_dst_offset = obu_header_size + length_field_size;
+  const size_t move_src_offset = obu_header_size;
+  const size_t move_size = obu_payload_size;
+  if (move_size > data_size || move_src_offset > data_size - move_size) {
+    assert(0 && "obu_memmove: output buffer overflow");
+    return 0;
+  }
+  if (move_dst_offset > data_size - move_size) {
+    // Buffer full.
+    return 0;
+  }
+  memmove(data + move_dst_offset, data + move_src_offset, move_size);
+  return length_field_size;
+}
+
+// Deprecated. Use obu_memmove() instead.
+static size_t obu_memmove_unsafe(size_t obu_header_size,
+                                 size_t obu_payload_size, uint8_t *data) {
   const size_t length_field_size = aom_uleb_size_in_bytes(obu_payload_size);
-  const size_t move_dst_offset = length_field_size + obu_header_size;
+  const size_t move_dst_offset = obu_header_size + length_field_size;
   const size_t move_src_offset = obu_header_size;
   const size_t move_size = obu_payload_size;
   memmove(data + move_dst_offset, data + move_src_offset, move_size);
@@ -3444,7 +3497,9 @@ static inline void write_bitstream_level(AV1_LEVEL seq_level_idx,
 }
 
 uint32_t av1_write_sequence_header_obu(const SequenceHeader *seq_params,
-                                       uint8_t *const dst) {
+                                       uint8_t *const dst, size_t dst_size) {
+  // TODO: bug 42302568 - Use dst_size.
+  (void)dst_size;
   struct aom_write_bit_buffer wb = { dst, 0 };
   uint32_t size = 0;
 
@@ -3607,9 +3662,9 @@ static void write_large_scale_tile_obu_size(
   *total_size += lst_obu->tg_hdr_size;
   const uint32_t obu_payload_size = *total_size - lst_obu->tg_hdr_size;
   const size_t length_field_size =
-      obu_memmove(lst_obu->tg_hdr_size, obu_payload_size, dst);
-  if (av1_write_uleb_obu_size(lst_obu->tg_hdr_size, obu_payload_size, dst) !=
-      AOM_CODEC_OK)
+      obu_memmove_unsafe(lst_obu->tg_hdr_size, obu_payload_size, dst);
+  if (av1_write_uleb_obu_size_unsafe(
+          obu_payload_size, dst + lst_obu->tg_hdr_size) != AOM_CODEC_OK)
     assert(0);
 
   *total_size += (uint32_t)length_field_size;
@@ -3831,15 +3886,15 @@ void av1_write_last_tile_info(
     uint8_t **tile_data_start, int *const largest_tile_id,
     int *const is_first_tg, uint32_t obu_header_size, uint8_t obu_extn_header) {
   // write current tile group size
-  const uint32_t obu_payload_size =
-      (uint32_t)(*curr_tg_data_size) - obu_header_size;
+  const size_t obu_payload_size = *curr_tg_data_size - obu_header_size;
   const size_t length_field_size =
-      obu_memmove(obu_header_size, obu_payload_size, curr_tg_start);
-  if (av1_write_uleb_obu_size(obu_header_size, obu_payload_size,
-                              curr_tg_start) != AOM_CODEC_OK) {
-    assert(0);
+      obu_memmove_unsafe(obu_header_size, obu_payload_size, curr_tg_start);
+  if (av1_write_uleb_obu_size_unsafe(
+          obu_payload_size, curr_tg_start + obu_header_size) != AOM_CODEC_OK) {
+    aom_internal_error(cpi->common.error, AOM_CODEC_ERROR,
+                       "av1_write_last_tile_info: output buffer full");
   }
-  *curr_tg_data_size += (int)length_field_size;
+  *curr_tg_data_size += length_field_size;
   *total_size += (uint32_t)length_field_size;
   *tile_data_start += length_field_size;
   if (cpi->num_tg == 1) {
@@ -3869,8 +3924,8 @@ void av1_write_last_tile_info(
         cpi->common.seq_params->has_nonzero_operating_point_idc,
         obu_extn_header, &curr_tg_start[fh_info->obu_header_byte_offset]);
 
-    *curr_tg_data_size += (int)(fh_info->total_length);
-    *total_size += (uint32_t)(fh_info->total_length);
+    *curr_tg_data_size += fh_info->total_length;
+    *total_size += (uint32_t)fh_info->total_length;
   }
   *is_first_tg = 0;
 }
@@ -4107,10 +4162,13 @@ static inline uint32_t pack_tiles_in_tg_obus(
 }
 
 static uint32_t write_tiles_in_tg_obus(AV1_COMP *const cpi, uint8_t *const dst,
+                                       size_t dst_size,
                                        struct aom_write_bit_buffer *saved_wb,
                                        uint8_t obu_extension_header,
                                        const FrameHeaderInfo *fh_info,
                                        int *const largest_tile_id) {
+  // TODO: bug 42302568 - Use dst_size.
+  (void)dst_size;
   AV1_COMMON *const cm = &cpi->common;
   const CommonTileParams *const tiles = &cm->tiles;
   *largest_tile_id = 0;
@@ -4135,21 +4193,26 @@ static uint32_t write_tiles_in_tg_obus(AV1_COMP *const cpi, uint8_t *const dst,
                                fh_info, largest_tile_id);
 }
 
+// Returns the number of bytes written on success. Returns 0 on failure.
 static size_t av1_write_metadata_obu(const aom_metadata_t *metadata,
-                                     uint8_t *const dst) {
+                                     uint8_t *const dst, size_t dst_size) {
   size_t coded_metadata_size = 0;
   const uint64_t metadata_type = (uint64_t)metadata->type;
-  if (aom_uleb_encode(metadata_type, sizeof(metadata_type), dst,
-                      &coded_metadata_size) != 0) {
+  if (aom_uleb_encode(metadata_type, dst_size, dst, &coded_metadata_size) !=
+      0) {
+    return 0;
+  }
+  if (coded_metadata_size + metadata->sz + 1 > dst_size) {
     return 0;
   }
   memcpy(dst + coded_metadata_size, metadata->payload, metadata->sz);
   // Add trailing bits.
   dst[coded_metadata_size + metadata->sz] = 0x80;
-  return (uint32_t)(coded_metadata_size + metadata->sz + 1);
+  return coded_metadata_size + metadata->sz + 1;
 }
 
-static size_t av1_write_metadata_array(AV1_COMP *const cpi, uint8_t *dst) {
+static size_t av1_write_metadata_array(AV1_COMP *const cpi, uint8_t *dst,
+                                       size_t dst_size) {
   if (!cpi->source) return 0;
   AV1_COMMON *const cm = &cpi->common;
   aom_metadata_array_t *arr = cpi->source->metadata;
@@ -4166,20 +4229,38 @@ static size_t av1_write_metadata_array(AV1_COMP *const cpi, uint8_t *dst) {
           (cm->current_frame.frame_type != KEY_FRAME &&
            current_metadata->insert_flag == AOM_MIF_NON_KEY_FRAME) ||
           current_metadata->insert_flag == AOM_MIF_ANY_FRAME) {
+        // OBU header is either one or two bytes.
+        if (dst_size < 2) {
+          aom_internal_error(cm->error, AOM_CODEC_ERROR,
+                             "av1_write_metadata_array: output buffer full");
+        }
         obu_header_size = av1_write_obu_header(
             &cpi->ppi->level_params, &cpi->frame_header_count, OBU_METADATA,
             cm->seq_params->has_nonzero_operating_point_idc, 0, dst);
+        assert(obu_header_size <= 2);
         obu_payload_size =
-            av1_write_metadata_obu(current_metadata, dst + obu_header_size);
-        length_field_size = obu_memmove(obu_header_size, obu_payload_size, dst);
-        if (av1_write_uleb_obu_size(obu_header_size, obu_payload_size, dst) ==
-            AOM_CODEC_OK) {
-          const size_t obu_size = obu_header_size + obu_payload_size;
-          dst += obu_size + length_field_size;
-          total_bytes_written += obu_size + length_field_size;
+            av1_write_metadata_obu(current_metadata, dst + obu_header_size,
+                                   dst_size - obu_header_size);
+        if (obu_payload_size == 0) {
+          aom_internal_error(cm->error, AOM_CODEC_ERROR,
+                             "av1_write_metadata_array: output buffer full");
+        }
+        length_field_size =
+            obu_memmove(obu_header_size, obu_payload_size, dst, dst_size);
+        if (length_field_size == 0) {
+          aom_internal_error(cm->error, AOM_CODEC_ERROR,
+                             "av1_write_metadata_array: output buffer full");
+        }
+        if (av1_write_uleb_obu_size(obu_payload_size, dst + obu_header_size,
+                                    length_field_size) == AOM_CODEC_OK) {
+          const size_t obu_size =
+              obu_header_size + length_field_size + obu_payload_size;
+          dst += obu_size;
+          dst_size -= obu_size;
+          total_bytes_written += obu_size;
         } else {
           aom_internal_error(cpi->common.error, AOM_CODEC_ERROR,
-                             "Error writing metadata OBU size");
+                             "av1_write_metadata_array: output buffer full");
         }
       }
     }
@@ -4187,10 +4268,10 @@ static size_t av1_write_metadata_array(AV1_COMP *const cpi, uint8_t *dst) {
   return total_bytes_written;
 }
 
-int av1_pack_bitstream(AV1_COMP *const cpi, uint8_t *dst, size_t *size,
-                       int *const largest_tile_id) {
+int av1_pack_bitstream(AV1_COMP *const cpi, uint8_t *dst, size_t dst_size,
+                       size_t *size, int *const largest_tile_id) {
   uint8_t *data = dst;
-  uint32_t data_size;
+  size_t data_size = dst_size;
   AV1_COMMON *const cm = &cpi->common;
   AV1LevelParams *const level_params = &cpi->ppi->level_params;
   uint32_t obu_header_size = 0;
@@ -4216,23 +4297,38 @@ int av1_pack_bitstream(AV1_COMP *const cpi, uint8_t *dst, size_t *size,
   // preceded by 4-byte size
   if (cm->current_frame.frame_type == INTRA_ONLY_FRAME ||
       cm->current_frame.frame_type == KEY_FRAME) {
+    // OBU header is either one or two bytes.
+    if (data_size < 2) {
+      return AOM_CODEC_ERROR;
+    }
     obu_header_size = av1_write_obu_header(
         level_params, &cpi->frame_header_count, OBU_SEQUENCE_HEADER,
         cm->seq_params->has_nonzero_operating_point_idc, 0, data);
-    obu_payload_size =
-        av1_write_sequence_header_obu(cm->seq_params, data + obu_header_size);
+    assert(obu_header_size <= 2);
+    obu_payload_size = av1_write_sequence_header_obu(
+        cm->seq_params, data + obu_header_size, data_size - obu_header_size);
     const size_t length_field_size =
-        obu_memmove(obu_header_size, obu_payload_size, data);
-    if (av1_write_uleb_obu_size(obu_header_size, obu_payload_size, data) !=
-        AOM_CODEC_OK) {
+        obu_memmove(obu_header_size, obu_payload_size, data, data_size);
+    if (length_field_size == 0) {
+      return AOM_CODEC_ERROR;
+    }
+    if (av1_write_uleb_obu_size(obu_payload_size, data + obu_header_size,
+                                length_field_size) != AOM_CODEC_OK) {
       return AOM_CODEC_ERROR;
     }
 
-    data += obu_header_size + obu_payload_size + length_field_size;
+    const size_t bytes_written =
+        obu_header_size + length_field_size + obu_payload_size;
+    data += bytes_written;
+    data_size -= bytes_written;
   }
 
   // write metadata obus before the frame obu that has the show_frame flag set
-  if (cm->show_frame) data += av1_write_metadata_array(cpi, data);
+  if (cm->show_frame) {
+    const size_t bytes_written = av1_write_metadata_array(cpi, data, data_size);
+    data += bytes_written;
+    data_size -= bytes_written;
+  }
 
   const int write_frame_header =
       (cpi->num_tg > 1 || encode_show_existing_frame(cm));
@@ -4241,27 +4337,40 @@ int av1_pack_bitstream(AV1_COMP *const cpi, uint8_t *dst, size_t *size,
   if (write_frame_header) {
     // Write Frame Header OBU.
     fh_info.frame_header = data;
+    // OBU header is either one or two bytes.
+    if (data_size < 2) {
+      return AOM_CODEC_ERROR;
+    }
     obu_header_size = av1_write_obu_header(
         level_params, &cpi->frame_header_count, OBU_FRAME_HEADER,
         cm->seq_params->has_nonzero_operating_point_idc, obu_extension_header,
         data);
+    // TODO: bug 42302568 - Pass data_size - obu_header_size to
+    // write_frame_header_obu().
     obu_payload_size = write_frame_header_obu(cpi, &cpi->td.mb.e_mbd, &saved_wb,
                                               data + obu_header_size, 1);
 
-    length_field = obu_memmove(obu_header_size, obu_payload_size, data);
-    if (av1_write_uleb_obu_size(obu_header_size, obu_payload_size, data) !=
-        AOM_CODEC_OK) {
+    length_field =
+        obu_memmove(obu_header_size, obu_payload_size, data, data_size);
+    if (length_field == 0) {
+      return AOM_CODEC_ERROR;
+    }
+    if (av1_write_uleb_obu_size(obu_payload_size, data + obu_header_size,
+                                length_field) != AOM_CODEC_OK) {
       return AOM_CODEC_ERROR;
     }
 
     fh_info.obu_header_byte_offset = 0;
-    fh_info.total_length = obu_header_size + obu_payload_size + length_field;
+    fh_info.total_length = obu_header_size + length_field + obu_payload_size;
+    // Make sure it is safe to cast fh_info.total_length to uint32_t.
+    if (fh_info.total_length > UINT32_MAX) {
+      return AOM_CODEC_ERROR;
+    }
     data += fh_info.total_length;
+    data_size -= fh_info.total_length;
   }
 
-  if (encode_show_existing_frame(cm)) {
-    data_size = 0;
-  } else {
+  if (!encode_show_existing_frame(cm)) {
     // Since length_field is determined adaptively after frame header
     // encoding, saved_wb must be adjusted accordingly.
     if (saved_wb.bit_buffer != NULL) {
@@ -4270,10 +4379,13 @@ int av1_pack_bitstream(AV1_COMP *const cpi, uint8_t *dst, size_t *size,
 
     //  Each tile group obu will be preceded by 4-byte size of the tile group
     //  obu
-    data_size = write_tiles_in_tg_obus(
-        cpi, data, &saved_wb, obu_extension_header, &fh_info, largest_tile_id);
+    const size_t bytes_written =
+        write_tiles_in_tg_obus(cpi, data, data_size, &saved_wb,
+                               obu_extension_header, &fh_info, largest_tile_id);
+    data += bytes_written;
+    data_size -= bytes_written;
   }
-  data += data_size;
   *size = data - dst;
+  (void)data_size;
   return AOM_CODEC_OK;
 }
diff --git a/av1/encoder/bitstream.h b/av1/encoder/bitstream.h
index a8f3cc541..48b22169e 100644
--- a/av1/encoder/bitstream.h
+++ b/av1/encoder/bitstream.h
@@ -17,6 +17,8 @@ extern "C" {
 #endif
 
 #include <stdbool.h>
+#include <stddef.h>
+#include <stdint.h>
 
 #include "av1/common/av1_common_int.h"
 #include "av1/common/blockd.h"
@@ -88,7 +90,7 @@ typedef struct {
 // payload written to 'dst'. This function does not write the OBU header, the
 // optional extension, or the OBU size to 'dst'.
 uint32_t av1_write_sequence_header_obu(const SequenceHeader *seq_params,
-                                       uint8_t *const dst);
+                                       uint8_t *const dst, size_t dst_size);
 
 // Writes the OBU header byte, and the OBU header extension byte when
 // has_nonzero_operating_point_idc is true and the OBU is part of a frame.
@@ -98,8 +100,14 @@ uint32_t av1_write_obu_header(AV1LevelParams *const level_params,
                               bool has_nonzero_operating_point_idc,
                               int obu_extension, uint8_t *const dst);
 
-int av1_write_uleb_obu_size(size_t obu_header_size, size_t obu_payload_size,
-                            uint8_t *dest);
+// Encodes obu_payload_size as a leb128 integer and writes it to the dest
+// buffer. The output must fill the buffer exactly. Returns AOM_CODEC_OK on
+// success, AOM_CODEC_ERROR on failure.
+int av1_write_uleb_obu_size(size_t obu_payload_size, uint8_t *dest,
+                            size_t dest_size);
+
+// Deprecated. Use av1_write_uleb_obu_size() instead.
+int av1_write_uleb_obu_size_unsafe(size_t obu_payload_size, uint8_t *dest);
 
 // Pack tile data in the bitstream with tile_group, frame
 // and OBU header.
@@ -118,7 +126,8 @@ void av1_write_last_tile_info(
  * \ingroup high_level_algo
  * \callgraph
  */
-int av1_pack_bitstream(struct AV1_COMP *const cpi, uint8_t *dst, size_t *size,
+int av1_pack_bitstream(struct AV1_COMP *const cpi, uint8_t *dst,
+                       size_t dst_size, size_t *size,
                        int *const largest_tile_id);
 
 void av1_write_tx_type(const AV1_COMMON *const cm, const MACROBLOCKD *xd,
diff --git a/av1/encoder/encode_strategy.c b/av1/encoder/encode_strategy.c
index d34a84c33..60a7b4114 100644
--- a/av1/encoder/encode_strategy.c
+++ b/av1/encoder/encode_strategy.c
@@ -716,9 +716,10 @@ int av1_get_refresh_frame_flags(
 // If the current frame does not require filtering, this function is identical
 // to av1_encode() except that tpl is not performed.
 static int denoise_and_encode(AV1_COMP *const cpi, uint8_t *const dest,
+                              size_t dest_size,
                               EncodeFrameInput *const frame_input,
                               const EncodeFrameParams *const frame_params,
-                              EncodeFrameResults *const frame_results) {
+                              size_t *const frame_size) {
 #if CONFIG_COLLECT_COMPONENT_TIMING
   if (cpi->oxcf.pass == 2) start_timing(cpi, denoise_and_encode_time);
 #endif
@@ -900,7 +901,7 @@ static int denoise_and_encode(AV1_COMP *const cpi, uint8_t *const dest,
 #endif  // CONFIG_BITRATE_ACCURACY && CONFIG_THREE_PASS
   }
 
-  if (av1_encode(cpi, dest, frame_input, frame_params, frame_results) !=
+  if (av1_encode(cpi, dest, dest_size, frame_input, frame_params, frame_size) !=
       AOM_CODEC_OK) {
     return AOM_CODEC_ERROR;
   }
@@ -1213,8 +1214,9 @@ void av1_get_ref_frames(RefFrameMapPair ref_frame_map_pairs[REF_FRAMES],
 }
 
 int av1_encode_strategy(AV1_COMP *const cpi, size_t *const size,
-                        uint8_t *const dest, unsigned int *frame_flags,
-                        int64_t *const time_stamp, int64_t *const time_end,
+                        uint8_t *const dest, size_t dest_size,
+                        unsigned int *frame_flags, int64_t *const time_stamp,
+                        int64_t *const time_end,
                         const aom_rational64_t *const timestamp_ratio,
                         int *const pop_lookahead, int flush) {
   AV1EncoderConfig *const oxcf = &cpi->oxcf;
@@ -1225,10 +1227,10 @@ int av1_encode_strategy(AV1_COMP *const cpi, size_t *const size,
 
   EncodeFrameInput frame_input;
   EncodeFrameParams frame_params;
-  EncodeFrameResults frame_results;
+  size_t frame_size;
   memset(&frame_input, 0, sizeof(frame_input));
   memset(&frame_params, 0, sizeof(frame_params));
-  memset(&frame_results, 0, sizeof(frame_results));
+  frame_size = 0;
 
 #if CONFIG_BITRATE_ACCURACY && CONFIG_THREE_PASS
   VBR_RATECTRL_INFO *vbr_rc_info = &cpi->vbr_rc_info;
@@ -1460,6 +1462,7 @@ int av1_encode_strategy(AV1_COMP *const cpi, size_t *const size,
     adjust_frame_rate(cpi, source->ts_start, source->ts_end);
 
   if (!frame_params.show_existing_frame) {
+#if !CONFIG_REALTIME_ONLY
     if (cpi->film_grain_table) {
       cm->cur_frame->film_grain_params_present = aom_film_grain_table_lookup(
           cpi->film_grain_table, *time_stamp, *time_end, 0 /* =erase */,
@@ -1468,6 +1471,7 @@ int av1_encode_strategy(AV1_COMP *const cpi, size_t *const size,
       cm->cur_frame->film_grain_params_present =
           cm->seq_params->film_grain_params_present;
     }
+#endif
     // only one operating point supported now
     const int64_t pts64 = ticks_to_timebase_units(timestamp_ratio, *time_stamp);
     if (pts64 < 0 || pts64 > UINT32_MAX) return AOM_CODEC_ERROR;
@@ -1669,19 +1673,19 @@ int av1_encode_strategy(AV1_COMP *const cpi, size_t *const size,
   }
 
 #if CONFIG_REALTIME_ONLY
-  if (av1_encode(cpi, dest, &frame_input, &frame_params, &frame_results) !=
-      AOM_CODEC_OK) {
+  if (av1_encode(cpi, dest, dest_size, &frame_input, &frame_params,
+                 &frame_size) != AOM_CODEC_OK) {
     return AOM_CODEC_ERROR;
   }
 #else
   if (has_no_stats_stage(cpi) && oxcf->mode == REALTIME &&
       gf_cfg->lag_in_frames == 0) {
-    if (av1_encode(cpi, dest, &frame_input, &frame_params, &frame_results) !=
-        AOM_CODEC_OK) {
+    if (av1_encode(cpi, dest, dest_size, &frame_input, &frame_params,
+                   &frame_size) != AOM_CODEC_OK) {
       return AOM_CODEC_ERROR;
     }
-  } else if (denoise_and_encode(cpi, dest, &frame_input, &frame_params,
-                                &frame_results) != AOM_CODEC_OK) {
+  } else if (denoise_and_encode(cpi, dest, dest_size, &frame_input,
+                                &frame_params, &frame_size) != AOM_CODEC_OK) {
     return AOM_CODEC_ERROR;
   }
 #endif  // CONFIG_REALTIME_ONLY
@@ -1721,8 +1725,7 @@ int av1_encode_strategy(AV1_COMP *const cpi, size_t *const size,
   }
 #endif
 
-  // Unpack frame_results:
-  *size = frame_results.size;
+  *size = frame_size;
 
   // Leave a signal for a higher level caller about if this frame is droppable
   if (*size > 0) {
diff --git a/av1/encoder/encode_strategy.h b/av1/encoder/encode_strategy.h
index 79615bec7..f6c6aa01e 100644
--- a/av1/encoder/encode_strategy.h
+++ b/av1/encoder/encode_strategy.h
@@ -38,8 +38,9 @@ extern "C" {
  * returns AOM_CODEC_OK.
  *
  * \param[in]    cpi         Top-level encoder structure
- * \param[in]    size        Bitstream size
- * \param[in]    dest        Bitstream output
+ * \param[out]   size        Bitstream size
+ * \param[out]   dest        Bitstream output buffer
+ * \param[in]    dest_size   Bitstream output buffer size
  * \param[in]    frame_flags Flags to decide how to encoding the frame
  * \param[out]   time_stamp  Time stamp of the frame
  * \param[out]   time_end    Time end
@@ -53,8 +54,9 @@ extern "C" {
  * \retval #AOM_CODEC_ERROR
  */
 int av1_encode_strategy(AV1_COMP *const cpi, size_t *const size,
-                        uint8_t *const dest, unsigned int *frame_flags,
-                        int64_t *const time_stamp, int64_t *const time_end,
+                        uint8_t *const dest, size_t dest_size,
+                        unsigned int *frame_flags, int64_t *const time_stamp,
+                        int64_t *const time_end,
                         const aom_rational64_t *const timestamp_ratio,
                         int *const pop_lookahead, int flush);
 
diff --git a/av1/encoder/encodeframe.c b/av1/encoder/encodeframe.c
index 1c0a8fba6..5456d5a0f 100644
--- a/av1/encoder/encodeframe.c
+++ b/av1/encoder/encodeframe.c
@@ -1461,8 +1461,10 @@ void av1_encode_tile(AV1_COMP *cpi, ThreadData *td, int tile_row,
   av1_init_above_context(&cm->above_contexts, av1_num_planes(cm), tile_row,
                          &td->mb.e_mbd);
 
+#if !CONFIG_REALTIME_ONLY
   if (cpi->oxcf.intra_mode_cfg.enable_cfl_intra)
     cfl_init(&td->mb.e_mbd.cfl, cm->seq_params);
+#endif
 
   if (td->mb.txfm_search_info.mb_rd_record != NULL) {
     av1_crc32c_calculator_init(
diff --git a/av1/encoder/encodemb.c b/av1/encoder/encodemb.c
index a300f88d7..447cae4c0 100644
--- a/av1/encoder/encodemb.c
+++ b/av1/encoder/encodemb.c
@@ -831,9 +831,11 @@ static void encode_block_intra(int plane, int block, int blk_row, int blk_col,
   // skip_txfm = 1 is very expensive.
   mbmi->skip_txfm = 0;
 
+#if !CONFIG_REALTIME_ONLY
   if (plane == AOM_PLANE_Y && xd->cfl.store_y) {
     cfl_store_tx(xd, blk_row, blk_col, tx_size, plane_bsize);
   }
+#endif
 }
 
 static void encode_block_intra_and_set_context(int plane, int block,
diff --git a/av1/encoder/encoder.c b/av1/encoder/encoder.c
index 60be4c8a7..ce142ec6d 100644
--- a/av1/encoder/encoder.c
+++ b/av1/encoder/encoder.c
@@ -88,7 +88,9 @@
 #include "av1/encoder/segmentation.h"
 #include "av1/encoder/speed_features.h"
 #include "av1/encoder/superres_scale.h"
+#if CONFIG_THREE_PASS
 #include "av1/encoder/thirdpass.h"
+#endif
 #include "av1/encoder/tpl_model.h"
 #include "av1/encoder/reconinter_enc.h"
 #include "av1/encoder/var_based_part.h"
@@ -485,9 +487,9 @@ static void set_bitstream_level_tier(AV1_PRIMARY *const ppi, int width,
   }
 }
 
-void av1_init_seq_coding_tools(AV1_PRIMARY *const ppi,
-                               const AV1EncoderConfig *oxcf,
-                               int disable_frame_id_numbers) {
+static void init_seq_coding_tools(AV1_PRIMARY *const ppi,
+                                  const AV1EncoderConfig *oxcf,
+                                  int disable_frame_id_numbers) {
   SequenceHeader *const seq = &ppi->seq_params;
   const FrameDimensionCfg *const frm_dim_cfg = &oxcf->frm_dim_cfg;
   const ToolCfg *const tool_cfg = &oxcf->tool_cfg;
@@ -749,7 +751,9 @@ void av1_change_config_seq(struct AV1_PRIMARY *ppi,
         10;  // Default value (not signaled)
   }
 
+#if !CONFIG_REALTIME_ONLY
   av1_update_film_grain_parameters_seq(ppi, oxcf);
+#endif
 
   int sb_size = seq_params->sb_size;
   // Superblock size should not be updated after the first key frame.
@@ -770,8 +774,8 @@ void av1_change_config_seq(struct AV1_PRIMARY *ppi,
         (ppi->number_spatial_layers > 1 || ppi->number_temporal_layers > 1)
             ? ppi->number_spatial_layers * ppi->number_temporal_layers - 1
             : 0;
-    av1_init_seq_coding_tools(
-        ppi, oxcf, ppi->use_svc || ppi->rtc_ref.set_ref_frame_config);
+    init_seq_coding_tools(ppi, oxcf,
+                          ppi->use_svc || ppi->rtc_ref.set_ref_frame_config);
   }
   seq_params->timing_info_present &= !seq_params->reduced_still_picture_hdr;
 
@@ -805,7 +809,9 @@ void av1_change_config(struct AV1_COMP *cpi, const AV1EncoderConfig *oxcf,
 
   cpi->oxcf = *oxcf;
 
+#if !CONFIG_REALTIME_ONLY
   av1_update_film_grain_parameters(cpi, oxcf);
+#endif
 
   // When user provides superres_mode = AOM_SUPERRES_AUTO, we still initialize
   // superres mode for current encoding = AOM_SUPERRES_NONE. This is to ensure
@@ -1613,10 +1619,12 @@ AV1_COMP *av1_create_compressor(AV1_PRIMARY *ppi, const AV1EncoderConfig *oxcf,
   av1_loop_restoration_precal();
 #endif
 
+#if CONFIG_THREE_PASS
   cpi->third_pass_ctx = NULL;
   if (cpi->oxcf.pass == AOM_RC_THIRD_PASS) {
     av1_init_thirdpass_ctx(cm, &cpi->third_pass_ctx, NULL);
   }
+#endif
 
   cpi->second_pass_log_stream = NULL;
   cpi->use_ducky_encode = 0;
@@ -1746,9 +1754,11 @@ void av1_remove_compressor(AV1_COMP *cpi) {
 #endif
   }
 
+#if CONFIG_THREE_PASS
   av1_free_thirdpass_ctx(cpi->third_pass_ctx);
 
   av1_close_second_pass_log(cpi);
+#endif
 
   dealloc_compressor_data(cpi);
 
@@ -2403,10 +2413,7 @@ static void loopfilter_frame(AV1_COMP *cpi, AV1_COMMON *cm) {
 
   const int use_loopfilter =
       is_loopfilter_used(cm) && !cpi->mt_info.pipeline_lpf_mt_with_enc;
-  const int use_cdef =
-      is_cdef_used(cm) && (!cpi->active_map.enabled ||
-                           cpi->rc.percent_blocks_inactive <=
-                               cpi->sf.rt_sf.thresh_active_maps_skip_lf_cdef);
+  const int use_cdef = is_cdef_used(cm);
   const int use_superres = av1_superres_scaled(cm);
   const int use_restoration = is_restoration_used(cm);
 
@@ -2767,14 +2774,16 @@ static int encode_without_recode(AV1_COMP *cpi) {
  *
  * \param[in]    cpi             Top-level encoder structure
  * \param[in]    size            Bitstream size
- * \param[in]    dest            Bitstream output
+ * \param[out]   dest            Bitstream output buffer
+ * \param[in]    dest_size       Bitstream output buffer size
  *
  * \return Returns a value to indicate if the encoding is done successfully.
  * \retval #AOM_CODEC_OK
  * \retval -1
  * \retval #AOM_CODEC_ERROR
  */
-static int encode_with_recode_loop(AV1_COMP *cpi, size_t *size, uint8_t *dest) {
+static int encode_with_recode_loop(AV1_COMP *cpi, size_t *size, uint8_t *dest,
+                                   size_t dest_size) {
   AV1_COMMON *const cm = &cpi->common;
   RATE_CONTROL *const rc = &cpi->rc;
   GlobalMotionInfo *const gm_info = &cpi->gm_info;
@@ -3051,7 +3060,7 @@ static int encode_with_recode_loop(AV1_COMP *cpi, size_t *size, uint8_t *dest) {
       av1_finalize_encoded_frame(cpi);
       int largest_tile_id = 0;  // Output from bitstream: unused here
       rc->coefficient_size = 0;
-      if (av1_pack_bitstream(cpi, dest, size, &largest_tile_id) !=
+      if (av1_pack_bitstream(cpi, dest, dest_size, size, &largest_tile_id) !=
           AOM_CODEC_OK) {
         return AOM_CODEC_ERROR;
       }
@@ -3171,7 +3180,8 @@ static void set_grain_syn_params(AV1_COMMON *cm) {
  *
  * \param[in]    cpi             Top-level encoder structure
  * \param[in]    size            Bitstream size
- * \param[in]    dest            Bitstream output
+ * \param[out]   dest            Bitstream output buffer
+ * \param[in]    dest_size       Bitstream output buffer size
  * \param[in]    sse             Total distortion of the frame
  * \param[in]    rate            Total rate of the frame
  * \param[in]    largest_tile_id Tile id of the last tile
@@ -3181,8 +3191,8 @@ static void set_grain_syn_params(AV1_COMMON *cm) {
  * \retval #AOM_CODEC_ERROR
  */
 static int encode_with_recode_loop_and_filter(AV1_COMP *cpi, size_t *size,
-                                              uint8_t *dest, int64_t *sse,
-                                              int64_t *rate,
+                                              uint8_t *dest, size_t dest_size,
+                                              int64_t *sse, int64_t *rate,
                                               int *largest_tile_id) {
 #if CONFIG_COLLECT_COMPONENT_TIMING
   start_timing(cpi, encode_with_or_without_recode_time);
@@ -3202,7 +3212,7 @@ static int encode_with_recode_loop_and_filter(AV1_COMP *cpi, size_t *size,
   if (cpi->sf.hl_sf.recode_loop == DISALLOW_RECODE)
     err = encode_without_recode(cpi);
   else
-    err = encode_with_recode_loop(cpi, size, dest);
+    err = encode_with_recode_loop(cpi, size, dest, dest_size);
 #endif
 #if CONFIG_COLLECT_COMPONENT_TIMING
   end_timing(cpi, encode_with_or_without_recode_time);
@@ -3281,7 +3291,8 @@ static int encode_with_recode_loop_and_filter(AV1_COMP *cpi, size_t *size,
   start_timing(cpi, av1_pack_bitstream_final_time);
 #endif
   cpi->rc.coefficient_size = 0;
-  if (av1_pack_bitstream(cpi, dest, size, largest_tile_id) != AOM_CODEC_OK)
+  if (av1_pack_bitstream(cpi, dest, dest_size, size, largest_tile_id) !=
+      AOM_CODEC_OK)
     return AOM_CODEC_ERROR;
 #if CONFIG_COLLECT_COMPONENT_TIMING
   end_timing(cpi, av1_pack_bitstream_final_time);
@@ -3325,7 +3336,7 @@ static int encode_with_recode_loop_and_filter(AV1_COMP *cpi, size_t *size,
 }
 
 static int encode_with_and_without_superres(AV1_COMP *cpi, size_t *size,
-                                            uint8_t *dest,
+                                            uint8_t *dest, size_t dest_size,
                                             int *largest_tile_id) {
   const AV1_COMMON *const cm = &cpi->common;
   assert(cm->seq_params->enable_superres);
@@ -3361,7 +3372,7 @@ static int encode_with_and_without_superres(AV1_COMP *cpi, size_t *size,
 
         cpi->superres_mode = AOM_SUPERRES_AUTO;  // Super-res on for this loop.
         err = encode_with_recode_loop_and_filter(
-            cpi, size, dest, &superres_sses[this_index],
+            cpi, size, dest, dest_size, &superres_sses[this_index],
             &superres_rates[this_index],
             &superres_largest_tile_ids[this_index]);
         cpi->superres_mode = AOM_SUPERRES_NONE;  // Reset to default (full-res).
@@ -3381,8 +3392,8 @@ static int encode_with_and_without_superres(AV1_COMP *cpi, size_t *size,
     }
     // Encode without superres.
     assert(cpi->superres_mode == AOM_SUPERRES_NONE);
-    err = encode_with_recode_loop_and_filter(cpi, size, dest, &sse2, &rate2,
-                                             &largest_tile_id2);
+    err = encode_with_recode_loop_and_filter(cpi, size, dest, dest_size, &sse2,
+                                             &rate2, &largest_tile_id2);
     if (err != AOM_CODEC_OK) return err;
 
     // Note: Both use common rdmult based on base qindex of fullres.
@@ -3422,8 +3433,8 @@ static int encode_with_and_without_superres(AV1_COMP *cpi, size_t *size,
       int64_t rate3 = INT64_MAX;
       cpi->superres_mode =
           AOM_SUPERRES_AUTO;  // Super-res on for this recode loop.
-      err = encode_with_recode_loop_and_filter(cpi, size, dest, &sse3, &rate3,
-                                               largest_tile_id);
+      err = encode_with_recode_loop_and_filter(cpi, size, dest, dest_size,
+                                               &sse3, &rate3, largest_tile_id);
       cpi->superres_mode = AOM_SUPERRES_NONE;  // Reset to default (full-res).
       assert(sse1 == sse3);
       assert(rate1 == rate3);
@@ -3438,15 +3449,15 @@ static int encode_with_and_without_superres(AV1_COMP *cpi, size_t *size,
     assert(cpi->sf.hl_sf.superres_auto_search_type == SUPERRES_AUTO_DUAL);
     cpi->superres_mode =
         AOM_SUPERRES_AUTO;  // Super-res on for this recode loop.
-    err = encode_with_recode_loop_and_filter(cpi, size, dest, &sse1, &rate1,
-                                             &largest_tile_id1);
+    err = encode_with_recode_loop_and_filter(cpi, size, dest, dest_size, &sse1,
+                                             &rate1, &largest_tile_id1);
     cpi->superres_mode = AOM_SUPERRES_NONE;  // Reset to default (full-res).
     if (err != AOM_CODEC_OK) return err;
     restore_all_coding_context(cpi);
     // Encode without superres.
     assert(cpi->superres_mode == AOM_SUPERRES_NONE);
-    err = encode_with_recode_loop_and_filter(cpi, size, dest, &sse2, &rate2,
-                                             &largest_tile_id2);
+    err = encode_with_recode_loop_and_filter(cpi, size, dest, dest_size, &sse2,
+                                             &rate2, &largest_tile_id2);
     if (err != AOM_CODEC_OK) return err;
 
     // Note: Both use common rdmult based on base qindex of fullres.
@@ -3466,8 +3477,8 @@ static int encode_with_and_without_superres(AV1_COMP *cpi, size_t *size,
       int64_t rate3 = INT64_MAX;
       cpi->superres_mode =
           AOM_SUPERRES_AUTO;  // Super-res on for this recode loop.
-      err = encode_with_recode_loop_and_filter(cpi, size, dest, &sse3, &rate3,
-                                               largest_tile_id);
+      err = encode_with_recode_loop_and_filter(cpi, size, dest, dest_size,
+                                               &sse3, &rate3, largest_tile_id);
       cpi->superres_mode = AOM_SUPERRES_NONE;  // Reset to default (full-res).
       assert(sse1 == sse3);
       assert(rate1 == rate3);
@@ -3579,14 +3590,15 @@ static void calculate_frame_avg_haar_energy(AV1_COMP *cpi) {
  *
  * \param[in]    cpi             Top-level encoder structure
  * \param[in]    size            Bitstream size
- * \param[in]    dest            Bitstream output
+ * \param[out]   dest            Bitstream output buffer
+ * \param[in]    dest_size       Bitstream output buffer size
  *
  * \return Returns a value to indicate if the encoding is done successfully.
  * \retval #AOM_CODEC_OK
  * \retval #AOM_CODEC_ERROR
  */
-static int encode_frame_to_data_rate(AV1_COMP *cpi, size_t *size,
-                                     uint8_t *dest) {
+static int encode_frame_to_data_rate(AV1_COMP *cpi, size_t *size, uint8_t *dest,
+                                     size_t dest_size) {
   AV1_COMMON *const cm = &cpi->common;
   SequenceHeader *const seq_params = cm->seq_params;
   CurrentFrame *const current_frame = &cm->current_frame;
@@ -3648,7 +3660,8 @@ static int encode_frame_to_data_rate(AV1_COMP *cpi, size_t *size,
     // Build the bitstream
     int largest_tile_id = 0;  // Output from bitstream: unused here
     cpi->rc.coefficient_size = 0;
-    if (av1_pack_bitstream(cpi, dest, size, &largest_tile_id) != AOM_CODEC_OK)
+    if (av1_pack_bitstream(cpi, dest, dest_size, size, &largest_tile_id) !=
+        AOM_CODEC_OK)
       return AOM_CODEC_ERROR;
 
     if (seq_params->frame_id_numbers_present_flag &&
@@ -3891,14 +3904,15 @@ static int encode_frame_to_data_rate(AV1_COMP *cpi, size_t *size,
 
   int largest_tile_id = 0;
   if (av1_superres_in_recode_allowed(cpi)) {
-    if (encode_with_and_without_superres(cpi, size, dest, &largest_tile_id) !=
-        AOM_CODEC_OK) {
+    if (encode_with_and_without_superres(cpi, size, dest, dest_size,
+                                         &largest_tile_id) != AOM_CODEC_OK) {
       return AOM_CODEC_ERROR;
     }
   } else {
     const aom_superres_mode orig_superres_mode = cpi->superres_mode;  // save
     cpi->superres_mode = cpi->oxcf.superres_cfg.superres_mode;
-    if (encode_with_recode_loop_and_filter(cpi, size, dest, NULL, NULL,
+    if (encode_with_recode_loop_and_filter(cpi, size, dest, dest_size, NULL,
+                                           NULL,
                                            &largest_tile_id) != AOM_CODEC_OK) {
       return AOM_CODEC_ERROR;
     }
@@ -3997,10 +4011,10 @@ static int encode_frame_to_data_rate(AV1_COMP *cpi, size_t *size,
   return AOM_CODEC_OK;
 }
 
-int av1_encode(AV1_COMP *const cpi, uint8_t *const dest,
+int av1_encode(AV1_COMP *const cpi, uint8_t *const dest, size_t dest_size,
                const EncodeFrameInput *const frame_input,
                const EncodeFrameParams *const frame_params,
-               EncodeFrameResults *const frame_results) {
+               size_t *const frame_size) {
   AV1_COMMON *const cm = &cpi->common;
   CurrentFrame *const current_frame = &cm->current_frame;
 
@@ -4049,7 +4063,7 @@ int av1_encode(AV1_COMP *const cpi, uint8_t *const dest,
 #endif
   } else if (cpi->oxcf.pass == AOM_RC_ONE_PASS ||
              cpi->oxcf.pass >= AOM_RC_SECOND_PASS) {
-    if (encode_frame_to_data_rate(cpi, &frame_results->size, dest) !=
+    if (encode_frame_to_data_rate(cpi, frame_size, dest, dest_size) !=
         AOM_CODEC_OK) {
       return AOM_CODEC_ERROR;
     }
@@ -4060,7 +4074,7 @@ int av1_encode(AV1_COMP *const cpi, uint8_t *const dest,
   return AOM_CODEC_OK;
 }
 
-#if CONFIG_DENOISE
+#if CONFIG_DENOISE && !CONFIG_REALTIME_ONLY
 static int apply_denoise_2d(AV1_COMP *cpi, const YV12_BUFFER_CONFIG *sd,
                             int block_size, float noise_level,
                             int64_t time_stamp, int64_t end_time) {
@@ -4149,11 +4163,11 @@ int av1_receive_raw_frame(AV1_COMP *cpi, aom_enc_frame_flags_t frame_flags,
       }
       cpi->oxcf.noise_level = (float)AOMMIN(5.0, cpi->oxcf.noise_level);
     }
-#endif
 
     if (apply_denoise_2d(cpi, sd, cpi->oxcf.noise_block_size,
                          cpi->oxcf.noise_level, time_stamp, end_time) < 0)
       res = -1;
+#endif  // !CONFIG_REALTIME_ONLY
   }
 #endif  //  CONFIG_DENOISE
 
@@ -4671,11 +4685,13 @@ void av1_post_encode_updates(AV1_COMP *const cpi,
     update_end_of_frame_stats(cpi);
   }
 
+#if CONFIG_THREE_PASS
   if (cpi->oxcf.pass == AOM_RC_THIRD_PASS && cpi->third_pass_ctx) {
     av1_pop_third_pass_info(cpi->third_pass_ctx);
   }
+#endif
 
-  if (ppi->rtc_ref.set_ref_frame_config) {
+  if (ppi->rtc_ref.set_ref_frame_config && !cpi->is_dropped_frame) {
     av1_svc_update_buffer_slot_refreshed(cpi);
     av1_svc_set_reference_was_previous(cpi);
   }
@@ -4696,9 +4712,11 @@ void av1_post_encode_updates(AV1_COMP *const cpi,
   }
 #endif  // CONFIG_INTERNAL_STATS
 
+#if CONFIG_THREE_PASS
   // Write frame info. Subtract 1 from frame index since if was incremented in
   // update_rc_counts.
   av1_write_second_pass_per_frame_info(cpi, cpi->gf_frame_index - 1);
+#endif
 }
 
 int av1_get_compressed_data(AV1_COMP *cpi, AV1_COMP_DATA *const cpi_data) {
@@ -4772,8 +4790,8 @@ int av1_get_compressed_data(AV1_COMP *cpi, AV1_COMP_DATA *const cpi_data) {
 #endif
 
   const int result = av1_encode_strategy(
-      cpi, &cpi_data->frame_size, cpi_data->cx_data, &cpi_data->lib_flags,
-      &cpi_data->ts_frame_start, &cpi_data->ts_frame_end,
+      cpi, &cpi_data->frame_size, cpi_data->cx_data, cpi_data->cx_data_sz,
+      &cpi_data->lib_flags, &cpi_data->ts_frame_start, &cpi_data->ts_frame_end,
       cpi_data->timestamp_ratio, &cpi_data->pop_lookahead, cpi_data->flush);
 
 #if CONFIG_COLLECT_COMPONENT_TIMING
@@ -4847,7 +4865,7 @@ int av1_get_compressed_data(AV1_COMP *cpi, AV1_COMP_DATA *const cpi_data) {
 
 // Populates cpi->scaled_ref_buf corresponding to frames in a parallel encode
 // set. Also sets the bitmask 'ref_buffers_used_map'.
-void av1_scale_references_fpmt(AV1_COMP *cpi, int *ref_buffers_used_map) {
+static void scale_references_fpmt(AV1_COMP *cpi, int *ref_buffers_used_map) {
   AV1_COMMON *cm = &cpi->common;
   MV_REFERENCE_FRAME ref_frame;
 
@@ -4881,8 +4899,8 @@ void av1_scale_references_fpmt(AV1_COMP *cpi, int *ref_buffers_used_map) {
 
 // Increments the ref_count of frame buffers referenced by cpi->scaled_ref_buf
 // corresponding to frames in a parallel encode set.
-void av1_increment_scaled_ref_counts_fpmt(BufferPool *buffer_pool,
-                                          int ref_buffers_used_map) {
+static void increment_scaled_ref_counts_fpmt(BufferPool *buffer_pool,
+                                             int ref_buffers_used_map) {
   for (int i = 0; i < buffer_pool->num_frame_bufs; ++i) {
     if (ref_buffers_used_map & (1 << i)) {
       ++buffer_pool->frame_bufs[i].ref_count;
@@ -4958,7 +4976,10 @@ AV1_COMP *av1_get_parallel_frame_enc_data(AV1_PRIMARY *const ppi,
   {
     AV1_COMP_DATA *data = &ppi->parallel_frames_data[cpi_idx - 1];
     assert(data->frame_size > 0);
-    assert(first_cpi_data->cx_data_sz > data->frame_size);
+    if (data->frame_size > first_cpi_data->cx_data_sz) {
+      aom_internal_error(&ppi->error, AOM_CODEC_ERROR,
+                         "first_cpi_data->cx_data buffer full");
+    }
 
     first_cpi_data->lib_flags = data->lib_flags;
     first_cpi_data->ts_frame_start = data->ts_frame_start;
@@ -5029,7 +5050,7 @@ int av1_init_parallel_frame_context(const AV1_COMP_DATA *const first_cpi_data,
   av1_get_ref_frames(first_ref_frame_map_pairs, cur_frame_disp, first_cpi,
                      gf_index_start, 1, first_cpi->common.remapped_ref_idx);
 
-  av1_scale_references_fpmt(first_cpi, ref_buffers_used_map);
+  scale_references_fpmt(first_cpi, ref_buffers_used_map);
   parallel_frame_count++;
 
   // Iterate through the GF_GROUP to find the remaining frame_parallel_level 2
@@ -5118,7 +5139,7 @@ int av1_init_parallel_frame_context(const AV1_COMP_DATA *const first_cpi_data,
 
       av1_get_ref_frames(first_ref_frame_map_pairs, cur_frame_disp, cur_cpi, i,
                          1, cur_cpi->common.remapped_ref_idx);
-      av1_scale_references_fpmt(cur_cpi, ref_buffers_used_map);
+      scale_references_fpmt(cur_cpi, ref_buffers_used_map);
       parallel_frame_count++;
     }
 
@@ -5134,8 +5155,8 @@ int av1_init_parallel_frame_context(const AV1_COMP_DATA *const first_cpi_data,
     }
   }
 
-  av1_increment_scaled_ref_counts_fpmt(first_cpi->common.buffer_pool,
-                                       *ref_buffers_used_map);
+  increment_scaled_ref_counts_fpmt(first_cpi->common.buffer_pool,
+                                   *ref_buffers_used_map);
 
   // Return the number of frames in the parallel encode set.
   return parallel_frame_count;
@@ -5210,56 +5231,61 @@ int av1_get_quantizer(AV1_COMP *cpi) {
   return cpi->common.quant_params.base_qindex;
 }
 
-int av1_convert_sect5obus_to_annexb(uint8_t *buffer, size_t *frame_size) {
+int av1_convert_sect5obus_to_annexb(uint8_t *buffer, size_t buffer_size,
+                                    size_t *frame_size) {
+  assert(*frame_size <= buffer_size);
   size_t output_size = 0;
-  size_t total_bytes_read = 0;
   size_t remaining_size = *frame_size;
   uint8_t *buff_ptr = buffer;
 
   // go through each OBUs
-  while (total_bytes_read < *frame_size) {
+  while (remaining_size > 0) {
     uint8_t saved_obu_header[2];
     uint64_t obu_payload_size;
     size_t length_of_payload_size;
     size_t length_of_obu_size;
-    uint32_t obu_header_size = (buff_ptr[0] >> 2) & 0x1 ? 2 : 1;
+    const uint32_t obu_header_size = (buff_ptr[0] >> 2) & 0x1 ? 2 : 1;
     size_t obu_bytes_read = obu_header_size;  // bytes read for current obu
 
     // save the obu header (1 or 2 bytes)
-    memmove(saved_obu_header, buff_ptr, obu_header_size);
+    memcpy(saved_obu_header, buff_ptr, obu_header_size);
     // clear the obu_has_size_field
-    saved_obu_header[0] = saved_obu_header[0] & (~0x2);
+    saved_obu_header[0] &= ~0x2;
 
     // get the payload_size and length of payload_size
-    if (aom_uleb_decode(buff_ptr + obu_header_size, remaining_size,
-                        &obu_payload_size, &length_of_payload_size) != 0) {
+    if (aom_uleb_decode(buff_ptr + obu_header_size,
+                        remaining_size - obu_header_size, &obu_payload_size,
+                        &length_of_payload_size) != 0) {
       return AOM_CODEC_ERROR;
     }
     obu_bytes_read += length_of_payload_size;
 
     // calculate the length of size of the obu header plus payload
-    length_of_obu_size =
-        aom_uleb_size_in_bytes((uint64_t)(obu_header_size + obu_payload_size));
+    const uint64_t obu_size = obu_header_size + obu_payload_size;
+    length_of_obu_size = aom_uleb_size_in_bytes(obu_size);
 
+    if (length_of_obu_size + obu_header_size >
+        buffer_size - output_size - (remaining_size - obu_bytes_read)) {
+      return AOM_CODEC_ERROR;
+    }
     // move the rest of data to new location
     memmove(buff_ptr + length_of_obu_size + obu_header_size,
             buff_ptr + obu_bytes_read, remaining_size - obu_bytes_read);
     obu_bytes_read += (size_t)obu_payload_size;
 
     // write the new obu size
-    const uint64_t obu_size = obu_header_size + obu_payload_size;
     size_t coded_obu_size;
-    if (aom_uleb_encode(obu_size, sizeof(obu_size), buff_ptr,
-                        &coded_obu_size) != 0) {
+    if (aom_uleb_encode(obu_size, length_of_obu_size, buff_ptr,
+                        &coded_obu_size) != 0 ||
+        coded_obu_size != length_of_obu_size) {
       return AOM_CODEC_ERROR;
     }
 
     // write the saved (modified) obu_header following obu size
-    memmove(buff_ptr + length_of_obu_size, saved_obu_header, obu_header_size);
+    memcpy(buff_ptr + length_of_obu_size, saved_obu_header, obu_header_size);
 
-    total_bytes_read += obu_bytes_read;
     remaining_size -= obu_bytes_read;
-    buff_ptr += length_of_obu_size + obu_size;
+    buff_ptr += length_of_obu_size + (size_t)obu_size;
     output_size += length_of_obu_size + (size_t)obu_size;
   }
 
@@ -5386,8 +5412,8 @@ aom_fixed_buf_t *av1_get_global_headers(AV1_PRIMARY *ppi) {
   if (!ppi) return NULL;
 
   uint8_t header_buf[512] = { 0 };
-  const uint32_t sequence_header_size =
-      av1_write_sequence_header_obu(&ppi->seq_params, &header_buf[0]);
+  const uint32_t sequence_header_size = av1_write_sequence_header_obu(
+      &ppi->seq_params, &header_buf[0], sizeof(header_buf));
   assert(sequence_header_size <= sizeof(header_buf));
   if (sequence_header_size == 0) return NULL;
 
diff --git a/av1/encoder/encoder.h b/av1/encoder/encoder.h
index a50188c8f..f1115592d 100644
--- a/av1/encoder/encoder.h
+++ b/av1/encoder/encoder.h
@@ -51,7 +51,9 @@
 #include "av1/encoder/speed_features.h"
 #include "av1/encoder/svc_layercontext.h"
 #include "av1/encoder/temporal_filter.h"
+#if CONFIG_THREE_PASS
 #include "av1/encoder/thirdpass.h"
+#endif
 #include "av1/encoder/tokenize.h"
 #include "av1/encoder/tpl_model.h"
 #include "av1/encoder/av1_noise_estimate.h"
@@ -2564,11 +2566,6 @@ typedef struct AV1_COMP_DATA {
    * Decide to pop the source for this frame from input buffer queue.
    */
   int pop_lookahead;
-
-  /*!
-   * Display order hint of frame whose packed data is in cx_data buffer.
-   */
-  int frame_display_order_hint;
 } AV1_COMP_DATA;
 
 /*!
@@ -3593,10 +3590,12 @@ typedef struct AV1_COMP {
    */
   TWO_PASS_FRAME twopass_frame;
 
+#if CONFIG_THREE_PASS
   /*!
    * Context needed for third pass encoding.
    */
   THIRD_PASS_DEC_CTX *third_pass_ctx;
+#endif
 
   /*!
    * File pointer to second pass log
@@ -3735,12 +3734,6 @@ typedef struct EncodeFrameParams {
 
 /*!\cond */
 
-// EncodeFrameResults contains information about the result of encoding a
-// single frame
-typedef struct {
-  size_t size;  // Size of resulting bitstream
-} EncodeFrameResults;
-
 void av1_initialize_enc(unsigned int usage, enum aom_rc_mode end_usage);
 
 struct AV1_COMP *av1_create_compressor(AV1_PRIMARY *ppi,
@@ -3773,17 +3766,9 @@ void av1_change_config(AV1_COMP *cpi, const AV1EncoderConfig *oxcf,
 aom_codec_err_t av1_check_initial_width(AV1_COMP *cpi, int use_highbitdepth,
                                         int subsampling_x, int subsampling_y);
 
-void av1_init_seq_coding_tools(AV1_PRIMARY *const ppi,
-                               const AV1EncoderConfig *oxcf, int use_svc);
-
 void av1_post_encode_updates(AV1_COMP *const cpi,
                              const AV1_COMP_DATA *const cpi_data);
 
-void av1_scale_references_fpmt(AV1_COMP *cpi, int *ref_buffers_used_map);
-
-void av1_increment_scaled_ref_counts_fpmt(BufferPool *buffer_pool,
-                                          int ref_buffers_used_map);
-
 void av1_release_scaled_references_fpmt(AV1_COMP *cpi);
 
 void av1_decrement_ref_counts_fpmt(BufferPool *buffer_pool,
@@ -3849,10 +3834,10 @@ int av1_get_compressed_data(AV1_COMP *cpi, AV1_COMP_DATA *const cpi_data);
  * \callgraph
  * \callergraph
  */
-int av1_encode(AV1_COMP *const cpi, uint8_t *const dest,
+int av1_encode(AV1_COMP *const cpi, uint8_t *const dest, size_t dest_size,
                const EncodeFrameInput *const frame_input,
                const EncodeFrameParams *const frame_params,
-               EncodeFrameResults *const frame_results);
+               size_t *const frame_size);
 
 /*!\cond */
 int av1_get_preview_raw_frame(AV1_COMP *cpi, YV12_BUFFER_CONFIG *dest);
@@ -3884,7 +3869,10 @@ int av1_set_internal_size(AV1EncoderConfig *const oxcf,
 
 int av1_get_quantizer(struct AV1_COMP *cpi);
 
-int av1_convert_sect5obus_to_annexb(uint8_t *buffer, size_t *input_size);
+// This function assumes that the input buffer contains valid OBUs. It should
+// not be called on untrusted input.
+int av1_convert_sect5obus_to_annexb(uint8_t *buffer, size_t buffer_size,
+                                    size_t *input_size);
 
 void av1_alloc_mb_wiener_var_pred_buf(AV1_COMMON *cm, ThreadData *td);
 
diff --git a/av1/encoder/encoder_alloc.h b/av1/encoder/encoder_alloc.h
index eb9e84201..14e0af42b 100644
--- a/av1/encoder/encoder_alloc.h
+++ b/av1/encoder/encoder_alloc.h
@@ -338,17 +338,19 @@ static inline void dealloc_compressor_data(AV1_COMP *cpi) {
     aom_free(cpi->td.mb.tmp_pred_bufs[j]);
   }
 
-#if CONFIG_DENOISE
+#if CONFIG_DENOISE && !CONFIG_REALTIME_ONLY
   if (cpi->denoise_and_model) {
     aom_denoise_and_model_free(cpi->denoise_and_model);
     cpi->denoise_and_model = NULL;
   }
 #endif
+#if !CONFIG_REALTIME_ONLY
   if (cpi->film_grain_table) {
     aom_film_grain_table_free(cpi->film_grain_table);
     aom_free(cpi->film_grain_table);
     cpi->film_grain_table = NULL;
   }
+#endif
 
   if (cpi->ppi->use_svc) av1_free_svc_cyclic_refresh(cpi);
   aom_free(cpi->svc.layer_context);
diff --git a/av1/encoder/encoder_utils.c b/av1/encoder/encoder_utils.c
index 64bafe126..32eb2f6ce 100644
--- a/av1/encoder/encoder_utils.c
+++ b/av1/encoder/encoder_utils.c
@@ -616,6 +616,7 @@ void av1_set_size_dependent_vars(AV1_COMP *cpi, int *q, int *bottom_index,
     configure_static_seg_features(cpi);
 }
 
+#if !CONFIG_REALTIME_ONLY
 static void reset_film_grain_chroma_params(aom_film_grain_t *pars) {
   pars->num_cr_points = 0;
   pars->cr_mult = 0;
@@ -686,6 +687,7 @@ void av1_update_film_grain_parameters(struct AV1_COMP *cpi,
     memset(&cm->film_grain_params, 0, sizeof(cm->film_grain_params));
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY
 
 void av1_scale_references(AV1_COMP *cpi, const InterpFilter filter,
                           const int phase, const int use_optimized_scaler) {
diff --git a/av1/encoder/encoder_utils.h b/av1/encoder/encoder_utils.h
index a4131cb7c..92ce7447b 100644
--- a/av1/encoder/encoder_utils.h
+++ b/av1/encoder/encoder_utils.h
@@ -1074,10 +1074,12 @@ static inline void refresh_reference_frames(AV1_COMP *cpi) {
   }
 }
 
+#if !CONFIG_REALTIME_ONLY
 void av1_update_film_grain_parameters_seq(struct AV1_PRIMARY *ppi,
                                           const AV1EncoderConfig *oxcf);
 void av1_update_film_grain_parameters(struct AV1_COMP *cpi,
                                       const AV1EncoderConfig *oxcf);
+#endif
 
 void av1_scale_references(AV1_COMP *cpi, const InterpFilter filter,
                           const int phase, const int use_optimized_scaler);
diff --git a/av1/encoder/ethread.c b/av1/encoder/ethread.c
index 82b1753f0..fcffd7af7 100644
--- a/av1/encoder/ethread.c
+++ b/av1/encoder/ethread.c
@@ -711,8 +711,9 @@ static int enc_row_mt_worker_hook(void *arg1, void *unused) {
 
     av1_init_above_context(&cm->above_contexts, av1_num_planes(cm), tile_row,
                            &td->mb.e_mbd);
-
+#if !CONFIG_REALTIME_ONLY
     cfl_init(&td->mb.e_mbd.cfl, cm->seq_params);
+#endif
     if (td->mb.txfm_search_info.mb_rd_record != NULL) {
       av1_crc32c_calculator_init(
           &td->mb.txfm_search_info.mb_rd_record->crc_calculator);
diff --git a/av1/encoder/global_motion.c b/av1/encoder/global_motion.c
index 4d216d051..18ea46fa2 100644
--- a/av1/encoder/global_motion.c
+++ b/av1/encoder/global_motion.c
@@ -335,12 +335,15 @@ int64_t av1_segmented_frame_error(int use_hbd, int bd, const uint8_t *ref,
                                p_height, segment_map, segment_map_stride);
 }
 
-int64_t av1_warp_error(WarpedMotionParams *wm, int use_hbd, int bd,
-                       const uint8_t *ref, int ref_width, int ref_height,
-                       int ref_stride, uint8_t *dst, int dst_stride, int p_col,
-                       int p_row, int p_width, int p_height, int subsampling_x,
-                       int subsampling_y, int64_t best_error,
-                       uint8_t *segment_map, int segment_map_stride) {
+// Returns the error between the result of applying motion 'wm' to the frame
+// described by 'ref' and the frame described by 'dst'.
+static int64_t get_warp_error(WarpedMotionParams *wm, int use_hbd, int bd,
+                              const uint8_t *ref, int ref_width, int ref_height,
+                              int ref_stride, uint8_t *dst, int dst_stride,
+                              int p_col, int p_row, int p_width, int p_height,
+                              int subsampling_x, int subsampling_y,
+                              int64_t best_error, uint8_t *segment_map,
+                              int segment_map_stride) {
   if (!av1_get_shear_params(wm)) return INT64_MAX;
 #if CONFIG_AV1_HIGHBITDEPTH
   if (use_hbd)
@@ -378,10 +381,10 @@ int64_t av1_refine_integerized_param(
 
   if (n_refinements == 0) {
     // Compute the maximum error value that will be accepted, so that
-    // av1_warp_error can terminate early if it proves the model will not
+    // get_warp_error can terminate early if it proves the model will not
     // be accepted.
     int64_t selection_threshold = (int64_t)lrint(ref_frame_error * erroradv_tr);
-    return av1_warp_error(wm, use_hbd, bd, ref, r_width, r_height, r_stride,
+    return get_warp_error(wm, use_hbd, bd, ref, r_width, r_height, r_stride,
                           dst + border * d_stride + border, d_stride, border,
                           border, d_width - 2 * border, d_height - 2 * border,
                           0, 0, selection_threshold, segment_map,
@@ -393,7 +396,7 @@ int64_t av1_refine_integerized_param(
   int64_t selection_threshold =
       (int64_t)lrint(ref_frame_error * erroradv_early_tr);
   best_error =
-      av1_warp_error(wm, use_hbd, bd, ref, r_width, r_height, r_stride,
+      get_warp_error(wm, use_hbd, bd, ref, r_width, r_height, r_stride,
                      dst + border * d_stride + border, d_stride, border, border,
                      d_width - 2 * border, d_height - 2 * border, 0, 0,
                      selection_threshold, segment_map, segment_map_stride);
@@ -415,7 +418,7 @@ int64_t av1_refine_integerized_param(
       *param = add_param_offset(p, curr_param, -step);
       force_wmtype(wm, wmtype);
       step_error =
-          av1_warp_error(wm, use_hbd, bd, ref, r_width, r_height, r_stride,
+          get_warp_error(wm, use_hbd, bd, ref, r_width, r_height, r_stride,
                          dst + border * d_stride + border, d_stride, border,
                          border, d_width - 2 * border, d_height - 2 * border, 0,
                          0, best_error, segment_map, segment_map_stride);
@@ -429,7 +432,7 @@ int64_t av1_refine_integerized_param(
       *param = add_param_offset(p, curr_param, step);
       force_wmtype(wm, wmtype);
       step_error =
-          av1_warp_error(wm, use_hbd, bd, ref, r_width, r_height, r_stride,
+          get_warp_error(wm, use_hbd, bd, ref, r_width, r_height, r_stride,
                          dst + border * d_stride + border, d_stride, border,
                          border, d_width - 2 * border, d_height - 2 * border, 0,
                          0, best_error, segment_map, segment_map_stride);
@@ -445,7 +448,7 @@ int64_t av1_refine_integerized_param(
         *param = add_param_offset(p, best_param, step * step_dir);
         force_wmtype(wm, wmtype);
         step_error =
-            av1_warp_error(wm, use_hbd, bd, ref, r_width, r_height, r_stride,
+            get_warp_error(wm, use_hbd, bd, ref, r_width, r_height, r_stride,
                            dst + border * d_stride + border, d_stride, border,
                            border, d_width - 2 * border, d_height - 2 * border,
                            0, 0, best_error, segment_map, segment_map_stride);
diff --git a/av1/encoder/global_motion.h b/av1/encoder/global_motion.h
index bb82d798d..4d8c84819 100644
--- a/av1/encoder/global_motion.h
+++ b/av1/encoder/global_motion.h
@@ -102,16 +102,7 @@ int64_t av1_segmented_frame_error(int use_hbd, int bd, const uint8_t *ref,
                                   int p_width, int p_height,
                                   uint8_t *segment_map, int segment_map_stride);
 
-// Returns the error between the result of applying motion 'wm' to the frame
-// described by 'ref' and the frame described by 'dst'.
-int64_t av1_warp_error(WarpedMotionParams *wm, int use_hbd, int bd,
-                       const uint8_t *ref, int ref_width, int ref_height,
-                       int ref_stride, uint8_t *dst, int dst_stride, int p_col,
-                       int p_row, int p_width, int p_height, int subsampling_x,
-                       int subsampling_y, int64_t best_error,
-                       uint8_t *segment_map, int segment_map_stride);
-
-// Returns the av1_warp_error between "dst" and the result of applying the
+// Returns the warp error between "dst" and the result of applying the
 // motion params that result from fine-tuning "wm" to "ref". Note that "wm" is
 // modified in place.
 int64_t av1_refine_integerized_param(
diff --git a/av1/encoder/grain_test_vectors.h b/av1/encoder/grain_test_vectors.h
index 0060d41e6..1c87b1016 100644
--- a/av1/encoder/grain_test_vectors.h
+++ b/av1/encoder/grain_test_vectors.h
@@ -16,6 +16,8 @@
  * not signaled in film grain metadata. The parameters are valid
  * for any bit depth.
  */
+
+#if !CONFIG_REALTIME_ONLY
 static aom_film_grain_t film_grain_test_vectors[16] = {
   /* Test 1 */
   {
@@ -778,4 +780,5 @@ static aom_film_grain_t film_grain_test_vectors[16] = {
       45231 /* random_seed */
   },
 };
+#endif  // !CONFIG_REALTIME_ONLY
 #endif  // AOM_AV1_ENCODER_GRAIN_TEST_VECTORS_H_
diff --git a/av1/encoder/hash_motion.c b/av1/encoder/hash_motion.c
index 78403a6e9..1f4131e83 100644
--- a/av1/encoder/hash_motion.c
+++ b/av1/encoder/hash_motion.c
@@ -102,7 +102,7 @@ void av1_hash_table_init(IntraBCHashInfo *intrabc_hash_info) {
   intrabc_hash_info->intrabc_hash_table.p_lookup_table = NULL;
 }
 
-void av1_hash_table_clear_all(hash_table *p_hash_table) {
+static void clear_all(hash_table *p_hash_table) {
   if (p_hash_table->p_lookup_table == NULL) {
     return;
   }
@@ -116,14 +116,14 @@ void av1_hash_table_clear_all(hash_table *p_hash_table) {
 }
 
 void av1_hash_table_destroy(hash_table *p_hash_table) {
-  av1_hash_table_clear_all(p_hash_table);
+  clear_all(p_hash_table);
   aom_free(p_hash_table->p_lookup_table);
   p_hash_table->p_lookup_table = NULL;
 }
 
 bool av1_hash_table_create(hash_table *p_hash_table) {
   if (p_hash_table->p_lookup_table != NULL) {
-    av1_hash_table_clear_all(p_hash_table);
+    clear_all(p_hash_table);
     return true;
   }
   p_hash_table->p_lookup_table =
@@ -170,24 +170,6 @@ Iterator av1_hash_get_first_iterator(hash_table *p_hash_table,
   return aom_vector_begin(p_hash_table->p_lookup_table[hash_value]);
 }
 
-int32_t av1_has_exact_match(hash_table *p_hash_table, uint32_t hash_value1,
-                            uint32_t hash_value2) {
-  if (p_hash_table->p_lookup_table[hash_value1] == NULL) {
-    return 0;
-  }
-  Iterator iterator =
-      aom_vector_begin(p_hash_table->p_lookup_table[hash_value1]);
-  Iterator last = aom_vector_end(p_hash_table->p_lookup_table[hash_value1]);
-  for (; !aom_iterator_equals(&iterator, &last);
-       aom_iterator_increment(&iterator)) {
-    if ((*(block_hash *)aom_iterator_get(&iterator)).hash_value2 ==
-        hash_value2) {
-      return 1;
-    }
-  }
-  return 0;
-}
-
 void av1_generate_block_2x2_hash_value(IntraBCHashInfo *intrabc_hash_info,
                                        const YV12_BUFFER_CONFIG *picture,
                                        uint32_t *pic_block_hash[2],
diff --git a/av1/encoder/hash_motion.h b/av1/encoder/hash_motion.h
index f78b0f679..f9686dc28 100644
--- a/av1/encoder/hash_motion.h
+++ b/av1/encoder/hash_motion.h
@@ -56,15 +56,12 @@ typedef struct intrabc_hash_info {
 } IntraBCHashInfo;
 
 void av1_hash_table_init(IntraBCHashInfo *intra_bc_hash_info);
-void av1_hash_table_clear_all(hash_table *p_hash_table);
 void av1_hash_table_destroy(hash_table *p_hash_table);
 bool av1_hash_table_create(hash_table *p_hash_table);
 int32_t av1_hash_table_count(const hash_table *p_hash_table,
                              uint32_t hash_value);
 Iterator av1_hash_get_first_iterator(hash_table *p_hash_table,
                                      uint32_t hash_value);
-int32_t av1_has_exact_match(hash_table *p_hash_table, uint32_t hash_value1,
-                            uint32_t hash_value2);
 void av1_generate_block_2x2_hash_value(IntraBCHashInfo *intra_bc_hash_info,
                                        const YV12_BUFFER_CONFIG *picture,
                                        uint32_t *pic_block_hash[2],
diff --git a/av1/encoder/mcomp.c b/av1/encoder/mcomp.c
index 1a5e5c531..0709e9baa 100644
--- a/av1/encoder/mcomp.c
+++ b/av1/encoder/mcomp.c
@@ -350,8 +350,8 @@ static inline int mvsad_err_cost_(const FULLPEL_MV *mv,
 
 // Search site initialization for DIAMOND / CLAMPED_DIAMOND search methods.
 // level = 0: DIAMOND, level = 1: CLAMPED_DIAMOND.
-void av1_init_dsmotion_compensation(search_site_config *cfg, int stride,
-                                    int level) {
+static void init_dsmotion_compensation(search_site_config *cfg, int stride,
+                                       int level) {
   int num_search_steps = 0;
   int stage_index = MAX_MVSEARCH_STEPS - 1;
 
@@ -433,8 +433,8 @@ void av1_init_motion_fpf(search_site_config *cfg, int stride) {
 
 // Search site initialization for NSTEP / NSTEP_8PT search methods.
 // level = 0: NSTEP, level = 1: NSTEP_8PT.
-void av1_init_motion_compensation_nstep(search_site_config *cfg, int stride,
-                                        int level) {
+static void init_motion_compensation_nstep(search_site_config *cfg, int stride,
+                                           int level) {
   int num_search_steps = 0;
   int stage_index = 0;
   cfg->stride = stride;
@@ -479,8 +479,8 @@ void av1_init_motion_compensation_nstep(search_site_config *cfg, int stride,
 
 // Search site initialization for BIGDIA / FAST_BIGDIA / FAST_DIAMOND
 // search methods.
-void av1_init_motion_compensation_bigdia(search_site_config *cfg, int stride,
-                                         int level) {
+static void init_motion_compensation_bigdia(search_site_config *cfg, int stride,
+                                            int level) {
   (void)level;
   cfg->stride = stride;
   // First scale has 4-closest points, the rest have 8 points in diamond
@@ -534,8 +534,8 @@ void av1_init_motion_compensation_bigdia(search_site_config *cfg, int stride,
 }
 
 // Search site initialization for SQUARE search method.
-void av1_init_motion_compensation_square(search_site_config *cfg, int stride,
-                                         int level) {
+static void init_motion_compensation_square(search_site_config *cfg, int stride,
+                                            int level) {
   (void)level;
   cfg->stride = stride;
   // All scales have 8 closest points in square shape.
@@ -588,8 +588,8 @@ void av1_init_motion_compensation_square(search_site_config *cfg, int stride,
 }
 
 // Search site initialization for HEX / FAST_HEX search methods.
-void av1_init_motion_compensation_hex(search_site_config *cfg, int stride,
-                                      int level) {
+static void init_motion_compensation_hex(search_site_config *cfg, int stride,
+                                         int level) {
   (void)level;
   cfg->stride = stride;
   // First scale has 8-closest points, the rest have 6 points in hex shape
@@ -638,10 +638,10 @@ void av1_init_motion_compensation_hex(search_site_config *cfg, int stride,
 
 const av1_init_search_site_config
     av1_init_motion_compensation[NUM_DISTINCT_SEARCH_METHODS] = {
-      av1_init_dsmotion_compensation,     av1_init_motion_compensation_nstep,
-      av1_init_motion_compensation_nstep, av1_init_dsmotion_compensation,
-      av1_init_motion_compensation_hex,   av1_init_motion_compensation_bigdia,
-      av1_init_motion_compensation_square
+      init_dsmotion_compensation,     init_motion_compensation_nstep,
+      init_motion_compensation_nstep, init_dsmotion_compensation,
+      init_motion_compensation_hex,   init_motion_compensation_bigdia,
+      init_motion_compensation_square
     };
 
 // Checks whether the mv is within range of the mv_limits
diff --git a/av1/encoder/mcomp.h b/av1/encoder/mcomp.h
index f55d8690f..05cbf870d 100644
--- a/av1/encoder/mcomp.h
+++ b/av1/encoder/mcomp.h
@@ -151,24 +151,8 @@ void av1_make_default_fullpel_ms_params(
 void av1_set_ms_to_intra_mode(FULLPEL_MOTION_SEARCH_PARAMS *ms_params,
                               const IntraBCMVCosts *dv_costs);
 
-// Sets up configs for fullpixel DIAMOND / CLAMPED_DIAMOND search method.
-void av1_init_dsmotion_compensation(search_site_config *cfg, int stride,
-                                    int level);
 // Sets up configs for firstpass motion search.
 void av1_init_motion_fpf(search_site_config *cfg, int stride);
-// Sets up configs for NSTEP / NSTEP_8PT motion search method.
-void av1_init_motion_compensation_nstep(search_site_config *cfg, int stride,
-                                        int level);
-// Sets up configs for BIGDIA / FAST_DIAMOND / FAST_BIGDIA
-// motion search method.
-void av1_init_motion_compensation_bigdia(search_site_config *cfg, int stride,
-                                         int level);
-// Sets up configs for HEX or FAST_HEX motion search method.
-void av1_init_motion_compensation_hex(search_site_config *cfg, int stride,
-                                      int level);
-// Sets up configs for SQUARE motion search method.
-void av1_init_motion_compensation_square(search_site_config *cfg, int stride,
-                                         int level);
 
 /*! Function pointer to search site config initialization of different search
  * method functions. */
diff --git a/av1/encoder/nonrd_pickmode.c b/av1/encoder/nonrd_pickmode.c
index db5c56c75..20344255e 100644
--- a/av1/encoder/nonrd_pickmode.c
+++ b/av1/encoder/nonrd_pickmode.c
@@ -1990,16 +1990,28 @@ static void set_color_sensitivity(AV1_COMP *cpi, MACROBLOCK *x,
     }
     return;
   }
+  // Divide factor for comparing uv_sad to y_sad.
   int shift = 3;
+  // Threshold for the block spatial source variance.
   unsigned int source_var_thr = 50;
-  int uv_sad_thr = 100;
+  // Thresholds for normalized uv_sad, the first one is used for
+  // low source_varaince.
+  int norm_uv_sad_thresh = 100;
+  int norm_uv_sad_thresh2 = 40;
   if (source_sad_nonrd >= kMedSad && x->source_variance > 0 && high_res)
     shift = 4;
   if (cpi->oxcf.tune_cfg.content == AOM_CONTENT_SCREEN) {
     if (cpi->rc.high_source_sad) shift = 6;
     if (source_sad_nonrd > kMedSad) {
       source_var_thr = 1200;
-      uv_sad_thr = 10;
+      norm_uv_sad_thresh = 10;
+    }
+    if (cpi->rc.percent_blocks_with_motion > 90 &&
+        cpi->rc.frame_source_sad > 10000 && source_sad_nonrd > kLowSad) {
+      // Aggressive setting for color_sensitiivty for this content.
+      shift = 10;
+      norm_uv_sad_thresh = 0;
+      norm_uv_sad_thresh2 = 0;
     }
   }
   NOISE_LEVEL noise_level = kLow;
@@ -2037,8 +2049,8 @@ static void set_color_sensitivity(AV1_COMP *cpi, MACROBLOCK *x,
       const int norm_uv_sad =
           uv_sad >> (b_width_log2_lookup[bs] + b_height_log2_lookup[bs]);
       x->color_sensitivity[COLOR_SENS_IDX(plane)] =
-          uv_sad > (y_sad >> shift) && norm_uv_sad > 40;
-      if (source_variance < source_var_thr && norm_uv_sad > uv_sad_thr)
+          uv_sad > (y_sad >> shift) && norm_uv_sad > norm_uv_sad_thresh2;
+      if (source_variance < source_var_thr && norm_uv_sad > norm_uv_sad_thresh)
         x->color_sensitivity[COLOR_SENS_IDX(plane)] = 1;
     }
   }
diff --git a/av1/encoder/palette.c b/av1/encoder/palette.c
index a3e3fbf86..c1ceb8c6d 100644
--- a/av1/encoder/palette.c
+++ b/av1/encoder/palette.c
@@ -35,7 +35,19 @@ static int int16_comparer(const void *a, const void *b) {
   return (*(int16_t *)a - *(int16_t *)b);
 }
 
-int av1_remove_duplicates(int16_t *centroids, int num_centroids) {
+/*!\brief Removes duplicated centroid indices.
+ *
+ * \ingroup palette_mode_search
+ * \param[in]    centroids          A list of centroids index.
+ * \param[in]    num_centroids      Number of centroids.
+ *
+ * \return Returns the number of unique centroids and saves the unique centroids
+ * in beginning of the centroids array.
+ *
+ * \attention The centroids should be rounded to integers before calling this
+ * method.
+ */
+static int remove_duplicates(int16_t *centroids, int num_centroids) {
   int num_unique;  // number of unique centroids
   int i;
   qsort(centroids, num_centroids, sizeof(*centroids), int16_comparer);
@@ -225,7 +237,7 @@ static inline void palette_rd_y(
   if (do_header_rd_based_breakout != NULL) *do_header_rd_based_breakout = false;
   optimize_palette_colors(color_cache, n_cache, n, 1, centroids,
                           cpi->common.seq_params->bit_depth);
-  const int num_unique_colors = av1_remove_duplicates(centroids, n);
+  const int num_unique_colors = remove_duplicates(centroids, n);
   if (num_unique_colors < PALETTE_MIN_SIZE) {
     // Too few unique colors to create a palette. And DC_PRED will work
     // well for that case anyway. So skip.
diff --git a/av1/encoder/palette.h b/av1/encoder/palette.h
index a0c428b5a..ce109497b 100644
--- a/av1/encoder/palette.h
+++ b/av1/encoder/palette.h
@@ -99,20 +99,6 @@ static inline void av1_k_means(const int16_t *data, int16_t *centroids,
   }
 }
 
-/*!\brief Removes duplicated centroid indices.
- *
- * \ingroup palette_mode_search
- * \param[in]    centroids          A list of centroids index.
- * \param[in]    num_centroids      Number of centroids.
- *
- * \return Returns the number of unique centroids and saves the unique centroids
- * in beginning of the centroids array.
- *
- * \attention The centroids should be rounded to integers before calling this
- * method.
- */
-int av1_remove_duplicates(int16_t *centroids, int num_centroids);
-
 /*!\brief Checks what colors are in the color cache.
  *
  * \ingroup palette_mode_search
diff --git a/av1/encoder/partition_search.c b/av1/encoder/partition_search.c
index 49294cbfd..d81efec2f 100644
--- a/av1/encoder/partition_search.c
+++ b/av1/encoder/partition_search.c
@@ -564,9 +564,11 @@ static void encode_superblock(const AV1_COMP *const cpi, TileDataEnc *tile_data,
                   (mbmi->skip_txfm || seg_skip) && is_inter_block(mbmi), xd);
   }
 
+#if !CONFIG_REALTIME_ONLY
   if (is_inter_block(mbmi) && !xd->is_chroma_ref && is_cfl_allowed(xd)) {
     cfl_store_block(xd, mbmi->bsize, mbmi->tx_size);
   }
+#endif
   if (!dry_run) {
     if (cpi->oxcf.pass == AOM_RC_ONE_PASS && cpi->svc.temporal_layer_id == 0 &&
         cpi->sf.rt_sf.use_temporal_noise_estimate &&
diff --git a/av1/encoder/partition_strategy.c b/av1/encoder/partition_strategy.c
index d7561135a..c1d3e3ea7 100644
--- a/av1/encoder/partition_strategy.c
+++ b/av1/encoder/partition_strategy.c
@@ -11,8 +11,12 @@
 
 #include <float.h>
 
+#include "config/aom_config.h"
+
 #include "av1/encoder/encodeframe_utils.h"
+#if CONFIG_THREE_PASS
 #include "av1/encoder/thirdpass.h"
+#endif
 #include "config/aom_dsp_rtcd.h"
 
 #include "av1/common/enums.h"
@@ -135,10 +139,10 @@ static void write_features_to_file(const char *const path,
 //   -- add support for pruning rectangular partitions
 //   -- use reconstructed pixels instead of source pixels for padding
 //   -- use chroma pixels in addition to luma pixels
-void av1_intra_mode_cnn_partition(const AV1_COMMON *const cm, MACROBLOCK *x,
-                                  int quad_tree_idx,
-                                  int intra_cnn_based_part_prune_level,
-                                  PartitionSearchState *part_state) {
+static void intra_mode_cnn_partition(const AV1_COMMON *const cm, MACROBLOCK *x,
+                                     int quad_tree_idx,
+                                     int intra_cnn_based_part_prune_level,
+                                     PartitionSearchState *part_state) {
   assert(cm->seq_params->sb_size >= BLOCK_64X64 &&
          "Invalid sb_size for intra_cnn!");
   const PartitionBlkParams *blk_params = &part_state->part_blk_params;
@@ -364,9 +368,12 @@ static inline int get_simple_motion_search_prune_agg(int qindex,
   return sms_prune_agg_qindex_based[qband];
 }
 
-void av1_simple_motion_search_based_split(AV1_COMP *const cpi, MACROBLOCK *x,
-                                          SIMPLE_MOTION_DATA_TREE *sms_tree,
-                                          PartitionSearchState *part_state) {
+// Performs a simple_motion_search with a single reference frame and extract
+// the variance of residues. Then use the features to determine whether we want
+// to go straight to splitting without trying PARTITION_NONE
+static void simple_motion_search_based_split(AV1_COMP *const cpi, MACROBLOCK *x,
+                                             SIMPLE_MOTION_DATA_TREE *sms_tree,
+                                             PartitionSearchState *part_state) {
   const AV1_COMMON *const cm = &cpi->common;
   const PartitionBlkParams *blk_params = &part_state->part_blk_params;
   const int mi_row = blk_params->mi_row, mi_col = blk_params->mi_col;
@@ -636,9 +643,12 @@ static inline void simple_motion_search_prune_part_features(
   features[f_idx++] = (float)mi_size_high_log2[left_bsize];
 }
 
-void av1_simple_motion_search_prune_rect(AV1_COMP *const cpi, MACROBLOCK *x,
-                                         SIMPLE_MOTION_DATA_TREE *sms_tree,
-                                         PartitionSearchState *part_state) {
+// Performs a simple_motion_search with two reference frames and extract
+// the variance of residues. Then use the features to determine whether we want
+// to prune some partitions.
+static void simple_motion_search_prune_rect(AV1_COMP *const cpi, MACROBLOCK *x,
+                                            SIMPLE_MOTION_DATA_TREE *sms_tree,
+                                            PartitionSearchState *part_state) {
   const AV1_COMMON *const cm = &cpi->common;
   const PartitionBlkParams *blk_params = &part_state->part_blk_params;
   const int mi_row = blk_params->mi_row, mi_col = blk_params->mi_col;
@@ -1206,10 +1216,10 @@ void av1_ml_prune_rect_partition(AV1_COMP *const cpi, const MACROBLOCK *const x,
 
 // Use a ML model to predict if horz_a, horz_b, vert_a, and vert_b should be
 // considered.
-void av1_ml_prune_ab_partition(AV1_COMP *const cpi, int part_ctx, int var_ctx,
-                               int64_t best_rd,
-                               PartitionSearchState *part_state,
-                               int *ab_partitions_allowed) {
+static void ml_prune_ab_partition(AV1_COMP *const cpi, int part_ctx,
+                                  int var_ctx, int64_t best_rd,
+                                  PartitionSearchState *part_state,
+                                  int *ab_partitions_allowed) {
   const PartitionBlkParams blk_params = part_state->part_blk_params;
   const int mi_row = blk_params.mi_row;
   const int mi_col = blk_params.mi_col;
@@ -1552,6 +1562,7 @@ void av1_prune_partitions_before_search(AV1_COMP *const cpi,
   const PartitionBlkParams *blk_params = &part_state->part_blk_params;
   const BLOCK_SIZE bsize = blk_params->bsize;
 
+#if CONFIG_THREE_PASS
   if (cpi->third_pass_ctx) {
     int mi_row = blk_params->mi_row;
     int mi_col = blk_params->mi_col;
@@ -1621,6 +1632,7 @@ void av1_prune_partitions_before_search(AV1_COMP *const cpi,
       }
     }
   }
+#endif  // CONFIG_THREE_PASS
 
   // Prune rectangular partitions for larger blocks.
   if (bsize > cpi->sf.part_sf.rect_partition_eval_thresh) {
@@ -1679,9 +1691,9 @@ void av1_prune_partitions_before_search(AV1_COMP *const cpi,
       av1_is_whole_blk_in_frame(blk_params, mi_params);
 
   if (try_intra_cnn_based_part_prune) {
-    av1_intra_mode_cnn_partition(
-        &cpi->common, x, x->part_search_info.quad_tree_idx,
-        cpi->sf.part_sf.intra_cnn_based_part_prune_level, part_state);
+    intra_mode_cnn_partition(&cpi->common, x, x->part_search_info.quad_tree_idx,
+                             cpi->sf.part_sf.intra_cnn_based_part_prune_level,
+                             part_state);
   }
 
   // Use simple motion search to prune out split or non-split partitions. This
@@ -1694,7 +1706,7 @@ void av1_prune_partitions_before_search(AV1_COMP *const cpi,
       !frame_is_intra_only(cm) && !av1_superres_scaled(cm);
 
   if (try_split_only) {
-    av1_simple_motion_search_based_split(cpi, x, sms_tree, part_state);
+    simple_motion_search_based_split(cpi, x, sms_tree, part_state);
   }
 
   // Use simple motion search to prune out rectangular partition in some
@@ -1718,7 +1730,7 @@ void av1_prune_partitions_before_search(AV1_COMP *const cpi,
                              !av1_superres_scaled(cm);
 
   if (try_prune_rect) {
-    av1_simple_motion_search_prune_rect(cpi, x, sms_tree, part_state);
+    simple_motion_search_prune_rect(cpi, x, sms_tree, part_state);
   }
 }
 
@@ -1892,9 +1904,9 @@ void av1_prune_ab_partitions(AV1_COMP *cpi, const MACROBLOCK *x,
     // TODO(huisu@google.com): x->source_variance may not be the current
     // block's variance. The correct one to use is pb_source_variance. Need to
     // re-train the model to fix it.
-    av1_ml_prune_ab_partition(cpi, pc_tree->partitioning,
-                              get_unsigned_bits(x->source_variance),
-                              best_rdcost, part_state, ab_partitions_allowed);
+    ml_prune_ab_partition(cpi, pc_tree->partitioning,
+                          get_unsigned_bits(x->source_variance), best_rdcost,
+                          part_state, ab_partitions_allowed);
   }
 
   // Pruning: pruning AB partitions based on the number of horz/vert wins
diff --git a/av1/encoder/partition_strategy.h b/av1/encoder/partition_strategy.h
index 2597dedfa..b95e96790 100644
--- a/av1/encoder/partition_strategy.h
+++ b/av1/encoder/partition_strategy.h
@@ -17,25 +17,6 @@
 #include "av1/encoder/encodemb.h"
 #include "av1/encoder/encoder.h"
 
-void av1_intra_mode_cnn_partition(const AV1_COMMON *const cm, MACROBLOCK *x,
-                                  int label_idx,
-                                  int intra_cnn_based_part_prune_level,
-                                  PartitionSearchState *part_state);
-
-// Performs a simple_motion_search with a single reference frame and extract
-// the variance of residues. Then use the features to determine whether we want
-// to go straight to splitting without trying PARTITION_NONE
-void av1_simple_motion_search_based_split(AV1_COMP *const cpi, MACROBLOCK *x,
-                                          SIMPLE_MOTION_DATA_TREE *sms_tree,
-                                          PartitionSearchState *part_state);
-
-// Performs a simple_motion_search with two reference frames and extract
-// the variance of residues. Then use the features to determine whether we want
-// to prune some partitions.
-void av1_simple_motion_search_prune_rect(AV1_COMP *const cpi, MACROBLOCK *x,
-                                         SIMPLE_MOTION_DATA_TREE *sms_tree,
-                                         PartitionSearchState *part_state);
-
 #if !CONFIG_REALTIME_ONLY
 // Early terminates PARTITION_NONE using simple_motion_search features and the
 // rate, distortion, and rdcost of PARTITION_NONE. This is only called when:
@@ -80,13 +61,6 @@ void av1_ml_prune_rect_partition(AV1_COMP *const cpi, const MACROBLOCK *const x,
                                  const int64_t *split_rd,
                                  PartitionSearchState *part_state);
 
-// Use a ML model to predict if horz_a, horz_b, vert_a, and vert_b should be
-// considered.
-void av1_ml_prune_ab_partition(AV1_COMP *const cpi, int part_ctx, int var_ctx,
-                               int64_t best_rd,
-                               PartitionSearchState *part_state,
-                               int *ab_partitions_allowed);
-
 // Use a ML model to predict if horz4 and vert4 should be considered.
 void av1_ml_prune_4_partition(AV1_COMP *const cpi, MACROBLOCK *const x,
                               int part_ctx, int64_t best_rd,
diff --git a/av1/encoder/pass2_strategy.c b/av1/encoder/pass2_strategy.c
index beff033c6..194019bab 100644
--- a/av1/encoder/pass2_strategy.c
+++ b/av1/encoder/pass2_strategy.c
@@ -38,7 +38,9 @@
 #include "av1/encoder/ratectrl.h"
 #include "av1/encoder/rc_utils.h"
 #include "av1/encoder/temporal_filter.h"
+#if CONFIG_THREE_PASS
 #include "av1/encoder/thirdpass.h"
+#endif
 #include "av1/encoder/tpl_model.h"
 #include "av1/encoder/encode_strategy.h"
 
@@ -47,8 +49,10 @@
 #define GROUP_ADAPTIVE_MAXQ 1
 
 static void init_gf_stats(GF_GROUP_STATS *gf_stats);
+#if CONFIG_THREE_PASS
 static int define_gf_group_pass3(AV1_COMP *cpi, EncodeFrameParams *frame_params,
                                  int is_final_pass);
+#endif
 
 // Calculate an active area of the image that discounts formatting
 // bars and partially discounts other 0 energy areas.
@@ -171,6 +175,7 @@ static void twopass_update_bpm_factor(AV1_COMP *cpi, int rate_err_tol) {
   const double min_fac = 1.0 - adj_limit;
   const double max_fac = 1.0 + adj_limit;
 
+#if CONFIG_THREE_PASS
   if (cpi->third_pass_ctx && cpi->third_pass_ctx->frame_info_count > 0) {
     int64_t actual_bits = 0;
     int64_t target_bits = 0;
@@ -198,6 +203,7 @@ static void twopass_update_bpm_factor(AV1_COMP *cpi, int rate_err_tol) {
           AOMMAX(min_fac, AOMMIN(max_fac, twopass->bpm_factor));
     }
   }
+#endif  // CONFIG_THREE_PASS
 
   int err_estimate = p_rc->rate_error_estimate;
   int64_t total_actual_bits = p_rc->total_actual_bits;
@@ -254,16 +260,18 @@ static void twopass_update_bpm_factor(AV1_COMP *cpi, int rate_err_tol) {
   }
 }
 
-static const double q_div_term[(QINDEX_RANGE >> 5) + 1] = { 32.0, 40.0, 46.0,
-                                                            52.0, 56.0, 60.0,
-                                                            64.0, 68.0, 72.0 };
+static const double q_div_term[(QINDEX_RANGE >> 4) + 1] = {
+  18.0, 30.0, 38.0, 44.0, 47.0, 50.0, 52.0, 54.0, 56.0,
+  58.0, 60.0, 62.0, 64.0, 66.0, 68.0, 70.0, 72.0
+};
+
 #define EPMB_SCALER 1250000
 static double calc_correction_factor(double err_per_mb, int q) {
   double power_term = 0.90;
-  const int index = q >> 5;
+  const int index = q >> 4;
   const double divisor =
       q_div_term[index] +
-      (((q_div_term[index + 1] - q_div_term[index]) * (q % 32)) / 32.0);
+      (((q_div_term[index + 1] - q_div_term[index]) * (q % 16)) / 16.0);
   double error_term = EPMB_SCALER * pow(err_per_mb, power_term);
   return error_term / divisor;
 }
@@ -337,6 +345,10 @@ static int get_twopass_worst_quality(AV1_COMP *cpi, const double av_frame_err,
     const uint64_t target_norm_bits_per_mb =
         ((uint64_t)av_target_bandwidth << BPER_MB_NORMBITS) / active_mbs;
     int rate_err_tol = AOMMIN(rc_cfg->under_shoot_pct, rc_cfg->over_shoot_pct);
+    const double size_factor =
+        (active_mbs < 500) ? 0.925 : ((active_mbs > 3000) ? 1.05 : 1.0);
+    const double speed_factor =
+        AOMMIN(1.02, (0.975 + (0.005 * cpi->oxcf.speed)));
 
     // Update bpm correction factor based on previous GOP rate error.
     twopass_update_bpm_factor(cpi, rate_err_tol);
@@ -345,8 +357,9 @@ static int get_twopass_worst_quality(AV1_COMP *cpi, const double av_frame_err,
     // content at the given rate.
     int q = find_qindex_by_rate_with_correction(
         target_norm_bits_per_mb, cpi->common.seq_params->bit_depth,
-        av_err_per_mb, cpi->ppi->twopass.bpm_factor, rc->best_quality,
-        rc->worst_quality);
+        av_err_per_mb,
+        cpi->ppi->twopass.bpm_factor * speed_factor * size_factor,
+        rc->best_quality, rc->worst_quality);
 
     // Restriction on active max q for constrained quality mode.
     if (rc_cfg->mode == AOM_CQ) q = AOMMAX(q, rc_cfg->cq_level);
@@ -2481,6 +2494,7 @@ static void define_gf_group(AV1_COMP *cpi, EncodeFrameParams *frame_params,
     return;
   }
 
+#if CONFIG_THREE_PASS
   if (cpi->third_pass_ctx && oxcf->pass == AOM_RC_THIRD_PASS) {
     int ret = define_gf_group_pass3(cpi, frame_params, is_final_pass);
     if (ret == 0) return;
@@ -2488,6 +2502,7 @@ static void define_gf_group(AV1_COMP *cpi, EncodeFrameParams *frame_params,
     av1_free_thirdpass_ctx(cpi->third_pass_ctx);
     cpi->third_pass_ctx = NULL;
   }
+#endif  // CONFIG_THREE_PASS
 
   // correct frames_to_key when lookahead queue is emptying
   if (cpi->ppi->lap_enabled) {
@@ -2588,6 +2603,7 @@ static void define_gf_group(AV1_COMP *cpi, EncodeFrameParams *frame_params,
         gf_group->update_type[cpi->gf_frame_index] == INTNL_ARF_UPDATE);
 }
 
+#if CONFIG_THREE_PASS
 /*!\brief Define a GF group for the third apss.
  *
  * \ingroup gf_group_algo
@@ -2660,6 +2676,7 @@ static int define_gf_group_pass3(AV1_COMP *cpi, EncodeFrameParams *frame_params,
   frame_params->show_frame = cpi->third_pass_ctx->frame_info[0].is_show_frame;
   return 0;
 }
+#endif  // CONFIG_THREE_PASS
 
 // #define FIXED_ARF_BITS
 #ifdef FIXED_ARF_BITS
@@ -3851,6 +3868,7 @@ void av1_get_second_pass_params(AV1_COMP *cpi,
     }
 
     int need_gf_len = 1;
+#if CONFIG_THREE_PASS
     if (cpi->third_pass_ctx && oxcf->pass == AOM_RC_THIRD_PASS) {
       // set up bitstream to read
       if (!cpi->third_pass_ctx->input_file_name && oxcf->two_pass_output) {
@@ -3884,6 +3902,7 @@ void av1_get_second_pass_params(AV1_COMP *cpi,
       p_rc->gf_intervals[0] = cpi->third_pass_ctx->gop_info.gf_length;
       need_gf_len = 0;
     }
+#endif  // CONFIG_THREE_PASS
 
     if (need_gf_len) {
       // If we cannot obtain GF group length from second_pass_file
@@ -3943,9 +3962,11 @@ void av1_get_second_pass_params(AV1_COMP *cpi,
 
     define_gf_group(cpi, frame_params, 1);
 
+#if CONFIG_THREE_PASS
     // write gop info if needed for third pass. Per-frame info is written after
     // each frame is encoded.
     av1_write_second_pass_gop_info(cpi);
+#endif  // CONFIG_THREE_PASS
 
     av1_tf_info_filtering(&cpi->ppi->tf_info, cpi, gf_group);
 
diff --git a/av1/encoder/pickrst.c b/av1/encoder/pickrst.c
index 0c75d96ae..3b6d8e41a 100644
--- a/av1/encoder/pickrst.c
+++ b/av1/encoder/pickrst.c
@@ -2079,14 +2079,14 @@ void av1_pick_filter_restoration(const YV12_BUFFER_CONFIG *src, AV1_COMP *cpi) {
   // and height aligned to multiple of 16 is considered for intrinsic purpose.
   rsc.dgd_avg = NULL;
   rsc.src_avg = NULL;
-#if HAVE_AVX2 || HAVE_SVE
+#if HAVE_AVX2 || HAVE_NEON || HAVE_SVE
   // The buffers allocated below are used during Wiener filter processing.
   // Hence, allocate the same when Wiener filter is enabled. Make sure to
   // allocate these buffers only for the SIMD extensions that make use of them
-  // (i.e. AVX2 for low bitdepth and SVE for low and high bitdepth).
+  // (i.e. AVX2 for low bitdepth and NEON and SVE for low and high bitdepth).
 #if HAVE_AVX2
   bool allocate_buffers = !cpi->sf.lpf_sf.disable_wiener_filter && !highbd;
-#elif HAVE_SVE
+#elif HAVE_NEON || HAVE_SVE
   bool allocate_buffers = !cpi->sf.lpf_sf.disable_wiener_filter;
 #endif
   if (allocate_buffers) {
@@ -2225,10 +2225,10 @@ void av1_pick_filter_restoration(const YV12_BUFFER_CONFIG *src, AV1_COMP *cpi) {
                               best_luma_unit_size);
   }
 
-#if HAVE_AVX2 || HAVE_SVE
+#if HAVE_AVX2 || HAVE_NEON || HAVE_SVE
 #if HAVE_AVX2
   bool free_buffers = !cpi->sf.lpf_sf.disable_wiener_filter && !highbd;
-#elif HAVE_SVE
+#elif HAVE_NEON || HAVE_SVE
   bool free_buffers = !cpi->sf.lpf_sf.disable_wiener_filter;
 #endif
   if (free_buffers) {
diff --git a/av1/encoder/ratectrl.c b/av1/encoder/ratectrl.c
index 0c6a008d2..6992a1903 100644
--- a/av1/encoder/ratectrl.c
+++ b/av1/encoder/ratectrl.c
@@ -188,24 +188,24 @@ static int adjust_rtc_keyframe(const RATE_CONTROL *rc, int enumerator) {
   if (rc->last_encoded_size_keyframe == 0 ||
       rc->frames_since_scene_change < rc->frames_since_key) {
     // Very first frame, or if scene change happened after last keyframe.
-    if (rc->spatial_variance_keyframe > 1000 ||
-        (rc->spatial_variance_keyframe > 500 &&
+    if (rc->frame_spatial_variance > 1000 ||
+        (rc->frame_spatial_variance > 500 &&
          rc->perc_flat_blocks_keyframe == 0))
       return enumerator << 3;
-    else if (rc->spatial_variance_keyframe > 500 &&
+    else if (rc->frame_spatial_variance > 500 &&
              rc->perc_flat_blocks_keyframe < 10)
       return enumerator << 2;
-    else if (rc->spatial_variance_keyframe > 400)
+    else if (rc->frame_spatial_variance > 400)
       return enumerator << 1;
   } else if (rc->frames_since_scene_change >= rc->frames_since_key) {
     // There was no scene change before previous encoded keyframe, so
     // use the last_encoded/target_size_keyframe.
     if (rc->last_encoded_size_keyframe > 4 * rc->last_target_size_keyframe &&
-        rc->spatial_variance_keyframe > 500)
+        rc->frame_spatial_variance > 500)
       return enumerator << 3;
     else if (rc->last_encoded_size_keyframe >
                  2 * rc->last_target_size_keyframe &&
-             rc->spatial_variance_keyframe > 200)
+             rc->frame_spatial_variance > 200)
       return enumerator << 2;
     else if (rc->last_encoded_size_keyframe > rc->last_target_size_keyframe)
       return enumerator << 1;
@@ -256,7 +256,7 @@ int av1_estimate_bits_at_q(const AV1_COMP *cpi, int q,
                 (int)((uint64_t)bpm * mbs) >> BPER_MB_NORMBITS);
 }
 
-int av1_rc_clamp_pframe_target_size(const AV1_COMP *const cpi, int64_t target,
+static int clamp_pframe_target_size(const AV1_COMP *const cpi, int64_t target,
                                     FRAME_UPDATE_TYPE frame_update_type) {
   const RATE_CONTROL *rc = &cpi->rc;
   const RateControlCfg *const rc_cfg = &cpi->oxcf.rc_cfg;
@@ -285,7 +285,7 @@ int av1_rc_clamp_pframe_target_size(const AV1_COMP *const cpi, int64_t target,
   return (int)target;
 }
 
-int av1_rc_clamp_iframe_target_size(const AV1_COMP *const cpi, int64_t target) {
+static int clamp_iframe_target_size(const AV1_COMP *const cpi, int64_t target) {
   const RATE_CONTROL *rc = &cpi->rc;
   const RateControlCfg *const rc_cfg = &cpi->oxcf.rc_cfg;
   if (rc_cfg->max_intra_bitrate_pct) {
@@ -317,7 +317,7 @@ static void update_layer_buffer_level(SVC *svc, int encoded_frame_size,
 
     // For screen-content mode: don't let buffer level go below threshold,
     // given here as -rc->maximum_ buffer_size, to allow buffer to come back
-    // up sooner after slide change with big oveshoot.
+    // up sooner after slide change with big overshoot.
     if (is_screen) {
       lp_rc->bits_off_target =
           AOMMAX(lp_rc->bits_off_target, -lp_rc->maximum_buffer_size);
@@ -340,9 +340,9 @@ static void update_buffer_level(AV1_COMP *cpi, int encoded_frame_size) {
   // Clip the buffer level to the maximum specified buffer size.
   p_rc->bits_off_target =
       AOMMIN(p_rc->bits_off_target, p_rc->maximum_buffer_size);
-  // For screen-content mode: don't let buffel level go below threshold,
+  // For screen-content mode: don't let buffer level go below threshold,
   // given here as -rc->maximum_ buffer_size, to allow buffer to come back
-  // up sooner after slide change with big oveshoot.
+  // up sooner after slide change with big overshoot.
   if (cpi->oxcf.tune_cfg.content == AOM_CONTENT_SCREEN)
     p_rc->bits_off_target =
         AOMMAX(p_rc->bits_off_target, -p_rc->maximum_buffer_size);
@@ -390,7 +390,10 @@ int av1_rc_get_default_min_gf_interval(int width, int height,
   // 4K60: 12
 }
 
-int av1_rc_get_default_max_gf_interval(double framerate, int min_gf_interval) {
+// Note get_default_max_gf_interval() requires the min_gf_interval to
+// be passed in to ensure that the max_gf_interval returned is at least as big
+// as that.
+static int get_default_max_gf_interval(double framerate, int min_gf_interval) {
   int interval = AOMMIN(MAX_GF_INTERVAL, (int)(framerate * 0.75));
   interval += (interval & 0x01);  // Round to even value
   interval = AOMMAX(MAX_GF_INTERVAL, interval);
@@ -410,7 +413,7 @@ void av1_primary_rc_init(const AV1EncoderConfig *oxcf,
         oxcf->frm_dim_cfg.width, oxcf->frm_dim_cfg.height,
         oxcf->input_cfg.init_framerate);
   if (max_gf_interval == 0)
-    max_gf_interval = av1_rc_get_default_max_gf_interval(
+    max_gf_interval = get_default_max_gf_interval(
         oxcf->input_cfg.init_framerate, min_gf_interval);
   p_rc->baseline_gf_interval = (min_gf_interval + max_gf_interval) / 2;
   p_rc->this_key_frame_forced = 0;
@@ -468,7 +471,7 @@ void av1_rc_init(const AV1EncoderConfig *oxcf, RATE_CONTROL *rc) {
         oxcf->frm_dim_cfg.width, oxcf->frm_dim_cfg.height,
         oxcf->input_cfg.init_framerate);
   if (rc->max_gf_interval == 0)
-    rc->max_gf_interval = av1_rc_get_default_max_gf_interval(
+    rc->max_gf_interval = get_default_max_gf_interval(
         oxcf->input_cfg.init_framerate, rc->min_gf_interval);
   rc->avg_frame_low_motion = 0;
 
@@ -621,6 +624,26 @@ static int adjust_q_cbr(const AV1_COMP *cpi, int q, int active_worst_quality,
                          ? AOMMIN(8, AOMMAX(1, rc->q_1_frame / 16))
                          : AOMMIN(16, AOMMAX(1, rc->q_1_frame / 8));
   }
+  // For screen static content with stable buffer level: relax the
+  // limit on max_delta_down and apply bias qp, based on buffer fullness.
+  // Only for high speeds levels for now to avoid bdrate regression.
+  if (cpi->sf.rt_sf.rc_faster_convergence_static == 1 &&
+      cpi->sf.rt_sf.check_scene_detection && rc->frame_source_sad == 0 &&
+      rc->static_since_last_scene_change &&
+      p_rc->buffer_level > (p_rc->optimal_buffer_level >> 1) &&
+      cpi->oxcf.q_cfg.aq_mode == CYCLIC_REFRESH_AQ &&
+      cpi->cyclic_refresh->counter_encode_maxq_scene_change > 4) {
+    int qp_delta = 32;
+    int qp_bias = 16;
+    if (p_rc->buffer_level > p_rc->optimal_buffer_level) {
+      qp_delta = 60;
+      qp_bias = 32;
+    }
+    if (cpi->rc.rc_1_frame == 1) q = q - qp_bias;
+    max_delta_down = AOMMAX(max_delta_down, qp_delta);
+    max_delta_up = AOMMIN(max_delta_up, 4);
+  }
+
   // If resolution changes or avg_frame_bandwidth significantly changed,
   // then set this flag to indicate change in target bits per macroblock.
   const int change_target_bits_mb =
@@ -743,7 +766,7 @@ static RATE_FACTOR_LEVEL get_rate_factor_level(const GF_GROUP *const gf_group,
 /*!\brief Gets a rate vs Q correction factor
  *
  * This function returns the current value of a correction factor used to
- * dynamilcally adjust the relationship between Q and the expected number
+ * dynamically adjust the relationship between Q and the expected number
  * of bits for the frame.
  *
  * \ingroup rate_control
@@ -802,7 +825,7 @@ static double get_rate_correction_factor(const AV1_COMP *cpi, int width,
 /*!\brief Sets a rate vs Q correction factor
  *
  * This function updates the current value of a correction factor used to
- * dynamilcally adjust the relationship between Q and the expected number
+ * dynamically adjust the relationship between Q and the expected number
  * of bits for the frame.
  *
  * \ingroup rate_control
@@ -929,7 +952,7 @@ void av1_rc_update_rate_correction_factors(AV1_COMP *cpi, int is_encode_stage,
     adjustment_limit = 0.75;
   }
 
-  // Adjustment to delta Q and number of blocks updated in cyclic refressh
+  // Adjustment to delta Q and number of blocks updated in cyclic refresh
   // based on over or under shoot of target in current frame.
   if (cyclic_refresh_active && cpi->rc.this_frame_target > 0) {
     CYCLIC_REFRESH *const cr = cpi->cyclic_refresh;
@@ -1525,7 +1548,7 @@ static int rc_pick_q_and_bounds_no_stats(const AV1_COMP *cpi, int width,
          p_rc->avg_frame_qindex[INTER_FRAME] < active_worst_quality)
             ? p_rc->avg_frame_qindex[INTER_FRAME]
             : p_rc->avg_frame_qindex[KEY_FRAME];
-    // For constrained quality dont allow Q less than the cq level
+    // For constrained quality don't allow Q less than the cq level
     if (rc_mode == AOM_CQ) {
       if (q < cq_level) q = cq_level;
       active_best_quality = get_gf_active_quality(p_rc, q, bit_depth);
@@ -2042,7 +2065,7 @@ static int rc_pick_q_and_bounds_q_mode(const AV1_COMP *cpi, int width,
 
 /*!\brief Picks q and q bounds given rate control parameters in \c cpi->rc.
  *
- * Handles the the general cases not covered by
+ * Handles the general cases not covered by
  * \ref rc_pick_q_and_bounds_no_stats_cbr() and
  * \ref rc_pick_q_and_bounds_no_stats()
  *
@@ -2120,7 +2143,7 @@ static int rc_pick_q_and_bounds(const AV1_COMP *cpi, int width, int height,
 
     // For alt_ref and GF frames (including internal arf frames) adjust the
     // worst allowed quality as well. This insures that even on hard
-    // sections we dont clamp the Q at the same value for arf frames and
+    // sections we don't clamp the Q at the same value for arf frames and
     // leaf (non arf) frames. This is important to the TPL model which assumes
     // Q drops with each arf level.
     if (!(rc->is_src_frame_alt_ref) &&
@@ -2283,14 +2306,15 @@ void av1_rc_set_frame_target(AV1_COMP *cpi, int target, int width, int height) {
 
   // Modify frame size target when down-scaled.
   if (av1_frame_scaled(cm) && cpi->oxcf.rc_cfg.mode != AOM_CBR) {
-    rc->this_frame_target =
-        (int)(rc->this_frame_target *
-              resize_rate_factor(&cpi->oxcf.frm_dim_cfg, width, height));
+    rc->this_frame_target = saturate_cast_double_to_int(
+        rc->this_frame_target *
+        resize_rate_factor(&cpi->oxcf.frm_dim_cfg, width, height));
   }
 
   // Target rate per SB64 (including partial SB64s.
-  rc->sb64_target_rate =
-      (int)(((int64_t)rc->this_frame_target << 12) / (width * height));
+  const int64_t sb64_target_rate =
+      ((int64_t)rc->this_frame_target << 12) / (width * height);
+  rc->sb64_target_rate = (int)AOMMIN(sb64_target_rate, INT_MAX);
 }
 
 static void update_alt_ref_frame_stats(AV1_COMP *cpi) {
@@ -2395,9 +2419,9 @@ void av1_rc_postencode_update(AV1_COMP *cpi, uint64_t bytes_used) {
   // Rolling monitors of whether we are over or underspending used to help
   // regulate min and Max Q in two pass.
   if (av1_frame_scaled(cm))
-    rc->this_frame_target = (int)(rc->this_frame_target /
-                                  resize_rate_factor(&cpi->oxcf.frm_dim_cfg,
-                                                     cm->width, cm->height));
+    rc->this_frame_target = saturate_cast_double_to_int(
+        rc->this_frame_target /
+        resize_rate_factor(&cpi->oxcf.frm_dim_cfg, cm->width, cm->height));
   if (current_frame->frame_type != KEY_FRAME) {
     p_rc->rolling_target_bits = (int)ROUND_POWER_OF_TWO_64(
         (int64_t)p_rc->rolling_target_bits * 3 + rc->this_frame_target, 2);
@@ -2456,12 +2480,6 @@ void av1_rc_postencode_update(AV1_COMP *cpi, uint64_t bytes_used) {
   rc->frame_number_encoded++;
   rc->prev_frame_is_dropped = 0;
   rc->drop_count_consec = 0;
-  // if (current_frame->frame_number == 1 && cm->show_frame)
-  /*
-  rc->this_frame_target =
-      (int)(rc->this_frame_target / resize_rate_factor(&cpi->oxcf.frm_dim_cfg,
-  cm->width, cm->height));
-      */
 }
 
 void av1_rc_postencode_update_drop_frame(AV1_COMP *cpi) {
@@ -2556,7 +2574,7 @@ int av1_compute_qdelta_by_rate(const AV1_COMP *cpi, FRAME_TYPE frame_type,
   return target_index - qindex;
 }
 
-void av1_rc_set_gf_interval_range(const AV1_COMP *const cpi,
+static void set_gf_interval_range(const AV1_COMP *const cpi,
                                   RATE_CONTROL *const rc) {
   const AV1EncoderConfig *const oxcf = &cpi->oxcf;
 
@@ -2573,8 +2591,8 @@ void av1_rc_set_gf_interval_range(const AV1_COMP *const cpi,
       rc->min_gf_interval = av1_rc_get_default_min_gf_interval(
           oxcf->frm_dim_cfg.width, oxcf->frm_dim_cfg.height, cpi->framerate);
     if (rc->max_gf_interval == 0)
-      rc->max_gf_interval = av1_rc_get_default_max_gf_interval(
-          cpi->framerate, rc->min_gf_interval);
+      rc->max_gf_interval =
+          get_default_max_gf_interval(cpi->framerate, rc->min_gf_interval);
     /*
      * Extended max interval for genuinely static scenes like slide shows.
      * The no.of.stats available in the case of LAP is limited,
@@ -2611,8 +2629,8 @@ void av1_rc_update_framerate(AV1_COMP *cpi, int width, int height) {
   // The baseline for this aligns with HW implementations that
   // can support decode of 1080P content up to a bitrate of MAX_MB_RATE bits
   // per 16x16 MB (averaged over a frame). However this limit is extended if
-  // a very high rate is given on the command line or the the rate cannnot
-  // be acheived because of a user specificed max q (e.g. when the user
+  // a very high rate is given on the command line or the rate cannot
+  // be achieved because of a user specified max q (e.g. when the user
   // specifies lossless encode.
   int64_t vbr_max_bits =
       (int64_t)rc->avg_frame_bandwidth * oxcf->rc_cfg.vbrmax_section / 100;
@@ -2621,7 +2639,7 @@ void av1_rc_update_framerate(AV1_COMP *cpi, int width, int height) {
   rc->max_frame_bandwidth =
       AOMMAX(AOMMAX((MBs * MAX_MB_RATE), MAXRATE_1080P), (int)vbr_max_bits);
 
-  av1_rc_set_gf_interval_range(cpi, rc);
+  set_gf_interval_range(cpi, rc);
 }
 
 #define VBR_PCT_ADJUSTMENT_LIMIT 50
@@ -2664,7 +2682,7 @@ static void vbr_rate_correction(AV1_COMP *cpi, int *this_frame_target) {
                               : p_rc->vbr_bits_off_target_fast;
 #endif
   // Fast redistribution of bits arising from massive local undershoot.
-  // Dont do it for kf,arf,gf or overlay frames.
+  // Don't do it for kf,arf,gf or overlay frames.
   if (!frame_is_kf_gf_arf(cpi) &&
 #if CONFIG_FPMT_TEST
       vbr_bits_off_target_fast &&
@@ -2694,7 +2712,7 @@ static void vbr_rate_correction(AV1_COMP *cpi, int *this_frame_target) {
     // Store the fast_extra_bits of the frame and reduce it from
     // vbr_bits_off_target_fast during postencode stage.
     rc->frame_level_fast_extra_bits = (int)fast_extra_bits;
-    // Retaining the condition to udpate during postencode stage since
+    // Retaining the condition to update during postencode stage since
     // fast_extra_bits are calculated based on vbr_bits_off_target_fast.
     cpi->do_update_vbr_bits_off_target_fast = 1;
   }
@@ -2732,14 +2750,14 @@ int av1_calc_pframe_target_size_one_pass_vbr(
 #else
   target = rc->avg_frame_bandwidth;
 #endif
-  return av1_rc_clamp_pframe_target_size(cpi, target, frame_update_type);
+  return clamp_pframe_target_size(cpi, target, frame_update_type);
 }
 
 int av1_calc_iframe_target_size_one_pass_vbr(const AV1_COMP *const cpi) {
   static const int kf_ratio = 25;
   const RATE_CONTROL *rc = &cpi->rc;
   const int64_t target = (int64_t)rc->avg_frame_bandwidth * kf_ratio;
-  return av1_rc_clamp_iframe_target_size(cpi, target);
+  return clamp_iframe_target_size(cpi, target);
 }
 
 int av1_calc_pframe_target_size_one_pass_cbr(
@@ -2820,7 +2838,7 @@ int av1_calc_iframe_target_size_one_pass_cbr(const AV1_COMP *cpi) {
     }
     target = ((int64_t)(16 + kf_boost) * rc->avg_frame_bandwidth) >> 4;
   }
-  return av1_rc_clamp_iframe_target_size(cpi, target);
+  return clamp_iframe_target_size(cpi, target);
 }
 
 static void set_golden_update(AV1_COMP *const cpi) {
@@ -2935,7 +2953,7 @@ void av1_adjust_gf_refresh_qp_one_pass_rt(AV1_COMP *cpi) {
 /*!\brief Setup the reference prediction structure for 1 pass real-time
  *
  * Set the reference prediction structure for 1 layer.
- * Current structue is to use 3 references (LAST, GOLDEN, ALTREF),
+ * Current structure is to use 3 references (LAST, GOLDEN, ALTREF),
  * where ALT_REF always behind current by lag_alt frames, and GOLDEN is
  * either updated on LAST with period baseline_gf_interval (fixed slot)
  * or always behind current by lag_gld (gld_fixed_slot = 0, lag_gld <= 7).
@@ -2976,7 +2994,7 @@ void av1_set_rtc_reference_structure_one_layer(AV1_COMP *cpi, int gf_update) {
     const uint64_t th_frame_sad[4][3] = {
       { 18000, 18000, 18000 },  // HDRES CPU 9
       { 25000, 25000, 25000 },  // MIDRES CPU 9
-      { 40000, 30000, 20000 },  // HDRES CPU10
+      { 40000, 30000, 20000 },  // HDRES CPU 10
       { 30000, 25000, 20000 }   // MIDRES CPU 10
     };
     int th_idx = cpi->sf.rt_sf.sad_based_adp_altref_lag - 1;
@@ -3228,7 +3246,7 @@ static void rc_scene_detection_onepass_rt(AV1_COMP *cpi,
   // TODO(marpan): There seems some difference along the bottom border when
   // using the source_last_tl0 for last_source (used for temporal layers or
   // when previous frame is dropped).
-  // Remove this bord parameter when issue is resolved: difference is that
+  // Remove this border parameter when issue is resolved: difference is that
   // non-zero sad exists along bottom border even though source is static.
   const int border =
       rc->prev_frame_is_dropped || cpi->svc.number_temporal_layers > 1;
@@ -3303,7 +3321,11 @@ static void rc_scene_detection_onepass_rt(AV1_COMP *cpi,
   if (num_samples > 0)
     rc->percent_blocks_with_motion =
         ((num_samples - num_zero_temp_sad) * 100) / num_samples;
-  if (rc->high_source_sad) cpi->rc.frames_since_scene_change = 0;
+  if (rc->frame_source_sad > 0) rc->static_since_last_scene_change = 0;
+  if (rc->high_source_sad) {
+    cpi->rc.frames_since_scene_change = 0;
+    rc->static_since_last_scene_change = 1;
+  }
   // Update the high_motion_content_screen_rtc flag on TL0. Avoid the update
   // if too many consecutive frame drops occurred.
   const uint64_t thresh_high_motion = 9 * 64 * 64;
@@ -3352,7 +3374,7 @@ static void rc_scene_detection_onepass_rt(AV1_COMP *cpi,
       }
     }
   }
-  // Scene detection is only on base SLO, and using full/orignal resolution.
+  // Scene detection is only on base SLO, and using full/original resolution.
   // Pass the state to the upper spatial layers.
   if (cpi->svc.number_spatial_layers > 1) {
     SVC *svc = &cpi->svc;
@@ -3394,7 +3416,7 @@ static const uint8_t AV1_VAR_OFFS[MAX_SB_SIZE] = {
  * \param[in]       src_ystride  Input source stride for y channel.
  *
  * \remark Nothing is returned. Instead the average spatial variance
- * computed is stored in flag \c cpi->rc.spatial_variance_keyframe.
+ * computed is stored in flag \c cpi->rc.frame_spatial_variance.
  */
 static void rc_spatial_act_keyframe_onepass_rt(AV1_COMP *cpi, uint8_t *src_y,
                                                int src_ystride) {
@@ -3428,7 +3450,7 @@ static void rc_spatial_act_keyframe_onepass_rt(AV1_COMP *cpi, uint8_t *src_y,
     cpi->rc.perc_flat_blocks_keyframe = 100 * num_zero_var_blocks / num_samples;
     avg_variance = avg_variance / num_samples;
   }
-  cpi->rc.spatial_variance_keyframe = avg_variance >> 12;
+  cpi->rc.frame_spatial_variance = avg_variance >> 12;
 }
 
 /*!\brief Set the GF baseline interval for 1 pass real-time mode.
@@ -3447,7 +3469,7 @@ static int set_gf_interval_update_onepass_rt(AV1_COMP *cpi,
   int gf_update = 0;
   const int resize_pending = is_frame_resize_pending(cpi);
   // GF update based on frames_till_gf_update_due, also
-  // force upddate on resize pending frame or for scene change.
+  // force update on resize pending frame or for scene change.
   if ((resize_pending || rc->high_source_sad ||
        rc->frames_till_gf_update_due == 0) &&
       cpi->svc.temporal_layer_id == 0 && cpi->svc.spatial_layer_id == 0) {
@@ -3520,7 +3542,7 @@ static void resize_reset_rc(AV1_COMP *cpi, int resize_width, int resize_height,
   }
 }
 
-/*!\brief ChecK for resize based on Q, for 1 pass real-time mode.
+/*!\brief Check for resize based on Q, for 1 pass real-time mode.
  *
  * Check if we should resize, based on average QP from past x frames.
  * Only allow for resize at most 1/2 scale down for now, Scaling factor
@@ -3675,8 +3697,10 @@ void av1_get_one_pass_rt_params(AV1_COMP *cpi, FRAME_TYPE *const frame_type,
       LAYER_IDS_TO_IDX(svc->spatial_layer_id, svc->temporal_layer_id,
                        svc->number_temporal_layers);
   if (cpi->oxcf.rc_cfg.max_consec_drop_ms > 0) {
+    double framerate =
+        cpi->framerate > 1 ? round(cpi->framerate) : cpi->framerate;
     rc->max_consec_drop = saturate_cast_double_to_int(
-        ceil(cpi->oxcf.rc_cfg.max_consec_drop_ms * cpi->framerate / 1000));
+        ceil(cpi->oxcf.rc_cfg.max_consec_drop_ms * framerate / 1000));
   }
   if (cpi->ppi->use_svc) {
     av1_update_temporal_layer_framerate(cpi);
@@ -3700,6 +3724,7 @@ void av1_get_one_pass_rt_params(AV1_COMP *cpi, FRAME_TYPE *const frame_type,
     }
     rc->frame_number_encoded = 0;
     cpi->ppi->rtc_ref.non_reference_frame = 0;
+    rc->static_since_last_scene_change = 0;
   } else {
     *frame_type = INTER_FRAME;
     gf_group->update_type[cpi->gf_frame_index] = LF_UPDATE;
@@ -3711,23 +3736,24 @@ void av1_get_one_pass_rt_params(AV1_COMP *cpi, FRAME_TYPE *const frame_type,
           svc->spatial_layer_id == 0
               ? 0
               : svc->layer_context[svc->temporal_layer_id].is_key_frame;
-      // If the user is setting the reference structure with
-      // set_ref_frame_config and did not set any references, set the
-      // frame type to Intra-only.
-      if (cpi->ppi->rtc_ref.set_ref_frame_config) {
-        int no_references_set = 1;
-        for (int i = 0; i < INTER_REFS_PER_FRAME; i++) {
-          if (cpi->ppi->rtc_ref.reference[i]) {
-            no_references_set = 0;
-            break;
-          }
+    }
+    // If the user is setting the reference structure with
+    // set_ref_frame_config and did not set any references, set the
+    // frame type to Intra-only.
+    if (cpi->ppi->rtc_ref.set_ref_frame_config) {
+      int no_references_set = 1;
+      for (int i = 0; i < INTER_REFS_PER_FRAME; i++) {
+        if (cpi->ppi->rtc_ref.reference[i]) {
+          no_references_set = 0;
+          break;
         }
-        // Set to intra_only_frame if no references are set.
-        // The stream can start decoding on INTRA_ONLY_FRAME so long as the
-        // layer with the intra_only_frame doesn't signal a reference to a slot
-        // that hasn't been set yet.
-        if (no_references_set) *frame_type = INTRA_ONLY_FRAME;
       }
+
+      // Set to intra_only_frame if no references are set.
+      // The stream can start decoding on INTRA_ONLY_FRAME so long as the
+      // layer with the intra_only_frame doesn't signal a reference to a slot
+      // that hasn't been set yet.
+      if (no_references_set) *frame_type = INTRA_ONLY_FRAME;
     }
   }
   if (cpi->active_map.enabled && cpi->rc.percent_blocks_inactive == 100) {
@@ -3745,7 +3771,8 @@ void av1_get_one_pass_rt_params(AV1_COMP *cpi, FRAME_TYPE *const frame_type,
       cpi->src_sad_blk_64x64 = NULL;
     }
   }
-  if (*frame_type == KEY_FRAME && cpi->sf.rt_sf.rc_adjust_keyframe &&
+  if (((*frame_type == KEY_FRAME && cpi->sf.rt_sf.rc_adjust_keyframe) ||
+       (cpi->sf.rt_sf.rc_compute_spatial_var_sc && rc->high_source_sad)) &&
       svc->spatial_layer_id == 0 && cm->seq_params->bit_depth == 8 &&
       cpi->oxcf.rc_cfg.max_intra_bitrate_pct > 0)
     rc_spatial_act_keyframe_onepass_rt(cpi, frame_input->source->y_buffer,
@@ -3835,10 +3862,25 @@ int av1_encodedframe_overshoot_cbr(AV1_COMP *cpi, int *q) {
   if (cpi->svc.spatial_layer_id > 0 && inter_layer_pred_on) {
     *q = (cpi->rc.worst_quality + *q) >> 1;
   } else {
-    *q = (3 * cpi->rc.worst_quality + *q) >> 2;
-    // For screen content use the max-q set by the user to allow for less
-    // overshoot on slide changes.
-    if (is_screen_content) *q = cpi->rc.worst_quality;
+    // For easy scene changes used lower QP, otherwise set max-q.
+    // If rt_sf->compute_spatial_var_sc is enabled relax the max-q
+    // condition based on frame spatial variance.
+    if (cpi->sf.rt_sf.rc_compute_spatial_var_sc) {
+      if (cpi->rc.frame_spatial_variance < 100) {
+        *q = (cpi->rc.worst_quality + *q) >> 1;
+      } else if (cpi->rc.frame_spatial_variance < 400 ||
+                 (cpi->rc.frame_source_sad < 80000 &&
+                  cpi->rc.frame_spatial_variance < 1000)) {
+        *q = (3 * cpi->rc.worst_quality + *q) >> 2;
+      } else {
+        *q = cpi->rc.worst_quality;
+      }
+    } else {
+      *q = (3 * cpi->rc.worst_quality + *q) >> 2;
+      // For screen content use the max-q set by the user to allow for less
+      // overshoot on slide changes.
+      if (is_screen_content) *q = cpi->rc.worst_quality;
+    }
   }
   // Adjust avg_frame_qindex, buffer_level, and rate correction factors, as
   // these parameters will affect QP selection for subsequent frames. If they
diff --git a/av1/encoder/ratectrl.h b/av1/encoder/ratectrl.h
index 21c7568d9..d1ea2e923 100644
--- a/av1/encoder/ratectrl.h
+++ b/av1/encoder/ratectrl.h
@@ -194,7 +194,8 @@ typedef struct {
   uint64_t avg_source_sad;
   uint64_t prev_avg_source_sad;
   uint64_t frame_source_sad;
-  uint64_t spatial_variance_keyframe;
+  uint64_t frame_spatial_variance;
+  int static_since_last_scene_change;
   int last_encoded_size_keyframe;
   int last_target_size_keyframe;
   int frames_since_scene_change;
@@ -590,10 +591,6 @@ double av1_convert_qindex_to_q(int qindex, aom_bit_depth_t bit_depth);
 void av1_rc_init_minq_luts(void);
 
 int av1_rc_get_default_min_gf_interval(int width, int height, double framerate);
-// Note av1_rc_get_default_max_gf_interval() requires the min_gf_interval to
-// be passed in to ensure that the max_gf_interval returned is at least as bis
-// as that.
-int av1_rc_get_default_max_gf_interval(double framerate, int min_gf_interval);
 
 // Generally at the high level, the following flow is expected
 // to be enforced for rate control:
@@ -691,12 +688,6 @@ int av1_rc_bits_per_mb(const struct AV1_COMP *cpi, FRAME_TYPE frame_type,
                        int qindex, double correction_factor,
                        int accurate_estimate);
 
-// Clamping utilities for bitrate targets for iframes and pframes.
-int av1_rc_clamp_iframe_target_size(const struct AV1_COMP *const cpi,
-                                    int64_t target);
-int av1_rc_clamp_pframe_target_size(const struct AV1_COMP *const cpi,
-                                    int64_t target, uint8_t frame_update_type);
-
 // Find q_index corresponding to desired_q, within [best_qindex, worst_qindex].
 // To be precise, 'q_index' is the smallest integer, for which the corresponding
 // q >= desired_q.
@@ -717,9 +708,6 @@ int av1_compute_qdelta_by_rate(const struct AV1_COMP *cpi,
 
 void av1_rc_update_framerate(struct AV1_COMP *cpi, int width, int height);
 
-void av1_rc_set_gf_interval_range(const struct AV1_COMP *const cpi,
-                                  RATE_CONTROL *const rc);
-
 void av1_set_target_rate(struct AV1_COMP *cpi, int width, int height);
 
 int av1_resize_one_pass_cbr(struct AV1_COMP *cpi);
diff --git a/av1/encoder/rc_utils.h b/av1/encoder/rc_utils.h
index 8d807af92..82ab1b201 100644
--- a/av1/encoder/rc_utils.h
+++ b/av1/encoder/rc_utils.h
@@ -27,7 +27,7 @@ static inline void check_reset_rc_flag(AV1_COMP *cpi) {
     if (cpi->ppi->use_svc) {
       av1_svc_check_reset_layer_rc_flag(cpi);
     } else {
-      if (rc->avg_frame_bandwidth > (3 * rc->prev_avg_frame_bandwidth >> 1) ||
+      if (rc->avg_frame_bandwidth / 3 > (rc->prev_avg_frame_bandwidth >> 1) ||
           rc->avg_frame_bandwidth < (rc->prev_avg_frame_bandwidth >> 1)) {
         rc->rc_1_frame = 0;
         rc->rc_2_frame = 0;
@@ -144,7 +144,8 @@ static inline int recode_loop_test(AV1_COMP *cpi, int high_limit, int low_limit,
       // Deal with frame undershoot and whether or not we are
       // below the automatically set cq level.
       if (q > oxcf->rc_cfg.cq_level &&
-          rc->projected_frame_size < ((rc->this_frame_target * 7) >> 3)) {
+          rc->projected_frame_size <
+              (((int64_t)rc->this_frame_target * 7) >> 3)) {
         force_recode = 1;
       }
     }
diff --git a/av1/encoder/rd.c b/av1/encoder/rd.c
index 403d12ac4..e77dbc7a1 100644
--- a/av1/encoder/rd.c
+++ b/av1/encoder/rd.c
@@ -35,6 +35,7 @@
 #include "av1/encoder/nonrd_opt.h"
 #include "av1/encoder/ratectrl.h"
 #include "av1/encoder/rd.h"
+#include "config/aom_config.h"
 
 #define RD_THRESH_POW 1.25
 
@@ -319,6 +320,7 @@ void av1_fill_mode_rates(AV1_COMMON *const cm, ModeCosts *mode_costs,
   }
 }
 
+#if !CONFIG_REALTIME_ONLY
 void av1_fill_lr_rates(ModeCosts *mode_costs, FRAME_CONTEXT *fc) {
   av1_cost_tokens_from_cdf(mode_costs->switchable_restore_cost,
                            fc->switchable_restore_cdf, NULL);
@@ -327,6 +329,7 @@ void av1_fill_lr_rates(ModeCosts *mode_costs, FRAME_CONTEXT *fc) {
   av1_cost_tokens_from_cdf(mode_costs->sgrproj_restore_cost,
                            fc->sgrproj_restore_cdf, NULL);
 }
+#endif  // !CONFIG_REALTIME_ONLY
 
 // Values are now correlated to quantizer.
 static int sad_per_bit_lut_8[QINDEX_RANGE];
@@ -467,6 +470,7 @@ int av1_adjust_q_from_delta_q_res(int delta_q_res, int prev_qindex,
   return adjust_qindex;
 }
 
+#if !CONFIG_REALTIME_ONLY
 int av1_get_adaptive_rdmult(const AV1_COMP *cpi, double beta) {
   assert(beta > 0.0);
   const AV1_COMMON *cm = &cpi->common;
@@ -485,6 +489,7 @@ int av1_get_adaptive_rdmult(const AV1_COMP *cpi, double beta) {
                    is_stat_consumption_stage(cpi)) /
                beta);
 }
+#endif  // !CONFIG_REALTIME_ONLY
 
 static int compute_rd_thresh_factor(int qindex, aom_bit_depth_t bit_depth) {
   double q;
@@ -936,114 +941,6 @@ static int sse_norm_curvfit_model_cat_lookup(double sse_norm) {
   return (sse_norm > 16.0);
 }
 
-// Models distortion by sse using a logistic function on
-// l = log2(sse / q^2) as:
-// dbysse = 16 / (1 + k exp(l + c))
-static double get_dbysse_logistic(double l, double c, double k) {
-  const double A = 16.0;
-  const double dbysse = A / (1 + k * exp(l + c));
-  return dbysse;
-}
-
-// Models rate using a clamped linear function on
-// l = log2(sse / q^2) as:
-// rate = max(0, a + b * l)
-static double get_rate_clamplinear(double l, double a, double b) {
-  const double rate = a + b * l;
-  return (rate < 0 ? 0 : rate);
-}
-
-static const uint8_t bsize_surffit_model_cat_lookup[BLOCK_SIZES_ALL] = {
-  0, 0, 0, 0, 1, 1, 2, 3, 3, 4, 5, 5, 6, 7, 7, 8, 0, 0, 2, 2, 4, 4
-};
-
-static const double surffit_rate_params[9][4] = {
-  {
-      638.390212,
-      2.253108,
-      166.585650,
-      -3.939401,
-  },
-  {
-      5.256905,
-      81.997240,
-      -1.321771,
-      17.694216,
-  },
-  {
-      -74.193045,
-      72.431868,
-      -19.033152,
-      15.407276,
-  },
-  {
-      416.770113,
-      14.794188,
-      167.686830,
-      -6.997756,
-  },
-  {
-      378.511276,
-      9.558376,
-      154.658843,
-      -6.635663,
-  },
-  {
-      277.818787,
-      4.413180,
-      150.317637,
-      -9.893038,
-  },
-  {
-      142.212132,
-      11.542038,
-      94.393964,
-      -5.518517,
-  },
-  {
-      219.100256,
-      4.007421,
-      108.932852,
-      -6.981310,
-  },
-  {
-      222.261971,
-      3.251049,
-      95.972916,
-      -5.609789,
-  },
-};
-
-static const double surffit_dist_params[7] = { 1.475844,  4.328362, -5.680233,
-                                               -0.500994, 0.554585, 4.839478,
-                                               -0.695837 };
-
-static void rate_surffit_model_params_lookup(BLOCK_SIZE bsize, double xm,
-                                             double *rpar) {
-  const int cat = bsize_surffit_model_cat_lookup[bsize];
-  rpar[0] = surffit_rate_params[cat][0] + surffit_rate_params[cat][1] * xm;
-  rpar[1] = surffit_rate_params[cat][2] + surffit_rate_params[cat][3] * xm;
-}
-
-static void dist_surffit_model_params_lookup(BLOCK_SIZE bsize, double xm,
-                                             double *dpar) {
-  (void)bsize;
-  const double *params = surffit_dist_params;
-  dpar[0] = params[0] + params[1] / (1 + exp((xm + params[2]) * params[3]));
-  dpar[1] = params[4] + params[5] * exp(params[6] * xm);
-}
-
-void av1_model_rd_surffit(BLOCK_SIZE bsize, double sse_norm, double xm,
-                          double yl, double *rate_f, double *distbysse_f) {
-  (void)sse_norm;
-  double rpar[2], dpar[2];
-  rate_surffit_model_params_lookup(bsize, xm, rpar);
-  dist_surffit_model_params_lookup(bsize, xm, dpar);
-
-  *rate_f = get_rate_clamplinear(yl, rpar[0], rpar[1]);
-  *distbysse_f = get_dbysse_logistic(yl, dpar[0], dpar[1]);
-}
-
 static const double interp_rgrid_curv[4][65] = {
   {
       0.000000,    0.000000,    0.000000,    0.000000,    0.000000,
diff --git a/av1/encoder/rd.h b/av1/encoder/rd.h
index a616e7ece..d4db276b8 100644
--- a/av1/encoder/rd.h
+++ b/av1/encoder/rd.h
@@ -20,6 +20,7 @@
 #include "av1/encoder/context_tree.h"
 #include "av1/encoder/cost.h"
 #include "av1/encoder/ratectrl.h"
+#include "config/aom_config.h"
 
 #ifdef __cplusplus
 extern "C" {
@@ -256,8 +257,6 @@ void av1_model_rd_from_var_lapndz(int64_t var, unsigned int n,
 
 void av1_model_rd_curvfit(BLOCK_SIZE bsize, double sse_norm, double xqr,
                           double *rate_f, double *distbysse_f);
-void av1_model_rd_surffit(BLOCK_SIZE bsize, double sse_norm, double xm,
-                          double yl, double *rate_f, double *distbysse_f);
 
 int av1_get_switchable_rate(const MACROBLOCK *x, const MACROBLOCKD *xd,
                             InterpFilter interp_filter, int dual_filter);
@@ -358,7 +357,9 @@ int av1_get_intra_cost_penalty(int qindex, int qdelta,
 void av1_fill_mode_rates(AV1_COMMON *const cm, ModeCosts *mode_costs,
                          FRAME_CONTEXT *fc);
 
+#if !CONFIG_REALTIME_ONLY
 void av1_fill_lr_rates(ModeCosts *mode_costs, FRAME_CONTEXT *fc);
+#endif
 
 void av1_fill_coeff_costs(CoeffCosts *coeff_costs, FRAME_CONTEXT *fc,
                           const int num_planes);
@@ -368,7 +369,9 @@ void av1_fill_mv_costs(const nmv_context *nmvc, int integer_mv, int usehp,
 
 void av1_fill_dv_costs(const nmv_context *ndvc, IntraBCMVCosts *dv_costs);
 
+#if !CONFIG_REALTIME_ONLY
 int av1_get_adaptive_rdmult(const struct AV1_COMP *cpi, double beta);
+#endif
 
 int av1_get_deltaq_offset(aom_bit_depth_t bit_depth, int qindex, double beta);
 
diff --git a/av1/encoder/speed_features.c b/av1/encoder/speed_features.c
index 2cc06598f..da9682bc2 100644
--- a/av1/encoder/speed_features.c
+++ b/av1/encoder/speed_features.c
@@ -1466,7 +1466,7 @@ static void set_rt_speed_feature_framesize_dependent(const AV1_COMP *const cpi,
     if (is_360p_or_larger) {
       sf->part_sf.fixed_partition_size = BLOCK_32X32;
       sf->rt_sf.use_fast_fixed_part = 1;
-      sf->mv_sf.subpel_force_stop = HALF_PEL;
+      sf->rt_sf.reduce_mv_pel_precision_lowcomplex = 2;
     }
     sf->rt_sf.increase_source_sad_thresh = 1;
     sf->rt_sf.part_early_exit_zeromv = 2;
@@ -1580,17 +1580,19 @@ static void set_rt_speed_feature_framesize_dependent(const AV1_COMP *const cpi,
       sf->rt_sf.dct_only_palette_nonrd = 1;
       sf->rt_sf.prune_palette_search_nonrd = 1;
       sf->rt_sf.prune_intra_mode_using_best_sad_so_far = true;
+      sf->rt_sf.rc_faster_convergence_static = 1;
+      sf->rt_sf.rc_compute_spatial_var_sc = 1;
     }
     if (speed >= 11) {
       sf->rt_sf.skip_lf_screen = 2;
       sf->rt_sf.skip_cdef_sb = 2;
-      sf->rt_sf.part_early_exit_zeromv = 2;
       sf->rt_sf.prune_palette_search_nonrd = 2;
       sf->rt_sf.increase_color_thresh_palette = 0;
       sf->rt_sf.prune_h_pred_using_best_mode_so_far = true;
       sf->rt_sf.enable_intra_mode_pruning_using_neighbors = true;
     }
-    sf->rt_sf.skip_encoding_non_reference_slide_change = 1;
+    sf->rt_sf.skip_encoding_non_reference_slide_change =
+        cpi->oxcf.rc_cfg.drop_frames_water_mark > 0 ? 1 : 0;
     sf->rt_sf.skip_newmv_flat_blocks_screen = 1;
     sf->rt_sf.use_idtx_nonrd = 1;
     sf->rt_sf.higher_thresh_scene_detection = 0;
@@ -2274,6 +2276,7 @@ static inline void init_rt_sf(REAL_TIME_SPEED_FEATURES *rt_sf) {
   rt_sf->overshoot_detection_cbr = NO_DETECTION;
   rt_sf->check_scene_detection = 0;
   rt_sf->rc_adjust_keyframe = 0;
+  rt_sf->rc_compute_spatial_var_sc = 0;
   rt_sf->prefer_large_partition_blocks = 0;
   rt_sf->use_temporal_noise_estimate = 0;
   rt_sf->fullpel_search_step_param = 0;
@@ -2327,6 +2330,7 @@ static inline void init_rt_sf(REAL_TIME_SPEED_FEATURES *rt_sf) {
   rt_sf->higher_thresh_scene_detection = 1;
   rt_sf->skip_newmv_flat_blocks_screen = 0;
   rt_sf->skip_encoding_non_reference_slide_change = 0;
+  rt_sf->rc_faster_convergence_static = 0;
 }
 
 static fractional_mv_step_fp
diff --git a/av1/encoder/speed_features.h b/av1/encoder/speed_features.h
index 5aec86f02..62f1a9e35 100644
--- a/av1/encoder/speed_features.h
+++ b/av1/encoder/speed_features.h
@@ -1651,6 +1651,9 @@ typedef struct REAL_TIME_SPEED_FEATURES {
   // For keyframes in rtc: adjust the rc_bits_per_mb, to reduce overshoot.
   int rc_adjust_keyframe;
 
+  // On scene change: compute spatial variance.
+  int rc_compute_spatial_var_sc;
+
   // For nonrd mode: Prefer larger partition blks in variance based partitioning
   // 0: disabled, 1-3: increasing aggressiveness
   int prefer_large_partition_blocks;
@@ -1922,6 +1925,10 @@ typedef struct REAL_TIME_SPEED_FEATURES {
 
   // Flag to force skip encoding for non_reference_frame on slide/scene changes.
   int skip_encoding_non_reference_slide_change;
+
+  // Flag to indicate more aggressive QP downward adjustment for screen static
+  // content, to make convergence to min_qp faster.
+  int rc_faster_convergence_static;
 } REAL_TIME_SPEED_FEATURES;
 
 /*!\endcond */
diff --git a/av1/encoder/svc_layercontext.c b/av1/encoder/svc_layercontext.c
index e8856986f..b2e584def 100644
--- a/av1/encoder/svc_layercontext.c
+++ b/av1/encoder/svc_layercontext.c
@@ -230,6 +230,8 @@ void av1_restore_layer_context(AV1_COMP *const cpi) {
   const int last_target_size_keyframe = cpi->rc.last_target_size_keyframe;
   const int max_consec_drop = cpi->rc.max_consec_drop;
   const int postencode_drop = cpi->rc.postencode_drop;
+  const int static_since_last_scene_change =
+      cpi->rc.static_since_last_scene_change;
   // Restore layer rate control.
   cpi->rc = lc->rc;
   cpi->ppi->p_rc = lc->p_rc;
@@ -247,6 +249,7 @@ void av1_restore_layer_context(AV1_COMP *const cpi) {
   cpi->rc.last_target_size_keyframe = last_target_size_keyframe;
   cpi->rc.max_consec_drop = max_consec_drop;
   cpi->rc.postencode_drop = postencode_drop;
+  cpi->rc.static_since_last_scene_change = static_since_last_scene_change;
   // For spatial-svc, allow cyclic-refresh to be applied on the spatial layers,
   // for the base temporal layer.
   if (cpi->oxcf.q_cfg.aq_mode == CYCLIC_REFRESH_AQ &&
@@ -296,7 +299,7 @@ void av1_svc_update_buffer_slot_refreshed(AV1_COMP *const cpi) {
   } else if (rtc_ref->set_ref_frame_config) {
     for (unsigned int i = 0; i < INTER_REFS_PER_FRAME; i++) {
       const int ref_frame_map_idx = rtc_ref->ref_idx[i];
-      if (cpi->ppi->rtc_ref.refresh[ref_frame_map_idx]) {
+      if (rtc_ref->refresh[ref_frame_map_idx]) {
         rtc_ref->buffer_time_index[ref_frame_map_idx] = current_frame;
         rtc_ref->buffer_spatial_layer[ref_frame_map_idx] =
             svc->spatial_layer_id;
@@ -328,12 +331,14 @@ void av1_save_layer_context(AV1_COMP *const cpi) {
     lc->actual_num_seg2_blocks = cr->actual_num_seg2_blocks;
     lc->counter_encode_maxq_scene_change = cr->counter_encode_maxq_scene_change;
   }
-  av1_svc_update_buffer_slot_refreshed(cpi);
-  for (unsigned int i = 0; i < REF_FRAMES; i++) {
-    if (frame_is_intra_only(cm) ||
-        cm->current_frame.refresh_frame_flags & (1 << i)) {
-      svc->spatial_layer_fb[i] = svc->spatial_layer_id;
-      svc->temporal_layer_fb[i] = svc->temporal_layer_id;
+  if (!cpi->is_dropped_frame) {
+    av1_svc_update_buffer_slot_refreshed(cpi);
+    for (unsigned int i = 0; i < REF_FRAMES; i++) {
+      if (frame_is_intra_only(cm) ||
+          cm->current_frame.refresh_frame_flags & (1 << i)) {
+        svc->spatial_layer_fb[i] = svc->spatial_layer_id;
+        svc->temporal_layer_fb[i] = svc->temporal_layer_id;
+      }
     }
   }
   if (svc->spatial_layer_id == svc->number_spatial_layers - 1) {
@@ -357,7 +362,8 @@ int av1_svc_primary_ref_frame(const AV1_COMP *const cpi) {
     // when set of enhancement layers are dropped (continued decoding starting
     // at next base TL0), so error_resilience can be off/0 for all layers.
     fb_idx = get_ref_frame_map_idx(cm, LAST_FRAME);
-    if (svc->spatial_layer_fb[fb_idx] == svc->spatial_layer_id &&
+    if (cpi->ppi->rtc_ref.reference[0] == 1 &&
+        svc->spatial_layer_fb[fb_idx] == svc->spatial_layer_id &&
         (svc->temporal_layer_fb[fb_idx] < svc->temporal_layer_id ||
          svc->temporal_layer_fb[fb_idx] == 0)) {
       primary_ref_frame = 0;  // LAST_FRAME: ref_frame - LAST_FRAME
@@ -479,8 +485,10 @@ void av1_set_svc_fixed_mode(AV1_COMP *const cpi) {
   // Set the reference map buffer idx for the 7 references:
   // LAST_FRAME (0), LAST2_FRAME(1), LAST3_FRAME(2), GOLDEN_FRAME(3),
   // BWDREF_FRAME(4), ALTREF2_FRAME(5), ALTREF_FRAME(6).
-  for (i = 0; i < INTER_REFS_PER_FRAME; i++) rtc_ref->ref_idx[i] = i;
-  for (i = 0; i < INTER_REFS_PER_FRAME; i++) rtc_ref->reference[i] = 0;
+  for (i = 0; i < INTER_REFS_PER_FRAME; i++) {
+    rtc_ref->reference[i] = 0;
+    rtc_ref->ref_idx[i] = i;
+  }
   for (i = 0; i < REF_FRAMES; i++) rtc_ref->refresh[i] = 0;
   // Always reference LAST, and reference GOLDEN on SL > 0.
   // For KSVC: GOLDEN reference will be removed on INTER_FRAMES later
@@ -620,7 +628,7 @@ void av1_svc_check_reset_layer_rc_flag(AV1_COMP *const cpi) {
       avg_frame_bandwidth = lrc->avg_frame_bandwidth;
       prev_avg_frame_bandwidth = lrc->prev_avg_frame_bandwidth;
     }
-    if (avg_frame_bandwidth > (3 * prev_avg_frame_bandwidth >> 1) ||
+    if (avg_frame_bandwidth / 3 > (prev_avg_frame_bandwidth >> 1) ||
         avg_frame_bandwidth < (prev_avg_frame_bandwidth >> 1)) {
       // Reset for all temporal layers with spatial layer sl.
       for (int tl = 0; tl < svc->number_temporal_layers; ++tl) {
@@ -684,7 +692,7 @@ int av1_svc_get_min_ref_dist(const AV1_COMP *cpi) {
       cpi->ppi->use_svc ? cpi->svc.current_superframe
                         : cpi->common.current_frame.frame_number;
   for (unsigned int i = 0; i < INTER_REFS_PER_FRAME; i++) {
-    if (cpi->ppi->rtc_ref.reference[i]) {
+    if (rtc_ref->reference[i]) {
       const int ref_frame_map_idx = rtc_ref->ref_idx[i];
       const int dist =
           current_frame_num - rtc_ref->buffer_time_index[ref_frame_map_idx];
diff --git a/av1/encoder/tpl_model.c b/av1/encoder/tpl_model.c
index e9319b182..582d1be9c 100644
--- a/av1/encoder/tpl_model.c
+++ b/av1/encoder/tpl_model.c
@@ -13,8 +13,11 @@
 #include <float.h>
 #include <stdint.h>
 
-#include "av1/encoder/thirdpass.h"
 #include "config/aom_config.h"
+
+#if CONFIG_THREE_PASS
+#include "av1/encoder/thirdpass.h"
+#endif
 #include "config/aom_dsp_rtcd.h"
 #include "config/aom_scale_rtcd.h"
 
@@ -538,8 +541,6 @@ static inline void mode_estimation(AV1_COMP *cpi, TplTxfmStats *tpl_txfm_stats,
   const int bw = 4 << mi_size_wide_log2[bsize];
   const int bh = 4 << mi_size_high_log2[bsize];
 
-  int frame_offset = tpl_data->frame_idx - cpi->gf_frame_index;
-
   int32_t best_intra_cost = INT32_MAX;
   int32_t intra_cost;
   PREDICTION_MODE best_mode = DC_PRED;
@@ -670,6 +671,9 @@ static inline void mode_estimation(AV1_COMP *cpi, TplTxfmStats *tpl_txfm_stats,
     tpl_stats->intra_rate = rate_cost;
   }
 
+#if CONFIG_THREE_PASS
+  const int frame_offset = tpl_data->frame_idx - cpi->gf_frame_index;
+
   if (cpi->third_pass_ctx &&
       frame_offset < cpi->third_pass_ctx->frame_info_count &&
       tpl_data->frame_idx < gf_group->size) {
@@ -699,6 +703,7 @@ static inline void mode_estimation(AV1_COMP *cpi, TplTxfmStats *tpl_txfm_stats,
       }
     }
   }
+#endif  // CONFIG_THREE_PASS
 
   // Motion compensated prediction
   xd->mi[0]->ref_frame[0] = INTRA_FRAME;
@@ -771,6 +776,7 @@ static inline void mode_estimation(AV1_COMP *cpi, TplTxfmStats *tpl_txfm_stats,
       }
     }
 
+#if CONFIG_THREE_PASS
     if (cpi->third_pass_ctx &&
         frame_offset < cpi->third_pass_ctx->frame_info_count &&
         tpl_data->frame_idx < gf_group->size) {
@@ -788,6 +794,7 @@ static inline void mode_estimation(AV1_COMP *cpi, TplTxfmStats *tpl_txfm_stats,
         center_mvs[0].mv = tp_mv;
       }
     }
+#endif  // CONFIG_THREE_PASS
 
     // Prune starting mvs
     if (tpl_sf->prune_starting_mv && refmv_count > 1) {
@@ -866,6 +873,7 @@ static inline void mode_estimation(AV1_COMP *cpi, TplTxfmStats *tpl_txfm_stats,
   int start_rf = 0;
   int end_rf = 3;
   if (!tpl_sf->allow_compound_pred) end_rf = 0;
+#if CONFIG_THREE_PASS
   if (cpi->third_pass_ctx &&
       frame_offset < cpi->third_pass_ctx->frame_info_count &&
       tpl_data->frame_idx < gf_group->size) {
@@ -895,6 +903,7 @@ static inline void mode_estimation(AV1_COMP *cpi, TplTxfmStats *tpl_txfm_stats,
       }
     }
   }
+#endif  // CONFIG_THREE_PASS
 
   xd->mi_row = mi_row;
   xd->mi_col = mi_col;
diff --git a/av1/encoder/tx_search.c b/av1/encoder/tx_search.c
index e3fd12991..7e51f21a9 100644
--- a/av1/encoder/tx_search.c
+++ b/av1/encoder/tx_search.c
@@ -2818,6 +2818,87 @@ static void ml_predict_intra_tx_depth_prune(MACROBLOCK *x, int blk_row,
 }
 #endif  // !CONFIG_REALTIME_ONLY
 
+/*!\brief Transform type search for luma macroblock with fixed transform size.
+ *
+ * \ingroup transform_search
+ * Search for the best transform type and return the transform coefficients RD
+ * cost of current luma macroblock with the given uniform transform size.
+ *
+ * \param[in]    x              Pointer to structure holding the data for the
+                                current encoding macroblock
+ * \param[in]    cpi            Top-level encoder structure
+ * \param[in]    rd_stats       Pointer to struct to keep track of the RD stats
+ * \param[in]    ref_best_rd    Best RD cost seen for this block so far
+ * \param[in]    bs             Size of the current macroblock
+ * \param[in]    tx_size        The given transform size
+ * \param[in]    ftxs_mode      Transform search mode specifying desired speed
+                                and quality tradeoff
+ * \param[in]    skip_trellis   Binary flag indicating if trellis optimization
+                                should be skipped
+ * \return       An int64_t value that is the best RD cost found.
+ */
+static int64_t uniform_txfm_yrd(const AV1_COMP *const cpi, MACROBLOCK *x,
+                                RD_STATS *rd_stats, int64_t ref_best_rd,
+                                BLOCK_SIZE bs, TX_SIZE tx_size,
+                                FAST_TX_SEARCH_MODE ftxs_mode,
+                                int skip_trellis) {
+  assert(IMPLIES(is_rect_tx(tx_size), is_rect_tx_allowed_bsize(bs)));
+  MACROBLOCKD *const xd = &x->e_mbd;
+  MB_MODE_INFO *const mbmi = xd->mi[0];
+  const TxfmSearchParams *txfm_params = &x->txfm_search_params;
+  const ModeCosts *mode_costs = &x->mode_costs;
+  const int is_inter = is_inter_block(mbmi);
+  const int tx_select = txfm_params->tx_mode_search_type == TX_MODE_SELECT &&
+                        block_signals_txsize(mbmi->bsize);
+  int tx_size_rate = 0;
+  if (tx_select) {
+    const int ctx = txfm_partition_context(
+        xd->above_txfm_context, xd->left_txfm_context, mbmi->bsize, tx_size);
+    tx_size_rate = is_inter ? mode_costs->txfm_partition_cost[ctx][0]
+                            : tx_size_cost(x, bs, tx_size);
+  }
+  const int skip_ctx = av1_get_skip_txfm_context(xd);
+  const int no_skip_txfm_rate = mode_costs->skip_txfm_cost[skip_ctx][0];
+  const int skip_txfm_rate = mode_costs->skip_txfm_cost[skip_ctx][1];
+  const int64_t skip_txfm_rd =
+      is_inter ? RDCOST(x->rdmult, skip_txfm_rate, 0) : INT64_MAX;
+  const int64_t no_this_rd =
+      RDCOST(x->rdmult, no_skip_txfm_rate + tx_size_rate, 0);
+
+  mbmi->tx_size = tx_size;
+  av1_txfm_rd_in_plane(x, cpi, rd_stats, ref_best_rd,
+                       AOMMIN(no_this_rd, skip_txfm_rd), AOM_PLANE_Y, bs,
+                       tx_size, ftxs_mode, skip_trellis);
+  if (rd_stats->rate == INT_MAX) return INT64_MAX;
+
+  int64_t rd;
+  // rdstats->rate should include all the rate except skip/non-skip cost as the
+  // same is accounted in the caller functions after rd evaluation of all
+  // planes. However the decisions should be done after considering the
+  // skip/non-skip header cost
+  if (rd_stats->skip_txfm && is_inter) {
+    rd = RDCOST(x->rdmult, skip_txfm_rate, rd_stats->sse);
+  } else {
+    // Intra blocks are always signalled as non-skip
+    rd = RDCOST(x->rdmult, rd_stats->rate + no_skip_txfm_rate + tx_size_rate,
+                rd_stats->dist);
+    rd_stats->rate += tx_size_rate;
+  }
+  // Check if forcing the block to skip transform leads to smaller RD cost.
+  if (is_inter && !rd_stats->skip_txfm && !xd->lossless[mbmi->segment_id]) {
+    int64_t temp_skip_txfm_rd =
+        RDCOST(x->rdmult, skip_txfm_rate, rd_stats->sse);
+    if (temp_skip_txfm_rd <= rd) {
+      rd = temp_skip_txfm_rd;
+      rd_stats->rate = 0;
+      rd_stats->dist = rd_stats->sse;
+      rd_stats->skip_txfm = 1;
+    }
+  }
+
+  return rd;
+}
+
 // Search for the best uniform transform size and type for current coding block.
 static inline void choose_tx_size_type_from_rd(const AV1_COMP *const cpi,
                                                MACROBLOCK *x,
@@ -2888,8 +2969,8 @@ static inline void choose_tx_size_type_from_rd(const AV1_COMP *const cpi,
         cpi->sf.tx_sf.use_rd_based_breakout_for_intra_tx_search
             ? AOMMIN(ref_best_rd, best_rd)
             : ref_best_rd;
-    rd[depth] = av1_uniform_txfm_yrd(cpi, x, &this_rd_stats, rd_thresh, bs,
-                                     tx_size, FTXS_NONE, skip_trellis);
+    rd[depth] = uniform_txfm_yrd(cpi, x, &this_rd_stats, rd_thresh, bs, tx_size,
+                                 FTXS_NONE, skip_trellis);
     if (rd[depth] < best_rd) {
       av1_copy_array(best_blk_skip, txfm_info->blk_skip, num_blks);
       av1_copy_array(best_txk_type_map, xd->tx_type_map, num_blks);
@@ -2964,10 +3045,12 @@ static inline void block_rd_txfm(int plane, int block, int blk_row, int blk_col,
                  &txb_ctx, args->ftxs_mode, args->skip_trellis,
                  args->best_rd - args->current_rd, &this_rd_stats);
 
+#if !CONFIG_REALTIME_ONLY
   if (plane == AOM_PLANE_Y && xd->cfl.store_y) {
     assert(!is_inter || plane_bsize < BLOCK_8X8);
     cfl_store_tx(xd, blk_row, blk_col, tx_size, plane_bsize);
   }
+#endif
 
 #if CONFIG_RD_DEBUG
   update_txb_coeff_cost(&this_rd_stats, plane, this_rd_stats.rate);
@@ -3127,67 +3210,6 @@ int64_t av1_estimate_txfm_yrd(const AV1_COMP *const cpi, MACROBLOCK *x,
   return rd;
 }
 
-int64_t av1_uniform_txfm_yrd(const AV1_COMP *const cpi, MACROBLOCK *x,
-                             RD_STATS *rd_stats, int64_t ref_best_rd,
-                             BLOCK_SIZE bs, TX_SIZE tx_size,
-                             FAST_TX_SEARCH_MODE ftxs_mode, int skip_trellis) {
-  assert(IMPLIES(is_rect_tx(tx_size), is_rect_tx_allowed_bsize(bs)));
-  MACROBLOCKD *const xd = &x->e_mbd;
-  MB_MODE_INFO *const mbmi = xd->mi[0];
-  const TxfmSearchParams *txfm_params = &x->txfm_search_params;
-  const ModeCosts *mode_costs = &x->mode_costs;
-  const int is_inter = is_inter_block(mbmi);
-  const int tx_select = txfm_params->tx_mode_search_type == TX_MODE_SELECT &&
-                        block_signals_txsize(mbmi->bsize);
-  int tx_size_rate = 0;
-  if (tx_select) {
-    const int ctx = txfm_partition_context(
-        xd->above_txfm_context, xd->left_txfm_context, mbmi->bsize, tx_size);
-    tx_size_rate = is_inter ? mode_costs->txfm_partition_cost[ctx][0]
-                            : tx_size_cost(x, bs, tx_size);
-  }
-  const int skip_ctx = av1_get_skip_txfm_context(xd);
-  const int no_skip_txfm_rate = mode_costs->skip_txfm_cost[skip_ctx][0];
-  const int skip_txfm_rate = mode_costs->skip_txfm_cost[skip_ctx][1];
-  const int64_t skip_txfm_rd =
-      is_inter ? RDCOST(x->rdmult, skip_txfm_rate, 0) : INT64_MAX;
-  const int64_t no_this_rd =
-      RDCOST(x->rdmult, no_skip_txfm_rate + tx_size_rate, 0);
-
-  mbmi->tx_size = tx_size;
-  av1_txfm_rd_in_plane(x, cpi, rd_stats, ref_best_rd,
-                       AOMMIN(no_this_rd, skip_txfm_rd), AOM_PLANE_Y, bs,
-                       tx_size, ftxs_mode, skip_trellis);
-  if (rd_stats->rate == INT_MAX) return INT64_MAX;
-
-  int64_t rd;
-  // rdstats->rate should include all the rate except skip/non-skip cost as the
-  // same is accounted in the caller functions after rd evaluation of all
-  // planes. However the decisions should be done after considering the
-  // skip/non-skip header cost
-  if (rd_stats->skip_txfm && is_inter) {
-    rd = RDCOST(x->rdmult, skip_txfm_rate, rd_stats->sse);
-  } else {
-    // Intra blocks are always signalled as non-skip
-    rd = RDCOST(x->rdmult, rd_stats->rate + no_skip_txfm_rate + tx_size_rate,
-                rd_stats->dist);
-    rd_stats->rate += tx_size_rate;
-  }
-  // Check if forcing the block to skip transform leads to smaller RD cost.
-  if (is_inter && !rd_stats->skip_txfm && !xd->lossless[mbmi->segment_id]) {
-    int64_t temp_skip_txfm_rd =
-        RDCOST(x->rdmult, skip_txfm_rate, rd_stats->sse);
-    if (temp_skip_txfm_rd <= rd) {
-      rd = temp_skip_txfm_rd;
-      rd_stats->rate = 0;
-      rd_stats->dist = rd_stats->sse;
-      rd_stats->skip_txfm = 1;
-    }
-  }
-
-  return rd;
-}
-
 // Search for the best transform type for a luma inter-predicted block, given
 // the transform block partitions.
 // This function is used only when some speed features are enabled.
diff --git a/av1/encoder/tx_search.h b/av1/encoder/tx_search.h
index 78efdb42d..918130475 100644
--- a/av1/encoder/tx_search.h
+++ b/av1/encoder/tx_search.h
@@ -72,30 +72,6 @@ int64_t av1_estimate_txfm_yrd(const AV1_COMP *const cpi, MACROBLOCK *x,
                               RD_STATS *rd_stats, int64_t ref_best_rd,
                               BLOCK_SIZE bs, TX_SIZE tx_size);
 
-/*!\brief Transform type search for luma macroblock with fixed transform size.
- *
- * \ingroup transform_search
- * Search for the best transform type and return the transform coefficients RD
- * cost of current luma macroblock with the given uniform transform size.
- *
- * \param[in]    x              Pointer to structure holding the data for the
-                                current encoding macroblock
- * \param[in]    cpi            Top-level encoder structure
- * \param[in]    rd_stats       Pointer to struct to keep track of the RD stats
- * \param[in]    ref_best_rd    Best RD cost seen for this block so far
- * \param[in]    bs             Size of the current macroblock
- * \param[in]    tx_size        The given transform size
- * \param[in]    ftxs_mode      Transform search mode specifying desired speed
-                                and quality tradeoff
- * \param[in]    skip_trellis   Binary flag indicating if trellis optimization
-                                should be skipped
- * \return       An int64_t value that is the best RD cost found.
- */
-int64_t av1_uniform_txfm_yrd(const AV1_COMP *const cpi, MACROBLOCK *x,
-                             RD_STATS *rd_stats, int64_t ref_best_rd,
-                             BLOCK_SIZE bs, TX_SIZE tx_size,
-                             FAST_TX_SEARCH_MODE ftxs_mode, int skip_trellis);
-
 /*!\brief Recursive transform size and type search.
  *
  * \ingroup transform_search
diff --git a/av1/encoder/var_based_part.c b/av1/encoder/var_based_part.c
index 666f80b9a..9e971cc05 100644
--- a/av1/encoder/var_based_part.c
+++ b/av1/encoder/var_based_part.c
@@ -421,14 +421,11 @@ static inline void fill_variance_4x4avg(const uint8_t *src_buf, int src_stride,
   }
 }
 
-// TODO(kyslov) Bring back threshold adjustment based on content state
 static int64_t scale_part_thresh_content(int64_t threshold_base, int speed,
-                                         int width, int height,
-                                         int non_reference_frame) {
-  (void)width;
-  (void)height;
+                                         int non_reference_frame,
+                                         int is_static) {
   int64_t threshold = threshold_base;
-  if (non_reference_frame) threshold = (3 * threshold) >> 1;
+  if (non_reference_frame && !is_static) threshold = (3 * threshold) >> 1;
   if (speed >= 8) {
     return (5 * threshold) >> 2;
   }
@@ -645,8 +642,8 @@ static inline int64_t tune_base_thresh_content(AV1_COMP *cpi,
       updated_thresh_base = (5 * updated_thresh_base) >> 2;
   }
   updated_thresh_base = scale_part_thresh_content(
-      updated_thresh_base, cpi->oxcf.speed, cm->width, cm->height,
-      cpi->ppi->rtc_ref.non_reference_frame);
+      updated_thresh_base, cpi->oxcf.speed,
+      cpi->ppi->rtc_ref.non_reference_frame, cpi->rc.frame_source_sad == 0);
   if (cpi->oxcf.speed >= 11 && source_sad_nonrd > kLowSad &&
       cpi->rc.high_motion_content_screen_rtc)
     updated_thresh_base = updated_thresh_base << 5;
@@ -1011,6 +1008,11 @@ static inline void chroma_check(AV1_COMP *cpi, MACROBLOCK *x, BLOCK_SIZE bsize,
   if (cpi->oxcf.tune_cfg.content == AOM_CONTENT_SCREEN &&
       cpi->rc.high_source_sad) {
     shift_lower_limit = 7;
+  } else if (cpi->oxcf.tune_cfg.content == AOM_CONTENT_SCREEN &&
+             cpi->rc.percent_blocks_with_motion > 90 &&
+             cpi->rc.frame_source_sad > 10000 && source_sad_nonrd > kLowSad) {
+    shift_lower_limit = 8;
+    shift_upper_limit = 3;
   } else if (source_sad_nonrd >= kMedSad && x->source_variance > 500 &&
              cpi->common.width * cpi->common.height >= 640 * 360) {
     shift_upper_limit = 2;
diff --git a/av1/exports_com b/av1/exports_com
index 5c8e0e09d..5ed6c495f 100644
--- a/av1/exports_com
+++ b/av1/exports_com
@@ -1,2 +1 @@
 text aom_read_obu_header_and_size
-text av1_resize_frame420
diff --git a/av1/exports_dec b/av1/exports_dec
index daabf6766..05860e8c0 100644
--- a/av1/exports_dec
+++ b/av1/exports_dec
@@ -1,3 +1,2 @@
 data aom_codec_av1_dx_algo
 text aom_codec_av1_dx
-text av1_add_film_grain
diff --git a/av1/exports_test b/av1/exports_test
deleted file mode 100644
index dab377575..000000000
--- a/av1/exports_test
+++ /dev/null
@@ -1,2 +0,0 @@
-text av1_get_fwd_txfm_cfg
-text av1_rtcd
diff --git a/build/cmake/exports_sources.cmake b/build/cmake/exports_sources.cmake
index 2387e341b..4ab8f1c51 100644
--- a/build/cmake/exports_sources.cmake
+++ b/build/cmake/exports_sources.cmake
@@ -28,8 +28,3 @@ if(CONFIG_AV1_ENCODER)
   list(APPEND AOM_EXPORTS_SOURCES "${AOM_ROOT}/aom/exports_enc"
               "${AOM_ROOT}/av1/exports_enc")
 endif()
-
-if(ENABLE_TESTS)
-  list(APPEND AOM_EXPORTS_SOURCES "${AOM_ROOT}/aom/exports_test"
-              "${AOM_ROOT}/av1/exports_test")
-endif()
diff --git a/build/cmake/rtcd.pl b/build/cmake/rtcd.pl
index 5d889cb74..464d1986c 100755
--- a/build/cmake/rtcd.pl
+++ b/build/cmake/rtcd.pl
@@ -264,13 +264,14 @@ EOF
 }
 
 sub common_bottom() {
+  my $include_guard = uc($opts{sym})."_H_";
   print <<EOF;
 
 #ifdef __cplusplus
 }  // extern "C"
 #endif
 
-#endif
+#endif  // ${include_guard}
 EOF
 }
 
diff --git a/build/cmake/version.pl b/build/cmake/version.pl
index 400107ce8..e611418d8 100755
--- a/build/cmake/version.pl
+++ b/build/cmake/version.pl
@@ -89,6 +89,8 @@ select $version_file;
 if (length($git_desc)) {
   print << "EOF";
 $lic_block
+#ifndef AOM_VERSION_H_
+#define AOM_VERSION_H_
 #define VERSION_MAJOR $version_major
 #define VERSION_MINOR $version_minor
 #define VERSION_PATCH $version_patch
@@ -97,10 +99,13 @@ $lic_block
   $version_packed
 #define VERSION_STRING_NOSP \"$git_desc\"
 #define VERSION_STRING \" $git_desc\"
+#endif  // AOM_VERSION_H_
 EOF
 } else {
   print << "EOF";
 $lic_block
+#ifndef AOM_VERSION_H_
+#define AOM_VERSION_H_
 #define VERSION_MAJOR $version_major
 #define VERSION_MINOR $version_minor
 #define VERSION_PATCH $version_patch
@@ -109,6 +114,7 @@ $lic_block
   $version_packed
 #define VERSION_STRING_NOSP \"v$version_string\"
 #define VERSION_STRING \" v$version_string\"
+#endif  // AOM_VERSION_H_
 EOF
 }
 close($version_file);
diff --git a/common/av1_config.c b/common/av1_config.c
index 6c6c2f941..41869dcbc 100644
--- a/common/av1_config.c
+++ b/common/av1_config.c
@@ -14,7 +14,6 @@
 #include "aom/aom_image.h"
 #include "aom/aom_integer.h"
 #include "aom_dsp/bitreader_buffer.h"
-#include "aom_dsp/bitwriter_buffer.h"
 #include "av1/common/obu_util.h"
 #include "common/av1_config.h"
 #include "config/aom_config.h"
@@ -475,33 +474,19 @@ int write_av1config(const Av1Config *config, size_t capacity,
                     size_t *bytes_written, uint8_t *buffer) {
   if (!config || !buffer || capacity < kAv1cSize || !bytes_written) return -1;
 
-  *bytes_written = 0;
-  memset(buffer, 0, kAv1cSize);
-
-  struct aom_write_bit_buffer writer = { buffer, 0 };
-
-  aom_wb_write_bit(&writer, config->marker);
-  aom_wb_write_literal(&writer, config->version, 7);
-  aom_wb_write_literal(&writer, config->seq_profile, 3);
-  aom_wb_write_literal(&writer, config->seq_level_idx_0, 5);
-  aom_wb_write_bit(&writer, config->seq_tier_0);
-  aom_wb_write_bit(&writer, config->high_bitdepth);
-  aom_wb_write_bit(&writer, config->twelve_bit);
-  aom_wb_write_bit(&writer, config->monochrome);
-  aom_wb_write_bit(&writer, config->chroma_subsampling_x);
-  aom_wb_write_bit(&writer, config->chroma_subsampling_y);
-  aom_wb_write_literal(&writer, config->chroma_sample_position, 2);
-  aom_wb_write_literal(&writer, 0, 3);  // reserved
-  aom_wb_write_bit(&writer, config->initial_presentation_delay_present);
-
+  buffer[0] = (config->marker << 7) | config->version;
+  buffer[1] = (config->seq_profile << 5) | config->seq_level_idx_0;
+  buffer[2] = (config->seq_tier_0 << 7) | (config->high_bitdepth << 6) |
+              (config->twelve_bit << 5) | (config->monochrome << 4) |
+              (config->chroma_subsampling_x << 3) |
+              (config->chroma_subsampling_y << 2) |
+              config->chroma_sample_position;
+  buffer[3] = config->initial_presentation_delay_present << 4;
   if (config->initial_presentation_delay_present) {
-    aom_wb_write_literal(&writer, config->initial_presentation_delay_minus_one,
-                         4);
-  } else {
-    aom_wb_write_literal(&writer, 0, 4);  // reserved
+    buffer[3] |= config->initial_presentation_delay_minus_one;
   }
 
-  *bytes_written = aom_wb_bytes_written(&writer);
+  *bytes_written = kAv1cSize;
   return 0;
 }
 
diff --git a/config/arm/config/aom_dsp_rtcd.h b/config/arm/config/aom_dsp_rtcd.h
index 0bd16fbd6..845f14e3a 100644
--- a/config/arm/config/aom_dsp_rtcd.h
+++ b/config/arm/config/aom_dsp_rtcd.h
@@ -6157,4 +6157,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AOM_DSP_RTCD_H_
diff --git a/config/arm/config/aom_scale_rtcd.h b/config/arm/config/aom_scale_rtcd.h
index ea025ad41..3eaffa813 100644
--- a/config/arm/config/aom_scale_rtcd.h
+++ b/config/arm/config/aom_scale_rtcd.h
@@ -88,4 +88,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AOM_SCALE_RTCD_H_
diff --git a/config/arm/config/av1_rtcd.h b/config/arm/config/av1_rtcd.h
index 0e1b7125d..4d789181b 100644
--- a/config/arm/config/av1_rtcd.h
+++ b/config/arm/config/av1_rtcd.h
@@ -616,11 +616,11 @@ void av1_round_shift_array_neon(int32_t *arr, int size, int bit);
 #define av1_round_shift_array av1_round_shift_array_neon
 
 int av1_selfguided_restoration_c(const uint8_t *dgd8, int width, int height,
-                                int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
-                                int sgr_params_idx, int bit_depth, int highbd);
+                                  int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
+                                  int sgr_params_idx, int bit_depth, int highbd);
 int av1_selfguided_restoration_neon(const uint8_t *dgd8, int width, int height,
-                                int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
-                                int sgr_params_idx, int bit_depth, int highbd);
+                                  int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
+                                  int sgr_params_idx, int bit_depth, int highbd);
 #define av1_selfguided_restoration av1_selfguided_restoration_neon
 
 void av1_txb_init_levels_c(const tran_low_t *const coeff, const int width, const int height, uint8_t *const levels);
@@ -754,4 +754,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AV1_RTCD_H_
diff --git a/config/arm64/config/aom_config.asm b/config/arm64/config/aom_config.asm
index 86e8a12d3..700d2dc98 100644
--- a/config/arm64/config/aom_config.asm
+++ b/config/arm64/config/aom_config.asm
@@ -87,8 +87,8 @@ HAVE_SSE3 equ 0
 HAVE_SSE4_1 equ 0
 HAVE_SSE4_2 equ 0
 HAVE_SSSE3 equ 0
-HAVE_SVE equ 0
-HAVE_SVE2 equ 0
+HAVE_SVE equ 1
+HAVE_SVE2 equ 1
 HAVE_UNISTD_H equ 1
 HAVE_VSX equ 0
 HAVE_WXWIDGETS equ 0
diff --git a/config/arm64/config/aom_config.c b/config/arm64/config/aom_config.c
index d2f13a0e8..b79daa23a 100644
--- a/config/arm64/config/aom_config.c
+++ b/config/arm64/config/aom_config.c
@@ -9,5 +9,5 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 #include "aom/aom_codec.h"
-static const char* const cfg = "cmake ../libaom -G \"Unix Makefiles\" -DCMAKE_TOOLCHAIN_FILE=\"../libaom/build/cmake/toolchains/arm64-linux-gcc.cmake\" -DCONFIG_AV1_ENCODER=1 -DCONFIG_AV1_HIGHBITDEPTH=1 -DCONFIG_RUNTIME_CPU_DETECT=1 -DCONFIG_MAX_DECODE_PROFILE=0 -DCONFIG_NORMAL_TILE_MODE=1 -DCONFIG_SIZE_LIMIT=1 -DDECODE_HEIGHT_LIMIT=16384 -DDECODE_WIDTH_LIMIT=16384 -DENABLE_SSE4_1=0";
+static const char* const cfg = "cmake ../libaom -G \"Unix Makefiles\" -DCMAKE_TOOLCHAIN_FILE=\"../libaom/build/cmake/toolchains/arm64-linux-clang.cmake\" -DCONFIG_AV1_ENCODER=1 -DCONFIG_AV1_HIGHBITDEPTH=1 -DCONFIG_RUNTIME_CPU_DETECT=1 -DCONFIG_MAX_DECODE_PROFILE=0 -DCONFIG_NORMAL_TILE_MODE=1 -DCONFIG_SIZE_LIMIT=1 -DDECODE_HEIGHT_LIMIT=16384 -DDECODE_WIDTH_LIMIT=16384 -DENABLE_SSE4_1=0";
 const char *aom_codec_build_config(void) {return cfg;}
diff --git a/config/arm64/config/aom_config.h b/config/arm64/config/aom_config.h
index d894b546b..bb7fdb4a5 100644
--- a/config/arm64/config/aom_config.h
+++ b/config/arm64/config/aom_config.h
@@ -89,8 +89,8 @@
 #define HAVE_SSE4_1 0
 #define HAVE_SSE4_2 0
 #define HAVE_SSSE3 0
-#define HAVE_SVE 0
-#define HAVE_SVE2 0
+#define HAVE_SVE 1
+#define HAVE_SVE2 1
 #define HAVE_UNISTD_H 1
 #define HAVE_VSX 0
 #define HAVE_WXWIDGETS 0
diff --git a/config/arm64/config/aom_dsp_rtcd.h b/config/arm64/config/aom_dsp_rtcd.h
index 5e44a9b77..48fa95395 100644
--- a/config/arm64/config/aom_dsp_rtcd.h
+++ b/config/arm64/config/aom_dsp_rtcd.h
@@ -70,7 +70,8 @@ double aom_compute_correlation_c(const unsigned char *frame1, int stride1, int x
 
 void aom_compute_flow_at_point_c(const uint8_t *src, const uint8_t *ref, int x, int y, int width, int height, int stride, double *u, double *v);
 void aom_compute_flow_at_point_neon(const uint8_t *src, const uint8_t *ref, int x, int y, int width, int height, int stride, double *u, double *v);
-#define aom_compute_flow_at_point aom_compute_flow_at_point_neon
+void aom_compute_flow_at_point_sve(const uint8_t *src, const uint8_t *ref, int x, int y, int width, int height, int stride, double *u, double *v);
+RTCD_EXTERN void (*aom_compute_flow_at_point)(const uint8_t *src, const uint8_t *ref, int x, int y, int width, int height, int stride, double *u, double *v);
 
 bool aom_compute_mean_stddev_c(const unsigned char *frame, int stride, int x, int y, double *mean, double *one_over_stddev);
 #define aom_compute_mean_stddev aom_compute_mean_stddev_c
@@ -615,7 +616,8 @@ void aom_fft8x8_float_c(const float *input, float *temp, float *output);
 
 void aom_get_blk_sse_sum_c(const int16_t *data, int stride, int bw, int bh, int *x_sum, int64_t *x2_sum);
 void aom_get_blk_sse_sum_neon(const int16_t *data, int stride, int bw, int bh, int *x_sum, int64_t *x2_sum);
-#define aom_get_blk_sse_sum aom_get_blk_sse_sum_neon
+void aom_get_blk_sse_sum_sve(const int16_t *data, int stride, int bw, int bh, int *x_sum, int64_t *x2_sum);
+RTCD_EXTERN void (*aom_get_blk_sse_sum)(const int16_t *data, int stride, int bw, int bh, int *x_sum, int64_t *x2_sum);
 
 unsigned int aom_get_mb_ss_c(const int16_t *);
 unsigned int aom_get_mb_ss_neon(const int16_t *);
@@ -913,19 +915,23 @@ unsigned int aom_highbd_10_masked_sub_pixel_variance8x8_neon(const uint8_t *src,
 
 unsigned int aom_highbd_10_mse16x16_c(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 unsigned int aom_highbd_10_mse16x16_neon(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
-#define aom_highbd_10_mse16x16 aom_highbd_10_mse16x16_neon
+unsigned int aom_highbd_10_mse16x16_sve(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_mse16x16)(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 
 unsigned int aom_highbd_10_mse16x8_c(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 unsigned int aom_highbd_10_mse16x8_neon(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
-#define aom_highbd_10_mse16x8 aom_highbd_10_mse16x8_neon
+unsigned int aom_highbd_10_mse16x8_sve(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_mse16x8)(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 
 unsigned int aom_highbd_10_mse8x16_c(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 unsigned int aom_highbd_10_mse8x16_neon(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
-#define aom_highbd_10_mse8x16 aom_highbd_10_mse8x16_neon
+unsigned int aom_highbd_10_mse8x16_sve(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_mse8x16)(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 
 unsigned int aom_highbd_10_mse8x8_c(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 unsigned int aom_highbd_10_mse8x8_neon(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
-#define aom_highbd_10_mse8x8 aom_highbd_10_mse8x8_neon
+unsigned int aom_highbd_10_mse8x8_sve(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_mse8x8)(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 
 unsigned int aom_highbd_10_obmc_sub_pixel_variance128x128_c(const uint8_t *pre, int pre_stride, int xoffset, int yoffset, const int32_t *wsrc, const int32_t *mask, unsigned int *sse);
 unsigned int aom_highbd_10_obmc_sub_pixel_variance128x128_neon(const uint8_t *pre, int pre_stride, int xoffset, int yoffset, const int32_t *wsrc, const int32_t *mask, unsigned int *sse);
@@ -1281,91 +1287,113 @@ uint32_t aom_highbd_10_sub_pixel_variance8x8_neon(const uint8_t *src_ptr, int so
 
 unsigned int aom_highbd_10_variance128x128_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance128x128_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance128x128 aom_highbd_10_variance128x128_neon
+unsigned int aom_highbd_10_variance128x128_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance128x128)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance128x64_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance128x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance128x64 aom_highbd_10_variance128x64_neon
+unsigned int aom_highbd_10_variance128x64_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance128x64)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance16x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance16x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance16x16 aom_highbd_10_variance16x16_neon
+unsigned int aom_highbd_10_variance16x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance16x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance16x32_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance16x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance16x32 aom_highbd_10_variance16x32_neon
+unsigned int aom_highbd_10_variance16x32_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance16x32)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance16x4_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance16x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance16x4 aom_highbd_10_variance16x4_neon
+unsigned int aom_highbd_10_variance16x4_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance16x4)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance16x64_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance16x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance16x64 aom_highbd_10_variance16x64_neon
+unsigned int aom_highbd_10_variance16x64_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance16x64)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance16x8_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance16x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance16x8 aom_highbd_10_variance16x8_neon
+unsigned int aom_highbd_10_variance16x8_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance16x8)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance32x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance32x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance32x16 aom_highbd_10_variance32x16_neon
+unsigned int aom_highbd_10_variance32x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance32x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance32x32_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance32x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance32x32 aom_highbd_10_variance32x32_neon
+unsigned int aom_highbd_10_variance32x32_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance32x32)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance32x64_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance32x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance32x64 aom_highbd_10_variance32x64_neon
+unsigned int aom_highbd_10_variance32x64_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance32x64)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance32x8_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance32x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance32x8 aom_highbd_10_variance32x8_neon
+unsigned int aom_highbd_10_variance32x8_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance32x8)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance4x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance4x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance4x16 aom_highbd_10_variance4x16_neon
+unsigned int aom_highbd_10_variance4x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance4x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance4x4_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance4x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance4x4 aom_highbd_10_variance4x4_neon
+unsigned int aom_highbd_10_variance4x4_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance4x4)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance4x8_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance4x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance4x8 aom_highbd_10_variance4x8_neon
+unsigned int aom_highbd_10_variance4x8_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance4x8)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance64x128_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance64x128_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance64x128 aom_highbd_10_variance64x128_neon
+unsigned int aom_highbd_10_variance64x128_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance64x128)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance64x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance64x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance64x16 aom_highbd_10_variance64x16_neon
+unsigned int aom_highbd_10_variance64x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance64x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance64x32_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance64x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance64x32 aom_highbd_10_variance64x32_neon
+unsigned int aom_highbd_10_variance64x32_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance64x32)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance64x64_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance64x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance64x64 aom_highbd_10_variance64x64_neon
+unsigned int aom_highbd_10_variance64x64_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance64x64)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance8x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance8x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance8x16 aom_highbd_10_variance8x16_neon
+unsigned int aom_highbd_10_variance8x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance8x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance8x32_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance8x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance8x32 aom_highbd_10_variance8x32_neon
+unsigned int aom_highbd_10_variance8x32_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance8x32)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance8x4_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance8x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance8x4 aom_highbd_10_variance8x4_neon
+unsigned int aom_highbd_10_variance8x4_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance8x4)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_10_variance8x8_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_10_variance8x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_10_variance8x8 aom_highbd_10_variance8x8_neon
+unsigned int aom_highbd_10_variance8x8_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_10_variance8x8)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 uint32_t aom_highbd_12_dist_wtd_sub_pixel_avg_variance128x128_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse, const uint8_t *second_pred, const DIST_WTD_COMP_PARAMS* jcp_param);
 uint32_t aom_highbd_12_dist_wtd_sub_pixel_avg_variance128x128_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse, const uint8_t *second_pred, const DIST_WTD_COMP_PARAMS* jcp_param);
@@ -1545,19 +1573,23 @@ unsigned int aom_highbd_12_masked_sub_pixel_variance8x8_neon(const uint8_t *src,
 
 unsigned int aom_highbd_12_mse16x16_c(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 unsigned int aom_highbd_12_mse16x16_neon(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
-#define aom_highbd_12_mse16x16 aom_highbd_12_mse16x16_neon
+unsigned int aom_highbd_12_mse16x16_sve(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_mse16x16)(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 
 unsigned int aom_highbd_12_mse16x8_c(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 unsigned int aom_highbd_12_mse16x8_neon(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
-#define aom_highbd_12_mse16x8 aom_highbd_12_mse16x8_neon
+unsigned int aom_highbd_12_mse16x8_sve(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_mse16x8)(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 
 unsigned int aom_highbd_12_mse8x16_c(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 unsigned int aom_highbd_12_mse8x16_neon(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
-#define aom_highbd_12_mse8x16 aom_highbd_12_mse8x16_neon
+unsigned int aom_highbd_12_mse8x16_sve(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_mse8x16)(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 
 unsigned int aom_highbd_12_mse8x8_c(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 unsigned int aom_highbd_12_mse8x8_neon(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
-#define aom_highbd_12_mse8x8 aom_highbd_12_mse8x8_neon
+unsigned int aom_highbd_12_mse8x8_sve(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_mse8x8)(const uint8_t *src_ptr, int  source_stride, const uint8_t *ref_ptr, int  recon_stride, unsigned int *sse);
 
 unsigned int aom_highbd_12_obmc_sub_pixel_variance128x128_c(const uint8_t *pre, int pre_stride, int xoffset, int yoffset, const int32_t *wsrc, const int32_t *mask, unsigned int *sse);
 unsigned int aom_highbd_12_obmc_sub_pixel_variance128x128_neon(const uint8_t *pre, int pre_stride, int xoffset, int yoffset, const int32_t *wsrc, const int32_t *mask, unsigned int *sse);
@@ -1913,91 +1945,113 @@ uint32_t aom_highbd_12_sub_pixel_variance8x8_neon(const uint8_t *src_ptr, int so
 
 unsigned int aom_highbd_12_variance128x128_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance128x128_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance128x128 aom_highbd_12_variance128x128_neon
+unsigned int aom_highbd_12_variance128x128_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance128x128)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance128x64_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance128x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance128x64 aom_highbd_12_variance128x64_neon
+unsigned int aom_highbd_12_variance128x64_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance128x64)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance16x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance16x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance16x16 aom_highbd_12_variance16x16_neon
+unsigned int aom_highbd_12_variance16x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance16x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance16x32_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance16x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance16x32 aom_highbd_12_variance16x32_neon
+unsigned int aom_highbd_12_variance16x32_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance16x32)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance16x4_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance16x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance16x4 aom_highbd_12_variance16x4_neon
+unsigned int aom_highbd_12_variance16x4_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance16x4)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance16x64_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance16x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance16x64 aom_highbd_12_variance16x64_neon
+unsigned int aom_highbd_12_variance16x64_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance16x64)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance16x8_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance16x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance16x8 aom_highbd_12_variance16x8_neon
+unsigned int aom_highbd_12_variance16x8_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance16x8)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance32x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance32x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance32x16 aom_highbd_12_variance32x16_neon
+unsigned int aom_highbd_12_variance32x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance32x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance32x32_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance32x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance32x32 aom_highbd_12_variance32x32_neon
+unsigned int aom_highbd_12_variance32x32_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance32x32)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance32x64_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance32x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance32x64 aom_highbd_12_variance32x64_neon
+unsigned int aom_highbd_12_variance32x64_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance32x64)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance32x8_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance32x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance32x8 aom_highbd_12_variance32x8_neon
+unsigned int aom_highbd_12_variance32x8_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance32x8)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance4x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance4x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance4x16 aom_highbd_12_variance4x16_neon
+unsigned int aom_highbd_12_variance4x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance4x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance4x4_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance4x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance4x4 aom_highbd_12_variance4x4_neon
+unsigned int aom_highbd_12_variance4x4_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance4x4)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance4x8_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance4x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance4x8 aom_highbd_12_variance4x8_neon
+unsigned int aom_highbd_12_variance4x8_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance4x8)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance64x128_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance64x128_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance64x128 aom_highbd_12_variance64x128_neon
+unsigned int aom_highbd_12_variance64x128_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance64x128)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance64x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance64x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance64x16 aom_highbd_12_variance64x16_neon
+unsigned int aom_highbd_12_variance64x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance64x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance64x32_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance64x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance64x32 aom_highbd_12_variance64x32_neon
+unsigned int aom_highbd_12_variance64x32_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance64x32)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance64x64_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance64x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance64x64 aom_highbd_12_variance64x64_neon
+unsigned int aom_highbd_12_variance64x64_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance64x64)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance8x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance8x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance8x16 aom_highbd_12_variance8x16_neon
+unsigned int aom_highbd_12_variance8x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance8x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance8x32_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance8x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance8x32 aom_highbd_12_variance8x32_neon
+unsigned int aom_highbd_12_variance8x32_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance8x32)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance8x4_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance8x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance8x4 aom_highbd_12_variance8x4_neon
+unsigned int aom_highbd_12_variance8x4_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance8x4)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_12_variance8x8_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_12_variance8x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_12_variance8x8 aom_highbd_12_variance8x8_neon
+unsigned int aom_highbd_12_variance8x8_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_12_variance8x8)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 uint32_t aom_highbd_8_dist_wtd_sub_pixel_avg_variance128x128_c(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse, const uint8_t *second_pred, const DIST_WTD_COMP_PARAMS* jcp_param);
 uint32_t aom_highbd_8_dist_wtd_sub_pixel_avg_variance128x128_neon(const uint8_t *src_ptr, int source_stride, int xoffset, int  yoffset, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse, const uint8_t *second_pred, const DIST_WTD_COMP_PARAMS* jcp_param);
@@ -2549,91 +2603,113 @@ uint32_t aom_highbd_8_sub_pixel_variance8x8_neon(const uint8_t *src_ptr, int sou
 
 unsigned int aom_highbd_8_variance128x128_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance128x128_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance128x128 aom_highbd_8_variance128x128_neon
+unsigned int aom_highbd_8_variance128x128_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance128x128)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance128x64_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance128x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance128x64 aom_highbd_8_variance128x64_neon
+unsigned int aom_highbd_8_variance128x64_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance128x64)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance16x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance16x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance16x16 aom_highbd_8_variance16x16_neon
+unsigned int aom_highbd_8_variance16x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance16x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance16x32_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance16x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance16x32 aom_highbd_8_variance16x32_neon
+unsigned int aom_highbd_8_variance16x32_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance16x32)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance16x4_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance16x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance16x4 aom_highbd_8_variance16x4_neon
+unsigned int aom_highbd_8_variance16x4_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance16x4)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance16x64_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance16x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance16x64 aom_highbd_8_variance16x64_neon
+unsigned int aom_highbd_8_variance16x64_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance16x64)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance16x8_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance16x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance16x8 aom_highbd_8_variance16x8_neon
+unsigned int aom_highbd_8_variance16x8_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance16x8)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance32x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance32x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance32x16 aom_highbd_8_variance32x16_neon
+unsigned int aom_highbd_8_variance32x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance32x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance32x32_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance32x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance32x32 aom_highbd_8_variance32x32_neon
+unsigned int aom_highbd_8_variance32x32_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance32x32)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance32x64_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance32x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance32x64 aom_highbd_8_variance32x64_neon
+unsigned int aom_highbd_8_variance32x64_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance32x64)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance32x8_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance32x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance32x8 aom_highbd_8_variance32x8_neon
+unsigned int aom_highbd_8_variance32x8_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance32x8)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance4x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance4x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance4x16 aom_highbd_8_variance4x16_neon
+unsigned int aom_highbd_8_variance4x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance4x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance4x4_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance4x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance4x4 aom_highbd_8_variance4x4_neon
+unsigned int aom_highbd_8_variance4x4_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance4x4)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance4x8_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance4x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance4x8 aom_highbd_8_variance4x8_neon
+unsigned int aom_highbd_8_variance4x8_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance4x8)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance64x128_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance64x128_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance64x128 aom_highbd_8_variance64x128_neon
+unsigned int aom_highbd_8_variance64x128_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance64x128)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance64x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance64x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance64x16 aom_highbd_8_variance64x16_neon
+unsigned int aom_highbd_8_variance64x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance64x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance64x32_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance64x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance64x32 aom_highbd_8_variance64x32_neon
+unsigned int aom_highbd_8_variance64x32_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance64x32)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance64x64_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance64x64_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance64x64 aom_highbd_8_variance64x64_neon
+unsigned int aom_highbd_8_variance64x64_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance64x64)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance8x16_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance8x16_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance8x16 aom_highbd_8_variance8x16_neon
+unsigned int aom_highbd_8_variance8x16_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance8x16)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance8x32_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance8x32_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance8x32 aom_highbd_8_variance8x32_neon
+unsigned int aom_highbd_8_variance8x32_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance8x32)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance8x4_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance8x4_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance8x4 aom_highbd_8_variance8x4_neon
+unsigned int aom_highbd_8_variance8x4_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance8x4)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_8_variance8x8_c(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 unsigned int aom_highbd_8_variance8x8_neon(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
-#define aom_highbd_8_variance8x8 aom_highbd_8_variance8x8_neon
+unsigned int aom_highbd_8_variance8x8_sve(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
+RTCD_EXTERN unsigned int (*aom_highbd_8_variance8x8)(const uint8_t *src_ptr, int source_stride, const uint8_t *ref_ptr, int ref_stride, uint32_t *sse);
 
 unsigned int aom_highbd_avg_4x4_c(const uint8_t *, int p);
 unsigned int aom_highbd_avg_4x4_neon(const uint8_t *, int p);
@@ -2669,11 +2745,13 @@ void aom_highbd_comp_mask_pred_neon(uint8_t *comp_pred, const uint8_t *pred8, in
 
 void aom_highbd_convolve8_horiz_c(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, int bd);
 void aom_highbd_convolve8_horiz_neon(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, int bd);
-#define aom_highbd_convolve8_horiz aom_highbd_convolve8_horiz_neon
+void aom_highbd_convolve8_horiz_sve(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, int bd);
+RTCD_EXTERN void (*aom_highbd_convolve8_horiz)(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, int bd);
 
 void aom_highbd_convolve8_vert_c(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, int bd);
 void aom_highbd_convolve8_vert_neon(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, int bd);
-#define aom_highbd_convolve8_vert aom_highbd_convolve8_vert_neon
+void aom_highbd_convolve8_vert_sve(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, int bd);
+RTCD_EXTERN void (*aom_highbd_convolve8_vert)(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, int bd);
 
 void aom_highbd_convolve_copy_c(const uint16_t *src, ptrdiff_t src_stride, uint16_t *dst, ptrdiff_t dst_stride, int w, int h);
 void aom_highbd_convolve_copy_neon(const uint16_t *src, ptrdiff_t src_stride, uint16_t *dst, ptrdiff_t dst_stride, int w, int h);
@@ -4243,7 +4321,8 @@ void aom_highbd_smooth_v_predictor_8x8_neon(uint16_t *dst, ptrdiff_t y_stride, c
 
 int64_t aom_highbd_sse_c(const uint8_t *a8, int a_stride, const uint8_t *b8,int b_stride, int width, int height);
 int64_t aom_highbd_sse_neon(const uint8_t *a8, int a_stride, const uint8_t *b8,int b_stride, int width, int height);
-#define aom_highbd_sse aom_highbd_sse_neon
+int64_t aom_highbd_sse_sve(const uint8_t *a8, int a_stride, const uint8_t *b8,int b_stride, int width, int height);
+RTCD_EXTERN int64_t (*aom_highbd_sse)(const uint8_t *a8, int a_stride, const uint8_t *b8,int b_stride, int width, int height);
 
 void aom_highbd_ssim_parms_8x8_c(const uint16_t *s, int sp, const uint16_t *r, int rp, uint32_t *sum_s, uint32_t *sum_r, uint32_t *sum_sq_s, uint32_t *sum_sq_r, uint32_t *sum_sxr);
 #define aom_highbd_ssim_parms_8x8 aom_highbd_ssim_parms_8x8_c
@@ -4661,7 +4740,8 @@ uint64_t aom_mse_wxh_16bit_neon(uint8_t *dst, int dstride,uint16_t *src, int sst
 
 uint64_t aom_mse_wxh_16bit_highbd_c(uint16_t *dst, int dstride,uint16_t *src, int sstride, int w, int h);
 uint64_t aom_mse_wxh_16bit_highbd_neon(uint16_t *dst, int dstride,uint16_t *src, int sstride, int w, int h);
-#define aom_mse_wxh_16bit_highbd aom_mse_wxh_16bit_highbd_neon
+uint64_t aom_mse_wxh_16bit_highbd_sve(uint16_t *dst, int dstride,uint16_t *src, int sstride, int w, int h);
+RTCD_EXTERN uint64_t (*aom_mse_wxh_16bit_highbd)(uint16_t *dst, int dstride,uint16_t *src, int sstride, int w, int h);
 
 unsigned int aom_obmc_sad128x128_c(const uint8_t *pre, int pre_stride, const int32_t *wsrc, const int32_t *mask);
 unsigned int aom_obmc_sad128x128_neon(const uint8_t *pre, int pre_stride, const int32_t *wsrc, const int32_t *mask);
@@ -6074,15 +6154,18 @@ void aom_subtract_block_neon(int rows, int cols, int16_t *diff_ptr, ptrdiff_t di
 
 uint64_t aom_sum_squares_2d_i16_c(const int16_t *src, int stride, int width, int height);
 uint64_t aom_sum_squares_2d_i16_neon(const int16_t *src, int stride, int width, int height);
-#define aom_sum_squares_2d_i16 aom_sum_squares_2d_i16_neon
+uint64_t aom_sum_squares_2d_i16_sve(const int16_t *src, int stride, int width, int height);
+RTCD_EXTERN uint64_t (*aom_sum_squares_2d_i16)(const int16_t *src, int stride, int width, int height);
 
 uint64_t aom_sum_squares_i16_c(const int16_t *src, uint32_t N);
 uint64_t aom_sum_squares_i16_neon(const int16_t *src, uint32_t N);
-#define aom_sum_squares_i16 aom_sum_squares_i16_neon
+uint64_t aom_sum_squares_i16_sve(const int16_t *src, uint32_t N);
+RTCD_EXTERN uint64_t (*aom_sum_squares_i16)(const int16_t *src, uint32_t N);
 
 uint64_t aom_sum_sse_2d_i16_c(const int16_t *src, int src_stride, int width, int height, int *sum);
 uint64_t aom_sum_sse_2d_i16_neon(const int16_t *src, int src_stride, int width, int height, int *sum);
-#define aom_sum_sse_2d_i16 aom_sum_sse_2d_i16_neon
+uint64_t aom_sum_sse_2d_i16_sve(const int16_t *src, int src_stride, int width, int height, int *sum);
+RTCD_EXTERN uint64_t (*aom_sum_sse_2d_i16)(const int16_t *src, int src_stride, int width, int height, int *sum);
 
 void aom_v_predictor_16x16_c(uint8_t *dst, ptrdiff_t y_stride, const uint8_t *above, const uint8_t *left);
 void aom_v_predictor_16x16_neon(uint8_t *dst, ptrdiff_t y_stride, const uint8_t *above, const uint8_t *left);
@@ -6162,7 +6245,8 @@ void aom_v_predictor_8x8_neon(uint8_t *dst, ptrdiff_t y_stride, const uint8_t *a
 
 uint64_t aom_var_2d_u16_c(uint8_t *src, int src_stride, int width, int height);
 uint64_t aom_var_2d_u16_neon(uint8_t *src, int src_stride, int width, int height);
-#define aom_var_2d_u16 aom_var_2d_u16_neon
+uint64_t aom_var_2d_u16_sve(uint8_t *src, int src_stride, int width, int height);
+RTCD_EXTERN uint64_t (*aom_var_2d_u16)(uint8_t *src, int src_stride, int width, int height);
 
 uint64_t aom_var_2d_u8_c(uint8_t *src, int src_stride, int width, int height);
 uint64_t aom_var_2d_u8_neon(uint8_t *src, int src_stride, int width, int height);
@@ -6281,7 +6365,8 @@ RTCD_EXTERN unsigned int (*aom_variance8x8)(const uint8_t *src_ptr, int source_s
 
 int aom_vector_var_c(const int16_t *ref, const int16_t *src, int bwl);
 int aom_vector_var_neon(const int16_t *ref, const int16_t *src, int bwl);
-#define aom_vector_var aom_vector_var_neon
+int aom_vector_var_sve(const int16_t *ref, const int16_t *src, int bwl);
+RTCD_EXTERN int (*aom_vector_var)(const int16_t *ref, const int16_t *src, int bwl);
 
 void aom_dsp_rtcd(void);
 
@@ -6295,6 +6380,8 @@ static void setup_rtcd_internal(void)
 
     (void)flags;
 
+    aom_compute_flow_at_point = aom_compute_flow_at_point_neon;
+    if (flags & HAS_SVE) aom_compute_flow_at_point = aom_compute_flow_at_point_sve;
     aom_convolve8_horiz = aom_convolve8_horiz_neon;
     if (flags & HAS_NEON_DOTPROD) aom_convolve8_horiz = aom_convolve8_horiz_neon_dotprod;
     if (flags & HAS_NEON_I8MM) aom_convolve8_horiz = aom_convolve8_horiz_neon_i8mm;
@@ -6331,10 +6418,116 @@ static void setup_rtcd_internal(void)
     if (flags & HAS_NEON_DOTPROD) aom_dist_wtd_sad64x32_avg = aom_dist_wtd_sad64x32_avg_neon_dotprod;
     aom_dist_wtd_sad64x64_avg = aom_dist_wtd_sad64x64_avg_neon;
     if (flags & HAS_NEON_DOTPROD) aom_dist_wtd_sad64x64_avg = aom_dist_wtd_sad64x64_avg_neon_dotprod;
+    aom_get_blk_sse_sum = aom_get_blk_sse_sum_neon;
+    if (flags & HAS_SVE) aom_get_blk_sse_sum = aom_get_blk_sse_sum_sve;
     aom_get_var_sse_sum_16x16_dual = aom_get_var_sse_sum_16x16_dual_neon;
     if (flags & HAS_NEON_DOTPROD) aom_get_var_sse_sum_16x16_dual = aom_get_var_sse_sum_16x16_dual_neon_dotprod;
     aom_get_var_sse_sum_8x8_quad = aom_get_var_sse_sum_8x8_quad_neon;
     if (flags & HAS_NEON_DOTPROD) aom_get_var_sse_sum_8x8_quad = aom_get_var_sse_sum_8x8_quad_neon_dotprod;
+    aom_highbd_10_mse16x16 = aom_highbd_10_mse16x16_neon;
+    if (flags & HAS_SVE) aom_highbd_10_mse16x16 = aom_highbd_10_mse16x16_sve;
+    aom_highbd_10_mse16x8 = aom_highbd_10_mse16x8_neon;
+    if (flags & HAS_SVE) aom_highbd_10_mse16x8 = aom_highbd_10_mse16x8_sve;
+    aom_highbd_10_mse8x16 = aom_highbd_10_mse8x16_neon;
+    if (flags & HAS_SVE) aom_highbd_10_mse8x16 = aom_highbd_10_mse8x16_sve;
+    aom_highbd_10_mse8x8 = aom_highbd_10_mse8x8_neon;
+    if (flags & HAS_SVE) aom_highbd_10_mse8x8 = aom_highbd_10_mse8x8_sve;
+    aom_highbd_10_variance128x128 = aom_highbd_10_variance128x128_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance128x128 = aom_highbd_10_variance128x128_sve;
+    aom_highbd_10_variance128x64 = aom_highbd_10_variance128x64_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance128x64 = aom_highbd_10_variance128x64_sve;
+    aom_highbd_10_variance16x16 = aom_highbd_10_variance16x16_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance16x16 = aom_highbd_10_variance16x16_sve;
+    aom_highbd_10_variance16x32 = aom_highbd_10_variance16x32_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance16x32 = aom_highbd_10_variance16x32_sve;
+    aom_highbd_10_variance16x4 = aom_highbd_10_variance16x4_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance16x4 = aom_highbd_10_variance16x4_sve;
+    aom_highbd_10_variance16x64 = aom_highbd_10_variance16x64_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance16x64 = aom_highbd_10_variance16x64_sve;
+    aom_highbd_10_variance16x8 = aom_highbd_10_variance16x8_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance16x8 = aom_highbd_10_variance16x8_sve;
+    aom_highbd_10_variance32x16 = aom_highbd_10_variance32x16_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance32x16 = aom_highbd_10_variance32x16_sve;
+    aom_highbd_10_variance32x32 = aom_highbd_10_variance32x32_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance32x32 = aom_highbd_10_variance32x32_sve;
+    aom_highbd_10_variance32x64 = aom_highbd_10_variance32x64_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance32x64 = aom_highbd_10_variance32x64_sve;
+    aom_highbd_10_variance32x8 = aom_highbd_10_variance32x8_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance32x8 = aom_highbd_10_variance32x8_sve;
+    aom_highbd_10_variance4x16 = aom_highbd_10_variance4x16_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance4x16 = aom_highbd_10_variance4x16_sve;
+    aom_highbd_10_variance4x4 = aom_highbd_10_variance4x4_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance4x4 = aom_highbd_10_variance4x4_sve;
+    aom_highbd_10_variance4x8 = aom_highbd_10_variance4x8_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance4x8 = aom_highbd_10_variance4x8_sve;
+    aom_highbd_10_variance64x128 = aom_highbd_10_variance64x128_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance64x128 = aom_highbd_10_variance64x128_sve;
+    aom_highbd_10_variance64x16 = aom_highbd_10_variance64x16_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance64x16 = aom_highbd_10_variance64x16_sve;
+    aom_highbd_10_variance64x32 = aom_highbd_10_variance64x32_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance64x32 = aom_highbd_10_variance64x32_sve;
+    aom_highbd_10_variance64x64 = aom_highbd_10_variance64x64_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance64x64 = aom_highbd_10_variance64x64_sve;
+    aom_highbd_10_variance8x16 = aom_highbd_10_variance8x16_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance8x16 = aom_highbd_10_variance8x16_sve;
+    aom_highbd_10_variance8x32 = aom_highbd_10_variance8x32_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance8x32 = aom_highbd_10_variance8x32_sve;
+    aom_highbd_10_variance8x4 = aom_highbd_10_variance8x4_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance8x4 = aom_highbd_10_variance8x4_sve;
+    aom_highbd_10_variance8x8 = aom_highbd_10_variance8x8_neon;
+    if (flags & HAS_SVE) aom_highbd_10_variance8x8 = aom_highbd_10_variance8x8_sve;
+    aom_highbd_12_mse16x16 = aom_highbd_12_mse16x16_neon;
+    if (flags & HAS_SVE) aom_highbd_12_mse16x16 = aom_highbd_12_mse16x16_sve;
+    aom_highbd_12_mse16x8 = aom_highbd_12_mse16x8_neon;
+    if (flags & HAS_SVE) aom_highbd_12_mse16x8 = aom_highbd_12_mse16x8_sve;
+    aom_highbd_12_mse8x16 = aom_highbd_12_mse8x16_neon;
+    if (flags & HAS_SVE) aom_highbd_12_mse8x16 = aom_highbd_12_mse8x16_sve;
+    aom_highbd_12_mse8x8 = aom_highbd_12_mse8x8_neon;
+    if (flags & HAS_SVE) aom_highbd_12_mse8x8 = aom_highbd_12_mse8x8_sve;
+    aom_highbd_12_variance128x128 = aom_highbd_12_variance128x128_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance128x128 = aom_highbd_12_variance128x128_sve;
+    aom_highbd_12_variance128x64 = aom_highbd_12_variance128x64_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance128x64 = aom_highbd_12_variance128x64_sve;
+    aom_highbd_12_variance16x16 = aom_highbd_12_variance16x16_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance16x16 = aom_highbd_12_variance16x16_sve;
+    aom_highbd_12_variance16x32 = aom_highbd_12_variance16x32_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance16x32 = aom_highbd_12_variance16x32_sve;
+    aom_highbd_12_variance16x4 = aom_highbd_12_variance16x4_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance16x4 = aom_highbd_12_variance16x4_sve;
+    aom_highbd_12_variance16x64 = aom_highbd_12_variance16x64_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance16x64 = aom_highbd_12_variance16x64_sve;
+    aom_highbd_12_variance16x8 = aom_highbd_12_variance16x8_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance16x8 = aom_highbd_12_variance16x8_sve;
+    aom_highbd_12_variance32x16 = aom_highbd_12_variance32x16_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance32x16 = aom_highbd_12_variance32x16_sve;
+    aom_highbd_12_variance32x32 = aom_highbd_12_variance32x32_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance32x32 = aom_highbd_12_variance32x32_sve;
+    aom_highbd_12_variance32x64 = aom_highbd_12_variance32x64_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance32x64 = aom_highbd_12_variance32x64_sve;
+    aom_highbd_12_variance32x8 = aom_highbd_12_variance32x8_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance32x8 = aom_highbd_12_variance32x8_sve;
+    aom_highbd_12_variance4x16 = aom_highbd_12_variance4x16_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance4x16 = aom_highbd_12_variance4x16_sve;
+    aom_highbd_12_variance4x4 = aom_highbd_12_variance4x4_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance4x4 = aom_highbd_12_variance4x4_sve;
+    aom_highbd_12_variance4x8 = aom_highbd_12_variance4x8_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance4x8 = aom_highbd_12_variance4x8_sve;
+    aom_highbd_12_variance64x128 = aom_highbd_12_variance64x128_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance64x128 = aom_highbd_12_variance64x128_sve;
+    aom_highbd_12_variance64x16 = aom_highbd_12_variance64x16_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance64x16 = aom_highbd_12_variance64x16_sve;
+    aom_highbd_12_variance64x32 = aom_highbd_12_variance64x32_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance64x32 = aom_highbd_12_variance64x32_sve;
+    aom_highbd_12_variance64x64 = aom_highbd_12_variance64x64_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance64x64 = aom_highbd_12_variance64x64_sve;
+    aom_highbd_12_variance8x16 = aom_highbd_12_variance8x16_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance8x16 = aom_highbd_12_variance8x16_sve;
+    aom_highbd_12_variance8x32 = aom_highbd_12_variance8x32_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance8x32 = aom_highbd_12_variance8x32_sve;
+    aom_highbd_12_variance8x4 = aom_highbd_12_variance8x4_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance8x4 = aom_highbd_12_variance8x4_sve;
+    aom_highbd_12_variance8x8 = aom_highbd_12_variance8x8_neon;
+    if (flags & HAS_SVE) aom_highbd_12_variance8x8 = aom_highbd_12_variance8x8_sve;
     aom_highbd_8_mse16x16 = aom_highbd_8_mse16x16_neon;
     if (flags & HAS_NEON_DOTPROD) aom_highbd_8_mse16x16 = aom_highbd_8_mse16x16_neon_dotprod;
     aom_highbd_8_mse16x8 = aom_highbd_8_mse16x8_neon;
@@ -6343,6 +6536,56 @@ static void setup_rtcd_internal(void)
     if (flags & HAS_NEON_DOTPROD) aom_highbd_8_mse8x16 = aom_highbd_8_mse8x16_neon_dotprod;
     aom_highbd_8_mse8x8 = aom_highbd_8_mse8x8_neon;
     if (flags & HAS_NEON_DOTPROD) aom_highbd_8_mse8x8 = aom_highbd_8_mse8x8_neon_dotprod;
+    aom_highbd_8_variance128x128 = aom_highbd_8_variance128x128_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance128x128 = aom_highbd_8_variance128x128_sve;
+    aom_highbd_8_variance128x64 = aom_highbd_8_variance128x64_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance128x64 = aom_highbd_8_variance128x64_sve;
+    aom_highbd_8_variance16x16 = aom_highbd_8_variance16x16_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance16x16 = aom_highbd_8_variance16x16_sve;
+    aom_highbd_8_variance16x32 = aom_highbd_8_variance16x32_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance16x32 = aom_highbd_8_variance16x32_sve;
+    aom_highbd_8_variance16x4 = aom_highbd_8_variance16x4_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance16x4 = aom_highbd_8_variance16x4_sve;
+    aom_highbd_8_variance16x64 = aom_highbd_8_variance16x64_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance16x64 = aom_highbd_8_variance16x64_sve;
+    aom_highbd_8_variance16x8 = aom_highbd_8_variance16x8_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance16x8 = aom_highbd_8_variance16x8_sve;
+    aom_highbd_8_variance32x16 = aom_highbd_8_variance32x16_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance32x16 = aom_highbd_8_variance32x16_sve;
+    aom_highbd_8_variance32x32 = aom_highbd_8_variance32x32_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance32x32 = aom_highbd_8_variance32x32_sve;
+    aom_highbd_8_variance32x64 = aom_highbd_8_variance32x64_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance32x64 = aom_highbd_8_variance32x64_sve;
+    aom_highbd_8_variance32x8 = aom_highbd_8_variance32x8_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance32x8 = aom_highbd_8_variance32x8_sve;
+    aom_highbd_8_variance4x16 = aom_highbd_8_variance4x16_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance4x16 = aom_highbd_8_variance4x16_sve;
+    aom_highbd_8_variance4x4 = aom_highbd_8_variance4x4_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance4x4 = aom_highbd_8_variance4x4_sve;
+    aom_highbd_8_variance4x8 = aom_highbd_8_variance4x8_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance4x8 = aom_highbd_8_variance4x8_sve;
+    aom_highbd_8_variance64x128 = aom_highbd_8_variance64x128_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance64x128 = aom_highbd_8_variance64x128_sve;
+    aom_highbd_8_variance64x16 = aom_highbd_8_variance64x16_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance64x16 = aom_highbd_8_variance64x16_sve;
+    aom_highbd_8_variance64x32 = aom_highbd_8_variance64x32_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance64x32 = aom_highbd_8_variance64x32_sve;
+    aom_highbd_8_variance64x64 = aom_highbd_8_variance64x64_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance64x64 = aom_highbd_8_variance64x64_sve;
+    aom_highbd_8_variance8x16 = aom_highbd_8_variance8x16_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance8x16 = aom_highbd_8_variance8x16_sve;
+    aom_highbd_8_variance8x32 = aom_highbd_8_variance8x32_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance8x32 = aom_highbd_8_variance8x32_sve;
+    aom_highbd_8_variance8x4 = aom_highbd_8_variance8x4_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance8x4 = aom_highbd_8_variance8x4_sve;
+    aom_highbd_8_variance8x8 = aom_highbd_8_variance8x8_neon;
+    if (flags & HAS_SVE) aom_highbd_8_variance8x8 = aom_highbd_8_variance8x8_sve;
+    aom_highbd_convolve8_horiz = aom_highbd_convolve8_horiz_neon;
+    if (flags & HAS_SVE) aom_highbd_convolve8_horiz = aom_highbd_convolve8_horiz_sve;
+    aom_highbd_convolve8_vert = aom_highbd_convolve8_vert_neon;
+    if (flags & HAS_SVE) aom_highbd_convolve8_vert = aom_highbd_convolve8_vert_sve;
+    aom_highbd_sse = aom_highbd_sse_neon;
+    if (flags & HAS_SVE) aom_highbd_sse = aom_highbd_sse_sve;
     aom_mse16x16 = aom_mse16x16_neon;
     if (flags & HAS_NEON_DOTPROD) aom_mse16x16 = aom_mse16x16_neon_dotprod;
     aom_mse16x8 = aom_mse16x8_neon;
@@ -6351,6 +6594,8 @@ static void setup_rtcd_internal(void)
     if (flags & HAS_NEON_DOTPROD) aom_mse8x16 = aom_mse8x16_neon_dotprod;
     aom_mse8x8 = aom_mse8x8_neon;
     if (flags & HAS_NEON_DOTPROD) aom_mse8x8 = aom_mse8x8_neon_dotprod;
+    aom_mse_wxh_16bit_highbd = aom_mse_wxh_16bit_highbd_neon;
+    if (flags & HAS_SVE) aom_mse_wxh_16bit_highbd = aom_mse_wxh_16bit_highbd_sve;
     aom_sad128x128 = aom_sad128x128_neon;
     if (flags & HAS_NEON_DOTPROD) aom_sad128x128 = aom_sad128x128_neon_dotprod;
     aom_sad128x128_avg = aom_sad128x128_avg_neon;
@@ -6536,6 +6781,14 @@ static void setup_rtcd_internal(void)
     if (flags & HAS_NEON_I8MM) aom_scaled_2d = aom_scaled_2d_neon_i8mm;
     aom_sse = aom_sse_neon;
     if (flags & HAS_NEON_DOTPROD) aom_sse = aom_sse_neon_dotprod;
+    aom_sum_squares_2d_i16 = aom_sum_squares_2d_i16_neon;
+    if (flags & HAS_SVE) aom_sum_squares_2d_i16 = aom_sum_squares_2d_i16_sve;
+    aom_sum_squares_i16 = aom_sum_squares_i16_neon;
+    if (flags & HAS_SVE) aom_sum_squares_i16 = aom_sum_squares_i16_sve;
+    aom_sum_sse_2d_i16 = aom_sum_sse_2d_i16_neon;
+    if (flags & HAS_SVE) aom_sum_sse_2d_i16 = aom_sum_sse_2d_i16_sve;
+    aom_var_2d_u16 = aom_var_2d_u16_neon;
+    if (flags & HAS_SVE) aom_var_2d_u16 = aom_var_2d_u16_sve;
     aom_var_2d_u8 = aom_var_2d_u8_neon;
     if (flags & HAS_NEON_DOTPROD) aom_var_2d_u8 = aom_var_2d_u8_neon_dotprod;
     aom_variance128x128 = aom_variance128x128_neon;
@@ -6582,6 +6835,8 @@ static void setup_rtcd_internal(void)
     if (flags & HAS_NEON_DOTPROD) aom_variance8x4 = aom_variance8x4_neon_dotprod;
     aom_variance8x8 = aom_variance8x8_neon;
     if (flags & HAS_NEON_DOTPROD) aom_variance8x8 = aom_variance8x8_neon_dotprod;
+    aom_vector_var = aom_vector_var_neon;
+    if (flags & HAS_SVE) aom_vector_var = aom_vector_var_sve;
 }
 #endif
 
@@ -6589,4 +6844,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AOM_DSP_RTCD_H_
diff --git a/config/arm64/config/aom_scale_rtcd.h b/config/arm64/config/aom_scale_rtcd.h
index ea025ad41..3eaffa813 100644
--- a/config/arm64/config/aom_scale_rtcd.h
+++ b/config/arm64/config/aom_scale_rtcd.h
@@ -88,4 +88,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AOM_SCALE_RTCD_H_
diff --git a/config/arm64/config/av1_rtcd.h b/config/arm64/config/av1_rtcd.h
index e77d564b1..1b26ba4fb 100644
--- a/config/arm64/config/av1_rtcd.h
+++ b/config/arm64/config/av1_rtcd.h
@@ -146,11 +146,13 @@ RTCD_EXTERN void (*av1_apply_temporal_filter)(const struct yv12_buffer_config *f
 
 int64_t av1_block_error_c(const tran_low_t *coeff, const tran_low_t *dqcoeff, intptr_t block_size, int64_t *ssz);
 int64_t av1_block_error_neon(const tran_low_t *coeff, const tran_low_t *dqcoeff, intptr_t block_size, int64_t *ssz);
-#define av1_block_error av1_block_error_neon
+int64_t av1_block_error_sve(const tran_low_t *coeff, const tran_low_t *dqcoeff, intptr_t block_size, int64_t *ssz);
+RTCD_EXTERN int64_t (*av1_block_error)(const tran_low_t *coeff, const tran_low_t *dqcoeff, intptr_t block_size, int64_t *ssz);
 
 int64_t av1_block_error_lp_c(const int16_t *coeff, const int16_t *dqcoeff, intptr_t block_size);
 int64_t av1_block_error_lp_neon(const int16_t *coeff, const int16_t *dqcoeff, intptr_t block_size);
-#define av1_block_error_lp av1_block_error_lp_neon
+int64_t av1_block_error_lp_sve(const int16_t *coeff, const int16_t *dqcoeff, intptr_t block_size);
+RTCD_EXTERN int64_t (*av1_block_error_lp)(const int16_t *coeff, const int16_t *dqcoeff, intptr_t block_size);
 
 void av1_build_compound_diffwtd_mask_c(uint8_t *mask, DIFFWTD_MASK_TYPE mask_type, const uint8_t *src0, int src0_stride, const uint8_t *src1, int src1_stride, int h, int w);
 void av1_build_compound_diffwtd_mask_neon(uint8_t *mask, DIFFWTD_MASK_TYPE mask_type, const uint8_t *src0, int src0_stride, const uint8_t *src1, int src1_stride, int h, int w);
@@ -201,11 +203,13 @@ bool av1_cnn_predict_c(const float **input, int in_width, int in_height, int in_
 
 void av1_compute_stats_c(int wiener_win, const uint8_t *dgd8, const uint8_t *src8, int16_t *dgd_avg, int16_t *src_avg, int h_start, int h_end, int v_start, int v_end, int dgd_stride, int src_stride, int64_t *M, int64_t *H, int use_downsampled_wiener_stats);
 void av1_compute_stats_neon(int wiener_win, const uint8_t *dgd8, const uint8_t *src8, int16_t *dgd_avg, int16_t *src_avg, int h_start, int h_end, int v_start, int v_end, int dgd_stride, int src_stride, int64_t *M, int64_t *H, int use_downsampled_wiener_stats);
-#define av1_compute_stats av1_compute_stats_neon
+void av1_compute_stats_sve(int wiener_win, const uint8_t *dgd8, const uint8_t *src8, int16_t *dgd_avg, int16_t *src_avg, int h_start, int h_end, int v_start, int v_end, int dgd_stride, int src_stride, int64_t *M, int64_t *H, int use_downsampled_wiener_stats);
+RTCD_EXTERN void (*av1_compute_stats)(int wiener_win, const uint8_t *dgd8, const uint8_t *src8, int16_t *dgd_avg, int16_t *src_avg, int h_start, int h_end, int v_start, int v_end, int dgd_stride, int src_stride, int64_t *M, int64_t *H, int use_downsampled_wiener_stats);
 
 void av1_compute_stats_highbd_c(int wiener_win, const uint8_t *dgd8, const uint8_t *src8, int16_t *dgd_avg, int16_t *src_avg, int h_start, int h_end, int v_start, int v_end, int dgd_stride, int src_stride, int64_t *M, int64_t *H, aom_bit_depth_t bit_depth);
 void av1_compute_stats_highbd_neon(int wiener_win, const uint8_t *dgd8, const uint8_t *src8, int16_t *dgd_avg, int16_t *src_avg, int h_start, int h_end, int v_start, int v_end, int dgd_stride, int src_stride, int64_t *M, int64_t *H, aom_bit_depth_t bit_depth);
-#define av1_compute_stats_highbd av1_compute_stats_highbd_neon
+void av1_compute_stats_highbd_sve(int wiener_win, const uint8_t *dgd8, const uint8_t *src8, int16_t *dgd_avg, int16_t *src_avg, int h_start, int h_end, int v_start, int v_end, int dgd_stride, int src_stride, int64_t *M, int64_t *H, aom_bit_depth_t bit_depth);
+RTCD_EXTERN void (*av1_compute_stats_highbd)(int wiener_win, const uint8_t *dgd8, const uint8_t *src8, int16_t *dgd_avg, int16_t *src_avg, int h_start, int h_end, int v_start, int v_end, int dgd_stride, int src_stride, int64_t *M, int64_t *H, aom_bit_depth_t bit_depth);
 
 void av1_convolve_2d_scale_c(const uint8_t *src, int src_stride, uint8_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int x_step_qn, const int subpel_y_qn, const int y_step_qn, ConvolveParams *conv_params);
 void av1_convolve_2d_scale_neon(const uint8_t *src, int src_stride, uint8_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int x_step_qn, const int subpel_y_qn, const int y_step_qn, ConvolveParams *conv_params);
@@ -217,6 +221,7 @@ void av1_convolve_2d_sr_c(const uint8_t *src, int src_stride, uint8_t *dst, int
 void av1_convolve_2d_sr_neon(const uint8_t *src, int src_stride, uint8_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params);
 void av1_convolve_2d_sr_neon_dotprod(const uint8_t *src, int src_stride, uint8_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params);
 void av1_convolve_2d_sr_neon_i8mm(const uint8_t *src, int src_stride, uint8_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params);
+void av1_convolve_2d_sr_sve2(const uint8_t *src, int src_stride, uint8_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params);
 RTCD_EXTERN void (*av1_convolve_2d_sr)(const uint8_t *src, int src_stride, uint8_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params);
 
 void av1_convolve_2d_sr_intrabc_c(const uint8_t *src, int src_stride, uint8_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params);
@@ -409,7 +414,8 @@ void av1_highbd_convolve_2d_scale_neon(const uint16_t *src, int src_stride, uint
 
 void av1_highbd_convolve_2d_sr_c(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params, int bd);
 void av1_highbd_convolve_2d_sr_neon(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params, int bd);
-#define av1_highbd_convolve_2d_sr av1_highbd_convolve_2d_sr_neon
+void av1_highbd_convolve_2d_sr_sve2(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params, int bd);
+RTCD_EXTERN void (*av1_highbd_convolve_2d_sr)(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params, int bd);
 
 void av1_highbd_convolve_2d_sr_intrabc_c(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params, int bd);
 void av1_highbd_convolve_2d_sr_intrabc_neon(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params, int bd);
@@ -427,7 +433,8 @@ void av1_highbd_convolve_horiz_rs_neon(const uint16_t *src, int src_stride, uint
 
 void av1_highbd_convolve_x_sr_c(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const int subpel_x_qn, ConvolveParams *conv_params, int bd);
 void av1_highbd_convolve_x_sr_neon(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const int subpel_x_qn, ConvolveParams *conv_params, int bd);
-#define av1_highbd_convolve_x_sr av1_highbd_convolve_x_sr_neon
+void av1_highbd_convolve_x_sr_sve2(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const int subpel_x_qn, ConvolveParams *conv_params, int bd);
+RTCD_EXTERN void (*av1_highbd_convolve_x_sr)(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const int subpel_x_qn, ConvolveParams *conv_params, int bd);
 
 void av1_highbd_convolve_x_sr_intrabc_c(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const int subpel_x_qn, ConvolveParams *conv_params, int bd);
 void av1_highbd_convolve_x_sr_intrabc_neon(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const int subpel_x_qn, ConvolveParams *conv_params, int bd);
@@ -435,7 +442,8 @@ void av1_highbd_convolve_x_sr_intrabc_neon(const uint16_t *src, int src_stride,
 
 void av1_highbd_convolve_y_sr_c(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_y, const int subpel_y_qn, int bd);
 void av1_highbd_convolve_y_sr_neon(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_y, const int subpel_y_qn, int bd);
-#define av1_highbd_convolve_y_sr av1_highbd_convolve_y_sr_neon
+void av1_highbd_convolve_y_sr_sve2(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_y, const int subpel_y_qn, int bd);
+RTCD_EXTERN void (*av1_highbd_convolve_y_sr)(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_y, const int subpel_y_qn, int bd);
 
 void av1_highbd_convolve_y_sr_intrabc_c(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_y, const int subpel_y_qn, int bd);
 void av1_highbd_convolve_y_sr_intrabc_neon(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_y, const int subpel_y_qn, int bd);
@@ -443,7 +451,8 @@ void av1_highbd_convolve_y_sr_intrabc_neon(const uint16_t *src, int src_stride,
 
 void av1_highbd_dist_wtd_convolve_2d_c(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params, int bd);
 void av1_highbd_dist_wtd_convolve_2d_neon(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params, int bd);
-#define av1_highbd_dist_wtd_convolve_2d av1_highbd_dist_wtd_convolve_2d_neon
+void av1_highbd_dist_wtd_convolve_2d_sve2(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params, int bd);
+RTCD_EXTERN void (*av1_highbd_dist_wtd_convolve_2d)(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const InterpFilterParams *filter_params_y, const int subpel_x_qn, const int subpel_y_qn, ConvolveParams *conv_params, int bd);
 
 void av1_highbd_dist_wtd_convolve_2d_copy_c(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, ConvolveParams *conv_params, int bd);
 void av1_highbd_dist_wtd_convolve_2d_copy_neon(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, ConvolveParams *conv_params, int bd);
@@ -451,11 +460,13 @@ void av1_highbd_dist_wtd_convolve_2d_copy_neon(const uint16_t *src, int src_stri
 
 void av1_highbd_dist_wtd_convolve_x_c(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const int subpel_x_qn, ConvolveParams *conv_params, int bd);
 void av1_highbd_dist_wtd_convolve_x_neon(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const int subpel_x_qn, ConvolveParams *conv_params, int bd);
-#define av1_highbd_dist_wtd_convolve_x av1_highbd_dist_wtd_convolve_x_neon
+void av1_highbd_dist_wtd_convolve_x_sve2(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const int subpel_x_qn, ConvolveParams *conv_params, int bd);
+RTCD_EXTERN void (*av1_highbd_dist_wtd_convolve_x)(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_x, const int subpel_x_qn, ConvolveParams *conv_params, int bd);
 
 void av1_highbd_dist_wtd_convolve_y_c(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_y, const int subpel_y_qn, ConvolveParams *conv_params, int bd);
 void av1_highbd_dist_wtd_convolve_y_neon(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_y, const int subpel_y_qn, ConvolveParams *conv_params, int bd);
-#define av1_highbd_dist_wtd_convolve_y av1_highbd_dist_wtd_convolve_y_neon
+void av1_highbd_dist_wtd_convolve_y_sve2(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_y, const int subpel_y_qn, ConvolveParams *conv_params, int bd);
+RTCD_EXTERN void (*av1_highbd_dist_wtd_convolve_y)(const uint16_t *src, int src_stride, uint16_t *dst, int dst_stride, int w, int h, const InterpFilterParams *filter_params_y, const int subpel_y_qn, ConvolveParams *conv_params, int bd);
 
 void av1_highbd_dr_prediction_z1_c(uint16_t *dst, ptrdiff_t stride, int bw, int bh, const uint16_t *above, const uint16_t *left, int upsample_above, int dx, int dy, int bd);
 void av1_highbd_dr_prediction_z1_neon(uint16_t *dst, ptrdiff_t stride, int bw, int bh, const uint16_t *above, const uint16_t *left, int upsample_above, int dx, int dy, int bd);
@@ -501,7 +512,8 @@ void av1_highbd_upsample_intra_edge_neon(uint16_t *p, int sz, int bd);
 
 void av1_highbd_warp_affine_c(const int32_t *mat, const uint16_t *ref, int width, int height, int stride, uint16_t *pred, int p_col, int p_row, int p_width, int p_height, int p_stride, int subsampling_x, int subsampling_y, int bd, ConvolveParams *conv_params, int16_t alpha, int16_t beta, int16_t gamma, int16_t delta);
 void av1_highbd_warp_affine_neon(const int32_t *mat, const uint16_t *ref, int width, int height, int stride, uint16_t *pred, int p_col, int p_row, int p_width, int p_height, int p_stride, int subsampling_x, int subsampling_y, int bd, ConvolveParams *conv_params, int16_t alpha, int16_t beta, int16_t gamma, int16_t delta);
-#define av1_highbd_warp_affine av1_highbd_warp_affine_neon
+void av1_highbd_warp_affine_sve(const int32_t *mat, const uint16_t *ref, int width, int height, int stride, uint16_t *pred, int p_col, int p_row, int p_width, int p_height, int p_stride, int subsampling_x, int subsampling_y, int bd, ConvolveParams *conv_params, int16_t alpha, int16_t beta, int16_t gamma, int16_t delta);
+RTCD_EXTERN void (*av1_highbd_warp_affine)(const int32_t *mat, const uint16_t *ref, int width, int height, int stride, uint16_t *pred, int p_col, int p_row, int p_width, int p_height, int p_stride, int subsampling_x, int subsampling_y, int bd, ConvolveParams *conv_params, int16_t alpha, int16_t beta, int16_t gamma, int16_t delta);
 
 void av1_highbd_wiener_convolve_add_src_c(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, const WienerConvolveParams *conv_params, int bd);
 void av1_highbd_wiener_convolve_add_src_neon(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, const WienerConvolveParams *conv_params, int bd);
@@ -622,7 +634,9 @@ void av1_quantize_lp_neon(const int16_t *coeff_ptr, intptr_t n_coeffs, const int
 
 void av1_resize_and_extend_frame_c(const YV12_BUFFER_CONFIG *src, YV12_BUFFER_CONFIG *dst, const InterpFilter filter, const int phase, const int num_planes);
 void av1_resize_and_extend_frame_neon(const YV12_BUFFER_CONFIG *src, YV12_BUFFER_CONFIG *dst, const InterpFilter filter, const int phase, const int num_planes);
-#define av1_resize_and_extend_frame av1_resize_and_extend_frame_neon
+void av1_resize_and_extend_frame_neon_dotprod(const YV12_BUFFER_CONFIG *src, YV12_BUFFER_CONFIG *dst, const InterpFilter filter, const int phase, const int num_planes);
+void av1_resize_and_extend_frame_neon_i8mm(const YV12_BUFFER_CONFIG *src, YV12_BUFFER_CONFIG *dst, const InterpFilter filter, const int phase, const int num_planes);
+RTCD_EXTERN void (*av1_resize_and_extend_frame)(const YV12_BUFFER_CONFIG *src, YV12_BUFFER_CONFIG *dst, const InterpFilter filter, const int phase, const int num_planes);
 
 void av1_resize_horz_dir_c(const uint8_t *const input, int in_stride, uint8_t *intbuf, int height, int filtered_length, int width2);
 #define av1_resize_horz_dir av1_resize_horz_dir_c
@@ -635,11 +649,11 @@ void av1_round_shift_array_neon(int32_t *arr, int size, int bit);
 #define av1_round_shift_array av1_round_shift_array_neon
 
 int av1_selfguided_restoration_c(const uint8_t *dgd8, int width, int height,
-                                int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
-                                int sgr_params_idx, int bit_depth, int highbd);
+                                  int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
+                                  int sgr_params_idx, int bit_depth, int highbd);
 int av1_selfguided_restoration_neon(const uint8_t *dgd8, int width, int height,
-                                int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
-                                int sgr_params_idx, int bit_depth, int highbd);
+                                  int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
+                                  int sgr_params_idx, int bit_depth, int highbd);
 #define av1_selfguided_restoration av1_selfguided_restoration_neon
 
 void av1_txb_init_levels_c(const tran_low_t *const coeff, const int width, const int height, uint8_t *const levels);
@@ -653,6 +667,7 @@ void av1_upsample_intra_edge_neon(uint8_t *p, int sz);
 void av1_warp_affine_c(const int32_t *mat, const uint8_t *ref, int width, int height, int stride, uint8_t *pred, int p_col, int p_row, int p_width, int p_height, int p_stride, int subsampling_x, int subsampling_y, ConvolveParams *conv_params, int16_t alpha, int16_t beta, int16_t gamma, int16_t delta);
 void av1_warp_affine_neon(const int32_t *mat, const uint8_t *ref, int width, int height, int stride, uint8_t *pred, int p_col, int p_row, int p_width, int p_height, int p_stride, int subsampling_x, int subsampling_y, ConvolveParams *conv_params, int16_t alpha, int16_t beta, int16_t gamma, int16_t delta);
 void av1_warp_affine_neon_i8mm(const int32_t *mat, const uint8_t *ref, int width, int height, int stride, uint8_t *pred, int p_col, int p_row, int p_width, int p_height, int p_stride, int subsampling_x, int subsampling_y, ConvolveParams *conv_params, int16_t alpha, int16_t beta, int16_t gamma, int16_t delta);
+void av1_warp_affine_sve(const int32_t *mat, const uint8_t *ref, int width, int height, int stride, uint8_t *pred, int p_col, int p_row, int p_width, int p_height, int p_stride, int subsampling_x, int subsampling_y, ConvolveParams *conv_params, int16_t alpha, int16_t beta, int16_t gamma, int16_t delta);
 RTCD_EXTERN void (*av1_warp_affine)(const int32_t *mat, const uint8_t *ref, int width, int height, int stride, uint8_t *pred, int p_col, int p_row, int p_width, int p_height, int p_stride, int subsampling_x, int subsampling_y, ConvolveParams *conv_params, int16_t alpha, int16_t beta, int16_t gamma, int16_t delta);
 
 void av1_wedge_compute_delta_squares_c(int16_t *d, const int16_t *a, const int16_t *b, int N);
@@ -661,11 +676,13 @@ void av1_wedge_compute_delta_squares_neon(int16_t *d, const int16_t *a, const in
 
 int8_t av1_wedge_sign_from_residuals_c(const int16_t *ds, const uint8_t *m, int N, int64_t limit);
 int8_t av1_wedge_sign_from_residuals_neon(const int16_t *ds, const uint8_t *m, int N, int64_t limit);
-#define av1_wedge_sign_from_residuals av1_wedge_sign_from_residuals_neon
+int8_t av1_wedge_sign_from_residuals_sve(const int16_t *ds, const uint8_t *m, int N, int64_t limit);
+RTCD_EXTERN int8_t (*av1_wedge_sign_from_residuals)(const int16_t *ds, const uint8_t *m, int N, int64_t limit);
 
 uint64_t av1_wedge_sse_from_residuals_c(const int16_t *r1, const int16_t *d, const uint8_t *m, int N);
 uint64_t av1_wedge_sse_from_residuals_neon(const int16_t *r1, const int16_t *d, const uint8_t *m, int N);
-#define av1_wedge_sse_from_residuals av1_wedge_sse_from_residuals_neon
+uint64_t av1_wedge_sse_from_residuals_sve(const int16_t *r1, const int16_t *d, const uint8_t *m, int N);
+RTCD_EXTERN uint64_t (*av1_wedge_sse_from_residuals)(const int16_t *r1, const int16_t *d, const uint8_t *m, int N);
 
 void av1_wiener_convolve_add_src_c(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, const WienerConvolveParams *conv_params);
 void av1_wiener_convolve_add_src_neon(const uint8_t *src, ptrdiff_t src_stride, uint8_t *dst, ptrdiff_t dst_stride, const int16_t *filter_x, int x_step_q4, const int16_t *filter_y, int y_step_q4, int w, int h, const WienerConvolveParams *conv_params);
@@ -769,12 +786,21 @@ static void setup_rtcd_internal(void)
 
     av1_apply_temporal_filter = av1_apply_temporal_filter_neon;
     if (flags & HAS_NEON_DOTPROD) av1_apply_temporal_filter = av1_apply_temporal_filter_neon_dotprod;
+    av1_block_error = av1_block_error_neon;
+    if (flags & HAS_SVE) av1_block_error = av1_block_error_sve;
+    av1_block_error_lp = av1_block_error_lp_neon;
+    if (flags & HAS_SVE) av1_block_error_lp = av1_block_error_lp_sve;
+    av1_compute_stats = av1_compute_stats_neon;
+    if (flags & HAS_SVE) av1_compute_stats = av1_compute_stats_sve;
+    av1_compute_stats_highbd = av1_compute_stats_highbd_neon;
+    if (flags & HAS_SVE) av1_compute_stats_highbd = av1_compute_stats_highbd_sve;
     av1_convolve_2d_scale = av1_convolve_2d_scale_neon;
     if (flags & HAS_NEON_DOTPROD) av1_convolve_2d_scale = av1_convolve_2d_scale_neon_dotprod;
     if (flags & HAS_NEON_I8MM) av1_convolve_2d_scale = av1_convolve_2d_scale_neon_i8mm;
     av1_convolve_2d_sr = av1_convolve_2d_sr_neon;
     if (flags & HAS_NEON_DOTPROD) av1_convolve_2d_sr = av1_convolve_2d_sr_neon_dotprod;
     if (flags & HAS_NEON_I8MM) av1_convolve_2d_sr = av1_convolve_2d_sr_neon_i8mm;
+    if (flags & HAS_SVE2) av1_convolve_2d_sr = av1_convolve_2d_sr_sve2;
     av1_convolve_x_sr = av1_convolve_x_sr_neon;
     if (flags & HAS_NEON_DOTPROD) av1_convolve_x_sr = av1_convolve_x_sr_neon_dotprod;
     if (flags & HAS_NEON_I8MM) av1_convolve_x_sr = av1_convolve_x_sr_neon_i8mm;
@@ -789,8 +815,30 @@ static void setup_rtcd_internal(void)
     if (flags & HAS_NEON_I8MM) av1_dist_wtd_convolve_x = av1_dist_wtd_convolve_x_neon_i8mm;
     av1_get_crc32c_value = av1_get_crc32c_value_c;
     if (flags & HAS_ARM_CRC32) av1_get_crc32c_value = av1_get_crc32c_value_arm_crc32;
+    av1_highbd_convolve_2d_sr = av1_highbd_convolve_2d_sr_neon;
+    if (flags & HAS_SVE2) av1_highbd_convolve_2d_sr = av1_highbd_convolve_2d_sr_sve2;
+    av1_highbd_convolve_x_sr = av1_highbd_convolve_x_sr_neon;
+    if (flags & HAS_SVE2) av1_highbd_convolve_x_sr = av1_highbd_convolve_x_sr_sve2;
+    av1_highbd_convolve_y_sr = av1_highbd_convolve_y_sr_neon;
+    if (flags & HAS_SVE2) av1_highbd_convolve_y_sr = av1_highbd_convolve_y_sr_sve2;
+    av1_highbd_dist_wtd_convolve_2d = av1_highbd_dist_wtd_convolve_2d_neon;
+    if (flags & HAS_SVE2) av1_highbd_dist_wtd_convolve_2d = av1_highbd_dist_wtd_convolve_2d_sve2;
+    av1_highbd_dist_wtd_convolve_x = av1_highbd_dist_wtd_convolve_x_neon;
+    if (flags & HAS_SVE2) av1_highbd_dist_wtd_convolve_x = av1_highbd_dist_wtd_convolve_x_sve2;
+    av1_highbd_dist_wtd_convolve_y = av1_highbd_dist_wtd_convolve_y_neon;
+    if (flags & HAS_SVE2) av1_highbd_dist_wtd_convolve_y = av1_highbd_dist_wtd_convolve_y_sve2;
+    av1_highbd_warp_affine = av1_highbd_warp_affine_neon;
+    if (flags & HAS_SVE) av1_highbd_warp_affine = av1_highbd_warp_affine_sve;
+    av1_resize_and_extend_frame = av1_resize_and_extend_frame_neon;
+    if (flags & HAS_NEON_DOTPROD) av1_resize_and_extend_frame = av1_resize_and_extend_frame_neon_dotprod;
+    if (flags & HAS_NEON_I8MM) av1_resize_and_extend_frame = av1_resize_and_extend_frame_neon_i8mm;
     av1_warp_affine = av1_warp_affine_neon;
     if (flags & HAS_NEON_I8MM) av1_warp_affine = av1_warp_affine_neon_i8mm;
+    if (flags & HAS_SVE) av1_warp_affine = av1_warp_affine_sve;
+    av1_wedge_sign_from_residuals = av1_wedge_sign_from_residuals_neon;
+    if (flags & HAS_SVE) av1_wedge_sign_from_residuals = av1_wedge_sign_from_residuals_sve;
+    av1_wedge_sse_from_residuals = av1_wedge_sse_from_residuals_neon;
+    if (flags & HAS_SVE) av1_wedge_sse_from_residuals = av1_wedge_sse_from_residuals_sve;
 }
 #endif
 
@@ -798,4 +846,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AV1_RTCD_H_
diff --git a/config/config/aom_version.h b/config/config/aom_version.h
index 478c386df..acd2a148e 100644
--- a/config/config/aom_version.h
+++ b/config/config/aom_version.h
@@ -9,11 +9,14 @@
  * PATENTS file, you can obtain it at www.aomedia.org/license/patent.
  */
 
+#ifndef AOM_VERSION_H_
+#define AOM_VERSION_H_
 #define VERSION_MAJOR 3
-#define VERSION_MINOR 10
+#define VERSION_MINOR 11
 #define VERSION_PATCH 0
-#define VERSION_EXTRA "429-g1c51ec456a"
+#define VERSION_EXTRA "433-g1cb6998458"
 #define VERSION_PACKED \
   ((VERSION_MAJOR << 16) | (VERSION_MINOR << 8) | (VERSION_PATCH))
-#define VERSION_STRING_NOSP "3.10.0-429-g1c51ec456a"
-#define VERSION_STRING " 3.10.0-429-g1c51ec456a"
+#define VERSION_STRING_NOSP "3.11.0-433-g1cb6998458"
+#define VERSION_STRING " 3.11.0-433-g1cb6998458"
+#endif  // AOM_VERSION_H_
diff --git a/config/riscv64/config/aom_dsp_rtcd.h b/config/riscv64/config/aom_dsp_rtcd.h
index 1ca3dcc28..d891c2ca7 100644
--- a/config/riscv64/config/aom_dsp_rtcd.h
+++ b/config/riscv64/config/aom_dsp_rtcd.h
@@ -4655,4 +4655,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AOM_DSP_RTCD_H_
diff --git a/config/riscv64/config/aom_scale_rtcd.h b/config/riscv64/config/aom_scale_rtcd.h
index 902408184..0700a403f 100644
--- a/config/riscv64/config/aom_scale_rtcd.h
+++ b/config/riscv64/config/aom_scale_rtcd.h
@@ -83,4 +83,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AOM_SCALE_RTCD_H_
diff --git a/config/riscv64/config/av1_rtcd.h b/config/riscv64/config/av1_rtcd.h
index 2e3bf8857..e2141fdb2 100644
--- a/config/riscv64/config/av1_rtcd.h
+++ b/config/riscv64/config/av1_rtcd.h
@@ -496,8 +496,8 @@ void av1_round_shift_array_c(int32_t *arr, int size, int bit);
 #define av1_round_shift_array av1_round_shift_array_c
 
 int av1_selfguided_restoration_c(const uint8_t *dgd8, int width, int height,
-                                int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
-                                int sgr_params_idx, int bit_depth, int highbd);
+                                  int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
+                                  int sgr_params_idx, int bit_depth, int highbd);
 #define av1_selfguided_restoration av1_selfguided_restoration_c
 
 void av1_txb_init_levels_c(const tran_low_t *const coeff, const int width, const int height, uint8_t *const levels);
@@ -598,4 +598,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AV1_RTCD_H_
diff --git a/config/x86/config/aom_dsp_rtcd.h b/config/x86/config/aom_dsp_rtcd.h
index 098c3c53a..f88e6b4c0 100644
--- a/config/x86/config/aom_dsp_rtcd.h
+++ b/config/x86/config/aom_dsp_rtcd.h
@@ -5610,4 +5610,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AOM_DSP_RTCD_H_
diff --git a/config/x86/config/aom_scale_rtcd.h b/config/x86/config/aom_scale_rtcd.h
index 0dfca5a09..bc8f330df 100644
--- a/config/x86/config/aom_scale_rtcd.h
+++ b/config/x86/config/aom_scale_rtcd.h
@@ -86,4 +86,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AOM_SCALE_RTCD_H_
diff --git a/config/x86/config/av1_rtcd.h b/config/x86/config/av1_rtcd.h
index f583b2ed9..b6331e684 100644
--- a/config/x86/config/av1_rtcd.h
+++ b/config/x86/config/av1_rtcd.h
@@ -542,8 +542,8 @@ void av1_round_shift_array_c(int32_t *arr, int size, int bit);
 #define av1_round_shift_array av1_round_shift_array_c
 
 int av1_selfguided_restoration_c(const uint8_t *dgd8, int width, int height,
-                                int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
-                                int sgr_params_idx, int bit_depth, int highbd);
+                                  int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
+                                  int sgr_params_idx, int bit_depth, int highbd);
 #define av1_selfguided_restoration av1_selfguided_restoration_c
 
 void av1_txb_init_levels_c(const tran_low_t *const coeff, const int width, const int height, uint8_t *const levels);
@@ -672,4 +672,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AV1_RTCD_H_
diff --git a/config/x86_64/config/aom_dsp_rtcd.h b/config/x86_64/config/aom_dsp_rtcd.h
index fb693efc2..ecb368152 100644
--- a/config/x86_64/config/aom_dsp_rtcd.h
+++ b/config/x86_64/config/aom_dsp_rtcd.h
@@ -5613,4 +5613,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AOM_DSP_RTCD_H_
diff --git a/config/x86_64/config/aom_scale_rtcd.h b/config/x86_64/config/aom_scale_rtcd.h
index 0dfca5a09..bc8f330df 100644
--- a/config/x86_64/config/aom_scale_rtcd.h
+++ b/config/x86_64/config/aom_scale_rtcd.h
@@ -86,4 +86,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AOM_SCALE_RTCD_H_
diff --git a/config/x86_64/config/av1_rtcd.h b/config/x86_64/config/av1_rtcd.h
index 07a9e0651..bdb6fb7c7 100644
--- a/config/x86_64/config/av1_rtcd.h
+++ b/config/x86_64/config/av1_rtcd.h
@@ -544,8 +544,8 @@ void av1_round_shift_array_c(int32_t *arr, int size, int bit);
 #define av1_round_shift_array av1_round_shift_array_c
 
 int av1_selfguided_restoration_c(const uint8_t *dgd8, int width, int height,
-                                int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
-                                int sgr_params_idx, int bit_depth, int highbd);
+                                  int dgd_stride, int32_t *flt0, int32_t *flt1, int flt_stride,
+                                  int sgr_params_idx, int bit_depth, int highbd);
 #define av1_selfguided_restoration av1_selfguided_restoration_c
 
 void av1_txb_init_levels_c(const tran_low_t *const coeff, const int width, const int height, uint8_t *const levels);
@@ -662,4 +662,4 @@ static void setup_rtcd_internal(void)
 }  // extern "C"
 #endif
 
-#endif
+#endif  // AV1_RTCD_H_
diff --git a/doc/dev_guide/av1_encoder.dox b/doc/dev_guide/av1_encoder.dox
index a40b58933..13d55e464 100644
--- a/doc/dev_guide/av1_encoder.dox
+++ b/doc/dev_guide/av1_encoder.dox
@@ -587,8 +587,13 @@ The command line for libaom does allow 1 Pass VBR, but this has not been
 properly optimised and behaves much like 1 pass CBR in most regards, with bits
 allocated to frames by the following functions:
 
-- \ref av1_calc_iframe_target_size_one_pass_vbr()
-- \ref av1_calc_pframe_target_size_one_pass_vbr()
+- \ref av1_calc_iframe_target_size_one_pass_vbr(
+           const struct AV1_COMP *const cpi)
+       "av1_calc_iframe_target_size_one_pass_vbr()"
+- \ref av1_calc_pframe_target_size_one_pass_vbr(
+           const struct AV1_COMP *const cpi,
+           FRAME_UPDATE_TYPE frame_update_type)
+       "av1_calc_pframe_target_size_one_pass_vbr()"
 
 \subsubsection architecture_enc_2pass_vbr 2 Pass VBR Encoding
 
@@ -701,7 +706,10 @@ time, for the two pass encoding pipeline are:
 
 - \ref rc_pick_q_and_bounds()
     - \ref get_q()
-        - \ref av1_rc_regulate_q()
+        - \ref av1_rc_regulate_q(
+                   const struct AV1_COMP *cpi, int target_bits_per_frame,
+                   int active_best_quality, int active_worst_quality,
+                   int width, int height) "av1_rc_regulate_q()"
         - \ref get_rate_correction_factor()
         - \ref set_rate_correction_factor()
         - \ref find_closest_qindex_by_rate()
diff --git a/examples/lightfield_bitstream_parsing.c b/examples/lightfield_bitstream_parsing.c
index 7f0b8d251..f092fe81c 100644
--- a/examples/lightfield_bitstream_parsing.c
+++ b/examples/lightfield_bitstream_parsing.c
@@ -40,11 +40,11 @@
 #include <stdlib.h>
 #include <string.h>
 
+#include "aom/aom_codec.h"
 #include "aom/aom_decoder.h"
 #include "aom/aom_encoder.h"
 #include "aom/aom_integer.h"
 #include "aom/aomdx.h"
-#include "aom_dsp/bitwriter_buffer.h"
 #include "common/tools_common.h"
 #include "common/video_reader.h"
 #include "common/video_writer.h"
@@ -101,7 +101,6 @@ static void process_tile_list(const TILE_LIST_INFO *tiles, int num_tiles,
                               uint8_t output_frame_width_in_tiles_minus_1,
                               uint8_t output_frame_height_in_tiles_minus_1) {
   unsigned char *tl = tl_buf;
-  struct aom_write_bit_buffer wb = { tl, 0 };
   unsigned char *saved_obu_size_loc = NULL;
   uint32_t tile_list_obu_header_size = 0;
   uint32_t tile_list_obu_size = 0;
@@ -109,26 +108,23 @@ static void process_tile_list(const TILE_LIST_INFO *tiles, int num_tiles,
   int i;
 
   // Write the tile list OBU header that is 1 byte long.
-  aom_wb_write_literal(&wb, 0, 1);  // forbidden bit.
-  aom_wb_write_literal(&wb, 8, 4);  // tile list OBU: "1000"
-  aom_wb_write_literal(&wb, 0, 1);  // obu_extension = 0
-  aom_wb_write_literal(&wb, 1, 1);  // obu_has_size_field
-  aom_wb_write_literal(&wb, 0, 1);  // reserved
-  tl++;
+  int obu_type = OBU_TILE_LIST;
+  int obu_has_size_field = 1;
+  *tl++ = (obu_type << 3) | (obu_has_size_field << 1);
   tile_list_obu_header_size++;
 
   // Write the OBU size using a fixed length_field_size of 4 bytes.
   saved_obu_size_loc = tl;
-  // aom_wb_write_unsigned_literal(&wb, data, bits) requires that bits <= 32.
-  aom_wb_write_unsigned_literal(&wb, 0, 32);
-  tl += 4;
+  for (i = 0; i < 4; i++) {
+    *tl++ = 0;
+  }
   tile_list_obu_header_size += 4;
 
   // write_tile_list_obu()
-  aom_wb_write_literal(&wb, output_frame_width_in_tiles_minus_1, 8);
-  aom_wb_write_literal(&wb, output_frame_height_in_tiles_minus_1, 8);
-  aom_wb_write_literal(&wb, num_tiles_minus_1, 16);
-  tl += 4;
+  *tl++ = output_frame_width_in_tiles_minus_1;
+  *tl++ = output_frame_height_in_tiles_minus_1;
+  *tl++ = (num_tiles_minus_1 >> 8) & 0xff;
+  *tl++ = num_tiles_minus_1 & 0xff;
   tile_list_obu_size += 4;
 
   // Write each tile's data
@@ -140,10 +136,6 @@ static void process_tile_list(const TILE_LIST_INFO *tiles, int num_tiles,
     int tc = tiles[i].tile_col;
     int tr = tiles[i].tile_row;
 
-    // Reset bit writer to the right location.
-    wb.bit_buffer = tl;
-    wb.bit_offset = 0;
-
     size_t frame_size = frame_sizes[image_idx];
     const unsigned char *frame = frames[image_idx];
 
@@ -163,11 +155,12 @@ static void process_tile_list(const TILE_LIST_INFO *tiles, int num_tiles,
     //  uint16_t coded_tile_data_size_minus_1;
     //  uint8_t *coded_tile_data;
     uint32_t tile_info_bytes = 5;
-    aom_wb_write_literal(&wb, ref_idx, 8);
-    aom_wb_write_literal(&wb, tr, 8);
-    aom_wb_write_literal(&wb, tc, 8);
-    aom_wb_write_literal(&wb, (int)tile_data.coded_tile_data_size - 1, 16);
-    tl += tile_info_bytes;
+    *tl++ = ref_idx;
+    *tl++ = tr;
+    *tl++ = tc;
+    int coded_tile_data_size_minus_1 = (int)tile_data.coded_tile_data_size - 1;
+    *tl++ = (coded_tile_data_size_minus_1 >> 8) & 0xff;
+    *tl++ = coded_tile_data_size_minus_1 & 0xff;
 
     memcpy(tl, (uint8_t *)tile_data.coded_tile_data,
            tile_data.coded_tile_data_size);
diff --git a/examples/svc_encoder_rtc.cc b/examples/svc_encoder_rtc.cc
index 4fb160e6e..3f628e00f 100644
--- a/examples/svc_encoder_rtc.cc
+++ b/examples/svc_encoder_rtc.cc
@@ -1694,7 +1694,6 @@ int main(int argc, const char **argv) {
   aom_codec_control(&codec, AV1E_SET_TUNE_CONTENT, app_input.tune_content);
   if (app_input.tune_content == AOM_CONTENT_SCREEN) {
     aom_codec_control(&codec, AV1E_SET_ENABLE_PALETTE, 1);
-    aom_codec_control(&codec, AV1E_SET_ENABLE_CFL_INTRA, 1);
     // INTRABC is currently disabled for rt mode, as it's too slow.
     aom_codec_control(&codec, AV1E_SET_ENABLE_INTRABC, 0);
   }
diff --git a/generate_config.sh b/generate_config.sh
index c4debecb2..6b7062c0a 100755
--- a/generate_config.sh
+++ b/generate_config.sh
@@ -123,7 +123,9 @@ reset_dirs arm
 gen_config_files arm "${toolchain}/armv7-linux-gcc.cmake ${all_platforms}"
 
 reset_dirs arm64
-gen_config_files arm64 "${toolchain}/arm64-linux-gcc.cmake ${all_platforms} \
+# Note clang is use to allow detection of SVE/SVE2; gcc as of version 13 is
+# missing the required arm_neon_sve_bridge.h header.
+gen_config_files arm64 "${toolchain}/arm64-linux-clang.cmake ${all_platforms} \
   -DCONFIG_RUNTIME_CPU_DETECT=1"
 
 reset_dirs riscv64
diff --git a/test/av1_c_vs_simd_encode.sh b/test/av1_c_vs_simd_encode.sh
index a55db5e3a..67e17b1eb 100755
--- a/test/av1_c_vs_simd_encode.sh
+++ b/test/av1_c_vs_simd_encode.sh
@@ -299,7 +299,7 @@ av1_enc_build() {
   mkdir -p $tmp_build_dir
   cd $tmp_build_dir
 
-  local cmake_common_args="-DCONFIG_EXCLUDE_SIMD_MISMATCH=1 \
+  local cmake_common_args="--fresh -DCONFIG_EXCLUDE_SIMD_MISMATCH=1 \
            -DCMAKE_BUILD_TYPE=Release \
            -DENABLE_CCACHE=1 \
            '-DCMAKE_C_FLAGS_RELEASE=-O3 -g' \
@@ -350,18 +350,28 @@ av1_enc_test() {
   local target="$3"
   local preset="$4"
   if [ -z "$(av1_enc_tool_path "${target}"  "${preset}")" ]; then
-    elog "aomenc_{preset} not found. It must exist in ${AOM_TEST_OUTPUT_DIR}/build_target_${target} path"
+    elog "aomenc_${preset} not found. It must exist in ${AOM_TEST_OUTPUT_DIR}/build_target_${target} path"
     return 1
   fi
 
   if [ "${preset}" = "good" ]; then
-    if [ "${arch}" = "x86_64" ]; then
-      local min_cpu_used=0
-      local max_cpu_used=6
-    elif [ "${arch}" = "x86" ]; then
-      local min_cpu_used=2
-      local max_cpu_used=3
-    fi
+    case "${arch}" in
+      arm64)
+        # Speed 0 is not tested as arm64 is run under emulation.
+        local min_cpu_used=1
+        local max_cpu_used=6
+        ;;
+      x86)
+        # x86 has a good amount of overlap with x86-64. Only a few values are
+        # tested to improve the runtime of the script.
+        local min_cpu_used=2
+        local max_cpu_used=3
+        ;;
+      *)
+        local min_cpu_used=0
+        local max_cpu_used=6
+        ;;
+    esac
     local test_params=av1_encode_good_params
   elif [ "${preset}" = "rt" ]; then
     local min_cpu_used=5
@@ -374,7 +384,7 @@ av1_enc_test() {
 
   for cpu in $(seq $min_cpu_used $max_cpu_used); do
     if [ "${preset}" = "good" ]; then
-      if [ "${arch}" = "x86_64" ]; then
+      if [ "${arch}" = "x86_64" -o "${arch}" = "arm64" ]; then
         if [ "${cpu}" -lt 2 ]; then
           local test_clips="${LOWBD_CIF_CLIP} ${HIGHBD_CLIP}"
         elif [ "${cpu}" -lt 5 ]; then
@@ -384,8 +394,9 @@ av1_enc_test() {
         fi
       elif [ "${arch}" = "x86" ]; then
         local test_clips="${LOWBD_CIF_CLIP} ${HIGHBD_CLIP}"
-      elif [ "${arch}" = "arm64" ]; then
-        local test_clips="${LOWBD_CIF_CLIP} ${HIGHBD_CLIP}"
+      else
+        elog "Unknown architecture: ${arch}"
+        return 1
       fi
     elif [ "${preset}" = "rt" ]; then
       if [ "${cpu}" -lt 8 ]; then
diff --git a/test/av1_fwd_txfm2d_test.cc b/test/av1_fwd_txfm2d_test.cc
index d93d59ad5..95b215172 100644
--- a/test/av1_fwd_txfm2d_test.cc
+++ b/test/av1_fwd_txfm2d_test.cc
@@ -660,7 +660,7 @@ static TX_SIZE Highbd_fwd_txfm_for_sse4_1[] = {
   TX_8X16, TX_16X8, TX_16X32, TX_32X16, TX_32X64, TX_64X32,
 #if !CONFIG_REALTIME_ONLY
   TX_4X16, TX_16X4, TX_8X32,  TX_32X8,  TX_16X64, TX_64X16,
-#endif
+#endif  // !CONFIG_REALTIME_ONLY
 };
 
 INSTANTIATE_TEST_SUITE_P(SSE4_1, AV1HighbdFwdTxfm2dTest,
@@ -679,8 +679,10 @@ INSTANTIATE_TEST_SUITE_P(AVX2, AV1HighbdFwdTxfm2dTest,
 #if HAVE_NEON
 static TX_SIZE Highbd_fwd_txfm_for_neon[] = {
   TX_4X4,  TX_8X8,  TX_16X16, TX_32X32, TX_64X64, TX_4X8,   TX_8X4,
-  TX_8X16, TX_16X8, TX_16X32, TX_32X16, TX_32X64, TX_64X32, TX_4X16,
-  TX_16X4, TX_8X32, TX_32X8,  TX_16X64, TX_64X16
+  TX_8X16, TX_16X8, TX_16X32, TX_32X16, TX_32X64, TX_64X32,
+#if !CONFIG_REALTIME_ONLY
+  TX_4X16, TX_16X4, TX_8X32,  TX_32X8,  TX_16X64, TX_64X16
+#endif  // !CONFIG_REALTIME_ONLY
 };
 
 INSTANTIATE_TEST_SUITE_P(NEON, AV1HighbdFwdTxfm2dTest,
diff --git a/test/av1_scale_test.cc b/test/av1_scale_test.cc
new file mode 100644
index 000000000..14f5c4d6b
--- /dev/null
+++ b/test/av1_scale_test.cc
@@ -0,0 +1,303 @@
+/*
+ *  Copyright (c) 2017 The WebM project authors. All Rights Reserved.
+ *  Copyright (c) 2024, Alliance for Open Media. All rights reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <assert.h>
+#include <string.h>
+
+#include <tuple>
+
+#include "gtest/gtest.h"
+
+#include "common/av1_config.h"
+#include "config/av1_rtcd.h"
+
+#include "config/aom_config.h"
+#include "config/aom_dsp_rtcd.h"
+
+#include "aom_dsp/aom_dsp_common.h"
+#include "aom_dsp/aom_filter.h"
+#include "aom_mem/aom_mem.h"
+#include "aom_ports/aom_timer.h"
+#include "aom_ports/mem.h"
+#include "av1/common/filter.h"
+#include "test/acm_random.h"
+#include "test/register_state_check.h"
+#include "test/util.h"
+
+namespace {
+
+using ResizeFrameFunc = void (*)(const YV12_BUFFER_CONFIG *src,
+                                 YV12_BUFFER_CONFIG *dst,
+                                 const InterpFilter filter, const int phase,
+                                 const int num_planes);
+
+class ResizeAndExtendTest : public ::testing::TestWithParam<ResizeFrameFunc> {
+ public:
+  ResizeAndExtendTest() { resize_fn_ = GetParam(); }
+  ~ResizeAndExtendTest() override = default;
+
+ protected:
+  const int kBufFiller = 123;
+  const int kBufMax = kBufFiller - 1;
+
+  void FillPlane(uint8_t *const buf, const int width, const int height,
+                 const int stride) {
+    for (int y = 0; y < height; ++y) {
+      for (int x = 0; x < width; ++x) {
+        buf[x + (y * stride)] = (x + (width * y)) % kBufMax;
+      }
+    }
+  }
+
+  void ResetResizeImage(YV12_BUFFER_CONFIG *const img, const int width,
+                        const int height, const int border) {
+    memset(img, 0, sizeof(*img));
+    ASSERT_EQ(0, aom_alloc_frame_buffer(img, width, height, 1, 1, 0, border, 16,
+                                        false, 0));
+    memset(img->buffer_alloc, kBufFiller, img->frame_size);
+  }
+
+  void ResetResizeImages(const int src_width, const int src_height,
+                         const int dst_width, const int dst_height,
+                         const int dst_border) {
+    ResetResizeImage(&img_, src_width, src_height, AOM_BORDER_IN_PIXELS);
+    ResetResizeImage(&ref_img_, dst_width, dst_height, dst_border);
+    ResetResizeImage(&dst_img_, dst_width, dst_height, dst_border);
+    FillPlane(img_.y_buffer, img_.y_crop_width, img_.y_crop_height,
+              img_.y_stride);
+    FillPlane(img_.u_buffer, img_.uv_crop_width, img_.uv_crop_height,
+              img_.uv_stride);
+    FillPlane(img_.v_buffer, img_.uv_crop_width, img_.uv_crop_height,
+              img_.uv_stride);
+  }
+
+  void DeallocResizeImages() {
+    aom_free_frame_buffer(&img_);
+    aom_free_frame_buffer(&ref_img_);
+    aom_free_frame_buffer(&dst_img_);
+  }
+
+  void RunTest(InterpFilter filter_type) {
+    static const int kNumSizesToTest = 22;
+    static const int kNumScaleFactorsToTest = 4;
+    static const int kNumDstBordersToTest = 2;
+    static const int kSizesToTest[] = { 1,  2,  3,  4,  6,   8,  10, 12,
+                                        14, 16, 18, 20, 22,  24, 26, 28,
+                                        30, 32, 34, 68, 128, 134 };
+    static const int kScaleFactors[] = { 1, 2, 3, 4 };
+    static const int kDstBorders[] = { 0, AOM_BORDER_IN_PIXELS };
+    for (int border = 0; border < kNumDstBordersToTest; ++border) {
+      const int dst_border = kDstBorders[border];
+      for (int phase_scaler = 0; phase_scaler < 16; ++phase_scaler) {
+        for (int h = 0; h < kNumSizesToTest; ++h) {
+          const int src_height = kSizesToTest[h];
+          for (int w = 0; w < kNumSizesToTest; ++w) {
+            const int src_width = kSizesToTest[w];
+            for (int sf_up_idx = 0; sf_up_idx < kNumScaleFactorsToTest;
+                 ++sf_up_idx) {
+              const int sf_up = kScaleFactors[sf_up_idx];
+              for (int sf_down_idx = 0; sf_down_idx < kNumScaleFactorsToTest;
+                   ++sf_down_idx) {
+                const int sf_down = kScaleFactors[sf_down_idx];
+                const int dst_width = src_width * sf_up / sf_down;
+                const int dst_height = src_height * sf_up / sf_down;
+                // TODO: bug aomedia:363916152 - Enable unit tests for 4 to 3
+                // scaling when Neon and SSSE3 implementation of
+                // av1_resize_and_extend_frame do not differ from scalar version
+                if (sf_down == 4 && sf_up == 3) {
+                  continue;
+                }
+
+                if (sf_up == sf_down && sf_up != 1) {
+                  continue;
+                }
+                // I420 frame width and height must be even.
+                if (!dst_width || !dst_height || dst_width & 1 ||
+                    dst_height & 1) {
+                  continue;
+                }
+                // aom_convolve8_c() has restriction on the step which cannot
+                // exceed 64 (ratio 1 to 4).
+                if (src_width > 4 * dst_width || src_height > 4 * dst_height) {
+                  continue;
+                }
+                ASSERT_NO_FATAL_FAILURE(ResetResizeImages(
+                    src_width, src_height, dst_width, dst_height, dst_border));
+
+                av1_resize_and_extend_frame_c(&img_, &ref_img_, filter_type,
+                                              phase_scaler, 1);
+                resize_fn_(&img_, &dst_img_, filter_type, phase_scaler, 1);
+
+                if (memcmp(dst_img_.buffer_alloc, ref_img_.buffer_alloc,
+                           ref_img_.frame_size)) {
+                  printf(
+                      "filter_type = %d, phase_scaler = %d, src_width = %4d, "
+                      "src_height = %4d, dst_width = %4d, dst_height = %4d, "
+                      "scale factor = %d:%d\n",
+                      filter_type, phase_scaler, src_width, src_height,
+                      dst_width, dst_height, sf_down, sf_up);
+                  PrintDiff();
+                }
+
+                EXPECT_EQ(ref_img_.frame_size, dst_img_.frame_size);
+                EXPECT_EQ(0,
+                          memcmp(ref_img_.buffer_alloc, dst_img_.buffer_alloc,
+                                 ref_img_.frame_size));
+
+                DeallocResizeImages();
+              }
+            }
+          }
+        }
+      }
+    }
+  }
+
+  void PrintDiffComponent(const uint8_t *const ref, const uint8_t *const opt,
+                          const int stride, const int width, const int height,
+                          const int plane_idx) const {
+    for (int y = 0; y < height; y++) {
+      for (int x = 0; x < width; x++) {
+        if (ref[y * stride + x] != opt[y * stride + x]) {
+          printf("Plane %d pixel[%d][%d] diff:%6d (ref),%6d (opt)\n", plane_idx,
+                 y, x, ref[y * stride + x], opt[y * stride + x]);
+          break;
+        }
+      }
+    }
+  }
+
+  void PrintDiff() const {
+    assert(ref_img_.y_stride == dst_img_.y_stride);
+    assert(ref_img_.y_width == dst_img_.y_width);
+    assert(ref_img_.y_height == dst_img_.y_height);
+    assert(ref_img_.uv_stride == dst_img_.uv_stride);
+    assert(ref_img_.uv_width == dst_img_.uv_width);
+    assert(ref_img_.uv_height == dst_img_.uv_height);
+
+    if (memcmp(dst_img_.buffer_alloc, ref_img_.buffer_alloc,
+               ref_img_.frame_size)) {
+      PrintDiffComponent(ref_img_.y_buffer, dst_img_.y_buffer,
+                         ref_img_.y_stride, ref_img_.y_width, ref_img_.y_height,
+                         0);
+      PrintDiffComponent(ref_img_.u_buffer, dst_img_.u_buffer,
+                         ref_img_.uv_stride, ref_img_.uv_width,
+                         ref_img_.uv_height, 1);
+      PrintDiffComponent(ref_img_.v_buffer, dst_img_.v_buffer,
+                         ref_img_.uv_stride, ref_img_.uv_width,
+                         ref_img_.uv_height, 2);
+    }
+  }
+
+  void SpeedTest() {
+    static const int kCountSpeedTestBlock = 100;
+    static const int kNumScaleFactorsToTest = 4;
+    static const int kNumInterpFiltersToTest = 3;
+    static const int kScaleFactors[] = { 1, 2, 3, 4 };
+    static const int kInterpFilters[] = { 0, 1, 3 };
+    const int src_width = 1280;
+    const int src_height = 720;
+    for (int filter = 0; filter < kNumInterpFiltersToTest; ++filter) {
+      const InterpFilter filter_type =
+          static_cast<InterpFilter>(kInterpFilters[filter]);
+      for (int phase_scaler = 0; phase_scaler < 2; ++phase_scaler) {
+        for (int sf_up_idx = 0; sf_up_idx < kNumScaleFactorsToTest;
+             ++sf_up_idx) {
+          const int sf_up = kScaleFactors[sf_up_idx];
+          for (int sf_down_idx = 0; sf_down_idx < kNumScaleFactorsToTest;
+               ++sf_down_idx) {
+            const int sf_down = kScaleFactors[sf_down_idx];
+            const int dst_width = src_width * sf_up / sf_down;
+            const int dst_height = src_height * sf_up / sf_down;
+            // TODO: bug aomedia:363916152 - Enable unit tests for 4 to 3
+            // scaling when Neon and SSSE3 implementation of
+            // av1_resize_and_extend_frame do not differ from scalar version
+            if (sf_down == 4 && sf_up == 3) {
+              continue;
+            }
+
+            if (sf_up == sf_down && sf_up != 1) {
+              continue;
+            }
+            // I420 frame width and height must be even.
+            if (dst_width & 1 || dst_height & 1) {
+              continue;
+            }
+            ASSERT_NO_FATAL_FAILURE(ResetResizeImages(src_width, src_height,
+                                                      dst_width, dst_height,
+                                                      AOM_BORDER_IN_PIXELS));
+
+            aom_usec_timer ref_timer;
+            aom_usec_timer_start(&ref_timer);
+            for (int i = 0; i < kCountSpeedTestBlock; ++i)
+              av1_resize_and_extend_frame_c(&img_, &ref_img_, filter_type,
+                                            phase_scaler, 1);
+            aom_usec_timer_mark(&ref_timer);
+            const int64_t ref_time = aom_usec_timer_elapsed(&ref_timer);
+
+            aom_usec_timer tst_timer;
+            aom_usec_timer_start(&tst_timer);
+            for (int i = 0; i < kCountSpeedTestBlock; ++i)
+              resize_fn_(&img_, &dst_img_, filter_type, phase_scaler, 1);
+            aom_usec_timer_mark(&tst_timer);
+            const int64_t tst_time = aom_usec_timer_elapsed(&tst_timer);
+            DeallocResizeImages();
+
+            std::cout << "[          ] C time = " << ref_time / 1000
+                      << " ms, SIMD time = " << tst_time / 1000 << " ms\n";
+          }
+        }
+      }
+    }
+  }
+
+  YV12_BUFFER_CONFIG img_;
+  YV12_BUFFER_CONFIG ref_img_;
+  YV12_BUFFER_CONFIG dst_img_;
+  ResizeFrameFunc resize_fn_;
+};
+
+GTEST_ALLOW_UNINSTANTIATED_PARAMETERIZED_TEST(ResizeAndExtendTest);
+
+TEST_P(ResizeAndExtendTest, ResizeFrame_EightTap) { RunTest(EIGHTTAP_REGULAR); }
+TEST_P(ResizeAndExtendTest, ResizeFrame_EightTapSmooth) {
+  RunTest(EIGHTTAP_SMOOTH);
+}
+TEST_P(ResizeAndExtendTest, ResizeFrame_Bilinear) { RunTest(BILINEAR); }
+TEST_P(ResizeAndExtendTest, DISABLED_Speed) { SpeedTest(); }
+
+// TODO: bug aomedia:363916152 - Enable SSSE3 unit tests when implementation of
+// av1_resize_and_extend_frame does not differ from scalar version
+#if HAVE_SSSE3
+INSTANTIATE_TEST_SUITE_P(DISABLED_SSSE3, ResizeAndExtendTest,
+                         ::testing::Values(av1_resize_and_extend_frame_ssse3));
+#endif  // HAVE_SSSE3
+
+#if HAVE_NEON
+INSTANTIATE_TEST_SUITE_P(NEON, ResizeAndExtendTest,
+                         ::testing::Values(av1_resize_and_extend_frame_neon));
+#endif  // HAVE_NEON
+
+#if HAVE_NEON_DOTPROD
+INSTANTIATE_TEST_SUITE_P(
+    NEON_DOTPROD, ResizeAndExtendTest,
+    ::testing::Values(av1_resize_and_extend_frame_neon_dotprod));
+
+#endif  // HAVE_NEON_DOTPROD
+
+#if HAVE_NEON_I8MM
+INSTANTIATE_TEST_SUITE_P(
+    NEON_I8MM, ResizeAndExtendTest,
+    ::testing::Values(av1_resize_and_extend_frame_neon_i8mm));
+
+#endif  // HAVE_NEON_I8MM
+
+}  // namespace
diff --git a/test/cdef_test.cc b/test/cdef_test.cc
index 7ce278c05..772cc6fef 100644
--- a/test/cdef_test.cc
+++ b/test/cdef_test.cc
@@ -488,6 +488,7 @@ class CDEFCopyRect8to16Test
 };
 GTEST_ALLOW_UNINSTANTIATED_PARAMETERIZED_TEST(CDEFCopyRect8to16Test);
 
+#if CONFIG_AV1_HIGHBITDEPTH
 using CDEFCopyRect16To16 = void (*)(uint16_t *dst, int dstride,
                                     const uint16_t *src, int sstride, int width,
                                     int height);
@@ -571,6 +572,7 @@ class CDEFCopyRect16to16Test
   CDEFCopyRect16To16 ref_func_;
 };
 GTEST_ALLOW_UNINSTANTIATED_PARAMETERIZED_TEST(CDEFCopyRect16to16Test);
+#endif  // CONFIG_AV1_HIGHBITDEPTH
 
 TEST_P(CDEFBlockTest, TestSIMDNoMismatch) {
   test_cdef(bsize, 1, cdef, ref_cdef, boundary, depth);
@@ -608,9 +610,11 @@ TEST_P(CDEFCopyRect8to16Test, TestSIMDNoMismatch) {
   test_copy_rect_8_to_16(test_func_, ref_func_);
 }
 
+#if CONFIG_AV1_HIGHBITDEPTH
 TEST_P(CDEFCopyRect16to16Test, TestSIMDNoMismatch) {
   test_copy_rect_16_to_16(test_func_, ref_func_);
 }
+#endif  // CONFIG_AV1_HIGHBITDEPTH
 
 using std::make_tuple;
 
@@ -663,10 +667,12 @@ INSTANTIATE_TEST_SUITE_P(
     ::testing::Values(make_tuple(&cdef_copy_rect8_8bit_to_16bit_c,
                                  &cdef_copy_rect8_8bit_to_16bit_ssse3)));
 
+#if CONFIG_AV1_HIGHBITDEPTH
 INSTANTIATE_TEST_SUITE_P(
     SSSE3, CDEFCopyRect16to16Test,
     ::testing::Values(make_tuple(&cdef_copy_rect8_16bit_to_16bit_c,
                                  &cdef_copy_rect8_16bit_to_16bit_ssse3)));
+#endif  // CONFIG_AV1_HIGHBITDEPTH
 #endif
 
 #if HAVE_SSE4_1
@@ -707,10 +713,12 @@ INSTANTIATE_TEST_SUITE_P(
     ::testing::Values(make_tuple(&cdef_copy_rect8_8bit_to_16bit_c,
                                  &cdef_copy_rect8_8bit_to_16bit_sse4_1)));
 
+#if CONFIG_AV1_HIGHBITDEPTH
 INSTANTIATE_TEST_SUITE_P(
     SSE4_1, CDEFCopyRect16to16Test,
     ::testing::Values(make_tuple(&cdef_copy_rect8_16bit_to_16bit_c,
                                  &cdef_copy_rect8_16bit_to_16bit_sse4_1)));
+#endif  // CONFIG_AV1_HIGHBITDEPTH
 #endif
 
 #if HAVE_AVX2
@@ -750,10 +758,12 @@ INSTANTIATE_TEST_SUITE_P(
     ::testing::Values(make_tuple(&cdef_copy_rect8_8bit_to_16bit_c,
                                  &cdef_copy_rect8_8bit_to_16bit_avx2)));
 
+#if CONFIG_AV1_HIGHBITDEPTH
 INSTANTIATE_TEST_SUITE_P(
     AVX2, CDEFCopyRect16to16Test,
     ::testing::Values(make_tuple(&cdef_copy_rect8_16bit_to_16bit_c,
                                  &cdef_copy_rect8_16bit_to_16bit_avx2)));
+#endif  // CONFIG_AV1_HIGHBITDEPTH
 #endif
 
 #if HAVE_NEON
@@ -793,10 +803,12 @@ INSTANTIATE_TEST_SUITE_P(
     ::testing::Values(make_tuple(&cdef_copy_rect8_8bit_to_16bit_c,
                                  &cdef_copy_rect8_8bit_to_16bit_neon)));
 
+#if CONFIG_AV1_HIGHBITDEPTH
 INSTANTIATE_TEST_SUITE_P(
     NEON, CDEFCopyRect16to16Test,
     ::testing::Values(make_tuple(&cdef_copy_rect8_16bit_to_16bit_c,
                                  &cdef_copy_rect8_16bit_to_16bit_neon)));
+#endif  // CONFIG_AV1_HIGHBITDEPTH
 #endif
 
 // Test speed for all supported architectures
diff --git a/test/datarate_test.cc b/test/datarate_test.cc
index a66c90e51..e1d6a1d02 100644
--- a/test/datarate_test.cc
+++ b/test/datarate_test.cc
@@ -110,7 +110,7 @@ class DatarateTestLarge
         << " The datarate for the file is lower than target by too much!";
     ASSERT_LE(effective_datarate_, cfg_.rc_target_bitrate * 1.19)
         << " The datarate for the file is greater than target by too much!";
-    ASSERT_LE(num_spikes_, 8);
+    ASSERT_LE(num_spikes_, 10);
     ASSERT_LT(num_spikes_high_, 1);
   }
 
diff --git a/test/intrapred_test.cc b/test/intrapred_test.cc
index 1371c7d79..bf9ab75d6 100644
--- a/test/intrapred_test.cc
+++ b/test/intrapred_test.cc
@@ -292,6 +292,7 @@ TEST_P(LowbdIntraPredTest, DISABLED_Speed) {
       &aom_highbd_##type##_predictor_##width##x##height##_c, width, height, \
       bd)
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #define highbd_intrapred(type, opt, bd)                                        \
   highbd_entry(type, 4, 4, opt, bd), highbd_entry(type, 4, 8, opt, bd),        \
       highbd_entry(type, 4, 16, opt, bd), highbd_entry(type, 8, 4, opt, bd),   \
@@ -305,6 +306,18 @@ TEST_P(LowbdIntraPredTest, DISABLED_Speed) {
       highbd_entry(type, 32, 64, opt, bd),                                     \
       highbd_entry(type, 64, 16, opt, bd),                                     \
       highbd_entry(type, 64, 32, opt, bd), highbd_entry(type, 64, 64, opt, bd)
+#else
+#define highbd_intrapred(type, opt, bd)                                       \
+  highbd_entry(type, 4, 4, opt, bd), highbd_entry(type, 4, 8, opt, bd),       \
+      highbd_entry(type, 8, 4, opt, bd), highbd_entry(type, 8, 8, opt, bd),   \
+      highbd_entry(type, 8, 16, opt, bd), highbd_entry(type, 16, 8, opt, bd), \
+      highbd_entry(type, 16, 16, opt, bd),                                    \
+      highbd_entry(type, 16, 32, opt, bd),                                    \
+      highbd_entry(type, 32, 16, opt, bd),                                    \
+      highbd_entry(type, 32, 32, opt, bd),                                    \
+      highbd_entry(type, 32, 64, opt, bd),                                    \
+      highbd_entry(type, 64, 32, opt, bd), highbd_entry(type, 64, 64, opt, bd)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // CONFIG_AV1_HIGHBITDEPTH
 
 // ---------------------------------------------------------------------------
@@ -315,6 +328,7 @@ TEST_P(LowbdIntraPredTest, DISABLED_Speed) {
                            &aom_##type##_predictor_##width##x##height##_c,     \
                            width, height, 8)
 
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #define lowbd_intrapred(type, opt)                                    \
   lowbd_entry(type, 4, 4, opt), lowbd_entry(type, 4, 8, opt),         \
       lowbd_entry(type, 4, 16, opt), lowbd_entry(type, 8, 4, opt),    \
@@ -326,6 +340,16 @@ TEST_P(LowbdIntraPredTest, DISABLED_Speed) {
       lowbd_entry(type, 32, 32, opt), lowbd_entry(type, 32, 64, opt), \
       lowbd_entry(type, 64, 16, opt), lowbd_entry(type, 64, 32, opt), \
       lowbd_entry(type, 64, 64, opt)
+#else
+#define lowbd_intrapred(type, opt)                                    \
+  lowbd_entry(type, 4, 4, opt), lowbd_entry(type, 4, 8, opt),         \
+      lowbd_entry(type, 8, 4, opt), lowbd_entry(type, 8, 8, opt),     \
+      lowbd_entry(type, 8, 16, opt), lowbd_entry(type, 16, 8, opt),   \
+      lowbd_entry(type, 16, 16, opt), lowbd_entry(type, 16, 32, opt), \
+      lowbd_entry(type, 32, 16, opt), lowbd_entry(type, 32, 32, opt), \
+      lowbd_entry(type, 32, 64, opt), lowbd_entry(type, 64, 32, opt), \
+      lowbd_entry(type, 64, 64, opt)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #if HAVE_SSE2
 const IntraPredFunc<IntraPred> LowbdIntraPredTestVector[] = {
@@ -365,6 +389,7 @@ INSTANTIATE_TEST_SUITE_P(SSSE3, LowbdIntraPredTest,
 
 #if HAVE_AVX2
 const IntraPredFunc<IntraPred> LowbdIntraPredTestVectorAvx2[] = {
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
   lowbd_entry(dc, 32, 16, avx2),      lowbd_entry(dc, 32, 32, avx2),
   lowbd_entry(dc, 32, 64, avx2),      lowbd_entry(dc, 64, 16, avx2),
   lowbd_entry(dc, 64, 32, avx2),      lowbd_entry(dc, 64, 64, avx2),
@@ -388,10 +413,37 @@ const IntraPredFunc<IntraPred> LowbdIntraPredTestVectorAvx2[] = {
   lowbd_entry(h, 32, 32, avx2),
 
   lowbd_entry(paeth, 16, 8, avx2),    lowbd_entry(paeth, 16, 16, avx2),
-  lowbd_entry(paeth, 16, 32, avx2),   lowbd_entry(paeth, 16, 64, avx2),
-  lowbd_entry(paeth, 32, 16, avx2),   lowbd_entry(paeth, 32, 32, avx2),
-  lowbd_entry(paeth, 32, 64, avx2),   lowbd_entry(paeth, 64, 16, avx2),
+  lowbd_entry(paeth, 16, 32, avx2),   lowbd_entry(paeth, 32, 16, avx2),
+  lowbd_entry(paeth, 32, 32, avx2),   lowbd_entry(paeth, 32, 64, avx2),
+  lowbd_entry(paeth, 64, 32, avx2),   lowbd_entry(paeth, 64, 64, avx2),
+#else
+  lowbd_entry(dc, 32, 16, avx2),      lowbd_entry(dc, 32, 32, avx2),
+  lowbd_entry(dc, 32, 64, avx2),      lowbd_entry(dc, 64, 32, avx2),
+  lowbd_entry(dc, 64, 64, avx2),
+
+  lowbd_entry(dc_top, 32, 16, avx2),  lowbd_entry(dc_top, 32, 32, avx2),
+  lowbd_entry(dc_top, 32, 64, avx2),  lowbd_entry(dc_top, 64, 32, avx2),
+  lowbd_entry(dc_top, 64, 64, avx2),
+
+  lowbd_entry(dc_left, 32, 16, avx2), lowbd_entry(dc_left, 32, 32, avx2),
+  lowbd_entry(dc_left, 32, 64, avx2), lowbd_entry(dc_left, 64, 32, avx2),
+  lowbd_entry(dc_left, 64, 64, avx2),
+
+  lowbd_entry(dc_128, 32, 16, avx2),  lowbd_entry(dc_128, 32, 32, avx2),
+  lowbd_entry(dc_128, 32, 64, avx2),  lowbd_entry(dc_128, 64, 32, avx2),
+  lowbd_entry(dc_128, 64, 64, avx2),
+
+  lowbd_entry(v, 32, 16, avx2),       lowbd_entry(v, 32, 32, avx2),
+  lowbd_entry(v, 32, 64, avx2),       lowbd_entry(v, 64, 32, avx2),
+  lowbd_entry(v, 64, 64, avx2),
+
+  lowbd_entry(h, 32, 32, avx2),
+
+  lowbd_entry(paeth, 16, 8, avx2),    lowbd_entry(paeth, 16, 16, avx2),
+  lowbd_entry(paeth, 16, 32, avx2),   lowbd_entry(paeth, 32, 16, avx2),
+  lowbd_entry(paeth, 32, 32, avx2),   lowbd_entry(paeth, 32, 64, avx2),
   lowbd_entry(paeth, 64, 32, avx2),   lowbd_entry(paeth, 64, 64, avx2),
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 };
 
 INSTANTIATE_TEST_SUITE_P(AVX2, LowbdIntraPredTest,
diff --git a/test/loopfilter_control_test.cc b/test/loopfilter_control_test.cc
index 04afa5bcc..30f04b28d 100644
--- a/test/loopfilter_control_test.cc
+++ b/test/loopfilter_control_test.cc
@@ -31,20 +31,20 @@ const int kBitrate = 500;
 std::unordered_map<std::string,
                    std::unordered_map<int, std::unordered_map<int, double>>>
     kPsnrThreshold = { { "park_joy_90p_8_420.y4m",
-                         { { 0, { { 0, 35.0 }, { 3, 35.8 } } },
-                           { 1, { { 0, 35.1 }, { 3, 35.9 } } },
-                           { 2, { { 0, 35.1 }, { 3, 36.1 } } },
-                           { 3, { { 0, 35.1 }, { 3, 36.1 } } } } },
+                         { { 0, { { 0, 33.0 }, { 3, 33.0 } } },
+                           { 1, { { 0, 33.0 }, { 3, 33.0 } } },
+                           { 2, { { 0, 33.0 }, { 3, 33.0 } } },
+                           { 3, { { 0, 33.0 }, { 3, 33.0 } } } } },
                        { "paris_352_288_30.y4m",
-                         { { 0, { { 0, 35.40 }, { 3, 36.0 } } },
-                           { 1, { { 0, 35.50 }, { 3, 36.0 } } },
-                           { 2, { { 0, 35.50 }, { 3, 36.0 } } },
-                           { 3, { { 0, 35.50 }, { 3, 36.0 } } } } },
+                         { { 0, { { 0, 33.0 }, { 3, 34.0 } } },
+                           { 1, { { 0, 33.0 }, { 3, 34.0 } } },
+                           { 2, { { 0, 33.0 }, { 3, 34.0 } } },
+                           { 3, { { 0, 33.0 }, { 3, 34.0 } } } } },
                        { "niklas_1280_720_30.y4m",
-                         { { 0, { { 0, 33.20 }, { 3, 32.90 } } },
-                           { 1, { { 0, 33.57 }, { 3, 33.22 } } },
-                           { 2, { { 0, 33.57 }, { 3, 33.22 } } },
-                           { 3, { { 0, 33.45 }, { 3, 33.10 } } } } } };
+                         { { 0, { { 0, 31.0 }, { 3, 30.0 } } },
+                           { 1, { { 0, 31.0 }, { 3, 31.0 } } },
+                           { 2, { { 0, 31.0 }, { 3, 31.0 } } },
+                           { 3, { { 0, 31.0 }, { 3, 31.0 } } } } } };
 
 typedef struct {
   const char *filename;
diff --git a/test/metadata_test.cc b/test/metadata_test.cc
index 537909f96..b48244740 100644
--- a/test/metadata_test.cc
+++ b/test/metadata_test.cc
@@ -15,7 +15,6 @@
 #include "aom/aom_image.h"
 #include "aom/internal/aom_image_internal.h"
 #include "aom_scale/yv12config.h"
-#include "av1/encoder/bitstream.h"
 #include "test/codec_factory.h"
 #include "test/encode_test_driver.h"
 #include "test/i420_video_source.h"
@@ -30,6 +29,7 @@ const uint8_t kMetadataPayloadT35[kMetadataPayloadSizeT35] = {
   0x0C, 0x0D, 0x0E, 0x0F, 0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17
 };
 
+#if !CONFIG_REALTIME_ONLY
 const size_t kMetadataPayloadSizeCll = 4;
 const uint8_t kMetadataPayloadCll[kMetadataPayloadSizeCll] = { 0xB5, 0x01, 0x02,
                                                                0x03 };
@@ -191,7 +191,7 @@ TEST_P(MetadataEncodeTest, TestMetadataEncoding) {
 
 AV1_INSTANTIATE_TEST_SUITE(MetadataEncodeTest,
                            ::testing::Values(::libaom_test::kOnePassGood));
-
+#endif  // !CONFIG_REALTIME_ONLY
 }  // namespace
 
 TEST(MetadataTest, MetadataAllocation) {
diff --git a/test/monochrome_test.cc b/test/monochrome_test.cc
index c157275d4..ca180f699 100644
--- a/test/monochrome_test.cc
+++ b/test/monochrome_test.cc
@@ -31,7 +31,7 @@ const double kPsnrThreshold[3] = { 29.0, 41.5, 41.5 };
 // kPsnrFluctuation represents the maximum allowed psnr fluctuation w.r.t first
 // frame. The indices correspond to one/two-pass, allintra and realtime
 // encoding modes.
-const double kPsnrFluctuation[3] = { 2.5, 0.3, 16.0 };
+const double kPsnrFluctuation[3] = { 2.5, 0.3, 17.0 };
 
 class MonochromeTest
     : public ::libaom_test::CodecTestWith3Params<libaom_test::TestMode, int,
@@ -183,6 +183,9 @@ TEST_P(MonochromeRealtimeTest, TestMonochromeEncoding) {
   cfg_.monochrome = 1;
   // Run at low bitrate.
   cfg_.rc_target_bitrate = 40;
+  cfg_.rc_buf_sz = 6000;
+  cfg_.rc_buf_initial_sz = 4000;
+  cfg_.rc_buf_optimal_sz = 5000;
   ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
 
   // Check that the chroma planes are equal across all frames
diff --git a/test/ratectrl_rtc_test.cc b/test/ratectrl_rtc_test.cc
index 20c4fbb17..f0cc3e3fc 100644
--- a/test/ratectrl_rtc_test.cc
+++ b/test/ratectrl_rtc_test.cc
@@ -59,6 +59,7 @@ class RcInterfaceTest : public ::libaom_test::EncoderTest,
     if (video->frame() == 0 && layer_frame_cnt_ == 0) {
       encoder->Control(AOME_SET_CPUUSED, 7);
       encoder->Control(AV1E_SET_AQ_MODE, aq_mode_);
+      encoder->Control(AV1E_SET_ENABLE_ORDER_HINT, 0);
       if (rc_cfg_.is_screen) {
         encoder->Control(AV1E_SET_TUNE_CONTENT, AOM_CONTENT_SCREEN);
       } else {
diff --git a/test/resize_test.cc b/test/resize_test.cc
index 6eacb412c..a8266da0f 100644
--- a/test/resize_test.cc
+++ b/test/resize_test.cc
@@ -250,6 +250,7 @@ TEST_P(ResizeTest, TestExternalResizeWorks) {
       AOMMAX(kInitialWidth, kInitialHeight);
   ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
 
+#if CONFIG_AV1_DECODER
   // Check we decoded the same number of frames as we attempted to encode
   ASSERT_EQ(frame_info_list_.size(), video.limit());
 
@@ -265,6 +266,7 @@ TEST_P(ResizeTest, TestExternalResizeWorks) {
     EXPECT_EQ(expected_h, info->h)
         << "Frame " << frame << " had unexpected height";
   }
+#endif
 }
 
 #if !CONFIG_REALTIME_ONLY
@@ -527,6 +529,7 @@ TEST_P(ResizeRealtimeTest, TestInternalResizeSetScaleMode1) {
   change_bitrate_ = false;
   mismatch_nframes_ = 0;
   ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
+#if CONFIG_AV1_DECODER
   // Check we decoded the same number of frames as we attempted to encode
   ASSERT_EQ(frame_info_list_.size(), video.limit());
   for (std::vector<FrameInfo>::const_iterator info = frame_info_list_.begin();
@@ -547,6 +550,9 @@ TEST_P(ResizeRealtimeTest, TestInternalResizeSetScaleMode1) {
         << "Frame " << frame << " had unexpected height";
     EXPECT_EQ(static_cast<unsigned int>(0), GetMismatchFrames());
   }
+#else
+  printf("Warning: AV1 decoder unavailable, unable to check resize count!\n");
+#endif
 }
 
 // Check the AOME_SET_SCALEMODE control by downsizing to
@@ -563,6 +569,7 @@ TEST_P(ResizeRealtimeTest, TestInternalResizeSetScaleMode1QVGA) {
   change_bitrate_ = false;
   mismatch_nframes_ = 0;
   ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
+#if CONFIG_AV1_DECODER
   // Check we decoded the same number of frames as we attempted to encode
   ASSERT_EQ(frame_info_list_.size(), video.limit());
   for (std::vector<FrameInfo>::const_iterator info = frame_info_list_.begin();
@@ -583,6 +590,9 @@ TEST_P(ResizeRealtimeTest, TestInternalResizeSetScaleMode1QVGA) {
         << "Frame " << frame << " had unexpected height";
     EXPECT_EQ(static_cast<unsigned int>(0), GetMismatchFrames());
   }
+#else
+  printf("Warning: AV1 decoder unavailable, unable to check resize count!\n");
+#endif
 }
 
 // Check the AOME_SET_SCALEMODE control by downsizing to
@@ -598,6 +608,7 @@ TEST_P(ResizeRealtimeTest, TestInternalResizeSetScaleMode2) {
   change_bitrate_ = false;
   mismatch_nframes_ = 0;
   ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
+#if CONFIG_AV1_DECODER
   // Check we decoded the same number of frames as we attempted to encode
   ASSERT_EQ(frame_info_list_.size(), video.limit());
   for (std::vector<FrameInfo>::const_iterator info = frame_info_list_.begin();
@@ -618,6 +629,9 @@ TEST_P(ResizeRealtimeTest, TestInternalResizeSetScaleMode2) {
         << "Frame " << frame << " had unexpected height";
     EXPECT_EQ(static_cast<unsigned int>(0), GetMismatchFrames());
   }
+#else
+  printf("Warning: AV1 decoder unavailable, unable to check resize count!\n");
+#endif
 }
 
 // Check the AOME_SET_SCALEMODE control by downsizing to
@@ -633,6 +647,7 @@ TEST_P(ResizeRealtimeTest, TestInternalResizeSetScaleMode3) {
   change_bitrate_ = false;
   mismatch_nframes_ = 0;
   ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
+#if CONFIG_AV1_DECODER
   // Check we decoded the same number of frames as we attempted to encode
   ASSERT_EQ(frame_info_list_.size(), video.limit());
   for (std::vector<FrameInfo>::const_iterator info = frame_info_list_.begin();
@@ -650,6 +665,9 @@ TEST_P(ResizeRealtimeTest, TestInternalResizeSetScaleMode3) {
         << "Frame " << frame << " had unexpected height";
     EXPECT_EQ(static_cast<unsigned int>(0), GetMismatchFrames());
   }
+#else
+  printf("Warning: AV1 decoder unavailable, unable to check resize count!\n");
+#endif
 }
 
 TEST_P(ResizeRealtimeTest, TestExternalResizeWorks) {
@@ -669,7 +687,7 @@ TEST_P(ResizeRealtimeTest, TestExternalResizeWorks) {
     video.change_start_resln_ = static_cast<bool>(i);
 
     ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
-
+#if CONFIG_AV1_DECODER
     // Check we decoded the same number of frames as we attempted to encode
     ASSERT_EQ(frame_info_list_.size(), video.limit());
     for (const auto &info : frame_info_list_) {
@@ -685,6 +703,9 @@ TEST_P(ResizeRealtimeTest, TestExternalResizeWorks) {
           << "Frame " << frame << " had unexpected height";
       EXPECT_EQ(static_cast<unsigned int>(0), GetMismatchFrames());
     }
+#else
+    printf("Warning: AV1 decoder unavailable, unable to check resize count!\n");
+#endif
     frame_info_list_.clear();
   }
 }
@@ -708,7 +729,7 @@ TEST_P(ResizeRealtimeTest, TestExternalResizeWorksUsePSNR) {
     video.change_start_resln_ = static_cast<bool>(i);
 
     ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
-
+#if CONFIG_AV1_DECODER
     // Check we decoded the same number of frames as we attempted to encode
     ASSERT_EQ(frame_info_list_.size(), video.limit());
     for (const auto &info : frame_info_list_) {
@@ -724,6 +745,9 @@ TEST_P(ResizeRealtimeTest, TestExternalResizeWorksUsePSNR) {
           << "Frame " << frame << " had unexpected height";
       EXPECT_EQ(static_cast<unsigned int>(0), GetMismatchFrames());
     }
+#else
+    printf("Warning: AV1 decoder unavailable, unable to check resize count!\n");
+#endif
     frame_info_list_.clear();
   }
 }
@@ -752,6 +776,7 @@ TEST_P(ResizeRealtimeTest, TestInternalResizeDown) {
   cfg_.g_forced_max_frame_height = 1280;
   ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
 
+#if CONFIG_AV1_DECODER
   unsigned int last_w = cfg_.g_w;
   unsigned int last_h = cfg_.g_h;
   int resize_down_count = 0;
@@ -767,7 +792,6 @@ TEST_P(ResizeRealtimeTest, TestInternalResizeDown) {
     }
   }
 
-#if CONFIG_AV1_DECODER
   // Verify that we get at lease 1 resize down event in this test.
   ASSERT_GE(resize_down_count, 1) << "Resizing should occur.";
   EXPECT_EQ(static_cast<unsigned int>(0), GetMismatchFrames());
@@ -803,6 +827,7 @@ TEST_P(ResizeRealtimeTest, TestInternalResizeDownUpChangeBitRate) {
   cfg_.g_forced_max_frame_height = 1280;
   ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
 
+#if CONFIG_AV1_DECODER
   unsigned int last_w = cfg_.g_w;
   unsigned int last_h = cfg_.g_h;
   unsigned int frame_number = 0;
@@ -828,7 +853,6 @@ TEST_P(ResizeRealtimeTest, TestInternalResizeDownUpChangeBitRate) {
     frame_number++;
   }
 
-#if CONFIG_AV1_DECODER
   // Verify that we get at least 2 resize events in this test.
   ASSERT_GE(resize_up_count, 1) << "Resizing up should occur at lease once.";
   ASSERT_GE(resize_down_count, 1)
@@ -867,6 +891,7 @@ TEST_P(ResizeRealtimeTest, TestInternalResizeDownUpChangeBitRateScreen) {
   cfg_.g_forced_max_frame_height = 1280;
   ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
 
+#if CONFIG_AV1_DECODER
   unsigned int last_w = cfg_.g_w;
   unsigned int last_h = cfg_.g_h;
   unsigned int frame_number = 0;
@@ -886,7 +911,6 @@ TEST_P(ResizeRealtimeTest, TestInternalResizeDownUpChangeBitRateScreen) {
     frame_number++;
   }
 
-#if CONFIG_AV1_DECODER
   // Verify that we get at least 1 resize event in this test.
   ASSERT_GE(resize_down_count, 1)
       << "Resizing down should occur at lease once.";
@@ -975,9 +999,11 @@ TEST_P(ResizeCspTest, TestResizeCspWorks) {
     cfg_.g_profile = (img_format == AOM_IMG_FMT_I420) ? 0 : 1;
     ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
 
+#if CONFIG_AV1_DECODER
     // Check we decoded the same number of frames as we attempted to encode
     ASSERT_EQ(frame_info_list_.size(), video.limit());
     frame_info_list_.clear();
+#endif
   }
 }
 
diff --git a/test/rt_end_to_end_test.cc b/test/rt_end_to_end_test.cc
index 1e9238c06..dced91542 100644
--- a/test/rt_end_to_end_test.cc
+++ b/test/rt_end_to_end_test.cc
@@ -32,33 +32,33 @@ const int kBitrate = 500;
 std::unordered_map<std::string,
                    std::unordered_map<int, std::unordered_map<int, double>>>
     kPsnrThreshold = { { "park_joy_90p_8_420.y4m",
-                         { { 5, { { 0, 35.4 }, { 3, 36.3 } } },
-                           { 6, { { 0, 35.3 }, { 3, 36.2 } } },
-                           { 7, { { 0, 34.9 }, { 3, 35.8 } } },
-                           { 8, { { 0, 35.0 }, { 3, 35.8 } } },
-                           { 9, { { 0, 34.9 }, { 3, 35.5 } } },
-                           { 10, { { 0, 34.7 }, { 3, 35.3 } } } } },
+                         { { 5, { { 0, 34.0 }, { 3, 35.0 } } },
+                           { 6, { { 0, 34.0 }, { 3, 35.0 } } },
+                           { 7, { { 0, 33.0 }, { 3, 34.0 } } },
+                           { 8, { { 0, 33.0 }, { 3, 34.0 } } },
+                           { 9, { { 0, 33.0 }, { 3, 34.0 } } },
+                           { 10, { { 0, 33.0 }, { 3, 34.0 } } } } },
                        { "paris_352_288_30.y4m",
-                         { { 5, { { 0, 36.2 }, { 3, 36.7 } } },
-                           { 6, { { 0, 36.1 }, { 3, 36.48 } } },
-                           { 7, { { 0, 35.5 }, { 3, 36.0 } } },
-                           { 8, { { 0, 35.8 }, { 3, 36.4 } } },
-                           { 9, { { 0, 35.5 }, { 3, 36.0 } } },
-                           { 10, { { 0, 35.3 }, { 3, 35.9 } } } } },
+                         { { 5, { { 0, 35.0 }, { 3, 35.0 } } },
+                           { 6, { { 0, 35.0 }, { 3, 35.0 } } },
+                           { 7, { { 0, 34.0 }, { 3, 34.0 } } },
+                           { 8, { { 0, 34.0 }, { 3, 35.0 } } },
+                           { 9, { { 0, 34.0 }, { 3, 34.0 } } },
+                           { 10, { { 0, 34.0 }, { 3, 34.0 } } } } },
                        { "niklas_1280_720_30.y4m",
-                         { { 5, { { 0, 34.4 }, { 3, 34.2 } } },
-                           { 6, { { 0, 34.1 }, { 3, 34.0 } } },
-                           { 7, { { 0, 33.5 }, { 3, 33.1 } } },
-                           { 8, { { 0, 33.3 }, { 3, 33.3 } } },
-                           { 9, { { 0, 33.3 }, { 3, 33.3 } } },
-                           { 10, { { 0, 33.1 }, { 3, 33.1 } } } } },
+                         { { 5, { { 0, 32.0 }, { 3, 32.0 } } },
+                           { 6, { { 0, 32.0 }, { 3, 32.0 } } },
+                           { 7, { { 0, 31.0 }, { 3, 31.0 } } },
+                           { 8, { { 0, 31.0 }, { 3, 31.0 } } },
+                           { 9, { { 0, 31.0 }, { 3, 31.0 } } },
+                           { 10, { { 0, 31.0 }, { 3, 31.0 } } } } },
                        { "hantro_collage_w352h288_nv12.yuv",
-                         { { 5, { { 0, 34.4 }, { 3, 34.2 } } },
-                           { 6, { { 0, 34.1 }, { 3, 34.1 } } },
-                           { 7, { { 0, 33.6 }, { 3, 33.6 } } },
-                           { 8, { { 0, 33.3 }, { 3, 33.3 } } },
-                           { 9, { { 0, 33.3 }, { 3, 33.3 } } },
-                           { 10, { { 0, 33.1 }, { 3, 33.1 } } } } } };
+                         { { 5, { { 0, 32.0 }, { 3, 32.0 } } },
+                           { 6, { { 0, 32.0 }, { 3, 32.0 } } },
+                           { 7, { { 0, 32.0 }, { 3, 32.0 } } },
+                           { 8, { { 0, 32.0 }, { 3, 32.0 } } },
+                           { 9, { { 0, 31.0 }, { 3, 31.0 } } },
+                           { 10, { { 0, 31.0 }, { 3, 31.0 } } } } } };
 
 typedef struct {
   const char *filename;
diff --git a/test/sum_squares_test.cc b/test/sum_squares_test.cc
index f9174c189..26d03613d 100644
--- a/test/sum_squares_test.cc
+++ b/test/sum_squares_test.cc
@@ -780,6 +780,7 @@ INSTANTIATE_TEST_SUITE_P(NEON_DOTPROD, Lowbd2dVarTest,
 
 #endif  // HAVE_NEON_DOTPROD
 
+#if CONFIG_AV1_HIGHBITDEPTH
 class Highbd2dVarTest : public ::testing::TestWithParam<TestFuncVar2D> {
  public:
   ~Highbd2dVarTest() override = default;
@@ -925,4 +926,5 @@ INSTANTIATE_TEST_SUITE_P(SVE, Highbd2dVarTest,
                                                          &aom_var_2d_u16_sve)));
 
 #endif  // HAVE_SVE
+#endif  // CONFIG_AV1_HIGHBITDEPTH
 }  // namespace
diff --git a/test/svc_datarate_test.cc b/test/svc_datarate_test.cc
index 7da21608c..92fce21a5 100644
--- a/test/svc_datarate_test.cc
+++ b/test/svc_datarate_test.cc
@@ -86,6 +86,7 @@ class DatarateTestSVC
     comp_pred_ = 0;
     dynamic_enable_disable_mode_ = 0;
     intra_only_ = 0;
+    intra_only_single_layer_ = false;
     frame_to_start_decoding_ = 0;
     layer_to_decode_ = 0;
     frame_sync_ = 0;
@@ -96,6 +97,8 @@ class DatarateTestSVC
     user_define_frame_qp_ = 0;
     set_speed_per_layer_ = false;
     simulcast_mode_ = false;
+    use_last_as_scaled_ = false;
+    use_last_as_scaled_single_ref_ = false;
   }
 
   void PreEncodeFrameHook(::libaom_test::VideoSource *video,
@@ -144,7 +147,8 @@ class DatarateTestSVC
         video->frame(), &layer_id_, &ref_frame_config_, &ref_frame_comp_pred_,
         spatial_layer_id, multi_ref_, comp_pred_,
         (video->frame() % cfg_.kf_max_dist) == 0, dynamic_enable_disable_mode_,
-        rps_mode_, rps_recovery_frame_, simulcast_mode_);
+        rps_mode_, rps_recovery_frame_, simulcast_mode_, use_last_as_scaled_,
+        use_last_as_scaled_single_ref_);
     if (intra_only_ == 1 && frame_sync_ > 0) {
       // Set an Intra-only frame on SL0 at frame_sync_.
       // In order to allow decoding to start on SL0 in mid-sequence we need to
@@ -653,7 +657,8 @@ class DatarateTestSVC
       aom_svc_ref_frame_comp_pred_t *ref_frame_comp_pred, int spatial_layer,
       int multi_ref, int comp_pred, int is_key_frame,
       int dynamic_enable_disable_mode, int rps_mode, int rps_recovery_frame,
-      int simulcast_mode) {
+      int simulcast_mode, bool use_last_as_scaled,
+      bool use_last_as_scaled_single_ref) {
     int lag_index = 0;
     int base_count = frame_cnt >> 2;
     layer_id->spatial_layer_id = spatial_layer;
@@ -675,9 +680,20 @@ class DatarateTestSVC
     // Always reference LAST.
     ref_frame_config->reference[0] = 1;
     if (number_temporal_layers_ == 1 && number_spatial_layers_ == 1) {
+      layer_id->temporal_layer_id = 0;
       ref_frame_config->refresh[0] = 1;
       if (rps_mode)
         ref_config_rps(ref_frame_config, frame_cnt, rps_recovery_frame);
+      if (intra_only_single_layer_) {
+        // This repros the crash in Bug: 363016123.
+        ref_frame_config->ref_idx[0] = 0;
+        ref_frame_config->ref_idx[3] = 1;
+        ref_frame_config->ref_idx[6] = 2;
+        if (frame_cnt == 1) {
+          for (int i = 0; i < INTER_REFS_PER_FRAME; i++)
+            ref_frame_config->reference[i] = 0;
+        }
+      }
     }
     if (number_temporal_layers_ == 2 && number_spatial_layers_ == 1) {
       // 2-temporal layer.
@@ -791,6 +807,11 @@ class DatarateTestSVC
         // Update slot 1 (LAST).
         for (int i = 0; i < 7; i++) ref_frame_config->ref_idx[i] = 0;
         ref_frame_config->ref_idx[0] = 1;
+        if (use_last_as_scaled) {
+          for (int i = 0; i < 7; i++) ref_frame_config->ref_idx[i] = 1;
+          ref_frame_config->ref_idx[0] = 0;
+          ref_frame_config->ref_idx[3] = 1;
+        }
         ref_frame_config->refresh[1] = 1;
       } else if (layer_id->spatial_layer_id == 2) {
         // Reference LAST and GOLDEN. Set buffer_idx for LAST to slot 2
@@ -806,7 +827,12 @@ class DatarateTestSVC
         }
       }
       // Reference GOLDEN.
-      if (layer_id->spatial_layer_id > 0) ref_frame_config->reference[3] = 1;
+      if (layer_id->spatial_layer_id > 0) {
+        if (use_last_as_scaled_single_ref)
+          ref_frame_config->reference[3] = 0;
+        else
+          ref_frame_config->reference[3] = 1;
+      }
     } else if (number_temporal_layers_ == 3 && number_spatial_layers_ == 3) {
       if (simulcast_mode) {
         ref_config_simulcast3SL3TL(ref_frame_config, layer_id, is_key_frame,
@@ -899,11 +925,13 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.60)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // Top temporal layers are non_reference, so exlcude them from
     // mismatch count, since loopfilter/cdef is not applied for these on
     // encoder side, but is always applied on decoder.
     // This means 150 = #frames(300) - #TL2_frames(150).
     EXPECT_EQ((int)GetMismatchFrames(), 150);
+#endif
   }
 
   virtual void SetFrameQpSVC3TL1SLTest() {
@@ -1000,6 +1028,7 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 2.0)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // Top temporal layers are non_reference, so exlcude them from
     // mismatch count, since loopfilter/cdef is not applied for these on
     // encoder side, but is always applied on decoder.
@@ -1007,6 +1036,7 @@ class DatarateTestSVC
     // We use LE for screen since loopfilter level can become very small
     // or zero and then the frame is not a mismatch.
     EXPECT_LE((int)GetMismatchFrames(), 30);
+#endif
   }
 
   virtual void BasicRateTargetingSVC2TL1SLScreenDropFrameTest() {
@@ -1038,6 +1068,7 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.8)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // Top temporal layers are non_reference, so exlcude them from
     // mismatch count, since loopfilter/cdef is not applied for these on
     // encoder side, but is always applied on decoder.
@@ -1045,6 +1076,7 @@ class DatarateTestSVC
     // We use LE for screen since loopfilter level can become very small
     // or zero and then the frame is not a mismatch.
     EXPECT_LE((int)GetMismatchFrames(), 150);
+#endif
   }
 
   virtual void BasicRateTargetingSVC1TL3SLScreenTest() {
@@ -1076,7 +1108,9 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.5)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     EXPECT_EQ((int)GetMismatchFrames(), 0);
+#endif
   }
 
   virtual void BasicRateTargetingSVC1TL1SLScreenScCutsMotionTest() {
@@ -1107,7 +1141,9 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.7)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     EXPECT_EQ((int)GetMismatchFrames(), 0);
+#endif
   }
 
   virtual void BasicRateTargetingSVC3TL1SLResizeTest() {
@@ -1140,6 +1176,7 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.60)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     unsigned int last_w = cfg_.g_w;
     unsigned int last_h = cfg_.g_h;
     int resize_down_count = 0;
@@ -1156,6 +1193,9 @@ class DatarateTestSVC
     }
     // Must be at least one resize down.
     ASSERT_GE(resize_down_count, 1);
+#else
+    printf("Warning: AV1 decoder unavailable, unable to check resize count!\n");
+#endif
   }
 
   virtual void BasicRateTargetingSVC1TL2SLTest() {
@@ -1232,9 +1272,11 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.60)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // Only base spatial layer is decoded and there are no non-referenece
     // frames on S0, so #mismatch must be 0.
     EXPECT_EQ((int)GetMismatchFrames(), 0);
+#endif
   }
 
   virtual void BasicRateTargetingSVC3TL3SLIntraMidSeqDecodeAll() {
@@ -1280,9 +1322,11 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.60)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // All 3 spatial layers are decoded, starting at frame 0, so there are
     // and there 300/2 = 150 non-reference frames, so mismatch is 150.
     EXPECT_EQ((int)GetMismatchFrames(), 150);
+#endif
   }
 
   virtual void BasicRateTargetingSVC3TL3SLSimulcast() {
@@ -1332,11 +1376,13 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.7)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // Only top spatial layer (SL2) is decoded, starting at frame 150
     // (frame_to_start_decoding_), so there (300 - 150) / 2 = 75
     // non-reference frames, so mismatch is 75.
     int num_mismatch = (num_frames - frame_to_start_decoding_) / 2;
     EXPECT_EQ((int)GetMismatchFrames(), num_mismatch);
+#endif
   }
 
   virtual void BasicRateTargetingSVC1TL2SLIntraOnlyTest() {
@@ -1369,6 +1415,32 @@ class DatarateTestSVC
     }
   }
 
+  virtual void BasicRateTargetingSVC1TL1SLIntraOnlyTest() {
+    cfg_.rc_buf_initial_sz = 500;
+    cfg_.rc_buf_optimal_sz = 500;
+    cfg_.rc_buf_sz = 1000;
+    cfg_.rc_dropframe_thresh = 0;
+    cfg_.rc_min_quantizer = 0;
+    cfg_.rc_max_quantizer = 63;
+    cfg_.rc_end_usage = AOM_CBR;
+    cfg_.g_lag_in_frames = 0;
+    cfg_.g_error_resilient = 0;
+
+    ::libaom_test::I420VideoSource video("hantro_collage_w352h288.yuv", 352,
+                                         288, 30, 1, 0, 300);
+    const int bitrate_array[2] = { 300, 600 };
+    cfg_.rc_target_bitrate = bitrate_array[GET_PARAM(4)];
+    ResetModel();
+    intra_only_single_layer_ = true;
+    number_temporal_layers_ = 1;
+    number_spatial_layers_ = 1;
+    ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
+    ASSERT_GE(effective_datarate_tl[0], cfg_.rc_target_bitrate * 0.80)
+        << " The datarate for the file is lower than target by too much!";
+    ASSERT_LE(effective_datarate_tl[0], cfg_.rc_target_bitrate * 1.60)
+        << " The datarate for the file is greater than target by too much!";
+  }
+
   virtual void BasicRateTargetingSVC1TL3SLTest() {
     cfg_.rc_buf_initial_sz = 500;
     cfg_.rc_buf_optimal_sz = 500;
@@ -1399,6 +1471,69 @@ class DatarateTestSVC
     }
   }
 
+  virtual void BasicRateTargetingSVC1TL3SLLastIsScaledTest() {
+    cfg_.rc_buf_initial_sz = 500;
+    cfg_.rc_buf_optimal_sz = 500;
+    cfg_.rc_buf_sz = 1000;
+    cfg_.rc_dropframe_thresh = 0;
+    cfg_.rc_min_quantizer = 0;
+    cfg_.rc_max_quantizer = 63;
+    cfg_.rc_end_usage = AOM_CBR;
+    cfg_.g_lag_in_frames = 0;
+    cfg_.g_error_resilient = 0;
+
+    ::libaom_test::I420VideoSource video("hantro_collage_w352h288.yuv", 352,
+                                         288, 30, 1, 0, 300);
+    const int bitrate_array[2] = { 500, 1000 };
+    cfg_.rc_target_bitrate = bitrate_array[GET_PARAM(4)];
+    ResetModel();
+    number_temporal_layers_ = 1;
+    number_spatial_layers_ = 3;
+    use_last_as_scaled_ = true;
+    target_layer_bitrate_[0] = 1 * cfg_.rc_target_bitrate / 8;
+    target_layer_bitrate_[1] = 3 * cfg_.rc_target_bitrate / 8;
+    target_layer_bitrate_[2] = 4 * cfg_.rc_target_bitrate / 8;
+    ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
+    for (int i = 0; i < number_temporal_layers_ * number_spatial_layers_; i++) {
+      ASSERT_GE(effective_datarate_tl[i], target_layer_bitrate_[i] * 0.80)
+          << " The datarate for the file is lower than target by too much!";
+      ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.38)
+          << " The datarate for the file is greater than target by too much!";
+    }
+  }
+
+  virtual void BasicRateTargetingSVC1TL3SLLastIsScaledSingleRefTest() {
+    cfg_.rc_buf_initial_sz = 500;
+    cfg_.rc_buf_optimal_sz = 500;
+    cfg_.rc_buf_sz = 1000;
+    cfg_.rc_dropframe_thresh = 0;
+    cfg_.rc_min_quantizer = 0;
+    cfg_.rc_max_quantizer = 63;
+    cfg_.rc_end_usage = AOM_CBR;
+    cfg_.g_lag_in_frames = 0;
+    cfg_.g_error_resilient = 0;
+
+    ::libaom_test::I420VideoSource video("hantro_collage_w352h288.yuv", 352,
+                                         288, 30, 1, 0, 300);
+    const int bitrate_array[2] = { 500, 1000 };
+    cfg_.rc_target_bitrate = bitrate_array[GET_PARAM(4)];
+    ResetModel();
+    number_temporal_layers_ = 1;
+    number_spatial_layers_ = 3;
+    use_last_as_scaled_ = true;
+    use_last_as_scaled_single_ref_ = true;
+    target_layer_bitrate_[0] = 1 * cfg_.rc_target_bitrate / 8;
+    target_layer_bitrate_[1] = 3 * cfg_.rc_target_bitrate / 8;
+    target_layer_bitrate_[2] = 4 * cfg_.rc_target_bitrate / 8;
+    ASSERT_NO_FATAL_FAILURE(RunLoop(&video));
+    for (int i = 0; i < number_temporal_layers_ * number_spatial_layers_; i++) {
+      ASSERT_GE(effective_datarate_tl[i], target_layer_bitrate_[i] * 0.80)
+          << " The datarate for the file is lower than target by too much!";
+      ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.38)
+          << " The datarate for the file is greater than target by too much!";
+    }
+  }
+
   virtual void BasicRateTargetingSVC1TL3SLMultiRefTest() {
     cfg_.rc_buf_initial_sz = 500;
     cfg_.rc_buf_optimal_sz = 500;
@@ -1919,11 +2054,13 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.60)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // Test that no mismatches have been found.
     std::cout << "          Decoded frames: " << GetDecodedFrames() << "\n";
     std::cout << "          Mismatch frames: " << GetMismatchFrames() << "\n";
     EXPECT_EQ(300 - GetDecodedFrames(), drop_frames_);
     EXPECT_EQ((int)GetMismatchFrames(), 0);
+#endif
   }
 
   virtual void BasicRateTargetingSVC3TL1SLDropAllEnhTest() {
@@ -1964,11 +2101,13 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.60)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // Test that no mismatches have been found.
     std::cout << "          Decoded frames: " << GetDecodedFrames() << "\n";
     std::cout << "          Mismatch frames: " << GetMismatchFrames() << "\n";
     EXPECT_EQ(300 - GetDecodedFrames(), drop_frames_);
     EXPECT_EQ((int)GetMismatchFrames(), 0);
+#endif
   }
 
   virtual void BasicRateTargetingSVC3TL1SLDropTL2EnhTest() {
@@ -2009,11 +2148,13 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.60)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // Test that no mismatches have been found.
     std::cout << "          Decoded frames: " << GetDecodedFrames() << "\n";
     std::cout << "          Mismatch frames: " << GetMismatchFrames() << "\n";
     EXPECT_EQ(300 - GetDecodedFrames(), drop_frames_);
     EXPECT_EQ((int)GetMismatchFrames(), 0);
+#endif
   }
 
   virtual void BasicRateTargetingSVC3TL1SLDropAllEnhFrameERTest() {
@@ -2055,11 +2196,13 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.60)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // Test that no mismatches have been found.
     std::cout << "          Decoded frames: " << GetDecodedFrames() << "\n";
     std::cout << "          Mismatch frames: " << GetMismatchFrames() << "\n";
     EXPECT_EQ(300 - GetDecodedFrames(), drop_frames_);
     EXPECT_EQ((int)GetMismatchFrames(), 0);
+#endif
   }
 
   virtual void BasicRateTargetingSVC3TL1SLDropSetEnhFrameERTest() {
@@ -2086,12 +2229,16 @@ class DatarateTestSVC
     // so we can continue decoding without mismatch (since LAST is the
     // only reference and error_resilient = 1 on TL1/TL2 frames).
     int n = 0;
+#if CONFIG_AV1_DECODER
     int num_nonref = 300 / 2;
+#endif
     for (int i = 101; i < 200; i++) {
       if (i % 4 != 0) {
         drop_frames_list_[n] = i;
         n++;
+#if CONFIG_AV1_DECODER
         if (i % 2 != 0) num_nonref -= 1;
+#endif
       }
     }
     drop_frames_ = n;
@@ -2106,11 +2253,13 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.60)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // Test that no mismatches have been found.
     std::cout << "          Decoded frames: " << GetDecodedFrames() << "\n";
     std::cout << "          Mismatch frames: " << GetMismatchFrames() << "\n";
     EXPECT_EQ(300 - GetDecodedFrames(), drop_frames_);
     EXPECT_EQ((int)GetMismatchFrames(), num_nonref);
+#endif
   }
 
   virtual void BasicRateTargetingSVC2TL1SLDropSetEnhER0Test() {
@@ -2137,12 +2286,16 @@ class DatarateTestSVC
     // so we can continue decoding without mismatch (since LAST is the
     // only reference).
     int n = 0;
+#if CONFIG_AV1_DECODER
     int num_nonref = 300 / 2;
+#endif
     for (int i = 101; i < 200; i++) {
       if (i % 2 != 0) {
         drop_frames_list_[n] = i;
         n++;
+#if CONFIG_AV1_DECODER
         if (i % 2 != 0) num_nonref -= 1;
+#endif
       }
     }
     drop_frames_ = n;
@@ -2156,11 +2309,13 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.60)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // Test that no mismatches have been found.
     std::cout << "          Decoded frames: " << GetDecodedFrames() << "\n";
     std::cout << "          Mismatch frames: " << GetMismatchFrames() << "\n";
     EXPECT_EQ(300 - GetDecodedFrames(), drop_frames_);
     EXPECT_EQ((int)GetMismatchFrames(), num_nonref);
+#endif
   }
 
   virtual void BasicRateTargetingSVC3TL1SLDropSetEnhER0Test() {
@@ -2187,12 +2342,16 @@ class DatarateTestSVC
     // so we can continue decoding without mismatch (since LAST is the
     // only reference).
     int n = 0;
+#if CONFIG_AV1_DECODER
     int num_nonref = 300 / 2;
+#endif
     for (int i = 101; i < 200; i++) {
       if (i % 4 != 0) {
         drop_frames_list_[n] = i;
         n++;
+#if CONFIG_AV1_DECODER
         if (i % 2 != 0) num_nonref -= 1;
+#endif
       }
     }
     drop_frames_ = n;
@@ -2207,11 +2366,13 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.60)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // Test that no mismatches have been found.
     std::cout << "          Decoded frames: " << GetDecodedFrames() << "\n";
     std::cout << "          Mismatch frames: " << GetMismatchFrames() << "\n";
     EXPECT_EQ(300 - GetDecodedFrames(), drop_frames_);
     EXPECT_EQ((int)GetMismatchFrames(), num_nonref);
+#endif
   }
 
   virtual void BasicRateTargetingSVC3TL3SLDropSetEnhER0Test() {
@@ -2236,12 +2397,16 @@ class DatarateTestSVC
     // only reference).
     // Drop here means drop whole superframe.
     int n = 0;
+#if CONFIG_AV1_DECODER
     int num_nonref = 300 / 2;
+#endif
     for (int i = 101; i < 200; i++) {
       if (i % 4 != 0) {
         drop_frames_list_[n] = i;
         n++;
+#if CONFIG_AV1_DECODER
         if (i % 2 != 0) num_nonref -= 1;
+#endif
       }
     }
     number_temporal_layers_ = 3;
@@ -2270,11 +2435,13 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.60)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // Test that no mismatches have been found.
     std::cout << "          Decoded frames: " << GetDecodedFrames() << "\n";
     std::cout << "          Mismatch frames: " << GetMismatchFrames() << "\n";
     EXPECT_EQ(300 * number_spatial_layers_ - GetDecodedFrames(), drop_frames_);
     EXPECT_EQ((int)GetMismatchFrames(), num_nonref);
+#endif
   }
 
   virtual void BasicRateTargetingSVC3TL1SLMultiRefCompoundTest() {
@@ -2413,11 +2580,13 @@ class DatarateTestSVC
       ASSERT_LE(effective_datarate_tl[i], target_layer_bitrate_[i] * 1.60)
           << " The datarate for the file is greater than target by too much!";
     }
+#if CONFIG_AV1_DECODER
     // Test that no mismatches have been found.
     std::cout << "          Decoded frames: " << GetDecodedFrames() << "\n";
     std::cout << "          Mismatch frames: " << GetMismatchFrames() << "\n";
     EXPECT_EQ(300 - GetDecodedFrames(), drop_frames_);
     EXPECT_EQ((int)GetMismatchFrames(), 0);
+#endif
   }
 
   int layer_frame_cnt_;
@@ -2442,6 +2611,7 @@ class DatarateTestSVC
   int comp_pred_;
   int dynamic_enable_disable_mode_;
   int intra_only_;
+  int intra_only_single_layer_;
   unsigned int frame_to_start_decoding_;
   unsigned int layer_to_decode_;
   unsigned int frame_sync_;
@@ -2450,6 +2620,8 @@ class DatarateTestSVC
   int rps_mode_;
   int rps_recovery_frame_;
   int simulcast_mode_;
+  bool use_last_as_scaled_;
+  bool use_last_as_scaled_single_ref_;
 
   int user_define_frame_qp_;
   int frame_qp_;
@@ -2532,11 +2704,30 @@ TEST_P(DatarateTestSVC, BasicRateTargetingSVC1TL2SLIntraOnly) {
   BasicRateTargetingSVC1TL2SLIntraOnlyTest();
 }
 
+// Check basic rate targeting for CBR, for 1 spatial layers, 1 temporal,
+// with Intra-only frame (frame with no references) inserted in the stream.
+TEST_P(DatarateTestSVC, BasicRateTargetingSVC1TL1SLIntraOnly) {
+  BasicRateTargetingSVC1TL1SLIntraOnlyTest();
+}
+
 // Check basic rate targeting for CBR, for 3 spatial layers, 1 temporal.
 TEST_P(DatarateTestSVC, BasicRateTargetingSVC1TL3SL) {
   BasicRateTargetingSVC1TL3SLTest();
 }
 
+// Check basic rate targeting for CBR, for 3 spatial layers, 1 temporal.
+// Force the spatial reference to be LAST, with a second temporal
+// reference (GOLDEN).
+TEST_P(DatarateTestSVC, BasicRateTargetingSVC1TL3SLLastIsScaled) {
+  BasicRateTargetingSVC1TL3SLLastIsScaledTest();
+}
+
+// Check basic rate targeting for CBR, for 3 spatial layers, 1 temporal.
+// Force the spatial reference to be LAST, and force only 1 reference.
+TEST_P(DatarateTestSVC, BasicRateTargetingSVC1TL3SLastIsScaledSingleRef) {
+  BasicRateTargetingSVC1TL3SLLastIsScaledSingleRefTest();
+}
+
 // Check basic rate targeting for CBR, for 3 spatial layers, 1 temporal,
 // with additional temporal reference for top spatial layer.
 TEST_P(DatarateTestSVC, BasicRateTargetingSVC1TL3SLMultiRef) {
diff --git a/test/test.cmake b/test/test.cmake
index 68caf0c22..891e23b4c 100644
--- a/test/test.cmake
+++ b/test/test.cmake
@@ -86,7 +86,6 @@ list(APPEND AOM_UNIT_TEST_ENCODER_SOURCES
             "${AOM_ROOT}/test/horz_superres_test.cc"
             "${AOM_ROOT}/test/i420_video_source.h"
             "${AOM_ROOT}/test/level_test.cc"
-            "${AOM_ROOT}/test/metadata_test.cc"
             "${AOM_ROOT}/test/monochrome_test.cc"
             "${AOM_ROOT}/test/postproc_filters_test.cc"
             "${AOM_ROOT}/test/resize_test.cc"
@@ -132,7 +131,6 @@ if(CONFIG_REALTIME_ONLY)
                    "${AOM_ROOT}/test/gf_pyr_height_test.cc"
                    "${AOM_ROOT}/test/horz_superres_test.cc"
                    "${AOM_ROOT}/test/level_test.cc"
-                   "${AOM_ROOT}/test/metadata_test.cc"
                    "${AOM_ROOT}/test/monochrome_test.cc"
                    "${AOM_ROOT}/test/postproc_filters_test.cc"
                    "${AOM_ROOT}/test/sharpness_test.cc")
@@ -142,6 +140,7 @@ if(NOT BUILD_SHARED_LIBS)
   list(APPEND AOM_UNIT_TEST_COMMON_SOURCES
               "${AOM_ROOT}/test/aom_mem_test.cc"
               "${AOM_ROOT}/test/av1_common_int_test.cc"
+              "${AOM_ROOT}/test/av1_scale_test.cc"
               "${AOM_ROOT}/test/cdef_test.cc"
               "${AOM_ROOT}/test/cfl_test.cc"
               "${AOM_ROOT}/test/convolve_test.cc"
@@ -156,6 +155,13 @@ if(NOT BUILD_SHARED_LIBS)
               "${AOM_ROOT}/test/simd_cmp_impl.inc"
               "${AOM_ROOT}/test/simd_impl.h")
 
+  if(CONFIG_REALTIME_ONLY AND NOT CONFIG_AV1_DECODER)
+    list(REMOVE_ITEM AOM_UNIT_TEST_COMMON_SOURCES "${AOM_ROOT}/test/cfl_test.cc"
+                     "${AOM_ROOT}/test/hiprec_convolve_test.cc"
+                     "${AOM_ROOT}/test/hiprec_convolve_test_util.cc"
+                     "${AOM_ROOT}/test/hiprec_convolve_test_util.h")
+  endif()
+
   if(HAVE_SSE2)
     list(APPEND AOM_UNIT_TEST_COMMON_INTRIN_SSE2
                 "${AOM_ROOT}/test/simd_cmp_sse2.cc")
@@ -214,10 +220,10 @@ if(NOT BUILD_SHARED_LIBS)
               "${AOM_ROOT}/test/horver_correlation_test.cc"
               "${AOM_ROOT}/test/masked_sad_test.cc"
               "${AOM_ROOT}/test/masked_variance_test.cc"
+              "${AOM_ROOT}/test/metadata_test.cc"
               "${AOM_ROOT}/test/minmax_test.cc"
               "${AOM_ROOT}/test/motion_vector_test.cc"
               "${AOM_ROOT}/test/mv_cost_test.cc"
-              "${AOM_ROOT}/test/noise_model_test.cc"
               "${AOM_ROOT}/test/obmc_sad_test.cc"
               "${AOM_ROOT}/test/obmc_variance_test.cc"
               "${AOM_ROOT}/test/pickrst_test.cc"
@@ -260,6 +266,7 @@ if(NOT BUILD_SHARED_LIBS)
                 "${AOM_ROOT}/test/film_grain_table_test.cc"
                 "${AOM_ROOT}/test/kf_test.cc"
                 "${AOM_ROOT}/test/lossless_test.cc"
+                "${AOM_ROOT}/test/noise_model_test.cc"
                 "${AOM_ROOT}/test/quant_test.cc"
                 "${AOM_ROOT}/test/ratectrl_test.cc"
                 "${AOM_ROOT}/test/rd_test.cc"
@@ -282,11 +289,14 @@ if(NOT BUILD_SHARED_LIBS)
                      "${AOM_ROOT}/test/altref_test.cc"
                      "${AOM_ROOT}/test/av1_encoder_parms_get_to_decoder.cc"
                      "${AOM_ROOT}/test/av1_ext_tile_test.cc"
+                     "${AOM_ROOT}/test/binary_codes_test.cc"
                      "${AOM_ROOT}/test/cnn_test.cc"
                      "${AOM_ROOT}/test/decode_multithreaded_test.cc"
                      "${AOM_ROOT}/test/error_resilience_test.cc"
+                     "${AOM_ROOT}/test/film_grain_table_test.cc"
                      "${AOM_ROOT}/test/kf_test.cc"
                      "${AOM_ROOT}/test/lossless_test.cc"
+                     "${AOM_ROOT}/test/noise_model_test.cc"
                      "${AOM_ROOT}/test/sb_multipass_test.cc"
                      "${AOM_ROOT}/test/sb_qp_sweep_test.cc"
                      "${AOM_ROOT}/test/selfguided_filter_test.cc"
diff --git a/test/test_intra_pred_speed.cc b/test/test_intra_pred_speed.cc
index b4849edcb..65468d9d1 100644
--- a/test/test_intra_pred_speed.cc
+++ b/test/test_intra_pred_speed.cc
@@ -421,13 +421,14 @@ INTRA_PRED_TEST(C, TX_4X8, aom_dc_predictor_4x8_c, aom_dc_left_predictor_4x8_c,
                 aom_v_predictor_4x8_c, aom_h_predictor_4x8_c,
                 aom_paeth_predictor_4x8_c, aom_smooth_predictor_4x8_c,
                 aom_smooth_v_predictor_4x8_c, aom_smooth_h_predictor_4x8_c)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(C, TX_4X16, aom_dc_predictor_4x16_c,
                 aom_dc_left_predictor_4x16_c, aom_dc_top_predictor_4x16_c,
                 aom_dc_128_predictor_4x16_c, aom_v_predictor_4x16_c,
                 aom_h_predictor_4x16_c, aom_paeth_predictor_4x16_c,
                 aom_smooth_predictor_4x16_c, aom_smooth_v_predictor_4x16_c,
                 aom_smooth_h_predictor_4x16_c)
-
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #if HAVE_SSE2
 INTRA_PRED_TEST(SSE2, TX_4X4, aom_dc_predictor_4x4_sse2,
                 aom_dc_left_predictor_4x4_sse2, aom_dc_top_predictor_4x4_sse2,
@@ -437,10 +438,12 @@ INTRA_PRED_TEST(SSE2, TX_4X8, aom_dc_predictor_4x8_sse2,
                 aom_dc_left_predictor_4x8_sse2, aom_dc_top_predictor_4x8_sse2,
                 aom_dc_128_predictor_4x8_sse2, aom_v_predictor_4x8_sse2,
                 aom_h_predictor_4x8_sse2, nullptr, nullptr, nullptr, nullptr)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(SSE2, TX_4X16, aom_dc_predictor_4x16_sse2,
                 aom_dc_left_predictor_4x16_sse2, aom_dc_top_predictor_4x16_sse2,
                 aom_dc_128_predictor_4x16_sse2, aom_v_predictor_4x16_sse2,
                 aom_h_predictor_4x16_sse2, nullptr, nullptr, nullptr, nullptr)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_SSE2
 
 #if HAVE_SSSE3
@@ -454,11 +457,13 @@ INTRA_PRED_TEST(SSSE3, TX_4X8, nullptr, nullptr, nullptr, nullptr, nullptr,
                 aom_smooth_predictor_4x8_ssse3,
                 aom_smooth_v_predictor_4x8_ssse3,
                 aom_smooth_h_predictor_4x8_ssse3)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(SSSE3, TX_4X16, nullptr, nullptr, nullptr, nullptr, nullptr,
                 nullptr, aom_paeth_predictor_4x16_ssse3,
                 aom_smooth_predictor_4x16_ssse3,
                 aom_smooth_v_predictor_4x16_ssse3,
                 aom_smooth_h_predictor_4x16_ssse3)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_SSSE3
 
 #if HAVE_NEON
@@ -474,6 +479,7 @@ INTRA_PRED_TEST(NEON, TX_4X8, aom_dc_predictor_4x8_neon,
                 aom_h_predictor_4x8_neon, aom_paeth_predictor_4x8_neon,
                 aom_smooth_predictor_4x8_neon, aom_smooth_v_predictor_4x8_neon,
                 aom_smooth_h_predictor_4x8_neon)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(NEON, TX_4X16, aom_dc_predictor_4x16_neon,
                 aom_dc_left_predictor_4x16_neon, aom_dc_top_predictor_4x16_neon,
                 aom_dc_128_predictor_4x16_neon, aom_v_predictor_4x16_neon,
@@ -481,6 +487,7 @@ INTRA_PRED_TEST(NEON, TX_4X16, aom_dc_predictor_4x16_neon,
                 aom_smooth_predictor_4x16_neon,
                 aom_smooth_v_predictor_4x16_neon,
                 aom_smooth_h_predictor_4x16_neon)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_NEON
 
 // -----------------------------------------------------------------------------
@@ -503,12 +510,14 @@ INTRA_PRED_TEST(C, TX_8X16, aom_dc_predictor_8x16_c,
                 aom_h_predictor_8x16_c, aom_paeth_predictor_8x16_c,
                 aom_smooth_predictor_8x16_c, aom_smooth_v_predictor_8x16_c,
                 aom_smooth_h_predictor_8x16_c)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(C, TX_8X32, aom_dc_predictor_8x32_c,
                 aom_dc_left_predictor_8x32_c, aom_dc_top_predictor_8x32_c,
                 aom_dc_128_predictor_8x32_c, aom_v_predictor_8x32_c,
                 aom_h_predictor_8x32_c, aom_paeth_predictor_8x32_c,
                 aom_smooth_predictor_8x32_c, aom_smooth_v_predictor_8x32_c,
                 aom_smooth_h_predictor_8x32_c)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #if HAVE_SSE2
 INTRA_PRED_TEST(SSE2, TX_8X8, aom_dc_predictor_8x8_sse2,
@@ -523,10 +532,12 @@ INTRA_PRED_TEST(SSE2, TX_8X16, aom_dc_predictor_8x16_sse2,
                 aom_dc_left_predictor_8x16_sse2, aom_dc_top_predictor_8x16_sse2,
                 aom_dc_128_predictor_8x16_sse2, aom_v_predictor_8x16_sse2,
                 aom_h_predictor_8x16_sse2, nullptr, nullptr, nullptr, nullptr)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(SSE2, TX_8X32, aom_dc_predictor_8x32_sse2,
                 aom_dc_left_predictor_8x32_sse2, aom_dc_top_predictor_8x32_sse2,
                 aom_dc_128_predictor_8x32_sse2, aom_v_predictor_8x32_sse2,
                 aom_h_predictor_8x32_sse2, nullptr, nullptr, nullptr, nullptr)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_SSE2
 
 #if HAVE_SSSE3
@@ -545,11 +556,13 @@ INTRA_PRED_TEST(SSSE3, TX_8X16, nullptr, nullptr, nullptr, nullptr, nullptr,
                 aom_smooth_predictor_8x16_ssse3,
                 aom_smooth_v_predictor_8x16_ssse3,
                 aom_smooth_h_predictor_8x16_ssse3)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(SSSE3, TX_8X32, nullptr, nullptr, nullptr, nullptr, nullptr,
                 nullptr, aom_paeth_predictor_8x32_ssse3,
                 aom_smooth_predictor_8x32_ssse3,
                 aom_smooth_v_predictor_8x32_ssse3,
                 aom_smooth_h_predictor_8x32_ssse3)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_SSSE3
 
 #if HAVE_NEON
@@ -572,6 +585,7 @@ INTRA_PRED_TEST(NEON, TX_8X16, aom_dc_predictor_8x16_neon,
                 aom_smooth_predictor_8x16_neon,
                 aom_smooth_v_predictor_8x16_neon,
                 aom_smooth_h_predictor_8x16_neon)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(NEON, TX_8X32, aom_dc_predictor_8x32_neon,
                 aom_dc_left_predictor_8x32_neon, aom_dc_top_predictor_8x32_neon,
                 aom_dc_128_predictor_8x32_neon, aom_v_predictor_8x32_neon,
@@ -579,6 +593,7 @@ INTRA_PRED_TEST(NEON, TX_8X32, aom_dc_predictor_8x32_neon,
                 aom_smooth_predictor_8x32_neon,
                 aom_smooth_v_predictor_8x32_neon,
                 aom_smooth_h_predictor_8x32_neon)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_NEON
 
 // -----------------------------------------------------------------------------
@@ -602,6 +617,7 @@ INTRA_PRED_TEST(C, TX_16X32, aom_dc_predictor_16x32_c,
                 aom_h_predictor_16x32_c, aom_paeth_predictor_16x32_c,
                 aom_smooth_predictor_16x32_c, aom_smooth_v_predictor_16x32_c,
                 aom_smooth_h_predictor_16x32_c)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(C, TX_16X4, aom_dc_predictor_16x4_c,
                 aom_dc_left_predictor_16x4_c, aom_dc_top_predictor_16x4_c,
                 aom_dc_128_predictor_16x4_c, aom_v_predictor_16x4_c,
@@ -614,6 +630,7 @@ INTRA_PRED_TEST(C, TX_16X64, aom_dc_predictor_16x64_c,
                 aom_h_predictor_16x64_c, aom_paeth_predictor_16x64_c,
                 aom_smooth_predictor_16x64_c, aom_smooth_v_predictor_16x64_c,
                 aom_smooth_h_predictor_16x64_c)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #if HAVE_SSE2
 INTRA_PRED_TEST(SSE2, TX_16X16, aom_dc_predictor_16x16_sse2,
@@ -630,6 +647,7 @@ INTRA_PRED_TEST(SSE2, TX_16X32, aom_dc_predictor_16x32_sse2,
                 aom_dc_top_predictor_16x32_sse2,
                 aom_dc_128_predictor_16x32_sse2, aom_v_predictor_16x32_sse2,
                 aom_h_predictor_16x32_sse2, nullptr, nullptr, nullptr, nullptr)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(SSE2, TX_16X64, aom_dc_predictor_16x64_sse2,
                 aom_dc_left_predictor_16x64_sse2,
                 aom_dc_top_predictor_16x64_sse2,
@@ -639,6 +657,7 @@ INTRA_PRED_TEST(SSE2, TX_16X4, aom_dc_predictor_16x4_sse2,
                 aom_dc_left_predictor_16x4_sse2, aom_dc_top_predictor_16x4_sse2,
                 aom_dc_128_predictor_16x4_sse2, aom_v_predictor_16x4_sse2,
                 aom_h_predictor_16x4_sse2, nullptr, nullptr, nullptr, nullptr)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_SSE2
 
 #if HAVE_SSSE3
@@ -657,6 +676,7 @@ INTRA_PRED_TEST(SSSE3, TX_16X32, nullptr, nullptr, nullptr, nullptr, nullptr,
                 aom_smooth_predictor_16x32_ssse3,
                 aom_smooth_v_predictor_16x32_ssse3,
                 aom_smooth_h_predictor_16x32_ssse3)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(SSSE3, TX_16X64, nullptr, nullptr, nullptr, nullptr, nullptr,
                 nullptr, aom_paeth_predictor_16x64_ssse3,
                 aom_smooth_predictor_16x64_ssse3,
@@ -667,6 +687,7 @@ INTRA_PRED_TEST(SSSE3, TX_16X4, nullptr, nullptr, nullptr, nullptr, nullptr,
                 aom_smooth_predictor_16x4_ssse3,
                 aom_smooth_v_predictor_16x4_ssse3,
                 aom_smooth_h_predictor_16x4_ssse3)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_SSSE3
 
 #if HAVE_AVX2
@@ -679,9 +700,11 @@ INTRA_PRED_TEST(AVX2, TX_16X8, nullptr, nullptr, nullptr, nullptr, nullptr,
 INTRA_PRED_TEST(AVX2, TX_16X32, nullptr, nullptr, nullptr, nullptr, nullptr,
                 nullptr, aom_paeth_predictor_16x32_avx2, nullptr, nullptr,
                 nullptr)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(AVX2, TX_16X64, nullptr, nullptr, nullptr, nullptr, nullptr,
                 nullptr, aom_paeth_predictor_16x64_avx2, nullptr, nullptr,
                 nullptr)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_AVX2
 
 #if HAVE_NEON
@@ -708,6 +731,7 @@ INTRA_PRED_TEST(NEON, TX_16X32, aom_dc_predictor_16x32_neon,
                 aom_smooth_predictor_16x32_neon,
                 aom_smooth_v_predictor_16x32_neon,
                 aom_smooth_h_predictor_16x32_neon)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(NEON, TX_16X4, aom_dc_predictor_16x4_neon,
                 aom_dc_left_predictor_16x4_neon, aom_dc_top_predictor_16x4_neon,
                 aom_dc_128_predictor_16x4_neon, aom_v_predictor_16x4_neon,
@@ -723,6 +747,7 @@ INTRA_PRED_TEST(NEON, TX_16X64, aom_dc_predictor_16x64_neon,
                 aom_smooth_predictor_16x64_neon,
                 aom_smooth_v_predictor_16x64_neon,
                 aom_smooth_h_predictor_16x64_neon)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_NEON
 
 // -----------------------------------------------------------------------------
@@ -746,12 +771,14 @@ INTRA_PRED_TEST(C, TX_32X64, aom_dc_predictor_32x64_c,
                 aom_h_predictor_32x64_c, aom_paeth_predictor_32x64_c,
                 aom_smooth_predictor_32x64_c, aom_smooth_v_predictor_32x64_c,
                 aom_smooth_h_predictor_32x64_c)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(C, TX_32X8, aom_dc_predictor_32x8_c,
                 aom_dc_left_predictor_32x8_c, aom_dc_top_predictor_32x8_c,
                 aom_dc_128_predictor_32x8_c, aom_v_predictor_32x8_c,
                 aom_h_predictor_32x8_c, aom_paeth_predictor_32x8_c,
                 aom_smooth_predictor_32x8_c, aom_smooth_v_predictor_32x8_c,
                 aom_smooth_h_predictor_32x8_c)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #if HAVE_SSE2
 INTRA_PRED_TEST(SSE2, TX_32X32, aom_dc_predictor_32x32_sse2,
@@ -769,10 +796,12 @@ INTRA_PRED_TEST(SSE2, TX_32X64, aom_dc_predictor_32x64_sse2,
                 aom_dc_top_predictor_32x64_sse2,
                 aom_dc_128_predictor_32x64_sse2, aom_v_predictor_32x64_sse2,
                 aom_h_predictor_32x64_sse2, nullptr, nullptr, nullptr, nullptr)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(SSE2, TX_32X8, aom_dc_predictor_32x8_sse2,
                 aom_dc_left_predictor_32x8_sse2, aom_dc_top_predictor_32x8_sse2,
                 aom_dc_128_predictor_32x8_sse2, aom_v_predictor_32x8_sse2,
                 aom_h_predictor_32x8_sse2, nullptr, nullptr, nullptr, nullptr)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_SSE2
 
 #if HAVE_SSSE3
@@ -791,11 +820,13 @@ INTRA_PRED_TEST(SSSE3, TX_32X64, nullptr, nullptr, nullptr, nullptr, nullptr,
                 aom_smooth_predictor_32x64_ssse3,
                 aom_smooth_v_predictor_32x64_ssse3,
                 aom_smooth_h_predictor_32x64_ssse3)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(SSSE3, TX_32X8, nullptr, nullptr, nullptr, nullptr, nullptr,
                 nullptr, aom_paeth_predictor_32x8_ssse3,
                 aom_smooth_predictor_32x8_ssse3,
                 aom_smooth_v_predictor_32x8_ssse3,
                 aom_smooth_h_predictor_32x8_ssse3)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_SSSE3
 
 #if HAVE_AVX2
@@ -844,6 +875,7 @@ INTRA_PRED_TEST(NEON, TX_32X64, aom_dc_predictor_32x64_neon,
                 aom_smooth_predictor_32x64_neon,
                 aom_smooth_v_predictor_32x64_neon,
                 aom_smooth_h_predictor_32x64_neon)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(NEON, TX_32X8, aom_dc_predictor_32x8_neon,
                 aom_dc_left_predictor_32x8_neon, aom_dc_top_predictor_32x8_neon,
                 aom_dc_128_predictor_32x8_neon, aom_v_predictor_32x8_neon,
@@ -851,6 +883,7 @@ INTRA_PRED_TEST(NEON, TX_32X8, aom_dc_predictor_32x8_neon,
                 aom_smooth_predictor_32x8_neon,
                 aom_smooth_v_predictor_32x8_neon,
                 aom_smooth_h_predictor_32x8_neon)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_NEON
 
 // -----------------------------------------------------------------------------
@@ -868,12 +901,14 @@ INTRA_PRED_TEST(C, TX_64X32, aom_dc_predictor_64x32_c,
                 aom_h_predictor_64x32_c, aom_paeth_predictor_64x32_c,
                 aom_smooth_predictor_64x32_c, aom_smooth_v_predictor_64x32_c,
                 aom_smooth_h_predictor_64x32_c)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(C, TX_64X16, aom_dc_predictor_64x16_c,
                 aom_dc_left_predictor_64x16_c, aom_dc_top_predictor_64x16_c,
                 aom_dc_128_predictor_64x16_c, aom_v_predictor_64x16_c,
                 aom_h_predictor_64x16_c, aom_paeth_predictor_64x16_c,
                 aom_smooth_predictor_64x16_c, aom_smooth_v_predictor_64x16_c,
                 aom_smooth_h_predictor_64x16_c)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #if HAVE_SSE2
 INTRA_PRED_TEST(SSE2, TX_64X64, aom_dc_predictor_64x64_sse2,
@@ -886,11 +921,13 @@ INTRA_PRED_TEST(SSE2, TX_64X32, aom_dc_predictor_64x32_sse2,
                 aom_dc_top_predictor_64x32_sse2,
                 aom_dc_128_predictor_64x32_sse2, aom_v_predictor_64x32_sse2,
                 aom_h_predictor_64x32_sse2, nullptr, nullptr, nullptr, nullptr)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(SSE2, TX_64X16, aom_dc_predictor_64x16_sse2,
                 aom_dc_left_predictor_64x16_sse2,
                 aom_dc_top_predictor_64x16_sse2,
                 aom_dc_128_predictor_64x16_sse2, aom_v_predictor_64x16_sse2,
                 aom_h_predictor_64x16_sse2, nullptr, nullptr, nullptr, nullptr)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif
 
 #if HAVE_SSSE3
@@ -904,11 +941,13 @@ INTRA_PRED_TEST(SSSE3, TX_64X32, nullptr, nullptr, nullptr, nullptr, nullptr,
                 aom_smooth_predictor_64x32_ssse3,
                 aom_smooth_v_predictor_64x32_ssse3,
                 aom_smooth_h_predictor_64x32_ssse3)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(SSSE3, TX_64X16, nullptr, nullptr, nullptr, nullptr, nullptr,
                 nullptr, aom_paeth_predictor_64x16_ssse3,
                 aom_smooth_predictor_64x16_ssse3,
                 aom_smooth_v_predictor_64x16_ssse3,
                 aom_smooth_h_predictor_64x16_ssse3)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif
 
 #if HAVE_AVX2
@@ -924,12 +963,14 @@ INTRA_PRED_TEST(AVX2, TX_64X32, aom_dc_predictor_64x32_avx2,
                 aom_dc_128_predictor_64x32_avx2, aom_v_predictor_64x32_avx2,
                 nullptr, aom_paeth_predictor_64x32_avx2, nullptr, nullptr,
                 nullptr)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(AVX2, TX_64X16, aom_dc_predictor_64x16_avx2,
                 aom_dc_left_predictor_64x16_avx2,
                 aom_dc_top_predictor_64x16_avx2,
                 aom_dc_128_predictor_64x16_avx2, aom_v_predictor_64x16_avx2,
                 nullptr, aom_paeth_predictor_64x16_avx2, nullptr, nullptr,
                 nullptr)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif
 
 #if HAVE_NEON
@@ -949,6 +990,7 @@ INTRA_PRED_TEST(NEON, TX_64X32, aom_dc_predictor_64x32_neon,
                 aom_smooth_predictor_64x32_neon,
                 aom_smooth_v_predictor_64x32_neon,
                 aom_smooth_h_predictor_64x32_neon)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 INTRA_PRED_TEST(NEON, TX_64X16, aom_dc_predictor_64x16_neon,
                 aom_dc_left_predictor_64x16_neon,
                 aom_dc_top_predictor_64x16_neon,
@@ -957,6 +999,7 @@ INTRA_PRED_TEST(NEON, TX_64X16, aom_dc_predictor_64x16_neon,
                 aom_smooth_predictor_64x16_neon,
                 aom_smooth_v_predictor_64x16_neon,
                 aom_smooth_h_predictor_64x16_neon)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_NEON
 
 #if CONFIG_AV1_HIGHBITDEPTH
@@ -1280,6 +1323,8 @@ HIGHBD_INTRA_PRED_TEST(
     aom_highbd_h_predictor_4x8_c, aom_highbd_paeth_predictor_4x8_c,
     aom_highbd_smooth_predictor_4x8_c, aom_highbd_smooth_v_predictor_4x8_c,
     aom_highbd_smooth_h_predictor_4x8_c)
+
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_INTRA_PRED_TEST(
     C, TX_4X16, aom_highbd_dc_predictor_4x16_c,
     aom_highbd_dc_left_predictor_4x16_c, aom_highbd_dc_top_predictor_4x16_c,
@@ -1287,6 +1332,8 @@ HIGHBD_INTRA_PRED_TEST(
     aom_highbd_h_predictor_4x16_c, aom_highbd_paeth_predictor_4x16_c,
     aom_highbd_smooth_predictor_4x16_c, aom_highbd_smooth_v_predictor_4x16_c,
     aom_highbd_smooth_h_predictor_4x16_c)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
+
 #if HAVE_SSE2
 HIGHBD_INTRA_PRED_TEST(SSE2, TX_4X4, aom_highbd_dc_predictor_4x4_sse2,
                        aom_highbd_dc_left_predictor_4x4_sse2,
@@ -1304,6 +1351,7 @@ HIGHBD_INTRA_PRED_TEST(SSE2, TX_4X8, aom_highbd_dc_predictor_4x8_sse2,
                        aom_highbd_h_predictor_4x8_sse2, nullptr, nullptr,
                        nullptr, nullptr)
 #endif
+
 #if HAVE_NEON
 HIGHBD_INTRA_PRED_TEST(NEON, TX_4X4, aom_highbd_dc_predictor_4x4_neon,
                        aom_highbd_dc_left_predictor_4x4_neon,
@@ -1325,6 +1373,7 @@ HIGHBD_INTRA_PRED_TEST(NEON, TX_4X8, aom_highbd_dc_predictor_4x8_neon,
                        aom_highbd_smooth_predictor_4x8_neon,
                        aom_highbd_smooth_v_predictor_4x8_neon,
                        aom_highbd_smooth_h_predictor_4x8_neon)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_INTRA_PRED_TEST(NEON, TX_4X16, aom_highbd_dc_predictor_4x16_neon,
                        aom_highbd_dc_left_predictor_4x16_neon,
                        aom_highbd_dc_top_predictor_4x16_neon,
@@ -1335,6 +1384,7 @@ HIGHBD_INTRA_PRED_TEST(NEON, TX_4X16, aom_highbd_dc_predictor_4x16_neon,
                        aom_highbd_smooth_predictor_4x16_neon,
                        aom_highbd_smooth_v_predictor_4x16_neon,
                        aom_highbd_smooth_h_predictor_4x16_neon)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_NEON
 
 // -----------------------------------------------------------------------------
@@ -1361,6 +1411,7 @@ HIGHBD_INTRA_PRED_TEST(
     aom_highbd_h_predictor_8x16_c, aom_highbd_paeth_predictor_8x16_c,
     aom_highbd_smooth_predictor_8x16_c, aom_highbd_smooth_v_predictor_8x16_c,
     aom_highbd_smooth_h_predictor_8x16_c)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_INTRA_PRED_TEST(
     C, TX_8X32, aom_highbd_dc_predictor_8x32_c,
     aom_highbd_dc_left_predictor_8x32_c, aom_highbd_dc_top_predictor_8x32_c,
@@ -1368,6 +1419,7 @@ HIGHBD_INTRA_PRED_TEST(
     aom_highbd_h_predictor_8x32_c, aom_highbd_paeth_predictor_8x32_c,
     aom_highbd_smooth_predictor_8x32_c, aom_highbd_smooth_v_predictor_8x32_c,
     aom_highbd_smooth_h_predictor_8x32_c)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #if HAVE_SSE2
 HIGHBD_INTRA_PRED_TEST(SSE2, TX_8X8, aom_highbd_dc_predictor_8x8_sse2,
@@ -1429,6 +1481,7 @@ HIGHBD_INTRA_PRED_TEST(NEON, TX_8X16, aom_highbd_dc_predictor_8x16_neon,
                        aom_highbd_smooth_predictor_8x16_neon,
                        aom_highbd_smooth_v_predictor_8x16_neon,
                        aom_highbd_smooth_h_predictor_8x16_neon)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_INTRA_PRED_TEST(NEON, TX_8X32, aom_highbd_dc_predictor_8x32_neon,
                        aom_highbd_dc_left_predictor_8x32_neon,
                        aom_highbd_dc_top_predictor_8x32_neon,
@@ -1439,6 +1492,7 @@ HIGHBD_INTRA_PRED_TEST(NEON, TX_8X32, aom_highbd_dc_predictor_8x32_neon,
                        aom_highbd_smooth_predictor_8x32_neon,
                        aom_highbd_smooth_v_predictor_8x32_neon,
                        aom_highbd_smooth_h_predictor_8x32_neon)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_NEON
 
 // -----------------------------------------------------------------------------
@@ -1465,6 +1519,7 @@ HIGHBD_INTRA_PRED_TEST(
     aom_highbd_h_predictor_16x32_c, aom_highbd_paeth_predictor_16x32_c,
     aom_highbd_smooth_predictor_16x32_c, aom_highbd_smooth_v_predictor_16x32_c,
     aom_highbd_smooth_h_predictor_16x32_c)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_INTRA_PRED_TEST(
     C, TX_16X4, aom_highbd_dc_predictor_16x4_c,
     aom_highbd_dc_left_predictor_16x4_c, aom_highbd_dc_top_predictor_16x4_c,
@@ -1479,6 +1534,7 @@ HIGHBD_INTRA_PRED_TEST(
     aom_highbd_h_predictor_16x64_c, aom_highbd_paeth_predictor_16x64_c,
     aom_highbd_smooth_predictor_16x64_c, aom_highbd_smooth_v_predictor_16x64_c,
     aom_highbd_smooth_h_predictor_16x64_c)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #if HAVE_SSE2
 HIGHBD_INTRA_PRED_TEST(SSE2, TX_16X16, aom_highbd_dc_predictor_16x16_sse2,
@@ -1551,6 +1607,7 @@ HIGHBD_INTRA_PRED_TEST(NEON, TX_16X32, aom_highbd_dc_predictor_16x32_neon,
                        aom_highbd_smooth_predictor_16x32_neon,
                        aom_highbd_smooth_v_predictor_16x32_neon,
                        aom_highbd_smooth_h_predictor_16x32_neon)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_INTRA_PRED_TEST(NEON, TX_16X4, aom_highbd_dc_predictor_16x4_neon,
                        aom_highbd_dc_left_predictor_16x4_neon,
                        aom_highbd_dc_top_predictor_16x4_neon,
@@ -1571,6 +1628,7 @@ HIGHBD_INTRA_PRED_TEST(NEON, TX_16X64, aom_highbd_dc_predictor_16x64_neon,
                        aom_highbd_smooth_predictor_16x64_neon,
                        aom_highbd_smooth_v_predictor_16x64_neon,
                        aom_highbd_smooth_h_predictor_16x64_neon)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_NEON
 
 // -----------------------------------------------------------------------------
@@ -1597,6 +1655,7 @@ HIGHBD_INTRA_PRED_TEST(
     aom_highbd_h_predictor_32x64_c, aom_highbd_paeth_predictor_32x64_c,
     aom_highbd_smooth_predictor_32x64_c, aom_highbd_smooth_v_predictor_32x64_c,
     aom_highbd_smooth_h_predictor_32x64_c)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_INTRA_PRED_TEST(
     C, TX_32X8, aom_highbd_dc_predictor_32x8_c,
     aom_highbd_dc_left_predictor_32x8_c, aom_highbd_dc_top_predictor_32x8_c,
@@ -1604,6 +1663,7 @@ HIGHBD_INTRA_PRED_TEST(
     aom_highbd_h_predictor_32x8_c, aom_highbd_paeth_predictor_32x8_c,
     aom_highbd_smooth_predictor_32x8_c, aom_highbd_smooth_v_predictor_32x8_c,
     aom_highbd_smooth_h_predictor_32x8_c)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #if HAVE_SSE2
 HIGHBD_INTRA_PRED_TEST(SSE2, TX_32X32, aom_highbd_dc_predictor_32x32_sse2,
@@ -1666,6 +1726,7 @@ HIGHBD_INTRA_PRED_TEST(NEON, TX_32X64, aom_highbd_dc_predictor_32x64_neon,
                        aom_highbd_smooth_predictor_32x64_neon,
                        aom_highbd_smooth_v_predictor_32x64_neon,
                        aom_highbd_smooth_h_predictor_32x64_neon)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_INTRA_PRED_TEST(NEON, TX_32X8, aom_highbd_dc_predictor_32x8_neon,
                        aom_highbd_dc_left_predictor_32x8_neon,
                        aom_highbd_dc_top_predictor_32x8_neon,
@@ -1676,6 +1737,7 @@ HIGHBD_INTRA_PRED_TEST(NEON, TX_32X8, aom_highbd_dc_predictor_32x8_neon,
                        aom_highbd_smooth_predictor_32x8_neon,
                        aom_highbd_smooth_v_predictor_32x8_neon,
                        aom_highbd_smooth_h_predictor_32x8_neon)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_NEON
 
 // -----------------------------------------------------------------------------
@@ -1695,6 +1757,7 @@ HIGHBD_INTRA_PRED_TEST(
     aom_highbd_h_predictor_64x32_c, aom_highbd_paeth_predictor_64x32_c,
     aom_highbd_smooth_predictor_64x32_c, aom_highbd_smooth_v_predictor_64x32_c,
     aom_highbd_smooth_h_predictor_64x32_c)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_INTRA_PRED_TEST(
     C, TX_64X16, aom_highbd_dc_predictor_64x16_c,
     aom_highbd_dc_left_predictor_64x16_c, aom_highbd_dc_top_predictor_64x16_c,
@@ -1702,6 +1765,7 @@ HIGHBD_INTRA_PRED_TEST(
     aom_highbd_h_predictor_64x16_c, aom_highbd_paeth_predictor_64x16_c,
     aom_highbd_smooth_predictor_64x16_c, aom_highbd_smooth_v_predictor_64x16_c,
     aom_highbd_smooth_h_predictor_64x16_c)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 
 #if HAVE_NEON
 HIGHBD_INTRA_PRED_TEST(NEON, TX_64X64, aom_highbd_dc_predictor_64x64_neon,
@@ -1724,6 +1788,7 @@ HIGHBD_INTRA_PRED_TEST(NEON, TX_64X32, aom_highbd_dc_predictor_64x32_neon,
                        aom_highbd_smooth_predictor_64x32_neon,
                        aom_highbd_smooth_v_predictor_64x32_neon,
                        aom_highbd_smooth_h_predictor_64x32_neon)
+#if !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 HIGHBD_INTRA_PRED_TEST(NEON, TX_64X16, aom_highbd_dc_predictor_64x16_neon,
                        aom_highbd_dc_left_predictor_64x16_neon,
                        aom_highbd_dc_top_predictor_64x16_neon,
@@ -1734,6 +1799,7 @@ HIGHBD_INTRA_PRED_TEST(NEON, TX_64X16, aom_highbd_dc_predictor_64x16_neon,
                        aom_highbd_smooth_predictor_64x16_neon,
                        aom_highbd_smooth_v_predictor_64x16_neon,
                        aom_highbd_smooth_h_predictor_64x16_neon)
+#endif  // !CONFIG_REALTIME_ONLY || CONFIG_AV1_DECODER
 #endif  // HAVE_NEON
 
 // -----------------------------------------------------------------------------
diff --git a/test/tools_common.sh b/test/tools_common.sh
index 79270ecaa..a2f76546f 100755
--- a/test/tools_common.sh
+++ b/test/tools_common.sh
@@ -17,6 +17,7 @@ AOM_TEST_TOOLS_COMMON_SH=included
 set -e
 devnull='> /dev/null 2>&1'
 AOM_TEST_PREFIX=""
+TOOLS_COMMON_DIR=$(cd "$(dirname "$0")"; pwd)
 
 elog() {
   echo "$@" 1>&2
@@ -90,8 +91,7 @@ cmake_version() {
 # version used by the cmake build when git is unavailable.
 source_version() {
   if git --version > /dev/null 2>&1; then
-    (cd "$(dirname "${0}")"
-    git describe)
+    git -C "${TOOLS_COMMON_DIR}" describe
   else
     cmake_version
   fi
diff --git a/test/variance_test.cc b/test/variance_test.cc
index 6f98ae4ad..c61c12ccc 100644
--- a/test/variance_test.cc
+++ b/test/variance_test.cc
@@ -89,14 +89,6 @@ static void RoundHighBitDepth(int bit_depth, int64_t *se, uint64_t *sse) {
   }
 }
 
-static unsigned int mb_ss_ref(const int16_t *src) {
-  unsigned int res = 0;
-  for (int i = 0; i < 256; ++i) {
-    res += src[i] * src[i];
-  }
-  return res;
-}
-
 /* Note:
  *  Our codebase calculates the "diff" value in the variance algorithm by
  *  (src - ref).
@@ -344,6 +336,7 @@ static uint32_t obmc_subpel_variance_ref(const uint8_t *pre, int l2w, int l2h,
 
 ////////////////////////////////////////////////////////////////////////////////
 
+#if !CONFIG_REALTIME_ONLY
 class SumOfSquaresTest : public ::testing::TestWithParam<SumOfSquaresFunction> {
  public:
   SumOfSquaresTest() : func_(GetParam()) {}
@@ -370,6 +363,14 @@ void SumOfSquaresTest::ConstTest() {
   }
 }
 
+unsigned int mb_ss_ref(const int16_t *src) {
+  unsigned int res = 0;
+  for (int i = 0; i < 256; ++i) {
+    res += src[i] * src[i];
+  }
+  return res;
+}
+
 void SumOfSquaresTest::RefTest() {
   int16_t mem[256];
   for (int i = 0; i < 100; ++i) {
@@ -383,6 +384,7 @@ void SumOfSquaresTest::RefTest() {
     EXPECT_EQ(expected, res);
   }
 }
+#endif  // !CONFIG_REALTIME_ONLY
 
 ////////////////////////////////////////////////////////////////////////////////
 // Encapsulating struct to store the function to test along with
@@ -1729,8 +1731,10 @@ TEST_P(GetSseSum16x16DualTest, RefMseSum) { RefTestSseSumDual(); }
 TEST_P(GetSseSum16x16DualTest, MinSseSum) { MinTestSseSumDual(); }
 TEST_P(GetSseSum16x16DualTest, MaxMseSum) { MaxTestSseSumDual(); }
 TEST_P(GetSseSum16x16DualTest, DISABLED_Speed) { SseSum_SpeedTestDual(); }
+#if !CONFIG_REALTIME_ONLY
 TEST_P(SumOfSquaresTest, Const) { ConstTest(); }
 TEST_P(SumOfSquaresTest, Ref) { RefTest(); }
+#endif  // !CONFIG_REALTIME_ONLY
 TEST_P(AvxSubpelVarianceTest, Ref) { RefTest(); }
 TEST_P(AvxSubpelVarianceTest, ExtremeRef) { ExtremeRefTest(); }
 TEST_P(AvxSubpelVarianceTest, DISABLED_Speed) { SpeedTest(); }
@@ -1756,8 +1760,10 @@ INSTANTIATE_TEST_SUITE_P(
                       Mse16xHParams(2, 3, &aom_mse_16xh_16bit_c, 8),
                       Mse16xHParams(2, 2, &aom_mse_16xh_16bit_c, 8)));
 
+#if !CONFIG_REALTIME_ONLY
 INSTANTIATE_TEST_SUITE_P(C, SumOfSquaresTest,
                          ::testing::Values(aom_get_mb_ss_c));
+#endif  // !CONFIG_REALTIME_ONLY
 
 typedef TestParams<VarianceMxNFunc> MseParams;
 INSTANTIATE_TEST_SUITE_P(C, AvxMseTest,
@@ -2729,8 +2735,10 @@ INSTANTIATE_TEST_SUITE_P(
                       Mse16xHParams(2, 3, &aom_mse_16xh_16bit_sse2, 8),
                       Mse16xHParams(2, 2, &aom_mse_16xh_16bit_sse2, 8)));
 
+#if !CONFIG_REALTIME_ONLY
 INSTANTIATE_TEST_SUITE_P(SSE2, SumOfSquaresTest,
                          ::testing::Values(aom_get_mb_ss_sse2));
+#endif  // !CONFIG_REALTIME_ONLY
 
 INSTANTIATE_TEST_SUITE_P(SSE2, AvxMseTest,
                          ::testing::Values(MseParams(4, 4, &aom_mse16x16_sse2),
@@ -3411,8 +3419,10 @@ INSTANTIATE_TEST_SUITE_P(
                       Mse16xHParams(2, 3, &aom_mse_16xh_16bit_neon, 8),
                       Mse16xHParams(2, 2, &aom_mse_16xh_16bit_neon, 8)));
 
+#if !CONFIG_REALTIME_ONLY
 INSTANTIATE_TEST_SUITE_P(NEON, SumOfSquaresTest,
                          ::testing::Values(aom_get_mb_ss_neon));
+#endif  // !CONFIG_REALTIME_ONLY
 
 INSTANTIATE_TEST_SUITE_P(NEON, AvxMseTest,
                          ::testing::Values(MseParams(3, 3, &aom_mse8x8_neon),
diff --git a/third_party/vector/README.libaom b/third_party/vector/README.libaom
index 729446dbc..4b6e4ddd8 100644
--- a/third_party/vector/README.libaom
+++ b/third_party/vector/README.libaom
@@ -14,3 +14,4 @@ Local Modifications:
 naming convention.
 2. Removed non-global functions from vector.h.
 3. Made all non-global functions in vector.c static.
+4. Commented out unused code.
diff --git a/third_party/vector/vector.c b/third_party/vector/vector.c
index 2295b8f08..0416f3817 100644
--- a/third_party/vector/vector.c
+++ b/third_party/vector/vector.c
@@ -36,20 +36,24 @@ static bool _vector_should_grow(Vector *vector) {
   return vector->size == vector->capacity;
 }
 
+#if 0
 static bool _vector_should_shrink(Vector *vector) {
   assert(vector->size <= vector->capacity);
   return vector->size == vector->capacity * VECTOR_SHRINK_THRESHOLD;
 }
+#endif  // 0
 
 static void *_vector_offset(Vector *vector, size_t index) {
   // return vector->data + (index * vector->element_size);
   return (unsigned char *)vector->data + (index * vector->element_size);
 }
 
+#if 0
 static const void *_vector_const_offset(const Vector *vector, size_t index) {
   // return vector->data + (index * vector->element_size);
   return (unsigned char *)vector->data + (index * vector->element_size);
 }
+#endif  // 0
 
 static void _vector_assign(Vector *vector, size_t index, void *element) {
   /* Insert the element */
@@ -57,6 +61,7 @@ static void _vector_assign(Vector *vector, size_t index, void *element) {
   memcpy(offset, element, vector->element_size);
 }
 
+#if 0
 static int _vector_move_right(Vector *vector, size_t index) {
   assert(vector->size < vector->capacity);
 
@@ -103,6 +108,7 @@ static void _vector_move_left(Vector *vector, size_t index) {
   memmove(offset, (unsigned char *)offset + vector->element_size,
           right_elements_in_bytes);
 }
+#endif  // 0
 
 static int _vector_reallocate(Vector *vector, size_t new_capacity) {
   size_t new_capacity_in_bytes;
@@ -150,11 +156,13 @@ static int _vector_adjust_capacity(Vector *vector) {
                             MAX(1, vector->size * VECTOR_GROWTH_FACTOR));
 }
 
+#if 0
 static void _vector_swap(size_t *first, size_t *second) {
   size_t temp = *first;
   *first = *second;
   *second = temp;
 }
+#endif  // 0
 
 int aom_vector_setup(Vector *vector, size_t capacity, size_t element_size) {
   assert(vector != NULL);
@@ -169,6 +177,7 @@ int aom_vector_setup(Vector *vector, size_t capacity, size_t element_size) {
   return vector->data == NULL ? VECTOR_ERROR : VECTOR_SUCCESS;
 }
 
+#if 0
 int aom_vector_copy(Vector *destination, Vector *source) {
   assert(destination != NULL);
   assert(source != NULL);
@@ -251,6 +260,7 @@ int aom_vector_swap(Vector *destination, Vector *source) {
 
   return VECTOR_SUCCESS;
 }
+#endif  // 0
 
 int aom_vector_destroy(Vector *vector) {
   assert(vector != NULL);
@@ -281,6 +291,7 @@ int aom_vector_push_back(Vector *vector, void *element) {
   return VECTOR_SUCCESS;
 }
 
+#if 0
 int aom_vector_push_front(Vector *vector, void *element) {
   return aom_vector_insert(vector, 0, element);
 }
@@ -402,6 +413,7 @@ void *aom_vector_front(Vector *vector) { return aom_vector_get(vector, 0); }
 void *aom_vector_back(Vector *vector) {
   return aom_vector_get(vector, vector->size - 1);
 }
+#endif  // 0
 
 /* Information */
 
@@ -413,6 +425,7 @@ size_t aom_vector_byte_size(const Vector *vector) {
   return vector->size * vector->element_size;
 }
 
+#if 0
 size_t aom_vector_free_space(const Vector *vector) {
   return vector->capacity - vector->size;
 }
@@ -450,13 +463,16 @@ int aom_vector_reserve(Vector *vector, size_t minimum_capacity) {
 int aom_vector_shrink_to_fit(Vector *vector) {
   return _vector_reallocate(vector, vector->size);
 }
+#endif  // 0
 
 /* Iterators */
 Iterator aom_vector_begin(Vector *vector) { return aom_vector_iterator(vector, 0); }
 
+#if 0
 Iterator aom_vector_end(Vector *vector) {
   return aom_vector_iterator(vector, vector->size);
 }
+#endif  // 0
 
 Iterator aom_vector_iterator(Vector *vector, size_t index) {
   Iterator iterator = { NULL, 0 };
@@ -476,6 +492,7 @@ Iterator aom_vector_iterator(Vector *vector, size_t index) {
 
 void *aom_iterator_get(Iterator *iterator) { return iterator->pointer; }
 
+#if 0
 int aom_iterator_erase(Vector *vector, Iterator *iterator) {
   size_t index = aom_iterator_index(vector, iterator);
 
@@ -487,6 +504,7 @@ int aom_iterator_erase(Vector *vector, Iterator *iterator) {
 
   return VECTOR_SUCCESS;
 }
+#endif  // 0
 
 void aom_iterator_increment(Iterator *iterator) {
   assert(iterator != NULL);
@@ -495,6 +513,7 @@ void aom_iterator_increment(Iterator *iterator) {
       (unsigned char *)iterator->pointer + iterator->element_size;
 }
 
+#if 0
 void aom_iterator_decrement(Iterator *iterator) {
   assert(iterator != NULL);
   // iterator->pointer -= iterator->element_size;
@@ -538,3 +557,4 @@ size_t aom_iterator_index(Vector *vector, Iterator *iterator) {
   return ((unsigned char *)iterator->pointer - (unsigned char *)vector->data) /
          vector->element_size;
 }
+#endif  // 0
diff --git a/third_party/vector/vector.h b/third_party/vector/vector.h
index acc70fe09..f91acddb7 100644
--- a/third_party/vector/vector.h
+++ b/third_party/vector/vector.h
@@ -59,6 +59,7 @@ typedef struct Iterator {
 /* Constructor */
 int aom_vector_setup(Vector *vector, size_t capacity, size_t element_size);
 
+#if 0
 /* Copy Constructor */
 int aom_vector_copy(Vector *destination, Vector *source);
 
@@ -72,16 +73,20 @@ int aom_vector_move(Vector *destination, Vector *source);
 int aom_vector_move_assign(Vector *destination, Vector *source);
 
 int aom_vector_swap(Vector *destination, Vector *source);
+#endif  // 0
 
 /* Destructor */
 int aom_vector_destroy(Vector *vector);
 
 /* Insertion */
 int aom_vector_push_back(Vector *vector, void *element);
+#if 0
 int aom_vector_push_front(Vector *vector, void *element);
 int aom_vector_insert(Vector *vector, size_t index, void *element);
 int aom_vector_assign(Vector *vector, size_t index, void *element);
+#endif  // 0
 
+#if 0
 /* Deletion */
 int aom_vector_pop_back(Vector *vector);
 int aom_vector_pop_front(Vector *vector);
@@ -95,10 +100,12 @@ void *aom_vector_front(Vector *vector);
 void *aom_vector_back(Vector *vector);
 #define VECTOR_GET_AS(type, aom_vector_pointer, index) \
   *((type *)aom_vector_get((aom_vector_pointer), (index)))
+#endif  // 0
 
 /* Information */
 bool aom_vector_is_initialized(const Vector *vector);
 size_t aom_vector_byte_size(const Vector *vector);
+#if 0
 size_t aom_vector_free_space(const Vector *vector);
 bool aom_vector_is_empty(const Vector *vector);
 
@@ -106,18 +113,24 @@ bool aom_vector_is_empty(const Vector *vector);
 int aom_vector_resize(Vector *vector, size_t new_size);
 int aom_vector_reserve(Vector *vector, size_t minimum_capacity);
 int aom_vector_shrink_to_fit(Vector *vector);
+#endif  // 0
 
 /* Iterators */
 Iterator aom_vector_begin(Vector *vector);
+#if 0
 Iterator aom_vector_end(Vector *vector);
+#endif  // 0
 Iterator aom_vector_iterator(Vector *vector, size_t index);
 
 void *aom_iterator_get(Iterator *iterator);
+#if 0
 #define ITERATOR_GET_AS(type, iterator) *((type *)aom_iterator_get((iterator)))
 
 int aom_iterator_erase(Vector *vector, Iterator *iterator);
+#endif  // 0
 
 void aom_iterator_increment(Iterator *iterator);
+#if 0
 void aom_iterator_decrement(Iterator *iterator);
 
 void *aom_iterator_next(Iterator *iterator);
@@ -134,5 +147,6 @@ size_t aom_iterator_index(Vector *vector, Iterator *iterator);
       end = aom_vector_end((aom_vector_pointer));                        \
        !aom_iterator_equals(&(iterator_name), &end);                     \
        aom_iterator_increment(&(iterator_name)))
+#endif  // 0
 
 #endif /* VECTOR_H */
```

